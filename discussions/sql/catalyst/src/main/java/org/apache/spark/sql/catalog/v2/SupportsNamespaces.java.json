[
  {
    "id" : "ff8e22bb-2c8a-4917-83ef-e0b807c6c2dd",
    "prId" : 24560,
    "prUrl" : "https://github.com/apache/spark/pull/24560#pullrequestreview-267285212",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "37be31b0-2418-44c0-83e1-1e96395d32b7",
        "parentId" : null,
        "authorId" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "body" : "Should this list the top-level namespaces or within the `defaultNamespace`? If it's also just going to return `a`, why does it need to return an array of arrays instead of just an array?",
        "createdAt" : "2019-07-25T22:18:53Z",
        "updatedAt" : "2019-08-04T21:21:40Z",
        "lastEditedBy" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "tags" : [
        ]
      },
      {
        "id" : "8c9ecbec-45dc-4551-b502-61444389dace",
        "parentId" : "37be31b0-2418-44c0-83e1-1e96395d32b7",
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "It should list top-level namespaces. The default namespace is the one used as Spark's current namespace when the catalog is the default and should not change the behavior of the catalog.\r\n\r\nIt returns an array of arrays so you can pass each result into `listNamespaces(Array[String])`. `Array[String]` is the type that we consistently use for a namespace, so I think it is correct to return it here.",
        "createdAt" : "2019-07-26T16:22:45Z",
        "updatedAt" : "2019-08-04T21:21:40Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      },
      {
        "id" : "2ef9f398-7612-4c52-9fe4-88de80548e63",
        "parentId" : "37be31b0-2418-44c0-83e1-1e96395d32b7",
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "Adding to my comment about listing top-level namespaces: if the default namespace changed the catalog's behavior, we would have to document exactly how it should change the behavior and have a way to configure it.\r\n\r\nI like keeping this simple instead, so these methods do the same thing every time and Spark doesn't require the plugin to maintain a current namespace as state.",
        "createdAt" : "2019-07-26T16:28:06Z",
        "updatedAt" : "2019-08-04T21:21:40Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      }
    ],
    "commit" : "44afb5ca97a02f2353669bfb16cb7733c9d53b17",
    "line" : 55,
    "diffHunk" : "@@ -1,1 +53,57 @@\n  /**\n   * List top-level namespaces from the catalog.\n   * <p>\n   * If an object such as a table, view, or function exists, its parent namespaces must also exist"
  },
  {
    "id" : "935f9fa5-a8f0-4dbc-80ab-820f6f204c52",
    "prId" : 24560,
    "prUrl" : "https://github.com/apache/spark/pull/24560#pullrequestreview-267284285",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "48fb540c-ca75-43b6-b81e-03e852f1cd47",
        "parentId" : null,
        "authorId" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "body" : "can we not have a boolean flag here called `cascade` that deletes all the Namespaces and Tables under this namespace if it is not empty?",
        "createdAt" : "2019-07-26T00:37:41Z",
        "updatedAt" : "2019-08-04T21:21:40Z",
        "lastEditedBy" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "tags" : [
        ]
      },
      {
        "id" : "a1a9d5e4-ebd2-4c9e-af7b-c72fa77259a8",
        "parentId" : "48fb540c-ca75-43b6-b81e-03e852f1cd47",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "+1",
        "createdAt" : "2019-07-26T02:23:14Z",
        "updatedAt" : "2019-08-04T21:21:40Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "852b2a3c-263c-4b7c-b336-6a0dbc637695",
        "parentId" : "48fb540c-ca75-43b6-b81e-03e852f1cd47",
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "We can consider adding it later. We should decide whether we want to require implementations to build that, or whether that should be done by Spark.",
        "createdAt" : "2019-07-26T16:15:44Z",
        "updatedAt" : "2019-08-04T21:21:40Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      },
      {
        "id" : "0d8cda3e-cdfd-4392-bec3-b1279f42a353",
        "parentId" : "48fb540c-ca75-43b6-b81e-03e852f1cd47",
        "authorId" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "body" : "I see what you mean. Spark can list all tables within all namespaces and delete them if cascade = true, and not have the catalog do it. I can see the pros of that. I think something like that opens you up to race conditions too though. This is fine for now, just wanted to bring it up. If there's a real need (ask) we can always add it later",
        "createdAt" : "2019-07-26T16:23:29Z",
        "updatedAt" : "2019-08-04T21:21:40Z",
        "lastEditedBy" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "tags" : [
        ]
      },
      {
        "id" : "5bbc759c-49f7-41f5-97d0-55b288c1720c",
        "parentId" : "48fb540c-ca75-43b6-b81e-03e852f1cd47",
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "I'm generally in favor of Spark taking care of operations like these. Otherwise it puts a lot of responsibility on catalog plugins and we want to avoid making those too complex. Required complexity in a plugin will lead to buggy implementations and unreliable behavior across plugins.",
        "createdAt" : "2019-07-26T16:26:07Z",
        "updatedAt" : "2019-08-04T21:21:40Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      }
    ],
    "commit" : "44afb5ca97a02f2353669bfb16cb7733c9d53b17",
    "line" : 144,
    "diffHunk" : "@@ -1,1 +142,146 @@   * @throws UnsupportedOperationException If drop is not a supported operation\n   */\n  boolean dropNamespace(String[] namespace) throws NoSuchNamespaceException;\n}"
  }
]