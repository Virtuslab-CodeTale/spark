[
  {
    "id" : "7f926042-d7f0-4b4d-bb5e-75c091874a46",
    "prId" : 29191,
    "prUrl" : "https://github.com/apache/spark/pull/29191#pullrequestreview-453835058",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a91a68e8-4558-4e68-bd10-a29ec66abf7c",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "shall we use consistent name `asCaseSensitiveMap`?",
        "createdAt" : "2020-07-23T03:34:07Z",
        "updatedAt" : "2020-07-23T06:10:34Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "65baf786-67a8-4d5f-9f8e-7d477df42339",
        "parentId" : "a91a68e8-4558-4e68-bd10-a29ec66abf7c",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Just for my understand. In that case, among all the existing `extraOptions.toMap`s, do I need to rename only `AppendData.byName` instance?\r\n```\r\nsql/core/src/main/scala/org/apache/spark/sql/DataFrameReader.scala:    val params = extraOptions.toMap ++ connectionProperties.asScala.toMap\r\nsql/core/src/main/scala/org/apache/spark/sql/DataFrameReader.scala:      extraOptions.toMap,\r\nsql/core/src/main/scala/org/apache/spark/sql/DataFrameReader.scala:      extraOptions.toMap,\r\nsql/core/src/main/scala/org/apache/spark/sql/DataFrameWriter.scala:              AppendData.byName(relation, df.logicalPlan, extraOptions.toMap)\r\nsql/core/src/main/scala/org/apache/spark/sql/DataFrameWriter.scala:                relation, df.logicalPlan, Literal(true), extraOptions.toMap)\r\nsql/core/src/main/scala/org/apache/spark/sql/DataFrameWriter.scala:                  extraOptions.toMap,\r\nsql/core/src/main/scala/org/apache/spark/sql/DataFrameWriter.scala:        options = extraOptions.toMap).planForWriting(mode, df.logicalPlan)\r\nsql/core/src/main/scala/org/apache/spark/sql/DataFrameWriter.scala:        AppendData.byPosition(table, df.logicalPlan, extraOptions.toMap)\r\nsql/core/src/main/scala/org/apache/spark/sql/DataFrameWriter.scala:          OverwritePartitionsDynamic.byPosition(table, df.logicalPlan, extraOptions.toMap)\r\nsql/core/src/main/scala/org/apache/spark/sql/DataFrameWriter.scala:          OverwriteByExpression.byPosition(table, df.logicalPlan, Literal(true), extraOptions.toMap)\r\nsql/core/src/main/scala/org/apache/spark/sql/DataFrameWriter.scala:        AppendData.byName(v2Relation, df.logicalPlan, extraOptions.toMap)\r\nsql/core/src/main/scala/org/apache/spark/sql/DataFrameWriter.scala:          extraOptions.toMap,\r\nsql/core/src/main/scala/org/apache/spark/sql/DataFrameWriter.scala:          extraOptions.toMap,\r\nsql/core/src/main/scala/org/apache/spark/sql/DataFrameWriter.scala:    val storage = DataSource.buildStorageFormatFromOptions(extraOptions.toMap)\r\nsql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamReader.scala:      options = extraOptions.toMap)\r\nsql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamWriter.scala:        extraOptions.toMap,\r\nsql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamWriter.scala:        extraOptions.toMap,\r\nsql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamWriter.scala:        extraOptions.toMap,\r\nsql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamWriter.scala:        extraOptions.toMap,\r\nsql/core/src/main/scala/org/apache/spark/sql/streaming/DataStreamWriter.scala:      options = extraOptions.toMap,\r\n```",
        "createdAt" : "2020-07-23T04:23:22Z",
        "updatedAt" : "2020-07-23T06:10:34Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "1185c090-7986-418d-9dd7-1fc34d551e24",
        "parentId" : "a91a68e8-4558-4e68-bd10-a29ec66abf7c",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "that's too many. let's keep `toMap` then",
        "createdAt" : "2020-07-23T04:54:30Z",
        "updatedAt" : "2020-07-23T06:10:34Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "81ac6c614c16e8d3c43ff14888216a797df04275",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +54,58 @@  }\n\n  def toMap: Map[String, T] = originalMap\n}\n"
  },
  {
    "id" : "febfdf76-2f9d-498a-953c-cc3d0ec6a99b",
    "prId" : 29172,
    "prUrl" : "https://github.com/apache/spark/pull/29172#pullrequestreview-452054695",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "27f1f28d-17f2-4dd4-abcc-65e3aacdc1da",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Please note that this is basically reusing the same logic for `deletion` at line 49.",
        "createdAt" : "2020-07-21T00:56:50Z",
        "updatedAt" : "2020-07-21T04:50:45Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "a6dc25dcacd34b315bf67d8bb1d28e52f2dec3bb",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +41,45 @@\n  override def +[B1 >: T](kv: (String, B1)): Map[String, B1] = {\n    new CaseInsensitiveMap(originalMap.filter(!_._1.equalsIgnoreCase(kv._1)) + kv)\n  }\n"
  }
]