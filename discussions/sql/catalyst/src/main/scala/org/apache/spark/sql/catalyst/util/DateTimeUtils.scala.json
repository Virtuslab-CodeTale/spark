[
  {
    "id" : "6030794a-0dfa-4ce7-a4e7-6f7ee530c084",
    "prId" : 32959,
    "prUrl" : "https://github.com/apache/spark/pull/32959#pullrequestreview-699672586",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c26a8b86-6b67-44be-b4e4-04395a2992a5",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "for `+12:12:12` do we fail or simply ignore the `+`?",
        "createdAt" : "2021-07-02T07:43:04Z",
        "updatedAt" : "2021-07-02T07:43:05Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "c0cbbf63-1d67-4715-9a42-02aae348c9cc",
        "parentId" : "c26a8b86-6b67-44be-b4e4-04395a2992a5",
        "authorId" : "29c89154-22f7-40ee-b295-115a55734d6c",
        "body" : "we simply ignore the `+`",
        "createdAt" : "2021-07-02T08:39:31Z",
        "updatedAt" : "2021-07-02T08:39:31Z",
        "lastEditedBy" : "29c89154-22f7-40ee-b295-115a55734d6c",
        "tags" : [
        ]
      },
      {
        "id" : "af877d78-37ab-428b-946f-ab7e50559766",
        "parentId" : "c26a8b86-6b67-44be-b4e4-04395a2992a5",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I think failing is better? Since we won't respect `-` here anyway.",
        "createdAt" : "2021-07-02T12:05:50Z",
        "updatedAt" : "2021-07-02T12:05:50Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "9ac87ba2-0867-4f4b-8e12-0150d813092c",
        "parentId" : "c26a8b86-6b67-44be-b4e4-04395a2992a5",
        "authorId" : "29c89154-22f7-40ee-b295-115a55734d6c",
        "body" : "actually the we will respect the `-`, for example:\r\n```\r\nscala> sql(\"select date'-12-12-12'\").show()\r\n+------------------+\r\n|DATE '-0012-12-12'|\r\n+------------------+\r\n|       -0012-12-12|\r\n+------------------+\r\n\r\nscala> sql(\"select date'+12-12-12'\").show()\r\n+-----------------+\r\n|DATE '0012-12-12'|\r\n+-----------------+\r\n|       0012-12-12|\r\n+-----------------+\r\n\r\n```",
        "createdAt" : "2021-07-05T02:48:07Z",
        "updatedAt" : "2021-07-05T02:48:07Z",
        "lastEditedBy" : "29c89154-22f7-40ee-b295-115a55734d6c",
        "tags" : [
        ]
      },
      {
        "id" : "9e93ba1e-eb7d-439e-b1e7-3a99ec964bc2",
        "parentId" : "c26a8b86-6b67-44be-b4e4-04395a2992a5",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "are you sure? It seems we only consider the sign for year: https://github.com/apache/spark/pull/32959/files#diff-f59286f6576476c68ae766de0430fc41d424804dbb0658e0dccd998b0d9a756dR404",
        "createdAt" : "2021-07-06T04:58:54Z",
        "updatedAt" : "2021-07-06T04:58:54Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "5c61b2c4-d1c3-4c33-90a0-beb0572e21ef",
        "parentId" : "c26a8b86-6b67-44be-b4e4-04395a2992a5",
        "authorId" : "29c89154-22f7-40ee-b295-115a55734d6c",
        "body" : "I see. good catch.",
        "createdAt" : "2021-07-06T08:24:40Z",
        "updatedAt" : "2021-07-06T08:24:40Z",
        "lastEditedBy" : "29c89154-22f7-40ee-b295-115a55734d6c",
        "tags" : [
        ]
      }
    ],
    "commit" : "538463a405c2eac1ddfccba8d82ca846f81e5a22",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +231,235 @@   * `[+-]yyyy*-[m]m-[d]d [h]h:[m]m:[s]s.[ms][ms][ms][us][us][us][zone_id]`\n   * `[+-]yyyy*-[m]m-[d]dT[h]h:[m]m:[s]s.[ms][ms][ms][us][us][us][zone_id]`\n   * `[h]h:[m]m:[s]s.[ms][ms][ms][us][us][us][zone_id]`\n   * `T[h]h:[m]m:[s]s.[ms][ms][ms][us][us][us][zone_id]`\n   *"
  },
  {
    "id" : "a06752c3-bfc2-4f57-8305-142a2ff49d6a",
    "prId" : 32959,
    "prUrl" : "https://github.com/apache/spark/pull/32959#pullrequestreview-702361484",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8d601ae4-ec6b-4303-810f-dbea949522fc",
        "parentId" : null,
        "authorId" : "29c89154-22f7-40ee-b295-115a55734d6c",
        "body" : "segments except year are allowed to have 0 digits before this PR. so I didn't do zero checks for these segments.\r\nfor example, before and after this PR, the below query is valid:\r\n```sql\r\nselect cast('12::' as timestamp); -- output: 2021-07-07 12:00:00\r\nselect cast('T' as timestamp); -- output: 2021-07-07 00:00:00\r\n```",
        "createdAt" : "2021-07-07T03:36:24Z",
        "updatedAt" : "2021-07-07T04:58:50Z",
        "lastEditedBy" : "29c89154-22f7-40ee-b295-115a55734d6c",
        "tags" : [
        ]
      },
      {
        "id" : "5ab0c04f-e5f4-4022-88ee-bc0c037acb54",
        "parentId" : "8d601ae4-ec6b-4303-810f-dbea949522fc",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "good catch, we should fail it. Let's do it in another PR.",
        "createdAt" : "2021-07-08T18:07:32Z",
        "updatedAt" : "2021-07-08T18:07:33Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "538463a405c2eac1ddfccba8d82ca846f81e5a22",
    "line" : 29,
    "diffHunk" : "@@ -1,1 +255,259 @@      // For the nanosecond part, more than 6 digits is allowed, but will be truncated.\n      segment == 6 || (segment == 0 && digits >= 4 && digits <= maxDigitsYear) ||\n        (segment != 0 && segment != 6 && digits <= 2)\n    }\n    if (s == null || s.trimAll().numBytes() == 0) {"
  },
  {
    "id" : "7b646c32-8e30-4474-b3a2-0cd48658ccd8",
    "prId" : 32839,
    "prUrl" : "https://github.com/apache/spark/pull/32839#pullrequestreview-680269083",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2a768081-8606-4263-83e8-cea20f6fa001",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "We know `sec` in advance, so, we can calculate `us` in advance too. Please, fold the arithmetic expression. ",
        "createdAt" : "2021-06-09T13:24:52Z",
        "updatedAt" : "2021-06-09T13:24:53Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "b135c721-5966-4ada-9156-59040e249ee5",
        "parentId" : "2a768081-8606-4263-83e8-cea20f6fa001",
        "authorId" : "87a71d79-d4d2-4f2c-8e9a-be56cf39b035",
        "body" : "Are you sure? as us might be (secs+1) or secs depend on case",
        "createdAt" : "2021-06-09T14:16:43Z",
        "updatedAt" : "2021-06-09T14:16:44Z",
        "lastEditedBy" : "87a71d79-d4d2-4f2c-8e9a-be56cf39b035",
        "tags" : [
        ]
      },
      {
        "id" : "006c2a2a-fd5a-4088-92ef-ecd4182191fb",
        "parentId" : "2a768081-8606-4263-83e8-cea20f6fa001",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "`Math.floorDiv(Long.MinValue, MICROS_PER_SECOND)` is constant, right?",
        "createdAt" : "2021-06-09T14:33:15Z",
        "updatedAt" : "2021-06-09T14:33:16Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "1e9682f8-95f0-43bb-a03c-f0805ebad831",
        "parentId" : "2a768081-8606-4263-83e8-cea20f6fa001",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I think it's rare to hit this branch and creating a constant for it is not necessary.",
        "createdAt" : "2021-06-09T15:20:41Z",
        "updatedAt" : "2021-06-09T15:20:41Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "042084de-d679-47b9-be06-e17f091003d5",
        "parentId" : "2a768081-8606-4263-83e8-cea20f6fa001",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "I mostly care of the comparison:\r\n```scala\r\nsecs == Math.floorDiv(Long.MinValue, MICROS_PER_SECOND)\r\n```\r\nwhich is performed per every input. Probably, JVM could optimize it but I would help to it and introduced a constant.",
        "createdAt" : "2021-06-09T15:40:15Z",
        "updatedAt" : "2021-06-09T15:40:15Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "de64f3a5-e96b-4cf0-a75f-425caff95432",
        "parentId" : "2a768081-8606-4263-83e8-cea20f6fa001",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Yea this should be a constant, but after we enter the if branch and reach here, it's rare and I'm OK to not have a constant for `val us = ...`",
        "createdAt" : "2021-06-09T16:04:36Z",
        "updatedAt" : "2021-06-09T16:04:36Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "ea7821f3-b668-4477-a496-105d8c63acdb",
        "parentId" : "2a768081-8606-4263-83e8-cea20f6fa001",
        "authorId" : "87a71d79-d4d2-4f2c-8e9a-be56cf39b035",
        "body" : "Done, thanks",
        "createdAt" : "2021-06-10T01:27:27Z",
        "updatedAt" : "2021-06-10T01:27:27Z",
        "lastEditedBy" : "87a71d79-d4d2-4f2c-8e9a-be56cf39b035",
        "tags" : [
        ]
      }
    ],
    "commit" : "134c356cb004283d477f1ce1a98cff97c194fc3f",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +391,395 @@    val secs = instant.getEpochSecond\n    if (secs == MIN_SECONDS) {\n      val us = Math.multiplyExact(secs + 1, MICROS_PER_SECOND)\n      Math.addExact(us, NANOSECONDS.toMicros(instant.getNano) - MICROS_PER_SECOND)\n    } else {"
  },
  {
    "id" : "c0c916ca-96cb-498a-8590-9cb48dae4765",
    "prId" : 31624,
    "prUrl" : "https://github.com/apache/spark/pull/31624#pullrequestreview-597006083",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5caec95e-3bec-4d68-81a8-80249867dd56",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "This change affects all the datetime related functionalities, so I think we need more tests and document updates.",
        "createdAt" : "2021-02-24T02:22:37Z",
        "updatedAt" : "2021-02-25T05:44:36Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "41e0cb063d06d621049aa7103458b33fcf3fd098",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +51,55 @@  val TIMEZONE_OPTION = \"timeZone\"\n\n  def getZoneId(timeZoneId: String): ZoneId = {\n    // To support the (+|-)h:mm format because it was supported before Spark 3.0.\n    ZoneId.of(timeZoneId.replaceFirst(\"(\\\\+|\\\\-)(\\\\d):\", \"$10$2:\"), ZoneId.SHORT_IDS)"
  },
  {
    "id" : "baa1e657-fc40-461c-91ca-c46c46c2b1e6",
    "prId" : 30687,
    "prUrl" : "https://github.com/apache/spark/pull/30687#pullrequestreview-550073041",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9b6be110-765d-424e-8471-629a33e7f5dd",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "can you also update `stringToTimestampAnsi` to follow this code style?",
        "createdAt" : "2020-12-11T06:53:43Z",
        "updatedAt" : "2020-12-11T12:54:13Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "7b2a782f-2708-49e6-b5b4-9d8647865f6b",
        "parentId" : "9b6be110-765d-424e-8471-629a33e7f5dd",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "sure, updated.",
        "createdAt" : "2020-12-11T12:54:58Z",
        "updatedAt" : "2020-12-11T12:54:58Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "c3c82ebfb5b613fcb2382c42d747b0233ee04a86",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +465,469 @@\n  def stringToDateAnsi(s: UTF8String, zoneId: ZoneId): Int = {\n    stringToDate(s, zoneId).getOrElse {\n      throw new DateTimeException(s\"Cannot cast $s to DateType.\")\n    }"
  },
  {
    "id" : "1cbae788-4d28-4067-8154-8d9270678cae",
    "prId" : 30445,
    "prUrl" : "https://github.com/apache/spark/pull/30445#pullrequestreview-536164449",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7ec559f1-d23c-4ac6-8919-0d490c694608",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "@gengliangwang If you would like to be compatible to PostgreSQL, you need to take the removed implementation:\r\n```scala\r\n/**\r\n   * Returns the number of seconds with fractional part in microsecond precision\r\n   * since 1970-01-01 00:00:00 local time.\r\n   */\r\n  def getEpoch(timestamp: SQLTimestamp, zoneId: ZoneId): Decimal = {\r\n    val offset = SECONDS.toMicros(\r\n      zoneId.getRules.getOffset(microsToInstant(timestamp)).getTotalSeconds)\r\n    val sinceEpoch = timestamp + offset\r\n    Decimal(sinceEpoch, 20, 6)\r\n  }\r\n```\r\nPostgreSQL takes seconds since the **local** epoch 1970-01-01 00:00:00-00 but your implementation calculates seconds since 1970-01-01 00:00:00-00**Z** (in UTC time zone).",
        "createdAt" : "2020-11-20T14:37:50Z",
        "updatedAt" : "2020-11-20T14:37:50Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "455b38f2-c664-495d-bc30-da56c298c084",
        "parentId" : "7ec559f1-d23c-4ac6-8919-0d490c694608",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "+1, thanks for pointing it out ðŸ‘ ",
        "createdAt" : "2020-11-23T03:47:03Z",
        "updatedAt" : "2020-11-23T03:47:03Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "6a5f31b4713b7efd78bb1e78dd7f257acef02299",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +498,502 @@   */\n  def getSecondsAfterEpoch(micros: Long, zoneId: ZoneId): Double = {\n    micros.toDouble / MICROS_PER_SECOND\n  }\n"
  },
  {
    "id" : "59b72c14-0167-41fb-9879-f8c2888446e2",
    "prId" : 28892,
    "prUrl" : "https://github.com/apache/spark/pull/28892#pullrequestreview-435455956",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "06dda257-fe0f-4cb7-9d6b-9ce43a194b8f",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "doesn't the `java.lang.String` have the `replace` method?",
        "createdAt" : "2020-06-23T04:51:59Z",
        "updatedAt" : "2020-06-23T04:51:59Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "b554d68a-6b86-404b-9a19-d65a971b57a0",
        "parentId" : "06dda257-fe0f-4cb7-9d6b-9ce43a194b8f",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "It has but look at how it is implemented via regexp. @JoshRosen implemented more effective replace in UTF8String https://github.com/apache/spark/pull/24707. That's why I took it. I hope it seems reasonable.",
        "createdAt" : "2020-06-23T05:39:55Z",
        "updatedAt" : "2020-06-23T05:39:56Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "bcea39121a36c6ac97740f6398f96f024d02b57a",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +207,211 @@  // The method is called by JSON/CSV parser to clean up the legacy timestamp string by removing\n  // the \"GMT\" string. For example, it returns 2000-01-01T00:00+01:00 for 2000-01-01T00:00GMT+01:00.\n  def cleanLegacyTimestampStr(s: UTF8String): UTF8String = s.replace(gmtUtf8, UTF8String.EMPTY_UTF8)\n\n  /**"
  },
  {
    "id" : "67547c3d-4de3-4351-907d-17da2d67d394",
    "prId" : 28445,
    "prUrl" : "https://github.com/apache/spark/pull/28445#pullrequestreview-404999743",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "518ca5e9-36bd-4d24-a1a7-4409b0ae7ee9",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "does timezone matter here?",
        "createdAt" : "2020-05-04T12:30:06Z",
        "updatedAt" : "2020-05-04T12:30:07Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "ac60f412-8633-4ced-a735-759f6d6c13df",
        "parentId" : "518ca5e9-36bd-4d24-a1a7-4409b0ae7ee9",
        "authorId" : "84d37f6c-0769-44e9-80b2-42fe4b57dce5",
        "body" : "Based on the implementation (https://github.com/JetBrains/jdk8u_jdk/blob/master/src/share/classes/java/util/GregorianCalendar.java#L819), I don't think timezone would make a difference here.",
        "createdAt" : "2020-05-04T13:44:00Z",
        "updatedAt" : "2020-05-04T13:44:00Z",
        "lastEditedBy" : "84d37f6c-0769-44e9-80b2-42fe4b57dce5",
        "tags" : [
        ]
      }
    ],
    "commit" : "ce2470a6951e059ede9e34b687b8d309b3cf688d",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +84,88 @@  private val threadLocalUtcGregorianCalendar = new ThreadLocal[GregorianCalendar] {\n    override def initialValue(): GregorianCalendar = {\n      new GregorianCalendar(TimeZoneUTC)\n    }\n  }"
  },
  {
    "id" : "5baf2d15-11fc-4e5a-9666-60a392565f8a",
    "prId" : 28443,
    "prUrl" : "https://github.com/apache/spark/pull/28443#pullrequestreview-404675883",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "60ed0385-4cf0-4490-9ac6-746124df8b9f",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "it is not pure Gregorian  calendar. For 1000-02-29, it is Julian calendar.",
        "createdAt" : "2020-05-03T21:25:45Z",
        "updatedAt" : "2020-05-03T21:38:54Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "616ed44b-0457-4c5b-8823-56ebf4b6b105",
        "parentId" : "60ed0385-4cf0-4490-9ac6-746124df8b9f",
        "authorId" : "84d37f6c-0769-44e9-80b2-42fe4b57dce5",
        "body" : "Ah, okay, should we close SPARK-31212 then? Is it the right behavior to not able to cast `1000-02-29`?",
        "createdAt" : "2020-05-03T21:43:03Z",
        "updatedAt" : "2020-05-03T21:43:03Z",
        "lastEditedBy" : "84d37f6c-0769-44e9-80b2-42fe4b57dce5",
        "tags" : [
        ]
      },
      {
        "id" : "ef726a43-ae47-44ce-9d9e-39213bbf6a61",
        "parentId" : "60ed0385-4cf0-4490-9ac6-746124df8b9f",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "The ticket is opened for Spark 2.4 where it is correct date but your PR is for master ",
        "createdAt" : "2020-05-03T22:19:50Z",
        "updatedAt" : "2020-05-03T22:19:50Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "9f0110dd-ff85-4dbf-915a-32a5b72ea5ed",
        "parentId" : "60ed0385-4cf0-4490-9ac6-746124df8b9f",
        "authorId" : "84d37f6c-0769-44e9-80b2-42fe4b57dce5",
        "body" : "I see. Didn't know that the semantic has changed in these two versions. Thanks for the explanation. So if I want to fix 2.4, which branch should I create a PR for? branch-2.4?",
        "createdAt" : "2020-05-03T23:06:05Z",
        "updatedAt" : "2020-05-03T23:06:06Z",
        "lastEditedBy" : "84d37f6c-0769-44e9-80b2-42fe4b57dce5",
        "tags" : [
        ]
      },
      {
        "id" : "c3eaf75f-9055-4f35-9f92-a99d6d2de1ff",
        "parentId" : "60ed0385-4cf0-4490-9ac6-746124df8b9f",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "if we can land a minimised fix, let's go ahead for branch-2.4. Otherwise, I think I would rather treat that calendar switching supersedes it.",
        "createdAt" : "2020-05-04T00:55:10Z",
        "updatedAt" : "2020-05-04T00:55:10Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "d295ed43b15feca4b0e3af8e1ba634ace5c800bd",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +216,220 @@      minute: Byte = 0,\n      sec: Byte = 0): SQLDate = {\n    val calendar = new GregorianCalendar(year, month - 1, day, hour, minute, sec)\n    calendar.setTimeZone(TimeZoneUTC)\n    calendar.setLenient(false)"
  },
  {
    "id" : "0f0ec652-f7c0-42d6-a6f5-f6535883b024",
    "prId" : 28310,
    "prUrl" : "https://github.com/apache/spark/pull/28310#pullrequestreview-406718836",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6126018d-e6b7-46a8-9c9b-d87daf7595b2",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Actually, the result depends on the order of `plusMonths()` and `plusDays()`. @yaooqinn Did you make the choice intentionally? I am asking you because adding days and months can be much cheaper.  ",
        "createdAt" : "2020-05-03T18:43:06Z",
        "updatedAt" : "2020-05-03T18:43:06Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "52e1d182-88b9-4861-a62b-37d370464ced",
        "parentId" : "6126018d-e6b7-46a8-9c9b-d87daf7595b2",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "Yesï¼Œhere we are follow the previous behavior of using timestamp + interval",
        "createdAt" : "2020-05-03T19:36:47Z",
        "updatedAt" : "2020-05-03T19:36:47Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "471ce787-4855-4acf-80a7-867b5e2e8cca",
        "parentId" : "6126018d-e6b7-46a8-9c9b-d87daf7595b2",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "I see, thanks. It would be nice to document such behavior of this function and timestampAddInterval somewhere. It is not obvious that we add month then days and then micros. The order could be opposite. ",
        "createdAt" : "2020-05-03T20:20:51Z",
        "updatedAt" : "2020-05-03T20:20:51Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "84652018-800c-4361-8d48-575316043899",
        "parentId" : "6126018d-e6b7-46a8-9c9b-d87daf7595b2",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "FYI, in snowflake `internal '1 month 1 day'` is different from `internal '1 day 1 month'`. We should at least document our own behavior.",
        "createdAt" : "2020-05-05T05:32:58Z",
        "updatedAt" : "2020-05-05T05:32:59Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "afa07988-240a-4056-af32-3214d309453b",
        "parentId" : "6126018d-e6b7-46a8-9c9b-d87daf7595b2",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "I will make time for that PR.",
        "createdAt" : "2020-05-06T15:17:27Z",
        "updatedAt" : "2020-05-19T12:05:49Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "c64d27ea81f73ab2bd67dbae4c88d6704639063d",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +631,635 @@    require(interval.microseconds == 0,\n      \"Cannot add hours, minutes or seconds, milliseconds, microseconds to a date\")\n    val ld = LocalDate.ofEpochDay(start).plusMonths(interval.months).plusDays(interval.days)\n    localDateToDays(ld)\n  }"
  },
  {
    "id" : "32107721-48e6-4c53-aa1d-f45e100f9643",
    "prId" : 28212,
    "prUrl" : "https://github.com/apache/spark/pull/28212#pullrequestreview-393423930",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fd07d468-a23a-4e36-bfa8-fb4560f5026a",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "The code is adopted from https://github.com/AdoptOpenJDK/openjdk-jdk8u/blob/aa318070b27849f1fe00d14684b2a40f7b29bf79/jdk/src/share/classes/java/util/GregorianCalendar.java#L2795-L2801",
        "createdAt" : "2020-04-14T13:30:49Z",
        "updatedAt" : "2020-04-14T17:28:18Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "4482e6fa-b2cf-44c1-b952-4f69f1041925",
        "parentId" : "fd07d468-a23a-4e36-bfa8-fb4560f5026a",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Shall we apply this change for spark 2.4 as well?",
        "createdAt" : "2020-04-14T13:55:43Z",
        "updatedAt" : "2020-04-14T17:28:18Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "72f24bce-c5df-42eb-90dd-5e2c039774f5",
        "parentId" : "fd07d468-a23a-4e36-bfa8-fb4560f5026a",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "If we can do that in 2.4, sure but see https://github.com/apache/spark/pull/28216#issuecomment-613581416",
        "createdAt" : "2020-04-14T17:45:09Z",
        "updatedAt" : "2020-04-14T17:45:09Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "7dca0211-3da1-4538-a2e1-2d2541a0046f",
        "parentId" : "fd07d468-a23a-4e36-bfa8-fb4560f5026a",
        "authorId" : "99e31844-a5b3-48a2-af71-412edb607a13",
        "body" : "According to https://spark.apache.org/versioning-policy.html,, a maintenance release would not include performance improvement patch.\r\n\r\nFor 3.0, it looks good due to the small size of this change.",
        "createdAt" : "2020-04-14T17:59:36Z",
        "updatedAt" : "2020-04-14T18:00:14Z",
        "lastEditedBy" : "99e31844-a5b3-48a2-af71-412edb607a13",
        "tags" : [
        ]
      },
      {
        "id" : "e2815ba6-c735-4463-a6ca-8c8272fe388f",
        "parentId" : "fd07d468-a23a-4e36-bfa8-fb4560f5026a",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Yes, it's best to follow the guide.",
        "createdAt" : "2020-04-15T00:49:13Z",
        "updatedAt" : "2020-04-15T00:49:14Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "902664a5-28d1-4dd9-9acd-45b8202ffd01",
        "parentId" : "fd07d468-a23a-4e36-bfa8-fb4560f5026a",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I was asking for the timezone offset change, which seems wrong in 2.4: https://github.com/apache/spark/blob/branch-2.4/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/DateTimeUtils.scala#L1083-L1118",
        "createdAt" : "2020-04-15T02:47:05Z",
        "updatedAt" : "2020-04-15T02:51:36Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "7d7a6ba3-d27a-49b4-a84f-eee43ce773e3",
        "parentId" : "fd07d468-a23a-4e36-bfa8-fb4560f5026a",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I can't tell that the 2 lines here behave the same as that long method in 2.4. If they are the same and it's just an improvement, then we shouldn't backport. If 2.4 is wrong and then we should fix it as it's a correctness issue.",
        "createdAt" : "2020-04-15T02:54:50Z",
        "updatedAt" : "2020-04-15T02:54:50Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "614ec2ab101d16af07809a0ac639d97b54cb31ec",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +129,133 @@      case zoneInfo: ZoneInfo => zoneInfo.getOffsetsByWall(localMillis, null)\n      case timeZone: TimeZone => timeZone.getOffset(localMillis - timeZone.getRawOffset)\n    }\n    new Date(localMillis - timeZoneOffset)\n  }"
  },
  {
    "id" : "60435503-0079-4cef-bef7-499f34a40483",
    "prId" : 28205,
    "prUrl" : "https://github.com/apache/spark/pull/28205#pullrequestreview-392647362",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a4e547df-0853-498e-a6c1-e6bb2e141877",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "@cloud-fan Spark 2.4 uses floating point ops, see https://github.com/apache/spark/blob/branch-2.4/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/DateTimeUtils.scala#L137",
        "createdAt" : "2020-04-14T06:54:11Z",
        "updatedAt" : "2020-04-14T07:32:44Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "ee1d78865069e3beb2192f5b360af0df63e5d1ea",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +104,108 @@    val millisUtc = date.getTime\n    val millisLocal = millisUtc + TimeZone.getDefault.getOffset(millisUtc)\n    val julianDays = Math.toIntExact(Math.floorDiv(millisLocal, MILLIS_PER_DAY))\n    rebaseJulianToGregorianDays(julianDays)\n  }"
  },
  {
    "id" : "bcf3f4fb-2840-4e31-bb6b-910084f4e49e",
    "prId" : 28189,
    "prUrl" : "https://github.com/apache/spark/pull/28189#pullrequestreview-392611508",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "eda0e677-4d37-4080-948b-69164edf9c8f",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "This seems to be missing in `branch-3.0`.\r\n```\r\n$ git grep millisToMicros\r\nsql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/DateTimeUtils.scala: \r\nval micros = millisToMicros(t.getTime) + (t.getNanos / NANOS_PER_MICROS) % MICROS_PER_MILLIS\r\n```",
        "createdAt" : "2020-04-14T05:25:56Z",
        "updatedAt" : "2020-04-14T05:26:28Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "a28f2a1bd7d053051cc252f37de4e412abef9ce6",
    "line" : 30,
    "diffHunk" : "@@ -1,1 +177,181 @@   */\n  def fromJavaTimestamp(t: Timestamp): SQLTimestamp = {\n    val micros = millisToMicros(t.getTime) + (t.getNanos / NANOS_PER_MICROS) % MICROS_PER_MILLIS\n    rebaseJulianToGregorianMicros(micros)\n  }"
  }
]