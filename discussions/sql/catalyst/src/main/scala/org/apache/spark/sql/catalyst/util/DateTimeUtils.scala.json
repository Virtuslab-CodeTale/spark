[
  {
    "id" : "6030794a-0dfa-4ce7-a4e7-6f7ee530c084",
    "prId" : 32959,
    "prUrl" : "https://github.com/apache/spark/pull/32959#pullrequestreview-699672586",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c26a8b86-6b67-44be-b4e4-04395a2992a5",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "for `+12:12:12` do we fail or simply ignore the `+`?",
        "createdAt" : "2021-07-02T07:43:04Z",
        "updatedAt" : "2021-07-02T07:43:05Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "c0cbbf63-1d67-4715-9a42-02aae348c9cc",
        "parentId" : "c26a8b86-6b67-44be-b4e4-04395a2992a5",
        "authorId" : "29c89154-22f7-40ee-b295-115a55734d6c",
        "body" : "we simply ignore the `+`",
        "createdAt" : "2021-07-02T08:39:31Z",
        "updatedAt" : "2021-07-02T08:39:31Z",
        "lastEditedBy" : "29c89154-22f7-40ee-b295-115a55734d6c",
        "tags" : [
        ]
      },
      {
        "id" : "af877d78-37ab-428b-946f-ab7e50559766",
        "parentId" : "c26a8b86-6b67-44be-b4e4-04395a2992a5",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I think failing is better? Since we won't respect `-` here anyway.",
        "createdAt" : "2021-07-02T12:05:50Z",
        "updatedAt" : "2021-07-02T12:05:50Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "9ac87ba2-0867-4f4b-8e12-0150d813092c",
        "parentId" : "c26a8b86-6b67-44be-b4e4-04395a2992a5",
        "authorId" : "29c89154-22f7-40ee-b295-115a55734d6c",
        "body" : "actually the we will respect the `-`, for example:\r\n```\r\nscala> sql(\"select date'-12-12-12'\").show()\r\n+------------------+\r\n|DATE '-0012-12-12'|\r\n+------------------+\r\n|       -0012-12-12|\r\n+------------------+\r\n\r\nscala> sql(\"select date'+12-12-12'\").show()\r\n+-----------------+\r\n|DATE '0012-12-12'|\r\n+-----------------+\r\n|       0012-12-12|\r\n+-----------------+\r\n\r\n```",
        "createdAt" : "2021-07-05T02:48:07Z",
        "updatedAt" : "2021-07-05T02:48:07Z",
        "lastEditedBy" : "29c89154-22f7-40ee-b295-115a55734d6c",
        "tags" : [
        ]
      },
      {
        "id" : "9e93ba1e-eb7d-439e-b1e7-3a99ec964bc2",
        "parentId" : "c26a8b86-6b67-44be-b4e4-04395a2992a5",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "are you sure? It seems we only consider the sign for year: https://github.com/apache/spark/pull/32959/files#diff-f59286f6576476c68ae766de0430fc41d424804dbb0658e0dccd998b0d9a756dR404",
        "createdAt" : "2021-07-06T04:58:54Z",
        "updatedAt" : "2021-07-06T04:58:54Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "5c61b2c4-d1c3-4c33-90a0-beb0572e21ef",
        "parentId" : "c26a8b86-6b67-44be-b4e4-04395a2992a5",
        "authorId" : "29c89154-22f7-40ee-b295-115a55734d6c",
        "body" : "I see. good catch.",
        "createdAt" : "2021-07-06T08:24:40Z",
        "updatedAt" : "2021-07-06T08:24:40Z",
        "lastEditedBy" : "29c89154-22f7-40ee-b295-115a55734d6c",
        "tags" : [
        ]
      }
    ],
    "commit" : "538463a405c2eac1ddfccba8d82ca846f81e5a22",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +231,235 @@   * `[+-]yyyy*-[m]m-[d]d [h]h:[m]m:[s]s.[ms][ms][ms][us][us][us][zone_id]`\n   * `[+-]yyyy*-[m]m-[d]dT[h]h:[m]m:[s]s.[ms][ms][ms][us][us][us][zone_id]`\n   * `[h]h:[m]m:[s]s.[ms][ms][ms][us][us][us][zone_id]`\n   * `T[h]h:[m]m:[s]s.[ms][ms][ms][us][us][us][zone_id]`\n   *"
  },
  {
    "id" : "a06752c3-bfc2-4f57-8305-142a2ff49d6a",
    "prId" : 32959,
    "prUrl" : "https://github.com/apache/spark/pull/32959#pullrequestreview-702361484",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8d601ae4-ec6b-4303-810f-dbea949522fc",
        "parentId" : null,
        "authorId" : "29c89154-22f7-40ee-b295-115a55734d6c",
        "body" : "segments except year are allowed to have 0 digits before this PR. so I didn't do zero checks for these segments.\r\nfor example, before and after this PR, the below query is valid:\r\n```sql\r\nselect cast('12::' as timestamp); -- output: 2021-07-07 12:00:00\r\nselect cast('T' as timestamp); -- output: 2021-07-07 00:00:00\r\n```",
        "createdAt" : "2021-07-07T03:36:24Z",
        "updatedAt" : "2021-07-07T04:58:50Z",
        "lastEditedBy" : "29c89154-22f7-40ee-b295-115a55734d6c",
        "tags" : [
        ]
      },
      {
        "id" : "5ab0c04f-e5f4-4022-88ee-bc0c037acb54",
        "parentId" : "8d601ae4-ec6b-4303-810f-dbea949522fc",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "good catch, we should fail it. Let's do it in another PR.",
        "createdAt" : "2021-07-08T18:07:32Z",
        "updatedAt" : "2021-07-08T18:07:33Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "538463a405c2eac1ddfccba8d82ca846f81e5a22",
    "line" : 29,
    "diffHunk" : "@@ -1,1 +255,259 @@      // For the nanosecond part, more than 6 digits is allowed, but will be truncated.\n      segment == 6 || (segment == 0 && digits >= 4 && digits <= maxDigitsYear) ||\n        (segment != 0 && segment != 6 && digits <= 2)\n    }\n    if (s == null || s.trimAll().numBytes() == 0) {"
  },
  {
    "id" : "7b646c32-8e30-4474-b3a2-0cd48658ccd8",
    "prId" : 32839,
    "prUrl" : "https://github.com/apache/spark/pull/32839#pullrequestreview-680269083",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2a768081-8606-4263-83e8-cea20f6fa001",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "We know `sec` in advance, so, we can calculate `us` in advance too. Please, fold the arithmetic expression. ",
        "createdAt" : "2021-06-09T13:24:52Z",
        "updatedAt" : "2021-06-09T13:24:53Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "b135c721-5966-4ada-9156-59040e249ee5",
        "parentId" : "2a768081-8606-4263-83e8-cea20f6fa001",
        "authorId" : "87a71d79-d4d2-4f2c-8e9a-be56cf39b035",
        "body" : "Are you sure? as us might be (secs+1) or secs depend on case",
        "createdAt" : "2021-06-09T14:16:43Z",
        "updatedAt" : "2021-06-09T14:16:44Z",
        "lastEditedBy" : "87a71d79-d4d2-4f2c-8e9a-be56cf39b035",
        "tags" : [
        ]
      },
      {
        "id" : "006c2a2a-fd5a-4088-92ef-ecd4182191fb",
        "parentId" : "2a768081-8606-4263-83e8-cea20f6fa001",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "`Math.floorDiv(Long.MinValue, MICROS_PER_SECOND)` is constant, right?",
        "createdAt" : "2021-06-09T14:33:15Z",
        "updatedAt" : "2021-06-09T14:33:16Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "1e9682f8-95f0-43bb-a03c-f0805ebad831",
        "parentId" : "2a768081-8606-4263-83e8-cea20f6fa001",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I think it's rare to hit this branch and creating a constant for it is not necessary.",
        "createdAt" : "2021-06-09T15:20:41Z",
        "updatedAt" : "2021-06-09T15:20:41Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "042084de-d679-47b9-be06-e17f091003d5",
        "parentId" : "2a768081-8606-4263-83e8-cea20f6fa001",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "I mostly care of the comparison:\r\n```scala\r\nsecs == Math.floorDiv(Long.MinValue, MICROS_PER_SECOND)\r\n```\r\nwhich is performed per every input. Probably, JVM could optimize it but I would help to it and introduced a constant.",
        "createdAt" : "2021-06-09T15:40:15Z",
        "updatedAt" : "2021-06-09T15:40:15Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "de64f3a5-e96b-4cf0-a75f-425caff95432",
        "parentId" : "2a768081-8606-4263-83e8-cea20f6fa001",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Yea this should be a constant, but after we enter the if branch and reach here, it's rare and I'm OK to not have a constant for `val us = ...`",
        "createdAt" : "2021-06-09T16:04:36Z",
        "updatedAt" : "2021-06-09T16:04:36Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "ea7821f3-b668-4477-a496-105d8c63acdb",
        "parentId" : "2a768081-8606-4263-83e8-cea20f6fa001",
        "authorId" : "87a71d79-d4d2-4f2c-8e9a-be56cf39b035",
        "body" : "Done, thanks",
        "createdAt" : "2021-06-10T01:27:27Z",
        "updatedAt" : "2021-06-10T01:27:27Z",
        "lastEditedBy" : "87a71d79-d4d2-4f2c-8e9a-be56cf39b035",
        "tags" : [
        ]
      }
    ],
    "commit" : "134c356cb004283d477f1ce1a98cff97c194fc3f",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +391,395 @@    val secs = instant.getEpochSecond\n    if (secs == MIN_SECONDS) {\n      val us = Math.multiplyExact(secs + 1, MICROS_PER_SECOND)\n      Math.addExact(us, NANOSECONDS.toMicros(instant.getNano) - MICROS_PER_SECOND)\n    } else {"
  },
  {
    "id" : "c0c916ca-96cb-498a-8590-9cb48dae4765",
    "prId" : 31624,
    "prUrl" : "https://github.com/apache/spark/pull/31624#pullrequestreview-597006083",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5caec95e-3bec-4d68-81a8-80249867dd56",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "This change affects all the datetime related functionalities, so I think we need more tests and document updates.",
        "createdAt" : "2021-02-24T02:22:37Z",
        "updatedAt" : "2021-02-25T05:44:36Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "41e0cb063d06d621049aa7103458b33fcf3fd098",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +51,55 @@  val TIMEZONE_OPTION = \"timeZone\"\n\n  def getZoneId(timeZoneId: String): ZoneId = {\n    // To support the (+|-)h:mm format because it was supported before Spark 3.0.\n    ZoneId.of(timeZoneId.replaceFirst(\"(\\\\+|\\\\-)(\\\\d):\", \"$10$2:\"), ZoneId.SHORT_IDS)"
  },
  {
    "id" : "baa1e657-fc40-461c-91ca-c46c46c2b1e6",
    "prId" : 30687,
    "prUrl" : "https://github.com/apache/spark/pull/30687#pullrequestreview-550073041",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9b6be110-765d-424e-8471-629a33e7f5dd",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "can you also update `stringToTimestampAnsi` to follow this code style?",
        "createdAt" : "2020-12-11T06:53:43Z",
        "updatedAt" : "2020-12-11T12:54:13Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "7b2a782f-2708-49e6-b5b4-9d8647865f6b",
        "parentId" : "9b6be110-765d-424e-8471-629a33e7f5dd",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "sure, updated.",
        "createdAt" : "2020-12-11T12:54:58Z",
        "updatedAt" : "2020-12-11T12:54:58Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "c3c82ebfb5b613fcb2382c42d747b0233ee04a86",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +465,469 @@\n  def stringToDateAnsi(s: UTF8String, zoneId: ZoneId): Int = {\n    stringToDate(s, zoneId).getOrElse {\n      throw new DateTimeException(s\"Cannot cast $s to DateType.\")\n    }"
  },
  {
    "id" : "1cbae788-4d28-4067-8154-8d9270678cae",
    "prId" : 30445,
    "prUrl" : "https://github.com/apache/spark/pull/30445#pullrequestreview-536164449",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7ec559f1-d23c-4ac6-8919-0d490c694608",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "@gengliangwang If you would like to be compatible to PostgreSQL, you need to take the removed implementation:\r\n```scala\r\n/**\r\n   * Returns the number of seconds with fractional part in microsecond precision\r\n   * since 1970-01-01 00:00:00 local time.\r\n   */\r\n  def getEpoch(timestamp: SQLTimestamp, zoneId: ZoneId): Decimal = {\r\n    val offset = SECONDS.toMicros(\r\n      zoneId.getRules.getOffset(microsToInstant(timestamp)).getTotalSeconds)\r\n    val sinceEpoch = timestamp + offset\r\n    Decimal(sinceEpoch, 20, 6)\r\n  }\r\n```\r\nPostgreSQL takes seconds since the **local** epoch 1970-01-01 00:00:00-00 but your implementation calculates seconds since 1970-01-01 00:00:00-00**Z** (in UTC time zone).",
        "createdAt" : "2020-11-20T14:37:50Z",
        "updatedAt" : "2020-11-20T14:37:50Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "455b38f2-c664-495d-bc30-da56c298c084",
        "parentId" : "7ec559f1-d23c-4ac6-8919-0d490c694608",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "+1, thanks for pointing it out ðŸ‘ ",
        "createdAt" : "2020-11-23T03:47:03Z",
        "updatedAt" : "2020-11-23T03:47:03Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "6a5f31b4713b7efd78bb1e78dd7f257acef02299",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +498,502 @@   */\n  def getSecondsAfterEpoch(micros: Long, zoneId: ZoneId): Double = {\n    micros.toDouble / MICROS_PER_SECOND\n  }\n"
  },
  {
    "id" : "59b72c14-0167-41fb-9879-f8c2888446e2",
    "prId" : 28892,
    "prUrl" : "https://github.com/apache/spark/pull/28892#pullrequestreview-435455956",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "06dda257-fe0f-4cb7-9d6b-9ce43a194b8f",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "doesn't the `java.lang.String` have the `replace` method?",
        "createdAt" : "2020-06-23T04:51:59Z",
        "updatedAt" : "2020-06-23T04:51:59Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "b554d68a-6b86-404b-9a19-d65a971b57a0",
        "parentId" : "06dda257-fe0f-4cb7-9d6b-9ce43a194b8f",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "It has but look at how it is implemented via regexp. @JoshRosen implemented more effective replace in UTF8String https://github.com/apache/spark/pull/24707. That's why I took it. I hope it seems reasonable.",
        "createdAt" : "2020-06-23T05:39:55Z",
        "updatedAt" : "2020-06-23T05:39:56Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "bcea39121a36c6ac97740f6398f96f024d02b57a",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +207,211 @@  // The method is called by JSON/CSV parser to clean up the legacy timestamp string by removing\n  // the \"GMT\" string. For example, it returns 2000-01-01T00:00+01:00 for 2000-01-01T00:00GMT+01:00.\n  def cleanLegacyTimestampStr(s: UTF8String): UTF8String = s.replace(gmtUtf8, UTF8String.EMPTY_UTF8)\n\n  /**"
  },
  {
    "id" : "67547c3d-4de3-4351-907d-17da2d67d394",
    "prId" : 28445,
    "prUrl" : "https://github.com/apache/spark/pull/28445#pullrequestreview-404999743",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "518ca5e9-36bd-4d24-a1a7-4409b0ae7ee9",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "does timezone matter here?",
        "createdAt" : "2020-05-04T12:30:06Z",
        "updatedAt" : "2020-05-04T12:30:07Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "ac60f412-8633-4ced-a735-759f6d6c13df",
        "parentId" : "518ca5e9-36bd-4d24-a1a7-4409b0ae7ee9",
        "authorId" : "84d37f6c-0769-44e9-80b2-42fe4b57dce5",
        "body" : "Based on the implementation (https://github.com/JetBrains/jdk8u_jdk/blob/master/src/share/classes/java/util/GregorianCalendar.java#L819), I don't think timezone would make a difference here.",
        "createdAt" : "2020-05-04T13:44:00Z",
        "updatedAt" : "2020-05-04T13:44:00Z",
        "lastEditedBy" : "84d37f6c-0769-44e9-80b2-42fe4b57dce5",
        "tags" : [
        ]
      }
    ],
    "commit" : "ce2470a6951e059ede9e34b687b8d309b3cf688d",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +84,88 @@  private val threadLocalUtcGregorianCalendar = new ThreadLocal[GregorianCalendar] {\n    override def initialValue(): GregorianCalendar = {\n      new GregorianCalendar(TimeZoneUTC)\n    }\n  }"
  },
  {
    "id" : "5baf2d15-11fc-4e5a-9666-60a392565f8a",
    "prId" : 28443,
    "prUrl" : "https://github.com/apache/spark/pull/28443#pullrequestreview-404675883",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "60ed0385-4cf0-4490-9ac6-746124df8b9f",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "it is not pure Gregorian  calendar. For 1000-02-29, it is Julian calendar.",
        "createdAt" : "2020-05-03T21:25:45Z",
        "updatedAt" : "2020-05-03T21:38:54Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "616ed44b-0457-4c5b-8823-56ebf4b6b105",
        "parentId" : "60ed0385-4cf0-4490-9ac6-746124df8b9f",
        "authorId" : "84d37f6c-0769-44e9-80b2-42fe4b57dce5",
        "body" : "Ah, okay, should we close SPARK-31212 then? Is it the right behavior to not able to cast `1000-02-29`?",
        "createdAt" : "2020-05-03T21:43:03Z",
        "updatedAt" : "2020-05-03T21:43:03Z",
        "lastEditedBy" : "84d37f6c-0769-44e9-80b2-42fe4b57dce5",
        "tags" : [
        ]
      },
      {
        "id" : "ef726a43-ae47-44ce-9d9e-39213bbf6a61",
        "parentId" : "60ed0385-4cf0-4490-9ac6-746124df8b9f",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "The ticket is opened for Spark 2.4 where it is correct date but your PR is for master ",
        "createdAt" : "2020-05-03T22:19:50Z",
        "updatedAt" : "2020-05-03T22:19:50Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "9f0110dd-ff85-4dbf-915a-32a5b72ea5ed",
        "parentId" : "60ed0385-4cf0-4490-9ac6-746124df8b9f",
        "authorId" : "84d37f6c-0769-44e9-80b2-42fe4b57dce5",
        "body" : "I see. Didn't know that the semantic has changed in these two versions. Thanks for the explanation. So if I want to fix 2.4, which branch should I create a PR for? branch-2.4?",
        "createdAt" : "2020-05-03T23:06:05Z",
        "updatedAt" : "2020-05-03T23:06:06Z",
        "lastEditedBy" : "84d37f6c-0769-44e9-80b2-42fe4b57dce5",
        "tags" : [
        ]
      },
      {
        "id" : "c3eaf75f-9055-4f35-9f92-a99d6d2de1ff",
        "parentId" : "60ed0385-4cf0-4490-9ac6-746124df8b9f",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "if we can land a minimised fix, let's go ahead for branch-2.4. Otherwise, I think I would rather treat that calendar switching supersedes it.",
        "createdAt" : "2020-05-04T00:55:10Z",
        "updatedAt" : "2020-05-04T00:55:10Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "d295ed43b15feca4b0e3af8e1ba634ace5c800bd",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +216,220 @@      minute: Byte = 0,\n      sec: Byte = 0): SQLDate = {\n    val calendar = new GregorianCalendar(year, month - 1, day, hour, minute, sec)\n    calendar.setTimeZone(TimeZoneUTC)\n    calendar.setLenient(false)"
  },
  {
    "id" : "0f0ec652-f7c0-42d6-a6f5-f6535883b024",
    "prId" : 28310,
    "prUrl" : "https://github.com/apache/spark/pull/28310#pullrequestreview-406718836",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6126018d-e6b7-46a8-9c9b-d87daf7595b2",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Actually, the result depends on the order of `plusMonths()` and `plusDays()`. @yaooqinn Did you make the choice intentionally? I am asking you because adding days and months can be much cheaper.  ",
        "createdAt" : "2020-05-03T18:43:06Z",
        "updatedAt" : "2020-05-03T18:43:06Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "52e1d182-88b9-4861-a62b-37d370464ced",
        "parentId" : "6126018d-e6b7-46a8-9c9b-d87daf7595b2",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "Yesï¼Œhere we are follow the previous behavior of using timestamp + interval",
        "createdAt" : "2020-05-03T19:36:47Z",
        "updatedAt" : "2020-05-03T19:36:47Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "471ce787-4855-4acf-80a7-867b5e2e8cca",
        "parentId" : "6126018d-e6b7-46a8-9c9b-d87daf7595b2",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "I see, thanks. It would be nice to document such behavior of this function and timestampAddInterval somewhere. It is not obvious that we add month then days and then micros. The order could be opposite. ",
        "createdAt" : "2020-05-03T20:20:51Z",
        "updatedAt" : "2020-05-03T20:20:51Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "84652018-800c-4361-8d48-575316043899",
        "parentId" : "6126018d-e6b7-46a8-9c9b-d87daf7595b2",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "FYI, in snowflake `internal '1 month 1 day'` is different from `internal '1 day 1 month'`. We should at least document our own behavior.",
        "createdAt" : "2020-05-05T05:32:58Z",
        "updatedAt" : "2020-05-05T05:32:59Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "afa07988-240a-4056-af32-3214d309453b",
        "parentId" : "6126018d-e6b7-46a8-9c9b-d87daf7595b2",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "I will make time for that PR.",
        "createdAt" : "2020-05-06T15:17:27Z",
        "updatedAt" : "2020-05-19T12:05:49Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "c64d27ea81f73ab2bd67dbae4c88d6704639063d",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +631,635 @@    require(interval.microseconds == 0,\n      \"Cannot add hours, minutes or seconds, milliseconds, microseconds to a date\")\n    val ld = LocalDate.ofEpochDay(start).plusMonths(interval.months).plusDays(interval.days)\n    localDateToDays(ld)\n  }"
  },
  {
    "id" : "32107721-48e6-4c53-aa1d-f45e100f9643",
    "prId" : 28212,
    "prUrl" : "https://github.com/apache/spark/pull/28212#pullrequestreview-393423930",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fd07d468-a23a-4e36-bfa8-fb4560f5026a",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "The code is adopted from https://github.com/AdoptOpenJDK/openjdk-jdk8u/blob/aa318070b27849f1fe00d14684b2a40f7b29bf79/jdk/src/share/classes/java/util/GregorianCalendar.java#L2795-L2801",
        "createdAt" : "2020-04-14T13:30:49Z",
        "updatedAt" : "2020-04-14T17:28:18Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "4482e6fa-b2cf-44c1-b952-4f69f1041925",
        "parentId" : "fd07d468-a23a-4e36-bfa8-fb4560f5026a",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Shall we apply this change for spark 2.4 as well?",
        "createdAt" : "2020-04-14T13:55:43Z",
        "updatedAt" : "2020-04-14T17:28:18Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "72f24bce-c5df-42eb-90dd-5e2c039774f5",
        "parentId" : "fd07d468-a23a-4e36-bfa8-fb4560f5026a",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "If we can do that in 2.4, sure but see https://github.com/apache/spark/pull/28216#issuecomment-613581416",
        "createdAt" : "2020-04-14T17:45:09Z",
        "updatedAt" : "2020-04-14T17:45:09Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "7dca0211-3da1-4538-a2e1-2d2541a0046f",
        "parentId" : "fd07d468-a23a-4e36-bfa8-fb4560f5026a",
        "authorId" : "99e31844-a5b3-48a2-af71-412edb607a13",
        "body" : "According to https://spark.apache.org/versioning-policy.html,, a maintenance release would not include performance improvement patch.\r\n\r\nFor 3.0, it looks good due to the small size of this change.",
        "createdAt" : "2020-04-14T17:59:36Z",
        "updatedAt" : "2020-04-14T18:00:14Z",
        "lastEditedBy" : "99e31844-a5b3-48a2-af71-412edb607a13",
        "tags" : [
        ]
      },
      {
        "id" : "e2815ba6-c735-4463-a6ca-8c8272fe388f",
        "parentId" : "fd07d468-a23a-4e36-bfa8-fb4560f5026a",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Yes, it's best to follow the guide.",
        "createdAt" : "2020-04-15T00:49:13Z",
        "updatedAt" : "2020-04-15T00:49:14Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "902664a5-28d1-4dd9-9acd-45b8202ffd01",
        "parentId" : "fd07d468-a23a-4e36-bfa8-fb4560f5026a",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I was asking for the timezone offset change, which seems wrong in 2.4: https://github.com/apache/spark/blob/branch-2.4/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/DateTimeUtils.scala#L1083-L1118",
        "createdAt" : "2020-04-15T02:47:05Z",
        "updatedAt" : "2020-04-15T02:51:36Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "7d7a6ba3-d27a-49b4-a84f-eee43ce773e3",
        "parentId" : "fd07d468-a23a-4e36-bfa8-fb4560f5026a",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I can't tell that the 2 lines here behave the same as that long method in 2.4. If they are the same and it's just an improvement, then we shouldn't backport. If 2.4 is wrong and then we should fix it as it's a correctness issue.",
        "createdAt" : "2020-04-15T02:54:50Z",
        "updatedAt" : "2020-04-15T02:54:50Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "614ec2ab101d16af07809a0ac639d97b54cb31ec",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +129,133 @@      case zoneInfo: ZoneInfo => zoneInfo.getOffsetsByWall(localMillis, null)\n      case timeZone: TimeZone => timeZone.getOffset(localMillis - timeZone.getRawOffset)\n    }\n    new Date(localMillis - timeZoneOffset)\n  }"
  },
  {
    "id" : "60435503-0079-4cef-bef7-499f34a40483",
    "prId" : 28205,
    "prUrl" : "https://github.com/apache/spark/pull/28205#pullrequestreview-392647362",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a4e547df-0853-498e-a6c1-e6bb2e141877",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "@cloud-fan Spark 2.4 uses floating point ops, see https://github.com/apache/spark/blob/branch-2.4/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/DateTimeUtils.scala#L137",
        "createdAt" : "2020-04-14T06:54:11Z",
        "updatedAt" : "2020-04-14T07:32:44Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "ee1d78865069e3beb2192f5b360af0df63e5d1ea",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +104,108 @@    val millisUtc = date.getTime\n    val millisLocal = millisUtc + TimeZone.getDefault.getOffset(millisUtc)\n    val julianDays = Math.toIntExact(Math.floorDiv(millisLocal, MILLIS_PER_DAY))\n    rebaseJulianToGregorianDays(julianDays)\n  }"
  },
  {
    "id" : "bcf3f4fb-2840-4e31-bb6b-910084f4e49e",
    "prId" : 28189,
    "prUrl" : "https://github.com/apache/spark/pull/28189#pullrequestreview-392611508",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "eda0e677-4d37-4080-948b-69164edf9c8f",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "This seems to be missing in `branch-3.0`.\r\n```\r\n$ git grep millisToMicros\r\nsql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/DateTimeUtils.scala: \r\nval micros = millisToMicros(t.getTime) + (t.getNanos / NANOS_PER_MICROS) % MICROS_PER_MILLIS\r\n```",
        "createdAt" : "2020-04-14T05:25:56Z",
        "updatedAt" : "2020-04-14T05:26:28Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "a28f2a1bd7d053051cc252f37de4e412abef9ce6",
    "line" : 30,
    "diffHunk" : "@@ -1,1 +177,181 @@   */\n  def fromJavaTimestamp(t: Timestamp): SQLTimestamp = {\n    val micros = millisToMicros(t.getTime) + (t.getNanos / NANOS_PER_MICROS) % MICROS_PER_MILLIS\n    rebaseJulianToGregorianMicros(micros)\n  }"
  },
  {
    "id" : "455be882-b636-4a08-a8d2-e6f546ba80d1",
    "prId" : 28101,
    "prUrl" : "https://github.com/apache/spark/pull/28101#pullrequestreview-386537265",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5a8ecb83-4c04-44af-a280-15e832fee82b",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "can we add some comment here? people may not understand what `cal.get(Calendar.DST_OFFSET) == 0` means",
        "createdAt" : "2020-04-02T14:01:09Z",
        "updatedAt" : "2020-04-02T16:35:21Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "e7fe5321-886e-4e93-ac9c-b5bf9ac537ad",
        "parentId" : "5a8ecb83-4c04-44af-a280-15e832fee82b",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "added",
        "createdAt" : "2020-04-02T15:24:23Z",
        "updatedAt" : "2020-04-02T16:35:21Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "29d4599bc542d3c87a1489933e679a31080801a6",
    "line" : 29,
    "diffHunk" : "@@ -1,1 +1040,1044 @@    // So, clocks go back one hour. We should correct zoned date-time and change\n    // the zone offset to the later of the two valid offsets at a local time-line overlap.\n    val adjustedZdt = if (cal.get(Calendar.DST_OFFSET) == 0) {\n      zonedDateTime.withLaterOffsetAtOverlap()\n    } else {"
  },
  {
    "id" : "8aa8eb63-2731-4b61-b6c4-41690a26dc5b",
    "prId" : 28067,
    "prUrl" : "https://github.com/apache/spark/pull/28067#pullrequestreview-384017910",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "493cc9a8-a8e8-4714-9253-8dd9f49ebadd",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "This doesn't explain what's the diff of dates before `julianGregDiffSwitchDay(0)`",
        "createdAt" : "2020-03-30T11:13:44Z",
        "updatedAt" : "2020-03-30T16:21:45Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "a014c781-9a21-4109-a320-e7f90e3a8c8e",
        "parentId" : "493cc9a8-a8e8-4714-9253-8dd9f49ebadd",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "The dates before `0001-01-01` is out of supported range, the current implementation just returns constant diff of 2 days.\r\n",
        "createdAt" : "2020-03-30T11:32:02Z",
        "updatedAt" : "2020-03-30T16:21:45Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "c7f42d04-67b1-4891-8bbd-fc8eab22e24f",
        "parentId" : "493cc9a8-a8e8-4714-9253-8dd9f49ebadd",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Then it means `julianGregDiffSwitchDay(0)` is useless? It's not a point that difference in days changes.",
        "createdAt" : "2020-03-30T11:51:31Z",
        "updatedAt" : "2020-03-30T16:21:45Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "fa6396a9-d0e3-431f-a2c0-9aab65bd7e88",
        "parentId" : "493cc9a8-a8e8-4714-9253-8dd9f49ebadd",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "It is not completely useless because I need some point to stop in searching. Let's say this is a diff switching day from undefined diff to concrete diff (2 days).",
        "createdAt" : "2020-03-30T13:06:21Z",
        "updatedAt" : "2020-03-30T16:21:45Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "40555a76-1621-401e-8574-52d7358edf72",
        "parentId" : "493cc9a8-a8e8-4714-9253-8dd9f49ebadd",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "let's document it clearly.",
        "createdAt" : "2020-03-30T13:13:39Z",
        "updatedAt" : "2020-03-30T16:21:45Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "2079c3b9-5eb0-4b65-bc34-eb9a4b73cd75",
        "parentId" : "493cc9a8-a8e8-4714-9253-8dd9f49ebadd",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "I updated comments for `gregJulianDiffSwitchDay` and `julianGregDiffSwitchDay`.",
        "createdAt" : "2020-03-30T16:17:52Z",
        "updatedAt" : "2020-03-30T16:21:45Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "b8fa18ee1968fb8b9aca84daa67f2419c16dca95",
    "line" : 31,
    "diffHunk" : "@@ -1,1 +1061,1065 @@  // The differences in days between Julian and Proleptic Gregorian dates.\n  // The diff at the index `i` is applicable for all days in the date interval:\n  // [julianGregDiffSwitchDay(i), julianGregDiffSwitchDay(i+1))\n  private val julianGregDiffs = Array(2, 1, 0, -1, -2, -3, -4, -5, -6, -7, -8, -9, -10, 0)\n  // The sorted days in Julian calendar when difference in days between Julian and"
  },
  {
    "id" : "fc5190ec-fdcf-450b-b0fe-4afba989b193",
    "prId" : 27915,
    "prUrl" : "https://github.com/apache/spark/pull/27915#pullrequestreview-375126521",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "28ead93b-271e-402d-ace2-2d7d87ea271e",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "We can use particular time zone here because the conversion of \"logical\" days is independent from time zone, actually. `UTC` is selected to avoid the problem of rounding micros to/from days because zone offset in UTC is 0.",
        "createdAt" : "2020-03-16T11:35:01Z",
        "updatedAt" : "2020-03-18T14:27:24Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "184fcd8a9d6d177352aa67732f974e7ebf8eaf20",
    "line" : 94,
    "diffHunk" : "@@ -1,1 +1000,1004 @@      // the Julian and Gregorian calendar systems\n      .setCalendarType(\"gregory\")\n      .setTimeZone(TimeZoneUTC)\n      .setInstant(Math.multiplyExact(days, MILLIS_PER_DAY))\n      .build()"
  },
  {
    "id" : "c0adcdb7-fdfe-42dd-b26f-ac6eaf404f80",
    "prId" : 27915,
    "prUrl" : "https://github.com/apache/spark/pull/27915#pullrequestreview-375390577",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "45becf9c-e1db-4cb8-a5ca-ef4681b82ca4",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "@cloud-fan I do believe this is a bug fix. We should rebase independently from new SQL config `spark.sql.legacy.parquet.rebaseDateTime.enabled` because days are stored as Julian days in Parquet INT96 timestamps.",
        "createdAt" : "2020-03-16T15:02:55Z",
        "updatedAt" : "2020-03-18T14:27:24Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "98538d51-851c-4d52-9b2d-9d634eee2b33",
        "parentId" : "45becf9c-e1db-4cb8-a5ca-ef4681b82ca4",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "This check\r\nhttps://github.com/apache/spark/pull/27915/files#diff-9bca6e248a070768e9f38ee5929411d8R897-R898\r\nreads INT96 timestamp saved by Spark 2.4.5 via:\r\n```scala\r\nscala> spark.conf.set(\"spark.sql.parquet.outputTimestampType\", \"INT96\")\r\n\r\nscala> val df = Seq(\"1001-01-01 01:02:03.123456\").toDF(\"tsS\").select($\"tsS\".cast(\"timestamp\").as(\"ts\"))\r\ndf: org.apache.spark.sql.DataFrame = [ts: timestamp]\r\n\r\nscala> df.write.parquet(\"/Users/maxim/tmp/before_1582/2_4_5_ts_int96\")\r\n\r\nscala> spark.read.parquet(\"/Users/maxim/tmp/before_1582/2_4_5_ts_int96\").show(false)\r\n+--------------------------+\r\n|ts                        |\r\n+--------------------------+\r\n|1001-01-01 01:02:03.123456|\r\n+--------------------------+\r\n```",
        "createdAt" : "2020-03-16T15:05:36Z",
        "updatedAt" : "2020-03-18T14:27:24Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "bd82a590-55da-4647-823b-62aabc6c8f82",
        "parentId" : "45becf9c-e1db-4cb8-a5ca-ef4681b82ca4",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "LGTM. INT96 is a legacy timestamp type in parquet and I'm not surprised if it follows the java 7 semantic.",
        "createdAt" : "2020-03-16T16:40:23Z",
        "updatedAt" : "2020-03-18T14:27:24Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "184fcd8a9d6d177352aa67732f974e7ebf8eaf20",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +150,154 @@    val seconds = (day - JULIAN_DAY_OF_EPOCH).toLong * SECONDS_PER_DAY\n    val micros = SECONDS.toMicros(seconds) + NANOSECONDS.toMicros(nanoseconds)\n    val rebased = rebaseJulianToGregorianMicros(micros)\n    rebased\n  }"
  },
  {
    "id" : "2953bf42-b4a7-4247-8850-1d4928553670",
    "prId" : 27807,
    "prUrl" : "https://github.com/apache/spark/pull/27807#pullrequestreview-372556263",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "34c947d0-c356-4d50-92fe-266de0f7f397",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@MaxGekk, so to clarify, are we going to follow the hybrid of Proleptic Gregorian calendar _and_ Julian calendars in Spark?\r\n\r\n```\r\nscala> sql(\"select date '1100-10-10'\").collect()\r\nres1: Array[org.apache.spark.sql.Row] = Array([1100-10-03])\r\n```\r\n\r\nisn't completely wrong in Proleptic Gregorian calendar up to my knowledge.\r\n",
        "createdAt" : "2020-03-10T12:11:42Z",
        "updatedAt" : "2020-03-11T06:21:33Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "3a9aef52-51dc-497a-80a0-002ed0e064cb",
        "parentId" : "34c947d0-c356-4d50-92fe-266de0f7f397",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Spark 3.0 is always Proleptic Gregorian calendar, this PR changes how Spark interacts with external datetime values (java `Date`/`Timestamp` for example).\r\n\r\n```\r\nscala> sql(\"select date '1100-10-10'\").collect()\r\nres1: Array[org.apache.spark.sql.Row] = Array([1100-10-03])\r\n```\r\nThis is wrong whatever calendar we use. Users write `1100-10-10` and then see `1100-10-03` without doing any operation. The reason is, java `Timestamp` uses hybrid calendar, and Spark needs to adjust the datetime values before outputing them to a system with different calendar.",
        "createdAt" : "2020-03-10T12:25:19Z",
        "updatedAt" : "2020-03-11T06:21:33Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "9db6b7de-51ae-4616-9f53-7444dee40159",
        "parentId" : "34c947d0-c356-4d50-92fe-266de0f7f397",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Five cents to Wenchen's answers from my side, the `java.sql.Timestamp`/`java.sql.Date`/`java.util.Date` classes have mixes semantics. They hold \"physical\" duration in milliseconds + nanos together with zoned date-time. The duration from the epoch does not depend on calendar. The classes uses calendar in sync/conversion to zoned date-time. For example, when you take the `year` component via `getYear()`:\r\n```java\r\n    public int getYear() {\r\n        return normalize().getYear() - 1900;\r\n    }\r\n```\r\nIt sets `year`, `month`, `hour` ... in the `normalize` methods using the combined calendar Julian + Gregorian.\r\n\r\nIf you use `java.sql.Timestamp` as a \"container\" for duration, it is ok. For example, we can return `java.sql.Timestamp`/`Date` in `collect()` and extract duration:\r\n```scala\r\nscala> val ts = sql(\"select date '1100-10-10'\").collect()(0).getDate(0)\r\nts: java.sql.Date = 1100-10-03\r\nscala> java.time.Instant.ofEpochMilli(ts.getTime).atZone(java.time.ZoneId.of(\"Europe/Moscow\")).toLocalDate\r\nres2: java.time.LocalDate = 1100-10-10\r\n```\r\nbut if you are going to use some methods of `java.sql.Timestamps`/`java.sql.Date`, they may return unexpected values for dates before `1582-10-15`.\r\n\r\nIn the proposed PR, I changed the ways of initialization of `java.sql.Timestamp`/`Date` - from duration to local date-time. So, if we have an instant which is mapped to 1582-01-01 in Gregorian calendar, we take the local date and interpret it as local date `1582-01-01` in Julian calendar. In this way, date-time components returned by Spark SQL functions like `year()`, `months()` and etc. will equal to values returned by `java.sql.Date.getYear`, `java.sql.Date.getMonth` and etc.\r\n\r\nOf course, the duration in `java.sql.Timestamp`/`java.sql.Date` returned by the `getTime()` method will be different from the values stored in Spark's TIMESTAMP/DATE columns.",
        "createdAt" : "2020-03-10T17:21:35Z",
        "updatedAt" : "2020-03-11T06:21:33Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "834f1e58-f0d8-46da-9fde-57f82e9ac8c3",
        "parentId" : "34c947d0-c356-4d50-92fe-266de0f7f397",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Okay. But why should we change the physical value of the timestamp, instead of doing this at actual time-related operations as we do for timezone? ",
        "createdAt" : "2020-03-11T08:26:58Z",
        "updatedAt" : "2020-03-11T08:28:49Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "8811fee4-cbcc-4ec2-b16f-6db51efc18f5",
        "parentId" : "34c947d0-c356-4d50-92fe-266de0f7f397",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Once Spark writes out datetime values to `Date`/`Timestamp`, users have already taken over the control and they can do anything they want. So we must adjust the UTC seconds when we create `Date`/`Timestamp` objects.",
        "createdAt" : "2020-03-11T08:34:57Z",
        "updatedAt" : "2020-03-11T08:34:58Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "407cc1fd8584f2a045b8db74cfe50023f13b7919",
    "line" : 67,
    "diffHunk" : "@@ -1,1 +139,143 @@    } else {\n      instantToMicros(t.toInstant)\n    }\n  }\n"
  },
  {
    "id" : "51f63d9a-9e9b-4f9d-a045-ef519d996ddb",
    "prId" : 27753,
    "prUrl" : "https://github.com/apache/spark/pull/27753#pullrequestreview-367003551",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "deab2907-1828-4fc4-aa21-15731287ccb1",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "`getZoneId()` is able to handle zone offsets w/ prefix `-` and `+` but it doesn't support the format `7:3` like in https://github.com/apache/spark/blob/ffc0935e64ddd56c40db1c1c7f3afb7e5e306861/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CastSuite.scala#L164 . So, I have to keep the code for backward compatibility.",
        "createdAt" : "2020-03-02T08:00:46Z",
        "updatedAt" : "2020-03-04T14:13:06Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "00a07ab500b63cb13edcd8bf71ad709d008e2d5b",
    "line" : 51,
    "diffHunk" : "@@ -1,1 +243,247 @@          }\n        } else if (i == 5 || i == 6) {\n          if (b == '-' || b == '+') {\n            segments(i) = currentSegmentValue\n            currentSegmentValue = 0"
  },
  {
    "id" : "a4121c2c-8e9d-4db9-a4b0-dc4deefaed2c",
    "prId" : 27753,
    "prUrl" : "https://github.com/apache/spark/pull/27753#pullrequestreview-368796106",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "de3b1adb-8e07-49ba-aa82-f55f9f8178c0",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "is `+` required or optional?",
        "createdAt" : "2020-03-04T10:22:35Z",
        "updatedAt" : "2020-03-04T14:13:06Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "6db3f18c-6a15-4390-b497-0007ada5a41e",
        "parentId" : "de3b1adb-8e07-49ba-aa82-f55f9f8178c0",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "It is required:\r\n```scala\r\nscala> stringToTimestamp(UTF8String.fromString(\"2020-03-04 10:11:12.00000010:00\"), ZoneOffset.UTC)\r\nres2: Option[org.apache.spark.sql.catalyst.util.DateTimeUtils.SQLTimestamp] = None\r\n\r\nscala> stringToTimestamp(UTF8String.fromString(\"2020-03-04 10:11:12.000000+10:00\"), ZoneOffset.UTC)\r\nres3: Option[org.apache.spark.sql.catalyst.util.DateTimeUtils.SQLTimestamp] = Some(1583280672000000)\r\n```",
        "createdAt" : "2020-03-04T13:56:30Z",
        "updatedAt" : "2020-03-04T14:13:06Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "00a07ab500b63cb13edcd8bf71ad709d008e2d5b",
    "line" : 27,
    "diffHunk" : "@@ -1,1 +178,182 @@   * where `zone_id` should have one of the forms:\n   *   - Z - Zulu time zone UTC+0\n   *   - +|-[h]h:[m]m\n   *   - A short id, see https://docs.oracle.com/javase/8/docs/api/java/time/ZoneId.html#SHORT_IDS\n   *   - An id with one of the prefixes UTC+, UTC-, GMT+, GMT-, UT+ or UT-,"
  },
  {
    "id" : "75b6c379-3098-4ba1-a5a7-90fccc9281f5",
    "prId" : 27753,
    "prUrl" : "https://github.com/apache/spark/pull/27753#pullrequestreview-368898417",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "747917f2-dfb3-46cc-ac10-f4ebc497b7f3",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "why not just `b.toChar.toString`?",
        "createdAt" : "2020-03-04T14:51:35Z",
        "updatedAt" : "2020-03-04T14:51:35Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "16c6328a-ecdd-4288-b50e-42b59f9bcec4",
        "parentId" : "747917f2-dfb3-46cc-ac10-f4ebc497b7f3",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Just for consistency with another change",
        "createdAt" : "2020-03-04T15:53:33Z",
        "updatedAt" : "2020-03-04T15:53:33Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "00a07ab500b63cb13edcd8bf71ad709d008e2d5b",
    "line" : 57,
    "diffHunk" : "@@ -1,1 +247,251 @@            currentSegmentValue = 0\n            i += 1\n            tz = Some(new String(bytes, j, 1))\n          } else if (b == '.' && i == 5) {\n            segments(i) = currentSegmentValue"
  },
  {
    "id" : "e890966e-66f2-4bba-94c1-5702e5667922",
    "prId" : 27710,
    "prUrl" : "https://github.com/apache/spark/pull/27710#pullrequestreview-367822880",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "08c948ed-4935-4a7b-8520-3221d4ba9b4d",
        "parentId" : null,
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "There are 3 cases for GMT formatting, `GMT+8`, `GMT+08:00` and `GMT`, do we need to check all in UnivocityParserSuite?",
        "createdAt" : "2020-03-03T01:55:42Z",
        "updatedAt" : "2020-03-03T02:19:02Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      },
      {
        "id" : "8f66d9cd-0d05-4ad2-a637-2a50011a29ef",
        "parentId" : "08c948ed-4935-4a7b-8520-3221d4ba9b4d",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "This will be fixed by #27753",
        "createdAt" : "2020-03-03T07:02:14Z",
        "updatedAt" : "2020-03-03T07:02:14Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "a5c19be6-89e5-4928-a21a-23786bebc713",
        "parentId" : "08c948ed-4935-4a7b-8520-3221d4ba9b4d",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "Oh yeah, got it thanks.",
        "createdAt" : "2020-03-03T09:33:58Z",
        "updatedAt" : "2020-03-03T09:33:59Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "16db32f50449bb4bb721101c709ea56cc305d324",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +171,175 @@      // ISO8601 with a weird time zone specifier (2000-01-01T00:00GMT+01:00)\n      val s0 = s.substring(0, indexOfGMT)\n      val s1 = s.substring(indexOfGMT + 3)\n      // Mapped to 2000-01-01T00:00+01:00\n      s0 + s1"
  }
]