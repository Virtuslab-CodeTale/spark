[
  {
    "id" : "571a1568-4372-4552-9d76-08e3357330d4",
    "prId" : 33213,
    "prUrl" : "https://github.com/apache/spark/pull/33213#pullrequestreview-698945652",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "862688d0-b05f-420c-9f50-e4db24778eef",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "This is to match https://github.com/apache/spark/pull/33213/files#diff-583171e935b2dc349378063a5841c5b98b30a2d57ac3743a9eccfe7bffcb8f2aL472",
        "createdAt" : "2021-07-05T08:51:05Z",
        "updatedAt" : "2021-07-05T08:51:06Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "52383baf47bdea4097a9ebbcd8722d03ca1c2b9c",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +373,377 @@\n            case _ =>\n              throw QueryCompilationErrors.invalidFieldName(fieldNames, normalizedPath)\n          }\n        }"
  },
  {
    "id" : "5d567a79-e1eb-44e5-b46b-639ce102956d",
    "prId" : 32448,
    "prUrl" : "https://github.com/apache/spark/pull/32448#pullrequestreview-658528270",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d586c077-6059-4e82-aea0-8c4376c9b1ab",
        "parentId" : null,
        "authorId" : "92aa9573-b64a-4a86-aebf-54c827df3ed0",
        "body" : "I can add tests for this if we want to keep this behavior",
        "createdAt" : "2021-05-06T01:36:25Z",
        "updatedAt" : "2021-05-06T01:36:25Z",
        "lastEditedBy" : "92aa9573-b64a-4a86-aebf-54c827df3ed0",
        "tags" : [
        ]
      },
      {
        "id" : "e2b60d31-03b3-437a-bc93-ba9205b988aa",
        "parentId" : "d586c077-6059-4e82-aea0-8c4376c9b1ab",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "+1 for adding some tests for it.",
        "createdAt" : "2021-05-10T19:51:39Z",
        "updatedAt" : "2021-05-10T19:51:39Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "1dc06b2e-5d0f-4f59-b58c-f253803c70b2",
        "parentId" : "d586c077-6059-4e82-aea0-8c4376c9b1ab",
        "authorId" : "92aa9573-b64a-4a86-aebf-54c827df3ed0",
        "body" : "Done",
        "createdAt" : "2021-05-13T02:09:15Z",
        "updatedAt" : "2021-05-13T02:09:15Z",
        "lastEditedBy" : "92aa9573-b64a-4a86-aebf-54c827df3ed0",
        "tags" : [
        ]
      }
    ],
    "commit" : "5762ddc83e96b640ffdd4125c182646bf9fb03ff",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +556,560 @@    }\n\n  private[sql] def merge(resolver: Resolver)(left: DataType, right: DataType): DataType =\n    (left, right) match {\n      case (ArrayType(leftElementType, leftContainsNull),"
  },
  {
    "id" : "f75b8afc-378d-46f1-bbe8-98f2acfba675",
    "prId" : 29587,
    "prUrl" : "https://github.com/apache/spark/pull/29587#pullrequestreview-478753912",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f4d65cf2-f9a5-48f7-970f-2a5f811cf100",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "~Where does this limitation come?; we don't need to support this case, or supporting it is technically difficult?~ Ah, I see. Is this an unsupported case, right?\r\nhttps://github.com/apache/spark/pull/29587/files#diff-4d656d696512d6bcb03a48f7e0af6251R106-R107",
        "createdAt" : "2020-08-31T08:39:21Z",
        "updatedAt" : "2020-10-16T19:47:41Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "acf23c99-a5b1-4739-a362-53a8b64b0f95",
        "parentId" : "f4d65cf2-f9a5-48f7-970f-2a5f811cf100",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "I leverage `WithFields` to add missing nested fields into structs. `WithFields` doesn't support array or map types currently.",
        "createdAt" : "2020-08-31T16:16:39Z",
        "updatedAt" : "2020-10-16T19:47:41Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "3d907d07233851d6169533d0475c5a53b02cb4a7",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +645,649 @@  /**\n   * Returns a `StructType` that contains missing fields recursively from `source` to `target`.\n   * Note that this doesn't support looking into array type and map type recursively.\n   */\n  def findMissingFields("
  },
  {
    "id" : "f8d7f61c-7f9a-4ee9-b871-e20e2b0e15c6",
    "prId" : 27350,
    "prUrl" : "https://github.com/apache/spark/pull/27350#pullrequestreview-350793183",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ff2a8c7e-ecfe-4532-9ee4-b57376797ee4",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "can we reuse `CatalogV2Implicits.MultipartIdentifierHelper.quoted` ?",
        "createdAt" : "2020-01-30T12:42:31Z",
        "updatedAt" : "2020-01-30T18:27:08Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "2b364e23927fbcdd8a6d219c9ae6bd861dbe3f17",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +319,323 @@      includeCollections: Boolean = false,\n      resolver: Resolver = _ == _): Option[(Seq[String], StructField)] = {\n    def prettyFieldName(nameParts: Seq[String]): String = {\n      import org.apache.spark.sql.connector.catalog.CatalogV2Implicits._\n      nameParts.quoted"
  },
  {
    "id" : "b6001aba-62c0-4119-9bb3-ffa79ba12429",
    "prId" : 27117,
    "prUrl" : "https://github.com/apache/spark/pull/27117#pullrequestreview-339461305",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3d493ef7-fc68-49b4-8214-1ae95dca5cba",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "```scala\r\n    val stringConcat = new StringUtils.StringConcat()\r\n    val len = fields.length\r\n    stringConcat.append(\"struct<\")\r\n    var i = 0\r\n    while (i < len) {\r\n      stringConcat.append(s\"${fields(i).name}:${fields(i).dataType.catalogString}\")\r\n      i += 1\r\n      if (i < len) stringConcat.append(\",\")\r\n    }\r\n    stringConcat.append(\">\")\r\n    stringConcat.toString\r\n```",
        "createdAt" : "2020-01-07T19:34:36Z",
        "updatedAt" : "2020-01-07T19:51:29Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "ac600fec-3eb7-4d0c-ab70-ff71c648b386",
        "parentId" : "3d493ef7-fc68-49b4-8214-1ae95dca5cba",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Thanks! I was wanting to use `init` to make it looks simple. Now seems not simpler than while loop...",
        "createdAt" : "2020-01-07T19:39:03Z",
        "updatedAt" : "2020-01-07T19:51:29Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "5f25d79e6908778303a205b4db077fcd4ead2d65",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +406,410 @@  }\n\n  override def catalogString: String = {\n    // in catalogString, we should not truncate\n    val stringConcat = new StringUtils.StringConcat()"
  },
  {
    "id" : "6f3fb6f4-d505-4453-b6f5-18482ebd3eb3",
    "prId" : 26644,
    "prUrl" : "https://github.com/apache/spark/pull/26644#pullrequestreview-321946909",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a5c2490d-4aff-4dec-95b3-a59761827577",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Since this is beyond our existing rule, shall we update the function description accordingly?\r\n- https://github.com/apache/spark/pull/26644/files#diff-e9d42ebfb254a463cccff76e8983242cR447-R455",
        "createdAt" : "2019-11-23T23:36:33Z",
        "updatedAt" : "2019-12-30T16:20:27Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "4af2ee66-33fc-44b8-a127-dcbca3b5f631",
        "parentId" : "a5c2490d-4aff-4dec-95b3-a59761827577",
        "authorId" : "e29a7794-ac2e-4e6f-a690-737e83e1bace",
        "body" : "I've added a Scaladoc to the private merge function. I think the Javadoc that you're pointing to, is describing the function at a different level. For example, it doesn't mention any UDT's at all. Let me know what you think.",
        "createdAt" : "2019-11-24T08:10:17Z",
        "updatedAt" : "2019-12-30T16:20:27Z",
        "lastEditedBy" : "e29a7794-ac2e-4e6f-a690-737e83e1bace",
        "tags" : [
        ]
      }
    ],
    "commit" : "dd3c913f0c552c975cd96cde554ba90f99b85bab",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +598,602 @@        if leftUdt.userClass == rightUdt.userClass => leftUdt\n\n      case (leftType, rightUdt: UserDefinedType[_])\n        if leftType == rightUdt.sqlType => leftType\n"
  },
  {
    "id" : "4fd9d049-68bc-4e32-a754-3a080f82e9a7",
    "prId" : 26644,
    "prUrl" : "https://github.com/apache/spark/pull/26644#pullrequestreview-322925515",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d4ef262d-6e31-498f-9883-836ce0bf46e4",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "The statements below are duplicate with ones in L447-L455. Can you add some statements for this fix based on the existing  L447-L455 statements instead of totally re-writing comments?",
        "createdAt" : "2019-11-26T00:14:29Z",
        "updatedAt" : "2019-12-30T16:20:27Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "6d5c507e-e276-4bda-b38d-6b053e5adf8b",
        "parentId" : "d4ef262d-6e31-498f-9883-836ce0bf46e4",
        "authorId" : "e29a7794-ac2e-4e6f-a690-737e83e1bace",
        "body" : "Okay, I've added a statement to the existing comment",
        "createdAt" : "2019-11-26T11:58:20Z",
        "updatedAt" : "2019-12-30T16:20:27Z",
        "lastEditedBy" : "e29a7794-ac2e-4e6f-a690-737e83e1bace",
        "tags" : [
        ]
      }
    ],
    "commit" : "dd3c913f0c552c975cd96cde554ba90f99b85bab",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +456,460 @@   * 4. Otherwise, `this` and `that` are considered as conflicting schemas and an exception would be\n   *    thrown.\n   *\n   * @throws org.apache.spark.SparkException In case the DataTypes are incompatible\n   * @return The compatible DataType that support both left and right"
  },
  {
    "id" : "63e88e43-0fce-469c-9440-466deb5b0dbb",
    "prId" : 26644,
    "prUrl" : "https://github.com/apache/spark/pull/26644#pullrequestreview-338435947",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "184b55c9-bb6b-4ae2-9ed8-7bf433264aa3",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@Fokko, sorry for my late response. I doubt if we should allow this case.\r\n\r\nCurrently, `merge` only allows the same types but UDT <> UDT's SQL types are not the same types. I think it makes less sense to allow this case alone.\r\n\r\nAlso, this https://github.com/apache/spark/pull/26644/files#r350486670 looks weird. `jsonValue` seems it should have JSON-serialized value of its own type.",
        "createdAt" : "2020-01-03T07:24:33Z",
        "updatedAt" : "2020-01-03T07:24:34Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "8a520407-7be3-4772-ae10-f7ff918e6bd3",
        "parentId" : "184b55c9-bb6b-4ae2-9ed8-7bf433264aa3",
        "authorId" : "e29a7794-ac2e-4e6f-a690-737e83e1bace",
        "body" : "Well, the `UserDefinedType` extends `DataType`, similar to `TimestampType`, `StringType`, and any other type. The thing is that a UserDefinedType can be compatible with any other type. For example, it is allowed to merge an int into a long. This is an explicit choice by the developer.",
        "createdAt" : "2020-01-05T19:04:35Z",
        "updatedAt" : "2020-01-05T19:04:36Z",
        "lastEditedBy" : "e29a7794-ac2e-4e6f-a690-737e83e1bace",
        "tags" : [
        ]
      },
      {
        "id" : "ba089b3d-e3b2-4fb7-aeea-553efc0903c4",
        "parentId" : "184b55c9-bb6b-4ae2-9ed8-7bf433264aa3",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "> it is allowed to merge an int into a long.\r\n\r\nBut this `StructType.merge` does not allow such type merging. Given that, it looks weird to allow only UDT.\r\n\r\n```scala\r\nscala> import org.apache.spark.sql.types._\r\nimport org.apache.spark.sql.types._\r\n\r\nscala> StructType.merge(LongType, IntegerType)\r\norg.apache.spark.SparkException: Failed to merge incompatible data types bigint and int\r\n  at org.apache.spark.sql.types.StructType$.merge(StructType.scala:600)\r\n  ... 49 elided\r\n```\r\n",
        "createdAt" : "2020-01-06T01:08:57Z",
        "updatedAt" : "2020-01-06T01:08:58Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "dd3c913f0c552c975cd96cde554ba90f99b85bab",
    "line" : 26,
    "diffHunk" : "@@ -1,1 +602,606 @@\n      case (leftUdt: UserDefinedType[_], rightType)\n        if leftUdt.sqlType == rightType => rightType\n\n      case (leftType, rightType) if leftType == rightType =>"
  }
]