[
  {
    "id" : "72d51ee4-af63-4579-94d9-d47d1a669573",
    "prId" : 32299,
    "prUrl" : "https://github.com/apache/spark/pull/32299#pullrequestreview-643202911",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5211f421-9291-4d71-8430-a4045f78604a",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "`JacksonGenerator` isn't an API so it doesn't make much sense to make it pluggable",
        "createdAt" : "2021-04-23T09:21:44Z",
        "updatedAt" : "2021-04-23T09:21:45Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "14c6c649-9b03-46c6-9f30-bc93a4e83bcd",
        "parentId" : "5211f421-9291-4d71-8430-a4045f78604a",
        "authorId" : "7e0b334a-95b7-401a-bdd1-9fdc370bcbe0",
        "body" : "As far I can see datasources are sometimes implemented under `org.apache.spark.sql`, thus accessing private and protected members, ie. https://github.com/datastax/spark-cassandra-connector/tree/master/connector/src/main/scala/org/apache/spark/sql/cassandra\r\nOf course relying on private API could lead to unpredictable breaking changes. Ideally `JacksonParser` and `JacksonGenerator` could evolve to a public API usable by any binary dataformat implementation.",
        "createdAt" : "2021-04-23T11:36:27Z",
        "updatedAt" : "2021-04-23T11:36:27Z",
        "lastEditedBy" : "7e0b334a-95b7-401a-bdd1-9fdc370bcbe0",
        "tags" : [
        ]
      }
    ],
    "commit" : "1bb7953e1a46f01710f50479a05a83e49ec47014",
    "line" : 2,
    "diffHunk" : "@@ -1,1 +36,40 @@ * a `MapType`, and vice verse.\n */\nprivate[sql] class JacksonGenerator(\n    dataType: DataType,\n    generator: JsonGenerator,"
  },
  {
    "id" : "4a2b59dc-4f13-4a05-8778-2157ab70e32b",
    "prId" : 32147,
    "prUrl" : "https://github.com/apache/spark/pull/32147#pullrequestreview-636400963",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6563755e-1696-4ab7-949e-9c03e4c8da30",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "This code means that we cannot set both `options.pretty` and `options.writeNonAsciiCharacterAsCodePoint`. If this is true, shall we document it somewhere?",
        "createdAt" : "2021-04-15T04:55:37Z",
        "updatedAt" : "2021-04-27T21:06:51Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "04c3d060-8182-4f2c-a34e-9e27db92e37a",
        "parentId" : "6563755e-1696-4ab7-949e-9c03e4c8da30",
        "authorId" : "3a227965-84e0-47cf-9974-11293764f028",
        "body" : "We can set both options. Thanks. I'll fix it.",
        "createdAt" : "2021-04-16T01:27:21Z",
        "updatedAt" : "2021-04-27T21:06:51Z",
        "lastEditedBy" : "3a227965-84e0-47cf-9974-11293764f028",
        "tags" : [
        ]
      }
    ],
    "commit" : "93efbb3c72edd58804d70d683de76d767eb8dc85",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +79,83 @@    }\n    if (options.writeNonAsciiCharacterAsCodePoint) {\n      generator.setHighestNonEscapedChar(0x7F)\n    }\n    generator"
  },
  {
    "id" : "aab1a865-c747-4cc7-a782-82f2a3ec9b82",
    "prId" : 26418,
    "prUrl" : "https://github.com/apache/spark/pull/26418#pullrequestreview-318174799",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fcf2988a-aa1c-478f-b01a-acae335af9a0",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "let's deal with json in another PR.",
        "createdAt" : "2019-11-12T16:51:09Z",
        "updatedAt" : "2019-11-16T15:45:17Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "27cb2fa8-dde4-429c-aa49-2d9b9a6a76c9",
        "parentId" : "fcf2988a-aa1c-478f-b01a-acae335af9a0",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "I guess this comment https://github.com/apache/spark/pull/26102#discussion_r344585132 is valid for JSON datasource as well.",
        "createdAt" : "2019-11-12T18:45:07Z",
        "updatedAt" : "2019-11-16T15:45:17Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "5d3961f4-379c-4c1a-b3ed-4b55b50409df",
        "parentId" : "fcf2988a-aa1c-478f-b01a-acae335af9a0",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Why wasn't this comment addressed? ",
        "createdAt" : "2019-11-18T08:57:00Z",
        "updatedAt" : "2019-11-18T08:57:00Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "0f54af89b8641ee7df80991dcca63e183a03d5ff",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +120,124 @@        gen.writeNumber(row.getDouble(ordinal))\n\n    case CalendarIntervalType =>\n      (row: SpecializedGetters, ordinal: Int) =>\n        gen.writeString(IntervalUtils.toMultiUnitsString(row.getInterval(ordinal)))"
  },
  {
    "id" : "9219d825-6507-4468-b142-2286889a9f2f",
    "prId" : 26418,
    "prUrl" : "https://github.com/apache/spark/pull/26418#pullrequestreview-318184713",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4b057304-0c6c-482f-b9e0-9152b18d6503",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "This can't happen actually. We don't allow writing out interval values. Do you have an example that can hit this code path?",
        "createdAt" : "2019-11-13T16:41:34Z",
        "updatedAt" : "2019-11-16T15:45:17Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "611c6b78-39f6-4c55-aa3c-197bb51c7a50",
        "parentId" : "4b057304-0c6c-482f-b9e0-9152b18d6503",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "https://github.com/apache/spark/pull/26418/commits/3aa723ca6a811898ea4f8c2d6100c7e4045b0c30#diff-8a461909033e3f735f8239825970c378R218-L229",
        "createdAt" : "2019-11-13T16:57:03Z",
        "updatedAt" : "2019-11-16T15:45:17Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "289903df-d111-4c96-a79e-516356b3ff57",
        "parentId" : "4b057304-0c6c-482f-b9e0-9152b18d6503",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "ah `to_json` do not have the interval type check. makes sense.",
        "createdAt" : "2019-11-13T17:12:39Z",
        "updatedAt" : "2019-11-16T15:45:17Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "568bec58-f11b-4272-bd75-7dd0cfa70f69",
        "parentId" : "4b057304-0c6c-482f-b9e0-9152b18d6503",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@yaooqinn, how about `to_csv`?",
        "createdAt" : "2019-11-18T09:15:30Z",
        "updatedAt" : "2019-11-18T09:15:30Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "0f54af89b8641ee7df80991dcca63e183a03d5ff",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +220,224 @@    val keyArray = map.keyArray()\n    val keyString = mapType.keyType match {\n      case CalendarIntervalType =>\n        (i: Int) => IntervalUtils.toMultiUnitsString(keyArray.getInterval(i))\n      case _ => (i: Int) => keyArray.get(i, mapType.keyType).toString"
  },
  {
    "id" : "b043ae48-60b8-43cb-baea-89b633f2b2c1",
    "prId" : 26418,
    "prUrl" : "https://github.com/apache/spark/pull/26418#pullrequestreview-318290194",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e12d388e-3793-495f-935c-4a2ec626395e",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "it's fragile to rely on `toString`. e.g. `UnsafeRow.toString` is not human readable. Shall we recursively write map key as json object? cc @HyukjinKwon ",
        "createdAt" : "2019-11-13T17:15:45Z",
        "updatedAt" : "2019-11-16T15:45:17Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "26ca8566-698d-4ae6-9887-461d15c55340",
        "parentId" : "e12d388e-3793-495f-935c-4a2ec626395e",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Ah, I am sorry I missed this cc. in JSON the key should be a string. We should either make it string always or explicitly disallow.",
        "createdAt" : "2019-11-18T08:45:12Z",
        "updatedAt" : "2019-11-18T08:45:13Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "6ddb265c-5ff8-4104-a64a-c1827cfe0e15",
        "parentId" : "e12d388e-3793-495f-935c-4a2ec626395e",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "cc @viirya I think we talked about this before.",
        "createdAt" : "2019-11-18T08:45:56Z",
        "updatedAt" : "2019-11-18T08:45:57Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "a5e631f9-6d9d-47bb-a65d-492d82ed8871",
        "parentId" : "e12d388e-3793-495f-935c-4a2ec626395e",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Yea, I think currently the map key is not very useful for some types. To make human readable map keys, we need do specific serialization for some map key types. Maybe I create a JIRA ticket to follow it up?\r\n",
        "createdAt" : "2019-11-18T10:28:24Z",
        "updatedAt" : "2019-11-18T10:28:24Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "80070ec7-012d-4406-977c-b320e758f834",
        "parentId" : "e12d388e-3793-495f-935c-4a2ec626395e",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Yeah .. +1 !",
        "createdAt" : "2019-11-18T10:31:59Z",
        "updatedAt" : "2019-11-18T10:32:00Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "bd267fb9-d8cb-4e9c-acfe-f3398cd51543",
        "parentId" : "e12d388e-3793-495f-935c-4a2ec626395e",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Created https://issues.apache.org/jira/browse/SPARK-29946 to follow it up.",
        "createdAt" : "2019-11-18T12:21:00Z",
        "updatedAt" : "2019-11-18T12:21:01Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "0f54af89b8641ee7df80991dcca63e183a03d5ff",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +222,226 @@      case CalendarIntervalType =>\n        (i: Int) => IntervalUtils.toMultiUnitsString(keyArray.getInterval(i))\n      case _ => (i: Int) => keyArray.get(i, mapType.keyType).toString\n    }\n    val valueArray = map.valueArray()"
  },
  {
    "id" : "08ffa2e3-dc9a-4654-921a-c679f945e1b1",
    "prId" : 26418,
    "prUrl" : "https://github.com/apache/spark/pull/26418#pullrequestreview-318169946",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d4a3b091-fc30-468a-9706-2eb7d11b45fa",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "This code path shouldn't be here per each map here BTW.",
        "createdAt" : "2019-11-18T08:47:38Z",
        "updatedAt" : "2019-11-18T08:47:39Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "0f54af89b8641ee7df80991dcca63e183a03d5ff",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +222,226 @@      case CalendarIntervalType =>\n        (i: Int) => IntervalUtils.toMultiUnitsString(keyArray.getInterval(i))\n      case _ => (i: Int) => keyArray.get(i, mapType.keyType).toString\n    }\n    val valueArray = map.valueArray()"
  },
  {
    "id" : "9bcecc61-a1f6-4c10-bc1e-e565abe59591",
    "prId" : 25708,
    "prUrl" : "https://github.com/apache/spark/pull/25708#pullrequestreview-285224533",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0c83b13a-ab5e-43ad-9a0a-6833391ae800",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "ditto: add tests?",
        "createdAt" : "2019-09-08T01:34:31Z",
        "updatedAt" : "2019-09-18T15:49:31Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "140f3a33-2e98-4a01-a054-e14433af1181",
        "parentId" : "0c83b13a-ab5e-43ad-9a0a-6833391ae800",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Added tests for `from_json`",
        "createdAt" : "2019-09-08T16:31:07Z",
        "updatedAt" : "2019-09-18T15:49:31Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "ff925318b3258ade6b2db13489b078f394a1fdbd",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +85,89 @@    options.dateFormat,\n    options.zoneId,\n    options.locale)\n\n  private def makeWriter(dataType: DataType): ValueWriter = dataType match {"
  }
]