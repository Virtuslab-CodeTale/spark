[
  {
    "id" : "e55547b7-af1c-462f-b86e-e9e069bf5854",
    "prId" : 33115,
    "prUrl" : "https://github.com/apache/spark/pull/33115#pullrequestreview-694068129",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7e93b888-ba23-4f38-a4b4-d6e728da4d5a",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Can we use a type collection like `TypeCollection.NumericAndInterval`?",
        "createdAt" : "2021-06-28T14:00:43Z",
        "updatedAt" : "2021-06-28T14:00:43Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "9f3fbaf1-35cb-41a0-9bab-86965be85877",
        "parentId" : "7e93b888-ba23-4f38-a4b4-d6e728da4d5a",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "I do have introduced `TypeCollection.AllTimestampTypes` in a previous PR.\r\nHowever, in this PR I find that having `AnyTimestampType` can simplify the code changes in `TypeCoercion.scala`. \r\n",
        "createdAt" : "2021-06-28T14:32:24Z",
        "updatedAt" : "2021-06-28T14:32:24Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "cbb18a00-0868-43f2-81ec-870f73e49a76",
        "parentId" : "7e93b888-ba23-4f38-a4b4-d6e728da4d5a",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Are we going to use `AnyTimestampType` everywhere and remove `AllTimestampTypes`?",
        "createdAt" : "2021-06-28T14:50:14Z",
        "updatedAt" : "2021-06-28T14:50:14Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "498df07e-c958-4968-8025-f5f2c911f597",
        "parentId" : "7e93b888-ba23-4f38-a4b4-d6e728da4d5a",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "Yes, I will have a follow-up for that.\r\nThere is another reason, in the default type coercion rules, when implicit casting a type to a `TypeCollection` type, Spark chooses the first convertible data type as the result. If we are going to make the default timestamp type configurable, having `AnyTimestampType` is better.",
        "createdAt" : "2021-06-28T15:00:21Z",
        "updatedAt" : "2021-06-28T15:00:21Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "50784f72-caad-4b02-bbfc-4e8a4bc3006c",
        "parentId" : "7e93b888-ba23-4f38-a4b4-d6e728da4d5a",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "So we should have such a new type for types of the same category, like `NumericType` and `IntegralType`.\r\nIf the data types are of different categories, it's fine to use `TypeCollection`",
        "createdAt" : "2021-06-28T15:02:31Z",
        "updatedAt" : "2021-06-28T15:02:40Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "52267f1e1b49af02981cb06aed2d4c680008a24d",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +219,223 @@}\n\nprivate[sql] object AnyTimestampType extends AbstractDataType with Serializable {\n  override private[sql] def defaultConcreteType: DataType = TimestampWithoutTZType\n"
  }
]