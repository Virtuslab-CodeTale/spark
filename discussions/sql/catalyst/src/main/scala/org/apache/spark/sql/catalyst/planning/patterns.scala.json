[
  {
    "id" : "88f4f848-3f5d-492d-912a-0f0950b5ab5d",
    "prId" : 29950,
    "prUrl" : "https://github.com/apache/spark/pull/29950#pullrequestreview-514095514",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5e7f1a49-2ff9-47e3-9aad-cb37577122b5",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "`PhysicalOperation` does not have the same issue?",
        "createdAt" : "2020-10-21T01:14:21Z",
        "updatedAt" : "2020-11-13T00:06:09Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "703b44d4-ee9b-4e7e-be3c-c0346672837d",
        "parentId" : "5e7f1a49-2ff9-47e3-9aad-cb37577122b5",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "I'm wondering how we decide to use `PhysicalOperation` or `ScanOperation`?\r\n\r\nI can see `ScanOperation` is used for re-construct Project/Filter around scan node. `PhysicalOperation` has mixed use cases, including re-construct Project/Filter around scan like `ScanOperation` (so can we replace `PhysicalOperation` with `ScanOperation` in these cases?), and others e.g. `OptimizeMetadataOnlyQuery`.\r\n\r\n ",
        "createdAt" : "2020-10-21T07:03:02Z",
        "updatedAt" : "2020-11-13T00:06:09Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "bd199106-82b2-44b5-bf73-902d682fbafe",
        "parentId" : "5e7f1a49-2ff9-47e3-9aad-cb37577122b5",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "So before we have clearer idea about `PhysicalOperation` and `ScanOperation`, I will leave `PhysicalOperation` as it is. ",
        "createdAt" : "2020-10-21T19:07:33Z",
        "updatedAt" : "2020-11-13T00:06:09Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "bbaae3ed9b96ca517ec5435838ac37feb578c959",
    "line" : 40,
    "diffHunk" : "@@ -1,1 +152,156 @@            // maximum allowed common outputs.\n            if (!hasCommonNonDeterministic(fields, aliases) &&\n                !moreThanMaxAllowedCommonOutput(fields, aliases)) {\n              val substitutedFields =\n                fields.map(substitute(aliases)).asInstanceOf[Seq[NamedExpression]]"
  },
  {
    "id" : "2ad5c226-04c3-40d2-b271-c822090e5d64",
    "prId" : 29304,
    "prUrl" : "https://github.com/apache/spark/pull/29304#pullrequestreview-459613992",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "00c9b975-81d4-4f8c-80e6-c2e68d13e422",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "nit format; how about this?\r\n```\r\n        val joinKeys = ArrayBuffer[(Expression, Expression)]()\r\n\r\n        // All predicate must match pattern condition: Or(EqualTo(a=b), IsNull(EqualTo(a=b)))\r\n        val allMatch = predicates.forall {\r\n          case Or(e @ EqualTo(leftExpr: Expression, rightExpr: Expression),\r\n              IsNull(e2 @ EqualTo(_, _))) if e.semanticEquals(e2) =>\r\n            if (canEvaluate(leftExpr, left) && canEvaluate(rightExpr, right)) {\r\n              joinKeys += ((leftExpr, rightExpr))\r\n              true\r\n            } else if (canEvaluate(leftExpr, right) && canEvaluate(rightExpr, left)) {\r\n              joinKeys += ((rightExpr, leftExpr))\r\n              true\r\n            } else {\r\n              false\r\n            }\r\n          case _ =>\r\n            false\r\n        }\r\n\r\n        if (allMatch) {\r\n          Some(joinKeys.unzip)\r\n        } else {\r\n          None\r\n        }\r\n```",
        "createdAt" : "2020-08-01T14:26:44Z",
        "updatedAt" : "2020-08-04T22:55:52Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "95b2eb7a-0475-4bf6-8716-7022833aaaf7",
        "parentId" : "00c9b975-81d4-4f8c-80e6-c2e68d13e422",
        "authorId" : "cce1c782-0596-44b6-8b99-6b77d2cca53c",
        "body" : "updated",
        "createdAt" : "2020-08-02T01:48:00Z",
        "updatedAt" : "2020-08-04T22:55:52Z",
        "lastEditedBy" : "cce1c782-0596-44b6-8b99-6b77d2cca53c",
        "tags" : [
        ]
      }
    ],
    "commit" : "054581ae1d8f6148db7ec952ac23e5668ff9dc75",
    "line" : 54,
    "diffHunk" : "@@ -1,1 +413,417 @@\n        // All predicate must match pattern condition: Or(EqualTo(a=b), IsNull(EqualTo(a=b)))\n        val allMatch = predicates.forall {\n          case Or(e @ EqualTo(leftExpr: Expression, rightExpr: Expression),\n              IsNull(e2 @ EqualTo(_, _))) if e.semanticEquals(e2) =>"
  },
  {
    "id" : "4b6fe113-1230-4ecc-a072-9936135c27f0",
    "prId" : 29304,
    "prUrl" : "https://github.com/apache/spark/pull/29304#pullrequestreview-459663332",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "37365877-7531-4961-b713-d518adf91ea0",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "hm, I think its better to add some fine-grained tests for this extractor. WDYT?",
        "createdAt" : "2020-08-02T12:05:01Z",
        "updatedAt" : "2020-08-04T22:55:52Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "0384d5b9-7418-447c-a524-f73734340c79",
        "parentId" : "37365877-7531-4961-b713-d518adf91ea0",
        "authorId" : "cce1c782-0596-44b6-8b99-6b77d2cca53c",
        "body" : "agreed, i will add test upon it.",
        "createdAt" : "2020-08-02T16:54:30Z",
        "updatedAt" : "2020-08-04T22:55:52Z",
        "lastEditedBy" : "cce1c782-0596-44b6-8b99-6b77d2cca53c",
        "tags" : [
        ]
      }
    ],
    "commit" : "054581ae1d8f6148db7ec952ac23e5668ff9dc75",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +394,398 @@}\n\nobject ExtractNullAwareAntiJoinKeys extends JoinSelectionHelper with PredicateHelper {\n\n  // FYI. Extra information about Null Aware Anti Join."
  },
  {
    "id" : "ae1bd537-5ea5-42d8-9f9a-aeaff7f3bdce",
    "prId" : 29304,
    "prUrl" : "https://github.com/apache/spark/pull/29304#pullrequestreview-459705011",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3be0c7ae-7ce6-4063-b966-1bd5db78ea2b",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "IIUC this pattern matching depends on the `RewritePredicateSubquery` code:\r\nhttps://github.com/apache/spark/blob/0693d8bbf2942ab96ffe705ef0fc3fe4b0d9ec11/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/subquery.scala#L140\r\n\r\nThis is okay now, but I'm a little worried that it does not work well if the `RewritePredicateSubquery` code will be updated; for example, if both attributes are non-nullable in a join condition, we might be able to remove `IsNull(c)`  for optimization in the `RewritePredicateSubquery` rule.",
        "createdAt" : "2020-08-03T01:11:12Z",
        "updatedAt" : "2020-08-04T22:55:52Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "46a20b57-3acd-4180-80ae-8b6255fa1155",
        "parentId" : "3be0c7ae-7ce6-4063-b966-1bd5db78ea2b",
        "authorId" : "cce1c782-0596-44b6-8b99-6b77d2cca53c",
        "body" : "yes. the IsNull being removed case is considered, we only do NAAJ optimize with the Or condition still exists.\r\n\r\nBasically, the NAAJ Optimize switch triggered at SparkStrategies, which I think optimizer is done its job. it's save to put this pattern check in physical plan state\r\n\r\n```\r\n// negative hand-written left anti join\r\n      // testData.key nullable false\r\n      // testData2.a nullable false\r\n      // isnull(key = a) isnull(key+1=a) will be optimized to true literal and removed\r\n      joinExec = assertJoin((\r\n        \"SELECT * FROM testData LEFT ANTI JOIN testData3 ON (key = a OR ISNULL(key = a)) \" +\r\n          \"AND (key + 1 = a OR ISNULL(key + 1 = a))\",\r\n        classOf[BroadcastHashJoinExec]))\r\n      assert(!joinExec.asInstanceOf[BroadcastHashJoinExec].isNullAwareAntiJoin)\r\n```",
        "createdAt" : "2020-08-03T01:39:47Z",
        "updatedAt" : "2020-08-04T22:55:52Z",
        "lastEditedBy" : "cce1c782-0596-44b6-8b99-6b77d2cca53c",
        "tags" : [
        ]
      },
      {
        "id" : "d4983291-72c2-4847-a551-fe65b0f859c4",
        "parentId" : "3be0c7ae-7ce6-4063-b966-1bd5db78ea2b",
        "authorId" : "cce1c782-0596-44b6-8b99-6b77d2cca53c",
        "body" : "in JoinSuite Line 1209. FYI.",
        "createdAt" : "2020-08-03T01:40:47Z",
        "updatedAt" : "2020-08-04T22:55:52Z",
        "lastEditedBy" : "cce1c782-0596-44b6-8b99-6b77d2cca53c",
        "tags" : [
        ]
      }
    ],
    "commit" : "054581ae1d8f6148db7ec952ac23e5668ff9dc75",
    "line" : 56,
    "diffHunk" : "@@ -1,1 +415,419 @@        val allMatch = predicates.forall {\n          case Or(e @ EqualTo(leftExpr: Expression, rightExpr: Expression),\n              IsNull(e2 @ EqualTo(_, _))) if e.semanticEquals(e2) =>\n            if (canEvaluate(leftExpr, left) && canEvaluate(rightExpr, right)) {\n              joinKeys += ((leftExpr, rightExpr))"
  },
  {
    "id" : "7a7bb348-e843-4ad1-86ef-30c5902e0666",
    "prId" : 29104,
    "prUrl" : "https://github.com/apache/spark/pull/29104#pullrequestreview-452940331",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "67c8e678-977a-47b7-a317-301c844c4a74",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "Thanks for adding this. Is this covered in the positive/negative test case below ?",
        "createdAt" : "2020-07-21T17:41:16Z",
        "updatedAt" : "2020-07-27T22:06:38Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "6c1865b1-f1bb-4238-8191-412f7d1b9f10",
        "parentId" : "67c8e678-977a-47b7-a317-301c844c4a74",
        "authorId" : "cce1c782-0596-44b6-8b99-6b77d2cca53c",
        "body" : "yes, added a negative case.",
        "createdAt" : "2020-07-22T01:48:38Z",
        "updatedAt" : "2020-07-27T22:06:38Z",
        "lastEditedBy" : "cce1c782-0596-44b6-8b99-6b77d2cca53c",
        "tags" : [
        ]
      }
    ],
    "commit" : "233eff6549377d6c7c850fe8d1990fcd58fe0ea0",
    "line" : 41,
    "diffHunk" : "@@ -1,1 +417,421 @@      if (canEvaluate(leftAttr, left) && canEvaluate(rightAttr, right)) {\n        Some(Seq(leftAttr), Seq(rightAttr))\n      } else if (canEvaluate(leftAttr, right) && canEvaluate(rightAttr, left)) {\n        Some(Seq(rightAttr), Seq(leftAttr))\n      } else {"
  },
  {
    "id" : "f9806a7e-2a23-40b5-a16d-e9abe342bf5c",
    "prId" : 27073,
    "prUrl" : "https://github.com/apache/spark/pull/27073#pullrequestreview-337646795",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "06c2f360-0a1a-407c-9e33-4cdaa00c08a9",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "we can move this line into the `if` block.",
        "createdAt" : "2020-01-02T12:54:48Z",
        "updatedAt" : "2020-01-03T08:01:31Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "12826776-e0f1-4803-a250-5d423e852737",
        "parentId" : "06c2f360-0a1a-407c-9e33-4cdaa00c08a9",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "`substitutedCondition` is needed by `canCombineFilters` predicate, which is out of `if` block.",
        "createdAt" : "2020-01-02T12:57:21Z",
        "updatedAt" : "2020-01-03T08:01:31Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "5fec1582b5329b163e210e1482104d17299e5743",
    "line" : 34,
    "diffHunk" : "@@ -1,1 +147,151 @@            // collected filter and doesn't have common non-deterministic expressions\n            // with lower Project.\n            val substitutedCondition = substitute(aliases)(condition)\n            val canCombineFilters = (filters.nonEmpty && filters.forall(_.deterministic) &&\n              substitutedCondition.deterministic) || filters.isEmpty"
  },
  {
    "id" : "c9c2e475-e389-4c45-808e-2133a0484430",
    "prId" : 26629,
    "prUrl" : "https://github.com/apache/spark/pull/26629#pullrequestreview-322686523",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c527a948-222c-45ae-97ab-e50bf80a7548",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "To check this is satisfied is caller's responsibility?",
        "createdAt" : "2019-11-26T01:39:58Z",
        "updatedAt" : "2019-11-27T02:37:39Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "02072861-b013-4926-acaf-45ccf6da19b0",
        "parentId" : "c527a948-222c-45ae-97ab-e50bf80a7548",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "No, `ScanOperation.collectProjectsAndFilters()` does the check by itself.",
        "createdAt" : "2019-11-26T01:48:56Z",
        "updatedAt" : "2019-11-27T02:37:39Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "f86cb82de6fb1ee9550b3c266eb9b3fb0019909f",
    "line" : 52,
    "diffHunk" : "@@ -1,1 +101,105 @@ * A variant of [[PhysicalOperation]]. It matches any number of project or filter\n * operations even if they are non-deterministic, as long as they satisfy the\n * requirement of CollapseProject and CombineFilters.\n */\nobject ScanOperation extends OperationHelper with PredicateHelper {"
  },
  {
    "id" : "f7e3b15e-d3c2-4404-a298-99ecae0404da",
    "prId" : 24973,
    "prUrl" : "https://github.com/apache/spark/pull/24973#pullrequestreview-257854286",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b8f5c982-2dd8-4255-8151-d8b6d6818b13",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Let's be more clear about the approach here. We try to weaken a predicate so that it only refers to partition columns, by removing some branches in AND. Let's also mention the corner case when there is no partition column attribute in the predicate(we should return Nil in this case?).",
        "createdAt" : "2019-07-01T01:07:30Z",
        "updatedAt" : "2019-07-01T01:43:47Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "6103c48b-c868-40cd-a97f-6729e9c1cfdf",
        "parentId" : "b8f5c982-2dd8-4255-8151-d8b6d6818b13",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "@cloud-fan  \r\nIn this place it will return (DT = 20190601 OR DT = 20190602). \r\nBut this whole condition will still return to filter.\r\nWhat I want to do is purely to avoid  read unnecessary partitions. When this case we only read partition(dt=20190601 + dt=20190602), If we don't push down this, we will read all data.\r\n\r\nIn  condition \" **(DT = 20190602 AND C = \"TEST\")** \", **DT = 20190602**  is **C = \"TEST\"**'s precondition. \r\n\r\nIf the whole condition is **DT = 20190601 OR (DT = 20190602 OR C = \"TEST\")**. We should return null, since **DT = 20190602**  is not  **C = \"TEST\"**'s  constraint. ",
        "createdAt" : "2019-07-01T01:39:06Z",
        "updatedAt" : "2019-07-01T01:43:47Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "5c3bc345-8f80-4127-8fa3-cc59a98ecdd3",
        "parentId" : "b8f5c982-2dd8-4255-8151-d8b6d6818b13",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "@cloud-fan \r\nIn my code. coming predicate Set[Expression] has a potential **AND** logical.\r\nFor one Expression, it will be   restricted by other same level Expression.\r\nand  :\r\n\r\n- if it is a combine of **AND** each side can be a constraint to others, so it one side is tenable, it can return a tenable condition.\r\n- if it is a combine of **OR**, if one side is out of control(such as have no condition about partition cols) this whole **OR** Expression should return NONE.  Only  when both side of **OR** 's child is reasonable, it can return a  tenable combine of **OR**. \r\n- if it 's a multilayer nested Expression combined by BinaryOperator.  It will visit the lowest level, if it found one level's **OR** Expression is untenable, it will break this Expression totally and return null.\r\n\r\n\r\n",
        "createdAt" : "2019-07-04T06:47:50Z",
        "updatedAt" : "2019-07-04T06:47:50Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "01386d3ee93b6fc96349ee5f3a02978ca8cefe63",
    "line" : 34,
    "diffHunk" : "@@ -1,1 +332,336 @@ *   WHERE DT = 20190601 OR (DT = 20190602 AND C = \"TEST\")\n *\n * Where condition \"DT = 20190601 OR (DT = 20190602 AND C = \"TEST\")\"\n * can't be pushed down since it's reference is not subsetOf partition cols\n * [[ExtractPartitionPredicates]] is to help extract hided partition logic in Or expression."
  }
]