[
  {
    "id" : "36751f58-ea76-4d94-9016-372d3a715639",
    "prId" : 33381,
    "prUrl" : "https://github.com/apache/spark/pull/33381#pullrequestreview-707957137",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d2847c5f-91d1-4992-b8a0-52fc5830f07c",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Is it safer to describe this update in the migration guide?",
        "createdAt" : "2021-07-16T02:29:18Z",
        "updatedAt" : "2021-07-16T02:30:32Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "d1713c10-2ded-40d8-998f-a563c2c01b59",
        "parentId" : "d2847c5f-91d1-4992-b8a0-52fc5830f07c",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I think it's okay ... I guess ... it was already no-op if somebody sets in runtime and it was documented as a static configuration already too .. I would think this is a bug ..",
        "createdAt" : "2021-07-16T02:32:34Z",
        "updatedAt" : "2021-07-16T02:32:34Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "2dfd21ee48802340b7812a61829021d9d20bc158",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +268,272 @@      .doc(\"Configures a list of JDBC connection providers, which are disabled. \" +\n        \"The list contains the name of the JDBC connection providers separated by comma.\")\n      .version(\"3.1.0\")\n      .stringConf\n      .createWithDefault(\"\")"
  },
  {
    "id" : "9582747c-67b5-4aee-93ab-20d057fa2ab3",
    "prId" : 28852,
    "prUrl" : "https://github.com/apache/spark/pull/28852#pullrequestreview-456901559",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4a0ef0b6-b59b-494b-a5e5-46c8e7472406",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "Could you update the message by using ${SQLConf.HIVE_FILESOURCE_PARTITION_FILE_CACHE_SIZE.key} ?\r\n\r\n",
        "createdAt" : "2020-07-28T18:29:28Z",
        "updatedAt" : "2020-07-28T18:29:28Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "1c10784b-6463-438a-a764-14ebeea7947e",
        "parentId" : "4a0ef0b6-b59b-494b-a5e5-46c8e7472406",
        "authorId" : "2e177144-1a17-4ac6-8d65-915343e9f2ea",
        "body" : "That was done in https://github.com/apache/spark/pull/29194 :) ",
        "createdAt" : "2020-07-28T18:32:22Z",
        "updatedAt" : "2020-07-28T18:32:23Z",
        "lastEditedBy" : "2e177144-1a17-4ac6-8d65-915343e9f2ea",
        "tags" : [
        ]
      }
    ],
    "commit" : "3e761dcd790b9c30e5cee7bffe916dfc2c82b7a5",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +234,238 @@      \"a positive value (> 0). It also requires setting \" +\n      s\"${StaticSQLConf.CATALOG_IMPLEMENTATION} to `hive`, setting \" +\n      s\"${SQLConf.HIVE_FILESOURCE_PARTITION_FILE_CACHE_SIZE} > 0 and setting \" +\n      s\"${SQLConf.HIVE_MANAGE_FILESOURCE_PARTITIONS} to `true` \" +\n      \"to be applied to the partition file metadata cache.\")"
  },
  {
    "id" : "5528ead3-ca9e-46bb-913f-318f3883b906",
    "prId" : 27981,
    "prUrl" : "https://github.com/apache/spark/pull/27981#pullrequestreview-378998209",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a47fd4e2-d2a2-42a6-98c6-66ce9e2d6011",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-14994, commit ID: 054f991c4350af1350af7a4109ee77f4a34822f0#diff-32bb9518401c0948c5ea19377b5069ab",
        "createdAt" : "2020-03-22T12:04:43Z",
        "updatedAt" : "2020-03-22T12:04:43Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "86d4eff3d3f8d5ba37d22eed81de912d8929309b",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +33,37 @@  val WAREHOUSE_PATH = buildStaticConf(\"spark.sql.warehouse.dir\")\n    .doc(\"The default location for managed databases and tables.\")\n    .version(\"2.0.0\")\n    .stringConf\n    .createWithDefault(Utils.resolveURI(\"spark-warehouse\").toString)"
  },
  {
    "id" : "52d4877a-96ec-44c3-8c92-acd12982c4e7",
    "prId" : 27981,
    "prUrl" : "https://github.com/apache/spark/pull/27981#pullrequestreview-378998229",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4efe127f-05d7-46cc-b52f-c1f37c9a3f77",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-14720 and SPARK-13643, commit ID: 8fc267ab3322e46db81e725a5cb1adb5a71b2b4d#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-22T12:04:59Z",
        "updatedAt" : "2020-03-22T12:05:00Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "86d4eff3d3f8d5ba37d22eed81de912d8929309b",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +39,43 @@  val CATALOG_IMPLEMENTATION = buildStaticConf(\"spark.sql.catalogImplementation\")\n    .internal()\n    .version(\"2.0.0\")\n    .stringConf\n    .checkValues(Set(\"hive\", \"in-memory\"))"
  },
  {
    "id" : "b5a39a1a-e836-4d47-97ee-10a61ecaec6d",
    "prId" : 27981,
    "prUrl" : "https://github.com/apache/spark/pull/27981#pullrequestreview-378998252",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "92831bf7-d7a7-4d37-ad8b-d580b8c0bd9a",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-17338, commit ID: 23ddff4b2b2744c3dc84d928e144c541ad5df376#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-22T12:05:21Z",
        "updatedAt" : "2020-03-22T12:05:21Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "86d4eff3d3f8d5ba37d22eed81de912d8929309b",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +46,50 @@  val GLOBAL_TEMP_DATABASE = buildStaticConf(\"spark.sql.globalTempDatabase\")\n    .internal()\n    .version(\"2.1.0\")\n    .stringConf\n    .transform(_.toLowerCase(Locale.ROOT))"
  },
  {
    "id" : "a3ead6c7-66e0-4be8-ae07-35eb8077db53",
    "prId" : 27981,
    "prUrl" : "https://github.com/apache/spark/pull/27981#pullrequestreview-378998282",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "89b4c79b-b07a-4cb3-aae6-1f5568b9c689",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-6024, commit ID: 6200f0709c5c8440decae8bf700d7859f32ac9d5#diff-41ef65b9ef5b518f77e2a03559893f4d",
        "createdAt" : "2020-03-22T12:05:48Z",
        "updatedAt" : "2020-03-22T12:05:48Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "86d4eff3d3f8d5ba37d22eed81de912d8929309b",
    "line" : 29,
    "diffHunk" : "@@ -1,1 +62,66 @@      .doc(\"The maximum length allowed in a single cell when \" +\n        \"storing additional schema information in Hive's metastore.\")\n      .version(\"1.3.1\")\n      .intConf\n      .createWithDefault(4000)"
  },
  {
    "id" : "0cf9adfc-401c-4504-9713-d2e9dc8a2f0f",
    "prId" : 27981,
    "prUrl" : "https://github.com/apache/spark/pull/27981#pullrequestreview-378998323",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6328e2f5-6ee8-418d-a03e-888855880a96",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-19265, commit ID: 9d9d67c7957f7cbbdbe889bdbc073568b2bfbb16#diff-32bb9518401c0948c5ea19377b5069ab",
        "createdAt" : "2020-03-22T12:06:11Z",
        "updatedAt" : "2020-03-22T12:06:12Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "86d4eff3d3f8d5ba37d22eed81de912d8929309b",
    "line" : 37,
    "diffHunk" : "@@ -1,1 +70,74 @@      .internal()\n      .doc(\"The maximum size of the cache that maps qualified table names to table relation plans.\")\n      .version(\"2.2.0\")\n      .intConf\n      .checkValue(cacheSize => cacheSize >= 0, \"The maximum size of the cache must not be negative\")"
  },
  {
    "id" : "07ee348d-1210-4197-83e6-192eda744c3b",
    "prId" : 27981,
    "prUrl" : "https://github.com/apache/spark/pull/27981#pullrequestreview-378998385",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c9bd1c08-12a8-44af-9915-d50e1801057a",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-24727, commit ID: b2deef64f604ddd9502a31105ed47cb63470ec85#diff-5081b9388de3add800b6e4a6ddf55c01",
        "createdAt" : "2020-03-22T12:07:04Z",
        "updatedAt" : "2020-03-22T12:07:05Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "86d4eff3d3f8d5ba37d22eed81de912d8929309b",
    "line" : 45,
    "diffHunk" : "@@ -1,1 +79,83 @@      .doc(\"When nonzero, enable caching of generated classes for operators and expressions. \" +\n        \"All jobs share the cache that can use up to the specified number for generated classes.\")\n      .version(\"2.4.0\")\n      .intConf\n      .checkValue(maxEntries => maxEntries >= 0, \"The maximum must not be negative\")"
  },
  {
    "id" : "9c6636ea-2662-41a4-b3e8-844bb542cbe6",
    "prId" : 27981,
    "prUrl" : "https://github.com/apache/spark/pull/27981#pullrequestreview-378998403",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "76813948-d74e-4877-b5a6-96d34fbd4b6b",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-15680, commit ID: f0e8738c1ec0e4c5526aeada6f50cf76428f9afd#diff-8bcc5aea39c73d4bf38aef6f6951d42c",
        "createdAt" : "2020-03-22T12:07:27Z",
        "updatedAt" : "2020-03-22T12:07:27Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "86d4eff3d3f8d5ba37d22eed81de912d8929309b",
    "line" : 53,
    "diffHunk" : "@@ -1,1 +89,93 @@      \"can be extremely expensive in certain cases, such as deeply-nested expressions which \" +\n      \"operate over inputs with wide schemas, default is false.\")\n    .version(\"2.0.0\")\n    .booleanConf\n    .createWithDefault(false)"
  },
  {
    "id" : "d25f6012-6c5a-4511-9b21-6aaf89104a0b",
    "prId" : 27981,
    "prUrl" : "https://github.com/apache/spark/pull/27981#pullrequestreview-378998425",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1ed56136-ea9a-4805-8610-78f6de2a052b",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-17899, commit ID: db8784feaa605adcbd37af4bc8b7146479b631f8#diff-32bb9518401c0948c5ea19377b5069ab",
        "createdAt" : "2020-03-22T12:07:46Z",
        "updatedAt" : "2020-03-22T12:07:47Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "86d4eff3d3f8d5ba37d22eed81de912d8929309b",
    "line" : 61,
    "diffHunk" : "@@ -1,1 +98,102 @@    .internal()\n    .doc(\"Only used for internal debugging. Not all functions are supported when it is enabled.\")\n    .version(\"2.1.0\")\n    .booleanConf\n    .createWithDefault(false)"
  },
  {
    "id" : "ee41be1a-e32a-4448-91e1-17187a00f4d1",
    "prId" : 27981,
    "prUrl" : "https://github.com/apache/spark/pull/27981#pullrequestreview-378998467",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6cb0fd4f-53a5-492c-b722-bb821ddadee3",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-11089, commit ID: 167ea61a6a604fd9c0b00122a94d1bc4b1de24ff#diff-ff50aea397a607b79df9bec6f2a841db",
        "createdAt" : "2020-03-22T12:08:08Z",
        "updatedAt" : "2020-03-22T12:08:08Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "86d4eff3d3f8d5ba37d22eed81de912d8929309b",
    "line" : 69,
    "diffHunk" : "@@ -1,1 +107,111 @@        \"All the JDBC/ODBC connections share the temporary views, function registries, \" +\n        \"SQL configuration and the current database.\")\n      .version(\"1.6.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "ac32006c-43eb-4689-b723-4938b845c59a",
    "prId" : 27981,
    "prUrl" : "https://github.com/apache/spark/pull/27981#pullrequestreview-378998497",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f8cd22be-9196-4c45-9159-7edaba32af58",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-18127, commit ID: f0de600797ff4883927d0c70732675fd8629e239#diff-5081b9388de3add800b6e4a6ddf55c01",
        "createdAt" : "2020-03-22T12:08:30Z",
        "updatedAt" : "2020-03-22T12:08:30Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "86d4eff3d3f8d5ba37d22eed81de912d8929309b",
    "line" : 77,
    "diffHunk" : "@@ -1,1 +119,123 @@      \"parser can delegate to its predecessor. For the case of function name conflicts, the last \" +\n      \"registered function name is used.\")\n    .version(\"2.2.0\")\n    .stringConf\n    .toSequence"
  },
  {
    "id" : "ee1364e9-8c71-4085-821a-f4c962b7fe70",
    "prId" : 27981,
    "prUrl" : "https://github.com/apache/spark/pull/27981#pullrequestreview-378998526",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8f431120-be78-49bc-9587-7a024bc309f8",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-19558, commit ID: bd4eb9ce57da7bacff69d9ed958c94f349b7e6fb#diff-5081b9388de3add800b6e4a6ddf55c01",
        "createdAt" : "2020-03-22T12:08:52Z",
        "updatedAt" : "2020-03-22T12:08:52Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "86d4eff3d3f8d5ba37d22eed81de912d8929309b",
    "line" : 85,
    "diffHunk" : "@@ -1,1 +128,132 @@      \"added to newly created sessions. The classes should have either a no-arg constructor, \" +\n      \"or a constructor that expects a SparkConf argument.\")\n    .version(\"2.3.0\")\n    .stringConf\n    .toSequence"
  },
  {
    "id" : "140ec22f-58ed-45e9-8f15-4945dd74cceb",
    "prId" : 27981,
    "prUrl" : "https://github.com/apache/spark/pull/27981#pullrequestreview-378998556",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0666b676-39d3-479f-b41a-5f8101d21d02",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-24479, commit ID: 7703b46d2843db99e28110c4c7ccf60934412504#diff-5081b9388de3add800b6e4a6ddf55c01",
        "createdAt" : "2020-03-22T12:09:14Z",
        "updatedAt" : "2020-03-22T12:09:14Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "86d4eff3d3f8d5ba37d22eed81de912d8929309b",
    "line" : 93,
    "diffHunk" : "@@ -1,1 +137,141 @@      \"added to newly created sessions. The classes should have either a no-arg constructor, \" +\n      \"or a constructor that expects a SparkConf argument.\")\n    .version(\"2.4.0\")\n    .stringConf\n    .toSequence"
  },
  {
    "id" : "bf1898a9-841f-4961-9753-ec871b3d7f99",
    "prId" : 27981,
    "prUrl" : "https://github.com/apache/spark/pull/27981#pullrequestreview-378998576",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "155b82eb-51b2-4acd-9012-5157d4d729e5",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-8861 and SPARK-8862, commit ID: ebc3aad272b91cf58e2e1b4aa92b49b8a947a045#diff-81764e4d52817f83bdd5336ef1226bd9",
        "createdAt" : "2020-03-22T12:09:34Z",
        "updatedAt" : "2020-03-22T12:09:34Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "86d4eff3d3f8d5ba37d22eed81de912d8929309b",
    "line" : 101,
    "diffHunk" : "@@ -1,1 +145,149 @@    buildStaticConf(\"spark.sql.ui.retainedExecutions\")\n      .doc(\"Number of executions to retain in the Spark UI.\")\n      .version(\"1.5.0\")\n      .intConf\n      .createWithDefault(1000)"
  },
  {
    "id" : "2bd8c499-36d1-43d1-806d-e4b07c3bdf31",
    "prId" : 27981,
    "prUrl" : "https://github.com/apache/spark/pull/27981#pullrequestreview-378998610",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e80cab79-8828-453c-b56c-ba0113ad7367",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-26601, commit ID: 126310ca68f2f248ea8b312c4637eccaba2fdc2b#diff-5081b9388de3add800b6e4a6ddf55c01",
        "createdAt" : "2020-03-22T12:09:57Z",
        "updatedAt" : "2020-03-22T12:09:57Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "86d4eff3d3f8d5ba37d22eed81de912d8929309b",
    "line" : 109,
    "diffHunk" : "@@ -1,1 +158,162 @@        \"cause longer waiting for other broadcasting. Also, increasing parallelism may \" +\n        \"cause memory problem.\")\n      .version(\"3.0.0\")\n      .intConf\n      .checkValue(thres => thres > 0 && thres <= 128, \"The threshold must be in (0,128].\")"
  },
  {
    "id" : "bc8f4023-ccbb-4b6b-9f02-4e5b11212ce7",
    "prId" : 27981,
    "prUrl" : "https://github.com/apache/spark/pull/27981#pullrequestreview-378998638",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9798a760-35bc-4712-9bbf-86867f38a474",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-30556, commit ID: 2fc562cafd71ec8f438f37a28b65118906ab2ad2#diff-5081b9388de3add800b6e4a6ddf55c01",
        "createdAt" : "2020-03-22T12:10:21Z",
        "updatedAt" : "2020-03-22T12:10:21Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "86d4eff3d3f8d5ba37d22eed81de912d8929309b",
    "line" : 117,
    "diffHunk" : "@@ -1,1 +167,171 @@      .internal()\n      .doc(\"The maximum degree of parallelism to execute the subquery.\")\n      .version(\"2.4.6\")\n      .intConf\n      .checkValue(thres => thres > 0 && thres <= 128, \"The threshold must be in (0,128].\")"
  },
  {
    "id" : "dda557e7-cd26-48c5-ad46-cc911f24ae39",
    "prId" : 27981,
    "prUrl" : "https://github.com/apache/spark/pull/27981#pullrequestreview-378998662",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b3428990-e60b-4ba3-9232-021554d27d5e",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-27045, commit ID: e60d8fce0b0cf2a6d766ea2fc5f994546550570a#diff-5081b9388de3add800b6e4a6ddf55c01",
        "createdAt" : "2020-03-22T12:10:43Z",
        "updatedAt" : "2020-03-22T12:10:44Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "86d4eff3d3f8d5ba37d22eed81de912d8929309b",
    "line" : 125,
    "diffHunk" : "@@ -1,1 +175,179 @@    .doc(\"Threshold of SQL length beyond which it will be truncated before adding to \" +\n      \"event. Defaults to no truncation. If set to 0, callsite will be logged instead.\")\n    .version(\"3.0.0\")\n    .intConf\n    .checkValue(_ >= 0, \"Must be set greater or equal to zero\")"
  },
  {
    "id" : "ec7705ad-9d01-41eb-814d-b2a855b36ee8",
    "prId" : 27981,
    "prUrl" : "https://github.com/apache/spark/pull/27981#pullrequestreview-378998701",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "adce7529-a0a5-4bbf-981c-6654a7a55923",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-27253, commit ID: 83f628b57da39ad9732d1393aebac373634a2eb9#diff-5081b9388de3add800b6e4a6ddf55c01",
        "createdAt" : "2020-03-22T12:11:07Z",
        "updatedAt" : "2020-03-22T12:11:07Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "86d4eff3d3f8d5ba37d22eed81de912d8929309b",
    "line" : 133,
    "diffHunk" : "@@ -1,1 +184,188 @@      .doc(\"Flag to revert to legacy behavior where a cloned SparkSession receives SparkConf \" +\n        \"defaults, dropping any overrides in its parent SparkSession.\")\n      .version(\"3.0.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "ccbe574a-ca11-4413-9e0f-8c33932595cd",
    "prId" : 27981,
    "prUrl" : "https://github.com/apache/spark/pull/27981#pullrequestreview-378998741",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bc3981d9-0c61-46d4-9d7c-7f70f810a78e",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-25694, commit ID: 8469614c0513fbed87977d4e741649db3fdd8add#diff-5081b9388de3add800b6e4a6ddf55c01",
        "createdAt" : "2020-03-22T12:11:30Z",
        "updatedAt" : "2020-03-22T12:11:31Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "86d4eff3d3f8d5ba37d22eed81de912d8929309b",
    "line" : 148,
    "diffHunk" : "@@ -1,1 +198,202 @@        \"conflicts with other protocol types such as `http` or `https`. See also SPARK-25694 \" +\n        \"and HADOOP-14598.\")\n      .version(\"3.0.0\")\n      .booleanConf\n      .createWithDefault(true)"
  },
  {
    "id" : "762a54a0-86fa-4f6e-932b-a6e2588ad3b8",
    "prId" : 27981,
    "prUrl" : "https://github.com/apache/spark/pull/27981#pullrequestreview-378998780",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6b8d24f1-3226-4916-a351-eb806b7c142f",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-29543, commit ID: f9b86370cb04b72a4f00cbd4d60873960aa2792c#diff-5081b9388de3add800b6e4a6ddf55c01",
        "createdAt" : "2020-03-22T12:11:53Z",
        "updatedAt" : "2020-03-22T12:11:53Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "86d4eff3d3f8d5ba37d22eed81de912d8929309b",
    "line" : 156,
    "diffHunk" : "@@ -1,1 +206,210 @@      .doc(\"Whether to run the Structured Streaming Web UI for the Spark application when the \" +\n        \"Spark Web UI is enabled.\")\n      .version(\"3.0.0\")\n      .booleanConf\n      .createWithDefault(true)"
  },
  {
    "id" : "e0543849-40d9-48c8-88a8-6290940eba0c",
    "prId" : 27981,
    "prUrl" : "https://github.com/apache/spark/pull/27981#pullrequestreview-378998789",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "645bdd25-3e22-4ea1-a311-7f06f1091a8a",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-29543, commit ID: f9b86370cb04b72a4f00cbd4d60873960aa2792c#diff-5081b9388de3add800b6e4a6ddf55c01",
        "createdAt" : "2020-03-22T12:11:59Z",
        "updatedAt" : "2020-03-22T12:12:00Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "86d4eff3d3f8d5ba37d22eed81de912d8929309b",
    "line" : 164,
    "diffHunk" : "@@ -1,1 +214,218 @@      .doc(\"The number of progress updates to retain for a streaming query for Structured \" +\n        \"Streaming UI.\")\n      .version(\"3.0.0\")\n      .intConf\n      .createWithDefault(100)"
  },
  {
    "id" : "62387eba-2ca9-4d67-95dc-8653045b7179",
    "prId" : 27981,
    "prUrl" : "https://github.com/apache/spark/pull/27981#pullrequestreview-378998797",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "64c86df2-2127-4891-ac46-63163d1bc85c",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-29543, commit ID: f9b86370cb04b72a4f00cbd4d60873960aa2792c#diff-5081b9388de3add800b6e4a6ddf55c01",
        "createdAt" : "2020-03-22T12:12:06Z",
        "updatedAt" : "2020-03-22T12:12:06Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "86d4eff3d3f8d5ba37d22eed81de912d8929309b",
    "line" : 171,
    "diffHunk" : "@@ -1,1 +221,225 @@    buildStaticConf(\"spark.sql.streaming.ui.retainedQueries\")\n      .doc(\"The number of inactive queries to retain for Structured Streaming UI.\")\n      .version(\"3.0.0\")\n      .intConf\n      .createWithDefault(100)"
  },
  {
    "id" : "e5b87d3a-49b3-4828-8efc-3e1a58e2e0ad",
    "prId" : 27849,
    "prUrl" : "https://github.com/apache/spark/pull/27849#pullrequestreview-373317328",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5b41b11e-e9b5-4729-99d7-31f6cc41f9aa",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "@Ngone51 . \r\nSince SPARK-31081 is filed as an `Improvement`, this should be `3.1.0`.\r\nIf you want to consider this as a bug or regression at 3.0.0, you had better change JIRA first.\r\n\r\ncc @rxin since he is the release manager of 3.0.0 (also cc @gatorsmile ).",
        "createdAt" : "2020-03-11T18:43:10Z",
        "updatedAt" : "2020-03-11T18:43:10Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "54b67a98-5a74-4e28-a143-b0c7fa822736",
        "parentId" : "5b41b11e-e9b5-4729-99d7-31f6cc41f9aa",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "This is a UI issue to me. The SQL web UI is really hard to read now with the stage id stuff. Ideally, we should revisit #26843 and think of a better way to improve readability. But no one has proposed an idea yet.\r\n\r\nThis PR disables the stage id stuff, which seems like a good compromise for 3.0: we keep the UI unchanged, but for people who really need the stage id info, they can still enable it.",
        "createdAt" : "2020-03-12T06:35:17Z",
        "updatedAt" : "2020-03-12T06:35:17Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "b4e54691-50d7-4515-93b6-d655e3d15488",
        "parentId" : "5b41b11e-e9b5-4729-99d7-31f6cc41f9aa",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "+1 for making this 3.0. The UI changes in #26843 can be confusing to users after 3.0 release.",
        "createdAt" : "2020-03-12T07:05:33Z",
        "updatedAt" : "2020-03-12T07:05:33Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "a41d9c4e710539025b003e2c2f5c672f3ffb715b",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +140,144 @@      .doc(\"If turn on, Spark will display stageId-stageAttemptId-taskId of the max metrics to \" +\n        \"tell where the max value comes from. It's useful to help debug job quicker.\")\n      .version(\"3.0.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "d0b39dc1-077a-4585-bf2f-5851d6d7586f",
    "prId" : 27849,
    "prUrl" : "https://github.com/apache/spark/pull/27849#pullrequestreview-373306836",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7114f924-5c92-4253-9c90-263219ca19b5",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "shall we make it an internal conf?",
        "createdAt" : "2020-03-12T06:36:21Z",
        "updatedAt" : "2020-03-12T06:36:21Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "a41d9c4e710539025b003e2c2f5c672f3ffb715b",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +137,141 @@\n  val DISPLAY_TASK_ID_FOR_MAX_METRIC =\n    buildStaticConf(\"spark.sql.ui.displayTaskInfoForMaxMetric\")\n      .doc(\"If turn on, Spark will display stageId-stageAttemptId-taskId of the max metrics to \" +\n        \"tell where the max value comes from. It's useful to help debug job quicker.\")"
  },
  {
    "id" : "82692be1-73bd-4d18-b11f-3ca002e75b40",
    "prId" : 27267,
    "prUrl" : "https://github.com/apache/spark/pull/27267#pullrequestreview-347395965",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d7b7e3cf-ff9e-4182-bc7d-a0c4f34fe685",
        "parentId" : null,
        "authorId" : "80d631a6-73e8-46a6-a01b-a80d1f1cc6cc",
        "body" : "This is actually a different change right?",
        "createdAt" : "2020-01-23T12:29:06Z",
        "updatedAt" : "2020-01-23T12:29:06Z",
        "lastEditedBy" : "80d631a6-73e8-46a6-a01b-a80d1f1cc6cc",
        "tags" : [
        ]
      },
      {
        "id" : "546da59d-6b13-4165-9e50-b257cc227e8e",
        "parentId" : "d7b7e3cf-ff9e-4182-bc7d-a0c4f34fe685",
        "authorId" : "80d631a6-73e8-46a6-a01b-a80d1f1cc6cc",
        "body" : "It should also be a static configuration since we can only change it at startup.",
        "createdAt" : "2020-01-23T12:30:25Z",
        "updatedAt" : "2020-01-23T12:30:26Z",
        "lastEditedBy" : "80d631a6-73e8-46a6-a01b-a80d1f1cc6cc",
        "tags" : [
        ]
      },
      {
        "id" : "fb7d99d4-8f2d-4671-83a0-4c7802fd8a41",
        "parentId" : "d7b7e3cf-ff9e-4182-bc7d-a0c4f34fe685",
        "authorId" : "2c1bd8aa-c665-4159-a689-57743063347c",
        "body" : "1. This is just opening as a configuration to make the change testable. DO you want me to raise separate PR just to make this configuration change seperate.?\r\n\r\n2. This is part of `StaticSQLConf` which is defined at startup, is there any other mechanism to define static conf.?",
        "createdAt" : "2020-01-23T12:44:55Z",
        "updatedAt" : "2020-01-23T12:47:51Z",
        "lastEditedBy" : "2c1bd8aa-c665-4159-a689-57743063347c",
        "tags" : [
        ]
      },
      {
        "id" : "4c2c67bc-44ec-4878-827c-f640434a91ed",
        "parentId" : "d7b7e3cf-ff9e-4182-bc7d-a0c4f34fe685",
        "authorId" : "80d631a6-73e8-46a6-a01b-a80d1f1cc6cc",
        "body" : "If its a static conf, then why isn't your unite test failing? Moreover, if its static then setting it in your test probably does not have any effect because we use the same JVM/SparkContext to run most tests, the chances are pretty high that it has been set before.",
        "createdAt" : "2020-01-23T13:10:42Z",
        "updatedAt" : "2020-01-23T13:10:43Z",
        "lastEditedBy" : "80d631a6-73e8-46a6-a01b-a80d1f1cc6cc",
        "tags" : [
        ]
      },
      {
        "id" : "3c03729a-6788-4713-9dc1-7746965d72a5",
        "parentId" : "d7b7e3cf-ff9e-4182-bc7d-a0c4f34fe685",
        "authorId" : "2c1bd8aa-c665-4159-a689-57743063347c",
        "body" : "i see. as per documentation in https://github.com/apache/spark/blob/705fc6ad9328c1092299b83071b6ec3b1d6f9c4d/sql/catalyst/src/main/scala/org/apache/spark/sql/internal/StaticSQLConf.scala#L26 its should not modified. I followed the same way `BroadcastExchangeExec` creates executionContext. My initial guess is executionContext is not created till first subquery hence it work for UT. i will further investigate and get back with analysis.",
        "createdAt" : "2020-01-23T15:39:31Z",
        "updatedAt" : "2020-01-23T15:39:31Z",
        "lastEditedBy" : "2c1bd8aa-c665-4159-a689-57743063347c",
        "tags" : [
        ]
      }
    ],
    "commit" : "f1cac4de513dbe698604d6157fc8f663ecf8af72",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +149,153 @@      .createWithDefault(128)\n\n  val SUBQUERY_MAX_THREAD_THRESHOLD =\n    buildStaticConf(\"spark.sql.subquery.maxThreadThreshold\")\n      .internal()"
  },
  {
    "id" : "376027a3-c353-4148-8e47-c1ab0c6362f8",
    "prId" : 27267,
    "prUrl" : "https://github.com/apache/spark/pull/27267#pullrequestreview-355691813",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "847ada0c-76f6-4740-85cf-5b282e2bbea0",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "I think we should submit a separate PR for this change. ",
        "createdAt" : "2020-02-09T06:06:01Z",
        "updatedAt" : "2020-02-09T06:06:02Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "805a3516-3de1-4fc9-a357-8672c97162af",
        "parentId" : "847ada0c-76f6-4740-85cf-5b282e2bbea0",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "I think we could keep this in StaticSQLConf.\r\n\r\n- I locally test the static config takes effect, it can only be set while startup.\r\n- The UT can pass because it was used in lazy val `SubqueryExec.relationFuture` on the executor side, so the `withSQLConf` in UT could set the config before executor start.\r\n\r\ncc @hvanhovell ",
        "createdAt" : "2020-02-10T04:59:03Z",
        "updatedAt" : "2020-02-10T04:59:04Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "f1cac4de513dbe698604d6157fc8f663ecf8af72",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +149,153 @@      .createWithDefault(128)\n\n  val SUBQUERY_MAX_THREAD_THRESHOLD =\n    buildStaticConf(\"spark.sql.subquery.maxThreadThreshold\")\n      .internal()"
  },
  {
    "id" : "dbb6ce4c-de0d-477b-8901-6146d3f924ed",
    "prId" : 24566,
    "prUrl" : "https://github.com/apache/spark/pull/24566#pullrequestreview-329012368",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "61918ef6-6263-4ea3-8fb5-bf7505746b70",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "This is not worth a new config and code path. user can already show the current DB right?",
        "createdAt" : "2019-12-08T15:03:02Z",
        "updatedAt" : "2019-12-08T15:03:03Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "109b7dc4-d9a7-4889-a0be-c8540fd6f911",
        "parentId" : "61918ef6-6263-4ea3-8fb5-bf7505746b70",
        "authorId" : "f2e3e1f3-82a3-4b68-a0c2-2f7fdf58cf35",
        "body" : "In case of beeline, there is a option to show the current database in the CLI prompt. Same thing I handled for spark-sql CLI",
        "createdAt" : "2019-12-09T05:12:37Z",
        "updatedAt" : "2019-12-09T05:12:37Z",
        "lastEditedBy" : "f2e3e1f3-82a3-4b68-a0c2-2f7fdf58cf35",
        "tags" : [
        ]
      },
      {
        "id" : "4510414d-e36a-489a-bc26-eff57d7cb4f0",
        "parentId" : "61918ef6-6263-4ea3-8fb5-bf7505746b70",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "beeline is from Hive though. I know we have a script to start it, but I don't think we need to emulate it in this respect.",
        "createdAt" : "2019-12-09T15:36:40Z",
        "updatedAt" : "2019-12-09T15:36:41Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "2d0f47aa-7000-48d6-ad55-0a9a0b9776bf",
        "parentId" : "61918ef6-6263-4ea3-8fb5-bf7505746b70",
        "authorId" : "f2e3e1f3-82a3-4b68-a0c2-2f7fdf58cf35",
        "body" : "I felt in some cases itâ€™s better if we display the database , for the same reason hive supports it . If you feel this is not required I can close the PR  ",
        "createdAt" : "2019-12-09T15:40:59Z",
        "updatedAt" : "2019-12-09T15:40:59Z",
        "lastEditedBy" : "f2e3e1f3-82a3-4b68-a0c2-2f7fdf58cf35",
        "tags" : [
        ]
      }
    ],
    "commit" : "79ddad11423fbd1067f856e426adfe2c97f31646",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +170,174 @@      .createWithDefault(true)\n\n  val SPARK_SQL_CLI_SHOW_CURRENT_DB_ENABLED =\n    buildStaticConf(\"spark.sql.cli.show.current.db.enabled\")\n      .doc(\"when true, current DB name will be displayed in the cli prompt\")"
  }
]