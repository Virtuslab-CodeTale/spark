[
  {
    "id" : "fa1c65fe-2855-4e4c-a155-b74b419353a4",
    "prId" : 32929,
    "prUrl" : "https://github.com/apache/spark/pull/32929#pullrequestreview-685916953",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9961315c-fad0-4239-bd81-7a346aa6bfce",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "@cloud-fan You had some concerns of listing all types because we can extend `YearMonthIntervalType`/`DayTimeIntervalType` by precision in the future. So, we cannot list all possible types as we cannot list all possible types of Decimal. WDYT? Could you look at this, please.",
        "createdAt" : "2021-06-16T19:54:42Z",
        "updatedAt" : "2021-06-16T19:54:42Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "7a9b87f1-8e33-4588-a4d7-63b304b1b15b",
        "parentId" : "9961315c-fad0-4239-bd81-7a346aa6bfce",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "It's internal code and I'm OK with this. You mentioned a good point before that we can reuse the sql parser here, and we can refactor this part later.",
        "createdAt" : "2021-06-17T06:37:31Z",
        "updatedAt" : "2021-06-17T06:37:31Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "2be136b01fb24c37e7bbd2e7c07b96d4371192a5",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +177,181 @@      YearMonthIntervalType(YEAR, YEAR),\n      YearMonthIntervalType(MONTH, MONTH),\n      YearMonthIntervalType(YEAR, MONTH),\n      TimestampWithoutTZType)\n      .map(t => t.typeName -> t).toMap"
  },
  {
    "id" : "2f73e70a-c02b-47c9-9cbb-386caf4b6387",
    "prId" : 32825,
    "prUrl" : "https://github.com/apache/spark/pull/32825#pullrequestreview-679677684",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5353eb00-85af-4524-8297-4861e1deaba4",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "shall we follow `FIXED_DECIMAL` and write a regex to support different units?",
        "createdAt" : "2021-06-09T09:38:16Z",
        "updatedAt" : "2021-06-09T09:38:17Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "0a5157e8-7e87-493c-9fa1-63b7b2e0eb13",
        "parentId" : "5353eb00-85af-4524-8297-4861e1deaba4",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "This is just stub to pass compiler errors. I don't think manually parsing is the right approach. I do believe that:\r\n- Having two type parsers is bad idea\r\n- We should use existing SQL parser.",
        "createdAt" : "2021-06-09T10:34:37Z",
        "updatedAt" : "2021-06-09T10:34:38Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "d9b1d942-0697-465c-a287-5180ecff32cc",
        "parentId" : "5353eb00-85af-4524-8297-4861e1deaba4",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I'm happy to see a refactor here to use the SQL parser instead.",
        "createdAt" : "2021-06-09T13:48:07Z",
        "updatedAt" : "2021-06-09T13:48:07Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "ef97d29888c5b42a47b530e56b1bfe6ec235e158",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +180,184 @@    name match {\n      case \"decimal\" => DecimalType.USER_DEFAULT\n      case \"interval year to month\" => YearMonthIntervalType()\n      case FIXED_DECIMAL(precision, scale) => DecimalType(precision.toInt, scale.toInt)\n      case CHAR_TYPE(length) => CharType(length.toInt)"
  },
  {
    "id" : "a7d03d72-7ffd-4332-9385-0a7b53b789e4",
    "prId" : 30412,
    "prUrl" : "https://github.com/apache/spark/pull/30412#pullrequestreview-537139009",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c55a51da-46b6-42c7-a73c-73c34f16000b",
        "parentId" : null,
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "A question, do we need to support default length if user not give a special value ?",
        "createdAt" : "2020-11-24T01:08:24Z",
        "updatedAt" : "2020-11-27T10:45:20Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      },
      {
        "id" : "18eb9c79-f963-44bc-8f70-e780dfb4db44",
        "parentId" : "c55a51da-46b6-42c7-a73c-73c34f16000b",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "We don't. We can set a default length/max length in followups.",
        "createdAt" : "2020-11-24T07:26:43Z",
        "updatedAt" : "2020-11-27T10:45:20Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "73b99dc7a98f5d0673d309adba457cd19144be92",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +126,130 @@  private val FIXED_DECIMAL = \"\"\"decimal\\(\\s*(\\d+)\\s*,\\s*(\\-?\\d+)\\s*\\)\"\"\".r\n  private val CHAR_TYPE = \"\"\"char\\(\\s*(\\d+)\\s*\\)\"\"\".r\n  private val VARCHAR_TYPE = \"\"\"varchar\\(\\s*(\\d+)\\s*\\)\"\"\".r\n\n  def fromDDL(ddl: String): DataType = {"
  },
  {
    "id" : "3e73d2a2-1140-4ab9-ba18-488fd0bcc9d5",
    "prId" : 26107,
    "prUrl" : "https://github.com/apache/spark/pull/26107#pullrequestreview-301836528",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a1b3907b-75fb-4cb3-b747-df50a13f2123",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I think we can write null to any nullable column even if it's strict policy. We can have a followup PR to discuss it further.",
        "createdAt" : "2019-10-15T11:32:44Z",
        "updatedAt" : "2019-10-15T13:21:47Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "b9abb67cd9761e59ee62ecb77b269deb725b1185",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +457,461 @@        }\n\n      case (_: NullType, _) if storeAssignmentPolicy == ANSI => true\n\n      case (w: AtomicType, r: AtomicType) if storeAssignmentPolicy == ANSI =>"
  },
  {
    "id" : "91e58464-dfe7-4755-970a-ab2e231b8fc6",
    "prId" : 25581,
    "prUrl" : "https://github.com/apache/spark/pull/25581#pullrequestreview-279627687",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9c4129bb-b2b3-406f-bd64-e90dfa35d543",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "maybe pass in a boolean flag `isStrict`?",
        "createdAt" : "2019-08-26T14:05:58Z",
        "updatedAt" : "2019-08-27T09:40:14Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "7746e429-7299-4b3f-a1b5-f1324a014589",
        "parentId" : "9c4129bb-b2b3-406f-bd64-e90dfa35d543",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "I think keeping the original policy is also fine. Otherwise, it is hard to tell that we are using ANSI mode if `isStrict` is `false`.",
        "createdAt" : "2019-08-26T14:31:46Z",
        "updatedAt" : "2019-08-27T09:40:14Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "af0f7390e61455682cb5089f8e8dd3c0e60d32b1",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +374,378 @@      resolver: Resolver,\n      context: String,\n      storeAssignmentPolicy: StoreAssignmentPolicy.Value,\n      addError: String => Unit): Boolean = {\n    (write, read) match {"
  }
]