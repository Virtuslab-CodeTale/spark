[
  {
    "id" : "454ac5d8-a09e-4146-97b2-31e8f99932f2",
    "prId" : 32301,
    "prUrl" : "https://github.com/apache/spark/pull/32301#pullrequestreview-643695923",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ccc0f886-44e6-4e74-9b3a-613e0eace721",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I think a good first step is to improve the classdoc to explain in detail what we are doing here, with examples.\r\n\r\nA good example is `DecorrelateInnerQuery`.",
        "createdAt" : "2021-04-23T15:09:56Z",
        "updatedAt" : "2021-04-29T04:38:39Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "e0ed6438-346f-48ff-b194-29e99eaf49f1",
        "parentId" : "ccc0f886-44e6-4e74-9b3a-613e0eace721",
        "authorId" : "ada71464-b8c9-4d7b-9020-d49c27251fdb",
        "body" : "I wasn't sure how much to add to the classdoc, but I added some more examples.",
        "createdAt" : "2021-04-23T22:16:02Z",
        "updatedAt" : "2021-04-29T04:38:39Z",
        "lastEditedBy" : "ada71464-b8c9-4d7b-9020-d49c27251fdb",
        "tags" : [
        ]
      }
    ],
    "commit" : "8a29e943447808391c17f860598e3f11ae41d54d",
    "line" : 70,
    "diffHunk" : "@@ -1,1 +80,84 @@ * The schema of the datasource relation will be pruned in the [[SchemaPruning]] optimizer rule.\n */\nobject NestedColumnAliasing {\n\n  def unapply(plan: LogicalPlan): Option[LogicalPlan] = plan match {"
  },
  {
    "id" : "aa0a2d2e-daa4-4868-befe-9e046eac0dfa",
    "prId" : 32301,
    "prUrl" : "https://github.com/apache/spark/pull/32301#pullrequestreview-644464727",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7d092651-d369-4238-9bd1-f18fdeeaa97b",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "we can add more comments:\r\n```\r\n..., because `a.b` means all the inner fields of `b` are needed, and we can't prune `a.b.c`.\r\n```",
        "createdAt" : "2021-04-26T09:37:34Z",
        "updatedAt" : "2021-04-29T04:38:39Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "8a29e943447808391c17f860598e3f11ae41d54d",
    "line" : 290,
    "diffHunk" : "@@ -1,1 +257,261 @@      .flatMap { case (attr: Attribute, nestedFields: Seq[ExtractValue]) =>\n        // Remove redundant [[ExtractValue]]s if they share the same parent nest field.\n        // For example, when `a.b` and `a.b.c` are in project list, we only need to alias `a.b`.\n        // Because `a.b` requires all of the inner fields of `b`, we cannot prune `a.b.c`.\n        val dedupNestedFields = nestedFields.filter {"
  },
  {
    "id" : "ecb816b6-e1eb-478f-af40-931b9ce76109",
    "prId" : 32301,
    "prUrl" : "https://github.com/apache/spark/pull/32301#pullrequestreview-661357736",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "26ac24e5-f51c-49c6-9059-591064df7004",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "we will call `.map(_.canonicalized).distinct` later. Do we need this `.distinct` here?",
        "createdAt" : "2021-04-26T09:42:57Z",
        "updatedAt" : "2021-04-29T04:38:39Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "9f371186-d466-46b5-b467-0cf1535fe0aa",
        "parentId" : "26ac24e5-f51c-49c6-9059-591064df7004",
        "authorId" : "ada71464-b8c9-4d7b-9020-d49c27251fdb",
        "body" : "We use `.map(_.canonicalized).distinct` only for counting the number of nested fields. I believe we still need `.distinct` here to deduplicate the nested fields.",
        "createdAt" : "2021-04-27T21:13:58Z",
        "updatedAt" : "2021-04-29T04:38:39Z",
        "lastEditedBy" : "ada71464-b8c9-4d7b-9020-d49c27251fdb",
        "tags" : [
        ]
      },
      {
        "id" : "ac551030-eb61-4672-a97a-5a480862f728",
        "parentId" : "26ac24e5-f51c-49c6-9059-591064df7004",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "why don't we do `.map(_.canonicalized).distinct` here?",
        "createdAt" : "2021-04-29T04:37:17Z",
        "updatedAt" : "2021-04-29T04:38:39Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "4de75402-1dc7-46b5-9b23-982f5f47e414",
        "parentId" : "26ac24e5-f51c-49c6-9059-591064df7004",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "We can also update the lookup side\r\n`nestedFieldToAlias.contains(f)` -> `nestedFieldToAlias.contains(f.canonicalized)`",
        "createdAt" : "2021-04-29T04:44:55Z",
        "updatedAt" : "2021-04-29T04:44:56Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "264a1833-0890-466b-a377-a05dc2712bae",
        "parentId" : "26ac24e5-f51c-49c6-9059-591064df7004",
        "authorId" : "ada71464-b8c9-4d7b-9020-d49c27251fdb",
        "body" : "When I tried this, the tests failed - turns out that canonicalization strips key information for analysis, such as the name of the AttributeReference (see [ignoreNameTypes](https://github.com/apache/spark/blob/2a335f2d7d1265cb9abd2e727f60d8eb2dfa356b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Canonicalize.scala#L44)).",
        "createdAt" : "2021-05-17T19:52:11Z",
        "updatedAt" : "2021-05-17T19:52:11Z",
        "lastEditedBy" : "ada71464-b8c9-4d7b-9020-d49c27251fdb",
        "tags" : [
        ]
      }
    ],
    "commit" : "8a29e943447808391c17f860598e3f11ae41d54d",
    "line" : 309,
    "diffHunk" : "@@ -1,1 +266,270 @@            nestedFields.forall(f => child.find(_.semanticEquals(f)).isEmpty)\n          case _ => true\n        }.distinct\n\n        // If all nested fields of `attr` are used, we don't need to introduce new aliases."
  },
  {
    "id" : "0d1858d5-064d-47bc-8efc-e93d052ca3cb",
    "prId" : 32301,
    "prUrl" : "https://github.com/apache/spark/pull/32301#pullrequestreview-669825938",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b78fa060-7c91-4c28-921e-3a1c78a0477a",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Not related to this PR: does it mean we don't support Project -> Limit -> Filter?",
        "createdAt" : "2021-05-25T15:30:43Z",
        "updatedAt" : "2021-05-25T15:30:43Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "d4406d01-3d9b-4496-842d-e1f831751dfe",
        "parentId" : "b78fa060-7c91-4c28-921e-3a1c78a0477a",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "cc @viirya @maropu ",
        "createdAt" : "2021-05-26T05:38:55Z",
        "updatedAt" : "2021-05-26T05:38:55Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "87a81423-93b5-4344-b977-1e17feeb878e",
        "parentId" : "b78fa060-7c91-4c28-921e-3a1c78a0477a",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "`Limit` is able to be pushed through, no?",
        "createdAt" : "2021-05-26T22:57:53Z",
        "updatedAt" : "2021-05-26T22:57:53Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "9170c80b-4cae-42db-9e0f-aec4bfe3a4ad",
        "parentId" : "b78fa060-7c91-4c28-921e-3a1c78a0477a",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Yea you are right, we will first generate a new project between Limit and Filter (the Project grandchild) to do nested column pruning, then the next batch we can match Project -> Filter and do nested column pruning further.",
        "createdAt" : "2021-05-27T07:22:29Z",
        "updatedAt" : "2021-05-27T07:22:30Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "8a29e943447808391c17f860598e3f11ae41d54d",
    "line" : 78,
    "diffHunk" : "@@ -1,1 +86,90 @@     * This pattern is needed to support [[Filter]] plan cases like\n     * [[Project]]->[[Filter]]->listed plan in [[canProjectPushThrough]] (e.g., [[Window]]).\n     * The reason why we don't simply add [[Filter]] in [[canProjectPushThrough]] is that\n     * the optimizer can hit an infinite loop during the [[PushDownPredicates]] rule.\n     */"
  },
  {
    "id" : "4e81af48-a975-47a6-97b7-76c2e6b1cf50",
    "prId" : 32301,
    "prUrl" : "https://github.com/apache/spark/pull/32301#pullrequestreview-669609196",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f50f5dce-81e6-45c7-b4c7-fcee86f56dd2",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "not related to this PR: we only add a new Project node as grandchild, but the scan node may still be at the bottom. Do we expect other rules to push down the Project through other nodes until it reaches the bottom?",
        "createdAt" : "2021-05-25T15:38:42Z",
        "updatedAt" : "2021-05-25T15:38:42Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "e288d31e-a54a-4179-9622-6bf8be28645b",
        "parentId" : "f50f5dce-81e6-45c7-b4c7-fcee86f56dd2",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "cc @viirya @maropu ",
        "createdAt" : "2021-05-26T05:39:00Z",
        "updatedAt" : "2021-05-26T05:39:01Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "9476847f-c31d-4290-a4f8-addac34b9337",
        "parentId" : "f50f5dce-81e6-45c7-b4c7-fcee86f56dd2",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Yes, this is how nested column pruning works now. The added Project with nested column references will be push down through other nodes until Scan.\r\n\r\nIf there are any nodes we cannot push through, then nested column pruning doesn't work.",
        "createdAt" : "2021-05-26T22:56:05Z",
        "updatedAt" : "2021-05-26T22:56:05Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "8a29e943447808391c17f860598e3f11ae41d54d",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +32,36 @@ * Then:\n * - Substitute the nested field references with alias attributes\n * - Add grandchild [[Project]]s transforming the nested fields to aliases\n *\n * Example 1: Project"
  },
  {
    "id" : "951f6949-8e19-409f-bcbd-bf4b2dcfe7be",
    "prId" : 32301,
    "prUrl" : "https://github.com/apache/spark/pull/32301#pullrequestreview-668023178",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "842e2681-be5f-479f-82ea-22475bf9c790",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I'm still a bit worried about calling distinct on uncanonicalized expressions. how about \r\n```\r\n.groupBy(_.canonicalized).values.map(_.head).toSeq\r\n```",
        "createdAt" : "2021-05-25T15:45:50Z",
        "updatedAt" : "2021-05-25T15:45:50Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "b64a9bfd-f7cd-4b9f-8b7b-232e172e3df4",
        "parentId" : "842e2681-be5f-479f-82ea-22475bf9c790",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Sorry I'm wrong. We can't do this as the caller side will look up the map from extract value to alias using the original expression.",
        "createdAt" : "2021-05-25T15:57:47Z",
        "updatedAt" : "2021-05-25T15:57:47Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "8a29e943447808391c17f860598e3f11ae41d54d",
    "line" : 309,
    "diffHunk" : "@@ -1,1 +266,270 @@            nestedFields.forall(f => child.find(_.semanticEquals(f)).isEmpty)\n          case _ => true\n        }.distinct\n\n        // If all nested fields of `attr` are used, we don't need to introduce new aliases."
  },
  {
    "id" : "1120e809-7c7c-4aa7-aaba-c71bb7ee516a",
    "prId" : 32301,
    "prUrl" : "https://github.com/apache/spark/pull/32301#pullrequestreview-668551472",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "acb3b7d4-d2ff-4ed2-8ce7-b9881143b2b9",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I didn't carefully review the code in the else branch. I assume it's the same as the previous code?",
        "createdAt" : "2021-05-25T16:04:29Z",
        "updatedAt" : "2021-05-25T16:04:29Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "717248d3-401c-4e13-89c9-f29cac3313b6",
        "parentId" : "acb3b7d4-d2ff-4ed2-8ce7-b9881143b2b9",
        "authorId" : "ada71464-b8c9-4d7b-9020-d49c27251fdb",
        "body" : "Most of this is cosmetic, but at the end there is an expression ID-based lookup. The current commit explicitly uses the exprId to look up, but I can wrap more variables as an AttributeMap.",
        "createdAt" : "2021-05-26T02:43:07Z",
        "updatedAt" : "2021-05-26T02:43:07Z",
        "lastEditedBy" : "ada71464-b8c9-4d7b-9020-d49c27251fdb",
        "tags" : [
        ]
      }
    ],
    "commit" : "8a29e943447808391c17f860598e3f11ae41d54d",
    "line" : 476,
    "diffHunk" : "@@ -1,1 +341,345 @@        Some(pushedThrough)\n      } else {\n        // Only one nested column accessor.\n        // E.g., df.select(explode($\"items\").as(\"item\")).select($\"item.a\")\n        val nestedFieldOnGenerator = nestedFieldsOnGenerator.head"
  },
  {
    "id" : "9735d070-2ac2-4038-902e-375083ad8836",
    "prId" : 31966,
    "prUrl" : "https://github.com/apache/spark/pull/31966#pullrequestreview-622850383",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0c3e098a-cfa4-42c2-91d6-00af2a0effa0",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Just a question, according to the logic, is this the same with checking `rewrittenG.generator.elementSchema.toAttributes.length == rewrittenG.generatorOutput`?",
        "createdAt" : "2021-03-27T18:55:14Z",
        "updatedAt" : "2021-04-25T00:49:15Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "4c5735ca-3a5d-48fe-90e5-acac1dba607d",
        "parentId" : "0c3e098a-cfa4-42c2-91d6-00af2a0effa0",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "yea, i think this is the same.",
        "createdAt" : "2021-03-29T03:51:08Z",
        "updatedAt" : "2021-04-25T00:49:15Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "a7194092ebc3a9ca97daba29fda0fcc0ec099cf4",
    "line" : 93,
    "diffHunk" : "@@ -1,1 +320,324 @@                  newAttr.withExprId(oldAttr.exprId).withName(oldAttr.name)\n                }\n                assert(updatedGeneratorOutput.length == rewrittenG.generatorOutput.length,\n                  \"Updated generator output must have the same length \" +\n                    \"with original generator output.\")"
  },
  {
    "id" : "58c84a6d-ec9f-47ae-af63-0d21203aaba7",
    "prId" : 31966,
    "prUrl" : "https://github.com/apache/spark/pull/31966#pullrequestreview-632952328",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cb9e157a-f076-42c3-9187-bf95749591d0",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "We need tests for case-sensitivity?",
        "createdAt" : "2021-03-29T01:43:00Z",
        "updatedAt" : "2021-04-25T00:49:15Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "0759814f-33a9-4ac8-bdd6-b77b7816bb20",
        "parentId" : "cb9e157a-f076-42c3-9187-bf95749591d0",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "let me add one.",
        "createdAt" : "2021-03-29T03:44:51Z",
        "updatedAt" : "2021-04-25T00:49:15Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "bcddb52a-3179-4655-b073-c476ad956137",
        "parentId" : "cb9e157a-f076-42c3-9187-bf95749591d0",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Not add this test case yet. Not related to this PR but seems there is a bug on case-sensitive nested column pruning. Let me figure it out first.",
        "createdAt" : "2021-04-05T07:30:57Z",
        "updatedAt" : "2021-04-25T00:49:15Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "b242bd08-3a89-470b-aa02-2e68d7bec6a8",
        "parentId" : "cb9e157a-f076-42c3-9187-bf95749591d0",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Found a bug and proposed a fix at #32059. cc @maropu @dongjoon-hyun ",
        "createdAt" : "2021-04-06T06:45:32Z",
        "updatedAt" : "2021-04-25T00:49:15Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "9736e9a1-f92b-4350-b0c8-1ce85d686837",
        "parentId" : "cb9e157a-f076-42c3-9187-bf95749591d0",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "As #32059 was merged, I add case-sensitivity test.",
        "createdAt" : "2021-04-10T20:55:50Z",
        "updatedAt" : "2021-04-25T00:49:15Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "a7194092ebc3a9ca97daba29fda0fcc0ec099cf4",
    "line" : 82,
    "diffHunk" : "@@ -1,1 +309,313 @@                        e.child\n                      case g: GetStructField =>\n                        ExtractValue(g.child, Literal(g.extractFieldName), SQLConf.get.resolver)\n                    }\n                    e.withNewChildren(Seq(extractor))"
  },
  {
    "id" : "de011d2c-8171-437c-9601-0aef7568bf9a",
    "prId" : 31966,
    "prUrl" : "https://github.com/apache/spark/pull/31966#pullrequestreview-627711741",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4f62ffec-289d-4888-9023-41b8214895d0",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Since these functions are used only once, how about inlining them?",
        "createdAt" : "2021-03-29T01:43:48Z",
        "updatedAt" : "2021-04-25T00:49:15Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "50eae2ee-9870-4450-a865-a5463cb37335",
        "parentId" : "4f62ffec-289d-4888-9023-41b8214895d0",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "okay for me. Put it as functions not for reuse but for making the code look simpler.",
        "createdAt" : "2021-03-29T03:52:43Z",
        "updatedAt" : "2021-04-25T00:49:15Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "3fd05e32-79f2-46f1-8ba0-57c623609f45",
        "parentId" : "4f62ffec-289d-4888-9023-41b8214895d0",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "If you don't mind, I prefer keep it as functions. This rule is already a bit complicated. Inlining these code make it looks harder to read.",
        "createdAt" : "2021-04-05T06:31:03Z",
        "updatedAt" : "2021-04-25T00:49:15Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "04db3048-5b03-4e6f-90a6-45bafde7c68b",
        "parentId" : "4f62ffec-289d-4888-9023-41b8214895d0",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "sgtm",
        "createdAt" : "2021-04-05T08:38:59Z",
        "updatedAt" : "2021-04-25T00:49:15Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "a7194092ebc3a9ca97daba29fda0fcc0ec099cf4",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +251,255 @@      pair._1.references.subsetOf(generatorOutputSet)\n    }\n  }\n\n  def unapply(plan: LogicalPlan): Option[LogicalPlan] = plan match {"
  },
  {
    "id" : "6d933ef1-f910-48dc-8b90-85f9e4be2978",
    "prId" : 31966,
    "prUrl" : "https://github.com/apache/spark/pull/31966#pullrequestreview-638404962",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "022a12ba-2dac-4ea3-a342-9b77396c439c",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "This PR targets at explode-like generators only? How about the others, e.g., `inline`?",
        "createdAt" : "2021-04-13T02:29:46Z",
        "updatedAt" : "2021-04-25T00:49:15Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "b1ee7d12-1b13-41bd-93f1-7f9562ef4ad9",
        "parentId" : "022a12ba-2dac-4ea3-a342-9b77396c439c",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Ideally it is okay too. We also support other generator pruning through (plz see `canPruneGenerator`). As the reported case is `explode`, I work on it first. We can add more support in follow-ups.",
        "createdAt" : "2021-04-15T07:07:42Z",
        "updatedAt" : "2021-04-25T00:49:15Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "bfab1246-deb3-4bd3-adf9-df53aa96b6a1",
        "parentId" : "022a12ba-2dac-4ea3-a342-9b77396c439c",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "yea, targeting `explode` in this PR sounds okay. Could you write it explicitly in the PR description?",
        "createdAt" : "2021-04-19T01:23:57Z",
        "updatedAt" : "2021-04-25T00:49:15Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "b8791b90-c15c-447f-8858-1a6d2b78ca3d",
        "parentId" : "022a12ba-2dac-4ea3-a342-9b77396c439c",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Thanks. Added a note in the description.",
        "createdAt" : "2021-04-19T01:28:44Z",
        "updatedAt" : "2021-04-25T00:49:15Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "a7194092ebc3a9ca97daba29fda0fcc0ec099cf4",
    "line" : 77,
    "diffHunk" : "@@ -1,1 +304,308 @@                //       df.select(explode($\"items.a\").as(\"item.a\"))\n                val rewrittenG = newG.transformExpressions {\n                  case e: ExplodeBase =>\n                    val extractor = nestedFieldsOnGenerator.head._1.transformUp {\n                      case _: Attribute =>"
  },
  {
    "id" : "b3a8f7fc-6775-46fa-a73a-437c929b0232",
    "prId" : 31966,
    "prUrl" : "https://github.com/apache/spark/pull/31966#pullrequestreview-644086163",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "39e6f218-4170-4376-a0fa-42aeee1dd714",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Got it. Let me play more with this PR for a while. It seems I need more tests.",
        "createdAt" : "2021-04-25T02:51:20Z",
        "updatedAt" : "2021-04-25T02:51:21Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "9cfc2dcb-0fb7-4361-9515-ce631ea4e333",
        "parentId" : "39e6f218-4170-4376-a0fa-42aeee1dd714",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Thank you!",
        "createdAt" : "2021-04-25T03:18:50Z",
        "updatedAt" : "2021-04-25T03:18:51Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "a7194092ebc3a9ca97daba29fda0fcc0ec099cf4",
    "line" : 54,
    "diffHunk" : "@@ -1,1 +281,285 @@          // i.e., attr.field when attr is a ArrayType(ArrayType(...)).\n          // Similarily, we also cannot push through if the child of generator is `MapType`.\n          g.generator.children.head.dataType match {\n            case _: MapType => return Some(pushedThrough)\n            case ArrayType(_: ArrayType, _) => return Some(pushedThrough)"
  },
  {
    "id" : "0edb826f-d52f-4ca2-9918-0d532551f8b3",
    "prId" : 29027,
    "prUrl" : "https://github.com/apache/spark/pull/29027#pullrequestreview-444327081",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f586d873-dc59-4f50-b19d-a231a5bf6ef7",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Is this the main difference from 3.0, `dedupNestedFields` -> `nestedFields`?",
        "createdAt" : "2020-07-07T23:32:07Z",
        "updatedAt" : "2020-07-07T23:32:07Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "399731e0-b95c-4cd9-8c2a-20a9deb76a4d",
        "parentId" : "f586d873-dc59-4f50-b19d-a231a5bf6ef7",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Yeah, for the change in `NestedColumnAliasing`. Another difference is test. One test in master branch cannot pass in branch-3.0.",
        "createdAt" : "2020-07-07T23:40:12Z",
        "updatedAt" : "2020-07-07T23:40:12Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "1aac4186-fbc9-4d9c-b7e7-8951f7a8001b",
        "parentId" : "f586d873-dc59-4f50-b19d-a231a5bf6ef7",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Yes. Test part looked correct because it's a subset. For this part, it looks a little different and needs more validation.",
        "createdAt" : "2020-07-07T23:42:58Z",
        "updatedAt" : "2020-07-07T23:42:59Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "a563f922-4b3f-4b2a-8f05-ed7d27ce4051",
        "parentId" : "f586d873-dc59-4f50-b19d-a231a5bf6ef7",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Thanks for check. Yes, there is a bit difference between master and branch-3.0 here. So no `dedupNestedFields` in branch-3.0.",
        "createdAt" : "2020-07-07T23:47:42Z",
        "updatedAt" : "2020-07-07T23:47:43Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "dbfde0b8-91c2-4422-9038-14fb68e5690c",
        "parentId" : "f586d873-dc59-4f50-b19d-a231a5bf6ef7",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Ah, I missed this part. I think the added test still fails if we don't have this change. Is this correct?",
        "createdAt" : "2020-07-07T23:54:15Z",
        "updatedAt" : "2020-07-07T23:54:15Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "d0ecaddf-fab6-4c52-91a0-4a46cb27032e",
        "parentId" : "f586d873-dc59-4f50-b19d-a231a5bf6ef7",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Yes. It does. The new test case still validate this patch in terms of that part.",
        "createdAt" : "2020-07-07T23:57:02Z",
        "updatedAt" : "2020-07-07T23:57:20Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "cc4a5d92-f264-495c-8bc8-7a83b219456e",
        "parentId" : "f586d873-dc59-4f50-b19d-a231a5bf6ef7",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Yes, this test fails in current branch-3.0.",
        "createdAt" : "2020-07-07T23:57:34Z",
        "updatedAt" : "2020-07-07T23:57:34Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "fb4e1653-d8ac-4521-9a3d-0665ac17b123",
        "parentId" : "f586d873-dc59-4f50-b19d-a231a5bf6ef7",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Nice.",
        "createdAt" : "2020-07-07T23:59:42Z",
        "updatedAt" : "2020-07-07T23:59:42Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "5e4a420b13adb588d7fda4215e03cdf582ae2ac6",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +138,142 @@        // nested field once.\n        if (nestedFieldToAlias.nonEmpty &&\n            nestedFields.map(_.canonicalized)\n              .distinct\n              .map { nestedField => totalFieldNum(nestedField.dataType) }"
  },
  {
    "id" : "34099b71-5758-46b8-87cb-b095bc69b64b",
    "prId" : 28988,
    "prUrl" : "https://github.com/apache/spark/pull/28988#pullrequestreview-442670195",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "79e61216-9364-450c-8547-21134e74b7de",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Nice catch, @viirya @frankyin-factual !",
        "createdAt" : "2020-07-05T11:45:06Z",
        "updatedAt" : "2020-07-07T15:39:33Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "d352dbcdecfbdab08607c2416748e7650294fa42",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +155,159 @@    val aliasSub = nestedFieldReferences.asInstanceOf[Seq[ExtractValue]]\n      .filter(!_.references.subsetOf(exclusiveAttrSet))\n      .groupBy(_.references.head.canonicalized.asInstanceOf[Attribute])\n      .flatMap { case (attr, nestedFields: Seq[ExtractValue]) =>\n        // Remove redundant `ExtractValue`s if they share the same parent nest field."
  },
  {
    "id" : "f5861962-8ead-4827-b371-e6de2464f7c6",
    "prId" : 28898,
    "prUrl" : "https://github.com/apache/spark/pull/28898#pullrequestreview-438139491",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "907a5f26-a597-4f4b-b8f9-67af62eaf562",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "I think we better leave a few comment explaining this case.",
        "createdAt" : "2020-06-26T02:25:23Z",
        "updatedAt" : "2020-07-24T06:11:32Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "e8a77ac3-801e-4919-a3c2-433ca16b5640",
        "parentId" : "907a5f26-a597-4f4b-b8f9-67af62eaf562",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "+1",
        "createdAt" : "2020-06-26T09:12:44Z",
        "updatedAt" : "2020-07-24T06:11:32Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "a0b8d070f027460dd1e5fdbd7dc35d0440450b0a",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +38,42 @@     * the optimizer can hit an infinite loop during the [[PushDownPredicates]] rule.\n     */\n    case Project(projectList, Filter(condition, child))\n        if SQLConf.get.nestedSchemaPruningEnabled && canProjectPushThrough(child) =>\n      val exprCandidatesToPrune = projectList ++ Seq(condition) ++ child.expressions"
  },
  {
    "id" : "b4283f65-71ff-4978-be66-0ecfab380bd7",
    "prId" : 28898,
    "prUrl" : "https://github.com/apache/spark/pull/28898#pullrequestreview-438742296",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7c7628b3-bbe4-47c7-95dd-f63203f71600",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "How about rephrasing it like tihs?\r\n```\r\n    /**\r\n     * This pattern is needed to support [[Filter]] plan cases like\r\n     * [[Project]]->[[Filter]]->listed plan in `canProjectPushThrough` (e.g., [[Window]]).\r\n     * The reason why we don't simply add [[Filter]] in `canProjectPushThrough` is that\r\n     * the optimizer can hit an infinite loop during the [[PushDownPredicates]] rule.\r\n     */\r\n```",
        "createdAt" : "2020-06-27T08:42:40Z",
        "updatedAt" : "2020-07-24T06:11:32Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "aeba5982-cc89-4fa1-8280-573c70676e79",
        "parentId" : "7c7628b3-bbe4-47c7-95dd-f63203f71600",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "btw, do you know why the optimizer can hit the issue? I think its better to check the root cause for future activities if possible.",
        "createdAt" : "2020-06-27T10:06:15Z",
        "updatedAt" : "2020-07-24T06:11:32Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "5607df78-f9a3-4304-94a0-b416d239c9d8",
        "parentId" : "7c7628b3-bbe4-47c7-95dd-f63203f71600",
        "authorId" : "e2104308-ae70-4ecf-9798-03460fcd8763",
        "body" : "I don't know exactly why it's broken, but here is a simple query that can reproduce this issue: \r\n`select name.last from contacts where name.first='Jane'`\r\nThe error message is like: \r\n```\r\n20/06/27 21:17:41 WARN internal.BaseSessionStateBuilder$$anon$2: Max iterations (100) reached for batch Operator Optimization before Inferring Filters, please set 'spark.sql.optimizer.maxIterations' to a larger value.\r\n20/06/27 21:17:41 WARN internal.BaseSessionStateBuilder$$anon$2: Max iterations (100) reached for batch Operator Optimization after Inferring Filters, please set 'spark.sql.optimizer.maxIterations' to a larger value.\r\n```",
        "createdAt" : "2020-06-28T04:22:16Z",
        "updatedAt" : "2020-07-24T06:11:32Z",
        "lastEditedBy" : "e2104308-ae70-4ecf-9798-03460fcd8763",
        "tags" : [
        ]
      }
    ],
    "commit" : "a0b8d070f027460dd1e5fdbd7dc35d0440450b0a",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +37,41 @@     * The reason why we don't simply add [[Filter]] in `canProjectPushThrough` is that\n     * the optimizer can hit an infinite loop during the [[PushDownPredicates]] rule.\n     */\n    case Project(projectList, Filter(condition, child))\n        if SQLConf.get.nestedSchemaPruningEnabled && canProjectPushThrough(child) =>"
  },
  {
    "id" : "3ffd55b0-1eae-495b-b1a0-2230fb759a56",
    "prId" : 28898,
    "prUrl" : "https://github.com/apache/spark/pull/28898#pullrequestreview-453023583",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e5267aa8-e2a4-4d1c-8178-970a52c75f9b",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "BTW, it's logically a little weird to me because the second pattern looks narrower than the first pattern. In Scala, we usually use specific patterns first. I'm saying that `case Project(projectList, Filter(condition, child))` is more specific than the previous pattern `case Project(projectList, child)`. Can we switch this case (line 48) and the previous case (line 34). Or, does it break something? If switching two patterns breaks something, it might be worth to mention.",
        "createdAt" : "2020-07-22T05:47:47Z",
        "updatedAt" : "2020-07-24T06:11:32Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "1ac1cdc1-76a0-49cd-9f5b-e88dd62538eb",
        "parentId" : "e5267aa8-e2a4-4d1c-8178-970a52c75f9b",
        "authorId" : "e2104308-ae70-4ecf-9798-03460fcd8763",
        "body" : "I just changed the order. I don't think it will break anything because those two clauses are mutually exclusive. ",
        "createdAt" : "2020-07-22T06:19:13Z",
        "updatedAt" : "2020-07-24T06:11:32Z",
        "lastEditedBy" : "e2104308-ae70-4ecf-9798-03460fcd8763",
        "tags" : [
        ]
      }
    ],
    "commit" : "a0b8d070f027460dd1e5fdbd7dc35d0440450b0a",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +38,42 @@     * the optimizer can hit an infinite loop during the [[PushDownPredicates]] rule.\n     */\n    case Project(projectList, Filter(condition, child))\n        if SQLConf.get.nestedSchemaPruningEnabled && canProjectPushThrough(child) =>\n      val exprCandidatesToPrune = projectList ++ Seq(condition) ++ child.expressions"
  },
  {
    "id" : "cafdb55e-8efb-4c3b-8cad-92ea66ad6b61",
    "prId" : 28560,
    "prUrl" : "https://github.com/apache/spark/pull/28560#pullrequestreview-413200738",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "594582a3-2f55-4e13-8005-eb9467703dd5",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Some change here is shared with #28556. ",
        "createdAt" : "2020-05-17T21:34:33Z",
        "updatedAt" : "2020-05-22T00:38:55Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "b2e56d55fe1dda39a47ba654e2b60a81fea7e492",
    "line" : 38,
    "diffHunk" : "@@ -1,1 +86,90 @@        nestedFieldToAlias(f).toAttribute\n    }\n  }\n\n  /**"
  },
  {
    "id" : "6dc83665-a67c-48c4-8fda-1a01537e56c4",
    "prId" : 28560,
    "prUrl" : "https://github.com/apache/spark/pull/28560#pullrequestreview-427819556",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b0b9f52c-8152-49f7-a48a-19bb4aeefd32",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Are the current entries all the case we can support for nested column pruning? How about `FlatMapGroupsInPandas`?",
        "createdAt" : "2020-05-18T02:26:29Z",
        "updatedAt" : "2020-05-22T00:38:55Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "8793c57e-7471-4352-961f-9767ba63ba33",
        "parentId" : "b0b9f52c-8152-49f7-a48a-19bb4aeefd32",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "I think FlatMapGroupsInPandas is also supported.\r\n\r\nAs you see, we do `transformExpressions` to replace `ExtractValue` in operator's expressions. So if the operator follows SparkSQL's operators, it should be fine.\r\n\r\nThis patch adds the change needed for general nested column pruning for the kind of operators which can prune nested column. Ideally we only need to add it in `canPruneOn`, and add test for it.\r\n\r\nCurrently I think nested column pruning test cases are all in Scala, no Python. I will think about how to add test.",
        "createdAt" : "2020-05-18T02:38:29Z",
        "updatedAt" : "2020-05-22T00:38:55Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "cc2eb1bb-7eba-4f2b-a9bb-9196bddb8949",
        "parentId" : "b0b9f52c-8152-49f7-a48a-19bb4aeefd32",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "> Currently I think nested column pruning test cases are all in Scala, no Python. I will think about how to add test.\r\n\r\nOkay, thanks for the check. Yea, having tests for Python cases looks nice.",
        "createdAt" : "2020-05-18T02:40:33Z",
        "updatedAt" : "2020-05-22T00:38:55Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "ea99b950-0ba6-4256-a747-2b1846aed01c",
        "parentId" : "b0b9f52c-8152-49f7-a48a-19bb4aeefd32",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "cc: @HyukjinKwon ",
        "createdAt" : "2020-05-18T02:47:43Z",
        "updatedAt" : "2020-05-22T00:38:55Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "a3c9d08d-502a-4129-9bfe-fa8a24b98091",
        "parentId" : "b0b9f52c-8152-49f7-a48a-19bb4aeefd32",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "I think I was wrong. Re-checking `FlatMapGroupsInPandas`'s Python API, it looks like\r\n\r\n```python\r\ndf.groupby(\"id\").apply(udf).show()\r\n```\r\n\r\nSo basically the python udf takes no nested column selection but a full columns of DataFrame. It doesn't do nested column pruning.\r\n\r\n`MapInPandas` is also the same.",
        "createdAt" : "2020-05-18T06:21:20Z",
        "updatedAt" : "2020-05-22T00:38:55Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "b25bd7e3-ff48-417f-8525-5bd2436a8b08",
        "parentId" : "b0b9f52c-8152-49f7-a48a-19bb4aeefd32",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Sorry I just checked this. Yes, I think ^ is correct.",
        "createdAt" : "2020-06-10T08:18:28Z",
        "updatedAt" : "2020-06-10T08:18:29Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "b2e56d55fe1dda39a47ba654e2b60a81fea7e492",
    "line" : 46,
    "diffHunk" : "@@ -1,1 +94,98 @@    case _: Aggregate => true\n    case _: Expand => true\n    case _ => false\n  }\n"
  },
  {
    "id" : "603411b6-7b48-439e-820b-064c7d85ac2d",
    "prId" : 28560,
    "prUrl" : "https://github.com/apache/spark/pull/28560#pullrequestreview-416567849",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f65f12f3-a190-47f9-93b0-8c6da3d13135",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "This case only matches `Aggregate` and `Expand` now?",
        "createdAt" : "2020-05-21T13:49:43Z",
        "updatedAt" : "2020-05-22T00:38:55Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "3e47e4e3-8ca6-4704-95aa-a1075f067737",
        "parentId" : "f65f12f3-a190-47f9-93b0-8c6da3d13135",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Yea, `canPruneOn` decides what operator will be pruned on.\r\n\r\n",
        "createdAt" : "2020-05-21T23:20:03Z",
        "updatedAt" : "2020-05-22T00:38:55Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "a8112ae4-3351-438a-8b20-dc652a23566c",
        "parentId" : "f65f12f3-a190-47f9-93b0-8c6da3d13135",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Ah, I see. Thanks for the check. If so, it might be better to explicitly filiter them here for readablity like `case other if canPruneOn(plan)`, or leave some comments about that. Both is okay to me.",
        "createdAt" : "2020-05-21T23:35:59Z",
        "updatedAt" : "2020-05-22T00:38:55Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "3adc8684-b09f-40c1-a726-dfefd49ef9be",
        "parentId" : "f65f12f3-a190-47f9-93b0-8c6da3d13135",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Let me leave some comments, running `canPruneOn` twice seems redundant.",
        "createdAt" : "2020-05-21T23:59:49Z",
        "updatedAt" : "2020-05-22T00:38:55Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "b2e56d55fe1dda39a47ba654e2b60a81fea7e492",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +57,61 @@\n    // The operators reaching here was already guarded by `canPruneOn`.\n    case other =>\n      replaceChildrenWithAliases(other, nestedFieldToAlias, attrToAliases)\n  }"
  },
  {
    "id" : "97a902d0-cfd9-4ba0-abb2-9373b7457b2f",
    "prId" : 28560,
    "prUrl" : "https://github.com/apache/spark/pull/28560#pullrequestreview-427851260",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "79950352-01ac-426e-bcb3-a3c8705bc2c8",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "What about removing this method and call `NestedColumnAliasing.replaceChildrenWithAliases` directly above?",
        "createdAt" : "2020-06-10T08:55:45Z",
        "updatedAt" : "2020-06-10T08:55:45Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "b2e56d55fe1dda39a47ba654e2b60a81fea7e492",
    "line" : 51,
    "diffHunk" : "@@ -1,1 +224,228 @@\n  private def pruneGenerate(\n      g: Generate,\n      nestedFieldToAlias: Map[ExtractValue, Alias],\n      attrToAliases: Map[ExprId, Seq[Alias]]): LogicalPlan = {"
  },
  {
    "id" : "1910bc02-8d19-42c9-9a0a-cf14140d8b29",
    "prId" : 28556,
    "prUrl" : "https://github.com/apache/spark/pull/28556#pullrequestreview-415972912",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7b3c82c5-8781-44d7-b695-1315d6b8dc0c",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "nit: Could you update the title like `Nested column aliasing for RepartitionByExpression/Join`?",
        "createdAt" : "2020-05-18T01:55:53Z",
        "updatedAt" : "2020-06-10T21:42:46Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "566dbbbf-ab6e-4772-b332-093511ced200",
        "parentId" : "7b3c82c5-8781-44d7-b695-1315d6b8dc0c",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Btw, joins update the nullability of input attributes (outer join cases), but does this logic work correctly? Either way, I think we need some tests for the case. It seems the current PR only has tests for inner join cases?",
        "createdAt" : "2020-05-18T02:00:19Z",
        "updatedAt" : "2020-06-10T21:42:46Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "c528300c-08ca-4157-9c74-4a646c26ac58",
        "parentId" : "7b3c82c5-8781-44d7-b695-1315d6b8dc0c",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Yea, I will add more tests for join, like outer join.\r\n\r\nThe nested column pruning here is for a Project on top of Join (or others). We just replace original complex output with nested pruned fields. IIUC, although joins update nullability, for null outputs, those nested pruned fields should be null too, because the nullability is determined by join semantics.\r\n\r\nI will add more tests to verify it.",
        "createdAt" : "2020-05-18T02:48:03Z",
        "updatedAt" : "2020-06-10T21:42:46Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "837e9aee-0312-4725-858e-ae86e7910dd8",
        "parentId" : "7b3c82c5-8781-44d7-b695-1315d6b8dc0c",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Added some tests for outer join.",
        "createdAt" : "2020-05-21T08:35:59Z",
        "updatedAt" : "2020-06-10T21:42:46Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "ce5d8dcc8b5cc61d66bad619638606716b3f44bf",
    "line" : 42,
    "diffHunk" : "@@ -1,1 +108,112 @@    case _: Sample => true\n    case _: RepartitionByExpression => true\n    case _: Join => true\n    case _ => false\n  }"
  },
  {
    "id" : "6810d417-25e3-4cd5-a962-461e70835469",
    "prId" : 28556,
    "prUrl" : "https://github.com/apache/spark/pull/28556#pullrequestreview-429879278",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d95fc1d9-1c5d-407a-9b80-8e0b983ac893",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "No big deal but I would rename `plan` to `p` to avoid shadowing the `plan` argument. At least my IDE complains on that.",
        "createdAt" : "2020-06-12T07:07:19Z",
        "updatedAt" : "2020-06-12T07:07:28Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "f5c04da1-314b-49e1-8fa0-8dd2b4cff7bf",
        "parentId" : "d95fc1d9-1c5d-407a-9b80-8e0b983ac893",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "I will change it in other PR. Thanks.",
        "createdAt" : "2020-06-12T16:27:16Z",
        "updatedAt" : "2020-06-12T16:27:16Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "ce5d8dcc8b5cc61d66bad619638606716b3f44bf",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +38,42 @@      getAliasSubMap(exprCandidatesToPrune, child.producedAttributes.toSeq)\n\n    case plan if SQLConf.get.nestedSchemaPruningEnabled && canPruneOn(plan) =>\n      val exprCandidatesToPrune = plan.expressions\n      getAliasSubMap(exprCandidatesToPrune, plan.producedAttributes.toSeq)"
  },
  {
    "id" : "d4907c69-f419-45e3-98cd-06b008f667dd",
    "prId" : 28556,
    "prUrl" : "https://github.com/apache/spark/pull/28556#pullrequestreview-429534293",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e358892a-873f-42cc-b6dc-1da4d1c644ad",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@viirya, just to clarify, you added `producedAttributes` here just to be safe but not related to the current changes (?). Seems `Join` and `RepartitionByExpression` have an empty `producedAttributes`.",
        "createdAt" : "2020-06-12T07:39:23Z",
        "updatedAt" : "2020-06-12T07:39:23Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "aa8d3f5a-f45d-4a49-b985-04aafc45426c",
        "parentId" : "e358892a-873f-42cc-b6dc-1da4d1c644ad",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Okay, if it's going to output, it shouldn't be pruned anyway.",
        "createdAt" : "2020-06-12T07:52:46Z",
        "updatedAt" : "2020-06-12T07:52:46Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "ce5d8dcc8b5cc61d66bad619638606716b3f44bf",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +36,40 @@        if SQLConf.get.nestedSchemaPruningEnabled && canProjectPushThrough(child) =>\n      val exprCandidatesToPrune = projectList ++ child.expressions\n      getAliasSubMap(exprCandidatesToPrune, child.producedAttributes.toSeq)\n\n    case plan if SQLConf.get.nestedSchemaPruningEnabled && canPruneOn(plan) =>"
  },
  {
    "id" : "d29b0f73-df18-44b5-8dec-dff1a11156a6",
    "prId" : 27702,
    "prUrl" : "https://github.com/apache/spark/pull/27702#pullrequestreview-364686549",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "31764a37-20a6-45f6-9c0e-cbcd619fdb44",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Ur, I see. This is a nit comment though, I feel sharing `getAliasSubMap` in both cases (`project w/generate case` and `simple project case`) makes the function arguments a bit complicated. So, its better to leave some comments about the argument `exclusiveAttrs` in the line 105.",
        "createdAt" : "2020-02-26T07:37:16Z",
        "updatedAt" : "2020-02-27T20:05:27Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "cf80d4eb-c588-4b64-94d5-e62a475ea7d4",
        "parentId" : "31764a37-20a6-45f6-9c0e-cbcd619fdb44",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Thanks. Added few comments.",
        "createdAt" : "2020-02-26T08:06:19Z",
        "updatedAt" : "2020-02-27T20:05:27Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "5862428c552c82d858e9edc29d59fd4d9419460f",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +111,115 @@   * 2. ExprId -> Seq[Alias]: A reference attribute has multiple aliases pointing it.\n   */\n  def getAliasSubMap(exprList: Seq[Expression], exclusiveAttrs: Seq[Attribute] = Seq.empty)\n    : Option[(Map[ExtractValue, Alias], Map[ExprId, Seq[Alias]])] = {\n    val (nestedFieldReferences, otherRootReferences) ="
  },
  {
    "id" : "b207b402-5515-44ba-a3cc-051045fc04d3",
    "prId" : 27702,
    "prUrl" : "https://github.com/apache/spark/pull/27702#pullrequestreview-365454477",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0bce4faf-c9fd-44f9-8a36-d00beeac0d7e",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@viirya, just to clarify, we will keep `exclusiveAttrs` instead of `skipAttrs`?",
        "createdAt" : "2020-02-27T06:34:50Z",
        "updatedAt" : "2020-02-27T20:05:27Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "56324fc8-1515-4650-b6b5-5523aeeec074",
        "parentId" : "0bce4faf-c9fd-44f9-8a36-d00beeac0d7e",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "I think yes, I will sync #27517 with this later.",
        "createdAt" : "2020-02-27T07:12:27Z",
        "updatedAt" : "2020-02-27T20:05:27Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "5862428c552c82d858e9edc29d59fd4d9419460f",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +111,115 @@   * 2. ExprId -> Seq[Alias]: A reference attribute has multiple aliases pointing it.\n   */\n  def getAliasSubMap(exprList: Seq[Expression], exclusiveAttrs: Seq[Attribute] = Seq.empty)\n    : Option[(Map[ExtractValue, Alias], Map[ExprId, Seq[Alias]])] = {\n    val (nestedFieldReferences, otherRootReferences) ="
  }
]