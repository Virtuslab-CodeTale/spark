[
  {
    "id" : "454ac5d8-a09e-4146-97b2-31e8f99932f2",
    "prId" : 32301,
    "prUrl" : "https://github.com/apache/spark/pull/32301#pullrequestreview-643695923",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ccc0f886-44e6-4e74-9b3a-613e0eace721",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I think a good first step is to improve the classdoc to explain in detail what we are doing here, with examples.\r\n\r\nA good example is `DecorrelateInnerQuery`.",
        "createdAt" : "2021-04-23T15:09:56Z",
        "updatedAt" : "2021-04-29T04:38:39Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "e0ed6438-346f-48ff-b194-29e99eaf49f1",
        "parentId" : "ccc0f886-44e6-4e74-9b3a-613e0eace721",
        "authorId" : "ada71464-b8c9-4d7b-9020-d49c27251fdb",
        "body" : "I wasn't sure how much to add to the classdoc, but I added some more examples.",
        "createdAt" : "2021-04-23T22:16:02Z",
        "updatedAt" : "2021-04-29T04:38:39Z",
        "lastEditedBy" : "ada71464-b8c9-4d7b-9020-d49c27251fdb",
        "tags" : [
        ]
      }
    ],
    "commit" : "8a29e943447808391c17f860598e3f11ae41d54d",
    "line" : 70,
    "diffHunk" : "@@ -1,1 +80,84 @@ * The schema of the datasource relation will be pruned in the [[SchemaPruning]] optimizer rule.\n */\nobject NestedColumnAliasing {\n\n  def unapply(plan: LogicalPlan): Option[LogicalPlan] = plan match {"
  },
  {
    "id" : "aa0a2d2e-daa4-4868-befe-9e046eac0dfa",
    "prId" : 32301,
    "prUrl" : "https://github.com/apache/spark/pull/32301#pullrequestreview-644464727",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7d092651-d369-4238-9bd1-f18fdeeaa97b",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "we can add more comments:\r\n```\r\n..., because `a.b` means all the inner fields of `b` are needed, and we can't prune `a.b.c`.\r\n```",
        "createdAt" : "2021-04-26T09:37:34Z",
        "updatedAt" : "2021-04-29T04:38:39Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "8a29e943447808391c17f860598e3f11ae41d54d",
    "line" : 290,
    "diffHunk" : "@@ -1,1 +257,261 @@      .flatMap { case (attr: Attribute, nestedFields: Seq[ExtractValue]) =>\n        // Remove redundant [[ExtractValue]]s if they share the same parent nest field.\n        // For example, when `a.b` and `a.b.c` are in project list, we only need to alias `a.b`.\n        // Because `a.b` requires all of the inner fields of `b`, we cannot prune `a.b.c`.\n        val dedupNestedFields = nestedFields.filter {"
  },
  {
    "id" : "ecb816b6-e1eb-478f-af40-931b9ce76109",
    "prId" : 32301,
    "prUrl" : "https://github.com/apache/spark/pull/32301#pullrequestreview-661357736",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "26ac24e5-f51c-49c6-9059-591064df7004",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "we will call `.map(_.canonicalized).distinct` later. Do we need this `.distinct` here?",
        "createdAt" : "2021-04-26T09:42:57Z",
        "updatedAt" : "2021-04-29T04:38:39Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "9f371186-d466-46b5-b467-0cf1535fe0aa",
        "parentId" : "26ac24e5-f51c-49c6-9059-591064df7004",
        "authorId" : "ada71464-b8c9-4d7b-9020-d49c27251fdb",
        "body" : "We use `.map(_.canonicalized).distinct` only for counting the number of nested fields. I believe we still need `.distinct` here to deduplicate the nested fields.",
        "createdAt" : "2021-04-27T21:13:58Z",
        "updatedAt" : "2021-04-29T04:38:39Z",
        "lastEditedBy" : "ada71464-b8c9-4d7b-9020-d49c27251fdb",
        "tags" : [
        ]
      },
      {
        "id" : "ac551030-eb61-4672-a97a-5a480862f728",
        "parentId" : "26ac24e5-f51c-49c6-9059-591064df7004",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "why don't we do `.map(_.canonicalized).distinct` here?",
        "createdAt" : "2021-04-29T04:37:17Z",
        "updatedAt" : "2021-04-29T04:38:39Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "4de75402-1dc7-46b5-9b23-982f5f47e414",
        "parentId" : "26ac24e5-f51c-49c6-9059-591064df7004",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "We can also update the lookup side\r\n`nestedFieldToAlias.contains(f)` -> `nestedFieldToAlias.contains(f.canonicalized)`",
        "createdAt" : "2021-04-29T04:44:55Z",
        "updatedAt" : "2021-04-29T04:44:56Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "264a1833-0890-466b-a377-a05dc2712bae",
        "parentId" : "26ac24e5-f51c-49c6-9059-591064df7004",
        "authorId" : "ada71464-b8c9-4d7b-9020-d49c27251fdb",
        "body" : "When I tried this, the tests failed - turns out that canonicalization strips key information for analysis, such as the name of the AttributeReference (see [ignoreNameTypes](https://github.com/apache/spark/blob/2a335f2d7d1265cb9abd2e727f60d8eb2dfa356b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Canonicalize.scala#L44)).",
        "createdAt" : "2021-05-17T19:52:11Z",
        "updatedAt" : "2021-05-17T19:52:11Z",
        "lastEditedBy" : "ada71464-b8c9-4d7b-9020-d49c27251fdb",
        "tags" : [
        ]
      }
    ],
    "commit" : "8a29e943447808391c17f860598e3f11ae41d54d",
    "line" : 309,
    "diffHunk" : "@@ -1,1 +266,270 @@            nestedFields.forall(f => child.find(_.semanticEquals(f)).isEmpty)\n          case _ => true\n        }.distinct\n\n        // If all nested fields of `attr` are used, we don't need to introduce new aliases."
  },
  {
    "id" : "0d1858d5-064d-47bc-8efc-e93d052ca3cb",
    "prId" : 32301,
    "prUrl" : "https://github.com/apache/spark/pull/32301#pullrequestreview-669825938",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b78fa060-7c91-4c28-921e-3a1c78a0477a",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Not related to this PR: does it mean we don't support Project -> Limit -> Filter?",
        "createdAt" : "2021-05-25T15:30:43Z",
        "updatedAt" : "2021-05-25T15:30:43Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "d4406d01-3d9b-4496-842d-e1f831751dfe",
        "parentId" : "b78fa060-7c91-4c28-921e-3a1c78a0477a",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "cc @viirya @maropu ",
        "createdAt" : "2021-05-26T05:38:55Z",
        "updatedAt" : "2021-05-26T05:38:55Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "87a81423-93b5-4344-b977-1e17feeb878e",
        "parentId" : "b78fa060-7c91-4c28-921e-3a1c78a0477a",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "`Limit` is able to be pushed through, no?",
        "createdAt" : "2021-05-26T22:57:53Z",
        "updatedAt" : "2021-05-26T22:57:53Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "9170c80b-4cae-42db-9e0f-aec4bfe3a4ad",
        "parentId" : "b78fa060-7c91-4c28-921e-3a1c78a0477a",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Yea you are right, we will first generate a new project between Limit and Filter (the Project grandchild) to do nested column pruning, then the next batch we can match Project -> Filter and do nested column pruning further.",
        "createdAt" : "2021-05-27T07:22:29Z",
        "updatedAt" : "2021-05-27T07:22:30Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "8a29e943447808391c17f860598e3f11ae41d54d",
    "line" : 78,
    "diffHunk" : "@@ -1,1 +86,90 @@     * This pattern is needed to support [[Filter]] plan cases like\n     * [[Project]]->[[Filter]]->listed plan in [[canProjectPushThrough]] (e.g., [[Window]]).\n     * The reason why we don't simply add [[Filter]] in [[canProjectPushThrough]] is that\n     * the optimizer can hit an infinite loop during the [[PushDownPredicates]] rule.\n     */"
  },
  {
    "id" : "4e81af48-a975-47a6-97b7-76c2e6b1cf50",
    "prId" : 32301,
    "prUrl" : "https://github.com/apache/spark/pull/32301#pullrequestreview-669609196",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f50f5dce-81e6-45c7-b4c7-fcee86f56dd2",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "not related to this PR: we only add a new Project node as grandchild, but the scan node may still be at the bottom. Do we expect other rules to push down the Project through other nodes until it reaches the bottom?",
        "createdAt" : "2021-05-25T15:38:42Z",
        "updatedAt" : "2021-05-25T15:38:42Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "e288d31e-a54a-4179-9622-6bf8be28645b",
        "parentId" : "f50f5dce-81e6-45c7-b4c7-fcee86f56dd2",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "cc @viirya @maropu ",
        "createdAt" : "2021-05-26T05:39:00Z",
        "updatedAt" : "2021-05-26T05:39:01Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "9476847f-c31d-4290-a4f8-addac34b9337",
        "parentId" : "f50f5dce-81e6-45c7-b4c7-fcee86f56dd2",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Yes, this is how nested column pruning works now. The added Project with nested column references will be push down through other nodes until Scan.\r\n\r\nIf there are any nodes we cannot push through, then nested column pruning doesn't work.",
        "createdAt" : "2021-05-26T22:56:05Z",
        "updatedAt" : "2021-05-26T22:56:05Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "8a29e943447808391c17f860598e3f11ae41d54d",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +32,36 @@ * Then:\n * - Substitute the nested field references with alias attributes\n * - Add grandchild [[Project]]s transforming the nested fields to aliases\n *\n * Example 1: Project"
  },
  {
    "id" : "951f6949-8e19-409f-bcbd-bf4b2dcfe7be",
    "prId" : 32301,
    "prUrl" : "https://github.com/apache/spark/pull/32301#pullrequestreview-668023178",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "842e2681-be5f-479f-82ea-22475bf9c790",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I'm still a bit worried about calling distinct on uncanonicalized expressions. how about \r\n```\r\n.groupBy(_.canonicalized).values.map(_.head).toSeq\r\n```",
        "createdAt" : "2021-05-25T15:45:50Z",
        "updatedAt" : "2021-05-25T15:45:50Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "b64a9bfd-f7cd-4b9f-8b7b-232e172e3df4",
        "parentId" : "842e2681-be5f-479f-82ea-22475bf9c790",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Sorry I'm wrong. We can't do this as the caller side will look up the map from extract value to alias using the original expression.",
        "createdAt" : "2021-05-25T15:57:47Z",
        "updatedAt" : "2021-05-25T15:57:47Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "8a29e943447808391c17f860598e3f11ae41d54d",
    "line" : 309,
    "diffHunk" : "@@ -1,1 +266,270 @@            nestedFields.forall(f => child.find(_.semanticEquals(f)).isEmpty)\n          case _ => true\n        }.distinct\n\n        // If all nested fields of `attr` are used, we don't need to introduce new aliases."
  },
  {
    "id" : "1120e809-7c7c-4aa7-aaba-c71bb7ee516a",
    "prId" : 32301,
    "prUrl" : "https://github.com/apache/spark/pull/32301#pullrequestreview-668551472",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "acb3b7d4-d2ff-4ed2-8ce7-b9881143b2b9",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I didn't carefully review the code in the else branch. I assume it's the same as the previous code?",
        "createdAt" : "2021-05-25T16:04:29Z",
        "updatedAt" : "2021-05-25T16:04:29Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "717248d3-401c-4e13-89c9-f29cac3313b6",
        "parentId" : "acb3b7d4-d2ff-4ed2-8ce7-b9881143b2b9",
        "authorId" : "ada71464-b8c9-4d7b-9020-d49c27251fdb",
        "body" : "Most of this is cosmetic, but at the end there is an expression ID-based lookup. The current commit explicitly uses the exprId to look up, but I can wrap more variables as an AttributeMap.",
        "createdAt" : "2021-05-26T02:43:07Z",
        "updatedAt" : "2021-05-26T02:43:07Z",
        "lastEditedBy" : "ada71464-b8c9-4d7b-9020-d49c27251fdb",
        "tags" : [
        ]
      }
    ],
    "commit" : "8a29e943447808391c17f860598e3f11ae41d54d",
    "line" : 476,
    "diffHunk" : "@@ -1,1 +341,345 @@        Some(pushedThrough)\n      } else {\n        // Only one nested column accessor.\n        // E.g., df.select(explode($\"items\").as(\"item\")).select($\"item.a\")\n        val nestedFieldOnGenerator = nestedFieldsOnGenerator.head"
  }
]