[
  {
    "id" : "0a2c5d7f-036c-4350-94ea-ef6df5722959",
    "prId" : 31955,
    "prUrl" : "https://github.com/apache/spark/pull/31955#pullrequestreview-622094664",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f32eb9c9-d775-4add-bceb-0337f42e76b7",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "This actually matches `CodeGenerator.setColumn`, which copies the value before setting the value to an InternalRow. ",
        "createdAt" : "2021-03-26T12:39:27Z",
        "updatedAt" : "2021-03-26T12:39:27Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "d00e70596e1b679d2331375629249905e9c75395",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +807,811 @@      keyVar.value.set(map.keyArray().get(i, keyVar.dataType))\n      valueVar.value.set(map.valueArray().get(i, valueVar.dataType))\n      val result = InternalRow.copyValue(functionForEval.eval(inputRow))\n      resultKeys.update(i, result)\n      i += 1"
  },
  {
    "id" : "889548b3-e548-47f5-b716-7218bc90cc8f",
    "prId" : 27788,
    "prUrl" : "https://github.com/apache/spark/pull/27788#pullrequestreview-369451061",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "38f3aab9-6375-4ea1-85ef-be69f954b70b",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "This is actually a followup of https://github.com/apache/spark/pull/27655. I am not going to add a test case because we don't guarantee on the naming against `NonSQLExpression` yet. It is just to make it prettier.\r\n\r\nI manually tested as below:\r\n\r\n```scala\r\nval identify = udf((input: Boolean) => input)\r\nspark.range(10).select(identify(exists(array(col(\"id\")), _ % 2 === 0))).show()\r\n```\r\n\r\nBefore:\r\n\r\n```\r\n+-------------------------------------------------------------------------------------+\r\n|UDF(exists(array(id), lambdafunction(((lambda 'x % 2) = 0), lambda 'x, false), true))|\r\n+-------------------------------------------------------------------------------------+\r\n|                                                                                 true|\r\n|                                                                                false|\r\n|                                                                                 true|\r\n...\r\n```\r\n\r\n\r\nAfter:\r\n\r\n```\r\n+-------------------------------------------------------------------------------+\r\n|UDF(exists(array(id), lambdafunction(((lambda 'x % 2) = 0), lambda 'x, false)))|\r\n+-------------------------------------------------------------------------------+\r\n|                                                                           true|\r\n|                                                                          false|\r\n|                                                                           true|\r\n```\r\n\r\nHere, now we hide properly `followThreeValuedLogic` in `ArrayExists`.",
        "createdAt" : "2020-03-05T10:24:38Z",
        "updatedAt" : "2020-03-05T10:26:20Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "e886efde6f29e4c5f6521179e3456743699f49ef",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +535,539 @@\n  override def stringArgs: Iterator[Any] = super.stringArgs.take(2)\n\n  override def nullable: Boolean =\n    if (followThreeValuedLogic) {"
  },
  {
    "id" : "9140c827-459f-46e2-9659-e791777a84dd",
    "prId" : 25728,
    "prUrl" : "https://github.com/apache/spark/pull/25728#pullrequestreview-318036689",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2e60dc1e-4d89-4f18-bc50-f81f75c6a02d",
        "parentId" : null,
        "authorId" : "99e31844-a5b3-48a2-af71-412edb607a13",
        "body" : "Is this phrase `in ascending order` always true?",
        "createdAt" : "2019-11-17T18:48:31Z",
        "updatedAt" : "2019-11-17T19:07:26Z",
        "lastEditedBy" : "99e31844-a5b3-48a2-af71-412edb607a13",
        "tags" : [
        ]
      }
    ],
    "commit" : "ef28d4fd63d6fa71081035b45293a90fc1575687",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +292,296 @@// scalastyle:off line.size.limit\n@ExpressionDescription(\n  usage = \"\"\"_FUNC_(expr, func) - Sorts the input array in ascending order. The elements of the\n    input array must be orderable. Null elements will be placed at the end of the returned\n    array. Since 3.0.0 this function also sorts and returns the array based on the given"
  },
  {
    "id" : "9fc73bbb-16ae-4982-a5b2-5abf0d3cbed8",
    "prId" : 25666,
    "prUrl" : "https://github.com/apache/spark/pull/25666#pullrequestreview-294748000",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "86ed8d3e-3b68-494b-9663-40f8db576597",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "We don't need to validate # of arguments here? (the case: arguments.size > 2)",
        "createdAt" : "2019-09-19T23:58:07Z",
        "updatedAt" : "2019-09-30T04:44:34Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "bf2bb33f-1bbe-496f-8ee5-5560e493b983",
        "parentId" : "86ed8d3e-3b68-494b-9663-40f8db576597",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Can you check the current error mesasage for the case?",
        "createdAt" : "2019-09-20T00:01:00Z",
        "updatedAt" : "2019-09-30T04:44:34Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "85d8f868-b291-4a6c-8255-5c208a5e95f3",
        "parentId" : "86ed8d3e-3b68-494b-9663-40f8db576597",
        "authorId" : "1c87984f-6b31-4210-aea5-f499ba77a2c3",
        "body" : "ArrayTransform doesn't validate arguments.size > 2. I'm not sure what happens in that case either.",
        "createdAt" : "2019-09-20T01:31:35Z",
        "updatedAt" : "2019-09-30T04:44:34Z",
        "lastEditedBy" : "1c87984f-6b31-4210-aea5-f499ba77a2c3",
        "tags" : [
        ]
      },
      {
        "id" : "7f1576bf-3510-446d-8d77-80da7021d715",
        "parentId" : "86ed8d3e-3b68-494b-9663-40f8db576597",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "nvm. I checked the error handling works well for the case.",
        "createdAt" : "2019-09-20T04:40:52Z",
        "updatedAt" : "2019-09-30T04:44:34Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "9bfa8c7b-5122-468a-9524-5ce1cfffd6a5",
        "parentId" : "86ed8d3e-3b68-494b-9663-40f8db576597",
        "authorId" : "ace9fc20-99c8-4843-9c7a-39b4c500f0c8",
        "body" : "Yes, it does. See the test here: https://github.com/apache/spark/pull/25666/files#diff-8e1a34391fdefa4a3a0349d7d454d86fR2204.",
        "createdAt" : "2019-09-30T04:45:16Z",
        "updatedAt" : "2019-09-30T04:45:16Z",
        "lastEditedBy" : "ace9fc20-99c8-4843-9c7a-39b4c500f0c8",
        "tags" : [
        ]
      }
    ],
    "commit" : "93048670b427bd8c1b54f4be3459738c90806f0b",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +367,371 @@        copy(function = f(function, (elementType, containsNull) :: (IntegerType, false) :: Nil))\n      case _ =>\n        copy(function = f(function, (elementType, containsNull) :: Nil))\n    }\n  }"
  },
  {
    "id" : "165819bd-cfb3-4a47-9456-001eabab42eb",
    "prId" : 25666,
    "prUrl" : "https://github.com/apache/spark/pull/25666#pullrequestreview-296573515",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cd1609ce-90d1-4447-8e9a-1c2bea12dd08",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Can you avoid this per-row check? The current code causes unnecessary runtime overheads.",
        "createdAt" : "2019-09-30T23:47:44Z",
        "updatedAt" : "2019-09-30T23:47:45Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "8d1c2d13-8d13-4619-bac1-59b1c9e92669",
        "parentId" : "cd1609ce-90d1-4447-8e9a-1c2bea12dd08",
        "authorId" : "ace9fc20-99c8-4843-9c7a-39b4c500f0c8",
        "body" : "@maropu do you have a suggestion about how to do this without implementing codegen? I tried rewriting the logic like so:\r\n\r\n```scala\r\n  @transient private lazy val evalFn: (InternalRow, Any) => Any = indexVar match {\r\n    case None => (inputRow, argumentValue) =>\r\n      val arr = argumentValue.asInstanceOf[ArrayData]\r\n      val f = functionForEval\r\n      val buffer = new mutable.ArrayBuffer[Any](arr.numElements)\r\n      var i = 0\r\n      while (i < arr.numElements) {\r\n        elementVar.value.set(arr.get(i, elementVar.dataType))\r\n        if (f.eval(inputRow).asInstanceOf[Boolean]) {\r\n          buffer += elementVar.value.get\r\n        }\r\n        i += 1\r\n      }\r\n      new GenericArrayData(buffer)\r\n\r\n    case Some(expr) => (inputRow, argumentValue) =>\r\n      val arr = argumentValue.asInstanceOf[ArrayData]\r\n      val f = functionForEval\r\n      val buffer = new mutable.ArrayBuffer[Any](arr.numElements)\r\n      var i = 0\r\n      while (i < arr.numElements) {\r\n        elementVar.value.set(arr.get(i, elementVar.dataType))\r\n        expr.value.set(i)\r\n        if (f.eval(inputRow).asInstanceOf[Boolean]) {\r\n          buffer += elementVar.value.get\r\n        }\r\n        i += 1\r\n      }\r\n      new GenericArrayData(buffer)\r\n\r\n  }\r\n\r\n\r\n  override def nullSafeEval(inputRow: InternalRow, argumentValue: Any): Any = {\r\n    evalFn(inputRow, argumentValue)\r\n  }\r\n```\r\n\r\nBut from some hacky microbenchmarking this doesn't seem to be meaningfully faster and if anything is marginally slower.",
        "createdAt" : "2019-10-01T19:17:02Z",
        "updatedAt" : "2019-10-01T19:17:03Z",
        "lastEditedBy" : "ace9fc20-99c8-4843-9c7a-39b4c500f0c8",
        "tags" : [
        ]
      },
      {
        "id" : "2f5146b6-1e24-412a-b6ca-dfc79d356526",
        "parentId" : "cd1609ce-90d1-4447-8e9a-1c2bea12dd08",
        "authorId" : "ace9fc20-99c8-4843-9c7a-39b4c500f0c8",
        "body" : "This is the benchmark code I was using:\r\n```scala\r\n  test(\"ArrayFilter - benchmark\") {\r\n    import scala.concurrent.duration._\r\n    val b = new Benchmark(\r\n      \"array_filter\",\r\n      1000,\r\n      warmupTime = 5.seconds,\r\n      minTime = 5.seconds)\r\n    val ai0 = Literal.create(Seq(1, 2, 3), ArrayType(IntegerType, containsNull = false))\r\n    val isEven: Expression => Expression = x => x % 2 === 0\r\n    b.addCase(\"filter\") { _ =>\r\n      var i = 0\r\n      while (i < 1000) {\r\n        filter(ai0, isEven).eval()\r\n        i += 1\r\n      }\r\n    }\r\n    b.run()\r\n  }\r\n```",
        "createdAt" : "2019-10-01T19:19:52Z",
        "updatedAt" : "2019-10-01T19:20:01Z",
        "lastEditedBy" : "ace9fc20-99c8-4843-9c7a-39b4c500f0c8",
        "tags" : [
        ]
      },
      {
        "id" : "faa15d95-c41d-43e4-858c-66c77331dd87",
        "parentId" : "cd1609ce-90d1-4447-8e9a-1c2bea12dd08",
        "authorId" : "3a12ce0f-9e73-4cfb-a4b4-b19368cddc2f",
        "body" : "@maropu @henrydavidge The best performing way to avoid the per-row check in a non-codegen setting is to introduce a new expression type, say `ArrayFilterWithIndex`.\r\n\r\nThe tradeoff between the inline per-row check and the lambda batch solution is that on input arrays that are small (like the one @henrydavidge used in his benchmark), the lambda invocation (which is not guaranteed to be inlined+optimized) overhead *may* exceed the per-row check overhead. You'd need a fairly large input array to amortize that.\r\n\r\nIf we want to make it stay simple for now, I'm okay with the inline per-row check version.",
        "createdAt" : "2019-10-01T19:33:38Z",
        "updatedAt" : "2019-10-01T23:23:08Z",
        "lastEditedBy" : "3a12ce0f-9e73-4cfb-a4b4-b19368cddc2f",
        "tags" : [
        ]
      },
      {
        "id" : "ca72f98b-d32c-4abf-aecd-f97e3deb333a",
        "parentId" : "cd1609ce-90d1-4447-8e9a-1c2bea12dd08",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "I thought code like this;\r\n```\r\n\r\n  @transient lazy val (elementVar, mayFillIndex) = function match {\r\n    case LambdaFunction(_, Seq(elemVar: NamedLambdaVariable), _) =>\r\n      (elemVar, (_: Int) => {})\r\n    case LambdaFunction(_, Seq(elemVar: NamedLambdaVariable, idxVar: NamedLambdaVariable), _) =>\r\n      (elemVar, (i: Int) => idxVar.value.set(i))\r\n  }\r\n\r\n  override def nullSafeEval(inputRow: InternalRow, argumentValue: Any): Any = {\r\n    val arr = argumentValue.asInstanceOf[ArrayData]\r\n    val f = functionForEval\r\n    val buffer = new mutable.ArrayBuffer[Any](arr.numElements)\r\n    var i = 0\r\n    while (i < arr.numElements) {\r\n      elementVar.value.set(arr.get(i, elementVar.dataType))\r\n      mayFillIndex(i)\r\n      if (f.eval(inputRow).asInstanceOf[Boolean]) {\r\n        buffer += elementVar.value.get\r\n      }\r\n      i += 1\r\n    }\r\n    new GenericArrayData(buffer)\r\n  }\r\n```",
        "createdAt" : "2019-10-02T03:23:24Z",
        "updatedAt" : "2019-10-02T03:23:25Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "afd0cbca-39f5-4352-bf76-90d0d583275e",
        "parentId" : "cd1609ce-90d1-4447-8e9a-1c2bea12dd08",
        "authorId" : "ace9fc20-99c8-4843-9c7a-39b4c500f0c8",
        "body" : "Ok, tried that as well. It doesn't seem to be significantly different from the others. ",
        "createdAt" : "2019-10-02T03:48:13Z",
        "updatedAt" : "2019-10-02T03:48:13Z",
        "lastEditedBy" : "ace9fc20-99c8-4843-9c7a-39b4c500f0c8",
        "tags" : [
        ]
      },
      {
        "id" : "f8349dac-bc99-489b-a64d-33f9309793cb",
        "parentId" : "cd1609ce-90d1-4447-8e9a-1c2bea12dd08",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Yea, if no big difference, I like the similar handling with the others, e.g., https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/stringExpressions.scala#L555",
        "createdAt" : "2019-10-02T03:51:20Z",
        "updatedAt" : "2019-10-02T03:51:20Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "061fc152-4367-4e85-ba97-334197cd2aa3",
        "parentId" : "cd1609ce-90d1-4447-8e9a-1c2bea12dd08",
        "authorId" : "43998e22-6c2f-401d-9914-8cecf6fad929",
        "body" : "I think this is good enough to go.\r\nHow about merging this for now, and addressing it in a separate PR?\r\n`transform` is doing the same way, so I think we should do the same thing if needed, maybe at the same time.",
        "createdAt" : "2019-10-02T05:22:04Z",
        "updatedAt" : "2019-10-02T05:22:05Z",
        "lastEditedBy" : "43998e22-6c2f-401d-9914-8cecf6fad929",
        "tags" : [
        ]
      },
      {
        "id" : "22ba1dd6-94aa-4f7d-ae3f-6520cf3f5f5c",
        "parentId" : "cd1609ce-90d1-4447-8e9a-1c2bea12dd08",
        "authorId" : "3a12ce0f-9e73-4cfb-a4b4-b19368cddc2f",
        "body" : "+1 for this is ready to go for now and we can address the optimization separately.\r\n\r\nSide-comment on the version that @maropu gave:\r\nThe lambda version that @henrydavidge gave (i.e. \"batch-wise lambda\") would technically have less overhead:\r\n```\r\n// lambda invocation overhead outside of loop\r\nfor each element in array\r\n  do specialized filter action\r\n```\r\nwhereas the version that @maropu gave (i.e. \"element-wise lambda\") would be:\r\n```\r\n// shared loop between the two versions\r\nfor each element in array\r\n  // lambda invocation overhead per element\r\n  invoke mayFillIndex lambda\r\n```\r\n\r\nWith @maropu 's version, let's assume that we're running on the HotSpot JVM and both the with-index and without-index paths have been used, then the best the HotSpot JIT compiler could have done is a profile-guided bimorphic devirtualization on that lambda call site, which will look like the following after devirtualization+inlining:\r\n```\r\nlocal_mayFillIndex = this.mayFillIndex\r\nklazz = local_mayFillIndex.klass\r\nfor each element in array\r\n  // ...\r\n  if (klazz == lambda_klass_1) {\r\n    // no-op\r\n  } else if (klazz == lambda_klass_2) {\r\n    idxVar.value.set(i)\r\n  } else {\r\n    uncommon_trap() // aka deoptimize, or potentially a full virtual call\r\n  }\r\n}\r\n```\r\nThe point is that this JIT-optimized version is actually a degenerated version of Henry's hand-written inline per-element check version, so I wouldn't want to go down this route.",
        "createdAt" : "2019-10-02T18:19:47Z",
        "updatedAt" : "2019-10-02T19:39:17Z",
        "lastEditedBy" : "3a12ce0f-9e73-4cfb-a4b4-b19368cddc2f",
        "tags" : [
        ]
      },
      {
        "id" : "5253ebad-a9fa-4e75-a790-dbf9c91ec0fa",
        "parentId" : "cd1609ce-90d1-4447-8e9a-1c2bea12dd08",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Thanks, kris! That explanation's very helpful to me.",
        "createdAt" : "2019-10-02T23:07:37Z",
        "updatedAt" : "2019-10-02T23:07:37Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "93048670b427bd8c1b54f4be3459738c90806f0b",
    "line" : 41,
    "diffHunk" : "@@ -1,1 +384,388 @@    while (i < arr.numElements) {\n      elementVar.value.set(arr.get(i, elementVar.dataType))\n      if (indexVar.isDefined) {\n        indexVar.get.value.set(i)\n      }"
  },
  {
    "id" : "4cd6c6a2-3ee8-4441-8c1f-005de0bd906c",
    "prId" : 25666,
    "prUrl" : "https://github.com/apache/spark/pull/25666#pullrequestreview-347132215",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5266fe5e-e67d-4cc9-96e0-af4d9a876893",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "Here, the indices start at 0. but it sounds like the other built-in functions start at 1. ",
        "createdAt" : "2020-01-23T08:22:07Z",
        "updatedAt" : "2020-01-23T08:22:08Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "5bf02215-1872-4b7a-b536-8da13f5e3f1b",
        "parentId" : "5266fe5e-e67d-4cc9-96e0-af4d9a876893",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "I remember there was the (not-merged) PR to standardize one-based column indexes in built-in funcs: https://github.com/apache/spark/pull/24051\r\nBetter to fix them up for consistency?",
        "createdAt" : "2020-01-23T08:46:07Z",
        "updatedAt" : "2020-01-23T08:46:08Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "93048670b427bd8c1b54f4be3459738c90806f0b",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +345,349 @@      > SELECT _FUNC_(array(1, 2, 3), x -> x % 2 == 1);\n       [1,3]\n      > SELECT _FUNC_(array(0, 2, 3), (x, i) -> x > i);\n       [2,3]\n  \"\"\","
  }
]