[
  {
    "id" : "2f20aef2-23a7-475c-be80-7012705daa34",
    "prId" : 33468,
    "prUrl" : "https://github.com/apache/spark/pull/33468#pullrequestreview-712493520",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "960f979b-d592-42ab-a1a3-c83842c55926",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "fixed a bug here: we should not always exclude the first element, as `condition` can be None",
        "createdAt" : "2021-07-21T18:14:15Z",
        "updatedAt" : "2021-07-21T18:14:15Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "8c2bccb7-8e80-4de1-8566-b33c1a15d191",
        "parentId" : "960f979b-d592-42ab-a1a3-c83842c55926",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "cc @sigmod ",
        "createdAt" : "2021-07-21T18:14:23Z",
        "updatedAt" : "2021-07-21T18:14:23Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "50e9f78c-1dc1-4317-8057-7cedf755624b",
        "parentId" : "960f979b-d592-42ab-a1a3-c83842c55926",
        "authorId" : "8b518862-583d-43a2-a10e-b33ef1b6d824",
        "body" : "Thanks for the fix, @cloud-fan!\r\n\r\ncc @dbaliafroozeh ",
        "createdAt" : "2021-07-22T01:07:56Z",
        "updatedAt" : "2021-07-22T01:08:06Z",
        "lastEditedBy" : "8b518862-583d-43a2-a10e-b33ef1b6d824",
        "tags" : [
        ]
      },
      {
        "id" : "79d13158-2b9f-472a-873b-ee75117bb614",
        "parentId" : "960f979b-d592-42ab-a1a3-c83842c55926",
        "authorId" : "def2dd9f-ce27-41ce-8156-f8225ddc68a9",
        "body" : "Thanks for the fix @cloud-fan!",
        "createdAt" : "2021-07-22T08:39:30Z",
        "updatedAt" : "2021-07-22T08:39:30Z",
        "lastEditedBy" : "def2dd9f-ce27-41ce-8156-f8225ddc68a9",
        "tags" : [
        ]
      }
    ],
    "commit" : "984b511adb2ca5b60f6acf1a84e1bbacc6effc30",
    "line" : 32,
    "diffHunk" : "@@ -1,1 +480,484 @@    copy(\n      condition = if (condition.isDefined) Some(newChildren.head) else None,\n      assignments = newChildren.takeRight(assignments.length).asInstanceOf[Seq[Assignment]])\n}\n"
  },
  {
    "id" : "67ffceca-0612-4306-a625-4cbca9bb6d97",
    "prId" : 33200,
    "prUrl" : "https://github.com/apache/spark/pull/33200#pullrequestreview-713087207",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4a3ec02c-fcf5-441c-9a28-136f0858d280",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "do we need the `path` to be resolved as well?",
        "createdAt" : "2021-07-22T12:42:42Z",
        "updatedAt" : "2021-07-22T12:42:42Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "83ca01a6-ac2a-43a1-89a3-c00a55df2f68",
        "parentId" : "4a3ec02c-fcf5-441c-9a28-136f0858d280",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "Updated.",
        "createdAt" : "2021-07-22T18:00:18Z",
        "updatedAt" : "2021-07-22T18:04:42Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "d2e691086b80fb7f64d1bce144bd1bdd03428b8d",
    "line" : 78,
    "diffHunk" : "@@ -1,1 +1102,1106 @@      require(col.path.forall(_.resolved),\n        \"FieldName should be resolved before it's converted to TableChange.\")\n      require(col.position.forall(_.resolved),\n        \"FieldPosition should be resolved before it's converted to TableChange.\")\n      TableChange.addColumn("
  },
  {
    "id" : "87e3ef46-b9ec-4002-990d-19d822aa9615",
    "prId" : 33200,
    "prUrl" : "https://github.com/apache/spark/pull/33200#pullrequestreview-714597259",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6e53d572-b188-4924-9f55-3572878883c6",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "since `QualifiedColType` is not an expression, the default `QueryPlan.resolved` won't work here. Can we override `resolved`?",
        "createdAt" : "2021-07-26T08:50:39Z",
        "updatedAt" : "2021-07-26T08:50:40Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "d2e691086b80fb7f64d1bce144bd1bdd03428b8d",
    "line" : 67,
    "diffHunk" : "@@ -1,1 +1091,1095 @@case class AlterTableAddColumns(\n    table: LogicalPlan,\n    columnsToAdd: Seq[QualifiedColType]) extends AlterTableColumnCommand {\n  columnsToAdd.foreach { c =>\n    TypeUtils.failWithIntervalType(c.dataType)"
  },
  {
    "id" : "db4e97cf-d4c9-4e91-9db1-dda823455e7f",
    "prId" : 32058,
    "prUrl" : "https://github.com/apache/spark/pull/32058#pullrequestreview-629010403",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "439dd3c9-f9c7-4648-bbeb-71645fc5a9ac",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "shall we simply add a new implementation of `MergeAction`: `AssignStarAction`?",
        "createdAt" : "2021-04-06T06:52:36Z",
        "updatedAt" : "2021-04-06T06:52:36Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "b4154043-d741-41f8-b225-87d3748bd877",
        "parentId" : "439dd3c9-f9c7-4648-bbeb-71645fc5a9ac",
        "authorId" : "9cd657f0-37cd-41bd-908a-f2c996b95a7a",
        "body" : "in that case shouldnt it be `InsertStarAction(condition)` and `UpdateStarAction(condition)`?",
        "createdAt" : "2021-04-06T14:03:21Z",
        "updatedAt" : "2021-04-06T14:03:21Z",
        "lastEditedBy" : "9cd657f0-37cd-41bd-908a-f2c996b95a7a",
        "tags" : [
        ]
      }
    ],
    "commit" : "26b5104a9283b070c11a13990aab8f0ac6ca687a",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +429,433 @@    condition: Option[Expression],\n    assignments: Seq[Assignment],\n    assignStar: Boolean) extends MergeAction {\n  override def children: Seq[Expression] = condition.toSeq ++ assignments\n}"
  },
  {
    "id" : "51060d65-c2d8-4c2e-bc79-abad25055a12",
    "prId" : 32032,
    "prUrl" : "https://github.com/apache/spark/pull/32032#pullrequestreview-634593801",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dc61a31e-d15f-43d4-aa10-77887d4c7729",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "is it possible that `newChildren` is empty? Probably safer to add `if (isAnalyzed) ... else ...`",
        "createdAt" : "2021-04-12T08:41:25Z",
        "updatedAt" : "2021-04-14T03:19:24Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "05900f9b-5c20-42d8-8602-6857dd174967",
        "parentId" : "dc61a31e-d15f-43d4-aa10-77887d4c7729",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "This should be called only when there exist children. Maybe an assert is better `assert(!isAnalyzed)`? WDYT?\r\n\r\nhttps://github.com/apache/spark/blob/e40fce919ab77f5faeb0bbd34dc86c56c04adbaa/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/trees/TreeNode.scala#L345-L350",
        "createdAt" : "2021-04-12T15:04:56Z",
        "updatedAt" : "2021-04-14T03:19:24Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      },
      {
        "id" : "1d2302f2-c6f1-40c5-991d-8a81a90120b2",
        "parentId" : "dc61a31e-d15f-43d4-aa10-77887d4c7729",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "assert SGTM",
        "createdAt" : "2021-04-13T13:32:20Z",
        "updatedAt" : "2021-04-14T03:19:24Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "111ef8be8bc03a62389aed4f0871b958714eb789",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +1028,1032 @@      newChildren: IndexedSeq[LogicalPlan]): CacheTableAsSelect = {\n    assert(!isAnalyzed)\n    copy(plan = newChildren.head)\n  }\n"
  },
  {
    "id" : "cc14bcb1-9560-404f-9eae-a6f28f3920d1",
    "prId" : 31932,
    "prUrl" : "https://github.com/apache/spark/pull/31932#pullrequestreview-624637882",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "46877903-2326-4240-a8e2-991f906fc18c",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "2 space indent",
        "createdAt" : "2021-03-30T19:48:39Z",
        "updatedAt" : "2021-03-30T19:50:21Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "3c0e5077722d39b25680870ba9d435aafc62466c",
    "line" : 207,
    "diffHunk" : "@@ -1,1 +432,436 @@\ncase class Assignment(key: Expression, value: Expression) extends Expression\n    with Unevaluable with BinaryLike[Expression] {\n  override def nullable: Boolean = false\n  override def dataType: DataType = throw new UnresolvedException(\"nullable\")"
  },
  {
    "id" : "3ae8de31-a85e-4b20-a6da-8f7301276818",
    "prId" : 31737,
    "prUrl" : "https://github.com/apache/spark/pull/31737#pullrequestreview-604139755",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "45912d7f-b4fd-4972-bb34-aa243eeb271e",
        "parentId" : null,
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "When check the code found name not consistence. Just change in this pr or make a followup? @cloud-fan ",
        "createdAt" : "2021-03-04T13:42:46Z",
        "updatedAt" : "2021-03-05T02:16:02Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "2046f644-5a16-438c-a16f-6b4be2ec0294",
        "parentId" : "45912d7f-b4fd-4972-bb34-aa243eeb271e",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "It's very trivial. I'm OK to fix the typo here.",
        "createdAt" : "2021-03-04T14:10:48Z",
        "updatedAt" : "2021-03-05T02:16:02Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "375954c36029bf1c54d9eff24f8f535b483878b8",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +304,308 @@\nobject DescribeNamespace {\n  def getOutputAttrs: Seq[Attribute] = Seq(\n    AttributeReference(\"info_name\", StringType, nullable = false,\n      new MetadataBuilder().putString(\"comment\", \"name of the namespace info\").build())(),"
  },
  {
    "id" : "7a2f554e-c676-4715-98e4-757690489de1",
    "prId" : 31705,
    "prUrl" : "https://github.com/apache/spark/pull/31705#pullrequestreview-601615051",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "21404ee0-5055-4125-a231-a34eaef6c83c",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "the v2 schema is weird: `name: string, value: string`. Shall we follow DESC COLUMN and use `info_name: string, info_value: string`?\r\n\r\nv2 command is not stable yet and we can still change it.",
        "createdAt" : "2021-03-02T09:21:23Z",
        "updatedAt" : "2021-03-03T12:23:24Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "a4015240-36c3-4ce7-b08e-18521cc76e4f",
        "parentId" : "21404ee0-5055-4125-a231-a34eaef6c83c",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> the v2 schema is weird: `name: string, value: string`. Shall we follow DESC COLUMN and use `info_name: string, info_value: string`?\r\n> \r\n> v2 command is not stable yet and we can still change it.\r\n\r\nKeep same should be better. If we want to change in the future, we can change all of them together.",
        "createdAt" : "2021-03-02T09:30:22Z",
        "updatedAt" : "2021-03-03T12:23:24Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "0e7aff7a-e369-40f3-96ac-29432b1c605c",
        "parentId" : "21404ee0-5055-4125-a231-a34eaef6c83c",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "Updated, how about current?",
        "createdAt" : "2021-03-02T09:39:01Z",
        "updatedAt" : "2021-03-03T12:23:24Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "72345b2351cd447c61b93cc186952e6ae526fdfd",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +304,308 @@\nobject DescribeNamespace {\n  def getOutputAttr: Seq[Attribute] = Seq(\n    AttributeReference(\"info_name\", StringType, nullable = false,\n      new MetadataBuilder().putString(\"comment\", \"name of the namespace info\").build())(),"
  },
  {
    "id" : "5e081c14-629a-4cf8-baa8-96c5d19df544",
    "prId" : 31700,
    "prUrl" : "https://github.com/apache/spark/pull/31700#pullrequestreview-604779608",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6837369b-f15a-4a65-a8bc-c013ee7ff5ea",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I don't think we have finalized the streaming writing semantic yet. Ideally, it should be similar to batch (upsert semantic), but we are not at that point yet.\r\n\r\nI think we should standardize the streaming writing query plan later. For now, let's just use one query plan with `OutputMode` as the parameter.",
        "createdAt" : "2021-03-04T13:04:27Z",
        "updatedAt" : "2021-03-04T13:04:27Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "78dae22e-3343-4f04-a6f6-79d9ff325f47",
        "parentId" : "6837369b-f15a-4a65-a8bc-c013ee7ff5ea",
        "authorId" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "body" : "Oh, I thought only update mode was under discussion. Are overwrite and append modes under discussion too?\r\n\r\nI saw @HeartSaVioR's PR to rename `SupportsStreamingUpdateAsAppend`. Is there a discussion I can take a look at?",
        "createdAt" : "2021-03-04T20:43:47Z",
        "updatedAt" : "2021-03-04T20:57:17Z",
        "lastEditedBy" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "tags" : [
        ]
      },
      {
        "id" : "0577f480-f65a-45db-acf1-cfd030eac94e",
        "parentId" : "6837369b-f15a-4a65-a8bc-c013ee7ff5ea",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Streaming write semantic is not same as batch one. The semantic is bound to the stateful operation; there should be only `append`, `update` (not same as overwrite), and `truncate and append (complete)`. For update we haven't constructed the proper way to define it.\r\n\r\nThe major concern is that the group keys in stateful operation must be used as keys in update mode. That is currently not possible (there are some sketched ideas on this though), but Spark has been dealing with update with the huge risk that we're doing the same as append, and the risk is delegated to the sink (or user). The sink or user has to deal with reflecting the appended output as \"upsert\". That's why I renamed `SupportsStreamingUpdate` as `SupportsStreamingUpdateAsAppend` to clarify the behavior.",
        "createdAt" : "2021-03-05T02:00:03Z",
        "updatedAt" : "2021-03-05T02:03:16Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "657e9f80-f722-4ba5-865b-8af1592e8113",
        "parentId" : "6837369b-f15a-4a65-a8bc-c013ee7ff5ea",
        "authorId" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "body" : "Thanks for the context, @HeartSaVioR!\r\n\r\nI think we have two ways to proceed:\r\n\r\nOption 1: Just adapt `WriteToMicroBatchDataSource ` to use the `Write` abstraction and handle it in `V2Writes`.\r\nOption 2: Define specific plans where we have clarity. For example, `append` and `complete` seem well-defined. We could define plans like `AppendStreamingData` and `TruncateAndAppendStreamingData` or anything like that and have something intermediate for `update`.\r\n\r\nI am fine either way but option 1 seems easier for this PR. The rest can be covered by SPARK-27484.\r\n\r\nTo start with, we should all agree this feature is useful for micro-batch streaming.",
        "createdAt" : "2021-03-05T03:40:03Z",
        "updatedAt" : "2021-03-05T03:40:03Z",
        "lastEditedBy" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "tags" : [
        ]
      }
    ],
    "commit" : "e7f167fb0103d7e62127421cf5ad2a594d5f4337",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +167,171 @@}\n\ncase class AppendMicroBatch(\n    table: SupportsWrite,\n    query: LogicalPlan,"
  },
  {
    "id" : "d0bcb3d1-fede-4914-9bcc-e799b24af90c",
    "prId" : 31637,
    "prUrl" : "https://github.com/apache/spark/pull/31637#pullrequestreview-598083200",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "acdedecc-13cf-4775-b455-c03fdfe082e4",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Maybe name it `requireExactMatchedPartitionSpec` or `shouldExactlyMatchPartitionSpec`? I don't feel strongly. I am good with leaving it as is too.",
        "createdAt" : "2021-02-25T01:33:41Z",
        "updatedAt" : "2021-02-26T06:59:55Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "a2c6575cf06660bee6245bc6a23da8713205c74c",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +62,66 @@trait V2PartitionCommand extends Command {\n  def table: LogicalPlan\n  def allowPartialPartitionSpec: Boolean = false\n}\n"
  },
  {
    "id" : "987c38db-63fe-4405-84c8-8950c872a957",
    "prId" : 31596,
    "prUrl" : "https://github.com/apache/spark/pull/31596#pullrequestreview-594998784",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f71bfd5b-de12-469a-b5a5-f4686b9405ca",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "The command incapsulates 2 commands actually: set table location and set partition location. Should we split it to `SetTableLocation` and `SetPartitionLocation` (separately in another PR).",
        "createdAt" : "2021-02-19T21:33:02Z",
        "updatedAt" : "2021-02-22T08:12:12Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "ea8aeafb-5bcd-4d41-ab68-8cec614ee20e",
        "parentId" : "f71bfd5b-de12-469a-b5a5-f4686b9405ca",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "SGTM",
        "createdAt" : "2021-02-22T04:15:16Z",
        "updatedAt" : "2021-02-22T08:12:12Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "f6ab9ab4c127723a24066c559e0389b0a48bf1f2",
    "line" : 86,
    "diffHunk" : "@@ -1,1 +877,881 @@ * The logical plan of the ALTER TABLE ... SET LOCATION command.\n */\ncase class SetTableLocation(\n    table: LogicalPlan,\n    partitionSpec: Option[TablePartitionSpec],"
  },
  {
    "id" : "b0246494-acba-4b63-843b-616edb8efdd4",
    "prId" : 31474,
    "prUrl" : "https://github.com/apache/spark/pull/31474#pullrequestreview-583450258",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b62cc569-84a4-4850-a406-ccb9c4b78734",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I think it's better to put the `output` field in the constructor so that it's more stable (copying the node will not regenerate the output attributes again). It also helps if we want to support self-join later.",
        "createdAt" : "2021-02-04T14:30:11Z",
        "updatedAt" : "2021-02-05T04:56:04Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "3042bd1a361594600835d7b5f3ffeaa1bba51bc6",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +327,331 @@    namespace: LogicalPlan,\n    pattern: Option[String],\n    override val output: Seq[Attribute] = ShowNamespaces.OUTPUT) extends Command {\n  override def children: Seq[LogicalPlan] = Seq(namespace)\n}"
  },
  {
    "id" : "3626b8c8-bf49-457a-b7b1-a948fcbaf9ec",
    "prId" : 30806,
    "prUrl" : "https://github.com/apache/spark/pull/30806#pullrequestreview-553807723",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4b45ef76-d58f-4931-b973-16b04a206459",
        "parentId" : null,
        "authorId" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "body" : "Making this optional allows us to reuse the same plan before we construct a write and after. Having `None` here means the logical write hasn't been constructed yet. This allows us to have idempotent rules in the optimizer. ",
        "createdAt" : "2020-12-16T15:34:38Z",
        "updatedAt" : "2020-12-21T15:35:33Z",
        "lastEditedBy" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "tags" : [
        ]
      }
    ],
    "commit" : "882e3210004c0e27aca914b58819008dafdde92a",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +98,102 @@    writeOptions: Map[String, String],\n    isByName: Boolean,\n    write: Option[Write] = None) extends V2WriteCommand {\n  override lazy val resolved: Boolean = {\n    table.resolved && query.resolved && outputResolved && deleteExpr.resolved"
  },
  {
    "id" : "9d9bb409-484a-475a-8078-087f2f982661",
    "prId" : 30806,
    "prUrl" : "https://github.com/apache/spark/pull/30806#pullrequestreview-556470186",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5fad4910-7f7c-4083-982b-1c1bfb298216",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Shall we add `override lazy val resolved = ... && write.isDefined` in `V2WriteCommand`? It's safer to make sure that the analyzer creates the `Write` object.",
        "createdAt" : "2020-12-21T14:31:18Z",
        "updatedAt" : "2020-12-21T15:35:33Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "b21327cf-a7af-482d-bf94-06171f525193",
        "parentId" : "5fad4910-7f7c-4083-982b-1c1bfb298216",
        "authorId" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "body" : "Sounds like a good idea but we actually construct the `Write` object in the optimizer after the operator optimization is done to ensure we operate on optimal expressions.",
        "createdAt" : "2020-12-21T15:24:48Z",
        "updatedAt" : "2020-12-21T15:35:33Z",
        "lastEditedBy" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "tags" : [
        ]
      },
      {
        "id" : "e6c6c01e-0113-4042-8a97-f558f48c9297",
        "parentId" : "5fad4910-7f7c-4083-982b-1c1bfb298216",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "ah I see, let's leave it then.",
        "createdAt" : "2020-12-21T15:39:37Z",
        "updatedAt" : "2020-12-21T15:39:37Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "882e3210004c0e27aca914b58819008dafdde92a",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +70,74 @@    write: Option[Write] = None) extends V2WriteCommand {\n  override def withNewQuery(newQuery: LogicalPlan): AppendData = copy(query = newQuery)\n  override def withNewTable(newTable: NamedRelation): AppendData = copy(table = newTable)\n}\n"
  },
  {
    "id" : "903b85a1-190f-447b-9842-d30f8c90db38",
    "prId" : 30806,
    "prUrl" : "https://github.com/apache/spark/pull/30806#pullrequestreview-562771490",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4e477203-583c-4653-adfc-0333a1cd4642",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Not related to this PR. I'm thinking that if we should have an optional `Scan` object in `DataSourceV2Relation`, instead of having a new logical plan `DataSourceV2ScanRelation`. It's simpler and consistent with the write logical plans. cc  @rdblue ",
        "createdAt" : "2020-12-21T15:42:17Z",
        "updatedAt" : "2020-12-21T15:42:17Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "3a1bd56f-3ec4-4cdb-8b30-fe0c419275ae",
        "parentId" : "4e477203-583c-4653-adfc-0333a1cd4642",
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "I think that's a good idea.",
        "createdAt" : "2020-12-21T18:05:05Z",
        "updatedAt" : "2020-12-21T18:05:05Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      },
      {
        "id" : "96e05524-ce4c-464a-8a2b-c79d252a633b",
        "parentId" : "4e477203-583c-4653-adfc-0333a1cd4642",
        "authorId" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "body" : "Quick question: `DataSourceV2Relation` is also used inside write nodes like `AppendData`. If we add an optional scan, will that mean we will leak a read-specific concept into write plans?\r\n\r\ncc @rdblue @cloud-fan ",
        "createdAt" : "2020-12-22T09:22:59Z",
        "updatedAt" : "2020-12-22T09:23:27Z",
        "lastEditedBy" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "tags" : [
        ]
      },
      {
        "id" : "ba195a01-a0ba-49a4-9b8c-51638bc6d8a2",
        "parentId" : "4e477203-583c-4653-adfc-0333a1cd4642",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "For `AppendData`, we intentionally do not treat the `table` as a child, which means the pushdown rule won't apply for it and the `Scan` object will always be None in `AppendData`.",
        "createdAt" : "2021-01-06T14:54:37Z",
        "updatedAt" : "2021-01-06T14:54:38Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "882e3210004c0e27aca914b58819008dafdde92a",
    "line" : 34,
    "diffHunk" : "@@ -1,1 +137,141 @@    writeOptions: Map[String, String],\n    isByName: Boolean,\n    write: Option[Write] = None) extends V2WriteCommand {\n  override def withNewQuery(newQuery: LogicalPlan): OverwritePartitionsDynamic = {\n    copy(query = newQuery)"
  },
  {
    "id" : "8704e81e-ee41-4529-a822-a9700318ea15",
    "prId" : 30598,
    "prUrl" : "https://github.com/apache/spark/pull/30598#pullrequestreview-545837669",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a09e4cd4-abfb-4e3d-971e-fbbb276b3bb3",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Can we separate `CacheTable` and `CacheTableAsSelect`? I think it still stands that `CacheTableAsSelect.table` should be a simple string.",
        "createdAt" : "2020-12-07T05:21:37Z",
        "updatedAt" : "2020-12-10T05:25:00Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "7baa81d3e0b22cb853c07b67511b42301f06a6f1",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +766,770 @@ */\ncase class CacheTable(\n    table: LogicalPlan,\n    multipartIdentifier: Seq[String],\n    isLazy: Boolean,"
  },
  {
    "id" : "3e399d52-9095-4cdd-8ee5-c908633a6021",
    "prId" : 30033,
    "prUrl" : "https://github.com/apache/spark/pull/30033#pullrequestreview-508467354",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1997df85-4274-4ba9-af0d-85bc5d64050d",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Hi, @HeartSaVioR . The original code looks like the same with `branch-2.4` but the issue is reported at 3.0.0+. Could you confirm that this is 3.0.0-only issue or not?\r\n```scala\r\n  override lazy val resolved: Boolean = {\r\n    table.resolved && query.resolved && query.output.size == table.output.size &&\r\n        query.output.zip(table.output).forall {\r\n          case (inAttr, outAttr) =>\r\n            // names and types must match, nullability must be compatible\r\n            inAttr.name == outAttr.name &&\r\n                DataType.equalsIgnoreCompatibleNullability(outAttr.dataType, inAttr.dataType) &&\r\n                (outAttr.nullable || !inAttr.nullable)\r\n        }\r\n  }\r\n```",
        "createdAt" : "2020-10-14T02:37:38Z",
        "updatedAt" : "2020-10-14T02:37:49Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "ec636bef-6c7f-409b-bb9e-a143bc916174",
        "parentId" : "1997df85-4274-4ba9-af0d-85bc5d64050d",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Ah thanks for noticing. Nice finding. I found it from V2WriteCommand so thought it was added later. Will check the code path in branch-2.4 and test it.",
        "createdAt" : "2020-10-14T03:04:13Z",
        "updatedAt" : "2020-10-14T03:04:14Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "6bfff95f-311b-4c0f-b0b1-25193daf7ba4",
        "parentId" : "1997df85-4274-4ba9-af0d-85bc5d64050d",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "The usage of AppendData is reverted in https://github.com/apache/spark/commit/b6e4aca0be7f3b863c326063a3c02aa8a1c266a3 for branch-2.4 and shipped to Spark 2.4.0. So while the code in AppendData for branch-2.4 is broken as well, it's a dead code.\r\n\r\nWe seem to have three options: 1) revert remaining part of AppendData in branch-2.4 2) fix the code but leave it as dead 3) leave it as it is. What's our preference?\r\n\r\ncc. @cloud-fan @HyukjinKwon ",
        "createdAt" : "2020-10-14T05:04:58Z",
        "updatedAt" : "2020-10-14T05:05:30Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "2f86c149-82d1-497a-b7d0-a19379fc9511",
        "parentId" : "1997df85-4274-4ba9-af0d-85bc5d64050d",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "I prefer (2) because there is a downstream using it in their fork.",
        "createdAt" : "2020-10-14T15:25:23Z",
        "updatedAt" : "2020-10-14T15:25:23Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "89d329c11baa61a81a4361d3e870e1e2c0a67203",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +46,50 @@            // names and types must match, nullability must be compatible\n            inAttr.name == outAttr.name &&\n              DataType.equalsIgnoreCompatibleNullability(inAttr.dataType, outAttr.dataType) &&\n              (outAttr.nullable || !inAttr.nullable)\n        }"
  },
  {
    "id" : "a1d49081-10f2-4b04-9577-88dfa80746c4",
    "prId" : 29880,
    "prUrl" : "https://github.com/apache/spark/pull/29880#pullrequestreview-506328587",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4a8135f4-95e0-4df9-a150-093d77ada207",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "is it possible that the column name is qualified like `catalog_name.ns1.tbl1.col`? Does Hive support qualified column names in `DESCRIBE`?",
        "createdAt" : "2020-09-29T13:32:02Z",
        "updatedAt" : "2020-10-06T19:46:53Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "467581ea-68a9-41c9-b9eb-825fc425e1c8",
        "parentId" : "4a8135f4-95e0-4df9-a150-093d77ada207",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "A simple idea is to put an `UnresolvedAttribute` here, and analyzer can do the work for us.",
        "createdAt" : "2020-09-29T13:55:34Z",
        "updatedAt" : "2020-10-06T19:46:53Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "b3c4ef05-8914-4083-bb9c-19bf2f7cdc2b",
        "parentId" : "4a8135f4-95e0-4df9-a150-093d77ada207",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "> Does Hive support qualified column names in `DESCRIBE`?\r\n\r\nHive [doesn't support qualified name](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-Describe) for column names. For example:\r\n\r\n```\r\nDESCRIBE hivesampletable default.hivesampletable.clientid\r\n\r\njava.sql.SQLException: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. cannot find field default from [0:clientid, 1:querytime, 2:market, 3:deviceplatform, 4:devicemake, 5:devicemodel, 6:state, 7:country, 8:querydwelltime, 9:sessionid, 10:sessionpagevieworder]\r\n```\r\n\r\n\r\n",
        "createdAt" : "2020-10-02T21:00:44Z",
        "updatedAt" : "2020-10-06T19:46:53Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      },
      {
        "id" : "7ca66d1b-264d-4499-8a68-c0728c8fd5a4",
        "parentId" : "4a8135f4-95e0-4df9-a150-093d77ada207",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "> A simple idea is to put an `UnresolvedAttribute` here, and analyzer can do the work for us.\r\n\r\nSince we need to have the relation resolved first, we need to match like the following in the analyzer:\r\n```scala\r\ncase DescribeColumn(r: ResolvedTable, u: UnresolvedAttribute, _) =>\r\n...\r\n```\r\nIs that what you had in mind?",
        "createdAt" : "2020-10-02T21:50:41Z",
        "updatedAt" : "2020-10-06T19:46:53Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      },
      {
        "id" : "5ac68fbf-79e3-4478-ba4c-725fbd86f65c",
        "parentId" : "4a8135f4-95e0-4df9-a150-093d77ada207",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "If Hive doesn't support it, it's OK to follow Hive here. Let's make sure that other commands only allow column name without qualifies too.",
        "createdAt" : "2020-10-06T07:44:04Z",
        "updatedAt" : "2020-10-06T19:46:53Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "0ec2e2d3-3867-425c-945a-4fd5c84fd582",
        "parentId" : "4a8135f4-95e0-4df9-a150-093d77ada207",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "Sure, will take a look.",
        "createdAt" : "2020-10-06T19:43:10Z",
        "updatedAt" : "2020-10-06T19:46:53Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      },
      {
        "id" : "269cbc97-d381-4807-acf0-1218c7ea2248",
        "parentId" : "4a8135f4-95e0-4df9-a150-093d77ada207",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "> is it possible that the column name is qualified like `catalog_name.ns1.tbl1.col`? Does Hive support qualified column names in `DESCRIBE`?\r\n\r\n@cloud-fan actually I didn't respond to your first question. Yes, Spark allows qualifying the column name (same in 2.4):\r\n```\r\nscala> sql(\"CREATE TABLE t (id bigint) USING csv\")\r\nscala> sql(\"describe t spark_catalog.default.t.id\").show\r\n+---------+----------+\r\n|info_name|info_value|\r\n+---------+----------+\r\n| col_name|        id|\r\n|data_type|    bigint|\r\n|  comment|      NULL|\r\n+---------+----------+\r\n```\r\n, whereas Hive does not support. I guess we cannot change the behavior at this point?\r\n",
        "createdAt" : "2020-10-08T21:11:06Z",
        "updatedAt" : "2020-10-08T21:11:06Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      },
      {
        "id" : "7d75ddf3-ce6f-4ee4-bed7-db4b9407c4b2",
        "parentId" : "4a8135f4-95e0-4df9-a150-093d77ada207",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Yea we can't break it. When we add v2 `DecribeColumExec` later, we may want to change `Seq[String]` to `Attribute` here, so that we can unify the column resolution logic for v1 and  v2 commands.",
        "createdAt" : "2020-10-12T06:39:02Z",
        "updatedAt" : "2020-10-12T06:39:03Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "97a4deb99439fb6f5903c44d3ca6b74bae1c2f78",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +321,325 @@case class DescribeColumn(\n    relation: LogicalPlan,\n    colNameParts: Seq[String],\n    isExtended: Boolean) extends Command {\n  override def children: Seq[LogicalPlan] = Seq(relation)"
  },
  {
    "id" : "d38fbbd4-b770-460b-9ae9-eea86af67824",
    "prId" : 29339,
    "prUrl" : "https://github.com/apache/spark/pull/29339#pullrequestreview-526791897",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "149e1864-fed5-409b-a0b7-9faf23e553b9",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "let's override `lazy val resolved` to make sure `parts` are all resolved.",
        "createdAt" : "2020-11-09T16:19:57Z",
        "updatedAt" : "2020-11-11T04:13:23Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "a6e207fd-2a2d-4ca8-9211-dd7d185a9fe6",
        "parentId" : "149e1864-fed5-409b-a0b7-9faf23e553b9",
        "authorId" : "0f4ef4e8-09be-436f-b743-bf8fbc343490",
        "body" : "good",
        "createdAt" : "2020-11-10T01:54:13Z",
        "updatedAt" : "2020-11-11T04:13:23Z",
        "lastEditedBy" : "0f4ef4e8-09be-436f-b743-bf8fbc343490",
        "tags" : [
        ]
      }
    ],
    "commit" : "d316e56d70ccf35fede0d7862aae2fed9ece18cd",
    "line" : 29,
    "diffHunk" : "@@ -1,1 +628,632 @@    childrenResolved && parts.forall(_.isInstanceOf[ResolvedPartitionSpec])\n\n  override def children: Seq[LogicalPlan] = child :: Nil\n}\n"
  },
  {
    "id" : "db665504-4920-4859-9c84-f0ac9e47b555",
    "prId" : 29339,
    "prUrl" : "https://github.com/apache/spark/pull/29339#pullrequestreview-526791897",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3eb0c827-ee1a-4c0a-830b-ea05a3a25ced",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "ditto",
        "createdAt" : "2020-11-09T16:20:06Z",
        "updatedAt" : "2020-11-11T04:13:23Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "4075069c-429d-4624-ac7e-2682f58fbfc3",
        "parentId" : "3eb0c827-ee1a-4c0a-830b-ea05a3a25ced",
        "authorId" : "0f4ef4e8-09be-436f-b743-bf8fbc343490",
        "body" : "done",
        "createdAt" : "2020-11-10T01:57:53Z",
        "updatedAt" : "2020-11-11T04:13:23Z",
        "lastEditedBy" : "0f4ef4e8-09be-436f-b743-bf8fbc343490",
        "tags" : [
        ]
      }
    ],
    "commit" : "d316e56d70ccf35fede0d7862aae2fed9ece18cd",
    "line" : 43,
    "diffHunk" : "@@ -1,1 +642,646 @@case class AlterTableDropPartition(\n    child: LogicalPlan,\n    parts: Seq[PartitionSpec],\n    ifExists: Boolean,\n    purge: Boolean,"
  },
  {
    "id" : "42172a98-7ae8-4ea6-9aae-e84f6de79cf7",
    "prId" : 27187,
    "prUrl" : "https://github.com/apache/spark/pull/27187#pullrequestreview-341676241",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d4a63700-ff8c-4007-aa97-0e25567377a9",
        "parentId" : null,
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "this `def` is needed for all commands resolved by the new framework, we may create a trait to reduced redundancy",
        "createdAt" : "2020-01-13T08:01:11Z",
        "updatedAt" : "2020-01-15T04:45:49Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "562232270f24c021c962015b4314dac38e40906b",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +322,326 @@    partitionSpec: TablePartitionSpec,\n    isExtended: Boolean) extends Command {\n  override def children: Seq[LogicalPlan] = Seq(relation)\n  override def output: Seq[Attribute] = DescribeTableSchema.describeTableAttributes()\n}"
  },
  {
    "id" : "3d158761-ffd7-4ed8-9295-404c4856c04f",
    "prId" : 26740,
    "prUrl" : "https://github.com/apache/spark/pull/26740#pullrequestreview-328645945",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b8932e30-bad3-4fe3-90ff-30b064688db5",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "indentation is off - please read through style guide.",
        "createdAt" : "2019-12-02T23:53:42Z",
        "updatedAt" : "2019-12-03T00:09:10Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "b0cdac4f-08ae-43a1-81cf-7f705044c3f9",
        "parentId" : "b8932e30-bad3-4fe3-90ff-30b064688db5",
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "I agree. These should be indented 4 spaces from where `case` started.",
        "createdAt" : "2019-12-09T00:31:57Z",
        "updatedAt" : "2019-12-09T00:31:57Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      }
    ],
    "commit" : "cab9c673099d9043cb0d39565440f9263a386231",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +305,309 @@ */\ncase class DeleteFromTable(\n                            table: LogicalPlan,\n                            condition: Option[Expression]) extends Command with SupportsSubquery {\n  override def children: Seq[LogicalPlan] = table :: Nil"
  },
  {
    "id" : "1e5e7244-9d77-4247-8cfa-5d8f4e9413ca",
    "prId" : 26740,
    "prUrl" : "https://github.com/apache/spark/pull/26740#pullrequestreview-325820447",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3e0f014a-e647-44ac-bf53-796587946176",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "It's just copy and paste of DeleteFromTable which is incorrect. Please fix it.",
        "createdAt" : "2019-12-02T23:54:10Z",
        "updatedAt" : "2019-12-03T00:09:10Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "cab9c673099d9043cb0d39565440f9263a386231",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +311,315 @@\n/**\n * The logical plan of the DELETE FROM command that works for v2 tables.\n */\ncase class VacuumTable(table: LogicalPlan) extends Command with SupportsSubquery {"
  },
  {
    "id" : "cde66c93-57a7-4d34-8ac6-36df463b3527",
    "prId" : 26464,
    "prUrl" : "https://github.com/apache/spark/pull/26464#pullrequestreview-314773348",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c79dfa86-b536-4180-a6e8-ce5a0d86a50b",
        "parentId" : null,
        "authorId" : "8afcf008-6988-44a5-92fa-2bef54bb4058",
        "body" : "Here we change the update set columns and values to `Seq[Assignment]`, to keep align with the `UpdateAction` in `MergeIntoTable`.",
        "createdAt" : "2019-11-11T09:16:14Z",
        "updatedAt" : "2019-11-13T08:22:00Z",
        "lastEditedBy" : "8afcf008-6988-44a5-92fa-2bef54bb4058",
        "tags" : [
        ]
      }
    ],
    "commit" : "567bb7c78e6fcda3965d0fe9bcf60609cd96a589",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +291,295 @@case class UpdateTable(\n    table: LogicalPlan,\n    assignments: Seq[Assignment],\n    condition: Option[Expression]) extends Command with SupportsSubquery {\n  override def children: Seq[LogicalPlan] = table :: Nil"
  },
  {
    "id" : "1c6b7748-a599-46cc-982a-7cbd348ba170",
    "prId" : 26219,
    "prUrl" : "https://github.com/apache/spark/pull/26219#pullrequestreview-315661931",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5a2df61c-11be-4e3d-afb1-aee5183b18ae",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "for the source table, what we really care is the table itself, not which catalog it comes from. I think it's better to define the plan as\r\n```\r\ncase class CreateTableLike(\r\n    targetCatalog: TableCatalog,\r\n    targetTableName: Seq[String],\r\n    sourceTable: NamedRelation,\r\n    location: Option[String],\r\n    provider: Option[String],\r\n    ifNotExists: Boolean)\r\n```\r\n\r\nIn the planner, we match `CreateTableLike(..., r: DataSourceV2Relation, ..)`, and create the physical plan with source table `r.table`",
        "createdAt" : "2019-11-12T07:09:02Z",
        "updatedAt" : "2019-11-12T07:09:39Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "ffa02818-e092-4d85-8f14-fc1008ef13fa",
        "parentId" : "5a2df61c-11be-4e3d-afb1-aee5183b18ae",
        "authorId" : "a2fcc15e-a51a-42f0-87a6-137048a28e30",
        "body" : "@cloud-fan I am a bit confused. The source can be both V1 or V2, right ? So how can we expect a DataSourceV2Relation as source ?",
        "createdAt" : "2019-11-12T08:27:47Z",
        "updatedAt" : "2019-11-12T08:28:03Z",
        "lastEditedBy" : "a2fcc15e-a51a-42f0-87a6-137048a28e30",
        "tags" : [
        ]
      },
      {
        "id" : "be3cba69-20a8-4579-b3dc-06650742d9ff",
        "parentId" : "5a2df61c-11be-4e3d-afb1-aee5183b18ae",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "the v1 table has a v2 adapter called `V1Table`. In `ResolveCatalogs`, we can lookup table from session catalog, create a `DataSourceV2Relation`, and pass it into `CreateTableLike`.",
        "createdAt" : "2019-11-12T16:20:08Z",
        "updatedAt" : "2019-11-12T16:20:08Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "11ea11b80898f006ffe314f626b046a524a15ff6",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +195,199 @@    targetTableName: Seq[String],\n    sourceCatalog: Option[TableCatalog],\n    sourceTableName: Seq[String],\n    ifNotExists: Boolean) extends Command\n"
  }
]