[
  {
    "id" : "2f20aef2-23a7-475c-be80-7012705daa34",
    "prId" : 33468,
    "prUrl" : "https://github.com/apache/spark/pull/33468#pullrequestreview-712493520",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "960f979b-d592-42ab-a1a3-c83842c55926",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "fixed a bug here: we should not always exclude the first element, as `condition` can be None",
        "createdAt" : "2021-07-21T18:14:15Z",
        "updatedAt" : "2021-07-21T18:14:15Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "8c2bccb7-8e80-4de1-8566-b33c1a15d191",
        "parentId" : "960f979b-d592-42ab-a1a3-c83842c55926",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "cc @sigmod ",
        "createdAt" : "2021-07-21T18:14:23Z",
        "updatedAt" : "2021-07-21T18:14:23Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "50e9f78c-1dc1-4317-8057-7cedf755624b",
        "parentId" : "960f979b-d592-42ab-a1a3-c83842c55926",
        "authorId" : "8b518862-583d-43a2-a10e-b33ef1b6d824",
        "body" : "Thanks for the fix, @cloud-fan!\r\n\r\ncc @dbaliafroozeh ",
        "createdAt" : "2021-07-22T01:07:56Z",
        "updatedAt" : "2021-07-22T01:08:06Z",
        "lastEditedBy" : "8b518862-583d-43a2-a10e-b33ef1b6d824",
        "tags" : [
        ]
      },
      {
        "id" : "79d13158-2b9f-472a-873b-ee75117bb614",
        "parentId" : "960f979b-d592-42ab-a1a3-c83842c55926",
        "authorId" : "def2dd9f-ce27-41ce-8156-f8225ddc68a9",
        "body" : "Thanks for the fix @cloud-fan!",
        "createdAt" : "2021-07-22T08:39:30Z",
        "updatedAt" : "2021-07-22T08:39:30Z",
        "lastEditedBy" : "def2dd9f-ce27-41ce-8156-f8225ddc68a9",
        "tags" : [
        ]
      }
    ],
    "commit" : "984b511adb2ca5b60f6acf1a84e1bbacc6effc30",
    "line" : 32,
    "diffHunk" : "@@ -1,1 +480,484 @@    copy(\n      condition = if (condition.isDefined) Some(newChildren.head) else None,\n      assignments = newChildren.takeRight(assignments.length).asInstanceOf[Seq[Assignment]])\n}\n"
  },
  {
    "id" : "67ffceca-0612-4306-a625-4cbca9bb6d97",
    "prId" : 33200,
    "prUrl" : "https://github.com/apache/spark/pull/33200#pullrequestreview-713087207",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4a3ec02c-fcf5-441c-9a28-136f0858d280",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "do we need the `path` to be resolved as well?",
        "createdAt" : "2021-07-22T12:42:42Z",
        "updatedAt" : "2021-07-22T12:42:42Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "83ca01a6-ac2a-43a1-89a3-c00a55df2f68",
        "parentId" : "4a3ec02c-fcf5-441c-9a28-136f0858d280",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "Updated.",
        "createdAt" : "2021-07-22T18:00:18Z",
        "updatedAt" : "2021-07-22T18:04:42Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "d2e691086b80fb7f64d1bce144bd1bdd03428b8d",
    "line" : 78,
    "diffHunk" : "@@ -1,1 +1102,1106 @@      require(col.path.forall(_.resolved),\n        \"FieldName should be resolved before it's converted to TableChange.\")\n      require(col.position.forall(_.resolved),\n        \"FieldPosition should be resolved before it's converted to TableChange.\")\n      TableChange.addColumn("
  },
  {
    "id" : "87e3ef46-b9ec-4002-990d-19d822aa9615",
    "prId" : 33200,
    "prUrl" : "https://github.com/apache/spark/pull/33200#pullrequestreview-714597259",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6e53d572-b188-4924-9f55-3572878883c6",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "since `QualifiedColType` is not an expression, the default `QueryPlan.resolved` won't work here. Can we override `resolved`?",
        "createdAt" : "2021-07-26T08:50:39Z",
        "updatedAt" : "2021-07-26T08:50:40Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "d2e691086b80fb7f64d1bce144bd1bdd03428b8d",
    "line" : 67,
    "diffHunk" : "@@ -1,1 +1091,1095 @@case class AlterTableAddColumns(\n    table: LogicalPlan,\n    columnsToAdd: Seq[QualifiedColType]) extends AlterTableColumnCommand {\n  columnsToAdd.foreach { c =>\n    TypeUtils.failWithIntervalType(c.dataType)"
  },
  {
    "id" : "db4e97cf-d4c9-4e91-9db1-dda823455e7f",
    "prId" : 32058,
    "prUrl" : "https://github.com/apache/spark/pull/32058#pullrequestreview-629010403",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "439dd3c9-f9c7-4648-bbeb-71645fc5a9ac",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "shall we simply add a new implementation of `MergeAction`: `AssignStarAction`?",
        "createdAt" : "2021-04-06T06:52:36Z",
        "updatedAt" : "2021-04-06T06:52:36Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "b4154043-d741-41f8-b225-87d3748bd877",
        "parentId" : "439dd3c9-f9c7-4648-bbeb-71645fc5a9ac",
        "authorId" : "9cd657f0-37cd-41bd-908a-f2c996b95a7a",
        "body" : "in that case shouldnt it be `InsertStarAction(condition)` and `UpdateStarAction(condition)`?",
        "createdAt" : "2021-04-06T14:03:21Z",
        "updatedAt" : "2021-04-06T14:03:21Z",
        "lastEditedBy" : "9cd657f0-37cd-41bd-908a-f2c996b95a7a",
        "tags" : [
        ]
      }
    ],
    "commit" : "26b5104a9283b070c11a13990aab8f0ac6ca687a",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +429,433 @@    condition: Option[Expression],\n    assignments: Seq[Assignment],\n    assignStar: Boolean) extends MergeAction {\n  override def children: Seq[Expression] = condition.toSeq ++ assignments\n}"
  },
  {
    "id" : "51060d65-c2d8-4c2e-bc79-abad25055a12",
    "prId" : 32032,
    "prUrl" : "https://github.com/apache/spark/pull/32032#pullrequestreview-634593801",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dc61a31e-d15f-43d4-aa10-77887d4c7729",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "is it possible that `newChildren` is empty? Probably safer to add `if (isAnalyzed) ... else ...`",
        "createdAt" : "2021-04-12T08:41:25Z",
        "updatedAt" : "2021-04-14T03:19:24Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "05900f9b-5c20-42d8-8602-6857dd174967",
        "parentId" : "dc61a31e-d15f-43d4-aa10-77887d4c7729",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "This should be called only when there exist children. Maybe an assert is better `assert(!isAnalyzed)`? WDYT?\r\n\r\nhttps://github.com/apache/spark/blob/e40fce919ab77f5faeb0bbd34dc86c56c04adbaa/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/trees/TreeNode.scala#L345-L350",
        "createdAt" : "2021-04-12T15:04:56Z",
        "updatedAt" : "2021-04-14T03:19:24Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      },
      {
        "id" : "1d2302f2-c6f1-40c5-991d-8a81a90120b2",
        "parentId" : "dc61a31e-d15f-43d4-aa10-77887d4c7729",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "assert SGTM",
        "createdAt" : "2021-04-13T13:32:20Z",
        "updatedAt" : "2021-04-14T03:19:24Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "111ef8be8bc03a62389aed4f0871b958714eb789",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +1028,1032 @@      newChildren: IndexedSeq[LogicalPlan]): CacheTableAsSelect = {\n    assert(!isAnalyzed)\n    copy(plan = newChildren.head)\n  }\n"
  },
  {
    "id" : "cc14bcb1-9560-404f-9eae-a6f28f3920d1",
    "prId" : 31932,
    "prUrl" : "https://github.com/apache/spark/pull/31932#pullrequestreview-624637882",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "46877903-2326-4240-a8e2-991f906fc18c",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "2 space indent",
        "createdAt" : "2021-03-30T19:48:39Z",
        "updatedAt" : "2021-03-30T19:50:21Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "3c0e5077722d39b25680870ba9d435aafc62466c",
    "line" : 207,
    "diffHunk" : "@@ -1,1 +432,436 @@\ncase class Assignment(key: Expression, value: Expression) extends Expression\n    with Unevaluable with BinaryLike[Expression] {\n  override def nullable: Boolean = false\n  override def dataType: DataType = throw new UnresolvedException(\"nullable\")"
  },
  {
    "id" : "3ae8de31-a85e-4b20-a6da-8f7301276818",
    "prId" : 31737,
    "prUrl" : "https://github.com/apache/spark/pull/31737#pullrequestreview-604139755",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "45912d7f-b4fd-4972-bb34-aa243eeb271e",
        "parentId" : null,
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "When check the code found name not consistence. Just change in this pr or make a followup? @cloud-fan ",
        "createdAt" : "2021-03-04T13:42:46Z",
        "updatedAt" : "2021-03-05T02:16:02Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "2046f644-5a16-438c-a16f-6b4be2ec0294",
        "parentId" : "45912d7f-b4fd-4972-bb34-aa243eeb271e",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "It's very trivial. I'm OK to fix the typo here.",
        "createdAt" : "2021-03-04T14:10:48Z",
        "updatedAt" : "2021-03-05T02:16:02Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "375954c36029bf1c54d9eff24f8f535b483878b8",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +304,308 @@\nobject DescribeNamespace {\n  def getOutputAttrs: Seq[Attribute] = Seq(\n    AttributeReference(\"info_name\", StringType, nullable = false,\n      new MetadataBuilder().putString(\"comment\", \"name of the namespace info\").build())(),"
  },
  {
    "id" : "7a2f554e-c676-4715-98e4-757690489de1",
    "prId" : 31705,
    "prUrl" : "https://github.com/apache/spark/pull/31705#pullrequestreview-601615051",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "21404ee0-5055-4125-a231-a34eaef6c83c",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "the v2 schema is weird: `name: string, value: string`. Shall we follow DESC COLUMN and use `info_name: string, info_value: string`?\r\n\r\nv2 command is not stable yet and we can still change it.",
        "createdAt" : "2021-03-02T09:21:23Z",
        "updatedAt" : "2021-03-03T12:23:24Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "a4015240-36c3-4ce7-b08e-18521cc76e4f",
        "parentId" : "21404ee0-5055-4125-a231-a34eaef6c83c",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> the v2 schema is weird: `name: string, value: string`. Shall we follow DESC COLUMN and use `info_name: string, info_value: string`?\r\n> \r\n> v2 command is not stable yet and we can still change it.\r\n\r\nKeep same should be better. If we want to change in the future, we can change all of them together.",
        "createdAt" : "2021-03-02T09:30:22Z",
        "updatedAt" : "2021-03-03T12:23:24Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "0e7aff7a-e369-40f3-96ac-29432b1c605c",
        "parentId" : "21404ee0-5055-4125-a231-a34eaef6c83c",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "Updated, how about current?",
        "createdAt" : "2021-03-02T09:39:01Z",
        "updatedAt" : "2021-03-03T12:23:24Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "72345b2351cd447c61b93cc186952e6ae526fdfd",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +304,308 @@\nobject DescribeNamespace {\n  def getOutputAttr: Seq[Attribute] = Seq(\n    AttributeReference(\"info_name\", StringType, nullable = false,\n      new MetadataBuilder().putString(\"comment\", \"name of the namespace info\").build())(),"
  },
  {
    "id" : "5e081c14-629a-4cf8-baa8-96c5d19df544",
    "prId" : 31700,
    "prUrl" : "https://github.com/apache/spark/pull/31700#pullrequestreview-604779608",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6837369b-f15a-4a65-a8bc-c013ee7ff5ea",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I don't think we have finalized the streaming writing semantic yet. Ideally, it should be similar to batch (upsert semantic), but we are not at that point yet.\r\n\r\nI think we should standardize the streaming writing query plan later. For now, let's just use one query plan with `OutputMode` as the parameter.",
        "createdAt" : "2021-03-04T13:04:27Z",
        "updatedAt" : "2021-03-04T13:04:27Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "78dae22e-3343-4f04-a6f6-79d9ff325f47",
        "parentId" : "6837369b-f15a-4a65-a8bc-c013ee7ff5ea",
        "authorId" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "body" : "Oh, I thought only update mode was under discussion. Are overwrite and append modes under discussion too?\r\n\r\nI saw @HeartSaVioR's PR to rename `SupportsStreamingUpdateAsAppend`. Is there a discussion I can take a look at?",
        "createdAt" : "2021-03-04T20:43:47Z",
        "updatedAt" : "2021-03-04T20:57:17Z",
        "lastEditedBy" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "tags" : [
        ]
      },
      {
        "id" : "0577f480-f65a-45db-acf1-cfd030eac94e",
        "parentId" : "6837369b-f15a-4a65-a8bc-c013ee7ff5ea",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Streaming write semantic is not same as batch one. The semantic is bound to the stateful operation; there should be only `append`, `update` (not same as overwrite), and `truncate and append (complete)`. For update we haven't constructed the proper way to define it.\r\n\r\nThe major concern is that the group keys in stateful operation must be used as keys in update mode. That is currently not possible (there are some sketched ideas on this though), but Spark has been dealing with update with the huge risk that we're doing the same as append, and the risk is delegated to the sink (or user). The sink or user has to deal with reflecting the appended output as \"upsert\". That's why I renamed `SupportsStreamingUpdate` as `SupportsStreamingUpdateAsAppend` to clarify the behavior.",
        "createdAt" : "2021-03-05T02:00:03Z",
        "updatedAt" : "2021-03-05T02:03:16Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "657e9f80-f722-4ba5-865b-8af1592e8113",
        "parentId" : "6837369b-f15a-4a65-a8bc-c013ee7ff5ea",
        "authorId" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "body" : "Thanks for the context, @HeartSaVioR!\r\n\r\nI think we have two ways to proceed:\r\n\r\nOption 1: Just adapt `WriteToMicroBatchDataSource ` to use the `Write` abstraction and handle it in `V2Writes`.\r\nOption 2: Define specific plans where we have clarity. For example, `append` and `complete` seem well-defined. We could define plans like `AppendStreamingData` and `TruncateAndAppendStreamingData` or anything like that and have something intermediate for `update`.\r\n\r\nI am fine either way but option 1 seems easier for this PR. The rest can be covered by SPARK-27484.\r\n\r\nTo start with, we should all agree this feature is useful for micro-batch streaming.",
        "createdAt" : "2021-03-05T03:40:03Z",
        "updatedAt" : "2021-03-05T03:40:03Z",
        "lastEditedBy" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "tags" : [
        ]
      }
    ],
    "commit" : "e7f167fb0103d7e62127421cf5ad2a594d5f4337",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +167,171 @@}\n\ncase class AppendMicroBatch(\n    table: SupportsWrite,\n    query: LogicalPlan,"
  },
  {
    "id" : "d0bcb3d1-fede-4914-9bcc-e799b24af90c",
    "prId" : 31637,
    "prUrl" : "https://github.com/apache/spark/pull/31637#pullrequestreview-598083200",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "acdedecc-13cf-4775-b455-c03fdfe082e4",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Maybe name it `requireExactMatchedPartitionSpec` or `shouldExactlyMatchPartitionSpec`? I don't feel strongly. I am good with leaving it as is too.",
        "createdAt" : "2021-02-25T01:33:41Z",
        "updatedAt" : "2021-02-26T06:59:55Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "a2c6575cf06660bee6245bc6a23da8713205c74c",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +62,66 @@trait V2PartitionCommand extends Command {\n  def table: LogicalPlan\n  def allowPartialPartitionSpec: Boolean = false\n}\n"
  },
  {
    "id" : "987c38db-63fe-4405-84c8-8950c872a957",
    "prId" : 31596,
    "prUrl" : "https://github.com/apache/spark/pull/31596#pullrequestreview-594998784",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f71bfd5b-de12-469a-b5a5-f4686b9405ca",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "The command incapsulates 2 commands actually: set table location and set partition location. Should we split it to `SetTableLocation` and `SetPartitionLocation` (separately in another PR).",
        "createdAt" : "2021-02-19T21:33:02Z",
        "updatedAt" : "2021-02-22T08:12:12Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "ea8aeafb-5bcd-4d41-ab68-8cec614ee20e",
        "parentId" : "f71bfd5b-de12-469a-b5a5-f4686b9405ca",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "SGTM",
        "createdAt" : "2021-02-22T04:15:16Z",
        "updatedAt" : "2021-02-22T08:12:12Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "f6ab9ab4c127723a24066c559e0389b0a48bf1f2",
    "line" : 86,
    "diffHunk" : "@@ -1,1 +877,881 @@ * The logical plan of the ALTER TABLE ... SET LOCATION command.\n */\ncase class SetTableLocation(\n    table: LogicalPlan,\n    partitionSpec: Option[TablePartitionSpec],"
  },
  {
    "id" : "b0246494-acba-4b63-843b-616edb8efdd4",
    "prId" : 31474,
    "prUrl" : "https://github.com/apache/spark/pull/31474#pullrequestreview-583450258",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b62cc569-84a4-4850-a406-ccb9c4b78734",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I think it's better to put the `output` field in the constructor so that it's more stable (copying the node will not regenerate the output attributes again). It also helps if we want to support self-join later.",
        "createdAt" : "2021-02-04T14:30:11Z",
        "updatedAt" : "2021-02-05T04:56:04Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "3042bd1a361594600835d7b5f3ffeaa1bba51bc6",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +327,331 @@    namespace: LogicalPlan,\n    pattern: Option[String],\n    override val output: Seq[Attribute] = ShowNamespaces.OUTPUT) extends Command {\n  override def children: Seq[LogicalPlan] = Seq(namespace)\n}"
  },
  {
    "id" : "3626b8c8-bf49-457a-b7b1-a948fcbaf9ec",
    "prId" : 30806,
    "prUrl" : "https://github.com/apache/spark/pull/30806#pullrequestreview-553807723",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4b45ef76-d58f-4931-b973-16b04a206459",
        "parentId" : null,
        "authorId" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "body" : "Making this optional allows us to reuse the same plan before we construct a write and after. Having `None` here means the logical write hasn't been constructed yet. This allows us to have idempotent rules in the optimizer. ",
        "createdAt" : "2020-12-16T15:34:38Z",
        "updatedAt" : "2020-12-21T15:35:33Z",
        "lastEditedBy" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "tags" : [
        ]
      }
    ],
    "commit" : "882e3210004c0e27aca914b58819008dafdde92a",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +98,102 @@    writeOptions: Map[String, String],\n    isByName: Boolean,\n    write: Option[Write] = None) extends V2WriteCommand {\n  override lazy val resolved: Boolean = {\n    table.resolved && query.resolved && outputResolved && deleteExpr.resolved"
  },
  {
    "id" : "9d9bb409-484a-475a-8078-087f2f982661",
    "prId" : 30806,
    "prUrl" : "https://github.com/apache/spark/pull/30806#pullrequestreview-556470186",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5fad4910-7f7c-4083-982b-1c1bfb298216",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Shall we add `override lazy val resolved = ... && write.isDefined` in `V2WriteCommand`? It's safer to make sure that the analyzer creates the `Write` object.",
        "createdAt" : "2020-12-21T14:31:18Z",
        "updatedAt" : "2020-12-21T15:35:33Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "b21327cf-a7af-482d-bf94-06171f525193",
        "parentId" : "5fad4910-7f7c-4083-982b-1c1bfb298216",
        "authorId" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "body" : "Sounds like a good idea but we actually construct the `Write` object in the optimizer after the operator optimization is done to ensure we operate on optimal expressions.",
        "createdAt" : "2020-12-21T15:24:48Z",
        "updatedAt" : "2020-12-21T15:35:33Z",
        "lastEditedBy" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "tags" : [
        ]
      },
      {
        "id" : "e6c6c01e-0113-4042-8a97-f558f48c9297",
        "parentId" : "5fad4910-7f7c-4083-982b-1c1bfb298216",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "ah I see, let's leave it then.",
        "createdAt" : "2020-12-21T15:39:37Z",
        "updatedAt" : "2020-12-21T15:39:37Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "882e3210004c0e27aca914b58819008dafdde92a",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +70,74 @@    write: Option[Write] = None) extends V2WriteCommand {\n  override def withNewQuery(newQuery: LogicalPlan): AppendData = copy(query = newQuery)\n  override def withNewTable(newTable: NamedRelation): AppendData = copy(table = newTable)\n}\n"
  },
  {
    "id" : "903b85a1-190f-447b-9842-d30f8c90db38",
    "prId" : 30806,
    "prUrl" : "https://github.com/apache/spark/pull/30806#pullrequestreview-562771490",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4e477203-583c-4653-adfc-0333a1cd4642",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Not related to this PR. I'm thinking that if we should have an optional `Scan` object in `DataSourceV2Relation`, instead of having a new logical plan `DataSourceV2ScanRelation`. It's simpler and consistent with the write logical plans. cc  @rdblue ",
        "createdAt" : "2020-12-21T15:42:17Z",
        "updatedAt" : "2020-12-21T15:42:17Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "3a1bd56f-3ec4-4cdb-8b30-fe0c419275ae",
        "parentId" : "4e477203-583c-4653-adfc-0333a1cd4642",
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "I think that's a good idea.",
        "createdAt" : "2020-12-21T18:05:05Z",
        "updatedAt" : "2020-12-21T18:05:05Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      },
      {
        "id" : "96e05524-ce4c-464a-8a2b-c79d252a633b",
        "parentId" : "4e477203-583c-4653-adfc-0333a1cd4642",
        "authorId" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "body" : "Quick question: `DataSourceV2Relation` is also used inside write nodes like `AppendData`. If we add an optional scan, will that mean we will leak a read-specific concept into write plans?\r\n\r\ncc @rdblue @cloud-fan ",
        "createdAt" : "2020-12-22T09:22:59Z",
        "updatedAt" : "2020-12-22T09:23:27Z",
        "lastEditedBy" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "tags" : [
        ]
      },
      {
        "id" : "ba195a01-a0ba-49a4-9b8c-51638bc6d8a2",
        "parentId" : "4e477203-583c-4653-adfc-0333a1cd4642",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "For `AppendData`, we intentionally do not treat the `table` as a child, which means the pushdown rule won't apply for it and the `Scan` object will always be None in `AppendData`.",
        "createdAt" : "2021-01-06T14:54:37Z",
        "updatedAt" : "2021-01-06T14:54:38Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "882e3210004c0e27aca914b58819008dafdde92a",
    "line" : 34,
    "diffHunk" : "@@ -1,1 +137,141 @@    writeOptions: Map[String, String],\n    isByName: Boolean,\n    write: Option[Write] = None) extends V2WriteCommand {\n  override def withNewQuery(newQuery: LogicalPlan): OverwritePartitionsDynamic = {\n    copy(query = newQuery)"
  }
]