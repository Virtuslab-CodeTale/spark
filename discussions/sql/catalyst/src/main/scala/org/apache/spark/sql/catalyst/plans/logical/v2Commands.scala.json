[
  {
    "id" : "2f20aef2-23a7-475c-be80-7012705daa34",
    "prId" : 33468,
    "prUrl" : "https://github.com/apache/spark/pull/33468#pullrequestreview-712493520",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "960f979b-d592-42ab-a1a3-c83842c55926",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "fixed a bug here: we should not always exclude the first element, as `condition` can be None",
        "createdAt" : "2021-07-21T18:14:15Z",
        "updatedAt" : "2021-07-21T18:14:15Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "8c2bccb7-8e80-4de1-8566-b33c1a15d191",
        "parentId" : "960f979b-d592-42ab-a1a3-c83842c55926",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "cc @sigmod ",
        "createdAt" : "2021-07-21T18:14:23Z",
        "updatedAt" : "2021-07-21T18:14:23Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "50e9f78c-1dc1-4317-8057-7cedf755624b",
        "parentId" : "960f979b-d592-42ab-a1a3-c83842c55926",
        "authorId" : "8b518862-583d-43a2-a10e-b33ef1b6d824",
        "body" : "Thanks for the fix, @cloud-fan!\r\n\r\ncc @dbaliafroozeh ",
        "createdAt" : "2021-07-22T01:07:56Z",
        "updatedAt" : "2021-07-22T01:08:06Z",
        "lastEditedBy" : "8b518862-583d-43a2-a10e-b33ef1b6d824",
        "tags" : [
        ]
      },
      {
        "id" : "79d13158-2b9f-472a-873b-ee75117bb614",
        "parentId" : "960f979b-d592-42ab-a1a3-c83842c55926",
        "authorId" : "def2dd9f-ce27-41ce-8156-f8225ddc68a9",
        "body" : "Thanks for the fix @cloud-fan!",
        "createdAt" : "2021-07-22T08:39:30Z",
        "updatedAt" : "2021-07-22T08:39:30Z",
        "lastEditedBy" : "def2dd9f-ce27-41ce-8156-f8225ddc68a9",
        "tags" : [
        ]
      }
    ],
    "commit" : "984b511adb2ca5b60f6acf1a84e1bbacc6effc30",
    "line" : 32,
    "diffHunk" : "@@ -1,1 +480,484 @@    copy(\n      condition = if (condition.isDefined) Some(newChildren.head) else None,\n      assignments = newChildren.takeRight(assignments.length).asInstanceOf[Seq[Assignment]])\n}\n"
  },
  {
    "id" : "67ffceca-0612-4306-a625-4cbca9bb6d97",
    "prId" : 33200,
    "prUrl" : "https://github.com/apache/spark/pull/33200#pullrequestreview-713087207",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4a3ec02c-fcf5-441c-9a28-136f0858d280",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "do we need the `path` to be resolved as well?",
        "createdAt" : "2021-07-22T12:42:42Z",
        "updatedAt" : "2021-07-22T12:42:42Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "83ca01a6-ac2a-43a1-89a3-c00a55df2f68",
        "parentId" : "4a3ec02c-fcf5-441c-9a28-136f0858d280",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "Updated.",
        "createdAt" : "2021-07-22T18:00:18Z",
        "updatedAt" : "2021-07-22T18:04:42Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "d2e691086b80fb7f64d1bce144bd1bdd03428b8d",
    "line" : 78,
    "diffHunk" : "@@ -1,1 +1102,1106 @@      require(col.path.forall(_.resolved),\n        \"FieldName should be resolved before it's converted to TableChange.\")\n      require(col.position.forall(_.resolved),\n        \"FieldPosition should be resolved before it's converted to TableChange.\")\n      TableChange.addColumn("
  },
  {
    "id" : "87e3ef46-b9ec-4002-990d-19d822aa9615",
    "prId" : 33200,
    "prUrl" : "https://github.com/apache/spark/pull/33200#pullrequestreview-714597259",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6e53d572-b188-4924-9f55-3572878883c6",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "since `QualifiedColType` is not an expression, the default `QueryPlan.resolved` won't work here. Can we override `resolved`?",
        "createdAt" : "2021-07-26T08:50:39Z",
        "updatedAt" : "2021-07-26T08:50:40Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "d2e691086b80fb7f64d1bce144bd1bdd03428b8d",
    "line" : 67,
    "diffHunk" : "@@ -1,1 +1091,1095 @@case class AlterTableAddColumns(\n    table: LogicalPlan,\n    columnsToAdd: Seq[QualifiedColType]) extends AlterTableColumnCommand {\n  columnsToAdd.foreach { c =>\n    TypeUtils.failWithIntervalType(c.dataType)"
  },
  {
    "id" : "db4e97cf-d4c9-4e91-9db1-dda823455e7f",
    "prId" : 32058,
    "prUrl" : "https://github.com/apache/spark/pull/32058#pullrequestreview-629010403",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "439dd3c9-f9c7-4648-bbeb-71645fc5a9ac",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "shall we simply add a new implementation of `MergeAction`: `AssignStarAction`?",
        "createdAt" : "2021-04-06T06:52:36Z",
        "updatedAt" : "2021-04-06T06:52:36Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "b4154043-d741-41f8-b225-87d3748bd877",
        "parentId" : "439dd3c9-f9c7-4648-bbeb-71645fc5a9ac",
        "authorId" : "9cd657f0-37cd-41bd-908a-f2c996b95a7a",
        "body" : "in that case shouldnt it be `InsertStarAction(condition)` and `UpdateStarAction(condition)`?",
        "createdAt" : "2021-04-06T14:03:21Z",
        "updatedAt" : "2021-04-06T14:03:21Z",
        "lastEditedBy" : "9cd657f0-37cd-41bd-908a-f2c996b95a7a",
        "tags" : [
        ]
      }
    ],
    "commit" : "26b5104a9283b070c11a13990aab8f0ac6ca687a",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +429,433 @@    condition: Option[Expression],\n    assignments: Seq[Assignment],\n    assignStar: Boolean) extends MergeAction {\n  override def children: Seq[Expression] = condition.toSeq ++ assignments\n}"
  },
  {
    "id" : "51060d65-c2d8-4c2e-bc79-abad25055a12",
    "prId" : 32032,
    "prUrl" : "https://github.com/apache/spark/pull/32032#pullrequestreview-634593801",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dc61a31e-d15f-43d4-aa10-77887d4c7729",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "is it possible that `newChildren` is empty? Probably safer to add `if (isAnalyzed) ... else ...`",
        "createdAt" : "2021-04-12T08:41:25Z",
        "updatedAt" : "2021-04-14T03:19:24Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "05900f9b-5c20-42d8-8602-6857dd174967",
        "parentId" : "dc61a31e-d15f-43d4-aa10-77887d4c7729",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "This should be called only when there exist children. Maybe an assert is better `assert(!isAnalyzed)`? WDYT?\r\n\r\nhttps://github.com/apache/spark/blob/e40fce919ab77f5faeb0bbd34dc86c56c04adbaa/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/trees/TreeNode.scala#L345-L350",
        "createdAt" : "2021-04-12T15:04:56Z",
        "updatedAt" : "2021-04-14T03:19:24Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      },
      {
        "id" : "1d2302f2-c6f1-40c5-991d-8a81a90120b2",
        "parentId" : "dc61a31e-d15f-43d4-aa10-77887d4c7729",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "assert SGTM",
        "createdAt" : "2021-04-13T13:32:20Z",
        "updatedAt" : "2021-04-14T03:19:24Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "111ef8be8bc03a62389aed4f0871b958714eb789",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +1028,1032 @@      newChildren: IndexedSeq[LogicalPlan]): CacheTableAsSelect = {\n    assert(!isAnalyzed)\n    copy(plan = newChildren.head)\n  }\n"
  },
  {
    "id" : "cc14bcb1-9560-404f-9eae-a6f28f3920d1",
    "prId" : 31932,
    "prUrl" : "https://github.com/apache/spark/pull/31932#pullrequestreview-624637882",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "46877903-2326-4240-a8e2-991f906fc18c",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "2 space indent",
        "createdAt" : "2021-03-30T19:48:39Z",
        "updatedAt" : "2021-03-30T19:50:21Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "3c0e5077722d39b25680870ba9d435aafc62466c",
    "line" : 207,
    "diffHunk" : "@@ -1,1 +432,436 @@\ncase class Assignment(key: Expression, value: Expression) extends Expression\n    with Unevaluable with BinaryLike[Expression] {\n  override def nullable: Boolean = false\n  override def dataType: DataType = throw new UnresolvedException(\"nullable\")"
  },
  {
    "id" : "3ae8de31-a85e-4b20-a6da-8f7301276818",
    "prId" : 31737,
    "prUrl" : "https://github.com/apache/spark/pull/31737#pullrequestreview-604139755",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "45912d7f-b4fd-4972-bb34-aa243eeb271e",
        "parentId" : null,
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "When check the code found name not consistence. Just change in this pr or make a followup? @cloud-fan ",
        "createdAt" : "2021-03-04T13:42:46Z",
        "updatedAt" : "2021-03-05T02:16:02Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "2046f644-5a16-438c-a16f-6b4be2ec0294",
        "parentId" : "45912d7f-b4fd-4972-bb34-aa243eeb271e",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "It's very trivial. I'm OK to fix the typo here.",
        "createdAt" : "2021-03-04T14:10:48Z",
        "updatedAt" : "2021-03-05T02:16:02Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "375954c36029bf1c54d9eff24f8f535b483878b8",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +304,308 @@\nobject DescribeNamespace {\n  def getOutputAttrs: Seq[Attribute] = Seq(\n    AttributeReference(\"info_name\", StringType, nullable = false,\n      new MetadataBuilder().putString(\"comment\", \"name of the namespace info\").build())(),"
  },
  {
    "id" : "7a2f554e-c676-4715-98e4-757690489de1",
    "prId" : 31705,
    "prUrl" : "https://github.com/apache/spark/pull/31705#pullrequestreview-601615051",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "21404ee0-5055-4125-a231-a34eaef6c83c",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "the v2 schema is weird: `name: string, value: string`. Shall we follow DESC COLUMN and use `info_name: string, info_value: string`?\r\n\r\nv2 command is not stable yet and we can still change it.",
        "createdAt" : "2021-03-02T09:21:23Z",
        "updatedAt" : "2021-03-03T12:23:24Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "a4015240-36c3-4ce7-b08e-18521cc76e4f",
        "parentId" : "21404ee0-5055-4125-a231-a34eaef6c83c",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> the v2 schema is weird: `name: string, value: string`. Shall we follow DESC COLUMN and use `info_name: string, info_value: string`?\r\n> \r\n> v2 command is not stable yet and we can still change it.\r\n\r\nKeep same should be better. If we want to change in the future, we can change all of them together.",
        "createdAt" : "2021-03-02T09:30:22Z",
        "updatedAt" : "2021-03-03T12:23:24Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "0e7aff7a-e369-40f3-96ac-29432b1c605c",
        "parentId" : "21404ee0-5055-4125-a231-a34eaef6c83c",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "Updated, how about current?",
        "createdAt" : "2021-03-02T09:39:01Z",
        "updatedAt" : "2021-03-03T12:23:24Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "72345b2351cd447c61b93cc186952e6ae526fdfd",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +304,308 @@\nobject DescribeNamespace {\n  def getOutputAttr: Seq[Attribute] = Seq(\n    AttributeReference(\"info_name\", StringType, nullable = false,\n      new MetadataBuilder().putString(\"comment\", \"name of the namespace info\").build())(),"
  },
  {
    "id" : "5e081c14-629a-4cf8-baa8-96c5d19df544",
    "prId" : 31700,
    "prUrl" : "https://github.com/apache/spark/pull/31700#pullrequestreview-604779608",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6837369b-f15a-4a65-a8bc-c013ee7ff5ea",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I don't think we have finalized the streaming writing semantic yet. Ideally, it should be similar to batch (upsert semantic), but we are not at that point yet.\r\n\r\nI think we should standardize the streaming writing query plan later. For now, let's just use one query plan with `OutputMode` as the parameter.",
        "createdAt" : "2021-03-04T13:04:27Z",
        "updatedAt" : "2021-03-04T13:04:27Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "78dae22e-3343-4f04-a6f6-79d9ff325f47",
        "parentId" : "6837369b-f15a-4a65-a8bc-c013ee7ff5ea",
        "authorId" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "body" : "Oh, I thought only update mode was under discussion. Are overwrite and append modes under discussion too?\r\n\r\nI saw @HeartSaVioR's PR to rename `SupportsStreamingUpdateAsAppend`. Is there a discussion I can take a look at?",
        "createdAt" : "2021-03-04T20:43:47Z",
        "updatedAt" : "2021-03-04T20:57:17Z",
        "lastEditedBy" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "tags" : [
        ]
      },
      {
        "id" : "0577f480-f65a-45db-acf1-cfd030eac94e",
        "parentId" : "6837369b-f15a-4a65-a8bc-c013ee7ff5ea",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Streaming write semantic is not same as batch one. The semantic is bound to the stateful operation; there should be only `append`, `update` (not same as overwrite), and `truncate and append (complete)`. For update we haven't constructed the proper way to define it.\r\n\r\nThe major concern is that the group keys in stateful operation must be used as keys in update mode. That is currently not possible (there are some sketched ideas on this though), but Spark has been dealing with update with the huge risk that we're doing the same as append, and the risk is delegated to the sink (or user). The sink or user has to deal with reflecting the appended output as \"upsert\". That's why I renamed `SupportsStreamingUpdate` as `SupportsStreamingUpdateAsAppend` to clarify the behavior.",
        "createdAt" : "2021-03-05T02:00:03Z",
        "updatedAt" : "2021-03-05T02:03:16Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "657e9f80-f722-4ba5-865b-8af1592e8113",
        "parentId" : "6837369b-f15a-4a65-a8bc-c013ee7ff5ea",
        "authorId" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "body" : "Thanks for the context, @HeartSaVioR!\r\n\r\nI think we have two ways to proceed:\r\n\r\nOption 1: Just adapt `WriteToMicroBatchDataSource ` to use the `Write` abstraction and handle it in `V2Writes`.\r\nOption 2: Define specific plans where we have clarity. For example, `append` and `complete` seem well-defined. We could define plans like `AppendStreamingData` and `TruncateAndAppendStreamingData` or anything like that and have something intermediate for `update`.\r\n\r\nI am fine either way but option 1 seems easier for this PR. The rest can be covered by SPARK-27484.\r\n\r\nTo start with, we should all agree this feature is useful for micro-batch streaming.",
        "createdAt" : "2021-03-05T03:40:03Z",
        "updatedAt" : "2021-03-05T03:40:03Z",
        "lastEditedBy" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "tags" : [
        ]
      }
    ],
    "commit" : "e7f167fb0103d7e62127421cf5ad2a594d5f4337",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +167,171 @@}\n\ncase class AppendMicroBatch(\n    table: SupportsWrite,\n    query: LogicalPlan,"
  }
]