[
  {
    "id" : "b2557a5c-84f5-4a19-9cba-0e0fd0cc19f0",
    "prId" : 30803,
    "prUrl" : "https://github.com/apache/spark/pull/30803#pullrequestreview-554542577",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7f8d5820-99b8-4102-b941-69f86926a38e",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Hm, I think you should use `DataFrame.crossJoin` explicitly. ",
        "createdAt" : "2020-12-17T02:16:06Z",
        "updatedAt" : "2020-12-23T17:12:37Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "1455af0a-92d5-414e-ae8f-ce4559a4cd25",
        "parentId" : "7f8d5820-99b8-4102-b941-69f86926a38e",
        "authorId" : "7d1c9b12-5f63-4118-b5b1-8f8f88df1f67",
        "body" : "Does that mean it's better to modify the documentation?\r\n\r\n[The join document](https://spark.apache.org/docs/3.0.1/api/python/pyspark.sql.html#pyspark.sql.DataFrame.join) says:\r\n```\r\nhow â€“ str, default inner. Must be one of: inner, cross, outer, full, fullouter, full_outer, left, leftouter, left_outer, right, rightouter, right_outer, semi, leftsemi, left_semi, anti, leftanti and left_anti.\r\n```\r\nIt says that cross is configurable.",
        "createdAt" : "2020-12-17T12:07:11Z",
        "updatedAt" : "2020-12-23T17:12:37Z",
        "lastEditedBy" : "7d1c9b12-5f63-4118-b5b1-8f8f88df1f67",
        "tags" : [
        ]
      }
    ],
    "commit" : "690ef81da1b06c189201782fe4cb1c332dd74287",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +103,107 @@\ncase class UsingJoin(tpe: JoinType, usingColumns: Seq[String]) extends JoinType {\n  require(Seq(Inner, LeftOuter, LeftSemi, RightOuter, FullOuter, LeftAnti, Cross).contains(tpe),\n    \"Unsupported using join type \" + tpe)\n  override def sql: String = \"USING \" + tpe.sql"
  }
]