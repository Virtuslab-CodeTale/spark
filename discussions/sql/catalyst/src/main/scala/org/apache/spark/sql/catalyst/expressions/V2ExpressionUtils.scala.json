[
  {
    "id" : "338570de-b00b-4e6f-81da-a8b4df898831",
    "prId" : 33096,
    "prUrl" : "https://github.com/apache/spark/pull/33096#pullrequestreview-693264108",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "eed6a122-c2d1-4676-a055-f3316bcf435e",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Do we need generic here? I think `resolve` just return `NamedExpression`?",
        "createdAt" : "2021-06-26T01:06:55Z",
        "updatedAt" : "2021-06-26T01:10:17Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "71ad48bc-8ff6-4ff1-95e2-41f0b1e89308",
        "parentId" : "eed6a122-c2d1-4676-a055-f3316bcf435e",
        "authorId" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "body" : "There are certain places where we need a bit more specific types. I could cast after calling but that does not seem better.\r\n\r\nFor example, I need `Attribute` [here](https://github.com/apache/spark/pull/32921/files#diff-9dc37f97148227618575e1c56f6177260412561e7b44ef93eb5d7acf7a0fee52R76). I also need `AttributeReference` [here](https://github.com/apache/spark/pull/33008/files#diff-c5574d47ec4d5764008276aab9acc836e4526d3a95c3fcbbf9c53c67b05538f8R110).",
        "createdAt" : "2021-06-26T01:17:43Z",
        "updatedAt" : "2021-06-26T01:17:43Z",
        "lastEditedBy" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "tags" : [
        ]
      },
      {
        "id" : "dc44901c-4093-4584-9251-603521ba1e08",
        "parentId" : "eed6a122-c2d1-4676-a055-f3316bcf435e",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Oh, I see. I cannot tell it from this change. Thanks.",
        "createdAt" : "2021-06-26T01:18:53Z",
        "updatedAt" : "2021-06-26T01:18:53Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "48131077-b5b8-494a-b164-d4efb6acd894",
        "parentId" : "eed6a122-c2d1-4676-a055-f3316bcf435e",
        "authorId" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "body" : "Yeah, totally!",
        "createdAt" : "2021-06-26T01:19:48Z",
        "updatedAt" : "2021-06-26T01:19:48Z",
        "lastEditedBy" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "tags" : [
        ]
      }
    ],
    "commit" : "3ac51da333f0400264081a3425e332646282c3a9",
    "line" : 33,
    "diffHunk" : "@@ -1,1 +31,35 @@  import org.apache.spark.sql.connector.catalog.CatalogV2Implicits.MultipartIdentifierHelper\n\n  def resolveRef[T <: NamedExpression](\n      ref: NamedReference,\n      plan: LogicalPlan,"
  },
  {
    "id" : "6729ba1b-cc0a-45a6-bfde-4384430d3e32",
    "prId" : 33096,
    "prUrl" : "https://github.com/apache/spark/pull/33096#pullrequestreview-694324518",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7b6a9798-b4d8-4d7e-997b-53ba54fbfbcf",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "We won't have to do `private[sql]` since catalyst module is:\r\n\r\n> All classes in catalyst are considered an internal API to Spark SQL\r\n\r\nThese were all removed from SPARK-16813",
        "createdAt" : "2021-06-28T00:55:06Z",
        "updatedAt" : "2021-06-28T00:55:06Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "4c289419-a93f-45e9-bedf-59d706fe6f31",
        "parentId" : "7b6a9798-b4d8-4d7e-997b-53ba54fbfbcf",
        "authorId" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "body" : "Makes sense. I'll update.",
        "createdAt" : "2021-06-28T19:35:26Z",
        "updatedAt" : "2021-06-28T19:35:27Z",
        "lastEditedBy" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "tags" : [
        ]
      }
    ],
    "commit" : "3ac51da333f0400264081a3425e332646282c3a9",
    "line" : 30,
    "diffHunk" : "@@ -1,1 +28,32 @@ * A utility class that converts public connector expressions into Catalyst expressions.\n */\nprivate[sql] object V2ExpressionUtils extends SQLConfHelper {\n  import org.apache.spark.sql.connector.catalog.CatalogV2Implicits.MultipartIdentifierHelper\n"
  },
  {
    "id" : "7572cad1-3215-4a5a-9241-4228795f3510",
    "prId" : 33096,
    "prUrl" : "https://github.com/apache/spark/pull/33096#pullrequestreview-694326890",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e8d59f19-5116-4daf-87b7-e7f01021b932",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "What is this for? Seems like it's not used in the current code base.",
        "createdAt" : "2021-06-28T00:55:57Z",
        "updatedAt" : "2021-06-28T00:55:57Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "3af7b6ef-a56a-4e4d-938f-a447d1b04e30",
        "parentId" : "e8d59f19-5116-4daf-87b7-e7f01021b932",
        "authorId" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "body" : "It will be used in #33008 and #32921. This PR has a subset of changes from #32921.",
        "createdAt" : "2021-06-28T19:38:35Z",
        "updatedAt" : "2021-06-28T19:38:35Z",
        "lastEditedBy" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "tags" : [
        ]
      }
    ],
    "commit" : "3ac51da333f0400264081a3425e332646282c3a9",
    "line" : 47,
    "diffHunk" : "@@ -1,1 +45,49 @@  }\n\n  def resolveRefs[T <: NamedExpression](\n      refs: Seq[NamedReference],\n      plan: LogicalPlan,"
  },
  {
    "id" : "c75029bd-4655-47fd-9739-ca0bc7bad3a0",
    "prId" : 33096,
    "prUrl" : "https://github.com/apache/spark/pull/33096#pullrequestreview-694527637",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "abf70a16-6295-4f19-b2c2-1df8743ba1e2",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "It's interesting that the complier doesn't complain about the default match missing.",
        "createdAt" : "2021-06-28T00:59:34Z",
        "updatedAt" : "2021-06-28T00:59:34Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "8e6bf732-8fd8-4eed-865a-b32cd0bcdadc",
        "parentId" : "abf70a16-6295-4f19-b2c2-1df8743ba1e2",
        "authorId" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "body" : "It is probably because the match statement is exhaustive as `V2SortDirection` is an enum and we check for both possible values here.\r\n\r\nJust copied the logic as it was.",
        "createdAt" : "2021-06-28T19:40:51Z",
        "updatedAt" : "2021-06-28T19:40:51Z",
        "lastEditedBy" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "tags" : [
        ]
      },
      {
        "id" : "274bffd6-4850-410c-bc8d-0a20e7e19878",
        "parentId" : "abf70a16-6295-4f19-b2c2-1df8743ba1e2",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "ðŸ‘Œ ",
        "createdAt" : "2021-06-29T02:02:58Z",
        "updatedAt" : "2021-06-29T02:02:58Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "3ac51da333f0400264081a3425e332646282c3a9",
    "line" : 73,
    "diffHunk" : "@@ -1,1 +71,75 @@  private def toCatalyst(direction: V2SortDirection): SortDirection = direction match {\n    case V2SortDirection.ASCENDING => Ascending\n    case V2SortDirection.DESCENDING => Descending\n  }\n"
  },
  {
    "id" : "61dedff8-ee40-4a18-8ceb-571efa1555fc",
    "prId" : 33096,
    "prUrl" : "https://github.com/apache/spark/pull/33096#pullrequestreview-694335024",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "492c8a3d-ee1a-49c1-adb6-c13b00c5831f",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Should we keep this parameter? Looks like we can just directly call `conf.resolver` inside the function.",
        "createdAt" : "2021-06-28T01:01:16Z",
        "updatedAt" : "2021-06-28T01:01:16Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "2167e264-6d2b-4664-aef7-f2ea419d72aa",
        "parentId" : "492c8a3d-ee1a-49c1-adb6-c13b00c5831f",
        "authorId" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "body" : "I was not sure whether there is a use case where we would want to pass a resolver explicitly. It seems Spark uses an explicit resolver in quite a few places (e.g. `PartitionPruning` uses the resolver from `HadoopFsRelation`) so it felt safer to expose an extra arg.\r\n\r\nI'm fine either way here. Let me know your preference, @HyukjinKwon. ",
        "createdAt" : "2021-06-28T19:47:58Z",
        "updatedAt" : "2021-06-28T19:47:58Z",
        "lastEditedBy" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "tags" : [
        ]
      },
      {
        "id" : "d651bc77-594e-490b-aac7-a15ac61fee3d",
        "parentId" : "492c8a3d-ee1a-49c1-adb6-c13b00c5831f",
        "authorId" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "body" : "Maybe, we can drop it and add if needed to keep the API simple.",
        "createdAt" : "2021-06-28T19:49:18Z",
        "updatedAt" : "2021-06-28T19:49:19Z",
        "lastEditedBy" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "tags" : [
        ]
      }
    ],
    "commit" : "3ac51da333f0400264081a3425e332646282c3a9",
    "line" : 50,
    "diffHunk" : "@@ -1,1 +48,52 @@      refs: Seq[NamedReference],\n      plan: LogicalPlan,\n      resolver: Resolver = conf.resolver): Seq[T] = {\n    refs.map(ref => resolveRef[T](ref, plan, resolver))\n  }"
  }
]