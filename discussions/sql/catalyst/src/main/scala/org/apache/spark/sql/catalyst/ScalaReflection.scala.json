[
  {
    "id" : "d7b4ac42-ebd7-427c-96c9-7f8d9a469c99",
    "prId" : 29403,
    "prUrl" : "https://github.com/apache/spark/pull/29403#pullrequestreview-498023439",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9684790f-c8fd-42ae-9680-d7d8439f33b9",
        "parentId" : null,
        "authorId" : "1d40bb30-68bd-4124-82bc-33d93ee4bc65",
        "body" : "Should also be `true`?",
        "createdAt" : "2020-09-28T15:35:34Z",
        "updatedAt" : "2020-09-29T06:45:16Z",
        "lastEditedBy" : "1d40bb30-68bd-4124-82bc-33d93ee4bc65",
        "tags" : [
        ]
      },
      {
        "id" : "8f4f69e4-46ec-4297-913e-4769582d3952",
        "parentId" : "9684790f-c8fd-42ae-9680-d7d8439f33b9",
        "authorId" : "1d40bb30-68bd-4124-82bc-33d93ee4bc65",
        "body" : "Also, if I'm correct, it would be good to include a test case for this case.  What happens if you actually do operations on the null value stored as an enum?  Is the nullability of the resulting schema correct. Do operations like `.isNotNull` work correctly?",
        "createdAt" : "2020-09-29T02:26:25Z",
        "updatedAt" : "2020-09-29T06:45:16Z",
        "lastEditedBy" : "1d40bb30-68bd-4124-82bc-33d93ee4bc65",
        "tags" : [
        ]
      },
      {
        "id" : "47f3d0e4-f638-4320-acf4-133728a1d29f",
        "parentId" : "9684790f-c8fd-42ae-9680-d7d8439f33b9",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "I think we need `returnNullable = false` since `Enumeration.withName` will never return null.",
        "createdAt" : "2020-09-29T02:28:03Z",
        "updatedAt" : "2020-09-29T06:45:16Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      },
      {
        "id" : "fa14c12d-9cbf-48f8-922e-3a588e53bcf5",
        "parentId" : "9684790f-c8fd-42ae-9680-d7d8439f33b9",
        "authorId" : "1d40bb30-68bd-4124-82bc-33d93ee4bc65",
        "body" : "Yeah, you are correct.",
        "createdAt" : "2020-09-29T02:34:49Z",
        "updatedAt" : "2020-09-29T06:45:16Z",
        "lastEditedBy" : "1d40bb30-68bd-4124-82bc-33d93ee4bc65",
        "tags" : [
        ]
      },
      {
        "id" : "74a0fc69-81af-428e-99ef-0e259d824e37",
        "parentId" : "9684790f-c8fd-42ae-9680-d7d8439f33b9",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "> What happens if you actually do operations on the null value stored as an enum\r\n\r\nWe will get an exception, and the behavior is same as Java enum what we supported.\r\n```\r\n        case other if other.isEnum =>\r\n          createSerializerForString(\r\n            Invoke(inputObject, \"name\", ObjectType(classOf[String]), returnNullable = false))\r\n```",
        "createdAt" : "2020-09-29T02:35:48Z",
        "updatedAt" : "2020-09-29T06:45:16Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      }
    ],
    "commit" : "2d675caa463dcbea5751b34c6463e7336a0fc76f",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +395,399 @@          \"withName\",\n          createDeserializerForString(path, false) :: Nil,\n          returnNullable = false)\n    }\n  }"
  },
  {
    "id" : "e0f2df48-9ba3-4c3e-807b-84124b075b70",
    "prId" : 29403,
    "prUrl" : "https://github.com/apache/spark/pull/29403#pullrequestreview-498023176",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7edf0ca7-5832-4425-852a-63cb328f6e86",
        "parentId" : null,
        "authorId" : "1d40bb30-68bd-4124-82bc-33d93ee4bc65",
        "body" : "`true`?",
        "createdAt" : "2020-09-28T15:36:18Z",
        "updatedAt" : "2020-09-29T06:45:16Z",
        "lastEditedBy" : "1d40bb30-68bd-4124-82bc-33d93ee4bc65",
        "tags" : [
        ]
      },
      {
        "id" : "f073b20d-08f2-40d6-9b3a-f29d20af5555",
        "parentId" : "7edf0ca7-5832-4425-852a-63cb328f6e86",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "Same as withName, toString also return a non-null value.",
        "createdAt" : "2020-09-29T02:28:59Z",
        "updatedAt" : "2020-09-29T06:45:16Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      },
      {
        "id" : "e269a353-6b2e-493a-993c-3520490e58c3",
        "parentId" : "7edf0ca7-5832-4425-852a-63cb328f6e86",
        "authorId" : "1d40bb30-68bd-4124-82bc-33d93ee4bc65",
        "body" : "Ah, I see. I agree.",
        "createdAt" : "2020-09-29T02:34:51Z",
        "updatedAt" : "2020-09-29T06:45:16Z",
        "lastEditedBy" : "1d40bb30-68bd-4124-82bc-33d93ee4bc65",
        "tags" : [
        ]
      }
    ],
    "commit" : "2d675caa463dcbea5751b34c6463e7336a0fc76f",
    "line" : 42,
    "diffHunk" : "@@ -1,1 +587,591 @@            \"toString\",\n            ObjectType(classOf[java.lang.String]),\n            returnNullable = false))\n\n      case _ =>"
  },
  {
    "id" : "1bb46199-3bcc-4392-85fd-484321e7d2bf",
    "prId" : 29403,
    "prUrl" : "https://github.com/apache/spark/pull/29403#pullrequestreview-498309506",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ce74ab9a-8e6f-43bd-8602-69e1d784009c",
        "parentId" : null,
        "authorId" : "9cd657f0-37cd-41bd-908a-f2c996b95a7a",
        "body" : "So we are returning the schema generated from enum as nullable, but the serializer expression is not nullable (because it does not produce null). Why is that?",
        "createdAt" : "2020-09-29T02:58:31Z",
        "updatedAt" : "2020-09-29T06:45:16Z",
        "lastEditedBy" : "9cd657f0-37cd-41bd-908a-f2c996b95a7a",
        "tags" : [
        ]
      },
      {
        "id" : "9bd1bbc8-46a8-4897-843d-332520a44234",
        "parentId" : "ce74ab9a-8e6f-43bd-8602-69e1d784009c",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "There are two side of `nullable`. \r\n* For schema, `nullable` means the column can be `null` value.\r\n* For `returnNullable` which used in `Invoke` and `StaticInvoke` means the method promise will never produce a `null` value.\r\n\r\nSome code get from `Invoke`\r\n```\r\ndef invoke(\r\n      obj: Any,\r\n      method: Method,\r\n      arguments: Seq[Expression],\r\n      input: InternalRow,\r\n      dataType: DataType): Any = {\r\n    val args = arguments.map(e => e.eval(input).asInstanceOf[Object])\r\n    if (needNullCheck && args.exists(_ == null)) {\r\n      // return null if one of arguments is null\r\n      null\r\n    } else {\r\n      val ret = method.invoke(obj, args: _*)\r\n      val boxedClass = ScalaReflection.typeBoxedJavaMapping.get(dataType)\r\n      if (boxedClass.isDefined) {\r\n        boxedClass.get.cast(ret)\r\n      } else {\r\n        ret\r\n      }\r\n    }\r\n  }\r\n```",
        "createdAt" : "2020-09-29T03:58:46Z",
        "updatedAt" : "2020-09-29T06:45:16Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      },
      {
        "id" : "6dfaccb0-ddb9-41f3-aedd-c03052821ae9",
        "parentId" : "ce74ab9a-8e6f-43bd-8602-69e1d784009c",
        "authorId" : "9cd657f0-37cd-41bd-908a-f2c996b95a7a",
        "body" : "So effectively, in the serialized form, we are allowing nulls to be present in the column mapping to the enum field.  But if there is indeed a row with a null in that column and we attempt to deserialize that rows, then it will cause a runtime failure...isnt it?\r\n\r\nIf this understanding is correct, then serializing will never produce null, and even if there is a null, serializing will fail.  Then why keep this column nullable = true? I am not necessarily opposed to it, I am just trying to understand the rationale. cc @marmbrus ",
        "createdAt" : "2020-09-29T06:18:08Z",
        "updatedAt" : "2020-09-29T06:45:16Z",
        "lastEditedBy" : "9cd657f0-37cd-41bd-908a-f2c996b95a7a",
        "tags" : [
        ]
      },
      {
        "id" : "e8509733-3e05-43cb-9f57-90e86e9fee27",
        "parentId" : "ce74ab9a-8e6f-43bd-8602-69e1d784009c",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "> But if there is indeed a row with a null in that column and we attempt to deserialize that rows, then it will cause a runtime failure...isnt it\r\n\r\nActually we will get null back. It's a trick that if input value is null the serializing will return null.",
        "createdAt" : "2020-09-29T10:03:05Z",
        "updatedAt" : "2020-09-29T10:03:05Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      }
    ],
    "commit" : "2d675caa463dcbea5751b34c6463e7336a0fc76f",
    "line" : 52,
    "diffHunk" : "@@ -1,1 +767,771 @@          }), nullable = true)\n      case t if isSubtype(t, localTypeOf[Enumeration#Value]) =>\n        Schema(StringType, nullable = true)\n      case other =>\n        throw new UnsupportedOperationException(s\"Schema for type $other is not supported\")"
  },
  {
    "id" : "97efe38b-d580-460f-8c56-be6c9f07edc1",
    "prId" : 29403,
    "prUrl" : "https://github.com/apache/spark/pull/29403#pullrequestreview-498307607",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "416228cd-402d-4179-acc0-21e881bb1dc3",
        "parentId" : null,
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "follow scala reflection.",
        "createdAt" : "2020-09-29T10:00:40Z",
        "updatedAt" : "2020-09-29T10:00:41Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      }
    ],
    "commit" : "2d675caa463dcbea5751b34c6463e7336a0fc76f",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +389,393 @@        // we can call example.Foo.withName to deserialize string to enumeration.\n        val parent = t.asInstanceOf[TypeRef].pre.typeSymbol.asClass\n        val cls = mirror.runtimeClass(parent)\n        StaticInvoke(\n          cls,"
  },
  {
    "id" : "c7de6a8a-eff9-4825-b3e2-96dfbfa5257b",
    "prId" : 28324,
    "prUrl" : "https://github.com/apache/spark/pull/28324#pullrequestreview-400023646",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "07876b26-ece5-4291-a23c-99e0ce5c15c5",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Thanks. It seems that these are matched with `dataTypeFor` now.",
        "createdAt" : "2020-04-24T14:58:03Z",
        "updatedAt" : "2020-04-24T16:42:23Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "75fc96b6988316e5d1a27ae9ec8343a18b91ea6f",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +129,133 @@      case t if isSubtype(t, localTypeOf[Array[Byte]]) => classOf[Array[Array[Byte]]]\n      case t if isSubtype(t, localTypeOf[CalendarInterval]) => classOf[Array[CalendarInterval]]\n      case t if isSubtype(t, localTypeOf[Decimal]) => classOf[Array[Decimal]]\n      case other =>\n        // There is probably a better way to do this, but I couldn't find it..."
  },
  {
    "id" : "821ba369-c5ba-432d-a848-b200a810f54f",
    "prId" : 28324,
    "prUrl" : "https://github.com/apache/spark/pull/28324#pullrequestreview-400445340",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ccf53c07-5691-4e7e-937a-c085bdf49d6c",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "nit: might be worth updating the comment above: https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/ScalaReflection.scala#L117-L118",
        "createdAt" : "2020-04-26T01:00:52Z",
        "updatedAt" : "2020-04-26T01:00:52Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "75fc96b6988316e5d1a27ae9ec8343a18b91ea6f",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +127,131 @@      case t if isSubtype(t, definitions.ByteTpe) => classOf[Array[Byte]]\n      case t if isSubtype(t, definitions.BooleanTpe) => classOf[Array[Boolean]]\n      case t if isSubtype(t, localTypeOf[Array[Byte]]) => classOf[Array[Array[Byte]]]\n      case t if isSubtype(t, localTypeOf[CalendarInterval]) => classOf[Array[CalendarInterval]]\n      case t if isSubtype(t, localTypeOf[Decimal]) => classOf[Array[Decimal]]"
  },
  {
    "id" : "95e59edd-896b-4380-bdf8-d30da5303b8d",
    "prId" : 28197,
    "prUrl" : "https://github.com/apache/spark/pull/28197#pullrequestreview-395199239",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "145df345-1571-4167-9035-5b7a82226b59",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "just for curiosity: why interval inside array/map/struct works before this PR?",
        "createdAt" : "2020-04-16T14:31:54Z",
        "updatedAt" : "2020-04-17T15:14:31Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "64ba3c56-c633-4b53-b46b-cc91a355c00e",
        "parentId" : "145df345-1571-4167-9035-5b7a82226b59",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "They don't work\r\n```scala\r\nscala> Seq(Map(new org.apache.spark.unsafe.types.CalendarInterval(1,2,3) -> new org.apache.spark.unsafe.types.CalendarInterval(1,2,3))).toDF\r\njava.lang.UnsupportedOperationException: Schema for type org.apache.spark.unsafe.types.CalendarInterval is not supported\r\n  at org.apache.spark.sql.catalyst.ScalaReflection$.$anonfun$schemaFor$1(ScalaReflection.scala:735)\r\n  at scala.reflect.internal.tpe.TypeConstraints$UndoLog.undo(TypeConstraints.scala:69)\r\n  at org.apache.spark.sql.catalyst.ScalaReflection.cleanUpReflectionObjects(ScalaReflection.scala:877)\r\n  at org.apache.spark.sql.catalyst.ScalaReflection.cleanUpReflectionObjects$(ScalaReflection.scala:876)\r\n  at org.apache.spark.sql.catalyst.ScalaReflection$.cleanUpReflectionObjects(ScalaReflection.scala:47)\r\n  at org.apache.spark.sql.catalyst.ScalaReflection$.schemaFor(ScalaReflection.scala:663)\r\n  at org.apache.spark.sql.catalyst.ScalaReflection$.$anonfun$schemaFor$1(ScalaReflection.scala:689)\r\n  at scala.reflect.internal.tpe.TypeConstraints$UndoLog.undo(TypeConstraints.scala:69)\r\n  at org.apache.spark.sql.catalyst.ScalaReflection.cleanUpReflectionObjects(ScalaReflection.scala:877)\r\n  at org.apache.spark.sql.catalyst.ScalaReflection.cleanUpReflectionObjects$(ScalaReflection.scala:876)\r\n  at org.apache.spark.sql.catalyst.ScalaReflection$.cleanUpReflectionObjects(ScalaReflection.scala:47)\r\n  at org.apache.spark.sql.catalyst.ScalaReflection$.schemaFor(ScalaReflection.scala:663)\r\n  at org.apache.spark.sql.catalyst.ScalaReflection$.deserializerForType(ScalaReflection.scala:164)\r\n  at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder$.apply(ExpressionEncoder.scala:55)\r\n  at org.apache.spark.sql.SQLImplicits.newMapEncoder(SQLImplicits.scala:175)\r\n  ... 49 elided\r\n\r\n\r\nscala> Seq(Seq(new org.apache.spark.unsafe.types.CalendarInterval(1,2,3))).toDF\r\njava.lang.UnsupportedOperationException: Schema for type org.apache.spark.unsafe.types.CalendarInterval is not supported\r\n  at org.apache.spark.sql.catalyst.ScalaReflection$.$anonfun$schemaFor$1(ScalaReflection.scala:735)\r\n  at scala.reflect.internal.tpe.TypeConstraints$UndoLog.undo(TypeConstraints.scala:69)\r\n  at org.apache.spark.sql.catalyst.ScalaReflection.cleanUpReflectionObjects(ScalaReflection.scala:877)\r\n  at org.apache.spark.sql.catalyst.ScalaReflection.cleanUpReflectionObjects$(ScalaReflection.scala:876)\r\n  at org.apache.spark.sql.catalyst.ScalaReflection$.cleanUpReflectionObjects(ScalaReflection.scala:47)\r\n  at org.apache.spark.sql.catalyst.ScalaReflection$.schemaFor(ScalaReflection.scala:663)\r\n  at org.apache.spark.sql.catalyst.ScalaReflection$.toCatalystArray$1(ScalaReflection.scala:430)\r\n  at org.apache.spark.sql.catalyst.ScalaReflection$.$anonfun$serializerFor$1(ScalaReflection.scala:448)\r\n  at scala.reflect.internal.tpe.TypeConstraints$UndoLog.undo(TypeConstraints.scala:69)\r\n  at org.apache.spark.sql.catalyst.ScalaReflection.cleanUpReflectionObjects(ScalaReflection.scala:877)\r\n  at org.apache.spark.sql.catalyst.ScalaReflection.cleanUpReflectionObjects$(ScalaReflection.scala:876)\r\n  at org.apache.spark.sql.catalyst.ScalaReflection$.cleanUpReflectionObjects(ScalaReflection.scala:47)\r\n  at org.apache.spark.sql.catalyst.ScalaReflection$.serializerFor(ScalaReflection.scala:410)\r\n  at org.apache.spark.sql.catalyst.ScalaReflection$.$anonfun$serializerForType$1(ScalaReflection.scala:399)\r\n  at scala.reflect.internal.tpe.TypeConstraints$UndoLog.undo(TypeConstraints.scala:69)\r\n  at org.apache.spark.sql.catalyst.ScalaReflection.cleanUpReflectionObjects(ScalaReflection.scala:877)\r\n  at org.apache.spark.sql.catalyst.ScalaReflection.cleanUpReflectionObjects$(ScalaReflection.scala:876)\r\n  at org.apache.spark.sql.catalyst.ScalaReflection$.cleanUpReflectionObjects(ScalaReflection.scala:47)\r\n  at org.apache.spark.sql.catalyst.ScalaReflection$.serializerForType(ScalaReflection.scala:391)\r\n  at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder$.apply(ExpressionEncoder.scala:54)\r\n  at org.apache.spark.sql.SQLImplicits.newSequenceEncoder(SQLImplicits.scala:171)\r\n  ... 49 elided\r\n\r\nscala> Seq(Array(new org.apache.spark.unsafe.types.CalendarInterval(1,2,3))).toDF\r\n<console>:28: error: value toDF is not a member of Seq[Array[org.apache.spark.unsafe.types.CalendarInterval]]\r\n       Seq(Array(new org.apache.spark.unsafe.types.CalendarInterval(1,2,3))).toDF\r\n```",
        "createdAt" : "2020-04-17T03:55:37Z",
        "updatedAt" : "2020-04-17T15:14:31Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "83fdbece-106d-460d-bcad-ce31c2d8891a",
        "parentId" : "145df345-1571-4167-9035-5b7a82226b59",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Sorry, I mean, struct:\r\n> CalenderInterval is available to be converted to/from internal Spark SQL representation when it is a member of a Scala's product type e.g tuples/ case class etc but not as a primitive type",
        "createdAt" : "2020-04-17T06:04:32Z",
        "updatedAt" : "2020-04-17T15:14:31Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "db7137ec-9a18-489d-b537-de29b25ddf9a",
        "parentId" : "145df345-1571-4167-9035-5b7a82226b59",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "with PR https://github.com/apache/spark/pull/28198/files  and the existing logic in `serializerFor`\r\n1. https://github.com/yaooqinn/spark/blob/6cd0bef7fe872b5d9472a2140db188fb9f1eede8/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/ScalaReflection.scala#L436\r\n2. https://github.com/yaooqinn/spark/blob/6cd0bef7fe872b5d9472a2140db188fb9f1eede8/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/ScalaReflection.scala#L554",
        "createdAt" : "2020-04-17T06:47:01Z",
        "updatedAt" : "2020-04-17T15:14:31Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "c4d4ef2cca80b6c8fa85a54723a52c004c303aec",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +237,241 @@        createDeserializerForString(path, returnNullable = false)\n\n      case t if isSubtype(t, localTypeOf[CalendarInterval]) => path\n      case t if isSubtype(t, localTypeOf[java.math.BigDecimal]) =>\n        createDeserializerForJavaBigDecimal(path, returnNullable = false)"
  },
  {
    "id" : "c6c8f664-295c-4a3a-9d3d-3c18178c3d1e",
    "prId" : 28197,
    "prUrl" : "https://github.com/apache/spark/pull/28197#pullrequestreview-395479601",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e05f5818-26dd-4b80-b522-be14d6e55269",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Is it really needed? `Decimal` is not handled here.",
        "createdAt" : "2020-04-17T13:53:32Z",
        "updatedAt" : "2020-04-17T15:14:31Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "747a6b1f-9aa3-4050-89d2-e0acc5bd125a",
        "parentId" : "e05f5818-26dd-4b80-b522-be14d6e55269",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "Decimal is not supported as primitive or array elementï¼ŒI will make it work in another PR",
        "createdAt" : "2020-04-17T14:00:21Z",
        "updatedAt" : "2020-04-17T15:14:31Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "c4d4ef2cca80b6c8fa85a54723a52c004c303aec",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +127,131 @@      case t if isSubtype(t, definitions.ByteTpe) => classOf[Array[Byte]]\n      case t if isSubtype(t, definitions.BooleanTpe) => classOf[Array[Boolean]]\n      case t if isSubtype(t, localTypeOf[CalendarInterval]) => classOf[Array[CalendarInterval]]\n      case other =>\n        // There is probably a better way to do this, but I couldn't find it..."
  },
  {
    "id" : "827127e3-8f8f-4e95-9de8-a35aaf6bd2c1",
    "prId" : 28184,
    "prUrl" : "https://github.com/apache/spark/pull/28184#pullrequestreview-391821585",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fcf55b70-723d-403d-ad38-20b82f440ceb",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "I'm not familiar with this method though, this check depends on users' Java runtime version? cc: @rednaxelafx @kiszk ",
        "createdAt" : "2020-04-11T00:26:34Z",
        "updatedAt" : "2020-04-12T08:20:21Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "1650429a-e33a-4ae6-9fdf-228923ad39fd",
        "parentId" : "fcf55b70-723d-403d-ad38-20b82f440ceb",
        "authorId" : "3a12ce0f-9e73-4cfb-a4b4-b19368cddc2f",
        "body" : "The `SourceVersion` class knows how to detect the spec version of the current running Java runtime, and can perform checks like identifier validation accordingly.\r\nc.f. http://hg.openjdk.java.net/jdk8u/jdk8u/langtools/file/01036da3155c/src/share/classes/javax/lang/model/SourceVersion.java#l183",
        "createdAt" : "2020-04-11T00:46:07Z",
        "updatedAt" : "2020-04-12T08:20:21Z",
        "lastEditedBy" : "3a12ce0f-9e73-4cfb-a4b4-b19368cddc2f",
        "tags" : [
        ]
      },
      {
        "id" : "4e611988-9074-41ac-9fff-e71447a0150d",
        "parentId" : "fcf55b70-723d-403d-ad38-20b82f440ceb",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Ah, I see. Thanks for the info, Kris. btw, the reserved keywords depend on Janino though, is it okay for the check to depend on running Java runtime?",
        "createdAt" : "2020-04-11T00:54:32Z",
        "updatedAt" : "2020-04-12T08:20:21Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "7af35721-60ae-4131-ba33-0c86ac613d61",
        "parentId" : "fcf55b70-723d-403d-ad38-20b82f440ceb",
        "authorId" : "3a227965-84e0-47cf-9974-11293764f028",
        "body" : "In the original implementation, `javaKeywords` contains `default` so how about checking keywords for Java 8 ?",
        "createdAt" : "2020-04-11T06:37:35Z",
        "updatedAt" : "2020-04-12T08:20:21Z",
        "lastEditedBy" : "3a227965-84e0-47cf-9974-11293764f028",
        "tags" : [
        ]
      },
      {
        "id" : "c86879c4-c99c-4534-87b5-81d444189d79",
        "parentId" : "fcf55b70-723d-403d-ad38-20b82f440ceb",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "How about this, @cloud-fan ? I found he defined the initial set for the keywords in https://github.com/apache/spark/pull/13485",
        "createdAt" : "2020-04-11T23:48:30Z",
        "updatedAt" : "2020-04-12T08:20:21Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "acba227e-0cdb-4c28-b639-f6185f80c027",
        "parentId" : "fcf55b70-723d-403d-ad38-20b82f440ceb",
        "authorId" : "99e31844-a5b3-48a2-af71-412edb607a13",
        "body" : "While the reserved keywords depend on Janino though, `SourceVersion.isKeyword(fieldName)` would have the super set of keywords that Janino cannot accept. I think that this change is reasonable. We can avoid maintain `javaKeywords`.",
        "createdAt" : "2020-04-12T04:32:08Z",
        "updatedAt" : "2020-04-12T08:20:21Z",
        "lastEditedBy" : "99e31844-a5b3-48a2-af71-412edb607a13",
        "tags" : [
        ]
      },
      {
        "id" : "c947e602-6798-44ae-bf12-bdd84bf66c08",
        "parentId" : "fcf55b70-723d-403d-ad38-20b82f440ceb",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "> SourceVersion.isKeyword(fieldName) would have the super set of keywords that Janino cannot accept\r\n\r\nLooks nice, thanks for the check, @kiszk ",
        "createdAt" : "2020-04-12T04:53:18Z",
        "updatedAt" : "2020-04-12T08:20:21Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "a42b169ecfa64dbe009f14069f941d4e0964d40e",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +542,546 @@        val params = getConstructorParameters(t)\n        val fields = params.map { case (fieldName, fieldType) =>\n          if (SourceVersion.isKeyword(fieldName) ||\n              !SourceVersion.isIdentifier(encodeFieldNameToIdentifier(fieldName))) {\n            throw new UnsupportedOperationException(s\"`$fieldName` is not a valid identifier of \" +"
  },
  {
    "id" : "615b5084-ebee-4b04-a3ce-5e93391c364f",
    "prId" : 28165,
    "prUrl" : "https://github.com/apache/spark/pull/28165#pullrequestreview-390697577",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1469e4a1-1105-4ca9-b044-a7a8e93f0aea",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Hey,\r\n\r\n> Since 3.0.0, we make CalendarInterval public ...\r\n\r\nCan we clarify which scope we want to change? According to your PR description, we should support read and write paths too, which I think we decided to don't implement.",
        "createdAt" : "2020-04-09T11:03:53Z",
        "updatedAt" : "2020-04-09T11:03:53Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "c1929fec-442b-445f-945e-bddcd9ebf02f",
        "parentId" : "1469e4a1-1105-4ca9-b044-a7a8e93f0aea",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "Oh, I separate this to the 3rd PR question, I will duplicate it in the first one.",
        "createdAt" : "2020-04-09T11:07:56Z",
        "updatedAt" : "2020-04-09T11:07:56Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "caaf92a48c4c94fb89eb1a15df85f932c03e55f1",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +702,706 @@      case t if isSubtype(t, localTypeOf[java.sql.Date]) => Schema(DateType, nullable = true)\n      case t if isSubtype(t, localTypeOf[CalendarInterval]) =>\n        Schema(CalendarIntervalType, nullable = true)\n      case t if isSubtype(t, localTypeOf[BigDecimal]) =>\n        Schema(DecimalType.SYSTEM_DEFAULT, nullable = true)"
  }
]