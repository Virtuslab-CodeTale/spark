[
  {
    "id" : "ba016c8b-88f4-4747-9832-10c0fa17dc97",
    "prId" : 32577,
    "prUrl" : "https://github.com/apache/spark/pull/32577#pullrequestreview-661663031",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9050e429-6e8e-4b7b-8644-df8b3407ec7a",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Can we switch the above two lines to support numeric set in a more natural way?\r\n```scala\r\n.sorted\r\n.map(elem => Literal(elem, child.dataType).toString)\r\n```",
        "createdAt" : "2021-05-18T04:11:02Z",
        "updatedAt" : "2021-05-18T04:11:02Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "4c7e4113-78d4-4d8c-b17e-0e41f39f00c6",
        "parentId" : "9050e429-6e8e-4b7b-8644-df8b3407ec7a",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "We cannot because `Seq[Any]` cannot be sorted;\r\n```\r\n[error] /Users/maropu/Repositories/spark/spark-master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/predicates.scala:648:8: No implicit Ordering defined for Any.\r\n[error]       .sorted\r\n```",
        "createdAt" : "2021-05-18T05:03:13Z",
        "updatedAt" : "2021-05-18T05:03:13Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "c7a16854-ed0b-4580-98a4-cf17fb9cbc1a",
        "parentId" : "9050e429-6e8e-4b7b-8644-df8b3407ec7a",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Got it. Thank you for checking.",
        "createdAt" : "2021-05-18T05:58:19Z",
        "updatedAt" : "2021-05-18T05:58:19Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "30945c3e9ca143457447ef3b8a7d51a69a4a07a9",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +549,553 @@      .map(elem => Literal(elem, child.dataType).toString)\n      // Sort elements for deterministic behaviours\n      .sorted\n      .mkString(\", \")\n    s\"$child INSET $listString\""
  },
  {
    "id" : "d9bdf945-a3dc-496f-9c81-66a2f5b1fe41",
    "prId" : 32577,
    "prUrl" : "https://github.com/apache/spark/pull/32577#pullrequestreview-661615007",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d054423f-5f95-4e2a-96c8-0bd88fdbfd92",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Ditto. Can we do `sort` first?",
        "createdAt" : "2021-05-18T04:12:16Z",
        "updatedAt" : "2021-05-18T04:12:17Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "30945c3e9ca143457447ef3b8a7d51a69a4a07a9",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +647,651 @@      .map(elem => Literal(elem, child.dataType).sql)\n      // Sort elements for deterministic behaviours\n      .sorted\n      .mkString(\", \")\n    s\"($valueSQL IN ($listSQL))\""
  },
  {
    "id" : "00588b38-5c28-4fe3-8dff-87a0da158e9b",
    "prId" : 29101,
    "prUrl" : "https://github.com/apache/spark/pull/29101#pullrequestreview-449728803",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fb5bffb7-65df-48e9-8412-a8b0560bc956",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "`a1 and b1 is convertible, , while a2 and b2 is not` -> `the output set is a1 and b1`?",
        "createdAt" : "2020-07-16T10:41:20Z",
        "updatedAt" : "2020-07-17T18:03:34Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "d37c21c25992b29649b6358a02ed483670d32014",
    "line" : 141,
    "diffHunk" : "@@ -1,1 +228,232 @@    // condition: (a1 AND a2) OR (b1 AND b2),\n    // outputSet: AttributeSet(a1, b1)\n    // a1 and b1 is convertible, while a2 and b2 is not.\n    // The predicate can be converted as\n    // (a1 OR b1) AND (a1 OR b2) AND (a2 OR b1) AND (a2 OR b2)"
  },
  {
    "id" : "2a642e02-d9bf-4b43-b99d-7d8d3add1949",
    "prId" : 29101,
    "prUrl" : "https://github.com/apache/spark/pull/29101#pullrequestreview-450030780",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "58ac926d-4f2c-4d08-8327-4072508a46c7",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "`it contains all possible constraints from condition` -> `it's a precondition to satisfy the given condition`",
        "createdAt" : "2020-07-16T16:18:03Z",
        "updatedAt" : "2020-07-17T18:03:34Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "01f7b684-4bb0-4d15-aeab-a0ca4e02b441",
        "parentId" : "58ac926d-4f2c-4d08-8327-4072508a46c7",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "We have to specify the result contains the maximum contrainsts here. Otherwise, any of `a`/`b`/`a&&b`  is  a condition that satisfy `a && b && c && d`  with outputSet as `Set(a,b,c,d)`",
        "createdAt" : "2020-07-16T16:47:09Z",
        "updatedAt" : "2020-07-17T18:03:34Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "d37c21c25992b29649b6358a02ed483670d32014",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +204,208 @@  /**\n   * Returns a filter that its reference is a subset of `outputSet` and it contains the maximum\n   * constraints from `condition`. This is used for predicate pushdown.\n   * When there is no such filter, `None` is returned.\n   */"
  },
  {
    "id" : "80b0087d-b250-4ecd-b29f-e4ad35785a2d",
    "prId" : 29075,
    "prUrl" : "https://github.com/apache/spark/pull/29075#pullrequestreview-447676258",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "45adbaeb-4bdf-4d68-ae57-310cb3632cca",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "nit: `A method to group expressions for reducing the size of pushed down predicates and corresponding codegen`?",
        "createdAt" : "2020-07-13T23:22:06Z",
        "updatedAt" : "2020-07-13T23:22:30Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "6fe106ce84e83641d66d572c45050b25761bbf3d",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +210,214 @@   * @param condition Condition to be converted into CNF.\n   * @param groupExpsFunc A method for grouping intermediate results so that the final result can be\n   *                      shorter.\n   * @return the CNF result as sequence of disjunctive expressions. If the number of expressions\n   *         exceeds threshold on converting `Or`, `Seq.empty` is returned."
  },
  {
    "id" : "8ee55bd4-769a-4779-bdc0-34b718ef36a3",
    "prId" : 28805,
    "prUrl" : "https://github.com/apache/spark/pull/28805#pullrequestreview-429497738",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "838aa0c8-a38d-4c81-b076-23585c61d74f",
        "parentId" : null,
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "Too many duplicate code here. Maybe you can organize code:\r\n```scala\r\n  def conjunctiveNormalFormAndGroupExpsByQualifier(condition: Expression): Seq[Expression] = {\r\n    conjunctiveNormalForm(condition,\r\n      (expressions: Seq[Expression]) =>\r\n        expressions.groupBy(_.references.map(_.qualifier)).map(_._2.reduceLeft(And)).toSeq)\r\n  }\r\n\r\n  def conjunctiveNormalFormAndGroupExpsByReference(condition: Expression): Seq[Expression] = {\r\n    conjunctiveNormalForm(condition,\r\n      (expressions: Seq[Expression]) =>\r\n        expressions.groupBy(_.references).map(_._2.reduceLeft(And)).toSeq)\r\n  }\r\n\r\n  def conjunctiveNormalForm(\r\n      condition: Expression,\r\n      groupExpsFunc: Seq[Expression] => Seq[Expression] = _.toSeq): Seq[Expression] = {\r\n    val postOrderNodes = postOrderTraversal(condition)\r\n    val resultStack = new mutable.Stack[Seq[Expression]]\r\n    val maxCnfNodeCount = SQLConf.get.maxCnfNodeCount\r\n    // Bottom up approach to get CNF of sub-expressions\r\n    while (postOrderNodes.nonEmpty) {\r\n      val cnf = postOrderNodes.pop() match {\r\n        case _: And =>\r\n          val right = resultStack.pop()\r\n          val left = resultStack.pop()\r\n          left ++ right\r\n        case _: Or =>\r\n          val right = groupExpsFunc(resultStack.pop())\r\n          val left = groupExpsFunc(resultStack.pop())\r\n          // Stop the loop whenever the result exceeds the `maxCnfNodeCount`\r\n          if (left.size * right.size > maxCnfNodeCount) {\r\n            logInfo(s\"As the result size exceeds the threshold $maxCnfNodeCount. \" +\r\n              \"The CNF conversion is skipped and returning Seq.empty now. To avoid this, you can \" +\r\n              s\"raise the limit ${SQLConf.MAX_CNF_NODE_COUNT.key}.\")\r\n            return Seq.empty\r\n          } else {\r\n            for { x <- left; y <- right } yield Or(x, y)\r\n          }\r\n        case other => other :: Nil\r\n      }\r\n      resultStack.push(cnf)\r\n    }\r\n    if (resultStack.length != 1) {\r\n      logWarning(\"The length of CNF conversion result stack is supposed to be 1. There might \" +\r\n        \"be something wrong with CNF conversion.\")\r\n      return Seq.empty\r\n    }\r\n    resultStack.top\r\n  }\r\n```",
        "createdAt" : "2020-06-12T06:43:35Z",
        "updatedAt" : "2020-07-01T08:39:56Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      }
    ],
    "commit" : "e2777c99144f91b463924530a0a89e6f1a39ab66",
    "line" : 59,
    "diffHunk" : "@@ -1,1 +283,287 @@        expressions.groupBy(e => AttributeSet(e.references)).map(_._2.reduceLeft(And)).toSeq)\n  }\n\n  /**\n   * Iterative post order traversal over a binary tree built by And/Or clauses with two stacks."
  },
  {
    "id" : "8440493b-b7db-4b03-9fdd-a5312391ef79",
    "prId" : 28805,
    "prUrl" : "https://github.com/apache/spark/pull/28805#pullrequestreview-440478325",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "92a12dc8-fb27-4476-90e0-32748694aaa7",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "nit format:\r\n```\r\n    conjunctiveNormalForm(condition, (expressions: Seq[Expression]) =>\r\n        expressions.groupBy(_.references.map(_.qualifier)).map(_._2.reduceLeft(And)).toSeq)\r\n```",
        "createdAt" : "2020-07-01T00:52:11Z",
        "updatedAt" : "2020-07-01T08:39:56Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "e2777c99144f91b463924530a0a89e6f1a39ab66",
    "line" : 39,
    "diffHunk" : "@@ -1,1 +263,267 @@  def CNFWithGroupExpressionsByQualifier(condition: Expression): Seq[Expression] = {\n    conjunctiveNormalForm(condition, (expressions: Seq[Expression]) =>\n        expressions.groupBy(_.references.map(_.qualifier)).map(_._2.reduceLeft(And)).toSeq)\n  }\n"
  },
  {
    "id" : "9fd76599-49aa-446b-a11b-ff7b879b852f",
    "prId" : 28733,
    "prUrl" : "https://github.com/apache/spark/pull/28733#pullrequestreview-426965671",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "63c1d45e-b51c-45a5-b2ee-fc36754a7978",
        "parentId" : null,
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "For  a test case  `dt = '1' OR (dt = '2' AND id = 1)` passed to   conjunctiveNormalForm, still return   `dt = '1' OR (dt = '2' AND id = 1)`. \r\n\r\nSee qualifier when groupby ,   they are\r\n```\r\nList(List(spark_catalog, default, t))\r\nList(List(spark_catalog, default, t))\r\n```\r\n",
        "createdAt" : "2020-06-08T07:18:27Z",
        "updatedAt" : "2020-06-11T00:53:20Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "507c04c0-86d3-426a-97ae-8916c4b65a62",
        "parentId" : "63c1d45e-b51c-45a5-b2ee-fc36754a7978",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "I think we can try \r\n```\r\nexpressions.groupBy(_.references.flatMap(_.qualifier).toSet).map(_._2.reduceLeft(And)).toSeq\r\n```\r\nI will update this PR later",
        "createdAt" : "2020-06-08T07:38:26Z",
        "updatedAt" : "2020-06-11T00:53:20Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "d1e2e0e6-f49f-4b59-9e25-6af925994f98",
        "parentId" : "63c1d45e-b51c-45a5-b2ee-fc36754a7978",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> ```\r\n> expressions.groupBy(_.references.flatMap(_.qualifier).toSet).map(_._2.reduceLeft(And)).toSeq\r\n> ```\r\n\r\nNot work,  just\r\n```\r\nexpressions.groupBy(_.references).map(_._2.reduceLeft(And)).toSeq\r\n```",
        "createdAt" : "2020-06-08T07:54:53Z",
        "updatedAt" : "2020-06-11T00:53:20Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "b13ec6c6-95ee-4025-b20a-73d0372ea6d9",
        "parentId" : "63c1d45e-b51c-45a5-b2ee-fc36754a7978",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "The qualifier is the table name which is able to be used for aggregating more expressions",
        "createdAt" : "2020-06-08T08:15:55Z",
        "updatedAt" : "2020-06-11T00:53:20Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "f3f66274-46c8-4a84-9f90-194ef96142d2",
        "parentId" : "63c1d45e-b51c-45a5-b2ee-fc36754a7978",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> The qualifier is the table name which is able to be used for aggregating more expressions\r\n\r\nGot the point, you did this for split condition to join children, I want convert scan predicate condition to optimize scan predicate. ",
        "createdAt" : "2020-06-08T08:35:14Z",
        "updatedAt" : "2020-06-11T00:53:20Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "92664ada-7f80-4ebf-beed-56b481840a31",
        "parentId" : "63c1d45e-b51c-45a5-b2ee-fc36754a7978",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "I think this PR is complex enough. Let's keep this part in this way for now.",
        "createdAt" : "2020-06-09T09:33:45Z",
        "updatedAt" : "2020-06-11T00:53:20Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "dd684098-04af-425f-8550-3a25b4f407b8",
        "parentId" : "63c1d45e-b51c-45a5-b2ee-fc36754a7978",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "Yea, I  will raise pr for other problem base on your code and change a little  after your pr merged.",
        "createdAt" : "2020-06-09T09:36:43Z",
        "updatedAt" : "2020-06-11T00:53:20Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "b42ce1dde86069a74ebbda44a1729cde39c2672d",
    "line" : 74,
    "diffHunk" : "@@ -1,1 +252,256 @@  private def groupExpressionsByQualifier(expressions: Seq[Expression]): Seq[Expression] = {\n    expressions.groupBy(_.references.map(_.qualifier)).map(_._2.reduceLeft(And)).toSeq\n  }\n\n  /**"
  },
  {
    "id" : "1829456b-eb60-4f65-9e6d-2361ce0e6bd5",
    "prId" : 28733,
    "prUrl" : "https://github.com/apache/spark/pull/28733#pullrequestreview-428392589",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f5b10638-48f7-446f-899e-9f38d9115d59",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Based on  the the `@return` statement above, how about `conjunctiveNormalForm ` -> `convertToDisjunctiveExpressions`?",
        "createdAt" : "2020-06-10T14:16:10Z",
        "updatedAt" : "2020-06-11T00:53:20Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "e031e8fa-1f8b-494c-aa4a-15e33e81f966",
        "parentId" : "f5b10638-48f7-446f-899e-9f38d9115d59",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "well, let me remove the word `Disjunctive` to avoid misunderstanding.",
        "createdAt" : "2020-06-10T20:12:17Z",
        "updatedAt" : "2020-06-11T00:53:20Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "b42ce1dde86069a74ebbda44a1729cde39c2672d",
    "line" : 34,
    "diffHunk" : "@@ -1,1 +212,216 @@   *         exceeds threshold on converting `Or`, `Seq.empty` is returned.\n   */\n  def conjunctiveNormalForm(condition: Expression): Seq[Expression] = {\n    val postOrderNodes = postOrderTraversal(condition)\n    val resultStack = new mutable.Stack[Seq[Expression]]"
  },
  {
    "id" : "d9356c50-7866-4c57-8157-d53a9212258f",
    "prId" : 28733,
    "prUrl" : "https://github.com/apache/spark/pull/28733#pullrequestreview-428089943",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2f9cbf87-b7fe-4e73-b5ca-2671e111d090",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "`the same references.` -> `the same table references.`?",
        "createdAt" : "2020-06-10T14:26:23Z",
        "updatedAt" : "2020-06-11T00:53:20Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "b42ce1dde86069a74ebbda44a1729cde39c2672d",
    "line" : 46,
    "diffHunk" : "@@ -1,1 +224,228 @@          left ++ right\n        case _: Or =>\n          // For each side, there is no need to expand predicates of the same references.\n          // So here we can aggregate predicates of the same qualifier as one single predicate,\n          // for reducing the size of pushed down predicates and corresponding codegen."
  },
  {
    "id" : "3eeb098a-6ae7-40c8-8517-cc96216f3b6b",
    "prId" : 28733,
    "prUrl" : "https://github.com/apache/spark/pull/28733#pullrequestreview-428089943",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "17b15245-bc35-4816-aa2d-6450ceea1fde",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Could we print some messages here by `logInfo` just like `WholeStageCodegenExec `? https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/WholeStageCodegenExec.scala#L729-L733",
        "createdAt" : "2020-06-10T14:28:33Z",
        "updatedAt" : "2020-06-11T00:53:20Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "b42ce1dde86069a74ebbda44a1729cde39c2672d",
    "line" : 52,
    "diffHunk" : "@@ -1,1 +230,234 @@          val left = groupExpressionsByQualifier(resultStack.pop())\n          // Stop the loop whenever the result exceeds the `maxCnfNodeCount`\n          if (left.size * right.size > maxCnfNodeCount) {\n            logInfo(s\"As the result size exceeds the threshold $maxCnfNodeCount. \" +\n              \"The CNF conversion is skipped and returning Seq.empty now. To avoid this, you can \" +"
  }
]