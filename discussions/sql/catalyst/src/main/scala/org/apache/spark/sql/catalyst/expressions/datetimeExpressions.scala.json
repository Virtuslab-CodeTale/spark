[
  {
    "id" : "67ff9743-018c-446d-bd63-fd72823a8e99",
    "prId" : 33775,
    "prUrl" : "https://github.com/apache/spark/pull/33775#pullrequestreview-732493139",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "14288cdd-1ae4-4928-852c-9ebf6ef96de7",
        "parentId" : null,
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "Add comment for why choosing `DecimalType(16, 6))`",
        "createdAt" : "2021-08-18T06:30:44Z",
        "updatedAt" : "2021-08-18T06:30:44Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "434c4f41-6abc-46cb-ba49-f9522eb54fe0",
        "parentId" : "14288cdd-1ae4-4928-852c-9ebf6ef96de7",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "OK",
        "createdAt" : "2021-08-18T06:52:08Z",
        "updatedAt" : "2021-08-18T06:52:08Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "147429f919dc0824d64de976fcd8f52652050d13",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +2561,2565 @@  // casted into decimal safely, we use DecimalType(16, 6) which is wider than DecimalType(10, 0).\n  override def inputTypes: Seq[AbstractDataType] =\n    Seq(IntegerType, IntegerType, IntegerType, IntegerType, IntegerType, DecimalType(16, 6)) ++\n      timezone.map(_ => StringType)\n  override def nullable: Boolean = if (failOnError) children.exists(_.nullable) else true"
  },
  {
    "id" : "125d7949-8940-4cd1-bc4e-2fc57029ce80",
    "prId" : 33290,
    "prUrl" : "https://github.com/apache/spark/pull/33290#pullrequestreview-703581578",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9c1d1c39-2fdd-4a02-8082-ea96a94dec64",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Please, use TIMESTAMP_TYPE.key. Using the SQL config has some benefits like:\r\n- Easer to search in IDE by config name\r\n- Refactoring. Maybe, we will rename it in the future. So, you will not need to modify this place.",
        "createdAt" : "2021-07-11T10:23:23Z",
        "updatedAt" : "2021-07-11T10:32:40Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "d0f1fdde-7d6b-4c3e-b5ad-ff3ccf96de57",
        "parentId" : "9c1d1c39-2fdd-4a02-8082-ea96a94dec64",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "There will be errors:\r\n```\r\nannotation argument needs to be a constant; found: \"_FUNC_(year, month, day, hour, min, sec[, timezone]) - Create timestamp from year, month, day, hour, min, sec and timezone fields. \".+(scala.StringContext.apply(\"The result data type is consistent ...\r\n```",
        "createdAt" : "2021-07-11T12:14:40Z",
        "updatedAt" : "2021-07-11T12:14:40Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "abce601cba887bc66040dfb4b658cc051ccd289c",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +2288,2292 @@@ExpressionDescription(\n  usage = \"_FUNC_(year, month, day, hour, min, sec[, timezone]) - Create timestamp from year, month, day, hour, min, sec and timezone fields. \" +\n    \"The result data type is consistent with the value of configuration `spark.sql.timestampType`\",\n  arguments = \"\"\"\n    Arguments:"
  },
  {
    "id" : "f7e0fe81-3467-4e5c-ace6-2fb8d97aa7d9",
    "prId" : 33280,
    "prUrl" : "https://github.com/apache/spark/pull/33280#pullrequestreview-703581608",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bd8e0dc0-9513-468a-a499-309375b6f363",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Use TIMESTAMP_TYPE.key",
        "createdAt" : "2021-07-11T08:35:05Z",
        "updatedAt" : "2021-07-11T08:35:41Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "e11a7943-b80b-44f3-aba7-eee33b430123",
        "parentId" : "bd8e0dc0-9513-468a-a499-309375b6f363",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "There will be errors:\r\n```\r\nannotation argument needs to be a constant\r\n```",
        "createdAt" : "2021-07-11T12:15:00Z",
        "updatedAt" : "2021-07-11T12:15:00Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "4c11b7f373e5d108634888745c7fa3bcb8c1abf4",
    "line" : 48,
    "diffHunk" : "@@ -1,1 +1911,1915 @@      to a timestamp. Returns null with invalid input. By default, it follows casting rules to\n      a timestamp if the `fmt` is omitted. The result data type is consistent with the value of\n      configuration `spark.sql.timestampType`.\n  \"\"\",\n  arguments = \"\"\""
  },
  {
    "id" : "de985542-fbda-4d5b-997a-7afba7c0897f",
    "prId" : 33258,
    "prUrl" : "https://github.com/apache/spark/pull/33258#pullrequestreview-701909940",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "964d12d7-9a37-4e17-ad6d-2b165afbf4d1",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "\r\n```suggestion\r\n  since = \"3.3.0\")\r\n```\r\n",
        "createdAt" : "2021-07-08T10:42:29Z",
        "updatedAt" : "2021-07-08T10:42:30Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "998089cd0f1c2368e5a2b2773651836561a0a209",
    "line" : 30,
    "diffHunk" : "@@ -1,1 +218,222 @@  \"\"\",\n  group = \"datetime_funcs\",\n  since = \"3.2.0\")\ncase class LocalTimestamp(timeZoneId: Option[String] = None) extends LeafExpression\n  with TimeZoneAwareExpression with CodegenFallback {"
  },
  {
    "id" : "f24b8ec0-187b-438e-944c-c817614197de",
    "prId" : 33258,
    "prUrl" : "https://github.com/apache/spark/pull/33258#pullrequestreview-704714981",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2a713bc1-7a17-4186-b5a9-072270f23be8",
        "parentId" : null,
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "Shall we add a new test case for the code change here?",
        "createdAt" : "2021-07-12T11:02:13Z",
        "updatedAt" : "2021-07-12T11:02:13Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "f474dea8-7299-40bf-bbed-8e6e8542787f",
        "parentId" : "2a713bc1-7a17-4186-b5a9-072270f23be8",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "I wonder how it is related to new function? If it is not related, I would exclude this from the PR, and open new PR with a test.",
        "createdAt" : "2021-07-12T14:21:12Z",
        "updatedAt" : "2021-07-12T14:21:12Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "a1768190-a03a-4228-9bd2-0f06ade07f37",
        "parentId" : "2a713bc1-7a17-4186-b5a9-072270f23be8",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "it's related: https://github.com/apache/spark/pull/33258/files#diff-cf2dc65d70aa7bc490d237f1c50f5fdf51e244fabed782977a94d2934c208551R565",
        "createdAt" : "2021-07-12T18:42:03Z",
        "updatedAt" : "2021-07-12T18:42:04Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "0f03b7ce-1c45-4895-a529-9e8994a9c470",
        "parentId" : "2a713bc1-7a17-4186-b5a9-072270f23be8",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "I will add new test case in this PR.",
        "createdAt" : "2021-07-13T03:36:12Z",
        "updatedAt" : "2021-07-13T07:24:19Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "998089cd0f1c2368e5a2b2773651836561a0a209",
    "line" : 59,
    "diffHunk" : "@@ -1,1 +276,280 @@      case _: TimestampType => Literal(timestampUs, TimestampType)\n      case _: TimestampNTZType =>\n        Literal(convertTz(timestampUs, ZoneOffset.UTC, zoneId), TimestampNTZType)\n      case _: DateType => Literal(microsToDays(timestampUs, zoneId), DateType)\n    }"
  },
  {
    "id" : "c8a80053-0c1d-41c6-a11a-0c4457109194",
    "prId" : 33258,
    "prUrl" : "https://github.com/apache/spark/pull/33258#pullrequestreview-704475565",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e5684cd7-9043-4f1e-8244-89102cdd281a",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "do we really need a timezone to get current local time?",
        "createdAt" : "2021-07-12T18:41:11Z",
        "updatedAt" : "2021-07-12T18:41:11Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "04e8ef42-7989-46f3-9c49-3e209a26b2d8",
        "parentId" : "e5684cd7-9043-4f1e-8244-89102cdd281a",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "@cloud-fan Yes,I think so. If you don't specify tz, the `LocalDateTime.now()` will take the default JVM time zone, see https://github.com/apache/spark/pull/33258#discussion_r667378378",
        "createdAt" : "2021-07-12T19:39:02Z",
        "updatedAt" : "2021-07-12T19:48:30Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "998089cd0f1c2368e5a2b2773651836561a0a209",
    "line" : 46,
    "diffHunk" : "@@ -1,1 +234,238 @@    copy(timeZoneId = Option(timeZoneId))\n\n  override def eval(input: InternalRow): Any = localDateTimeToMicros(LocalDateTime.now(zoneId))\n\n  override def prettyName: String = \"localtimestamp\""
  },
  {
    "id" : "dce31f3c-37fa-47e2-979b-73650d1a8669",
    "prId" : 32995,
    "prUrl" : "https://github.com/apache/spark/pull/32995#pullrequestreview-688653603",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c51df2f6-c8bc-4dd4-804c-704d03ea1595",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Shall we put this logic in `TimestampFormatterHelper`?\r\n```\r\nfinal protected def getFormatter(fmt: String): TimestampFormatter = {\r\n  if (forTimestampWithoutTZ) new Iso8601TimestampFormatter(...) else ...\r\n}\r\n```",
        "createdAt" : "2021-06-21T16:31:21Z",
        "updatedAt" : "2021-06-21T16:31:21Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "c53469dd-5714-4227-8944-577af558ec85",
        "parentId" : "c51df2f6-c8bc-4dd4-804c-704d03ea1595",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "Well I prefer to put all the logic in `TimestampFormatter.getFormatter`",
        "createdAt" : "2021-06-21T16:39:18Z",
        "updatedAt" : "2021-06-21T16:39:18Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "d5afd6abdb6eb56e91b2d95b0fecad6468d2a64b",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +68,72 @@\n  // Whether the timestamp formatter is for TimestampWithoutTZType.\n  // If yes, the formatter is always `Iso8601TimestampFormatter`.\n  protected def forTimestampWithoutTZ: Boolean = false\n"
  },
  {
    "id" : "8928a812-8768-4b96-9c25-a6879c29cf28",
    "prId" : 32949,
    "prUrl" : "https://github.com/apache/spark/pull/32949#pullrequestreview-686246121",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f1ef51de-7180-4a89-9351-3cea6e99b0c0",
        "parentId" : null,
        "authorId" : "3a227965-84e0-47cf-9974-11293764f028",
        "body" : "`days` cannot be listed with `year` or `months`. So I change this part.",
        "createdAt" : "2021-06-17T12:31:27Z",
        "updatedAt" : "2021-06-17T12:31:27Z",
        "lastEditedBy" : "3a227965-84e0-47cf-9974-11293764f028",
        "tags" : [
        ]
      }
    ],
    "commit" : "48f080fc5b3bb2a2d704a0cc6bddb8d6be56f328",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +2527,2531 @@      > SELECT _FUNC_('SECONDS', timestamp'2019-10-01 00:00:01.000001');\n       1.000001\n      > SELECT _FUNC_('days', interval 5 days 3 hours 7 minutes);\n       5\n      > SELECT _FUNC_('seconds', interval 5 hours 30 seconds 1 milliseconds 1 microseconds);"
  },
  {
    "id" : "629519ec-ec19-48fe-bcb5-aedf7d8a0441",
    "prId" : 32849,
    "prUrl" : "https://github.com/apache/spark/pull/32849#pullrequestreview-681836349",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a3d389f8-0502-4b8f-8566-5562fa19815f",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "TODO: return interval second?",
        "createdAt" : "2021-06-11T09:35:43Z",
        "updatedAt" : "2021-06-11T09:35:43Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "cc384402-2440-4b3f-83f0-6fbfde9ff193",
        "parentId" : "a3d389f8-0502-4b8f-8566-5562fa19815f",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Here, any interval types that ends to SECOND works. Why not the default one: INTERVAL DAY TO SECOND?",
        "createdAt" : "2021-06-11T09:45:02Z",
        "updatedAt" : "2021-06-11T09:45:02Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "ea350db0-4948-496f-ac53-22d9af574484",
        "parentId" : "a3d389f8-0502-4b8f-8566-5562fa19815f",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "because interval second is the most precise one here. ",
        "createdAt" : "2021-06-11T13:33:45Z",
        "updatedAt" : "2021-06-11T13:33:45Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "fa00741c5319ae91dfcc3cf30beb2d2fbb427c64",
    "line" : 32,
    "diffHunk" : "@@ -1,1 +2551,2555 @@  override def inputTypes: Seq[AbstractDataType] = Seq(TimestampType, TimestampType)\n  override def dataType: DataType =\n    if (legacyInterval) CalendarIntervalType else DayTimeIntervalType()\n\n  override def withTimeZone(timeZoneId: String): TimeZoneAwareExpression ="
  },
  {
    "id" : "cad3a48d-b56c-48be-b710-f40bd9a05823",
    "prId" : 32262,
    "prUrl" : "https://github.com/apache/spark/pull/32262#pullrequestreview-640770378",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cab6d2d4-76a6-413c-b8c2-5ce821d17507",
        "parentId" : null,
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "Quick question, why `SubtractTimestamps` extends `BinaryExpression` instead of `BinaryOperator`? This reminds me the similar code in `BinaryOperator`",
        "createdAt" : "2021-04-20T16:41:02Z",
        "updatedAt" : "2021-04-20T18:02:52Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "745be525-281d-4a6d-9e00-3f508ab09538",
        "parentId" : "cab6d2d4-76a6-413c-b8c2-5ce821d17507",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "I don't have good answer to your question. Probably, we could refactor SubtractDates and SubtractTimestamps.",
        "createdAt" : "2021-04-20T17:33:43Z",
        "updatedAt" : "2021-04-20T18:02:52Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "feb741be-85f6-4349-bd78-af68d93b8cb2",
        "parentId" : "cab6d2d4-76a6-413c-b8c2-5ce821d17507",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Here is the PR https://github.com/apache/spark/pull/32267. Please, review it.",
        "createdAt" : "2021-04-21T08:32:45Z",
        "updatedAt" : "2021-04-21T08:32:45Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "9bdfe66b61b74f4d5c83984eef1b07c6dd665628",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +2558,2562 @@\n  override def toString: String = s\"($left - $right)\"\n  override def sql: String = s\"(${left.sql} - ${right.sql})\"\n\n  override protected def withNewChildrenInternal("
  },
  {
    "id" : "5024f1e2-741a-43e4-9f62-16d52863afbf",
    "prId" : 32176,
    "prUrl" : "https://github.com/apache/spark/pull/32176#pullrequestreview-636676078",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5ad0e6ec-2bc1-473a-b611-b716a32f885e",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "SPARK-35090 for ANSI intervals",
        "createdAt" : "2021-04-15T13:19:02Z",
        "updatedAt" : "2021-04-16T11:24:15Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "7668405ebd902cea698b76c373d0c06b1e759426",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +2413,2417 @@       1.000001\n      > SET spark.sql.legacy.interval.enabled=true;\n       spark.sql.legacy.interval.enabled\ttrue\n      > SELECT _FUNC_('days', interval 1 year 10 months 5 days);\n       5"
  },
  {
    "id" : "6f55027d-9cc2-4088-8f15-ff87ca9c7cab",
    "prId" : 32176,
    "prUrl" : "https://github.com/apache/spark/pull/32176#pullrequestreview-636676867",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "18e3a6a5-e203-40a2-9e9c-cde8979421d6",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Created SPARK-35091 to support ANSI intervals by `date_part()`",
        "createdAt" : "2021-04-15T13:19:47Z",
        "updatedAt" : "2021-04-16T11:24:15Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "7668405ebd902cea698b76c373d0c06b1e759426",
    "line" : 43,
    "diffHunk" : "@@ -1,1 +2483,2487 @@      > SELECT _FUNC_(SECONDS FROM timestamp'2019-10-01 00:00:01.000001');\n       1.000001\n      > SET spark.sql.legacy.interval.enabled=true;\n       spark.sql.legacy.interval.enabled\ttrue\n      > SELECT _FUNC_(days FROM interval 1 year 10 months 5 days);"
  },
  {
    "id" : "229eb851-d2f4-4943-bed1-fb01aed892f9",
    "prId" : 31832,
    "prUrl" : "https://github.com/apache/spark/pull/31832#pullrequestreview-611954715",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "72daba1b-58b3-44d4-8881-f267b6d05fd1",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Shall we make it general and accept both year-month and day-time intervals? The implementation can have 2 branches:\r\n```\r\noverride def nullSafeEval(micros: Any, months: Any): Any = interval.dataType match {\r\n  case YearMonthIntervalType =>\r\n    timestampAddMonths(micros.asInstanceOf[Long], months.asInstanceOf[Int], zoneId)\r\n  case DayTimeIntervalType => ...\r\n}\r\n```",
        "createdAt" : "2021-03-15T08:53:07Z",
        "updatedAt" : "2021-03-15T08:53:07Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "91437c3e-3ddf-446d-a113-d63f87c35ad4",
        "parentId" : "72daba1b-58b3-44d4-8881-f267b6d05fd1",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "I plan to support `timestamp + day-time interval` separately in another PR. The implementation itself is not complex, need to write more tests. ",
        "createdAt" : "2021-03-15T08:59:46Z",
        "updatedAt" : "2021-03-15T08:59:46Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "d8069abf0e59ff9f94c277c7eebb3609ce2dda74",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1523,1527 @@\n// Adds the year-month interval to the timestamp\ncase class TimestampAddYMInterval(\n    timestamp: Expression,\n    interval: Expression,"
  },
  {
    "id" : "6d466fbb-0586-4a6c-a7e0-f01bcd9e5a1b",
    "prId" : 31000,
    "prUrl" : "https://github.com/apache/spark/pull/31000#pullrequestreview-562047981",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c5810a96-08c7-4e44-9048-727e7a1d4bac",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Please, fix indentation according to https://github.com/databricks/scala-style-guide#spacing-and-indentation",
        "createdAt" : "2021-01-05T07:43:52Z",
        "updatedAt" : "2021-01-05T19:29:02Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "b401d8e0-3d54-497a-b9c2-a3233234a1e6",
        "parentId" : "c5810a96-08c7-4e44-9048-727e7a1d4bac",
        "authorId" : "e1bdd418-d9b9-4129-91a0-0865740d24d2",
        "body" : "I read the scala-style-guide and wasn't able to decipher the best practice for this specific scenario.  I opened [a scala-style-guide issue to clarify](https://github.com/databricks/scala-style-guide/issues/81).  Do you think this is better?\r\n\r\n```scala\r\n  extends BinaryExpression with ImplicitCastInputTypes with NullIntolerant\r\n  with TimeZoneAwareExpression {\r\n```\r\n\r\nI'm glad you made the comment because it's really important to manage whitespace consistently throughout the codebase.",
        "createdAt" : "2021-01-05T18:55:39Z",
        "updatedAt" : "2021-01-05T19:29:02Z",
        "lastEditedBy" : "e1bdd418-d9b9-4129-91a0-0865740d24d2",
        "tags" : [
        ]
      }
    ],
    "commit" : "ac476573bfe788d0e306c05cc0cff2ed9adb1294",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +1451,1455 @@case class AddHours(startTime: Expression, numHours: Expression, timeZoneId: Option[String] = None)\n  extends BinaryExpression with ImplicitCastInputTypes with NullIntolerant\n    with TimeZoneAwareExpression {\n\n  def this(startTime: Expression, numHours: Expression) = this(startTime, numHours, None)"
  },
  {
    "id" : "d2572016-727c-4f86-b5d5-3a28d883bdda",
    "prId" : 30807,
    "prUrl" : "https://github.com/apache/spark/pull/30807#pullrequestreview-560814360",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c249f1b7-474b-48b7-a79c-2ff0c9b69d19",
        "parentId" : null,
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "nit: is this useful given that `failOnError` already has a default value?",
        "createdAt" : "2020-12-29T20:57:29Z",
        "updatedAt" : "2021-01-04T08:23:29Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "6d382317-758e-4aa6-b233-fda84ba53988",
        "parentId" : "c249f1b7-474b-48b7-a79c-2ff0c9b69d19",
        "authorId" : "3cb8df92-1760-4e0e-94a5-f3ccadcc2882",
        "body" : "We do need this otherwise unit tests will fail for exemple this one: `sql/testOnly *SQLQueryTestSuite -- -z datetime.sql`\r\n\r\nSimilar PRs implemented this as well, foe exemple https://github.com/apache/spark/pull/30297/files#diff-da163d97a5f0fc534aad719c4a39eca97116f25bfc05b7d8941b342a3ed96036R1930\r\n\r\nI can't really well explain why it is necessary here. Someone can share some  idea here?",
        "createdAt" : "2020-12-29T21:43:16Z",
        "updatedAt" : "2021-01-04T08:23:29Z",
        "lastEditedBy" : "3cb8df92-1760-4e0e-94a5-f3ccadcc2882",
        "tags" : [
        ]
      },
      {
        "id" : "09a30eb9-6e6c-4470-8e6a-85de898e45e8",
        "parentId" : "c249f1b7-474b-48b7-a79c-2ff0c9b69d19",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "The `FunctionRegistry` requires a standard class constructor, not the `apply` method of case class.",
        "createdAt" : "2020-12-30T05:18:04Z",
        "updatedAt" : "2021-01-04T08:23:29Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "4445787a-5aac-4089-98cf-1d61cdd7c500",
        "parentId" : "c249f1b7-474b-48b7-a79c-2ff0c9b69d19",
        "authorId" : "3cb8df92-1760-4e0e-94a5-f3ccadcc2882",
        "body" : "thx!",
        "createdAt" : "2021-01-04T07:11:06Z",
        "updatedAt" : "2021-01-04T08:23:29Z",
        "lastEditedBy" : "3cb8df92-1760-4e0e-94a5-f3ccadcc2882",
        "tags" : [
        ]
      }
    ],
    "commit" : "e458546a7ddbe66a2f69c6d264961334d6d400a2",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +1185,1189 @@  override def right: Expression = dayOfWeek\n\n  def this(left: Expression, right: Expression) = this(left, right, SQLConf.get.ansiEnabled)\n\n  override def inputTypes: Seq[AbstractDataType] = Seq(DateType, StringType)"
  },
  {
    "id" : "cf6a6f81-42f9-49dd-aba1-258c7912afe0",
    "prId" : 30442,
    "prUrl" : "https://github.com/apache/spark/pull/30442#pullrequestreview-539742185",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2947c318-43a6-433e-9165-f2082c53cceb",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "can we update the `nullability` based on the `failOnError` flag?",
        "createdAt" : "2020-11-25T13:15:22Z",
        "updatedAt" : "2020-11-27T10:14:54Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "c56c254a-e27a-470a-ae68-59ed77a7d502",
        "parentId" : "2947c318-43a6-433e-9165-f2082c53cceb",
        "authorId" : "cce1c782-0596-44b6-8b99-6b77d2cca53c",
        "body" : "done.",
        "createdAt" : "2020-11-27T06:55:22Z",
        "updatedAt" : "2020-11-27T10:14:54Z",
        "lastEditedBy" : "cce1c782-0596-44b6-8b99-6b77d2cca53c",
        "tags" : [
        ]
      }
    ],
    "commit" : "e4fe5ee34b47bb5f57aad76fa41caa2c27eee53d",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +723,727 @@    timeZoneId: Option[String] = None,\n    failOnError: Boolean = SQLConf.get.ansiEnabled)\n  extends UnixTime {\n\n  def this(timeExp: Expression, format: Expression) ="
  },
  {
    "id" : "919cbf0c-08b5-422f-a075-4ee15c8e771f",
    "prId" : 30400,
    "prUrl" : "https://github.com/apache/spark/pull/30400#pullrequestreview-535869920",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "db39245e-68b4-4b6d-bbfd-70de91cb34ff",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "You need to add this func in the ignored set: https://github.com/apache/spark/blob/master/sql/core/src/test/scala/org/apache/spark/sql/expressions/ExpressionInfoSuite.scala#L148",
        "createdAt" : "2020-11-17T23:22:39Z",
        "updatedAt" : "2020-11-22T06:16:55Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "9076ddfc-2302-4769-b64a-7916790f5375",
        "parentId" : "db39245e-68b4-4b6d-bbfd-70de91cb34ff",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "We don't need to print `an offset from UTC (e.g., +08:35)`? It seems presto/sql server do so (IMO printing it looks meaningful).",
        "createdAt" : "2020-11-18T11:55:10Z",
        "updatedAt" : "2020-11-22T06:16:55Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "0d7c64b0-02e5-4bb4-a9f7-33531c7c974f",
        "parentId" : "db39245e-68b4-4b6d-bbfd-70de91cb34ff",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "Presto seems return the zone id, not see the offset. (checked both use and code).",
        "createdAt" : "2020-11-18T13:22:44Z",
        "updatedAt" : "2020-11-22T06:16:55Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      },
      {
        "id" : "c27975d1-9a9a-40c4-ba7f-970647fff2cf",
        "parentId" : "db39245e-68b4-4b6d-bbfd-70de91cb34ff",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "@ulysses-you . Oh, interesting. Does it mean Presto documentation has a bug?\r\n- https://prestodb.io/docs/current/functions/datetime.html\r\n\r\n> Returns the current time zone in the format defined by IANA (e.g., America/Los_Angeles) or as fixed offset from UTC (e.g., +08:35)",
        "createdAt" : "2020-11-20T18:54:49Z",
        "updatedAt" : "2020-11-22T06:16:55Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "9d42755a-5937-4683-9dfe-b382ee939f6d",
        "parentId" : "db39245e-68b4-4b6d-bbfd-70de91cb34ff",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "I also read it and feel a little confused. Maybe it depend on the location since the doc said `zone or offset`.",
        "createdAt" : "2020-11-21T01:46:17Z",
        "updatedAt" : "2020-11-22T06:16:55Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      }
    ],
    "commit" : "c83da46530ad1d4c5527a05e94c7fbad3eafba84",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +79,83 @@    Examples:\n      > SELECT _FUNC_();\n       Asia/Shanghai\n  \"\"\",\n  group = \"datetime_funcs\","
  },
  {
    "id" : "a3782892-144b-4ab7-9d2a-30df2a1d34d0",
    "prId" : 30400,
    "prUrl" : "https://github.com/apache/spark/pull/30400#pullrequestreview-533094192",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9ee61e16-1ed1-488a-a161-e25de7871cc7",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Please update the golden file, too. https://github.com/apache/spark/blob/master/sql/core/src/test/scala/org/apache/spark/sql/ExpressionsSchemaSuite.scala#L38-L41",
        "createdAt" : "2020-11-17T23:23:55Z",
        "updatedAt" : "2020-11-22T06:16:55Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "907ad416-7e04-48b6-a302-d1d9afabd4e9",
        "parentId" : "9ee61e16-1ed1-488a-a161-e25de7871cc7",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "thanks for the remind, get it.",
        "createdAt" : "2020-11-18T04:22:23Z",
        "updatedAt" : "2020-11-22T06:16:55Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      }
    ],
    "commit" : "c83da46530ad1d4c5527a05e94c7fbad3eafba84",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +83,87 @@  group = \"datetime_funcs\",\n  since = \"3.1.0\")\ncase class CurrentTimeZone() extends LeafExpression with Unevaluable {\n  override def nullable: Boolean = false\n  override def dataType: DataType = StringType"
  },
  {
    "id" : "a3b72a45-0a80-429a-9b13-a0172b67fb64",
    "prId" : 28894,
    "prUrl" : "https://github.com/apache/spark/pull/28894#pullrequestreview-435134941",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "32f55702-fb45-41fc-be95-7bc9a1120570",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "When I made it as `abstract class` with 2 args `func` and `funcName`, I got the following exception on tests from `DateExpressionsSuite`:\r\n```\r\norg.apache.spark.sql.catalyst.expressions.Second; no valid constructor\r\njava.io.InvalidClassException: org.apache.spark.sql.catalyst.expressions.Second; no valid constructor\r\n\tat java.io.ObjectStreamClass$ExceptionInfo.newInvalidClassException(ObjectStreamClass.java:169)\r\n\tat java.io.ObjectStreamClass.checkDeserialize(ObjectStreamClass.java:885)\r\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2100)\r\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\r\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)\r\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:423)\r\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)\r\n\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:109)\r\n\tat org.apache.spark.sql.catalyst.expressions.ExpressionEvalHelper.prepareEvaluation(ExpressionEvalHelper.scala:75)\r\n```\r\nI decided to leave it as a trait.",
        "createdAt" : "2020-06-22T17:31:02Z",
        "updatedAt" : "2020-06-22T17:33:06Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "f2a278e90b2578552d960aba700e874c41f2bebe",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +275,279 @@}\n\ntrait GetTimeField extends UnaryExpression\n  with TimeZoneAwareExpression with ImplicitCastInputTypes with NullIntolerant {\n"
  },
  {
    "id" : "3eb36246-aef1-4d92-8483-bfbd1cd32f21",
    "prId" : 28886,
    "prUrl" : "https://github.com/apache/spark/pull/28886#pullrequestreview-434916112",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "139b53d8-e9b2-4089-8b33-986bd7ecf475",
        "parentId" : null,
        "authorId" : "c96c0fe4-f4e4-4569-bb4e-7736f9be7711",
        "body" : "I doubt that this is the right place to ask, but I tried to Google this but didn't find anything. I was just wondering what does `assert(secAndMicros.scale == 6` do exactly? I am assuming that it changes the Decimal's scale to 6, but what does that exactly help with? And is the added tail just a bunch of zeros? For instance does 3.14 become 3.140000?\r\nThanks!",
        "createdAt" : "2020-06-21T23:27:59Z",
        "updatedAt" : "2020-06-21T23:27:59Z",
        "lastEditedBy" : "c96c0fe4-f4e4-4569-bb4e-7736f9be7711",
        "tags" : [
        ]
      },
      {
        "id" : "bd693e7e-a199-4eed-9190-6f78ae127227",
        "parentId" : "139b53d8-e9b2-4089-8b33-986bd7ecf475",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "> I am assuming that it changes the Decimal's scale to 6\r\n\r\nThis assert doesn't change the scale, it just reads it.\r\n\r\n> And is the added tail just a bunch of zeros? For instance does 3.14 become 3.140000?\r\n\r\nIt shifts the decimal point. For example, if you have 3.14 with precision 6 and scale 3 that means 003.140. If you set\r\n- scale to 0, it becomes 3140\r\n- scale to 4 -> 0.314",
        "createdAt" : "2020-06-22T07:18:14Z",
        "updatedAt" : "2020-06-22T07:18:15Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "1805c9f4-48c3-47c2-b4c5-1dcc62dcf8db",
        "parentId" : "139b53d8-e9b2-4089-8b33-986bd7ecf475",
        "authorId" : "c96c0fe4-f4e4-4569-bb4e-7736f9be7711",
        "body" : "Awesome, thank you so much!",
        "createdAt" : "2020-06-22T13:21:54Z",
        "updatedAt" : "2020-06-22T13:21:54Z",
        "lastEditedBy" : "c96c0fe4-f4e4-4569-bb4e-7736f9be7711",
        "tags" : [
        ]
      }
    ],
    "commit" : "b9e2ee097bdbe6c6ff1d5680025ade7cc9fca9e7",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +1998,2002 @@      zoneId: ZoneId): Any = {\n    try {\n      assert(secAndMicros.scale == 6,\n        s\"Seconds fraction must have 6 digits for microseconds but got ${secAndMicros.scale}\")\n      val unscaledSecFrac = secAndMicros.toUnscaledLong"
  }
]