[
  {
    "id" : "67ff9743-018c-446d-bd63-fd72823a8e99",
    "prId" : 33775,
    "prUrl" : "https://github.com/apache/spark/pull/33775#pullrequestreview-732493139",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "14288cdd-1ae4-4928-852c-9ebf6ef96de7",
        "parentId" : null,
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "Add comment for why choosing `DecimalType(16, 6))`",
        "createdAt" : "2021-08-18T06:30:44Z",
        "updatedAt" : "2021-08-18T06:30:44Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "434c4f41-6abc-46cb-ba49-f9522eb54fe0",
        "parentId" : "14288cdd-1ae4-4928-852c-9ebf6ef96de7",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "OK",
        "createdAt" : "2021-08-18T06:52:08Z",
        "updatedAt" : "2021-08-18T06:52:08Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "147429f919dc0824d64de976fcd8f52652050d13",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +2561,2565 @@  // casted into decimal safely, we use DecimalType(16, 6) which is wider than DecimalType(10, 0).\n  override def inputTypes: Seq[AbstractDataType] =\n    Seq(IntegerType, IntegerType, IntegerType, IntegerType, IntegerType, DecimalType(16, 6)) ++\n      timezone.map(_ => StringType)\n  override def nullable: Boolean = if (failOnError) children.exists(_.nullable) else true"
  },
  {
    "id" : "125d7949-8940-4cd1-bc4e-2fc57029ce80",
    "prId" : 33290,
    "prUrl" : "https://github.com/apache/spark/pull/33290#pullrequestreview-703581578",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9c1d1c39-2fdd-4a02-8082-ea96a94dec64",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Please, use TIMESTAMP_TYPE.key. Using the SQL config has some benefits like:\r\n- Easer to search in IDE by config name\r\n- Refactoring. Maybe, we will rename it in the future. So, you will not need to modify this place.",
        "createdAt" : "2021-07-11T10:23:23Z",
        "updatedAt" : "2021-07-11T10:32:40Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "d0f1fdde-7d6b-4c3e-b5ad-ff3ccf96de57",
        "parentId" : "9c1d1c39-2fdd-4a02-8082-ea96a94dec64",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "There will be errors:\r\n```\r\nannotation argument needs to be a constant; found: \"_FUNC_(year, month, day, hour, min, sec[, timezone]) - Create timestamp from year, month, day, hour, min, sec and timezone fields. \".+(scala.StringContext.apply(\"The result data type is consistent ...\r\n```",
        "createdAt" : "2021-07-11T12:14:40Z",
        "updatedAt" : "2021-07-11T12:14:40Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "abce601cba887bc66040dfb4b658cc051ccd289c",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +2288,2292 @@@ExpressionDescription(\n  usage = \"_FUNC_(year, month, day, hour, min, sec[, timezone]) - Create timestamp from year, month, day, hour, min, sec and timezone fields. \" +\n    \"The result data type is consistent with the value of configuration `spark.sql.timestampType`\",\n  arguments = \"\"\"\n    Arguments:"
  },
  {
    "id" : "f7e0fe81-3467-4e5c-ace6-2fb8d97aa7d9",
    "prId" : 33280,
    "prUrl" : "https://github.com/apache/spark/pull/33280#pullrequestreview-703581608",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bd8e0dc0-9513-468a-a499-309375b6f363",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Use TIMESTAMP_TYPE.key",
        "createdAt" : "2021-07-11T08:35:05Z",
        "updatedAt" : "2021-07-11T08:35:41Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "e11a7943-b80b-44f3-aba7-eee33b430123",
        "parentId" : "bd8e0dc0-9513-468a-a499-309375b6f363",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "There will be errors:\r\n```\r\nannotation argument needs to be a constant\r\n```",
        "createdAt" : "2021-07-11T12:15:00Z",
        "updatedAt" : "2021-07-11T12:15:00Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "4c11b7f373e5d108634888745c7fa3bcb8c1abf4",
    "line" : 48,
    "diffHunk" : "@@ -1,1 +1911,1915 @@      to a timestamp. Returns null with invalid input. By default, it follows casting rules to\n      a timestamp if the `fmt` is omitted. The result data type is consistent with the value of\n      configuration `spark.sql.timestampType`.\n  \"\"\",\n  arguments = \"\"\""
  },
  {
    "id" : "de985542-fbda-4d5b-997a-7afba7c0897f",
    "prId" : 33258,
    "prUrl" : "https://github.com/apache/spark/pull/33258#pullrequestreview-701909940",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "964d12d7-9a37-4e17-ad6d-2b165afbf4d1",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "\r\n```suggestion\r\n  since = \"3.3.0\")\r\n```\r\n",
        "createdAt" : "2021-07-08T10:42:29Z",
        "updatedAt" : "2021-07-08T10:42:30Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "998089cd0f1c2368e5a2b2773651836561a0a209",
    "line" : 30,
    "diffHunk" : "@@ -1,1 +218,222 @@  \"\"\",\n  group = \"datetime_funcs\",\n  since = \"3.2.0\")\ncase class LocalTimestamp(timeZoneId: Option[String] = None) extends LeafExpression\n  with TimeZoneAwareExpression with CodegenFallback {"
  },
  {
    "id" : "f24b8ec0-187b-438e-944c-c817614197de",
    "prId" : 33258,
    "prUrl" : "https://github.com/apache/spark/pull/33258#pullrequestreview-704714981",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2a713bc1-7a17-4186-b5a9-072270f23be8",
        "parentId" : null,
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "Shall we add a new test case for the code change here?",
        "createdAt" : "2021-07-12T11:02:13Z",
        "updatedAt" : "2021-07-12T11:02:13Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "f474dea8-7299-40bf-bbed-8e6e8542787f",
        "parentId" : "2a713bc1-7a17-4186-b5a9-072270f23be8",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "I wonder how it is related to new function? If it is not related, I would exclude this from the PR, and open new PR with a test.",
        "createdAt" : "2021-07-12T14:21:12Z",
        "updatedAt" : "2021-07-12T14:21:12Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "a1768190-a03a-4228-9bd2-0f06ade07f37",
        "parentId" : "2a713bc1-7a17-4186-b5a9-072270f23be8",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "it's related: https://github.com/apache/spark/pull/33258/files#diff-cf2dc65d70aa7bc490d237f1c50f5fdf51e244fabed782977a94d2934c208551R565",
        "createdAt" : "2021-07-12T18:42:03Z",
        "updatedAt" : "2021-07-12T18:42:04Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "0f03b7ce-1c45-4895-a529-9e8994a9c470",
        "parentId" : "2a713bc1-7a17-4186-b5a9-072270f23be8",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "I will add new test case in this PR.",
        "createdAt" : "2021-07-13T03:36:12Z",
        "updatedAt" : "2021-07-13T07:24:19Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "998089cd0f1c2368e5a2b2773651836561a0a209",
    "line" : 59,
    "diffHunk" : "@@ -1,1 +276,280 @@      case _: TimestampType => Literal(timestampUs, TimestampType)\n      case _: TimestampNTZType =>\n        Literal(convertTz(timestampUs, ZoneOffset.UTC, zoneId), TimestampNTZType)\n      case _: DateType => Literal(microsToDays(timestampUs, zoneId), DateType)\n    }"
  },
  {
    "id" : "c8a80053-0c1d-41c6-a11a-0c4457109194",
    "prId" : 33258,
    "prUrl" : "https://github.com/apache/spark/pull/33258#pullrequestreview-704475565",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e5684cd7-9043-4f1e-8244-89102cdd281a",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "do we really need a timezone to get current local time?",
        "createdAt" : "2021-07-12T18:41:11Z",
        "updatedAt" : "2021-07-12T18:41:11Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "04e8ef42-7989-46f3-9c49-3e209a26b2d8",
        "parentId" : "e5684cd7-9043-4f1e-8244-89102cdd281a",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "@cloud-fan Yes,I think so. If you don't specify tz, the `LocalDateTime.now()` will take the default JVM time zone, see https://github.com/apache/spark/pull/33258#discussion_r667378378",
        "createdAt" : "2021-07-12T19:39:02Z",
        "updatedAt" : "2021-07-12T19:48:30Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "998089cd0f1c2368e5a2b2773651836561a0a209",
    "line" : 46,
    "diffHunk" : "@@ -1,1 +234,238 @@    copy(timeZoneId = Option(timeZoneId))\n\n  override def eval(input: InternalRow): Any = localDateTimeToMicros(LocalDateTime.now(zoneId))\n\n  override def prettyName: String = \"localtimestamp\""
  },
  {
    "id" : "dce31f3c-37fa-47e2-979b-73650d1a8669",
    "prId" : 32995,
    "prUrl" : "https://github.com/apache/spark/pull/32995#pullrequestreview-688653603",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c51df2f6-c8bc-4dd4-804c-704d03ea1595",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Shall we put this logic in `TimestampFormatterHelper`?\r\n```\r\nfinal protected def getFormatter(fmt: String): TimestampFormatter = {\r\n  if (forTimestampWithoutTZ) new Iso8601TimestampFormatter(...) else ...\r\n}\r\n```",
        "createdAt" : "2021-06-21T16:31:21Z",
        "updatedAt" : "2021-06-21T16:31:21Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "c53469dd-5714-4227-8944-577af558ec85",
        "parentId" : "c51df2f6-c8bc-4dd4-804c-704d03ea1595",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "Well I prefer to put all the logic in `TimestampFormatter.getFormatter`",
        "createdAt" : "2021-06-21T16:39:18Z",
        "updatedAt" : "2021-06-21T16:39:18Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "d5afd6abdb6eb56e91b2d95b0fecad6468d2a64b",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +68,72 @@\n  // Whether the timestamp formatter is for TimestampWithoutTZType.\n  // If yes, the formatter is always `Iso8601TimestampFormatter`.\n  protected def forTimestampWithoutTZ: Boolean = false\n"
  },
  {
    "id" : "8928a812-8768-4b96-9c25-a6879c29cf28",
    "prId" : 32949,
    "prUrl" : "https://github.com/apache/spark/pull/32949#pullrequestreview-686246121",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f1ef51de-7180-4a89-9351-3cea6e99b0c0",
        "parentId" : null,
        "authorId" : "3a227965-84e0-47cf-9974-11293764f028",
        "body" : "`days` cannot be listed with `year` or `months`. So I change this part.",
        "createdAt" : "2021-06-17T12:31:27Z",
        "updatedAt" : "2021-06-17T12:31:27Z",
        "lastEditedBy" : "3a227965-84e0-47cf-9974-11293764f028",
        "tags" : [
        ]
      }
    ],
    "commit" : "48f080fc5b3bb2a2d704a0cc6bddb8d6be56f328",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +2527,2531 @@      > SELECT _FUNC_('SECONDS', timestamp'2019-10-01 00:00:01.000001');\n       1.000001\n      > SELECT _FUNC_('days', interval 5 days 3 hours 7 minutes);\n       5\n      > SELECT _FUNC_('seconds', interval 5 hours 30 seconds 1 milliseconds 1 microseconds);"
  },
  {
    "id" : "629519ec-ec19-48fe-bcb5-aedf7d8a0441",
    "prId" : 32849,
    "prUrl" : "https://github.com/apache/spark/pull/32849#pullrequestreview-681836349",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a3d389f8-0502-4b8f-8566-5562fa19815f",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "TODO: return interval second?",
        "createdAt" : "2021-06-11T09:35:43Z",
        "updatedAt" : "2021-06-11T09:35:43Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "cc384402-2440-4b3f-83f0-6fbfde9ff193",
        "parentId" : "a3d389f8-0502-4b8f-8566-5562fa19815f",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Here, any interval types that ends to SECOND works. Why not the default one: INTERVAL DAY TO SECOND?",
        "createdAt" : "2021-06-11T09:45:02Z",
        "updatedAt" : "2021-06-11T09:45:02Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "ea350db0-4948-496f-ac53-22d9af574484",
        "parentId" : "a3d389f8-0502-4b8f-8566-5562fa19815f",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "because interval second is the most precise one here. ",
        "createdAt" : "2021-06-11T13:33:45Z",
        "updatedAt" : "2021-06-11T13:33:45Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "fa00741c5319ae91dfcc3cf30beb2d2fbb427c64",
    "line" : 32,
    "diffHunk" : "@@ -1,1 +2551,2555 @@  override def inputTypes: Seq[AbstractDataType] = Seq(TimestampType, TimestampType)\n  override def dataType: DataType =\n    if (legacyInterval) CalendarIntervalType else DayTimeIntervalType()\n\n  override def withTimeZone(timeZoneId: String): TimeZoneAwareExpression ="
  },
  {
    "id" : "cad3a48d-b56c-48be-b710-f40bd9a05823",
    "prId" : 32262,
    "prUrl" : "https://github.com/apache/spark/pull/32262#pullrequestreview-640770378",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cab6d2d4-76a6-413c-b8c2-5ce821d17507",
        "parentId" : null,
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "Quick question, why `SubtractTimestamps` extends `BinaryExpression` instead of `BinaryOperator`? This reminds me the similar code in `BinaryOperator`",
        "createdAt" : "2021-04-20T16:41:02Z",
        "updatedAt" : "2021-04-20T18:02:52Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "745be525-281d-4a6d-9e00-3f508ab09538",
        "parentId" : "cab6d2d4-76a6-413c-b8c2-5ce821d17507",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "I don't have good answer to your question. Probably, we could refactor SubtractDates and SubtractTimestamps.",
        "createdAt" : "2021-04-20T17:33:43Z",
        "updatedAt" : "2021-04-20T18:02:52Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "feb741be-85f6-4349-bd78-af68d93b8cb2",
        "parentId" : "cab6d2d4-76a6-413c-b8c2-5ce821d17507",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Here is the PR https://github.com/apache/spark/pull/32267. Please, review it.",
        "createdAt" : "2021-04-21T08:32:45Z",
        "updatedAt" : "2021-04-21T08:32:45Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "9bdfe66b61b74f4d5c83984eef1b07c6dd665628",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +2558,2562 @@\n  override def toString: String = s\"($left - $right)\"\n  override def sql: String = s\"(${left.sql} - ${right.sql})\"\n\n  override protected def withNewChildrenInternal("
  },
  {
    "id" : "5024f1e2-741a-43e4-9f62-16d52863afbf",
    "prId" : 32176,
    "prUrl" : "https://github.com/apache/spark/pull/32176#pullrequestreview-636676078",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5ad0e6ec-2bc1-473a-b611-b716a32f885e",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "SPARK-35090 for ANSI intervals",
        "createdAt" : "2021-04-15T13:19:02Z",
        "updatedAt" : "2021-04-16T11:24:15Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "7668405ebd902cea698b76c373d0c06b1e759426",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +2413,2417 @@       1.000001\n      > SET spark.sql.legacy.interval.enabled=true;\n       spark.sql.legacy.interval.enabled\ttrue\n      > SELECT _FUNC_('days', interval 1 year 10 months 5 days);\n       5"
  },
  {
    "id" : "6f55027d-9cc2-4088-8f15-ff87ca9c7cab",
    "prId" : 32176,
    "prUrl" : "https://github.com/apache/spark/pull/32176#pullrequestreview-636676867",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "18e3a6a5-e203-40a2-9e9c-cde8979421d6",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Created SPARK-35091 to support ANSI intervals by `date_part()`",
        "createdAt" : "2021-04-15T13:19:47Z",
        "updatedAt" : "2021-04-16T11:24:15Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "7668405ebd902cea698b76c373d0c06b1e759426",
    "line" : 43,
    "diffHunk" : "@@ -1,1 +2483,2487 @@      > SELECT _FUNC_(SECONDS FROM timestamp'2019-10-01 00:00:01.000001');\n       1.000001\n      > SET spark.sql.legacy.interval.enabled=true;\n       spark.sql.legacy.interval.enabled\ttrue\n      > SELECT _FUNC_(days FROM interval 1 year 10 months 5 days);"
  },
  {
    "id" : "229eb851-d2f4-4943-bed1-fb01aed892f9",
    "prId" : 31832,
    "prUrl" : "https://github.com/apache/spark/pull/31832#pullrequestreview-611954715",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "72daba1b-58b3-44d4-8881-f267b6d05fd1",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Shall we make it general and accept both year-month and day-time intervals? The implementation can have 2 branches:\r\n```\r\noverride def nullSafeEval(micros: Any, months: Any): Any = interval.dataType match {\r\n  case YearMonthIntervalType =>\r\n    timestampAddMonths(micros.asInstanceOf[Long], months.asInstanceOf[Int], zoneId)\r\n  case DayTimeIntervalType => ...\r\n}\r\n```",
        "createdAt" : "2021-03-15T08:53:07Z",
        "updatedAt" : "2021-03-15T08:53:07Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "91437c3e-3ddf-446d-a113-d63f87c35ad4",
        "parentId" : "72daba1b-58b3-44d4-8881-f267b6d05fd1",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "I plan to support `timestamp + day-time interval` separately in another PR. The implementation itself is not complex, need to write more tests. ",
        "createdAt" : "2021-03-15T08:59:46Z",
        "updatedAt" : "2021-03-15T08:59:46Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "d8069abf0e59ff9f94c277c7eebb3609ce2dda74",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1523,1527 @@\n// Adds the year-month interval to the timestamp\ncase class TimestampAddYMInterval(\n    timestamp: Expression,\n    interval: Expression,"
  },
  {
    "id" : "6d466fbb-0586-4a6c-a7e0-f01bcd9e5a1b",
    "prId" : 31000,
    "prUrl" : "https://github.com/apache/spark/pull/31000#pullrequestreview-562047981",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c5810a96-08c7-4e44-9048-727e7a1d4bac",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Please, fix indentation according to https://github.com/databricks/scala-style-guide#spacing-and-indentation",
        "createdAt" : "2021-01-05T07:43:52Z",
        "updatedAt" : "2021-01-05T19:29:02Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "b401d8e0-3d54-497a-b9c2-a3233234a1e6",
        "parentId" : "c5810a96-08c7-4e44-9048-727e7a1d4bac",
        "authorId" : "e1bdd418-d9b9-4129-91a0-0865740d24d2",
        "body" : "I read the scala-style-guide and wasn't able to decipher the best practice for this specific scenario.  I opened [a scala-style-guide issue to clarify](https://github.com/databricks/scala-style-guide/issues/81).  Do you think this is better?\r\n\r\n```scala\r\n  extends BinaryExpression with ImplicitCastInputTypes with NullIntolerant\r\n  with TimeZoneAwareExpression {\r\n```\r\n\r\nI'm glad you made the comment because it's really important to manage whitespace consistently throughout the codebase.",
        "createdAt" : "2021-01-05T18:55:39Z",
        "updatedAt" : "2021-01-05T19:29:02Z",
        "lastEditedBy" : "e1bdd418-d9b9-4129-91a0-0865740d24d2",
        "tags" : [
        ]
      }
    ],
    "commit" : "ac476573bfe788d0e306c05cc0cff2ed9adb1294",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +1451,1455 @@case class AddHours(startTime: Expression, numHours: Expression, timeZoneId: Option[String] = None)\n  extends BinaryExpression with ImplicitCastInputTypes with NullIntolerant\n    with TimeZoneAwareExpression {\n\n  def this(startTime: Expression, numHours: Expression) = this(startTime, numHours, None)"
  },
  {
    "id" : "d2572016-727c-4f86-b5d5-3a28d883bdda",
    "prId" : 30807,
    "prUrl" : "https://github.com/apache/spark/pull/30807#pullrequestreview-560814360",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c249f1b7-474b-48b7-a79c-2ff0c9b69d19",
        "parentId" : null,
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "nit: is this useful given that `failOnError` already has a default value?",
        "createdAt" : "2020-12-29T20:57:29Z",
        "updatedAt" : "2021-01-04T08:23:29Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "6d382317-758e-4aa6-b233-fda84ba53988",
        "parentId" : "c249f1b7-474b-48b7-a79c-2ff0c9b69d19",
        "authorId" : "3cb8df92-1760-4e0e-94a5-f3ccadcc2882",
        "body" : "We do need this otherwise unit tests will fail for exemple this one: `sql/testOnly *SQLQueryTestSuite -- -z datetime.sql`\r\n\r\nSimilar PRs implemented this as well, foe exemple https://github.com/apache/spark/pull/30297/files#diff-da163d97a5f0fc534aad719c4a39eca97116f25bfc05b7d8941b342a3ed96036R1930\r\n\r\nI can't really well explain why it is necessary here. Someone can share some  idea here?",
        "createdAt" : "2020-12-29T21:43:16Z",
        "updatedAt" : "2021-01-04T08:23:29Z",
        "lastEditedBy" : "3cb8df92-1760-4e0e-94a5-f3ccadcc2882",
        "tags" : [
        ]
      },
      {
        "id" : "09a30eb9-6e6c-4470-8e6a-85de898e45e8",
        "parentId" : "c249f1b7-474b-48b7-a79c-2ff0c9b69d19",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "The `FunctionRegistry` requires a standard class constructor, not the `apply` method of case class.",
        "createdAt" : "2020-12-30T05:18:04Z",
        "updatedAt" : "2021-01-04T08:23:29Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "4445787a-5aac-4089-98cf-1d61cdd7c500",
        "parentId" : "c249f1b7-474b-48b7-a79c-2ff0c9b69d19",
        "authorId" : "3cb8df92-1760-4e0e-94a5-f3ccadcc2882",
        "body" : "thx!",
        "createdAt" : "2021-01-04T07:11:06Z",
        "updatedAt" : "2021-01-04T08:23:29Z",
        "lastEditedBy" : "3cb8df92-1760-4e0e-94a5-f3ccadcc2882",
        "tags" : [
        ]
      }
    ],
    "commit" : "e458546a7ddbe66a2f69c6d264961334d6d400a2",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +1185,1189 @@  override def right: Expression = dayOfWeek\n\n  def this(left: Expression, right: Expression) = this(left, right, SQLConf.get.ansiEnabled)\n\n  override def inputTypes: Seq[AbstractDataType] = Seq(DateType, StringType)"
  },
  {
    "id" : "cf6a6f81-42f9-49dd-aba1-258c7912afe0",
    "prId" : 30442,
    "prUrl" : "https://github.com/apache/spark/pull/30442#pullrequestreview-539742185",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2947c318-43a6-433e-9165-f2082c53cceb",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "can we update the `nullability` based on the `failOnError` flag?",
        "createdAt" : "2020-11-25T13:15:22Z",
        "updatedAt" : "2020-11-27T10:14:54Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "c56c254a-e27a-470a-ae68-59ed77a7d502",
        "parentId" : "2947c318-43a6-433e-9165-f2082c53cceb",
        "authorId" : "cce1c782-0596-44b6-8b99-6b77d2cca53c",
        "body" : "done.",
        "createdAt" : "2020-11-27T06:55:22Z",
        "updatedAt" : "2020-11-27T10:14:54Z",
        "lastEditedBy" : "cce1c782-0596-44b6-8b99-6b77d2cca53c",
        "tags" : [
        ]
      }
    ],
    "commit" : "e4fe5ee34b47bb5f57aad76fa41caa2c27eee53d",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +723,727 @@    timeZoneId: Option[String] = None,\n    failOnError: Boolean = SQLConf.get.ansiEnabled)\n  extends UnixTime {\n\n  def this(timeExp: Expression, format: Expression) ="
  },
  {
    "id" : "919cbf0c-08b5-422f-a075-4ee15c8e771f",
    "prId" : 30400,
    "prUrl" : "https://github.com/apache/spark/pull/30400#pullrequestreview-535869920",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "db39245e-68b4-4b6d-bbfd-70de91cb34ff",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "You need to add this func in the ignored set: https://github.com/apache/spark/blob/master/sql/core/src/test/scala/org/apache/spark/sql/expressions/ExpressionInfoSuite.scala#L148",
        "createdAt" : "2020-11-17T23:22:39Z",
        "updatedAt" : "2020-11-22T06:16:55Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "9076ddfc-2302-4769-b64a-7916790f5375",
        "parentId" : "db39245e-68b4-4b6d-bbfd-70de91cb34ff",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "We don't need to print `an offset from UTC (e.g., +08:35)`? It seems presto/sql server do so (IMO printing it looks meaningful).",
        "createdAt" : "2020-11-18T11:55:10Z",
        "updatedAt" : "2020-11-22T06:16:55Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "0d7c64b0-02e5-4bb4-a9f7-33531c7c974f",
        "parentId" : "db39245e-68b4-4b6d-bbfd-70de91cb34ff",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "Presto seems return the zone id, not see the offset. (checked both use and code).",
        "createdAt" : "2020-11-18T13:22:44Z",
        "updatedAt" : "2020-11-22T06:16:55Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      },
      {
        "id" : "c27975d1-9a9a-40c4-ba7f-970647fff2cf",
        "parentId" : "db39245e-68b4-4b6d-bbfd-70de91cb34ff",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "@ulysses-you . Oh, interesting. Does it mean Presto documentation has a bug?\r\n- https://prestodb.io/docs/current/functions/datetime.html\r\n\r\n> Returns the current time zone in the format defined by IANA (e.g., America/Los_Angeles) or as fixed offset from UTC (e.g., +08:35)",
        "createdAt" : "2020-11-20T18:54:49Z",
        "updatedAt" : "2020-11-22T06:16:55Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "9d42755a-5937-4683-9dfe-b382ee939f6d",
        "parentId" : "db39245e-68b4-4b6d-bbfd-70de91cb34ff",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "I also read it and feel a little confused. Maybe it depend on the location since the doc said `zone or offset`.",
        "createdAt" : "2020-11-21T01:46:17Z",
        "updatedAt" : "2020-11-22T06:16:55Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      }
    ],
    "commit" : "c83da46530ad1d4c5527a05e94c7fbad3eafba84",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +79,83 @@    Examples:\n      > SELECT _FUNC_();\n       Asia/Shanghai\n  \"\"\",\n  group = \"datetime_funcs\","
  },
  {
    "id" : "a3782892-144b-4ab7-9d2a-30df2a1d34d0",
    "prId" : 30400,
    "prUrl" : "https://github.com/apache/spark/pull/30400#pullrequestreview-533094192",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9ee61e16-1ed1-488a-a161-e25de7871cc7",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Please update the golden file, too. https://github.com/apache/spark/blob/master/sql/core/src/test/scala/org/apache/spark/sql/ExpressionsSchemaSuite.scala#L38-L41",
        "createdAt" : "2020-11-17T23:23:55Z",
        "updatedAt" : "2020-11-22T06:16:55Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "907ad416-7e04-48b6-a302-d1d9afabd4e9",
        "parentId" : "9ee61e16-1ed1-488a-a161-e25de7871cc7",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "thanks for the remind, get it.",
        "createdAt" : "2020-11-18T04:22:23Z",
        "updatedAt" : "2020-11-22T06:16:55Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      }
    ],
    "commit" : "c83da46530ad1d4c5527a05e94c7fbad3eafba84",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +83,87 @@  group = \"datetime_funcs\",\n  since = \"3.1.0\")\ncase class CurrentTimeZone() extends LeafExpression with Unevaluable {\n  override def nullable: Boolean = false\n  override def dataType: DataType = StringType"
  },
  {
    "id" : "a3b72a45-0a80-429a-9b13-a0172b67fb64",
    "prId" : 28894,
    "prUrl" : "https://github.com/apache/spark/pull/28894#pullrequestreview-435134941",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "32f55702-fb45-41fc-be95-7bc9a1120570",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "When I made it as `abstract class` with 2 args `func` and `funcName`, I got the following exception on tests from `DateExpressionsSuite`:\r\n```\r\norg.apache.spark.sql.catalyst.expressions.Second; no valid constructor\r\njava.io.InvalidClassException: org.apache.spark.sql.catalyst.expressions.Second; no valid constructor\r\n\tat java.io.ObjectStreamClass$ExceptionInfo.newInvalidClassException(ObjectStreamClass.java:169)\r\n\tat java.io.ObjectStreamClass.checkDeserialize(ObjectStreamClass.java:885)\r\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2100)\r\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\r\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)\r\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:423)\r\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)\r\n\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:109)\r\n\tat org.apache.spark.sql.catalyst.expressions.ExpressionEvalHelper.prepareEvaluation(ExpressionEvalHelper.scala:75)\r\n```\r\nI decided to leave it as a trait.",
        "createdAt" : "2020-06-22T17:31:02Z",
        "updatedAt" : "2020-06-22T17:33:06Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "f2a278e90b2578552d960aba700e874c41f2bebe",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +275,279 @@}\n\ntrait GetTimeField extends UnaryExpression\n  with TimeZoneAwareExpression with ImplicitCastInputTypes with NullIntolerant {\n"
  },
  {
    "id" : "3eb36246-aef1-4d92-8483-bfbd1cd32f21",
    "prId" : 28886,
    "prUrl" : "https://github.com/apache/spark/pull/28886#pullrequestreview-434916112",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "139b53d8-e9b2-4089-8b33-986bd7ecf475",
        "parentId" : null,
        "authorId" : "c96c0fe4-f4e4-4569-bb4e-7736f9be7711",
        "body" : "I doubt that this is the right place to ask, but I tried to Google this but didn't find anything. I was just wondering what does `assert(secAndMicros.scale == 6` do exactly? I am assuming that it changes the Decimal's scale to 6, but what does that exactly help with? And is the added tail just a bunch of zeros? For instance does 3.14 become 3.140000?\r\nThanks!",
        "createdAt" : "2020-06-21T23:27:59Z",
        "updatedAt" : "2020-06-21T23:27:59Z",
        "lastEditedBy" : "c96c0fe4-f4e4-4569-bb4e-7736f9be7711",
        "tags" : [
        ]
      },
      {
        "id" : "bd693e7e-a199-4eed-9190-6f78ae127227",
        "parentId" : "139b53d8-e9b2-4089-8b33-986bd7ecf475",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "> I am assuming that it changes the Decimal's scale to 6\r\n\r\nThis assert doesn't change the scale, it just reads it.\r\n\r\n> And is the added tail just a bunch of zeros? For instance does 3.14 become 3.140000?\r\n\r\nIt shifts the decimal point. For example, if you have 3.14 with precision 6 and scale 3 that means 003.140. If you set\r\n- scale to 0, it becomes 3140\r\n- scale to 4 -> 0.314",
        "createdAt" : "2020-06-22T07:18:14Z",
        "updatedAt" : "2020-06-22T07:18:15Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "1805c9f4-48c3-47c2-b4c5-1dcc62dcf8db",
        "parentId" : "139b53d8-e9b2-4089-8b33-986bd7ecf475",
        "authorId" : "c96c0fe4-f4e4-4569-bb4e-7736f9be7711",
        "body" : "Awesome, thank you so much!",
        "createdAt" : "2020-06-22T13:21:54Z",
        "updatedAt" : "2020-06-22T13:21:54Z",
        "lastEditedBy" : "c96c0fe4-f4e4-4569-bb4e-7736f9be7711",
        "tags" : [
        ]
      }
    ],
    "commit" : "b9e2ee097bdbe6c6ff1d5680025ade7cc9fca9e7",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +1998,2002 @@      zoneId: ZoneId): Any = {\n    try {\n      assert(secAndMicros.scale == 6,\n        s\"Seconds fraction must have 6 digits for microseconds but got ${secAndMicros.scale}\")\n      val unscaledSecFrac = secAndMicros.toUnscaledLong"
  },
  {
    "id" : "44f89cd3-6dca-4cd2-bc97-648f2c872db2",
    "prId" : 28650,
    "prUrl" : "https://github.com/apache/spark/pull/28650#pullrequestreview-419906731",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "005a1f51-c119-40ca-a0bd-1528e0790b44",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "shall we still use `NonFatal` here to be safe?",
        "createdAt" : "2020-05-28T07:46:03Z",
        "updatedAt" : "2020-06-09T10:32:20Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "ce98413c-edb8-425c-96a8-1fc721c44b86",
        "parentId" : "005a1f51-c119-40ca-a0bd-1528e0790b44",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "Should it be better to be consistent with the codegen mode?",
        "createdAt" : "2020-05-28T07:52:30Z",
        "updatedAt" : "2020-06-09T10:32:20Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "9ebf15b8-6e74-4875-bec1-656a816bf28c",
        "parentId" : "005a1f51-c119-40ca-a0bd-1528e0790b44",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "ah it's to match the codegen version. LGTM",
        "createdAt" : "2020-05-28T08:52:55Z",
        "updatedAt" : "2020-06-09T10:32:20Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "8139dfc1d064ab7b1b1d7be1af0c8f903d3e6b13",
    "line" : 182,
    "diffHunk" : "@@ -1,1 +908,912 @@              formatter.parse(t.asInstanceOf[UTF8String].toString) / downScaleFactor\n            } catch {\n              case _: DateTimeParseException |\n                   _: DateTimeException |\n                   _: ParseException => null"
  },
  {
    "id" : "a966286a-7a3e-42a0-931a-4739116d2eac",
    "prId" : 28650,
    "prUrl" : "https://github.com/apache/spark/pull/28650#pullrequestreview-420259859",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fdc01b33-aa9d-4456-979e-a54028b3515d",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "shall we use `nullSafeCodeGen`?",
        "createdAt" : "2020-05-28T14:29:11Z",
        "updatedAt" : "2020-06-09T10:32:20Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "7f799a9a-9f0f-4a6a-98d9-d4510bf17b1c",
        "parentId" : "fdc01b33-aa9d-4456-979e-a54028b3515d",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "`defineCodeGen` seems enough here as a shorthand for  `nullSafeCodeGen `.",
        "createdAt" : "2020-05-28T15:04:48Z",
        "updatedAt" : "2020-06-09T10:32:20Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "87db3535-f12b-401c-85aa-f0c506e9eea6",
        "parentId" : "fdc01b33-aa9d-4456-979e-a54028b3515d",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "what if the `seconds` is null?",
        "createdAt" : "2020-05-28T15:43:41Z",
        "updatedAt" : "2020-06-09T10:32:20Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "6d234431-d5ae-4b4d-9730-47256a291dbc",
        "parentId" : "fdc01b33-aa9d-4456-979e-a54028b3515d",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "```scala\r\n /**\r\n   * Short hand for generating binary evaluation code.\r\n   * If either of the sub-expressions is null, the result of this computation\r\n   * is assumed to be null.\r\n   *\r\n   * @param f accepts two variable names and returns Java code to compute the output.\r\n   */\r\n  protected def defineCodeGen(\r\n      ctx: CodegenContext,\r\n      ev: ExprCode,\r\n      f: (String, String) => String): ExprCode = {\r\n    nullSafeCodeGen(ctx, ev, (eval1, eval2) => {\r\n      s\"${ev.value} = ${f(eval1, eval2)};\"\r\n    })\r\n  }\r\n```\r\nit is just a wrapper for `nullSafeCodeGen`",
        "createdAt" : "2020-05-28T15:49:25Z",
        "updatedAt" : "2020-06-09T10:32:20Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "236a186f-d900-4c04-a885-4eb4deb370df",
        "parentId" : "fdc01b33-aa9d-4456-979e-a54028b3515d",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "ah i see",
        "createdAt" : "2020-05-28T15:53:01Z",
        "updatedAt" : "2020-06-09T10:32:20Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "3f0a050b-ff07-4139-b2aa-adeb43cd7846",
        "parentId" : "fdc01b33-aa9d-4456-979e-a54028b3515d",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "FYI https://github.com/apache/spark/pull/28650/files#diff-b83497f7bc11578a0b63a814a2a30f48R744",
        "createdAt" : "2020-05-28T15:57:34Z",
        "updatedAt" : "2020-06-09T10:32:20Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "8139dfc1d064ab7b1b1d7be1af0c8f903d3e6b13",
    "line" : 369,
    "diffHunk" : "@@ -1,1 +1040,1044 @@    formatterOption.map { f =>\n      val formatterName = ctx.addReferenceObj(\"formatter\", f)\n      defineCodeGen(ctx, ev, (seconds, _) =>\n        s\"UTF8String.fromString($formatterName.format($seconds * 1000000L))\")\n    }.getOrElse {"
  },
  {
    "id" : "e27d87a0-4692-41db-91be-78c4f67f79e6",
    "prId" : 28650,
    "prUrl" : "https://github.com/apache/spark/pull/28650#pullrequestreview-420246069",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f75294b7-a4e6-47f3-9706-4c46526fd380",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "ditto",
        "createdAt" : "2020-05-28T15:43:54Z",
        "updatedAt" : "2020-06-09T10:32:20Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "8139dfc1d064ab7b1b1d7be1af0c8f903d3e6b13",
    "line" : 376,
    "diffHunk" : "@@ -1,1 +1046,1050 @@      val ldf = LegacyDateFormats.getClass.getName.stripSuffix(\"$\")\n      val zid = ctx.addReferenceObj(\"zoneId\", zoneId, classOf[ZoneId].getName)\n      defineCodeGen(ctx, ev, (seconds, format) =>\n        s\"\"\"\n           |UTF8String.fromString("
  },
  {
    "id" : "b4ea886d-77e6-44c8-b571-6dd4f14818d1",
    "prId" : 28650,
    "prUrl" : "https://github.com/apache/spark/pull/28650#pullrequestreview-426910004",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "91a3b1c1-123d-4a36-9d81-def7560c9945",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "do we have expressions that create DateFormatter?",
        "createdAt" : "2020-06-09T08:33:48Z",
        "updatedAt" : "2020-06-09T10:32:20Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "0d131a90-9803-4a2d-87b2-a12acdb85315",
        "parentId" : "91a3b1c1-123d-4a36-9d81-def7560c9945",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "no datetime functions, only csv/json ones",
        "createdAt" : "2020-06-09T08:35:34Z",
        "updatedAt" : "2020-06-09T10:32:20Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "8139dfc1d064ab7b1b1d7be1af0c8f903d3e6b13",
    "line" : 30,
    "diffHunk" : "@@ -1,1 +55,59 @@}\n\ntrait TimestampFormatterHelper extends TimeZoneAwareExpression {\n\n  protected def formatString: Expression"
  },
  {
    "id" : "49020399-af88-4b76-adf1-e6c5975aa0c1",
    "prId" : 28650,
    "prUrl" : "https://github.com/apache/spark/pull/28650#pullrequestreview-427059157",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b0cec9b8-6453-4373-922e-3a231b59b1df",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "shall we add the `NullIntolerant` in the base class `TimestampFormatterHelper`? and it seems better to do it in a new PR.",
        "createdAt" : "2020-06-09T11:44:57Z",
        "updatedAt" : "2020-06-09T11:44:57Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "3626499e-dcba-4a65-ab3c-c8dd8e8ff11d",
        "parentId" : "b0cec9b8-6453-4373-922e-3a231b59b1df",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "Copy that.",
        "createdAt" : "2020-06-09T11:50:44Z",
        "updatedAt" : "2020-06-09T11:50:44Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "8139dfc1d064ab7b1b1d7be1af0c8f903d3e6b13",
    "line" : 283,
    "diffHunk" : "@@ -1,1 +1011,1015 @@case class FromUnixTime(sec: Expression, format: Expression, timeZoneId: Option[String] = None)\n  extends BinaryExpression with TimestampFormatterHelper with ImplicitCastInputTypes\n    with NullIntolerant {\n\n  def this(sec: Expression, format: Expression) = this(sec, format, None)"
  },
  {
    "id" : "213ae79a-de8c-45fd-8fb8-74f11ec30828",
    "prId" : 28342,
    "prUrl" : "https://github.com/apache/spark/pull/28342#pullrequestreview-400464369",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "308c6220-f0d1-44c2-a337-899e52888c76",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Do you remember when we started to support this syntax? I roughly remember it was added relatively recently. If the added version is different, it might be best to note it in `note` while we're here.",
        "createdAt" : "2020-04-26T03:34:11Z",
        "updatedAt" : "2020-04-26T06:56:52Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "b977d9a1-7502-4862-8cb4-2ce0a140d412",
        "parentId" : "308c6220-f0d1-44c2-a337-899e52888c76",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "I found the PR to implement the syntax: https://github.com/apache/spark/pull/14442\r\nSpark without the hive support has supported it since 2.0. Yea, I'll add this info as note.",
        "createdAt" : "2020-04-26T06:28:42Z",
        "updatedAt" : "2020-04-26T06:56:52Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "463c4d3f69022fa8b2f5ae34675155ca0d87bec7",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +71,75 @@      > SELECT _FUNC_();\n       2020-04-25\n      > SELECT _FUNC_;\n       2020-04-25\n  \"\"\","
  },
  {
    "id" : "ab9de445-4033-4165-acb3-566ec033f647",
    "prId" : 28342,
    "prUrl" : "https://github.com/apache/spark/pull/28342#pullrequestreview-400465260",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e1ffdebb-3581-4ff4-8b43-9bf7d6ef2165",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "nit. Do you think we can have `examples = ` for `now()` since `now` usage is different(having different class and only functional form) from `current_timestamp`? Of course, we can skip it.",
        "createdAt" : "2020-04-26T05:06:22Z",
        "updatedAt" : "2020-04-26T06:56:52Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "38fdb8ab-c0b8-4035-bcfc-1cfdbda11b9f",
        "parentId" : "e1ffdebb-3581-4ff4-8b43-9bf7d6ef2165",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Ah I see. I'll add an example for `now` in this PR, too. Actually, I think it is better to add examples for the expressions that do not have examples now.",
        "createdAt" : "2020-04-26T06:38:13Z",
        "updatedAt" : "2020-04-26T06:56:52Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "463c4d3f69022fa8b2f5ae34675155ca0d87bec7",
    "line" : 81,
    "diffHunk" : "@@ -1,1 +133,137 @@\n@ExpressionDescription(\n  usage = \"_FUNC_() - Returns the current timestamp at the start of query evaluation.\",\n  examples = \"\"\"\n    Examples:"
  },
  {
    "id" : "0391a0b8-2da3-4ad2-9352-299a56907979",
    "prId" : 28284,
    "prUrl" : "https://github.com/apache/spark/pull/28284#pullrequestreview-397837378",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7ad0c653-8fb2-4037-8b96-e500822aa3cc",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Unlike the PR title, this PR is adding `DAYOFWEEK_ISO` newly. Which platform are you referring for `DAYOFWEEK_ISO`? IIRC, @gatorsmile commented that we should not consider IBM DB i.\r\n> isodow is PostgreSQL specific but iso as a suffix is more commonly used across platforms ",
        "createdAt" : "2020-04-21T18:09:04Z",
        "updatedAt" : "2020-04-21T18:11:57Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "0a2f34bf-5c33-4ab6-a98a-0e8ea7e087e3",
        "parentId" : "7ad0c653-8fb2-4037-8b96-e500822aa3cc",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "Hi @dongjoon-hyun, for historical reasons, we have [`dayofweek`, `dow`] implemented for representing a non-ISO day-of-week and a newly added `isodow` from PostgreSQL for ISO day-of-week. Many other systems only have one week-numbering system support and use either full names or abbreviations. Things in spark become a little bit complicated. \r\n1. because of the existence of `isodow`, so we need to add iso-prefix to `dayofweek` to make a pair for it too. [`dayofweek`, `isodayofweek`, `dow` and `isodow`] \r\n2. because there are rare `iso`-prefixed systems and more systems choose `iso`-suffixed way, so we may result in [`dayofweek`, `dayofweekiso`, `dow`, `dowiso`]\r\n3. `dayofweekiso` looks nice and has use cases in the platforms listed above, e.g. snowflake, but `dowiso` looks weird and no use cases found.\r\n4. after a discussion with @cloud-fan, we have both agreed with an underscore before `iso` may look much better because `isodow` is new and there is no standard for `iso` kind of things, so this may be good for us to make it simple and clear for end-users if they are well documented too.\r\n\r\nThus, we finally result in [`dayofweek`, `dow`] for Non-ISO week-numbering system and [`dayofweek_iso`, `dow_iso`] for ISO system",
        "createdAt" : "2020-04-22T02:24:26Z",
        "updatedAt" : "2020-04-22T03:22:43Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "b6de5a90-ead2-48fe-9d65-f630f866f768",
        "parentId" : "7ad0c653-8fb2-4037-8b96-e500822aa3cc",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "@yaooqinn . Ya. It's clear for that part. Could you enumerate the name of systems which supports `DAYOFWEEK_ISO`? (except IBM DB i). I'm wondering that specifically.\r\n\r\nIt's because you wrote like `Many other systems` in the PR description.",
        "createdAt" : "2020-04-22T03:16:27Z",
        "updatedAt" : "2020-04-22T03:19:53Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "d99e04d3-72a4-4b78-a211-f8d00d7c822f",
        "parentId" : "7ad0c653-8fb2-4037-8b96-e500822aa3cc",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "Specifically, snowflake use dayofweek_iso except db2. I couldn't find more because most platforms do not have two day-of-week system implemented.",
        "createdAt" : "2020-04-22T03:49:59Z",
        "updatedAt" : "2020-04-22T03:51:31Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "e31e7d9a-dc52-4093-b6d6-738ec71f867b",
        "parentId" : "7ad0c653-8fb2-4037-8b96-e500822aa3cc",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Got it. Thanks. I understand that Snowflake is considered important. Could you add some of the above comment(https://github.com/apache/spark/pull/28284#discussion_r412621357) to the PR description then?",
        "createdAt" : "2020-04-22T03:58:15Z",
        "updatedAt" : "2020-04-22T03:58:15Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "85bed87d-b9f3-4406-9638-a8fc043eab30",
        "parentId" : "7ad0c653-8fb2-4037-8b96-e500822aa3cc",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "thanks @dongjoon-hyun. PR description updated",
        "createdAt" : "2020-04-22T04:36:16Z",
        "updatedAt" : "2020-04-22T04:36:16Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "521bc2cd45b21d01bc1e0d374f913406d1afeb42",
    "line" : 165,
    "diffHunk" : "@@ -1,1 +2005,2009 @@    case \"DAY\" | \"D\" | \"DAYS\" => DayOfMonth(source)\n    case \"DAYOFWEEK\" | \"DOW\" => DayOfWeek(source)\n    case \"DAYOFWEEK_ISO\" | \"DOW_ISO\" => Add(WeekDay(source), Literal(1))\n    case \"DOY\" => DayOfYear(source)\n    case \"HOUR\" | \"H\" | \"HOURS\" | \"HR\" | \"HRS\" => Hour(source)"
  },
  {
    "id" : "7cc18f07-6963-4433-98cd-54ccb58b4694",
    "prId" : 28248,
    "prUrl" : "https://github.com/apache/spark/pull/28248#pullrequestreview-396131417",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2aea8072-a5dd-41cb-9553-d5ba70780270",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I said that the `DOW` behavior looks more reasonable, but unfortunately, we already have `DAYOFWEEK` in Spark 2.4 and we can't change that. It's more important to keep internal consistency.",
        "createdAt" : "2020-04-20T04:26:08Z",
        "updatedAt" : "2020-04-21T05:17:18Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "10b733cf32d5875d266460659eece2435051cb17",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +2120,2124 @@    case \"WEEK\" | \"W\" | \"WEEKS\" => WeekOfYear(source)\n    case \"DAY\" | \"D\" | \"DAYS\" => DayOfMonth(source)\n    case \"DAYOFWEEK\" | \"DOW\" => DayOfWeek(source)\n    case \"ISODOW\" => Add(WeekDay(source), Literal(1))\n    case \"DOY\" => DayOfYear(source)"
  },
  {
    "id" : "f932c868-6345-488e-8237-dfffb83975bd",
    "prId" : 27537,
    "prUrl" : "https://github.com/apache/spark/pull/27537#pullrequestreview-363169775",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a41dd535-48bb-4329-9f34-16bc9d57b180",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "can we create a util function to do it? e.g.\r\n```\r\ndef parseTimestamp(formatter: TimestampFormatter,  str: String, result: LongWrapper): Boolean = {\r\n  try {\r\n    ...\r\n    result.set(...)\r\n    true\r\n  } catch {\r\n    case e: DateTimeParseException =>\r\n      if (FAIL) throw ...\r\n      else false\r\n    case NonFatal(_) => false\r\n  }\r\n}\r\n```\r\n\r\nand the caller side\r\n```\r\nval result = new LongWrapper\r\nif (parseTimestamp(formatter, str, result)) {\r\n  result.get ... // whatever operations\r\n} else {\r\n  null\r\n}\r\n```",
        "createdAt" : "2020-02-21T08:12:31Z",
        "updatedAt" : "2020-03-05T02:00:49Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "7ebcaf82-47ea-43c9-9db5-41a382ccf4b6",
        "parentId" : "a41dd535-48bb-4329-9f34-16bc9d57b180",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "and do we need to fix more places? Can we check all the places that access LEGACY_TIME_PARSER_ENABLED?",
        "createdAt" : "2020-02-21T08:15:06Z",
        "updatedAt" : "2020-03-05T02:00:49Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "61d88681-62e5-4e29-8994-9938bd308ea4",
        "parentId" : "a41dd535-48bb-4329-9f34-16bc9d57b180",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "> can we create a util function to do it?\r\n\r\nThe function demo you provided is clearer, but since we need to change both sides of code generation and normal mode, the current abstract might be better?\r\n\r\n> and do we need to fix more places?\r\n\r\nActually, in this PR, we tracked all the place which used the original config `LEGACY_TIME_PARSER_ENABLED ` and do the fix in UT. For every case that `legacy` mode have a different result with `corrected` mode, I added an explicit check in the UT. (The only exception is https://github.com/apache/spark/pull/27537/files#diff-ab881d9b6bceac0a14656dee940917f4R720, which use different format pattern in legacy and corrected mode, so I think this shouldn't be the cases we want to fix here.)\r\ncc @MaxGekk for double-checking.\r\n\r\n",
        "createdAt" : "2020-02-24T04:56:14Z",
        "updatedAt" : "2020-03-05T02:00:49Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "d01c7502ea7b4126bc6be362f1c2c20cebc5f288",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +788,792 @@          } else {\n            try {\n              formatter.parse(\n                t.asInstanceOf[UTF8String].toString) / downScaleFactor\n            } catch {"
  },
  {
    "id" : "99348e79-495f-43c9-8e43-41ce0a0d8f9b",
    "prId" : 26867,
    "prUrl" : "https://github.com/apache/spark/pull/26867#pullrequestreview-331564110",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "972d500e-77da-48d9-acc1-23310f021f5b",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Hi, @johnhany97 .\r\nPlease open a PR to master first.",
        "createdAt" : "2019-12-12T22:20:27Z",
        "updatedAt" : "2019-12-19T16:44:55Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "e89205e8-9871-4cd8-bd0c-f5eac645ba18",
        "parentId" : "972d500e-77da-48d9-acc1-23310f021f5b",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "If we bypass `master` branch, Apache Spark 3.0+ document will have the same issue.",
        "createdAt" : "2019-12-12T22:22:18Z",
        "updatedAt" : "2019-12-19T16:44:55Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "e205373987a87f25f5b192db5b3cf988cf88f88c",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +548,552 @@      * fmt - Date/time format pattern to follow. See `java.text.SimpleDateFormat` for valid date\n              and time format patterns.\n  \"\"\",\n  examples = \"\"\"\n    Examples:"
  },
  {
    "id" : "001cfa11-7e33-4eba-a9cb-396494c642b0",
    "prId" : 26702,
    "prUrl" : "https://github.com/apache/spark/pull/26702#pullrequestreview-328787837",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "25f9c4d8-039e-4f33-82bb-e3e86c4016f9",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Can you do this replacement directly in `ResolveDateTimeOverlaps`?",
        "createdAt" : "2019-11-29T04:02:35Z",
        "updatedAt" : "2019-12-03T02:27:22Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "6d5918d8-2bb8-4c62-8163-beb47da3ca63",
        "parentId" : "25f9c4d8-039e-4f33-82bb-e3e86c4016f9",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "Yes but can you tell me why Methinks it is better we have a override def sql here.",
        "createdAt" : "2019-11-29T04:25:56Z",
        "updatedAt" : "2019-12-03T02:27:22Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "f70e8ee1-b5ef-4a09-834f-c76f46e4d172",
        "parentId" : "25f9c4d8-039e-4f33-82bb-e3e86c4016f9",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Ah, I see. Better to keep the current one.",
        "createdAt" : "2019-11-29T04:34:31Z",
        "updatedAt" : "2019-12-03T02:27:22Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "e0db7146-a802-4172-8dbe-ffdb9c6f7dd8",
        "parentId" : "25f9c4d8-039e-4f33-82bb-e3e86c4016f9",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "On second thought, this replacement looks difficult to me in terms of code maintainability. I tried to implement the overlap w/o `RuntimeReplaceable` like this: https://github.com/maropu/spark/commit/4e30922822fac497950bafd150929f02577ab5a4\r\nThe implementation and the generated code looks simpler. I think we need more comments about the approach from the other's reviewers. cc: @cloud-fan @viirya ",
        "createdAt" : "2019-12-09T08:17:56Z",
        "updatedAt" : "2019-12-09T08:41:31Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "d189f2aa-80e2-4156-afc6-324af397b483",
        "parentId" : "25f9c4d8-039e-4f33-82bb-e3e86c4016f9",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "looks nice",
        "createdAt" : "2019-12-09T09:37:01Z",
        "updatedAt" : "2019-12-09T09:37:01Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "3b39ec1bbeb9d76f2f2551094feb1a7c08573f13",
    "line" : 47,
    "diffHunk" : "@@ -1,1 +2209,2213 @@                Subtract(rightStart, rightEnd), Subtract(rightEnd, rightStart))\n              LessThan(Subtract(Greatest(timePoints), Least(timePoints)),\n                Add(leftPeriod, rightPeriod))\n            }\n          )"
  },
  {
    "id" : "7aa27f00-894f-4c01-8e96-7995c4d3cd6a",
    "prId" : 26412,
    "prUrl" : "https://github.com/apache/spark/pull/26412#pullrequestreview-326650398",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "43ae5718-7e5a-47b4-806d-1ac9af204a35",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "can we add some UT in `DateExpressionsSuite` to make sure byte/short works?",
        "createdAt" : "2019-12-04T07:22:49Z",
        "updatedAt" : "2019-12-05T08:14:49Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "127c64d9-425c-4b4d-b090-7e2b8cd15e1e",
        "parentId" : "43ae5718-7e5a-47b4-806d-1ac9af204a35",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "done",
        "createdAt" : "2019-12-04T07:45:12Z",
        "updatedAt" : "2019-12-05T08:14:49Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "a44948eccea4ac3f033248babe84c8ab6792d5e0",
    "line" : 39,
    "diffHunk" : "@@ -1,1 +197,201 @@\n  override def nullSafeEval(start: Any, d: Any): Any = {\n    start.asInstanceOf[Int] - d.asInstanceOf[Number].intValue()\n  }\n"
  },
  {
    "id" : "c6eeb703-5532-4208-b828-ddc2e4f0bf7b",
    "prId" : 26134,
    "prUrl" : "https://github.com/apache/spark/pull/26134#pullrequestreview-309943576",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "64058585-f650-475d-957e-226765b24e7c",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "can we change `timestampAddInterval` to accept `CalendarInterval` as parameter? Then the caller side can be simplified: `DateTimeUtils.timestampAddInterval(start, itvl.negative())` or `DateTimeUtils.timestampAddInterval(start, itvl)`",
        "createdAt" : "2019-10-31T11:22:35Z",
        "updatedAt" : "2019-11-01T05:44:13Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "b900f751-7793-4b8b-a1b6-32e0a0e532a5",
        "parentId" : "64058585-f650-475d-957e-226765b24e7c",
        "authorId" : "58a5dec8-caad-405f-b8fe-9510071c1cb9",
        "body" : "good idea",
        "createdAt" : "2019-10-31T14:55:20Z",
        "updatedAt" : "2019-11-01T05:44:13Z",
        "lastEditedBy" : "58a5dec8-caad-405f-b8fe-9510071c1cb9",
        "tags" : [
        ]
      }
    ],
    "commit" : "2f901894eb24d7deec31947708b9afdaae2a4866",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +1206,1210 @@    val itvl = interval.asInstanceOf[CalendarInterval]\n    DateTimeUtils.timestampAddInterval(\n      start.asInstanceOf[Long], 0 - itvl.months, 0 - itvl.days, 0 - itvl.microseconds, zoneId)\n  }\n"
  },
  {
    "id" : "7b3e172f-d180-49a6-a7c3-52e4dc0a241a",
    "prId" : 26034,
    "prUrl" : "https://github.com/apache/spark/pull/26034#pullrequestreview-297811891",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "541ae64c-4bf7-4345-a68c-f42e4dddf234",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Maybe, one-liner `SQLConf.get.ansiEnabled && SQLConf.get.getConf(DIALECT) == Dialect.POSTGRESQL.toString` is enough. Please note that I suggested `Dialect.POSTGRESQL`.",
        "createdAt" : "2019-10-05T22:00:51Z",
        "updatedAt" : "2019-10-07T17:27:44Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "97be0a27-b1a5-4bbd-8ceb-2b2f7fd8132b",
        "parentId" : "541ae64c-4bf7-4345-a68c-f42e4dddf234",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "> Please note that I suggested `Dialect.POSTGRESQL`\r\n\r\nIf you do what you suggest, this will change results of `date.sql`:\r\n```diff\r\n-- !query 46\r\n SELECT f1 - date '2000-01-01' AS `Days From 2K` FROM DATE_TBL\r\n -- !query 46 schema\r\n-struct<Days From 2K:int>\r\n+struct<Days From 2K:interval>\r\n -- !query 46 output\r\n--1035\r\n--1036\r\n--1037\r\n--1400\r\n--1401\r\n--1402\r\n--1403\r\n--15542\r\n--15607\r\n-13977\r\n-14343\r\n-14710\r\n-91\r\n-92\r\n-93\r\n+interval -2 years -10 months\r\n+interval -2 years -10 months -1 days\r\n+interval -2 years -9 months -4 weeks -2 days\r\n+interval -3 years -10 months\r\n+interval -3 years -10 months -1 days\r\n+interval -3 years -10 months -2 days\r\n+interval -3 years -9 months -4 weeks -2 days\r\n+interval -42 years -6 months -2 weeks -4 days\r\n+interval -42 years -8 months -3 weeks -1 days\r\n+interval 3 months\r\n+interval 3 months 1 days\r\n+interval 3 months 2 days\r\n+interval 38 years 3 months 1 weeks\r\n+interval 39 years 3 months 1 weeks 1 days\r\n+interval 40 years 3 months 1 weeks 2 days\r\n```",
        "createdAt" : "2019-10-05T22:50:54Z",
        "updatedAt" : "2019-10-07T17:27:44Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "776987f8-dc7d-4ea3-b84c-64c6ecd32cd0",
        "parentId" : "541ae64c-4bf7-4345-a68c-f42e4dddf234",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "That test suite aims to test `ANSI` and `PostgreSQL` dialect. Given that, the regeneration will be a correct solution for the question. Or, if we want to test only `Spark` dialect there, we can turn off `ANSI` mode before that test case by adding one-line configuration. (Also, turn on back after that test case).\r\n\r\ncc @maropu , @gatorsmile , @gengliangwang .",
        "createdAt" : "2019-10-05T23:03:53Z",
        "updatedAt" : "2019-10-07T17:27:44Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "a55fe7b6e4e0d81287137b8640460e4250e789c8",
    "line" : 46,
    "diffHunk" : "@@ -1,1 +1704,1708 @@  private val returnInterval: Boolean = {\n    val isSparkDialect = SQLConf.get.getConf(DIALECT) == Dialect.SPARK.toString()\n    SQLConf.get.ansiEnabled && isSparkDialect\n  }\n  override def dataType: DataType = if (returnInterval) CalendarIntervalType else IntegerType"
  },
  {
    "id" : "15609825-caf9-4dea-a5b2-971f2ccb5c14",
    "prId" : 25981,
    "prUrl" : "https://github.com/apache/spark/pull/25981#pullrequestreview-295980427",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "92dbb22b-a42e-4778-94ca-b67891ef5ebb",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "So, the difference is only excluding the following?\r\n```\r\n\"ISOYEAR\",\r\n\"WEEK\", (\"W\", \"WEEKS\"),\r\n\"DAYOFWEEK\",\r\n\"DOW\",\r\n\"ISODOW\",\r\n\"DOY\",\r\n```",
        "createdAt" : "2019-10-01T03:33:58Z",
        "updatedAt" : "2019-10-18T07:28:33Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "c13fe585-b8c4-46b3-b6c0-68edcb2dd863",
        "parentId" : "92dbb22b-a42e-4778-94ca-b67891ef5ebb",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Yes, do you want to point out the difference only?",
        "createdAt" : "2019-10-01T05:15:31Z",
        "updatedAt" : "2019-10-18T07:28:33Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "0ff6075a-4e17-417f-a855-016d4670d6d7",
        "parentId" : "92dbb22b-a42e-4778-94ca-b67891ef5ebb",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "For now, the enumeration also looks enough. I also verified those cases in PostgreSQL.",
        "createdAt" : "2019-10-02T01:55:44Z",
        "updatedAt" : "2019-10-18T07:28:33Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "5620472a26f57590f2d5a67b24b5295980641f1c",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +2057,2061 @@                 \"MICROSECONDS\", (\"USEC\", \"USECS\", \"USECONDS\", \"MICROSECON\", \"US\"),\n                 \"EPOCH\"]\n                Supported string values of `field` for intervals are:\n                 [\"MILLENNIUM\", (\"MILLENNIA\", \"MIL\", \"MILS\"),\n                   \"CENTURY\", (\"CENTURIES\", \"C\", \"CENT\"),"
  },
  {
    "id" : "a94f8448-a456-42d7-aefc-046ce5405ab1",
    "prId" : 25981,
    "prUrl" : "https://github.com/apache/spark/pull/25981#pullrequestreview-298846287",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "297b5066-eacc-4f79-b1c4-ec648e5a527a",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "So, feel free to tell me to go look up a reference, but I am sort of confused about the semantics. The \"days\" part of \"interval 1 year 10 months 5 days\" is 5 days, not the interval days (roughly 365 + 10 *31 + 5).\r\n\r\nBut below, the \"seconds\" part of \"30 seconds 1 ms 1 us\" is 30.001001, not 30.\r\n\r\nSo what's the number of days in \"interval 1 year 10 months 5 days 12 hours\"? 5 or 5.5?",
        "createdAt" : "2019-10-07T18:33:14Z",
        "updatedAt" : "2019-10-18T07:28:33Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "868c125e-96cc-4d69-a780-479dccc3399d",
        "parentId" : "297b5066-eacc-4f79-b1c4-ec648e5a527a",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Sort of answering my own question. From PostgreSQL, at least:\r\nhttps://www.postgresql.org/docs/9.1/functions-datetime.html#FUNCTIONS-DATETIME-EXTRACT\r\n\r\n```\r\nSELECT date_part('day', TIMESTAMP '2001-02-16 20:38:40');\r\nResult: 16\r\n\r\nSELECT date_part('hour', INTERVAL '4 hours 3 minutes');\r\nResult: 4\r\n```\r\n\r\nIt seems like the answer to the second example here should be 30?\r\n\r\nI'm getting off on a tangent, but, can you specify \"interval 5 minutes 90 seconds\"? if so, what's the minute part -- 5 or 6? if you can't specify that, can you specify \"interval 90 seconds\"? if not why not?\r\n\r\nJust getting confused about the intended semantics of the date part of an interval!",
        "createdAt" : "2019-10-07T18:37:26Z",
        "updatedAt" : "2019-10-18T07:28:33Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "5eeb4827-9d88-4fc7-9377-38d9d4e32676",
        "parentId" : "297b5066-eacc-4f79-b1c4-ec648e5a527a",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "> So, feel free to tell me to go look up a reference, but I am sort of confused about the semantics.\r\n\r\nSemantic is defined by internal representation of Catalyst's `CalendarIntervalType`. Purpose of the type is to represent components of an interval not duration of interval (see the discussions in https://github.com/apache/spark/pull/25022). `CalendarIntervalType` stores the `year`, `months`, `weeks`, `days`, `hours`, `minutes`, `seconds`, `milliseconds`, `microseconds` components in 2 independent variables  - `months` and `microseconds`. Due to independence and unlimited (limited by long/int size) of the `microseconds` field, we can store more than 31 days in `microseconds`. \r\nBecause purpose of `CalendarIntervalType` is to represent interval as components but not duration, the same duration in microseconds can have multiple representations in Catalyst `INTERVAL` type. Let's say:\r\n390000000 microseconds = `interval 5 minutes 90 seconds` = `interval 6 minutes 30 seconds` = `interval 1 minute 330 seconds` and so on. \r\n\r\nWhen an user request components of an interval, we need to select one of its representation. We do that by constructing \"normalized\" representation in where number of `years` and `days` is unlimited (limited by int or long) and `months` should be in `[0..12)`, `hours` is in `[0, 12)`, `minutes` is `[0, 60)` ... `microseconds` is in `[0, 1000000)`. `seconds` and `milliseconds` are special cases because they have the fractional part as well but they limited too. `seconds` - `[0, 59.999999]`, milliseconds - `[0, 59999.999]`. This behavior is inherited from PostgreSQL. In this way, there is only one \"normalized\" representation of any interval from which the `date_part()` function extracts components.",
        "createdAt" : "2019-10-07T19:25:06Z",
        "updatedAt" : "2019-10-18T07:28:33Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "f0384a8e-2fe7-4cb9-92d8-bbbf61eb3b34",
        "parentId" : "297b5066-eacc-4f79-b1c4-ec648e5a527a",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Yeah I get why we split the months and ms part, and can see what the convenient implementation is w.r.t. the current implementation. Maybe the details of the semantics aren't that important for practical purposes.\r\n\r\nBut it does seem like this proposal doesn't match PostgreSQL in the `SELECT date_part('hour', INTERVAL '4 hours 3 minutes');` case, at least. But it's impractical to follow this behavior given the internal representation.\r\n\r\n`SELECT date_part('hour', INTERVAL '1 month 1 day');` gives 24, right? because the days are separable from the months.\r\n\r\n`SELECT date_part('month', INTERVAL '1 year 1 month');` gives 13, not 1, right? because the month part isn't separable.\r\n\r\nThis feels inconsistent. What if we construed the semantics to always mean 'the given interval in the given units'? that's consistent, but doesn't quite sound like what `date_part` does, as it's no longer a 'part'.\r\n\r\nAm I right about that so far?",
        "createdAt" : "2019-10-07T20:02:36Z",
        "updatedAt" : "2019-10-18T07:28:33Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "dba902e2-f746-4f86-bcd0-c94a89841a71",
        "parentId" : "297b5066-eacc-4f79-b1c4-ec648e5a527a",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "> But it does seem like this proposal doesn't match PostgreSQL in the `SELECT date_part('hour', INTERVAL '4 hours 3 minutes')`;\r\n\r\nIt does. In PostgreSQL:\r\n```sql\r\nmaxim=# SELECT date_part('hour', INTERVAL '4 hours 3 minutes');\r\n date_part \r\n-----------\r\n         4\r\n```\r\nIn SparkSQL:\r\n```sql\r\nspark-sql> SELECT date_part('hour', interval 4 hours 3 minutes);\r\n4\r\n```\r\n\r\n> SELECT date_part('hour', INTERVAL '1 month 1 day'); gives 24, right? because the days are separable from the months.\r\n\r\nNo, it is 0 because the range of hours is `[0, 23]`:\r\nIn PostgreSQL:\r\n```sql\r\nmaxim=# SELECT date_part('hour', INTERVAL '1 month 1 day');\r\n date_part \r\n-----------\r\n         0\r\n```\r\nIn SparkSQL:\r\n```sql\r\nspark-sql> SELECT date_part('hour', INTERVAL 1 month 1 day);\r\n0\r\n```\r\n\r\n> SELECT date_part('month', INTERVAL '1 year 1 month'); gives 13, not 1, right? because the month part isn't separable.\r\n\r\nNo, it gives 1.\r\nIn PostgreSQL:\r\n```sql\r\nmaxim=# SELECT date_part('month', INTERVAL '1 year 1 month');\r\n date_part \r\n-----------\r\n         1\r\n```\r\nIn SparkSQL:\r\n```sql\r\nspark-sql> SELECT date_part('month', INTERVAL 1 year 1 month);\r\n1\r\n```\r\n\r\n> This feels inconsistent.\r\n\r\nI cannot agree with you so far.",
        "createdAt" : "2019-10-07T20:24:28Z",
        "updatedAt" : "2019-10-18T07:28:33Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "70a4dfb8-28e0-494e-9911-2c27c942b4ee",
        "parentId" : "297b5066-eacc-4f79-b1c4-ec648e5a527a",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "I see, that's good. So as far as you know it's consistent with PostgreSQL? and correctly returns the time-unit part of an interval, not the interval in the time-unit? in that case I'm not sure why this is correct:\r\n\r\n```\r\ndate_part('seconds', interval 30 seconds 1 milliseconds 1 microseconds);\r\n       30.001001\r\n```\r\n\r\nShouldn't it be 30?",
        "createdAt" : "2019-10-07T23:04:14Z",
        "updatedAt" : "2019-10-18T07:28:33Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "4b9e9295-f5c9-4754-9499-dda36c86c990",
        "parentId" : "297b5066-eacc-4f79-b1c4-ec648e5a527a",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "> Shouldn't it be 30?\r\n\r\nI agree with you that `seconds` should be `30`, `milliseconds` should be `1`, and `microseconds` is `1` but PostgreSQL developers decided to implement this in different way. Maybe this is because `seconds`, `milliseconds` are always have the fractional part in other PostgreSQL functions.\r\n\r\nFor `seconds`, it looks reasonable but I don't know what PostgreSQL developers thought of when they defined `milliseconds` as milliseconds in a minute:\r\n```sql\r\nmaxim=# select date_part('milliseconds', interval '30 seconds 1 milliseconds 1 microseconds');\r\n date_part \r\n-----------\r\n 30001.001\r\n``` \r\n```sql\r\nmaxim=# select date_part('microseconds', interval '30 seconds 1 milliseconds 1 microseconds');\r\n date_part \r\n-----------\r\n  30001001\r\n```\r\n\r\nI interpret this as the `second`, `millisecond`, `microsecond` form one interval component with values in the range `[0, 6000000)` where 0-1 digits belongs to seconds, 2-4 digits to milliseconds, and 5-7 digits to microseconds. \r\n\r\nPrecisely follow PostgreSQL implementation is still open question for me. I believe PostgreSQL has badly designed date-time API. As you can see similar `date_part('HOUR', <interval>)` and `date_part('MILLISECONDS', <interval>)` must have similar semantic but they don't. ",
        "createdAt" : "2019-10-08T08:35:21Z",
        "updatedAt" : "2019-10-18T07:28:33Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "98af3a28-60f0-467a-b98f-d6b0ed320581",
        "parentId" : "297b5066-eacc-4f79-b1c4-ec648e5a527a",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Yeah, it seems inconsistent. Sometimes the entire interval's length is return in the given unit, sometimes just a portion of the interval matching that unit is returned. I wonder if there is any other standard to look at.\r\n\r\nDoesn't this current PR have the same consistency question, given the current test output?\r\n\r\nI'm mostly concerned with having consistent semantics, whatever they are. If it can't be done reasonably, hm, should we implement this?\r\n\r\nI'm trying to figure out the use case for \"a date part of an interval\". I can only think of cases where the interval should be converted entirely into some unit.",
        "createdAt" : "2019-10-08T12:34:41Z",
        "updatedAt" : "2019-10-18T07:28:33Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "c664ac0f-b3a8-4423-9175-526bab11fffe",
        "parentId" : "297b5066-eacc-4f79-b1c4-ec648e5a527a",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "> Sometimes the entire interval's length is return in the given unit ...\r\n\r\nThe entire interval's length is returned for `EPOCH` only, and components for other `field` values.\r\n\r\n> I wonder if there is any other standard to look at.\r\n\r\nI like java 8 approach where there are 2 types `java.time.Duration` and `java.time.Period`. The first one is to store interval duration in nanosecond precision, the second one is to store components `year`, `months`, `days`. I would reuse this model and extend the types slightly:\r\n- `DURATION` type is to store interval duration in microseconds. `long` should be enough to store difference between any supported timestamps.\r\n- `PERIOD` type should store `years`, `months`, `days`, `hours`, `minutes`, `seconds`, `milliseconds` and `microseconds`. For example, (-10 years, 5 months, -3 hours, 100 microseconds).\r\n\r\n> I'm mostly concerned with having consistent semantics, whatever they are. If it can't be done reasonably, hm, should we implement this?\r\n\r\nI don't know. From my point of semantic of each extraction field is well defined. There is a difference in implementation of this PR and PostgreSQL. As I wrote above Spark store interval in 2 variables - `months` and `microseconds`, but PostgreSQL uses 3 vars - `month`, `day` and `time` (in microseconds). In this way, `days` are independent from other components. And as consequence, `hours` are not limited by `[0, 24)`:\r\n```sql\r\nmaxim=# SELECT interval '-100 years 100 days -100 hours'; \r\n            interval             \r\n---------------------------------\r\n -100 years +100 days -100:00:00\r\n(1 row)\r\n```\r\n\r\n> I'm trying to figure out the use case for \"a date part of an interval\". I can only think of cases where the interval should be converted entirely into some unit.\r\n\r\nI can image at least 2 use cases:\r\n- Adjusting some of timestamp/date components. For example, we have `timestamp '2019-10-08 10:11:12.123456'`, and we want to adjust it by `-1 year 10 hours`. As result, we will have `timestamp '2018-10-08 20:11:12.123456'`. The `PERIOD` type could be useful here.\r\n- When we need `Ordered` (and comparable) intervals. We can calculate absolute interval duration in some units. For example, in the query:\r\n```sql\r\nSELECT name, place\r\nFROM t1, t2\r\nWHERE t1.sellTimestamp - t2.deliveredTimestamp < interval 1 month 15 days;\r\n```\r\nHere, the `DURATION` should be used. We cannot use `PERIOD` because its values cannot be ordered. Spark's CalendarIntervalType cannot be used here too. ",
        "createdAt" : "2019-10-08T15:13:45Z",
        "updatedAt" : "2019-10-18T07:28:33Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "e0475448-a940-49e0-8252-6b9b47c27ff5",
        "parentId" : "297b5066-eacc-4f79-b1c4-ec648e5a527a",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "> The entire interval's length is returned for `EPOCH` only, and components for other `field` values.\r\n\r\nHm, but then back to the original question; why is the answer not 30 below?\r\n```\r\nSELECT _FUNC_('seconds', interval 30 seconds 1 milliseconds 1 microseconds);\r\n       30.001001\r\n```\r\n\r\nI'm not concerned necessarily about matching PostgreSQL, but am concerned about internal consistency. Unless I'm really missing something, these two examples here seem to have different semantics. I understand if you mean it's the natural way to implement this given the representation, but, I'm not sure that makes it right?\r\n\r\nI think the discussion of `PERIOD` is separate. Here the question is the use case for `date_part` on an interval. I'm sort of struggling to think of when that's useful, though I accept it exists in other DBs. Your example doesn't show using `date_part`. I can imagine wanting to render an interval in given units -- what is the length of this whole interval in seconds? -- but that's not what this does, nor what the function name seems to imply.\r\n\r\nThat's why I'm questioning supporting this at all.",
        "createdAt" : "2019-10-08T15:24:35Z",
        "updatedAt" : "2019-10-18T07:28:33Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      }
    ],
    "commit" : "5620472a26f57590f2d5a67b24b5295980641f1c",
    "line" : 50,
    "diffHunk" : "@@ -1,1 +2083,2087 @@      > SELECT _FUNC_('SECONDS', timestamp'2019-10-01 00:00:01.000001');\n       1.000001\n      > SELECT _FUNC_('days', interval 1 year 10 months 5 days);\n       5\n      > SELECT _FUNC_('seconds', interval 5 hours 30 seconds 1 milliseconds 1 microseconds);"
  },
  {
    "id" : "bc5bd3c6-f7de-40e7-9150-68617fa6e804",
    "prId" : 25981,
    "prUrl" : "https://github.com/apache/spark/pull/25981#pullrequestreview-298853434",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cb052bb5-163e-44d6-ad7f-5df842aa141e",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Nit: is this missing parens, or space, after `timestamp` or am I missing something?",
        "createdAt" : "2019-10-08T15:20:15Z",
        "updatedAt" : "2019-10-18T07:28:33Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "646fc23d-7c3a-4fc1-bf36-c27279bfc3c0",
        "parentId" : "cb052bb5-163e-44d6-ad7f-5df842aa141e",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "This code is executed by the https://github.com/apache/spark/blob/7aca0dd658b8bda05574b3df3254aaf66eb2a174/sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala#L149-L197 , and this is the syntax of timestamp literals.",
        "createdAt" : "2019-10-08T15:29:53Z",
        "updatedAt" : "2019-10-18T07:28:33Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "5620472a26f57590f2d5a67b24b5295980641f1c",
    "line" : 48,
    "diffHunk" : "@@ -1,1 +2081,2085 @@      > SELECT _FUNC_('doy', DATE'2019-08-12');\n       224\n      > SELECT _FUNC_('SECONDS', timestamp'2019-10-01 00:00:01.000001');\n       1.000001\n      > SELECT _FUNC_('days', interval 1 year 10 months 5 days);"
  },
  {
    "id" : "c6b5f5ca-e4f3-461b-a31f-0d5c224aaf60",
    "prId" : 25942,
    "prUrl" : "https://github.com/apache/spark/pull/25942#pullrequestreview-293918271",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6e2ee82e-d8a3-4430-9d7f-fe324cc9f228",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Oh, surprising.",
        "createdAt" : "2019-09-26T15:58:06Z",
        "updatedAt" : "2019-09-27T05:51:10Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "73ca16c5-4559-41d1-8f91-a073f66d054f",
        "parentId" : "6e2ee82e-d8a3-4430-9d7f-fe324cc9f228",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "The `yyyy-MM-dd HH:mm:ss` pattern does not contain the time zone sub-pattern. If you point out it, you will see something like:\r\n```\r\nspark-sql> SELECT from_unixtime(0, 'yyyy-MM-dd HH:mm:ssXXX');\r\n1970-01-01 03:00:00+03:00\r\n```",
        "createdAt" : "2019-09-26T16:22:36Z",
        "updatedAt" : "2019-09-27T05:51:10Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "13e8c8a2-fb28-490e-a58f-6bc1d47fd7a9",
        "parentId" : "6e2ee82e-d8a3-4430-9d7f-fe324cc9f228",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "And you can change your current time zone to `UTC` to see `1970-01-01 00:00:00`:\r\n```sql\r\nspark-sql> set spark.sql.session.timeZone=UTC;\r\nspark.sql.session.timeZone\tUTC\r\nspark-sql> SELECT from_unixtime(0, 'yyyy-MM-dd HH:mm:ssXXX');\r\n1970-01-01 00:00:00Z\r\n```",
        "createdAt" : "2019-09-26T16:25:37Z",
        "updatedAt" : "2019-09-27T05:51:10Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "74910772-3b92-488f-abf9-15ffac47fdd6",
        "parentId" : "6e2ee82e-d8a3-4430-9d7f-fe324cc9f228",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Ya. The timezone issue will make a failure on different timezone machines.",
        "createdAt" : "2019-09-26T16:29:18Z",
        "updatedAt" : "2019-09-27T05:51:10Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "be2c070b-2f6c-405f-ac27-c5fbda9bcc07",
        "parentId" : "6e2ee82e-d8a3-4430-9d7f-fe324cc9f228",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "but the time zone is forcibly set to `\"America/Los_Angeles\"` in tests:\r\nhttps://github.com/apache/spark/blob/97dc4c0bfc3a15d364a376c6f87cb921d8d6980d/sql/core/src/test/scala/org/apache/spark/sql/QueryTest.scala#L36\r\nhttps://github.com/apache/spark/blob/a1213d5f963f6e8815bbfaff308b0a24112fff54/sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala#L1384",
        "createdAt" : "2019-09-26T18:25:26Z",
        "updatedAt" : "2019-09-27T05:51:10Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "cee6709d24c41ebd1c17bad4b2acd26b24d67d08",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +843,847 @@    Examples:\n      > SELECT _FUNC_(0, 'yyyy-MM-dd HH:mm:ss');\n       1969-12-31 16:00:00\n  \"\"\",\n  since = \"1.5.0\")"
  },
  {
    "id" : "98bdbda1-30fc-4939-8c87-af50f03b9ee5",
    "prId" : 25865,
    "prUrl" : "https://github.com/apache/spark/pull/25865#pullrequestreview-291076865",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "94813840-82d3-43a7-a138-fb1149110f12",
        "parentId" : null,
        "authorId" : "1b042d83-ef8f-494b-9256-66d2f5434320",
        "body" : "DoubleType ?",
        "createdAt" : "2019-09-20T10:02:52Z",
        "updatedAt" : "2019-09-20T10:02:52Z",
        "lastEditedBy" : "1b042d83-ef8f-494b-9256-66d2f5434320",
        "tags" : [
        ]
      },
      {
        "id" : "38b60c4c-f919-4411-bdb1-958e0bd380f0",
        "parentId" : "94813840-82d3-43a7-a138-fb1149110f12",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "See the example in the description. PostgreSQL returns `NULL` of the `double precision` type which is equivalent of `DOUBLE` type in Spark SQL. Which type would you use here?",
        "createdAt" : "2019-09-20T10:18:03Z",
        "updatedAt" : "2019-09-20T10:18:04Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "b39adaaea65a8772f2307eb5858449e0eb8c8358",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +2056,2060 @@      val fieldEval = field.eval()\n      if (fieldEval == null) {\n        Literal(null, DoubleType)\n      } else {\n        val fieldStr = fieldEval.asInstanceOf[UTF8String].toString"
  },
  {
    "id" : "08135e9b-f539-4e2d-bdcb-689f897a1026",
    "prId" : 25796,
    "prUrl" : "https://github.com/apache/spark/pull/25796#pullrequestreview-288354365",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "50759ec8-61d5-4d97-803a-525c66cfcd09",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Look at https://github.com/apache/spark/pull/25796/files#diff-b83497f7bc11578a0b63a814a2a30f48L677",
        "createdAt" : "2019-09-15T09:56:18Z",
        "updatedAt" : "2019-09-15T09:56:18Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "d021210c6ceb1dc6946a192e08c9aca563124fb7",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +648,652 @@ * See [https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatter.html].\n * Note that hive Language Manual says it returns 0 if fail, but in fact it returns null.\n * If the second parameter is missing, use \"uuuu-MM-dd HH:mm:ss\".\n * If no parameters provided, the first parameter will be current_timestamp.\n * If the first parameter is a Date or Timestamp instead of String, we will ignore the"
  },
  {
    "id" : "19d4ca6a-1905-4116-989b-11c2c496eedf",
    "prId" : 25782,
    "prUrl" : "https://github.com/apache/spark/pull/25782#pullrequestreview-288020409",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "76d6318f-2696-497d-bb0a-9fa340d4e1f3",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "How about `.getOrElse`?",
        "createdAt" : "2019-09-13T13:09:29Z",
        "updatedAt" : "2019-09-16T05:05:48Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "0f5ec81e-13d5-42d8-81c9-779876dd6c9a",
        "parentId" : "76d6318f-2696-497d-bb0a-9fa340d4e1f3",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "`.getOrElse` has some overhead of calling the lambda function. I explicitly avoided its usage in the interpreted mode. For consistency, I could do the same in the codegen function but I don't think it does matter.",
        "createdAt" : "2019-09-13T13:35:54Z",
        "updatedAt" : "2019-09-16T05:05:48Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "a9fd8e43d6ec60819fd85134bc3e11e9d4cc0357",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +600,604 @@\n  override protected def nullSafeEval(timestamp: Any, format: Any): Any = {\n    val tf = if (formatter.isEmpty) {\n      TimestampFormatter(format.toString, zoneId)\n    } else {"
  },
  {
    "id" : "adcf7d24-f451-4440-aab6-e3d24751e26e",
    "prId" : 25782,
    "prUrl" : "https://github.com/apache/spark/pull/25782#pullrequestreview-288307487",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "66f4e17a-bbf2-4307-91bc-a8d35a288cf1",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "So, do you mean we have both test coverage for this new line and the other existing part (inside `getOrElse`)? Did I understand correctly?",
        "createdAt" : "2019-09-13T22:17:05Z",
        "updatedAt" : "2019-09-16T05:05:48Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "3a509eb1-77ed-4bf6-812c-66cde64e247a",
        "parentId" : "66f4e17a-bbf2-4307-91bc-a8d35a288cf1",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Yes, I put breakpoints in IntelliJ IDEA to both part, and checked that they are covered by `DateFunctionsSuite` and `DateExpressionsSuite`.",
        "createdAt" : "2019-09-14T05:01:06Z",
        "updatedAt" : "2019-09-16T05:05:48Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "a9fd8e43d6ec60819fd85134bc3e11e9d4cc0357",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +610,614 @@  override def doGenCode(ctx: CodegenContext, ev: ExprCode): ExprCode = {\n    formatter.map { tf =>\n      val timestampFormatter = ctx.addReferenceObj(\"timestampFormatter\", tf)\n      defineCodeGen(ctx, ev, (timestamp, _) => {\n        s\"\"\"UTF8String.fromString($timestampFormatter.format($timestamp))\"\"\""
  },
  {
    "id" : "4e2be778-b0b2-4bf6-b664-05b446316169",
    "prId" : 25410,
    "prUrl" : "https://github.com/apache/spark/pull/25410#pullrequestreview-273669713",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "012c3864-8f1c-4c4f-9b21-3ba6ca28864c",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "As you see the UT failure, I enforced the function description by UT, @MaxGekk . You shall not pass. :grin: Please add a proper description annotation.",
        "createdAt" : "2019-08-12T02:45:48Z",
        "updatedAt" : "2019-09-06T06:56:57Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "a53d7584-9eef-4dce-b31d-fd3e75dd386f",
        "parentId" : "012c3864-8f1c-4c4f-9b21-3ba6ca28864c",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "I added the description. Just in case, can I run the check locally and get the error earlier?",
        "createdAt" : "2019-08-12T06:12:49Z",
        "updatedAt" : "2019-09-06T06:56:57Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "1615aed8-5801-4731-8f8b-60510cade132",
        "parentId" : "012c3864-8f1c-4c4f-9b21-3ba6ca28864c",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Of course, you can run SQL module test always when you are writing SQL PRs.",
        "createdAt" : "2019-08-12T06:22:12Z",
        "updatedAt" : "2019-09-06T06:56:57Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "9f85e4b0-8a00-464c-9b41-97194321cb72",
        "parentId" : "012c3864-8f1c-4c4f-9b21-3ba6ca28864c",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "I ran new test `date_part.sql` via:\r\n```shell\r\nbuild/sbt \"sql/test-only *SQLQueryTestSuite -- -z date_part.sql\"\r\n```\r\nbut didn't get the error.",
        "createdAt" : "2019-08-12T07:14:39Z",
        "updatedAt" : "2019-09-06T06:56:57Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "6d1b5e52-f694-4b52-a83c-93c98f81ce26",
        "parentId" : "012c3864-8f1c-4c4f-9b21-3ba6ca28864c",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "@MaxGekk . \r\n- First of all, where did you get that idea about that CLI command? The failed test case in Jekins is [SQLQuerySuite.SPARK-14415](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/108938/testReport/org.apache.spark.sql/SQLQuerySuite/SPARK_14415__All_functions_should_have_own_descriptions/), not `SQLQueryTestSuite`.\r\n- Second, what I recommended was the SQL **module** test which is `build/sbt \"project sql\" test`. \r\n\r\nSince you are one of the active SQL-area contributors in Apache Spark community, let me recommend you once more to run **the module test** at least before making a PR.\r\n- If you touch `catalyst` module, you should run `build/sbt \"project catalyst\" test` at least. \r\n- if you touch `sql` module, you should run `build/sbt \"project sql\" test` at least.",
        "createdAt" : "2019-08-12T10:14:56Z",
        "updatedAt" : "2019-09-06T06:56:57Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "d7599e0c-252a-4a3a-9a82-b7940001edec",
        "parentId" : "012c3864-8f1c-4c4f-9b21-3ba6ca28864c",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Thank you for the recommendations. I will do that next time.",
        "createdAt" : "2019-08-12T12:49:39Z",
        "updatedAt" : "2019-09-06T06:56:57Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "600eee6d8ceef00f6554b64d03aaff404917c975",
    "line" : 72,
    "diffHunk" : "@@ -1,1 +2032,2036 @@  \"\"\",\n  since = \"3.0.0\")\ncase class DatePart(field: Expression, source: Expression, child: Expression)\n  extends RuntimeReplaceable {\n"
  },
  {
    "id" : "e25b1d58-4acc-4e80-b2fb-874fbd56b189",
    "prId" : 25410,
    "prUrl" : "https://github.com/apache/spark/pull/25410#pullrequestreview-275290494",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "569a6fff-a265-48ef-bfd7-7acdb41c2465",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "This follows the standard? It seems pgSQL accept a non-foldable value;\r\n```\r\npostgres=# create table t (a text, b timestamp);\r\nCREATE TABLE\r\npostgres=# insert into t values ('year', '2019-08-12 01:00:00.123456');\r\nINSERT 0 1\r\npostgres=# select * from t;\r\n  a   |             b              \r\n------+----------------------------\r\n year | 2019-08-12 01:00:00.123456\r\n(1 row)\r\n\r\npostgres=# select date_part(a, b) from t;\r\n date_part \r\n-----------\r\n      2019\r\n(1 row)\r\n```",
        "createdAt" : "2019-08-15T00:33:46Z",
        "updatedAt" : "2019-09-06T06:56:57Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "9750a7dd-7952-480c-9726-113e15af4f38",
        "parentId" : "569a6fff-a265-48ef-bfd7-7acdb41c2465",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "According to PostgreSQL docs https://www.postgresql.org/docs/11/functions-datetime.html#FUNCTIONS-DATETIME-EXTRACT:\r\n\r\n>    _source_ must be a value expression ...  the _**field**_ parameter needs to be **a string value**\r\n\r\nAccepting _field_ as an expression is undocumented feature. We could support that separately if it is needed.  ",
        "createdAt" : "2019-08-15T06:35:59Z",
        "updatedAt" : "2019-09-06T06:56:57Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "7900145b-c7a9-449b-9318-19d44cb134f1",
        "parentId" : "569a6fff-a265-48ef-bfd7-7acdb41c2465",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "I checked the mysql behaivour and it doesn't support the case. So, its ok to keep as it is.",
        "createdAt" : "2019-08-15T07:19:29Z",
        "updatedAt" : "2019-09-06T06:56:57Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "600eee6d8ceef00f6554b64d03aaff404917c975",
    "line" : 78,
    "diffHunk" : "@@ -1,1 +2038,2042 @@    this(field, source, {\n      if (!field.foldable) {\n        throw new AnalysisException(\"The field parameter needs to be a foldable string value.\")\n      }\n      val fieldStr = field.eval().asInstanceOf[UTF8String].toString"
  },
  {
    "id" : "959f486f-fd0c-48a7-8ff7-03eb5db70ba3",
    "prId" : 25410,
    "prUrl" : "https://github.com/apache/spark/pull/25410#pullrequestreview-275778675",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "23ed9e67-a7ab-4684-98d4-d1f07f543130",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "How about using parentheses for these alternatives like this?\r\n```\r\n      * field - selects which part of the source should be extracted. Supported string values are:\r\n                [\"MILLENNIUM\" (\"MILLENNIA\", \"MIL\", \"MILS\"),\r\n                 \"CENTURY\" (\"CENTURIES\", \"C\", \"CENT\"),\r\n                 \"DECADE\" (\"DECADES\", \"DEC\", \"DECS\"),\r\n                 \"YEAR\" (\"Y\", \"YEARS\", \"YR\", \"YRS\"),\r\n                 \"ISOYEAR\",\r\n                 \"QUARTER\" (\"QTR\"),\r\n                 \"MONTH\" (\"MON\", \"MONS\", \"MONTHS\"),\r\n                 \"WEEK\" (\"W\", \"WEEKS\"),\r\n                 \"DAY\" (\"D\", \"DAYS\"),\r\n                 \"DAYOFWEEK\" (\"DOW\", \"ISODOW\", \"DOY\"),\r\n                 \"HOUR\" (\"H\", \"HOURS\", \"HR\", \"HRS\"),\r\n                 \"MINUTE\" (\"M\", \"MIN\", \"MINS\", \"MINUTES\"),\r\n                 \"SECOND\" (\"S\", \"SEC\", \"SECONDS\", \"SECS\"),\r\n                 \"MILLISECONDS\" (\"MSEC\", \"MSECS\", \"MILLISECON\", \"MSECONDS\", \"MS\"),\r\n                 \"MICROSECONDS\" (\"USEC\", \"USECS\", \"USECONDS\", \"MICROSECON\", \"US\"),\r\n                 \"EPOCH\"]\r\n```",
        "createdAt" : "2019-08-16T04:49:37Z",
        "updatedAt" : "2019-09-06T06:56:57Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "45381846-ae29-4ac9-a8ad-c4e07a80719f",
        "parentId" : "23ed9e67-a7ab-4684-98d4-d1f07f543130",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Sounds good. I will do that.",
        "createdAt" : "2019-08-16T05:22:28Z",
        "updatedAt" : "2019-09-06T06:56:57Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "600eee6d8ceef00f6554b64d03aaff404917c975",
    "line" : 59,
    "diffHunk" : "@@ -1,1 +2019,2023 @@                 \"MILLISECONDS\", (\"MSEC\", \"MSECS\", \"MILLISECON\", \"MSECONDS\", \"MS\"),\n                 \"MICROSECONDS\", (\"USEC\", \"USECS\", \"USECONDS\", \"MICROSECON\", \"US\"),\n                 \"EPOCH\"]\n      * source - a date (or timestamp) column from where `field` should be extracted\n  \"\"\","
  },
  {
    "id" : "d0229708-022a-44bc-a74d-5133280c5716",
    "prId" : 25408,
    "prUrl" : "https://github.com/apache/spark/pull/25408#pullrequestreview-273527374",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "09470b24-95fc-498a-8614-2c9c3fe9dd05",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@MaxGekk, Out of curiosity, why is it `DecimalType(20, 6)`?",
        "createdAt" : "2019-08-12T03:19:41Z",
        "updatedAt" : "2019-08-14T06:51:54Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "f9cfb0b3-1988-4072-b18b-fc4fab9387ef",
        "parentId" : "09470b24-95fc-498a-8614-2c9c3fe9dd05",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "I tried `DoubleType` but I was loosing 1 microsecond in some tests. Regarding `precision` (20) and `scale` (6).  6 was taken to have microseconds, 20 to cover supported range `[1, 9999]` of years + additional numbers to cover negative years that are out of the range ( but it would be nice to cover them).",
        "createdAt" : "2019-08-12T06:34:15Z",
        "updatedAt" : "2019-08-14T06:51:54Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "626f1ad2-5e33-4004-a6f8-94fcf9e6d734",
        "parentId" : "09470b24-95fc-498a-8614-2c9c3fe9dd05",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Could you add a comment about the above before line 1938, @MaxGekk ?",
        "createdAt" : "2019-08-12T06:48:13Z",
        "updatedAt" : "2019-08-14T06:51:54Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "153d2ff745385fcb50870d6eea939bed24da02ab",
    "line" : 85,
    "diffHunk" : "@@ -1,1 +1950,1954 @@  // the fraction. The precision 20 should cover whole valid range of years [1, 9999]\n  // plus negative years that can be used in some cases though are not officially supported.\n  override def dataType: DataType = DecimalType(20, 6)\n  override def withTimeZone(timeZoneId: String): TimeZoneAwareExpression =\n    copy(timeZoneId = Option(timeZoneId))"
  },
  {
    "id" : "f52cdec1-469e-455f-8f42-371e2758f978",
    "prId" : 25388,
    "prUrl" : "https://github.com/apache/spark/pull/25388#pullrequestreview-272832449",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "db1aada5-ec60-406c-b0a6-f16fe1903c85",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "~Could you update the title since this PR adds these function beyond `EXTRACT` syntax?~\r\nCould you remove there `ExpressionDescription` annotations?",
        "createdAt" : "2019-08-08T20:54:45Z",
        "updatedAt" : "2019-08-09T06:51:40Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "9d9a0ada9c6854dcc1951f709a737066e5b07c1d",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1830,1834 @@}\n\ncase class Millennium(child: Expression) extends UnaryExpression with ImplicitCastInputTypes {\n\n  override def inputTypes: Seq[AbstractDataType] = Seq(DateType)"
  },
  {
    "id" : "380b7011-d67e-455e-ab65-d5402b33121a",
    "prId" : 25336,
    "prUrl" : "https://github.com/apache/spark/pull/25336#pullrequestreview-272683262",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1dc37f27-1d31-4c39-99bc-9104ab2ee441",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "just for curiosity: why `century` and `millennium` need to add one year but `DECADE` doesn't need to?",
        "createdAt" : "2019-08-08T15:54:37Z",
        "updatedAt" : "2019-08-08T15:54:37Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "c87b6c02-dd2e-4ce5-9c2a-bd0ee0b592b5",
        "parentId" : "1dc37f27-1d31-4c39-99bc-9104ab2ee441",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Century (and millennium as well) starts from the first year, see https://en.wikipedia.org/wiki/Century#Viewpoint_1:_Strict_usage and https://en.wikipedia.org/wiki/Millennium#Viewpoint_1:_Strict_usage . Decades are calculated as PostgreSQL does which follows common practice: https://en.wikipedia.org/wiki/List_of_decades",
        "createdAt" : "2019-08-08T16:12:28Z",
        "updatedAt" : "2019-08-08T16:12:28Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "dd6f092f5cbc94eacca91dc8b9caba6c6b01746e",
    "line" : 72,
    "diffHunk" : "@@ -1,1 +1489,1493 @@       1901-01-01\n      > SELECT _FUNC_('1981-01-19', 'millennium');\n       1001-01-01\n  \"\"\",\n  since = \"1.5.0\")"
  },
  {
    "id" : "dc9f9995-4f91-43e0-a18f-cb033cb997a1",
    "prId" : 25220,
    "prUrl" : "https://github.com/apache/spark/pull/25220#pullrequestreview-266075688",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d1ab9c73-ae9d-4ec3-9fe4-b58d86c35b4a",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "We need timezone example, too.",
        "createdAt" : "2019-07-22T22:47:50Z",
        "updatedAt" : "2019-07-29T06:25:33Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "310a4f18-cbbc-4f3c-9871-075a25186b0d",
        "parentId" : "d1ab9c73-ae9d-4ec3-9fe4-b58d86c35b4a",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Added an example",
        "createdAt" : "2019-07-24T14:49:34Z",
        "updatedAt" : "2019-07-29T06:25:33Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "cfb8ce168675524231c3c413d4854cf840b46414",
    "line" : 32,
    "diffHunk" : "@@ -1,1 +1677,1681 @@    Examples:\n      > SELECT _FUNC_(2014, 12, 28, 6, 30, 45.887);\n       2014-12-28 06:30:45.887\n      > SELECT _FUNC_(2014, 12, 28, 6, 30, 45.887, 'CET');\n       2014-12-28 10:30:45.887"
  },
  {
    "id" : "e06ee487-8c30-4b9c-bf99-7006f64438e9",
    "prId" : 25220,
    "prUrl" : "https://github.com/apache/spark/pull/25220#pullrequestreview-266459189",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8928052e-d5c8-46f1-b215-e53cf449dc81",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "For a record, PostgreSQL `second` can be `60`.\r\n```\r\npostgres=# select make_timestamp(1,1,1,0,0,60);\r\n   make_timestamp\r\n---------------------\r\n 0001-01-01 00:01:00\r\n(1 row)\r\n```",
        "createdAt" : "2019-07-22T22:50:59Z",
        "updatedAt" : "2019-07-29T06:25:33Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "f5bfd0e4-6b5b-4f15-a064-711697a84608",
        "parentId" : "8928052e-d5c8-46f1-b215-e53cf449dc81",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "I would support the feature separately. I will open an JIRA ticket for that. ",
        "createdAt" : "2019-07-24T14:56:41Z",
        "updatedAt" : "2019-07-29T06:25:33Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "e53d73f1-df5f-4310-81c1-d644b2b9d2a2",
        "parentId" : "8928052e-d5c8-46f1-b215-e53cf449dc81",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "60 seconds have been supported",
        "createdAt" : "2019-07-25T08:01:53Z",
        "updatedAt" : "2019-07-29T06:25:33Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "cfb8ce168675524231c3c413d4854cf840b46414",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +1673,1677 @@              to 0 and 1 minute is added to the final timestamp.\n      * timezone - the time zone identifier. For example, CET, UTC and etc.\n  \"\"\",\n  examples = \"\"\"\n    Examples:"
  },
  {
    "id" : "06fb3bdc-69f6-419f-959c-1c5009b2fb60",
    "prId" : 25220,
    "prUrl" : "https://github.com/apache/spark/pull/25220#pullrequestreview-267513349",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0d16a03d-2991-453c-986b-c2ec3ca085a0",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "We should handle `seconds == 60 && nanos == 0` and `seconds == 60 && nanos > 0` should be null.",
        "createdAt" : "2019-07-28T16:21:44Z",
        "updatedAt" : "2019-07-29T06:25:33Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "cfb8ce168675524231c3c413d4854cf840b46414",
    "line" : 97,
    "diffHunk" : "@@ -1,1 +1742,1746 @@      val seconds = secAndNanos.toInt\n      val nanos = ((secAndNanos - seconds) * NANOS_PER_SECOND).toInt\n      val ldt = if (seconds == 60) {\n        if (nanos == 0) {\n          // This case of sec = 60 and nanos = 0 is supported for compatibility with PostgreSQL"
  },
  {
    "id" : "77d79b5c-f82f-466e-a03e-b5a0def1b138",
    "prId" : 25220,
    "prUrl" : "https://github.com/apache/spark/pull/25220#pullrequestreview-267513416",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "33763b71-030e-4915-8b24-1fee02cdc1cf",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "~This is used only once and the logic is simple and the overhead is big. Let's embedded it.~\r\nNever mind.",
        "createdAt" : "2019-07-28T16:24:06Z",
        "updatedAt" : "2019-07-29T06:25:33Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "cfb8ce168675524231c3c413d4854cf840b46414",
    "line" : 86,
    "diffHunk" : "@@ -1,1 +1731,1735 @@    copy(timeZoneId = Option(timeZoneId))\n\n  private def toMicros(\n      year: Int,\n      month: Int,"
  },
  {
    "id" : "4d5a8bf9-0935-4f42-97bf-406327de380a",
    "prId" : 25220,
    "prUrl" : "https://github.com/apache/spark/pull/25220#pullrequestreview-267513488",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ec683819-1420-4535-a8e0-d0bf2b18ce41",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Please change this logic, too.",
        "createdAt" : "2019-07-28T16:25:53Z",
        "updatedAt" : "2019-07-29T06:25:33Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "cfb8ce168675524231c3c413d4854cf840b46414",
    "line" : 144,
    "diffHunk" : "@@ -1,1 +1789,1793 @@        int nanos = (int)(($secAndNanos - seconds) * 1000000000L);\n        java.time.LocalDateTime ldt;\n        if (seconds == 60) {\n          if (nanos == 0) {\n            ldt = java.time.LocalDateTime.of("
  },
  {
    "id" : "0896ae11-6725-4b90-859c-c05b74d2cf40",
    "prId" : 25220,
    "prUrl" : "https://github.com/apache/spark/pull/25220#pullrequestreview-267520605",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "29f362a6-d445-4772-96ef-287236b91f81",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Ur, not exception. I mean `null` literally.",
        "createdAt" : "2019-07-28T19:54:18Z",
        "updatedAt" : "2019-07-29T06:25:33Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "bbe40b08-4d1f-4beb-ae90-4fab02083436",
        "parentId" : "29f362a6-d445-4772-96ef-287236b91f81",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Oh.. It's catched at line 1756?",
        "createdAt" : "2019-07-28T19:54:57Z",
        "updatedAt" : "2019-07-29T06:25:33Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "a11d7a29-b21a-4269-a565-abe8ffeeaaca",
        "parentId" : "29f362a6-d445-4772-96ef-287236b91f81",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Yes. The code looks better from my point of view with the exception.",
        "createdAt" : "2019-07-28T19:58:05Z",
        "updatedAt" : "2019-07-29T06:25:33Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "300192b8-4841-4e88-8e5f-07c72ec0da5f",
        "parentId" : "29f362a6-d445-4772-96ef-287236b91f81",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "`LocalDateTime.of` throws the `DateTimeException` for invalid inputs, and need to catch it in any case. Here we have some kind of similar case.",
        "createdAt" : "2019-07-28T20:02:35Z",
        "updatedAt" : "2019-07-29T06:25:33Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "5f73240b-c714-4994-a19b-a1f553301979",
        "parentId" : "29f362a6-d445-4772-96ef-287236b91f81",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Got it.",
        "createdAt" : "2019-07-28T20:09:00Z",
        "updatedAt" : "2019-07-29T06:25:33Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "cfb8ce168675524231c3c413d4854cf840b46414",
    "line" : 102,
    "diffHunk" : "@@ -1,1 +1747,1751 @@          LocalDateTime.of(year, month, day, hour, min, 0, 0).plusMinutes(1)\n        } else {\n          throw new DateTimeException(\"The fraction of sec must be zero. Valid range is [0, 60].\")\n        }\n      } else {"
  },
  {
    "id" : "5d0776c9-5536-4959-8fb3-b197f9f3ec47",
    "prId" : 25220,
    "prUrl" : "https://github.com/apache/spark/pull/25220#pullrequestreview-267527077",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "adf53d27-8718-4c4a-903e-4fac2d890b91",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Note for me. This exception message is not used.",
        "createdAt" : "2019-07-28T23:12:56Z",
        "updatedAt" : "2019-07-29T06:25:33Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "cfb8ce168675524231c3c413d4854cf840b46414",
    "line" : 102,
    "diffHunk" : "@@ -1,1 +1747,1751 @@          LocalDateTime.of(year, month, day, hour, min, 0, 0).plusMinutes(1)\n        } else {\n          throw new DateTimeException(\"The fraction of sec must be zero. Valid range is [0, 60].\")\n        }\n      } else {"
  },
  {
    "id" : "187fcd3a-63e9-4471-a0b5-f8b706a9fdf7",
    "prId" : 25210,
    "prUrl" : "https://github.com/apache/spark/pull/25210#pullrequestreview-264637897",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ee5a2351-96e7-4990-b305-bad40f12f7e9",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "Also give an example in which the inputs are invalid?",
        "createdAt" : "2019-07-21T21:01:23Z",
        "updatedAt" : "2019-07-22T08:52:40Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "a48c29e0-1364-41e2-bbee-f369954d1a4d",
        "parentId" : "ee5a2351-96e7-4990-b305-bad40f12f7e9",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Added a few examples",
        "createdAt" : "2019-07-22T07:26:21Z",
        "updatedAt" : "2019-07-22T08:52:40Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "15c64d2dfcb616334e1d054f9fdb485f1870ed81",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +1618,1622 @@    Examples:\n      > SELECT _FUNC_(2013, 7, 15);\n       2013-07-15\n      > SELECT _FUNC_(2019, 13, 1);\n       NULL"
  },
  {
    "id" : "335860ab-54e5-4c72-b68f-ffceb5f19ddb",
    "prId" : 24420,
    "prUrl" : "https://github.com/apache/spark/pull/24420#pullrequestreview-228953452",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2053b48b-32a8-4b3b-8258-801ca62fcc89",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "let's add a comment about what `downScaleFactor` means.",
        "createdAt" : "2019-04-22T04:23:14Z",
        "updatedAt" : "2019-04-22T07:38:18Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "a4cf960cce0218b15e149e860d01c2b9333fcef5",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +632,636 @@  // The result of the conversion to timestamp is microseconds divided by this factor.\n  // For example if the factor is 1000000, the result of the expression is in seconds.\n  protected def downScaleFactor: Long\n\n  override def inputTypes: Seq[AbstractDataType] ="
  }
]