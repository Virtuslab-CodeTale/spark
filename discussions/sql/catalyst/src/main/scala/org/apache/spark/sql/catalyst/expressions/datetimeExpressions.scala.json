[
  {
    "id" : "67ff9743-018c-446d-bd63-fd72823a8e99",
    "prId" : 33775,
    "prUrl" : "https://github.com/apache/spark/pull/33775#pullrequestreview-732493139",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "14288cdd-1ae4-4928-852c-9ebf6ef96de7",
        "parentId" : null,
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "Add comment for why choosing `DecimalType(16, 6))`",
        "createdAt" : "2021-08-18T06:30:44Z",
        "updatedAt" : "2021-08-18T06:30:44Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "434c4f41-6abc-46cb-ba49-f9522eb54fe0",
        "parentId" : "14288cdd-1ae4-4928-852c-9ebf6ef96de7",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "OK",
        "createdAt" : "2021-08-18T06:52:08Z",
        "updatedAt" : "2021-08-18T06:52:08Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "147429f919dc0824d64de976fcd8f52652050d13",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +2561,2565 @@  // casted into decimal safely, we use DecimalType(16, 6) which is wider than DecimalType(10, 0).\n  override def inputTypes: Seq[AbstractDataType] =\n    Seq(IntegerType, IntegerType, IntegerType, IntegerType, IntegerType, DecimalType(16, 6)) ++\n      timezone.map(_ => StringType)\n  override def nullable: Boolean = if (failOnError) children.exists(_.nullable) else true"
  },
  {
    "id" : "125d7949-8940-4cd1-bc4e-2fc57029ce80",
    "prId" : 33290,
    "prUrl" : "https://github.com/apache/spark/pull/33290#pullrequestreview-703581578",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9c1d1c39-2fdd-4a02-8082-ea96a94dec64",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Please, use TIMESTAMP_TYPE.key. Using the SQL config has some benefits like:\r\n- Easer to search in IDE by config name\r\n- Refactoring. Maybe, we will rename it in the future. So, you will not need to modify this place.",
        "createdAt" : "2021-07-11T10:23:23Z",
        "updatedAt" : "2021-07-11T10:32:40Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "d0f1fdde-7d6b-4c3e-b5ad-ff3ccf96de57",
        "parentId" : "9c1d1c39-2fdd-4a02-8082-ea96a94dec64",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "There will be errors:\r\n```\r\nannotation argument needs to be a constant; found: \"_FUNC_(year, month, day, hour, min, sec[, timezone]) - Create timestamp from year, month, day, hour, min, sec and timezone fields. \".+(scala.StringContext.apply(\"The result data type is consistent ...\r\n```",
        "createdAt" : "2021-07-11T12:14:40Z",
        "updatedAt" : "2021-07-11T12:14:40Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "abce601cba887bc66040dfb4b658cc051ccd289c",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +2288,2292 @@@ExpressionDescription(\n  usage = \"_FUNC_(year, month, day, hour, min, sec[, timezone]) - Create timestamp from year, month, day, hour, min, sec and timezone fields. \" +\n    \"The result data type is consistent with the value of configuration `spark.sql.timestampType`\",\n  arguments = \"\"\"\n    Arguments:"
  },
  {
    "id" : "f7e0fe81-3467-4e5c-ace6-2fb8d97aa7d9",
    "prId" : 33280,
    "prUrl" : "https://github.com/apache/spark/pull/33280#pullrequestreview-703581608",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bd8e0dc0-9513-468a-a499-309375b6f363",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Use TIMESTAMP_TYPE.key",
        "createdAt" : "2021-07-11T08:35:05Z",
        "updatedAt" : "2021-07-11T08:35:41Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "e11a7943-b80b-44f3-aba7-eee33b430123",
        "parentId" : "bd8e0dc0-9513-468a-a499-309375b6f363",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "There will be errors:\r\n```\r\nannotation argument needs to be a constant\r\n```",
        "createdAt" : "2021-07-11T12:15:00Z",
        "updatedAt" : "2021-07-11T12:15:00Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "4c11b7f373e5d108634888745c7fa3bcb8c1abf4",
    "line" : 48,
    "diffHunk" : "@@ -1,1 +1911,1915 @@      to a timestamp. Returns null with invalid input. By default, it follows casting rules to\n      a timestamp if the `fmt` is omitted. The result data type is consistent with the value of\n      configuration `spark.sql.timestampType`.\n  \"\"\",\n  arguments = \"\"\""
  },
  {
    "id" : "de985542-fbda-4d5b-997a-7afba7c0897f",
    "prId" : 33258,
    "prUrl" : "https://github.com/apache/spark/pull/33258#pullrequestreview-701909940",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "964d12d7-9a37-4e17-ad6d-2b165afbf4d1",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "\r\n```suggestion\r\n  since = \"3.3.0\")\r\n```\r\n",
        "createdAt" : "2021-07-08T10:42:29Z",
        "updatedAt" : "2021-07-08T10:42:30Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "998089cd0f1c2368e5a2b2773651836561a0a209",
    "line" : 30,
    "diffHunk" : "@@ -1,1 +218,222 @@  \"\"\",\n  group = \"datetime_funcs\",\n  since = \"3.2.0\")\ncase class LocalTimestamp(timeZoneId: Option[String] = None) extends LeafExpression\n  with TimeZoneAwareExpression with CodegenFallback {"
  },
  {
    "id" : "f24b8ec0-187b-438e-944c-c817614197de",
    "prId" : 33258,
    "prUrl" : "https://github.com/apache/spark/pull/33258#pullrequestreview-704714981",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2a713bc1-7a17-4186-b5a9-072270f23be8",
        "parentId" : null,
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "Shall we add a new test case for the code change here?",
        "createdAt" : "2021-07-12T11:02:13Z",
        "updatedAt" : "2021-07-12T11:02:13Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "f474dea8-7299-40bf-bbed-8e6e8542787f",
        "parentId" : "2a713bc1-7a17-4186-b5a9-072270f23be8",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "I wonder how it is related to new function? If it is not related, I would exclude this from the PR, and open new PR with a test.",
        "createdAt" : "2021-07-12T14:21:12Z",
        "updatedAt" : "2021-07-12T14:21:12Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "a1768190-a03a-4228-9bd2-0f06ade07f37",
        "parentId" : "2a713bc1-7a17-4186-b5a9-072270f23be8",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "it's related: https://github.com/apache/spark/pull/33258/files#diff-cf2dc65d70aa7bc490d237f1c50f5fdf51e244fabed782977a94d2934c208551R565",
        "createdAt" : "2021-07-12T18:42:03Z",
        "updatedAt" : "2021-07-12T18:42:04Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "0f03b7ce-1c45-4895-a529-9e8994a9c470",
        "parentId" : "2a713bc1-7a17-4186-b5a9-072270f23be8",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "I will add new test case in this PR.",
        "createdAt" : "2021-07-13T03:36:12Z",
        "updatedAt" : "2021-07-13T07:24:19Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "998089cd0f1c2368e5a2b2773651836561a0a209",
    "line" : 59,
    "diffHunk" : "@@ -1,1 +276,280 @@      case _: TimestampType => Literal(timestampUs, TimestampType)\n      case _: TimestampNTZType =>\n        Literal(convertTz(timestampUs, ZoneOffset.UTC, zoneId), TimestampNTZType)\n      case _: DateType => Literal(microsToDays(timestampUs, zoneId), DateType)\n    }"
  },
  {
    "id" : "c8a80053-0c1d-41c6-a11a-0c4457109194",
    "prId" : 33258,
    "prUrl" : "https://github.com/apache/spark/pull/33258#pullrequestreview-704475565",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e5684cd7-9043-4f1e-8244-89102cdd281a",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "do we really need a timezone to get current local time?",
        "createdAt" : "2021-07-12T18:41:11Z",
        "updatedAt" : "2021-07-12T18:41:11Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "04e8ef42-7989-46f3-9c49-3e209a26b2d8",
        "parentId" : "e5684cd7-9043-4f1e-8244-89102cdd281a",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "@cloud-fan Yes,I think so. If you don't specify tz, the `LocalDateTime.now()` will take the default JVM time zone, see https://github.com/apache/spark/pull/33258#discussion_r667378378",
        "createdAt" : "2021-07-12T19:39:02Z",
        "updatedAt" : "2021-07-12T19:48:30Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "998089cd0f1c2368e5a2b2773651836561a0a209",
    "line" : 46,
    "diffHunk" : "@@ -1,1 +234,238 @@    copy(timeZoneId = Option(timeZoneId))\n\n  override def eval(input: InternalRow): Any = localDateTimeToMicros(LocalDateTime.now(zoneId))\n\n  override def prettyName: String = \"localtimestamp\""
  },
  {
    "id" : "dce31f3c-37fa-47e2-979b-73650d1a8669",
    "prId" : 32995,
    "prUrl" : "https://github.com/apache/spark/pull/32995#pullrequestreview-688653603",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c51df2f6-c8bc-4dd4-804c-704d03ea1595",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Shall we put this logic in `TimestampFormatterHelper`?\r\n```\r\nfinal protected def getFormatter(fmt: String): TimestampFormatter = {\r\n  if (forTimestampWithoutTZ) new Iso8601TimestampFormatter(...) else ...\r\n}\r\n```",
        "createdAt" : "2021-06-21T16:31:21Z",
        "updatedAt" : "2021-06-21T16:31:21Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "c53469dd-5714-4227-8944-577af558ec85",
        "parentId" : "c51df2f6-c8bc-4dd4-804c-704d03ea1595",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "Well I prefer to put all the logic in `TimestampFormatter.getFormatter`",
        "createdAt" : "2021-06-21T16:39:18Z",
        "updatedAt" : "2021-06-21T16:39:18Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "d5afd6abdb6eb56e91b2d95b0fecad6468d2a64b",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +68,72 @@\n  // Whether the timestamp formatter is for TimestampWithoutTZType.\n  // If yes, the formatter is always `Iso8601TimestampFormatter`.\n  protected def forTimestampWithoutTZ: Boolean = false\n"
  },
  {
    "id" : "8928a812-8768-4b96-9c25-a6879c29cf28",
    "prId" : 32949,
    "prUrl" : "https://github.com/apache/spark/pull/32949#pullrequestreview-686246121",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f1ef51de-7180-4a89-9351-3cea6e99b0c0",
        "parentId" : null,
        "authorId" : "3a227965-84e0-47cf-9974-11293764f028",
        "body" : "`days` cannot be listed with `year` or `months`. So I change this part.",
        "createdAt" : "2021-06-17T12:31:27Z",
        "updatedAt" : "2021-06-17T12:31:27Z",
        "lastEditedBy" : "3a227965-84e0-47cf-9974-11293764f028",
        "tags" : [
        ]
      }
    ],
    "commit" : "48f080fc5b3bb2a2d704a0cc6bddb8d6be56f328",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +2527,2531 @@      > SELECT _FUNC_('SECONDS', timestamp'2019-10-01 00:00:01.000001');\n       1.000001\n      > SELECT _FUNC_('days', interval 5 days 3 hours 7 minutes);\n       5\n      > SELECT _FUNC_('seconds', interval 5 hours 30 seconds 1 milliseconds 1 microseconds);"
  },
  {
    "id" : "629519ec-ec19-48fe-bcb5-aedf7d8a0441",
    "prId" : 32849,
    "prUrl" : "https://github.com/apache/spark/pull/32849#pullrequestreview-681836349",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a3d389f8-0502-4b8f-8566-5562fa19815f",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "TODO: return interval second?",
        "createdAt" : "2021-06-11T09:35:43Z",
        "updatedAt" : "2021-06-11T09:35:43Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "cc384402-2440-4b3f-83f0-6fbfde9ff193",
        "parentId" : "a3d389f8-0502-4b8f-8566-5562fa19815f",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Here, any interval types that ends to SECOND works. Why not the default one: INTERVAL DAY TO SECOND?",
        "createdAt" : "2021-06-11T09:45:02Z",
        "updatedAt" : "2021-06-11T09:45:02Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "ea350db0-4948-496f-ac53-22d9af574484",
        "parentId" : "a3d389f8-0502-4b8f-8566-5562fa19815f",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "because interval second is the most precise one here. ",
        "createdAt" : "2021-06-11T13:33:45Z",
        "updatedAt" : "2021-06-11T13:33:45Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "fa00741c5319ae91dfcc3cf30beb2d2fbb427c64",
    "line" : 32,
    "diffHunk" : "@@ -1,1 +2551,2555 @@  override def inputTypes: Seq[AbstractDataType] = Seq(TimestampType, TimestampType)\n  override def dataType: DataType =\n    if (legacyInterval) CalendarIntervalType else DayTimeIntervalType()\n\n  override def withTimeZone(timeZoneId: String): TimeZoneAwareExpression ="
  },
  {
    "id" : "cad3a48d-b56c-48be-b710-f40bd9a05823",
    "prId" : 32262,
    "prUrl" : "https://github.com/apache/spark/pull/32262#pullrequestreview-640770378",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cab6d2d4-76a6-413c-b8c2-5ce821d17507",
        "parentId" : null,
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "Quick question, why `SubtractTimestamps` extends `BinaryExpression` instead of `BinaryOperator`? This reminds me the similar code in `BinaryOperator`",
        "createdAt" : "2021-04-20T16:41:02Z",
        "updatedAt" : "2021-04-20T18:02:52Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "745be525-281d-4a6d-9e00-3f508ab09538",
        "parentId" : "cab6d2d4-76a6-413c-b8c2-5ce821d17507",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "I don't have good answer to your question. Probably, we could refactor SubtractDates and SubtractTimestamps.",
        "createdAt" : "2021-04-20T17:33:43Z",
        "updatedAt" : "2021-04-20T18:02:52Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "feb741be-85f6-4349-bd78-af68d93b8cb2",
        "parentId" : "cab6d2d4-76a6-413c-b8c2-5ce821d17507",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Here is the PR https://github.com/apache/spark/pull/32267. Please, review it.",
        "createdAt" : "2021-04-21T08:32:45Z",
        "updatedAt" : "2021-04-21T08:32:45Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "9bdfe66b61b74f4d5c83984eef1b07c6dd665628",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +2558,2562 @@\n  override def toString: String = s\"($left - $right)\"\n  override def sql: String = s\"(${left.sql} - ${right.sql})\"\n\n  override protected def withNewChildrenInternal("
  },
  {
    "id" : "5024f1e2-741a-43e4-9f62-16d52863afbf",
    "prId" : 32176,
    "prUrl" : "https://github.com/apache/spark/pull/32176#pullrequestreview-636676078",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5ad0e6ec-2bc1-473a-b611-b716a32f885e",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "SPARK-35090 for ANSI intervals",
        "createdAt" : "2021-04-15T13:19:02Z",
        "updatedAt" : "2021-04-16T11:24:15Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "7668405ebd902cea698b76c373d0c06b1e759426",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +2413,2417 @@       1.000001\n      > SET spark.sql.legacy.interval.enabled=true;\n       spark.sql.legacy.interval.enabled\ttrue\n      > SELECT _FUNC_('days', interval 1 year 10 months 5 days);\n       5"
  },
  {
    "id" : "6f55027d-9cc2-4088-8f15-ff87ca9c7cab",
    "prId" : 32176,
    "prUrl" : "https://github.com/apache/spark/pull/32176#pullrequestreview-636676867",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "18e3a6a5-e203-40a2-9e9c-cde8979421d6",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Created SPARK-35091 to support ANSI intervals by `date_part()`",
        "createdAt" : "2021-04-15T13:19:47Z",
        "updatedAt" : "2021-04-16T11:24:15Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "7668405ebd902cea698b76c373d0c06b1e759426",
    "line" : 43,
    "diffHunk" : "@@ -1,1 +2483,2487 @@      > SELECT _FUNC_(SECONDS FROM timestamp'2019-10-01 00:00:01.000001');\n       1.000001\n      > SET spark.sql.legacy.interval.enabled=true;\n       spark.sql.legacy.interval.enabled\ttrue\n      > SELECT _FUNC_(days FROM interval 1 year 10 months 5 days);"
  },
  {
    "id" : "229eb851-d2f4-4943-bed1-fb01aed892f9",
    "prId" : 31832,
    "prUrl" : "https://github.com/apache/spark/pull/31832#pullrequestreview-611954715",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "72daba1b-58b3-44d4-8881-f267b6d05fd1",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Shall we make it general and accept both year-month and day-time intervals? The implementation can have 2 branches:\r\n```\r\noverride def nullSafeEval(micros: Any, months: Any): Any = interval.dataType match {\r\n  case YearMonthIntervalType =>\r\n    timestampAddMonths(micros.asInstanceOf[Long], months.asInstanceOf[Int], zoneId)\r\n  case DayTimeIntervalType => ...\r\n}\r\n```",
        "createdAt" : "2021-03-15T08:53:07Z",
        "updatedAt" : "2021-03-15T08:53:07Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "91437c3e-3ddf-446d-a113-d63f87c35ad4",
        "parentId" : "72daba1b-58b3-44d4-8881-f267b6d05fd1",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "I plan to support `timestamp + day-time interval` separately in another PR. The implementation itself is not complex, need to write more tests. ",
        "createdAt" : "2021-03-15T08:59:46Z",
        "updatedAt" : "2021-03-15T08:59:46Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "d8069abf0e59ff9f94c277c7eebb3609ce2dda74",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1523,1527 @@\n// Adds the year-month interval to the timestamp\ncase class TimestampAddYMInterval(\n    timestamp: Expression,\n    interval: Expression,"
  },
  {
    "id" : "6d466fbb-0586-4a6c-a7e0-f01bcd9e5a1b",
    "prId" : 31000,
    "prUrl" : "https://github.com/apache/spark/pull/31000#pullrequestreview-562047981",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c5810a96-08c7-4e44-9048-727e7a1d4bac",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Please, fix indentation according to https://github.com/databricks/scala-style-guide#spacing-and-indentation",
        "createdAt" : "2021-01-05T07:43:52Z",
        "updatedAt" : "2021-01-05T19:29:02Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "b401d8e0-3d54-497a-b9c2-a3233234a1e6",
        "parentId" : "c5810a96-08c7-4e44-9048-727e7a1d4bac",
        "authorId" : "e1bdd418-d9b9-4129-91a0-0865740d24d2",
        "body" : "I read the scala-style-guide and wasn't able to decipher the best practice for this specific scenario.  I opened [a scala-style-guide issue to clarify](https://github.com/databricks/scala-style-guide/issues/81).  Do you think this is better?\r\n\r\n```scala\r\n  extends BinaryExpression with ImplicitCastInputTypes with NullIntolerant\r\n  with TimeZoneAwareExpression {\r\n```\r\n\r\nI'm glad you made the comment because it's really important to manage whitespace consistently throughout the codebase.",
        "createdAt" : "2021-01-05T18:55:39Z",
        "updatedAt" : "2021-01-05T19:29:02Z",
        "lastEditedBy" : "e1bdd418-d9b9-4129-91a0-0865740d24d2",
        "tags" : [
        ]
      }
    ],
    "commit" : "ac476573bfe788d0e306c05cc0cff2ed9adb1294",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +1451,1455 @@case class AddHours(startTime: Expression, numHours: Expression, timeZoneId: Option[String] = None)\n  extends BinaryExpression with ImplicitCastInputTypes with NullIntolerant\n    with TimeZoneAwareExpression {\n\n  def this(startTime: Expression, numHours: Expression) = this(startTime, numHours, None)"
  }
]