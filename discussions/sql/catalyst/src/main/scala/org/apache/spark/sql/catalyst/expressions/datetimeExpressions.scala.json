[
  {
    "id" : "67ff9743-018c-446d-bd63-fd72823a8e99",
    "prId" : 33775,
    "prUrl" : "https://github.com/apache/spark/pull/33775#pullrequestreview-732493139",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "14288cdd-1ae4-4928-852c-9ebf6ef96de7",
        "parentId" : null,
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "Add comment for why choosing `DecimalType(16, 6))`",
        "createdAt" : "2021-08-18T06:30:44Z",
        "updatedAt" : "2021-08-18T06:30:44Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "434c4f41-6abc-46cb-ba49-f9522eb54fe0",
        "parentId" : "14288cdd-1ae4-4928-852c-9ebf6ef96de7",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "OK",
        "createdAt" : "2021-08-18T06:52:08Z",
        "updatedAt" : "2021-08-18T06:52:08Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "147429f919dc0824d64de976fcd8f52652050d13",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +2561,2565 @@  // casted into decimal safely, we use DecimalType(16, 6) which is wider than DecimalType(10, 0).\n  override def inputTypes: Seq[AbstractDataType] =\n    Seq(IntegerType, IntegerType, IntegerType, IntegerType, IntegerType, DecimalType(16, 6)) ++\n      timezone.map(_ => StringType)\n  override def nullable: Boolean = if (failOnError) children.exists(_.nullable) else true"
  },
  {
    "id" : "125d7949-8940-4cd1-bc4e-2fc57029ce80",
    "prId" : 33290,
    "prUrl" : "https://github.com/apache/spark/pull/33290#pullrequestreview-703581578",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9c1d1c39-2fdd-4a02-8082-ea96a94dec64",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Please, use TIMESTAMP_TYPE.key. Using the SQL config has some benefits like:\r\n- Easer to search in IDE by config name\r\n- Refactoring. Maybe, we will rename it in the future. So, you will not need to modify this place.",
        "createdAt" : "2021-07-11T10:23:23Z",
        "updatedAt" : "2021-07-11T10:32:40Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "d0f1fdde-7d6b-4c3e-b5ad-ff3ccf96de57",
        "parentId" : "9c1d1c39-2fdd-4a02-8082-ea96a94dec64",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "There will be errors:\r\n```\r\nannotation argument needs to be a constant; found: \"_FUNC_(year, month, day, hour, min, sec[, timezone]) - Create timestamp from year, month, day, hour, min, sec and timezone fields. \".+(scala.StringContext.apply(\"The result data type is consistent ...\r\n```",
        "createdAt" : "2021-07-11T12:14:40Z",
        "updatedAt" : "2021-07-11T12:14:40Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "abce601cba887bc66040dfb4b658cc051ccd289c",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +2288,2292 @@@ExpressionDescription(\n  usage = \"_FUNC_(year, month, day, hour, min, sec[, timezone]) - Create timestamp from year, month, day, hour, min, sec and timezone fields. \" +\n    \"The result data type is consistent with the value of configuration `spark.sql.timestampType`\",\n  arguments = \"\"\"\n    Arguments:"
  },
  {
    "id" : "f7e0fe81-3467-4e5c-ace6-2fb8d97aa7d9",
    "prId" : 33280,
    "prUrl" : "https://github.com/apache/spark/pull/33280#pullrequestreview-703581608",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bd8e0dc0-9513-468a-a499-309375b6f363",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Use TIMESTAMP_TYPE.key",
        "createdAt" : "2021-07-11T08:35:05Z",
        "updatedAt" : "2021-07-11T08:35:41Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "e11a7943-b80b-44f3-aba7-eee33b430123",
        "parentId" : "bd8e0dc0-9513-468a-a499-309375b6f363",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "There will be errors:\r\n```\r\nannotation argument needs to be a constant\r\n```",
        "createdAt" : "2021-07-11T12:15:00Z",
        "updatedAt" : "2021-07-11T12:15:00Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "4c11b7f373e5d108634888745c7fa3bcb8c1abf4",
    "line" : 48,
    "diffHunk" : "@@ -1,1 +1911,1915 @@      to a timestamp. Returns null with invalid input. By default, it follows casting rules to\n      a timestamp if the `fmt` is omitted. The result data type is consistent with the value of\n      configuration `spark.sql.timestampType`.\n  \"\"\",\n  arguments = \"\"\""
  },
  {
    "id" : "de985542-fbda-4d5b-997a-7afba7c0897f",
    "prId" : 33258,
    "prUrl" : "https://github.com/apache/spark/pull/33258#pullrequestreview-701909940",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "964d12d7-9a37-4e17-ad6d-2b165afbf4d1",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "\r\n```suggestion\r\n  since = \"3.3.0\")\r\n```\r\n",
        "createdAt" : "2021-07-08T10:42:29Z",
        "updatedAt" : "2021-07-08T10:42:30Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "998089cd0f1c2368e5a2b2773651836561a0a209",
    "line" : 30,
    "diffHunk" : "@@ -1,1 +218,222 @@  \"\"\",\n  group = \"datetime_funcs\",\n  since = \"3.2.0\")\ncase class LocalTimestamp(timeZoneId: Option[String] = None) extends LeafExpression\n  with TimeZoneAwareExpression with CodegenFallback {"
  },
  {
    "id" : "f24b8ec0-187b-438e-944c-c817614197de",
    "prId" : 33258,
    "prUrl" : "https://github.com/apache/spark/pull/33258#pullrequestreview-704714981",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2a713bc1-7a17-4186-b5a9-072270f23be8",
        "parentId" : null,
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "Shall we add a new test case for the code change here?",
        "createdAt" : "2021-07-12T11:02:13Z",
        "updatedAt" : "2021-07-12T11:02:13Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "f474dea8-7299-40bf-bbed-8e6e8542787f",
        "parentId" : "2a713bc1-7a17-4186-b5a9-072270f23be8",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "I wonder how it is related to new function? If it is not related, I would exclude this from the PR, and open new PR with a test.",
        "createdAt" : "2021-07-12T14:21:12Z",
        "updatedAt" : "2021-07-12T14:21:12Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "a1768190-a03a-4228-9bd2-0f06ade07f37",
        "parentId" : "2a713bc1-7a17-4186-b5a9-072270f23be8",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "it's related: https://github.com/apache/spark/pull/33258/files#diff-cf2dc65d70aa7bc490d237f1c50f5fdf51e244fabed782977a94d2934c208551R565",
        "createdAt" : "2021-07-12T18:42:03Z",
        "updatedAt" : "2021-07-12T18:42:04Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "0f03b7ce-1c45-4895-a529-9e8994a9c470",
        "parentId" : "2a713bc1-7a17-4186-b5a9-072270f23be8",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "I will add new test case in this PR.",
        "createdAt" : "2021-07-13T03:36:12Z",
        "updatedAt" : "2021-07-13T07:24:19Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "998089cd0f1c2368e5a2b2773651836561a0a209",
    "line" : 59,
    "diffHunk" : "@@ -1,1 +276,280 @@      case _: TimestampType => Literal(timestampUs, TimestampType)\n      case _: TimestampNTZType =>\n        Literal(convertTz(timestampUs, ZoneOffset.UTC, zoneId), TimestampNTZType)\n      case _: DateType => Literal(microsToDays(timestampUs, zoneId), DateType)\n    }"
  },
  {
    "id" : "c8a80053-0c1d-41c6-a11a-0c4457109194",
    "prId" : 33258,
    "prUrl" : "https://github.com/apache/spark/pull/33258#pullrequestreview-704475565",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e5684cd7-9043-4f1e-8244-89102cdd281a",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "do we really need a timezone to get current local time?",
        "createdAt" : "2021-07-12T18:41:11Z",
        "updatedAt" : "2021-07-12T18:41:11Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "04e8ef42-7989-46f3-9c49-3e209a26b2d8",
        "parentId" : "e5684cd7-9043-4f1e-8244-89102cdd281a",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "@cloud-fan Yes,I think so. If you don't specify tz, the `LocalDateTime.now()` will take the default JVM time zone, see https://github.com/apache/spark/pull/33258#discussion_r667378378",
        "createdAt" : "2021-07-12T19:39:02Z",
        "updatedAt" : "2021-07-12T19:48:30Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "998089cd0f1c2368e5a2b2773651836561a0a209",
    "line" : 46,
    "diffHunk" : "@@ -1,1 +234,238 @@    copy(timeZoneId = Option(timeZoneId))\n\n  override def eval(input: InternalRow): Any = localDateTimeToMicros(LocalDateTime.now(zoneId))\n\n  override def prettyName: String = \"localtimestamp\""
  },
  {
    "id" : "dce31f3c-37fa-47e2-979b-73650d1a8669",
    "prId" : 32995,
    "prUrl" : "https://github.com/apache/spark/pull/32995#pullrequestreview-688653603",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c51df2f6-c8bc-4dd4-804c-704d03ea1595",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Shall we put this logic in `TimestampFormatterHelper`?\r\n```\r\nfinal protected def getFormatter(fmt: String): TimestampFormatter = {\r\n  if (forTimestampWithoutTZ) new Iso8601TimestampFormatter(...) else ...\r\n}\r\n```",
        "createdAt" : "2021-06-21T16:31:21Z",
        "updatedAt" : "2021-06-21T16:31:21Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "c53469dd-5714-4227-8944-577af558ec85",
        "parentId" : "c51df2f6-c8bc-4dd4-804c-704d03ea1595",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "Well I prefer to put all the logic in `TimestampFormatter.getFormatter`",
        "createdAt" : "2021-06-21T16:39:18Z",
        "updatedAt" : "2021-06-21T16:39:18Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "d5afd6abdb6eb56e91b2d95b0fecad6468d2a64b",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +68,72 @@\n  // Whether the timestamp formatter is for TimestampWithoutTZType.\n  // If yes, the formatter is always `Iso8601TimestampFormatter`.\n  protected def forTimestampWithoutTZ: Boolean = false\n"
  },
  {
    "id" : "8928a812-8768-4b96-9c25-a6879c29cf28",
    "prId" : 32949,
    "prUrl" : "https://github.com/apache/spark/pull/32949#pullrequestreview-686246121",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f1ef51de-7180-4a89-9351-3cea6e99b0c0",
        "parentId" : null,
        "authorId" : "3a227965-84e0-47cf-9974-11293764f028",
        "body" : "`days` cannot be listed with `year` or `months`. So I change this part.",
        "createdAt" : "2021-06-17T12:31:27Z",
        "updatedAt" : "2021-06-17T12:31:27Z",
        "lastEditedBy" : "3a227965-84e0-47cf-9974-11293764f028",
        "tags" : [
        ]
      }
    ],
    "commit" : "48f080fc5b3bb2a2d704a0cc6bddb8d6be56f328",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +2527,2531 @@      > SELECT _FUNC_('SECONDS', timestamp'2019-10-01 00:00:01.000001');\n       1.000001\n      > SELECT _FUNC_('days', interval 5 days 3 hours 7 minutes);\n       5\n      > SELECT _FUNC_('seconds', interval 5 hours 30 seconds 1 milliseconds 1 microseconds);"
  },
  {
    "id" : "629519ec-ec19-48fe-bcb5-aedf7d8a0441",
    "prId" : 32849,
    "prUrl" : "https://github.com/apache/spark/pull/32849#pullrequestreview-681836349",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a3d389f8-0502-4b8f-8566-5562fa19815f",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "TODO: return interval second?",
        "createdAt" : "2021-06-11T09:35:43Z",
        "updatedAt" : "2021-06-11T09:35:43Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "cc384402-2440-4b3f-83f0-6fbfde9ff193",
        "parentId" : "a3d389f8-0502-4b8f-8566-5562fa19815f",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Here, any interval types that ends to SECOND works. Why not the default one: INTERVAL DAY TO SECOND?",
        "createdAt" : "2021-06-11T09:45:02Z",
        "updatedAt" : "2021-06-11T09:45:02Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "ea350db0-4948-496f-ac53-22d9af574484",
        "parentId" : "a3d389f8-0502-4b8f-8566-5562fa19815f",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "because interval second is the most precise one here. ",
        "createdAt" : "2021-06-11T13:33:45Z",
        "updatedAt" : "2021-06-11T13:33:45Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "fa00741c5319ae91dfcc3cf30beb2d2fbb427c64",
    "line" : 32,
    "diffHunk" : "@@ -1,1 +2551,2555 @@  override def inputTypes: Seq[AbstractDataType] = Seq(TimestampType, TimestampType)\n  override def dataType: DataType =\n    if (legacyInterval) CalendarIntervalType else DayTimeIntervalType()\n\n  override def withTimeZone(timeZoneId: String): TimeZoneAwareExpression ="
  },
  {
    "id" : "cad3a48d-b56c-48be-b710-f40bd9a05823",
    "prId" : 32262,
    "prUrl" : "https://github.com/apache/spark/pull/32262#pullrequestreview-640770378",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cab6d2d4-76a6-413c-b8c2-5ce821d17507",
        "parentId" : null,
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "Quick question, why `SubtractTimestamps` extends `BinaryExpression` instead of `BinaryOperator`? This reminds me the similar code in `BinaryOperator`",
        "createdAt" : "2021-04-20T16:41:02Z",
        "updatedAt" : "2021-04-20T18:02:52Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "745be525-281d-4a6d-9e00-3f508ab09538",
        "parentId" : "cab6d2d4-76a6-413c-b8c2-5ce821d17507",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "I don't have good answer to your question. Probably, we could refactor SubtractDates and SubtractTimestamps.",
        "createdAt" : "2021-04-20T17:33:43Z",
        "updatedAt" : "2021-04-20T18:02:52Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "feb741be-85f6-4349-bd78-af68d93b8cb2",
        "parentId" : "cab6d2d4-76a6-413c-b8c2-5ce821d17507",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Here is the PR https://github.com/apache/spark/pull/32267. Please, review it.",
        "createdAt" : "2021-04-21T08:32:45Z",
        "updatedAt" : "2021-04-21T08:32:45Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "9bdfe66b61b74f4d5c83984eef1b07c6dd665628",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +2558,2562 @@\n  override def toString: String = s\"($left - $right)\"\n  override def sql: String = s\"(${left.sql} - ${right.sql})\"\n\n  override protected def withNewChildrenInternal("
  },
  {
    "id" : "5024f1e2-741a-43e4-9f62-16d52863afbf",
    "prId" : 32176,
    "prUrl" : "https://github.com/apache/spark/pull/32176#pullrequestreview-636676078",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5ad0e6ec-2bc1-473a-b611-b716a32f885e",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "SPARK-35090 for ANSI intervals",
        "createdAt" : "2021-04-15T13:19:02Z",
        "updatedAt" : "2021-04-16T11:24:15Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "7668405ebd902cea698b76c373d0c06b1e759426",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +2413,2417 @@       1.000001\n      > SET spark.sql.legacy.interval.enabled=true;\n       spark.sql.legacy.interval.enabled\ttrue\n      > SELECT _FUNC_('days', interval 1 year 10 months 5 days);\n       5"
  },
  {
    "id" : "6f55027d-9cc2-4088-8f15-ff87ca9c7cab",
    "prId" : 32176,
    "prUrl" : "https://github.com/apache/spark/pull/32176#pullrequestreview-636676867",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "18e3a6a5-e203-40a2-9e9c-cde8979421d6",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Created SPARK-35091 to support ANSI intervals by `date_part()`",
        "createdAt" : "2021-04-15T13:19:47Z",
        "updatedAt" : "2021-04-16T11:24:15Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "7668405ebd902cea698b76c373d0c06b1e759426",
    "line" : 43,
    "diffHunk" : "@@ -1,1 +2483,2487 @@      > SELECT _FUNC_(SECONDS FROM timestamp'2019-10-01 00:00:01.000001');\n       1.000001\n      > SET spark.sql.legacy.interval.enabled=true;\n       spark.sql.legacy.interval.enabled\ttrue\n      > SELECT _FUNC_(days FROM interval 1 year 10 months 5 days);"
  },
  {
    "id" : "229eb851-d2f4-4943-bed1-fb01aed892f9",
    "prId" : 31832,
    "prUrl" : "https://github.com/apache/spark/pull/31832#pullrequestreview-611954715",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "72daba1b-58b3-44d4-8881-f267b6d05fd1",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Shall we make it general and accept both year-month and day-time intervals? The implementation can have 2 branches:\r\n```\r\noverride def nullSafeEval(micros: Any, months: Any): Any = interval.dataType match {\r\n  case YearMonthIntervalType =>\r\n    timestampAddMonths(micros.asInstanceOf[Long], months.asInstanceOf[Int], zoneId)\r\n  case DayTimeIntervalType => ...\r\n}\r\n```",
        "createdAt" : "2021-03-15T08:53:07Z",
        "updatedAt" : "2021-03-15T08:53:07Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "91437c3e-3ddf-446d-a113-d63f87c35ad4",
        "parentId" : "72daba1b-58b3-44d4-8881-f267b6d05fd1",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "I plan to support `timestamp + day-time interval` separately in another PR. The implementation itself is not complex, need to write more tests. ",
        "createdAt" : "2021-03-15T08:59:46Z",
        "updatedAt" : "2021-03-15T08:59:46Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "d8069abf0e59ff9f94c277c7eebb3609ce2dda74",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1523,1527 @@\n// Adds the year-month interval to the timestamp\ncase class TimestampAddYMInterval(\n    timestamp: Expression,\n    interval: Expression,"
  },
  {
    "id" : "6d466fbb-0586-4a6c-a7e0-f01bcd9e5a1b",
    "prId" : 31000,
    "prUrl" : "https://github.com/apache/spark/pull/31000#pullrequestreview-562047981",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c5810a96-08c7-4e44-9048-727e7a1d4bac",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Please, fix indentation according to https://github.com/databricks/scala-style-guide#spacing-and-indentation",
        "createdAt" : "2021-01-05T07:43:52Z",
        "updatedAt" : "2021-01-05T19:29:02Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "b401d8e0-3d54-497a-b9c2-a3233234a1e6",
        "parentId" : "c5810a96-08c7-4e44-9048-727e7a1d4bac",
        "authorId" : "e1bdd418-d9b9-4129-91a0-0865740d24d2",
        "body" : "I read the scala-style-guide and wasn't able to decipher the best practice for this specific scenario.  I opened [a scala-style-guide issue to clarify](https://github.com/databricks/scala-style-guide/issues/81).  Do you think this is better?\r\n\r\n```scala\r\n  extends BinaryExpression with ImplicitCastInputTypes with NullIntolerant\r\n  with TimeZoneAwareExpression {\r\n```\r\n\r\nI'm glad you made the comment because it's really important to manage whitespace consistently throughout the codebase.",
        "createdAt" : "2021-01-05T18:55:39Z",
        "updatedAt" : "2021-01-05T19:29:02Z",
        "lastEditedBy" : "e1bdd418-d9b9-4129-91a0-0865740d24d2",
        "tags" : [
        ]
      }
    ],
    "commit" : "ac476573bfe788d0e306c05cc0cff2ed9adb1294",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +1451,1455 @@case class AddHours(startTime: Expression, numHours: Expression, timeZoneId: Option[String] = None)\n  extends BinaryExpression with ImplicitCastInputTypes with NullIntolerant\n    with TimeZoneAwareExpression {\n\n  def this(startTime: Expression, numHours: Expression) = this(startTime, numHours, None)"
  },
  {
    "id" : "d2572016-727c-4f86-b5d5-3a28d883bdda",
    "prId" : 30807,
    "prUrl" : "https://github.com/apache/spark/pull/30807#pullrequestreview-560814360",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c249f1b7-474b-48b7-a79c-2ff0c9b69d19",
        "parentId" : null,
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "nit: is this useful given that `failOnError` already has a default value?",
        "createdAt" : "2020-12-29T20:57:29Z",
        "updatedAt" : "2021-01-04T08:23:29Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "6d382317-758e-4aa6-b233-fda84ba53988",
        "parentId" : "c249f1b7-474b-48b7-a79c-2ff0c9b69d19",
        "authorId" : "3cb8df92-1760-4e0e-94a5-f3ccadcc2882",
        "body" : "We do need this otherwise unit tests will fail for exemple this one: `sql/testOnly *SQLQueryTestSuite -- -z datetime.sql`\r\n\r\nSimilar PRs implemented this as well, foe exemple https://github.com/apache/spark/pull/30297/files#diff-da163d97a5f0fc534aad719c4a39eca97116f25bfc05b7d8941b342a3ed96036R1930\r\n\r\nI can't really well explain why it is necessary here. Someone can share some  idea here?",
        "createdAt" : "2020-12-29T21:43:16Z",
        "updatedAt" : "2021-01-04T08:23:29Z",
        "lastEditedBy" : "3cb8df92-1760-4e0e-94a5-f3ccadcc2882",
        "tags" : [
        ]
      },
      {
        "id" : "09a30eb9-6e6c-4470-8e6a-85de898e45e8",
        "parentId" : "c249f1b7-474b-48b7-a79c-2ff0c9b69d19",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "The `FunctionRegistry` requires a standard class constructor, not the `apply` method of case class.",
        "createdAt" : "2020-12-30T05:18:04Z",
        "updatedAt" : "2021-01-04T08:23:29Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "4445787a-5aac-4089-98cf-1d61cdd7c500",
        "parentId" : "c249f1b7-474b-48b7-a79c-2ff0c9b69d19",
        "authorId" : "3cb8df92-1760-4e0e-94a5-f3ccadcc2882",
        "body" : "thx!",
        "createdAt" : "2021-01-04T07:11:06Z",
        "updatedAt" : "2021-01-04T08:23:29Z",
        "lastEditedBy" : "3cb8df92-1760-4e0e-94a5-f3ccadcc2882",
        "tags" : [
        ]
      }
    ],
    "commit" : "e458546a7ddbe66a2f69c6d264961334d6d400a2",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +1185,1189 @@  override def right: Expression = dayOfWeek\n\n  def this(left: Expression, right: Expression) = this(left, right, SQLConf.get.ansiEnabled)\n\n  override def inputTypes: Seq[AbstractDataType] = Seq(DateType, StringType)"
  },
  {
    "id" : "cf6a6f81-42f9-49dd-aba1-258c7912afe0",
    "prId" : 30442,
    "prUrl" : "https://github.com/apache/spark/pull/30442#pullrequestreview-539742185",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2947c318-43a6-433e-9165-f2082c53cceb",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "can we update the `nullability` based on the `failOnError` flag?",
        "createdAt" : "2020-11-25T13:15:22Z",
        "updatedAt" : "2020-11-27T10:14:54Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "c56c254a-e27a-470a-ae68-59ed77a7d502",
        "parentId" : "2947c318-43a6-433e-9165-f2082c53cceb",
        "authorId" : "cce1c782-0596-44b6-8b99-6b77d2cca53c",
        "body" : "done.",
        "createdAt" : "2020-11-27T06:55:22Z",
        "updatedAt" : "2020-11-27T10:14:54Z",
        "lastEditedBy" : "cce1c782-0596-44b6-8b99-6b77d2cca53c",
        "tags" : [
        ]
      }
    ],
    "commit" : "e4fe5ee34b47bb5f57aad76fa41caa2c27eee53d",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +723,727 @@    timeZoneId: Option[String] = None,\n    failOnError: Boolean = SQLConf.get.ansiEnabled)\n  extends UnixTime {\n\n  def this(timeExp: Expression, format: Expression) ="
  },
  {
    "id" : "919cbf0c-08b5-422f-a075-4ee15c8e771f",
    "prId" : 30400,
    "prUrl" : "https://github.com/apache/spark/pull/30400#pullrequestreview-535869920",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "db39245e-68b4-4b6d-bbfd-70de91cb34ff",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "You need to add this func in the ignored set: https://github.com/apache/spark/blob/master/sql/core/src/test/scala/org/apache/spark/sql/expressions/ExpressionInfoSuite.scala#L148",
        "createdAt" : "2020-11-17T23:22:39Z",
        "updatedAt" : "2020-11-22T06:16:55Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "9076ddfc-2302-4769-b64a-7916790f5375",
        "parentId" : "db39245e-68b4-4b6d-bbfd-70de91cb34ff",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "We don't need to print `an offset from UTC (e.g., +08:35)`? It seems presto/sql server do so (IMO printing it looks meaningful).",
        "createdAt" : "2020-11-18T11:55:10Z",
        "updatedAt" : "2020-11-22T06:16:55Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "0d7c64b0-02e5-4bb4-a9f7-33531c7c974f",
        "parentId" : "db39245e-68b4-4b6d-bbfd-70de91cb34ff",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "Presto seems return the zone id, not see the offset. (checked both use and code).",
        "createdAt" : "2020-11-18T13:22:44Z",
        "updatedAt" : "2020-11-22T06:16:55Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      },
      {
        "id" : "c27975d1-9a9a-40c4-ba7f-970647fff2cf",
        "parentId" : "db39245e-68b4-4b6d-bbfd-70de91cb34ff",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "@ulysses-you . Oh, interesting. Does it mean Presto documentation has a bug?\r\n- https://prestodb.io/docs/current/functions/datetime.html\r\n\r\n> Returns the current time zone in the format defined by IANA (e.g., America/Los_Angeles) or as fixed offset from UTC (e.g., +08:35)",
        "createdAt" : "2020-11-20T18:54:49Z",
        "updatedAt" : "2020-11-22T06:16:55Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "9d42755a-5937-4683-9dfe-b382ee939f6d",
        "parentId" : "db39245e-68b4-4b6d-bbfd-70de91cb34ff",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "I also read it and feel a little confused. Maybe it depend on the location since the doc said `zone or offset`.",
        "createdAt" : "2020-11-21T01:46:17Z",
        "updatedAt" : "2020-11-22T06:16:55Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      }
    ],
    "commit" : "c83da46530ad1d4c5527a05e94c7fbad3eafba84",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +79,83 @@    Examples:\n      > SELECT _FUNC_();\n       Asia/Shanghai\n  \"\"\",\n  group = \"datetime_funcs\","
  },
  {
    "id" : "a3782892-144b-4ab7-9d2a-30df2a1d34d0",
    "prId" : 30400,
    "prUrl" : "https://github.com/apache/spark/pull/30400#pullrequestreview-533094192",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9ee61e16-1ed1-488a-a161-e25de7871cc7",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Please update the golden file, too. https://github.com/apache/spark/blob/master/sql/core/src/test/scala/org/apache/spark/sql/ExpressionsSchemaSuite.scala#L38-L41",
        "createdAt" : "2020-11-17T23:23:55Z",
        "updatedAt" : "2020-11-22T06:16:55Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "907ad416-7e04-48b6-a302-d1d9afabd4e9",
        "parentId" : "9ee61e16-1ed1-488a-a161-e25de7871cc7",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "thanks for the remind, get it.",
        "createdAt" : "2020-11-18T04:22:23Z",
        "updatedAt" : "2020-11-22T06:16:55Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      }
    ],
    "commit" : "c83da46530ad1d4c5527a05e94c7fbad3eafba84",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +83,87 @@  group = \"datetime_funcs\",\n  since = \"3.1.0\")\ncase class CurrentTimeZone() extends LeafExpression with Unevaluable {\n  override def nullable: Boolean = false\n  override def dataType: DataType = StringType"
  },
  {
    "id" : "a3b72a45-0a80-429a-9b13-a0172b67fb64",
    "prId" : 28894,
    "prUrl" : "https://github.com/apache/spark/pull/28894#pullrequestreview-435134941",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "32f55702-fb45-41fc-be95-7bc9a1120570",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "When I made it as `abstract class` with 2 args `func` and `funcName`, I got the following exception on tests from `DateExpressionsSuite`:\r\n```\r\norg.apache.spark.sql.catalyst.expressions.Second; no valid constructor\r\njava.io.InvalidClassException: org.apache.spark.sql.catalyst.expressions.Second; no valid constructor\r\n\tat java.io.ObjectStreamClass$ExceptionInfo.newInvalidClassException(ObjectStreamClass.java:169)\r\n\tat java.io.ObjectStreamClass.checkDeserialize(ObjectStreamClass.java:885)\r\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2100)\r\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1625)\r\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:465)\r\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:423)\r\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)\r\n\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:109)\r\n\tat org.apache.spark.sql.catalyst.expressions.ExpressionEvalHelper.prepareEvaluation(ExpressionEvalHelper.scala:75)\r\n```\r\nI decided to leave it as a trait.",
        "createdAt" : "2020-06-22T17:31:02Z",
        "updatedAt" : "2020-06-22T17:33:06Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "f2a278e90b2578552d960aba700e874c41f2bebe",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +275,279 @@}\n\ntrait GetTimeField extends UnaryExpression\n  with TimeZoneAwareExpression with ImplicitCastInputTypes with NullIntolerant {\n"
  },
  {
    "id" : "3eb36246-aef1-4d92-8483-bfbd1cd32f21",
    "prId" : 28886,
    "prUrl" : "https://github.com/apache/spark/pull/28886#pullrequestreview-434916112",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "139b53d8-e9b2-4089-8b33-986bd7ecf475",
        "parentId" : null,
        "authorId" : "c96c0fe4-f4e4-4569-bb4e-7736f9be7711",
        "body" : "I doubt that this is the right place to ask, but I tried to Google this but didn't find anything. I was just wondering what does `assert(secAndMicros.scale == 6` do exactly? I am assuming that it changes the Decimal's scale to 6, but what does that exactly help with? And is the added tail just a bunch of zeros? For instance does 3.14 become 3.140000?\r\nThanks!",
        "createdAt" : "2020-06-21T23:27:59Z",
        "updatedAt" : "2020-06-21T23:27:59Z",
        "lastEditedBy" : "c96c0fe4-f4e4-4569-bb4e-7736f9be7711",
        "tags" : [
        ]
      },
      {
        "id" : "bd693e7e-a199-4eed-9190-6f78ae127227",
        "parentId" : "139b53d8-e9b2-4089-8b33-986bd7ecf475",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "> I am assuming that it changes the Decimal's scale to 6\r\n\r\nThis assert doesn't change the scale, it just reads it.\r\n\r\n> And is the added tail just a bunch of zeros? For instance does 3.14 become 3.140000?\r\n\r\nIt shifts the decimal point. For example, if you have 3.14 with precision 6 and scale 3 that means 003.140. If you set\r\n- scale to 0, it becomes 3140\r\n- scale to 4 -> 0.314",
        "createdAt" : "2020-06-22T07:18:14Z",
        "updatedAt" : "2020-06-22T07:18:15Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "1805c9f4-48c3-47c2-b4c5-1dcc62dcf8db",
        "parentId" : "139b53d8-e9b2-4089-8b33-986bd7ecf475",
        "authorId" : "c96c0fe4-f4e4-4569-bb4e-7736f9be7711",
        "body" : "Awesome, thank you so much!",
        "createdAt" : "2020-06-22T13:21:54Z",
        "updatedAt" : "2020-06-22T13:21:54Z",
        "lastEditedBy" : "c96c0fe4-f4e4-4569-bb4e-7736f9be7711",
        "tags" : [
        ]
      }
    ],
    "commit" : "b9e2ee097bdbe6c6ff1d5680025ade7cc9fca9e7",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +1998,2002 @@      zoneId: ZoneId): Any = {\n    try {\n      assert(secAndMicros.scale == 6,\n        s\"Seconds fraction must have 6 digits for microseconds but got ${secAndMicros.scale}\")\n      val unscaledSecFrac = secAndMicros.toUnscaledLong"
  },
  {
    "id" : "44f89cd3-6dca-4cd2-bc97-648f2c872db2",
    "prId" : 28650,
    "prUrl" : "https://github.com/apache/spark/pull/28650#pullrequestreview-419906731",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "005a1f51-c119-40ca-a0bd-1528e0790b44",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "shall we still use `NonFatal` here to be safe?",
        "createdAt" : "2020-05-28T07:46:03Z",
        "updatedAt" : "2020-06-09T10:32:20Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "ce98413c-edb8-425c-96a8-1fc721c44b86",
        "parentId" : "005a1f51-c119-40ca-a0bd-1528e0790b44",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "Should it be better to be consistent with the codegen mode?",
        "createdAt" : "2020-05-28T07:52:30Z",
        "updatedAt" : "2020-06-09T10:32:20Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "9ebf15b8-6e74-4875-bec1-656a816bf28c",
        "parentId" : "005a1f51-c119-40ca-a0bd-1528e0790b44",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "ah it's to match the codegen version. LGTM",
        "createdAt" : "2020-05-28T08:52:55Z",
        "updatedAt" : "2020-06-09T10:32:20Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "8139dfc1d064ab7b1b1d7be1af0c8f903d3e6b13",
    "line" : 182,
    "diffHunk" : "@@ -1,1 +908,912 @@              formatter.parse(t.asInstanceOf[UTF8String].toString) / downScaleFactor\n            } catch {\n              case _: DateTimeParseException |\n                   _: DateTimeException |\n                   _: ParseException => null"
  },
  {
    "id" : "a966286a-7a3e-42a0-931a-4739116d2eac",
    "prId" : 28650,
    "prUrl" : "https://github.com/apache/spark/pull/28650#pullrequestreview-420259859",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fdc01b33-aa9d-4456-979e-a54028b3515d",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "shall we use `nullSafeCodeGen`?",
        "createdAt" : "2020-05-28T14:29:11Z",
        "updatedAt" : "2020-06-09T10:32:20Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "7f799a9a-9f0f-4a6a-98d9-d4510bf17b1c",
        "parentId" : "fdc01b33-aa9d-4456-979e-a54028b3515d",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "`defineCodeGen` seems enough here as a shorthand for  `nullSafeCodeGen `.",
        "createdAt" : "2020-05-28T15:04:48Z",
        "updatedAt" : "2020-06-09T10:32:20Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "87db3535-f12b-401c-85aa-f0c506e9eea6",
        "parentId" : "fdc01b33-aa9d-4456-979e-a54028b3515d",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "what if the `seconds` is null?",
        "createdAt" : "2020-05-28T15:43:41Z",
        "updatedAt" : "2020-06-09T10:32:20Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "6d234431-d5ae-4b4d-9730-47256a291dbc",
        "parentId" : "fdc01b33-aa9d-4456-979e-a54028b3515d",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "```scala\r\n /**\r\n   * Short hand for generating binary evaluation code.\r\n   * If either of the sub-expressions is null, the result of this computation\r\n   * is assumed to be null.\r\n   *\r\n   * @param f accepts two variable names and returns Java code to compute the output.\r\n   */\r\n  protected def defineCodeGen(\r\n      ctx: CodegenContext,\r\n      ev: ExprCode,\r\n      f: (String, String) => String): ExprCode = {\r\n    nullSafeCodeGen(ctx, ev, (eval1, eval2) => {\r\n      s\"${ev.value} = ${f(eval1, eval2)};\"\r\n    })\r\n  }\r\n```\r\nit is just a wrapper for `nullSafeCodeGen`",
        "createdAt" : "2020-05-28T15:49:25Z",
        "updatedAt" : "2020-06-09T10:32:20Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "236a186f-d900-4c04-a885-4eb4deb370df",
        "parentId" : "fdc01b33-aa9d-4456-979e-a54028b3515d",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "ah i see",
        "createdAt" : "2020-05-28T15:53:01Z",
        "updatedAt" : "2020-06-09T10:32:20Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "3f0a050b-ff07-4139-b2aa-adeb43cd7846",
        "parentId" : "fdc01b33-aa9d-4456-979e-a54028b3515d",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "FYI https://github.com/apache/spark/pull/28650/files#diff-b83497f7bc11578a0b63a814a2a30f48R744",
        "createdAt" : "2020-05-28T15:57:34Z",
        "updatedAt" : "2020-06-09T10:32:20Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "8139dfc1d064ab7b1b1d7be1af0c8f903d3e6b13",
    "line" : 369,
    "diffHunk" : "@@ -1,1 +1040,1044 @@    formatterOption.map { f =>\n      val formatterName = ctx.addReferenceObj(\"formatter\", f)\n      defineCodeGen(ctx, ev, (seconds, _) =>\n        s\"UTF8String.fromString($formatterName.format($seconds * 1000000L))\")\n    }.getOrElse {"
  },
  {
    "id" : "e27d87a0-4692-41db-91be-78c4f67f79e6",
    "prId" : 28650,
    "prUrl" : "https://github.com/apache/spark/pull/28650#pullrequestreview-420246069",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f75294b7-a4e6-47f3-9706-4c46526fd380",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "ditto",
        "createdAt" : "2020-05-28T15:43:54Z",
        "updatedAt" : "2020-06-09T10:32:20Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "8139dfc1d064ab7b1b1d7be1af0c8f903d3e6b13",
    "line" : 376,
    "diffHunk" : "@@ -1,1 +1046,1050 @@      val ldf = LegacyDateFormats.getClass.getName.stripSuffix(\"$\")\n      val zid = ctx.addReferenceObj(\"zoneId\", zoneId, classOf[ZoneId].getName)\n      defineCodeGen(ctx, ev, (seconds, format) =>\n        s\"\"\"\n           |UTF8String.fromString("
  },
  {
    "id" : "b4ea886d-77e6-44c8-b571-6dd4f14818d1",
    "prId" : 28650,
    "prUrl" : "https://github.com/apache/spark/pull/28650#pullrequestreview-426910004",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "91a3b1c1-123d-4a36-9d81-def7560c9945",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "do we have expressions that create DateFormatter?",
        "createdAt" : "2020-06-09T08:33:48Z",
        "updatedAt" : "2020-06-09T10:32:20Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "0d131a90-9803-4a2d-87b2-a12acdb85315",
        "parentId" : "91a3b1c1-123d-4a36-9d81-def7560c9945",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "no datetime functions, only csv/json ones",
        "createdAt" : "2020-06-09T08:35:34Z",
        "updatedAt" : "2020-06-09T10:32:20Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "8139dfc1d064ab7b1b1d7be1af0c8f903d3e6b13",
    "line" : 30,
    "diffHunk" : "@@ -1,1 +55,59 @@}\n\ntrait TimestampFormatterHelper extends TimeZoneAwareExpression {\n\n  protected def formatString: Expression"
  },
  {
    "id" : "49020399-af88-4b76-adf1-e6c5975aa0c1",
    "prId" : 28650,
    "prUrl" : "https://github.com/apache/spark/pull/28650#pullrequestreview-427059157",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b0cec9b8-6453-4373-922e-3a231b59b1df",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "shall we add the `NullIntolerant` in the base class `TimestampFormatterHelper`? and it seems better to do it in a new PR.",
        "createdAt" : "2020-06-09T11:44:57Z",
        "updatedAt" : "2020-06-09T11:44:57Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "3626499e-dcba-4a65-ab3c-c8dd8e8ff11d",
        "parentId" : "b0cec9b8-6453-4373-922e-3a231b59b1df",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "Copy that.",
        "createdAt" : "2020-06-09T11:50:44Z",
        "updatedAt" : "2020-06-09T11:50:44Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "8139dfc1d064ab7b1b1d7be1af0c8f903d3e6b13",
    "line" : 283,
    "diffHunk" : "@@ -1,1 +1011,1015 @@case class FromUnixTime(sec: Expression, format: Expression, timeZoneId: Option[String] = None)\n  extends BinaryExpression with TimestampFormatterHelper with ImplicitCastInputTypes\n    with NullIntolerant {\n\n  def this(sec: Expression, format: Expression) = this(sec, format, None)"
  },
  {
    "id" : "213ae79a-de8c-45fd-8fb8-74f11ec30828",
    "prId" : 28342,
    "prUrl" : "https://github.com/apache/spark/pull/28342#pullrequestreview-400464369",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "308c6220-f0d1-44c2-a337-899e52888c76",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Do you remember when we started to support this syntax? I roughly remember it was added relatively recently. If the added version is different, it might be best to note it in `note` while we're here.",
        "createdAt" : "2020-04-26T03:34:11Z",
        "updatedAt" : "2020-04-26T06:56:52Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "b977d9a1-7502-4862-8cb4-2ce0a140d412",
        "parentId" : "308c6220-f0d1-44c2-a337-899e52888c76",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "I found the PR to implement the syntax: https://github.com/apache/spark/pull/14442\r\nSpark without the hive support has supported it since 2.0. Yea, I'll add this info as note.",
        "createdAt" : "2020-04-26T06:28:42Z",
        "updatedAt" : "2020-04-26T06:56:52Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "463c4d3f69022fa8b2f5ae34675155ca0d87bec7",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +71,75 @@      > SELECT _FUNC_();\n       2020-04-25\n      > SELECT _FUNC_;\n       2020-04-25\n  \"\"\","
  },
  {
    "id" : "ab9de445-4033-4165-acb3-566ec033f647",
    "prId" : 28342,
    "prUrl" : "https://github.com/apache/spark/pull/28342#pullrequestreview-400465260",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e1ffdebb-3581-4ff4-8b43-9bf7d6ef2165",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "nit. Do you think we can have `examples = ` for `now()` since `now` usage is different(having different class and only functional form) from `current_timestamp`? Of course, we can skip it.",
        "createdAt" : "2020-04-26T05:06:22Z",
        "updatedAt" : "2020-04-26T06:56:52Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "38fdb8ab-c0b8-4035-bcfc-1cfdbda11b9f",
        "parentId" : "e1ffdebb-3581-4ff4-8b43-9bf7d6ef2165",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Ah I see. I'll add an example for `now` in this PR, too. Actually, I think it is better to add examples for the expressions that do not have examples now.",
        "createdAt" : "2020-04-26T06:38:13Z",
        "updatedAt" : "2020-04-26T06:56:52Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "463c4d3f69022fa8b2f5ae34675155ca0d87bec7",
    "line" : 81,
    "diffHunk" : "@@ -1,1 +133,137 @@\n@ExpressionDescription(\n  usage = \"_FUNC_() - Returns the current timestamp at the start of query evaluation.\",\n  examples = \"\"\"\n    Examples:"
  },
  {
    "id" : "0391a0b8-2da3-4ad2-9352-299a56907979",
    "prId" : 28284,
    "prUrl" : "https://github.com/apache/spark/pull/28284#pullrequestreview-397837378",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7ad0c653-8fb2-4037-8b96-e500822aa3cc",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Unlike the PR title, this PR is adding `DAYOFWEEK_ISO` newly. Which platform are you referring for `DAYOFWEEK_ISO`? IIRC, @gatorsmile commented that we should not consider IBM DB i.\r\n> isodow is PostgreSQL specific but iso as a suffix is more commonly used across platforms ",
        "createdAt" : "2020-04-21T18:09:04Z",
        "updatedAt" : "2020-04-21T18:11:57Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "0a2f34bf-5c33-4ab6-a98a-0e8ea7e087e3",
        "parentId" : "7ad0c653-8fb2-4037-8b96-e500822aa3cc",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "Hi @dongjoon-hyun, for historical reasons, we have [`dayofweek`, `dow`] implemented for representing a non-ISO day-of-week and a newly added `isodow` from PostgreSQL for ISO day-of-week. Many other systems only have one week-numbering system support and use either full names or abbreviations. Things in spark become a little bit complicated. \r\n1. because of the existence of `isodow`, so we need to add iso-prefix to `dayofweek` to make a pair for it too. [`dayofweek`, `isodayofweek`, `dow` and `isodow`] \r\n2. because there are rare `iso`-prefixed systems and more systems choose `iso`-suffixed way, so we may result in [`dayofweek`, `dayofweekiso`, `dow`, `dowiso`]\r\n3. `dayofweekiso` looks nice and has use cases in the platforms listed above, e.g. snowflake, but `dowiso` looks weird and no use cases found.\r\n4. after a discussion with @cloud-fan, we have both agreed with an underscore before `iso` may look much better because `isodow` is new and there is no standard for `iso` kind of things, so this may be good for us to make it simple and clear for end-users if they are well documented too.\r\n\r\nThus, we finally result in [`dayofweek`, `dow`] for Non-ISO week-numbering system and [`dayofweek_iso`, `dow_iso`] for ISO system",
        "createdAt" : "2020-04-22T02:24:26Z",
        "updatedAt" : "2020-04-22T03:22:43Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "b6de5a90-ead2-48fe-9d65-f630f866f768",
        "parentId" : "7ad0c653-8fb2-4037-8b96-e500822aa3cc",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "@yaooqinn . Ya. It's clear for that part. Could you enumerate the name of systems which supports `DAYOFWEEK_ISO`? (except IBM DB i). I'm wondering that specifically.\r\n\r\nIt's because you wrote like `Many other systems` in the PR description.",
        "createdAt" : "2020-04-22T03:16:27Z",
        "updatedAt" : "2020-04-22T03:19:53Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "d99e04d3-72a4-4b78-a211-f8d00d7c822f",
        "parentId" : "7ad0c653-8fb2-4037-8b96-e500822aa3cc",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "Specifically, snowflake use dayofweek_iso except db2. I couldn't find more because most platforms do not have two day-of-week system implemented.",
        "createdAt" : "2020-04-22T03:49:59Z",
        "updatedAt" : "2020-04-22T03:51:31Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "e31e7d9a-dc52-4093-b6d6-738ec71f867b",
        "parentId" : "7ad0c653-8fb2-4037-8b96-e500822aa3cc",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Got it. Thanks. I understand that Snowflake is considered important. Could you add some of the above comment(https://github.com/apache/spark/pull/28284#discussion_r412621357) to the PR description then?",
        "createdAt" : "2020-04-22T03:58:15Z",
        "updatedAt" : "2020-04-22T03:58:15Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "85bed87d-b9f3-4406-9638-a8fc043eab30",
        "parentId" : "7ad0c653-8fb2-4037-8b96-e500822aa3cc",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "thanks @dongjoon-hyun. PR description updated",
        "createdAt" : "2020-04-22T04:36:16Z",
        "updatedAt" : "2020-04-22T04:36:16Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "521bc2cd45b21d01bc1e0d374f913406d1afeb42",
    "line" : 165,
    "diffHunk" : "@@ -1,1 +2005,2009 @@    case \"DAY\" | \"D\" | \"DAYS\" => DayOfMonth(source)\n    case \"DAYOFWEEK\" | \"DOW\" => DayOfWeek(source)\n    case \"DAYOFWEEK_ISO\" | \"DOW_ISO\" => Add(WeekDay(source), Literal(1))\n    case \"DOY\" => DayOfYear(source)\n    case \"HOUR\" | \"H\" | \"HOURS\" | \"HR\" | \"HRS\" => Hour(source)"
  }
]