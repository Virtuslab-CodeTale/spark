[
  {
    "id" : "e43824a5-57dc-4202-8a77-8beb62919121",
    "prId" : 33750,
    "prUrl" : "https://github.com/apache/spark/pull/33750#pullrequestreview-730660945",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8d729ce8-ed95-4645-b34a-b1dad2c040f8",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "is it possible to share this code between ansi and non-ansi?",
        "createdAt" : "2021-08-16T11:38:04Z",
        "updatedAt" : "2021-08-16T11:38:04Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "b0f0b997-a682-4667-bfd0-3753f4834c18",
        "parentId" : "8d729ce8-ed95-4645-b34a-b1dad2c040f8",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "Yes and no. Here is for binary comparison, while in ANSI mode it is for binary operators...It is hard to create a function name for this.",
        "createdAt" : "2021-08-16T12:36:04Z",
        "updatedAt" : "2021-08-16T12:36:04Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "9f01da13cd628426be63e2d610a20c0600cabbd4",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +864,868 @@\n  // Return whether a string literal can be promoted as the give data type in a binary comparison.\n  private def canPromoteAsInBinaryComparison(dt: DataType) = dt match {\n    // If a binary comparison contains interval type and string type, we can't decide which\n    // interval type the string should be promoted as. There are many possible interval"
  },
  {
    "id" : "f79e48b4-078c-4bfe-8904-c33c10d6ab8d",
    "prId" : 33344,
    "prUrl" : "https://github.com/apache/spark/pull/33344#pullrequestreview-706219094",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7554f017-089e-48fc-b390-cd5c13ab8b0f",
        "parentId" : null,
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "This is needed for a test case with mixed `Date` & `TimestampNTZ` partition columns",
        "createdAt" : "2021-07-14T12:35:34Z",
        "updatedAt" : "2021-07-14T12:35:35Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "aa853c5660d21f2e53439d076a851a9e67c037fc",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +868,872 @@        Some(TimestampType)\n\n      case (_: TimestampNTZType, _: DateType) | (_: DateType, _: TimestampNTZType) =>\n        Some(TimestampNTZType)\n"
  },
  {
    "id" : "2ff39d9c-690c-4e1e-b343-31006fc37319",
    "prId" : 33156,
    "prUrl" : "https://github.com/apache/spark/pull/33156#pullrequestreview-696227981",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "73593936-d4a9-480b-a319-7158992f0b07",
        "parentId" : null,
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "This addresses comment in https://github.com/apache/spark/pull/33136#discussion_r661468789",
        "createdAt" : "2021-06-30T14:20:37Z",
        "updatedAt" : "2021-06-30T14:20:37Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "afa6903e1d779f9b5de4104997df74d3cc21f2d0",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +962,966 @@      // Implicit cast between date time types\n      case (DateType, TimestampType) => TimestampType\n      case (DateType, AnyTimestampType) => AnyTimestampType.defaultConcreteType\n      case (TimestampType | TimestampWithoutTZType, DateType) => DateType\n"
  },
  {
    "id" : "850e2132-f9b4-4a9e-a0b5-b588eaa00517",
    "prId" : 33136,
    "prUrl" : "https://github.com/apache/spark/pull/33136#pullrequestreview-695908229",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "31d03340-c681-4eb8-ab2e-50d19717becb",
        "parentId" : null,
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "Here the result is the same if casting both to `TimestampWithoutTZType` or `TimestampType`.  Casting to TimestampWithoutTZType is more straightforward.",
        "createdAt" : "2021-06-30T09:35:37Z",
        "updatedAt" : "2021-06-30T09:35:38Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "c206f2a9d05a3413494e6634c48d84cfc396225c",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +640,644 @@      case s @ SubtractTimestamps(AnyTimestampType(), AnyTimestampType(), _, _)\n        if s.left.dataType != s.right.dataType =>\n        val newLeft = castIfNotSameType(s.left, TimestampWithoutTZType)\n        val newRight = castIfNotSameType(s.right, TimestampWithoutTZType)\n        s.copy(left = newLeft, right = newRight)"
  },
  {
    "id" : "24e52734-0029-4c7b-af5a-6a94e29dc967",
    "prId" : 33136,
    "prUrl" : "https://github.com/apache/spark/pull/33136#pullrequestreview-696163874",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "118303fc-c59d-43d6-bc8a-3120b541cc6d",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "here should be `AnyTimestampType.defaultConcreteType` as well",
        "createdAt" : "2021-06-30T13:23:10Z",
        "updatedAt" : "2021-06-30T13:23:11Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "65a91252-79ec-454d-b277-29e44c39a590",
        "parentId" : "118303fc-c59d-43d6-bc8a-3120b541cc6d",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "I will address this in the PR for extracting date fields.",
        "createdAt" : "2021-06-30T13:30:26Z",
        "updatedAt" : "2021-06-30T13:30:26Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "c206f2a9d05a3413494e6634c48d84cfc396225c",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +962,966 @@      // Implicit cast between date time types\n      case (DateType, TimestampType) => TimestampType\n      case (DateType, AnyTimestampType) => TimestampType\n      case (TimestampType, DateType) => DateType\n"
  },
  {
    "id" : "e33b7daa-ea9d-41f8-9e23-9c4cc90cf375",
    "prId" : 31349,
    "prUrl" : "https://github.com/apache/spark/pull/31349#pullrequestreview-582467158",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "342ca765-6023-4f7f-a038-ff85a462673a",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "we should document it",
        "createdAt" : "2021-01-27T15:16:28Z",
        "updatedAt" : "2021-02-24T04:07:09Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "29fa6b83-a250-4003-8d70-780eedcb8047",
        "parentId" : "342ca765-6023-4f7f-a038-ff85a462673a",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "Done",
        "createdAt" : "2021-02-03T15:09:04Z",
        "updatedAt" : "2021-02-24T04:07:09Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "1f5ecdc58e7ca1a87675880f722f74ae2777f753",
    "line" : 94,
    "diffHunk" : "@@ -1,1 +45,49 @@   * the operation. Those rules are implemented in [[DecimalPrecision]].\n   */\n  def findTightestCommonType(type1: DataType, type2: DataType): Option[DataType]\n\n  /**"
  },
  {
    "id" : "8ebec8d9-3c85-4310-a7f7-6a55c3af15ec",
    "prId" : 31349,
    "prUrl" : "https://github.com/apache/spark/pull/31349#pullrequestreview-596510718",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a1aba983-f1b9-498d-99b1-5c524b0e62f8",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "this can be final",
        "createdAt" : "2021-02-23T08:02:24Z",
        "updatedAt" : "2021-02-24T04:07:09Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "d9e347ad-0230-4874-b28e-ad11ed362dcc",
        "parentId" : "a1aba983-f1b9-498d-99b1-5c524b0e62f8",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "Let's skip `final` for now",
        "createdAt" : "2021-02-23T16:07:28Z",
        "updatedAt" : "2021-02-24T04:07:09Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "1f5ecdc58e7ca1a87675880f722f74ae2777f753",
    "line" : 155,
    "diffHunk" : "@@ -1,1 +77,81 @@  def canCast(from: DataType, to: DataType): Boolean\n\n  protected def findTypeForComplex(\n      t1: DataType,\n      t2: DataType,"
  },
  {
    "id" : "7ebcc3f5-f1db-4456-ab04-c7812d1594cf",
    "prId" : 29643,
    "prUrl" : "https://github.com/apache/spark/pull/29643#pullrequestreview-483598067",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "17055334-a168-4dc5-893c-d773d2deb323",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Out of curiosity; why we need to set timezone here?",
        "createdAt" : "2020-09-07T14:12:06Z",
        "updatedAt" : "2020-09-07T14:12:06Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "c544af8d-828a-4112-acad-2f994df4c962",
        "parentId" : "17055334-a168-4dc5-893c-d773d2deb323",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "otherwise `WidenSetOperationTypes` will return invalid attribute mapping (unresolved Alias with unresolved cast) when calling `resolveOperatorsUpWithNewOutput`",
        "createdAt" : "2020-09-07T14:30:20Z",
        "updatedAt" : "2020-09-07T14:30:20Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "0857791bc310f54dd8ed1f4bcf5a5b274ffa33c6",
    "line" : 85,
    "diffHunk" : "@@ -1,1 +405,409 @@      val casted = plan.output.zip(targetTypes).map {\n        case (e, dt) if e.dataType != dt =>\n          Alias(Cast(e, dt, Some(SQLConf.get.sessionLocalTimeZone)), e.name)()\n        case (e, _) => e\n      }"
  },
  {
    "id" : "fb2e0a9b-d183-479b-8409-06ebb79d0fc5",
    "prId" : 29485,
    "prUrl" : "https://github.com/apache/spark/pull/29485#pullrequestreview-481891667",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dda1783d-cfed-400c-bb6e-6edd292a5d7e",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "what are we doing here?",
        "createdAt" : "2020-09-03T12:46:03Z",
        "updatedAt" : "2020-09-03T12:46:04Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "2c0eb1f7-4414-4893-90b3-dc5059677603",
        "parentId" : "dda1783d-cfed-400c-bb6e-6edd292a5d7e",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "This generates a rewrite map used for `Analyzer.rewritePlan`. The `rewritePlan` assumes a plan structure is the same before/after plan rewriting, so this `WidenSetOperationTypes` rule does two-phase transformation now as follows;\r\n```\r\n### Input Plan (Query described in the PR description) ###\r\nProject [v#1]\r\n+- SubqueryAlias t\r\n   +- Union\r\n      :+- Project [v#1]\r\n      :   +- SubqueryAlias t3\r\n      :      ...\r\n      +- Project [v#2]\r\n         +- Project [CheckOverflow((promote_precision(cast(v#1 as decimal(11,0))) + promote_precision(cast(v#1 as decimal(11,0)))), DecimalType(11,0), true) AS v#2]\r\n            +- SubqueryAlias t3\r\n               ...\r\n\r\n### Phase-1 (Adds Project, but not update ExprId) ###\r\nProject [v#1]\r\n+- SubqueryAlias t\r\n   +- Union\r\n      :- Project [cast(v#1 as decimal(11,0)) AS v#1] <--- !!!Adds Project to widen a type!!!\r\n      :  +- Project [v#1]\r\n      :     +- SubqueryAlias t3\r\n      :        ...\r\n      +- Project [v#2]\r\n         +- Project [CheckOverflow((promote_precision(cast(v#1 as decimal(11,0))) + promote_precision(cast(v#1 as decimal(11,0)))), DecimalType(11,0), true) AS v#2]\r\n            ...\r\n\r\n### Phase-2 ###\r\n// Analyzer.rewritePlan updates ExprIds based on a rewrite map:\r\n// `Project [cast(v#1 as decimal(11,0)) AS v#1]` => Project [cast(v#1 as decimal(11,0)) AS v#3]\r\nProject [v#3] <--- !!!Updates ExprId!!!\r\n+- SubqueryAlias t\r\n   +- Union\r\n      :- Project [cast(v#1 as decimal(11,0)) AS v#3] <--- !!!Updates ExprId!!!\r\n      :  +- Project [v#1]\r\n      :     +- SubqueryAlias t3\r\n      :        ...\r\n      +- Project [v#2]\r\n         +- Project [CheckOverflow((promote_precision(cast(v#1 as decimal(11,0))) + promote_precision(cast(v#1 as decimal(11,0)))), DecimalType(11,0), true) AS v#2]\r\n            ...\r\n```",
        "createdAt" : "2020-09-03T14:23:20Z",
        "updatedAt" : "2020-09-03T14:23:21Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "6372515769c1d3c299b865d4ec4b8378bdc5e84c",
    "line" : 101,
    "diffHunk" : "@@ -1,1 +418,422 @@          e -> e\n      }.unzip\n      Project(casted._1, plan) -> Project(casted._2, plan)\n    }\n  }"
  },
  {
    "id" : "cebe7f98-f5fd-4d8f-a4aa-a1d81631f733",
    "prId" : 28600,
    "prUrl" : "https://github.com/apache/spark/pull/28600#pullrequestreview-416706055",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ce1e07c8-1af7-409d-b829-0843ea0f36e9",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Let's update `IntegralDivide.inputType`, to only accpet `LongType` and `DecimalType`",
        "createdAt" : "2020-05-22T07:34:07Z",
        "updatedAt" : "2020-05-23T06:29:22Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "3a1a061bb955f1186675f5dda7525e6250614b17",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +693,697 @@    override protected def coerceTypes(plan: LogicalPlan): LogicalPlan = plan resolveExpressions {\n      case e if !e.childrenResolved => e\n      case d @ IntegralDivide(left, right) =>\n        IntegralDivide(mayCastToLong(left), mayCastToLong(right))\n    }"
  },
  {
    "id" : "46aae0f3-4ddd-48cb-ac3b-7e5dec61ff9c",
    "prId" : 28600,
    "prUrl" : "https://github.com/apache/spark/pull/28600#pullrequestreview-417255283",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "81fd8b4f-08ee-4d6a-b9f6-faea7ffd4fe6",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Plz add fine-grained tests  for this rule in `TypeCoercionSuite`.",
        "createdAt" : "2020-05-22T23:53:53Z",
        "updatedAt" : "2020-05-23T06:29:22Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "259dc996-99c4-44c1-82b6-86dbcf555015",
        "parentId" : "81fd8b4f-08ee-4d6a-b9f6-faea7ffd4fe6",
        "authorId" : "f2e3e1f3-82a3-4b68-a0c2-2f7fdf58cf35",
        "body" : "Added tests in `TypeCoercionSuite`",
        "createdAt" : "2020-05-23T06:30:05Z",
        "updatedAt" : "2020-05-23T06:30:05Z",
        "lastEditedBy" : "f2e3e1f3-82a3-4b68-a0c2-2f7fdf58cf35",
        "tags" : [
        ]
      }
    ],
    "commit" : "3a1a061bb955f1186675f5dda7525e6250614b17",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +690,694 @@   * This rule cast the integral inputs to long type, to avoid overflow during calculation.\n   */\n  object IntegralDivision extends TypeCoercionRule {\n    override protected def coerceTypes(plan: LogicalPlan): LogicalPlan = plan resolveExpressions {\n      case e if !e.childrenResolved => e"
  },
  {
    "id" : "a4eb0c79-75da-4efb-857e-791f95647761",
    "prId" : 27965,
    "prUrl" : "https://github.com/apache/spark/pull/27965#pullrequestreview-378998435",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dddb7d00-dce1-4036-a0ba-c90ed1187869",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "This causes a behavior difference in arithmatic operations, too. Could you describe the following change in the PR description? New one looks reasonable to me.\r\n\r\n**2.4.5 and 3.0.0-preview2**\r\n```\r\nscala> sql(\"select (cast('2020-03-28' AS DATE) + '1')\").show\r\norg.apache.spark.sql.AnalysisException: cannot resolve '(CAST('2020-03-28' AS DATE) + CAST('1' AS DOUBLE))' due to data type mismatch: differing types in '(CAST('2020-03-28' AS DATE) + CAST('1' AS DOUBLE))' (date and double).; line 1 pos 8;\r\n```\r\n\r\nThis PR.\r\n```\r\nscala> sql(\"select (cast('2020-03-28' AS DATE) + '1')\").show\r\n+-------------------------------------+\r\n|date_add(CAST(2020-03-28 AS DATE), 1)|\r\n+-------------------------------------+\r\n|                           2020-03-29|\r\n+-------------------------------------+\r\n```",
        "createdAt" : "2020-03-20T17:34:39Z",
        "updatedAt" : "2020-03-23T11:05:16Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "6f7dd07b-5973-4603-aae7-5f5582e0c3b4",
        "parentId" : "dddb7d00-dce1-4036-a0ba-c90ed1187869",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "You forgot to write a function name in the second query above?\r\n```\r\nscala> sql(\"select (cast('2020-03-28' AS DATE) + '1')\").show\r\n                 ^^^^\r\n```",
        "createdAt" : "2020-03-20T23:10:06Z",
        "updatedAt" : "2020-03-23T11:05:16Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "d30c465f-9c0f-4ec1-92e5-85bd79184fcc",
        "parentId" : "dddb7d00-dce1-4036-a0ba-c90ed1187869",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "No~ The both queries are the same. What I meant was it's the behavior of this PR; this PR extends **expressions**, too.",
        "createdAt" : "2020-03-22T00:52:07Z",
        "updatedAt" : "2020-03-23T11:05:16Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "9f6cca49-1d8b-42ac-8a59-697c109e5cb3",
        "parentId" : "dddb7d00-dce1-4036-a0ba-c90ed1187869",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Oh, I see.",
        "createdAt" : "2020-03-22T12:07:53Z",
        "updatedAt" : "2020-03-23T11:05:16Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "bd6be1620d54e4e5ac65f6f9b930af9ccd604997",
    "line" : 26,
    "diffHunk" : "@@ -1,1 +1052,1056 @@   * TODO(SPARK-28589): implement ANSI type type coercion and handle string literals.\n   */\n  object StringLiteralCoercion extends TypeCoercionRule {\n    override protected def coerceTypes(plan: LogicalPlan): LogicalPlan = plan resolveExpressions {\n      // Skip nodes who's children have not been resolved yet."
  },
  {
    "id" : "ff980f48-c1bf-4da0-98c0-2c64780be9c7",
    "prId" : 26337,
    "prUrl" : "https://github.com/apache/spark/pull/26337#pullrequestreview-314222138",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f2c25aba-1cb7-47fa-ae14-1ecd8293498d",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "This is a little hacky. Maybe we should introduce `UnresolvedMultiply` and `UnresolvedDivide`, so that we don't need to hack the type coercion rules.\r\n\r\nWe can try it in followup.",
        "createdAt" : "2019-11-08T14:09:49Z",
        "updatedAt" : "2019-11-08T14:09:50Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "5404d701d702e9308ca91f354aa5304d7beac789",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +856,860 @@        DivideInterval(l, r)\n\n      case b @ BinaryOperator(l @ CalendarIntervalType(), r @ NullType()) =>\n        b.withNewChildren(Seq(l, Cast(r, CalendarIntervalType)))\n      case b @ BinaryOperator(l @ NullType(), r @ CalendarIntervalType()) =>"
  }
]