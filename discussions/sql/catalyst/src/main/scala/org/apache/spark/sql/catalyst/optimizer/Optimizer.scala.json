[
  {
    "id" : "cd79d402-0e8b-4076-8d9c-4d93277dd5ea",
    "prId" : 33651,
    "prUrl" : "https://github.com/apache/spark/pull/33651#pullrequestreview-723483793",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cad47f42-f12c-4eb6-9c1f-655b555e308f",
        "parentId" : null,
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "It's flaky that this expression highly depend on `ConstantFolding`.",
        "createdAt" : "2021-08-05T15:13:46Z",
        "updatedAt" : "2021-08-05T15:13:46Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      }
    ],
    "commit" : "6f28a37f420f781486c3e8c102156e02d9f830d7",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +1677,1681 @@\n    case GlobalLimit(le, GlobalLimit(ne, grandChild)) =>\n      GlobalLimit(Literal(Least(Seq(ne, le)).eval().asInstanceOf[Int]), grandChild)\n    case LocalLimit(le, LocalLimit(ne, grandChild)) =>\n      LocalLimit(Literal(Least(Seq(ne, le)).eval().asInstanceOf[Int]), grandChild)"
  },
  {
    "id" : "e658d204-1883-44dc-8966-77b53bdc53ca",
    "prId" : 33587,
    "prUrl" : "https://github.com/apache/spark/pull/33587#pullrequestreview-723105990",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3b87979e-e583-48be-aaf5-b598ce034d3b",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "let's add the default case to be safe:\r\n```\r\ncase (other, _) => other\r\n```",
        "createdAt" : "2021-08-05T08:42:18Z",
        "updatedAt" : "2021-08-05T08:42:19Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "cfac5e7d-e36e-4cde-9fa3-9e9a9ead9751",
        "parentId" : "3b87979e-e583-48be-aaf5-b598ce034d3b",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "Okï¼Œ done",
        "createdAt" : "2021-08-05T08:57:35Z",
        "updatedAt" : "2021-08-05T08:57:35Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "1f93b6f14001d5a9e71cc9af207e12217f36cd5d",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +521,525 @@    projectList.zip(originalNames).map {\n      case (attr: Attribute, name) => attr.withName(name)\n      case (alias: Alias, name) => alias.withName(name)\n      case (other, _) => other\n    }"
  },
  {
    "id" : "258971e3-e28d-4638-8c85-97af7394a661",
    "prId" : 33583,
    "prUrl" : "https://github.com/apache/spark/pull/33583#pullrequestreview-724892088",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f1177a7f-bbca-4e79-8332-93e208610eec",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Could you also update the comment?",
        "createdAt" : "2021-08-08T07:03:12Z",
        "updatedAt" : "2021-08-08T07:03:12Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "a334e694-ed28-469d-ad6b-d2096ee11494",
        "parentId" : "f1177a7f-bbca-4e79-8332-93e208610eec",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "Done",
        "createdAt" : "2021-08-08T11:32:02Z",
        "updatedAt" : "2021-08-08T11:32:02Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "44c744988b9e219f34a96a3a5835aff150607d42",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +47,51 @@  // - is still resolved\n  // - only host special expressions in supported operators\n  // - has globally-unique attribute IDs\n  // - optimized plan have same schema with previous plan.\n  override protected def isPlanIntegral("
  },
  {
    "id" : "42db3364-50fe-4fb7-b770-f58b74bd4ee9",
    "prId" : 33100,
    "prUrl" : "https://github.com/apache/spark/pull/33100#pullrequestreview-693527498",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c9c3b4e9-1c57-4fa6-871f-92d8570dbb41",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "The change here looks good. I wonder if we should better create a separate file to group similar stuff though .... e.g.) we can remove deduplicate too.",
        "createdAt" : "2021-06-28T03:28:55Z",
        "updatedAt" : "2021-06-28T03:28:56Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "82b03f052441b8830100b4202e6037a283aef18f",
    "line" : 25,
    "diffHunk" : "@@ -1,1 +1220,1224 @@\n  private val applyLocally: PartialFunction[LogicalPlan, LogicalPlan] = {\n    case Sort(_, _, child) if child.maxRows.exists(_ <= 1L) => child\n    case s @ Sort(orders, _, child) if orders.isEmpty || orders.exists(_.child.foldable) =>\n      val newOrders = orders.filterNot(_.child.foldable)"
  },
  {
    "id" : "62bcb8d2-b9ac-4128-b95a-e2ac4e78d469",
    "prId" : 33099,
    "prUrl" : "https://github.com/apache/spark/pull/33099#pullrequestreview-694148158",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d508edda-5fbf-40ef-810c-79402e663dab",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "No, we can't do this. `RebalancePartitions` is a best-effort and does not guarantee partitioning. We intentionally do not let `RebalancePartitions` extend `RepartitionOperation`, to avoid wrong optimizations.\r\n\r\nPlease revert this.",
        "createdAt" : "2021-06-28T05:24:28Z",
        "updatedAt" : "2021-06-28T05:24:38Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "ab5420ff-0ef9-4190-a76e-8e0a43360d94",
        "parentId" : "d508edda-5fbf-40ef-810c-79402e663dab",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "See https://github.com/apache/spark/pull/32932#discussion_r656249153",
        "createdAt" : "2021-06-28T05:25:32Z",
        "updatedAt" : "2021-06-28T05:25:33Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "25b01e2a-686b-4af7-ac5c-ee467ef15c49",
        "parentId" : "d508edda-5fbf-40ef-810c-79402e663dab",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "OK",
        "createdAt" : "2021-06-28T08:18:00Z",
        "updatedAt" : "2021-06-28T08:18:00Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      },
      {
        "id" : "6abd5891-125d-4c26-bfa0-e0f84cbded16",
        "parentId" : "d508edda-5fbf-40ef-810c-79402e663dab",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "Reverted at: https://github.com/apache/spark/commit/108635af1708173a72bec0e36bf3f2cea5b088c4",
        "createdAt" : "2021-06-28T08:43:03Z",
        "updatedAt" : "2021-06-28T08:43:03Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      },
      {
        "id" : "ddebaa85-5310-41de-9ad7-81b0e426928a",
        "parentId" : "d508edda-5fbf-40ef-810c-79402e663dab",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Oh, got it. Thank you for reverting, @cloud-fan and @wangyum .",
        "createdAt" : "2021-06-28T16:16:50Z",
        "updatedAt" : "2021-06-28T16:16:51Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "67642d87f7d1799e97f925444484f52354e6af72",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +914,918 @@\n    // Case 3: When a RebalancePartitions has a child of Repartition or RepartitionByExpression\n    // we can remove the child.\n    case r @ RebalancePartitions(_, child: RepartitionOperation) =>\n      r.copy(child = child.child)"
  },
  {
    "id" : "8c82774d-b56c-468f-b4d6-c0a705557303",
    "prId" : 32439,
    "prUrl" : "https://github.com/apache/spark/pull/32439#pullrequestreview-656933962",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "00cb0166-5c63-4606-b4cd-6455dc59448e",
        "parentId" : null,
        "authorId" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "body" : "I might have missed sth. from the previous commit, but what difference is there between regular `transform` and `transformWithPruning(AlwaysProcess.fn, ...)` ?",
        "createdAt" : "2021-05-11T03:50:17Z",
        "updatedAt" : "2021-05-11T06:53:50Z",
        "lastEditedBy" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "tags" : [
        ]
      },
      {
        "id" : "ef9935e5-c3f6-46e6-92b3-f0b96a3c55da",
        "parentId" : "00cb0166-5c63-4606-b4cd-6455dc59448e",
        "authorId" : "8b518862-583d-43a2-a10e-b33ef1b6d824",
        "body" : "`transform` internally calls\r\n`transformWithPruning(AlwaysProcess.fn, UnknownRuleId)` ...\r\n\r\nThe argument comments are here:\r\nhttps://github.com/apache/spark/blob/e08c40fa3f7054bdc713873c99d3aaf0014d9314/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/trees/TreeNode.scala#L434-L441\r\n\r\n So here, \r\n `plan.transformWithPruning(AlwaysProcess.fn, ruleId)` means there's no pruning based on TreePattern bits, but there's pruning based on ruleIds (if the rule is known to ineffective on a tree instance `T`,  it will be skipped next time when it is invoked on the same tree instance `T`).",
        "createdAt" : "2021-05-11T03:54:56Z",
        "updatedAt" : "2021-05-11T06:53:50Z",
        "lastEditedBy" : "8b518862-583d-43a2-a10e-b33ef1b6d824",
        "tags" : [
        ]
      },
      {
        "id" : "f04922ab-afc7-46da-bcae-7b82d3b9e3da",
        "parentId" : "00cb0166-5c63-4606-b4cd-6455dc59448e",
        "authorId" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "body" : "I see. Thanks!",
        "createdAt" : "2021-05-11T16:05:29Z",
        "updatedAt" : "2021-05-11T16:05:29Z",
        "lastEditedBy" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "tags" : [
        ]
      }
    ],
    "commit" : "5027ebcec31e9ea21df0bf16daa3e3427d4fe700",
    "line" : 76,
    "diffHunk" : "@@ -1,1 +756,760 @@\n  def apply(plan: LogicalPlan): LogicalPlan = removeProjectBeforeFilter(\n    plan.transformWithPruning(AlwaysProcess.fn, ruleId) {\n    // Prunes the unused columns from project list of Project/Aggregate/Expand\n    case p @ Project(_, p2: Project) if !p2.outputSet.subsetOf(p.references) =>"
  },
  {
    "id" : "422b02ff-1db1-4e86-9116-78455a60e690",
    "prId" : 31980,
    "prUrl" : "https://github.com/apache/spark/pull/31980#pullrequestreview-622655510",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6b1f75ba-4045-46b8-b4c9-6c65761ea30a",
        "parentId" : null,
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "`w1.references.subsetOf(grandChild.outputSet)` Do we need this check?",
        "createdAt" : "2021-03-27T14:25:45Z",
        "updatedAt" : "2021-04-02T07:14:22Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      },
      {
        "id" : "d230a834-b878-4aeb-bfa1-fbb011ba6801",
        "parentId" : "6b1f75ba-4045-46b8-b4c9-6c65761ea30a",
        "authorId" : "a8e23d47-3ae4-4385-848c-38a216d1bd08",
        "body" : "The test case `SPARK-34807: don't transpose two windows if project between them generates an input column` would fail without it.",
        "createdAt" : "2021-03-27T14:46:32Z",
        "updatedAt" : "2021-04-02T07:14:22Z",
        "lastEditedBy" : "a8e23d47-3ae4-4385-848c-38a216d1bd08",
        "tags" : [
        ]
      }
    ],
    "commit" : "60c6643f0bfda390baa32273f7ca1371a44d5771",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +1002,1006 @@\n    case w1 @ Window(_, _, _, Project(pl, w2 @ Window(_, _, _, grandChild)))\n      if windowsCompatible(w1, w2) && w1.references.subsetOf(grandChild.outputSet) =>\n      Project(\n        pl ++ w1.windowOutputSet,"
  },
  {
    "id" : "6f3247d1-b155-4ce3-9ca4-395595639158",
    "prId" : 31980,
    "prUrl" : "https://github.com/apache/spark/pull/31980#pullrequestreview-622660526",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ef9dad48-b6c1-4903-9d3d-2cf8831ee500",
        "parentId" : null,
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "Can we use `w1.output` instead of `pl ++ w1.windowOutputSet`?",
        "createdAt" : "2021-03-27T15:45:52Z",
        "updatedAt" : "2021-04-02T07:14:22Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      },
      {
        "id" : "7842ad25-1318-49f5-aed0-a0badc0e5c17",
        "parentId" : "ef9dad48-b6c1-4903-9d3d-2cf8831ee500",
        "authorId" : "a8e23d47-3ae4-4385-848c-38a216d1bd08",
        "body" : "The `w1.output` contains only the attributes (name + id), but not how the attributes are computed based on the input attributes. This information is in the project list.",
        "createdAt" : "2021-03-27T15:56:17Z",
        "updatedAt" : "2021-04-02T07:14:22Z",
        "lastEditedBy" : "a8e23d47-3ae4-4385-848c-38a216d1bd08",
        "tags" : [
        ]
      }
    ],
    "commit" : "60c6643f0bfda390baa32273f7ca1371a44d5771",
    "line" : 26,
    "diffHunk" : "@@ -1,1 +1004,1008 @@      if windowsCompatible(w1, w2) && w1.references.subsetOf(grandChild.outputSet) =>\n      Project(\n        pl ++ w1.windowOutputSet,\n        w2.copy(child = w1.copy(child = grandChild)))\n  }"
  },
  {
    "id" : "3a03e160-a9e7-4f0b-8b1b-f974e3b419e6",
    "prId" : 31980,
    "prUrl" : "https://github.com/apache/spark/pull/31980#pullrequestreview-622747073",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1f78dce7-f0c6-42be-aa4c-3f4cdff68d4a",
        "parentId" : null,
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "Two more spaces?",
        "createdAt" : "2021-03-28T13:05:16Z",
        "updatedAt" : "2021-04-02T07:14:22Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      }
    ],
    "commit" : "60c6643f0bfda390baa32273f7ca1371a44d5771",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +998,1002 @@    _.containsPattern(WINDOW), ruleId) {\n    case w1 @ Window(_, _, _, w2 @ Window(_, _, _, grandChild))\n      if windowsCompatible(w1, w2) =>\n      Project(w1.output, w2.copy(child = w1.copy(child = grandChild)))\n"
  },
  {
    "id" : "eecc4e6b-7aa5-45dd-a1ec-42634e192d00",
    "prId" : 31980,
    "prUrl" : "https://github.com/apache/spark/pull/31980#pullrequestreview-622747082",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c4abdcde-d297-43c5-9466-d9c10013cf12",
        "parentId" : null,
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "Two more spaces?",
        "createdAt" : "2021-03-28T13:05:21Z",
        "updatedAt" : "2021-04-02T07:14:22Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      }
    ],
    "commit" : "60c6643f0bfda390baa32273f7ca1371a44d5771",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +1002,1006 @@\n    case w1 @ Window(_, _, _, Project(pl, w2 @ Window(_, _, _, grandChild)))\n      if windowsCompatible(w1, w2) && w1.references.subsetOf(grandChild.outputSet) =>\n      Project(\n        pl ++ w1.windowOutputSet,"
  },
  {
    "id" : "3b40baba-cc77-4b7f-8585-d5654dd8de8d",
    "prId" : 31740,
    "prUrl" : "https://github.com/apache/spark/pull/31740#pullrequestreview-605758877",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5793afbb-e9f3-4f15-b942-78a29e37fa2e",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Looks like this is the case that the logic of sub common exprs should handle? cc: @viirya @cloud-fan ",
        "createdAt" : "2021-03-04T23:50:29Z",
        "updatedAt" : "2021-03-06T14:34:25Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "a51e488b-1414-4b85-80c4-48c2f4897b1f",
        "parentId" : "5793afbb-e9f3-4f15-b942-78a29e37fa2e",
        "authorId" : "a8e23d47-3ae4-4385-848c-38a216d1bd08",
        "body" : "Isn't the subexpression elimination wholestage codegen specific?",
        "createdAt" : "2021-03-05T21:00:30Z",
        "updatedAt" : "2021-03-06T14:34:25Z",
        "lastEditedBy" : "a8e23d47-3ae4-4385-848c-38a216d1bd08",
        "tags" : [
        ]
      },
      {
        "id" : "be0dbd41-cb3e-4237-a333-35af3f8b03e6",
        "parentId" : "5793afbb-e9f3-4f15-b942-78a29e37fa2e",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "The interpreted mode has the optimization, too, e.g., `SubExprEvaluationRuntime`.",
        "createdAt" : "2021-03-06T11:56:18Z",
        "updatedAt" : "2021-03-06T14:34:25Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "11e482fc-f5d6-4384-8191-b7b71bc1e663",
        "parentId" : "5793afbb-e9f3-4f15-b942-78a29e37fa2e",
        "authorId" : "a8e23d47-3ae4-4385-848c-38a216d1bd08",
        "body" : "I see, thanks for the reference.\r\nIndeed, the common subexpression elimination seems to eliminate these duplicates (if I did follow the code correctly).\r\nI would argue, that there still is benefit in removing these duplicates in the optimizer. If nothing else, then it at least would clean up the plan - when I saw the duplicates in the plan, then it prompted me to try to rewrite the query in such way that it would avoid them.\r\nAlso the subexpression cache has a limited size (although configurable) and this would reduce the runtime overhead of cache lookups.\r\n\r\n ",
        "createdAt" : "2021-03-06T14:01:13Z",
        "updatedAt" : "2021-03-06T14:34:25Z",
        "lastEditedBy" : "a8e23d47-3ae4-4385-848c-38a216d1bd08",
        "tags" : [
        ]
      }
    ],
    "commit" : "624a41dd5e0232c746c62629bbb4983259ea9b0c",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +949,953 @@\n/**\n * Replaces duplicate window expressions with an alias in a Project above the Window node.\n */\nobject RemoveDuplicateWindowExprs extends Rule[LogicalPlan] {"
  },
  {
    "id" : "bb41c349-5166-48b0-b718-1c49c514a933",
    "prId" : 31740,
    "prUrl" : "https://github.com/apache/spark/pull/31740#pullrequestreview-606091871",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d9a66b1f-9f4c-4505-a1ca-ba14df935760",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Shall we do the same thing for project and aggregate? Another big idea is to re-implement subexpression elimination in the plan level.",
        "createdAt" : "2021-03-08T09:16:12Z",
        "updatedAt" : "2021-03-08T09:16:12Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "624a41dd5e0232c746c62629bbb4983259ea9b0c",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +951,955 @@ * Replaces duplicate window expressions with an alias in a Project above the Window node.\n */\nobject RemoveDuplicateWindowExprs extends Rule[LogicalPlan] {\n  def apply(plan: LogicalPlan): LogicalPlan = plan transform {\n    case w @ Window(wes, _, _, _) if wes.length > 1 =>"
  },
  {
    "id" : "6e5d187d-dcda-48d6-8b95-bd1eaa239463",
    "prId" : 31739,
    "prUrl" : "https://github.com/apache/spark/pull/31739#pullrequestreview-618928946",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9eafbf53-27fb-4a17-9d7f-a194bbf8d8c5",
        "parentId" : null,
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "Thanks @wangyum for adding this. I think there might be a list of operators safe to push through besides `Project` - e.g. `Sort`, `RepartitionByExpression`, `ScriptTransformation`, etc.\r\n\r\nShall we add push down through `Project` separately? It should not be only restricted to `Project(Join)` right? It can be `Project(OtherOperator)` as well?\r\n\r\n```\r\ncase LocalLimit(exp, p: Project) =>\r\n  LocalLimit(exp, p.copy(maybePushLocalLimit(exp, _)))\r\n```",
        "createdAt" : "2021-03-04T21:38:33Z",
        "updatedAt" : "2021-03-29T13:38:22Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      },
      {
        "id" : "6e3f2501-3eb4-45eb-9c1b-e03f4b414d3b",
        "parentId" : "9eafbf53-27fb-4a17-9d7f-a194bbf8d8c5",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "I change it to `case LocalLimit(exp, project @ Project(_, child)) if !child.isInstanceOf[LeafNode] =>` to avoid do some useless work if direct pushdown limit through Project:\r\n```\r\n=== Applying Rule org.apache.spark.sql.catalyst.optimizer.LimitPushDown ===\r\n GlobalLimit 5                                                GlobalLimit 5\r\n +- LocalLimit 5                                              +- LocalLimit 5\r\n    +- Project [a#0]                                             +- Project [a#0]\r\n!      +- Join LeftOuter, ((a#0 = x#3) AND (b#1 = y#4))             +- LocalLimit 5\r\n!         :- LocalLimit 5                                              +- Join LeftOuter, ((a#0 = x#3) AND (b#1 = y#4))\r\n!         :  +- Project [a#0, b#1]                                        :- LocalLimit 5\r\n!         :     +- Relation default.t1[a#0,b#1,c#2] parquet               :  +- Project [a#0, b#1]\r\n!         +- Project [x#3, y#4]                                           :     +- LocalLimit 5\r\n!            +- Filter (isnotnull(x#3) AND isnotnull(y#4))                :        +- Relation default.t1[a#0,b#1,c#2] parquet\r\n!               +- Relation default.t2[x#3,y#4,z#5] parquet               +- Project [x#3, y#4]\r\n!                                                                            +- Filter (isnotnull(x#3) AND isnotnull(y#4))\r\n!                                                                               +- Relation default.t2[x#3,y#4,z#5] parquet\r\n\r\n\r\n=== Applying Rule org.apache.spark.sql.catalyst.optimizer.CollapseProject ===\r\n GlobalLimit 5                                                            GlobalLimit 5\r\n +- LocalLimit 5                                                          +- LocalLimit 5\r\n!   +- Project [a#0]                                                         +- LocalLimit 5\r\n!      +- LocalLimit 5                                                          +- Project [a#0]\r\n!         +- Project [a#0]                                                         +- Join LeftOuter, ((a#0 = x#3) AND (b#1 = y#4))\r\n!            +- Join LeftOuter, ((a#0 = x#3) AND (b#1 = y#4))                         :- LocalLimit 5\r\n!               :- LocalLimit 5                                                       :  +- LocalLimit 5\r\n!               :  +- Project [a#0, b#1]                                              :     +- Project [a#0, b#1]\r\n!               :     +- LocalLimit 5                                                 :        +- Relation default.t1[a#0,b#1,c#2] parquet\r\n!               :        +- Project [a#0, b#1]                                        +- Project [x#3, y#4]\r\n!               :           +- Relation default.t1[a#0,b#1,c#2] parquet                  +- Filter (isnotnull(x#3) AND isnotnull(y#4))\r\n!               +- Project [x#3, y#4]                                                       +- Relation default.t2[x#3,y#4,z#5] parquet\r\n!                  +- Filter (isnotnull(x#3) AND isnotnull(y#4))          \r\n!                     +- Relation default.t2[x#3,y#4,z#5] parquet         \r\n           \r\n=== Applying Rule org.apache.spark.sql.catalyst.optimizer.EliminateLimits ===\r\n GlobalLimit 5                                                      GlobalLimit 5\r\n!+- LocalLimit 5                                                    +- LocalLimit least(5, 5)\r\n!   +- LocalLimit 5                                                    +- Project [a#0]\r\n!      +- Project [a#0]                                                   +- Join LeftOuter, ((a#0 = x#3) AND (b#1 = y#4))\r\n!         +- Join LeftOuter, ((a#0 = x#3) AND (b#1 = y#4))                   :- LocalLimit least(5, 5)\r\n!            :- LocalLimit 5                                                 :  +- Project [a#0, b#1]\r\n!            :  +- LocalLimit 5                                              :     +- Relation default.t1[a#0,b#1,c#2] parquet\r\n!            :     +- Project [a#0, b#1]                                     +- Project [x#3, y#4]\r\n!            :        +- Relation default.t1[a#0,b#1,c#2] parquet               +- Filter (isnotnull(x#3) AND isnotnull(y#4))\r\n!            +- Project [x#3, y#4]                                                 +- Relation default.t2[x#3,y#4,z#5] parquet\r\n!               +- Filter (isnotnull(x#3) AND isnotnull(y#4))       \r\n!                  +- Relation default.t2[x#3,y#4,z#5] parquet      \r\n \r\n```",
        "createdAt" : "2021-03-05T00:52:58Z",
        "updatedAt" : "2021-03-29T13:38:22Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      },
      {
        "id" : "a06e76c3-e24f-4559-9a93-1f37f051ed06",
        "parentId" : "9eafbf53-27fb-4a17-9d7f-a194bbf8d8c5",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Sounds like a good idea. We can have an allowlist of unary operators.",
        "createdAt" : "2021-03-22T15:47:44Z",
        "updatedAt" : "2021-03-29T13:38:22Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "1e7d73ff-c0ec-4ba9-adb3-65c93af7b3de",
        "parentId" : "9eafbf53-27fb-4a17-9d7f-a194bbf8d8c5",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "It will introduce useless pushdown even only allow `Join`.\r\n```scala\r\n    case LocalLimit(exp, p: Project) if p.child.isInstanceOf[Join] =>\r\n      LocalLimit(exp, p.copy(child = maybePushLocalLimit(exp, p.child)))\r\n```\r\nFor example:\r\n```scala\r\nspark.range(200L).selectExpr(\"id AS a\", \"id AS b\").write.saveAsTable(\"t1\")\r\nspark.range(300L).selectExpr(\"id AS x\", \"id AS y\").write.saveAsTable(\"t2\")\r\nspark.sql(\"SELECT 1 FROM t1 INNER JOIN t2 ON a = x limit 10\").explain(true)\r\n```\r\n\r\n```\r\n== Optimized Logical Plan ==\r\nGlobalLimit 10\r\n+- LocalLimit 10\r\n   +- Project [1 AS 1#20]\r\n      +- LocalLimit 10\r\n         +- Project\r\n            +- Join Inner, (a#16L = x#18L)\r\n               :- Project [a#16L]\r\n               :  +- Filter isnotnull(a#16L)\r\n               :     +- Relation default.t1[a#16L,b#17L] parquet\r\n               +- Project [x#18L]\r\n                  +- Filter isnotnull(x#18L)\r\n                     +- Relation default.t2[x#18L,y#19L] parquet\r\n\r\n== Physical Plan ==\r\nAdaptiveSparkPlan isFinalPlan=false\r\n+- CollectLimit 10\r\n   +- Project [1 AS 1#20]\r\n      +- LocalLimit 10\r\n         +- Project\r\n            +- BroadcastHashJoin [a#16L], [x#18L], Inner, BuildLeft, false\r\n               :- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [id=#68]\r\n               :  +- Filter isnotnull(a#16L)\r\n               :     +- FileScan parquet default.t1[a#16L] \r\n               +- Filter isnotnull(x#18L)\r\n                  +- FileScan parquet default.t2[x#18L]\r\n\r\n```\r\n\r\nAnother example is TPC-DS q32:\r\nhttps://github.com/apache/spark/blob/66f5a42ca5d259038f0749ae2b9a04cc2f658880/sql/core/src/test/resources/tpcds/q32.sql#L1-L15\r\n",
        "createdAt" : "2021-03-22T23:59:31Z",
        "updatedAt" : "2021-03-29T13:38:22Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      },
      {
        "id" : "dd5fbc66-05e0-46ec-b03c-cdabe748b348",
        "parentId" : "9eafbf53-27fb-4a17-9d7f-a194bbf8d8c5",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "@c21 @maropu @cloud-fan \r\nI think `Project` is the most used, and this will not introduce useless `LocalLimit`.",
        "createdAt" : "2021-03-23T07:53:58Z",
        "updatedAt" : "2021-03-29T13:38:22Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      },
      {
        "id" : "a95fa0e8-3ce3-48f1-b22b-d493e8a3ffe3",
        "parentId" : "9eafbf53-27fb-4a17-9d7f-a194bbf8d8c5",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "@wangyum and @cloud-fan - I think one followup can be to pass `LocalLimit` through eligible operators. We don't add more `LocalLimit` operators, but push the existing `LocalLimit` further down in query plan.",
        "createdAt" : "2021-03-23T18:20:35Z",
        "updatedAt" : "2021-03-29T13:38:22Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      }
    ],
    "commit" : "4c7bc54c69ca347c01d84ac8d2ab54033d96aadf",
    "line" : 49,
    "diffHunk" : "@@ -1,1 +674,678 @@      LocalLimit(exp, pushLocalLimitThroughJoin(exp, join))\n    // There is a Project between LocalLimit and Join if they do not have the same output.\n    case LocalLimit(exp, project @ Project(_, join: Join)) =>\n      LocalLimit(exp, project.copy(child = pushLocalLimitThroughJoin(exp, join)))\n  }"
  },
  {
    "id" : "787dc104-2184-4703-95b1-b7be63642260",
    "prId" : 31677,
    "prUrl" : "https://github.com/apache/spark/pull/31677#pullrequestreview-622714876",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "16cb3ad1-48a6-425c-9ae4-d3751a88bf5c",
        "parentId" : null,
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "Could we move `windowsCompatible` before `apply`. We usually put the private function before the `apply`. See `TransposeWindow`.",
        "createdAt" : "2021-03-28T02:13:24Z",
        "updatedAt" : "2021-03-28T13:48:42Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      }
    ],
    "commit" : "7097ec3c724356a0d4c226b1222845b6df738e39",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +951,955 @@ */\nobject CollapseWindow extends Rule[LogicalPlan] {\n  private def windowsCompatible(w1: Window, w2: Window): Boolean = {\n    w1.partitionSpec == w2.partitionSpec &&\n      w1.orderSpec == w2.orderSpec &&"
  },
  {
    "id" : "651c8635-ce67-4aa5-a7d7-c475cb37dac1",
    "prId" : 31656,
    "prUrl" : "https://github.com/apache/spark/pull/31656#pullrequestreview-599964928",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9a6ec167-10e1-429b-bcf8-a3cd8edb0ab7",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "We need to keep `LogicalPlan` itself? We cannot use `semanticHash` instead?",
        "createdAt" : "2021-02-26T12:31:04Z",
        "updatedAt" : "2021-03-01T07:38:01Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "40586090-8d54-4ce6-a16e-210576b3eb42",
        "parentId" : "9a6ec167-10e1-429b-bcf8-a3cd8edb0ab7",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "I think `semanticHash` also works. Can use it.",
        "createdAt" : "2021-02-26T21:43:47Z",
        "updatedAt" : "2021-03-01T07:38:01Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "ef057af3-3a1f-4bda-bbb8-66fc68f08843",
        "parentId" : "9a6ec167-10e1-429b-bcf8-a3cd8edb0ab7",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Oh, rethinking about it, isn't `canonicalized` more reliable? It is less likely, but hash code might collision.",
        "createdAt" : "2021-02-26T21:46:42Z",
        "updatedAt" : "2021-03-01T07:38:01Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "6a420d497d91be512ad3e0317756dcc7f96e605d",
    "line" : 31,
    "diffHunk" : "@@ -1,1 +530,534 @@  private def simplifyUnion(u: Union): LogicalPlan = {\n    val uniqueChildren = mutable.ArrayBuffer.empty[LogicalPlan]\n    val uniqueChildrenKey = mutable.HashSet.empty[LogicalPlan]\n\n    u.children.foreach { c =>"
  },
  {
    "id" : "8addd2f9-6560-4fb3-9915-6b677d7f5f43",
    "prId" : 31656,
    "prUrl" : "https://github.com/apache/spark/pull/31656#pullrequestreview-600061245",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ecea004c-94ff-4d85-8f33-8d2ab79d97d6",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Does the added test in `DataFrameSetOperationsSuite` fail without this reorder? The change itself looks reasonable though.",
        "createdAt" : "2021-02-26T12:36:07Z",
        "updatedAt" : "2021-03-01T07:38:01Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "591c08b1-f6a4-4d85-a189-2b62aa196546",
        "parentId" : "ecea004c-94ff-4d85-8f33-8d2ab79d97d6",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Added test is fine. But there will be an [error](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/135486/testReport/org.apache.spark.sql/SQLQueryTestSuite/subquery_in_subquery_in_set_operations_sql/) of \"Once strategy's idempotence is broken for batch Union\".\r\n\r\nIt is because `CombineUnions` might combine two or more `Union` and they have redundant children under `Distinct`. So the rule order `RemoveNoopUnion` -> `CombineUnions` is not idempotence for Once strategy. ",
        "createdAt" : "2021-02-26T17:25:25Z",
        "updatedAt" : "2021-03-01T07:38:01Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "b915524d-ea7d-4447-ad9a-36d44a6d00e8",
        "parentId" : "ecea004c-94ff-4d85-8f33-8d2ab79d97d6",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Ah, I see.",
        "createdAt" : "2021-02-27T01:43:04Z",
        "updatedAt" : "2021-03-01T07:38:01Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "6a420d497d91be512ad3e0317756dcc7f96e605d",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +159,163 @@      RemoveNoopOperators,\n      CombineUnions,\n      RemoveNoopUnion) ::\n    Batch(\"OptimizeLimitZero\", Once,\n      OptimizeLimitZero) ::"
  },
  {
    "id" : "5ec3057c-722f-4bb9-bcfd-17d82c12f028",
    "prId" : 31656,
    "prUrl" : "https://github.com/apache/spark/pull/31656#pullrequestreview-600450725",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8d450b3d-d965-4066-9948-b8c3878c8952",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "We need plan canonicalization here? It looks enough to check if the output is the same just like `LogicalPlan.sameOutput`?",
        "createdAt" : "2021-02-27T01:51:19Z",
        "updatedAt" : "2021-03-01T07:38:01Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "a36c23bd-4fce-45f2-aad6-92ffe1e6a8f9",
        "parentId" : "8d450b3d-d965-4066-9948-b8c3878c8952",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "`canonicalized` should cover more cases than `sameOutput`. Note that by definition two plans with same `canonicalized` plans will always evaluate to the same result. But their outputs might not be the same.",
        "createdAt" : "2021-03-01T07:36:11Z",
        "updatedAt" : "2021-03-01T07:38:01Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "6a420d497d91be512ad3e0317756dcc7f96e605d",
    "line" : 34,
    "diffHunk" : "@@ -1,1 +533,537 @@\n    u.children.foreach { c =>\n      val key = removeAliasOnlyProject(c).canonicalized\n      if (!uniqueChildrenKey.contains(key)) {\n        uniqueChildren += c"
  },
  {
    "id" : "f8b89e8c-7417-4738-9964-ed6fb518f074",
    "prId" : 31630,
    "prUrl" : "https://github.com/apache/spark/pull/31630#pullrequestreview-597395340",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "219add80-8145-4053-80c7-20d44d893863",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "hm, in this case, we need the join itself?\r\n```\r\nscala> sql(\"select * from l1\").show()\r\n+----+\r\n|  id|\r\n+----+\r\n|   1|\r\n|   2|\r\n|null|\r\n+----+\r\n\r\n\r\nscala> sql(\"select * from r1\").show()\r\n+----+\r\n|  id|\r\n+----+\r\n|   2|\r\n|null|\r\n+----+\r\n\r\n\r\nscala> sql(\"select * from l1 left semi join r1\").show()\r\n+----+\r\n|  id|\r\n+----+\r\n|   1|\r\n|   2|\r\n|null|\r\n+----+\r\n\r\n\r\nscala> sql(\"select * from l1 left anti join r1\").show()\r\n+---+\r\n| id|\r\n+---+\r\n+---+\r\n```",
        "createdAt" : "2021-02-24T04:09:38Z",
        "updatedAt" : "2021-02-24T06:42:46Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "3005e518-a212-45a4-945a-7071fe40da58",
        "parentId" : "219add80-8145-4053-80c7-20d44d893863",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "I think we still need. Whether to output all rows or nothing, is depending on whether right side is empty, and this can only be known during runtime.",
        "createdAt" : "2021-02-24T04:19:54Z",
        "updatedAt" : "2021-02-24T06:42:46Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      },
      {
        "id" : "fb7db00f-35f4-4cb0-9276-9fd5850d596e",
        "parentId" : "219add80-8145-4053-80c7-20d44d893863",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "@maropu - this actually reminds me whether we can further optimize during runtime, and I found I already did it for LEFT SEMI with AQE - https://github.com/apache/spark/pull/29484 . Similarly for LEFT ANTI join without condition, we can convert join logical plan node to an empty relation if right build side is not empty. Will submit a followup PR tomorrow.\r\n\r\nIn addition, after taking a deep look at `BroadcastNestedLoopJoinExec` (never looked closely to that because it's not popular in our environment), I found many places that we can optimize:\r\n* populate `outputOrdering` and `outputPartitioning` when possible to avoid shuffle/sort in later stage.\r\n* shortcut for `LEFT SEMI/ANTI` in `defaultJoin()` as we don't need to look through all rows when there's no join condition.\r\n* code-gen the operator.\r\n\r\nI will file an umbrella JIRA with minor priority and do it gradually.",
        "createdAt" : "2021-02-24T09:02:42Z",
        "updatedAt" : "2021-02-24T09:02:43Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      },
      {
        "id" : "746ce87f-5db7-44aa-964a-873bb910af96",
        "parentId" : "219add80-8145-4053-80c7-20d44d893863",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "> Similarly for LEFT ANTI join without condition, we can convert join logical plan node to an empty relation if right build side is not empty. Will submit a followup PR tomorrow.\r\n\r\nAh, I see. That sounds reasonable.  Nice idea, @c21 .",
        "createdAt" : "2021-02-24T11:52:24Z",
        "updatedAt" : "2021-02-24T11:52:25Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "22bfd5e13ffb5b51a6a29851161146a1af6a5666",
    "line" : 37,
    "diffHunk" : "@@ -1,1 +561,565 @@            right = maybePushLocalLimit(exp, right))\n        case LeftSemi | LeftAnti if conditionOpt.isEmpty =>\n          join.copy(left = maybePushLocalLimit(exp, left))\n        case _ => join\n      }"
  },
  {
    "id" : "dc09c4d1-1ab6-40b0-93aa-16e2f242e559",
    "prId" : 31595,
    "prUrl" : "https://github.com/apache/spark/pull/31595#pullrequestreview-594798657",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1e720cfa-b11b-48c6-a2a6-a0a64d80a326",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "If we handle only two cases (`Distinct` and `Deduplicate`), let's say explicitly `Distinct/Deduplicate` instead of saying `Distinct like operators`. In general, we use a `whitelist` approach.",
        "createdAt" : "2021-02-21T01:34:49Z",
        "updatedAt" : "2021-02-25T04:01:53Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "b5d704112549310e2592aad21bd8308386ff0cf8",
    "line" : 49,
    "diffHunk" : "@@ -1,1 +540,544 @@      removeUnion(u).map(c => d.withNewChildren(Seq(c))).getOrElse(d)\n\n    case d @ Deduplicate(_, u: Union) =>\n      removeUnion(u).map(c => d.withNewChildren(Seq(c))).getOrElse(d)\n  }"
  },
  {
    "id" : "cd503e49-5b2d-4c35-a96f-854af7e378b6",
    "prId" : 31595,
    "prUrl" : "https://github.com/apache/spark/pull/31595#pullrequestreview-595772582",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f1ae7f79-873b-4f72-9b4f-d33ffbf772a8",
        "parentId" : null,
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "I am wondering why this is needed, given we already have `Eliminate no-op Projects` below at line 511. Shouldn't this kind of alias-only project already be removed by that?",
        "createdAt" : "2021-02-22T07:26:39Z",
        "updatedAt" : "2021-02-25T04:01:53Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      },
      {
        "id" : "73b3a6a6-4dbb-4ce7-9ec1-7b245d323e49",
        "parentId" : "f1ae7f79-873b-4f72-9b4f-d33ffbf772a8",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "Also just FYI there's some ongoing change for this same rule - https://github.com/apache/spark/pull/31538 , ",
        "createdAt" : "2021-02-22T07:31:56Z",
        "updatedAt" : "2021-02-25T04:01:53Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      },
      {
        "id" : "f73560d8-3437-40a8-8555-3590cd274c9d",
        "parentId" : "f1ae7f79-873b-4f72-9b4f-d33ffbf772a8",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Alias-only project is not no-op project here. It will change the outputs.",
        "createdAt" : "2021-02-22T20:53:46Z",
        "updatedAt" : "2021-02-25T04:01:53Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "b5d704112549310e2592aad21bd8308386ff0cf8",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +511,515 @@   * from its child.\n   */\n  private def removeAliasOnlyProject(plan: LogicalPlan): LogicalPlan = plan match {\n    case p @ Project(projectList, child) =>\n      val aliasOnly = projectList.length == child.output.length &&"
  },
  {
    "id" : "15d7c702-ef7d-4c5b-971b-0639a9e8e487",
    "prId" : 31595,
    "prUrl" : "https://github.com/apache/spark/pull/31595#pullrequestreview-595993938",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "43785d49-1e00-4c94-958a-8c2ebe399a8a",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Thank you for splitting the rule!",
        "createdAt" : "2021-02-23T04:48:33Z",
        "updatedAt" : "2021-02-25T04:01:53Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "b5d704112549310e2592aad21bd8308386ff0cf8",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +158,162 @@    Batch(\"Union\", Once,\n      RemoveNoopOperators,\n      RemoveNoopUnion,\n      CombineUnions) ::\n    Batch(\"OptimizeLimitZero\", Once,"
  },
  {
    "id" : "35bc364f-2a18-4e45-8271-101990d787fa",
    "prId" : 31595,
    "prUrl" : "https://github.com/apache/spark/pull/31595#pullrequestreview-596999205",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a3576640-7c17-440e-b7ca-1608622c29d8",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "How about a `Aggregate` case?",
        "createdAt" : "2021-02-24T00:49:44Z",
        "updatedAt" : "2021-02-25T04:01:53Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "0b377946-7976-451e-b372-2659b01fd9e5",
        "parentId" : "a3576640-7c17-440e-b7ca-1608622c29d8",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "You mean a no-op Aggregation like `select key as key2, value from table group by key value`?",
        "createdAt" : "2021-02-24T01:32:29Z",
        "updatedAt" : "2021-02-25T04:01:53Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "b480d3a1-1b45-4cc1-9ae7-d23f46c8f1d0",
        "parentId" : "a3576640-7c17-440e-b7ca-1608622c29d8",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "I thought it was like this:\r\n```\r\nscala> sql(\"select distinct * from (select id from t group by id union select id as key from t group by id)\").explain(true)\r\n...\r\n== Analyzed Logical Plan ==\r\nid: bigint\r\nDistinct\r\n+- Project [id#8L]\r\n   +- SubqueryAlias __auto_generated_subquery_name\r\n      +- Distinct\r\n         +- Union false, false\r\n            :- Aggregate [id#8L], [id#8L]\r\n            :  +- SubqueryAlias spark_catalog.default.t\r\n            :     +- Relation[id#8L] parquet\r\n            +- Aggregate [id#8L], [id#8L AS key#7L]\r\n               +- SubqueryAlias spark_catalog.default.t\r\n                  +- Relation[id#8L] parquet\r\n\r\n== Optimized Logical Plan ==\r\nAggregate [id#8L], [id#8L]\r\n+- Aggregate [id#8L], [id#8L]\r\n   +- Union false, false\r\n      :- Aggregate [id#8L], [id#8L]\r\n      :  +- Relation[id#8L] parquet\r\n      +- Aggregate [id#8L], [id#8L AS key#7L]\r\n         +- Relation[id#8L] parquet\r\n```",
        "createdAt" : "2021-02-24T01:43:12Z",
        "updatedAt" : "2021-02-25T04:01:53Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "88481667-a10c-4c16-bdb9-94d679bab002",
        "parentId" : "a3576640-7c17-440e-b7ca-1608622c29d8",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "yea, that is what I said a no-op aggregate. I think it is okay to add it. Maybe I can add it in follow up.",
        "createdAt" : "2021-02-24T02:03:00Z",
        "updatedAt" : "2021-02-25T04:01:53Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "50b9d306-22e7-4f53-9c1b-79c2038dbaf7",
        "parentId" : "a3576640-7c17-440e-b7ca-1608622c29d8",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "yea, sgtm.",
        "createdAt" : "2021-02-24T02:06:30Z",
        "updatedAt" : "2021-02-25T04:01:53Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "b5d704112549310e2592aad21bd8308386ff0cf8",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +512,516 @@   */\n  private def removeAliasOnlyProject(plan: LogicalPlan): LogicalPlan = plan match {\n    case p @ Project(projectList, child) =>\n      val aliasOnly = projectList.length == child.output.length &&\n        projectList.zip(child.output).forall {"
  },
  {
    "id" : "5ffa3d52-b8e9-43f4-8c39-965ef6162b73",
    "prId" : 31567,
    "prUrl" : "https://github.com/apache/spark/pull/31567#pullrequestreview-590873109",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e5b45719-a23b-4f5f-81af-367747d12dc6",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Shall we update the comments here too to describe the case when we push the limit with cross join?",
        "createdAt" : "2021-02-16T04:13:14Z",
        "updatedAt" : "2021-02-23T03:20:22Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "e0915b93a6cd528b6c07e603b98762a1134a7640",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +545,549 @@    // below FULL OUTER JOIN in the general case without a more invasive rewrite.\n    // We also need to ensure that this limit pushdown rule will not eventually introduce limits\n    // on both sides if it is applied multiple times. Therefore:\n    //   - If one side is already limited, stack another limit on top if the new limit is smaller.\n    //     The redundant limit will be collapsed by the CombineLimits rule."
  },
  {
    "id" : "c7488cbe-b793-4763-b953-77b29f42cd47",
    "prId" : 31567,
    "prUrl" : "https://github.com/apache/spark/pull/31567#pullrequestreview-591899952",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "86ad22ad-c3cf-48c2-8492-322dd541adc1",
        "parentId" : null,
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "It seems that we can also push down limit into left side, for LEFT SEMI and LEFT ANTI join, right?\r\nI can create a minor PR if it's not on your plan @wangyum , thanks.",
        "createdAt" : "2021-02-17T05:42:03Z",
        "updatedAt" : "2021-02-23T03:20:22Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      },
      {
        "id" : "2659c7b5-fd93-4d4d-a599-0266c680ed39",
        "parentId" : "86ad22ad-c3cf-48c2-8492-322dd541adc1",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "It seems we can not pushdown `LEFT SEMI JOIN`, for example:\r\n```scala\r\nspark.range(20).selectExpr(\"id % 10 as id\").repartition(1).write.saveAsTable(\"t1\")\r\nspark.range(5, 9, 1).repartition(1).write.saveAsTable(\"t2\")\r\nval df = spark.sql(\"select * from t1 LEFT SEMI JOIN t2 on t1.id = t2.id limit 3\")\r\ndf.explain()\r\ndf.show\r\n```\r\n\r\nCurrent:\r\n```\r\n== Physical Plan ==\r\nAdaptiveSparkPlan isFinalPlan=false\r\n+- CollectLimit 3\r\n   +- BroadcastHashJoin [id#10L], [id#11L], LeftSemi, BuildRight, false\r\n      :- Filter isnotnull(id#10L)\r\n      :  +- FileScan parquet default.t1[id#10L] Batched: true, DataFilters: [isnotnull(id#10L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/yumwang/spark/SPARK-28169/spark-warehouse/org.apache.spark..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:bigint>\r\n      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [id=#69]\r\n         +- Filter isnotnull(id#11L)\r\n            +- FileScan parquet default.t2[id#11L] Batched: true, DataFilters: [isnotnull(id#11L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/yumwang/spark/SPARK-28169/spark-warehouse/org.apache.spark..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:bigint>\r\n\r\n\r\n+---+\r\n| id|\r\n+---+\r\n|  5|\r\n|  6|\r\n|  7|\r\n+---+\r\n```\r\n\r\nPushdown:\r\n```\r\n== Physical Plan ==\r\nAdaptiveSparkPlan isFinalPlan=false\r\n+- CollectLimit 3\r\n   +- BroadcastHashJoin [id#10L], [id#11L], LeftSemi, BuildRight, false\r\n      :- LocalLimit 3\r\n      :  +- Filter isnotnull(id#10L)\r\n      :     +- LocalLimit 3\r\n      :        +- FileScan parquet default.t1[id#10L] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/yumwang/spark/SPARK-28169/spark-warehouse/org.apache.spark..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:bigint>\r\n      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [id=#77]\r\n         +- Filter isnotnull(id#11L)\r\n            +- FileScan parquet default.t2[id#11L] Batched: true, DataFilters: [isnotnull(id#11L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/yumwang/spark/SPARK-28169/spark-warehouse/org.apache.spark..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:bigint>\r\n\r\n\r\n+---+\r\n| id|\r\n+---+\r\n+---+\r\n```",
        "createdAt" : "2021-02-17T07:13:10Z",
        "updatedAt" : "2021-02-23T03:20:22Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      },
      {
        "id" : "aed680e6-a507-41ae-a477-49a4419b6d45",
        "parentId" : "86ad22ad-c3cf-48c2-8492-322dd541adc1",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "@wangyum - yes, you are right. My bad. LEFT OUTER/RIGHT OUTER join will output the left/right side rows anyway no matter of being matched or not. So they are safe for limit push down, but not LEFT SEMI/LEFT ANTI.",
        "createdAt" : "2021-02-17T07:20:19Z",
        "updatedAt" : "2021-02-23T03:20:22Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      },
      {
        "id" : "0e55ea6a-b10c-44f6-9696-f99756f2358b",
        "parentId" : "86ad22ad-c3cf-48c2-8492-322dd541adc1",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "Actually, what about LEFT SEMI / LEFT ANTI join without condition? They should be planned into physical operator `BroadcastNestedLoopJoin` instead. ",
        "createdAt" : "2021-02-17T07:28:02Z",
        "updatedAt" : "2021-02-23T03:20:22Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      }
    ],
    "commit" : "e0915b93a6cd528b6c07e603b98762a1134a7640",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +551,555 @@      val newJoin = joinType match {\n        case RightOuter => join.copy(right = maybePushLocalLimit(exp, right))\n        case LeftOuter => join.copy(left = maybePushLocalLimit(exp, left))\n        case _: InnerLike if conditionOpt.isEmpty =>\n          join.copy("
  },
  {
    "id" : "0076c246-974b-4e8f-99ac-b0f2bff2a585",
    "prId" : 31538,
    "prUrl" : "https://github.com/apache/spark/pull/31538#pullrequestreview-588906078",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1d2698ad-adfb-4ef6-9d75-b92f193dfc6f",
        "parentId" : null,
        "authorId" : "a8e23d47-3ae4-4385-848c-38a216d1bd08",
        "body" : "Would this affect other places where the `sameOutput` method is used? It kinda looks like fixing a symptom and the real problem is a bit deeper. The `sameOutput` is used only in few places, but it internally uses `semanticEquals` that is very common.",
        "createdAt" : "2021-02-11T19:16:22Z",
        "updatedAt" : "2021-02-11T19:16:22Z",
        "lastEditedBy" : "a8e23d47-3ae4-4385-848c-38a216d1bd08",
        "tags" : [
        ]
      },
      {
        "id" : "2595b143-1e98-4cbf-bb82-86b263b68ad4",
        "parentId" : "1d2698ad-adfb-4ef6-9d75-b92f193dfc6f",
        "authorId" : "a8e23d47-3ae4-4385-848c-38a216d1bd08",
        "body" : "In the `LogicalPlan` there is this comment:\r\n```\r\n  /**\r\n   * This method checks if the same `ExprId` refers to an unique attribute in a plan tree.\r\n   * Some plan transformers (e.g., `RemoveNoopOperators`) rewrite logical\r\n   * plans based on this assumption.\r\n   */\r\n  def checkIfExprIdsAreGloballyUnique(plan: LogicalPlan): Boolean = {\r\n    checkIfSameExprIdNotReused(plan) && hasUniqueExprIdsForOutput(plan)\r\n  }\r\n```\r\n\r\nIt sounds like the `RemoveNoopOperators` is correct and the deeper issue is with the `ExprId` getting reused.\r\n",
        "createdAt" : "2021-02-11T19:22:28Z",
        "updatedAt" : "2021-02-11T19:22:28Z",
        "lastEditedBy" : "a8e23d47-3ae4-4385-848c-38a216d1bd08",
        "tags" : [
        ]
      }
    ],
    "commit" : "9212a85ddb82ffc2178b3e8e6c361eb18bfb3e6a",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +496,500 @@    // Eliminate no-op Projects\n    case p @ Project(projectList, child)\n      if projectList.forall(isAttribute) && child.sameOutput(p) => child\n\n    // Eliminate no-op Window"
  },
  {
    "id" : "69de1e81-6863-4e87-8ab9-6b05c89ab4c8",
    "prId" : 31404,
    "prUrl" : "https://github.com/apache/spark/pull/31404#pullrequestreview-590023719",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ed505b24-5011-432f-8095-cdb6ab531db0",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Why we need move this rule?",
        "createdAt" : "2021-02-10T20:44:01Z",
        "updatedAt" : "2021-02-19T02:38:37Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "425e9634-3a55-4873-8e78-fbc0e7ec9720",
        "parentId" : "ed505b24-5011-432f-8095-cdb6ab531db0",
        "authorId" : "ab8f521d-7cf6-437f-8ea5-922bd77ef56b",
        "body" : "It needs to put the rule 'CombineUnions' before ReplaceDistinctWithAggregate and ReplaceDeduplicateWithAggregate, otherwise these two rules will replace Distinct/Deduplicate to Aggregate and lead to rule 'CombineUnions' is invalid. On the other hand putting the rule 'ReplaceDeduplicateWithAggregate' in batch 'Replace Operators' is more intuitional.",
        "createdAt" : "2021-02-14T08:06:20Z",
        "updatedAt" : "2021-02-19T02:38:37Z",
        "lastEditedBy" : "ab8f521d-7cf6-437f-8ea5-922bd77ef56b",
        "tags" : [
        ]
      }
    ],
    "commit" : "1385f2b5963a6ec88dc9d9881f2a4bdd5c735e7a",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +185,189 @@      ReplaceExceptWithAntiJoin,\n      ReplaceDistinctWithAggregate,\n      ReplaceDeduplicateWithAggregate) ::\n    Batch(\"Aggregate\", fixedPoint,\n      RemoveLiteralFromGroupExpressions,"
  },
  {
    "id" : "1ae319f6-a6bf-4db9-beed-78ccd8b3ea53",
    "prId" : 31045,
    "prUrl" : "https://github.com/apache/spark/pull/31045#pullrequestreview-562925959",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8860136f-95ed-4779-a142-4d63bb0768c5",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Just out of curiosity, this rule can always make performance better? For example, how about the case where a right table has too many duplicates? ",
        "createdAt" : "2021-01-05T23:15:34Z",
        "updatedAt" : "2021-01-06T17:31:26Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "629e2fcd-3ec4-4d4f-8316-9bedb9124edb",
        "parentId" : "8860136f-95ed-4779-a142-4d63bb0768c5",
        "authorId" : "a8e23d47-3ae4-4385-848c-38a216d1bd08",
        "body" : "I benchmarked a bit and you are right - this can cause a performance regression. For example q87 took a 10% hit on sf100. I think it's best to close this PR and issue.",
        "createdAt" : "2021-01-06T18:02:08Z",
        "updatedAt" : "2021-01-06T18:02:08Z",
        "lastEditedBy" : "a8e23d47-3ae4-4385-848c-38a216d1bd08",
        "tags" : [
        ]
      }
    ],
    "commit" : "3bb7cd59433498c6b3d1d6df14d166b082e7d0ce",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +1618,1622 @@  def apply(plan: LogicalPlan): LogicalPlan = plan transformDown {\n    // The [[Distinct]] can be ignored if it is the right child of a left semi or anti join.\n    case j @ Join(_, Distinct(right), LeftSemiOrAnti(_), _, _) => j.copy(right = right)\n    case Distinct(child) => Aggregate(child.output, child.output, child)\n  }"
  },
  {
    "id" : "cf90cac8-ea43-4d58-a94b-d06ffe641fcb",
    "prId" : 31012,
    "prUrl" : "https://github.com/apache/spark/pull/31012#pullrequestreview-576449208",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c9d24cf5-ec20-41a3-b313-3c640eba5d22",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "This is the only non-testing change in this PR, but it's very minor and obvious.",
        "createdAt" : "2021-01-26T15:13:06Z",
        "updatedAt" : "2021-01-26T15:13:06Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "1aa22f055db370e77e506403666000e8e1e19f9c",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +419,423 @@    // may accidentally change the output schema name of the root plan.\n    case a @ Alias(attr: Attribute, name)\n      if (a.metadata == Metadata.empty || a.metadata == attr.metadata) &&\n        name == attr.name &&\n        !excludeList.contains(attr) &&"
  },
  {
    "id" : "5d55881a-5b57-429d-8226-e34aa9d5b5db",
    "prId" : 30558,
    "prUrl" : "https://github.com/apache/spark/pull/30558#pullrequestreview-553895404",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bcea21ff-4e55-4c78-9342-9935ddddafd4",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "Basically, what you want to do is to add an extension point/batch between heuristics-based optimizer and cost-based optimizer. \r\n\r\nThe batch name and comments look not good to me. We need a better name here. ",
        "createdAt" : "2020-12-02T04:16:49Z",
        "updatedAt" : "2020-12-02T04:16:49Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "5f0bdf32-f8a3-44e9-b322-219b789e3a96",
        "parentId" : "bcea21ff-4e55-4c78-9342-9935ddddafd4",
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "cc @maryannxue @hvanhovell ",
        "createdAt" : "2020-12-02T04:17:20Z",
        "updatedAt" : "2020-12-02T04:17:20Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "3f74984f-a65a-4d50-a0b9-a6ebdd096d82",
        "parentId" : "bcea21ff-4e55-4c78-9342-9935ddddafd4",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Could you propose a name then, @gatorsmile ?",
        "createdAt" : "2020-12-02T07:10:15Z",
        "updatedAt" : "2020-12-02T07:10:16Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "41526bb9-90a7-4530-8154-862507e7e9cf",
        "parentId" : "bcea21ff-4e55-4c78-9342-9935ddddafd4",
        "authorId" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "body" : "I am open to alternatives.",
        "createdAt" : "2020-12-02T21:44:11Z",
        "updatedAt" : "2020-12-02T21:44:11Z",
        "lastEditedBy" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "tags" : [
        ]
      },
      {
        "id" : "1d8bf356-0625-4240-8e76-1dba91d7eac9",
        "parentId" : "bcea21ff-4e55-4c78-9342-9935ddddafd4",
        "authorId" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "body" : "We should probably combine this batch with the one below: `earlyScanPushDownRules`, and give it a more general name similar to `extendedOperatorOptimizationRules`.",
        "createdAt" : "2020-12-09T18:59:44Z",
        "updatedAt" : "2020-12-09T18:59:45Z",
        "lastEditedBy" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "tags" : [
        ]
      },
      {
        "id" : "960ef1a3-9e04-467b-bb20-b404e60e31f7",
        "parentId" : "bcea21ff-4e55-4c78-9342-9935ddddafd4",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Ping, @aokolnychyi and @gatorsmile , for the above comment.",
        "createdAt" : "2020-12-09T19:20:54Z",
        "updatedAt" : "2020-12-09T19:20:55Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "4a2d6a35-5f59-4899-8b3e-7e42d6c68dd5",
        "parentId" : "bcea21ff-4e55-4c78-9342-9935ddddafd4",
        "authorId" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "body" : "Thanks for getting back, @maryannxue! I think that was the original suggestion we discussed [here](https://github.com/apache/spark/pull/29066#discussion_r530663744).\r\n\r\nHowever, `earlyScanPushDownRules` is exposed in a couple of places including the session builder. That's the reason I went with a new batch. I'll be happy to reconsider if we want to change all existing places.\r\n\r\nIt seems like we all agree that a better name is needed. Could you suggest one, @maryannxue?",
        "createdAt" : "2020-12-09T20:44:02Z",
        "updatedAt" : "2020-12-09T20:44:02Z",
        "lastEditedBy" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "tags" : [
        ]
      },
      {
        "id" : "634cd998-d3ae-4fe1-bd01-4965e44ef2f4",
        "parentId" : "bcea21ff-4e55-4c78-9342-9935ddddafd4",
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "This also needs to run before the early filter and pushdown because those rules need to transform any new DSv2Relation into DSv2ScanRelation.",
        "createdAt" : "2020-12-09T21:01:52Z",
        "updatedAt" : "2020-12-09T21:01:53Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      },
      {
        "id" : "b4862394-496b-43fc-96df-11561cd870bf",
        "parentId" : "bcea21ff-4e55-4c78-9342-9935ddddafd4",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "So `dataSourceRewriteRules` looks like definitely not the appropriate name that we should expose out as an API. This is merged to Spark 3.1. Shall we revert in Spark 3.1 and think about better naming since we all agree with having a better name? specially considering \"Alternatives to Breaking an API\" in https://spark.apache.org/versioning-policy.html.",
        "createdAt" : "2020-12-15T01:29:18Z",
        "updatedAt" : "2020-12-15T01:29:18Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "9978a849-dc1d-43df-b454-0c8d7fe65499",
        "parentId" : "bcea21ff-4e55-4c78-9342-9935ddddafd4",
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "It would be better to rename rather than remove it. If no one can come up with a better name, then the existing one is fine and we should use it. If there is a better name proposed, then we can simply port the rename in the 3.1 branch. No need to remove something that is useful to people that made it in before the branch was cut.",
        "createdAt" : "2020-12-15T01:40:39Z",
        "updatedAt" : "2020-12-15T01:40:40Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      },
      {
        "id" : "782e3f38-1c8b-425a-b559-f531050168f5",
        "parentId" : "bcea21ff-4e55-4c78-9342-9935ddddafd4",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@rdblue, it would be great if we can stick to having a good APIs as discussed and documented in \"Alternatives to Breaking an API\" in https://spark.apache.org/versioning-policy.html.\r\n\r\nTo clarify, I am _not_ suggesting to revert in `master` branch. I am suggesting to remove in `branch-3.1` alone to have a better designed API out. Keeping something only because it's useful to people is the exact reason why Spark happened to have a lot of cruft. We can't just remove them out for legacy reason and what we discussed in https://spark.apache.org/versioning-policy.html, but this one is not released out yet.",
        "createdAt" : "2020-12-15T01:46:17Z",
        "updatedAt" : "2020-12-15T01:46:52Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "4906e739-91de-4566-b25c-577b1a2f9c93",
        "parentId" : "bcea21ff-4e55-4c78-9342-9935ddddafd4",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I think we could have three options:\r\n\r\n- I personally more prefer to have any name that can address these comments above, and change it in `branch-3.1` too have a good API. I filed a blocker JIRA to rename this here SPARK-33784 in case we go this way.\r\n\r\n- If we're good with breaking it in minor releases since this is `@Unstable` API, then it works to me with AS-IS.\r\n\r\n- Otherwise, another option is to remove this in `branch-3.1`.\r\n\r\nI will take an action for the JIRA for which option we take.",
        "createdAt" : "2020-12-15T02:02:46Z",
        "updatedAt" : "2020-12-15T02:02:46Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "9299c935-dde0-445c-aad5-e4021d631ed2",
        "parentId" : "bcea21ff-4e55-4c78-9342-9935ddddafd4",
        "authorId" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "body" : "Something like `preCBORules`, @rdblue ?",
        "createdAt" : "2020-12-15T03:27:45Z",
        "updatedAt" : "2020-12-15T03:27:45Z",
        "lastEditedBy" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "tags" : [
        ]
      },
      {
        "id" : "6ed2a61a-8add-48d9-a548-92fa1556a909",
        "parentId" : "bcea21ff-4e55-4c78-9342-9935ddddafd4",
        "authorId" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "body" : "I'd fix the name and cherry-pick it to 3.1 rather than revert the change completely. Moreover, I'd suggest including the feature in full, not just the new batch. PR https://github.com/apache/spark/pull/30577 was submitted before 3.1 was cut as I wanted to keep the scope of PRs small and it was merged a couple of days after 3.1 branch was created. We have many places to inject custom rules but there is no way to inject rules after operator optimization. I feel it is going to be really useful for many customers. I've seen people fork session builders just because of the lack of this functionality.\r\n\r\nSomething like `postOperatorOptimizationRules` or `preCostBasedOptimizationRules` or `preCBORules` sounds OK to me.",
        "createdAt" : "2020-12-15T11:17:00Z",
        "updatedAt" : "2020-12-15T11:17:00Z",
        "lastEditedBy" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "tags" : [
        ]
      },
      {
        "id" : "28a4d613-50c2-4777-a066-bc58b10de6d8",
        "parentId" : "bcea21ff-4e55-4c78-9342-9935ddddafd4",
        "authorId" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "body" : "also cc @viirya @dbtsai @sunchao @cloud-fan @dongjoon-hyun ",
        "createdAt" : "2020-12-15T11:19:16Z",
        "updatedAt" : "2020-12-15T11:19:16Z",
        "lastEditedBy" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "tags" : [
        ]
      },
      {
        "id" : "8a25f716-45ec-4895-af59-050418a1f70c",
        "parentId" : "bcea21ff-4e55-4c78-9342-9935ddddafd4",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@aokolnychyi, Are you saying a bad API is justified if users want?\r\n\r\nI prefer to rename it in too. That's why I filed a blocker JIRA SPARK-33784 to keep this. I assigned this ticket to you.\r\n\r\nTo clarify again, I am suggesting to revert it in `branch-3.1` if we're in deadlock because exposing a bad API is not a good idea as discussed and documented in \"Alternatives to Breaking an API\" at https://spark.apache.org/versioning-policy.html.",
        "createdAt" : "2020-12-15T11:51:39Z",
        "updatedAt" : "2020-12-15T11:51:39Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "69aae010-9280-4e44-bf9f-c557bdf7ac85",
        "parentId" : "bcea21ff-4e55-4c78-9342-9935ddddafd4",
        "authorId" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "body" : "@HyukjinKwon, no, I meant I'd rather work on the name together to make sure it is good enough rather than revert the change. Here is a quote from my earlier comment:\r\n\r\n> I'd fix the name and cherry-pick it to 3.1 rather than revert the change completely.\r\n\r\nI think we are on the same page here and I'll fix the name as part of SPARK-33784. Thanks for creating it, btw!\r\n\r\nCurrently proposed names are:\r\n\r\n- `postOperatorOptimizationRules`\r\n- `preCostBasedOptimizationRules`\r\n- `preCBORules`\r\n\r\nWhat does everybody think about them? Any preferences?\r\n\r\n",
        "createdAt" : "2020-12-15T12:05:29Z",
        "updatedAt" : "2020-12-15T12:05:29Z",
        "lastEditedBy" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "tags" : [
        ]
      },
      {
        "id" : "c7ea2e03-8c7a-4eda-bc7c-9e5294e0b20d",
        "parentId" : "bcea21ff-4e55-4c78-9342-9935ddddafd4",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "To me any of them works. WDYT @maryannxue @gatorsmile and @rdblue?",
        "createdAt" : "2020-12-15T12:07:20Z",
        "updatedAt" : "2020-12-15T12:07:20Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "72060fdb-cdbd-4c7c-bb73-2c482c54e27f",
        "parentId" : "bcea21ff-4e55-4c78-9342-9935ddddafd4",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I'd vote for `preCBORules`.",
        "createdAt" : "2020-12-15T13:50:00Z",
        "updatedAt" : "2020-12-15T13:50:01Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "2f4940e8-c7c9-4638-8897-46a15c453515",
        "parentId" : "bcea21ff-4e55-4c78-9342-9935ddddafd4",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "According to the current discussion, could you make a follow-up PR with `preCBORules`, @aokolnychyi ? If we have a PR, it would be easier to make a final decision.",
        "createdAt" : "2020-12-15T16:54:40Z",
        "updatedAt" : "2020-12-15T16:54:41Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "11505ad5-e22e-47b6-b477-b798b88857e3",
        "parentId" : "bcea21ff-4e55-4c78-9342-9935ddddafd4",
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "I don't think that `preCBORules` is a good name. This batch is for rewrites that need to happen after basic optimization simplifies expressions and then pushes filters and projections. It also needs to happen before early pushdown, which in turn needs to come before CBO. All of that is before CBO, and that name doesn't capture what this is to be used for.\r\n\r\nA more descriptive name is \"planRewriteRules\" because this is for rewriting plans after initial optimization, but before other optimizer rules that need to run after that rewrite, like early pushdown, CBO, etc.\r\n\r\nThe name \"postOperatorOptimizationRules\" is okay, but not very descriptive.",
        "createdAt" : "2020-12-15T17:34:23Z",
        "updatedAt" : "2020-12-15T17:34:24Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      },
      {
        "id" : "c7c16c1a-6e88-49c9-813d-307a00417ad8",
        "parentId" : "bcea21ff-4e55-4c78-9342-9935ddddafd4",
        "authorId" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "body" : "Let's finish this discussion in a separate PR. I'll create one now.",
        "createdAt" : "2020-12-16T15:44:44Z",
        "updatedAt" : "2020-12-16T15:44:44Z",
        "lastEditedBy" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "tags" : [
        ]
      },
      {
        "id" : "51654abf-ef88-4eca-abc1-d71c2876097b",
        "parentId" : "bcea21ff-4e55-4c78-9342-9935ddddafd4",
        "authorId" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "body" : "I created PR https://github.com/apache/spark/pull/30808",
        "createdAt" : "2020-12-16T17:00:48Z",
        "updatedAt" : "2020-12-16T17:00:48Z",
        "lastEditedBy" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "tags" : [
        ]
      }
    ],
    "commit" : "11f07cd7c44bb92543dc534a30eac491a1553954",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +188,192 @@    // This batch rewrites data source plans and should be run after the operator\n    // optimization batch and before any batches that depend on stats.\n    Batch(\"Data Source Rewrite Rules\", Once, dataSourceRewriteRules: _*) :+\n    // This batch pushes filters and projections into scan nodes. Before this batch, the logical\n    // plan may contain nodes that do not report stats. Anything that uses stats must run after"
  },
  {
    "id" : "3a8353ed-d8a7-4625-a297-ec4f4147f479",
    "prId" : 30368,
    "prUrl" : "https://github.com/apache/spark/pull/30368#pullrequestreview-530074738",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "113f4177-c207-4a67-a92c-0f85f0fd4a04",
        "parentId" : null,
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "We still need the old patterns for such case `plan.limit(2).limit(1)`.",
        "createdAt" : "2020-11-13T09:24:15Z",
        "updatedAt" : "2020-11-17T14:37:40Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      },
      {
        "id" : "25b6cc22-e645-4eaa-8745-7357dab067b9",
        "parentId" : "113f4177-c207-4a67-a92c-0f85f0fd4a04",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "`transformDown` -> `transform`?",
        "createdAt" : "2020-11-13T12:20:51Z",
        "updatedAt" : "2020-11-17T14:37:40Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "98c27b39-b395-4d40-9d3d-02d653805a2f",
        "parentId" : "113f4177-c207-4a67-a92c-0f85f0fd4a04",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "We need `transformDown` here, so better to use `transformDown` explicitly ?",
        "createdAt" : "2020-11-13T13:49:02Z",
        "updatedAt" : "2020-11-17T14:37:40Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      }
    ],
    "commit" : "2f55ec034ade0d430bd7ab179f68addf0d65f84a",
    "line" : 27,
    "diffHunk" : "@@ -1,1 +1462,1466 @@  }\n\n  def apply(plan: LogicalPlan): LogicalPlan = plan transformDown {\n    case Limit(l, child) if canEliminate(l, child) =>\n      child"
  },
  {
    "id" : "0254d3e4-002e-4db7-8084-c3647988a08b",
    "prId" : 30278,
    "prUrl" : "https://github.com/apache/spark/pull/30278#pullrequestreview-526403956",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1e94f589-e98a-4153-b7c5-280fc7369d1f",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "This change affects the `PushDownLeftSemiAntiJoin` rule, too. So, could you add tests for the case?",
        "createdAt" : "2020-11-09T00:50:17Z",
        "updatedAt" : "2020-11-09T23:53:45Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "53e4a037-9c73-4c76-b8d3-302b73042170",
        "parentId" : "1e94f589-e98a-4153-b7c5-280fc7369d1f",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> This change affects the `PushDownLeftSemiAntiJoin` rule, too. So, could you add tests for the case?\r\n\r\nDouble check the case, seems current master fix this case by some pr, but 3.0 is still as jira desc.",
        "createdAt" : "2020-11-09T03:49:44Z",
        "updatedAt" : "2020-11-09T23:53:45Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "e12b576b-6859-47c6-985c-486b762ffa63",
        "parentId" : "1e94f589-e98a-4153-b7c5-280fc7369d1f",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "> seems current master fix this case\r\n\r\nWhat do you mean by \"fix this case\"?",
        "createdAt" : "2020-11-09T08:26:15Z",
        "updatedAt" : "2020-11-09T23:53:45Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "7779f2b3-4be8-4492-90cd-e7adde70c105",
        "parentId" : "1e94f589-e98a-4153-b7c5-280fc7369d1f",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> > seems current master fix this case\r\n> \r\n> What do you mean by \"fix this case\"?\r\n\r\nI have found the pr #29673  \r\nBefore this pr, SQL\r\n```\r\nSELECT\r\n       years,\r\n       appversion,                                               \r\n       SUM(uusers) AS users                                      \r\nFROM   (SELECT\r\n               Date_trunc('year', dt)          AS years,\r\n               CASE                                              \r\n                 WHEN h.pid = 3 THEN 'iOS'           \r\n                 WHEN h.pid = 4 THEN 'Android'       \r\n                 ELSE 'Other'                                    \r\n               END                             AS viewport,      \r\n               h.vs                            AS appversion,\r\n               Count(DISTINCT u.uid)           AS uusers\r\n               ,Count(DISTINCT u.suid)         AS srcusers\r\n        FROM   t1 u                                   \r\n               join t2 h                              \r\n                 ON h.uid = u.uid            \r\n        GROUP  BY 1,                                             \r\n                  2,                                             \r\n                  3) AS a\r\nWHERE  viewport = 'iOS'                                          \r\nGROUP  BY 1,                                                     \r\n          2\r\n```\r\n\r\nOptimized plan is  \r\n```\r\n== Optimized Logical Plan ==\r\nAggregate [years#0, appversion#2], [years#0, appversion#2, sum(uusers#3L) AS users#5L]\r\n+- Aggregate [date_trunc('year', CAST(u.`dt` AS TIMESTAMP))#24, CASE WHEN (h.`pid` = 3) THEN 'iOS' WHEN (h.`pid` = 4) THEN 'Android' ELSE 'Other' END#25, vs#17], [date_trunc('year', CAST(u.`dt` AS TIMESTAMP))#24 AS years#0, vs#17 AS appversion#2, count(if ((gid#23 = 1)) u.`uid`#26 else null) AS uusers#3L]\r\n   +- Aggregate [date_trunc('year', CAST(u.`dt` AS TIMESTAMP))#24, CASE WHEN (h.`pid` = 3) THEN 'iOS' WHEN (h.`pid` = 4) THEN 'Android' ELSE 'Other' END#25, vs#17, u.`uid`#26, u.`suid`#27, gid#23], [date_trunc('year', CAST(u.`dt` AS TIMESTAMP))#24, CASE WHEN (h.`pid` = 3) THEN 'iOS' WHEN (h.`pid` = 4) THEN 'Android' ELSE 'Other' END#25, vs#17, u.`uid`#26, gid#23]\r\n      +- Filter (CASE WHEN (h.`pid` = 3) THEN 'iOS' WHEN (h.`pid` = 4) THEN 'Android' ELSE 'Other' END#25 = iOS)\r\n         +- Expand [ArrayBuffer(date_trunc(year, cast(dt#14 as timestamp), Some(Asia/Shanghai)), CASE WHEN (pid#16 = 3) THEN iOS WHEN (pid#16 = 4) THEN Android ELSE Other END, vs#17, uid#12, null, 1), ArrayBuffer(date_trunc(year, cast(dt#14 as timestamp), Some(Asia/Shanghai)), CASE WHEN (pid#16 = 3) THEN iOS WHEN (pid#16 = 4) THEN Android ELSE Other END, vs#17, null, suid#15, 2)], [date_trunc('year', CAST(u.`dt` AS TIMESTAMP))#24, CASE WHEN (h.`pid` = 3) THEN 'iOS' WHEN (h.`pid` = 4) THEN 'Android' ELSE 'Other' END#25, vs#17, u.`uid`#26, u.`suid`#27, gid#23]\r\n            +- Project [uid#12, dt#14, suid#15, pid#16, vs#17]\r\n               +- Join Inner, (uid#18 = uid#12)\r\n                  :- Project [uid#12, dt#14, suid#15]\r\n                  :  +- Filter isnotnull(uid#12)\r\n                  :     +- Relation[pid#11,uid#12,sid#13,dt#14,suid#15] parquet\r\n                  +- Project [pid#16, vs#17, uid#18]\r\n                     +- Filter isnotnull(uid#18)\r\n                        +- Relation[pid#16,vs#17,uid#18,csid#19] parquet\r\n```\r\n\r\nAfter that pr, Optimized plan is \r\n```\r\n== Optimized Logical Plan ==\r\nAggregate [years#0, appversion#2], [years#0, appversion#2, sum(uusers#3L) AS users#5L]\r\n+- Aggregate [date_trunc(year, cast(dt#14 as timestamp), Some(Asia/Shanghai)), CASE WHEN (pid#16 = 3) THEN iOS WHEN (pid#16 = 4) THEN Android ELSE Other END, vs#17], [date_trunc(year, cast(dt#14 as timestamp), Some(Asia/Shanghai)) AS years#0, vs#17 AS appversion#2, count(distinct uid#12) AS uusers#3L]\r\n   +- Project [uid#12, dt#14, pid#16, vs#17]\r\n      +- Join Inner, (uid#18 = uid#12)\r\n         :- Project [uid#12, dt#14]\r\n         :  +- Filter isnotnull(uid#12)\r\n         :     +- Relation[pid#11,uid#12,sid#13,dt#14,suid#15] parquet\r\n         +- Project [pid#16, vs#17, uid#18]\r\n            +- Filter ((CASE WHEN (pid#16 = 3) THEN iOS WHEN (pid#16 = 4) THEN Android ELSE Other END = iOS) AND isnotnull(uid#18))\r\n               +- Relation[pid#16,vs#17,uid#18,csid#19] parquet\r\n```\r\n\r\n`Filter((CASE WHEN (pid#16 = 3) THEN iOS WHEN (pid#16 = 4) THEN Android ELSE Other END = iOS))` is pushed down and won't generate  `Expand`",
        "createdAt" : "2020-11-09T09:22:03Z",
        "updatedAt" : "2020-11-09T23:53:45Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "dc89f7c0-6f32-42aa-b3a9-9e9d4619d8c6",
        "parentId" : "1e94f589-e98a-4153-b7c5-280fc7369d1f",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "how does it related to left semi join?",
        "createdAt" : "2020-11-09T09:32:39Z",
        "updatedAt" : "2020-11-09T23:53:45Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "858b1254-439d-4693-b801-a4fad884e223",
        "parentId" : "1e94f589-e98a-4153-b7c5-280fc7369d1f",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> how does it related to left semi join?\r\n\r\nNot related,   I didn't notice that the API would be used by  `PushDownLeftSemiAntiJoin `",
        "createdAt" : "2020-11-09T09:34:38Z",
        "updatedAt" : "2020-11-09T23:53:45Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "54e18663-1956-43cb-b173-e27ffd3c49e4",
        "parentId" : "1e94f589-e98a-4153-b7c5-280fc7369d1f",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "> This change affects the PushDownLeftSemiAntiJoin rule, too. So, could you add tests for the case?\r\n\r\nSo can we add such a test?",
        "createdAt" : "2020-11-09T09:43:54Z",
        "updatedAt" : "2020-11-09T23:53:45Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "8088493a-25d0-43c4-b703-7a4c2e5fd32b",
        "parentId" : "1e94f589-e98a-4153-b7c5-280fc7369d1f",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> > This change affects the PushDownLeftSemiAntiJoin rule, too. So, could you add tests for the case?\r\n> \r\n> So can we add such a test?\r\n\r\nWith test case in  `LeftSemiPushdownSuite`\r\n```\r\n  test(\"Unary: LeftSemi join push down through expand\") {\r\n    val expand = Expand(Seq(Seq('a, 'b, \"null\"), Seq('a, \"null\", 'c)),\r\n      Seq('a, 'b, 'c), testRelation)\r\n    val originalQuery = expand\r\n      .join(testRelation1, joinType = LeftSemi, condition = Some('b === 'd && 'b === 1))\r\n\r\n    val optimized = Optimize.execute(originalQuery.analyze)\r\n    val correctAnswer = Expand(Seq(Seq('a, 'b, \"null\"), Seq('a, \"null\", 'c)),\r\n      Seq('a, 'b, 'c), Filter(EqualTo('b, 1), testRelation))\r\n        .join(testRelation1, joinType = LeftSemi, condition = Some('b === 'd))\r\n        .analyze\r\n\r\n    comparePlans(optimized, correctAnswer)\r\n  }\r\n```\r\noriginalQuery is \r\n```\r\n'Join LeftSemi, (('b = 'd) AND ('b = 1))\r\n:- 'Expand [List('a, 'b, null), List('a, null, 'c)], ['a, 'b, 'c]\r\n:  +- LocalRelation <empty>, [a#0, b#1, c#2]\r\n+- LocalRelation <empty>, [d#3]\r\n```\r\n\r\nTest result is\r\n```\r\n== FAIL: Plans do not match ===\r\n!'Expand [List(a#0, b#0, null), List(a#0, null, c#0)], [a#0, b#0, c#0]   'Join LeftSemi, (b#0 = d#0)\r\n!+- 'Join LeftSemi, ((b#0 = 1) AND (b#0 = d#0))                          :- Expand [List(a#0, b#0, null), List(a#0, null, c#0)], [a#0, b#0, c#0]\r\n!   :- LocalRelation <empty>, [a#0, b#0, c#0]                            :  +- Filter (b#0 = 1)\r\n!   +- LocalRelation <empty>, [d#0]                                      :     +- LocalRelation <empty>, [a#0, b#0, c#0]\r\n!                                                                        +- LocalRelation <empty>, [d#0]\r\n    \r\n```\r\n\r\nExpand will be promoted below Join, so should we ignore this case or add a parameter  in  `canPushThrough` like below\r\n```\r\n def canPushThrough(p: UnaryNode, isFilterPushDown: Boolean = false): Boolean = p match {\r\n    // Note that some operators (e.g. project, aggregate, union) are being handled separately\r\n    // (earlier in this rule).\r\n    case _: AppendColumns => true\r\n    case _: Distinct => true\r\n    case _: Generate => true\r\n    case _: Pivot => true\r\n    case _: RepartitionByExpression => true\r\n    case _: Repartition => true\r\n    case _: ScriptTransformation => true\r\n    case _: Sort => true\r\n    case _: BatchEvalPython => true\r\n    case _: ArrowEvalPython => true\r\n    case _: Expand => isFilterPushDown\r\n    case _ => false\r\n  }\r\n```",
        "createdAt" : "2020-11-09T10:18:20Z",
        "updatedAt" : "2020-11-09T23:53:45Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "69f07bb4-f2b5-425d-8ee6-9d00a0e1bd6f",
        "parentId" : "1e94f589-e98a-4153-b7c5-280fc7369d1f",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I'm sorry I didn't get it. What's the issue here? We can't pushdown left-semi join through expand?",
        "createdAt" : "2020-11-09T13:02:05Z",
        "updatedAt" : "2020-11-09T23:53:45Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "762dc7f9-ef90-44a5-8638-693e0cb560ff",
        "parentId" : "1e94f589-e98a-4153-b7c5-280fc7369d1f",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "The optimized (left-side) plan above looks correct to me...",
        "createdAt" : "2020-11-09T14:03:59Z",
        "updatedAt" : "2020-11-09T23:53:45Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "9d0ccb6e-4a22-4bc1-b496-4ccdf69ddff2",
        "parentId" : "1e94f589-e98a-4153-b7c5-280fc7369d1f",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> I'm sorry I didn't get it. What's the issue here? We can't pushdown left-semi join through expand?\r\n\r\noh..my mistake,  I misunderstood some code about `PushDownLeftSemiAntiJoin `",
        "createdAt" : "2020-11-09T15:59:27Z",
        "updatedAt" : "2020-11-09T23:53:45Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "0dca006e-1264-4f31-b9b6-45f0d454943e",
        "parentId" : "1e94f589-e98a-4153-b7c5-280fc7369d1f",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> The optimized (left-side) plan above looks correct to me...\r\n\r\nMy fault, I  misunderstand some code about `PushDownLeftSemiAntiJoin`,  test case added ==",
        "createdAt" : "2020-11-09T16:00:52Z",
        "updatedAt" : "2020-11-09T23:53:45Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "a09f8364cf4b9853cdf1ff4dc00d761b3a5b6291",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +1270,1274 @@    case _: BatchEvalPython => true\n    case _: ArrowEvalPython => true\n    case _: Expand => true\n    case _ => false\n  }"
  },
  {
    "id" : "3495fd5f-6f8d-4b46-8d28-39fc9e517538",
    "prId" : 30178,
    "prUrl" : "https://github.com/apache/spark/pull/30178#pullrequestreview-534027055",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "12bd34a1-f800-4023-97d0-603767e15edb",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "shall we also check if the lower bound is `UnboundedPreceding`? otherwise we can't use the offset optimization for nth_value and `first` is probably faster than `nth_value(1)`",
        "createdAt" : "2020-11-18T05:36:57Z",
        "updatedAt" : "2020-11-18T05:36:57Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "5432e403-e02f-4795-9dec-85c752b0d44e",
        "parentId" : "12bd34a1-f800-4023-97d0-603767e15edb",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "OK. I created the https://github.com/apache/spark/pull/30419 to make this check.",
        "createdAt" : "2020-11-19T02:10:10Z",
        "updatedAt" : "2020-11-19T04:13:26Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "3a7f4e740eb5a9cecf880bc5cc294b2459e98cf1",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +815,819 @@    case we @ WindowExpression(AggregateExpression(first: First, _, _, _, _), spec)\n      if spec.orderSpec.nonEmpty &&\n        spec.frameSpecification.asInstanceOf[SpecifiedWindowFrame].frameType == RowFrame =>\n      we.copy(windowFunction = NthValue(first.child, Literal(1), first.ignoreNulls))\n  }"
  },
  {
    "id" : "45b0041b-c56d-479e-828e-eab49b0bcff8",
    "prId" : 30018,
    "prUrl" : "https://github.com/apache/spark/pull/30018#pullrequestreview-510408431",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f6ab3e3b-1ef5-4941-aa64-a254e3559260",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "nit: `lowerIsRedundant` -> `hasRedundantAggregate`?",
        "createdAt" : "2020-10-15T07:20:46Z",
        "updatedAt" : "2021-03-19T04:24:38Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "2ee82dad-dae4-483c-8c53-1dc6ffa06fff",
        "parentId" : "f6ab3e3b-1ef5-4941-aa64-a254e3559260",
        "authorId" : "a8e23d47-3ae4-4385-848c-38a216d1bd08",
        "body" : "I think that the current one is more explicit. With your proposition it is not obvious which one is redundant. Perhaps there are situations where `upperIsRedundant`.",
        "createdAt" : "2020-10-16T12:08:26Z",
        "updatedAt" : "2021-03-19T04:24:38Z",
        "lastEditedBy" : "a8e23d47-3ae4-4385-848c-38a216d1bd08",
        "tags" : [
        ]
      }
    ],
    "commit" : "07e758d25e0840bf40129a36ad5bdb90a005058a",
    "line" : 36,
    "diffHunk" : "@@ -1,1 +514,518 @@  }\n\n  private def lowerIsRedundant(upper: Aggregate, lower: Aggregate): Boolean = {\n    val upperHasNoAggregateExpressions = !upper.aggregateExpressions.exists(isAggregate)\n"
  },
  {
    "id" : "9e23376b-a16a-4176-a7ec-fa1551560ee6",
    "prId" : 30018,
    "prUrl" : "https://github.com/apache/spark/pull/30018#pullrequestreview-513424905",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "42bc38e2-b847-447d-a5a5-8de168e67fac",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "`transformUp` -> `transform`? Seems like this transformation does not depend on the order.",
        "createdAt" : "2020-10-15T07:22:01Z",
        "updatedAt" : "2021-03-19T04:24:38Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "ecc0b48c-e1bf-4c28-8a19-c5f1b7ce1b76",
        "parentId" : "42bc38e2-b847-447d-a5a5-8de168e67fac",
        "authorId" : "a8e23d47-3ae4-4385-848c-38a216d1bd08",
        "body" : "There is a slight difference - `transformUp` can handle more than two consecutive aggregates in a single iteration.\r\nIf there are 3 aggregates `A1(A2(A3(...)))`, where `A2` and  `A3` are redundant, then with `transformDown` on first iteration `A2` is removed and then `A3` on the second iteration. With `transformUp` it removes `A3` and then `A2` on the same iteration.",
        "createdAt" : "2020-10-15T15:30:49Z",
        "updatedAt" : "2021-03-19T04:24:38Z",
        "lastEditedBy" : "a8e23d47-3ae4-4385-848c-38a216d1bd08",
        "tags" : [
        ]
      },
      {
        "id" : "234342ac-1eda-4394-a7b3-ce3a4ecd7215",
        "parentId" : "42bc38e2-b847-447d-a5a5-8de168e67fac",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : ">> where A2 and A3 are redundant\r\n\r\nIf only A2 and A3 are redundant, `lowerIsRedundant(A1, A2)` returns false and it seems like A2/A3 are removed in the same iteration even in `transform` (the first acse). I miss something?",
        "createdAt" : "2020-10-21T02:20:49Z",
        "updatedAt" : "2021-03-19T04:24:38Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "ceb61bac-8da9-4e41-89fc-3c6959907c5c",
        "parentId" : "42bc38e2-b847-447d-a5a5-8de168e67fac",
        "authorId" : "a8e23d47-3ae4-4385-848c-38a216d1bd08",
        "body" : "I meant that `lowerIsRedundant(A1, A2)` would return true, If it would return false, then A2 would not be removed at all.\r\n\r\nWith `transformDown` it would check `A1` and `A2` and remove `A2`, but then it would not check `A1` and `A3` on the same iteration.",
        "createdAt" : "2020-10-21T03:20:15Z",
        "updatedAt" : "2021-03-19T04:24:38Z",
        "lastEditedBy" : "a8e23d47-3ae4-4385-848c-38a216d1bd08",
        "tags" : [
        ]
      },
      {
        "id" : "0c2f1284-b9fb-4f72-9f5d-95ffcae654b9",
        "parentId" : "42bc38e2-b847-447d-a5a5-8de168e67fac",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "> transformUp can handle more than two consecutive aggregates in a single iteration.\r\n\r\nYou meant that `transformUp` can transform `A1(A2(A3(...))) => A1()` in a single iteration? Probably, I miss some conditions for the case you described above, so could you give me a concrete query example for it?",
        "createdAt" : "2020-10-21T05:12:16Z",
        "updatedAt" : "2021-03-19T04:24:38Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "f0613169-2d1d-456c-bac8-d7f744b627bf",
        "parentId" : "42bc38e2-b847-447d-a5a5-8de168e67fac",
        "authorId" : "a8e23d47-3ae4-4385-848c-38a216d1bd08",
        "body" : "For example the UT `Remove 2 redundant aggregates`.\r\nWith `transformUp` it runs 2 iterations (1 extra to detect that all changes are done).\r\nWith `transformDown` it runs 3 iterations",
        "createdAt" : "2020-10-21T05:44:09Z",
        "updatedAt" : "2021-03-19T04:24:38Z",
        "lastEditedBy" : "a8e23d47-3ae4-4385-848c-38a216d1bd08",
        "tags" : [
        ]
      },
      {
        "id" : "c04e0993-c001-4f0c-a061-6cb9a744c2a8",
        "parentId" : "42bc38e2-b847-447d-a5a5-8de168e67fac",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Ah, ok. I got your point. Looks okay as it is.",
        "createdAt" : "2020-10-21T08:23:56Z",
        "updatedAt" : "2021-03-19T04:24:38Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "07e758d25e0840bf40129a36ad5bdb90a005058a",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +495,499 @@ */\nobject RemoveRedundantAggregates extends Rule[LogicalPlan] with AliasHelper {\n  def apply(plan: LogicalPlan): LogicalPlan = plan transformUp {\n    case upper @ Aggregate(_, _, lower: Aggregate) if lowerIsRedundant(upper, lower) =>\n      val aliasMap = getAliasMap(lower)"
  },
  {
    "id" : "b5fdc4c6-9097-4d59-8164-1d834d5d8a1e",
    "prId" : 30018,
    "prUrl" : "https://github.com/apache/spark/pull/30018#pullrequestreview-556092897",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fa69cc4b-a547-4ae1-bac2-c876bf949f2d",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Could you add test cases for `PythonUDF.isGroupedAggPandasUDF(e)`, too?",
        "createdAt" : "2020-12-21T02:04:09Z",
        "updatedAt" : "2021-03-19T04:24:38Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "07e758d25e0840bf40129a36ad5bdb90a005058a",
    "line" : 52,
    "diffHunk" : "@@ -1,1 +530,534 @@  private def isAggregate(expr: Expression): Boolean = {\n    expr.find(e => e.isInstanceOf[AggregateExpression] ||\n      PythonUDF.isGroupedAggPandasUDF(e)).isDefined\n  }\n}"
  },
  {
    "id" : "f491a234-b970-4ae3-a438-186570c498f3",
    "prId" : 30018,
    "prUrl" : "https://github.com/apache/spark/pull/30018#pullrequestreview-616944479",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2d854c10-0d8f-4075-ae13-da9a9fbdf4f1",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "nit. Usually, `isXXX` is better and consistent with Apache Spark convention.",
        "createdAt" : "2021-03-20T21:10:10Z",
        "updatedAt" : "2021-03-20T21:10:11Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "07e758d25e0840bf40129a36ad5bdb90a005058a",
    "line" : 36,
    "diffHunk" : "@@ -1,1 +514,518 @@  }\n\n  private def lowerIsRedundant(upper: Aggregate, lower: Aggregate): Boolean = {\n    val upperHasNoAggregateExpressions = !upper.aggregateExpressions.exists(isAggregate)\n"
  },
  {
    "id" : "d91ca3cb-087b-4e3a-98f5-bd0915b6b63c",
    "prId" : 30018,
    "prUrl" : "https://github.com/apache/spark/pull/30018#pullrequestreview-616944661",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "648f0ae0-b8a5-4108-af99-9bc0de71c3d2",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "- `introduces` -> `introduced`\r\n- `expression` -> `expressions`",
        "createdAt" : "2021-03-20T21:13:24Z",
        "updatedAt" : "2021-03-20T21:14:01Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "07e758d25e0840bf40129a36ad5bdb90a005058a",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +506,510 @@      )\n\n      // We might have introduces non-deterministic grouping expression\n      if (newAggregate.groupingExpressions.exists(!_.deterministic)) {\n        PullOutNondeterministic.applyLocally.applyOrElse(newAggregate, identity[LogicalPlan])"
  },
  {
    "id" : "c003fc4f-eadc-4669-91e2-d2176c191c17",
    "prId" : 30018,
    "prUrl" : "https://github.com/apache/spark/pull/30018#pullrequestreview-616945018",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "160cc0cb-c5f9-442e-966a-8f0a9f7d2326",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Could you move this optimizer into a new file please, @tanelk ?",
        "createdAt" : "2021-03-20T21:19:11Z",
        "updatedAt" : "2021-03-20T21:19:11Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "07e758d25e0840bf40129a36ad5bdb90a005058a",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +494,498 @@ * only goal is to keep distinct values, while its parent aggregate would ignore duplicate values.\n */\nobject RemoveRedundantAggregates extends Rule[LogicalPlan] with AliasHelper {\n  def apply(plan: LogicalPlan): LogicalPlan = plan transformUp {\n    case upper @ Aggregate(_, _, lower: Aggregate) if lowerIsRedundant(upper, lower) =>"
  },
  {
    "id" : "21785bda-f303-45c7-9ddb-559a210a0f7e",
    "prId" : 30018,
    "prUrl" : "https://github.com/apache/spark/pull/30018#pullrequestreview-616946037",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "996018b3-503a-495f-84d5-062ce6b6ae92",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "```scala\r\n- .filter(!isAggregate(_))\r\n+ .filterNot(isAggregate)\r\n```",
        "createdAt" : "2021-03-20T21:37:32Z",
        "updatedAt" : "2021-03-20T21:37:32Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "07e758d25e0840bf40129a36ad5bdb90a005058a",
    "line" : 43,
    "diffHunk" : "@@ -1,1 +521,525 @@        .aggregateExpressions\n        .filter(_.deterministic)\n        .filter(!isAggregate(_))\n        .map(_.toAttribute)\n    ))"
  },
  {
    "id" : "0bc81166-149d-4123-84b6-2ce32ff948c2",
    "prId" : 29950,
    "prUrl" : "https://github.com/apache/spark/pull/29950#pullrequestreview-502548398",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "764f1c27-198c-437e-9535-84cca201246a",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "We could extend to other cases like `case p @ Project(_, agg: Aggregate)`, but leave it untouched for now.",
        "createdAt" : "2020-10-06T02:50:52Z",
        "updatedAt" : "2020-11-13T00:06:08Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "bbaae3ed9b96ca517ec5435838ac37feb578c959",
    "line" : 56,
    "diffHunk" : "@@ -1,1 +773,777 @@      case a: Alias => a.toAttribute -> a\n    })\n  }\n\n  // Whether the largest times common outputs from lower operator used in upper operators is"
  },
  {
    "id" : "3a7e872a-38d4-43f0-8eee-b8de9d0a7ef6",
    "prId" : 29950,
    "prUrl" : "https://github.com/apache/spark/pull/29950#pullrequestreview-505950881",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "61dd2c29-50d0-4424-8781-138c2b6c4131",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Could you update the optimizer description according to the new conf?",
        "createdAt" : "2020-10-09T09:42:35Z",
        "updatedAt" : "2020-11-13T00:06:08Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "11ceec9a-107b-4df7-bb7d-544c1fbb6ff3",
        "parentId" : "61dd2c29-50d0-4424-8781-138c2b6c4131",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Yea, updated. thanks.",
        "createdAt" : "2020-10-09T19:52:58Z",
        "updatedAt" : "2020-11-13T00:06:08Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "bbaae3ed9b96ca517ec5435838ac37feb578c959",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +730,734 @@ * 2. When two [[Project]] operators have LocalLimit/Sample/Repartition operator between them\n *    and the upper project consists of the same number of columns which is equal or aliasing.\n *    `GlobalLimit(LocalLimit)` pattern is also considered.\n */\nobject CollapseProject extends Rule[LogicalPlan] with AliasHelper {"
  },
  {
    "id" : "b82f40a3-1baa-4a01-8a3c-f02327c1422d",
    "prId" : 29950,
    "prUrl" : "https://github.com/apache/spark/pull/29950#pullrequestreview-506110086",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9867ad56-e91c-4cd2-a298-ee2bbcf0960c",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "`ColumnPruning` cannot finish all collapsing in one shot. `Once` will break idempotence.",
        "createdAt" : "2020-10-10T16:35:06Z",
        "updatedAt" : "2020-11-13T00:06:08Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "bbaae3ed9b96ca517ec5435838ac37feb578c959",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +219,223 @@      RewritePredicateSubquery,\n      ColumnPruning,\n      CollapseProject,\n      RemoveNoopOperators) :+\n    // This batch must be executed after the `RewriteSubquery` batch, which creates joins."
  },
  {
    "id" : "19cca6df-d5ae-45a7-a492-70b5dabe2304",
    "prId" : 29950,
    "prUrl" : "https://github.com/apache/spark/pull/29950#pullrequestreview-507068822",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "35b396ee-3663-4cc4-ab71-29a2f3594520",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "`FixedPoint(1)` instead? Also, could you leave comments about why we use it instead of `Once`?",
        "createdAt" : "2020-10-12T00:51:01Z",
        "updatedAt" : "2020-11-13T00:06:08Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "ac566f54-c920-4d6a-9940-f089342f4dc0",
        "parentId" : "35b396ee-3663-4cc4-ab71-29a2f3594520",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Isn't `FixedPoint(1)` also to run the batch once?",
        "createdAt" : "2020-10-13T04:38:49Z",
        "updatedAt" : "2020-11-13T00:06:09Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "5574ece0-a184-4566-a315-bc77d09da08f",
        "parentId" : "35b396ee-3663-4cc4-ab71-29a2f3594520",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Added one comment.",
        "createdAt" : "2020-10-13T04:57:05Z",
        "updatedAt" : "2020-11-13T00:06:09Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "bbaae3ed9b96ca517ec5435838ac37feb578c959",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +216,220 @@      CheckCartesianProducts) :+\n    // `CollapseProject` cannot collapse all projects in once. So we need `fixedPoint` here.\n    Batch(\"RewriteSubquery\", fixedPoint,\n      RewritePredicateSubquery,\n      ColumnPruning,"
  },
  {
    "id" : "446d1d9c-e5a5-4c58-ae81-c17fb57d24a9",
    "prId" : 29950,
    "prUrl" : "https://github.com/apache/spark/pull/29950#pullrequestreview-529768736",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3d3443ff-b634-4caf-aaba-95dea9d0b577",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Is there a reason to change from `transformUp` to `transformDown`? If the all test passed, it would be safe if we keep the original one.",
        "createdAt" : "2020-11-13T05:36:08Z",
        "updatedAt" : "2020-11-13T05:36:08Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "f1d33554-d9d2-44f4-9d46-a9aee69be201",
        "parentId" : "3d3443ff-b634-4caf-aaba-95dea9d0b577",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "I found the previous comment about supporting `withColumn`. If this is designed for that, shall we add a test case for that?\r\n- https://github.com/apache/spark/pull/29950#discussion_r501976989",
        "createdAt" : "2020-11-13T06:11:59Z",
        "updatedAt" : "2020-11-13T06:12:00Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "bbaae3ed9b96ca517ec5435838ac37feb578c959",
    "line" : 30,
    "diffHunk" : "@@ -1,1 +734,738 @@object CollapseProject extends Rule[LogicalPlan] with AliasHelper {\n\n  def apply(plan: LogicalPlan): LogicalPlan = plan transformDown {\n    case p @ Project(_, _: Project) =>\n      collapseProjects(p)"
  },
  {
    "id" : "ade969ba-03e7-4bff-a200-3dd7dd9df326",
    "prId" : 29950,
    "prUrl" : "https://github.com/apache/spark/pull/29950#pullrequestreview-529752959",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "547ba3db-71b1-451f-9e2f-efccac31adc9",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "`upper operators` -> `upper operator`?",
        "createdAt" : "2020-11-13T05:52:12Z",
        "updatedAt" : "2020-11-13T05:52:12Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "bbaae3ed9b96ca517ec5435838ac37feb578c959",
    "line" : 58,
    "diffHunk" : "@@ -1,1 +775,779 @@  }\n\n  // Whether the largest times common outputs from lower operator used in upper operators is\n  // larger than allowed.\n  private def moreThanMaxAllowedCommonOutput("
  },
  {
    "id" : "2e71c8b1-565a-4822-a2c1-e54b75ceb217",
    "prId" : 29950,
    "prUrl" : "https://github.com/apache/spark/pull/29950#pullrequestreview-529753264",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6c2e4d50-cabc-4410-8d79-2fffef785925",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "`than allowed` -> `than the maximum`?",
        "createdAt" : "2020-11-13T05:53:16Z",
        "updatedAt" : "2020-11-13T05:53:16Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "bbaae3ed9b96ca517ec5435838ac37feb578c959",
    "line" : 59,
    "diffHunk" : "@@ -1,1 +776,780 @@\n  // Whether the largest times common outputs from lower operator used in upper operators is\n  // larger than allowed.\n  private def moreThanMaxAllowedCommonOutput(\n      upper: Seq[NamedExpression], lower: Seq[NamedExpression]): Boolean = {"
  },
  {
    "id" : "4b4ece74-013c-4e85-a918-627d4ee2c480",
    "prId" : 29807,
    "prUrl" : "https://github.com/apache/spark/pull/29807#pullrequestreview-492180478",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "13542d08-e30b-4689-b6ee-7d56d495b55d",
        "parentId" : null,
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "If not change this, even we write sql like \r\n```\r\nSELECT c1,\r\ncase\r\n  when a=1 then \"a\"\r\n  when a=2 then \"b\"\r\nend as s_type\r\nFROM (\r\n SELECT c1, get_json_object(s,'$.a') as a\r\n FROM t\r\n ) tmp\r\nWHERE a in (1, 2)\r\n```\r\nwill be resolved as \r\n```\r\n== Physical Plan ==\r\n*(1) Project [c1#1, CASE WHEN (cast(get_json_object(s#2, $.a) as int) = 1) THEN a WHEN (cast(get_json_object(s#2, $.a) as int) = 2) THEN b END AS s_type#0]\r\n+- *(1) Filter get_json_object(s#2, $.a) IN (1,2)\r\n   +- Scan hive default.t [c1#1, s#2], HiveTableRelation `default`.`t`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [c1#1, s#2], [P1#3], Statistics(sizeInBytes=8.0 EiB)\r\n```",
        "createdAt" : "2020-09-20T08:21:03Z",
        "updatedAt" : "2020-09-20T08:21:41Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "05ae9ecb42986c51bfd85979a5d4d4449f586cf2",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +1131,1135 @@    case f @ Filter(condition, project @ Project(fields, grandChild))\n      if fields.forall(_.deterministic) && canPushThroughCondition(grandChild, condition) &&\n        fields.flatMap(extractExpensiveExprs(_)).isEmpty =>\n      val aliasMap = getAliasMap(project)\n      project.copy(child = Filter(replaceAlias(condition, aliasMap), grandChild))"
  },
  {
    "id" : "b08326a7-f0c6-4e96-bd82-be7773a2053b",
    "prId" : 29673,
    "prUrl" : "https://github.com/apache/spark/pull/29673#pullrequestreview-484055826",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6cb70d4c-76bb-4ccb-b148-dd01ef371676",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "can we add a comment to say: this batch must be run after \"Decimal Optimizations\", as \"Decimal Optimizations\" may change the aggregate distinct column?",
        "createdAt" : "2020-09-08T11:51:25Z",
        "updatedAt" : "2020-09-11T00:37:38Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "8510ff92177554d9e87bd2e30e6fa9e43f89b8e3",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +198,202 @@    // This batch must run after \"Decimal Optimizations\", as that one may change the\n    // aggregate distinct column\n    Batch(\"Distinct Aggregate Rewrite\", Once,\n      RewriteDistinctAggregates) :+\n    Batch(\"Object Expressions Optimization\", fixedPoint,"
  },
  {
    "id" : "7a01fa85-cb84-4ab1-acb9-9aad50295d3e",
    "prId" : 29585,
    "prUrl" : "https://github.com/apache/spark/pull/29585#pullrequestreview-484132264",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f07cc162-5d15-4f1a-b963-e6200f970cc9",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "`d.output` is actually `child.output`, but `newAgg.output` may have different number of columns if `keys.isEmpty`. how about\r\n```\r\nif (keys.isEmpty) {\r\n  Aggregate(Seq(Literal(1)), aggCols, child)\r\n} else {\r\n  val newAgg  = ...\r\n  ...\r\n}\r\n```",
        "createdAt" : "2020-09-08T08:35:48Z",
        "updatedAt" : "2020-09-29T12:28:36Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "b23bf163-2b74-4116-a57e-f6b49912c5ab",
        "parentId" : "f07cc162-5d15-4f1a-b963-e6200f970cc9",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Looks the length of `Aggregate.output` is `aggCols.length` and `aggCols.length` is the same with `Deduplicate.child.length`?",
        "createdAt" : "2020-09-08T13:04:26Z",
        "updatedAt" : "2020-09-29T12:28:36Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "c3a6775c-c2b3-4193-8345-64d05b55dbba",
        "parentId" : "f07cc162-5d15-4f1a-b963-e6200f970cc9",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "ah sorry, it's the grouping key, nvm.",
        "createdAt" : "2020-09-08T13:21:18Z",
        "updatedAt" : "2020-09-29T12:28:36Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "e3c87429e7e6f463c4e7740c7a42e8b2def528b0",
    "line" : 45,
    "diffHunk" : "@@ -1,1 +1603,1607 @@      val nonemptyKeys = if (keys.isEmpty) Literal(1) :: Nil else keys\n      val newAgg = Aggregate(nonemptyKeys, aggCols, child)\n      val attrMapping = d.output.zip(newAgg.output)\n      newAgg -> attrMapping\n  }"
  },
  {
    "id" : "13c61d1c-6165-407b-a0f2-a849079d7c6e",
    "prId" : 29369,
    "prUrl" : "https://github.com/apache/spark/pull/29369#pullrequestreview-463939814",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8dc8316a-765b-49fd-8f47-60ec0d42d02c",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Can we put this rule in the main optimizer batch?",
        "createdAt" : "2020-08-10T02:31:55Z",
        "updatedAt" : "2020-08-11T09:55:06Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "719fc2fc-aedb-40e1-b768-2bb497009a4a",
        "parentId" : "8dc8316a-765b-49fd-8f47-60ec0d42d02c",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "OK.",
        "createdAt" : "2020-08-10T03:16:10Z",
        "updatedAt" : "2020-08-11T09:55:06Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "690e326d129d54e30a114188a71a664436b77caf",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +346,350 @@ * This rule should be applied before RewriteDistinctAggregates.\n */\nobject EliminateAggregateFilter extends Rule[LogicalPlan] {\n  override def apply(plan: LogicalPlan): LogicalPlan = plan transformExpressions  {\n    case ae @ AggregateExpression(_, _, _, Some(Literal.TrueLiteral), _) =>"
  },
  {
    "id" : "879dbf99-5874-4d95-8880-97e5aa65028a",
    "prId" : 29360,
    "prUrl" : "https://github.com/apache/spark/pull/29360#pullrequestreview-462148218",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ba05fad9-e8bc-45c0-ade1-ec3deafc99f2",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "nit: remove this single blank.",
        "createdAt" : "2020-08-06T02:39:18Z",
        "updatedAt" : "2020-08-25T02:45:14Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "b30d2ba400af5e2a46fad11cc2131542db1ab246",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +1848,1852 @@  }\n}\n\n/**\n * Split [[Expand]] into several Expand if the projection size of Expand is larger"
  },
  {
    "id" : "2d0a5f8a-d20a-41de-8480-3b77d8fbda13",
    "prId" : 29360,
    "prUrl" : "https://github.com/apache/spark/pull/29360#pullrequestreview-465534175",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "da1560ef-71a2-4db4-a796-1f65030bfbb6",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "These split expand + aggregate read same data. That's said this optimization will read same data multiple times. ",
        "createdAt" : "2020-08-09T16:11:59Z",
        "updatedAt" : "2020-08-25T02:45:14Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "8fe87390-4483-442f-83ce-3be4f9f0f4d8",
        "parentId" : "da1560ef-71a2-4db4-a796-1f65030bfbb6",
        "authorId" : "152034df-2d74-457f-aa5c-5d1f5614022c",
        "body" : "not the same data, they have passed the sql tests, the output is the same as expected",
        "createdAt" : "2020-08-12T01:35:51Z",
        "updatedAt" : "2020-08-25T02:45:14Z",
        "lastEditedBy" : "152034df-2d74-457f-aa5c-5d1f5614022c",
        "tags" : [
        ]
      }
    ],
    "commit" : "b30d2ba400af5e2a46fad11cc2131542db1ab246",
    "line" : 33,
    "diffHunk" : "@@ -1,1 +1869,1873 @@        val aggregates: Seq[Aggregate] = subExpands.map { expand =>\n          Aggregate(a.groupingExpressions, a.aggregateExpressions, expand)\n        }\n        Union(aggregates)\n      } else {"
  },
  {
    "id" : "6cf6d009-8ee8-4690-9af4-2e600420ac0d",
    "prId" : 29360,
    "prUrl" : "https://github.com/apache/spark/pull/29360#pullrequestreview-463885794",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1a559641-8199-434b-bb77-e7f4e3b1ab22",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "I'm curious about how this reduces shuffled data. I think the original idea is, in the cases of Expand will produce huge data, splitting it to small ones.\r\n\r\nBut shuffle is happened during Aggregate here, right? By splitting, the total amount of shuffled data is not changed, but split into several ones. Does it really result significant improvement?\r\n\r\nI'm curious to look at the benchmark results.",
        "createdAt" : "2020-08-09T16:18:50Z",
        "updatedAt" : "2020-08-25T02:45:14Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "b30d2ba400af5e2a46fad11cc2131542db1ab246",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +1853,1857 @@ * than default projection size.\n */\nobject SplitAggregateWithExpand extends Rule[LogicalPlan] {\n  private def splitExpand(expand: Expand, num: Int): Seq[Expand] = {\n    val groupedProjections = expand.projections.grouped(num).toList"
  },
  {
    "id" : "873fe874-84e4-4d49-b1ba-6d14e55e91b1",
    "prId" : 29170,
    "prUrl" : "https://github.com/apache/spark/pull/29170#pullrequestreview-452156354",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7d78ee35-d9eb-4cfa-985c-569378b89ed7",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Note: This rule was separated because https://github.com/apache/spark/pull/19149",
        "createdAt" : "2020-07-21T06:20:35Z",
        "updatedAt" : "2020-07-24T01:11:51Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "2b3c82af48a8890a951cc3bf5f85d92b698c4d31",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +119,123 @@      Batch(\"Infer Filters\", fixedPoint,\n        PushDownPredicates,\n        InferFiltersFromConstraints) ::\n      Batch(\"Operator Optimization after Inferring Filters\", fixedPoint,\n        rulesWithoutInferFiltersFromConstraints: _*) ::"
  },
  {
    "id" : "a6b68ea8-9599-46c7-98dc-6d90c78f9890",
    "prId" : 29170,
    "prUrl" : "https://github.com/apache/spark/pull/29170#pullrequestreview-526567429",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f9b4d6cb-58ba-44a2-9386-d0653a69f45b",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "My concern is that the change above might cause the issue described in #19149. Instead, (just a suggestion), we couldn't improve  `InferFiltersFromConstraints` to cover the case you pointed out in the PR description?",
        "createdAt" : "2020-11-06T07:30:42Z",
        "updatedAt" : "2020-11-06T07:30:42Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "727304af-4884-4e47-9160-2b85ba7b7519",
        "parentId" : "f9b4d6cb-58ba-44a2-9386-d0653a69f45b",
        "authorId" : "3d4870da-39a8-4406-b00d-131930d14cd8",
        "body" : "Unfortunately, there is no UT in the commit to see what was the exact issue and why `InferFiltersFromConstraints` needs to be separated entirely from other rules.\r\n\r\nIf I get the issue described in https://issues.apache.org/jira/browse/SPARK-21652 right, if we combine `InferFiltersFromConstraints` with other rules into a batch then we can end up in an infinite loop in that batch. But this is only because `InferFiltersFromConstraints` can create a new constraint that a subsequent rule removes. The example in https://issues.apache.org/jira/browse/SPARK-21652 required running `ConstantPropagation`, `ConstantFolding` and then `BooleanSimplification` to remove such an inferred constraint/filter.\r\nI think if we combine `InferFiltersFromConstraints` with other rules that doesn't reduce constraints (like `PushDownPredicates`) then we are still good. But I might be wrong so please share your thoughts on this.\r\n\r\nSidebar:\r\nI think the source of the above mentioned loop is that `ConstantPropagation` doesn't propagate constants into a join condition (it handles filters only). I had an old PR to enhance that rule where I also commented why it is not so simple to propagate constants into a join: https://github.com/apache/spark/pull/24553/files#diff-d43484d56a4d9991066b5c00d12ec2465c75131e055fc02ee7fb6dfd45b5006fR76-R79 but it is doable if we fix https://issues.apache.org/jira/browse/SPARK-30598 (https://github.com/apache/spark/pull/27309).\r\nBut I'm not saying that `ConstantPropagation` is the only reduction rule that can collide with `InferFiltersFromConstraints`.",
        "createdAt" : "2020-11-06T11:30:01Z",
        "updatedAt" : "2020-11-06T11:30:01Z",
        "lastEditedBy" : "3d4870da-39a8-4406-b00d-131930d14cd8",
        "tags" : [
        ]
      },
      {
        "id" : "7f690443-cac0-4e69-9b79-c3d98d1e3f2d",
        "parentId" : "f9b4d6cb-58ba-44a2-9386-d0653a69f45b",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "> I think if we combine InferFiltersFromConstraints with other rules that doesn't reduce constraints (like PushDownPredicates) then we are still good. But I might be wrong so please share your thoughts on this.\r\n\r\nYea, I don't have a specific query to deny your thought and I also think it is true, but, in my current feeling, I a bit hesitate to change `Once` -> `fixedPoint ` for this batch without adding no logic to avoid the infinite loop case (and without any evidence that it will not affect users queries...). WDTY, @cloud-fan @viirya ?",
        "createdAt" : "2020-11-09T01:36:19Z",
        "updatedAt" : "2020-11-09T01:36:19Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "b3a32ca7-964e-487e-ab28-64fda805452b",
        "parentId" : "f9b4d6cb-58ba-44a2-9386-d0653a69f45b",
        "authorId" : "f7e8d656-68f6-4783-b24e-5d70947b2f1d",
        "body" : "I initially looked at trying to see if InferFiltersFromConstraints could itself handle this, but I think I could not figure out a way to solve the case in the unit test. I also don't think that this can cause an infinite loop, but let me take another look at this to see if we can either be sure that it cannot result in an infinite loop or if we can push this into InferFiltersFromConstraints so that we don't need the fixedPoint.",
        "createdAt" : "2020-11-09T03:30:17Z",
        "updatedAt" : "2020-11-09T03:30:18Z",
        "lastEditedBy" : "f7e8d656-68f6-4783-b24e-5d70947b2f1d",
        "tags" : [
        ]
      },
      {
        "id" : "d6d4efad-c5ee-4938-aa42-9871ec15ce70",
        "parentId" : "f9b4d6cb-58ba-44a2-9386-d0653a69f45b",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Not look at `InferFiltersFromConstraints`, but I tend to agree with @maropu's initial comment. Can we possibly make `InferFiltersFromConstraints` deal with this kind of case? ",
        "createdAt" : "2020-11-09T19:04:11Z",
        "updatedAt" : "2020-11-09T19:04:12Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "2b3c82af48a8890a951cc3bf5f85d92b698c4d31",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +118,122 @@        rulesWithoutInferFiltersFromConstraints: _*) ::\n      Batch(\"Infer Filters\", fixedPoint,\n        PushDownPredicates,\n        InferFiltersFromConstraints) ::\n      Batch(\"Operator Optimization after Inferring Filters\", fixedPoint,"
  },
  {
    "id" : "8c92ca14-3a62-4b93-8352-a351e2fb6fcd",
    "prId" : 29142,
    "prUrl" : "https://github.com/apache/spark/pull/29142#pullrequestreview-451270129",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7f96efc1-6496-4afc-b0bb-d986ebb8cb55",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Adding this itself looks fine to me.",
        "createdAt" : "2020-07-20T03:05:05Z",
        "updatedAt" : "2020-07-20T03:59:14Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "cd93b707dfd9e033a0580d688a19fe044af379f9",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1005,1009 @@  private def isOrderIrrelevantAggs(aggs: Seq[NamedExpression]): Boolean = {\n    def isOrderIrrelevantAggFunction(func: AggregateFunction): Boolean = func match {\n      case _: Min | _: Max | _: Count | _: MaxMinBy => true\n      // Arithmetic operations for floating-point values are order-sensitive\n      // (they are not associative)."
  },
  {
    "id" : "2d31b056-7e33-4312-b565-1b217b1aeddf",
    "prId" : 29094,
    "prUrl" : "https://github.com/apache/spark/pull/29094#pullrequestreview-453043324",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b6afa864-282b-404f-9c74-aff0f06e6641",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Seems like you can simplify it like this?;\r\n```\r\n  private def hasTooManyExprs(exprs: Seq[Expression]): Boolean = {\r\n    var numExprs = 0\r\n    exprs.foreach { _.foreach { _ => numExprs += 1 } }\r\n    numExprs > SQLConf.get.XXXX\r\n  }\r\n\r\n  def apply(plan: LogicalPlan): LogicalPlan = plan transformUp {\r\n    case p1 @ Project(_, p2: Project) if hasTooManyExprs(p2.projectList) => // skip\r\n      p1\r\n\r\n    case p1 @ Project(_, p2: Project) =>\r\n```",
        "createdAt" : "2020-07-21T15:41:30Z",
        "updatedAt" : "2020-07-22T12:03:43Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "4b5bb9ff-e549-45dd-9817-158c764461fb",
        "parentId" : "b6afa864-282b-404f-9c74-aff0f06e6641",
        "authorId" : "7753faf4-1cf0-4d0e-a47b-b3791190ca2f",
        "body" : "Sure, I can simplify the condition check logic, do you suggest to add a new SQLConf instead of the hard limit? And for the case statement there is already a condition check called 'haveCommonNonDeterministicOutput', so I put them together. Also the same for  'case p @ Project(_, agg: Aggregate)' ",
        "createdAt" : "2020-07-22T06:50:38Z",
        "updatedAt" : "2020-07-22T12:03:43Z",
        "lastEditedBy" : "7753faf4-1cf0-4d0e-a47b-b3791190ca2f",
        "tags" : [
        ]
      },
      {
        "id" : "337fe296-7fac-4f73-8b5d-15d2d6a3f8c9",
        "parentId" : "b6afa864-282b-404f-9c74-aff0f06e6641",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Yea, if we add this logic, I think we need a conf for that.",
        "createdAt" : "2020-07-22T06:59:17Z",
        "updatedAt" : "2020-07-22T12:03:43Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "06a0bcedc02b055d8f5a01989e05e2e2a27d8aca",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +699,703 @@object CollapseProject extends Rule[LogicalPlan] {\n  def apply(plan: LogicalPlan): LogicalPlan = plan transformUp {\n    case p1 @ Project(_, p2: Project) =>\n      if (haveCommonNonDeterministicOutput(p1.projectList, p2.projectList)\n        || hasTooManyExprs(p2.projectList)) {"
  },
  {
    "id" : "2f752f1b-c8cd-46ab-a97b-38a1c20573d3",
    "prId" : 29092,
    "prUrl" : "https://github.com/apache/spark/pull/29092#pullrequestreview-506774842",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "101b55e8-6abc-4fef-a8da-1e90ded6dffd",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Why did you put this rule only in this batch? https://github.com/apache/spark/pull/29092/files#diff-a636a87d8843eeccca90140be91d4fafR82",
        "createdAt" : "2020-10-07T01:44:31Z",
        "updatedAt" : "2020-10-13T09:00:48Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "32a54771-fac4-476b-b91b-a4e92e062b01",
        "parentId" : "101b55e8-6abc-4fef-a8da-1e90ded6dffd",
        "authorId" : "a8e23d47-3ae4-4385-848c-38a216d1bd08",
        "body" : "I'm sorry, I don't get the question.",
        "createdAt" : "2020-10-07T01:47:17Z",
        "updatedAt" : "2020-10-13T09:00:48Z",
        "lastEditedBy" : "a8e23d47-3ae4-4385-848c-38a216d1bd08",
        "tags" : [
        ]
      },
      {
        "id" : "198758a0-b8be-43e9-8f97-2ba6b99d4f59",
        "parentId" : "101b55e8-6abc-4fef-a8da-1e90ded6dffd",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Ah, ok. Looks okay as it is. Since `operatorOptimizationRuleSet` has `InferFiltersFromConstraints`, I thought we'd better to put `InferFiltersFromConstraints`/`InferFiltersFromGenerate` there together.",
        "createdAt" : "2020-10-07T04:27:14Z",
        "updatedAt" : "2020-10-13T09:00:48Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "f7bc6f73-0441-442b-9d59-e77221e5ebe7",
        "parentId" : "101b55e8-6abc-4fef-a8da-1e90ded6dffd",
        "authorId" : "a8e23d47-3ae4-4385-848c-38a216d1bd08",
        "body" : "Actually the `InferFiltersFromConstraints` is allways filtered out from the `operatorOptimizationRuleSet`:\r\nhttps://github.com/apache/spark/blob/5ce321dc80a699fa525ca5b69bf2c28e10f8a12a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala#L120-L123\r\n\r\nI think it is very misleading , perhaps we could remove it from there and avoid further confusion?\r\n\r\nIt was asked in the original PR, put looks like it was forgotten:\r\nhttps://github.com/apache/spark/pull/19149/files#r157777935",
        "createdAt" : "2020-10-07T04:44:08Z",
        "updatedAt" : "2020-10-13T09:00:48Z",
        "lastEditedBy" : "a8e23d47-3ae4-4385-848c-38a216d1bd08",
        "tags" : [
        ]
      },
      {
        "id" : "770e385e-5eae-4e92-b092-48922bacb76f",
        "parentId" : "101b55e8-6abc-4fef-a8da-1e90ded6dffd",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Yea, but I also don't know why. cc: @cloud-fan ",
        "createdAt" : "2020-10-07T04:52:27Z",
        "updatedAt" : "2020-10-13T09:00:48Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "19023a64-1263-460a-acce-a677b2abe1b1",
        "parentId" : "101b55e8-6abc-4fef-a8da-1e90ded6dffd",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I don't quite remember, maybe we can remove it.",
        "createdAt" : "2020-10-12T14:15:24Z",
        "updatedAt" : "2020-10-13T09:00:48Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "72aa579a-e668-41c1-9502-432ba05d7752",
        "parentId" : "101b55e8-6abc-4fef-a8da-1e90ded6dffd",
        "authorId" : "a8e23d47-3ae4-4385-848c-38a216d1bd08",
        "body" : "I went ahead and removed it from the `operatorOptimizationRuleSet`. All the tests are still passing as was expected.",
        "createdAt" : "2020-10-12T16:47:26Z",
        "updatedAt" : "2020-10-13T09:00:48Z",
        "lastEditedBy" : "a8e23d47-3ae4-4385-848c-38a216d1bd08",
        "tags" : [
        ]
      }
    ],
    "commit" : "857c0078f20609e9aca6839034b52708991aa1bf",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +120,124 @@        operatorOptimizationRuleSet: _*) ::\n      Batch(\"Infer Filters\", Once,\n        InferFiltersFromGenerate,\n        InferFiltersFromConstraints) ::\n      Batch(\"Operator Optimization after Inferring Filters\", fixedPoint,"
  },
  {
    "id" : "545633ed-47a9-4149-9e92-5bd0e3a291b8",
    "prId" : 29092,
    "prUrl" : "https://github.com/apache/spark/pull/29092#pullrequestreview-506983708",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8c3654f9-0a3d-4b82-9b43-d79b2ad631cb",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "nit: `operatorOptimizationRuleSet` -> `rulesWithoutInferFilters`?",
        "createdAt" : "2020-10-12T23:50:52Z",
        "updatedAt" : "2020-10-13T09:00:48Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "857c0078f20609e9aca6839034b52708991aa1bf",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +123,127 @@        InferFiltersFromConstraints) ::\n      Batch(\"Operator Optimization after Inferring Filters\", fixedPoint,\n        operatorOptimizationRuleSet: _*) ::\n      // Set strategy to Once to avoid pushing filter every time because we do not change the\n      // join condition."
  },
  {
    "id" : "b5618532-4f33-48b4-bc7f-f0a2f72bb90f",
    "prId" : 29092,
    "prUrl" : "https://github.com/apache/spark/pull/29092#pullrequestreview-507607013",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6174a669-a16a-4052-9bd5-625865604fce",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "This can be useful to filter rows before join, but can we pushdown `Size` expression to datasource?",
        "createdAt" : "2020-10-13T16:11:11Z",
        "updatedAt" : "2020-10-13T16:11:11Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "8d7cf669-140a-43a8-946e-93c2775777d5",
        "parentId" : "6174a669-a16a-4052-9bd5-625865604fce",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I don't think so...",
        "createdAt" : "2020-10-13T16:12:46Z",
        "updatedAt" : "2020-10-13T16:12:47Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "857c0078f20609e9aca6839034b52708991aa1bf",
    "line" : 46,
    "diffHunk" : "@@ -1,1 +883,887 @@      val inferredFilters = ExpressionSet(\n        Seq(\n          GreaterThan(Size(g.children.head), Literal(0)),\n          IsNotNull(g.children.head)\n        )"
  },
  {
    "id" : "c75d06b9-ac8c-4771-b46b-ec0cffc56295",
    "prId" : 28575,
    "prUrl" : "https://github.com/apache/spark/pull/28575#pullrequestreview-421512448",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "26c79a4c-07d4-4b79-b3f3-562bf7c09067",
        "parentId" : null,
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "group by  qualifier to avoid generating too many predicates. For example:\r\nTPCDS q85:\r\nWithout group by qualifier:\r\n```\r\n== Physical Plan ==\r\nTakeOrderedAndProject(limit=100, orderBy=[substr(r_reason_desc, 1, 20)#137 ASC NULLS FIRST,aggOrder#142 ASC NULLS FIRST,avg(wr_refunded_cash)#139 ASC NULLS FIRST,avg(wr_fee)#140 ASC NULLS FIRST], output=[substr(r_reason_desc, 1, 20)#137,avg(ws_quantity)#138,avg(wr_refunded_cash)#139,avg(wr_fee)#140])\r\n+- *(9) HashAggregate(keys=[r_reason_desc#124], functions=[avg(cast(ws_quantity#18 as bigint)), avg(UnscaledValue(wr_refunded_cash#54)), avg(UnscaledValue(wr_fee#52))])\r\n   +- Exchange hashpartitioning(r_reason_desc#124, 5), true, [id=#351]\r\n      +- *(8) HashAggregate(keys=[r_reason_desc#124], functions=[partial_avg(cast(ws_quantity#18 as bigint)), partial_avg(UnscaledValue(wr_refunded_cash#54)), partial_avg(UnscaledValue(wr_fee#52))])\r\n         +- *(8) Project [ws_quantity#18, wr_fee#52, wr_refunded_cash#54, r_reason_desc#124]\r\n            +- *(8) BroadcastHashJoin [wr_reason_sk#46L], [cast(r_reason_sk#122 as bigint)], Inner, BuildRight\r\n               :- *(8) Project [ws_quantity#18, wr_reason_sk#46L, wr_fee#52, wr_refunded_cash#54]\r\n               :  +- *(8) BroadcastHashJoin [ws_sold_date_sk#0], [d_date_sk#94], Inner, BuildRight\r\n               :     :- *(8) Project [ws_sold_date_sk#0, ws_quantity#18, wr_reason_sk#46L, wr_fee#52, wr_refunded_cash#54]\r\n               :     :  +- *(8) BroadcastHashJoin [wr_refunded_addr_sk#40L], [cast(ca_address_sk#81 as bigint)], Inner, BuildRight, ((((ca_state#89 IN (IN,OH,NJ) AND (ws_net_profit#33 >= 100.00)) AND (ws_net_profit#33 <= 200.00)) OR ((ca_state#89 IN (WI,CT,KY) AND (ws_net_profit#33 >= 150.00)) AND (ws_net_profit#33 <= 300.00))) OR ((ca_state#89 IN (LA,IA,AR) AND (ws_net_profit#33 >= 50.00)) AND (ws_net_profit#33 <= 250.00)))\r\n               :     :     :- *(8) Project [ws_sold_date_sk#0, ws_quantity#18, ws_net_profit#33, wr_refunded_addr_sk#40L, wr_reason_sk#46L, wr_fee#52, wr_refunded_cash#54]\r\n               :     :     :  +- *(8) BroadcastHashJoin [wr_returning_cdemo_sk#42L, cd_marital_status#74, cd_education_status#75], [cast(cd_demo_sk#125 as bigint), cd_marital_status#127, cd_education_status#128], Inner, BuildRight\r\n               :     :     :     :- *(8) Project [ws_sold_date_sk#0, ws_quantity#18, ws_net_profit#33, wr_refunded_addr_sk#40L, wr_returning_cdemo_sk#42L, wr_reason_sk#46L, wr_fee#52, wr_refunded_cash#54, cd_marital_status#74, cd_education_status#75]\r\n               :     :     :     :  +- *(8) BroadcastHashJoin [wr_refunded_cdemo_sk#38L], [cast(cd_demo_sk#72 as bigint)], Inner, BuildRight, ((((((cd_marital_status#74 = M) AND (cd_education_status#75 = Advanced Degree)) AND (ws_sales_price#21 >= 100.00)) AND (ws_sales_price#21 <= 150.00)) OR ((((cd_marital_status#74 = S) AND (cd_education_status#75 = College)) AND (ws_sales_price#21 >= 50.00)) AND (ws_sales_price#21 <= 100.00))) OR ((((cd_marital_status#74 = W) AND (cd_education_status#75 = 2 yr Degree)) AND (ws_sales_price#21 >= 150.00)) AND (ws_sales_price#21 <= 200.00)))\r\n               :     :     :     :     :- *(8) Project [ws_sold_date_sk#0, ws_quantity#18, ws_sales_price#21, ws_net_profit#33, wr_refunded_cdemo_sk#38L, wr_refunded_addr_sk#40L, wr_returning_cdemo_sk#42L, wr_reason_sk#46L, wr_fee#52, wr_refunded_cash#54]\r\n               :     :     :     :     :  +- *(8) BroadcastHashJoin [ws_web_page_sk#12], [wp_web_page_sk#58], Inner, BuildRight\r\n               :     :     :     :     :     :- *(8) Project [ws_sold_date_sk#0, ws_web_page_sk#12, ws_quantity#18, ws_sales_price#21, ws_net_profit#33, wr_refunded_cdemo_sk#38L, wr_refunded_addr_sk#40L, wr_returning_cdemo_sk#42L, wr_reason_sk#46L, wr_fee#52, wr_refunded_cash#54]\r\n               :     :     :     :     :     :  +- *(8) BroadcastHashJoin [cast(ws_item_sk#3 as bigint), cast(ws_order_number#17 as bigint)], [wr_item_sk#36L, wr_order_number#47L], Inner, BuildRight\r\n               :     :     :     :     :     :     :- *(8) Project [ws_sold_date_sk#0, ws_item_sk#3, ws_web_page_sk#12, ws_order_number#17, ws_quantity#18, ws_sales_price#21, ws_net_profit#33]\r\n               :     :     :     :     :     :     :  +- *(8) Filter (((((((((((((((((((isnotnull(ws_item_sk#3) AND isnotnull(ws_order_number#17)) AND isnotnull(ws_web_page_sk#12)) AND isnotnull(ws_sold_date_sk#0)) AND (((ws_sales_price#21 >= 100.00) OR (ws_sales_price#21 >= 50.00)) OR (ws_sales_price#21 >= 150.00))) AND (((ws_sales_price#21 >= 100.00) OR (ws_sales_price#21 <= 100.00)) OR (ws_sales_price#21 >= 150.00))) AND (((ws_sales_price#21 <= 150.00) OR (ws_sales_price#21 >= 50.00)) OR (ws_sales_price#21 >= 150.00))) AND (((ws_sales_price#21 <= 150.00) OR (ws_sales_price#21 <= 100.00)) OR (ws_sales_price#21 >= 150.00))) AND (((ws_sales_price#21 >= 100.00) OR (ws_sales_price#21 >= 50.00)) OR (ws_sales_price#21 <= 200.00))) AND (((ws_sales_price#21 >= 100.00) OR (ws_sales_price#21 <= 100.00)) OR (ws_sales_price#21 <= 200.00))) AND (((ws_sales_price#21 <= 150.00) OR (ws_sales_price#21 >= 50.00)) OR (ws_sales_price#21 <= 200.00))) AND (((ws_sales_price#21 <= 150.00) OR (ws_sales_price#21 <= 100.00)) OR (ws_sales_price#21 <= 200.00))) AND (((ws_net_profit#33 >= 100.00) OR (ws_net_profit#33 >= 150.00)) OR (ws_net_profit#33 >= 50.00))) AND (((ws_net_profit#33 >= 100.00) OR (ws_net_profit#33 <= 300.00)) OR (ws_net_profit#33 >= 50.00))) AND (((ws_net_profit#33 <= 200.00) OR (ws_net_profit#33 >= 150.00)) OR (ws_net_profit#33 >= 50.00))) AND (((ws_net_profit#33 <= 200.00) OR (ws_net_profit#33 <= 300.00)) OR (ws_net_profit#33 >= 50.00))) AND (((ws_net_profit#33 >= 100.00) OR (ws_net_profit#33 >= 150.00)) OR (ws_net_profit#33 <= 250.00))) AND (((ws_net_profit#33 >= 100.00) OR (ws_net_profit#33 <= 300.00)) OR (ws_net_profit#33 <= 250.00))) AND (((ws_net_profit#33 <= 200.00) OR (ws_net_profit#33 >= 150.00)) OR (ws_net_profit#33 <= 250.00))) AND (((ws_net_profit#33 <= 200.00) OR (ws_net_profit#33 <= 300.00)) OR (ws_net_profit#33 <= 250.00)))\r\n               :     :     :     :     :     :     :     +- *(8) ColumnarToRow\r\n               :     :     :     :     :     :     :        +- FileScan parquet default.web_sales[ws_sold_date_sk#0,ws_item_sk#3,ws_web_page_sk#12,ws_order_number#17,ws_quantity#18,ws_sales_price#21,ws_net_profit#33] Batched: true, DataFilters: [isnotnull(ws_item_sk#3), isnotnull(ws_order_number#17), isnotnull(ws_web_page_sk#12), isnotnull(..., Format: Parquet, Location: InMemoryFileIndex[file:/Users/yumwang/spark/SPARK-28216/sql/core/spark-warehouse/org.apache.spark..., PartitionFilters: [], PushedFilters: [IsNotNull(ws_item_sk), IsNotNull(ws_order_number), IsNotNull(ws_web_page_sk), IsNotNull(ws_sold_..., ReadSchema: struct<ws_sold_date_sk:int,ws_item_sk:int,ws_web_page_sk:int,ws_order_number:int,ws_quantity:int,...\r\n               :     :     :     :     :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, true], input[5, bigint, true])), [id=#291]\r\n               :     :     :     :     :     :        +- *(1) Project [wr_item_sk#36L, wr_refunded_cdemo_sk#38L, wr_refunded_addr_sk#40L, wr_returning_cdemo_sk#42L, wr_reason_sk#46L, wr_order_number#47L, wr_fee#52, wr_refunded_cash#54]\r\n               :     :     :     :     :     :           +- *(1) Filter (((((isnotnull(wr_item_sk#36L) AND isnotnull(wr_order_number#47L)) AND isnotnull(wr_refunded_cdemo_sk#38L)) AND isnotnull(wr_returning_cdemo_sk#42L)) AND isnotnull(wr_refunded_addr_sk#40L)) AND isnotnull(wr_reason_sk#46L))\r\n               :     :     :     :     :     :              +- *(1) ColumnarToRow\r\n               :     :     :     :     :     :                 +- FileScan parquet default.web_returns[wr_item_sk#36L,wr_refunded_cdemo_sk#38L,wr_refunded_addr_sk#40L,wr_returning_cdemo_sk#42L,wr_reason_sk#46L,wr_order_number#47L,wr_fee#52,wr_refunded_cash#54] Batched: true, DataFilters: [isnotnull(wr_item_sk#36L), isnotnull(wr_order_number#47L), isnotnull(wr_refunded_cdemo_sk#38L), ..., Format: Parquet, Location: InMemoryFileIndex[file:/Users/yumwang/spark/SPARK-28216/sql/core/spark-warehouse/org.apache.spark..., PartitionFilters: [], PushedFilters: [IsNotNull(wr_item_sk), IsNotNull(wr_order_number), IsNotNull(wr_refunded_cdemo_sk), IsNotNull(wr..., ReadSchema: struct<wr_item_sk:bigint,wr_refunded_cdemo_sk:bigint,wr_refunded_addr_sk:bigint,wr_returning_cdem...\r\n               :     :     :     :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint))), [id=#300]\r\n               :     :     :     :     :        +- *(2) Project [wp_web_page_sk#58]\r\n               :     :     :     :     :           +- *(2) Filter isnotnull(wp_web_page_sk#58)\r\n               :     :     :     :     :              +- *(2) ColumnarToRow\r\n               :     :     :     :     :                 +- FileScan parquet default.web_page[wp_web_page_sk#58] Batched: true, DataFilters: [isnotnull(wp_web_page_sk#58)], Format: Parquet, Location: InMemoryFileIndex[file:/Users/yumwang/spark/SPARK-28216/sql/core/spark-warehouse/org.apache.spark..., PartitionFilters: [], PushedFilters: [IsNotNull(wp_web_page_sk)], ReadSchema: struct<wp_web_page_sk:int>\r\n               :     :     :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint))), [id=#309]\r\n               :     :     :     :        +- *(3) Project [cd_demo_sk#72, cd_marital_status#74, cd_education_status#75]\r\n               :     :     :     :           +- *(3) Filter ((((((((((isnotnull(cd_demo_sk#72) AND isnotnull(cd_education_status#75)) AND isnotnull(cd_marital_status#74)) AND (((cd_marital_status#74 = M) OR (cd_marital_status#74 = S)) OR (cd_marital_status#74 = W))) AND (((cd_marital_status#74 = M) OR (cd_marital_status#74 = S)) OR (cd_education_status#75 = 2 yr Degree))) AND (((cd_marital_status#74 = M) OR (cd_education_status#75 = College)) OR (cd_marital_status#74 = W))) AND (((cd_marital_status#74 = M) OR (cd_education_status#75 = College)) OR (cd_education_status#75 = 2 yr Degree))) AND (((cd_education_status#75 = Advanced Degree) OR (cd_marital_status#74 = S)) OR (cd_marital_status#74 = W))) AND (((cd_education_status#75 = Advanced Degree) OR (cd_marital_status#74 = S)) OR (cd_education_status#75 = 2 yr Degree))) AND (((cd_education_status#75 = Advanced Degree) OR (cd_education_status#75 = College)) OR (cd_marital_status#74 = W))) AND (((cd_education_status#75 = Advanced Degree) OR (cd_education_status#75 = College)) OR (cd_education_status#75 = 2 yr Degree)))\r\n               :     :     :     :              +- *(3) ColumnarToRow\r\n               :     :     :     :                 +- FileScan parquet default.customer_demographics[cd_demo_sk#72,cd_marital_status#74,cd_education_status#75] Batched: true, DataFilters: [isnotnull(cd_demo_sk#72), isnotnull(cd_education_status#75), isnotnull(cd_marital_status#74), ((..., Format: Parquet, Location: InMemoryFileIndex[file:/Users/yumwang/spark/SPARK-28216/sql/core/spark-warehouse/org.apache.spark..., PartitionFilters: [], PushedFilters: [IsNotNull(cd_demo_sk), IsNotNull(cd_education_status), IsNotNull(cd_marital_status), Or(Or(Equal..., ReadSchema: struct<cd_demo_sk:int,cd_marital_status:string,cd_education_status:string>\r\n               :     :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint), input[1, string, true], input[2, string, true])), [id=#318]\r\n               :     :     :        +- *(4) Project [cd_demo_sk#125, cd_marital_status#127, cd_education_status#128]\r\n               :     :     :           +- *(4) Filter ((isnotnull(cd_demo_sk#125) AND isnotnull(cd_education_status#128)) AND isnotnull(cd_marital_status#127))\r\n               :     :     :              +- *(4) ColumnarToRow\r\n               :     :     :                 +- FileScan parquet default.customer_demographics[cd_demo_sk#125,cd_marital_status#127,cd_education_status#128] Batched: true, DataFilters: [isnotnull(cd_demo_sk#125), isnotnull(cd_education_status#128), isnotnull(cd_marital_status#127)], Format: Parquet, Location: InMemoryFileIndex[file:/Users/yumwang/spark/SPARK-28216/sql/core/spark-warehouse/org.apache.spark..., PartitionFilters: [], PushedFilters: [IsNotNull(cd_demo_sk), IsNotNull(cd_education_status), IsNotNull(cd_marital_status)], ReadSchema: struct<cd_demo_sk:int,cd_marital_status:string,cd_education_status:string>\r\n               :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint))), [id=#327]\r\n               :     :        +- *(5) Project [ca_address_sk#81, ca_state#89]\r\n               :     :           +- *(5) Filter (((isnotnull(ca_country#91) AND (ca_country#91 = United States)) AND isnotnull(ca_address_sk#81)) AND ((ca_state#89 IN (IN,OH,NJ) OR ca_state#89 IN (WI,CT,KY)) OR ca_state#89 IN (LA,IA,AR)))\r\n               :     :              +- *(5) ColumnarToRow\r\n               :     :                 +- FileScan parquet default.customer_address[ca_address_sk#81,ca_state#89,ca_country#91] Batched: true, DataFilters: [isnotnull(ca_country#91), (ca_country#91 = United States), isnotnull(ca_address_sk#81), ((ca_sta..., Format: Parquet, Location: InMemoryFileIndex[file:/Users/yumwang/spark/SPARK-28216/sql/core/spark-warehouse/org.apache.spark..., PartitionFilters: [], PushedFilters: [IsNotNull(ca_country), EqualTo(ca_country,United States), IsNotNull(ca_address_sk), Or(Or(In(ca_..., ReadSchema: struct<ca_address_sk:int,ca_state:string,ca_country:string>\r\n               :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint))), [id=#336]\r\n               :        +- *(6) Project [d_date_sk#94]\r\n               :           +- *(6) Filter ((isnotnull(d_year#100) AND (d_year#100 = 2000)) AND isnotnull(d_date_sk#94))\r\n               :              +- *(6) ColumnarToRow\r\n               :                 +- FileScan parquet default.date_dim[d_date_sk#94,d_year#100] Batched: true, DataFilters: [isnotnull(d_year#100), (d_year#100 = 2000), isnotnull(d_date_sk#94)], Format: Parquet, Location: InMemoryFileIndex[file:/Users/yumwang/spark/SPARK-28216/sql/core/spark-warehouse/org.apache.spark..., PartitionFilters: [], PushedFilters: [IsNotNull(d_year), EqualTo(d_year,2000), IsNotNull(d_date_sk)], ReadSchema: struct<d_date_sk:int,d_year:int>\r\n               +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint))), [id=#345]\r\n                  +- *(7) Project [r_reason_sk#122, r_reason_desc#124]\r\n                     +- *(7) Filter isnotnull(r_reason_sk#122)\r\n                        +- *(7) ColumnarToRow\r\n                           +- FileScan parquet default.reason[r_reason_sk#122,r_reason_desc#124] Batched: true, DataFilters: [isnotnull(r_reason_sk#122)], Format: Parquet, Location: InMemoryFileIndex[file:/Users/yumwang/spark/SPARK-28216/sql/core/spark-warehouse/org.apache.spark..., PartitionFilters: [], PushedFilters: [IsNotNull(r_reason_sk)], ReadSchema: struct<r_reason_sk:int,r_reason_desc:string>\r\n\r\n```\r\n\r\nGroup by qualifier:\r\n```\r\n== Physical Plan ==\r\nTakeOrderedAndProject(limit=100, orderBy=[substr(r_reason_desc, 1, 20)#137 ASC NULLS FIRST,aggOrder#142 ASC NULLS FIRST,avg(wr_refunded_cash)#139 ASC NULLS FIRST,avg(wr_fee)#140 ASC NULLS FIRST], output=[substr(r_reason_desc, 1, 20)#137,avg(ws_quantity)#138,avg(wr_refunded_cash)#139,avg(wr_fee)#140])\r\n+- *(9) HashAggregate(keys=[r_reason_desc#124], functions=[avg(cast(ws_quantity#18 as bigint)), avg(UnscaledValue(wr_refunded_cash#54)), avg(UnscaledValue(wr_fee#52))])\r\n   +- Exchange hashpartitioning(r_reason_desc#124, 5), true, [id=#351]\r\n      +- *(8) HashAggregate(keys=[r_reason_desc#124], functions=[partial_avg(cast(ws_quantity#18 as bigint)), partial_avg(UnscaledValue(wr_refunded_cash#54)), partial_avg(UnscaledValue(wr_fee#52))])\r\n         +- *(8) Project [ws_quantity#18, wr_fee#52, wr_refunded_cash#54, r_reason_desc#124]\r\n            +- *(8) BroadcastHashJoin [wr_reason_sk#46L], [cast(r_reason_sk#122 as bigint)], Inner, BuildRight\r\n               :- *(8) Project [ws_quantity#18, wr_reason_sk#46L, wr_fee#52, wr_refunded_cash#54]\r\n               :  +- *(8) BroadcastHashJoin [ws_sold_date_sk#0], [d_date_sk#94], Inner, BuildRight\r\n               :     :- *(8) Project [ws_sold_date_sk#0, ws_quantity#18, wr_reason_sk#46L, wr_fee#52, wr_refunded_cash#54]\r\n               :     :  +- *(8) BroadcastHashJoin [wr_refunded_addr_sk#40L], [cast(ca_address_sk#81 as bigint)], Inner, BuildRight, ((((ca_state#89 IN (IN,OH,NJ) AND (ws_net_profit#33 >= 100.00)) AND (ws_net_profit#33 <= 200.00)) OR ((ca_state#89 IN (WI,CT,KY) AND (ws_net_profit#33 >= 150.00)) AND (ws_net_profit#33 <= 300.00))) OR ((ca_state#89 IN (LA,IA,AR) AND (ws_net_profit#33 >= 50.00)) AND (ws_net_profit#33 <= 250.00)))\r\n               :     :     :- *(8) Project [ws_sold_date_sk#0, ws_quantity#18, ws_net_profit#33, wr_refunded_addr_sk#40L, wr_reason_sk#46L, wr_fee#52, wr_refunded_cash#54]\r\n               :     :     :  +- *(8) BroadcastHashJoin [wr_returning_cdemo_sk#42L, cd_marital_status#74, cd_education_status#75], [cast(cd_demo_sk#125 as bigint), cd_marital_status#127, cd_education_status#128], Inner, BuildRight\r\n               :     :     :     :- *(8) Project [ws_sold_date_sk#0, ws_quantity#18, ws_net_profit#33, wr_refunded_addr_sk#40L, wr_returning_cdemo_sk#42L, wr_reason_sk#46L, wr_fee#52, wr_refunded_cash#54, cd_marital_status#74, cd_education_status#75]\r\n               :     :     :     :  +- *(8) BroadcastHashJoin [wr_refunded_cdemo_sk#38L], [cast(cd_demo_sk#72 as bigint)], Inner, BuildRight, ((((((cd_marital_status#74 = M) AND (cd_education_status#75 = Advanced Degree)) AND (ws_sales_price#21 >= 100.00)) AND (ws_sales_price#21 <= 150.00)) OR ((((cd_marital_status#74 = S) AND (cd_education_status#75 = College)) AND (ws_sales_price#21 >= 50.00)) AND (ws_sales_price#21 <= 100.00))) OR ((((cd_marital_status#74 = W) AND (cd_education_status#75 = 2 yr Degree)) AND (ws_sales_price#21 >= 150.00)) AND (ws_sales_price#21 <= 200.00)))\r\n               :     :     :     :     :- *(8) Project [ws_sold_date_sk#0, ws_quantity#18, ws_sales_price#21, ws_net_profit#33, wr_refunded_cdemo_sk#38L, wr_refunded_addr_sk#40L, wr_returning_cdemo_sk#42L, wr_reason_sk#46L, wr_fee#52, wr_refunded_cash#54]\r\n               :     :     :     :     :  +- *(8) BroadcastHashJoin [ws_web_page_sk#12], [wp_web_page_sk#58], Inner, BuildRight\r\n               :     :     :     :     :     :- *(8) Project [ws_sold_date_sk#0, ws_web_page_sk#12, ws_quantity#18, ws_sales_price#21, ws_net_profit#33, wr_refunded_cdemo_sk#38L, wr_refunded_addr_sk#40L, wr_returning_cdemo_sk#42L, wr_reason_sk#46L, wr_fee#52, wr_refunded_cash#54]\r\n               :     :     :     :     :     :  +- *(8) BroadcastHashJoin [cast(ws_item_sk#3 as bigint), cast(ws_order_number#17 as bigint)], [wr_item_sk#36L, wr_order_number#47L], Inner, BuildRight\r\n               :     :     :     :     :     :     :- *(8) Project [ws_sold_date_sk#0, ws_item_sk#3, ws_web_page_sk#12, ws_order_number#17, ws_quantity#18, ws_sales_price#21, ws_net_profit#33]\r\n               :     :     :     :     :     :     :  +- *(8) Filter (((((isnotnull(ws_item_sk#3) AND isnotnull(ws_order_number#17)) AND isnotnull(ws_web_page_sk#12)) AND isnotnull(ws_sold_date_sk#0)) AND ((((ws_sales_price#21 >= 100.00) AND (ws_sales_price#21 <= 150.00)) OR ((ws_sales_price#21 >= 50.00) AND (ws_sales_price#21 <= 100.00))) OR ((ws_sales_price#21 >= 150.00) AND (ws_sales_price#21 <= 200.00)))) AND ((((ws_net_profit#33 >= 100.00) AND (ws_net_profit#33 <= 200.00)) OR ((ws_net_profit#33 >= 150.00) AND (ws_net_profit#33 <= 300.00))) OR ((ws_net_profit#33 >= 50.00) AND (ws_net_profit#33 <= 250.00))))\r\n               :     :     :     :     :     :     :     +- *(8) ColumnarToRow\r\n               :     :     :     :     :     :     :        +- FileScan parquet default.web_sales[ws_sold_date_sk#0,ws_item_sk#3,ws_web_page_sk#12,ws_order_number#17,ws_quantity#18,ws_sales_price#21,ws_net_profit#33] Batched: true, DataFilters: [isnotnull(ws_item_sk#3), isnotnull(ws_order_number#17), isnotnull(ws_web_page_sk#12), isnotnull(..., Format: Parquet, Location: InMemoryFileIndex[file:/Users/yumwang/spark/SPARK-28216/sql/core/spark-warehouse/org.apache.spark..., PartitionFilters: [], PushedFilters: [IsNotNull(ws_item_sk), IsNotNull(ws_order_number), IsNotNull(ws_web_page_sk), IsNotNull(ws_sold_..., ReadSchema: struct<ws_sold_date_sk:int,ws_item_sk:int,ws_web_page_sk:int,ws_order_number:int,ws_quantity:int,...\r\n               :     :     :     :     :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, true], input[5, bigint, true])), [id=#291]\r\n               :     :     :     :     :     :        +- *(1) Project [wr_item_sk#36L, wr_refunded_cdemo_sk#38L, wr_refunded_addr_sk#40L, wr_returning_cdemo_sk#42L, wr_reason_sk#46L, wr_order_number#47L, wr_fee#52, wr_refunded_cash#54]\r\n               :     :     :     :     :     :           +- *(1) Filter (((((isnotnull(wr_item_sk#36L) AND isnotnull(wr_order_number#47L)) AND isnotnull(wr_refunded_cdemo_sk#38L)) AND isnotnull(wr_returning_cdemo_sk#42L)) AND isnotnull(wr_refunded_addr_sk#40L)) AND isnotnull(wr_reason_sk#46L))\r\n               :     :     :     :     :     :              +- *(1) ColumnarToRow\r\n               :     :     :     :     :     :                 +- FileScan parquet default.web_returns[wr_item_sk#36L,wr_refunded_cdemo_sk#38L,wr_refunded_addr_sk#40L,wr_returning_cdemo_sk#42L,wr_reason_sk#46L,wr_order_number#47L,wr_fee#52,wr_refunded_cash#54] Batched: true, DataFilters: [isnotnull(wr_item_sk#36L), isnotnull(wr_order_number#47L), isnotnull(wr_refunded_cdemo_sk#38L), ..., Format: Parquet, Location: InMemoryFileIndex[file:/Users/yumwang/spark/SPARK-28216/sql/core/spark-warehouse/org.apache.spark..., PartitionFilters: [], PushedFilters: [IsNotNull(wr_item_sk), IsNotNull(wr_order_number), IsNotNull(wr_refunded_cdemo_sk), IsNotNull(wr..., ReadSchema: struct<wr_item_sk:bigint,wr_refunded_cdemo_sk:bigint,wr_refunded_addr_sk:bigint,wr_returning_cdem...\r\n               :     :     :     :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint))), [id=#300]\r\n               :     :     :     :     :        +- *(2) Project [wp_web_page_sk#58]\r\n               :     :     :     :     :           +- *(2) Filter isnotnull(wp_web_page_sk#58)\r\n               :     :     :     :     :              +- *(2) ColumnarToRow\r\n               :     :     :     :     :                 +- FileScan parquet default.web_page[wp_web_page_sk#58] Batched: true, DataFilters: [isnotnull(wp_web_page_sk#58)], Format: Parquet, Location: InMemoryFileIndex[file:/Users/yumwang/spark/SPARK-28216/sql/core/spark-warehouse/org.apache.spark..., PartitionFilters: [], PushedFilters: [IsNotNull(wp_web_page_sk)], ReadSchema: struct<wp_web_page_sk:int>\r\n               :     :     :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint))), [id=#309]\r\n               :     :     :     :        +- *(3) Project [cd_demo_sk#72, cd_marital_status#74, cd_education_status#75]\r\n               :     :     :     :           +- *(3) Filter (((isnotnull(cd_demo_sk#72) AND isnotnull(cd_education_status#75)) AND isnotnull(cd_marital_status#74)) AND ((((cd_marital_status#74 = M) AND (cd_education_status#75 = Advanced Degree)) OR ((cd_marital_status#74 = S) AND (cd_education_status#75 = College))) OR ((cd_marital_status#74 = W) AND (cd_education_status#75 = 2 yr Degree))))\r\n               :     :     :     :              +- *(3) ColumnarToRow\r\n               :     :     :     :                 +- FileScan parquet default.customer_demographics[cd_demo_sk#72,cd_marital_status#74,cd_education_status#75] Batched: true, DataFilters: [isnotnull(cd_demo_sk#72), isnotnull(cd_education_status#75), isnotnull(cd_marital_status#74), ((..., Format: Parquet, Location: InMemoryFileIndex[file:/Users/yumwang/spark/SPARK-28216/sql/core/spark-warehouse/org.apache.spark..., PartitionFilters: [], PushedFilters: [IsNotNull(cd_demo_sk), IsNotNull(cd_education_status), IsNotNull(cd_marital_status), Or(Or(And(E..., ReadSchema: struct<cd_demo_sk:int,cd_marital_status:string,cd_education_status:string>\r\n               :     :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint), input[1, string, true], input[2, string, true])), [id=#318]\r\n               :     :     :        +- *(4) Project [cd_demo_sk#125, cd_marital_status#127, cd_education_status#128]\r\n               :     :     :           +- *(4) Filter ((isnotnull(cd_demo_sk#125) AND isnotnull(cd_education_status#128)) AND isnotnull(cd_marital_status#127))\r\n               :     :     :              +- *(4) ColumnarToRow\r\n               :     :     :                 +- FileScan parquet default.customer_demographics[cd_demo_sk#125,cd_marital_status#127,cd_education_status#128] Batched: true, DataFilters: [isnotnull(cd_demo_sk#125), isnotnull(cd_education_status#128), isnotnull(cd_marital_status#127)], Format: Parquet, Location: InMemoryFileIndex[file:/Users/yumwang/spark/SPARK-28216/sql/core/spark-warehouse/org.apache.spark..., PartitionFilters: [], PushedFilters: [IsNotNull(cd_demo_sk), IsNotNull(cd_education_status), IsNotNull(cd_marital_status)], ReadSchema: struct<cd_demo_sk:int,cd_marital_status:string,cd_education_status:string>\r\n               :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint))), [id=#327]\r\n               :     :        +- *(5) Project [ca_address_sk#81, ca_state#89]\r\n               :     :           +- *(5) Filter (((isnotnull(ca_country#91) AND (ca_country#91 = United States)) AND isnotnull(ca_address_sk#81)) AND ((ca_state#89 IN (IN,OH,NJ) OR ca_state#89 IN (WI,CT,KY)) OR ca_state#89 IN (LA,IA,AR)))\r\n               :     :              +- *(5) ColumnarToRow\r\n               :     :                 +- FileScan parquet default.customer_address[ca_address_sk#81,ca_state#89,ca_country#91] Batched: true, DataFilters: [isnotnull(ca_country#91), (ca_country#91 = United States), isnotnull(ca_address_sk#81), ((ca_sta..., Format: Parquet, Location: InMemoryFileIndex[file:/Users/yumwang/spark/SPARK-28216/sql/core/spark-warehouse/org.apache.spark..., PartitionFilters: [], PushedFilters: [IsNotNull(ca_country), EqualTo(ca_country,United States), IsNotNull(ca_address_sk), Or(Or(In(ca_..., ReadSchema: struct<ca_address_sk:int,ca_state:string,ca_country:string>\r\n               :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint))), [id=#336]\r\n               :        +- *(6) Project [d_date_sk#94]\r\n               :           +- *(6) Filter ((isnotnull(d_year#100) AND (d_year#100 = 2000)) AND isnotnull(d_date_sk#94))\r\n               :              +- *(6) ColumnarToRow\r\n               :                 +- FileScan parquet default.date_dim[d_date_sk#94,d_year#100] Batched: true, DataFilters: [isnotnull(d_year#100), (d_year#100 = 2000), isnotnull(d_date_sk#94)], Format: Parquet, Location: InMemoryFileIndex[file:/Users/yumwang/spark/SPARK-28216/sql/core/spark-warehouse/org.apache.spark..., PartitionFilters: [], PushedFilters: [IsNotNull(d_year), EqualTo(d_year,2000), IsNotNull(d_date_sk)], ReadSchema: struct<d_date_sk:int,d_year:int>\r\n               +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint))), [id=#345]\r\n                  +- *(7) Project [r_reason_sk#122, r_reason_desc#124]\r\n                     +- *(7) Filter isnotnull(r_reason_sk#122)\r\n                        +- *(7) ColumnarToRow\r\n                           +- FileScan parquet default.reason[r_reason_sk#122,r_reason_desc#124] Batched: true, DataFilters: [isnotnull(r_reason_sk#122)], Format: Parquet, Location: InMemoryFileIndex[file:/Users/yumwang/spark/SPARK-28216/sql/core/spark-warehouse/org.apache.spark..., PartitionFilters: [], PushedFilters: [IsNotNull(r_reason_sk)], ReadSchema: struct<r_reason_sk:int,r_reason_desc:string>\r\n\r\n\r\n```",
        "createdAt" : "2020-05-31T17:18:00Z",
        "updatedAt" : "2020-06-03T10:41:12Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      }
    ],
    "commit" : "e8f24714d90ef7a2fbcd2458463d91c05e28ebbf",
    "line" : 44,
    "diffHunk" : "@@ -1,1 +1395,1399 @@      condition match {\n        case or @ Or(left: And, right: And) =>\n          val lhs = splitConjunctivePredicates(left).groupBy(_.references.map(_.qualifier))\n          val rhs = splitConjunctivePredicates(right).groupBy(_.references.map(_.qualifier))\n          if (lhs.size > 1) {"
  },
  {
    "id" : "24690829-b83e-480b-9e06-b53cf74a49cc",
    "prId" : 28575,
    "prUrl" : "https://github.com/apache/spark/pull/28575#pullrequestreview-423625987",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6566bac2-0a1c-4e38-92a7-25684f425404",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "does it apply to all the predicates? like when we pushdown filters to the data source?",
        "createdAt" : "2020-06-03T08:24:58Z",
        "updatedAt" : "2020-06-03T10:41:12Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "2159c96b-a4be-4864-bc41-7146a6a36e89",
        "parentId" : "6566bac2-0a1c-4e38-92a7-25684f425404",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "Yes, such as the step of scanning web_sales table for TPC-DS q85.\r\n\r\nBefore this pr:\r\n```\r\n+- *(8) Filter (((isnotnull(ws_item_sk#3) AND isnotnull(ws_order_number#17)) AND isnotnull(ws_web_page_sk#12)) AND isnotnull(ws_sold_date_sk#0))\r\n   +- *(8) ColumnarToRow\r\n      +- FileScan parquet default.web_sales[ws_sold_date_sk#0,ws_item_sk#3,ws_web_page_sk#12,ws_order_number#17,ws_quantity#18,ws_sales_price#21,ws_net_profit#33] Batched: true, DataFilters: [isnotnull(ws_item_sk#3), isnotnull(ws_order_number#17), isnotnull(ws_web_page_sk#12), isnotnull(ws_sold_date_sk#0)], Format: Parquet, Location: InMemoryFileIndex[file:/Users/yumwang/spark/SPARK-28216/sql/core/spark-warehouse/org.apache.spark.sql.TPCDSQuerySuite/web_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ws_item_sk), IsNotNull(ws_order_number), IsNotNull(ws_web_page_sk), IsNotNull(ws_sold_date_sk)], ReadSchema: struct<ws_sold_date_sk:int,ws_item_sk:int,ws_web_page_sk:int,ws_order_number:int,ws_quantity:int,ws_sales_price:decimal(7,2),ws_net_profit:decimal(7,2)>\r\n\r\n```\r\n\r\nAfter this pr:\r\n```\r\n+- *(8) Filter (((((isnotnull(ws_item_sk#3) AND isnotnull(ws_order_number#17)) AND isnotnull(ws_web_page_sk#12)) AND isnotnull(ws_sold_date_sk#0)) AND ((((ws_sales_price#21 >= 100.00) AND (ws_sales_price#21 <= 150.00)) OR ((ws_sales_price#21 >= 50.00) AND (ws_sales_price#21 <= 100.00))) OR ((ws_sales_price#21 >= 150.00) AND (ws_sales_price#21 <= 200.00)))) AND ((((ws_net_profit#33 >= 100.00) AND (ws_net_profit#33 <= 200.00)) OR ((ws_net_profit#33 >= 150.00) AND (ws_net_profit#33 <= 300.00))) OR ((ws_net_profit#33 >= 50.00) AND (ws_net_profit#33 <= 250.00))))\r\n   +- *(8) ColumnarToRow\r\n      +- FileScan parquet default.web_sales[ws_sold_date_sk#0,ws_item_sk#3,ws_web_page_sk#12,ws_order_number#17,ws_quantity#18,ws_sales_price#21,ws_net_profit#33] Batched: true, DataFilters: [isnotnull(ws_item_sk#3), isnotnull(ws_order_number#17), isnotnull(ws_web_page_sk#12), isnotnull(ws_sold_date_sk#0), ((((ws_sales_price#21 >= 100.00) AND (ws_sales_price#21 <= 150.00)) OR ((ws_sales_price#21 >= 50.00) AND (ws_sales_price#21 <= 100.00))) OR ((ws_sales_price#21 >= 150.00) AND (ws_sales_price#21 <= 200.00))), ((((ws_net_profit#33 >= 100.00) AND (ws_net_profit#33 <= 200.00)) OR ((ws_net_profit#33 >= 150.00) AND (ws_net_profit#33 <= 300.00))) OR ((ws_net_profit#33 >= 50.00) AND (ws_net_profit#33 <= 250.00)))], Format: Parquet, Location: InMemoryFileIndex[file:/Users/yumwang/spark/SPARK-28216/sql/core/spark-warehouse/org.apache.spark.sql.TPCDSQuerySuite/web_sales], PartitionFilters: [], PushedFilters: [IsNotNull(ws_item_sk), IsNotNull(ws_order_number), IsNotNull(ws_web_page_sk), IsNotNull(ws_sold_date_sk), Or(Or(And(GreaterThanOrEqual(ws_sales_price,100.00),LessThanOrEqual(ws_sales_price,150.00)),And(GreaterThanOrEqual(ws_sales_price,50.00),LessThanOrEqual(ws_sales_price,100.00))),And(GreaterThanOrEqual(ws_sales_price,150.00),LessThanOrEqual(ws_sales_price,200.00))), Or(Or(And(GreaterThanOrEqual(ws_net_profit,100.00),LessThanOrEqual(ws_net_profit,200.00)),And(GreaterThanOrEqual(ws_net_profit,150.00),LessThanOrEqual(ws_net_profit,300.00))),And(GreaterThanOrEqual(ws_net_profit,50.00),LessThanOrEqual(ws_net_profit,250.00)))], ReadSchema: struct<ws_sold_date_sk:int,ws_item_sk:int,ws_web_page_sk:int,ws_order_number:int,ws_quantity:int,ws_sales_price:decimal(7,2),ws_net_profit:decimal(7,2)>\r\n\r\n```",
        "createdAt" : "2020-06-03T14:43:13Z",
        "updatedAt" : "2020-06-03T14:43:14Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      }
    ],
    "commit" : "e8f24714d90ef7a2fbcd2458463d91c05e28ebbf",
    "line" : 31,
    "diffHunk" : "@@ -1,1 +1382,1386 @@ * more predicate.\n */\nobject PushPredicateThroughJoinByCNF extends Rule[LogicalPlan] with PredicateHelper {\n  /**\n   * Rewrite pattern:"
  },
  {
    "id" : "eda8d336-5a8a-4406-abd9-414fc2c1c0c9",
    "prId" : 28575,
    "prUrl" : "https://github.com/apache/spark/pull/28575#pullrequestreview-423415921",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "33408ed1-a105-4bbd-a4df-0fa7aaecbe8d",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "shall we pick `rhs` if it has more conjunctive predicates?",
        "createdAt" : "2020-06-03T08:44:48Z",
        "updatedAt" : "2020-06-03T10:41:12Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "533c3f4b-2798-4c74-86e7-214e387c1eef",
        "parentId" : "33408ed1-a105-4bbd-a4df-0fa7aaecbe8d",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "Yes. We will pick it at `case or @ Or(left, right: And) =>` or `case or @ Or(left: And, right: And) =>`.\r\nE.g.: `(a && b) || (c && d)`. The rewriting steps are:\r\n`(a && b) || (c && d)` --> `(a || (c && d)) && (b || (c && d))` --> `(a || c) && (a || d) && (b || c) && (b && d)`.\r\n\r\nWe will pick it at `case or @ Or(left, right: And) =>` if `a` is fixed .",
        "createdAt" : "2020-06-03T10:20:07Z",
        "updatedAt" : "2020-06-03T10:41:12Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      }
    ],
    "commit" : "e8f24714d90ef7a2fbcd2458463d91c05e28ebbf",
    "line" : 46,
    "diffHunk" : "@@ -1,1 +1397,1401 @@          val lhs = splitConjunctivePredicates(left).groupBy(_.references.map(_.qualifier))\n          val rhs = splitConjunctivePredicates(right).groupBy(_.references.map(_.qualifier))\n          if (lhs.size > 1) {\n            lhs.values.map(_.reduceLeft(And)).map { e =>\n              toCNF(Or(toCNF(e, depth + 1), toCNF(right, depth + 1)), depth + 1)"
  },
  {
    "id" : "5ce3bc74-3d59-4044-bd88-53c517fe0f46",
    "prId" : 28575,
    "prUrl" : "https://github.com/apache/spark/pull/28575#pullrequestreview-424138393",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bad08e34-c9b9-4c34-9dd0-b9e3866831bd",
        "parentId" : null,
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "1 .  (a && b) || (c && d) --> (a || c) && (a || d) && (b || c) && (b || d) ï¼Ÿ",
        "createdAt" : "2020-06-04T06:25:33Z",
        "updatedAt" : "2020-06-04T06:25:41Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "d0f12858-8ad3-4d4b-97e6-073770397e1e",
        "parentId" : "bad08e34-c9b9-4c34-9dd0-b9e3866831bd",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "Thank you. I will update it in the next commit.",
        "createdAt" : "2020-06-04T06:33:30Z",
        "updatedAt" : "2020-06-04T06:33:31Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      }
    ],
    "commit" : "e8f24714d90ef7a2fbcd2458463d91c05e28ebbf",
    "line" : 34,
    "diffHunk" : "@@ -1,1 +1385,1389 @@  /**\n   * Rewrite pattern:\n   * 1. (a && b) || (c && d) --> (a || c) && (a || d) && (b || c) && (b && d)\n   * 2. (a && b) || c --> (a || c) && (b || c)\n   * 3. a || (b && c) --> (a || b) && (a || c)"
  },
  {
    "id" : "70f3b495-799d-489d-807c-768188bac766",
    "prId" : 28424,
    "prUrl" : "https://github.com/apache/spark/pull/28424#pullrequestreview-407695276",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "599a2a64-1b27-4809-ac98-a27654508243",
        "parentId" : null,
        "authorId" : "f3166ab8-4dba-4d22-b10b-31a984dfa2ad",
        "body" : "1. Should we write a separate rule so that all queries with distinct followed by left semi will benefit? (`select distinct(a.col1) from a left semi join b on a.col2 = b.col2`)\r\n2. Can we make it generic so that distinct followed by right semi join also gets handled?",
        "createdAt" : "2020-05-07T17:41:48Z",
        "updatedAt" : "2020-05-07T17:57:28Z",
        "lastEditedBy" : "f3166ab8-4dba-4d22-b10b-31a984dfa2ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "29719f9de51e7026584b7882755d3cd7806ae8f5",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +1563,1567 @@ * If Distinct is pushdown on left side, it won't apply Distinct after Join as it is not needed.\n */\nobject ReplaceIntersectWithSemiJoin extends Rule[LogicalPlan] {\n  private def canBroadcast(plan: LogicalPlan): Boolean = {\n    plan.stats.sizeInBytes >= 0 && plan.stats.sizeInBytes <= SQLConf.get.autoBroadcastJoinThreshold"
  },
  {
    "id" : "aea5a7f2-af2e-4901-8e27-666467bd6ebe",
    "prId" : 28424,
    "prUrl" : "https://github.com/apache/spark/pull/28424#pullrequestreview-407695276",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "674374fb-8106-46a1-8abf-81f6888052e1",
        "parentId" : null,
        "authorId" : "f3166ab8-4dba-4d22-b10b-31a984dfa2ad",
        "body" : "Should we check only for `canBroadcast(Distinct(right)`, since `canBroadcast(right)` will be checked during physical planning in `JoinSelection`.. That way we could avoid having to copy `canBroadcast` logic? ",
        "createdAt" : "2020-05-07T17:44:40Z",
        "updatedAt" : "2020-05-07T17:57:28Z",
        "lastEditedBy" : "f3166ab8-4dba-4d22-b10b-31a984dfa2ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "29719f9de51e7026584b7882755d3cd7806ae8f5",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +1571,1575 @@    if (!SQLConf.get.optimizeIntersectEnabled) {\n      false\n    } else if (!canBroadcast(right) && canBroadcast(Distinct(right))) {\n      true\n    } else {"
  },
  {
    "id" : "e3d629fb-ca7a-4bc3-ae56-a4517994d8dc",
    "prId" : 28424,
    "prUrl" : "https://github.com/apache/spark/pull/28424#pullrequestreview-407695276",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9fe6284c-8ef6-41da-9b0c-3580ffdb39e6",
        "parentId" : null,
        "authorId" : "f3166ab8-4dba-4d22-b10b-31a984dfa2ad",
        "body" : "Do we want to leave the old code as is and add a sperate branch of case match which gets activated on `SQLConf.get.optimizeIntersectEnabled`, that way we could avoid checks for the flag in each of the methods?",
        "createdAt" : "2020-05-07T17:46:18Z",
        "updatedAt" : "2020-05-07T17:57:28Z",
        "lastEditedBy" : "f3166ab8-4dba-4d22-b10b-31a984dfa2ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "29719f9de51e7026584b7882755d3cd7806ae8f5",
    "line" : 59,
    "diffHunk" : "@@ -1,1 +1609,1613 @@\n  def apply(plan: LogicalPlan): LogicalPlan = plan transform {\n    case Intersect(left, right, false) =>\n      assert(left.output.size == right.output.size)\n      val applyDistinctToLeftSide = shouldApplyDistinctToLeftSide(left)"
  },
  {
    "id" : "7edadfda-e313-4746-b006-0e7baefa0458",
    "prId" : 28424,
    "prUrl" : "https://github.com/apache/spark/pull/28424#pullrequestreview-407695276",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9676a1e4-e53b-4540-92ff-fe83113cff2f",
        "parentId" : null,
        "authorId" : "f3166ab8-4dba-4d22-b10b-31a984dfa2ad",
        "body" : "nit: `SQLConf.get.optimizeIntersectEnabled && shouldApplyDistinctBasedOnReductionThreshold(left)`",
        "createdAt" : "2020-05-07T17:46:51Z",
        "updatedAt" : "2020-05-07T17:57:28Z",
        "lastEditedBy" : "f3166ab8-4dba-4d22-b10b-31a984dfa2ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "29719f9de51e7026584b7882755d3cd7806ae8f5",
    "line" : 29,
    "diffHunk" : "@@ -1,1 +1579,1583 @@\n  private def shouldApplyDistinctToLeftSide(left: LogicalPlan): Boolean = {\n    if (!SQLConf.get.optimizeIntersectEnabled) {\n      false\n    } else {"
  },
  {
    "id" : "05dd8316-fa3f-4118-a2d3-0951aecf09b3",
    "prId" : 28424,
    "prUrl" : "https://github.com/apache/spark/pull/28424#pullrequestreview-407695276",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d838e440-c48f-450a-99f2-3d5ca287cfeb",
        "parentId" : null,
        "authorId" : "f3166ab8-4dba-4d22-b10b-31a984dfa2ad",
        "body" : "nit: remove if.. else.. `reductionRatio >= SQLConf.get.optimizeIntersectDistinctReductionThreshold` should do?",
        "createdAt" : "2020-05-07T17:47:53Z",
        "updatedAt" : "2020-05-07T17:57:28Z",
        "lastEditedBy" : "f3166ab8-4dba-4d22-b10b-31a984dfa2ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "29719f9de51e7026584b7882755d3cd7806ae8f5",
    "line" : 47,
    "diffHunk" : "@@ -1,1 +1597,1601 @@          rowCountBefore\n        }\n        if (reductionRatio >= SQLConf.get.optimizeIntersectDistinctReductionThreshold) {\n          true\n        } else {"
  },
  {
    "id" : "22a9ea53-344c-46b2-aa42-e2e09e4bad2d",
    "prId" : 28424,
    "prUrl" : "https://github.com/apache/spark/pull/28424#pullrequestreview-407695276",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c79cf0c6-ed2e-4629-9115-9354fac171dc",
        "parentId" : null,
        "authorId" : "f3166ab8-4dba-4d22-b10b-31a984dfa2ad",
        "body" : "nit: `case _ =>`",
        "createdAt" : "2020-05-07T17:48:07Z",
        "updatedAt" : "2020-05-07T17:57:28Z",
        "lastEditedBy" : "f3166ab8-4dba-4d22-b10b-31a984dfa2ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "29719f9de51e7026584b7882755d3cd7806ae8f5",
    "line" : 52,
    "diffHunk" : "@@ -1,1 +1602,1606 @@          false\n        }\n      case (_, _) =>\n        // Stats are not available\n        false"
  },
  {
    "id" : "fe4487db-0639-4936-b1f6-80ffc277c704",
    "prId" : 28424,
    "prUrl" : "https://github.com/apache/spark/pull/28424#pullrequestreview-407711165",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "21851dc3-ad72-4af1-b999-8f30f514ed03",
        "parentId" : null,
        "authorId" : "f3166ab8-4dba-4d22-b10b-31a984dfa2ad",
        "body" : "If left leg of join is broadcastable, will it still be useful to push the distinct to the. left leg?",
        "createdAt" : "2020-05-07T18:03:07Z",
        "updatedAt" : "2020-05-07T18:03:13Z",
        "lastEditedBy" : "f3166ab8-4dba-4d22-b10b-31a984dfa2ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "29719f9de51e7026584b7882755d3cd7806ae8f5",
    "line" : 32,
    "diffHunk" : "@@ -1,1 +1582,1586 @@      false\n    } else {\n      shouldApplyDistinctBasedOnReductionThreshold(left)\n    }\n  }"
  },
  {
    "id" : "f464ca63-da06-4bc2-8218-9a416f2b30e9",
    "prId" : 28424,
    "prUrl" : "https://github.com/apache/spark/pull/28424#pullrequestreview-429874731",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e396b79e-62a5-44f6-80ba-2b8cf4c7a9ac",
        "parentId" : null,
        "authorId" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "body" : "Now that we have `JoinSelectionHelper`, can we use the methods over there?",
        "createdAt" : "2020-06-12T16:20:24Z",
        "updatedAt" : "2020-06-12T16:20:24Z",
        "lastEditedBy" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "tags" : [
        ]
      }
    ],
    "commit" : "29719f9de51e7026584b7882755d3cd7806ae8f5",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +1564,1568 @@ */\nobject ReplaceIntersectWithSemiJoin extends Rule[LogicalPlan] {\n  private def canBroadcast(plan: LogicalPlan): Boolean = {\n    plan.stats.sizeInBytes >= 0 && plan.stats.sizeInBytes <= SQLConf.get.autoBroadcastJoinThreshold\n  }"
  },
  {
    "id" : "694ca68a-ed04-4557-94c8-1bb9ac98b61b",
    "prId" : 28043,
    "prUrl" : "https://github.com/apache/spark/pull/28043#pullrequestreview-384358418",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cd43d134-cbd3-48bf-ae80-8b4c4358f37d",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Although this passes PR Builder, this causes consistent failure after merging for all builds.\r\n```\r\norg.apache.spark.sql.hive.HiveSparkSubmitSuite.temporary Hive UDF: define a UDF and use it\r\norg.apache.spark.sql.hive.HiveSparkSubmitSuite.permanent Hive UDF: define a UDF and use it\r\norg.apache.spark.sql.hive.HiveSparkSubmitSuite.permanent Hive UDF: use a already defined permanent function\r\n```",
        "createdAt" : "2020-03-31T02:06:36Z",
        "updatedAt" : "2020-03-31T02:06:36Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "e281c85c-8d83-4d00-a002-028cbc101ad7",
        "parentId" : "cd43d134-cbd3-48bf-ae80-8b4c4358f37d",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "This fails at `checkBatchIdempotence` of `RuleExecutor`.\r\n```\r\n[info]   2020-03-30 19:00:21.097 - stderr> 20/03/30 19:00:21 INFO TemporaryHiveUDFTest: Running a simple query on the table.\r\n[info]   2020-03-30 19:00:21.198 - stderr> Exception in thread \"main\" org.apache.spark.sql.catalyst.errors.package$TreeNodeException:\r\n[info]   2020-03-30 19:00:21.198 - stderr> Once strategy's idempotence is broken for batch RewriteSubquery\r\n[info]   2020-03-30 19:00:21.198 - stderr>  Aggregate [count(1) AS count#15L]       Aggregate [count(1) AS count#15L]\r\n[info]   2020-03-30 19:00:21.198 - stderr>  +- Aggregate [val#5]                    +- Aggregate [val#5]\r\n[info]   2020-03-30 19:00:21.198 - stderr> !   +- Project [val#5]                      +- LocalRelation [val#5]\r\n[info]   2020-03-30 19:00:21.198 - stderr> !      +- LocalRelation [key#4, val#5]\r\n[info]   2020-03-30 19:00:21.198 - stderr>           , tree:\r\n[info]   2020-03-30 19:00:21.198 - stderr> Aggregate [count(1) AS count#15L]\r\n[info]   2020-03-30 19:00:21.198 - stderr> +- Aggregate [val#5]\r\n[info]   2020-03-30 19:00:21.198 - stderr>    +- LocalRelation [val#5]\r\n```",
        "createdAt" : "2020-03-31T02:07:56Z",
        "updatedAt" : "2020-03-31T02:07:56Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "7bfdabad-3535-4ade-85d4-45a93b0de589",
        "parentId" : "cd43d134-cbd3-48bf-ae80-8b4c4358f37d",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "I see. We need a separate batch to propagate empty relation after rewriting subquery ",
        "createdAt" : "2020-03-31T02:18:46Z",
        "updatedAt" : "2020-03-31T02:18:46Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "b084a27dec2f68d179a6c245f2845b45232baf63",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +200,204 @@      RewritePredicateSubquery,\n      ConvertToLocalRelation,\n      PropagateEmptyRelation,\n      ColumnPruning,\n      CollapseProject,"
  },
  {
    "id" : "46d461c9-0de7-44a8-8ef3-70c212f87fb8",
    "prId" : 27648,
    "prUrl" : "https://github.com/apache/spark/pull/27648#pullrequestreview-367673128",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0f891618-4074-4368-a4c4-094669b86559",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "So, what you're doing right now is basically https://github.com/apache/spark/pull/8718 which was rejected.",
        "createdAt" : "2020-02-27T14:09:43Z",
        "updatedAt" : "2020-02-27T14:09:45Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "7270ba00-a2e9-4b46-9d67-5138d6348fc3",
        "parentId" : "0f891618-4074-4368-a4c4-094669b86559",
        "authorId" : "832e1988-205f-40f9-89d5-c37ea16224c9",
        "body" : "In this solution i am not generalising casting. This PR only deals with Byte and Short columns and Values are only downcasted in case they are within byte/short range. Otherwise whole Predicate is left unchanged. \r\nAlso before pushing i am explicitly checking whether these values can be pushed down or not. ",
        "createdAt" : "2020-02-28T13:09:19Z",
        "updatedAt" : "2020-02-28T13:09:19Z",
        "lastEditedBy" : "832e1988-205f-40f9-89d5-c37ea16224c9",
        "tags" : [
        ]
      },
      {
        "id" : "f9978c5e-2f59-49d0-a5d7-e49f859e943c",
        "parentId" : "0f891618-4074-4368-a4c4-094669b86559",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "The approach basically looks same. Also, we should better generalise it so other data sources can support. The current approach seems difficult to extend.",
        "createdAt" : "2020-03-03T02:55:21Z",
        "updatedAt" : "2020-03-03T02:55:22Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "5ee3d424b2a9270a3cb910ce363d1ab4c7f81655",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +1807,1811 @@/**\n * Removes Cast expressions from predicates to push down the filters. This will allow filter push\n * down for byte and short columns without explicitly casting literals.\n */\nobject PushDownByteShortWithoutCasting extends Rule[LogicalPlan] {"
  },
  {
    "id" : "8c0de1a2-6db4-42a0-9004-759a6fb4c0da",
    "prId" : 27224,
    "prUrl" : "https://github.com/apache/spark/pull/27224#pullrequestreview-343639918",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bf506b8e-baa4-4145-ac4c-19fd073e6a2c",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "any concern for multiple inner aggregates for now?",
        "createdAt" : "2020-01-16T01:02:15Z",
        "updatedAt" : "2020-01-23T23:36:08Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "2cfb0388-4452-41cf-be0a-950c7c6a6a36",
        "parentId" : "bf506b8e-baa4-4145-ac4c-19fd073e6a2c",
        "authorId" : "88ab38a7-4ab4-463d-ac99-878435e95f58",
        "body" : "I don't believe there are lots of cases that would be collapsible, there is one case in the tests that is marked as \"this should be collapsible\" along these lines. Are there some in particular you are thinking of?\r\n\r\n```\r\n  // I think this could be collapsed, I would need to make the rule smarter to understand\r\n  // that sums can be added together with the standard scalar expression addition operator\r\n  test(\"Cannot collapse nested Agg references, complex outer agg expression with sums\") {\r\n    val query =\r\n      \"\"\"|SELECT sum(sumEarnings + sumYear), instructor from (\r\n         |   select sum(earnings) as sumEarnings, sum(year) as sumYear, course, instructor FROM courseSalesWider GROUP BY course, instructor\r\n         |) group by instructor\r\n      \"\"\".stripMargin\r\n```\r\n",
        "createdAt" : "2020-01-16T02:20:31Z",
        "updatedAt" : "2020-01-23T23:36:08Z",
        "lastEditedBy" : "88ab38a7-4ab4-463d-ac99-878435e95f58",
        "tags" : [
        ]
      }
    ],
    "commit" : "5773296dc92497ad2ee0014fed0ad4376109e7d0",
    "line" : 113,
    "diffHunk" : "@@ -1,1 +1067,1071 @@                    ex => outerAggExpr.references.exists(_.name == ex.name))\n                  // this rule only handles cases where outer expressions are simple\n                  // and reference a single column from the inner aggregate.\n                  // Possible future enhancement also handle cases like:\n                  // select sum(innerSum1 + innerSum2) from ("
  },
  {
    "id" : "5d6ee0e3-27d3-421f-972d-15ffe889f5cf",
    "prId" : 27224,
    "prUrl" : "https://github.com/apache/spark/pull/27224#pullrequestreview-343639331",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "63ec05e8-92e4-4171-9af0-626ad2a2fd70",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Looks like the cases can be optimized by this rule is limited:\r\n\r\n1. Max, Min, Sum only\r\n2. Only one inner aggregate is referred at outer aggregate\r\n3. Outer uses subset grouping expressions of inner\r\n\r\nIs it common to have the cases this rule can optimize?",
        "createdAt" : "2020-01-16T01:11:00Z",
        "updatedAt" : "2020-01-23T23:36:08Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "c3fd1027-f4ea-4726-8dd2-5a140fcb1fa9",
        "parentId" : "63ec05e8-92e4-4171-9af0-626ad2a2fd70",
        "authorId" : "88ab38a7-4ab4-463d-ac99-878435e95f58",
        "body" : "It does handle the outer aggregate referring to multiple inner aggregates, there are some tests included that show this, but they do need to be referred to by different outer aggregate expressions.\r\n\r\nThe rule is somewhat limited, but these cases come up, especially when it isn't a single person writing the whole query. If you are consuming a view and adding to it, you can be interested in aggregating the same types of columns that the writer of the previous view did, but your view has been rolled up to some granularity, say by region and weekly bucketing, and for your use case you want to just aggregate to straight region level.\r\n\r\nI am happy to try to expand the cases covered by the rule to make it more useful, this is just the set of cases I was able to figure out so far.",
        "createdAt" : "2020-01-16T02:18:02Z",
        "updatedAt" : "2020-01-23T23:36:08Z",
        "lastEditedBy" : "88ab38a7-4ab4-463d-ac99-878435e95f58",
        "tags" : [
        ]
      }
    ],
    "commit" : "5773296dc92497ad2ee0014fed0ad4376109e7d0",
    "line" : 108,
    "diffHunk" : "@@ -1,1 +1062,1066 @@              outerAggExpr.aggregateFunction match {\n                case _: Max | _: Min | _: Sum =>\n                  // look for the expressions in the inner aggregate that produce\n                  // the columns used by the outer aggregate\n                  val resolvedInnerExprs = childAggExprs.filter("
  },
  {
    "id" : "59abb38b-82b5-4a88-8fce-fdadf830756f",
    "prId" : 27077,
    "prUrl" : "https://github.com/apache/spark/pull/27077#pullrequestreview-340316650",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "11d7f62a-9057-4549-a45a-44b66e21f36a",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "How about the case , `global sort => project/filter => local sort`?",
        "createdAt" : "2020-01-08T07:55:52Z",
        "updatedAt" : "2020-01-12T10:12:25Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "51975dae-07b1-43f0-95d2-28334f019b2b",
        "parentId" : "11d7f62a-9057-4549-a45a-44b66e21f36a",
        "authorId" : "b47e5162-4228-4a66-81f1-71f3e0489170",
        "body" : "New UT added for this.",
        "createdAt" : "2020-01-09T06:50:40Z",
        "updatedAt" : "2020-01-12T10:12:25Z",
        "lastEditedBy" : "b47e5162-4228-4a66-81f1-71f3e0489170",
        "tags" : [
        ]
      }
    ],
    "commit" : "92dce7e96d60b667961c9af12e716b83237d8ca8",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +990,994 @@  private def recursiveRemoveSort(parent: Sort, plan: LogicalPlan): LogicalPlan = plan match {\n    case s @ Sort(_, _, child) =>\n      if (parent.global == s.global) {\n        recursiveRemoveSort(s, child)\n      } else {"
  },
  {
    "id" : "f9732e85-32d8-409d-9f37-61ceb36eade1",
    "prId" : 27066,
    "prUrl" : "https://github.com/apache/spark/pull/27066#pullrequestreview-441158706",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bf51795d-503c-430f-ad72-09c528c8e207",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "nit: let's avoid unnecessary change.",
        "createdAt" : "2020-07-01T14:21:15Z",
        "updatedAt" : "2020-07-03T12:43:28Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "5e973b40-f06d-4662-9275-5047d348a700",
        "parentId" : "bf51795d-503c-430f-ad72-09c528c8e207",
        "authorId" : "6fc88871-fca6-485c-a47e-5e17709c7b68",
        "body" : "done. ",
        "createdAt" : "2020-07-01T19:59:29Z",
        "updatedAt" : "2020-07-03T12:43:29Z",
        "lastEditedBy" : "6fc88871-fca6-485c-a47e-5e17709c7b68",
        "tags" : [
        ]
      }
    ],
    "commit" : "4315e9244829cc41ffcf224f29f8fdcbcd88b981",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +206,210 @@    Batch(\"NormalizeFloatingNumbers\", Once, NormalizeFloatingNumbers) :+\n    Batch(\"ReplaceWithFieldsExpression\", Once, ReplaceWithFieldsExpression)\n\n    // remove any batches with no rules. this may happen when subclasses do not add optional rules.\n    batches.filter(_.rules.nonEmpty)"
  },
  {
    "id" : "689115bb-7d07-4a6f-8072-b26623538c54",
    "prId" : 27048,
    "prUrl" : "https://github.com/apache/spark/pull/27048#pullrequestreview-337538553",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "27245980-00eb-4852-ab5c-6012f8a6b850",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "how about limit -> project -> union?",
        "createdAt" : "2020-01-02T05:32:47Z",
        "updatedAt" : "2020-01-02T05:32:47Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "07a4890f-b720-4563-a6b2-e17b8c056e4c",
        "parentId" : "27245980-00eb-4852-ab5c-6012f8a6b850",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "It pushed.  When execute `explain extended select c from test union all select c from test limit 1`, the optimized plan is \r\n```\r\nGlobalLimit 1\r\n+- LocalLimit 1\r\n   +- Union\r\n      :- LocalLimit 1\r\n      :  +- Relation[c#50] parquet\r\n      +- LocalLimit 1\r\n         +- Relation[c#50] parquet\r\n```\r\n\r\nThe difference is another optimizer `PushProjectionThroughUnion` has pushed down `Project`.",
        "createdAt" : "2020-01-02T06:02:59Z",
        "updatedAt" : "2020-01-02T06:03:00Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      }
    ],
    "commit" : "70ea37154f114b3b9028a48d56d331dad24c5633",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +497,501 @@      LocalLimit(exp, newJoin)\n    // Pushdown limit to outer join through projects if projects are deterministic.\n    case l @ LocalLimit(exp, proj @ Project(projs, join @ Join(left, right, joinType, _, _))) =>\n      if (projs.forall(_.deterministic)) {\n        val newJoin = joinType match {"
  },
  {
    "id" : "57a3bde6-d4ed-401b-9e69-6be0522c977e",
    "prId" : 26978,
    "prUrl" : "https://github.com/apache/spark/pull/26978#pullrequestreview-342867536",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ffd13dd4-c843-4b73-8681-f0d831b7aad4",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "@maropu @dongjoon-hyun The cases of Generate (Project + Generate) has its speciality. Added few comment explaining it.",
        "createdAt" : "2020-01-14T01:55:37Z",
        "updatedAt" : "2020-01-24T22:56:29Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "2993b498-9ea5-413a-9e97-c894f3928809",
        "parentId" : "ffd13dd4-c843-4b73-8681-f0d831b7aad4",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "We already have tests for that case? Probably, [the test](https://github.com/apache/spark/pull/26978/files#diff-957112380b0a2ef014abc8227d0b70acR272) fails without this code change?",
        "createdAt" : "2020-01-14T03:47:27Z",
        "updatedAt" : "2020-01-24T22:56:29Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "c3622e66-b986-409a-9297-73b49f871202",
        "parentId" : "ffd13dd4-c843-4b73-8681-f0d831b7aad4",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Above test failures in ColumnPruningSuite are due to this.",
        "createdAt" : "2020-01-14T04:58:15Z",
        "updatedAt" : "2020-01-24T22:56:29Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "eafc1164-4527-417f-9eb0-52a602a2f2b8",
        "parentId" : "ffd13dd4-c843-4b73-8681-f0d831b7aad4",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Ah, I see.",
        "createdAt" : "2020-01-14T05:13:21Z",
        "updatedAt" : "2020-01-24T22:56:29Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "b5bddce7-20af-44b8-b9a2-76e2c83a0023",
        "parentId" : "ffd13dd4-c843-4b73-8681-f0d831b7aad4",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Now I add one end-to-end test case in SchemaPruningSuite too.",
        "createdAt" : "2020-01-14T08:02:08Z",
        "updatedAt" : "2020-01-24T22:56:29Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "15ce7b79-4667-4385-95c8-82d7c34bf4f9",
        "parentId" : "ffd13dd4-c843-4b73-8681-f0d831b7aad4",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Thanks!",
        "createdAt" : "2020-01-14T08:03:53Z",
        "updatedAt" : "2020-01-24T22:56:29Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "fe39a8c8-5429-4dd4-bedb-a4db7c8b5461",
        "parentId" : "ffd13dd4-c843-4b73-8681-f0d831b7aad4",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Got it. Also, thanks you adding more test coverage for nested fields.",
        "createdAt" : "2020-01-14T22:00:05Z",
        "updatedAt" : "2020-01-24T22:56:29Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "35b32ec3eb72629ffcfbc7ab736fe536b0055fd2",
    "line" : 43,
    "diffHunk" : "@@ -1,1 +613,617 @@      // Otherwise by transforming down to Generate, it could be pruned individually,\n      // and causes nested column on top Project unable to resolve.\n      GeneratorNestedColumnAliasing.unapply(currP).getOrElse(currP)\n\n    // prune unrequired nested fields from `Generate`."
  },
  {
    "id" : "a0ae155e-a7bd-44b4-b57e-fb9d6272506a",
    "prId" : 26604,
    "prUrl" : "https://github.com/apache/spark/pull/26604#pullrequestreview-319552340",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3cd5c345-de5d-45ec-be1e-f6edaef073e6",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "This is to optimize local relation so perf doesn't matter too much. The change should be fine.",
        "createdAt" : "2019-11-20T06:08:06Z",
        "updatedAt" : "2019-11-20T07:50:42Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "567294fb-d250-4892-a9a7-bcbf5966dc43",
        "parentId" : "3cd5c345-de5d-45ec-be1e-f6edaef073e6",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "ok",
        "createdAt" : "2019-11-20T06:10:11Z",
        "updatedAt" : "2019-11-20T07:50:42Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "164507ce26ddcf20e9c970cccf7746cc78a6119d",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1508,1512 @@    case Filter(condition, LocalRelation(output, data, isStreaming))\n        if !hasUnevaluableExpr(condition) =>\n      val predicate = Predicate.create(condition, output)\n      predicate.initialize(0)\n      LocalRelation(output, data.filter(row => predicate.eval(row)), isStreaming)"
  },
  {
    "id" : "a0dd4756-5233-496a-aee8-117a9a80372b",
    "prId" : 26534,
    "prUrl" : "https://github.com/apache/spark/pull/26534#pullrequestreview-317720635",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1f3bab75-87a1-4e78-a457-9da030c6d184",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Thank you for adding more description.",
        "createdAt" : "2019-11-15T16:14:09Z",
        "updatedAt" : "2019-11-15T22:17:09Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "ebec3175b726828cf38ce79fd430c9d8957835ca",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +1005,1009 @@      case _: Min | _: Max | _: Count => true\n      // Arithmetic operations for floating-point values are order-sensitive\n      // (they are not associative).\n      case _: Sum | _: Average | _: CentralMomentAgg =>\n        !Seq(FloatType, DoubleType).exists(_.sameType(func.children.head.dataType))"
  },
  {
    "id" : "46cbb8bb-4d05-4011-9809-09b2615edb11",
    "prId" : 26534,
    "prUrl" : "https://github.com/apache/spark/pull/26534#pullrequestreview-317906340",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ff307c95-faf5-4d25-b2c3-b51fad9c83fa",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Also you can write I think `_: Sum | Average | CentralMomentAgg`",
        "createdAt" : "2019-11-15T16:32:37Z",
        "updatedAt" : "2019-11-15T22:17:09Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "3e28a55f-d42d-4411-8989-329d38f7af26",
        "parentId" : "ff307c95-faf5-4d25-b2c3-b51fad9c83fa",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Is that a legal syntax? I tried to change though, I hit a compiler error.",
        "createdAt" : "2019-11-15T22:18:55Z",
        "updatedAt" : "2019-11-15T22:18:55Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "58c8ec6d-9d05-4b45-92d4-ee9173b8165f",
        "parentId" : "ff307c95-faf5-4d25-b2c3-b51fad9c83fa",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Not sure honestly, I tried a similar simpler example locally and it worked but maybe not here. Disregard.",
        "createdAt" : "2019-11-15T22:31:21Z",
        "updatedAt" : "2019-11-15T22:31:21Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "c6641a61-50dc-40ee-91ea-79792c4c9339",
        "parentId" : "ff307c95-faf5-4d25-b2c3-b51fad9c83fa",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "ok, thanks!",
        "createdAt" : "2019-11-15T22:37:01Z",
        "updatedAt" : "2019-11-15T22:37:02Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "ebec3175b726828cf38ce79fd430c9d8957835ca",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +1006,1010 @@      // Arithmetic operations for floating-point values are order-sensitive\n      // (they are not associative).\n      case _: Sum | _: Average | _: CentralMomentAgg =>\n        !Seq(FloatType, DoubleType).exists(_.sameType(func.children.head.dataType))\n      case _ => false"
  },
  {
    "id" : "87f4f0f9-4200-49a8-a2fa-0ce6507f1d84",
    "prId" : 26011,
    "prUrl" : "https://github.com/apache/spark/pull/26011#pullrequestreview-310861142",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d45da67d-6202-4442-ac95-f3f280cac72b",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "nit: private?",
        "createdAt" : "2019-11-04T00:23:51Z",
        "updatedAt" : "2019-11-04T02:09:43Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "e9adc7d2-da4a-4afe-a76f-83a18d4385aa",
        "parentId" : "d45da67d-6202-4442-ac95-f3f280cac72b",
        "authorId" : "31f9c2ba-7775-44e4-b6fa-5e9ab26c8dff",
        "body" : "We cannot make it private because this is a nested function within `private isOrderIrrelevantAggs` ",
        "createdAt" : "2019-11-04T02:15:07Z",
        "updatedAt" : "2019-11-04T02:15:08Z",
        "lastEditedBy" : "31f9c2ba-7775-44e4-b6fa-5e9ab26c8dff",
        "tags" : [
        ]
      },
      {
        "id" : "de5f9d98-2fdd-4977-9cab-cb33b7d24270",
        "parentId" : "d45da67d-6202-4442-ac95-f3f280cac72b",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Ur, this is an inner func. I see.",
        "createdAt" : "2019-11-04T02:18:22Z",
        "updatedAt" : "2019-11-04T02:18:22Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "4eccd2ab965919f91abeedefae574b1f9b3fbaf5",
    "line" : 83,
    "diffHunk" : "@@ -1,1 +1000,1004 @@    }\n\n    def checkValidAggregateExpression(expr: Expression): Boolean = expr match {\n      case _: AttributeReference => true\n      case ae: AggregateExpression => isOrderIrrelevantAggFunction(ae.aggregateFunction)"
  },
  {
    "id" : "651dc461-7f55-49e2-8539-1e26b8cfe41f",
    "prId" : 26011,
    "prUrl" : "https://github.com/apache/spark/pull/26011#pullrequestreview-317621237",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "16a487d9-c1ec-42fa-9045-ac356585d942",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "We could still have a precision difference after eliminating the sort for the floating point data type. I am afraid some end users might prefer to adding a sort in these cases to ensure the results are consistent? ",
        "createdAt" : "2019-11-11T07:25:55Z",
        "updatedAt" : "2019-11-11T07:25:55Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "24a45a80-ee20-48d9-93dc-679adcd5a390",
        "parentId" : "16a487d9-c1ec-42fa-9045-ac356585d942",
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "cc @maryannxue @cloud-fan ",
        "createdAt" : "2019-11-11T07:26:56Z",
        "updatedAt" : "2019-11-11T07:26:57Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "a0b1387c-a306-46ed-ba2c-ad6dfe5b9dd8",
        "parentId" : "16a487d9-c1ec-42fa-9045-ac356585d942",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "ah that's a good point. AVG over floating values is order sensitive. Not sure if this can really affect queries in practice, but better to be conservative here. @WangGuangxin can you fix it in a followup?",
        "createdAt" : "2019-11-11T07:33:05Z",
        "updatedAt" : "2019-11-11T07:33:05Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "4ec98084-3725-47ad-882c-5bbf16e11b27",
        "parentId" : "16a487d9-c1ec-42fa-9045-ac356585d942",
        "authorId" : "31f9c2ba-7775-44e4-b6fa-5e9ab26c8dff",
        "body" : "Sure, I'll fix it in a followup",
        "createdAt" : "2019-11-12T02:09:42Z",
        "updatedAt" : "2019-11-12T02:09:43Z",
        "lastEditedBy" : "31f9c2ba-7775-44e4-b6fa-5e9ab26c8dff",
        "tags" : [
        ]
      },
      {
        "id" : "835a2256-a60e-48dc-ac68-5f2f7d7ae75f",
        "parentId" : "16a487d9-c1ec-42fa-9045-ac356585d942",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "@WangGuangxin do you have no time for the follow-up now? Could I take this over?",
        "createdAt" : "2019-11-14T23:44:25Z",
        "updatedAt" : "2019-11-14T23:44:26Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "391f35fe-a966-4a5a-b86c-a36ce23edeab",
        "parentId" : "16a487d9-c1ec-42fa-9045-ac356585d942",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "https://github.com/apache/spark/pull/26534",
        "createdAt" : "2019-11-15T01:48:15Z",
        "updatedAt" : "2019-11-15T01:48:15Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "f6c8295d-1658-4684-b9b6-196f2fb87a0a",
        "parentId" : "16a487d9-c1ec-42fa-9045-ac356585d942",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Same will be true of all central moments, BTW",
        "createdAt" : "2019-11-15T08:32:52Z",
        "updatedAt" : "2019-11-15T08:32:53Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "44195d26-d2dd-468b-b1d0-b9d3879e063c",
        "parentId" : "16a487d9-c1ec-42fa-9045-ac356585d942",
        "authorId" : "31f9c2ba-7775-44e4-b6fa-5e9ab26c8dff",
        "body" : "I revisit this and have a small question.  In fact `Avg` is transformed to `Sum` and `Count`, so I think there should be no precision problem? ",
        "createdAt" : "2019-11-15T10:32:47Z",
        "updatedAt" : "2019-11-15T10:32:47Z",
        "lastEditedBy" : "31f9c2ba-7775-44e4-b6fa-5e9ab26c8dff",
        "tags" : [
        ]
      },
      {
        "id" : "77bf5ae7-506b-4cda-9c65-27f0a91e4dde",
        "parentId" : "16a487d9-c1ec-42fa-9045-ac356585d942",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "`Sum` is the issue itself, actually; see the followup PR",
        "createdAt" : "2019-11-15T13:39:06Z",
        "updatedAt" : "2019-11-15T13:39:06Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      }
    ],
    "commit" : "4eccd2ab965919f91abeedefae574b1f9b3fbaf5",
    "line" : 78,
    "diffHunk" : "@@ -1,1 +995,999 @@      case _: Max => true\n      case _: Count => true\n      case _: Average => true\n      case _: CentralMomentAgg => true\n      case _ => false"
  },
  {
    "id" : "d5c3beff-eb38-4989-b330-c3f0bda6dfdd",
    "prId" : 25955,
    "prUrl" : "https://github.com/apache/spark/pull/25955#pullrequestreview-296335450",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "188611eb-8975-4759-b566-a5337c7fa2d8",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "can we use the `extendedOperatorOptimizationRules`? It also happens before any rules that depending on stats",
        "createdAt" : "2019-10-02T03:31:43Z",
        "updatedAt" : "2019-10-30T18:21:04Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "6b05bdd7-f57c-4b6c-8d92-58e97e2d8d9b",
        "parentId" : "188611eb-8975-4759-b566-a5337c7fa2d8",
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "No.\r\n\r\nThis needs to be done after filters and projected columns are pushed through operators to colocate them with the scan nodes.",
        "createdAt" : "2019-10-02T15:38:18Z",
        "updatedAt" : "2019-10-30T18:21:04Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      }
    ],
    "commit" : "c223e0537796c1e0a52bc5a655660f89ab539192",
    "line" : 37,
    "diffHunk" : "@@ -1,1 +264,268 @@   * Override to provide additional rules for early projection and filter pushdown to scans.\n   */\n  def earlyScanPushDownRules: Seq[Rule[LogicalPlan]] = Nil\n\n  /**"
  },
  {
    "id" : "b4d965b0-3970-4068-bdcc-b2d4cd202b85",
    "prId" : 25955,
    "prUrl" : "https://github.com/apache/spark/pull/25955#pullrequestreview-298992867",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "38d57af9-0aa7-4a59-9215-6fc3c706cf55",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I get it that we want to keep the diff small so we leave the indentation wrong here, but we can avoid this problem completely.\r\n\r\nAn empty batch has nearly 0 overhead. The `RuleExecutor` just runs an empty loop and goes to the next batch. The join-reorder optimization can be turned off and we still keep its batch.",
        "createdAt" : "2019-10-07T06:42:55Z",
        "updatedAt" : "2019-10-30T18:21:04Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "702b87e7-b64d-4a43-a8e4-766db25e32e6",
        "parentId" : "38d57af9-0aa7-4a59-9215-6fc3c706cf55",
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "Keeping the empty batch causes test failures because tests assume that it will be filtered out. It's better to filter it out here.",
        "createdAt" : "2019-10-08T19:21:58Z",
        "updatedAt" : "2019-10-30T18:21:04Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      }
    ],
    "commit" : "c223e0537796c1e0a52bc5a655660f89ab539192",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +120,124 @@    }\n\n    val batches = (Batch(\"Eliminate Distinct\", Once, EliminateDistinct) ::\n    // Technically some of the rules in Finish Analysis are not optimizer rules and belong more\n    // in the analyzer, because they are needed for correctness (e.g. ComputeCurrentTime)."
  },
  {
    "id" : "833265ac-586e-4083-8d46-feceaf33433e",
    "prId" : 25600,
    "prUrl" : "https://github.com/apache/spark/pull/25600#pullrequestreview-280605574",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4f4b386d-4ef0-4b02-81b8-6dbbc16001f9",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Yup, +1 for now.",
        "createdAt" : "2019-08-28T05:02:29Z",
        "updatedAt" : "2019-09-04T14:47:15Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "ecf9e2c1-f630-454f-b504-73c9a6f18238",
        "parentId" : "4f4b386d-4ef0-4b02-81b8-6dbbc16001f9",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Copied and pasted from the offline discussion with @maryannxue:\r\n\r\nSeems the problem is that, `ExtractPythonUDFFromAggregate` seems not idempotent.\r\nso in `OptimizeSubqueries`, when it optimizes this plan at here https://github.com/apache/spark/blob/bab88c48b1432249571aae90bc56b40d74f2fa88/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala#L240\r\n\r\nHere, `s.plan` is as below:\r\n\r\n```\r\nSubquery\r\n+- Project [pyUDF(agg#32) AS CAST(pyUDF(cast(min(k) as string)) AS STRING)#27]\r\n   +- Aggregate [min(k#18) AS agg#32]\r\n      +- Project [_1#13 AS k#18]\r\n         +- LocalRelation [_1#13, _2#14]\r\n```\r\n\r\nAfter `CollapseProject`:\r\n\r\n```\r\nSubquery\r\n+- Aggregate [pyUDF(min(k#18)) AS CAST(pyUDF(cast(min(k) as string)) AS STRING)#27]\r\n   +- Project [_1#13 AS k#18]\r\n      +- LocalRelation [_1#13, _2#14]\r\n```\r\n\r\nAfter `ExtractPythonUDFFromAggregate`:\r\n\r\n```\r\nSubquery\r\n+- Project [pyUDF(agg#33) AS CAST(pyUDF(cast(min(k) as string)) AS STRING)#27]\r\n   +- Aggregate [min(k#18) AS agg#33]\r\n      +- Project [_1#13 AS k#18]\r\n         +- LocalRelation [_1#13, _2#14]\r\n```\r\nseems the new alias is made at https://github.com/apache/spark/blob/77c7e91e029a9a70678435acb141154f2f51882e/sql/core/src/main/scala/org/apache/spark/sql/execution/python/ExtractPythonUDFs.scala#L63\r\n\r\nSo adding it to blacklisting makes sense to me for now.",
        "createdAt" : "2019-08-28T06:05:57Z",
        "updatedAt" : "2019-09-04T14:47:15Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "b00225078c6471b5b01f8a920ab28a2361b8e48b",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +50,54 @@  override protected val blacklistedOnceBatches: Set[String] =\n    Set(\n      \"PartitionPruning\",\n      \"Extract Python UDFs\")\n"
  },
  {
    "id" : "5044a9f0-f32c-4e9d-928e-98633a2ce8aa",
    "prId" : 25258,
    "prUrl" : "https://github.com/apache/spark/pull/25258#pullrequestreview-267106439",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3f9ddafb-fd04-411f-98f1-984ff57ad681",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "why do we need to separate the column pruning rule?",
        "createdAt" : "2019-07-26T08:31:51Z",
        "updatedAt" : "2019-07-29T20:50:52Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "f3a69c64-eaa7-4568-a84f-44f443afba6c",
        "parentId" : "3f9ddafb-fd04-411f-98f1-984ff57ad681",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "there is a subquery related bug fix: https://github.com/apache/spark/pull/25204\r\n\r\nIs it related to your change as well?",
        "createdAt" : "2019-07-26T08:33:52Z",
        "updatedAt" : "2019-07-29T20:50:52Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "dd84725a-586c-4019-9c0a-51345a53afc4",
        "parentId" : "3f9ddafb-fd04-411f-98f1-984ff57ad681",
        "authorId" : "a2fcc15e-a51a-42f0-87a6-137048a28e30",
        "body" : "@cloud-fan I took a very quick look. It does not seem related to this PR.",
        "createdAt" : "2019-07-26T08:36:25Z",
        "updatedAt" : "2019-07-29T20:50:52Z",
        "lastEditedBy" : "a2fcc15e-a51a-42f0-87a6-137048a28e30",
        "tags" : [
        ]
      },
      {
        "id" : "9c824cf9-44e9-4917-9261-ba6abcf45d5d",
        "parentId" : "3f9ddafb-fd04-411f-98f1-984ff57ad681",
        "authorId" : "a2fcc15e-a51a-42f0-87a6-137048a28e30",
        "body" : "@cloud-fan \r\n> why do we need to separate the column pruning rule?\r\n\r\nPerhaps there is a better way to do this. But here is the problem. Please take a look at `RewriteSubquerySuite: Column pruning after rewriting predicate subquery`. This test case is expecting that we perform column pruning to filter out un-needed columns before the join. Here is the input plan  : \r\n```\r\nProject [a#0]                    \r\n +- Join LeftSemi, (a#0 = x#2)    \r\n   +-LocalRelation [a#0, b#1]       \r\n   +- LocalRelation [x#2]\r\n```\r\nDue to the presence Project on top of LeftSemi, the regular ColumnPruning rule is not able to add the Project on top of the left child of LeftSemiJoin. This is done to avoid the cycle between ColumnPruning and PushPredicateThroughProject. Thats why i created this FinalColumnPruning rule that does not have the logic to remove the project.",
        "createdAt" : "2019-07-26T09:21:11Z",
        "updatedAt" : "2019-07-29T20:50:52Z",
        "lastEditedBy" : "a2fcc15e-a51a-42f0-87a6-137048a28e30",
        "tags" : [
        ]
      },
      {
        "id" : "c51b0289-725f-4776-a6c6-bc0a00737d5c",
        "parentId" : "3f9ddafb-fd04-411f-98f1-984ff57ad681",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "> This is done to avoid the cycle between ColumnPruning and PushPredicateThroughProject\r\n\r\nI don't see a filter in the input plan, how is it related to `PushPredicateThroughProject`?",
        "createdAt" : "2019-07-26T09:47:24Z",
        "updatedAt" : "2019-07-29T20:50:52Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "e7bcd0a9-5158-4ebe-a961-c77ae3bfcd0b",
        "parentId" : "3f9ddafb-fd04-411f-98f1-984ff57ad681",
        "authorId" : "a2fcc15e-a51a-42f0-87a6-137048a28e30",
        "body" : "@cloud-fan so the LeftSemi/Anti pattern is treated like a Filter in modified ColumnPruning rule. Since we convert the subqueries (which was in Filter form) to join early now, we are basically treating it like a Filter in related rules.\r\n\r\n>  how is it related to PushPredicateThroughProject\r\n\r\nSorry.. just to clarify, In the prior PR, we have added `PushDownLeftSemiAntiJoin` where a LeftSemi/Anti join is pushed down below Project. So the Cycle would be between ColumnPruning and PushDownLeftSemiAntiJoin.",
        "createdAt" : "2019-07-26T09:58:38Z",
        "updatedAt" : "2019-07-29T20:50:52Z",
        "lastEditedBy" : "a2fcc15e-a51a-42f0-87a6-137048a28e30",
        "tags" : [
        ]
      },
      {
        "id" : "e6423087-b05e-4894-b96e-7be80b8f1e67",
        "parentId" : "3f9ddafb-fd04-411f-98f1-984ff57ad681",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "ah i see, thanks for explaination!",
        "createdAt" : "2019-07-26T10:15:26Z",
        "updatedAt" : "2019-07-29T20:50:52Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "24be0574ddb74b27c952c1e3f5971b01e487f360",
    "line" : 67,
    "diffHunk" : "@@ -1,1 +584,588 @@ * Attempts to eliminate the reading of unneeded columns from the query plan.\n */\nobject FinalColumnPruning extends Rule[LogicalPlan] {\n  def apply(plan: LogicalPlan): LogicalPlan = plan transform {\n    // Prunes the unused columns from project list of Project/Aggregate/Expand"
  },
  {
    "id" : "ef499652-19da-44fe-a06b-17c4e8f5626a",
    "prId" : 25258,
    "prUrl" : "https://github.com/apache/spark/pull/25258#pullrequestreview-270422361",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "81e02e65-a019-4752-84b3-8d35e3a9dcce",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "Will it affect CBO?",
        "createdAt" : "2019-08-02T18:13:29Z",
        "updatedAt" : "2019-08-02T18:13:29Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "ab2ec9a2-0574-4c71-b23d-1cdccbe70045",
        "parentId" : "81e02e65-a019-4752-84b3-8d35e3a9dcce",
        "authorId" : "a2fcc15e-a51a-42f0-87a6-137048a28e30",
        "body" : "@gatorsmile Could you please elaborate ? My hope is that we can get better optimized plans as we will expose the full plan to entire set of optimizations ? As an example, lets say we have a rule that can convert leftsemi join to inner join, with this, we can hope to have this conversion and if we do, then CBO will be able to impact positively to re-order joins as required. Please let me know if i am missing something..",
        "createdAt" : "2019-08-02T19:27:59Z",
        "updatedAt" : "2019-08-02T19:27:59Z",
        "lastEditedBy" : "a2fcc15e-a51a-42f0-87a6-137048a28e30",
        "tags" : [
        ]
      },
      {
        "id" : "c28756be-eeab-49cc-9a73-d10c3b68730a",
        "parentId" : "81e02e65-a019-4752-84b3-8d35e3a9dcce",
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "Our CBO is based on the pattern matching. We only support InnerLike joins. \r\n\r\nIs that possible the pattern matched before but it does not matched after we convert subquery to join?  RewritePredicateSubquery adds LeftSemi or LeftAnti joins into the original plan. ",
        "createdAt" : "2019-08-02T22:10:59Z",
        "updatedAt" : "2019-08-02T22:10:59Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "d18b39df-cbab-4525-81a0-ea7bec1237ee",
        "parentId" : "81e02e65-a019-4752-84b3-8d35e3a9dcce",
        "authorId" : "a2fcc15e-a51a-42f0-87a6-137048a28e30",
        "body" : "@gatorsmile At the moment, i am unable to think of a case when by moving the rewrite up would impact the pattern matching in CBO in a negative way since `RewritePredicateSubquery` needs to happen any way (it just happens later) . The leftsemi/anti join should end up being in the same position in the plan tree as we have the same pushdown rules for leftsemi and leftanti as there are for filters. If you have a case in mind, i can give it a quick try ?",
        "createdAt" : "2019-08-02T22:44:46Z",
        "updatedAt" : "2019-08-02T22:44:46Z",
        "lastEditedBy" : "a2fcc15e-a51a-42f0-87a6-137048a28e30",
        "tags" : [
        ]
      }
    ],
    "commit" : "24be0574ddb74b27c952c1e3f5971b01e487f360",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +156,160 @@      OptimizeSubqueries,\n      PullupCorrelatedPredicates,\n      RewritePredicateSubquery) ::\n    Batch(\"Replace Operators\", fixedPoint,\n      RewriteExceptAll,"
  },
  {
    "id" : "d92c61ae-f120-44f5-89a2-adcc528a727a",
    "prId" : 25249,
    "prUrl" : "https://github.com/apache/spark/pull/25249#pullrequestreview-267315600",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d1f96e54-b6dc-4349-aac0-0adfee241021",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "Please create an umbrella JIRA and create a separate JIRA for each rule?",
        "createdAt" : "2019-07-26T06:43:15Z",
        "updatedAt" : "2019-07-26T06:43:15Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "d83a1836-f6aa-4928-80f8-1205aecd9ae8",
        "parentId" : "d1f96e54-b6dc-4349-aac0-0adfee241021",
        "authorId" : "88f0c774-9c59-4485-885d-f6aee36efcea",
        "body" : "Sure. https://issues.apache.org/jira/browse/SPARK-28528?filter=-2",
        "createdAt" : "2019-07-26T17:35:54Z",
        "updatedAt" : "2019-07-26T18:01:11Z",
        "lastEditedBy" : "88f0c774-9c59-4485-885d-f6aee36efcea",
        "tags" : [
        ]
      }
    ],
    "commit" : "02e17006654b2e010b6867a7b20a583b9b0e30ed",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +52,56 @@      \"Join Reorder\",\n      \"Subquery\",\n      \"Extract Python UDFs\"\n    )\n"
  },
  {
    "id" : "22013e26-6d6d-414e-8040-f1a976df5030",
    "prId" : 25204,
    "prUrl" : "https://github.com/apache/spark/pull/25204#pullrequestreview-266534133",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b33a2e09-0ef1-442b-9967-5082490fd069",
        "parentId" : null,
        "authorId" : "24e1dd39-ae3f-4bbb-a391-f60afb62d075",
        "body" : "I am not sure about this change. This may cause serious perf regression",
        "createdAt" : "2019-07-25T08:50:22Z",
        "updatedAt" : "2019-07-26T13:25:29Z",
        "lastEditedBy" : "24e1dd39-ae3f-4bbb-a391-f60afb62d075",
        "tags" : [
        ]
      },
      {
        "id" : "9f453d81-0752-4721-a456-ff7ee03b3e4f",
        "parentId" : "b33a2e09-0ef1-442b-9967-5082490fd069",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "How can we remove project that's not attribute-only?",
        "createdAt" : "2019-07-25T08:55:24Z",
        "updatedAt" : "2019-07-26T13:25:29Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "af17013f-2cbf-4c53-93ef-6f3f84d05265",
        "parentId" : "b33a2e09-0ef1-442b-9967-5082490fd069",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I'd say it was wrong previously, but if a project's output has same expr IDs with its child, it's usually attribute-only.",
        "createdAt" : "2019-07-25T08:56:38Z",
        "updatedAt" : "2019-07-26T13:25:29Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "eedfd870-cfc3-4bab-a58a-a50c106c29ba",
        "parentId" : "b33a2e09-0ef1-442b-9967-5082490fd069",
        "authorId" : "24e1dd39-ae3f-4bbb-a391-f60afb62d075",
        "body" : "Mmmmh... I may be missing something, but I'd imagine a case like this:\r\n```\r\nselect a, b from\r\n(select a, b, very_expensive_operation as c from ... where a = 1)\r\n```\r\nBefore this change, would be optimized as:\r\n```\r\nselect a, b from\r\n(select a, b from ... where a = 1)\r\n```\r\nwhile after it is not. Am I wrong?",
        "createdAt" : "2019-07-25T09:28:29Z",
        "updatedAt" : "2019-07-26T13:25:29Z",
        "lastEditedBy" : "24e1dd39-ae3f-4bbb-a391-f60afb62d075",
        "tags" : [
        ]
      },
      {
        "id" : "2375a88e-db6b-4565-9169-5a7711700ec2",
        "parentId" : "b33a2e09-0ef1-442b-9967-5082490fd069",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "In above case, it has a Alias in project list, so it's not an attribute-only project. And I think it also create new attr c, so `p2.outputSet.subsetOf(child.outputSet)` is not met too.\r\n\r\nI think the rules in `ColumnPruning` will trim `very_expensive_operation` in the end.",
        "createdAt" : "2019-07-25T09:59:18Z",
        "updatedAt" : "2019-07-26T13:25:29Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "83636ed6-e958-484c-b4c4-4e4f51c5154b",
        "parentId" : "b33a2e09-0ef1-442b-9967-5082490fd069",
        "authorId" : "24e1dd39-ae3f-4bbb-a391-f60afb62d075",
        "body" : "I see now, sorry. Why do we need this? Seems an unrelated change to the fix in this PR, isn't it?",
        "createdAt" : "2019-07-25T10:10:29Z",
        "updatedAt" : "2019-07-26T13:25:29Z",
        "lastEditedBy" : "24e1dd39-ae3f-4bbb-a391-f60afb62d075",
        "tags" : [
        ]
      },
      {
        "id" : "6c95763d-612a-400e-954a-fb8eabce2b75",
        "parentId" : "b33a2e09-0ef1-442b-9967-5082490fd069",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "oh, the issue was seen in previous comment https://github.com/apache/spark/pull/25204/commits/33441a32412eacbe84f2b188f046b2959a8b60ee. It was overwritten now.\r\n\r\nWe added a column for count bug. The column checks a always-true leading column `alwaysTrueExpr`, returns special value if `alwaysTrueExpr` is null, to simulate empty input case.\r\n\r\nThis column reuses expr id of original output in the subquery. In non-foldable expression case, the added column in a potential Project-Filter-Project, will be trimmed by `removeProjectBeforeFilter`, because the second project meets `p2.outputSet.subsetOf(child.outputSet)`.\r\n\r\nMy original fix is to create an expr id. Replace original expr id with new one in the subquery. Looks complicated. This seems a simple fix, and looks reasonable.\r\n\r\n",
        "createdAt" : "2019-07-25T10:22:12Z",
        "updatedAt" : "2019-07-26T13:25:29Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "fd296776b826f6ce041a52cdc4dce4e278d3d786",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +653,657 @@      if p2.outputSet.subsetOf(child.outputSet) &&\n        // We only remove attribute-only project.\n        p2.projectList.forall(_.isInstanceOf[AttributeReference]) =>\n      p1.copy(child = f.copy(child = child))\n  }"
  },
  {
    "id" : "530510d4-c75b-47d5-9922-fca1e57f45cc",
    "prId" : 25020,
    "prUrl" : "https://github.com/apache/spark/pull/25020#pullrequestreview-263989120",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "547bf148-e1b9-45f7-8b05-1aa6ab956da4",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Can we please avoid such hacky and widely affected change to fix a corner case?",
        "createdAt" : "2019-07-19T02:28:37Z",
        "updatedAt" : "2019-07-19T02:29:16Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "1c03495f83770aa9a0c6aaa6ce9f768cc91a939e",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +1245,1249 @@            // totally pushed down being skipped during rule transform down. Also these useless\n            // filter will be pruned in rule `PruneFilters` later.\n            Filter(Literal(true), join)\n          }\n        case RightOuter =>"
  }
]