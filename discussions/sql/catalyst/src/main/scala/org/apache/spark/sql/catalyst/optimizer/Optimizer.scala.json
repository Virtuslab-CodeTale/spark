[
  {
    "id" : "cd79d402-0e8b-4076-8d9c-4d93277dd5ea",
    "prId" : 33651,
    "prUrl" : "https://github.com/apache/spark/pull/33651#pullrequestreview-723483793",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cad47f42-f12c-4eb6-9c1f-655b555e308f",
        "parentId" : null,
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "It's flaky that this expression highly depend on `ConstantFolding`.",
        "createdAt" : "2021-08-05T15:13:46Z",
        "updatedAt" : "2021-08-05T15:13:46Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      }
    ],
    "commit" : "6f28a37f420f781486c3e8c102156e02d9f830d7",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +1677,1681 @@\n    case GlobalLimit(le, GlobalLimit(ne, grandChild)) =>\n      GlobalLimit(Literal(Least(Seq(ne, le)).eval().asInstanceOf[Int]), grandChild)\n    case LocalLimit(le, LocalLimit(ne, grandChild)) =>\n      LocalLimit(Literal(Least(Seq(ne, le)).eval().asInstanceOf[Int]), grandChild)"
  },
  {
    "id" : "e658d204-1883-44dc-8966-77b53bdc53ca",
    "prId" : 33587,
    "prUrl" : "https://github.com/apache/spark/pull/33587#pullrequestreview-723105990",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3b87979e-e583-48be-aaf5-b598ce034d3b",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "let's add the default case to be safe:\r\n```\r\ncase (other, _) => other\r\n```",
        "createdAt" : "2021-08-05T08:42:18Z",
        "updatedAt" : "2021-08-05T08:42:19Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "cfac5e7d-e36e-4cde-9fa3-9e9a9ead9751",
        "parentId" : "3b87979e-e583-48be-aaf5-b598ce034d3b",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "Okï¼Œ done",
        "createdAt" : "2021-08-05T08:57:35Z",
        "updatedAt" : "2021-08-05T08:57:35Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "1f93b6f14001d5a9e71cc9af207e12217f36cd5d",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +521,525 @@    projectList.zip(originalNames).map {\n      case (attr: Attribute, name) => attr.withName(name)\n      case (alias: Alias, name) => alias.withName(name)\n      case (other, _) => other\n    }"
  },
  {
    "id" : "258971e3-e28d-4638-8c85-97af7394a661",
    "prId" : 33583,
    "prUrl" : "https://github.com/apache/spark/pull/33583#pullrequestreview-724892088",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f1177a7f-bbca-4e79-8332-93e208610eec",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Could you also update the comment?",
        "createdAt" : "2021-08-08T07:03:12Z",
        "updatedAt" : "2021-08-08T07:03:12Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "a334e694-ed28-469d-ad6b-d2096ee11494",
        "parentId" : "f1177a7f-bbca-4e79-8332-93e208610eec",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "Done",
        "createdAt" : "2021-08-08T11:32:02Z",
        "updatedAt" : "2021-08-08T11:32:02Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "44c744988b9e219f34a96a3a5835aff150607d42",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +47,51 @@  // - is still resolved\n  // - only host special expressions in supported operators\n  // - has globally-unique attribute IDs\n  // - optimized plan have same schema with previous plan.\n  override protected def isPlanIntegral("
  },
  {
    "id" : "42db3364-50fe-4fb7-b770-f58b74bd4ee9",
    "prId" : 33100,
    "prUrl" : "https://github.com/apache/spark/pull/33100#pullrequestreview-693527498",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c9c3b4e9-1c57-4fa6-871f-92d8570dbb41",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "The change here looks good. I wonder if we should better create a separate file to group similar stuff though .... e.g.) we can remove deduplicate too.",
        "createdAt" : "2021-06-28T03:28:55Z",
        "updatedAt" : "2021-06-28T03:28:56Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "82b03f052441b8830100b4202e6037a283aef18f",
    "line" : 25,
    "diffHunk" : "@@ -1,1 +1220,1224 @@\n  private val applyLocally: PartialFunction[LogicalPlan, LogicalPlan] = {\n    case Sort(_, _, child) if child.maxRows.exists(_ <= 1L) => child\n    case s @ Sort(orders, _, child) if orders.isEmpty || orders.exists(_.child.foldable) =>\n      val newOrders = orders.filterNot(_.child.foldable)"
  },
  {
    "id" : "62bcb8d2-b9ac-4128-b95a-e2ac4e78d469",
    "prId" : 33099,
    "prUrl" : "https://github.com/apache/spark/pull/33099#pullrequestreview-694148158",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d508edda-5fbf-40ef-810c-79402e663dab",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "No, we can't do this. `RebalancePartitions` is a best-effort and does not guarantee partitioning. We intentionally do not let `RebalancePartitions` extend `RepartitionOperation`, to avoid wrong optimizations.\r\n\r\nPlease revert this.",
        "createdAt" : "2021-06-28T05:24:28Z",
        "updatedAt" : "2021-06-28T05:24:38Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "ab5420ff-0ef9-4190-a76e-8e0a43360d94",
        "parentId" : "d508edda-5fbf-40ef-810c-79402e663dab",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "See https://github.com/apache/spark/pull/32932#discussion_r656249153",
        "createdAt" : "2021-06-28T05:25:32Z",
        "updatedAt" : "2021-06-28T05:25:33Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "25b01e2a-686b-4af7-ac5c-ee467ef15c49",
        "parentId" : "d508edda-5fbf-40ef-810c-79402e663dab",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "OK",
        "createdAt" : "2021-06-28T08:18:00Z",
        "updatedAt" : "2021-06-28T08:18:00Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      },
      {
        "id" : "6abd5891-125d-4c26-bfa0-e0f84cbded16",
        "parentId" : "d508edda-5fbf-40ef-810c-79402e663dab",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "Reverted at: https://github.com/apache/spark/commit/108635af1708173a72bec0e36bf3f2cea5b088c4",
        "createdAt" : "2021-06-28T08:43:03Z",
        "updatedAt" : "2021-06-28T08:43:03Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      },
      {
        "id" : "ddebaa85-5310-41de-9ad7-81b0e426928a",
        "parentId" : "d508edda-5fbf-40ef-810c-79402e663dab",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Oh, got it. Thank you for reverting, @cloud-fan and @wangyum .",
        "createdAt" : "2021-06-28T16:16:50Z",
        "updatedAt" : "2021-06-28T16:16:51Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "67642d87f7d1799e97f925444484f52354e6af72",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +914,918 @@\n    // Case 3: When a RebalancePartitions has a child of Repartition or RepartitionByExpression\n    // we can remove the child.\n    case r @ RebalancePartitions(_, child: RepartitionOperation) =>\n      r.copy(child = child.child)"
  },
  {
    "id" : "8c82774d-b56c-468f-b4d6-c0a705557303",
    "prId" : 32439,
    "prUrl" : "https://github.com/apache/spark/pull/32439#pullrequestreview-656933962",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "00cb0166-5c63-4606-b4cd-6455dc59448e",
        "parentId" : null,
        "authorId" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "body" : "I might have missed sth. from the previous commit, but what difference is there between regular `transform` and `transformWithPruning(AlwaysProcess.fn, ...)` ?",
        "createdAt" : "2021-05-11T03:50:17Z",
        "updatedAt" : "2021-05-11T06:53:50Z",
        "lastEditedBy" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "tags" : [
        ]
      },
      {
        "id" : "ef9935e5-c3f6-46e6-92b3-f0b96a3c55da",
        "parentId" : "00cb0166-5c63-4606-b4cd-6455dc59448e",
        "authorId" : "8b518862-583d-43a2-a10e-b33ef1b6d824",
        "body" : "`transform` internally calls\r\n`transformWithPruning(AlwaysProcess.fn, UnknownRuleId)` ...\r\n\r\nThe argument comments are here:\r\nhttps://github.com/apache/spark/blob/e08c40fa3f7054bdc713873c99d3aaf0014d9314/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/trees/TreeNode.scala#L434-L441\r\n\r\n So here, \r\n `plan.transformWithPruning(AlwaysProcess.fn, ruleId)` means there's no pruning based on TreePattern bits, but there's pruning based on ruleIds (if the rule is known to ineffective on a tree instance `T`,  it will be skipped next time when it is invoked on the same tree instance `T`).",
        "createdAt" : "2021-05-11T03:54:56Z",
        "updatedAt" : "2021-05-11T06:53:50Z",
        "lastEditedBy" : "8b518862-583d-43a2-a10e-b33ef1b6d824",
        "tags" : [
        ]
      },
      {
        "id" : "f04922ab-afc7-46da-bcae-7b82d3b9e3da",
        "parentId" : "00cb0166-5c63-4606-b4cd-6455dc59448e",
        "authorId" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "body" : "I see. Thanks!",
        "createdAt" : "2021-05-11T16:05:29Z",
        "updatedAt" : "2021-05-11T16:05:29Z",
        "lastEditedBy" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "tags" : [
        ]
      }
    ],
    "commit" : "5027ebcec31e9ea21df0bf16daa3e3427d4fe700",
    "line" : 76,
    "diffHunk" : "@@ -1,1 +756,760 @@\n  def apply(plan: LogicalPlan): LogicalPlan = removeProjectBeforeFilter(\n    plan.transformWithPruning(AlwaysProcess.fn, ruleId) {\n    // Prunes the unused columns from project list of Project/Aggregate/Expand\n    case p @ Project(_, p2: Project) if !p2.outputSet.subsetOf(p.references) =>"
  },
  {
    "id" : "422b02ff-1db1-4e86-9116-78455a60e690",
    "prId" : 31980,
    "prUrl" : "https://github.com/apache/spark/pull/31980#pullrequestreview-622655510",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6b1f75ba-4045-46b8-b4c9-6c65761ea30a",
        "parentId" : null,
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "`w1.references.subsetOf(grandChild.outputSet)` Do we need this check?",
        "createdAt" : "2021-03-27T14:25:45Z",
        "updatedAt" : "2021-04-02T07:14:22Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      },
      {
        "id" : "d230a834-b878-4aeb-bfa1-fbb011ba6801",
        "parentId" : "6b1f75ba-4045-46b8-b4c9-6c65761ea30a",
        "authorId" : "a8e23d47-3ae4-4385-848c-38a216d1bd08",
        "body" : "The test case `SPARK-34807: don't transpose two windows if project between them generates an input column` would fail without it.",
        "createdAt" : "2021-03-27T14:46:32Z",
        "updatedAt" : "2021-04-02T07:14:22Z",
        "lastEditedBy" : "a8e23d47-3ae4-4385-848c-38a216d1bd08",
        "tags" : [
        ]
      }
    ],
    "commit" : "60c6643f0bfda390baa32273f7ca1371a44d5771",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +1002,1006 @@\n    case w1 @ Window(_, _, _, Project(pl, w2 @ Window(_, _, _, grandChild)))\n      if windowsCompatible(w1, w2) && w1.references.subsetOf(grandChild.outputSet) =>\n      Project(\n        pl ++ w1.windowOutputSet,"
  },
  {
    "id" : "6f3247d1-b155-4ce3-9ca4-395595639158",
    "prId" : 31980,
    "prUrl" : "https://github.com/apache/spark/pull/31980#pullrequestreview-622660526",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ef9dad48-b6c1-4903-9d3d-2cf8831ee500",
        "parentId" : null,
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "Can we use `w1.output` instead of `pl ++ w1.windowOutputSet`?",
        "createdAt" : "2021-03-27T15:45:52Z",
        "updatedAt" : "2021-04-02T07:14:22Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      },
      {
        "id" : "7842ad25-1318-49f5-aed0-a0badc0e5c17",
        "parentId" : "ef9dad48-b6c1-4903-9d3d-2cf8831ee500",
        "authorId" : "a8e23d47-3ae4-4385-848c-38a216d1bd08",
        "body" : "The `w1.output` contains only the attributes (name + id), but not how the attributes are computed based on the input attributes. This information is in the project list.",
        "createdAt" : "2021-03-27T15:56:17Z",
        "updatedAt" : "2021-04-02T07:14:22Z",
        "lastEditedBy" : "a8e23d47-3ae4-4385-848c-38a216d1bd08",
        "tags" : [
        ]
      }
    ],
    "commit" : "60c6643f0bfda390baa32273f7ca1371a44d5771",
    "line" : 26,
    "diffHunk" : "@@ -1,1 +1004,1008 @@      if windowsCompatible(w1, w2) && w1.references.subsetOf(grandChild.outputSet) =>\n      Project(\n        pl ++ w1.windowOutputSet,\n        w2.copy(child = w1.copy(child = grandChild)))\n  }"
  },
  {
    "id" : "3a03e160-a9e7-4f0b-8b1b-f974e3b419e6",
    "prId" : 31980,
    "prUrl" : "https://github.com/apache/spark/pull/31980#pullrequestreview-622747073",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1f78dce7-f0c6-42be-aa4c-3f4cdff68d4a",
        "parentId" : null,
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "Two more spaces?",
        "createdAt" : "2021-03-28T13:05:16Z",
        "updatedAt" : "2021-04-02T07:14:22Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      }
    ],
    "commit" : "60c6643f0bfda390baa32273f7ca1371a44d5771",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +998,1002 @@    _.containsPattern(WINDOW), ruleId) {\n    case w1 @ Window(_, _, _, w2 @ Window(_, _, _, grandChild))\n      if windowsCompatible(w1, w2) =>\n      Project(w1.output, w2.copy(child = w1.copy(child = grandChild)))\n"
  },
  {
    "id" : "eecc4e6b-7aa5-45dd-a1ec-42634e192d00",
    "prId" : 31980,
    "prUrl" : "https://github.com/apache/spark/pull/31980#pullrequestreview-622747082",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c4abdcde-d297-43c5-9466-d9c10013cf12",
        "parentId" : null,
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "Two more spaces?",
        "createdAt" : "2021-03-28T13:05:21Z",
        "updatedAt" : "2021-04-02T07:14:22Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      }
    ],
    "commit" : "60c6643f0bfda390baa32273f7ca1371a44d5771",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +1002,1006 @@\n    case w1 @ Window(_, _, _, Project(pl, w2 @ Window(_, _, _, grandChild)))\n      if windowsCompatible(w1, w2) && w1.references.subsetOf(grandChild.outputSet) =>\n      Project(\n        pl ++ w1.windowOutputSet,"
  }
]