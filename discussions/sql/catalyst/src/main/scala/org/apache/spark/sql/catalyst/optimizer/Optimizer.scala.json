[
  {
    "id" : "cd79d402-0e8b-4076-8d9c-4d93277dd5ea",
    "prId" : 33651,
    "prUrl" : "https://github.com/apache/spark/pull/33651#pullrequestreview-723483793",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cad47f42-f12c-4eb6-9c1f-655b555e308f",
        "parentId" : null,
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "It's flaky that this expression highly depend on `ConstantFolding`.",
        "createdAt" : "2021-08-05T15:13:46Z",
        "updatedAt" : "2021-08-05T15:13:46Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      }
    ],
    "commit" : "6f28a37f420f781486c3e8c102156e02d9f830d7",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +1677,1681 @@\n    case GlobalLimit(le, GlobalLimit(ne, grandChild)) =>\n      GlobalLimit(Literal(Least(Seq(ne, le)).eval().asInstanceOf[Int]), grandChild)\n    case LocalLimit(le, LocalLimit(ne, grandChild)) =>\n      LocalLimit(Literal(Least(Seq(ne, le)).eval().asInstanceOf[Int]), grandChild)"
  },
  {
    "id" : "e658d204-1883-44dc-8966-77b53bdc53ca",
    "prId" : 33587,
    "prUrl" : "https://github.com/apache/spark/pull/33587#pullrequestreview-723105990",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3b87979e-e583-48be-aaf5-b598ce034d3b",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "let's add the default case to be safe:\r\n```\r\ncase (other, _) => other\r\n```",
        "createdAt" : "2021-08-05T08:42:18Z",
        "updatedAt" : "2021-08-05T08:42:19Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "cfac5e7d-e36e-4cde-9fa3-9e9a9ead9751",
        "parentId" : "3b87979e-e583-48be-aaf5-b598ce034d3b",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "Okï¼Œ done",
        "createdAt" : "2021-08-05T08:57:35Z",
        "updatedAt" : "2021-08-05T08:57:35Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "1f93b6f14001d5a9e71cc9af207e12217f36cd5d",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +521,525 @@    projectList.zip(originalNames).map {\n      case (attr: Attribute, name) => attr.withName(name)\n      case (alias: Alias, name) => alias.withName(name)\n      case (other, _) => other\n    }"
  },
  {
    "id" : "258971e3-e28d-4638-8c85-97af7394a661",
    "prId" : 33583,
    "prUrl" : "https://github.com/apache/spark/pull/33583#pullrequestreview-724892088",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f1177a7f-bbca-4e79-8332-93e208610eec",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Could you also update the comment?",
        "createdAt" : "2021-08-08T07:03:12Z",
        "updatedAt" : "2021-08-08T07:03:12Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "a334e694-ed28-469d-ad6b-d2096ee11494",
        "parentId" : "f1177a7f-bbca-4e79-8332-93e208610eec",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "Done",
        "createdAt" : "2021-08-08T11:32:02Z",
        "updatedAt" : "2021-08-08T11:32:02Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "44c744988b9e219f34a96a3a5835aff150607d42",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +47,51 @@  // - is still resolved\n  // - only host special expressions in supported operators\n  // - has globally-unique attribute IDs\n  // - optimized plan have same schema with previous plan.\n  override protected def isPlanIntegral("
  },
  {
    "id" : "42db3364-50fe-4fb7-b770-f58b74bd4ee9",
    "prId" : 33100,
    "prUrl" : "https://github.com/apache/spark/pull/33100#pullrequestreview-693527498",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c9c3b4e9-1c57-4fa6-871f-92d8570dbb41",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "The change here looks good. I wonder if we should better create a separate file to group similar stuff though .... e.g.) we can remove deduplicate too.",
        "createdAt" : "2021-06-28T03:28:55Z",
        "updatedAt" : "2021-06-28T03:28:56Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "82b03f052441b8830100b4202e6037a283aef18f",
    "line" : 25,
    "diffHunk" : "@@ -1,1 +1220,1224 @@\n  private val applyLocally: PartialFunction[LogicalPlan, LogicalPlan] = {\n    case Sort(_, _, child) if child.maxRows.exists(_ <= 1L) => child\n    case s @ Sort(orders, _, child) if orders.isEmpty || orders.exists(_.child.foldable) =>\n      val newOrders = orders.filterNot(_.child.foldable)"
  },
  {
    "id" : "62bcb8d2-b9ac-4128-b95a-e2ac4e78d469",
    "prId" : 33099,
    "prUrl" : "https://github.com/apache/spark/pull/33099#pullrequestreview-694148158",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d508edda-5fbf-40ef-810c-79402e663dab",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "No, we can't do this. `RebalancePartitions` is a best-effort and does not guarantee partitioning. We intentionally do not let `RebalancePartitions` extend `RepartitionOperation`, to avoid wrong optimizations.\r\n\r\nPlease revert this.",
        "createdAt" : "2021-06-28T05:24:28Z",
        "updatedAt" : "2021-06-28T05:24:38Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "ab5420ff-0ef9-4190-a76e-8e0a43360d94",
        "parentId" : "d508edda-5fbf-40ef-810c-79402e663dab",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "See https://github.com/apache/spark/pull/32932#discussion_r656249153",
        "createdAt" : "2021-06-28T05:25:32Z",
        "updatedAt" : "2021-06-28T05:25:33Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "25b01e2a-686b-4af7-ac5c-ee467ef15c49",
        "parentId" : "d508edda-5fbf-40ef-810c-79402e663dab",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "OK",
        "createdAt" : "2021-06-28T08:18:00Z",
        "updatedAt" : "2021-06-28T08:18:00Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      },
      {
        "id" : "6abd5891-125d-4c26-bfa0-e0f84cbded16",
        "parentId" : "d508edda-5fbf-40ef-810c-79402e663dab",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "Reverted at: https://github.com/apache/spark/commit/108635af1708173a72bec0e36bf3f2cea5b088c4",
        "createdAt" : "2021-06-28T08:43:03Z",
        "updatedAt" : "2021-06-28T08:43:03Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      },
      {
        "id" : "ddebaa85-5310-41de-9ad7-81b0e426928a",
        "parentId" : "d508edda-5fbf-40ef-810c-79402e663dab",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Oh, got it. Thank you for reverting, @cloud-fan and @wangyum .",
        "createdAt" : "2021-06-28T16:16:50Z",
        "updatedAt" : "2021-06-28T16:16:51Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "67642d87f7d1799e97f925444484f52354e6af72",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +914,918 @@\n    // Case 3: When a RebalancePartitions has a child of Repartition or RepartitionByExpression\n    // we can remove the child.\n    case r @ RebalancePartitions(_, child: RepartitionOperation) =>\n      r.copy(child = child.child)"
  },
  {
    "id" : "8c82774d-b56c-468f-b4d6-c0a705557303",
    "prId" : 32439,
    "prUrl" : "https://github.com/apache/spark/pull/32439#pullrequestreview-656933962",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "00cb0166-5c63-4606-b4cd-6455dc59448e",
        "parentId" : null,
        "authorId" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "body" : "I might have missed sth. from the previous commit, but what difference is there between regular `transform` and `transformWithPruning(AlwaysProcess.fn, ...)` ?",
        "createdAt" : "2021-05-11T03:50:17Z",
        "updatedAt" : "2021-05-11T06:53:50Z",
        "lastEditedBy" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "tags" : [
        ]
      },
      {
        "id" : "ef9935e5-c3f6-46e6-92b3-f0b96a3c55da",
        "parentId" : "00cb0166-5c63-4606-b4cd-6455dc59448e",
        "authorId" : "8b518862-583d-43a2-a10e-b33ef1b6d824",
        "body" : "`transform` internally calls\r\n`transformWithPruning(AlwaysProcess.fn, UnknownRuleId)` ...\r\n\r\nThe argument comments are here:\r\nhttps://github.com/apache/spark/blob/e08c40fa3f7054bdc713873c99d3aaf0014d9314/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/trees/TreeNode.scala#L434-L441\r\n\r\n So here, \r\n `plan.transformWithPruning(AlwaysProcess.fn, ruleId)` means there's no pruning based on TreePattern bits, but there's pruning based on ruleIds (if the rule is known to ineffective on a tree instance `T`,  it will be skipped next time when it is invoked on the same tree instance `T`).",
        "createdAt" : "2021-05-11T03:54:56Z",
        "updatedAt" : "2021-05-11T06:53:50Z",
        "lastEditedBy" : "8b518862-583d-43a2-a10e-b33ef1b6d824",
        "tags" : [
        ]
      },
      {
        "id" : "f04922ab-afc7-46da-bcae-7b82d3b9e3da",
        "parentId" : "00cb0166-5c63-4606-b4cd-6455dc59448e",
        "authorId" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "body" : "I see. Thanks!",
        "createdAt" : "2021-05-11T16:05:29Z",
        "updatedAt" : "2021-05-11T16:05:29Z",
        "lastEditedBy" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "tags" : [
        ]
      }
    ],
    "commit" : "5027ebcec31e9ea21df0bf16daa3e3427d4fe700",
    "line" : 76,
    "diffHunk" : "@@ -1,1 +756,760 @@\n  def apply(plan: LogicalPlan): LogicalPlan = removeProjectBeforeFilter(\n    plan.transformWithPruning(AlwaysProcess.fn, ruleId) {\n    // Prunes the unused columns from project list of Project/Aggregate/Expand\n    case p @ Project(_, p2: Project) if !p2.outputSet.subsetOf(p.references) =>"
  },
  {
    "id" : "422b02ff-1db1-4e86-9116-78455a60e690",
    "prId" : 31980,
    "prUrl" : "https://github.com/apache/spark/pull/31980#pullrequestreview-622655510",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6b1f75ba-4045-46b8-b4c9-6c65761ea30a",
        "parentId" : null,
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "`w1.references.subsetOf(grandChild.outputSet)` Do we need this check?",
        "createdAt" : "2021-03-27T14:25:45Z",
        "updatedAt" : "2021-04-02T07:14:22Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      },
      {
        "id" : "d230a834-b878-4aeb-bfa1-fbb011ba6801",
        "parentId" : "6b1f75ba-4045-46b8-b4c9-6c65761ea30a",
        "authorId" : "a8e23d47-3ae4-4385-848c-38a216d1bd08",
        "body" : "The test case `SPARK-34807: don't transpose two windows if project between them generates an input column` would fail without it.",
        "createdAt" : "2021-03-27T14:46:32Z",
        "updatedAt" : "2021-04-02T07:14:22Z",
        "lastEditedBy" : "a8e23d47-3ae4-4385-848c-38a216d1bd08",
        "tags" : [
        ]
      }
    ],
    "commit" : "60c6643f0bfda390baa32273f7ca1371a44d5771",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +1002,1006 @@\n    case w1 @ Window(_, _, _, Project(pl, w2 @ Window(_, _, _, grandChild)))\n      if windowsCompatible(w1, w2) && w1.references.subsetOf(grandChild.outputSet) =>\n      Project(\n        pl ++ w1.windowOutputSet,"
  },
  {
    "id" : "6f3247d1-b155-4ce3-9ca4-395595639158",
    "prId" : 31980,
    "prUrl" : "https://github.com/apache/spark/pull/31980#pullrequestreview-622660526",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ef9dad48-b6c1-4903-9d3d-2cf8831ee500",
        "parentId" : null,
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "Can we use `w1.output` instead of `pl ++ w1.windowOutputSet`?",
        "createdAt" : "2021-03-27T15:45:52Z",
        "updatedAt" : "2021-04-02T07:14:22Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      },
      {
        "id" : "7842ad25-1318-49f5-aed0-a0badc0e5c17",
        "parentId" : "ef9dad48-b6c1-4903-9d3d-2cf8831ee500",
        "authorId" : "a8e23d47-3ae4-4385-848c-38a216d1bd08",
        "body" : "The `w1.output` contains only the attributes (name + id), but not how the attributes are computed based on the input attributes. This information is in the project list.",
        "createdAt" : "2021-03-27T15:56:17Z",
        "updatedAt" : "2021-04-02T07:14:22Z",
        "lastEditedBy" : "a8e23d47-3ae4-4385-848c-38a216d1bd08",
        "tags" : [
        ]
      }
    ],
    "commit" : "60c6643f0bfda390baa32273f7ca1371a44d5771",
    "line" : 26,
    "diffHunk" : "@@ -1,1 +1004,1008 @@      if windowsCompatible(w1, w2) && w1.references.subsetOf(grandChild.outputSet) =>\n      Project(\n        pl ++ w1.windowOutputSet,\n        w2.copy(child = w1.copy(child = grandChild)))\n  }"
  },
  {
    "id" : "3a03e160-a9e7-4f0b-8b1b-f974e3b419e6",
    "prId" : 31980,
    "prUrl" : "https://github.com/apache/spark/pull/31980#pullrequestreview-622747073",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1f78dce7-f0c6-42be-aa4c-3f4cdff68d4a",
        "parentId" : null,
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "Two more spaces?",
        "createdAt" : "2021-03-28T13:05:16Z",
        "updatedAt" : "2021-04-02T07:14:22Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      }
    ],
    "commit" : "60c6643f0bfda390baa32273f7ca1371a44d5771",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +998,1002 @@    _.containsPattern(WINDOW), ruleId) {\n    case w1 @ Window(_, _, _, w2 @ Window(_, _, _, grandChild))\n      if windowsCompatible(w1, w2) =>\n      Project(w1.output, w2.copy(child = w1.copy(child = grandChild)))\n"
  },
  {
    "id" : "eecc4e6b-7aa5-45dd-a1ec-42634e192d00",
    "prId" : 31980,
    "prUrl" : "https://github.com/apache/spark/pull/31980#pullrequestreview-622747082",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c4abdcde-d297-43c5-9466-d9c10013cf12",
        "parentId" : null,
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "Two more spaces?",
        "createdAt" : "2021-03-28T13:05:21Z",
        "updatedAt" : "2021-04-02T07:14:22Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      }
    ],
    "commit" : "60c6643f0bfda390baa32273f7ca1371a44d5771",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +1002,1006 @@\n    case w1 @ Window(_, _, _, Project(pl, w2 @ Window(_, _, _, grandChild)))\n      if windowsCompatible(w1, w2) && w1.references.subsetOf(grandChild.outputSet) =>\n      Project(\n        pl ++ w1.windowOutputSet,"
  },
  {
    "id" : "3b40baba-cc77-4b7f-8585-d5654dd8de8d",
    "prId" : 31740,
    "prUrl" : "https://github.com/apache/spark/pull/31740#pullrequestreview-605758877",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5793afbb-e9f3-4f15-b942-78a29e37fa2e",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Looks like this is the case that the logic of sub common exprs should handle? cc: @viirya @cloud-fan ",
        "createdAt" : "2021-03-04T23:50:29Z",
        "updatedAt" : "2021-03-06T14:34:25Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "a51e488b-1414-4b85-80c4-48c2f4897b1f",
        "parentId" : "5793afbb-e9f3-4f15-b942-78a29e37fa2e",
        "authorId" : "a8e23d47-3ae4-4385-848c-38a216d1bd08",
        "body" : "Isn't the subexpression elimination wholestage codegen specific?",
        "createdAt" : "2021-03-05T21:00:30Z",
        "updatedAt" : "2021-03-06T14:34:25Z",
        "lastEditedBy" : "a8e23d47-3ae4-4385-848c-38a216d1bd08",
        "tags" : [
        ]
      },
      {
        "id" : "be0dbd41-cb3e-4237-a333-35af3f8b03e6",
        "parentId" : "5793afbb-e9f3-4f15-b942-78a29e37fa2e",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "The interpreted mode has the optimization, too, e.g., `SubExprEvaluationRuntime`.",
        "createdAt" : "2021-03-06T11:56:18Z",
        "updatedAt" : "2021-03-06T14:34:25Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "11e482fc-f5d6-4384-8191-b7b71bc1e663",
        "parentId" : "5793afbb-e9f3-4f15-b942-78a29e37fa2e",
        "authorId" : "a8e23d47-3ae4-4385-848c-38a216d1bd08",
        "body" : "I see, thanks for the reference.\r\nIndeed, the common subexpression elimination seems to eliminate these duplicates (if I did follow the code correctly).\r\nI would argue, that there still is benefit in removing these duplicates in the optimizer. If nothing else, then it at least would clean up the plan - when I saw the duplicates in the plan, then it prompted me to try to rewrite the query in such way that it would avoid them.\r\nAlso the subexpression cache has a limited size (although configurable) and this would reduce the runtime overhead of cache lookups.\r\n\r\n ",
        "createdAt" : "2021-03-06T14:01:13Z",
        "updatedAt" : "2021-03-06T14:34:25Z",
        "lastEditedBy" : "a8e23d47-3ae4-4385-848c-38a216d1bd08",
        "tags" : [
        ]
      }
    ],
    "commit" : "624a41dd5e0232c746c62629bbb4983259ea9b0c",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +949,953 @@\n/**\n * Replaces duplicate window expressions with an alias in a Project above the Window node.\n */\nobject RemoveDuplicateWindowExprs extends Rule[LogicalPlan] {"
  },
  {
    "id" : "bb41c349-5166-48b0-b718-1c49c514a933",
    "prId" : 31740,
    "prUrl" : "https://github.com/apache/spark/pull/31740#pullrequestreview-606091871",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d9a66b1f-9f4c-4505-a1ca-ba14df935760",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Shall we do the same thing for project and aggregate? Another big idea is to re-implement subexpression elimination in the plan level.",
        "createdAt" : "2021-03-08T09:16:12Z",
        "updatedAt" : "2021-03-08T09:16:12Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "624a41dd5e0232c746c62629bbb4983259ea9b0c",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +951,955 @@ * Replaces duplicate window expressions with an alias in a Project above the Window node.\n */\nobject RemoveDuplicateWindowExprs extends Rule[LogicalPlan] {\n  def apply(plan: LogicalPlan): LogicalPlan = plan transform {\n    case w @ Window(wes, _, _, _) if wes.length > 1 =>"
  },
  {
    "id" : "6e5d187d-dcda-48d6-8b95-bd1eaa239463",
    "prId" : 31739,
    "prUrl" : "https://github.com/apache/spark/pull/31739#pullrequestreview-618928946",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9eafbf53-27fb-4a17-9d7f-a194bbf8d8c5",
        "parentId" : null,
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "Thanks @wangyum for adding this. I think there might be a list of operators safe to push through besides `Project` - e.g. `Sort`, `RepartitionByExpression`, `ScriptTransformation`, etc.\r\n\r\nShall we add push down through `Project` separately? It should not be only restricted to `Project(Join)` right? It can be `Project(OtherOperator)` as well?\r\n\r\n```\r\ncase LocalLimit(exp, p: Project) =>\r\n  LocalLimit(exp, p.copy(maybePushLocalLimit(exp, _)))\r\n```",
        "createdAt" : "2021-03-04T21:38:33Z",
        "updatedAt" : "2021-03-29T13:38:22Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      },
      {
        "id" : "6e3f2501-3eb4-45eb-9c1b-e03f4b414d3b",
        "parentId" : "9eafbf53-27fb-4a17-9d7f-a194bbf8d8c5",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "I change it to `case LocalLimit(exp, project @ Project(_, child)) if !child.isInstanceOf[LeafNode] =>` to avoid do some useless work if direct pushdown limit through Project:\r\n```\r\n=== Applying Rule org.apache.spark.sql.catalyst.optimizer.LimitPushDown ===\r\n GlobalLimit 5                                                GlobalLimit 5\r\n +- LocalLimit 5                                              +- LocalLimit 5\r\n    +- Project [a#0]                                             +- Project [a#0]\r\n!      +- Join LeftOuter, ((a#0 = x#3) AND (b#1 = y#4))             +- LocalLimit 5\r\n!         :- LocalLimit 5                                              +- Join LeftOuter, ((a#0 = x#3) AND (b#1 = y#4))\r\n!         :  +- Project [a#0, b#1]                                        :- LocalLimit 5\r\n!         :     +- Relation default.t1[a#0,b#1,c#2] parquet               :  +- Project [a#0, b#1]\r\n!         +- Project [x#3, y#4]                                           :     +- LocalLimit 5\r\n!            +- Filter (isnotnull(x#3) AND isnotnull(y#4))                :        +- Relation default.t1[a#0,b#1,c#2] parquet\r\n!               +- Relation default.t2[x#3,y#4,z#5] parquet               +- Project [x#3, y#4]\r\n!                                                                            +- Filter (isnotnull(x#3) AND isnotnull(y#4))\r\n!                                                                               +- Relation default.t2[x#3,y#4,z#5] parquet\r\n\r\n\r\n=== Applying Rule org.apache.spark.sql.catalyst.optimizer.CollapseProject ===\r\n GlobalLimit 5                                                            GlobalLimit 5\r\n +- LocalLimit 5                                                          +- LocalLimit 5\r\n!   +- Project [a#0]                                                         +- LocalLimit 5\r\n!      +- LocalLimit 5                                                          +- Project [a#0]\r\n!         +- Project [a#0]                                                         +- Join LeftOuter, ((a#0 = x#3) AND (b#1 = y#4))\r\n!            +- Join LeftOuter, ((a#0 = x#3) AND (b#1 = y#4))                         :- LocalLimit 5\r\n!               :- LocalLimit 5                                                       :  +- LocalLimit 5\r\n!               :  +- Project [a#0, b#1]                                              :     +- Project [a#0, b#1]\r\n!               :     +- LocalLimit 5                                                 :        +- Relation default.t1[a#0,b#1,c#2] parquet\r\n!               :        +- Project [a#0, b#1]                                        +- Project [x#3, y#4]\r\n!               :           +- Relation default.t1[a#0,b#1,c#2] parquet                  +- Filter (isnotnull(x#3) AND isnotnull(y#4))\r\n!               +- Project [x#3, y#4]                                                       +- Relation default.t2[x#3,y#4,z#5] parquet\r\n!                  +- Filter (isnotnull(x#3) AND isnotnull(y#4))          \r\n!                     +- Relation default.t2[x#3,y#4,z#5] parquet         \r\n           \r\n=== Applying Rule org.apache.spark.sql.catalyst.optimizer.EliminateLimits ===\r\n GlobalLimit 5                                                      GlobalLimit 5\r\n!+- LocalLimit 5                                                    +- LocalLimit least(5, 5)\r\n!   +- LocalLimit 5                                                    +- Project [a#0]\r\n!      +- Project [a#0]                                                   +- Join LeftOuter, ((a#0 = x#3) AND (b#1 = y#4))\r\n!         +- Join LeftOuter, ((a#0 = x#3) AND (b#1 = y#4))                   :- LocalLimit least(5, 5)\r\n!            :- LocalLimit 5                                                 :  +- Project [a#0, b#1]\r\n!            :  +- LocalLimit 5                                              :     +- Relation default.t1[a#0,b#1,c#2] parquet\r\n!            :     +- Project [a#0, b#1]                                     +- Project [x#3, y#4]\r\n!            :        +- Relation default.t1[a#0,b#1,c#2] parquet               +- Filter (isnotnull(x#3) AND isnotnull(y#4))\r\n!            +- Project [x#3, y#4]                                                 +- Relation default.t2[x#3,y#4,z#5] parquet\r\n!               +- Filter (isnotnull(x#3) AND isnotnull(y#4))       \r\n!                  +- Relation default.t2[x#3,y#4,z#5] parquet      \r\n \r\n```",
        "createdAt" : "2021-03-05T00:52:58Z",
        "updatedAt" : "2021-03-29T13:38:22Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      },
      {
        "id" : "a06e76c3-e24f-4559-9a93-1f37f051ed06",
        "parentId" : "9eafbf53-27fb-4a17-9d7f-a194bbf8d8c5",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Sounds like a good idea. We can have an allowlist of unary operators.",
        "createdAt" : "2021-03-22T15:47:44Z",
        "updatedAt" : "2021-03-29T13:38:22Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "1e7d73ff-c0ec-4ba9-adb3-65c93af7b3de",
        "parentId" : "9eafbf53-27fb-4a17-9d7f-a194bbf8d8c5",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "It will introduce useless pushdown even only allow `Join`.\r\n```scala\r\n    case LocalLimit(exp, p: Project) if p.child.isInstanceOf[Join] =>\r\n      LocalLimit(exp, p.copy(child = maybePushLocalLimit(exp, p.child)))\r\n```\r\nFor example:\r\n```scala\r\nspark.range(200L).selectExpr(\"id AS a\", \"id AS b\").write.saveAsTable(\"t1\")\r\nspark.range(300L).selectExpr(\"id AS x\", \"id AS y\").write.saveAsTable(\"t2\")\r\nspark.sql(\"SELECT 1 FROM t1 INNER JOIN t2 ON a = x limit 10\").explain(true)\r\n```\r\n\r\n```\r\n== Optimized Logical Plan ==\r\nGlobalLimit 10\r\n+- LocalLimit 10\r\n   +- Project [1 AS 1#20]\r\n      +- LocalLimit 10\r\n         +- Project\r\n            +- Join Inner, (a#16L = x#18L)\r\n               :- Project [a#16L]\r\n               :  +- Filter isnotnull(a#16L)\r\n               :     +- Relation default.t1[a#16L,b#17L] parquet\r\n               +- Project [x#18L]\r\n                  +- Filter isnotnull(x#18L)\r\n                     +- Relation default.t2[x#18L,y#19L] parquet\r\n\r\n== Physical Plan ==\r\nAdaptiveSparkPlan isFinalPlan=false\r\n+- CollectLimit 10\r\n   +- Project [1 AS 1#20]\r\n      +- LocalLimit 10\r\n         +- Project\r\n            +- BroadcastHashJoin [a#16L], [x#18L], Inner, BuildLeft, false\r\n               :- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [id=#68]\r\n               :  +- Filter isnotnull(a#16L)\r\n               :     +- FileScan parquet default.t1[a#16L] \r\n               +- Filter isnotnull(x#18L)\r\n                  +- FileScan parquet default.t2[x#18L]\r\n\r\n```\r\n\r\nAnother example is TPC-DS q32:\r\nhttps://github.com/apache/spark/blob/66f5a42ca5d259038f0749ae2b9a04cc2f658880/sql/core/src/test/resources/tpcds/q32.sql#L1-L15\r\n",
        "createdAt" : "2021-03-22T23:59:31Z",
        "updatedAt" : "2021-03-29T13:38:22Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      },
      {
        "id" : "dd5fbc66-05e0-46ec-b03c-cdabe748b348",
        "parentId" : "9eafbf53-27fb-4a17-9d7f-a194bbf8d8c5",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "@c21 @maropu @cloud-fan \r\nI think `Project` is the most used, and this will not introduce useless `LocalLimit`.",
        "createdAt" : "2021-03-23T07:53:58Z",
        "updatedAt" : "2021-03-29T13:38:22Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      },
      {
        "id" : "a95fa0e8-3ce3-48f1-b22b-d493e8a3ffe3",
        "parentId" : "9eafbf53-27fb-4a17-9d7f-a194bbf8d8c5",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "@wangyum and @cloud-fan - I think one followup can be to pass `LocalLimit` through eligible operators. We don't add more `LocalLimit` operators, but push the existing `LocalLimit` further down in query plan.",
        "createdAt" : "2021-03-23T18:20:35Z",
        "updatedAt" : "2021-03-29T13:38:22Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      }
    ],
    "commit" : "4c7bc54c69ca347c01d84ac8d2ab54033d96aadf",
    "line" : 49,
    "diffHunk" : "@@ -1,1 +674,678 @@      LocalLimit(exp, pushLocalLimitThroughJoin(exp, join))\n    // There is a Project between LocalLimit and Join if they do not have the same output.\n    case LocalLimit(exp, project @ Project(_, join: Join)) =>\n      LocalLimit(exp, project.copy(child = pushLocalLimitThroughJoin(exp, join)))\n  }"
  },
  {
    "id" : "787dc104-2184-4703-95b1-b7be63642260",
    "prId" : 31677,
    "prUrl" : "https://github.com/apache/spark/pull/31677#pullrequestreview-622714876",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "16cb3ad1-48a6-425c-9ae4-d3751a88bf5c",
        "parentId" : null,
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "Could we move `windowsCompatible` before `apply`. We usually put the private function before the `apply`. See `TransposeWindow`.",
        "createdAt" : "2021-03-28T02:13:24Z",
        "updatedAt" : "2021-03-28T13:48:42Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      }
    ],
    "commit" : "7097ec3c724356a0d4c226b1222845b6df738e39",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +951,955 @@ */\nobject CollapseWindow extends Rule[LogicalPlan] {\n  private def windowsCompatible(w1: Window, w2: Window): Boolean = {\n    w1.partitionSpec == w2.partitionSpec &&\n      w1.orderSpec == w2.orderSpec &&"
  },
  {
    "id" : "651c8635-ce67-4aa5-a7d7-c475cb37dac1",
    "prId" : 31656,
    "prUrl" : "https://github.com/apache/spark/pull/31656#pullrequestreview-599964928",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9a6ec167-10e1-429b-bcf8-a3cd8edb0ab7",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "We need to keep `LogicalPlan` itself? We cannot use `semanticHash` instead?",
        "createdAt" : "2021-02-26T12:31:04Z",
        "updatedAt" : "2021-03-01T07:38:01Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "40586090-8d54-4ce6-a16e-210576b3eb42",
        "parentId" : "9a6ec167-10e1-429b-bcf8-a3cd8edb0ab7",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "I think `semanticHash` also works. Can use it.",
        "createdAt" : "2021-02-26T21:43:47Z",
        "updatedAt" : "2021-03-01T07:38:01Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "ef057af3-3a1f-4bda-bbb8-66fc68f08843",
        "parentId" : "9a6ec167-10e1-429b-bcf8-a3cd8edb0ab7",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Oh, rethinking about it, isn't `canonicalized` more reliable? It is less likely, but hash code might collision.",
        "createdAt" : "2021-02-26T21:46:42Z",
        "updatedAt" : "2021-03-01T07:38:01Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "6a420d497d91be512ad3e0317756dcc7f96e605d",
    "line" : 31,
    "diffHunk" : "@@ -1,1 +530,534 @@  private def simplifyUnion(u: Union): LogicalPlan = {\n    val uniqueChildren = mutable.ArrayBuffer.empty[LogicalPlan]\n    val uniqueChildrenKey = mutable.HashSet.empty[LogicalPlan]\n\n    u.children.foreach { c =>"
  },
  {
    "id" : "8addd2f9-6560-4fb3-9915-6b677d7f5f43",
    "prId" : 31656,
    "prUrl" : "https://github.com/apache/spark/pull/31656#pullrequestreview-600061245",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ecea004c-94ff-4d85-8f33-8d2ab79d97d6",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Does the added test in `DataFrameSetOperationsSuite` fail without this reorder? The change itself looks reasonable though.",
        "createdAt" : "2021-02-26T12:36:07Z",
        "updatedAt" : "2021-03-01T07:38:01Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "591c08b1-f6a4-4d85-a189-2b62aa196546",
        "parentId" : "ecea004c-94ff-4d85-8f33-8d2ab79d97d6",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Added test is fine. But there will be an [error](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/135486/testReport/org.apache.spark.sql/SQLQueryTestSuite/subquery_in_subquery_in_set_operations_sql/) of \"Once strategy's idempotence is broken for batch Union\".\r\n\r\nIt is because `CombineUnions` might combine two or more `Union` and they have redundant children under `Distinct`. So the rule order `RemoveNoopUnion` -> `CombineUnions` is not idempotence for Once strategy. ",
        "createdAt" : "2021-02-26T17:25:25Z",
        "updatedAt" : "2021-03-01T07:38:01Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "b915524d-ea7d-4447-ad9a-36d44a6d00e8",
        "parentId" : "ecea004c-94ff-4d85-8f33-8d2ab79d97d6",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Ah, I see.",
        "createdAt" : "2021-02-27T01:43:04Z",
        "updatedAt" : "2021-03-01T07:38:01Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "6a420d497d91be512ad3e0317756dcc7f96e605d",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +159,163 @@      RemoveNoopOperators,\n      CombineUnions,\n      RemoveNoopUnion) ::\n    Batch(\"OptimizeLimitZero\", Once,\n      OptimizeLimitZero) ::"
  },
  {
    "id" : "5ec3057c-722f-4bb9-bcfd-17d82c12f028",
    "prId" : 31656,
    "prUrl" : "https://github.com/apache/spark/pull/31656#pullrequestreview-600450725",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8d450b3d-d965-4066-9948-b8c3878c8952",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "We need plan canonicalization here? It looks enough to check if the output is the same just like `LogicalPlan.sameOutput`?",
        "createdAt" : "2021-02-27T01:51:19Z",
        "updatedAt" : "2021-03-01T07:38:01Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "a36c23bd-4fce-45f2-aad6-92ffe1e6a8f9",
        "parentId" : "8d450b3d-d965-4066-9948-b8c3878c8952",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "`canonicalized` should cover more cases than `sameOutput`. Note that by definition two plans with same `canonicalized` plans will always evaluate to the same result. But their outputs might not be the same.",
        "createdAt" : "2021-03-01T07:36:11Z",
        "updatedAt" : "2021-03-01T07:38:01Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "6a420d497d91be512ad3e0317756dcc7f96e605d",
    "line" : 34,
    "diffHunk" : "@@ -1,1 +533,537 @@\n    u.children.foreach { c =>\n      val key = removeAliasOnlyProject(c).canonicalized\n      if (!uniqueChildrenKey.contains(key)) {\n        uniqueChildren += c"
  },
  {
    "id" : "f8b89e8c-7417-4738-9964-ed6fb518f074",
    "prId" : 31630,
    "prUrl" : "https://github.com/apache/spark/pull/31630#pullrequestreview-597395340",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "219add80-8145-4053-80c7-20d44d893863",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "hm, in this case, we need the join itself?\r\n```\r\nscala> sql(\"select * from l1\").show()\r\n+----+\r\n|  id|\r\n+----+\r\n|   1|\r\n|   2|\r\n|null|\r\n+----+\r\n\r\n\r\nscala> sql(\"select * from r1\").show()\r\n+----+\r\n|  id|\r\n+----+\r\n|   2|\r\n|null|\r\n+----+\r\n\r\n\r\nscala> sql(\"select * from l1 left semi join r1\").show()\r\n+----+\r\n|  id|\r\n+----+\r\n|   1|\r\n|   2|\r\n|null|\r\n+----+\r\n\r\n\r\nscala> sql(\"select * from l1 left anti join r1\").show()\r\n+---+\r\n| id|\r\n+---+\r\n+---+\r\n```",
        "createdAt" : "2021-02-24T04:09:38Z",
        "updatedAt" : "2021-02-24T06:42:46Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "3005e518-a212-45a4-945a-7071fe40da58",
        "parentId" : "219add80-8145-4053-80c7-20d44d893863",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "I think we still need. Whether to output all rows or nothing, is depending on whether right side is empty, and this can only be known during runtime.",
        "createdAt" : "2021-02-24T04:19:54Z",
        "updatedAt" : "2021-02-24T06:42:46Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      },
      {
        "id" : "fb7db00f-35f4-4cb0-9276-9fd5850d596e",
        "parentId" : "219add80-8145-4053-80c7-20d44d893863",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "@maropu - this actually reminds me whether we can further optimize during runtime, and I found I already did it for LEFT SEMI with AQE - https://github.com/apache/spark/pull/29484 . Similarly for LEFT ANTI join without condition, we can convert join logical plan node to an empty relation if right build side is not empty. Will submit a followup PR tomorrow.\r\n\r\nIn addition, after taking a deep look at `BroadcastNestedLoopJoinExec` (never looked closely to that because it's not popular in our environment), I found many places that we can optimize:\r\n* populate `outputOrdering` and `outputPartitioning` when possible to avoid shuffle/sort in later stage.\r\n* shortcut for `LEFT SEMI/ANTI` in `defaultJoin()` as we don't need to look through all rows when there's no join condition.\r\n* code-gen the operator.\r\n\r\nI will file an umbrella JIRA with minor priority and do it gradually.",
        "createdAt" : "2021-02-24T09:02:42Z",
        "updatedAt" : "2021-02-24T09:02:43Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      },
      {
        "id" : "746ce87f-5db7-44aa-964a-873bb910af96",
        "parentId" : "219add80-8145-4053-80c7-20d44d893863",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "> Similarly for LEFT ANTI join without condition, we can convert join logical plan node to an empty relation if right build side is not empty. Will submit a followup PR tomorrow.\r\n\r\nAh, I see. That sounds reasonable.  Nice idea, @c21 .",
        "createdAt" : "2021-02-24T11:52:24Z",
        "updatedAt" : "2021-02-24T11:52:25Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "22bfd5e13ffb5b51a6a29851161146a1af6a5666",
    "line" : 37,
    "diffHunk" : "@@ -1,1 +561,565 @@            right = maybePushLocalLimit(exp, right))\n        case LeftSemi | LeftAnti if conditionOpt.isEmpty =>\n          join.copy(left = maybePushLocalLimit(exp, left))\n        case _ => join\n      }"
  },
  {
    "id" : "dc09c4d1-1ab6-40b0-93aa-16e2f242e559",
    "prId" : 31595,
    "prUrl" : "https://github.com/apache/spark/pull/31595#pullrequestreview-594798657",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1e720cfa-b11b-48c6-a2a6-a0a64d80a326",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "If we handle only two cases (`Distinct` and `Deduplicate`), let's say explicitly `Distinct/Deduplicate` instead of saying `Distinct like operators`. In general, we use a `whitelist` approach.",
        "createdAt" : "2021-02-21T01:34:49Z",
        "updatedAt" : "2021-02-25T04:01:53Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "b5d704112549310e2592aad21bd8308386ff0cf8",
    "line" : 49,
    "diffHunk" : "@@ -1,1 +540,544 @@      removeUnion(u).map(c => d.withNewChildren(Seq(c))).getOrElse(d)\n\n    case d @ Deduplicate(_, u: Union) =>\n      removeUnion(u).map(c => d.withNewChildren(Seq(c))).getOrElse(d)\n  }"
  },
  {
    "id" : "cd503e49-5b2d-4c35-a96f-854af7e378b6",
    "prId" : 31595,
    "prUrl" : "https://github.com/apache/spark/pull/31595#pullrequestreview-595772582",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f1ae7f79-873b-4f72-9b4f-d33ffbf772a8",
        "parentId" : null,
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "I am wondering why this is needed, given we already have `Eliminate no-op Projects` below at line 511. Shouldn't this kind of alias-only project already be removed by that?",
        "createdAt" : "2021-02-22T07:26:39Z",
        "updatedAt" : "2021-02-25T04:01:53Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      },
      {
        "id" : "73b3a6a6-4dbb-4ce7-9ec1-7b245d323e49",
        "parentId" : "f1ae7f79-873b-4f72-9b4f-d33ffbf772a8",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "Also just FYI there's some ongoing change for this same rule - https://github.com/apache/spark/pull/31538 , ",
        "createdAt" : "2021-02-22T07:31:56Z",
        "updatedAt" : "2021-02-25T04:01:53Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      },
      {
        "id" : "f73560d8-3437-40a8-8555-3590cd274c9d",
        "parentId" : "f1ae7f79-873b-4f72-9b4f-d33ffbf772a8",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Alias-only project is not no-op project here. It will change the outputs.",
        "createdAt" : "2021-02-22T20:53:46Z",
        "updatedAt" : "2021-02-25T04:01:53Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "b5d704112549310e2592aad21bd8308386ff0cf8",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +511,515 @@   * from its child.\n   */\n  private def removeAliasOnlyProject(plan: LogicalPlan): LogicalPlan = plan match {\n    case p @ Project(projectList, child) =>\n      val aliasOnly = projectList.length == child.output.length &&"
  },
  {
    "id" : "15d7c702-ef7d-4c5b-971b-0639a9e8e487",
    "prId" : 31595,
    "prUrl" : "https://github.com/apache/spark/pull/31595#pullrequestreview-595993938",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "43785d49-1e00-4c94-958a-8c2ebe399a8a",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Thank you for splitting the rule!",
        "createdAt" : "2021-02-23T04:48:33Z",
        "updatedAt" : "2021-02-25T04:01:53Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "b5d704112549310e2592aad21bd8308386ff0cf8",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +158,162 @@    Batch(\"Union\", Once,\n      RemoveNoopOperators,\n      RemoveNoopUnion,\n      CombineUnions) ::\n    Batch(\"OptimizeLimitZero\", Once,"
  },
  {
    "id" : "35bc364f-2a18-4e45-8271-101990d787fa",
    "prId" : 31595,
    "prUrl" : "https://github.com/apache/spark/pull/31595#pullrequestreview-596999205",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a3576640-7c17-440e-b7ca-1608622c29d8",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "How about a `Aggregate` case?",
        "createdAt" : "2021-02-24T00:49:44Z",
        "updatedAt" : "2021-02-25T04:01:53Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "0b377946-7976-451e-b372-2659b01fd9e5",
        "parentId" : "a3576640-7c17-440e-b7ca-1608622c29d8",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "You mean a no-op Aggregation like `select key as key2, value from table group by key value`?",
        "createdAt" : "2021-02-24T01:32:29Z",
        "updatedAt" : "2021-02-25T04:01:53Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "b480d3a1-1b45-4cc1-9ae7-d23f46c8f1d0",
        "parentId" : "a3576640-7c17-440e-b7ca-1608622c29d8",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "I thought it was like this:\r\n```\r\nscala> sql(\"select distinct * from (select id from t group by id union select id as key from t group by id)\").explain(true)\r\n...\r\n== Analyzed Logical Plan ==\r\nid: bigint\r\nDistinct\r\n+- Project [id#8L]\r\n   +- SubqueryAlias __auto_generated_subquery_name\r\n      +- Distinct\r\n         +- Union false, false\r\n            :- Aggregate [id#8L], [id#8L]\r\n            :  +- SubqueryAlias spark_catalog.default.t\r\n            :     +- Relation[id#8L] parquet\r\n            +- Aggregate [id#8L], [id#8L AS key#7L]\r\n               +- SubqueryAlias spark_catalog.default.t\r\n                  +- Relation[id#8L] parquet\r\n\r\n== Optimized Logical Plan ==\r\nAggregate [id#8L], [id#8L]\r\n+- Aggregate [id#8L], [id#8L]\r\n   +- Union false, false\r\n      :- Aggregate [id#8L], [id#8L]\r\n      :  +- Relation[id#8L] parquet\r\n      +- Aggregate [id#8L], [id#8L AS key#7L]\r\n         +- Relation[id#8L] parquet\r\n```",
        "createdAt" : "2021-02-24T01:43:12Z",
        "updatedAt" : "2021-02-25T04:01:53Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "88481667-a10c-4c16-bdb9-94d679bab002",
        "parentId" : "a3576640-7c17-440e-b7ca-1608622c29d8",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "yea, that is what I said a no-op aggregate. I think it is okay to add it. Maybe I can add it in follow up.",
        "createdAt" : "2021-02-24T02:03:00Z",
        "updatedAt" : "2021-02-25T04:01:53Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "50b9d306-22e7-4f53-9c1b-79c2038dbaf7",
        "parentId" : "a3576640-7c17-440e-b7ca-1608622c29d8",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "yea, sgtm.",
        "createdAt" : "2021-02-24T02:06:30Z",
        "updatedAt" : "2021-02-25T04:01:53Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "b5d704112549310e2592aad21bd8308386ff0cf8",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +512,516 @@   */\n  private def removeAliasOnlyProject(plan: LogicalPlan): LogicalPlan = plan match {\n    case p @ Project(projectList, child) =>\n      val aliasOnly = projectList.length == child.output.length &&\n        projectList.zip(child.output).forall {"
  },
  {
    "id" : "5ffa3d52-b8e9-43f4-8c39-965ef6162b73",
    "prId" : 31567,
    "prUrl" : "https://github.com/apache/spark/pull/31567#pullrequestreview-590873109",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e5b45719-a23b-4f5f-81af-367747d12dc6",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Shall we update the comments here too to describe the case when we push the limit with cross join?",
        "createdAt" : "2021-02-16T04:13:14Z",
        "updatedAt" : "2021-02-23T03:20:22Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "e0915b93a6cd528b6c07e603b98762a1134a7640",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +545,549 @@    // below FULL OUTER JOIN in the general case without a more invasive rewrite.\n    // We also need to ensure that this limit pushdown rule will not eventually introduce limits\n    // on both sides if it is applied multiple times. Therefore:\n    //   - If one side is already limited, stack another limit on top if the new limit is smaller.\n    //     The redundant limit will be collapsed by the CombineLimits rule."
  },
  {
    "id" : "c7488cbe-b793-4763-b953-77b29f42cd47",
    "prId" : 31567,
    "prUrl" : "https://github.com/apache/spark/pull/31567#pullrequestreview-591899952",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "86ad22ad-c3cf-48c2-8492-322dd541adc1",
        "parentId" : null,
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "It seems that we can also push down limit into left side, for LEFT SEMI and LEFT ANTI join, right?\r\nI can create a minor PR if it's not on your plan @wangyum , thanks.",
        "createdAt" : "2021-02-17T05:42:03Z",
        "updatedAt" : "2021-02-23T03:20:22Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      },
      {
        "id" : "2659c7b5-fd93-4d4d-a599-0266c680ed39",
        "parentId" : "86ad22ad-c3cf-48c2-8492-322dd541adc1",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "It seems we can not pushdown `LEFT SEMI JOIN`, for example:\r\n```scala\r\nspark.range(20).selectExpr(\"id % 10 as id\").repartition(1).write.saveAsTable(\"t1\")\r\nspark.range(5, 9, 1).repartition(1).write.saveAsTable(\"t2\")\r\nval df = spark.sql(\"select * from t1 LEFT SEMI JOIN t2 on t1.id = t2.id limit 3\")\r\ndf.explain()\r\ndf.show\r\n```\r\n\r\nCurrent:\r\n```\r\n== Physical Plan ==\r\nAdaptiveSparkPlan isFinalPlan=false\r\n+- CollectLimit 3\r\n   +- BroadcastHashJoin [id#10L], [id#11L], LeftSemi, BuildRight, false\r\n      :- Filter isnotnull(id#10L)\r\n      :  +- FileScan parquet default.t1[id#10L] Batched: true, DataFilters: [isnotnull(id#10L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/yumwang/spark/SPARK-28169/spark-warehouse/org.apache.spark..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:bigint>\r\n      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [id=#69]\r\n         +- Filter isnotnull(id#11L)\r\n            +- FileScan parquet default.t2[id#11L] Batched: true, DataFilters: [isnotnull(id#11L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/yumwang/spark/SPARK-28169/spark-warehouse/org.apache.spark..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:bigint>\r\n\r\n\r\n+---+\r\n| id|\r\n+---+\r\n|  5|\r\n|  6|\r\n|  7|\r\n+---+\r\n```\r\n\r\nPushdown:\r\n```\r\n== Physical Plan ==\r\nAdaptiveSparkPlan isFinalPlan=false\r\n+- CollectLimit 3\r\n   +- BroadcastHashJoin [id#10L], [id#11L], LeftSemi, BuildRight, false\r\n      :- LocalLimit 3\r\n      :  +- Filter isnotnull(id#10L)\r\n      :     +- LocalLimit 3\r\n      :        +- FileScan parquet default.t1[id#10L] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/yumwang/spark/SPARK-28169/spark-warehouse/org.apache.spark..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:bigint>\r\n      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [id=#77]\r\n         +- Filter isnotnull(id#11L)\r\n            +- FileScan parquet default.t2[id#11L] Batched: true, DataFilters: [isnotnull(id#11L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/yumwang/spark/SPARK-28169/spark-warehouse/org.apache.spark..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:bigint>\r\n\r\n\r\n+---+\r\n| id|\r\n+---+\r\n+---+\r\n```",
        "createdAt" : "2021-02-17T07:13:10Z",
        "updatedAt" : "2021-02-23T03:20:22Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      },
      {
        "id" : "aed680e6-a507-41ae-a477-49a4419b6d45",
        "parentId" : "86ad22ad-c3cf-48c2-8492-322dd541adc1",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "@wangyum - yes, you are right. My bad. LEFT OUTER/RIGHT OUTER join will output the left/right side rows anyway no matter of being matched or not. So they are safe for limit push down, but not LEFT SEMI/LEFT ANTI.",
        "createdAt" : "2021-02-17T07:20:19Z",
        "updatedAt" : "2021-02-23T03:20:22Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      },
      {
        "id" : "0e55ea6a-b10c-44f6-9696-f99756f2358b",
        "parentId" : "86ad22ad-c3cf-48c2-8492-322dd541adc1",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "Actually, what about LEFT SEMI / LEFT ANTI join without condition? They should be planned into physical operator `BroadcastNestedLoopJoin` instead. ",
        "createdAt" : "2021-02-17T07:28:02Z",
        "updatedAt" : "2021-02-23T03:20:22Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      }
    ],
    "commit" : "e0915b93a6cd528b6c07e603b98762a1134a7640",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +551,555 @@      val newJoin = joinType match {\n        case RightOuter => join.copy(right = maybePushLocalLimit(exp, right))\n        case LeftOuter => join.copy(left = maybePushLocalLimit(exp, left))\n        case _: InnerLike if conditionOpt.isEmpty =>\n          join.copy("
  },
  {
    "id" : "0076c246-974b-4e8f-99ac-b0f2bff2a585",
    "prId" : 31538,
    "prUrl" : "https://github.com/apache/spark/pull/31538#pullrequestreview-588906078",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1d2698ad-adfb-4ef6-9d75-b92f193dfc6f",
        "parentId" : null,
        "authorId" : "a8e23d47-3ae4-4385-848c-38a216d1bd08",
        "body" : "Would this affect other places where the `sameOutput` method is used? It kinda looks like fixing a symptom and the real problem is a bit deeper. The `sameOutput` is used only in few places, but it internally uses `semanticEquals` that is very common.",
        "createdAt" : "2021-02-11T19:16:22Z",
        "updatedAt" : "2021-02-11T19:16:22Z",
        "lastEditedBy" : "a8e23d47-3ae4-4385-848c-38a216d1bd08",
        "tags" : [
        ]
      },
      {
        "id" : "2595b143-1e98-4cbf-bb82-86b263b68ad4",
        "parentId" : "1d2698ad-adfb-4ef6-9d75-b92f193dfc6f",
        "authorId" : "a8e23d47-3ae4-4385-848c-38a216d1bd08",
        "body" : "In the `LogicalPlan` there is this comment:\r\n```\r\n  /**\r\n   * This method checks if the same `ExprId` refers to an unique attribute in a plan tree.\r\n   * Some plan transformers (e.g., `RemoveNoopOperators`) rewrite logical\r\n   * plans based on this assumption.\r\n   */\r\n  def checkIfExprIdsAreGloballyUnique(plan: LogicalPlan): Boolean = {\r\n    checkIfSameExprIdNotReused(plan) && hasUniqueExprIdsForOutput(plan)\r\n  }\r\n```\r\n\r\nIt sounds like the `RemoveNoopOperators` is correct and the deeper issue is with the `ExprId` getting reused.\r\n",
        "createdAt" : "2021-02-11T19:22:28Z",
        "updatedAt" : "2021-02-11T19:22:28Z",
        "lastEditedBy" : "a8e23d47-3ae4-4385-848c-38a216d1bd08",
        "tags" : [
        ]
      }
    ],
    "commit" : "9212a85ddb82ffc2178b3e8e6c361eb18bfb3e6a",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +496,500 @@    // Eliminate no-op Projects\n    case p @ Project(projectList, child)\n      if projectList.forall(isAttribute) && child.sameOutput(p) => child\n\n    // Eliminate no-op Window"
  },
  {
    "id" : "69de1e81-6863-4e87-8ab9-6b05c89ab4c8",
    "prId" : 31404,
    "prUrl" : "https://github.com/apache/spark/pull/31404#pullrequestreview-590023719",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ed505b24-5011-432f-8095-cdb6ab531db0",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Why we need move this rule?",
        "createdAt" : "2021-02-10T20:44:01Z",
        "updatedAt" : "2021-02-19T02:38:37Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "425e9634-3a55-4873-8e78-fbc0e7ec9720",
        "parentId" : "ed505b24-5011-432f-8095-cdb6ab531db0",
        "authorId" : "ab8f521d-7cf6-437f-8ea5-922bd77ef56b",
        "body" : "It needs to put the rule 'CombineUnions' before ReplaceDistinctWithAggregate and ReplaceDeduplicateWithAggregate, otherwise these two rules will replace Distinct/Deduplicate to Aggregate and lead to rule 'CombineUnions' is invalid. On the other hand putting the rule 'ReplaceDeduplicateWithAggregate' in batch 'Replace Operators' is more intuitional.",
        "createdAt" : "2021-02-14T08:06:20Z",
        "updatedAt" : "2021-02-19T02:38:37Z",
        "lastEditedBy" : "ab8f521d-7cf6-437f-8ea5-922bd77ef56b",
        "tags" : [
        ]
      }
    ],
    "commit" : "1385f2b5963a6ec88dc9d9881f2a4bdd5c735e7a",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +185,189 @@      ReplaceExceptWithAntiJoin,\n      ReplaceDistinctWithAggregate,\n      ReplaceDeduplicateWithAggregate) ::\n    Batch(\"Aggregate\", fixedPoint,\n      RemoveLiteralFromGroupExpressions,"
  }
]