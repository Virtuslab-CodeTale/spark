[
  {
    "id" : "cd79d402-0e8b-4076-8d9c-4d93277dd5ea",
    "prId" : 33651,
    "prUrl" : "https://github.com/apache/spark/pull/33651#pullrequestreview-723483793",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cad47f42-f12c-4eb6-9c1f-655b555e308f",
        "parentId" : null,
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "It's flaky that this expression highly depend on `ConstantFolding`.",
        "createdAt" : "2021-08-05T15:13:46Z",
        "updatedAt" : "2021-08-05T15:13:46Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      }
    ],
    "commit" : "6f28a37f420f781486c3e8c102156e02d9f830d7",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +1677,1681 @@\n    case GlobalLimit(le, GlobalLimit(ne, grandChild)) =>\n      GlobalLimit(Literal(Least(Seq(ne, le)).eval().asInstanceOf[Int]), grandChild)\n    case LocalLimit(le, LocalLimit(ne, grandChild)) =>\n      LocalLimit(Literal(Least(Seq(ne, le)).eval().asInstanceOf[Int]), grandChild)"
  },
  {
    "id" : "e658d204-1883-44dc-8966-77b53bdc53ca",
    "prId" : 33587,
    "prUrl" : "https://github.com/apache/spark/pull/33587#pullrequestreview-723105990",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3b87979e-e583-48be-aaf5-b598ce034d3b",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "let's add the default case to be safe:\r\n```\r\ncase (other, _) => other\r\n```",
        "createdAt" : "2021-08-05T08:42:18Z",
        "updatedAt" : "2021-08-05T08:42:19Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "cfac5e7d-e36e-4cde-9fa3-9e9a9ead9751",
        "parentId" : "3b87979e-e583-48be-aaf5-b598ce034d3b",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "Okï¼Œ done",
        "createdAt" : "2021-08-05T08:57:35Z",
        "updatedAt" : "2021-08-05T08:57:35Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "1f93b6f14001d5a9e71cc9af207e12217f36cd5d",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +521,525 @@    projectList.zip(originalNames).map {\n      case (attr: Attribute, name) => attr.withName(name)\n      case (alias: Alias, name) => alias.withName(name)\n      case (other, _) => other\n    }"
  },
  {
    "id" : "258971e3-e28d-4638-8c85-97af7394a661",
    "prId" : 33583,
    "prUrl" : "https://github.com/apache/spark/pull/33583#pullrequestreview-724892088",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f1177a7f-bbca-4e79-8332-93e208610eec",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Could you also update the comment?",
        "createdAt" : "2021-08-08T07:03:12Z",
        "updatedAt" : "2021-08-08T07:03:12Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "a334e694-ed28-469d-ad6b-d2096ee11494",
        "parentId" : "f1177a7f-bbca-4e79-8332-93e208610eec",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "Done",
        "createdAt" : "2021-08-08T11:32:02Z",
        "updatedAt" : "2021-08-08T11:32:02Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "44c744988b9e219f34a96a3a5835aff150607d42",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +47,51 @@  // - is still resolved\n  // - only host special expressions in supported operators\n  // - has globally-unique attribute IDs\n  // - optimized plan have same schema with previous plan.\n  override protected def isPlanIntegral("
  },
  {
    "id" : "42db3364-50fe-4fb7-b770-f58b74bd4ee9",
    "prId" : 33100,
    "prUrl" : "https://github.com/apache/spark/pull/33100#pullrequestreview-693527498",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c9c3b4e9-1c57-4fa6-871f-92d8570dbb41",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "The change here looks good. I wonder if we should better create a separate file to group similar stuff though .... e.g.) we can remove deduplicate too.",
        "createdAt" : "2021-06-28T03:28:55Z",
        "updatedAt" : "2021-06-28T03:28:56Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "82b03f052441b8830100b4202e6037a283aef18f",
    "line" : 25,
    "diffHunk" : "@@ -1,1 +1220,1224 @@\n  private val applyLocally: PartialFunction[LogicalPlan, LogicalPlan] = {\n    case Sort(_, _, child) if child.maxRows.exists(_ <= 1L) => child\n    case s @ Sort(orders, _, child) if orders.isEmpty || orders.exists(_.child.foldable) =>\n      val newOrders = orders.filterNot(_.child.foldable)"
  },
  {
    "id" : "62bcb8d2-b9ac-4128-b95a-e2ac4e78d469",
    "prId" : 33099,
    "prUrl" : "https://github.com/apache/spark/pull/33099#pullrequestreview-694148158",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d508edda-5fbf-40ef-810c-79402e663dab",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "No, we can't do this. `RebalancePartitions` is a best-effort and does not guarantee partitioning. We intentionally do not let `RebalancePartitions` extend `RepartitionOperation`, to avoid wrong optimizations.\r\n\r\nPlease revert this.",
        "createdAt" : "2021-06-28T05:24:28Z",
        "updatedAt" : "2021-06-28T05:24:38Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "ab5420ff-0ef9-4190-a76e-8e0a43360d94",
        "parentId" : "d508edda-5fbf-40ef-810c-79402e663dab",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "See https://github.com/apache/spark/pull/32932#discussion_r656249153",
        "createdAt" : "2021-06-28T05:25:32Z",
        "updatedAt" : "2021-06-28T05:25:33Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "25b01e2a-686b-4af7-ac5c-ee467ef15c49",
        "parentId" : "d508edda-5fbf-40ef-810c-79402e663dab",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "OK",
        "createdAt" : "2021-06-28T08:18:00Z",
        "updatedAt" : "2021-06-28T08:18:00Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      },
      {
        "id" : "6abd5891-125d-4c26-bfa0-e0f84cbded16",
        "parentId" : "d508edda-5fbf-40ef-810c-79402e663dab",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "Reverted at: https://github.com/apache/spark/commit/108635af1708173a72bec0e36bf3f2cea5b088c4",
        "createdAt" : "2021-06-28T08:43:03Z",
        "updatedAt" : "2021-06-28T08:43:03Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      },
      {
        "id" : "ddebaa85-5310-41de-9ad7-81b0e426928a",
        "parentId" : "d508edda-5fbf-40ef-810c-79402e663dab",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Oh, got it. Thank you for reverting, @cloud-fan and @wangyum .",
        "createdAt" : "2021-06-28T16:16:50Z",
        "updatedAt" : "2021-06-28T16:16:51Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "67642d87f7d1799e97f925444484f52354e6af72",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +914,918 @@\n    // Case 3: When a RebalancePartitions has a child of Repartition or RepartitionByExpression\n    // we can remove the child.\n    case r @ RebalancePartitions(_, child: RepartitionOperation) =>\n      r.copy(child = child.child)"
  },
  {
    "id" : "8c82774d-b56c-468f-b4d6-c0a705557303",
    "prId" : 32439,
    "prUrl" : "https://github.com/apache/spark/pull/32439#pullrequestreview-656933962",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "00cb0166-5c63-4606-b4cd-6455dc59448e",
        "parentId" : null,
        "authorId" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "body" : "I might have missed sth. from the previous commit, but what difference is there between regular `transform` and `transformWithPruning(AlwaysProcess.fn, ...)` ?",
        "createdAt" : "2021-05-11T03:50:17Z",
        "updatedAt" : "2021-05-11T06:53:50Z",
        "lastEditedBy" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "tags" : [
        ]
      },
      {
        "id" : "ef9935e5-c3f6-46e6-92b3-f0b96a3c55da",
        "parentId" : "00cb0166-5c63-4606-b4cd-6455dc59448e",
        "authorId" : "8b518862-583d-43a2-a10e-b33ef1b6d824",
        "body" : "`transform` internally calls\r\n`transformWithPruning(AlwaysProcess.fn, UnknownRuleId)` ...\r\n\r\nThe argument comments are here:\r\nhttps://github.com/apache/spark/blob/e08c40fa3f7054bdc713873c99d3aaf0014d9314/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/trees/TreeNode.scala#L434-L441\r\n\r\n So here, \r\n `plan.transformWithPruning(AlwaysProcess.fn, ruleId)` means there's no pruning based on TreePattern bits, but there's pruning based on ruleIds (if the rule is known to ineffective on a tree instance `T`,  it will be skipped next time when it is invoked on the same tree instance `T`).",
        "createdAt" : "2021-05-11T03:54:56Z",
        "updatedAt" : "2021-05-11T06:53:50Z",
        "lastEditedBy" : "8b518862-583d-43a2-a10e-b33ef1b6d824",
        "tags" : [
        ]
      },
      {
        "id" : "f04922ab-afc7-46da-bcae-7b82d3b9e3da",
        "parentId" : "00cb0166-5c63-4606-b4cd-6455dc59448e",
        "authorId" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "body" : "I see. Thanks!",
        "createdAt" : "2021-05-11T16:05:29Z",
        "updatedAt" : "2021-05-11T16:05:29Z",
        "lastEditedBy" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "tags" : [
        ]
      }
    ],
    "commit" : "5027ebcec31e9ea21df0bf16daa3e3427d4fe700",
    "line" : 76,
    "diffHunk" : "@@ -1,1 +756,760 @@\n  def apply(plan: LogicalPlan): LogicalPlan = removeProjectBeforeFilter(\n    plan.transformWithPruning(AlwaysProcess.fn, ruleId) {\n    // Prunes the unused columns from project list of Project/Aggregate/Expand\n    case p @ Project(_, p2: Project) if !p2.outputSet.subsetOf(p.references) =>"
  },
  {
    "id" : "422b02ff-1db1-4e86-9116-78455a60e690",
    "prId" : 31980,
    "prUrl" : "https://github.com/apache/spark/pull/31980#pullrequestreview-622655510",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6b1f75ba-4045-46b8-b4c9-6c65761ea30a",
        "parentId" : null,
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "`w1.references.subsetOf(grandChild.outputSet)` Do we need this check?",
        "createdAt" : "2021-03-27T14:25:45Z",
        "updatedAt" : "2021-04-02T07:14:22Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      },
      {
        "id" : "d230a834-b878-4aeb-bfa1-fbb011ba6801",
        "parentId" : "6b1f75ba-4045-46b8-b4c9-6c65761ea30a",
        "authorId" : "a8e23d47-3ae4-4385-848c-38a216d1bd08",
        "body" : "The test case `SPARK-34807: don't transpose two windows if project between them generates an input column` would fail without it.",
        "createdAt" : "2021-03-27T14:46:32Z",
        "updatedAt" : "2021-04-02T07:14:22Z",
        "lastEditedBy" : "a8e23d47-3ae4-4385-848c-38a216d1bd08",
        "tags" : [
        ]
      }
    ],
    "commit" : "60c6643f0bfda390baa32273f7ca1371a44d5771",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +1002,1006 @@\n    case w1 @ Window(_, _, _, Project(pl, w2 @ Window(_, _, _, grandChild)))\n      if windowsCompatible(w1, w2) && w1.references.subsetOf(grandChild.outputSet) =>\n      Project(\n        pl ++ w1.windowOutputSet,"
  },
  {
    "id" : "6f3247d1-b155-4ce3-9ca4-395595639158",
    "prId" : 31980,
    "prUrl" : "https://github.com/apache/spark/pull/31980#pullrequestreview-622660526",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ef9dad48-b6c1-4903-9d3d-2cf8831ee500",
        "parentId" : null,
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "Can we use `w1.output` instead of `pl ++ w1.windowOutputSet`?",
        "createdAt" : "2021-03-27T15:45:52Z",
        "updatedAt" : "2021-04-02T07:14:22Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      },
      {
        "id" : "7842ad25-1318-49f5-aed0-a0badc0e5c17",
        "parentId" : "ef9dad48-b6c1-4903-9d3d-2cf8831ee500",
        "authorId" : "a8e23d47-3ae4-4385-848c-38a216d1bd08",
        "body" : "The `w1.output` contains only the attributes (name + id), but not how the attributes are computed based on the input attributes. This information is in the project list.",
        "createdAt" : "2021-03-27T15:56:17Z",
        "updatedAt" : "2021-04-02T07:14:22Z",
        "lastEditedBy" : "a8e23d47-3ae4-4385-848c-38a216d1bd08",
        "tags" : [
        ]
      }
    ],
    "commit" : "60c6643f0bfda390baa32273f7ca1371a44d5771",
    "line" : 26,
    "diffHunk" : "@@ -1,1 +1004,1008 @@      if windowsCompatible(w1, w2) && w1.references.subsetOf(grandChild.outputSet) =>\n      Project(\n        pl ++ w1.windowOutputSet,\n        w2.copy(child = w1.copy(child = grandChild)))\n  }"
  },
  {
    "id" : "3a03e160-a9e7-4f0b-8b1b-f974e3b419e6",
    "prId" : 31980,
    "prUrl" : "https://github.com/apache/spark/pull/31980#pullrequestreview-622747073",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1f78dce7-f0c6-42be-aa4c-3f4cdff68d4a",
        "parentId" : null,
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "Two more spaces?",
        "createdAt" : "2021-03-28T13:05:16Z",
        "updatedAt" : "2021-04-02T07:14:22Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      }
    ],
    "commit" : "60c6643f0bfda390baa32273f7ca1371a44d5771",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +998,1002 @@    _.containsPattern(WINDOW), ruleId) {\n    case w1 @ Window(_, _, _, w2 @ Window(_, _, _, grandChild))\n      if windowsCompatible(w1, w2) =>\n      Project(w1.output, w2.copy(child = w1.copy(child = grandChild)))\n"
  },
  {
    "id" : "eecc4e6b-7aa5-45dd-a1ec-42634e192d00",
    "prId" : 31980,
    "prUrl" : "https://github.com/apache/spark/pull/31980#pullrequestreview-622747082",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c4abdcde-d297-43c5-9466-d9c10013cf12",
        "parentId" : null,
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "Two more spaces?",
        "createdAt" : "2021-03-28T13:05:21Z",
        "updatedAt" : "2021-04-02T07:14:22Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      }
    ],
    "commit" : "60c6643f0bfda390baa32273f7ca1371a44d5771",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +1002,1006 @@\n    case w1 @ Window(_, _, _, Project(pl, w2 @ Window(_, _, _, grandChild)))\n      if windowsCompatible(w1, w2) && w1.references.subsetOf(grandChild.outputSet) =>\n      Project(\n        pl ++ w1.windowOutputSet,"
  },
  {
    "id" : "3b40baba-cc77-4b7f-8585-d5654dd8de8d",
    "prId" : 31740,
    "prUrl" : "https://github.com/apache/spark/pull/31740#pullrequestreview-605758877",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5793afbb-e9f3-4f15-b942-78a29e37fa2e",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Looks like this is the case that the logic of sub common exprs should handle? cc: @viirya @cloud-fan ",
        "createdAt" : "2021-03-04T23:50:29Z",
        "updatedAt" : "2021-03-06T14:34:25Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "a51e488b-1414-4b85-80c4-48c2f4897b1f",
        "parentId" : "5793afbb-e9f3-4f15-b942-78a29e37fa2e",
        "authorId" : "a8e23d47-3ae4-4385-848c-38a216d1bd08",
        "body" : "Isn't the subexpression elimination wholestage codegen specific?",
        "createdAt" : "2021-03-05T21:00:30Z",
        "updatedAt" : "2021-03-06T14:34:25Z",
        "lastEditedBy" : "a8e23d47-3ae4-4385-848c-38a216d1bd08",
        "tags" : [
        ]
      },
      {
        "id" : "be0dbd41-cb3e-4237-a333-35af3f8b03e6",
        "parentId" : "5793afbb-e9f3-4f15-b942-78a29e37fa2e",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "The interpreted mode has the optimization, too, e.g., `SubExprEvaluationRuntime`.",
        "createdAt" : "2021-03-06T11:56:18Z",
        "updatedAt" : "2021-03-06T14:34:25Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "11e482fc-f5d6-4384-8191-b7b71bc1e663",
        "parentId" : "5793afbb-e9f3-4f15-b942-78a29e37fa2e",
        "authorId" : "a8e23d47-3ae4-4385-848c-38a216d1bd08",
        "body" : "I see, thanks for the reference.\r\nIndeed, the common subexpression elimination seems to eliminate these duplicates (if I did follow the code correctly).\r\nI would argue, that there still is benefit in removing these duplicates in the optimizer. If nothing else, then it at least would clean up the plan - when I saw the duplicates in the plan, then it prompted me to try to rewrite the query in such way that it would avoid them.\r\nAlso the subexpression cache has a limited size (although configurable) and this would reduce the runtime overhead of cache lookups.\r\n\r\n ",
        "createdAt" : "2021-03-06T14:01:13Z",
        "updatedAt" : "2021-03-06T14:34:25Z",
        "lastEditedBy" : "a8e23d47-3ae4-4385-848c-38a216d1bd08",
        "tags" : [
        ]
      }
    ],
    "commit" : "624a41dd5e0232c746c62629bbb4983259ea9b0c",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +949,953 @@\n/**\n * Replaces duplicate window expressions with an alias in a Project above the Window node.\n */\nobject RemoveDuplicateWindowExprs extends Rule[LogicalPlan] {"
  },
  {
    "id" : "bb41c349-5166-48b0-b718-1c49c514a933",
    "prId" : 31740,
    "prUrl" : "https://github.com/apache/spark/pull/31740#pullrequestreview-606091871",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d9a66b1f-9f4c-4505-a1ca-ba14df935760",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Shall we do the same thing for project and aggregate? Another big idea is to re-implement subexpression elimination in the plan level.",
        "createdAt" : "2021-03-08T09:16:12Z",
        "updatedAt" : "2021-03-08T09:16:12Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "624a41dd5e0232c746c62629bbb4983259ea9b0c",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +951,955 @@ * Replaces duplicate window expressions with an alias in a Project above the Window node.\n */\nobject RemoveDuplicateWindowExprs extends Rule[LogicalPlan] {\n  def apply(plan: LogicalPlan): LogicalPlan = plan transform {\n    case w @ Window(wes, _, _, _) if wes.length > 1 =>"
  },
  {
    "id" : "6e5d187d-dcda-48d6-8b95-bd1eaa239463",
    "prId" : 31739,
    "prUrl" : "https://github.com/apache/spark/pull/31739#pullrequestreview-618928946",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9eafbf53-27fb-4a17-9d7f-a194bbf8d8c5",
        "parentId" : null,
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "Thanks @wangyum for adding this. I think there might be a list of operators safe to push through besides `Project` - e.g. `Sort`, `RepartitionByExpression`, `ScriptTransformation`, etc.\r\n\r\nShall we add push down through `Project` separately? It should not be only restricted to `Project(Join)` right? It can be `Project(OtherOperator)` as well?\r\n\r\n```\r\ncase LocalLimit(exp, p: Project) =>\r\n  LocalLimit(exp, p.copy(maybePushLocalLimit(exp, _)))\r\n```",
        "createdAt" : "2021-03-04T21:38:33Z",
        "updatedAt" : "2021-03-29T13:38:22Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      },
      {
        "id" : "6e3f2501-3eb4-45eb-9c1b-e03f4b414d3b",
        "parentId" : "9eafbf53-27fb-4a17-9d7f-a194bbf8d8c5",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "I change it to `case LocalLimit(exp, project @ Project(_, child)) if !child.isInstanceOf[LeafNode] =>` to avoid do some useless work if direct pushdown limit through Project:\r\n```\r\n=== Applying Rule org.apache.spark.sql.catalyst.optimizer.LimitPushDown ===\r\n GlobalLimit 5                                                GlobalLimit 5\r\n +- LocalLimit 5                                              +- LocalLimit 5\r\n    +- Project [a#0]                                             +- Project [a#0]\r\n!      +- Join LeftOuter, ((a#0 = x#3) AND (b#1 = y#4))             +- LocalLimit 5\r\n!         :- LocalLimit 5                                              +- Join LeftOuter, ((a#0 = x#3) AND (b#1 = y#4))\r\n!         :  +- Project [a#0, b#1]                                        :- LocalLimit 5\r\n!         :     +- Relation default.t1[a#0,b#1,c#2] parquet               :  +- Project [a#0, b#1]\r\n!         +- Project [x#3, y#4]                                           :     +- LocalLimit 5\r\n!            +- Filter (isnotnull(x#3) AND isnotnull(y#4))                :        +- Relation default.t1[a#0,b#1,c#2] parquet\r\n!               +- Relation default.t2[x#3,y#4,z#5] parquet               +- Project [x#3, y#4]\r\n!                                                                            +- Filter (isnotnull(x#3) AND isnotnull(y#4))\r\n!                                                                               +- Relation default.t2[x#3,y#4,z#5] parquet\r\n\r\n\r\n=== Applying Rule org.apache.spark.sql.catalyst.optimizer.CollapseProject ===\r\n GlobalLimit 5                                                            GlobalLimit 5\r\n +- LocalLimit 5                                                          +- LocalLimit 5\r\n!   +- Project [a#0]                                                         +- LocalLimit 5\r\n!      +- LocalLimit 5                                                          +- Project [a#0]\r\n!         +- Project [a#0]                                                         +- Join LeftOuter, ((a#0 = x#3) AND (b#1 = y#4))\r\n!            +- Join LeftOuter, ((a#0 = x#3) AND (b#1 = y#4))                         :- LocalLimit 5\r\n!               :- LocalLimit 5                                                       :  +- LocalLimit 5\r\n!               :  +- Project [a#0, b#1]                                              :     +- Project [a#0, b#1]\r\n!               :     +- LocalLimit 5                                                 :        +- Relation default.t1[a#0,b#1,c#2] parquet\r\n!               :        +- Project [a#0, b#1]                                        +- Project [x#3, y#4]\r\n!               :           +- Relation default.t1[a#0,b#1,c#2] parquet                  +- Filter (isnotnull(x#3) AND isnotnull(y#4))\r\n!               +- Project [x#3, y#4]                                                       +- Relation default.t2[x#3,y#4,z#5] parquet\r\n!                  +- Filter (isnotnull(x#3) AND isnotnull(y#4))          \r\n!                     +- Relation default.t2[x#3,y#4,z#5] parquet         \r\n           \r\n=== Applying Rule org.apache.spark.sql.catalyst.optimizer.EliminateLimits ===\r\n GlobalLimit 5                                                      GlobalLimit 5\r\n!+- LocalLimit 5                                                    +- LocalLimit least(5, 5)\r\n!   +- LocalLimit 5                                                    +- Project [a#0]\r\n!      +- Project [a#0]                                                   +- Join LeftOuter, ((a#0 = x#3) AND (b#1 = y#4))\r\n!         +- Join LeftOuter, ((a#0 = x#3) AND (b#1 = y#4))                   :- LocalLimit least(5, 5)\r\n!            :- LocalLimit 5                                                 :  +- Project [a#0, b#1]\r\n!            :  +- LocalLimit 5                                              :     +- Relation default.t1[a#0,b#1,c#2] parquet\r\n!            :     +- Project [a#0, b#1]                                     +- Project [x#3, y#4]\r\n!            :        +- Relation default.t1[a#0,b#1,c#2] parquet               +- Filter (isnotnull(x#3) AND isnotnull(y#4))\r\n!            +- Project [x#3, y#4]                                                 +- Relation default.t2[x#3,y#4,z#5] parquet\r\n!               +- Filter (isnotnull(x#3) AND isnotnull(y#4))       \r\n!                  +- Relation default.t2[x#3,y#4,z#5] parquet      \r\n \r\n```",
        "createdAt" : "2021-03-05T00:52:58Z",
        "updatedAt" : "2021-03-29T13:38:22Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      },
      {
        "id" : "a06e76c3-e24f-4559-9a93-1f37f051ed06",
        "parentId" : "9eafbf53-27fb-4a17-9d7f-a194bbf8d8c5",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Sounds like a good idea. We can have an allowlist of unary operators.",
        "createdAt" : "2021-03-22T15:47:44Z",
        "updatedAt" : "2021-03-29T13:38:22Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "1e7d73ff-c0ec-4ba9-adb3-65c93af7b3de",
        "parentId" : "9eafbf53-27fb-4a17-9d7f-a194bbf8d8c5",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "It will introduce useless pushdown even only allow `Join`.\r\n```scala\r\n    case LocalLimit(exp, p: Project) if p.child.isInstanceOf[Join] =>\r\n      LocalLimit(exp, p.copy(child = maybePushLocalLimit(exp, p.child)))\r\n```\r\nFor example:\r\n```scala\r\nspark.range(200L).selectExpr(\"id AS a\", \"id AS b\").write.saveAsTable(\"t1\")\r\nspark.range(300L).selectExpr(\"id AS x\", \"id AS y\").write.saveAsTable(\"t2\")\r\nspark.sql(\"SELECT 1 FROM t1 INNER JOIN t2 ON a = x limit 10\").explain(true)\r\n```\r\n\r\n```\r\n== Optimized Logical Plan ==\r\nGlobalLimit 10\r\n+- LocalLimit 10\r\n   +- Project [1 AS 1#20]\r\n      +- LocalLimit 10\r\n         +- Project\r\n            +- Join Inner, (a#16L = x#18L)\r\n               :- Project [a#16L]\r\n               :  +- Filter isnotnull(a#16L)\r\n               :     +- Relation default.t1[a#16L,b#17L] parquet\r\n               +- Project [x#18L]\r\n                  +- Filter isnotnull(x#18L)\r\n                     +- Relation default.t2[x#18L,y#19L] parquet\r\n\r\n== Physical Plan ==\r\nAdaptiveSparkPlan isFinalPlan=false\r\n+- CollectLimit 10\r\n   +- Project [1 AS 1#20]\r\n      +- LocalLimit 10\r\n         +- Project\r\n            +- BroadcastHashJoin [a#16L], [x#18L], Inner, BuildLeft, false\r\n               :- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [id=#68]\r\n               :  +- Filter isnotnull(a#16L)\r\n               :     +- FileScan parquet default.t1[a#16L] \r\n               +- Filter isnotnull(x#18L)\r\n                  +- FileScan parquet default.t2[x#18L]\r\n\r\n```\r\n\r\nAnother example is TPC-DS q32:\r\nhttps://github.com/apache/spark/blob/66f5a42ca5d259038f0749ae2b9a04cc2f658880/sql/core/src/test/resources/tpcds/q32.sql#L1-L15\r\n",
        "createdAt" : "2021-03-22T23:59:31Z",
        "updatedAt" : "2021-03-29T13:38:22Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      },
      {
        "id" : "dd5fbc66-05e0-46ec-b03c-cdabe748b348",
        "parentId" : "9eafbf53-27fb-4a17-9d7f-a194bbf8d8c5",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "@c21 @maropu @cloud-fan \r\nI think `Project` is the most used, and this will not introduce useless `LocalLimit`.",
        "createdAt" : "2021-03-23T07:53:58Z",
        "updatedAt" : "2021-03-29T13:38:22Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      },
      {
        "id" : "a95fa0e8-3ce3-48f1-b22b-d493e8a3ffe3",
        "parentId" : "9eafbf53-27fb-4a17-9d7f-a194bbf8d8c5",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "@wangyum and @cloud-fan - I think one followup can be to pass `LocalLimit` through eligible operators. We don't add more `LocalLimit` operators, but push the existing `LocalLimit` further down in query plan.",
        "createdAt" : "2021-03-23T18:20:35Z",
        "updatedAt" : "2021-03-29T13:38:22Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      }
    ],
    "commit" : "4c7bc54c69ca347c01d84ac8d2ab54033d96aadf",
    "line" : 49,
    "diffHunk" : "@@ -1,1 +674,678 @@      LocalLimit(exp, pushLocalLimitThroughJoin(exp, join))\n    // There is a Project between LocalLimit and Join if they do not have the same output.\n    case LocalLimit(exp, project @ Project(_, join: Join)) =>\n      LocalLimit(exp, project.copy(child = pushLocalLimitThroughJoin(exp, join)))\n  }"
  },
  {
    "id" : "787dc104-2184-4703-95b1-b7be63642260",
    "prId" : 31677,
    "prUrl" : "https://github.com/apache/spark/pull/31677#pullrequestreview-622714876",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "16cb3ad1-48a6-425c-9ae4-d3751a88bf5c",
        "parentId" : null,
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "Could we move `windowsCompatible` before `apply`. We usually put the private function before the `apply`. See `TransposeWindow`.",
        "createdAt" : "2021-03-28T02:13:24Z",
        "updatedAt" : "2021-03-28T13:48:42Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      }
    ],
    "commit" : "7097ec3c724356a0d4c226b1222845b6df738e39",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +951,955 @@ */\nobject CollapseWindow extends Rule[LogicalPlan] {\n  private def windowsCompatible(w1: Window, w2: Window): Boolean = {\n    w1.partitionSpec == w2.partitionSpec &&\n      w1.orderSpec == w2.orderSpec &&"
  },
  {
    "id" : "651c8635-ce67-4aa5-a7d7-c475cb37dac1",
    "prId" : 31656,
    "prUrl" : "https://github.com/apache/spark/pull/31656#pullrequestreview-599964928",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9a6ec167-10e1-429b-bcf8-a3cd8edb0ab7",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "We need to keep `LogicalPlan` itself? We cannot use `semanticHash` instead?",
        "createdAt" : "2021-02-26T12:31:04Z",
        "updatedAt" : "2021-03-01T07:38:01Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "40586090-8d54-4ce6-a16e-210576b3eb42",
        "parentId" : "9a6ec167-10e1-429b-bcf8-a3cd8edb0ab7",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "I think `semanticHash` also works. Can use it.",
        "createdAt" : "2021-02-26T21:43:47Z",
        "updatedAt" : "2021-03-01T07:38:01Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "ef057af3-3a1f-4bda-bbb8-66fc68f08843",
        "parentId" : "9a6ec167-10e1-429b-bcf8-a3cd8edb0ab7",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Oh, rethinking about it, isn't `canonicalized` more reliable? It is less likely, but hash code might collision.",
        "createdAt" : "2021-02-26T21:46:42Z",
        "updatedAt" : "2021-03-01T07:38:01Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "6a420d497d91be512ad3e0317756dcc7f96e605d",
    "line" : 31,
    "diffHunk" : "@@ -1,1 +530,534 @@  private def simplifyUnion(u: Union): LogicalPlan = {\n    val uniqueChildren = mutable.ArrayBuffer.empty[LogicalPlan]\n    val uniqueChildrenKey = mutable.HashSet.empty[LogicalPlan]\n\n    u.children.foreach { c =>"
  },
  {
    "id" : "8addd2f9-6560-4fb3-9915-6b677d7f5f43",
    "prId" : 31656,
    "prUrl" : "https://github.com/apache/spark/pull/31656#pullrequestreview-600061245",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ecea004c-94ff-4d85-8f33-8d2ab79d97d6",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Does the added test in `DataFrameSetOperationsSuite` fail without this reorder? The change itself looks reasonable though.",
        "createdAt" : "2021-02-26T12:36:07Z",
        "updatedAt" : "2021-03-01T07:38:01Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "591c08b1-f6a4-4d85-a189-2b62aa196546",
        "parentId" : "ecea004c-94ff-4d85-8f33-8d2ab79d97d6",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Added test is fine. But there will be an [error](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/135486/testReport/org.apache.spark.sql/SQLQueryTestSuite/subquery_in_subquery_in_set_operations_sql/) of \"Once strategy's idempotence is broken for batch Union\".\r\n\r\nIt is because `CombineUnions` might combine two or more `Union` and they have redundant children under `Distinct`. So the rule order `RemoveNoopUnion` -> `CombineUnions` is not idempotence for Once strategy. ",
        "createdAt" : "2021-02-26T17:25:25Z",
        "updatedAt" : "2021-03-01T07:38:01Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "b915524d-ea7d-4447-ad9a-36d44a6d00e8",
        "parentId" : "ecea004c-94ff-4d85-8f33-8d2ab79d97d6",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Ah, I see.",
        "createdAt" : "2021-02-27T01:43:04Z",
        "updatedAt" : "2021-03-01T07:38:01Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "6a420d497d91be512ad3e0317756dcc7f96e605d",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +159,163 @@      RemoveNoopOperators,\n      CombineUnions,\n      RemoveNoopUnion) ::\n    Batch(\"OptimizeLimitZero\", Once,\n      OptimizeLimitZero) ::"
  },
  {
    "id" : "5ec3057c-722f-4bb9-bcfd-17d82c12f028",
    "prId" : 31656,
    "prUrl" : "https://github.com/apache/spark/pull/31656#pullrequestreview-600450725",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8d450b3d-d965-4066-9948-b8c3878c8952",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "We need plan canonicalization here? It looks enough to check if the output is the same just like `LogicalPlan.sameOutput`?",
        "createdAt" : "2021-02-27T01:51:19Z",
        "updatedAt" : "2021-03-01T07:38:01Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "a36c23bd-4fce-45f2-aad6-92ffe1e6a8f9",
        "parentId" : "8d450b3d-d965-4066-9948-b8c3878c8952",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "`canonicalized` should cover more cases than `sameOutput`. Note that by definition two plans with same `canonicalized` plans will always evaluate to the same result. But their outputs might not be the same.",
        "createdAt" : "2021-03-01T07:36:11Z",
        "updatedAt" : "2021-03-01T07:38:01Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "6a420d497d91be512ad3e0317756dcc7f96e605d",
    "line" : 34,
    "diffHunk" : "@@ -1,1 +533,537 @@\n    u.children.foreach { c =>\n      val key = removeAliasOnlyProject(c).canonicalized\n      if (!uniqueChildrenKey.contains(key)) {\n        uniqueChildren += c"
  },
  {
    "id" : "f8b89e8c-7417-4738-9964-ed6fb518f074",
    "prId" : 31630,
    "prUrl" : "https://github.com/apache/spark/pull/31630#pullrequestreview-597395340",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "219add80-8145-4053-80c7-20d44d893863",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "hm, in this case, we need the join itself?\r\n```\r\nscala> sql(\"select * from l1\").show()\r\n+----+\r\n|  id|\r\n+----+\r\n|   1|\r\n|   2|\r\n|null|\r\n+----+\r\n\r\n\r\nscala> sql(\"select * from r1\").show()\r\n+----+\r\n|  id|\r\n+----+\r\n|   2|\r\n|null|\r\n+----+\r\n\r\n\r\nscala> sql(\"select * from l1 left semi join r1\").show()\r\n+----+\r\n|  id|\r\n+----+\r\n|   1|\r\n|   2|\r\n|null|\r\n+----+\r\n\r\n\r\nscala> sql(\"select * from l1 left anti join r1\").show()\r\n+---+\r\n| id|\r\n+---+\r\n+---+\r\n```",
        "createdAt" : "2021-02-24T04:09:38Z",
        "updatedAt" : "2021-02-24T06:42:46Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "3005e518-a212-45a4-945a-7071fe40da58",
        "parentId" : "219add80-8145-4053-80c7-20d44d893863",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "I think we still need. Whether to output all rows or nothing, is depending on whether right side is empty, and this can only be known during runtime.",
        "createdAt" : "2021-02-24T04:19:54Z",
        "updatedAt" : "2021-02-24T06:42:46Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      },
      {
        "id" : "fb7db00f-35f4-4cb0-9276-9fd5850d596e",
        "parentId" : "219add80-8145-4053-80c7-20d44d893863",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "@maropu - this actually reminds me whether we can further optimize during runtime, and I found I already did it for LEFT SEMI with AQE - https://github.com/apache/spark/pull/29484 . Similarly for LEFT ANTI join without condition, we can convert join logical plan node to an empty relation if right build side is not empty. Will submit a followup PR tomorrow.\r\n\r\nIn addition, after taking a deep look at `BroadcastNestedLoopJoinExec` (never looked closely to that because it's not popular in our environment), I found many places that we can optimize:\r\n* populate `outputOrdering` and `outputPartitioning` when possible to avoid shuffle/sort in later stage.\r\n* shortcut for `LEFT SEMI/ANTI` in `defaultJoin()` as we don't need to look through all rows when there's no join condition.\r\n* code-gen the operator.\r\n\r\nI will file an umbrella JIRA with minor priority and do it gradually.",
        "createdAt" : "2021-02-24T09:02:42Z",
        "updatedAt" : "2021-02-24T09:02:43Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      },
      {
        "id" : "746ce87f-5db7-44aa-964a-873bb910af96",
        "parentId" : "219add80-8145-4053-80c7-20d44d893863",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "> Similarly for LEFT ANTI join without condition, we can convert join logical plan node to an empty relation if right build side is not empty. Will submit a followup PR tomorrow.\r\n\r\nAh, I see. That sounds reasonable.  Nice idea, @c21 .",
        "createdAt" : "2021-02-24T11:52:24Z",
        "updatedAt" : "2021-02-24T11:52:25Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "22bfd5e13ffb5b51a6a29851161146a1af6a5666",
    "line" : 37,
    "diffHunk" : "@@ -1,1 +561,565 @@            right = maybePushLocalLimit(exp, right))\n        case LeftSemi | LeftAnti if conditionOpt.isEmpty =>\n          join.copy(left = maybePushLocalLimit(exp, left))\n        case _ => join\n      }"
  },
  {
    "id" : "dc09c4d1-1ab6-40b0-93aa-16e2f242e559",
    "prId" : 31595,
    "prUrl" : "https://github.com/apache/spark/pull/31595#pullrequestreview-594798657",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1e720cfa-b11b-48c6-a2a6-a0a64d80a326",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "If we handle only two cases (`Distinct` and `Deduplicate`), let's say explicitly `Distinct/Deduplicate` instead of saying `Distinct like operators`. In general, we use a `whitelist` approach.",
        "createdAt" : "2021-02-21T01:34:49Z",
        "updatedAt" : "2021-02-25T04:01:53Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "b5d704112549310e2592aad21bd8308386ff0cf8",
    "line" : 49,
    "diffHunk" : "@@ -1,1 +540,544 @@      removeUnion(u).map(c => d.withNewChildren(Seq(c))).getOrElse(d)\n\n    case d @ Deduplicate(_, u: Union) =>\n      removeUnion(u).map(c => d.withNewChildren(Seq(c))).getOrElse(d)\n  }"
  },
  {
    "id" : "cd503e49-5b2d-4c35-a96f-854af7e378b6",
    "prId" : 31595,
    "prUrl" : "https://github.com/apache/spark/pull/31595#pullrequestreview-595772582",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f1ae7f79-873b-4f72-9b4f-d33ffbf772a8",
        "parentId" : null,
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "I am wondering why this is needed, given we already have `Eliminate no-op Projects` below at line 511. Shouldn't this kind of alias-only project already be removed by that?",
        "createdAt" : "2021-02-22T07:26:39Z",
        "updatedAt" : "2021-02-25T04:01:53Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      },
      {
        "id" : "73b3a6a6-4dbb-4ce7-9ec1-7b245d323e49",
        "parentId" : "f1ae7f79-873b-4f72-9b4f-d33ffbf772a8",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "Also just FYI there's some ongoing change for this same rule - https://github.com/apache/spark/pull/31538 , ",
        "createdAt" : "2021-02-22T07:31:56Z",
        "updatedAt" : "2021-02-25T04:01:53Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      },
      {
        "id" : "f73560d8-3437-40a8-8555-3590cd274c9d",
        "parentId" : "f1ae7f79-873b-4f72-9b4f-d33ffbf772a8",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Alias-only project is not no-op project here. It will change the outputs.",
        "createdAt" : "2021-02-22T20:53:46Z",
        "updatedAt" : "2021-02-25T04:01:53Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "b5d704112549310e2592aad21bd8308386ff0cf8",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +511,515 @@   * from its child.\n   */\n  private def removeAliasOnlyProject(plan: LogicalPlan): LogicalPlan = plan match {\n    case p @ Project(projectList, child) =>\n      val aliasOnly = projectList.length == child.output.length &&"
  },
  {
    "id" : "15d7c702-ef7d-4c5b-971b-0639a9e8e487",
    "prId" : 31595,
    "prUrl" : "https://github.com/apache/spark/pull/31595#pullrequestreview-595993938",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "43785d49-1e00-4c94-958a-8c2ebe399a8a",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Thank you for splitting the rule!",
        "createdAt" : "2021-02-23T04:48:33Z",
        "updatedAt" : "2021-02-25T04:01:53Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "b5d704112549310e2592aad21bd8308386ff0cf8",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +158,162 @@    Batch(\"Union\", Once,\n      RemoveNoopOperators,\n      RemoveNoopUnion,\n      CombineUnions) ::\n    Batch(\"OptimizeLimitZero\", Once,"
  },
  {
    "id" : "35bc364f-2a18-4e45-8271-101990d787fa",
    "prId" : 31595,
    "prUrl" : "https://github.com/apache/spark/pull/31595#pullrequestreview-596999205",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a3576640-7c17-440e-b7ca-1608622c29d8",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "How about a `Aggregate` case?",
        "createdAt" : "2021-02-24T00:49:44Z",
        "updatedAt" : "2021-02-25T04:01:53Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "0b377946-7976-451e-b372-2659b01fd9e5",
        "parentId" : "a3576640-7c17-440e-b7ca-1608622c29d8",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "You mean a no-op Aggregation like `select key as key2, value from table group by key value`?",
        "createdAt" : "2021-02-24T01:32:29Z",
        "updatedAt" : "2021-02-25T04:01:53Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "b480d3a1-1b45-4cc1-9ae7-d23f46c8f1d0",
        "parentId" : "a3576640-7c17-440e-b7ca-1608622c29d8",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "I thought it was like this:\r\n```\r\nscala> sql(\"select distinct * from (select id from t group by id union select id as key from t group by id)\").explain(true)\r\n...\r\n== Analyzed Logical Plan ==\r\nid: bigint\r\nDistinct\r\n+- Project [id#8L]\r\n   +- SubqueryAlias __auto_generated_subquery_name\r\n      +- Distinct\r\n         +- Union false, false\r\n            :- Aggregate [id#8L], [id#8L]\r\n            :  +- SubqueryAlias spark_catalog.default.t\r\n            :     +- Relation[id#8L] parquet\r\n            +- Aggregate [id#8L], [id#8L AS key#7L]\r\n               +- SubqueryAlias spark_catalog.default.t\r\n                  +- Relation[id#8L] parquet\r\n\r\n== Optimized Logical Plan ==\r\nAggregate [id#8L], [id#8L]\r\n+- Aggregate [id#8L], [id#8L]\r\n   +- Union false, false\r\n      :- Aggregate [id#8L], [id#8L]\r\n      :  +- Relation[id#8L] parquet\r\n      +- Aggregate [id#8L], [id#8L AS key#7L]\r\n         +- Relation[id#8L] parquet\r\n```",
        "createdAt" : "2021-02-24T01:43:12Z",
        "updatedAt" : "2021-02-25T04:01:53Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "88481667-a10c-4c16-bdb9-94d679bab002",
        "parentId" : "a3576640-7c17-440e-b7ca-1608622c29d8",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "yea, that is what I said a no-op aggregate. I think it is okay to add it. Maybe I can add it in follow up.",
        "createdAt" : "2021-02-24T02:03:00Z",
        "updatedAt" : "2021-02-25T04:01:53Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "50b9d306-22e7-4f53-9c1b-79c2038dbaf7",
        "parentId" : "a3576640-7c17-440e-b7ca-1608622c29d8",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "yea, sgtm.",
        "createdAt" : "2021-02-24T02:06:30Z",
        "updatedAt" : "2021-02-25T04:01:53Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "b5d704112549310e2592aad21bd8308386ff0cf8",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +512,516 @@   */\n  private def removeAliasOnlyProject(plan: LogicalPlan): LogicalPlan = plan match {\n    case p @ Project(projectList, child) =>\n      val aliasOnly = projectList.length == child.output.length &&\n        projectList.zip(child.output).forall {"
  },
  {
    "id" : "5ffa3d52-b8e9-43f4-8c39-965ef6162b73",
    "prId" : 31567,
    "prUrl" : "https://github.com/apache/spark/pull/31567#pullrequestreview-590873109",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e5b45719-a23b-4f5f-81af-367747d12dc6",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Shall we update the comments here too to describe the case when we push the limit with cross join?",
        "createdAt" : "2021-02-16T04:13:14Z",
        "updatedAt" : "2021-02-23T03:20:22Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "e0915b93a6cd528b6c07e603b98762a1134a7640",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +545,549 @@    // below FULL OUTER JOIN in the general case without a more invasive rewrite.\n    // We also need to ensure that this limit pushdown rule will not eventually introduce limits\n    // on both sides if it is applied multiple times. Therefore:\n    //   - If one side is already limited, stack another limit on top if the new limit is smaller.\n    //     The redundant limit will be collapsed by the CombineLimits rule."
  },
  {
    "id" : "c7488cbe-b793-4763-b953-77b29f42cd47",
    "prId" : 31567,
    "prUrl" : "https://github.com/apache/spark/pull/31567#pullrequestreview-591899952",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "86ad22ad-c3cf-48c2-8492-322dd541adc1",
        "parentId" : null,
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "It seems that we can also push down limit into left side, for LEFT SEMI and LEFT ANTI join, right?\r\nI can create a minor PR if it's not on your plan @wangyum , thanks.",
        "createdAt" : "2021-02-17T05:42:03Z",
        "updatedAt" : "2021-02-23T03:20:22Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      },
      {
        "id" : "2659c7b5-fd93-4d4d-a599-0266c680ed39",
        "parentId" : "86ad22ad-c3cf-48c2-8492-322dd541adc1",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "It seems we can not pushdown `LEFT SEMI JOIN`, for example:\r\n```scala\r\nspark.range(20).selectExpr(\"id % 10 as id\").repartition(1).write.saveAsTable(\"t1\")\r\nspark.range(5, 9, 1).repartition(1).write.saveAsTable(\"t2\")\r\nval df = spark.sql(\"select * from t1 LEFT SEMI JOIN t2 on t1.id = t2.id limit 3\")\r\ndf.explain()\r\ndf.show\r\n```\r\n\r\nCurrent:\r\n```\r\n== Physical Plan ==\r\nAdaptiveSparkPlan isFinalPlan=false\r\n+- CollectLimit 3\r\n   +- BroadcastHashJoin [id#10L], [id#11L], LeftSemi, BuildRight, false\r\n      :- Filter isnotnull(id#10L)\r\n      :  +- FileScan parquet default.t1[id#10L] Batched: true, DataFilters: [isnotnull(id#10L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/yumwang/spark/SPARK-28169/spark-warehouse/org.apache.spark..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:bigint>\r\n      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [id=#69]\r\n         +- Filter isnotnull(id#11L)\r\n            +- FileScan parquet default.t2[id#11L] Batched: true, DataFilters: [isnotnull(id#11L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/yumwang/spark/SPARK-28169/spark-warehouse/org.apache.spark..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:bigint>\r\n\r\n\r\n+---+\r\n| id|\r\n+---+\r\n|  5|\r\n|  6|\r\n|  7|\r\n+---+\r\n```\r\n\r\nPushdown:\r\n```\r\n== Physical Plan ==\r\nAdaptiveSparkPlan isFinalPlan=false\r\n+- CollectLimit 3\r\n   +- BroadcastHashJoin [id#10L], [id#11L], LeftSemi, BuildRight, false\r\n      :- LocalLimit 3\r\n      :  +- Filter isnotnull(id#10L)\r\n      :     +- LocalLimit 3\r\n      :        +- FileScan parquet default.t1[id#10L] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/yumwang/spark/SPARK-28169/spark-warehouse/org.apache.spark..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:bigint>\r\n      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [id=#77]\r\n         +- Filter isnotnull(id#11L)\r\n            +- FileScan parquet default.t2[id#11L] Batched: true, DataFilters: [isnotnull(id#11L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/yumwang/spark/SPARK-28169/spark-warehouse/org.apache.spark..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:bigint>\r\n\r\n\r\n+---+\r\n| id|\r\n+---+\r\n+---+\r\n```",
        "createdAt" : "2021-02-17T07:13:10Z",
        "updatedAt" : "2021-02-23T03:20:22Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      },
      {
        "id" : "aed680e6-a507-41ae-a477-49a4419b6d45",
        "parentId" : "86ad22ad-c3cf-48c2-8492-322dd541adc1",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "@wangyum - yes, you are right. My bad. LEFT OUTER/RIGHT OUTER join will output the left/right side rows anyway no matter of being matched or not. So they are safe for limit push down, but not LEFT SEMI/LEFT ANTI.",
        "createdAt" : "2021-02-17T07:20:19Z",
        "updatedAt" : "2021-02-23T03:20:22Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      },
      {
        "id" : "0e55ea6a-b10c-44f6-9696-f99756f2358b",
        "parentId" : "86ad22ad-c3cf-48c2-8492-322dd541adc1",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "Actually, what about LEFT SEMI / LEFT ANTI join without condition? They should be planned into physical operator `BroadcastNestedLoopJoin` instead. ",
        "createdAt" : "2021-02-17T07:28:02Z",
        "updatedAt" : "2021-02-23T03:20:22Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      }
    ],
    "commit" : "e0915b93a6cd528b6c07e603b98762a1134a7640",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +551,555 @@      val newJoin = joinType match {\n        case RightOuter => join.copy(right = maybePushLocalLimit(exp, right))\n        case LeftOuter => join.copy(left = maybePushLocalLimit(exp, left))\n        case _: InnerLike if conditionOpt.isEmpty =>\n          join.copy("
  },
  {
    "id" : "0076c246-974b-4e8f-99ac-b0f2bff2a585",
    "prId" : 31538,
    "prUrl" : "https://github.com/apache/spark/pull/31538#pullrequestreview-588906078",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1d2698ad-adfb-4ef6-9d75-b92f193dfc6f",
        "parentId" : null,
        "authorId" : "a8e23d47-3ae4-4385-848c-38a216d1bd08",
        "body" : "Would this affect other places where the `sameOutput` method is used? It kinda looks like fixing a symptom and the real problem is a bit deeper. The `sameOutput` is used only in few places, but it internally uses `semanticEquals` that is very common.",
        "createdAt" : "2021-02-11T19:16:22Z",
        "updatedAt" : "2021-02-11T19:16:22Z",
        "lastEditedBy" : "a8e23d47-3ae4-4385-848c-38a216d1bd08",
        "tags" : [
        ]
      },
      {
        "id" : "2595b143-1e98-4cbf-bb82-86b263b68ad4",
        "parentId" : "1d2698ad-adfb-4ef6-9d75-b92f193dfc6f",
        "authorId" : "a8e23d47-3ae4-4385-848c-38a216d1bd08",
        "body" : "In the `LogicalPlan` there is this comment:\r\n```\r\n  /**\r\n   * This method checks if the same `ExprId` refers to an unique attribute in a plan tree.\r\n   * Some plan transformers (e.g., `RemoveNoopOperators`) rewrite logical\r\n   * plans based on this assumption.\r\n   */\r\n  def checkIfExprIdsAreGloballyUnique(plan: LogicalPlan): Boolean = {\r\n    checkIfSameExprIdNotReused(plan) && hasUniqueExprIdsForOutput(plan)\r\n  }\r\n```\r\n\r\nIt sounds like the `RemoveNoopOperators` is correct and the deeper issue is with the `ExprId` getting reused.\r\n",
        "createdAt" : "2021-02-11T19:22:28Z",
        "updatedAt" : "2021-02-11T19:22:28Z",
        "lastEditedBy" : "a8e23d47-3ae4-4385-848c-38a216d1bd08",
        "tags" : [
        ]
      }
    ],
    "commit" : "9212a85ddb82ffc2178b3e8e6c361eb18bfb3e6a",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +496,500 @@    // Eliminate no-op Projects\n    case p @ Project(projectList, child)\n      if projectList.forall(isAttribute) && child.sameOutput(p) => child\n\n    // Eliminate no-op Window"
  },
  {
    "id" : "69de1e81-6863-4e87-8ab9-6b05c89ab4c8",
    "prId" : 31404,
    "prUrl" : "https://github.com/apache/spark/pull/31404#pullrequestreview-590023719",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ed505b24-5011-432f-8095-cdb6ab531db0",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Why we need move this rule?",
        "createdAt" : "2021-02-10T20:44:01Z",
        "updatedAt" : "2021-02-19T02:38:37Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "425e9634-3a55-4873-8e78-fbc0e7ec9720",
        "parentId" : "ed505b24-5011-432f-8095-cdb6ab531db0",
        "authorId" : "ab8f521d-7cf6-437f-8ea5-922bd77ef56b",
        "body" : "It needs to put the rule 'CombineUnions' before ReplaceDistinctWithAggregate and ReplaceDeduplicateWithAggregate, otherwise these two rules will replace Distinct/Deduplicate to Aggregate and lead to rule 'CombineUnions' is invalid. On the other hand putting the rule 'ReplaceDeduplicateWithAggregate' in batch 'Replace Operators' is more intuitional.",
        "createdAt" : "2021-02-14T08:06:20Z",
        "updatedAt" : "2021-02-19T02:38:37Z",
        "lastEditedBy" : "ab8f521d-7cf6-437f-8ea5-922bd77ef56b",
        "tags" : [
        ]
      }
    ],
    "commit" : "1385f2b5963a6ec88dc9d9881f2a4bdd5c735e7a",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +185,189 @@      ReplaceExceptWithAntiJoin,\n      ReplaceDistinctWithAggregate,\n      ReplaceDeduplicateWithAggregate) ::\n    Batch(\"Aggregate\", fixedPoint,\n      RemoveLiteralFromGroupExpressions,"
  },
  {
    "id" : "1ae319f6-a6bf-4db9-beed-78ccd8b3ea53",
    "prId" : 31045,
    "prUrl" : "https://github.com/apache/spark/pull/31045#pullrequestreview-562925959",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8860136f-95ed-4779-a142-4d63bb0768c5",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Just out of curiosity, this rule can always make performance better? For example, how about the case where a right table has too many duplicates? ",
        "createdAt" : "2021-01-05T23:15:34Z",
        "updatedAt" : "2021-01-06T17:31:26Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "629e2fcd-3ec4-4d4f-8316-9bedb9124edb",
        "parentId" : "8860136f-95ed-4779-a142-4d63bb0768c5",
        "authorId" : "a8e23d47-3ae4-4385-848c-38a216d1bd08",
        "body" : "I benchmarked a bit and you are right - this can cause a performance regression. For example q87 took a 10% hit on sf100. I think it's best to close this PR and issue.",
        "createdAt" : "2021-01-06T18:02:08Z",
        "updatedAt" : "2021-01-06T18:02:08Z",
        "lastEditedBy" : "a8e23d47-3ae4-4385-848c-38a216d1bd08",
        "tags" : [
        ]
      }
    ],
    "commit" : "3bb7cd59433498c6b3d1d6df14d166b082e7d0ce",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +1618,1622 @@  def apply(plan: LogicalPlan): LogicalPlan = plan transformDown {\n    // The [[Distinct]] can be ignored if it is the right child of a left semi or anti join.\n    case j @ Join(_, Distinct(right), LeftSemiOrAnti(_), _, _) => j.copy(right = right)\n    case Distinct(child) => Aggregate(child.output, child.output, child)\n  }"
  },
  {
    "id" : "cf90cac8-ea43-4d58-a94b-d06ffe641fcb",
    "prId" : 31012,
    "prUrl" : "https://github.com/apache/spark/pull/31012#pullrequestreview-576449208",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c9d24cf5-ec20-41a3-b313-3c640eba5d22",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "This is the only non-testing change in this PR, but it's very minor and obvious.",
        "createdAt" : "2021-01-26T15:13:06Z",
        "updatedAt" : "2021-01-26T15:13:06Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "1aa22f055db370e77e506403666000e8e1e19f9c",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +419,423 @@    // may accidentally change the output schema name of the root plan.\n    case a @ Alias(attr: Attribute, name)\n      if (a.metadata == Metadata.empty || a.metadata == attr.metadata) &&\n        name == attr.name &&\n        !excludeList.contains(attr) &&"
  },
  {
    "id" : "5d55881a-5b57-429d-8226-e34aa9d5b5db",
    "prId" : 30558,
    "prUrl" : "https://github.com/apache/spark/pull/30558#pullrequestreview-553895404",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bcea21ff-4e55-4c78-9342-9935ddddafd4",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "Basically, what you want to do is to add an extension point/batch between heuristics-based optimizer and cost-based optimizer. \r\n\r\nThe batch name and comments look not good to me. We need a better name here. ",
        "createdAt" : "2020-12-02T04:16:49Z",
        "updatedAt" : "2020-12-02T04:16:49Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "5f0bdf32-f8a3-44e9-b322-219b789e3a96",
        "parentId" : "bcea21ff-4e55-4c78-9342-9935ddddafd4",
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "cc @maryannxue @hvanhovell ",
        "createdAt" : "2020-12-02T04:17:20Z",
        "updatedAt" : "2020-12-02T04:17:20Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "3f74984f-a65a-4d50-a0b9-a6ebdd096d82",
        "parentId" : "bcea21ff-4e55-4c78-9342-9935ddddafd4",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Could you propose a name then, @gatorsmile ?",
        "createdAt" : "2020-12-02T07:10:15Z",
        "updatedAt" : "2020-12-02T07:10:16Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "41526bb9-90a7-4530-8154-862507e7e9cf",
        "parentId" : "bcea21ff-4e55-4c78-9342-9935ddddafd4",
        "authorId" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "body" : "I am open to alternatives.",
        "createdAt" : "2020-12-02T21:44:11Z",
        "updatedAt" : "2020-12-02T21:44:11Z",
        "lastEditedBy" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "tags" : [
        ]
      },
      {
        "id" : "1d8bf356-0625-4240-8e76-1dba91d7eac9",
        "parentId" : "bcea21ff-4e55-4c78-9342-9935ddddafd4",
        "authorId" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "body" : "We should probably combine this batch with the one below: `earlyScanPushDownRules`, and give it a more general name similar to `extendedOperatorOptimizationRules`.",
        "createdAt" : "2020-12-09T18:59:44Z",
        "updatedAt" : "2020-12-09T18:59:45Z",
        "lastEditedBy" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "tags" : [
        ]
      },
      {
        "id" : "960ef1a3-9e04-467b-bb20-b404e60e31f7",
        "parentId" : "bcea21ff-4e55-4c78-9342-9935ddddafd4",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Ping, @aokolnychyi and @gatorsmile , for the above comment.",
        "createdAt" : "2020-12-09T19:20:54Z",
        "updatedAt" : "2020-12-09T19:20:55Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "4a2d6a35-5f59-4899-8b3e-7e42d6c68dd5",
        "parentId" : "bcea21ff-4e55-4c78-9342-9935ddddafd4",
        "authorId" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "body" : "Thanks for getting back, @maryannxue! I think that was the original suggestion we discussed [here](https://github.com/apache/spark/pull/29066#discussion_r530663744).\r\n\r\nHowever, `earlyScanPushDownRules` is exposed in a couple of places including the session builder. That's the reason I went with a new batch. I'll be happy to reconsider if we want to change all existing places.\r\n\r\nIt seems like we all agree that a better name is needed. Could you suggest one, @maryannxue?",
        "createdAt" : "2020-12-09T20:44:02Z",
        "updatedAt" : "2020-12-09T20:44:02Z",
        "lastEditedBy" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "tags" : [
        ]
      },
      {
        "id" : "634cd998-d3ae-4fe1-bd01-4965e44ef2f4",
        "parentId" : "bcea21ff-4e55-4c78-9342-9935ddddafd4",
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "This also needs to run before the early filter and pushdown because those rules need to transform any new DSv2Relation into DSv2ScanRelation.",
        "createdAt" : "2020-12-09T21:01:52Z",
        "updatedAt" : "2020-12-09T21:01:53Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      },
      {
        "id" : "b4862394-496b-43fc-96df-11561cd870bf",
        "parentId" : "bcea21ff-4e55-4c78-9342-9935ddddafd4",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "So `dataSourceRewriteRules` looks like definitely not the appropriate name that we should expose out as an API. This is merged to Spark 3.1. Shall we revert in Spark 3.1 and think about better naming since we all agree with having a better name? specially considering \"Alternatives to Breaking an API\" in https://spark.apache.org/versioning-policy.html.",
        "createdAt" : "2020-12-15T01:29:18Z",
        "updatedAt" : "2020-12-15T01:29:18Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "9978a849-dc1d-43df-b454-0c8d7fe65499",
        "parentId" : "bcea21ff-4e55-4c78-9342-9935ddddafd4",
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "It would be better to rename rather than remove it. If no one can come up with a better name, then the existing one is fine and we should use it. If there is a better name proposed, then we can simply port the rename in the 3.1 branch. No need to remove something that is useful to people that made it in before the branch was cut.",
        "createdAt" : "2020-12-15T01:40:39Z",
        "updatedAt" : "2020-12-15T01:40:40Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      },
      {
        "id" : "782e3f38-1c8b-425a-b559-f531050168f5",
        "parentId" : "bcea21ff-4e55-4c78-9342-9935ddddafd4",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@rdblue, it would be great if we can stick to having a good APIs as discussed and documented in \"Alternatives to Breaking an API\" in https://spark.apache.org/versioning-policy.html.\r\n\r\nTo clarify, I am _not_ suggesting to revert in `master` branch. I am suggesting to remove in `branch-3.1` alone to have a better designed API out. Keeping something only because it's useful to people is the exact reason why Spark happened to have a lot of cruft. We can't just remove them out for legacy reason and what we discussed in https://spark.apache.org/versioning-policy.html, but this one is not released out yet.",
        "createdAt" : "2020-12-15T01:46:17Z",
        "updatedAt" : "2020-12-15T01:46:52Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "4906e739-91de-4566-b25c-577b1a2f9c93",
        "parentId" : "bcea21ff-4e55-4c78-9342-9935ddddafd4",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I think we could have three options:\r\n\r\n- I personally more prefer to have any name that can address these comments above, and change it in `branch-3.1` too have a good API. I filed a blocker JIRA to rename this here SPARK-33784 in case we go this way.\r\n\r\n- If we're good with breaking it in minor releases since this is `@Unstable` API, then it works to me with AS-IS.\r\n\r\n- Otherwise, another option is to remove this in `branch-3.1`.\r\n\r\nI will take an action for the JIRA for which option we take.",
        "createdAt" : "2020-12-15T02:02:46Z",
        "updatedAt" : "2020-12-15T02:02:46Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "9299c935-dde0-445c-aad5-e4021d631ed2",
        "parentId" : "bcea21ff-4e55-4c78-9342-9935ddddafd4",
        "authorId" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "body" : "Something like `preCBORules`, @rdblue ?",
        "createdAt" : "2020-12-15T03:27:45Z",
        "updatedAt" : "2020-12-15T03:27:45Z",
        "lastEditedBy" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "tags" : [
        ]
      },
      {
        "id" : "6ed2a61a-8add-48d9-a548-92fa1556a909",
        "parentId" : "bcea21ff-4e55-4c78-9342-9935ddddafd4",
        "authorId" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "body" : "I'd fix the name and cherry-pick it to 3.1 rather than revert the change completely. Moreover, I'd suggest including the feature in full, not just the new batch. PR https://github.com/apache/spark/pull/30577 was submitted before 3.1 was cut as I wanted to keep the scope of PRs small and it was merged a couple of days after 3.1 branch was created. We have many places to inject custom rules but there is no way to inject rules after operator optimization. I feel it is going to be really useful for many customers. I've seen people fork session builders just because of the lack of this functionality.\r\n\r\nSomething like `postOperatorOptimizationRules` or `preCostBasedOptimizationRules` or `preCBORules` sounds OK to me.",
        "createdAt" : "2020-12-15T11:17:00Z",
        "updatedAt" : "2020-12-15T11:17:00Z",
        "lastEditedBy" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "tags" : [
        ]
      },
      {
        "id" : "28a4d613-50c2-4777-a066-bc58b10de6d8",
        "parentId" : "bcea21ff-4e55-4c78-9342-9935ddddafd4",
        "authorId" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "body" : "also cc @viirya @dbtsai @sunchao @cloud-fan @dongjoon-hyun ",
        "createdAt" : "2020-12-15T11:19:16Z",
        "updatedAt" : "2020-12-15T11:19:16Z",
        "lastEditedBy" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "tags" : [
        ]
      },
      {
        "id" : "8a25f716-45ec-4895-af59-050418a1f70c",
        "parentId" : "bcea21ff-4e55-4c78-9342-9935ddddafd4",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@aokolnychyi, Are you saying a bad API is justified if users want?\r\n\r\nI prefer to rename it in too. That's why I filed a blocker JIRA SPARK-33784 to keep this. I assigned this ticket to you.\r\n\r\nTo clarify again, I am suggesting to revert it in `branch-3.1` if we're in deadlock because exposing a bad API is not a good idea as discussed and documented in \"Alternatives to Breaking an API\" at https://spark.apache.org/versioning-policy.html.",
        "createdAt" : "2020-12-15T11:51:39Z",
        "updatedAt" : "2020-12-15T11:51:39Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "69aae010-9280-4e44-bf9f-c557bdf7ac85",
        "parentId" : "bcea21ff-4e55-4c78-9342-9935ddddafd4",
        "authorId" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "body" : "@HyukjinKwon, no, I meant I'd rather work on the name together to make sure it is good enough rather than revert the change. Here is a quote from my earlier comment:\r\n\r\n> I'd fix the name and cherry-pick it to 3.1 rather than revert the change completely.\r\n\r\nI think we are on the same page here and I'll fix the name as part of SPARK-33784. Thanks for creating it, btw!\r\n\r\nCurrently proposed names are:\r\n\r\n- `postOperatorOptimizationRules`\r\n- `preCostBasedOptimizationRules`\r\n- `preCBORules`\r\n\r\nWhat does everybody think about them? Any preferences?\r\n\r\n",
        "createdAt" : "2020-12-15T12:05:29Z",
        "updatedAt" : "2020-12-15T12:05:29Z",
        "lastEditedBy" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "tags" : [
        ]
      },
      {
        "id" : "c7ea2e03-8c7a-4eda-bc7c-9e5294e0b20d",
        "parentId" : "bcea21ff-4e55-4c78-9342-9935ddddafd4",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "To me any of them works. WDYT @maryannxue @gatorsmile and @rdblue?",
        "createdAt" : "2020-12-15T12:07:20Z",
        "updatedAt" : "2020-12-15T12:07:20Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "72060fdb-cdbd-4c7c-bb73-2c482c54e27f",
        "parentId" : "bcea21ff-4e55-4c78-9342-9935ddddafd4",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I'd vote for `preCBORules`.",
        "createdAt" : "2020-12-15T13:50:00Z",
        "updatedAt" : "2020-12-15T13:50:01Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "2f4940e8-c7c9-4638-8897-46a15c453515",
        "parentId" : "bcea21ff-4e55-4c78-9342-9935ddddafd4",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "According to the current discussion, could you make a follow-up PR with `preCBORules`, @aokolnychyi ? If we have a PR, it would be easier to make a final decision.",
        "createdAt" : "2020-12-15T16:54:40Z",
        "updatedAt" : "2020-12-15T16:54:41Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "11505ad5-e22e-47b6-b477-b798b88857e3",
        "parentId" : "bcea21ff-4e55-4c78-9342-9935ddddafd4",
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "I don't think that `preCBORules` is a good name. This batch is for rewrites that need to happen after basic optimization simplifies expressions and then pushes filters and projections. It also needs to happen before early pushdown, which in turn needs to come before CBO. All of that is before CBO, and that name doesn't capture what this is to be used for.\r\n\r\nA more descriptive name is \"planRewriteRules\" because this is for rewriting plans after initial optimization, but before other optimizer rules that need to run after that rewrite, like early pushdown, CBO, etc.\r\n\r\nThe name \"postOperatorOptimizationRules\" is okay, but not very descriptive.",
        "createdAt" : "2020-12-15T17:34:23Z",
        "updatedAt" : "2020-12-15T17:34:24Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      },
      {
        "id" : "c7c16c1a-6e88-49c9-813d-307a00417ad8",
        "parentId" : "bcea21ff-4e55-4c78-9342-9935ddddafd4",
        "authorId" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "body" : "Let's finish this discussion in a separate PR. I'll create one now.",
        "createdAt" : "2020-12-16T15:44:44Z",
        "updatedAt" : "2020-12-16T15:44:44Z",
        "lastEditedBy" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "tags" : [
        ]
      },
      {
        "id" : "51654abf-ef88-4eca-abc1-d71c2876097b",
        "parentId" : "bcea21ff-4e55-4c78-9342-9935ddddafd4",
        "authorId" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "body" : "I created PR https://github.com/apache/spark/pull/30808",
        "createdAt" : "2020-12-16T17:00:48Z",
        "updatedAt" : "2020-12-16T17:00:48Z",
        "lastEditedBy" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "tags" : [
        ]
      }
    ],
    "commit" : "11f07cd7c44bb92543dc534a30eac491a1553954",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +188,192 @@    // This batch rewrites data source plans and should be run after the operator\n    // optimization batch and before any batches that depend on stats.\n    Batch(\"Data Source Rewrite Rules\", Once, dataSourceRewriteRules: _*) :+\n    // This batch pushes filters and projections into scan nodes. Before this batch, the logical\n    // plan may contain nodes that do not report stats. Anything that uses stats must run after"
  },
  {
    "id" : "3a8353ed-d8a7-4625-a297-ec4f4147f479",
    "prId" : 30368,
    "prUrl" : "https://github.com/apache/spark/pull/30368#pullrequestreview-530074738",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "113f4177-c207-4a67-a92c-0f85f0fd4a04",
        "parentId" : null,
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "We still need the old patterns for such case `plan.limit(2).limit(1)`.",
        "createdAt" : "2020-11-13T09:24:15Z",
        "updatedAt" : "2020-11-17T14:37:40Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      },
      {
        "id" : "25b6cc22-e645-4eaa-8745-7357dab067b9",
        "parentId" : "113f4177-c207-4a67-a92c-0f85f0fd4a04",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "`transformDown` -> `transform`?",
        "createdAt" : "2020-11-13T12:20:51Z",
        "updatedAt" : "2020-11-17T14:37:40Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "98c27b39-b395-4d40-9d3d-02d653805a2f",
        "parentId" : "113f4177-c207-4a67-a92c-0f85f0fd4a04",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "We need `transformDown` here, so better to use `transformDown` explicitly ?",
        "createdAt" : "2020-11-13T13:49:02Z",
        "updatedAt" : "2020-11-17T14:37:40Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      }
    ],
    "commit" : "2f55ec034ade0d430bd7ab179f68addf0d65f84a",
    "line" : 27,
    "diffHunk" : "@@ -1,1 +1462,1466 @@  }\n\n  def apply(plan: LogicalPlan): LogicalPlan = plan transformDown {\n    case Limit(l, child) if canEliminate(l, child) =>\n      child"
  },
  {
    "id" : "0254d3e4-002e-4db7-8084-c3647988a08b",
    "prId" : 30278,
    "prUrl" : "https://github.com/apache/spark/pull/30278#pullrequestreview-526403956",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1e94f589-e98a-4153-b7c5-280fc7369d1f",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "This change affects the `PushDownLeftSemiAntiJoin` rule, too. So, could you add tests for the case?",
        "createdAt" : "2020-11-09T00:50:17Z",
        "updatedAt" : "2020-11-09T23:53:45Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "53e4a037-9c73-4c76-b8d3-302b73042170",
        "parentId" : "1e94f589-e98a-4153-b7c5-280fc7369d1f",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> This change affects the `PushDownLeftSemiAntiJoin` rule, too. So, could you add tests for the case?\r\n\r\nDouble check the case, seems current master fix this case by some pr, but 3.0 is still as jira desc.",
        "createdAt" : "2020-11-09T03:49:44Z",
        "updatedAt" : "2020-11-09T23:53:45Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "e12b576b-6859-47c6-985c-486b762ffa63",
        "parentId" : "1e94f589-e98a-4153-b7c5-280fc7369d1f",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "> seems current master fix this case\r\n\r\nWhat do you mean by \"fix this case\"?",
        "createdAt" : "2020-11-09T08:26:15Z",
        "updatedAt" : "2020-11-09T23:53:45Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "7779f2b3-4be8-4492-90cd-e7adde70c105",
        "parentId" : "1e94f589-e98a-4153-b7c5-280fc7369d1f",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> > seems current master fix this case\r\n> \r\n> What do you mean by \"fix this case\"?\r\n\r\nI have found the pr #29673  \r\nBefore this pr, SQL\r\n```\r\nSELECT\r\n       years,\r\n       appversion,                                               \r\n       SUM(uusers) AS users                                      \r\nFROM   (SELECT\r\n               Date_trunc('year', dt)          AS years,\r\n               CASE                                              \r\n                 WHEN h.pid = 3 THEN 'iOS'           \r\n                 WHEN h.pid = 4 THEN 'Android'       \r\n                 ELSE 'Other'                                    \r\n               END                             AS viewport,      \r\n               h.vs                            AS appversion,\r\n               Count(DISTINCT u.uid)           AS uusers\r\n               ,Count(DISTINCT u.suid)         AS srcusers\r\n        FROM   t1 u                                   \r\n               join t2 h                              \r\n                 ON h.uid = u.uid            \r\n        GROUP  BY 1,                                             \r\n                  2,                                             \r\n                  3) AS a\r\nWHERE  viewport = 'iOS'                                          \r\nGROUP  BY 1,                                                     \r\n          2\r\n```\r\n\r\nOptimized plan is  \r\n```\r\n== Optimized Logical Plan ==\r\nAggregate [years#0, appversion#2], [years#0, appversion#2, sum(uusers#3L) AS users#5L]\r\n+- Aggregate [date_trunc('year', CAST(u.`dt` AS TIMESTAMP))#24, CASE WHEN (h.`pid` = 3) THEN 'iOS' WHEN (h.`pid` = 4) THEN 'Android' ELSE 'Other' END#25, vs#17], [date_trunc('year', CAST(u.`dt` AS TIMESTAMP))#24 AS years#0, vs#17 AS appversion#2, count(if ((gid#23 = 1)) u.`uid`#26 else null) AS uusers#3L]\r\n   +- Aggregate [date_trunc('year', CAST(u.`dt` AS TIMESTAMP))#24, CASE WHEN (h.`pid` = 3) THEN 'iOS' WHEN (h.`pid` = 4) THEN 'Android' ELSE 'Other' END#25, vs#17, u.`uid`#26, u.`suid`#27, gid#23], [date_trunc('year', CAST(u.`dt` AS TIMESTAMP))#24, CASE WHEN (h.`pid` = 3) THEN 'iOS' WHEN (h.`pid` = 4) THEN 'Android' ELSE 'Other' END#25, vs#17, u.`uid`#26, gid#23]\r\n      +- Filter (CASE WHEN (h.`pid` = 3) THEN 'iOS' WHEN (h.`pid` = 4) THEN 'Android' ELSE 'Other' END#25 = iOS)\r\n         +- Expand [ArrayBuffer(date_trunc(year, cast(dt#14 as timestamp), Some(Asia/Shanghai)), CASE WHEN (pid#16 = 3) THEN iOS WHEN (pid#16 = 4) THEN Android ELSE Other END, vs#17, uid#12, null, 1), ArrayBuffer(date_trunc(year, cast(dt#14 as timestamp), Some(Asia/Shanghai)), CASE WHEN (pid#16 = 3) THEN iOS WHEN (pid#16 = 4) THEN Android ELSE Other END, vs#17, null, suid#15, 2)], [date_trunc('year', CAST(u.`dt` AS TIMESTAMP))#24, CASE WHEN (h.`pid` = 3) THEN 'iOS' WHEN (h.`pid` = 4) THEN 'Android' ELSE 'Other' END#25, vs#17, u.`uid`#26, u.`suid`#27, gid#23]\r\n            +- Project [uid#12, dt#14, suid#15, pid#16, vs#17]\r\n               +- Join Inner, (uid#18 = uid#12)\r\n                  :- Project [uid#12, dt#14, suid#15]\r\n                  :  +- Filter isnotnull(uid#12)\r\n                  :     +- Relation[pid#11,uid#12,sid#13,dt#14,suid#15] parquet\r\n                  +- Project [pid#16, vs#17, uid#18]\r\n                     +- Filter isnotnull(uid#18)\r\n                        +- Relation[pid#16,vs#17,uid#18,csid#19] parquet\r\n```\r\n\r\nAfter that pr, Optimized plan is \r\n```\r\n== Optimized Logical Plan ==\r\nAggregate [years#0, appversion#2], [years#0, appversion#2, sum(uusers#3L) AS users#5L]\r\n+- Aggregate [date_trunc(year, cast(dt#14 as timestamp), Some(Asia/Shanghai)), CASE WHEN (pid#16 = 3) THEN iOS WHEN (pid#16 = 4) THEN Android ELSE Other END, vs#17], [date_trunc(year, cast(dt#14 as timestamp), Some(Asia/Shanghai)) AS years#0, vs#17 AS appversion#2, count(distinct uid#12) AS uusers#3L]\r\n   +- Project [uid#12, dt#14, pid#16, vs#17]\r\n      +- Join Inner, (uid#18 = uid#12)\r\n         :- Project [uid#12, dt#14]\r\n         :  +- Filter isnotnull(uid#12)\r\n         :     +- Relation[pid#11,uid#12,sid#13,dt#14,suid#15] parquet\r\n         +- Project [pid#16, vs#17, uid#18]\r\n            +- Filter ((CASE WHEN (pid#16 = 3) THEN iOS WHEN (pid#16 = 4) THEN Android ELSE Other END = iOS) AND isnotnull(uid#18))\r\n               +- Relation[pid#16,vs#17,uid#18,csid#19] parquet\r\n```\r\n\r\n`Filter((CASE WHEN (pid#16 = 3) THEN iOS WHEN (pid#16 = 4) THEN Android ELSE Other END = iOS))` is pushed down and won't generate  `Expand`",
        "createdAt" : "2020-11-09T09:22:03Z",
        "updatedAt" : "2020-11-09T23:53:45Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "dc89f7c0-6f32-42aa-b3a9-9e9d4619d8c6",
        "parentId" : "1e94f589-e98a-4153-b7c5-280fc7369d1f",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "how does it related to left semi join?",
        "createdAt" : "2020-11-09T09:32:39Z",
        "updatedAt" : "2020-11-09T23:53:45Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "858b1254-439d-4693-b801-a4fad884e223",
        "parentId" : "1e94f589-e98a-4153-b7c5-280fc7369d1f",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> how does it related to left semi join?\r\n\r\nNot related,   I didn't notice that the API would be used by  `PushDownLeftSemiAntiJoin `",
        "createdAt" : "2020-11-09T09:34:38Z",
        "updatedAt" : "2020-11-09T23:53:45Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "54e18663-1956-43cb-b173-e27ffd3c49e4",
        "parentId" : "1e94f589-e98a-4153-b7c5-280fc7369d1f",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "> This change affects the PushDownLeftSemiAntiJoin rule, too. So, could you add tests for the case?\r\n\r\nSo can we add such a test?",
        "createdAt" : "2020-11-09T09:43:54Z",
        "updatedAt" : "2020-11-09T23:53:45Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "8088493a-25d0-43c4-b703-7a4c2e5fd32b",
        "parentId" : "1e94f589-e98a-4153-b7c5-280fc7369d1f",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> > This change affects the PushDownLeftSemiAntiJoin rule, too. So, could you add tests for the case?\r\n> \r\n> So can we add such a test?\r\n\r\nWith test case in  `LeftSemiPushdownSuite`\r\n```\r\n  test(\"Unary: LeftSemi join push down through expand\") {\r\n    val expand = Expand(Seq(Seq('a, 'b, \"null\"), Seq('a, \"null\", 'c)),\r\n      Seq('a, 'b, 'c), testRelation)\r\n    val originalQuery = expand\r\n      .join(testRelation1, joinType = LeftSemi, condition = Some('b === 'd && 'b === 1))\r\n\r\n    val optimized = Optimize.execute(originalQuery.analyze)\r\n    val correctAnswer = Expand(Seq(Seq('a, 'b, \"null\"), Seq('a, \"null\", 'c)),\r\n      Seq('a, 'b, 'c), Filter(EqualTo('b, 1), testRelation))\r\n        .join(testRelation1, joinType = LeftSemi, condition = Some('b === 'd))\r\n        .analyze\r\n\r\n    comparePlans(optimized, correctAnswer)\r\n  }\r\n```\r\noriginalQuery is \r\n```\r\n'Join LeftSemi, (('b = 'd) AND ('b = 1))\r\n:- 'Expand [List('a, 'b, null), List('a, null, 'c)], ['a, 'b, 'c]\r\n:  +- LocalRelation <empty>, [a#0, b#1, c#2]\r\n+- LocalRelation <empty>, [d#3]\r\n```\r\n\r\nTest result is\r\n```\r\n== FAIL: Plans do not match ===\r\n!'Expand [List(a#0, b#0, null), List(a#0, null, c#0)], [a#0, b#0, c#0]   'Join LeftSemi, (b#0 = d#0)\r\n!+- 'Join LeftSemi, ((b#0 = 1) AND (b#0 = d#0))                          :- Expand [List(a#0, b#0, null), List(a#0, null, c#0)], [a#0, b#0, c#0]\r\n!   :- LocalRelation <empty>, [a#0, b#0, c#0]                            :  +- Filter (b#0 = 1)\r\n!   +- LocalRelation <empty>, [d#0]                                      :     +- LocalRelation <empty>, [a#0, b#0, c#0]\r\n!                                                                        +- LocalRelation <empty>, [d#0]\r\n    \r\n```\r\n\r\nExpand will be promoted below Join, so should we ignore this case or add a parameter  in  `canPushThrough` like below\r\n```\r\n def canPushThrough(p: UnaryNode, isFilterPushDown: Boolean = false): Boolean = p match {\r\n    // Note that some operators (e.g. project, aggregate, union) are being handled separately\r\n    // (earlier in this rule).\r\n    case _: AppendColumns => true\r\n    case _: Distinct => true\r\n    case _: Generate => true\r\n    case _: Pivot => true\r\n    case _: RepartitionByExpression => true\r\n    case _: Repartition => true\r\n    case _: ScriptTransformation => true\r\n    case _: Sort => true\r\n    case _: BatchEvalPython => true\r\n    case _: ArrowEvalPython => true\r\n    case _: Expand => isFilterPushDown\r\n    case _ => false\r\n  }\r\n```",
        "createdAt" : "2020-11-09T10:18:20Z",
        "updatedAt" : "2020-11-09T23:53:45Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "69f07bb4-f2b5-425d-8ee6-9d00a0e1bd6f",
        "parentId" : "1e94f589-e98a-4153-b7c5-280fc7369d1f",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I'm sorry I didn't get it. What's the issue here? We can't pushdown left-semi join through expand?",
        "createdAt" : "2020-11-09T13:02:05Z",
        "updatedAt" : "2020-11-09T23:53:45Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "762dc7f9-ef90-44a5-8638-693e0cb560ff",
        "parentId" : "1e94f589-e98a-4153-b7c5-280fc7369d1f",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "The optimized (left-side) plan above looks correct to me...",
        "createdAt" : "2020-11-09T14:03:59Z",
        "updatedAt" : "2020-11-09T23:53:45Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "9d0ccb6e-4a22-4bc1-b496-4ccdf69ddff2",
        "parentId" : "1e94f589-e98a-4153-b7c5-280fc7369d1f",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> I'm sorry I didn't get it. What's the issue here? We can't pushdown left-semi join through expand?\r\n\r\noh..my mistake,  I misunderstood some code about `PushDownLeftSemiAntiJoin `",
        "createdAt" : "2020-11-09T15:59:27Z",
        "updatedAt" : "2020-11-09T23:53:45Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "0dca006e-1264-4f31-b9b6-45f0d454943e",
        "parentId" : "1e94f589-e98a-4153-b7c5-280fc7369d1f",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> The optimized (left-side) plan above looks correct to me...\r\n\r\nMy fault, I  misunderstand some code about `PushDownLeftSemiAntiJoin`,  test case added ==",
        "createdAt" : "2020-11-09T16:00:52Z",
        "updatedAt" : "2020-11-09T23:53:45Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "a09f8364cf4b9853cdf1ff4dc00d761b3a5b6291",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +1270,1274 @@    case _: BatchEvalPython => true\n    case _: ArrowEvalPython => true\n    case _: Expand => true\n    case _ => false\n  }"
  },
  {
    "id" : "3495fd5f-6f8d-4b46-8d28-39fc9e517538",
    "prId" : 30178,
    "prUrl" : "https://github.com/apache/spark/pull/30178#pullrequestreview-534027055",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "12bd34a1-f800-4023-97d0-603767e15edb",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "shall we also check if the lower bound is `UnboundedPreceding`? otherwise we can't use the offset optimization for nth_value and `first` is probably faster than `nth_value(1)`",
        "createdAt" : "2020-11-18T05:36:57Z",
        "updatedAt" : "2020-11-18T05:36:57Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "5432e403-e02f-4795-9dec-85c752b0d44e",
        "parentId" : "12bd34a1-f800-4023-97d0-603767e15edb",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "OK. I created the https://github.com/apache/spark/pull/30419 to make this check.",
        "createdAt" : "2020-11-19T02:10:10Z",
        "updatedAt" : "2020-11-19T04:13:26Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "3a7f4e740eb5a9cecf880bc5cc294b2459e98cf1",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +815,819 @@    case we @ WindowExpression(AggregateExpression(first: First, _, _, _, _), spec)\n      if spec.orderSpec.nonEmpty &&\n        spec.frameSpecification.asInstanceOf[SpecifiedWindowFrame].frameType == RowFrame =>\n      we.copy(windowFunction = NthValue(first.child, Literal(1), first.ignoreNulls))\n  }"
  },
  {
    "id" : "45b0041b-c56d-479e-828e-eab49b0bcff8",
    "prId" : 30018,
    "prUrl" : "https://github.com/apache/spark/pull/30018#pullrequestreview-510408431",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f6ab3e3b-1ef5-4941-aa64-a254e3559260",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "nit: `lowerIsRedundant` -> `hasRedundantAggregate`?",
        "createdAt" : "2020-10-15T07:20:46Z",
        "updatedAt" : "2021-03-19T04:24:38Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "2ee82dad-dae4-483c-8c53-1dc6ffa06fff",
        "parentId" : "f6ab3e3b-1ef5-4941-aa64-a254e3559260",
        "authorId" : "a8e23d47-3ae4-4385-848c-38a216d1bd08",
        "body" : "I think that the current one is more explicit. With your proposition it is not obvious which one is redundant. Perhaps there are situations where `upperIsRedundant`.",
        "createdAt" : "2020-10-16T12:08:26Z",
        "updatedAt" : "2021-03-19T04:24:38Z",
        "lastEditedBy" : "a8e23d47-3ae4-4385-848c-38a216d1bd08",
        "tags" : [
        ]
      }
    ],
    "commit" : "07e758d25e0840bf40129a36ad5bdb90a005058a",
    "line" : 36,
    "diffHunk" : "@@ -1,1 +514,518 @@  }\n\n  private def lowerIsRedundant(upper: Aggregate, lower: Aggregate): Boolean = {\n    val upperHasNoAggregateExpressions = !upper.aggregateExpressions.exists(isAggregate)\n"
  },
  {
    "id" : "9e23376b-a16a-4176-a7ec-fa1551560ee6",
    "prId" : 30018,
    "prUrl" : "https://github.com/apache/spark/pull/30018#pullrequestreview-513424905",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "42bc38e2-b847-447d-a5a5-8de168e67fac",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "`transformUp` -> `transform`? Seems like this transformation does not depend on the order.",
        "createdAt" : "2020-10-15T07:22:01Z",
        "updatedAt" : "2021-03-19T04:24:38Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "ecc0b48c-e1bf-4c28-8a19-c5f1b7ce1b76",
        "parentId" : "42bc38e2-b847-447d-a5a5-8de168e67fac",
        "authorId" : "a8e23d47-3ae4-4385-848c-38a216d1bd08",
        "body" : "There is a slight difference - `transformUp` can handle more than two consecutive aggregates in a single iteration.\r\nIf there are 3 aggregates `A1(A2(A3(...)))`, where `A2` and  `A3` are redundant, then with `transformDown` on first iteration `A2` is removed and then `A3` on the second iteration. With `transformUp` it removes `A3` and then `A2` on the same iteration.",
        "createdAt" : "2020-10-15T15:30:49Z",
        "updatedAt" : "2021-03-19T04:24:38Z",
        "lastEditedBy" : "a8e23d47-3ae4-4385-848c-38a216d1bd08",
        "tags" : [
        ]
      },
      {
        "id" : "234342ac-1eda-4394-a7b3-ce3a4ecd7215",
        "parentId" : "42bc38e2-b847-447d-a5a5-8de168e67fac",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : ">> where A2 and A3 are redundant\r\n\r\nIf only A2 and A3 are redundant, `lowerIsRedundant(A1, A2)` returns false and it seems like A2/A3 are removed in the same iteration even in `transform` (the first acse). I miss something?",
        "createdAt" : "2020-10-21T02:20:49Z",
        "updatedAt" : "2021-03-19T04:24:38Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "ceb61bac-8da9-4e41-89fc-3c6959907c5c",
        "parentId" : "42bc38e2-b847-447d-a5a5-8de168e67fac",
        "authorId" : "a8e23d47-3ae4-4385-848c-38a216d1bd08",
        "body" : "I meant that `lowerIsRedundant(A1, A2)` would return true, If it would return false, then A2 would not be removed at all.\r\n\r\nWith `transformDown` it would check `A1` and `A2` and remove `A2`, but then it would not check `A1` and `A3` on the same iteration.",
        "createdAt" : "2020-10-21T03:20:15Z",
        "updatedAt" : "2021-03-19T04:24:38Z",
        "lastEditedBy" : "a8e23d47-3ae4-4385-848c-38a216d1bd08",
        "tags" : [
        ]
      },
      {
        "id" : "0c2f1284-b9fb-4f72-9f5d-95ffcae654b9",
        "parentId" : "42bc38e2-b847-447d-a5a5-8de168e67fac",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "> transformUp can handle more than two consecutive aggregates in a single iteration.\r\n\r\nYou meant that `transformUp` can transform `A1(A2(A3(...))) => A1()` in a single iteration? Probably, I miss some conditions for the case you described above, so could you give me a concrete query example for it?",
        "createdAt" : "2020-10-21T05:12:16Z",
        "updatedAt" : "2021-03-19T04:24:38Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "f0613169-2d1d-456c-bac8-d7f744b627bf",
        "parentId" : "42bc38e2-b847-447d-a5a5-8de168e67fac",
        "authorId" : "a8e23d47-3ae4-4385-848c-38a216d1bd08",
        "body" : "For example the UT `Remove 2 redundant aggregates`.\r\nWith `transformUp` it runs 2 iterations (1 extra to detect that all changes are done).\r\nWith `transformDown` it runs 3 iterations",
        "createdAt" : "2020-10-21T05:44:09Z",
        "updatedAt" : "2021-03-19T04:24:38Z",
        "lastEditedBy" : "a8e23d47-3ae4-4385-848c-38a216d1bd08",
        "tags" : [
        ]
      },
      {
        "id" : "c04e0993-c001-4f0c-a061-6cb9a744c2a8",
        "parentId" : "42bc38e2-b847-447d-a5a5-8de168e67fac",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Ah, ok. I got your point. Looks okay as it is.",
        "createdAt" : "2020-10-21T08:23:56Z",
        "updatedAt" : "2021-03-19T04:24:38Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "07e758d25e0840bf40129a36ad5bdb90a005058a",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +495,499 @@ */\nobject RemoveRedundantAggregates extends Rule[LogicalPlan] with AliasHelper {\n  def apply(plan: LogicalPlan): LogicalPlan = plan transformUp {\n    case upper @ Aggregate(_, _, lower: Aggregate) if lowerIsRedundant(upper, lower) =>\n      val aliasMap = getAliasMap(lower)"
  },
  {
    "id" : "b5fdc4c6-9097-4d59-8164-1d834d5d8a1e",
    "prId" : 30018,
    "prUrl" : "https://github.com/apache/spark/pull/30018#pullrequestreview-556092897",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fa69cc4b-a547-4ae1-bac2-c876bf949f2d",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Could you add test cases for `PythonUDF.isGroupedAggPandasUDF(e)`, too?",
        "createdAt" : "2020-12-21T02:04:09Z",
        "updatedAt" : "2021-03-19T04:24:38Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "07e758d25e0840bf40129a36ad5bdb90a005058a",
    "line" : 52,
    "diffHunk" : "@@ -1,1 +530,534 @@  private def isAggregate(expr: Expression): Boolean = {\n    expr.find(e => e.isInstanceOf[AggregateExpression] ||\n      PythonUDF.isGroupedAggPandasUDF(e)).isDefined\n  }\n}"
  },
  {
    "id" : "f491a234-b970-4ae3-a438-186570c498f3",
    "prId" : 30018,
    "prUrl" : "https://github.com/apache/spark/pull/30018#pullrequestreview-616944479",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2d854c10-0d8f-4075-ae13-da9a9fbdf4f1",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "nit. Usually, `isXXX` is better and consistent with Apache Spark convention.",
        "createdAt" : "2021-03-20T21:10:10Z",
        "updatedAt" : "2021-03-20T21:10:11Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "07e758d25e0840bf40129a36ad5bdb90a005058a",
    "line" : 36,
    "diffHunk" : "@@ -1,1 +514,518 @@  }\n\n  private def lowerIsRedundant(upper: Aggregate, lower: Aggregate): Boolean = {\n    val upperHasNoAggregateExpressions = !upper.aggregateExpressions.exists(isAggregate)\n"
  },
  {
    "id" : "d91ca3cb-087b-4e3a-98f5-bd0915b6b63c",
    "prId" : 30018,
    "prUrl" : "https://github.com/apache/spark/pull/30018#pullrequestreview-616944661",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "648f0ae0-b8a5-4108-af99-9bc0de71c3d2",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "- `introduces` -> `introduced`\r\n- `expression` -> `expressions`",
        "createdAt" : "2021-03-20T21:13:24Z",
        "updatedAt" : "2021-03-20T21:14:01Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "07e758d25e0840bf40129a36ad5bdb90a005058a",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +506,510 @@      )\n\n      // We might have introduces non-deterministic grouping expression\n      if (newAggregate.groupingExpressions.exists(!_.deterministic)) {\n        PullOutNondeterministic.applyLocally.applyOrElse(newAggregate, identity[LogicalPlan])"
  },
  {
    "id" : "c003fc4f-eadc-4669-91e2-d2176c191c17",
    "prId" : 30018,
    "prUrl" : "https://github.com/apache/spark/pull/30018#pullrequestreview-616945018",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "160cc0cb-c5f9-442e-966a-8f0a9f7d2326",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Could you move this optimizer into a new file please, @tanelk ?",
        "createdAt" : "2021-03-20T21:19:11Z",
        "updatedAt" : "2021-03-20T21:19:11Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "07e758d25e0840bf40129a36ad5bdb90a005058a",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +494,498 @@ * only goal is to keep distinct values, while its parent aggregate would ignore duplicate values.\n */\nobject RemoveRedundantAggregates extends Rule[LogicalPlan] with AliasHelper {\n  def apply(plan: LogicalPlan): LogicalPlan = plan transformUp {\n    case upper @ Aggregate(_, _, lower: Aggregate) if lowerIsRedundant(upper, lower) =>"
  },
  {
    "id" : "21785bda-f303-45c7-9ddb-559a210a0f7e",
    "prId" : 30018,
    "prUrl" : "https://github.com/apache/spark/pull/30018#pullrequestreview-616946037",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "996018b3-503a-495f-84d5-062ce6b6ae92",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "```scala\r\n- .filter(!isAggregate(_))\r\n+ .filterNot(isAggregate)\r\n```",
        "createdAt" : "2021-03-20T21:37:32Z",
        "updatedAt" : "2021-03-20T21:37:32Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "07e758d25e0840bf40129a36ad5bdb90a005058a",
    "line" : 43,
    "diffHunk" : "@@ -1,1 +521,525 @@        .aggregateExpressions\n        .filter(_.deterministic)\n        .filter(!isAggregate(_))\n        .map(_.toAttribute)\n    ))"
  },
  {
    "id" : "0bc81166-149d-4123-84b6-2ce32ff948c2",
    "prId" : 29950,
    "prUrl" : "https://github.com/apache/spark/pull/29950#pullrequestreview-502548398",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "764f1c27-198c-437e-9535-84cca201246a",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "We could extend to other cases like `case p @ Project(_, agg: Aggregate)`, but leave it untouched for now.",
        "createdAt" : "2020-10-06T02:50:52Z",
        "updatedAt" : "2020-11-13T00:06:08Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "bbaae3ed9b96ca517ec5435838ac37feb578c959",
    "line" : 56,
    "diffHunk" : "@@ -1,1 +773,777 @@      case a: Alias => a.toAttribute -> a\n    })\n  }\n\n  // Whether the largest times common outputs from lower operator used in upper operators is"
  },
  {
    "id" : "3a7e872a-38d4-43f0-8eee-b8de9d0a7ef6",
    "prId" : 29950,
    "prUrl" : "https://github.com/apache/spark/pull/29950#pullrequestreview-505950881",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "61dd2c29-50d0-4424-8781-138c2b6c4131",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Could you update the optimizer description according to the new conf?",
        "createdAt" : "2020-10-09T09:42:35Z",
        "updatedAt" : "2020-11-13T00:06:08Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "11ceec9a-107b-4df7-bb7d-544c1fbb6ff3",
        "parentId" : "61dd2c29-50d0-4424-8781-138c2b6c4131",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Yea, updated. thanks.",
        "createdAt" : "2020-10-09T19:52:58Z",
        "updatedAt" : "2020-11-13T00:06:08Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "bbaae3ed9b96ca517ec5435838ac37feb578c959",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +730,734 @@ * 2. When two [[Project]] operators have LocalLimit/Sample/Repartition operator between them\n *    and the upper project consists of the same number of columns which is equal or aliasing.\n *    `GlobalLimit(LocalLimit)` pattern is also considered.\n */\nobject CollapseProject extends Rule[LogicalPlan] with AliasHelper {"
  },
  {
    "id" : "b82f40a3-1baa-4a01-8a3c-f02327c1422d",
    "prId" : 29950,
    "prUrl" : "https://github.com/apache/spark/pull/29950#pullrequestreview-506110086",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9867ad56-e91c-4cd2-a298-ee2bbcf0960c",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "`ColumnPruning` cannot finish all collapsing in one shot. `Once` will break idempotence.",
        "createdAt" : "2020-10-10T16:35:06Z",
        "updatedAt" : "2020-11-13T00:06:08Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "bbaae3ed9b96ca517ec5435838ac37feb578c959",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +219,223 @@      RewritePredicateSubquery,\n      ColumnPruning,\n      CollapseProject,\n      RemoveNoopOperators) :+\n    // This batch must be executed after the `RewriteSubquery` batch, which creates joins."
  },
  {
    "id" : "19cca6df-d5ae-45a7-a492-70b5dabe2304",
    "prId" : 29950,
    "prUrl" : "https://github.com/apache/spark/pull/29950#pullrequestreview-507068822",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "35b396ee-3663-4cc4-ab71-29a2f3594520",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "`FixedPoint(1)` instead? Also, could you leave comments about why we use it instead of `Once`?",
        "createdAt" : "2020-10-12T00:51:01Z",
        "updatedAt" : "2020-11-13T00:06:08Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "ac566f54-c920-4d6a-9940-f089342f4dc0",
        "parentId" : "35b396ee-3663-4cc4-ab71-29a2f3594520",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Isn't `FixedPoint(1)` also to run the batch once?",
        "createdAt" : "2020-10-13T04:38:49Z",
        "updatedAt" : "2020-11-13T00:06:09Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "5574ece0-a184-4566-a315-bc77d09da08f",
        "parentId" : "35b396ee-3663-4cc4-ab71-29a2f3594520",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Added one comment.",
        "createdAt" : "2020-10-13T04:57:05Z",
        "updatedAt" : "2020-11-13T00:06:09Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "bbaae3ed9b96ca517ec5435838ac37feb578c959",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +216,220 @@      CheckCartesianProducts) :+\n    // `CollapseProject` cannot collapse all projects in once. So we need `fixedPoint` here.\n    Batch(\"RewriteSubquery\", fixedPoint,\n      RewritePredicateSubquery,\n      ColumnPruning,"
  },
  {
    "id" : "446d1d9c-e5a5-4c58-ae81-c17fb57d24a9",
    "prId" : 29950,
    "prUrl" : "https://github.com/apache/spark/pull/29950#pullrequestreview-529768736",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3d3443ff-b634-4caf-aaba-95dea9d0b577",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Is there a reason to change from `transformUp` to `transformDown`? If the all test passed, it would be safe if we keep the original one.",
        "createdAt" : "2020-11-13T05:36:08Z",
        "updatedAt" : "2020-11-13T05:36:08Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "f1d33554-d9d2-44f4-9d46-a9aee69be201",
        "parentId" : "3d3443ff-b634-4caf-aaba-95dea9d0b577",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "I found the previous comment about supporting `withColumn`. If this is designed for that, shall we add a test case for that?\r\n- https://github.com/apache/spark/pull/29950#discussion_r501976989",
        "createdAt" : "2020-11-13T06:11:59Z",
        "updatedAt" : "2020-11-13T06:12:00Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "bbaae3ed9b96ca517ec5435838ac37feb578c959",
    "line" : 30,
    "diffHunk" : "@@ -1,1 +734,738 @@object CollapseProject extends Rule[LogicalPlan] with AliasHelper {\n\n  def apply(plan: LogicalPlan): LogicalPlan = plan transformDown {\n    case p @ Project(_, _: Project) =>\n      collapseProjects(p)"
  },
  {
    "id" : "ade969ba-03e7-4bff-a200-3dd7dd9df326",
    "prId" : 29950,
    "prUrl" : "https://github.com/apache/spark/pull/29950#pullrequestreview-529752959",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "547ba3db-71b1-451f-9e2f-efccac31adc9",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "`upper operators` -> `upper operator`?",
        "createdAt" : "2020-11-13T05:52:12Z",
        "updatedAt" : "2020-11-13T05:52:12Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "bbaae3ed9b96ca517ec5435838ac37feb578c959",
    "line" : 58,
    "diffHunk" : "@@ -1,1 +775,779 @@  }\n\n  // Whether the largest times common outputs from lower operator used in upper operators is\n  // larger than allowed.\n  private def moreThanMaxAllowedCommonOutput("
  },
  {
    "id" : "2e71c8b1-565a-4822-a2c1-e54b75ceb217",
    "prId" : 29950,
    "prUrl" : "https://github.com/apache/spark/pull/29950#pullrequestreview-529753264",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6c2e4d50-cabc-4410-8d79-2fffef785925",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "`than allowed` -> `than the maximum`?",
        "createdAt" : "2020-11-13T05:53:16Z",
        "updatedAt" : "2020-11-13T05:53:16Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "bbaae3ed9b96ca517ec5435838ac37feb578c959",
    "line" : 59,
    "diffHunk" : "@@ -1,1 +776,780 @@\n  // Whether the largest times common outputs from lower operator used in upper operators is\n  // larger than allowed.\n  private def moreThanMaxAllowedCommonOutput(\n      upper: Seq[NamedExpression], lower: Seq[NamedExpression]): Boolean = {"
  }
]