[
  {
    "id" : "d0e389f6-f58a-4036-b38d-1a1596f96bfa",
    "prId" : 31448,
    "prUrl" : "https://github.com/apache/spark/pull/31448#pullrequestreview-608288509",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "101e7e8f-dfbe-494a-8e3d-69fe82c02566",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "It's a bit weird to define a format pattern string only for parsing. Are we going to add the formatting function?",
        "createdAt" : "2021-03-09T12:43:54Z",
        "updatedAt" : "2021-03-09T12:43:54Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "bc8b2a9f-fb86-4848-b031-4f23d5c92741",
        "parentId" : "101e7e8f-dfbe-494a-8e3d-69fe82c02566",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "I got it.",
        "createdAt" : "2021-03-10T02:51:45Z",
        "updatedAt" : "2021-03-10T02:51:45Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "c4b2ce6e20f49a85a91da94c457eba184bb26826",
    "line" : 101,
    "diffHunk" : "@@ -1,1 +2510,2514 @@      'D':  decimal point (uses locale)\n      'G':  group separator (uses locale)\n      '$':  specifies that the input value has a leading $ (Dollar) sign.\n  \"\"\",\n  examples = \"\"\""
  },
  {
    "id" : "3c31998b-0636-4fef-b0db-99a56784cc29",
    "prId" : 30479,
    "prUrl" : "https://github.com/apache/spark/pull/30479#pullrequestreview-549816617",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "72eac6e7-af01-4b9d-9c9d-1ba47f943544",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "hm, IMHO it would be good to rename the existing `decode` func or assign a new name into the new `decode` func because the generated doc for `decode` will be complicated.",
        "createdAt" : "2020-12-09T07:12:16Z",
        "updatedAt" : "2020-12-09T09:13:00Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "467fc0f0-ea51-4d82-bd99-acae8818bd70",
        "parentId" : "72eac6e7-af01-4b9d-9c9d-1ba47f943544",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "If rename the `existing` decode func will break the behavior. ",
        "createdAt" : "2020-12-09T07:36:13Z",
        "updatedAt" : "2020-12-09T09:13:00Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      },
      {
        "id" : "94a51e9c-b862-4e47-ac4f-fcd5aa8a7095",
        "parentId" : "72eac6e7-af01-4b9d-9c9d-1ba47f943544",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Yea, so I think we need add a legacy config, update the migration guide, ... if we rename it. Anyway, the latter approach (assigning a new name into the new `decode` func) looks better if possible.",
        "createdAt" : "2020-12-09T07:48:32Z",
        "updatedAt" : "2020-12-09T09:13:00Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "d19ebdcd-5860-4062-a52b-b7768a52c2c3",
        "parentId" : "72eac6e7-af01-4b9d-9c9d-1ba47f943544",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "This PR could avoid migration and support the new `decode` as well. cc @cloud-fan ",
        "createdAt" : "2020-12-09T08:40:13Z",
        "updatedAt" : "2020-12-09T09:13:00Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      },
      {
        "id" : "fd258f76-c83a-4731-a343-a91e8a11cc0a",
        "parentId" : "72eac6e7-af01-4b9d-9c9d-1ba47f943544",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "We can probably rename the function in ansi mode.",
        "createdAt" : "2020-12-11T05:46:21Z",
        "updatedAt" : "2020-12-11T05:46:22Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "eea967a4aa4ad18760c5a65c275c1b3f68971f85",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +2090,2094 @@        throw new AnalysisException(\"Invalid number of arguments for function decode. \" +\n          s\"Expected: 2; Found: ${params.length}\")\n      case 2 => StringDecode(params.head, params.last)\n      case _ =>\n        val input = params.head"
  },
  {
    "id" : "07479b20-4921-4afe-8c49-5aaf19b1ae1a",
    "prId" : 30479,
    "prUrl" : "https://github.com/apache/spark/pull/30479#pullrequestreview-547909412",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d20d288d-e075-457a-a87c-c8586ed01e84",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Could you follow the other format? e.g., https://github.com/apache/spark/blob/c88eddac3bf860d04bba91fc913f8b2069a94153/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/stringExpressions.scala#L479-L486",
        "createdAt" : "2020-12-09T07:16:22Z",
        "updatedAt" : "2020-12-09T09:13:00Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "969e1545-614f-44e4-8e8a-444022729e9d",
        "parentId" : "d20d288d-e075-457a-a87c-c8586ed01e84",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "The new `decode` function is the same as other function uses `RuntimeReplaceable`. The parameter list is not fixed.",
        "createdAt" : "2020-12-09T07:41:02Z",
        "updatedAt" : "2020-12-09T09:13:00Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "eea967a4aa4ad18760c5a65c275c1b3f68971f85",
    "line" : 47,
    "diffHunk" : "@@ -1,1 +2119,2123 @@            |  to each search value one by one. If expr is equal to a search, returns the corresponding result.\n            |  If no match is found, then Oracle returns default. If default is omitted, returns null.\n          \"\"\",\n  examples = \"\"\"\n    Examples:"
  },
  {
    "id" : "5ebcd09e-a278-4dc9-96b4-11d473dfbf13",
    "prId" : 30479,
    "prUrl" : "https://github.com/apache/spark/pull/30479#pullrequestreview-549823403",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "376e68fe-1e10-44f4-9815-96133bb66379",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "So this replaces the existing `decode` in SQL side only, right? shall we update migration guide?",
        "createdAt" : "2020-12-11T05:55:37Z",
        "updatedAt" : "2020-12-11T05:55:37Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "b3da8dfb-08c1-4497-96c3-ad6f6b9c70d0",
        "parentId" : "376e68fe-1e10-44f4-9815-96133bb66379",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "The new `decode` could keep compatible with old one.",
        "createdAt" : "2020-12-11T06:06:52Z",
        "updatedAt" : "2020-12-11T06:07:01Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "eea967a4aa4ad18760c5a65c275c1b3f68971f85",
    "line" : 61,
    "diffHunk" : "@@ -1,1 +2133,2137 @@  since = \"3.2.0\")\n// scalastyle:on line.size.limit\ncase class Decode(params: Seq[Expression], child: Expression) extends RuntimeReplaceable {\n\n  def this(params: Seq[Expression]) = {"
  },
  {
    "id" : "38cf5ac5-526c-4ecd-bf0b-b49b274151cf",
    "prId" : 30479,
    "prUrl" : "https://github.com/apache/spark/pull/30479#pullrequestreview-549823447",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7002a47a-28ab-4556-9e80-220d0b34bcf0",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Do we have an alternative for the previous `decode` after this change?",
        "createdAt" : "2020-12-11T05:56:50Z",
        "updatedAt" : "2020-12-11T05:56:50Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "8700a284-a318-4c72-86b7-ca4e9d9d2c8b",
        "parentId" : "7002a47a-28ab-4556-9e80-220d0b34bcf0",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "This only renames the expression, not the function. This PR just adds an overload of the `decode` function when there are more than 3 parameters.",
        "createdAt" : "2020-12-11T06:00:45Z",
        "updatedAt" : "2020-12-11T06:00:46Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "a2edd5b6-6bab-40ce-b70c-a596026f77f9",
        "parentId" : "7002a47a-28ab-4556-9e80-220d0b34bcf0",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I'd suggest the following followup:\r\n1. add a new function name for `StringDecode`. Probably \"string_decode\".\r\n2. require `decode` function to take more than 2 parameters, under ansi mode(or a legacy config)",
        "createdAt" : "2020-12-11T06:06:58Z",
        "updatedAt" : "2020-12-11T06:06:58Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "eea967a4aa4ad18760c5a65c275c1b3f68971f85",
    "line" : 79,
    "diffHunk" : "@@ -1,1 +2158,2162 @@  since = \"1.5.0\")\n// scalastyle:on line.size.limit\ncase class StringDecode(bin: Expression, charset: Expression)\n  extends BinaryExpression with ImplicitCastInputTypes with NullIntolerant {\n"
  },
  {
    "id" : "74bdaaef-3816-4881-b99f-4ad1219e5557",
    "prId" : 30399,
    "prUrl" : "https://github.com/apache/spark/pull/30399#pullrequestreview-532559298",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6fb45619-662c-4a56-a21d-4ebc20e24033",
        "parentId" : null,
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "To clarify, the raw exception message is something like `Illegal character in query at index 33: https://a.b.c/index.php?params1=a|b&params2=x`.",
        "createdAt" : "2020-11-17T16:34:08Z",
        "updatedAt" : "2020-11-20T02:42:53Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      }
    ],
    "commit" : "d7e437b326dc9564a9460946bdbc0856e6876322",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +1407,1411 @@    } catch {\n      case e: URISyntaxException if failOnError =>\n        throw new IllegalArgumentException(s\"Find an invaild url string ${url.toString}\", e)\n      case _: URISyntaxException => null\n    }"
  },
  {
    "id" : "ef2eeeb3-b51e-4fb1-91bb-cb8759de7ebb",
    "prId" : 30297,
    "prUrl" : "https://github.com/apache/spark/pull/30297#pullrequestreview-526881857",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c315b3ad-c8ab-4997-83ef-d6cb55e0fcc1",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "The same: https://github.com/apache/spark/pull/30297/files#r520305929",
        "createdAt" : "2020-11-10T06:05:15Z",
        "updatedAt" : "2020-11-12T05:25:04Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "4b161a4ef4f3be2f1a239c0c19f8aa62b5a03706",
    "line" : 27,
    "diffHunk" : "@@ -1,1 +246,250 @@  since = \"2.0.0\")\n// scalastyle:on line.size.limit\ncase class Elt(\n    children: Seq[Expression],\n    failOnError: Boolean = SQLConf.get.ansiEnabled) extends Expression {"
  },
  {
    "id" : "3a31e0dc-eef0-41d7-9e07-03a0e357b217",
    "prId" : 29577,
    "prUrl" : "https://github.com/apache/spark/pull/29577#pullrequestreview-481382988",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "999133b7-e261-4925-8626-c13637b1d8a5",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Is this change related to this PR? btw, could we add tests to check if a `since` field is defined in all the expressions? I think it is important to define this field when adding a new expr. ",
        "createdAt" : "2020-08-30T12:58:42Z",
        "updatedAt" : "2020-08-30T12:58:43Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "af42c905-063f-4863-817d-463d74917f32",
        "parentId" : "999133b7-e261-4925-8626-c13637b1d8a5",
        "authorId" : "a8e23d47-3ae4-4385-848c-38a216d1bd08",
        "body" : "In master there are currently 67 expressions without the since tag:\r\n```\r\n  test(\"Since has a valid value\") {\r\n    val badExpressions = spark.sessionState.functionRegistry.listFunction()\r\n      .map(spark.sessionState.catalog.lookupFunctionInfo)\r\n      .filter(funcInfo => !funcInfo.getSince.matches(\"[0-9]+\\\\.[0-9]+\\\\.[0-9]+\"))\r\n      .map(_.getClassName)\r\n      .distinct\r\n      .sorted\r\n\r\n    if (badExpressions.nonEmpty) {\r\n      fail(s\"${badExpressions.length} expressions with invalid 'since':\\n\"\r\n        + badExpressions.mkString(\"\\n\"))\r\n    }\r\n  }\r\n```\r\n\r\n```\r\n[info] - Since has a valid value *** FAILED *** (16 milliseconds)\r\n[info]   67 expressions with invalid 'since':\r\n[info]   org.apache.spark.sql.catalyst.expressions.Abs\r\n[info]   org.apache.spark.sql.catalyst.expressions.Add\r\n[info]   org.apache.spark.sql.catalyst.expressions.And\r\n[info]   org.apache.spark.sql.catalyst.expressions.ArrayContains\r\n[info]   org.apache.spark.sql.catalyst.expressions.AssertTrue\r\n[info]   org.apache.spark.sql.catalyst.expressions.BitwiseAnd\r\n[info]   org.apache.spark.sql.catalyst.expressions.BitwiseNot\r\n[info]   org.apache.spark.sql.catalyst.expressions.BitwiseOr\r\n[info]   org.apache.spark.sql.catalyst.expressions.BitwiseXor\r\n[info]   org.apache.spark.sql.catalyst.expressions.CallMethodViaReflection\r\n[info]   org.apache.spark.sql.catalyst.expressions.CaseWhen\r\n[info]   org.apache.spark.sql.catalyst.expressions.Cast\r\n[info]   org.apache.spark.sql.catalyst.expressions.Concat\r\n[info]   org.apache.spark.sql.catalyst.expressions.Crc32\r\n[info]   org.apache.spark.sql.catalyst.expressions.CreateArray\r\n[info]   org.apache.spark.sql.catalyst.expressions.CreateMap\r\n[info]   org.apache.spark.sql.catalyst.expressions.CreateNamedStruct\r\n[info]   org.apache.spark.sql.catalyst.expressions.CurrentDatabase\r\n[info]   org.apache.spark.sql.catalyst.expressions.Divide\r\n[info]   org.apache.spark.sql.catalyst.expressions.EqualNullSafe\r\n[info]   org.apache.spark.sql.catalyst.expressions.EqualTo\r\n[info]   org.apache.spark.sql.catalyst.expressions.Explode\r\n[info]   org.apache.spark.sql.catalyst.expressions.GetJsonObject\r\n[info]   org.apache.spark.sql.catalyst.expressions.GreaterThan\r\n[info]   org.apache.spark.sql.catalyst.expressions.GreaterThanOrEqual\r\n[info]   org.apache.spark.sql.catalyst.expressions.Greatest\r\n[info]   org.apache.spark.sql.catalyst.expressions.If\r\n[info]   org.apache.spark.sql.catalyst.expressions.In\r\n[info]   org.apache.spark.sql.catalyst.expressions.Inline\r\n[info]   org.apache.spark.sql.catalyst.expressions.InputFileBlockLength\r\n[info]   org.apache.spark.sql.catalyst.expressions.InputFileBlockStart\r\n[info]   org.apache.spark.sql.catalyst.expressions.InputFileName\r\n[info]   org.apache.spark.sql.catalyst.expressions.JsonTuple\r\n[info]   org.apache.spark.sql.catalyst.expressions.Least\r\n[info]   org.apache.spark.sql.catalyst.expressions.LessThan\r\n[info]   org.apache.spark.sql.catalyst.expressions.LessThanOrEqual\r\n[info]   org.apache.spark.sql.catalyst.expressions.MapKeys\r\n[info]   org.apache.spark.sql.catalyst.expressions.MapValues\r\n[info]   org.apache.spark.sql.catalyst.expressions.Md5\r\n[info]   org.apache.spark.sql.catalyst.expressions.MonotonicallyIncreasingID\r\n[info]   org.apache.spark.sql.catalyst.expressions.Multiply\r\n[info]   org.apache.spark.sql.catalyst.expressions.Murmur3Hash\r\n[info]   org.apache.spark.sql.catalyst.expressions.Not\r\n[info]   org.apache.spark.sql.catalyst.expressions.Or\r\n[info]   org.apache.spark.sql.catalyst.expressions.Overlay\r\n[info]   org.apache.spark.sql.catalyst.expressions.Pmod\r\n[info]   org.apache.spark.sql.catalyst.expressions.PosExplode\r\n[info]   org.apache.spark.sql.catalyst.expressions.Remainder\r\n[info]   org.apache.spark.sql.catalyst.expressions.Sha1\r\n[info]   org.apache.spark.sql.catalyst.expressions.Sha2\r\n[info]   org.apache.spark.sql.catalyst.expressions.Size\r\n[info]   org.apache.spark.sql.catalyst.expressions.SortArray\r\n[info]   org.apache.spark.sql.catalyst.expressions.SparkPartitionID\r\n[info]   org.apache.spark.sql.catalyst.expressions.Stack\r\n[info]   org.apache.spark.sql.catalyst.expressions.Subtract\r\n[info]   org.apache.spark.sql.catalyst.expressions.TimeWindow\r\n[info]   org.apache.spark.sql.catalyst.expressions.UnaryMinus\r\n[info]   org.apache.spark.sql.catalyst.expressions.UnaryPositive\r\n[info]   org.apache.spark.sql.catalyst.expressions.Uuid\r\n[info]   org.apache.spark.sql.catalyst.expressions.xml.XPathBoolean\r\n[info]   org.apache.spark.sql.catalyst.expressions.xml.XPathDouble\r\n[info]   org.apache.spark.sql.catalyst.expressions.xml.XPathFloat\r\n[info]   org.apache.spark.sql.catalyst.expressions.xml.XPathInt\r\n[info]   org.apache.spark.sql.catalyst.expressions.xml.XPathList\r\n[info]   org.apache.spark.sql.catalyst.expressions.xml.XPathLong\r\n[info]   org.apache.spark.sql.catalyst.expressions.xml.XPathShort\r\n[info]   org.apache.spark.sql.catalyst.expressions.xml.XPathString (ExpressionInfoSuite.scala:204)\r\n```",
        "createdAt" : "2020-08-30T17:22:01Z",
        "updatedAt" : "2020-08-30T17:22:01Z",
        "lastEditedBy" : "a8e23d47-3ae4-4385-848c-38a216d1bd08",
        "tags" : [
        ]
      },
      {
        "id" : "8d7c5019-7cf5-478b-9f6c-f26da8e8ba7c",
        "parentId" : "999133b7-e261-4925-8626-c13637b1d8a5",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Thanks for the check, @tanelk. Ah, I see. I think its better to add a `since` tag for the expressions above and tests in a separate PR.",
        "createdAt" : "2020-08-31T00:43:42Z",
        "updatedAt" : "2020-08-31T00:43:42Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "b4ef280b-9c60-4bca-b039-49a461450cad",
        "parentId" : "999133b7-e261-4925-8626-c13637b1d8a5",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "thanks a bunch for the check~",
        "createdAt" : "2020-08-31T02:06:22Z",
        "updatedAt" : "2020-08-31T02:06:22Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "03e4c5d0-6780-4647-adf3-fac5b9bde583",
        "parentId" : "999133b7-e261-4925-8626-c13637b1d8a5",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "@tanelk You're working on that? Could you file a jira for it?",
        "createdAt" : "2020-08-31T07:01:24Z",
        "updatedAt" : "2020-08-31T07:01:24Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "9073692a-c1e6-4c7a-a4c6-bec6d507ae8c",
        "parentId" : "999133b7-e261-4925-8626-c13637b1d8a5",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Filed: https://issues.apache.org/jira/browse/SPARK-32780",
        "createdAt" : "2020-09-02T23:35:52Z",
        "updatedAt" : "2020-09-02T23:35:52Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "8ca2af7a37211dbd2e1c60eb5b0969174912f66d",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +543,547 @@       Structured SQL\n  \"\"\",\n  since = \"3.0.0\")\n// scalastyle:on line.size.limit\ncase class Overlay(input: Expression, replace: Expression, pos: Expression, len: Expression)"
  },
  {
    "id" : "cf2c52f7-8a5e-47bb-adb2-6c515398f576",
    "prId" : 29052,
    "prUrl" : "https://github.com/apache/spark/pull/29052#pullrequestreview-445452984",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7ef8f5e6-7715-4f9c-bdea-e5dc2f594f85",
        "parentId" : null,
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "the expected input types here should match child expressions. The i-th position in the returned seq indicates the type requirement for the i-th child.\r\n\r\nSo I guess this change is wrong.",
        "createdAt" : "2020-07-09T09:04:37Z",
        "updatedAt" : "2020-07-09T09:04:37Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "c7498739-6f85-44d8-9c9f-cf436409bf95",
        "parentId" : "7ef8f5e6-7715-4f9c-bdea-e5dc2f594f85",
        "authorId" : "a67e5600-3f69-4b96-9c53-73795f48a0f6",
        "body" : "Please ignore me~",
        "createdAt" : "2020-07-09T09:39:40Z",
        "updatedAt" : "2020-07-09T09:39:41Z",
        "lastEditedBy" : "a67e5600-3f69-4b96-9c53-73795f48a0f6",
        "tags" : [
        ]
      }
    ],
    "commit" : "91e02dce69ef52fe289dcd41a0c55a9e8db59161",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1160,1164 @@  override def nullable: Boolean = substr.nullable || str.nullable\n  override def dataType: DataType = IntegerType\n  override def inputTypes: Seq[DataType] = Seq(StringType, IntegerType)\n\n  override def eval(input: InternalRow): Any = {"
  },
  {
    "id" : "2403e381-4953-4b22-bc9a-07a5b0050335",
    "prId" : 28831,
    "prUrl" : "https://github.com/apache/spark/pull/28831#pullrequestreview-433723726",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c08b6fcf-6638-4b70-a713-8fbbf80a8c50",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Don't we need to evaluate `evals.head.code`? Previously it is in `codes` but now `argBuilds` only includes `children.tail`.",
        "createdAt" : "2020-06-19T00:02:42Z",
        "updatedAt" : "2020-06-19T00:18:30Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "0bf46276-a70c-4618-834c-584edbf004f5",
        "parentId" : "c08b6fcf-6638-4b70-a713-8fbbf80a8c50",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "~~If I understand correctly, evals.head is a delimiter (so that doesn't need to be coupled with varargXXX), and it's used in `UTF8String.concatWs`.~~",
        "createdAt" : "2020-06-19T00:05:29Z",
        "updatedAt" : "2020-06-19T00:18:30Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "5a2b6e98-b168-4fab-b598-96242b40caa9",
        "parentId" : "c08b6fcf-6638-4b70-a713-8fbbf80a8c50",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Ah OK I see the case, where delimiter is not literal. I'll add it. Thanks!",
        "createdAt" : "2020-06-19T00:08:22Z",
        "updatedAt" : "2020-06-19T00:18:30Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "2477d119fd1de18a07e02d0b0d5e19f268472d9b",
    "line" : 105,
    "diffHunk" : "@@ -1,1 +211,215 @@        boolean[] $isNullArgs = new boolean[${children.length - 1}];\n        Object[] $valueArgs = new Object[${children.length - 1}];\n        $argBuilds\n        int $varargNum = ${children.count(_.dataType == StringType) - 1};\n        int $idxVararg = 0;"
  },
  {
    "id" : "fdb70871-584a-43aa-8b50-284d13b77e54",
    "prId" : 25963,
    "prUrl" : "https://github.com/apache/spark/pull/25963#pullrequestreview-300618834",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9e619d66-3e6a-4f77-94c6-6194990dce05",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "This comments duplicates the `usage` part. I don't think it is useful.",
        "createdAt" : "2019-10-09T14:04:38Z",
        "updatedAt" : "2019-10-21T03:16:03Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "d3726f5d-7f77-4ed6-b4bd-8bbf275391e0",
        "parentId" : "9e619d66-3e6a-4f77-94c6-6194990dce05",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "I will preserve this line and change the `usage` part as `Convert `strExpr` to a number based on the `patternExpr`.`",
        "createdAt" : "2019-10-11T11:15:46Z",
        "updatedAt" : "2019-10-21T03:16:03Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "afa19a89d786e4fa4158487ab1748ba11a5c69dc",
    "line" : 65,
    "diffHunk" : "@@ -1,1 +2329,2333 @@\n/**\n * A function that converts string to numeric.\n */\n@ExpressionDescription("
  }
]