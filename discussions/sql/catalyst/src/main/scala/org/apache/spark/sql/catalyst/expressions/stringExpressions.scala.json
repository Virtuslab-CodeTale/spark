[
  {
    "id" : "d0e389f6-f58a-4036-b38d-1a1596f96bfa",
    "prId" : 31448,
    "prUrl" : "https://github.com/apache/spark/pull/31448#pullrequestreview-608288509",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "101e7e8f-dfbe-494a-8e3d-69fe82c02566",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "It's a bit weird to define a format pattern string only for parsing. Are we going to add the formatting function?",
        "createdAt" : "2021-03-09T12:43:54Z",
        "updatedAt" : "2021-03-09T12:43:54Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "bc8b2a9f-fb86-4848-b031-4f23d5c92741",
        "parentId" : "101e7e8f-dfbe-494a-8e3d-69fe82c02566",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "I got it.",
        "createdAt" : "2021-03-10T02:51:45Z",
        "updatedAt" : "2021-03-10T02:51:45Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "c4b2ce6e20f49a85a91da94c457eba184bb26826",
    "line" : 101,
    "diffHunk" : "@@ -1,1 +2510,2514 @@      'D':  decimal point (uses locale)\n      'G':  group separator (uses locale)\n      '$':  specifies that the input value has a leading $ (Dollar) sign.\n  \"\"\",\n  examples = \"\"\""
  },
  {
    "id" : "3c31998b-0636-4fef-b0db-99a56784cc29",
    "prId" : 30479,
    "prUrl" : "https://github.com/apache/spark/pull/30479#pullrequestreview-549816617",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "72eac6e7-af01-4b9d-9c9d-1ba47f943544",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "hm, IMHO it would be good to rename the existing `decode` func or assign a new name into the new `decode` func because the generated doc for `decode` will be complicated.",
        "createdAt" : "2020-12-09T07:12:16Z",
        "updatedAt" : "2020-12-09T09:13:00Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "467fc0f0-ea51-4d82-bd99-acae8818bd70",
        "parentId" : "72eac6e7-af01-4b9d-9c9d-1ba47f943544",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "If rename the `existing` decode func will break the behavior. ",
        "createdAt" : "2020-12-09T07:36:13Z",
        "updatedAt" : "2020-12-09T09:13:00Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      },
      {
        "id" : "94a51e9c-b862-4e47-ac4f-fcd5aa8a7095",
        "parentId" : "72eac6e7-af01-4b9d-9c9d-1ba47f943544",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Yea, so I think we need add a legacy config, update the migration guide, ... if we rename it. Anyway, the latter approach (assigning a new name into the new `decode` func) looks better if possible.",
        "createdAt" : "2020-12-09T07:48:32Z",
        "updatedAt" : "2020-12-09T09:13:00Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "d19ebdcd-5860-4062-a52b-b7768a52c2c3",
        "parentId" : "72eac6e7-af01-4b9d-9c9d-1ba47f943544",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "This PR could avoid migration and support the new `decode` as well. cc @cloud-fan ",
        "createdAt" : "2020-12-09T08:40:13Z",
        "updatedAt" : "2020-12-09T09:13:00Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      },
      {
        "id" : "fd258f76-c83a-4731-a343-a91e8a11cc0a",
        "parentId" : "72eac6e7-af01-4b9d-9c9d-1ba47f943544",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "We can probably rename the function in ansi mode.",
        "createdAt" : "2020-12-11T05:46:21Z",
        "updatedAt" : "2020-12-11T05:46:22Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "eea967a4aa4ad18760c5a65c275c1b3f68971f85",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +2090,2094 @@        throw new AnalysisException(\"Invalid number of arguments for function decode. \" +\n          s\"Expected: 2; Found: ${params.length}\")\n      case 2 => StringDecode(params.head, params.last)\n      case _ =>\n        val input = params.head"
  },
  {
    "id" : "07479b20-4921-4afe-8c49-5aaf19b1ae1a",
    "prId" : 30479,
    "prUrl" : "https://github.com/apache/spark/pull/30479#pullrequestreview-547909412",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d20d288d-e075-457a-a87c-c8586ed01e84",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Could you follow the other format? e.g., https://github.com/apache/spark/blob/c88eddac3bf860d04bba91fc913f8b2069a94153/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/stringExpressions.scala#L479-L486",
        "createdAt" : "2020-12-09T07:16:22Z",
        "updatedAt" : "2020-12-09T09:13:00Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "969e1545-614f-44e4-8e8a-444022729e9d",
        "parentId" : "d20d288d-e075-457a-a87c-c8586ed01e84",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "The new `decode` function is the same as other function uses `RuntimeReplaceable`. The parameter list is not fixed.",
        "createdAt" : "2020-12-09T07:41:02Z",
        "updatedAt" : "2020-12-09T09:13:00Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "eea967a4aa4ad18760c5a65c275c1b3f68971f85",
    "line" : 47,
    "diffHunk" : "@@ -1,1 +2119,2123 @@            |  to each search value one by one. If expr is equal to a search, returns the corresponding result.\n            |  If no match is found, then Oracle returns default. If default is omitted, returns null.\n          \"\"\",\n  examples = \"\"\"\n    Examples:"
  },
  {
    "id" : "5ebcd09e-a278-4dc9-96b4-11d473dfbf13",
    "prId" : 30479,
    "prUrl" : "https://github.com/apache/spark/pull/30479#pullrequestreview-549823403",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "376e68fe-1e10-44f4-9815-96133bb66379",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "So this replaces the existing `decode` in SQL side only, right? shall we update migration guide?",
        "createdAt" : "2020-12-11T05:55:37Z",
        "updatedAt" : "2020-12-11T05:55:37Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "b3da8dfb-08c1-4497-96c3-ad6f6b9c70d0",
        "parentId" : "376e68fe-1e10-44f4-9815-96133bb66379",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "The new `decode` could keep compatible with old one.",
        "createdAt" : "2020-12-11T06:06:52Z",
        "updatedAt" : "2020-12-11T06:07:01Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "eea967a4aa4ad18760c5a65c275c1b3f68971f85",
    "line" : 61,
    "diffHunk" : "@@ -1,1 +2133,2137 @@  since = \"3.2.0\")\n// scalastyle:on line.size.limit\ncase class Decode(params: Seq[Expression], child: Expression) extends RuntimeReplaceable {\n\n  def this(params: Seq[Expression]) = {"
  },
  {
    "id" : "38cf5ac5-526c-4ecd-bf0b-b49b274151cf",
    "prId" : 30479,
    "prUrl" : "https://github.com/apache/spark/pull/30479#pullrequestreview-549823447",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7002a47a-28ab-4556-9e80-220d0b34bcf0",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Do we have an alternative for the previous `decode` after this change?",
        "createdAt" : "2020-12-11T05:56:50Z",
        "updatedAt" : "2020-12-11T05:56:50Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "8700a284-a318-4c72-86b7-ca4e9d9d2c8b",
        "parentId" : "7002a47a-28ab-4556-9e80-220d0b34bcf0",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "This only renames the expression, not the function. This PR just adds an overload of the `decode` function when there are more than 3 parameters.",
        "createdAt" : "2020-12-11T06:00:45Z",
        "updatedAt" : "2020-12-11T06:00:46Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "a2edd5b6-6bab-40ce-b70c-a596026f77f9",
        "parentId" : "7002a47a-28ab-4556-9e80-220d0b34bcf0",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I'd suggest the following followup:\r\n1. add a new function name for `StringDecode`. Probably \"string_decode\".\r\n2. require `decode` function to take more than 2 parameters, under ansi mode(or a legacy config)",
        "createdAt" : "2020-12-11T06:06:58Z",
        "updatedAt" : "2020-12-11T06:06:58Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "eea967a4aa4ad18760c5a65c275c1b3f68971f85",
    "line" : 79,
    "diffHunk" : "@@ -1,1 +2158,2162 @@  since = \"1.5.0\")\n// scalastyle:on line.size.limit\ncase class StringDecode(bin: Expression, charset: Expression)\n  extends BinaryExpression with ImplicitCastInputTypes with NullIntolerant {\n"
  },
  {
    "id" : "74bdaaef-3816-4881-b99f-4ad1219e5557",
    "prId" : 30399,
    "prUrl" : "https://github.com/apache/spark/pull/30399#pullrequestreview-532559298",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6fb45619-662c-4a56-a21d-4ebc20e24033",
        "parentId" : null,
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "To clarify, the raw exception message is something like `Illegal character in query at index 33: https://a.b.c/index.php?params1=a|b&params2=x`.",
        "createdAt" : "2020-11-17T16:34:08Z",
        "updatedAt" : "2020-11-20T02:42:53Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      }
    ],
    "commit" : "d7e437b326dc9564a9460946bdbc0856e6876322",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +1407,1411 @@    } catch {\n      case e: URISyntaxException if failOnError =>\n        throw new IllegalArgumentException(s\"Find an invaild url string ${url.toString}\", e)\n      case _: URISyntaxException => null\n    }"
  },
  {
    "id" : "ef2eeeb3-b51e-4fb1-91bb-cb8759de7ebb",
    "prId" : 30297,
    "prUrl" : "https://github.com/apache/spark/pull/30297#pullrequestreview-526881857",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c315b3ad-c8ab-4997-83ef-d6cb55e0fcc1",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "The same: https://github.com/apache/spark/pull/30297/files#r520305929",
        "createdAt" : "2020-11-10T06:05:15Z",
        "updatedAt" : "2020-11-12T05:25:04Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "4b161a4ef4f3be2f1a239c0c19f8aa62b5a03706",
    "line" : 27,
    "diffHunk" : "@@ -1,1 +246,250 @@  since = \"2.0.0\")\n// scalastyle:on line.size.limit\ncase class Elt(\n    children: Seq[Expression],\n    failOnError: Boolean = SQLConf.get.ansiEnabled) extends Expression {"
  },
  {
    "id" : "3a31e0dc-eef0-41d7-9e07-03a0e357b217",
    "prId" : 29577,
    "prUrl" : "https://github.com/apache/spark/pull/29577#pullrequestreview-481382988",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "999133b7-e261-4925-8626-c13637b1d8a5",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Is this change related to this PR? btw, could we add tests to check if a `since` field is defined in all the expressions? I think it is important to define this field when adding a new expr. ",
        "createdAt" : "2020-08-30T12:58:42Z",
        "updatedAt" : "2020-08-30T12:58:43Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "af42c905-063f-4863-817d-463d74917f32",
        "parentId" : "999133b7-e261-4925-8626-c13637b1d8a5",
        "authorId" : "a8e23d47-3ae4-4385-848c-38a216d1bd08",
        "body" : "In master there are currently 67 expressions without the since tag:\r\n```\r\n  test(\"Since has a valid value\") {\r\n    val badExpressions = spark.sessionState.functionRegistry.listFunction()\r\n      .map(spark.sessionState.catalog.lookupFunctionInfo)\r\n      .filter(funcInfo => !funcInfo.getSince.matches(\"[0-9]+\\\\.[0-9]+\\\\.[0-9]+\"))\r\n      .map(_.getClassName)\r\n      .distinct\r\n      .sorted\r\n\r\n    if (badExpressions.nonEmpty) {\r\n      fail(s\"${badExpressions.length} expressions with invalid 'since':\\n\"\r\n        + badExpressions.mkString(\"\\n\"))\r\n    }\r\n  }\r\n```\r\n\r\n```\r\n[info] - Since has a valid value *** FAILED *** (16 milliseconds)\r\n[info]   67 expressions with invalid 'since':\r\n[info]   org.apache.spark.sql.catalyst.expressions.Abs\r\n[info]   org.apache.spark.sql.catalyst.expressions.Add\r\n[info]   org.apache.spark.sql.catalyst.expressions.And\r\n[info]   org.apache.spark.sql.catalyst.expressions.ArrayContains\r\n[info]   org.apache.spark.sql.catalyst.expressions.AssertTrue\r\n[info]   org.apache.spark.sql.catalyst.expressions.BitwiseAnd\r\n[info]   org.apache.spark.sql.catalyst.expressions.BitwiseNot\r\n[info]   org.apache.spark.sql.catalyst.expressions.BitwiseOr\r\n[info]   org.apache.spark.sql.catalyst.expressions.BitwiseXor\r\n[info]   org.apache.spark.sql.catalyst.expressions.CallMethodViaReflection\r\n[info]   org.apache.spark.sql.catalyst.expressions.CaseWhen\r\n[info]   org.apache.spark.sql.catalyst.expressions.Cast\r\n[info]   org.apache.spark.sql.catalyst.expressions.Concat\r\n[info]   org.apache.spark.sql.catalyst.expressions.Crc32\r\n[info]   org.apache.spark.sql.catalyst.expressions.CreateArray\r\n[info]   org.apache.spark.sql.catalyst.expressions.CreateMap\r\n[info]   org.apache.spark.sql.catalyst.expressions.CreateNamedStruct\r\n[info]   org.apache.spark.sql.catalyst.expressions.CurrentDatabase\r\n[info]   org.apache.spark.sql.catalyst.expressions.Divide\r\n[info]   org.apache.spark.sql.catalyst.expressions.EqualNullSafe\r\n[info]   org.apache.spark.sql.catalyst.expressions.EqualTo\r\n[info]   org.apache.spark.sql.catalyst.expressions.Explode\r\n[info]   org.apache.spark.sql.catalyst.expressions.GetJsonObject\r\n[info]   org.apache.spark.sql.catalyst.expressions.GreaterThan\r\n[info]   org.apache.spark.sql.catalyst.expressions.GreaterThanOrEqual\r\n[info]   org.apache.spark.sql.catalyst.expressions.Greatest\r\n[info]   org.apache.spark.sql.catalyst.expressions.If\r\n[info]   org.apache.spark.sql.catalyst.expressions.In\r\n[info]   org.apache.spark.sql.catalyst.expressions.Inline\r\n[info]   org.apache.spark.sql.catalyst.expressions.InputFileBlockLength\r\n[info]   org.apache.spark.sql.catalyst.expressions.InputFileBlockStart\r\n[info]   org.apache.spark.sql.catalyst.expressions.InputFileName\r\n[info]   org.apache.spark.sql.catalyst.expressions.JsonTuple\r\n[info]   org.apache.spark.sql.catalyst.expressions.Least\r\n[info]   org.apache.spark.sql.catalyst.expressions.LessThan\r\n[info]   org.apache.spark.sql.catalyst.expressions.LessThanOrEqual\r\n[info]   org.apache.spark.sql.catalyst.expressions.MapKeys\r\n[info]   org.apache.spark.sql.catalyst.expressions.MapValues\r\n[info]   org.apache.spark.sql.catalyst.expressions.Md5\r\n[info]   org.apache.spark.sql.catalyst.expressions.MonotonicallyIncreasingID\r\n[info]   org.apache.spark.sql.catalyst.expressions.Multiply\r\n[info]   org.apache.spark.sql.catalyst.expressions.Murmur3Hash\r\n[info]   org.apache.spark.sql.catalyst.expressions.Not\r\n[info]   org.apache.spark.sql.catalyst.expressions.Or\r\n[info]   org.apache.spark.sql.catalyst.expressions.Overlay\r\n[info]   org.apache.spark.sql.catalyst.expressions.Pmod\r\n[info]   org.apache.spark.sql.catalyst.expressions.PosExplode\r\n[info]   org.apache.spark.sql.catalyst.expressions.Remainder\r\n[info]   org.apache.spark.sql.catalyst.expressions.Sha1\r\n[info]   org.apache.spark.sql.catalyst.expressions.Sha2\r\n[info]   org.apache.spark.sql.catalyst.expressions.Size\r\n[info]   org.apache.spark.sql.catalyst.expressions.SortArray\r\n[info]   org.apache.spark.sql.catalyst.expressions.SparkPartitionID\r\n[info]   org.apache.spark.sql.catalyst.expressions.Stack\r\n[info]   org.apache.spark.sql.catalyst.expressions.Subtract\r\n[info]   org.apache.spark.sql.catalyst.expressions.TimeWindow\r\n[info]   org.apache.spark.sql.catalyst.expressions.UnaryMinus\r\n[info]   org.apache.spark.sql.catalyst.expressions.UnaryPositive\r\n[info]   org.apache.spark.sql.catalyst.expressions.Uuid\r\n[info]   org.apache.spark.sql.catalyst.expressions.xml.XPathBoolean\r\n[info]   org.apache.spark.sql.catalyst.expressions.xml.XPathDouble\r\n[info]   org.apache.spark.sql.catalyst.expressions.xml.XPathFloat\r\n[info]   org.apache.spark.sql.catalyst.expressions.xml.XPathInt\r\n[info]   org.apache.spark.sql.catalyst.expressions.xml.XPathList\r\n[info]   org.apache.spark.sql.catalyst.expressions.xml.XPathLong\r\n[info]   org.apache.spark.sql.catalyst.expressions.xml.XPathShort\r\n[info]   org.apache.spark.sql.catalyst.expressions.xml.XPathString (ExpressionInfoSuite.scala:204)\r\n```",
        "createdAt" : "2020-08-30T17:22:01Z",
        "updatedAt" : "2020-08-30T17:22:01Z",
        "lastEditedBy" : "a8e23d47-3ae4-4385-848c-38a216d1bd08",
        "tags" : [
        ]
      },
      {
        "id" : "8d7c5019-7cf5-478b-9f6c-f26da8e8ba7c",
        "parentId" : "999133b7-e261-4925-8626-c13637b1d8a5",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Thanks for the check, @tanelk. Ah, I see. I think its better to add a `since` tag for the expressions above and tests in a separate PR.",
        "createdAt" : "2020-08-31T00:43:42Z",
        "updatedAt" : "2020-08-31T00:43:42Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "b4ef280b-9c60-4bca-b039-49a461450cad",
        "parentId" : "999133b7-e261-4925-8626-c13637b1d8a5",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "thanks a bunch for the check~",
        "createdAt" : "2020-08-31T02:06:22Z",
        "updatedAt" : "2020-08-31T02:06:22Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "03e4c5d0-6780-4647-adf3-fac5b9bde583",
        "parentId" : "999133b7-e261-4925-8626-c13637b1d8a5",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "@tanelk You're working on that? Could you file a jira for it?",
        "createdAt" : "2020-08-31T07:01:24Z",
        "updatedAt" : "2020-08-31T07:01:24Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "9073692a-c1e6-4c7a-a4c6-bec6d507ae8c",
        "parentId" : "999133b7-e261-4925-8626-c13637b1d8a5",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Filed: https://issues.apache.org/jira/browse/SPARK-32780",
        "createdAt" : "2020-09-02T23:35:52Z",
        "updatedAt" : "2020-09-02T23:35:52Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "8ca2af7a37211dbd2e1c60eb5b0969174912f66d",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +543,547 @@       Structured SQL\n  \"\"\",\n  since = \"3.0.0\")\n// scalastyle:on line.size.limit\ncase class Overlay(input: Expression, replace: Expression, pos: Expression, len: Expression)"
  },
  {
    "id" : "cf2c52f7-8a5e-47bb-adb2-6c515398f576",
    "prId" : 29052,
    "prUrl" : "https://github.com/apache/spark/pull/29052#pullrequestreview-445452984",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7ef8f5e6-7715-4f9c-bdea-e5dc2f594f85",
        "parentId" : null,
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "the expected input types here should match child expressions. The i-th position in the returned seq indicates the type requirement for the i-th child.\r\n\r\nSo I guess this change is wrong.",
        "createdAt" : "2020-07-09T09:04:37Z",
        "updatedAt" : "2020-07-09T09:04:37Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "c7498739-6f85-44d8-9c9f-cf436409bf95",
        "parentId" : "7ef8f5e6-7715-4f9c-bdea-e5dc2f594f85",
        "authorId" : "a67e5600-3f69-4b96-9c53-73795f48a0f6",
        "body" : "Please ignore me~",
        "createdAt" : "2020-07-09T09:39:40Z",
        "updatedAt" : "2020-07-09T09:39:41Z",
        "lastEditedBy" : "a67e5600-3f69-4b96-9c53-73795f48a0f6",
        "tags" : [
        ]
      }
    ],
    "commit" : "91e02dce69ef52fe289dcd41a0c55a9e8db59161",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1160,1164 @@  override def nullable: Boolean = substr.nullable || str.nullable\n  override def dataType: DataType = IntegerType\n  override def inputTypes: Seq[DataType] = Seq(StringType, IntegerType)\n\n  override def eval(input: InternalRow): Any = {"
  },
  {
    "id" : "2403e381-4953-4b22-bc9a-07a5b0050335",
    "prId" : 28831,
    "prUrl" : "https://github.com/apache/spark/pull/28831#pullrequestreview-433723726",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c08b6fcf-6638-4b70-a713-8fbbf80a8c50",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Don't we need to evaluate `evals.head.code`? Previously it is in `codes` but now `argBuilds` only includes `children.tail`.",
        "createdAt" : "2020-06-19T00:02:42Z",
        "updatedAt" : "2020-06-19T00:18:30Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "0bf46276-a70c-4618-834c-584edbf004f5",
        "parentId" : "c08b6fcf-6638-4b70-a713-8fbbf80a8c50",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "~~If I understand correctly, evals.head is a delimiter (so that doesn't need to be coupled with varargXXX), and it's used in `UTF8String.concatWs`.~~",
        "createdAt" : "2020-06-19T00:05:29Z",
        "updatedAt" : "2020-06-19T00:18:30Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "5a2b6e98-b168-4fab-b598-96242b40caa9",
        "parentId" : "c08b6fcf-6638-4b70-a713-8fbbf80a8c50",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Ah OK I see the case, where delimiter is not literal. I'll add it. Thanks!",
        "createdAt" : "2020-06-19T00:08:22Z",
        "updatedAt" : "2020-06-19T00:18:30Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "2477d119fd1de18a07e02d0b0d5e19f268472d9b",
    "line" : 105,
    "diffHunk" : "@@ -1,1 +211,215 @@        boolean[] $isNullArgs = new boolean[${children.length - 1}];\n        Object[] $valueArgs = new Object[${children.length - 1}];\n        $argBuilds\n        int $varargNum = ${children.count(_.dataType == StringType) - 1};\n        int $idxVararg = 0;"
  },
  {
    "id" : "fdb70871-584a-43aa-8b50-284d13b77e54",
    "prId" : 25963,
    "prUrl" : "https://github.com/apache/spark/pull/25963#pullrequestreview-300618834",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9e619d66-3e6a-4f77-94c6-6194990dce05",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "This comments duplicates the `usage` part. I don't think it is useful.",
        "createdAt" : "2019-10-09T14:04:38Z",
        "updatedAt" : "2019-10-21T03:16:03Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "d3726f5d-7f77-4ed6-b4bd-8bbf275391e0",
        "parentId" : "9e619d66-3e6a-4f77-94c6-6194990dce05",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "I will preserve this line and change the `usage` part as `Convert `strExpr` to a number based on the `patternExpr`.`",
        "createdAt" : "2019-10-11T11:15:46Z",
        "updatedAt" : "2019-10-21T03:16:03Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "afa19a89d786e4fa4158487ab1748ba11a5c69dc",
    "line" : 65,
    "diffHunk" : "@@ -1,1 +2329,2333 @@\n/**\n * A function that converts string to numeric.\n */\n@ExpressionDescription("
  },
  {
    "id" : "90132f6b-bbc5-4f11-90f7-c3c78a94cb1d",
    "prId" : 25172,
    "prUrl" : "https://github.com/apache/spark/pull/25172#pullrequestreview-265322592",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f89be9a9-6bb5-46a2-b828-1a28be6bafae",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Can't we expand this logic into generated code directly instead of calling this helper function?",
        "createdAt" : "2019-07-23T00:15:10Z",
        "updatedAt" : "2019-09-09T11:18:09Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "0b4d620f-0e63-4ad9-b730-0f36901dfee0",
        "parentId" : "f89be9a9-6bb5-46a2-b828-1a28be6bafae",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "I am not very good at using generated code. If this is a strong suggestion, I will try to use generated code. But I think this helper function is OK too.",
        "createdAt" : "2019-07-23T04:31:30Z",
        "updatedAt" : "2019-09-09T11:18:09Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      },
      {
        "id" : "b8a288bb-78f1-4349-ae98-3b616f300512",
        "parentId" : "f89be9a9-6bb5-46a2-b828-1a28be6bafae",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "cc: @viirya @mgaido91",
        "createdAt" : "2019-07-23T04:33:17Z",
        "updatedAt" : "2019-09-09T11:18:09Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "e9c81105-7402-489c-a380-0883a7d8c372",
        "parentId" : "f89be9a9-6bb5-46a2-b828-1a28be6bafae",
        "authorId" : "24e1dd39-ae3f-4bbb-a391-f60afb62d075",
        "body" : "we are already doing the same for the string case. I don't see strong reasons for inlining this in the generated code",
        "createdAt" : "2019-07-23T09:12:04Z",
        "updatedAt" : "2019-09-09T11:18:09Z",
        "lastEditedBy" : "24e1dd39-ae3f-4bbb-a391-f60afb62d075",
        "tags" : [
        ]
      },
      {
        "id" : "abe636f4-b0ba-4b82-bcf9-d80449c87f2d",
        "parentId" : "f89be9a9-6bb5-46a2-b828-1a28be6bafae",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "ok, thanks for the check, @mgaido91 ",
        "createdAt" : "2019-07-23T09:41:17Z",
        "updatedAt" : "2019-09-09T11:18:09Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "c5f1c237-b2b6-48ec-9090-210373e0152d",
        "parentId" : "f89be9a9-6bb5-46a2-b828-1a28be6bafae",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Seems good to me too.",
        "createdAt" : "2019-07-23T10:45:23Z",
        "updatedAt" : "2019-09-09T11:18:09Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "6201a3725036ee860f0d5c6d8b3053b79dbb1a62",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +485,489 @@    ByteArray.concat(ByteArray.subStringSQL(input, 1, pos - 1),\n      replace, ByteArray.subStringSQL(input, pos + length, Int.MaxValue))\n  }\n}\n"
  },
  {
    "id" : "5edc956a-6053-4319-9bfb-99312ff36e16",
    "prId" : 25172,
    "prUrl" : "https://github.com/apache/spark/pull/25172#pullrequestreview-265931103",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ae396c09-5313-4003-ae84-9b53cdfbf9a6",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "what if the case (string, binary) or (binary, string)?",
        "createdAt" : "2019-07-23T00:26:14Z",
        "updatedAt" : "2019-09-09T11:18:09Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "66492158-a5e5-4889-a656-7298965cc173",
        "parentId" : "ae396c09-5313-4003-ae84-9b53cdfbf9a6",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "This case not be allowed.",
        "createdAt" : "2019-07-23T02:29:10Z",
        "updatedAt" : "2019-09-09T11:18:09Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      },
      {
        "id" : "acbf6351-569b-498e-bf77-998acc533ae2",
        "parentId" : "ae396c09-5313-4003-ae84-9b53cdfbf9a6",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "It seems its accepted in pg;\r\n```\r\npostgres=# select overlay('aaaaaa' placing '000'::bytea from 2);\r\n    overlay     \r\n----------------\r\n \\x613030306161\r\n(1 row)\r\n```\r\nNot allowed in the other systems and the standard?",
        "createdAt" : "2019-07-23T03:24:39Z",
        "updatedAt" : "2019-09-09T11:18:09Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "404f797a-6055-427f-ae00-4280855c2fc7",
        "parentId" : "ae396c09-5313-4003-ae84-9b53cdfbf9a6",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "According ANSI SQL,  only (binary, binary) or (string, string)",
        "createdAt" : "2019-07-23T04:18:55Z",
        "updatedAt" : "2019-09-09T11:18:09Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      },
      {
        "id" : "139bcca0-b36e-44b3-b937-785111de7681",
        "parentId" : "ae396c09-5313-4003-ae84-9b53cdfbf9a6",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "```\r\n<character overlay function> ::=\r\nOVERLAY <left paren> <character value expression> PLACING <character value expression>\r\nFROM <start position> [ FOR <string length> ]\r\n[ USING <char length units> ] <right paren>\r\n```\r\nand\r\n```\r\n<binary overlay function> ::=\r\nOVERLAY <left paren> <binary value expression> PLACING <binary value expression>\r\nFROM <start position> [ FOR <string length> ] <right paren>\r\n```\r\nI can't find any other case.",
        "createdAt" : "2019-07-23T04:21:54Z",
        "updatedAt" : "2019-09-09T11:18:09Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      },
      {
        "id" : "20bdbcc5-88b7-4f03-bb9d-5bc411bed9a9",
        "parentId" : "ae396c09-5313-4003-ae84-9b53cdfbf9a6",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "I think this is an issue about implicit casts, not function arguments.\r\nIn the example above, the left argument (text) is casted as binary then `overlay(binary, binary)` is called actually. So, my question is that we need to extend `ImplicitCastInputTypes` for overlay?",
        "createdAt" : "2019-07-23T04:29:30Z",
        "updatedAt" : "2019-09-09T11:18:09Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "873d2e01-7c44-411b-8db7-51447357380a",
        "parentId" : "ae396c09-5313-4003-ae84-9b53cdfbf9a6",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "This is a good idea. I think the behavior of Postgresql's overlay seems a expand of ANSI standard. If we decide to do this, I will happy to change.",
        "createdAt" : "2019-07-23T08:06:42Z",
        "updatedAt" : "2019-09-09T11:18:09Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      },
      {
        "id" : "90f195c5-4c15-40e0-80f9-51cc679b034d",
        "parentId" : "ae396c09-5313-4003-ae84-9b53cdfbf9a6",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Just in case, I just want to know the other DBMS behaviours, could you check?",
        "createdAt" : "2019-07-24T01:37:17Z",
        "updatedAt" : "2019-09-09T11:18:09Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "931c027d-cdcb-4826-8e04-56f0ef5178a6",
        "parentId" : "ae396c09-5313-4003-ae84-9b53cdfbf9a6",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "As I know,\r\nVertica and DB2 only support `overlay(string, string)`.\r\nOracle support `overlay(string, string)`, `overlay(binary, binary)`, `overlay(string, binary)`, `overlay(binary, string)`.\r\n",
        "createdAt" : "2019-07-24T02:09:56Z",
        "updatedAt" : "2019-09-09T11:18:09Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      },
      {
        "id" : "228292eb-a34b-4b41-b63c-57be807c9152",
        "parentId" : "ae396c09-5313-4003-ae84-9b53cdfbf9a6",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "I just find `Flink` only support `overlay(string, string)` too.",
        "createdAt" : "2019-07-24T10:30:04Z",
        "updatedAt" : "2019-09-09T11:18:09Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "6201a3725036ee860f0d5c6d8b3053b79dbb1a62",
    "line" : 49,
    "diffHunk" : "@@ -1,1 +525,529 @@  override def children: Seq[Expression] = input :: replace :: pos :: len :: Nil\n\n  override def checkInputDataTypes(): TypeCheckResult = {\n    val inputTypeCheck = super.checkInputDataTypes()\n    if (inputTypeCheck.isSuccess) {"
  },
  {
    "id" : "b024e154-67dd-4952-a12f-25f4674fdc30",
    "prId" : 25172,
    "prUrl" : "https://github.com/apache/spark/pull/25172#pullrequestreview-285383953",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8cd307a9-e7d7-48dd-a386-3fefc7151420",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Can we add `examples`? You could use `bin` function.",
        "createdAt" : "2019-09-08T04:21:15Z",
        "updatedAt" : "2019-09-09T11:18:09Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "1f37564f-e225-4d90-92d5-e15b85a6129e",
        "parentId" : "8cd307a9-e7d7-48dd-a386-3fefc7151420",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "> Can we add `examples`? You could use `bin` function.\r\n\r\nSorry, what is `bin` function? I don't know it.",
        "createdAt" : "2019-09-09T02:04:10Z",
        "updatedAt" : "2019-09-09T11:18:09Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      },
      {
        "id" : "01c1002a-1bbe-4b1c-8062-269bf935b69f",
        "parentId" : "8cd307a9-e7d7-48dd-a386-3fefc7151420",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "`bin` function in SparkSQL:\r\n\r\n```\r\nspark-sql> SELECT bin(13);\r\n1101\r\n```",
        "createdAt" : "2019-09-09T06:25:35Z",
        "updatedAt" : "2019-09-09T11:18:09Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "193503f2-4661-463d-ba92-28cfff8a6694",
        "parentId" : "8cd307a9-e7d7-48dd-a386-3fefc7151420",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "OK. I know you means. But there are exists some examples. I don't know the reason to use `bin` function. Thanks.",
        "createdAt" : "2019-09-09T08:04:44Z",
        "updatedAt" : "2019-09-09T11:18:09Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      },
      {
        "id" : "413d46c7-8a55-4387-95a3-061f6ac57ca6",
        "parentId" : "8cd307a9-e7d7-48dd-a386-3fefc7151420",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I meant to add an example for this function under 502 lines. Since SparkSQL doesn't support binary literal you can use `bin` function in the example.",
        "createdAt" : "2019-09-09T08:39:46Z",
        "updatedAt" : "2019-09-09T11:18:09Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "08f20a5f-9b68-4f04-a6aa-d833919a879c",
        "parentId" : "8cd307a9-e7d7-48dd-a386-3fefc7151420",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "or you can use other expressions like encode or unbase64 in the example to add.",
        "createdAt" : "2019-09-09T08:52:33Z",
        "updatedAt" : "2019-09-09T11:18:09Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "8233d018-5f7c-4524-8634-08b2342bf69f",
        "parentId" : "8cd307a9-e7d7-48dd-a386-3fefc7151420",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "I think `bin` function used to translate long to binary but byte array.\r\nI will look up other function so that supplement the examples.",
        "createdAt" : "2019-09-09T09:30:46Z",
        "updatedAt" : "2019-09-09T11:18:09Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "6201a3725036ee860f0d5c6d8b3053b79dbb1a62",
    "line" : 36,
    "diffHunk" : "@@ -1,1 +515,519 @@\n  def this(str: Expression, replace: Expression, pos: Expression) = {\n    this(str, replace, pos, Literal.create(-1, IntegerType))\n  }\n"
  }
]