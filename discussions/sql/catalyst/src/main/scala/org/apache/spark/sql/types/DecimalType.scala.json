[
  {
    "id" : "cffdf0fa-5b6f-4e3b-bbe2-b0c92b961edb",
    "prId" : 26881,
    "prUrl" : "https://github.com/apache/spark/pull/26881#pullrequestreview-346018651",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3361805c-f2ce-4cca-b8ef-c9132e4312d2",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "nit: no need s\"\".",
        "createdAt" : "2020-01-21T16:33:09Z",
        "updatedAt" : "2020-01-21T16:33:09Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "563853b4d7b9a460c44b3a3de6552335d59fb3c0",
    "line" : 43,
    "diffHunk" : "@@ -1,1 +161,165 @@      throw new AnalysisException(s\"Negative scale is not allowed: $scale. \" +\n        s\"You can use spark.sql.legacy.allowNegativeScaleOfDecimal.enabled=true \" +\n        s\"to enable legacy mode to allow it.\")\n    }\n  }"
  },
  {
    "id" : "89b306ad-d9b8-4816-9b88-20a8b1d4870e",
    "prId" : 24849,
    "prUrl" : "https://github.com/apache/spark/pull/24849#pullrequestreview-248604490",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "84f7327b-4695-462e-833f-41057c782ed1",
        "parentId" : null,
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "This PR also added comment to explain why we can't upcast double/float to decimal.",
        "createdAt" : "2019-06-12T08:10:40Z",
        "updatedAt" : "2019-06-12T08:10:40Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "44b9fef2504d9d16740aaf247d4ed64848937710",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +78,82 @@    case dt: IntegralType =>\n      isWiderThan(DecimalType.forType(dt))\n    // For DoubleType/FloatType, the value can be NaN, PositiveInfinity or NegativeInfinity.\n    case _ => false\n  }"
  }
]