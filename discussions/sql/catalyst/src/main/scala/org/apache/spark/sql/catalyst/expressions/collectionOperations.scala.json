[
  {
    "id" : "9b3fe2f4-08ed-4516-8523-2176bcab95aa",
    "prId" : 33106,
    "prUrl" : "https://github.com/apache/spark/pull/33106#pullrequestreview-693604377",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4341e28f-a3c5-46c6-a428-9aa9e6bfa5c4",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "shall we follow `CreateStruct.apply` and use `NamePlaceholder`?",
        "createdAt" : "2021-06-28T06:47:03Z",
        "updatedAt" : "2021-06-28T06:47:03Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "ce5d0b163b2c5cefbcdeda57d812125e811a4e70",
    "line" : 25,
    "diffHunk" : "@@ -1,1 +188,192 @@    this(\n      children,\n      children.zipWithIndex.map {\n        case (u: UnresolvedAttribute, _) => Literal(u.nameParts.last)\n        case (e: NamedExpression, _) if e.resolved => Literal(e.name)"
  },
  {
    "id" : "d5a3e9ae-843c-47a0-99af-8f4857347580",
    "prId" : 32311,
    "prUrl" : "https://github.com/apache/spark/pull/32311#pullrequestreview-644114142",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "20243a75-df6e-48b4-ac19-f16009981c31",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "I am not sure it is good idea to use `timestampAddInterval()` in `InternalSequenceBase.eval` for adding months to dates. I guess `DateTimeUtils.dateAddMonths()` and `DateTimeUtils.timestampAddInterval` can return different result, especially taking into account that `dateAddMonths()` **does not depend** on the current time zone.",
        "createdAt" : "2021-04-23T15:31:08Z",
        "updatedAt" : "2021-04-26T03:28:44Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "8a91c80f-2e36-4597-99e1-dd8c33d45e53",
        "parentId" : "20243a75-df6e-48b4-ac19-f16009981c31",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "In fact, the current implement uses `DateTimeUtils.timestampAddInterval` and it's behavior seems good.",
        "createdAt" : "2021-04-25T09:09:26Z",
        "updatedAt" : "2021-04-26T03:28:44Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      },
      {
        "id" : "d54b0671-497b-4b6b-a6b0-3b18d0a281ff",
        "parentId" : "20243a75-df6e-48b4-ac19-f16009981c31",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "ok. Let's use `timestampAddInterval` since we don't have an example that could demonstrate any issues caused by `timestampAddInterval()`.",
        "createdAt" : "2021-04-25T09:11:51Z",
        "updatedAt" : "2021-04-26T03:28:44Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "47853e8610a0bf37d377b552cdf6739b3cbeee36",
    "line" : 145,
    "diffHunk" : "@@ -1,1 +2755,2759 @@  private class PeriodSequenceImpl[T: ClassTag]\n      (dt: IntegralType, scale: Long, fromLong: Long => T, zoneId: ZoneId)\n      (implicit num: Integral[T]) extends InternalSequenceBase(dt, scale, fromLong, zoneId) {\n\n    override val defaultStep: DefaultStep = new DefaultStep("
  },
  {
    "id" : "4e8d4d9c-44bc-4d34-8a3f-cf4a6a26d25b",
    "prId" : 32176,
    "prUrl" : "https://github.com/apache/spark/pull/32176#pullrequestreview-636675225",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "792ff0f7-f489-4e4f-829a-6825288210eb",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Created JIRA SPARK-35088 for ANSI intervals",
        "createdAt" : "2021-04-15T13:18:15Z",
        "updatedAt" : "2021-04-16T11:24:15Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "7668405ebd902cea698b76c373d0c06b1e759426",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +2505,2509 @@       [5,4,3,2,1]\n      > SET spark.sql.legacy.interval.enabled=true;\n       spark.sql.legacy.interval.enabled\ttrue\n      > SELECT _FUNC_(to_date('2018-01-01'), to_date('2018-03-01'), interval 1 month);\n       [2018-01-01,2018-02-01,2018-03-01]"
  },
  {
    "id" : "81bc38b2-30a2-49c0-8903-23e5c1b63f9e",
    "prId" : 30867,
    "prUrl" : "https://github.com/apache/spark/pull/30867#pullrequestreview-556099735",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bb85d463-3887-4dc8-a5f5-3c35db4367b1",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Sorry but this is not `Adds a group tag`. This is re-categorization.",
        "createdAt" : "2020-12-21T01:47:39Z",
        "updatedAt" : "2020-12-21T07:20:26Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "36c4aea7-be46-4157-9e91-0c4c5e1851e5",
        "parentId" : "bb85d463-3887-4dc8-a5f5-3c35db4367b1",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Ah, yea, you're right. I'll update the PR and the description.",
        "createdAt" : "2020-12-21T02:37:04Z",
        "updatedAt" : "2020-12-21T07:20:26Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "4e82580cdc1b44a1680631ad54121e327f37b9e0",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +1020,1024 @@       [3,4,1,2]\n  \"\"\",\n  group = \"collection_funcs\",\n  since = \"1.5.0\",\n  note = \"\"\""
  },
  {
    "id" : "be2af2bc-131e-4fcd-a0a1-e9710a9ce81d",
    "prId" : 30867,
    "prUrl" : "https://github.com/apache/spark/pull/30867#pullrequestreview-556089853",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d797ee86-53f6-4904-b5e2-325efe5a3a3b",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "ditto.",
        "createdAt" : "2020-12-21T01:48:38Z",
        "updatedAt" : "2020-12-21T07:20:26Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "4e82580cdc1b44a1680631ad54121e327f37b9e0",
    "line" : 34,
    "diffHunk" : "@@ -1,1 +2100,2104 @@    Concat logic for arrays is available since 2.4.0.\n  \"\"\",\n  group = \"collection_funcs\",\n  since = \"1.5.0\")\ncase class Concat(children: Seq[Expression]) extends ComplexTypeMergingExpression {"
  },
  {
    "id" : "503344ec-278f-4986-ae63-2ee2622ff3e8",
    "prId" : 30297,
    "prUrl" : "https://github.com/apache/spark/pull/30297#pullrequestreview-526875070",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5e495fec-5aed-4a04-ad94-36319c732e89",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Please update the usage above of `ExpressionDescription `, too?",
        "createdAt" : "2020-11-10T06:00:41Z",
        "updatedAt" : "2020-11-12T05:25:04Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "4b161a4ef4f3be2f1a239c0c19f8aa62b5a03706",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +1922,1926 @@  \"\"\",\n  since = \"2.4.0\")\ncase class ElementAt(\n    left: Expression,\n    right: Expression,"
  },
  {
    "id" : "554a983b-4a52-496d-af0f-0f038b928a8c",
    "prId" : 30297,
    "prUrl" : "https://github.com/apache/spark/pull/30297#pullrequestreview-527979548",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0893c27f-41ef-43e8-9435-fd9441c93415",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Why not just have `val failOnError: Boolean = SQLConf.get.ansiEnabled`? Do you need to assign it a value other than `SQLConf.get.ansiEnabled` when constructing?",
        "createdAt" : "2020-11-11T07:51:25Z",
        "updatedAt" : "2020-11-12T05:25:04Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "d9477601-79a1-4b6c-a1e9-ea5e65aea0b8",
        "parentId" : "0893c27f-41ef-43e8-9435-fd9441c93415",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Making it a parameter is more robust to retain this info. Otherwise, we may change it when transform and copy the expresion.\r\n\r\nThis also helps if we want to support SAFE prefix like https://cloud.google.com/bigquery/docs/reference/standard-sql/functions-and-operators",
        "createdAt" : "2020-11-11T07:59:06Z",
        "updatedAt" : "2020-11-12T05:25:04Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "c508b0af-1e49-476d-8057-722f01fca1d8",
        "parentId" : "0893c27f-41ef-43e8-9435-fd9441c93415",
        "authorId" : "cce1c782-0596-44b6-8b99-6b77d2cca53c",
        "body" : "When doing case class copy, it's better to just copy it from case class, but not re-get from SQLConf.get, it might change. I saw lot of same sample code inside spark.",
        "createdAt" : "2020-11-11T08:54:01Z",
        "updatedAt" : "2020-11-12T05:25:04Z",
        "lastEditedBy" : "cce1c782-0596-44b6-8b99-6b77d2cca53c",
        "tags" : [
        ]
      },
      {
        "id" : "265f6b50-e5ef-461c-9f05-0c61a34eda92",
        "parentId" : "0893c27f-41ef-43e8-9435-fd9441c93415",
        "authorId" : "cce1c782-0596-44b6-8b99-6b77d2cca53c",
        "body" : "for example \r\n```\r\ncase class MultiplyInterval(\r\n    interval: Expression,\r\n    num: Expression,\r\n    failOnError: Boolean = SQLConf.get.ansiEnabled)\r\n```",
        "createdAt" : "2020-11-11T08:58:27Z",
        "updatedAt" : "2020-11-12T05:25:04Z",
        "lastEditedBy" : "cce1c782-0596-44b6-8b99-6b77d2cca53c",
        "tags" : [
        ]
      }
    ],
    "commit" : "4b161a4ef4f3be2f1a239c0c19f8aa62b5a03706",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +1925,1929 @@    left: Expression,\n    right: Expression,\n    failOnError: Boolean = SQLConf.get.ansiEnabled)\n  extends GetMapValueUtil with GetArrayItemUtil with NullIntolerant {\n"
  },
  {
    "id" : "e8816a85-149d-455c-ad59-fc499cec5ea6",
    "prId" : 30297,
    "prUrl" : "https://github.com/apache/spark/pull/30297#pullrequestreview-528687059",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "66fbb9d4-9bfe-4ae5-a1b5-8335b9d76a5c",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Do we want to make `ElementAt`behavior consistent on map type?",
        "createdAt" : "2020-11-11T17:25:21Z",
        "updatedAt" : "2020-11-12T05:25:04Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "a654eaba-262a-4c20-a0c1-7999685a2274",
        "parentId" : "66fbb9d4-9bfe-4ae5-a1b5-8335b9d76a5c",
        "authorId" : "cce1c782-0596-44b6-8b99-6b77d2cca53c",
        "body" : "it's about to support ANSI mode for map type in next PR.",
        "createdAt" : "2020-11-12T02:18:50Z",
        "updatedAt" : "2020-11-12T05:25:04Z",
        "lastEditedBy" : "cce1c782-0596-44b6-8b99-6b77d2cca53c",
        "tags" : [
        ]
      }
    ],
    "commit" : "4b161a4ef4f3be2f1a239c0c19f8aa62b5a03706",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +1912,1916 @@      for invalid indices.\n\n    _FUNC_(map, key) - Returns value for given key, or NULL if the key is not contained in the map\n  \"\"\",\n  examples = \"\"\""
  },
  {
    "id" : "c6edc366-9b82-4838-927d-fb502dc3d085",
    "prId" : 30243,
    "prUrl" : "https://github.com/apache/spark/pull/30243#pullrequestreview-556851119",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c84ebb03-2474-4ac3-b280-845ef36dee15",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Do you use ordering for comparing the elements?",
        "createdAt" : "2020-12-22T01:45:05Z",
        "updatedAt" : "2020-12-22T02:10:38Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "fbe50d1d-53f8-4299-b07e-e8b283a67ab0",
        "parentId" : "c84ebb03-2474-4ac3-b280-845ef36dee15",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "Yea, similar as `ArrayContains`\r\nhttps://github.com/apache/spark/blob/a1024f27b73a9dc41b4fbd246f4a468d79f3222c/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala#L4102-L4109",
        "createdAt" : "2020-12-22T06:40:01Z",
        "updatedAt" : "2020-12-22T06:40:01Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "a1024f27b73a9dc41b4fbd246f4a468d79f3222c",
    "line" : 34,
    "diffHunk" : "@@ -1,1 +4032,4036 @@    val typeCheckResult = super.checkInputDataTypes()\n    if (typeCheckResult.isSuccess) {\n      TypeUtils.checkForOrderingExpr(et, s\"function $prettyName\")\n    } else {\n      typeCheckResult"
  },
  {
    "id" : "e160c870-a566-4798-aa56-1259265b92c8",
    "prId" : 30040,
    "prUrl" : "https://github.com/apache/spark/pull/30040#pullrequestreview-509257206",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "af24ac60-4381-49e8-bf93-19e46d892429",
        "parentId" : null,
        "authorId" : "a8e23d47-3ae4-4385-848c-38a216d1bd08",
        "body" : "`Size` is one of the more problematic ones - should it be an array_func or a map_func or better yet both?\r\nThere are more like this",
        "createdAt" : "2020-10-14T17:15:46Z",
        "updatedAt" : "2020-11-29T19:19:42Z",
        "lastEditedBy" : "a8e23d47-3ae4-4385-848c-38a216d1bd08",
        "tags" : [
        ]
      },
      {
        "id" : "12184466-a408-42c4-bc3b-79a440e31033",
        "parentId" : "af24ac60-4381-49e8-bf93-19e46d892429",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "hm, I see. Another idea: how about making a group for more general cases, e.g., `collection_funcs`? cc: @HyukjinKwon ",
        "createdAt" : "2020-10-14T23:43:52Z",
        "updatedAt" : "2020-11-29T19:19:42Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "bc2dabbf-b32f-4f2f-a05d-6b3ea19f1d16",
        "parentId" : "af24ac60-4381-49e8-bf93-19e46d892429",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Seems like `size` is `collection_funcs` in `functions.scala`. I think we can just stick to it.",
        "createdAt" : "2020-10-15T10:58:15Z",
        "updatedAt" : "2020-11-29T19:19:42Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "6838428b094c5832ca76387c3a66df3fab29ea23",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +94,98 @@  group = \"array_funcs\")\ncase class Size(child: Expression, legacySizeOfNull: Boolean)\n  extends UnaryExpression with ExpectsInputTypes {\n\n  def this(child: Expression) = this(child, SQLConf.get.legacySizeOfNull)"
  },
  {
    "id" : "c659449d-0819-4478-ac5c-1f13227083d5",
    "prId" : 29790,
    "prUrl" : "https://github.com/apache/spark/pull/29790#pullrequestreview-493564785",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ce7db4ea-05c7-4f2e-8988-fd5e54c973c5",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "shall we just remove this line? input data type check is part of the resolution procedure, and we don't need to do it again when accessing the data type.",
        "createdAt" : "2020-09-22T08:17:35Z",
        "updatedAt" : "2020-09-29T12:49:10Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "7d8ac714-9d87-469d-ba08-6c693d809ba8",
        "parentId" : "ce7db4ea-05c7-4f2e-8988-fd5e54c973c5",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "+1",
        "createdAt" : "2020-09-22T15:07:56Z",
        "updatedAt" : "2020-09-29T12:49:10Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      }
    ],
    "commit" : "37d0786201ec9109b661e19a5152fa53b1d1cad4",
    "line" : 42,
    "diffHunk" : "@@ -1,1 +3748,3752 @@\n  private lazy val internalDataType: DataType = {\n    dataTypeCheck\n    left.dataType\n  }"
  },
  {
    "id" : "51cf5e9e-a98b-4e8a-b664-9e7520a82ad0",
    "prId" : 29790,
    "prUrl" : "https://github.com/apache/spark/pull/29790#pullrequestreview-493224017",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2539f2f0-d50d-4216-9f58-fe4517d99349",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "ditto",
        "createdAt" : "2020-09-22T08:17:58Z",
        "updatedAt" : "2020-09-29T12:49:10Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "37d0786201ec9109b661e19a5152fa53b1d1cad4",
    "line" : 25,
    "diffHunk" : "@@ -1,1 +3503,3507 @@\n  private lazy val internalDataType: DataType = {\n    dataTypeCheck\n    ArrayType(elementType,\n      left.dataType.asInstanceOf[ArrayType].containsNull &&"
  },
  {
    "id" : "a52c9ace-6606-4d17-b4a1-2f5a8de074ac",
    "prId" : 29790,
    "prUrl" : "https://github.com/apache/spark/pull/29790#pullrequestreview-493444429",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f1c3195b-0294-4597-bd6e-a5c6bf768101",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "is it expensive? it just creates a few objects.",
        "createdAt" : "2020-09-22T08:18:27Z",
        "updatedAt" : "2020-09-29T12:49:10Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "203f6551-b06a-432f-88a8-4a68513772d2",
        "parentId" : "f1c3195b-0294-4597-bd6e-a5c6bf768101",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "This is to improve this case:\r\n![image](https://user-images.githubusercontent.com/5399861/93886687-55474d00-fd18-11ea-9948-95f6e0072d1c.png)\r\n\r\n\r\nBenchmark code | Before this PR(Milliseconds) | After this PR(Milliseconds)\r\n-- | -- | --\r\nspark.range(100000000L).selectExpr(\"approx_count_distinct(map_entries(map(1,   id)))\").collect() | 21787 | 15551\r\n\r\n\r\n",
        "createdAt" : "2020-09-22T13:13:50Z",
        "updatedAt" : "2020-09-29T12:49:10Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      }
    ],
    "commit" : "37d0786201ec9109b661e19a5152fa53b1d1cad4",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +369,373 @@  @transient private lazy val childDataType: MapType = child.dataType.asInstanceOf[MapType]\n\n  private lazy val internalDataType: DataType = {\n    ArrayType(\n      StructType("
  }
]