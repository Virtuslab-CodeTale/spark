[
  {
    "id" : "9b3fe2f4-08ed-4516-8523-2176bcab95aa",
    "prId" : 33106,
    "prUrl" : "https://github.com/apache/spark/pull/33106#pullrequestreview-693604377",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4341e28f-a3c5-46c6-a428-9aa9e6bfa5c4",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "shall we follow `CreateStruct.apply` and use `NamePlaceholder`?",
        "createdAt" : "2021-06-28T06:47:03Z",
        "updatedAt" : "2021-06-28T06:47:03Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "ce5d0b163b2c5cefbcdeda57d812125e811a4e70",
    "line" : 25,
    "diffHunk" : "@@ -1,1 +188,192 @@    this(\n      children,\n      children.zipWithIndex.map {\n        case (u: UnresolvedAttribute, _) => Literal(u.nameParts.last)\n        case (e: NamedExpression, _) if e.resolved => Literal(e.name)"
  },
  {
    "id" : "d5a3e9ae-843c-47a0-99af-8f4857347580",
    "prId" : 32311,
    "prUrl" : "https://github.com/apache/spark/pull/32311#pullrequestreview-644114142",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "20243a75-df6e-48b4-ac19-f16009981c31",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "I am not sure it is good idea to use `timestampAddInterval()` in `InternalSequenceBase.eval` for adding months to dates. I guess `DateTimeUtils.dateAddMonths()` and `DateTimeUtils.timestampAddInterval` can return different result, especially taking into account that `dateAddMonths()` **does not depend** on the current time zone.",
        "createdAt" : "2021-04-23T15:31:08Z",
        "updatedAt" : "2021-04-26T03:28:44Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "8a91c80f-2e36-4597-99e1-dd8c33d45e53",
        "parentId" : "20243a75-df6e-48b4-ac19-f16009981c31",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "In fact, the current implement uses `DateTimeUtils.timestampAddInterval` and it's behavior seems good.",
        "createdAt" : "2021-04-25T09:09:26Z",
        "updatedAt" : "2021-04-26T03:28:44Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      },
      {
        "id" : "d54b0671-497b-4b6b-a6b0-3b18d0a281ff",
        "parentId" : "20243a75-df6e-48b4-ac19-f16009981c31",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "ok. Let's use `timestampAddInterval` since we don't have an example that could demonstrate any issues caused by `timestampAddInterval()`.",
        "createdAt" : "2021-04-25T09:11:51Z",
        "updatedAt" : "2021-04-26T03:28:44Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "47853e8610a0bf37d377b552cdf6739b3cbeee36",
    "line" : 145,
    "diffHunk" : "@@ -1,1 +2755,2759 @@  private class PeriodSequenceImpl[T: ClassTag]\n      (dt: IntegralType, scale: Long, fromLong: Long => T, zoneId: ZoneId)\n      (implicit num: Integral[T]) extends InternalSequenceBase(dt, scale, fromLong, zoneId) {\n\n    override val defaultStep: DefaultStep = new DefaultStep("
  },
  {
    "id" : "4e8d4d9c-44bc-4d34-8a3f-cf4a6a26d25b",
    "prId" : 32176,
    "prUrl" : "https://github.com/apache/spark/pull/32176#pullrequestreview-636675225",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "792ff0f7-f489-4e4f-829a-6825288210eb",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Created JIRA SPARK-35088 for ANSI intervals",
        "createdAt" : "2021-04-15T13:18:15Z",
        "updatedAt" : "2021-04-16T11:24:15Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "7668405ebd902cea698b76c373d0c06b1e759426",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +2505,2509 @@       [5,4,3,2,1]\n      > SET spark.sql.legacy.interval.enabled=true;\n       spark.sql.legacy.interval.enabled\ttrue\n      > SELECT _FUNC_(to_date('2018-01-01'), to_date('2018-03-01'), interval 1 month);\n       [2018-01-01,2018-02-01,2018-03-01]"
  },
  {
    "id" : "81bc38b2-30a2-49c0-8903-23e5c1b63f9e",
    "prId" : 30867,
    "prUrl" : "https://github.com/apache/spark/pull/30867#pullrequestreview-556099735",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bb85d463-3887-4dc8-a5f5-3c35db4367b1",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Sorry but this is not `Adds a group tag`. This is re-categorization.",
        "createdAt" : "2020-12-21T01:47:39Z",
        "updatedAt" : "2020-12-21T07:20:26Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "36c4aea7-be46-4157-9e91-0c4c5e1851e5",
        "parentId" : "bb85d463-3887-4dc8-a5f5-3c35db4367b1",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Ah, yea, you're right. I'll update the PR and the description.",
        "createdAt" : "2020-12-21T02:37:04Z",
        "updatedAt" : "2020-12-21T07:20:26Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "4e82580cdc1b44a1680631ad54121e327f37b9e0",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +1020,1024 @@       [3,4,1,2]\n  \"\"\",\n  group = \"collection_funcs\",\n  since = \"1.5.0\",\n  note = \"\"\""
  },
  {
    "id" : "be2af2bc-131e-4fcd-a0a1-e9710a9ce81d",
    "prId" : 30867,
    "prUrl" : "https://github.com/apache/spark/pull/30867#pullrequestreview-556089853",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d797ee86-53f6-4904-b5e2-325efe5a3a3b",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "ditto.",
        "createdAt" : "2020-12-21T01:48:38Z",
        "updatedAt" : "2020-12-21T07:20:26Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "4e82580cdc1b44a1680631ad54121e327f37b9e0",
    "line" : 34,
    "diffHunk" : "@@ -1,1 +2100,2104 @@    Concat logic for arrays is available since 2.4.0.\n  \"\"\",\n  group = \"collection_funcs\",\n  since = \"1.5.0\")\ncase class Concat(children: Seq[Expression]) extends ComplexTypeMergingExpression {"
  },
  {
    "id" : "503344ec-278f-4986-ae63-2ee2622ff3e8",
    "prId" : 30297,
    "prUrl" : "https://github.com/apache/spark/pull/30297#pullrequestreview-526875070",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5e495fec-5aed-4a04-ad94-36319c732e89",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Please update the usage above of `ExpressionDescription `, too?",
        "createdAt" : "2020-11-10T06:00:41Z",
        "updatedAt" : "2020-11-12T05:25:04Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "4b161a4ef4f3be2f1a239c0c19f8aa62b5a03706",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +1922,1926 @@  \"\"\",\n  since = \"2.4.0\")\ncase class ElementAt(\n    left: Expression,\n    right: Expression,"
  },
  {
    "id" : "554a983b-4a52-496d-af0f-0f038b928a8c",
    "prId" : 30297,
    "prUrl" : "https://github.com/apache/spark/pull/30297#pullrequestreview-527979548",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0893c27f-41ef-43e8-9435-fd9441c93415",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Why not just have `val failOnError: Boolean = SQLConf.get.ansiEnabled`? Do you need to assign it a value other than `SQLConf.get.ansiEnabled` when constructing?",
        "createdAt" : "2020-11-11T07:51:25Z",
        "updatedAt" : "2020-11-12T05:25:04Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "d9477601-79a1-4b6c-a1e9-ea5e65aea0b8",
        "parentId" : "0893c27f-41ef-43e8-9435-fd9441c93415",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Making it a parameter is more robust to retain this info. Otherwise, we may change it when transform and copy the expresion.\r\n\r\nThis also helps if we want to support SAFE prefix like https://cloud.google.com/bigquery/docs/reference/standard-sql/functions-and-operators",
        "createdAt" : "2020-11-11T07:59:06Z",
        "updatedAt" : "2020-11-12T05:25:04Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "c508b0af-1e49-476d-8057-722f01fca1d8",
        "parentId" : "0893c27f-41ef-43e8-9435-fd9441c93415",
        "authorId" : "cce1c782-0596-44b6-8b99-6b77d2cca53c",
        "body" : "When doing case class copy, it's better to just copy it from case class, but not re-get from SQLConf.get, it might change. I saw lot of same sample code inside spark.",
        "createdAt" : "2020-11-11T08:54:01Z",
        "updatedAt" : "2020-11-12T05:25:04Z",
        "lastEditedBy" : "cce1c782-0596-44b6-8b99-6b77d2cca53c",
        "tags" : [
        ]
      },
      {
        "id" : "265f6b50-e5ef-461c-9f05-0c61a34eda92",
        "parentId" : "0893c27f-41ef-43e8-9435-fd9441c93415",
        "authorId" : "cce1c782-0596-44b6-8b99-6b77d2cca53c",
        "body" : "for example \r\n```\r\ncase class MultiplyInterval(\r\n    interval: Expression,\r\n    num: Expression,\r\n    failOnError: Boolean = SQLConf.get.ansiEnabled)\r\n```",
        "createdAt" : "2020-11-11T08:58:27Z",
        "updatedAt" : "2020-11-12T05:25:04Z",
        "lastEditedBy" : "cce1c782-0596-44b6-8b99-6b77d2cca53c",
        "tags" : [
        ]
      }
    ],
    "commit" : "4b161a4ef4f3be2f1a239c0c19f8aa62b5a03706",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +1925,1929 @@    left: Expression,\n    right: Expression,\n    failOnError: Boolean = SQLConf.get.ansiEnabled)\n  extends GetMapValueUtil with GetArrayItemUtil with NullIntolerant {\n"
  },
  {
    "id" : "e8816a85-149d-455c-ad59-fc499cec5ea6",
    "prId" : 30297,
    "prUrl" : "https://github.com/apache/spark/pull/30297#pullrequestreview-528687059",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "66fbb9d4-9bfe-4ae5-a1b5-8335b9d76a5c",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Do we want to make `ElementAt`behavior consistent on map type?",
        "createdAt" : "2020-11-11T17:25:21Z",
        "updatedAt" : "2020-11-12T05:25:04Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "a654eaba-262a-4c20-a0c1-7999685a2274",
        "parentId" : "66fbb9d4-9bfe-4ae5-a1b5-8335b9d76a5c",
        "authorId" : "cce1c782-0596-44b6-8b99-6b77d2cca53c",
        "body" : "it's about to support ANSI mode for map type in next PR.",
        "createdAt" : "2020-11-12T02:18:50Z",
        "updatedAt" : "2020-11-12T05:25:04Z",
        "lastEditedBy" : "cce1c782-0596-44b6-8b99-6b77d2cca53c",
        "tags" : [
        ]
      }
    ],
    "commit" : "4b161a4ef4f3be2f1a239c0c19f8aa62b5a03706",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +1912,1916 @@      for invalid indices.\n\n    _FUNC_(map, key) - Returns value for given key, or NULL if the key is not contained in the map\n  \"\"\",\n  examples = \"\"\""
  },
  {
    "id" : "c6edc366-9b82-4838-927d-fb502dc3d085",
    "prId" : 30243,
    "prUrl" : "https://github.com/apache/spark/pull/30243#pullrequestreview-556851119",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c84ebb03-2474-4ac3-b280-845ef36dee15",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Do you use ordering for comparing the elements?",
        "createdAt" : "2020-12-22T01:45:05Z",
        "updatedAt" : "2020-12-22T02:10:38Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "fbe50d1d-53f8-4299-b07e-e8b283a67ab0",
        "parentId" : "c84ebb03-2474-4ac3-b280-845ef36dee15",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "Yea, similar as `ArrayContains`\r\nhttps://github.com/apache/spark/blob/a1024f27b73a9dc41b4fbd246f4a468d79f3222c/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala#L4102-L4109",
        "createdAt" : "2020-12-22T06:40:01Z",
        "updatedAt" : "2020-12-22T06:40:01Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "a1024f27b73a9dc41b4fbd246f4a468d79f3222c",
    "line" : 34,
    "diffHunk" : "@@ -1,1 +4032,4036 @@    val typeCheckResult = super.checkInputDataTypes()\n    if (typeCheckResult.isSuccess) {\n      TypeUtils.checkForOrderingExpr(et, s\"function $prettyName\")\n    } else {\n      typeCheckResult"
  },
  {
    "id" : "e160c870-a566-4798-aa56-1259265b92c8",
    "prId" : 30040,
    "prUrl" : "https://github.com/apache/spark/pull/30040#pullrequestreview-509257206",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "af24ac60-4381-49e8-bf93-19e46d892429",
        "parentId" : null,
        "authorId" : "a8e23d47-3ae4-4385-848c-38a216d1bd08",
        "body" : "`Size` is one of the more problematic ones - should it be an array_func or a map_func or better yet both?\r\nThere are more like this",
        "createdAt" : "2020-10-14T17:15:46Z",
        "updatedAt" : "2020-11-29T19:19:42Z",
        "lastEditedBy" : "a8e23d47-3ae4-4385-848c-38a216d1bd08",
        "tags" : [
        ]
      },
      {
        "id" : "12184466-a408-42c4-bc3b-79a440e31033",
        "parentId" : "af24ac60-4381-49e8-bf93-19e46d892429",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "hm, I see. Another idea: how about making a group for more general cases, e.g., `collection_funcs`? cc: @HyukjinKwon ",
        "createdAt" : "2020-10-14T23:43:52Z",
        "updatedAt" : "2020-11-29T19:19:42Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "bc2dabbf-b32f-4f2f-a05d-6b3ea19f1d16",
        "parentId" : "af24ac60-4381-49e8-bf93-19e46d892429",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Seems like `size` is `collection_funcs` in `functions.scala`. I think we can just stick to it.",
        "createdAt" : "2020-10-15T10:58:15Z",
        "updatedAt" : "2020-11-29T19:19:42Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "6838428b094c5832ca76387c3a66df3fab29ea23",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +94,98 @@  group = \"array_funcs\")\ncase class Size(child: Expression, legacySizeOfNull: Boolean)\n  extends UnaryExpression with ExpectsInputTypes {\n\n  def this(child: Expression) = this(child, SQLConf.get.legacySizeOfNull)"
  },
  {
    "id" : "c659449d-0819-4478-ac5c-1f13227083d5",
    "prId" : 29790,
    "prUrl" : "https://github.com/apache/spark/pull/29790#pullrequestreview-493564785",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ce7db4ea-05c7-4f2e-8988-fd5e54c973c5",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "shall we just remove this line? input data type check is part of the resolution procedure, and we don't need to do it again when accessing the data type.",
        "createdAt" : "2020-09-22T08:17:35Z",
        "updatedAt" : "2020-09-29T12:49:10Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "7d8ac714-9d87-469d-ba08-6c693d809ba8",
        "parentId" : "ce7db4ea-05c7-4f2e-8988-fd5e54c973c5",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "+1",
        "createdAt" : "2020-09-22T15:07:56Z",
        "updatedAt" : "2020-09-29T12:49:10Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      }
    ],
    "commit" : "37d0786201ec9109b661e19a5152fa53b1d1cad4",
    "line" : 42,
    "diffHunk" : "@@ -1,1 +3748,3752 @@\n  private lazy val internalDataType: DataType = {\n    dataTypeCheck\n    left.dataType\n  }"
  },
  {
    "id" : "51cf5e9e-a98b-4e8a-b664-9e7520a82ad0",
    "prId" : 29790,
    "prUrl" : "https://github.com/apache/spark/pull/29790#pullrequestreview-493224017",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2539f2f0-d50d-4216-9f58-fe4517d99349",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "ditto",
        "createdAt" : "2020-09-22T08:17:58Z",
        "updatedAt" : "2020-09-29T12:49:10Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "37d0786201ec9109b661e19a5152fa53b1d1cad4",
    "line" : 25,
    "diffHunk" : "@@ -1,1 +3503,3507 @@\n  private lazy val internalDataType: DataType = {\n    dataTypeCheck\n    ArrayType(elementType,\n      left.dataType.asInstanceOf[ArrayType].containsNull &&"
  },
  {
    "id" : "a52c9ace-6606-4d17-b4a1-2f5a8de074ac",
    "prId" : 29790,
    "prUrl" : "https://github.com/apache/spark/pull/29790#pullrequestreview-493444429",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f1c3195b-0294-4597-bd6e-a5c6bf768101",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "is it expensive? it just creates a few objects.",
        "createdAt" : "2020-09-22T08:18:27Z",
        "updatedAt" : "2020-09-29T12:49:10Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "203f6551-b06a-432f-88a8-4a68513772d2",
        "parentId" : "f1c3195b-0294-4597-bd6e-a5c6bf768101",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "This is to improve this case:\r\n![image](https://user-images.githubusercontent.com/5399861/93886687-55474d00-fd18-11ea-9948-95f6e0072d1c.png)\r\n\r\n\r\nBenchmark code | Before this PR(Milliseconds) | After this PR(Milliseconds)\r\n-- | -- | --\r\nspark.range(100000000L).selectExpr(\"approx_count_distinct(map_entries(map(1,   id)))\").collect() | 21787 | 15551\r\n\r\n\r\n",
        "createdAt" : "2020-09-22T13:13:50Z",
        "updatedAt" : "2020-09-29T12:49:10Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      }
    ],
    "commit" : "37d0786201ec9109b661e19a5152fa53b1d1cad4",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +369,373 @@  @transient private lazy val childDataType: MapType = child.dataType.asInstanceOf[MapType]\n\n  private lazy val internalDataType: DataType = {\n    ArrayType(\n      StructType("
  },
  {
    "id" : "dc28160e-946d-49ef-ac98-476f5dd4bc30",
    "prId" : 28926,
    "prUrl" : "https://github.com/apache/spark/pull/28926#pullrequestreview-442724774",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ae1fafac-bb59-4e5b-a226-eaaa171013c9",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "we can remove this branch now?",
        "createdAt" : "2020-06-29T11:50:09Z",
        "updatedAt" : "2020-07-06T10:00:49Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "7d915ee8-80dd-48f2-bd1d-936d8923a59f",
        "parentId" : "ae1fafac-bb59-4e5b-a226-eaaa171013c9",
        "authorId" : "919368dc-8bac-4d8f-9fa6-be962263fc1e",
        "body" : "Seems we need the `require` check in eval, and I remove `SPARK-32198` branch code from this PR, is it ok ?",
        "createdAt" : "2020-06-29T23:31:22Z",
        "updatedAt" : "2020-07-06T10:00:49Z",
        "lastEditedBy" : "919368dc-8bac-4d8f-9fa6-be962263fc1e",
        "tags" : [
        ]
      },
      {
        "id" : "76e99d08-7396-40fc-b335-2906d10fd55b",
        "parentId" : "ae1fafac-bb59-4e5b-a226-eaaa171013c9",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "@TJX2014 . What is SPARK-32198? It's not created yet.\r\n> I remove SPARK-32198 branch code from this PR",
        "createdAt" : "2020-07-05T23:46:19Z",
        "updatedAt" : "2020-07-06T10:00:49Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "86ef0a5c-41a8-40d8-a36d-257aca84e725",
        "parentId" : "ae1fafac-bb59-4e5b-a226-eaaa171013c9",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Also, I have the same question like @cloud-fan .",
        "createdAt" : "2020-07-05T23:48:12Z",
        "updatedAt" : "2020-07-06T10:00:49Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "4874c909-6949-4b78-929f-1c32c4673cd0",
        "parentId" : "ae1fafac-bb59-4e5b-a226-eaaa171013c9",
        "authorId" : "919368dc-8bac-4d8f-9fa6-be962263fc1e",
        "body" : "@dongjoon-hyun  Sorry, the correct PR is [SPARK-31982](https://github.com/apache/spark/pull/28856), this PR is forbid step and the other is cross the timezone.",
        "createdAt" : "2020-07-06T00:59:51Z",
        "updatedAt" : "2020-07-06T10:00:49Z",
        "lastEditedBy" : "919368dc-8bac-4d8f-9fa6-be962263fc1e",
        "tags" : [
        ]
      }
    ],
    "commit" : "8c4ffaa75552987d9303b2ee3186fdc8abf14c68",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +2618,2622 @@      }\n\n      if (stepMonths == 0 && stepMicros == 0 && scale == MICROS_PER_DAY) {\n        // Adding pure days to date start/end\n        backedSequenceImpl.eval(start, stop, fromLong(stepDays))"
  },
  {
    "id" : "a3b7beed-74b4-47bb-8ec8-b08405ae88cf",
    "prId" : 28926,
    "prUrl" : "https://github.com/apache/spark/pull/28926#pullrequestreview-442805973",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "36366d56-935e-4daa-a48a-5a4b33774413",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@TJX2014 can we just use `require` instead?",
        "createdAt" : "2020-07-06T05:54:07Z",
        "updatedAt" : "2020-07-06T10:00:49Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "fd8817b8-439f-4cb1-95a5-61936ab882da",
        "parentId" : "36366d56-935e-4daa-a48a-5a4b33774413",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I asked to throw an exception explicitly, to match the codegen code.",
        "createdAt" : "2020-07-06T06:31:55Z",
        "updatedAt" : "2020-07-06T10:00:49Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "a40cb3e1-d2f1-435b-952c-41424fb481d2",
        "parentId" : "36366d56-935e-4daa-a48a-5a4b33774413",
        "authorId" : "919368dc-8bac-4d8f-9fa6-be962263fc1e",
        "body" : "I am both ok, thank you for your attention.  :-)",
        "createdAt" : "2020-07-06T06:35:46Z",
        "updatedAt" : "2020-07-06T10:00:49Z",
        "lastEditedBy" : "919368dc-8bac-4d8f-9fa6-be962263fc1e",
        "tags" : [
        ]
      }
    ],
    "commit" : "8c4ffaa75552987d9303b2ee3186fdc8abf14c68",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +2614,2618 @@\n      if (scale == MICROS_PER_DAY && stepMonths == 0 && stepDays == 0) {\n        throw new IllegalArgumentException(\n          \"sequence step must be a day interval if start and end values are dates\")\n      }"
  },
  {
    "id" : "a4a0474b-1fab-4530-87b8-144e4a282e06",
    "prId" : 28926,
    "prUrl" : "https://github.com/apache/spark/pull/28926#pullrequestreview-442944077",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9260e49a-4f74-493f-9612-102a025db120",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "We can probably add comments for each branch. For example, this branch is for adding days to date start/end.",
        "createdAt" : "2020-07-06T06:33:57Z",
        "updatedAt" : "2020-07-06T10:00:49Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "1d5242d4-780c-4365-a527-b67552714f09",
        "parentId" : "9260e49a-4f74-493f-9612-102a025db120",
        "authorId" : "919368dc-8bac-4d8f-9fa6-be962263fc1e",
        "body" : "Done.",
        "createdAt" : "2020-07-06T10:06:43Z",
        "updatedAt" : "2020-07-06T10:06:43Z",
        "lastEditedBy" : "919368dc-8bac-4d8f-9fa6-be962263fc1e",
        "tags" : [
        ]
      }
    ],
    "commit" : "8c4ffaa75552987d9303b2ee3186fdc8abf14c68",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +2618,2622 @@      }\n\n      if (stepMonths == 0 && stepMicros == 0 && scale == MICROS_PER_DAY) {\n        // Adding pure days to date start/end\n        backedSequenceImpl.eval(start, stop, fromLong(stepDays))"
  },
  {
    "id" : "d16456b6-6ba1-4373-82f2-a76cbbfbc6cb",
    "prId" : 28926,
    "prUrl" : "https://github.com/apache/spark/pull/28926#pullrequestreview-442942999",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "61125eb8-c3e2-4d64-b7bb-ec2b58f7c5d2",
        "parentId" : null,
        "authorId" : "919368dc-8bac-4d8f-9fa6-be962263fc1e",
        "body" : "@cloud-fan Thanks, I add more comments for pure days and months branch, the former exception check seems the exception content has enough informs, and the last branch seems have detail content.",
        "createdAt" : "2020-07-06T10:05:08Z",
        "updatedAt" : "2020-07-06T10:05:35Z",
        "lastEditedBy" : "919368dc-8bac-4d8f-9fa6-be962263fc1e",
        "tags" : [
        ]
      }
    ],
    "commit" : "8c4ffaa75552987d9303b2ee3186fdc8abf14c68",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +2623,2627 @@\n      } else if (stepMonths == 0 && stepDays == 0 && scale == 1) {\n        // Adding pure microseconds to timestamp start/end\n        backedSequenceImpl.eval(start, stop, fromLong(stepMicros))\n"
  },
  {
    "id" : "9963cb2a-e1b9-48db-862a-33f7652a8cd7",
    "prId" : 28856,
    "prUrl" : "https://github.com/apache/spark/pull/28856#pullrequestreview-434992770",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4c79d635-9193-4688-9511-565fc559cf1f",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "it looks like the step should not be physical seconds, but logical interval. cc @MaxGekk ",
        "createdAt" : "2020-06-18T12:04:29Z",
        "updatedAt" : "2020-06-22T15:01:55Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "299415b6-a3b3-403d-be14-2eb5183a3e94",
        "parentId" : "4c79d635-9193-4688-9511-565fc559cf1f",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "The current implementation is strange mix, it seems. There are following options:\r\n1. Step is an interval of (months, days, micros):\r\n    1. If start point is TimestampType, we should convert it to local date-time in the session time zone, and add the interval by time components. The intermediate local timestamps should be converted back to micros using the session time zone but we should keep adding the interval to local timestamp \"accumulator\".\r\n    2. The same for dates - convert `start` to a local date. Time zone shouldn't involved here.\r\n\r\n2. If the step is a duration in micros or days (this is not our case)\r\n    1. start is TimestampType, we shouldn't convert it to local timestamp, and just add micros to instants. So, time zone will be not involved here.\r\n    2. start is DateType, just add number of days. The same as for timestamps, time zone is not involved here.\r\n",
        "createdAt" : "2020-06-18T16:49:59Z",
        "updatedAt" : "2020-06-22T15:01:55Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "bbb1b8f1-e6f6-40b9-b329-00b713706d1e",
        "parentId" : "4c79d635-9193-4688-9511-565fc559cf1f",
        "authorId" : "919368dc-8bac-4d8f-9fa6-be962263fc1e",
        "body" : "hi @cloud-fan，as  @MaxGekk  explain here, I am not sure if this patch looks ok，I am willing to `add more documents to TemporalSequenceImpl`but I am not sure if  we can follow this way or refactor a little.",
        "createdAt" : "2020-06-22T14:42:35Z",
        "updatedAt" : "2020-06-22T15:19:02Z",
        "lastEditedBy" : "919368dc-8bac-4d8f-9fa6-be962263fc1e",
        "tags" : [
        ]
      }
    ],
    "commit" : "6a341bff3cbd6196cd1d742d9b66818185b68e4e",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +2624,2628 @@        // To estimate the resulted array length we need to make assumptions\n        // about a month length in days and a day length in microseconds\n        val intervalStepInMicros =\n          stepMicros + stepMonths * microsPerMonth + stepDays * microsPerDay\n"
  },
  {
    "id" : "6f8d6e3c-7942-4b97-83b4-25578c6d4a01",
    "prId" : 28856,
    "prUrl" : "https://github.com/apache/spark/pull/28856#pullrequestreview-433781687",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d53a4adf-2f20-4510-b732-b59a7f600812",
        "parentId" : null,
        "authorId" : "919368dc-8bac-4d8f-9fa6-be962263fc1e",
        "body" : "We could get date from `Math.round(t / scale.toFloat)`, if the target is timestamp,  the former ` fromLong(t / scale)` could be used.",
        "createdAt" : "2020-06-19T03:36:53Z",
        "updatedAt" : "2020-06-22T15:01:55Z",
        "lastEditedBy" : "919368dc-8bac-4d8f-9fa6-be962263fc1e",
        "tags" : [
        ]
      }
    ],
    "commit" : "6a341bff3cbd6196cd1d742d9b66818185b68e4e",
    "line" : 36,
    "diffHunk" : "@@ -1,1 +2649,2653 @@            fromLong(t)\n          } else {\n            fromLong(Math.round(t / scale.toFloat))\n          }\n          i += 1"
  },
  {
    "id" : "c4ea403e-821c-443a-97d2-d307b0072a5e",
    "prId" : 28856,
    "prUrl" : "https://github.com/apache/spark/pull/28856#pullrequestreview-435037571",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9df96b69-0eb4-4b63-84fe-a50f7866986b",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "can you explain a bit more? It's hard to understand this change without any comment.",
        "createdAt" : "2020-06-22T08:25:46Z",
        "updatedAt" : "2020-06-22T15:01:55Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "971f738a-a674-442f-babc-052052589355",
        "parentId" : "9df96b69-0eb4-4b63-84fe-a50f7866986b",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Yeh, please, explain why if `scale` != 1, `start` and `stop` contain days.",
        "createdAt" : "2020-06-22T08:36:43Z",
        "updatedAt" : "2020-06-22T15:01:55Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "49715d41-4357-4f9e-845e-61d2b54cada3",
        "parentId" : "9df96b69-0eb4-4b63-84fe-a50f7866986b",
        "authorId" : "919368dc-8bac-4d8f-9fa6-be962263fc1e",
        "body" : "Because when `scale` != 1，it is converted to day count，so we may need to use zone info to translate into microseconds to get a correct result，rather than just multiply `MICROS_PER_DAY` which ignore timezone.",
        "createdAt" : "2020-06-22T13:33:38Z",
        "updatedAt" : "2020-06-22T15:01:56Z",
        "lastEditedBy" : "919368dc-8bac-4d8f-9fa6-be962263fc1e",
        "tags" : [
        ]
      },
      {
        "id" : "97c304b5-cb51-45e3-8b27-a8b0bfe7e259",
        "parentId" : "9df96b69-0eb4-4b63-84fe-a50f7866986b",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "> Because when scale != 1，it is converted to day count\r\n\r\nHow can we tell it?",
        "createdAt" : "2020-06-22T13:41:58Z",
        "updatedAt" : "2020-06-22T15:01:56Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "819ef44d-f2cd-496e-af88-5b3814fbe3ae",
        "parentId" : "9df96b69-0eb4-4b63-84fe-a50f7866986b",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "maybe we should add more documents to `TemporalSequenceImpl` first, to understand what it is doing",
        "createdAt" : "2020-06-22T13:42:46Z",
        "updatedAt" : "2020-06-22T15:01:56Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "7e0cedf6-6a99-4bbe-9a7a-3c26f08572a4",
        "parentId" : "9df96b69-0eb4-4b63-84fe-a50f7866986b",
        "authorId" : "919368dc-8bac-4d8f-9fa6-be962263fc1e",
        "body" : "Done, maybe we can pass the scale through constructor：\r\nprivate class TemporalSequenceImpl[T: ClassTag]\r\n      (dt: IntegralType, **scale: Long**, fromLong: Long => T, zoneId: ZoneId)",
        "createdAt" : "2020-06-22T15:28:56Z",
        "updatedAt" : "2020-06-22T15:28:56Z",
        "lastEditedBy" : "919368dc-8bac-4d8f-9fa6-be962263fc1e",
        "tags" : [
        ]
      }
    ],
    "commit" : "6a341bff3cbd6196cd1d742d9b66818185b68e4e",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +2633,2637 @@        else {\n          (daysToMicros(num.toInt(start), zoneId),\n            daysToMicros(num.toInt(stop), zoneId))\n        }\n"
  },
  {
    "id" : "d45ced4d-a8b1-4dd8-a057-ac79e48f835a",
    "prId" : 28856,
    "prUrl" : "https://github.com/apache/spark/pull/28856#pullrequestreview-434921382",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b9d9e2d2-cd7b-4e29-b9b4-68d1d59e3c04",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Don't think this is correct, see my comment https://github.com/apache/spark/pull/28856/files#r442366706 but it is at least backward compatible?",
        "createdAt" : "2020-06-22T08:38:15Z",
        "updatedAt" : "2020-06-22T15:01:56Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "cda8dc1a-a527-409d-bd72-665bcfeaedd5",
        "parentId" : "b9d9e2d2-cd7b-4e29-b9b4-68d1d59e3c04",
        "authorId" : "919368dc-8bac-4d8f-9fa6-be962263fc1e",
        "body" : "Maybe we could separate this into different methods ?",
        "createdAt" : "2020-06-22T13:27:43Z",
        "updatedAt" : "2020-06-22T15:01:56Z",
        "lastEditedBy" : "919368dc-8bac-4d8f-9fa6-be962263fc1e",
        "tags" : [
        ]
      }
    ],
    "commit" : "6a341bff3cbd6196cd1d742d9b66818185b68e4e",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +2629,2633 @@        // Date to timestamp is not equal from GMT and Chicago timezones\n        val (startMicros, stopMicros) = if (scale == 1) {\n          (num.toLong(start), num.toLong(stop))\n        }\n        else {"
  },
  {
    "id" : "bd1741c7-9fca-4b68-b044-ce56aca14033",
    "prId" : 28856,
    "prUrl" : "https://github.com/apache/spark/pull/28856#pullrequestreview-436746823",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a5029ea8-0701-4829-8f50-77f9f40aea95",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "if start/end is date, can the step by seconds/minutes/hours?",
        "createdAt" : "2020-06-23T06:06:21Z",
        "updatedAt" : "2020-06-23T06:06:21Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "950f180c-e18b-42bb-b281-1b9de4fd61a1",
        "parentId" : "a5029ea8-0701-4829-8f50-77f9f40aea95",
        "authorId" : "919368dc-8bac-4d8f-9fa6-be962263fc1e",
        "body" : "Yes, seems we can, the result as follows：\r\n`scala> sql(\"select explode(sequence(cast('2011-03-01' as date), cast('2011-05-01' as date), interval 1 hour))\").count\r\nres19: Long = 1465\r\n\r\nscala> sql(\"select explode(sequence(cast('2011-03-01' as date), cast('2011-05-01' as date), interval 1 minute))\").count\r\nres20: Long = 87841\r\n\r\nscala> sql(\"select explode(sequence(cast('2011-03-01' as date), cast('2011-05-01' as date), interval 1 second))\").count\r\nres21: Long = 5270401\r\nscala> sql(\"select explode(sequence(cast('2011-03-01' as date), cast('2011-05-01' as date), interval 1 minute))\").head(3)\r\nres25: Array[org.apache.spark.sql.Row] = Array([2011-03-01], [2011-03-01], [2011-03-01])\r\n\r\nscala> sql(\"select explode(sequence(cast('2011-03-01' as date), cast('2011-05-01' as date), interval 1 second))\").head(3)\r\n\r\nres26: Array[org.apache.spark.sql.Row] = Array([2011-03-01], [2011-03-01], [2011-03-01])\r\n\r\nscala> sql(\"select explode(sequence(cast('2011-03-01' as date), cast('2011-05-01' as date), interval 1 minute))\").head(3)\r\nres27: Array[org.apache.spark.sql.Row] = Array([2011-03-01], [2011-03-01], [2011-03-01])\r\n\r\nscala> sql(\"select explode(sequence(cast('2011-03-01' as date), cast('2011-05-01' as date), interval 1 hour))\").head(3)\r\nres28: Array[org.apache.spark.sql.Row] = Array([2011-03-01], [2011-03-01], [2011-03-01])\r\n`",
        "createdAt" : "2020-06-23T08:18:23Z",
        "updatedAt" : "2020-06-23T08:23:02Z",
        "lastEditedBy" : "919368dc-8bac-4d8f-9fa6-be962263fc1e",
        "tags" : [
        ]
      },
      {
        "id" : "40e4586b-61e8-4f1d-bf0c-95d77ca347c3",
        "parentId" : "a5029ea8-0701-4829-8f50-77f9f40aea95",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "does pgsql support it?",
        "createdAt" : "2020-06-23T09:11:37Z",
        "updatedAt" : "2020-06-23T09:11:37Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "daf1ede1-2611-4688-a709-b199e7fb8413",
        "parentId" : "a5029ea8-0701-4829-8f50-77f9f40aea95",
        "authorId" : "919368dc-8bac-4d8f-9fa6-be962263fc1e",
        "body" : "Seems pgsql can only support int as follows：\r\npostgres= create sequence seq_test;\r\nCREATE SEQUENCE\r\npostgres= select nextval('seq_test');\r\n       1\r\n(1 行记录)\r\npostgres= select nextval('seq_test');\r\n       2\r\n(1 行记录)\r\n",
        "createdAt" : "2020-06-23T15:06:02Z",
        "updatedAt" : "2020-06-23T15:06:02Z",
        "lastEditedBy" : "919368dc-8bac-4d8f-9fa6-be962263fc1e",
        "tags" : [
        ]
      },
      {
        "id" : "55dcb157-40ae-4048-8566-670baa2c1acf",
        "parentId" : "a5029ea8-0701-4829-8f50-77f9f40aea95",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Actually this function is from presto: https://prestodb.io/docs/current/functions/array.html\r\n\r\nCan you check the behavior of presto? It looks confusing to use time fields as the step for date start/stop.",
        "createdAt" : "2020-06-24T06:27:14Z",
        "updatedAt" : "2020-06-24T06:27:15Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "8d391b13-9eb6-4a37-b2a4-cc4f93b70414",
        "parentId" : "a5029ea8-0701-4829-8f50-77f9f40aea95",
        "authorId" : "919368dc-8bac-4d8f-9fa6-be962263fc1e",
        "body" : "Base `presto-server-0.236`：\r\npresto> select sequence(date('2011-03-01'),date('2011-03-02'),interval '1' hour);\r\nQuery 20200624_122744_00002_pehix failed: sequence step must be a day interval if start and end values are dates\r\npresto> select sequence(date('2011-03-01'),date('2011-03-02'),interval '1' day);\r\n          _col0           \r\n [2011-03-01, 2011-03-02] \r\n(1 row)\r\nQuery 20200624_122757_00003_pehix, FINISHED, 1 node\r\nSplits: 17 total, 17 done (100.00%)\r\n0:00 [0 rows, 0B] [0 rows/s, 0B/s]\r\npresto> select sequence(date('2011-03-01'),date('2011-03-02'),interval '1' month);\r\n    _col0     \r\n [2011-03-01] \r\n(1 row)\r\n\r\nQuery 20200624_122806_00004_pehix, FINISHED, 1 node\r\nSplits: 17 total, 17 done (100.00%)\r\n0:00 [0 rows, 0B] [0 rows/s, 0B/s]\r\npresto> select sequence(date('2011-03-01'),date('2011-03-02'),interval '1' year);\r\n    _col0     \r\n [2011-03-01] \r\n(1 row)\r\nQuery 20200624_122810_00005_pehix, FINISHED, 1 node\r\nSplits: 17 total, 17 done (100.00%)\r\n0:00 [0 rows, 0B] [0 rows/s, 0B/s]",
        "createdAt" : "2020-06-24T12:30:10Z",
        "updatedAt" : "2020-06-24T12:37:17Z",
        "lastEditedBy" : "919368dc-8bac-4d8f-9fa6-be962263fc1e",
        "tags" : [
        ]
      },
      {
        "id" : "a443a5f9-3d76-4259-8f7f-08933484ece4",
        "parentId" : "a5029ea8-0701-4829-8f50-77f9f40aea95",
        "authorId" : "919368dc-8bac-4d8f-9fa6-be962263fc1e",
        "body" : "@cloud-fan Done, It seems can be sequence `day`,`month`,`year` when start and end are `DateType` in presto.",
        "createdAt" : "2020-06-24T12:33:07Z",
        "updatedAt" : "2020-06-24T12:36:52Z",
        "lastEditedBy" : "919368dc-8bac-4d8f-9fa6-be962263fc1e",
        "tags" : [
        ]
      },
      {
        "id" : "5190ba46-76ce-47ff-8f8b-b1d72ad4d3f1",
        "parentId" : "a5029ea8-0701-4829-8f50-77f9f40aea95",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I think the presto behavior makes sense. Can we send a PR to follow it first? e.g. throw an exception if the step is time fields while start/end is date. This can also simplify the implementation.",
        "createdAt" : "2020-06-24T15:03:58Z",
        "updatedAt" : "2020-06-24T15:03:58Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "00329047-2473-4760-8557-5c9575ef72f1",
        "parentId" : "a5029ea8-0701-4829-8f50-77f9f40aea95",
        "authorId" : "919368dc-8bac-4d8f-9fa6-be962263fc1e",
        "body" : "Ok, I will do it tomorrow.",
        "createdAt" : "2020-06-24T15:12:43Z",
        "updatedAt" : "2020-06-24T15:12:43Z",
        "lastEditedBy" : "919368dc-8bac-4d8f-9fa6-be962263fc1e",
        "tags" : [
        ]
      }
    ],
    "commit" : "6a341bff3cbd6196cd1d742d9b66818185b68e4e",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +2591,2595 @@\n  // To generate time sequences, we use scale 1 in TemporalSequenceImpl\n  // for `TimestampType`, while MICROS_PER_DAY for `DateType`\n  private class TemporalSequenceImpl[T: ClassTag]\n      (dt: IntegralType, scale: Long, fromLong: Long => T, zoneId: ZoneId)"
  },
  {
    "id" : "20154db1-2e23-49fb-a132-0bd0388f31e2",
    "prId" : 28856,
    "prUrl" : "https://github.com/apache/spark/pull/28856#pullrequestreview-442818523",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f31fc7f0-d34c-4b91-8f04-fa10a66398cc",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Why do these codes depend on few specific timezones? Also, other comments look valid here. We should take the timezone into account for timestamps too",
        "createdAt" : "2020-07-06T05:30:46Z",
        "updatedAt" : "2020-07-06T05:30:46Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "87f1b7f8-33aa-4283-98b1-0d28f1511dfc",
        "parentId" : "f31fc7f0-d34c-4b91-8f04-fa10a66398cc",
        "authorId" : "919368dc-8bac-4d8f-9fa6-be962263fc1e",
        "body" : "Seems the date if different from west to east, when it is date, we might need to consider to zone info to convert to time stamp, if it is already a time stamp, not a date here, we may ignore the zone because the time stamp is already consider it.",
        "createdAt" : "2020-07-06T07:01:36Z",
        "updatedAt" : "2020-07-06T07:01:36Z",
        "lastEditedBy" : "919368dc-8bac-4d8f-9fa6-be962263fc1e",
        "tags" : [
        ]
      }
    ],
    "commit" : "6a341bff3cbd6196cd1d742d9b66818185b68e4e",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +2627,2631 @@          stepMicros + stepMonths * microsPerMonth + stepDays * microsPerDay\n\n        // Date to timestamp is not equal from GMT and Chicago timezones\n        val (startMicros, stopMicros) = if (scale == 1) {\n          (num.toLong(start), num.toLong(stop))"
  },
  {
    "id" : "b863727d-5879-4e07-98c9-9815c68dce13",
    "prId" : 26371,
    "prUrl" : "https://github.com/apache/spark/pull/26371#pullrequestreview-310863819",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "07d4e7a4-11a1-4a11-a28d-23700b251a95",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "You need to handle `nullable` and `foldable`, too.",
        "createdAt" : "2019-11-02T23:01:42Z",
        "updatedAt" : "2019-11-04T08:51:03Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "4661830e-0a2a-42a1-a0bd-91f11ed5bb30",
        "parentId" : "07d4e7a4-11a1-4a11-a28d-23700b251a95",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "@yaooqinn You forgot to override `foldable`...",
        "createdAt" : "2019-11-04T00:08:50Z",
        "updatedAt" : "2019-11-04T08:51:03Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "16d121f1-d557-49f0-a377-0e4eb8b9179d",
        "parentId" : "07d4e7a4-11a1-4a11-a28d-23700b251a95",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "the foldable here is the same as `BinaryExpression`, what about we just inherit it？",
        "createdAt" : "2019-11-04T02:31:24Z",
        "updatedAt" : "2019-11-04T08:51:03Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "8271afe4-a883-4889-9b48-f6a56a40be20",
        "parentId" : "07d4e7a4-11a1-4a11-a28d-23700b251a95",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Ur, I see. I missed this is a binary expr. Thanks.",
        "createdAt" : "2019-11-04T02:42:48Z",
        "updatedAt" : "2019-11-04T08:51:03Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "b8274175dfde798b24096b0042f1253a0c1fb656",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +3947,3951 @@  since = \"3.0.0\")\ncase class ArrayAppend(left: Expression, right: Expression) extends BinaryExpression {\n\n  private lazy val elementType: DataType = left.dataType match {\n    case ArrayType(elementType, containsNull) => elementType"
  },
  {
    "id" : "9045334c-1647-4cb0-a101-6504d2e411af",
    "prId" : 26371,
    "prUrl" : "https://github.com/apache/spark/pull/26371#pullrequestreview-310798074",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "71856f1e-738e-41c8-835e-093f04eec07a",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "You need to take care of `right.nullable`.",
        "createdAt" : "2019-11-02T23:10:03Z",
        "updatedAt" : "2019-11-04T08:51:03Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "9e9bc92e-acd8-4436-b9ca-17f4f6739c57",
        "parentId" : "71856f1e-738e-41c8-835e-093f04eec07a",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "all checked in `checkInputDataTypes`",
        "createdAt" : "2019-11-03T02:30:18Z",
        "updatedAt" : "2019-11-04T08:51:03Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "eb322b46-799d-4d54-9464-b83cdf76a40e",
        "parentId" : "71856f1e-738e-41c8-835e-093f04eec07a",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "I got your point here. thanks.",
        "createdAt" : "2019-11-03T03:22:03Z",
        "updatedAt" : "2019-11-04T08:51:03Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "32b67a41-a1d6-4b92-913a-df644fb31837",
        "parentId" : "71856f1e-738e-41c8-835e-093f04eec07a",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Yea, thanks.",
        "createdAt" : "2019-11-03T03:35:26Z",
        "updatedAt" : "2019-11-04T08:51:03Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "b8274175dfde798b24096b0042f1253a0c1fb656",
    "line" : 149,
    "diffHunk" : "@@ -1,1 +4074,4078 @@  }\n\n  override def dataType: DataType = left.dataType match {\n    case NullType => ArrayType(right.dataType, containsNull = false)\n    case ArrayType(elementType, containsNull) =>"
  },
  {
    "id" : "108e0c58-55fd-4324-aa7d-1d28483cd14c",
    "prId" : 26371,
    "prUrl" : "https://github.com/apache/spark/pull/26371#pullrequestreview-311705124",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "970288f6-139f-44b6-b4d4-5026364835fc",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "This is equivalent to Concat(Seq(left + CreateArray(Seq(right)))). Shall we use existing expressions instead adding duplicate codes?",
        "createdAt" : "2019-11-03T21:35:02Z",
        "updatedAt" : "2019-11-04T08:51:03Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "bb122f55-c304-4ddd-9c90-b72c2db81bbe",
        "parentId" : "970288f6-139f-44b6-b4d4-5026364835fc",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "There are some cases result different.",
        "createdAt" : "2019-11-04T01:51:03Z",
        "updatedAt" : "2019-11-04T08:51:03Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "eb6eac63-4986-4bc7-a39b-7b9e646ffc13",
        "parentId" : "970288f6-139f-44b6-b4d4-5026364835fc",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "Besides the Concat function for array is completely different from Postgres, I think that we have it implemented wrong ",
        "createdAt" : "2019-11-04T02:19:08Z",
        "updatedAt" : "2019-11-04T08:51:03Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "dfa9a354-2691-4a68-9229-04e9d9360b13",
        "parentId" : "970288f6-139f-44b6-b4d4-5026364835fc",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Could you please put the case here?",
        "createdAt" : "2019-11-04T02:21:15Z",
        "updatedAt" : "2019-11-04T08:51:03Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "c958bea4-02b5-407c-8eeb-cccbfc520809",
        "parentId" : "970288f6-139f-44b6-b4d4-5026364835fc",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "```sql\r\npostgres=# select array_cat(array[1,2], array[2]);\r\n array_cat\r\n-----------\r\n {1,2,2}\r\n(1 row)\r\n\r\npostgres=# select concat(array[1,2], array[2]);\r\n  concat\r\n----------\r\n {1,2}{2}\r\n(1 row)\r\n```\r\n\r\n```sql\r\nselect concat(array(1,2), array(2));\r\n[1,2,2]\r\n```",
        "createdAt" : "2019-11-04T02:25:21Z",
        "updatedAt" : "2019-11-04T08:51:03Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "b80cda4d-54be-4ac9-97f3-c92add140f7c",
        "parentId" : "970288f6-139f-44b6-b4d4-5026364835fc",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "between array_append and concat arrays, list only concat cases, the array_append can be found in the added ut cases.\r\n```sql\r\nspark-sql> select concat(null, array(1));\r\nError in query: cannot resolve 'concat(CAST(NULL AS STRING), array(1))' due to data type mismatch: input to function concat should all be the same type, but it's [string, array<int>]; line 1 pos 7;\r\n'Project [unresolvedalias(concat(cast(null as string), array(1)), None)]\r\n+- OneRowRelation\r\n\r\nspark-sql>\r\n```\r\n\r\n```\r\nspark-sql> select concat(array(1), array(null));\r\n19/11/04 10:26:33 ERROR SparkSQLDriver: Failed in [select concat(array(1), array(null))]\r\nscala.MatchError: NullType (of class org.apache.spark.sql.types.NullType$)\r\n\tat org.apache.spark.sql.catalyst.expressions.Cast.castToInt(Cast.scala:434)\r\n\tat org.apache.spark.sql.catalyst.expressions.Cast.org$apache$spark$sql$catalyst$expressions$Cast$$cast(Cast.scala:622)\r\n\tat org.apache.spark.sql.catalyst.expressions.Cast.castArray(Cast.scala:563)\r\n\tat org.apache.spark.sql.catalyst.expressions.Cast.org$apache$spark$sql$catalyst$expressions$Cast$$cast(Cast.scala:627)\r\n\tat org.apache.spark.sql.catalyst.expressions.Cast.cast$lzycompute(Cast.scala:639)\r\n```",
        "createdAt" : "2019-11-04T02:26:57Z",
        "updatedAt" : "2019-11-04T08:51:03Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "c1d0613f-9ca4-4d95-b2a0-ec60caeff7fb",
        "parentId" : "970288f6-139f-44b6-b4d4-5026364835fc",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "```\r\nspark-sql> select concat(null, array(1));\r\n```\r\nIs this valid in `ArrayAppend`? I think `checkInputDataTypes` just throws an exception in the case.\r\n\r\n```\r\nspark-sql> select concat(array(1), array(null));\r\n```\r\nWhat's the spark version that you used? I tried in the current master;\r\n```\r\nscala> sql(\"\"\"select concat(array(1), array(null))\"\"\").show()\r\n+-----------------------------+\r\n|concat(array(1), array(NULL))|\r\n+-----------------------------+\r\n|                         [1,]|\r\n+-----------------------------+\r\n```\r\n\r\nAnyway, I personally think you just need to handle these error cases for ArrayAppend.",
        "createdAt" : "2019-11-04T02:38:37Z",
        "updatedAt" : "2019-11-04T08:51:03Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "021dd2db-9c62-4d73-8f8a-67d8ce8beea8",
        "parentId" : "970288f6-139f-44b6-b4d4-5026364835fc",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "> ```sql\r\n> postgres=# select array_cat(array[1,2], array[2]);\r\n>  array_cat\r\n> -----------\r\n>  {1,2,2}\r\n> (1 row)\r\n> \r\n> postgres=# select concat(array[1,2], array[2]);\r\n>   concat\r\n> ----------\r\n>  {1,2}{2}\r\n> (1 row)\r\n> ```\r\n> \r\n> ```sql\r\n> select concat(array(1,2), array(2));\r\n> [1,2,2]\r\n> ```\r\n\r\nThis looks like Spark concat for array type is like Presto's array_cat. May not means it is wrong.",
        "createdAt" : "2019-11-04T02:42:17Z",
        "updatedAt" : "2019-11-04T08:51:03Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "f6ed7202-54f3-4e4d-8660-dde3042784e6",
        "parentId" : "970288f6-139f-44b6-b4d4-5026364835fc",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "```\r\n-- !query 13\r\nselect array_append(array(1,2), null)\r\n-- !query 13 schema\r\nstruct<array_append(array(1, 2), NULL):array<int>>\r\n-- !query 13 output\r\n[1,2,null]\r\n```",
        "createdAt" : "2019-11-04T02:42:47Z",
        "updatedAt" : "2019-11-04T08:51:03Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "16e6b4a8-936b-44fb-a843-4eadc3aac98f",
        "parentId" : "970288f6-139f-44b6-b4d4-5026364835fc",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "Sorry I made a mistake of the cases, i will recheck the master branch",
        "createdAt" : "2019-11-04T02:44:11Z",
        "updatedAt" : "2019-11-04T08:51:03Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "7eb80601-457f-408a-8183-f818076393c5",
        "parentId" : "970288f6-139f-44b6-b4d4-5026364835fc",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "I just have a quick research on the master branch which I build on 2019-10-29, \r\n\r\n```sql\r\nspark-sql> select concat(array(null), null);\r\nError in query: cannot resolve 'concat(array(NULL), CAST(NULL AS STRING))' due to data type mismatch: input to function concat should all be the same type, but it's [array<null>, string]; line 1 pos 7;\r\n'Project [unresolvedalias(concat(array(null), cast(null as string)), None)]\r\n+- OneRowRelation\r\n\r\nspark-sql> select concat(array(1, 2), null);\r\nError in query: cannot resolve 'concat(array(1, 2), CAST(NULL AS STRING))' due to data type mismatch: input to function concat should all be the same type, but it's [array<int>, string]; line 1 pos 7;\r\n'Project [unresolvedalias(concat(array(1, 2), cast(null as string)), None)]\r\n+- OneRowRelation\r\n\r\nspark-sql> select concat(array(1, 2), array(null));\r\n[1,2,null]\r\nTime taken: 2.908 seconds, Fetched 1 row(s)\r\n```\r\n\r\nAlso, currently I don't know how to change `select array_append(null, 3);` this to concat\r\n",
        "createdAt" : "2019-11-04T02:52:22Z",
        "updatedAt" : "2019-11-04T08:51:03Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "f1740b29-1d8a-4e3d-8378-a7dc7a93709e",
        "parentId" : "970288f6-139f-44b6-b4d4-5026364835fc",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "> > ```sql\r\n> > postgres=# select array_cat(array[1,2], array[2]);\r\n> >  array_cat\r\n> > -----------\r\n> >  {1,2,2}\r\n> > (1 row)\r\n> > \r\n> > postgres=# select concat(array[1,2], array[2]);\r\n> >   concat\r\n> > ----------\r\n> >  {1,2}{2}\r\n> > (1 row)\r\n> > ```\r\n> > \r\n> > \r\n> > ```sql\r\n> > select concat(array(1,2), array(2));\r\n> > [1,2,2]\r\n> > ```\r\n> \r\n> This looks like Spark concat for array type is like Presto's array_cat. May not means it is wrong.\r\n\r\nConcat used to be a string function as Hive before 2.4, and I do not know which standard or practice it follows now",
        "createdAt" : "2019-11-04T03:00:01Z",
        "updatedAt" : "2019-11-04T08:51:03Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "c7797acd-0ea8-42c1-8b0a-96931ec40f6b",
        "parentId" : "970288f6-139f-44b6-b4d4-5026364835fc",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "Hi @maropu , the key difference is when the array is null and the new element is not, array_append will create new array but concat's result will be null. please see the following cases.\r\n```sql\r\n-- !query 18\r\nselect array_append(a, e) from VALUES (array(1,2), 3), (array(3, 4), null), (null, 3), (null, null) tbl(a, e)\r\n-- !query 18 schema\r\nstruct<array_append(a, e):array<int>>\r\n-- !query 18 output\r\nNULL\r\n[1,2,3]\r\n[3,4,null]\r\n[3]\r\n\r\n-- !query 19\r\nselect concat(a, array(e)) from VALUES (array(1,2), 3), (array(3, 4), null), (null, 3), (null, null) tbl(a, e)\r\n-- !query 19 schema\r\nstruct<concat(a, array(e)):array<int>>\r\n-- !query 19 output\r\nNULL\r\nNULL\r\n[1,2,3]\r\n[3,4,null]\r\n```",
        "createdAt" : "2019-11-04T08:50:05Z",
        "updatedAt" : "2019-11-04T08:52:03Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "fdca8536-5cdc-4ac6-bba2-4bd224c447c1",
        "parentId" : "970288f6-139f-44b6-b4d4-5026364835fc",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "This is what postgres results：\r\n```SQL\r\npostgres=# select array_append(a, e) from (VALUES (array[1,2], 3), (array[3, 4], null), (null, 3), (null, null)) tbl(a, e);\r\n array_append\r\n--------------\r\n {1,2,3}\r\n {3,4,NULL}\r\n {3}\r\n {NULL}\r\n(4 rows)\r\n```",
        "createdAt" : "2019-11-04T09:08:18Z",
        "updatedAt" : "2019-11-04T09:08:18Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "0f72345b-cdf1-4fcf-830d-00e40bedc562",
        "parentId" : "970288f6-139f-44b6-b4d4-5026364835fc",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "I personally think it is more natural that the result is null if the left input (array) is null. But, I'm not sure which one is best. WDYT? @viirya @cloud-fan ",
        "createdAt" : "2019-11-04T22:39:59Z",
        "updatedAt" : "2019-11-04T22:41:03Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "0a4469e8-d08a-4d76-a62f-8e86a0d8be96",
        "parentId" : "970288f6-139f-44b6-b4d4-5026364835fc",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "yea, I think null is not an empty array, so appending something to a null (left) sounds to be a null too.",
        "createdAt" : "2019-11-04T23:30:58Z",
        "updatedAt" : "2019-11-04T23:30:58Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "a2a99fc7-a751-4344-95f9-1828ec1b12b3",
        "parentId" : "970288f6-139f-44b6-b4d4-5026364835fc",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "also cc @HyukjinKwon @gatorsmile @wangyum @dongjoon-hyun ",
        "createdAt" : "2019-11-05T10:22:51Z",
        "updatedAt" : "2019-11-05T10:22:52Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "d673f105-d84d-4cd2-95e0-a12dd7e1a5c6",
        "parentId" : "970288f6-139f-44b6-b4d4-5026364835fc",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I don't have a strong preference. Seems fine to follow postgres here, i.e. `array_append` is not nullable.",
        "createdAt" : "2019-11-05T12:42:59Z",
        "updatedAt" : "2019-11-05T12:43:00Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "b8274175dfde798b24096b0042f1253a0c1fb656",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +3946,3950 @@  \"\"\",\n  since = \"3.0.0\")\ncase class ArrayAppend(left: Expression, right: Expression) extends BinaryExpression {\n\n  private lazy val elementType: DataType = left.dataType match {"
  }
]