[
  {
    "id" : "cddf2148-679a-4056-903e-ea8e7c8a5340",
    "prId" : 33200,
    "prUrl" : "https://github.com/apache/spark/pull/33200#pullrequestreview-713087207",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a203ecd9-5a4d-4be1-a290-10a88bd903bd",
        "parentId" : null,
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "New requirement for the replace columns.",
        "createdAt" : "2021-07-22T17:59:54Z",
        "updatedAt" : "2021-07-22T18:04:42Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "d2e691086b80fb7f64d1bce144bd1bdd03428b8d",
    "line" : 54,
    "diffHunk" : "@@ -1,1 +3745,3749 @@        if (col.path.isDefined) {\n          throw QueryParsingErrors.operationInHiveStyleCommandUnsupportedError(\n            \"Replacing with a nested column\", \"REPLACE COLUMNS\", ctx)\n        }\n        col"
  },
  {
    "id" : "dd29ba44-b899-4f2e-9684-dd9959c6ba2f",
    "prId" : 33176,
    "prUrl" : "https://github.com/apache/spark/pull/33176#pullrequestreview-697511898",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "646965fc-b5e7-493b-8235-3206163bec66",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "We need to support parsing type string for LTZ and NTZ specifically so that people can avoid relying on config in their queries.",
        "createdAt" : "2021-07-01T17:29:49Z",
        "updatedAt" : "2021-07-01T17:30:10Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "c97ce11f-145e-488c-aaf9-3c3266c388fc",
        "parentId" : "646965fc-b5e7-493b-8235-3206163bec66",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "I would prefer doing that in separate PRs to keep this one clean. We should support the new keyword both on data type parsing and the type literals.\r\nhttps://issues.apache.org/jira/browse/SPARK-35977\r\nhttps://issues.apache.org/jira/browse/SPARK-35978",
        "createdAt" : "2021-07-01T17:51:11Z",
        "updatedAt" : "2021-07-01T17:51:19Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "6c6bf3a04572a832ff9224c1a3cd61b81784c625",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +2503,2507 @@      case (\"double\", Nil) => DoubleType\n      case (\"date\", Nil) => DateType\n      case (\"timestamp\", Nil) => SQLConf.get.timestampType\n      case (\"string\", Nil) => StringType\n      case (\"character\" | \"char\", length :: Nil) => CharType(length.getText.toInt)"
  },
  {
    "id" : "47b5b052-c0ef-447f-9279-d79cbb6b578a",
    "prId" : 32922,
    "prUrl" : "https://github.com/apache/spark/pull/32922#pullrequestreview-690240933",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1d5c6bef-a57c-410a-ba76-89195dbe4fc7",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "shall we forbid `interval month to month`?",
        "createdAt" : "2021-06-23T03:12:04Z",
        "updatedAt" : "2021-06-23T03:12:29Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "b0d22082-a147-4fe0-93dd-ff0489d9a6c1",
        "parentId" : "1d5c6bef-a57c-410a-ba76-89195dbe4fc7",
        "authorId" : "3a227965-84e0-47cf-9974-11293764f028",
        "body" : "Sounds reasonable. I'll open a following-up PR for year-month/day-time intervals.",
        "createdAt" : "2021-06-23T06:02:23Z",
        "updatedAt" : "2021-06-23T06:02:23Z",
        "lastEditedBy" : "3a227965-84e0-47cf-9974-11293764f028",
        "tags" : [
        ]
      }
    ],
    "commit" : "d260bb1bc27d68f0cdf2287662413351e6b7b3c0",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +2517,2521 @@    val start = YearMonthIntervalType.stringToField(ctx.from.getText.toLowerCase(Locale.ROOT))\n    val end = if (ctx.to != null) {\n      YearMonthIntervalType.stringToField(ctx.to.getText.toLowerCase(Locale.ROOT))\n    } else {\n      start"
  },
  {
    "id" : "ad2f24db-2307-4f17-ab7a-2e9d384364e8",
    "prId" : 32892,
    "prUrl" : "https://github.com/apache/spark/pull/32892#pullrequestreview-682442151",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "468c20e8-9a5a-4d3e-9643-5ad55fe39795",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "I have seen this already in other PR. Does it make sense to put it to a common place `case object DayTimeIntervalType`?",
        "createdAt" : "2021-06-13T19:54:01Z",
        "updatedAt" : "2021-06-13T19:54:01Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "cc67ff97-77f3-489d-bae1-3dd0500debdd",
        "parentId" : "468c20e8-9a5a-4d3e-9643-5ad55fe39795",
        "authorId" : "3a227965-84e0-47cf-9974-11293764f028",
        "body" : "Yeah, it seems a common logic. After this and #32893 are merged, I'll open a followup PR.",
        "createdAt" : "2021-06-13T20:20:42Z",
        "updatedAt" : "2021-06-13T20:20:42Z",
        "lastEditedBy" : "3a227965-84e0-47cf-9974-11293764f028",
        "tags" : [
        ]
      }
    ],
    "commit" : "eef88fcbe7770d942fd8cf1177791eebf0ad79e7",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +2358,2362 @@      } else {\n        assert(calendarInterval.months == 0)\n        val strToFieldIndex = DayTimeIntervalType.dayTimeFields.map(i =>\n          DayTimeIntervalType.fieldToString(i) -> i).toMap\n        val fromUnit ="
  },
  {
    "id" : "56c537db-a6f2-437e-af38-1775c313f062",
    "prId" : 32303,
    "prUrl" : "https://github.com/apache/spark/pull/32303#pullrequestreview-678799840",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9c2dfc3e-ea7e-4ea1-a831-a69fb571012d",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "We do not handle join hints for the lateral case?",
        "createdAt" : "2021-06-08T04:00:20Z",
        "updatedAt" : "2021-06-08T04:39:01Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "85153080-1127-4185-b343-2a7c40486203",
        "parentId" : "9c2dfc3e-ea7e-4ea1-a831-a69fb571012d",
        "authorId" : "b3ba992a-312c-46eb-b3c3-8d861d15ac40",
        "body" : "Good point. I will add it.",
        "createdAt" : "2021-06-08T05:06:02Z",
        "updatedAt" : "2021-06-08T05:06:02Z",
        "lastEditedBy" : "b3ba992a-312c-46eb-b3c3-8d861d15ac40",
        "tags" : [
        ]
      },
      {
        "id" : "16732917-5652-4fa6-91e2-4f56916e8d11",
        "parentId" : "9c2dfc3e-ea7e-4ea1-a831-a69fb571012d",
        "authorId" : "b3ba992a-312c-46eb-b3c3-8d861d15ac40",
        "body" : "Actually, does it make sense to have join hints for lateral joins? A lateral join is essentially a nested loop join. Ideally, the evaluation logic should be for each row in the left, plug the outer query attribute values into the outer references and evaluate the subquery. So it should only be planned as a (correlated) nested loop join. But since Spark doesn't support such execution, it first decorrelates the subquery and then rewrites the lateral join as a normal join. This seems to be an implementation detail and it doesn't make sense to add join hints here.",
        "createdAt" : "2021-06-08T06:18:37Z",
        "updatedAt" : "2021-06-08T06:18:37Z",
        "lastEditedBy" : "b3ba992a-312c-46eb-b3c3-8d861d15ac40",
        "tags" : [
        ]
      },
      {
        "id" : "ecef3641-218e-4406-8b0e-664fee6df5bc",
        "parentId" : "9c2dfc3e-ea7e-4ea1-a831-a69fb571012d",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Hints exist for the physical planner and the planner does not tell a difference between normal joins and lateral joins (cuz the optimizer rewrites lateral join into normal ones), so adding hints for this case makes sense to me. In fact, I think users will eventually want to control physical joins types for lateral joins, too, to tune user's queries for their workloads.ã€€Anyway, since this is an improvement, I think it is okay to fix it in follow-up.",
        "createdAt" : "2021-06-08T08:28:07Z",
        "updatedAt" : "2021-06-08T08:28:07Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "1bd22cb9-034a-4e01-b593-7249b9b90222",
        "parentId" : "9c2dfc3e-ea7e-4ea1-a831-a69fb571012d",
        "authorId" : "b3ba992a-312c-46eb-b3c3-8d861d15ac40",
        "body" : "Sounds good.",
        "createdAt" : "2021-06-08T17:29:46Z",
        "updatedAt" : "2021-06-08T17:29:46Z",
        "lastEditedBy" : "b3ba992a-312c-46eb-b3c3-8d861d15ac40",
        "tags" : [
        ]
      }
    ],
    "commit" : "01fa70ec156ad5540b862c4b13dd1257062f31fb",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +874,878 @@      val join = right.optionalMap(left) { (left, right) =>\n        if (relation.LATERAL != null) {\n          LateralJoin(left, LateralSubquery(right), Inner, None)\n        } else {\n          Join(left, right, Inner, None, JoinHint.NONE)"
  },
  {
    "id" : "70c306e0-ea7d-4690-87c0-584a1cc9861a",
    "prId" : 32209,
    "prUrl" : "https://github.com/apache/spark/pull/32209#pullrequestreview-638653318",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bff317e5-dbd8-414f-bcd4-e3ff8a10472e",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "A little bit of opinion. Why not create a new `parseIntervalLiteral` for ansi interval, but reuse `parseIntervalLiteral`?",
        "createdAt" : "2021-04-19T09:13:17Z",
        "updatedAt" : "2021-04-19T09:13:55Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      },
      {
        "id" : "5041b4f1-4e13-42ed-b79b-cd23e22c8f76",
        "parentId" : "bff317e5-dbd8-414f-bcd4-e3ff8a10472e",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "I think we will have to do that at the end to support all combinations of `to` and `from` + precision ( `like INTERVAL '1 2:3:4.123' DAY(6) TO SECOND(3)`). But for now, we implement simplest possible solution to unblock other sub-tasks needed for Milestone 1 in SPARK-27790. The Milestone 1 supposes feature parity of new ANSI types with `CalendarIntervalType`. ",
        "createdAt" : "2021-04-19T09:25:57Z",
        "updatedAt" : "2021-04-19T09:25:58Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "9609d563-8621-4026-9afa-bbe75d298948",
        "parentId" : "bff317e5-dbd8-414f-bcd4-e3ff8a10472e",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "I got it. @MaxGekk Thans for your explanation.",
        "createdAt" : "2021-04-19T09:28:41Z",
        "updatedAt" : "2021-04-19T09:28:41Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "790815029a4072346ac38c57737b98a619080aa0",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +2316,2320 @@    val calendarInterval = parseIntervalLiteral(ctx)\n    if (ctx.errorCapturingUnitToUnitInterval != null && !conf.legacyIntervalEnabled) {\n      // Check the `to` unit to distinguish year-month and day-time intervals because\n      // `CalendarInterval` doesn't have enough info. For instance, new CalendarInterval(0, 0, 0)\n      // can be derived from INTERVAL '0-0' YEAR TO MONTH as well as from"
  },
  {
    "id" : "b239a73d-5ba7-4839-9140-2e682d8e8e1f",
    "prId" : 31421,
    "prUrl" : "https://github.com/apache/spark/pull/31421#pullrequestreview-581055792",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c5ef10e8-6f9a-49b4-b107-84624c9c36d8",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "nit: 4 spaces indentation",
        "createdAt" : "2021-02-02T07:39:19Z",
        "updatedAt" : "2021-02-02T07:39:20Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "971ccbd5947924992a0c24acd40d40f1587d7741",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +504,508 @@   */\n  protected def visitStringConstant(\n    ctx: ConstantContext,\n    processNullLiteral: Boolean): String = withOrigin(ctx) {\n    ctx match {"
  },
  {
    "id" : "7741d79b-0a9c-493c-954c-32ceab21f16a",
    "prId" : 31421,
    "prUrl" : "https://github.com/apache/spark/pull/31421#pullrequestreview-581056073",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fe29d78d-700c-4abd-a8e6-12ae05e8c5ef",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "this name is weird, how about `legacyNullAsString`?",
        "createdAt" : "2021-02-02T07:39:50Z",
        "updatedAt" : "2021-02-02T07:39:51Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "971ccbd5947924992a0c24acd40d40f1587d7741",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +473,477 @@  override def visitPartitionSpec(\n      ctx: PartitionSpecContext): Map[String, Option[String]] = withOrigin(ctx) {\n    val processNullLiteral =\n      !conf.getConf(SQLConf.LEGACY_PARSE_NULL_PARTITION_SPEC_AS_STRING_LITERAL)\n    val parts = ctx.partitionVal.asScala.map { pVal =>"
  },
  {
    "id" : "740fdd56-8005-4339-9323-a362f19a2c54",
    "prId" : 30943,
    "prUrl" : "https://github.com/apache/spark/pull/30943#pullrequestreview-559864763",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6d78cca9-031c-4909-8353-69ac05b3635c",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "`val ignoreNulls = Option(ctx.nullsOption).map(_.getType == SqlBaseParser.IGNORE).getOrElse(false)`",
        "createdAt" : "2020-12-30T05:15:38Z",
        "updatedAt" : "2020-12-30T06:46:08Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "f437e157-3c6f-4ddc-8817-e49b9e37dcdd",
        "parentId" : "6d78cca9-031c-4909-8353-69ac05b3635c",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "OK",
        "createdAt" : "2020-12-30T06:02:53Z",
        "updatedAt" : "2020-12-30T06:46:08Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "4cc5eb1c6c4ee31186dca059154621753b0ee62f",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +1698,1702 @@    }\n    val filter = Option(ctx.where).map(expression(_))\n    val ignoreNulls =\n      Option(ctx.nullsOption).map(_.getType == SqlBaseParser.IGNORE).getOrElse(false)\n    val function = UnresolvedFunction("
  },
  {
    "id" : "3ba68c02-ddc9-4fcf-b962-7f433574e51a",
    "prId" : 30813,
    "prUrl" : "https://github.com/apache/spark/pull/30813#pullrequestreview-554250192",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a13a3664-5e41-40c6-8907-02fdaabe29dc",
        "parentId" : null,
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "`partitionSpec` is not using `UnresolvedPartitionSpec` since v2 command is not supported.",
        "createdAt" : "2020-12-17T03:15:22Z",
        "updatedAt" : "2020-12-17T03:15:36Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "b59a7d7b510f6597a673431157b1835486faf7b6",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +3817,3821 @@   */\n  override def visitSetTableSerDe(ctx: SetTableSerDeContext): LogicalPlan = withOrigin(ctx) {\n    AlterTableSerDeProperties(\n      UnresolvedTable(\n        visitMultipartIdentifier(ctx.multipartIdentifier),"
  },
  {
    "id" : "15545733-0348-4e85-8b74-5d6524542570",
    "prId" : 30648,
    "prUrl" : "https://github.com/apache/spark/pull/30648#pullrequestreview-547885813",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "17abb4e8-bf5a-4cfe-b559-fac9034700d9",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "`ANALYZE TABLES IN default COMPUTE STATISTICS;` -> `ANALYZE TABLES IN multi_part_name COMPUTE STATISTICS [NOSCAN];`?",
        "createdAt" : "2020-12-07T23:38:34Z",
        "updatedAt" : "2021-02-25T03:06:41Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "754e7425-890a-47d2-9174-c777c1e8d51c",
        "parentId" : "17abb4e8-bf5a-4cfe-b559-fac9034700d9",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "This is example, similar to https://github.com/apache/spark/blob/29fed23ba16d580e6247b6e70e9c9eef0698aa95/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala#L3494-L3508",
        "createdAt" : "2020-12-09T06:54:20Z",
        "updatedAt" : "2021-02-25T03:06:41Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      }
    ],
    "commit" : "81efb9d1f0dc43b3053d03d3ea183e436a972576",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +3659,3663 @@   * Example SQL for analyzing all tables in default database:\n   * {{{\n   *   ANALYZE TABLES IN default COMPUTE STATISTICS;\n   * }}}\n   */"
  },
  {
    "id" : "e5aecca3-d867-4209-8168-773153d307af",
    "prId" : 30648,
    "prUrl" : "https://github.com/apache/spark/pull/30648#pullrequestreview-547883320",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a7de128c-0faa-46bc-b883-95538632c6e6",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "nit: `Seq.empty[String]` -> `Nil`?",
        "createdAt" : "2020-12-07T23:39:36Z",
        "updatedAt" : "2021-02-25T03:06:41Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "27be8c2e-39d8-4c89-9e8d-560ffbfc5f22",
        "parentId" : "a7de128c-0faa-46bc-b883-95538632c6e6",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "+1",
        "createdAt" : "2020-12-09T06:48:36Z",
        "updatedAt" : "2021-02-25T03:06:41Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      }
    ],
    "commit" : "81efb9d1f0dc43b3053d03d3ea183e436a972576",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +3670,3674 @@    val multiPart = Option(ctx.multipartIdentifier).map(visitMultipartIdentifier)\n    AnalyzeTables(\n      UnresolvedNamespace(multiPart.getOrElse(Seq.empty[String])),\n      noScan = ctx.identifier != null)\n  }"
  },
  {
    "id" : "51df87cd-a6e4-4a7d-a894-d695f1b87196",
    "prId" : 30610,
    "prUrl" : "https://github.com/apache/spark/pull/30610#pullrequestreview-546668403",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4e164ced-e040-4eb8-9bb0-d0e149e8bffb",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "if `isView=false`, shall we create `UnresolvedTable`?",
        "createdAt" : "2020-12-07T08:28:38Z",
        "updatedAt" : "2020-12-07T08:28:38Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "3f915a09-565c-4f1c-af71-e78905bc71c1",
        "parentId" : "4e164ced-e040-4eb8-9bb0-d0e149e8bffb",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "I can't because `ALTER TABLE ... RENAME TO` supports temp views as well.",
        "createdAt" : "2020-12-08T01:10:32Z",
        "updatedAt" : "2020-12-08T01:10:32Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "d1031753346babfe91f540a5d96ef433e5c04313",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +3856,3860 @@    val relationStr = if (isView) \"VIEW\" else \"TABLE\"\n    RenameTable(\n      UnresolvedTableOrView(\n        visitMultipartIdentifier(ctx.from),\n        s\"ALTER $relationStr ... RENAME TO\"),"
  },
  {
    "id" : "566c6ac1-13a1-4599-838b-2bcc76418f00",
    "prId" : 30465,
    "prUrl" : "https://github.com/apache/spark/pull/30465#pullrequestreview-537288660",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1e9d9da8-0a51-4257-95c5-8317f22b89be",
        "parentId" : null,
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "It seems we do not need empty check in Line#1387 now: https://github.com/apache/spark/pull/30465/files#diff-26fe6b511f4b8aedf5bdffae0d7c03aba69463c09fa4f881d77520125796e0faR1387",
        "createdAt" : "2020-11-23T13:37:23Z",
        "updatedAt" : "2020-11-30T02:17:53Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      },
      {
        "id" : "26f9095a-f512-4289-ae08-7b21611de645",
        "parentId" : "1e9d9da8-0a51-4257-95c5-8317f22b89be",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "Let's remove `getLikeQuantifierExprs`",
        "createdAt" : "2020-11-24T09:20:29Z",
        "updatedAt" : "2020-11-30T02:17:53Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "2be24d1455a3078da2021781bab97c22979913be",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +1397,1401 @@        Option(ctx.quantifier).map(_.getType) match {\n          case Some(SqlBaseParser.ANY) | Some(SqlBaseParser.SOME) =>\n            validate(!ctx.expression.isEmpty, \"Expected something between '(' and ')'.\", ctx)\n            val expressions = expressionList(ctx.expression)\n            if (expressions.forall(_.foldable) && expressions.forall(_.dataType == StringType)) {"
  },
  {
    "id" : "562ac1ab-2b59-436e-b573-9dabcca2fc08",
    "prId" : 30421,
    "prUrl" : "https://github.com/apache/spark/pull/30421#pullrequestreview-599381116",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "35e702a9-0b00-405c-9226-058de8b5e285",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "can we add a TODO here? For v2 commands, we will cast the string back to its actual value, which is a waste and can be improved in the future.",
        "createdAt" : "2021-02-26T08:33:52Z",
        "updatedAt" : "2021-03-02T13:10:58Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "f710bd84-2b0e-4144-9a23-9f5c0be3d7bb",
        "parentId" : "35e702a9-0b00-405c-9226-058de8b5e285",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> can we add a TODO here? For v2 commands, we will cast the string back to its actual value, which is a waste and can be improved in the future.\r\n\r\nDone",
        "createdAt" : "2021-02-26T09:10:52Z",
        "updatedAt" : "2021-03-02T13:10:58Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "08c55f66226a4cd3bd5f6389225d510e9c821b23",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +513,517 @@      case Literal(null, _) if !legacyNullAsString => null\n      case l @ Literal(null, _) => l.toString\n      case l: Literal =>\n        // TODO For v2 commands, we will cast the string back to its actual value,\n        //  which is a waste and can be improved in the future."
  },
  {
    "id" : "289983b9-82b8-420d-b473-b3428ad2fe16",
    "prId" : 30421,
    "prUrl" : "https://github.com/apache/spark/pull/30421#pullrequestreview-599380738",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d12d2cbf-f4a6-434a-972a-6a89ff8a133a",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "does it work for `ADD PARTITION(a=date'xxx')`, etc.?",
        "createdAt" : "2021-02-26T08:43:01Z",
        "updatedAt" : "2021-03-02T13:10:58Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "d4db2c46-80b2-428b-bcae-51ca81af4999",
        "parentId" : "d12d2cbf-f4a6-434a-972a-6a89ff8a133a",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> does it work for `ADD PARTITION(a=date'xxx')`, etc.?\r\n\r\nSupport and add test case  in UT",
        "createdAt" : "2021-02-26T09:10:28Z",
        "updatedAt" : "2021-03-02T13:10:58Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "08c55f66226a4cd3bd5f6389225d510e9c821b23",
    "line" : 1,
    "diffHunk" : "@@ -1,1 +507,511 @@   * String -> Literal -> String.\n   */\n  protected def visitStringConstant(\n      ctx: ConstantContext,\n      legacyNullAsString: Boolean): String = withOrigin(ctx) {"
  },
  {
    "id" : "dd7d5c20-3454-4511-a258-275d94b83fb3",
    "prId" : 30229,
    "prUrl" : "https://github.com/apache/spark/pull/30229#pullrequestreview-523079967",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cae319fb-9ad7-403b-af69-c900817c4f7b",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "`AnalyzeColumn` allows temp view but `AnalyzeTable`  does not?",
        "createdAt" : "2020-11-04T05:09:25Z",
        "updatedAt" : "2020-11-04T05:09:25Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "d767bea3-0aac-4ab6-b854-6c46353e752b",
        "parentId" : "cae319fb-9ad7-403b-af69-c900817c4f7b",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "Yes, that is what I see. The following is for `AnalyzeColumnCommand`:\r\nhttps://github.com/apache/spark/blob/1740b29b3f006abd08bc01b0ca807c3721d4bb0e/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzeColumnCommand.scala#L53-L57\r\n\r\nOn the other hand, I don't see `getTempView` or `getTempViewOrPermanentTableMetadata` inside `AnalyzeTableCommand`. But looks like we can support temporary views similar to permanent views in `AnalyzeTableCommand`?",
        "createdAt" : "2020-11-04T05:47:42Z",
        "updatedAt" : "2020-11-04T05:47:42Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      },
      {
        "id" : "bc57162c-1da6-4311-9577-01b904966884",
        "parentId" : "cae319fb-9ad7-403b-af69-c900817c4f7b",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "OK,  let's keep the current behavior for now, and improve it later.",
        "createdAt" : "2020-11-04T06:49:58Z",
        "updatedAt" : "2020-11-04T06:49:58Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "c80e8eb5828ff68a1ebab4f9ee56b55b852ba7d0",
    "line" : 31,
    "diffHunk" : "@@ -1,1 +3264,3268 @@      checkPartitionSpec()\n      AnalyzeColumn(\n        UnresolvedTableOrView(tableName),\n        Option(visitIdentifierSeq(ctx.identifierSeq())),\n        allColumns = false)"
  },
  {
    "id" : "d141991c-c884-4ea2-bcf1-ca6bc2ca6d57",
    "prId" : 30212,
    "prUrl" : "https://github.com/apache/spark/pull/30212#pullrequestreview-555947622",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "755a2cbf-6627-4c26-85d2-d486b4886ffd",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "nit: indent",
        "createdAt" : "2020-12-19T13:29:33Z",
        "updatedAt" : "2021-03-30T05:55:50Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "e78ff9c5-232b-496f-8a23-01f691f58e66",
        "parentId" : "755a2cbf-6627-4c26-85d2-d486b4886ffd",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "Done",
        "createdAt" : "2020-12-19T15:10:09Z",
        "updatedAt" : "2021-03-30T05:55:50Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "bfc03afaf9fc0974cb218b3b3121829ae9e5c413",
    "line" : 55,
    "diffHunk" : "@@ -1,1 +944,948 @@            if (groupingAnalytics != null) {\n              val groupingSets = groupingAnalytics.groupingSet.asScala\n                .map(_.expression.asScala.map(e => expression(e)).toSeq)\n              if (groupingAnalytics.CUBE != null) {\n                // CUBE(A, B, (A, B), ()) is not supported."
  },
  {
    "id" : "65d5f537-85d9-4191-9690-34ed8750fdc7",
    "prId" : 30212,
    "prUrl" : "https://github.com/apache/spark/pull/30212#pullrequestreview-621959222",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "57c87db7-5c2d-4f1e-988d-01f389079b50",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Seems we don't distinguish `(a)` and `a`, then how do we handle `GROUPING SETS(a, b, c)` and `GROUPING SETS((a), (b), (c))`?\r\n\r\nAccording to the document, `GROUPING SETS(a, b, c)` is `GROUPING SETS((a, b), (a, c), (b, c), (a), (b), (c))`.",
        "createdAt" : "2021-03-26T09:20:35Z",
        "updatedAt" : "2021-03-30T05:55:50Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "ba4aa899-3068-4b85-b5b8-49154bf6eb9d",
        "parentId" : "57c87db7-5c2d-4f1e-988d-01f389079b50",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "or is my understanding totally wrong? `a` is the same as `(a)`? Then we should update the document in https://github.com/apache/spark/pull/30212/files#diff-e895c94a06560e4db8e99be99cc7864c065c2302402dcd158cd825f3b09f878fR63",
        "createdAt" : "2021-03-26T09:33:12Z",
        "updatedAt" : "2021-03-30T05:55:50Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "27f06022-fb83-47ca-bf97-b1ed8e4c7170",
        "parentId" : "57c87db7-5c2d-4f1e-988d-01f389079b50",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> Seems we don't distinguish `(a)` and `a`, then how do we handle `GROUPING SETS(a, b, c)` and `GROUPING SETS((a), (b), (c))`?\r\n> \r\n> According to the document, `GROUPING SETS(a, b, c)` is `GROUPING SETS((a, b), (a, c), (b, c), (a), (b), (c))`.\r\n\r\n`GROUPING SETS(a, b, c)`  is same as  `GROUPING SETS((a), (b), (c))`\r\n",
        "createdAt" : "2021-03-26T09:42:16Z",
        "updatedAt" : "2021-03-30T05:55:50Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "99bb6130-beec-458d-ad02-f14989d545f9",
        "parentId" : "57c87db7-5c2d-4f1e-988d-01f389079b50",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "here the group set  depending on the parser logic right?\r\nhttps://github.com/apache/spark/blob/8945e7c52f8d09282f6847c19bd926ac4c20e11a/sql/catalyst/src/main/antlr4/org/apache/spark/sql/catalyst/parser/SqlBase.g4#L604-L611\r\n\r\nAccording to the definition, `a` can be a grouping_set, `(a)` can be a grouping_set, `(a, b)` can be a grouping_set.\r\n\r\nYou may make a miss about how expand `cube/rollup` and origin `grouping set`",
        "createdAt" : "2021-03-26T09:57:32Z",
        "updatedAt" : "2021-03-30T05:55:50Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "a0751f19-8392-4ce7-bde2-bd35f4d4d3ba",
        "parentId" : "57c87db7-5c2d-4f1e-988d-01f389079b50",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "For SQL\r\n```\r\nselect id, city, car_model, sum(quantity)\r\nfrom dealer group by grouping sets((id), (city), (car_model))\r\nUNION all\r\nselect id, city, car_model, sum(quantity)\r\nfrom dealer group by grouping sets(id, city, car_model)\r\norder by id, city, car_model\r\n```\r\nResult in PGSQL\r\n![image](https://user-images.githubusercontent.com/46485123/112615257-3b203000-8e5d-11eb-958e-fb5196c9592c.png)\r\n\r\n",
        "createdAt" : "2021-03-26T10:01:17Z",
        "updatedAt" : "2021-03-30T05:55:50Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "bfc03afaf9fc0974cb218b3b3121829ae9e5c413",
    "line" : 33,
    "diffHunk" : "@@ -1,1 +923,927 @@        // subset of the columns appearing in group by expression.\n        val groupingSets =\n          ctx.groupingSet.asScala.map(_.expression.asScala.map(e => expression(e)).toSeq)\n        Aggregate(Seq(GroupingSets(groupingSets.toSeq, groupByExpressions)),\n          selectExpressions, query)"
  },
  {
    "id" : "92a1b74c-bbce-4e6a-971e-75cf6bc5c7b7",
    "prId" : 30212,
    "prUrl" : "https://github.com/apache/spark/pull/30212#pullrequestreview-623867728",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "09689e85-42c4-4012-8988-8156cba01563",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "could you handle the two error cases separately?",
        "createdAt" : "2021-03-30T05:13:03Z",
        "updatedAt" : "2021-03-30T05:55:50Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "11c4054f-b74c-4e5b-9b2f-e6f115bf5da1",
        "parentId" : "09689e85-42c4-4012-8988-8156cba01563",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> could you handle the two error cases separately?\r\n\r\nDone",
        "createdAt" : "2021-03-30T05:56:03Z",
        "updatedAt" : "2021-03-30T05:56:04Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "bfc03afaf9fc0974cb218b3b3121829ae9e5c413",
    "line" : 81,
    "diffHunk" : "@@ -1,1 +970,974 @@      val (groupingSet, expressions) = groupByExpressions.partition(_.isInstanceOf[GroupingSet])\n      if (expressions.nonEmpty && groupingSet.nonEmpty) {\n        throw new ParseException(\"Partial CUBE/ROLLUP/GROUPING SETS like \" +\n          \"`GROUP BY a, b, CUBE(a, b)` is not supported.\",\n          ctx)"
  },
  {
    "id" : "85efcc6d-95e8-4536-b802-360b3d92abc7",
    "prId" : 29728,
    "prUrl" : "https://github.com/apache/spark/pull/29728#pullrequestreview-487685289",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5c49af63-b030-472b-87b9-3f247b8de9d2",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "How about the case, `select * from t1 where (a > 3) = (b < 5) = (c > 3)` ?",
        "createdAt" : "2020-09-14T12:06:10Z",
        "updatedAt" : "2020-09-14T12:11:52Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "d1c08607-4ff6-455b-a918-bb676789f5e8",
        "parentId" : "5c49af63-b030-472b-87b9-3f247b8de9d2",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "btw, have you check if we could update the ANTLR parser rules to prevent this case?",
        "createdAt" : "2020-09-14T12:08:19Z",
        "updatedAt" : "2020-09-14T12:11:52Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "92f1272d16383046fdb260ca4fcafcbda104a6e0",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +1328,1332 @@    val left = expression(ctx.left)\n    val right = expression(ctx.right)\n    val isUnsupportedComparison = (left, right) match {\n      case (_: Attribute, _: Predicate) => true\n      case (_: Predicate, _: Attribute) => true"
  },
  {
    "id" : "6201ddd9-56d2-4040-abd7-9737e800c58a",
    "prId" : 29728,
    "prUrl" : "https://github.com/apache/spark/pull/29728#pullrequestreview-487685289",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e3c63eb9-ffba-4977-816d-064892c272b8",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Could we check this error case at the beginning of this function?\r\n```\r\n  override def visitComparison(ctx: ComparisonContext): Expression = withOrigin(ctx) {\r\n    val left = expression(ctx.left)\r\n    val right = expression(ctx.right)\r\n    if (<Error checks>) {\r\n      throw new ParseException(\"Syntax error at or near\", ctx)\r\n    }\r\n    val operator = ctx.comparisonOperator().getChild(0).asInstanceOf[TerminalNode]\r\n    operator.getSymbol.getType match {\r\n      case SqlBaseParser.EQ =>\r\n        EqualTo(left, right)\r\n      case SqlBaseParser.NSEQ =>\r\n        EqualNullSafe(left, right)\r\n      case SqlBaseParser.NEQ | SqlBaseParser.NEQJ =>\r\n        Not(EqualTo(left, right))\r\n        ...\r\n      case SqlBaseParser.GTE =>\r\n        GreaterThanOrEqual(left, right)\r\n    }\r\n```",
        "createdAt" : "2020-09-14T12:11:16Z",
        "updatedAt" : "2020-09-14T12:11:52Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "92f1272d16383046fdb260ca4fcafcbda104a6e0",
    "line" : 33,
    "diffHunk" : "@@ -1,1 +1350,1354 @@        GreaterThanOrEqual(left, right)\n      case _ =>\n        throw new ParseException(\"Syntax error at or near\", ctx)\n    }\n  }"
  },
  {
    "id" : "02f7c95b-ea58-4de7-9473-07803a929453",
    "prId" : 29728,
    "prUrl" : "https://github.com/apache/spark/pull/29728#pullrequestreview-487776547",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "31a3a67f-ef75-4789-b762-855c6eef3caa",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Total nit, but isn't it clearer to write `isSupportedComparison` and avoid inverting the logic everywhere?",
        "createdAt" : "2020-09-14T13:37:30Z",
        "updatedAt" : "2020-09-14T13:37:30Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      }
    ],
    "commit" : "92f1272d16383046fdb260ca4fcafcbda104a6e0",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +1328,1332 @@    val left = expression(ctx.left)\n    val right = expression(ctx.right)\n    val isUnsupportedComparison = (left, right) match {\n      case (_: Attribute, _: Predicate) => true\n      case (_: Predicate, _: Attribute) => true"
  },
  {
    "id" : "a834190a-2007-41ca-8ecc-83a5f0d1163e",
    "prId" : 29198,
    "prUrl" : "https://github.com/apache/spark/pull/29198#pullrequestreview-456497879",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a35b5421-ed2b-43ef-a98c-b493ec5cc1a3",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "will analyzer give a nice error if `UnresolvedFunc` is left to `CheckAnalysis`?",
        "createdAt" : "2020-07-28T10:18:09Z",
        "updatedAt" : "2020-07-28T16:26:35Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "674689de-cefe-412f-804b-4d61003be16d",
        "parentId" : "a35b5421-ed2b-43ef-a98c-b493ec5cc1a3",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "nvm, this won't happen for now.",
        "createdAt" : "2020-07-28T10:19:28Z",
        "updatedAt" : "2020-07-28T16:26:35Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "c662619bd00e74d4f683fcb565868bf72e4890d6",
    "line" : 29,
    "diffHunk" : "@@ -1,1 +3633,3637 @@    val functionName = visitMultipartIdentifier(ctx.multipartIdentifier)\n    DropFunction(\n      UnresolvedFunc(functionName),\n      ctx.EXISTS != null,\n      ctx.TEMPORARY != null)"
  },
  {
    "id" : "fc965b93-791e-478b-a364-c37ea4084e50",
    "prId" : 29087,
    "prUrl" : "https://github.com/apache/spark/pull/29087#pullrequestreview-561508268",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bea8da5b-c93b-46ad-aae8-f41776c8cff2",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "In hive, one cannot use `distinct` for TRANSFORM? e.g., `SELECT TRANSFORM(distinct a, b)`?",
        "createdAt" : "2021-01-05T00:31:17Z",
        "updatedAt" : "2021-03-25T08:21:50Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "9a0fa32d-02de-4cff-b372-2754a3201f2f",
        "parentId" : "bea8da5b-c93b-46ad-aae8-f41776c8cff2",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> In hive, one cannot use `distinct` for TRANSFORM? e.g., `SELECT TRANSFORM(distinct a, b)`?\r\n\r\nNot support. Do we need to support this?\r\n```\r\nhive>\r\n    >\r\n    > select transform(distinct a) using 'cat' from (select 1 as a) temp;\r\nNoViableAltException(96@[104:1: selectExpression : ( ( tableAllColumns )=> tableAllColumns | expression );])\r\n\tat org.apache.hadoop.hive.ql.parse.HiveParser_SelectClauseParser$DFA19.specialStateTransition(HiveParser_SelectClauseParser.java:5380)\r\n\tat org.antlr.runtime.DFA.predict(DFA.java:80)\r\n\tat org.apache.hadoop.hive.ql.parse.HiveParser_SelectClauseParser.selectExpression(HiveParser_SelectClauseParser.java:2215)\r\n\tat org.apache.hadoop.hive.ql.parse.HiveParser_SelectClauseParser.selectExpressionList(HiveParser_SelectClauseParser.java:2297)\r\n\tat org.apache.hadoop.hive.ql.parse.HiveParser_SelectClauseParser.selectTrfmClause(HiveParser_SelectClauseParser.java:1299)\r\n\tat org.apache.hadoop.hive.ql.parse.HiveParser_SelectClauseParser.selectClause(HiveParser_SelectClauseParser.java:968)\r\n\tat org.apache.hadoop.hive.ql.parse.HiveParser.selectClause(HiveParser.java:41964)\r\n\tat org.apache.hadoop.hive.ql.parse.HiveParser.atomSelectStatement(HiveParser.java:36720)\r\n\tat org.apache.hadoop.hive.ql.parse.HiveParser.selectStatement(HiveParser.java:36987)\r\n\tat org.apache.hadoop.hive.ql.parse.HiveParser.regularBody(HiveParser.java:36633)\r\n\tat org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpressionBody(HiveParser.java:35822)\r\n\tat org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpression(HiveParser.java:35710)\r\n\tat org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:2284)\r\n\tat org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1333)\r\n\tat org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:208)\r\n\tat org.apache.hadoop.hive.ql.parse.ParseUtils.parse(ParseUtils.java:77)\r\n\tat org.apache.hadoop.hive.ql.parse.ParseUtils.parse(ParseUtils.java:70)\r\n\tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:468)\r\n\tat org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1317)\r\n\tat org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1457)\r\n\tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:1237)\r\n\tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:1227)\r\n\tat org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:233)\r\n\tat org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:184)\r\n\tat org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:403)\r\n\tat org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:821)\r\n\tat org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:759)\r\n\tat org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:686)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:606)\r\n\tat org.apache.hadoop.util.RunJar.run(RunJar.java:221)\r\n\tat org.apache.hadoop.util.RunJar.main(RunJar.java:136)\r\nFAILED: ParseException line 1:17 cannot recognize input near 'distinct' 'a' ')' in select expression\r\n```",
        "createdAt" : "2021-01-05T02:12:12Z",
        "updatedAt" : "2021-03-25T08:21:51Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "28f7201a-fece-4b70-a2ab-c2b7138ee7dd",
        "parentId" : "bea8da5b-c93b-46ad-aae8-f41776c8cff2",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "I think we don't need to support it in this PR and its okay to just follow the hive behaviour.",
        "createdAt" : "2021-01-05T03:07:52Z",
        "updatedAt" : "2021-03-25T08:21:51Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "8c5b295a-ed3b-4526-b03c-528635a29050",
        "parentId" : "bea8da5b-c93b-46ad-aae8-f41776c8cff2",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> I think we don't need to support it in this PR and its okay to just follow the hive behaviour.\r\n\r\nYea",
        "createdAt" : "2021-01-05T04:34:00Z",
        "updatedAt" : "2021-03-25T08:21:51Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "12787053aec9d015506d5c59c58e91dd23d5bb82",
    "line" : 63,
    "diffHunk" : "@@ -1,1 +684,688 @@      havingClause,\n      windowClause,\n      isDistinct = false)\n\n    ScriptTransformation("
  },
  {
    "id" : "2f8bba72-6fb3-4a86-8c18-559f34ca5adb",
    "prId" : 29087,
    "prUrl" : "https://github.com/apache/spark/pull/29087#pullrequestreview-561580503",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2a63b793-4d54-44ad-8db3-08bd93cbeea2",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "I thinks its better to leave comments about how the analyzer handle this `UnresolvedStar` later.",
        "createdAt" : "2021-01-05T07:51:43Z",
        "updatedAt" : "2021-03-25T08:21:51Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "9b398acb-23b6-40ee-94d9-3b2f64bcb9f4",
        "parentId" : "2a63b793-4d54-44ad-8db3-08bd93cbeea2",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "Done",
        "createdAt" : "2021-01-05T08:03:11Z",
        "updatedAt" : "2021-03-25T08:21:51Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "12787053aec9d015506d5c59c58e91dd23d5bb82",
    "line" : 68,
    "diffHunk" : "@@ -1,1 +688,692 @@    ScriptTransformation(\n      // TODO: Remove this logic and see SPARK-34035\n      Seq(UnresolvedStar(None)),\n      string(transformClause.script),\n      attributes,"
  },
  {
    "id" : "24af0c9f-cce8-4e58-ae2c-5796b54044e0",
    "prId" : 29087,
    "prUrl" : "https://github.com/apache/spark/pull/29087#pullrequestreview-563169948",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "001e3602-2903-49db-81b0-370cb21bddf8",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "hm, on second thought, we cannot remove this param `input` from `ScriptTransformation` in this PR? Since the input exprs of the current `ScriptTransformation` implementaiton always coms from child's output, IIUC we don't need this param anymore?",
        "createdAt" : "2021-01-06T13:45:47Z",
        "updatedAt" : "2021-03-25T08:21:51Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "ac13bf02-9085-4b67-b768-8abe6d74d6e4",
        "parentId" : "001e3602-2903-49db-81b0-370cb21bddf8",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> hm, on second thought, we cannot remove this param `input` from `ScriptTransformation` in this PR? Since the input exprs of the current `ScriptTransformation` implementaiton always coms from child's output, IIUC we don't need this param anymore?\r\n\r\nIt looks like this. We can replace `input` with `child.output` directly. That's a really nice suggestion since current way(converting LogicalPlan it looks a little weird). If we remove this `input` parameter and add correct  comment.\r\nThe whole process looks more natural.  I have tried it in local, a new big diff. How about a new ticket about refactor this? ",
        "createdAt" : "2021-01-06T14:39:56Z",
        "updatedAt" : "2021-03-25T08:21:51Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "b17d758b-0d85-4dcb-be08-aa8626cd4b7a",
        "parentId" : "001e3602-2903-49db-81b0-370cb21bddf8",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Nice. Just removing the param causes a big diff? I'd like to remove[ the current the weird Analyzer code ](https://github.com/apache/spark/pull/29087/files#diff-ed19f376a63eba52eea59ca71f3355d4495fad4fad4db9a3324aade0d4986a47R1520-R1526) to handle the unresolved star in this PR though.",
        "createdAt" : "2021-01-06T23:44:51Z",
        "updatedAt" : "2021-03-25T08:21:51Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "07ca28c3-e633-40c7-b1b6-9583b7bc92d5",
        "parentId" : "001e3602-2903-49db-81b0-370cb21bddf8",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> Nice. Just removing the param causes a big diff? I'd like to remove[ the current the weird Analyzer code ](https://github.com/apache/spark/pull/29087/files#diff-ed19f376a63eba52eea59ca71f3355d4495fad4fad4db9a3324aade0d4986a47R1520-R1526) to handle the unresolved star in this PR though.\r\n\r\nYea, will change a lot of file.  Compared to the origin code here(the current the weird Analyzer code) ï¼Œcurrent change seem s not so weird, but  for the whole process, really weird.\r\n\r\nCreate a ticket for this https://issues.apache.org/jira/browse/SPARK-34035\r\n![image](https://user-images.githubusercontent.com/46485123/103833742-afe49900-50bc-11eb-979a-b610582383d0.png)\r\n",
        "createdAt" : "2021-01-06T23:53:41Z",
        "updatedAt" : "2021-03-25T08:21:51Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "325f29b9-cd6c-40c1-8a9b-e48cfb63f7aa",
        "parentId" : "001e3602-2903-49db-81b0-370cb21bddf8",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "> Yea, will change a lot of file. \r\n\r\nHow many changed lines of codes there?",
        "createdAt" : "2021-01-07T00:36:27Z",
        "updatedAt" : "2021-03-25T08:21:51Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "1f5f00df-e95b-4336-bb82-fd674b75e921",
        "parentId" : "001e3602-2903-49db-81b0-370cb21bddf8",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> > Yea, will change a lot of file.\r\n> \r\n> How many changed lines of codes there?\r\n\r\nNearly 100 lines, most of changes are about UT",
        "createdAt" : "2021-01-07T01:27:50Z",
        "updatedAt" : "2021-03-25T08:21:51Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "d2e5394b-46c7-44b1-9294-08e22359e74d",
        "parentId" : "001e3602-2903-49db-81b0-370cb21bddf8",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "okay, I don't have a strong opinion on it, so please follow other reviewer's comment.",
        "createdAt" : "2021-01-07T01:42:39Z",
        "updatedAt" : "2021-03-25T08:21:51Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "5c1d508f-f4cd-414c-871f-86f7567e8f74",
        "parentId" : "001e3602-2903-49db-81b0-370cb21bddf8",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> okay, I don't have a strong opinion on it, so please follow other reviewer's comment.\r\n\r\nYea",
        "createdAt" : "2021-01-07T02:24:07Z",
        "updatedAt" : "2021-03-25T08:21:51Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "12787053aec9d015506d5c59c58e91dd23d5bb82",
    "line" : 68,
    "diffHunk" : "@@ -1,1 +688,692 @@    ScriptTransformation(\n      // TODO: Remove this logic and see SPARK-34035\n      Seq(UnresolvedStar(None)),\n      string(transformClause.script),\n      attributes,"
  },
  {
    "id" : "1064ce60-f72b-4e1c-9289-95b7b0be0d13",
    "prId" : 29085,
    "prUrl" : "https://github.com/apache/spark/pull/29085#pullrequestreview-453357346",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4f6a20cb-a931-4465-83a4-d6d149784a1f",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Plz add tests in `PlanParserSuite`",
        "createdAt" : "2020-07-22T12:05:35Z",
        "updatedAt" : "2020-07-23T03:08:37Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "29bd62ab-3342-45fd-a6a2-e0becd8bcd74",
        "parentId" : "4f6a20cb-a931-4465-83a4-d6d149784a1f",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> Plz add tests in `PlanParserSuite`\r\n\r\nDone",
        "createdAt" : "2020-07-22T14:11:58Z",
        "updatedAt" : "2020-07-23T03:08:37Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "03d3409f6ca641a4f10fdc2ac71479445220f676",
    "line" : 30,
    "diffHunk" : "@@ -1,1 +770,774 @@   * Create a [[ScriptInputOutputSchema]].\n   */\n  protected def withScriptIOSchema(\n      ctx: ParserRuleContext,\n      inRowFormat: RowFormatContext,"
  },
  {
    "id" : "880db780-a8cf-4742-91d4-9f4f2f24588e",
    "prId" : 29022,
    "prUrl" : "https://github.com/apache/spark/pull/29022#pullrequestreview-444439479",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "62103494-8b8d-4cf0-b160-ad2f995327cc",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Could you add a function description like the other functions around here, @yaooqinn ?",
        "createdAt" : "2020-07-08T04:12:20Z",
        "updatedAt" : "2020-07-08T06:13:01Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "23fd096e-c4e4-4fbf-8fbc-640ee6e466bf",
        "parentId" : "62103494-8b8d-4cf0-b160-ad2f995327cc",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "done, thanks!",
        "createdAt" : "2020-07-08T06:13:26Z",
        "updatedAt" : "2020-07-08T06:13:27Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "6e422ff503d03191d47d44d6659fe9a6027a4fd9",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +2034,2038 @@   * Create a Float Literal expression.\n   */\n  override def visitFloatLiteral(ctx: FloatLiteralContext): Literal = {\n    val rawStrippedQualifier = ctx.getText.substring(0, ctx.getText.length - 1)\n    numericLiteral(ctx, rawStrippedQualifier,"
  },
  {
    "id" : "a33fde68-e5c7-471f-86be-c9912cca8a21",
    "prId" : 28935,
    "prUrl" : "https://github.com/apache/spark/pull/28935#pullrequestreview-438885052",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "90487e31-1e43-4d08-b8a9-cbf48550ae7d",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "We can just reuse `NullType`. We should forbid creating table with null type completely, including `spark.catalog.createTable`.",
        "createdAt" : "2020-06-29T04:53:29Z",
        "updatedAt" : "2020-06-30T01:45:25Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "3fa76cfaf7aaca5c5d2ca8ad2743bdefae208b61",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +2213,2217 @@        DecimalType(precision.getText.toInt, scale.getText.toInt)\n      case (\"interval\", Nil) => CalendarIntervalType\n      case (\"void\", Nil) => HiveVoidType\n      case (dt, params) =>\n        val dtStr = if (params.nonEmpty) s\"$dt(${params.mkString(\",\")})\" else dt"
  },
  {
    "id" : "b3279b34-a8ac-4557-94e1-94d50a76cfd7",
    "prId" : 28935,
    "prUrl" : "https://github.com/apache/spark/pull/28935#pullrequestreview-440486121",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "27400703-1d4c-4699-86a5-4c2ced96d9c6",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I don't get it why we need `HiveVoidType`. What happens if we just parse `void` to `NullType`?",
        "createdAt" : "2020-06-30T07:01:11Z",
        "updatedAt" : "2020-06-30T07:01:11Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "8fd7ccc7-5915-4af1-9e28-e97f553600c9",
        "parentId" : "27400703-1d4c-4699-86a5-4c2ced96d9c6",
        "authorId" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "body" : "Because that could indicate VOID is a Hive type, the handle processing is more unified. Or, we can just use the PR #28833 ",
        "createdAt" : "2020-06-30T08:23:15Z",
        "updatedAt" : "2020-06-30T08:23:15Z",
        "lastEditedBy" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "tags" : [
        ]
      },
      {
        "id" : "08d394fe-31df-43bc-b6de-b5ad56e6235d",
        "parentId" : "27400703-1d4c-4699-86a5-4c2ced96d9c6",
        "authorId" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "body" : "For example,  below function will point the failure is due to the legacy hive void type. If we mix VOID and NULL, I am not sure it would be better than separation.\r\n```scala\r\n  def failVoidType(dt: DataType): Unit = {\r\n    if (HiveVoidType.containsVoidType(dt)) {\r\n      throw new AnalysisException(\r\n        \"Cannot create tables with Hive VOID type.\")\r\n    }\r\n  }\r\n```",
        "createdAt" : "2020-06-30T09:35:01Z",
        "updatedAt" : "2020-06-30T09:35:01Z",
        "lastEditedBy" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "tags" : [
        ]
      },
      {
        "id" : "5e2c11ab-1400-4e44-b345-83b7736207b9",
        "parentId" : "27400703-1d4c-4699-86a5-4c2ced96d9c6",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "VOID and NULL are indeed the same type. We can just check null type and fail with error message: `Cannot create tables with VOID type`",
        "createdAt" : "2020-06-30T15:13:59Z",
        "updatedAt" : "2020-06-30T15:13:59Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "cc3c0595-ce3c-4a25-8a9d-dd8c46ce95cc",
        "parentId" : "27400703-1d4c-4699-86a5-4c2ced96d9c6",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "The point is consistency: The VOID type in SQL statement should be the same as `NullType` specified by Scala API in `spark.catalog.createTable`.",
        "createdAt" : "2020-06-30T15:18:26Z",
        "updatedAt" : "2020-06-30T15:18:26Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "eb3383be-0613-4d3f-a1f5-465d41cbbff8",
        "parentId" : "27400703-1d4c-4699-86a5-4c2ced96d9c6",
        "authorId" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "body" : "@cloud-fan , ok. I will follow your suggestion to fix it in #28833 , since this PR is a refator with new type `HiveVoidType`. Now we don't need it.",
        "createdAt" : "2020-07-01T01:18:23Z",
        "updatedAt" : "2020-07-01T01:18:23Z",
        "lastEditedBy" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "tags" : [
        ]
      }
    ],
    "commit" : "3fa76cfaf7aaca5c5d2ca8ad2743bdefae208b61",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +2185,2189 @@   */\n  private def visitSparkDataType(ctx: DataTypeContext): DataType = {\n    HiveVoidType.replaceVoidType(HiveStringType.replaceCharType(typedVisit(ctx)))\n  }\n"
  },
  {
    "id" : "de7b5ad2-8dee-40a8-b4b0-c571156eca71",
    "prId" : 28875,
    "prUrl" : "https://github.com/apache/spark/pull/28875#pullrequestreview-438913830",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cd5e17d6-b2c7-4b6e-98a3-62662c3737e2",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "we can still write `.children.isEmpty`, then we don't need to change `v2Commands.scala`",
        "createdAt" : "2020-06-23T04:17:12Z",
        "updatedAt" : "2020-06-28T03:03:06Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "defe3825-dbf7-4c6a-b886-e0494753aeba",
        "parentId" : "cd5e17d6-b2c7-4b6e-98a3-62662c3737e2",
        "authorId" : "8afcf008-6988-44a5-92fa-2bef54bb4058",
        "body" : "I don't think so, because the `children` of `InsertAction` and `UpdateAction` actually include `condition` and `assignments`. There may be cases where there're `assignments` and `condition` being ignored but `children` is nonEmpty.",
        "createdAt" : "2020-06-23T06:59:41Z",
        "updatedAt" : "2020-06-28T03:03:06Z",
        "lastEditedBy" : "8afcf008-6988-44a5-92fa-2bef54bb4058",
        "tags" : [
        ]
      },
      {
        "id" : "90afe381-e5a5-4448-80b8-f649a4c996b5",
        "parentId" : "cd5e17d6-b2c7-4b6e-98a3-62662c3737e2",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "then this is an existing bug?",
        "createdAt" : "2020-06-23T11:38:00Z",
        "updatedAt" : "2020-06-28T03:03:06Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "dc28e587-cf22-4380-8717-9fa90f21d925",
        "parentId" : "cd5e17d6-b2c7-4b6e-98a3-62662c3737e2",
        "authorId" : "8afcf008-6988-44a5-92fa-2bef54bb4058",
        "body" : "Yes, it was a bug.",
        "createdAt" : "2020-06-28T03:08:56Z",
        "updatedAt" : "2020-06-28T03:08:57Z",
        "lastEditedBy" : "8afcf008-6988-44a5-92fa-2bef54bb4058",
        "tags" : [
        ]
      },
      {
        "id" : "bb94071b-cc89-4710-9928-7e110ee02135",
        "parentId" : "cd5e17d6-b2c7-4b6e-98a3-62662c3737e2",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "can you send a new PR against branch 3.0 to fix this bug?",
        "createdAt" : "2020-06-29T02:24:04Z",
        "updatedAt" : "2020-06-29T02:26:19Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "819d5157-0f39-478c-bea1-3572751f5c72",
        "parentId" : "cd5e17d6-b2c7-4b6e-98a3-62662c3737e2",
        "authorId" : "8afcf008-6988-44a5-92fa-2bef54bb4058",
        "body" : "Submitted a pr at https://github.com/apache/spark/pull/28943.",
        "createdAt" : "2020-06-29T06:22:13Z",
        "updatedAt" : "2020-06-29T06:22:13Z",
        "lastEditedBy" : "8afcf008-6988-44a5-92fa-2bef54bb4058",
        "tags" : [
        ]
      }
    ],
    "commit" : "d5edef3c2b950440614fc5c9ee1e770bcd0b9884",
    "line" : 40,
    "diffHunk" : "@@ -1,1 +460,464 @@    // children being empty means that the condition is not set\n    val matchedActionSize = matchedActions.length\n    if (matchedActionSize >= 2 && !matchedActions.init.forall(_.condition.nonEmpty)) {\n      throw new ParseException(\"When there are more than one MATCHED clauses in a MERGE \" +\n          \"statement, only the last MATCHED clause can omit the condition.\", ctx)"
  },
  {
    "id" : "16b92101-c46a-48b3-a83b-f48cbf3b7a30",
    "prId" : 28833,
    "prUrl" : "https://github.com/apache/spark/pull/28833#pullrequestreview-430519164",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7be89ab7-72e0-4f8b-bace-039fd7454ece",
        "parentId" : null,
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "If change here and `NullType`, spark will also support `void` type for table. Is it needed ?",
        "createdAt" : "2020-06-15T09:42:52Z",
        "updatedAt" : "2020-07-08T00:27:04Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      },
      {
        "id" : "8672848c-d93e-439f-9610-59f2600391ab",
        "parentId" : "7be89ab7-72e0-4f8b-bace-039fd7454ece",
        "authorId" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "body" : "Just re-submit this PR since I don't know how to reopen. It's based on the latest discussions in #17953. Don't worry, we can close this if the community thinks the changes worthless to do.",
        "createdAt" : "2020-06-15T10:32:49Z",
        "updatedAt" : "2020-07-08T00:27:04Z",
        "lastEditedBy" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "tags" : [
        ]
      }
    ],
    "commit" : "9ad57d17bac47ea0f801004ec0aba9197e631bc7",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +2212,2216 @@      case (\"decimal\" | \"dec\" | \"numeric\", precision :: scale :: Nil) =>\n        DecimalType(precision.getText.toInt, scale.getText.toInt)\n      case (\"void\", Nil) => NullType\n      case (\"interval\", Nil) => CalendarIntervalType\n      case (dt, params) =>"
  },
  {
    "id" : "67616d52-c749-4379-a636-9012fcdde591",
    "prId" : 28501,
    "prUrl" : "https://github.com/apache/spark/pull/28501#pullrequestreview-412353658",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "309160e6-c7db-4a1f-989d-b4dfdb2970c8",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "does global aggregate still work? e.g. `UnresolvedHaving(Project(agg_func ...))`",
        "createdAt" : "2020-05-14T13:18:23Z",
        "updatedAt" : "2020-05-15T13:39:06Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "137c0b25-d60d-4c8c-994c-377f72df6663",
        "parentId" : "309160e6-c7db-4a1f-989d-b4dfdb2970c8",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "Yes, it still works, the `UnresolvedHaving` will be changed to Filter in rule `ResolveAggregateFunction`.",
        "createdAt" : "2020-05-15T05:18:54Z",
        "updatedAt" : "2020-05-15T13:39:07Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "1de0c759a90c7d868484b448289bef9c3865ab5d",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +630,634 @@      case e => Cast(e, BooleanType)\n    }\n    UnresolvedHaving(predicate, plan)\n  }\n"
  },
  {
    "id" : "320d33b1-5994-4746-8ad9-43cd0f9a6ebd",
    "prId" : 28294,
    "prUrl" : "https://github.com/apache/spark/pull/28294#pullrequestreview-400474442",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5c408ae4-c9aa-4bdd-9468-ddf8370c9616",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "what if we also create having here? This is for global aggregate, right?",
        "createdAt" : "2020-04-24T08:35:24Z",
        "updatedAt" : "2020-04-27T12:56:29Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "ea1ce4d5-cb92-42e4-9277-bdace049f4c2",
        "parentId" : "5c408ae4-c9aa-4bdd-9468-ddf8370c9616",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "This is also for GROUPING SET.",
        "createdAt" : "2020-04-26T08:19:24Z",
        "updatedAt" : "2020-04-27T12:56:29Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "18d857fa993e2acd7abcb837700e6bc4f3511dd5",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +634,638 @@        AggregateWithHaving(predicate, aggregate)\n      case _ =>\n        Filter(predicate, plan)\n    }\n  }"
  },
  {
    "id" : "0009954a-d55c-4c0f-9127-6969e6f6e099",
    "prId" : 28026,
    "prUrl" : "https://github.com/apache/spark/pull/28026#pullrequestreview-384301498",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b0a3a033-e4f8-4bff-88e3-68e437b71b85",
        "parentId" : null,
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "@HeartSaVioR, this is where `PARTITION BY` is validated. It can be either column-referencing expressions, or column definitions but not a mix of the two.",
        "createdAt" : "2020-03-30T23:16:46Z",
        "updatedAt" : "2020-11-24T17:03:15Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      },
      {
        "id" : "e11e2b14-a190-4fdc-99dd-22b73c174edc",
        "parentId" : "b0a3a033-e4f8-4bff-88e3-68e437b71b85",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "I guess @cloud-fan commented regarding this.",
        "createdAt" : "2020-03-30T23:25:02Z",
        "updatedAt" : "2020-11-24T17:03:15Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "25ec746753f29acd5e248d03db48211a3876a7c1",
    "line" : 263,
    "diffHunk" : "@@ -1,1 +2974,2978 @@          s\"\"\"PARTITION BY: Cannot mix partition expressions and partition columns:\n             |Expressions: $references\n             |Columns: $columns\"\"\".stripMargin, ctx)\n\n      }"
  },
  {
    "id" : "451d8a67-f6bb-4a4d-9b93-684daacdd76f",
    "prId" : 28026,
    "prUrl" : "https://github.com/apache/spark/pull/28026#pullrequestreview-521200321",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "78415ab5-2052-401c-a41f-e6f7b4a269ba",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "nit: we need to update `USING table_provider` to `[USING table_provider]`",
        "createdAt" : "2020-11-01T07:45:03Z",
        "updatedAt" : "2020-11-24T17:03:15Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "25ec746753f29acd5e248d03db48211a3876a7c1",
    "line" : 302,
    "diffHunk" : "@@ -1,1 +3012,3016 @@   *     col_name, transform(col_name), transform(constant, col_name), ... |\n   *     col_name data_type [NOT NULL] [COMMENT col_comment], ...\n   * }}}\n   */\n  override def visitCreateTable(ctx: CreateTableContext): LogicalPlan = withOrigin(ctx) {"
  },
  {
    "id" : "b4c50660-5890-4496-926b-be2420689cde",
    "prId" : 28026,
    "prUrl" : "https://github.com/apache/spark/pull/28026#pullrequestreview-521213824",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ce9f6bf6-d822-45c5-a7d5-2496a24dd6e3",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "We should add the temp table check here following https://github.com/apache/spark/pull/28026/files#diff-77a9aad2da3dc60210a2c4d2f3165d5f1d0acd54ca4811072a053225170ed748L396\r\n```\r\n    if (temp) {\r\n      operationNotAllowed(\"CREATE TEMPORARY TABLE is not supported yet. \" +\r\n        \"Please use CREATE TEMPORARY VIEW as an alternative.\", ctx)\r\n    }\r\n```\r\n\r\nThe error message is better than https://github.com/apache/spark/pull/28026/files#diff-26fe6b511f4b8aedf5bdffae0d7c03aba69463c09fa4f881d77520125796e0faR3029\r\n\r\nSince we are merging the syntax, we should pick the better error message from the hive syntax side.",
        "createdAt" : "2020-11-01T11:08:10Z",
        "updatedAt" : "2020-11-24T17:03:15Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "25ec746753f29acd5e248d03db48211a3876a7c1",
    "line" : 322,
    "diffHunk" : "@@ -1,1 +3024,3028 @@    if (provider.isDefined && serdeInfo.isDefined) {\n      operationNotAllowed(s\"CREATE TABLE ... USING ... ${serdeInfo.get.describe}\", ctx)\n    }\n\n    if (temp) {"
  },
  {
    "id" : "ca83e65d-b7a4-445e-95de-b6a1df3f311b",
    "prId" : 28026,
    "prUrl" : "https://github.com/apache/spark/pull/28026#pullrequestreview-521214389",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1946dad5-1172-43b6-804b-dd9d22b58b70",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "`schema` is only accessed in this branch. It's better to define the variable only in the branch that needs it.\r\n\r\nBTW we can follow the previous code style which has comments and looks simpler: https://github.com/apache/spark/pull/28026/files#diff-77a9aad2da3dc60210a2c4d2f3165d5f1d0acd54ca4811072a053225170ed748L421\r\n```\r\nval columns = Option(ctx.colTypeList()).map(visitColTypeList).getOrElse(Nil)\r\n...\r\nOption(ctx.query).map(plan) match {\r\n  ...\r\n  case _ =>\r\n    // Note: `PARTITION BY col type` requires partition columns to be distinct from the schema, so we need\r\n    // to include the partition columns here explicitly\r\n    val schema = StructType(columns ++ partCols)\r\n}\r\n```",
        "createdAt" : "2020-11-01T11:16:42Z",
        "updatedAt" : "2020-11-24T17:03:15Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "25ec746753f29acd5e248d03db48211a3876a7c1",
    "line" : 356,
    "diffHunk" : "@@ -1,1 +3051,3055 @@          writeOptions = Map.empty, serdeInfo, external = external, ifNotExists = ifNotExists)\n\n      case _ =>\n        // Note: table schema includes both the table columns list and the partition columns\n        // with data type."
  },
  {
    "id" : "e8b74380-b2f9-4c54-9ac0-155a7c4449dd",
    "prId" : 28026,
    "prUrl" : "https://github.com/apache/spark/pull/28026#pullrequestreview-521313207",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9181450d-4887-42d2-b1e6-3e3a6eaf0af3",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "shall we reconsider it? How can we re-create an external table?",
        "createdAt" : "2020-11-01T11:39:31Z",
        "updatedAt" : "2020-11-24T17:03:15Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "8f39cbfb-761e-42cd-b373-b9ef405b1c30",
        "parentId" : "9181450d-4887-42d2-b1e6-3e3a6eaf0af3",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "nvm, let's forbid it for now. `REPLACE EXTERNAL TABLE` looks ambiguous. It's not clear if the existing table is EXTERNAL, or we want to create an EXTERNAL TABLE to replace the existing one.",
        "createdAt" : "2020-11-02T03:30:59Z",
        "updatedAt" : "2020-11-24T17:03:15Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "25ec746753f29acd5e248d03db48211a3876a7c1",
    "line" : 403,
    "diffHunk" : "@@ -1,1 +3095,3099 @@    }\n\n    if (external) {\n      operationNotAllowed(\"REPLACE EXTERNAL TABLE ...\", ctx)\n    }"
  },
  {
    "id" : "f87c3b75-4d45-4fd8-9360-28aebb8d3cff",
    "prId" : 28026,
    "prUrl" : "https://github.com/apache/spark/pull/28026#pullrequestreview-521216143",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c20801a7-a424-4b83-9d24-700ba5e823c6",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "nit: update method doc `USING table_provider` to `[USING table_provider]`",
        "createdAt" : "2020-11-01T11:41:02Z",
        "updatedAt" : "2020-11-24T17:03:15Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "25ec746753f29acd5e248d03db48211a3876a7c1",
    "line" : 393,
    "diffHunk" : "@@ -1,1 +3086,3090 @@   * }}}\n   */\n  override def visitReplaceTable(ctx: ReplaceTableContext): LogicalPlan = withOrigin(ctx) {\n    val (table, temp, ifNotExists, external) = visitReplaceTableHeader(ctx.replaceTableHeader)\n    val orCreate = ctx.replaceTableHeader().CREATE() != null"
  },
  {
    "id" : "01705a2c-07ba-4b51-bab3-251b0d426541",
    "prId" : 28026,
    "prUrl" : "https://github.com/apache/spark/pull/28026#pullrequestreview-521312090",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "66b7c4b7-dc79-4c46-89e0-0140f104d568",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "let's follow the existing error message in the hive syntax which looks better:\r\n```\r\nval errorMessage = \"Create Partitioned Table As Select cannot specify data type for \" +\r\n  \"the partition columns of the target table.\"\r\noperationNotAllowed(errorMessage, ctx)\r\n```\r\nSee https://github.com/apache/spark/pull/28026/files#diff-77a9aad2da3dc60210a2c4d2f3165d5f1d0acd54ca4811072a053225170ed748L481",
        "createdAt" : "2020-11-02T03:25:59Z",
        "updatedAt" : "2020-11-24T17:03:15Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "25ec746753f29acd5e248d03db48211a3876a7c1",
    "line" : 340,
    "diffHunk" : "@@ -1,1 +3041,3045 @@\n      case Some(_) if partCols.nonEmpty =>\n        // non-reference partition columns are not allowed because schema can't be specified\n        operationNotAllowed(\n          \"Partition column types may not be specified in Create Table As Select (CTAS)\","
  },
  {
    "id" : "2588fb7d-750e-456a-9a8f-1858607e714d",
    "prId" : 28026,
    "prUrl" : "https://github.com/apache/spark/pull/28026#pullrequestreview-521313486",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "637b7b69-bece-49e2-97ed-0f0af989536a",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "`CREATE` -> `REPLACE`",
        "createdAt" : "2020-11-02T03:32:11Z",
        "updatedAt" : "2020-11-24T17:03:15Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "25ec746753f29acd5e248d03db48211a3876a7c1",
    "line" : 421,
    "diffHunk" : "@@ -1,1 +3109,3113 @@\n    if (provider.isDefined && serdeInfo.isDefined) {\n      operationNotAllowed(s\"CREATE TABLE ... USING ... ${serdeInfo.get.describe}\", ctx)\n    }\n"
  },
  {
    "id" : "7e3d8fd5-f818-4f8f-92c2-0488080b70be",
    "prId" : 27861,
    "prUrl" : "https://github.com/apache/spark/pull/27861#pullrequestreview-372533736",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2f748c55-755e-493c-b257-e2b49adaf630",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "From the ANSI SQL spec: `If WD has no window ordering clause, then the window ordering is implementation-dependent, and all rows are peers.`\r\n\r\nI don't think this is a bug fix but rather a new feature. We need to justify it: what's the behavior of other popular SQL systems like presto, snowflake, redshift, etc.? And what's the benefit for end users?",
        "createdAt" : "2020-03-11T07:48:17Z",
        "updatedAt" : "2020-03-22T05:39:39Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "769aa0fd-7ff6-415c-bb8f-349bd47409b6",
        "parentId" : "2f748c55-755e-493c-b257-e2b49adaf630",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> From the ANSI SQL spec: `If WD has no window ordering clause, then the window ordering is implementation-dependent, and all rows are peers.`\r\n> \r\n> I don't think this is a bug fix but rather a new feature. We need to justify it: what's the behavior of other popular SQL systems like presto, snowflake, redshift, etc.? And what's the benefit for end users?\r\n\r\nI will check other SQL system later.\r\nIn our production, one benefit is we can migrate hive sql to spark sql  smother and don't need to rewrite sql one by one",
        "createdAt" : "2020-03-11T07:52:23Z",
        "updatedAt" : "2020-03-22T05:39:39Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "d369cbc61811f4334f511fcda9e191a89c040e3c",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +1699,1703 @@      ctx.sortItem.asScala.map(visitSortItem)\n    } else {\n      // Same default behaviors like hive, when order spec is null\n      // set partition spec expression as order spec\n      ctx.partition.asScala.map { expr =>"
  },
  {
    "id" : "4489706a-3dd8-4f3c-81c9-cac5b34c7f98",
    "prId" : 27861,
    "prUrl" : "https://github.com/apache/spark/pull/27861#pullrequestreview-398783744",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "77cb5bae-7721-4b47-a028-cffa55e13229",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Wait .. why do we set the ordering column as partition column? We should just leave it unspecified so only (non-window) aggregation functions work together with unbounded windows so it doesn't get affected by the order. This is what Scala API does.",
        "createdAt" : "2020-04-23T02:10:19Z",
        "updatedAt" : "2020-04-23T02:10:19Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "96719da4-78c9-4c9f-9597-557c5fee8b62",
        "parentId" : "77cb5bae-7721-4b47-a028-cffa55e13229",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> Wait .. why do we set the ordering column as partition column? We should just leave it unspecified so only (non-window) aggregation functions work together with unbounded windows so it doesn't get affected by the order. This is what Scala API does.\r\n\r\nemmmmï¼Œ hive doing like this...for me, when user not set order by clause, means he don't care about result order.  For Range DataFrame we can't support this.",
        "createdAt" : "2020-04-23T02:16:30Z",
        "updatedAt" : "2020-04-23T02:16:30Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "896ed6b7-966f-4484-a1a6-f20b83ede6ce",
        "parentId" : "77cb5bae-7721-4b47-a028-cffa55e13229",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "But the results will be useless. When can it be useful if the order is indeterministic for the functions dependent on the order .. ?",
        "createdAt" : "2020-04-23T02:20:45Z",
        "updatedAt" : "2020-04-23T02:20:45Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "7dbb8298-eb5e-4f3d-807d-143f4c9386be",
        "parentId" : "77cb5bae-7721-4b47-a028-cffa55e13229",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> But the results will be useless. When can it be useful if the order is indeterministic for the functions dependent on the order .. ?\r\n\r\nIn postgre sql , if we don't specify order column, the result is according to partition column 's default sort order.\r\n```\r\nangerszhu=# explain analyze verbose select id, num, lead(id) over (partition by num) from s4;\r\n                                                    QUERY PLAN                                                     \r\n-------------------------------------------------------------------------------------------------------------------\r\n WindowAgg  (cost=158.51..198.06 rows=2260 width=12) (actual time=0.107..0.122 rows=6 loops=1)\r\n   Output: id, num, lead(id) OVER (?)\r\n   ->  Sort  (cost=158.51..164.16 rows=2260 width=8) (actual time=0.079..0.081 rows=6 loops=1)\r\n         Output: num, id\r\n         Sort Key: s4.num\r\n         Sort Method: quicksort  Memory: 25kB\r\n         ->  Seq Scan on public.s4  (cost=0.00..32.60 rows=2260 width=8) (actual time=0.057..0.061 rows=6 loops=1)\r\n               Output: num, id\r\n Planning Time: 0.114 ms\r\n Execution Time: 0.214 ms\r\n\r\nangerszhu=# explain analyze verbose select id, num, lead(id) over (partition by num order by id) from s4;\r\n                                                    QUERY PLAN                                                     \r\n-------------------------------------------------------------------------------------------------------------------\r\n WindowAgg  (cost=158.51..203.71 rows=2260 width=12) (actual time=0.976..1.017 rows=6 loops=1)\r\n   Output: id, num, lead(id) OVER (?)\r\n   ->  Sort  (cost=158.51..164.16 rows=2260 width=8) (actual time=0.067..0.070 rows=6 loops=1)\r\n         Output: id, num\r\n         Sort Key: s4.num, s4.id\r\n         Sort Method: quicksort  Memory: 25kB\r\n         ->  Seq Scan on public.s4  (cost=0.00..32.60 rows=2260 width=8) (actual time=0.042..0.045 rows=6 loops=1)\r\n               Output: id, num\r\n Planning Time: 0.155 ms\r\n Execution Time: 1.208 ms\r\n(10 rows)\r\n```",
        "createdAt" : "2020-04-23T02:36:52Z",
        "updatedAt" : "2020-04-23T02:36:52Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "7dfef1dd-3e41-4e9c-b581-bc3613f3c204",
        "parentId" : "77cb5bae-7721-4b47-a028-cffa55e13229",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I guess because PostgreSQL can keep the natural order. Spark can't keep the natural order. Is PostgreSQL result deterministic?",
        "createdAt" : "2020-04-23T02:47:34Z",
        "updatedAt" : "2020-04-23T02:47:34Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "3a9fee6f-f3de-47dd-a77c-4fb78551027d",
        "parentId" : "77cb5bae-7721-4b47-a028-cffa55e13229",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> deterministic\r\n\r\nFor same sql, result is deterministic.\r\n\r\nAnd we add partition column as order by column by default can keep result deterministic. \r\n\r\n\r\n\r\nI meet this problem when migration hive sql to spark sql.",
        "createdAt" : "2020-04-23T03:14:11Z",
        "updatedAt" : "2020-04-23T03:14:11Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "245de7b7-e5a1-4665-bb1a-eb4c35625805",
        "parentId" : "77cb5bae-7721-4b47-a028-cffa55e13229",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I think we should not fix it because Spark side at least the results will be non-deterministic. I doubt if this is good to add this support only because of compatibility with other DMBSes when the output is expected to be useless.\r\n\r\nMaybe disallowing it might be a better idea than finding another problem later caused by the different and indeterministic data.\r\n\r\nDo you maybe know other cases from other distributed DBMSs such as presto?",
        "createdAt" : "2020-04-23T04:01:17Z",
        "updatedAt" : "2020-04-23T04:01:17Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "80177ae0-5b63-4fc3-b640-35291eb3f996",
        "parentId" : "77cb5bae-7721-4b47-a028-cffa55e13229",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> I think we should not fix it because Spark side at least the results will be non-deterministic. I doubt if this is good to add this support only because of compatibility with other DMBSes when the output is expected to be useless.\r\n> \r\n> Maybe disallowing it might be a better idea than finding another problem later caused by the different and indeterministic data.\r\n> \r\n> Do you maybe know other cases from other distributed DBMSs such as presto?\r\n\r\nbut in my fix,  we add default order spec, the result will be deterministic.\r\nIn origin way, this kind sql can't run since it will get non-deterministic result and is rejected by  https://github.com/apache/spark/blob/a28ed86a387b286745b30cd4d90b3d558205a5a7/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala#L2773-L2776",
        "createdAt" : "2020-04-23T06:08:04Z",
        "updatedAt" : "2020-04-23T06:08:04Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "d369cbc61811f4334f511fcda9e191a89c040e3c",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +1702,1706 @@      // set partition spec expression as order spec\n      ctx.partition.asScala.map { expr =>\n        SortOrder(expression(expr), Ascending, Ascending.defaultNullOrdering, Set.empty)\n      }\n    }"
  },
  {
    "id" : "f4298a4c-abf5-426f-b45a-f6872ba60b32",
    "prId" : 27314,
    "prUrl" : "https://github.com/apache/spark/pull/27314#pullrequestreview-346739180",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9dca55ef-ced8-445e-9765-75210adcd69c",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "shouldn't we use `AlterViewSetProperties`?",
        "createdAt" : "2020-01-22T16:46:04Z",
        "updatedAt" : "2020-01-22T16:46:04Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "d3b3c278f92e3e519e94ce6ba0a72ad95bb914c9",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +3060,3064 @@    val cleanedTableProperties = cleanTableProperties(ctx, properties)\n    if (ctx.VIEW != null) {\n      AlterTableSetProperties(UnresolvedView(identifier), cleanedTableProperties)\n    } else {\n      AlterTableSetProperties(UnresolvedTable(identifier), cleanedTableProperties)"
  },
  {
    "id" : "7433a3ba-8064-41e6-9e20-7244a0000132",
    "prId" : 27314,
    "prUrl" : "https://github.com/apache/spark/pull/27314#pullrequestreview-346739380",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "49a3b1fa-73c0-4e51-933d-2af36d99d368",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "ditto",
        "createdAt" : "2020-01-22T16:46:17Z",
        "updatedAt" : "2020-01-22T16:46:17Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "d3b3c278f92e3e519e94ce6ba0a72ad95bb914c9",
    "line" : 31,
    "diffHunk" : "@@ -1,1 +3083,3087 @@    val ifExists = ctx.EXISTS != null\n    if (ctx.VIEW != null) {\n      AlterTableUnsetProperties(UnresolvedView(identifier), cleanedProperties, ifExists)\n    } else {\n      AlterTableUnsetProperties(UnresolvedTable(identifier), cleanedProperties, ifExists)"
  },
  {
    "id" : "ff1c7897-1df8-4676-99ca-1459d3dc0870",
    "prId" : 27243,
    "prUrl" : "https://github.com/apache/spark/pull/27243#pullrequestreview-345022368",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9e1d7045-34e9-4adb-af2a-29ca5fbf91cd",
        "parentId" : null,
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "I will be making a change to use `UnresolvedTableOrView` so that we can keep the existing error message like `Cannot alter a view with ALTER TABLE. Please use ALTER VIEW instead`.\r\n\r\nIf not, we will just see `v1 is a view not a table`. @cloud-fan, do you agree, or `v1 is a view not a table` is good enough without any 'hints'.",
        "createdAt" : "2020-01-17T03:41:08Z",
        "updatedAt" : "2020-01-20T07:17:36Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      },
      {
        "id" : "2596abca-2bff-4ca9-835b-588505939435",
        "parentId" : "9e1d7045-34e9-4adb-af2a-29ca5fbf91cd",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "`UnresolvedTableOrView` wouldn't work since it can resolve to a temp view. Do you have any suggestion if we need to provide the hint? (btw, the hint is used only for V1 commands)",
        "createdAt" : "2020-01-17T04:34:13Z",
        "updatedAt" : "2020-01-20T07:17:36Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      },
      {
        "id" : "28567c56-c46a-4fe6-9f54-37481c12d0ac",
        "parentId" : "9e1d7045-34e9-4adb-af2a-29ca5fbf91cd",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I think `v1 is a view not a table` is good enough for now. In the future we can add an error message parameter in `UnresolveTable/View`, so that each command can customize its error message.",
        "createdAt" : "2020-01-17T05:26:14Z",
        "updatedAt" : "2020-01-20T07:17:36Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "1a9038c3-779f-4587-9ce4-b37442bfff89",
        "parentId" : "9e1d7045-34e9-4adb-af2a-29ca5fbf91cd",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "or we can do it now, so that we don't need to update a lot of tests in this PR.",
        "createdAt" : "2020-01-17T05:28:06Z",
        "updatedAt" : "2020-01-20T07:17:36Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "50ef698c-330f-4c49-b25b-8effc96c2a46",
        "parentId" : "9e1d7045-34e9-4adb-af2a-29ca5fbf91cd",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "Thanks, I will do it in this PR.\r\n\r\nBut I wasn't sure if the error message parameter would work well because you can't guarantee the error message is mapped to the actual error condition (unless I misunderstood).\r\n\r\nHow about we introduce the \"actual\" resolved plan? For example, in `Analyzer.scala`\r\n```\r\ncase u @ UnresolvedTable(identifier) =>\r\n  lookupTableOrView(identifier).map {\r\n    case v: ResolvedView => u.copy(actual = v)\r\n    case table => table\r\n  }.getOrElse(u)\r\n```\r\nThen, in `CheckAnalysis.checkAnalysis`,\r\n```\r\ncase AlterTable(u @ UnresolvedTable(_, actual: ResolvedView), _) =>\r\n   u.failAnalysis(\"Cannot alter a view with ALTER TABLE. Please use ALTER VIEW instead\")\r\n```\r\n",
        "createdAt" : "2020-01-17T05:59:05Z",
        "updatedAt" : "2020-01-20T07:17:36Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      },
      {
        "id" : "59cd35a4-1e28-4546-9acf-abbe5e111722",
        "parentId" : "9e1d7045-34e9-4adb-af2a-29ca5fbf91cd",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "yea that also works.\r\n\r\nnit: we should skip relation lookup if the actual plan is already set. Or we can introduce a new node `UnresolvedTableWithViewExists` to explicitly indicate this case.",
        "createdAt" : "2020-01-17T06:29:17Z",
        "updatedAt" : "2020-01-20T07:17:36Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "727938a6-81fe-44bd-8c7c-57f576c3f9fb",
        "parentId" : "9e1d7045-34e9-4adb-af2a-29ca5fbf91cd",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "I went with your suggestion since it was simpler. Thanks!",
        "createdAt" : "2020-01-19T19:58:05Z",
        "updatedAt" : "2020-01-20T07:17:36Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "bb21f16203ff1917fae60bcb2e7244aac0eaa8db",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +2918,2922 @@  override def visitAddTableColumns(ctx: AddTableColumnsContext): LogicalPlan = withOrigin(ctx) {\n    AlterTableAddColumns(\n      UnresolvedTable(visitMultipartIdentifier(ctx.multipartIdentifier)),\n      ctx.columns.qualifiedColTypeWithPosition.asScala.map(typedVisit[QualifiedColType])\n    )"
  },
  {
    "id" : "5847d26a-46c8-4dd3-a0dd-9f015a35d861",
    "prId" : 27197,
    "prUrl" : "https://github.com/apache/spark/pull/27197#pullrequestreview-343000835",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7d321130-9a6d-4b53-9d83-246bf3e85fd8",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "can we update `cleanNamespaceProperties` to use filter as well?",
        "createdAt" : "2020-01-14T14:33:25Z",
        "updatedAt" : "2020-01-16T05:29:11Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "436748ed-c120-4984-acd6-74eedca6b567",
        "parentId" : "7d321130-9a6d-4b53-9d83-246bf3e85fd8",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "done",
        "createdAt" : "2020-01-15T05:55:41Z",
        "updatedAt" : "2020-01-16T05:29:11Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "91cc7743e2e4419225803be1cae87f7a93a332e6",
    "line" : 61,
    "diffHunk" : "@@ -1,1 +2672,2676 @@    import TableCatalog._\n    val legacyOn = conf.getConf(SQLConf.LEGACY_PROPERTY_NON_RESERVED)\n    properties.filter {\n      case (PROP_PROVIDER, _) if !legacyOn =>\n        throw new ParseException(s\"$PROP_PROVIDER is a reserved table property, please use\" +"
  },
  {
    "id" : "7a0fa951-f55e-4e89-9076-adcbb4d60661",
    "prId" : 27197,
    "prUrl" : "https://github.com/apache/spark/pull/27197#pullrequestreview-343686332",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bc4b124c-f036-42f5-9552-7e550322d3b7",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "we should revert it now.",
        "createdAt" : "2020-01-16T03:46:58Z",
        "updatedAt" : "2020-01-16T05:29:11Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "aa19b046-45f5-4143-8190-cf03188804a7",
        "parentId" : "bc4b124c-f036-42f5-9552-7e550322d3b7",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "I correct the order here,  or maybe we can keep this change?",
        "createdAt" : "2020-01-16T03:52:39Z",
        "updatedAt" : "2020-01-16T05:29:11Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "fccd58ff-0bf1-426c-9b5b-11d71469c088",
        "parentId" : "bc4b124c-f036-42f5-9552-7e550322d3b7",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "ah let's keep it.",
        "createdAt" : "2020-01-16T05:54:30Z",
        "updatedAt" : "2020-01-16T05:54:30Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "91cc7743e2e4419225803be1cae87f7a93a332e6",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +2400,2404 @@  /**\n   * Type to keep track of table clauses:\n   * (partitioning, bucketSpec, properties, options, location, comment).\n   */\n  type TableClauses = (Seq[Transform], Option[BucketSpec], Map[String, String],"
  },
  {
    "id" : "bfbd376e-10c2-41ba-99fb-68ba22248163",
    "prId" : 27110,
    "prUrl" : "https://github.com/apache/spark/pull/27110#pullrequestreview-340271375",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "20139d20-fc81-44e9-9c4b-38ee89328952",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Shall we add the documentation with `SET` and `DROP` syntax?",
        "createdAt" : "2020-01-09T03:23:41Z",
        "updatedAt" : "2020-01-09T10:55:37Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "1832e16ef79eb5d5172fe964884a484db46e5dfa",
    "line" : 55,
    "diffHunk" : "@@ -1,1 +2903,2907 @@      position = Option(ctx.colPosition).map(typedVisit[ColumnPosition]))\n  }\n\n  /**\n   * Parse a [[AlterTableAlterColumnStatement]] command to change column nullability."
  },
  {
    "id" : "21759e2a-f78e-444d-a2de-9dfad155b701",
    "prId" : 26860,
    "prUrl" : "https://github.com/apache/spark/pull/26860#pullrequestreview-331157212",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d7cee033-58cb-4012-a514-8590267708fb",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "We cannot avoid this case in the antlr side (SqlBase.g4)?",
        "createdAt" : "2019-12-12T08:06:50Z",
        "updatedAt" : "2019-12-12T11:35:21Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "633be250-1b89-45a1-909e-a6f687d2c58d",
        "parentId" : "d7cee033-58cb-4012-a514-8590267708fb",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "no, the antlr side just mach a string literal.",
        "createdAt" : "2019-12-12T08:44:37Z",
        "updatedAt" : "2019-12-12T11:35:21Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "bdb49524-d24d-4e9a-956b-041feeceb8bc",
        "parentId" : "d7cee033-58cb-4012-a514-8590267708fb",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "You mean like this ?\r\n```\r\nESCAPE_STRING\r\n    : '\\'' ( ~('\\''|'\\\\') | ('\\\\' .) | ~('%' | '_') )* '\\''\r\n    | '\\'' ( ~('\\''|'\\\\') | ('\\\\' .) | ~('%' | '_') )* '\\''\r\n    ;\r\n\r\nNOT? kind=LIKE pattern=valueExpression (ESCAPE escapeChar=ESCAPE_STRING)?\r\n```",
        "createdAt" : "2019-12-12T08:49:43Z",
        "updatedAt" : "2019-12-12T11:35:21Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      },
      {
        "id" : "24a40ff8-41be-4c1d-8d43-d772f95c2048",
        "parentId" : "d7cee033-58cb-4012-a514-8590267708fb",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "seems much easier to do at scala side...",
        "createdAt" : "2019-12-12T08:57:15Z",
        "updatedAt" : "2019-12-12T11:35:21Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "928963ae-88d9-4ae7-907d-c79fa4b820d4",
        "parentId" : "d7cee033-58cb-4012-a514-8590267708fb",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "with better error message.",
        "createdAt" : "2019-12-12T08:57:33Z",
        "updatedAt" : "2019-12-12T11:35:21Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "1721838f-ff08-4cee-8910-155dc833067d",
        "parentId" : "d7cee033-58cb-4012-a514-8590267708fb",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "Yes, I do agree.",
        "createdAt" : "2019-12-12T08:59:15Z",
        "updatedAt" : "2019-12-12T11:35:21Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      },
      {
        "id" : "b34d4a4a-e940-493d-8988-d31e024413be",
        "parentId" : "d7cee033-58cb-4012-a514-8590267708fb",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "ok.",
        "createdAt" : "2019-12-12T11:14:41Z",
        "updatedAt" : "2019-12-12T11:35:21Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "3def8ff1bd5dbc5f7ef281ee675e21c662b65c92",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +1397,1401 @@            throw new ParseException(\"Invalid escape string.\" +\n              \"Escape string can not be '%', '_'.\", ctx)\n        }\n        invertIfNotDefined(Like(e, expression(ctx.pattern), escapeChar))\n      case SqlBaseParser.RLIKE =>"
  },
  {
    "id" : "10b5780f-4ed0-4e10-8ed8-ea4f12c566c5",
    "prId" : 26847,
    "prUrl" : "https://github.com/apache/spark/pull/26847#pullrequestreview-335194586",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f46487c8-2732-4820-b560-fa882ad4259a",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "shall we distinguish between null and empty string? maybe the `comment` should be `Option[String]`",
        "createdAt" : "2019-12-20T06:36:40Z",
        "updatedAt" : "2020-01-03T03:50:51Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "dbc33564-8e05-408c-b9c8-d589fdc4d976",
        "parentId" : "f46487c8-2732-4820-b560-fa882ad4259a",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "null to delete the key? but reserved ones are tenured",
        "createdAt" : "2019-12-20T06:39:35Z",
        "updatedAt" : "2020-01-03T03:50:51Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "535a3a5d-c958-4e26-89b4-6ba8834ceafb",
        "parentId" : "f46487c8-2732-4820-b560-fa882ad4259a",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "> null to delete the key\r\n\r\nThat's my expectation. What's the behavior of pgsql and presto? I feel it's a reasonable request to remove a comment.",
        "createdAt" : "2019-12-20T09:16:01Z",
        "updatedAt" : "2020-01-03T03:50:51Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "99032ca6-1c5b-4b54-bc1f-2daea8c4df92",
        "parentId" : "f46487c8-2732-4820-b560-fa882ad4259a",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "I have checked the Postgres source code `NULL` and empty string are the same, \r\n```c\r\n/*\r\n * CreateComments --\r\n *\r\n * Create a comment for the specified object descriptor.  Inserts a new\r\n * pg_description tuple, or replaces an existing one with the same key.\r\n *\r\n * If the comment given is null or an empty string, instead delete any\r\n * existing comment for the specified key.\r\n */\r\n```\r\n\r\n",
        "createdAt" : "2019-12-20T10:11:10Z",
        "updatedAt" : "2020-01-03T03:50:51Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "afdbf06d-570c-42fd-ad0e-402ce7b3b944",
        "parentId" : "f46487c8-2732-4820-b560-fa882ad4259a",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "but null does mean to delete the comment, right? We don't need `Option[String]` because empty string also means to delete the comment.",
        "createdAt" : "2019-12-20T10:43:09Z",
        "updatedAt" : "2020-01-03T03:50:51Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "0c95f497-f10d-4573-8ace-15705ff8ac55",
        "parentId" : "f46487c8-2732-4820-b560-fa882ad4259a",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "yes",
        "createdAt" : "2019-12-20T10:54:06Z",
        "updatedAt" : "2020-01-03T03:50:51Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "377e8467-718b-4fcb-8b22-f3aa843f4682",
        "parentId" : "f46487c8-2732-4820-b560-fa882ad4259a",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "for anyone may concern about this, please refer to https://github.com/postgres/postgres/blob/master/src/backend/commands/comment.c#L133",
        "createdAt" : "2019-12-20T11:00:21Z",
        "updatedAt" : "2020-01-03T03:50:51Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "d20b1b2a2beb7cd395b92b4330724bc2c1a2d6bd",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +3433,3437 @@  override def visitCommentNamespace(ctx: CommentNamespaceContext): LogicalPlan = withOrigin(ctx) {\n    val comment = ctx.commennt.getType match {\n      case SqlBaseParser.NULL => \"\"\n      case _ => string(ctx.STRING)\n    }"
  },
  {
    "id" : "215fc953-87a5-46bb-9186-b7e6f5ebfb1d",
    "prId" : 26817,
    "prUrl" : "https://github.com/apache/spark/pull/26817#pullrequestreview-330320964",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2a3b3829-b9c9-4db9-99ed-acfe11675218",
        "parentId" : null,
        "authorId" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "body" : "`.map(visitColPosition)`?",
        "createdAt" : "2019-12-10T21:32:38Z",
        "updatedAt" : "2019-12-13T07:48:00Z",
        "lastEditedBy" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "tags" : [
        ]
      },
      {
        "id" : "5083d900-1d64-4d42-836a-ca19c0839904",
        "parentId" : "2a3b3829-b9c9-4db9-99ed-acfe11675218",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "They are the same. `typedVisit` would call `visitColPosition` under the hood (via looking at the context). We use `typedVisit` a lot in this file.",
        "createdAt" : "2019-12-11T06:29:47Z",
        "updatedAt" : "2019-12-13T07:48:00Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "c01f565d048f9f84aa08616113941e5f072158c4",
    "line" : 33,
    "diffHunk" : "@@ -1,1 +2821,2825 @@      typedVisit[DataType](ctx.dataType),\n      Option(ctx.comment).map(string),\n      Option(ctx.colPosition).map(typedVisit[ColumnPosition]))\n  }\n"
  },
  {
    "id" : "09c4b732-1bd2-4795-befc-7dafbf59bea0",
    "prId" : 26649,
    "prUrl" : "https://github.com/apache/spark/pull/26649#pullrequestreview-322035342",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9254ad12-422e-4430-b622-c6929376fe44",
        "parentId" : null,
        "authorId" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "body" : "I can't access ```PersistedView/GlobalTempView/LocalTempView``` in this class. I use 1/2/3 and map them back to ```PersistedView/GlobalTempView/LocalTempView``` in ```ResolveSessionCatalog```",
        "createdAt" : "2019-11-24T04:20:59Z",
        "updatedAt" : "2019-11-25T19:59:20Z",
        "lastEditedBy" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "tags" : [
        ]
      },
      {
        "id" : "e832cacd-63e7-420d-99bd-7b6061dff750",
        "parentId" : "9254ad12-422e-4430-b622-c6929376fe44",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Can we move PersistedView etc.?",
        "createdAt" : "2019-11-24T05:05:28Z",
        "updatedAt" : "2019-11-25T19:59:20Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "866ecb05-453a-4fe2-88c5-288f974090c3",
        "parentId" : "9254ad12-422e-4430-b622-c6929376fe44",
        "authorId" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "body" : "Yes. We can move these to catalyst. What is a good place to put these? ```org.apache.spark.sql.catalyst.analysis.view.scala```? ```org.apache.spark.sql.catalyst.catalog.interface.scala```?",
        "createdAt" : "2019-11-24T21:53:28Z",
        "updatedAt" : "2019-11-25T19:59:20Z",
        "lastEditedBy" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "tags" : [
        ]
      },
      {
        "id" : "e6f267e7-317e-4c66-b6e8-dd5b646271ff",
        "parentId" : "9254ad12-422e-4430-b622-c6929376fe44",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "I think view.scala is better. cc @cloud-fan ",
        "createdAt" : "2019-11-24T23:13:08Z",
        "updatedAt" : "2019-11-25T19:59:20Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "c29f9ec2-99b7-46c6-ab9e-ccf6ed7c52b6",
        "parentId" : "9254ad12-422e-4430-b622-c6929376fe44",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "+1 for view.scala",
        "createdAt" : "2019-11-25T04:30:23Z",
        "updatedAt" : "2019-11-25T19:59:20Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "e878207877ef699ebd99aaa1cd1b490118f9ee90",
    "line" : 41,
    "diffHunk" : "@@ -1,1 +3265,3269 @@    } else {\n      LocalTempView\n    }\n    CreateViewStatement(\n      visitMultipartIdentifier(ctx.multipartIdentifier),"
  },
  {
    "id" : "99bb4017-60fe-4d4a-ae94-ca9f8215ce26",
    "prId" : 26537,
    "prUrl" : "https://github.com/apache/spark/pull/26537#pullrequestreview-327404758",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ce82c56d-94c1-41e0-9923-615135ade7c2",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I take a look at SQL standard, float and real are different\r\n```\r\nFLOAT specifies the data type approximate numeric, with binary precision equal to or greater than the\r\nvalue of the specified <precision>. The maximum value of <precision> is implementation-defined.\r\n<precision> shall not be greater than this value.\r\n\r\nREAL specifies the data type approximate numeric, with implementation-defined precision.\r\n```\r\n\r\nWe can say that now Spark supports real type, and its precision is the same as java float. But it needs to be proposed explicitly, e.g. send an email to dev list.",
        "createdAt" : "2019-11-15T07:21:39Z",
        "updatedAt" : "2019-12-10T13:05:48Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "54587f60-4d12-4604-be65-6bdb90a44fcf",
        "parentId" : "ce82c56d-94c1-41e0-9923-615135ade7c2",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "It seems numeric and decimal are not exactly the same...",
        "createdAt" : "2019-11-15T13:09:03Z",
        "updatedAt" : "2019-12-10T13:05:48Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "8bc3b148-9ee4-40f0-af19-1822cc8a13bd",
        "parentId" : "ce82c56d-94c1-41e0-9923-615135ade7c2",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "I want to gather more information about these before sending an email to discuss.\r\n\r\n1. postgresql \r\nhttps://www.postgresql.org/docs/9.3/datatype-numeric.html \r\n\r\nName | Storage Size | Description | Range\r\n-- | -- | -- | --\r\ndecimal | variable | user-specified precision, exact | up to 131072 digits before the decimal point; up to 16383 digits after the decimal point\r\nnumeric | variable | user-specified precision, exact | up to 131072 digits before the decimal point; up to 16383 digits after the decimal point\r\nreal | 4 bytes | variable-precision, inexact | 6 decimal digits precision\r\ndouble precision | 8 bytes | variable-precision, inexact | 15 decimal digits precision\r\n\r\n> The types decimal and numeric are equivalent. Both types are part of the SQL standard.",
        "createdAt" : "2019-11-15T15:13:33Z",
        "updatedAt" : "2019-12-10T13:05:48Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "a20a8afa-df73-48f6-a1a4-bf8c1fc62703",
        "parentId" : "ce82c56d-94c1-41e0-9923-615135ade7c2",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "2. mimer_sql_engine\r\nhttps://download.mimer.com/pub/developer/docs/html_100/Mimer_SQL_Engine_DocSet/Syntax_Rules4.html#wp1228955\r\n\r\nData Type | Abbrevi-ation | Description | Range\r\n-- | -- | -- | --\r\nDECIMAL(p,s) | DEC(p,s) | Exact numerical,precision p, scale s. | 1 <= p <= 450 <= s <= p\r\nNUMERIC(p,s) | N/A | Exact numerical, precision p, scale s.(Same asÂ DECIMAL). | 1 <= p <= 450 <= s <= p\r\nFLOAT(p) | N/A | Approximate numerical,mantissa precision p. | 1 <= p <= 45Zero or absolute value10-999 to 10+999\r\nREAL | N/A | Approximate numericalmantissa precision 7. | Zero or absolute value10-38Â to 10+38Corresponds to a single precision float.\r\nFLOAT | N/A | Approximate numericalmantissa precision 16. | Zero or absolute value10-308Â to 10+308Corresponds to a double precision float.\r\nDOUBLE PRECISION | N/A | Approximate numericalmantissa precision 16. | Zero or absolute value10-308Â to 10+308\r\n\r\n> Note: In Mimer SQL the NUMERIC data type is exactly equivalent to DECIMAL.\r\n",
        "createdAt" : "2019-11-15T15:21:25Z",
        "updatedAt" : "2019-12-10T13:05:48Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "d31ff8fb-6610-4139-83e4-01c303cc1594",
        "parentId" : "ce82c56d-94c1-41e0-9923-615135ade7c2",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "3. prestosql\r\nhttps://prestosql.io/docs/current/language/types.html#floating-point\r\n> REAL\r\n>> A real is a 32-bit inexact, variable-precision implementing the IEEE Standard 754 for Binary Floating-Point Arithmetic.\r\n> DECIMAL\r\n>> A fixed precision decimal number. Precision up to 38 digits is supported but performance is best up to 18 digits.\r\n\r\nNo documentation found about numeric type ",
        "createdAt" : "2019-11-15T15:38:07Z",
        "updatedAt" : "2019-12-10T13:05:48Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "61ed6ddd-d42f-4065-89a5-2923a2c6f584",
        "parentId" : "ce82c56d-94c1-41e0-9923-615135ade7c2",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "3. SQL Server\r\n\r\nhttps://docs.microsoft.com/en-us/openspecs/standards_support/MS-STDSUPLP/17a32be7-10b3-4025-bea4-133a66b4c689\r\n\r\n#### Decimal\r\n\r\n> SQL Server 2008 R2 and SQL Server 2012 vary as follows:\r\n>> Transact-SQL partially supports this data type. The xs:decimal type represents arbitrary precision\r\ndecimal numbers. Transact-SQL does not support variable precision decimals. Minimally conforming\r\nXML processors are required to support decimal numbers with a minimum of totalDigits=18. TransactSQL supports totalDigits=38, but limits the fractional digits to 10. All xs:decimal-instanced values are\r\nrepresented internally on the server by the SQL type numeric (38, 10).\r\n\r\n>> Values of this type need to comply with the format of the **SQL numeric type**. This type internally\r\nrepresents the support of numbers up to a total of 38 digits, with 10 of those digit positions reserved\r\nfor fractional precision.\r\n\r\n### Float\r\n> SQL Server 2008 R2 and SQL Server 2012 vary as follows:\r\n>> Transact-SQL partially supports this data type. Values of this type need to comply with the format of\r\nthe **SQL real type**\r\n",
        "createdAt" : "2019-11-15T16:25:05Z",
        "updatedAt" : "2019-12-10T13:05:48Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "bac51e8d-2bc3-4555-9441-a58f9653a951",
        "parentId" : "ce82c56d-94c1-41e0-9923-615135ade7c2",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "In this case it's not very useful to look at other databases. There are many stuff that is \"implementation-defined\".\r\n\r\nPersonally I'm fine to treat real as float in Spark, just need an official proposal.\r\n\r\nFor numeric/decimal, I don't have a strong preference.",
        "createdAt" : "2019-11-16T09:32:57Z",
        "updatedAt" : "2019-12-10T13:05:48Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "650789bb-780f-4a51-9b5c-199c89476e1d",
        "parentId" : "ce82c56d-94c1-41e0-9923-615135ade7c2",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "The below notes from SQL standard might have some information to help us whether or not to involve real or numeric\r\n> An SQL-implementation is permitted to regard certain <exact numeric type>s as equivalent, if they have the\r\nsame precision, scale, and radix, as permitted by the Syntax Rules of Subclause 6.1, â€œ<data type>â€. When two\r\nor more <exact numeric type>s are equivalent, the SQL-implementation chooses one of these equivalent <exact\r\nnumeric type>s as the normal form representing that equivalence class of <exact numeric type>s. The normal\r\nform determines the name of the exact numeric type in the numeric type descriptor.\r\nSimilarly, an SQL-implementation is permitted to regard certain <approximate numeric type>s as equivalent,\r\nas permitted by the Syntax Rules of Subclause 6.1, â€œ<data type>â€, in which case the SQL-implementation\r\nchooses a normal form to represent each equivalence class of <approximate numeric type> and the normal\r\nform determines the name of the approximate numeric type.",
        "createdAt" : "2019-11-16T14:08:54Z",
        "updatedAt" : "2019-12-10T13:05:48Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "25d92c0b-ff53-4164-906f-853db5cb88e2",
        "parentId" : "ce82c56d-94c1-41e0-9923-615135ade7c2",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "> 25) For the exact numeric types DECIMAL and NUMERIC:\r\na) The maximum value of precision is implementation-defined. precision shall not be greater than this value.\r\nb) The maximum value of scale is implementation-defined. scale shall not be greater than this maximum value.\r\n> 26) NUMERIC specifies the data type exact numeric, with the decimal precision and scale specified by the precision and scale.\r\n> 27) DECIMAL specifies the data type exact numeric, with the decimal scale specified by the scale and the implementation-defined decimal precision equal to or greater than the value of the specified precision.\r\n\r\nI don't know why 26th - NUMERIC and 27th - DECIMAL have different definitions, but IIUC with restraint of the 25th, they are even, both with `implementation-defined` precision and scale, the user-specified ones can not be greater than these.",
        "createdAt" : "2019-11-16T14:43:50Z",
        "updatedAt" : "2019-12-10T13:05:48Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "5ab3066c-243a-48f9-a571-d50da230617c",
        "parentId" : "ce82c56d-94c1-41e0-9923-615135ade7c2",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "So seems the key difference between DEICMAL and NUMERIC are:\r\n\r\n> implementation-defined decimal precision equal to or greater than the value of the specified precision.\r\n\r\nDecimal can have _at least_ specified precision (which means can have more) whereas numeric should have exactly specified precision.\r\n\r\nSpark's decimal satisfy both so I think NUMERIC as a synonym of DECIMAL makes sense.",
        "createdAt" : "2019-12-05T09:23:11Z",
        "updatedAt" : "2019-12-10T13:05:48Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "d5d1f8edd9b47ff41d2f50966da9897100b8a38c",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +2180,2184 @@      case (\"int\" | \"integer\", Nil) => IntegerType\n      case (\"bigint\" | \"long\", Nil) => LongType\n      case (\"float\" | \"real\", Nil) => FloatType\n      case (\"double\", Nil) => DoubleType\n      case (\"date\", Nil) => DateType"
  },
  {
    "id" : "78a32c30-7855-4209-a6f5-1f6b808ca4a4",
    "prId" : 26291,
    "prUrl" : "https://github.com/apache/spark/pull/26291#pullrequestreview-308911470",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5c2b1ebd-81c2-4c4f-baa2-cb13c2762983",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Out of curiosity, how many such typed literals are there left?",
        "createdAt" : "2019-10-30T00:37:54Z",
        "updatedAt" : "2019-10-30T00:37:55Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "1960b291519d9ebcdc4356811d7d283ffa3ca3c4",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +1800,1804 @@          val padding = if (value.length % 2 != 0) \"0\" else \"\"\n          Literal(DatatypeConverter.parseHexBinary(padding + value))\n        case \"INTEGER\" =>\n          val i = try {\n            value.toInt"
  },
  {
    "id" : "e9cc0926-8f77-43a4-8fb4-a1417685ccad",
    "prId" : 26182,
    "prUrl" : "https://github.com/apache/spark/pull/26182#pullrequestreview-308077959",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f9346862-551a-4645-8f36-9cb5af72c133",
        "parentId" : null,
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "You can simplify the code a bit if you want:\r\n```scala\r\n    val table = visitMultipartIdentifier(ctx.tableName)\r\n    val namespace = Option(ctx.multipartIdentifier).map(visitMultipartIdentifier)\r\n    if (namespace.isDefined && namespace.get.length > 1 && table.length > 1) {\r\n      throw new ParseException(\r\n        s\"If namespace is defined, table can not be multipart: ${table.mkString(\".\")}\", ctx)\r\n    }\r\n    ShowColumnsStatement(table, namespace)\r\n```",
        "createdAt" : "2019-10-27T16:09:16Z",
        "updatedAt" : "2019-10-30T18:05:23Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      },
      {
        "id" : "f2cc6090-fdbf-4bf5-9ca2-7ff993e6c154",
        "parentId" : "f9346862-551a-4645-8f36-9cb5af72c133",
        "authorId" : "51be1be0-7c72-4f32-a21f-91b1707ec363",
        "body" : "Elegant solution, thank you!!",
        "createdAt" : "2019-10-28T18:58:41Z",
        "updatedAt" : "2019-10-30T18:05:23Z",
        "lastEditedBy" : "51be1be0-7c72-4f32-a21f-91b1707ec363",
        "tags" : [
        ]
      }
    ],
    "commit" : "5365243ee4ff41ec244d975745b0fce272a65b81",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +2927,2931 @@    val namespace = Option(ctx.namespace).map(visitMultipartIdentifier)\n    ShowColumnsStatement(table, namespace)\n  }\n\n  /**"
  },
  {
    "id" : "e606d441-0ae3-4328-8d7e-32413aa06a09",
    "prId" : 26151,
    "prUrl" : "https://github.com/apache/spark/pull/26151#pullrequestreview-303652786",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fc875a3a-a2ca-4360-836e-ac90a354f7a3",
        "parentId" : null,
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "The error message is still inconsistent. `Invalid interval` vs `Cannot parse the DATE value`:\r\n```\r\nscala> spark.sql(\"select Interval '10000 secondss'\").show(false)\r\norg.apache.spark.sql.catalyst.parser.ParseException:\r\nInvalid interval: 10000 secondss(line 1, pos 7)\r\n```\r\n\r\n```\r\nscala> spark.sql(\"select date '10000 secondss'\").show(false)\r\norg.apache.spark.sql.catalyst.parser.ParseException:\r\nCannot parse the DATE value: 10000 secondss(line 1, pos 7)\r\n\r\n== SQL ==\r\nselect date '10000 secondss'\r\n-------^^^\r\n```",
        "createdAt" : "2019-10-18T03:49:18Z",
        "updatedAt" : "2019-10-18T15:29:39Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      }
    ],
    "commit" : "b27be9f6d8cfb4917ef15cc6b309b76aaec3cafa",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +1777,1781 @@              ex.setStackTrace(e.getStackTrace)\n              throw ex\n          }\n          Literal(interval, CalendarIntervalType)\n        case \"X\" =>"
  },
  {
    "id" : "8c89002b-6e66-4327-8d83-425b66133a2b",
    "prId" : 25367,
    "prUrl" : "https://github.com/apache/spark/pull/25367#pullrequestreview-395731244",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "038fc645-6656-4a59-8e50-1e54f5536272",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "It's very confusing that `DOW` returns different results from `DAYOFWEEK`. `DOW` should just be an abbreviation of `DAYOFWEEK`. See also https://docs.snowflake.com/en/sql-reference/functions-date-time.html#label-supported-date-time-parts\r\n\r\nDo we have a clear document about what these names mean? cc @HyukjinKwon @maropu @wangyum ",
        "createdAt" : "2020-04-17T05:58:10Z",
        "updatedAt" : "2020-04-17T05:58:10Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "0d3b2d6a-d5f2-4c51-a682-33237cdba2c3",
        "parentId" : "038fc645-6656-4a59-8e50-1e54f5536272",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Here we just follow PostgreSQL definition of `DOW`, see:\r\n```\r\ndow\r\nThe day of the week as Sunday (0) to Saturday (6)\r\n\r\nSELECT EXTRACT(DOW FROM TIMESTAMP '2001-02-16 20:38:40');\r\nResult: 5\r\n```\r\nDo you propose to break compatibility with PostgreSQL?",
        "createdAt" : "2020-04-17T06:56:30Z",
        "updatedAt" : "2020-04-17T06:56:30Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "82a54e69-bec5-4a13-91c1-944230e9cbd4",
        "parentId" : "038fc645-6656-4a59-8e50-1e54f5536272",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "We never promise pgsql compatibility and internal consistency is much more important for Spark.\r\n\r\nFor this case, I think the behavior of `DOW` from pgsql is correct and commonly used by other databases. We should fix our `DAYOFWEEK` instead.\r\n",
        "createdAt" : "2020-04-17T07:36:10Z",
        "updatedAt" : "2020-04-17T07:36:10Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "1f43ac62-1ef7-4881-9b06-41bb29562464",
        "parentId" : "038fc645-6656-4a59-8e50-1e54f5536272",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "> Do we have a clear document about what these names mean? cc @HyukjinKwon @maropu @wangyum\r\n\r\nCurrently, we just list up these names in `ExpressionDescription` of `DatePart`: https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/datetimeExpressions.scala#L2124\r\nIf [we add `ExpressionDescription` for `EXTRACT`](https://github.com/apache/spark/pull/21479#discussion_r409933251), the list will appear for `EXTRACT`, too, in [the Built-in Functions document ](https://spark.apache.org/docs/latest/api/sql/index.html) for Spark 3.0. If we need to describe more about these parameters, I think we need to update the `argument` section there.\r\n",
        "createdAt" : "2020-04-17T09:43:24Z",
        "updatedAt" : "2020-04-17T09:43:25Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "249185d7-cfc4-485e-99dc-5d91a8604494",
        "parentId" : "038fc645-6656-4a59-8e50-1e54f5536272",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "BTW, @cloud-fan . `DAYOFWEEK` is added two years ago. (https://github.com/apache/spark/pull/21479)\r\n> We should fix our DAYOFWEEK instead.\r\n\r\ncc @marmbrus and @gatorsmile since the request might mean a behavior change.",
        "createdAt" : "2020-04-17T19:51:02Z",
        "updatedAt" : "2020-04-17T19:51:27Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "ef3552beb2698f28fe4a68e2143c761380552e7a",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1410,1414 @@        DayOfWeek(expression(ctx.source))\n      case \"DOW\" =>\n        Subtract(DayOfWeek(expression(ctx.source)), Literal(1))\n      case \"ISODOW\" =>\n        Add(WeekDay(expression(ctx.source)), Literal(1))"
  },
  {
    "id" : "ea9a46e5-6ae6-42dd-bd66-76b2a8180743",
    "prId" : 25115,
    "prUrl" : "https://github.com/apache/spark/pull/25115#pullrequestreview-269468163",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "da99f8da-3bf8-49dc-911e-ee0a6c1224d0",
        "parentId" : null,
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "This needs to create a `DeleteFromStatement`. We use statement logical plans to separate logic in the parser from implementation details.",
        "createdAt" : "2019-07-29T16:49:51Z",
        "updatedAt" : "2019-08-14T01:57:20Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      },
      {
        "id" : "c8ed4414-9989-4da8-aea6-403034c798b8",
        "parentId" : "da99f8da-3bf8-49dc-911e-ee0a6c1224d0",
        "authorId" : "8afcf008-6988-44a5-92fa-2bef54bb4058",
        "body" : "Thanks. Has introduced `DeleteFromStatement` in the latest code as an intermediate object.",
        "createdAt" : "2019-08-01T08:29:10Z",
        "updatedAt" : "2019-08-14T01:57:20Z",
        "lastEditedBy" : "8afcf008-6988-44a5-92fa-2bef54bb4058",
        "tags" : [
        ]
      }
    ],
    "commit" : "bbf515666495cbf5f12731b3cdab4a23960f3d77",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +339,343 @@  }\n\n  override def visitDeleteFromTable(\n      ctx: DeleteFromTableContext): LogicalPlan = withOrigin(ctx) {\n"
  },
  {
    "id" : "6fbcf200-6074-45cf-99ac-ed6d7f0fa609",
    "prId" : 25074,
    "prUrl" : "https://github.com/apache/spark/pull/25074#pullrequestreview-364549290",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7b448147-b618-4143-902a-7030015b914f",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "isn't it simply `EqualNullSafe(e, Literal(true))`? Why do we create a bunch of new expressions?",
        "createdAt" : "2020-02-21T13:55:32Z",
        "updatedAt" : "2020-02-21T13:55:33Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "fa387245-5f1f-433c-a46d-b0522d34afc7",
        "parentId" : "7b448147-b618-4143-902a-7030015b914f",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "@cloud-fan It looks more reasonable. \r\nBut there exists a different the input expression must be of a boolean type. The input of `EqualNullSafe` is any `DataType`.\r\nDo I need to adjust? cc @dongjoon-hyun @maropu ",
        "createdAt" : "2020-02-21T14:54:43Z",
        "updatedAt" : "2020-02-21T15:01:39Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      },
      {
        "id" : "fc7a61ac-92c2-401a-b910-f2045a375580",
        "parentId" : "7b448147-b618-4143-902a-7030015b914f",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "`EqualNullSafe` requires the left type to be the same as right type, so `EqualNullSafe(e, Literal(true))` requires `e` to be boolean.\r\n\r\n`IsNull` is a problem so I think that change makes sense.",
        "createdAt" : "2020-02-21T15:04:01Z",
        "updatedAt" : "2020-02-21T15:04:02Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "45ce609b-d9b6-405e-be4f-343196733a06",
        "parentId" : "7b448147-b618-4143-902a-7030015b914f",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Ah, I missed that approach and the @cloud-fan one sounds more reasonable. If @dongjoon-hyun is not against the idea, can you make a PR for that? @beliefer ",
        "createdAt" : "2020-02-21T22:58:36Z",
        "updatedAt" : "2020-02-21T22:58:52Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "ccf7869f-bb70-4b62-b17f-5697e251e03a",
        "parentId" : "7b448147-b618-4143-902a-7030015b914f",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "@cloud-fan @maropu Let's wait @dongjoon-hyun , then I will make a PR.",
        "createdAt" : "2020-02-22T00:34:57Z",
        "updatedAt" : "2020-02-22T00:34:57Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      },
      {
        "id" : "c9f0acb8-08f1-4b4d-8240-3eff4203a344",
        "parentId" : "7b448147-b618-4143-902a-7030015b914f",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Sorry for being late. +1 for that approach.",
        "createdAt" : "2020-02-25T22:05:58Z",
        "updatedAt" : "2020-02-25T22:05:59Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "7383c7f0-fe20-432a-8e99-6a0f6f4cecf9",
        "parentId" : "7b448147-b618-4143-902a-7030015b914f",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Thanks for the check, @dongjoon-hyun ! Plz go ahead, @beliefer ",
        "createdAt" : "2020-02-25T23:15:34Z",
        "updatedAt" : "2020-02-25T23:15:35Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "3b9c4872-8768-47b6-ac24-3111d2fbfe1b",
        "parentId" : "7b448147-b618-4143-902a-7030015b914f",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "OK.",
        "createdAt" : "2020-02-26T00:33:16Z",
        "updatedAt" : "2020-02-26T00:33:16Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ae126e7aee616d30916d8223d07e5f37384b21f4",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +1246,1250 @@        IsNull(e)\n      case SqlBaseParser.TRUE => ctx.NOT match {\n        case null => IsTrue(e)\n        case _ => IsNotTrue(e)\n      }"
  },
  {
    "id" : "2e5c7d12-9453-4b6f-b456-331ebba36bd6",
    "prId" : 25028,
    "prUrl" : "https://github.com/apache/spark/pull/25028#pullrequestreview-308272027",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2ea9b60f-e040-4dc1-a4c9-3639fbb5673d",
        "parentId" : null,
        "authorId" : "3e45f06a-4916-48a9-b89a-8092d4eb0a30",
        "body" : "re discussion in https://github.com/apache/spark/pull/25028#issuecomment-547230458 and https://github.com/apache/spark/pull/25028#issuecomment-547246714 \r\n\r\nAlternative suggestion:\r\n```\r\n   val aliasFunc = (position: Int, e: Expression) => s\"gen_alias_${position}\"\r\n\r\n    val namedExpressions = expressions.zipWithIndex.map {\r\n      case (e: NamedExpression, _) => e\r\n      case (e: Expression, index) => UnresolvedAlias(e, Some(aliasFunc(index, _)))\r\n    }\r\n```",
        "createdAt" : "2019-10-29T04:29:17Z",
        "updatedAt" : "2019-10-29T04:29:26Z",
        "lastEditedBy" : "3e45f06a-4916-48a9-b89a-8092d4eb0a30",
        "tags" : [
        ]
      },
      {
        "id" : "5ae9529f-a6d6-4566-a061-8c0f376e29a6",
        "parentId" : "2ea9b60f-e040-4dc1-a4c9-3639fbb5673d",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "It is ok even to rename all expression with `aliasFunc`  in this place.\r\nAfter current busy job I will restart this pr and follow your nice suggestion.\r\n",
        "createdAt" : "2019-10-29T04:38:24Z",
        "updatedAt" : "2019-10-29T04:38:24Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "9e60de60689c9b82bef551c87076fb99ba70d8f4",
    "line" : 42,
    "diffHunk" : "@@ -1,1 +515,519 @@      case e: NamedExpression => e\n      case e: Expression => UnresolvedAlias(e)\n    }\n\n    def createProject() = if (namedExpressions.nonEmpty) {"
  },
  {
    "id" : "9bf4ae18-fc21-4ded-9960-e78d05fddfd8",
    "prId" : 24949,
    "prUrl" : "https://github.com/apache/spark/pull/24949#pullrequestreview-258298784",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "db1974b7-b739-44dc-962c-fc3f75a71eba",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Actually, this is the more important change than `adding test case`. Could you update the PR title?",
        "createdAt" : "2019-07-05T08:06:18Z",
        "updatedAt" : "2019-07-05T10:45:54Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "ec03adcf-037a-4348-be37-e86e05fb0c86",
        "parentId" : "db1974b7-b739-44dc-962c-fc3f75a71eba",
        "authorId" : "3d4870da-39a8-4406-b00d-131930d14cd8",
        "body" : "Thanks, I've updated it.",
        "createdAt" : "2019-07-05T09:06:38Z",
        "updatedAt" : "2019-07-05T10:45:54Z",
        "lastEditedBy" : "3d4870da-39a8-4406-b00d-131930d14cd8",
        "tags" : [
        ]
      }
    ],
    "commit" : "0c39fb2d449c91952cdeae759d4bbaaf2d35c1ee",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +132,136 @@    val duplicates = ctes.groupBy(_._1).filter(_._2.size > 1).keys\n    if (duplicates.nonEmpty) {\n      throw new ParseException(\n        s\"CTE definition can't have duplicate names: ${duplicates.mkString(\"'\", \"', '\", \"'\")}.\",\n        ctx)"
  },
  {
    "id" : "6453b683-7a83-44cd-90be-f8a2ac3cc079",
    "prId" : 24832,
    "prUrl" : "https://github.com/apache/spark/pull/24832#pullrequestreview-261498163",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8791eac8-5bdf-4451-836d-3a11e653f4ab",
        "parentId" : null,
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "I think this needs to be updated to remove the `ParseException` thrown when `IF NOT EXISTS` is present and there are dynamic partitions. I think that is an analysis problem, not a parse problem.\r\n\r\nAlso, I don't see a reason why IF NOT EXISTS would not be supported with dynamic partitions. Wouldn't that fail if any partitions would be overwritten? It seems to make sense to me, but maybe there is a good reason why this is not allowed? @gatorsmile can you comment?",
        "createdAt" : "2019-06-10T21:21:31Z",
        "updatedAt" : "2019-07-25T16:05:47Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      },
      {
        "id" : "bab915d0-a154-43dd-adad-0a9b761d3873",
        "parentId" : "8791eac8-5bdf-4451-836d-3a11e653f4ab",
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "We discussed this in the DSv2 sync last night and decided to add a method to the write builder to pass this `IF NOT EXISTS` flag. This will be done in a follow-up to avoid over-complicating this commit.",
        "createdAt" : "2019-06-27T16:10:48Z",
        "updatedAt" : "2019-07-25T16:05:47Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      },
      {
        "id" : "c41febdd-afaf-459d-954c-aa5770279622",
        "parentId" : "8791eac8-5bdf-4451-836d-3a11e653f4ab",
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "Opened https://issues.apache.org/jira/browse/SPARK-28374",
        "createdAt" : "2019-07-12T22:27:07Z",
        "updatedAt" : "2019-07-25T16:05:47Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      }
    ],
    "commit" : "7f193ca074b7fd0dea9f7b28b4e1776819e6d512",
    "line" : 67,
    "diffHunk" : "@@ -1,1 +311,315 @@      ctx: InsertOverwriteTableContext): InsertTableParams = withOrigin(ctx) {\n    assert(ctx.OVERWRITE() != null)\n    val tableIdent = visitMultipartIdentifier(ctx.multipartIdentifier)\n    val partitionKeys = Option(ctx.partitionSpec).map(visitPartitionSpec).getOrElse(Map.empty)\n"
  },
  {
    "id" : "17cb54c4-204d-4791-8603-21c0f79d0f02",
    "prId" : 24832,
    "prUrl" : "https://github.com/apache/spark/pull/24832#pullrequestreview-266725519",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f9dd97eb-1d29-40e1-88ee-37850b0b8431",
        "parentId" : null,
        "authorId" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "body" : "what's the point of adding this to the parser, if we're not going to support it?",
        "createdAt" : "2019-07-25T03:37:07Z",
        "updatedAt" : "2019-07-25T16:05:47Z",
        "lastEditedBy" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "tags" : [
        ]
      },
      {
        "id" : "fd520386-7a43-46d0-bc2c-3d2504ec2406",
        "parentId" : "f9dd97eb-1d29-40e1-88ee-37850b0b8431",
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "For a better error message that is testable. Before, there were no tests for this case and the error message listed expected symbols.",
        "createdAt" : "2019-07-25T15:53:06Z",
        "updatedAt" : "2019-07-25T16:05:47Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      },
      {
        "id" : "b67a0912-e4dd-442f-b406-0371548cf335",
        "parentId" : "f9dd97eb-1d29-40e1-88ee-37850b0b8431",
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "Also, since the PARTITION clause is optional for the above case, it shouldn't group the two together either. It is semantically incorrect because a write to a partitioned table is always a partitioned write.",
        "createdAt" : "2019-07-25T15:54:54Z",
        "updatedAt" : "2019-07-25T16:05:47Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      }
    ],
    "commit" : "7f193ca074b7fd0dea9f7b28b4e1776819e6d512",
    "line" : 55,
    "diffHunk" : "@@ -1,1 +298,302 @@    val partitionKeys = Option(ctx.partitionSpec).map(visitPartitionSpec).getOrElse(Map.empty)\n\n    if (ctx.EXISTS != null) {\n      operationNotAllowed(\"INSERT INTO ... IF NOT EXISTS\", ctx)\n    }"
  },
  {
    "id" : "d198f593-9d7f-4534-b372-85678c43fabd",
    "prId" : 24832,
    "prUrl" : "https://github.com/apache/spark/pull/24832#pullrequestreview-266722781",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c8c1970d-8e5a-49a8-a367-f25e4c36bfd7",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "why do we change the error message here?",
        "createdAt" : "2019-07-25T08:11:35Z",
        "updatedAt" : "2019-07-25T16:05:47Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "ebfd27a1-e651-46de-b348-f89c02c07c19",
        "parentId" : "c8c1970d-8e5a-49a8-a367-f25e4c36bfd7",
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "This uses `operationNotAllowed` instead of throwing a custom `ParseException`, like other methods that do not allow specific combinations. I think it's a good idea to standardize on the existing method. This also makes the error message like the others, where it states clear what is not allowed.",
        "createdAt" : "2019-07-25T15:50:52Z",
        "updatedAt" : "2019-07-25T16:05:47Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      }
    ],
    "commit" : "7f193ca074b7fd0dea9f7b28b4e1776819e6d512",
    "line" : 74,
    "diffHunk" : "@@ -1,1 +316,320 @@    val dynamicPartitionKeys: Map[String, Option[String]] = partitionKeys.filter(_._2.isEmpty)\n    if (ctx.EXISTS != null && dynamicPartitionKeys.nonEmpty) {\n      operationNotAllowed(\"IF NOT EXISTS with dynamic partitions: \" +\n        dynamicPartitionKeys.keys.mkString(\", \"), ctx)\n    }"
  },
  {
    "id" : "1ea23540-4de5-4a2c-af29-e2bc11de3ca4",
    "prId" : 24809,
    "prUrl" : "https://github.com/apache/spark/pull/24809#pullrequestreview-246818812",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f6b9b58e-1345-4fea-a7cf-85f79078d632",
        "parentId" : null,
        "authorId" : "88f0c774-9c59-4485-885d-f6aee36efcea",
        "body" : "Make these functions class methods since they are now reused in several other places.",
        "createdAt" : "2019-06-06T21:13:04Z",
        "updatedAt" : "2019-06-11T21:14:51Z",
        "lastEditedBy" : "88f0c774-9c59-4485-885d-f6aee36efcea",
        "tags" : [
        ]
      }
    ],
    "commit" : "7b174e52b6f7bad2ea85f6fa844af3f1e1ffbbb1",
    "line" : 150,
    "diffHunk" : "@@ -1,1 +446,450 @@  }\n\n  override def visitNamedExpressionSeq(\n      ctx: NamedExpressionSeqContext): Seq[Expression] = {\n    Option(ctx).toSeq"
  },
  {
    "id" : "6469e3aa-d341-4c11-bbb4-d9cf6a649f2a",
    "prId" : 24809,
    "prUrl" : "https://github.com/apache/spark/pull/24809#pullrequestreview-246819475",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ae5499c2-4ff0-4c1e-a3ea-6c4a8e15627f",
        "parentId" : null,
        "authorId" : "88f0c774-9c59-4485-885d-f6aee36efcea",
        "body" : "The core method to generate the query plan for a query specification. These methods have multiple arguments because they need to be reused in both regular queries and hive-style queries. I can't think of an elegant way of handling this.",
        "createdAt" : "2019-06-06T21:14:40Z",
        "updatedAt" : "2019-06-11T21:15:54Z",
        "lastEditedBy" : "88f0c774-9c59-4485-885d-f6aee36efcea",
        "tags" : [
        ]
      }
    ],
    "commit" : "7b174e52b6f7bad2ea85f6fa844af3f1e1ffbbb1",
    "line" : 211,
    "diffHunk" : "@@ -1,1 +474,478 @@  }\n\n  /**\n   * Add a hive-style transform (SELECT TRANSFORM/MAP/REDUCE) query specification to a logical plan.\n   */"
  },
  {
    "id" : "54ee5360-5dba-4126-8458-884aee887c72",
    "prId" : 24472,
    "prUrl" : "https://github.com/apache/spark/pull/24472#pullrequestreview-244701041",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f23adf34-6f6e-40eb-b2f9-d871e0c5deed",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "It sounds like we only added a very specific case. https://docs.teradata.com/reader/S0Fw2AVH8ff3MDA0wDOHlQ/RADkJCor1nDoyeD2T1VX5A \r\n\r\nAny reason?",
        "createdAt" : "2019-06-03T05:50:33Z",
        "updatedAt" : "2019-06-03T05:50:33Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "980c033a-e990-4246-adc2-579df13fb7d6",
        "parentId" : "f23adf34-6f6e-40eb-b2f9-d871e0c5deed",
        "authorId" : "aa2391d2-258d-487d-9173-0c2dcee36c47",
        "body" : "This is because `INTERVAL ... HOUR to SECOND` are most used in our existing scripts. If needed, maybe other cases like `INTERVAL .. X to X` can be contributed.",
        "createdAt" : "2019-06-03T06:43:19Z",
        "updatedAt" : "2019-06-03T06:43:19Z",
        "lastEditedBy" : "aa2391d2-258d-487d-9173-0c2dcee36c47",
        "tags" : [
        ]
      }
    ],
    "commit" : "0ee3bc9e870ca583c85029b1c7e29c4f089365f8",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1771,1775 @@   * supported:\n   * - Single unit.\n   * - From-To unit (only 'YEAR TO MONTH' and 'DAY TO SECOND' and 'HOUR to SECOND' are supported).\n   */\n  override def visitIntervalField(ctx: IntervalFieldContext): CalendarInterval = withOrigin(ctx) {"
  },
  {
    "id" : "c8bf8022-e0a8-4069-b113-a57ccb78614c",
    "prId" : 24472,
    "prUrl" : "https://github.com/apache/spark/pull/24472#pullrequestreview-249295788",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "73fd4c3b-aa16-4524-b4dd-bfe0996d09ed",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "nit: `'YEAR TO MONTH' and 'DAY TO SECOND'` -> `'YEAR TO MONTH', 'DAY TO SECOND'`",
        "createdAt" : "2019-06-13T11:53:44Z",
        "updatedAt" : "2019-06-13T11:53:45Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "0ee3bc9e870ca583c85029b1c7e29c4f089365f8",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1771,1775 @@   * supported:\n   * - Single unit.\n   * - From-To unit (only 'YEAR TO MONTH' and 'DAY TO SECOND' and 'HOUR to SECOND' are supported).\n   */\n  override def visitIntervalField(ctx: IntervalFieldContext): CalendarInterval = withOrigin(ctx) {"
  }
]