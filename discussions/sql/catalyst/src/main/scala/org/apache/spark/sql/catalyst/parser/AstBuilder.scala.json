[
  {
    "id" : "cddf2148-679a-4056-903e-ea8e7c8a5340",
    "prId" : 33200,
    "prUrl" : "https://github.com/apache/spark/pull/33200#pullrequestreview-713087207",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a203ecd9-5a4d-4be1-a290-10a88bd903bd",
        "parentId" : null,
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "New requirement for the replace columns.",
        "createdAt" : "2021-07-22T17:59:54Z",
        "updatedAt" : "2021-07-22T18:04:42Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "d2e691086b80fb7f64d1bce144bd1bdd03428b8d",
    "line" : 54,
    "diffHunk" : "@@ -1,1 +3745,3749 @@        if (col.path.isDefined) {\n          throw QueryParsingErrors.operationInHiveStyleCommandUnsupportedError(\n            \"Replacing with a nested column\", \"REPLACE COLUMNS\", ctx)\n        }\n        col"
  },
  {
    "id" : "dd29ba44-b899-4f2e-9684-dd9959c6ba2f",
    "prId" : 33176,
    "prUrl" : "https://github.com/apache/spark/pull/33176#pullrequestreview-697511898",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "646965fc-b5e7-493b-8235-3206163bec66",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "We need to support parsing type string for LTZ and NTZ specifically so that people can avoid relying on config in their queries.",
        "createdAt" : "2021-07-01T17:29:49Z",
        "updatedAt" : "2021-07-01T17:30:10Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "c97ce11f-145e-488c-aaf9-3c3266c388fc",
        "parentId" : "646965fc-b5e7-493b-8235-3206163bec66",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "I would prefer doing that in separate PRs to keep this one clean. We should support the new keyword both on data type parsing and the type literals.\r\nhttps://issues.apache.org/jira/browse/SPARK-35977\r\nhttps://issues.apache.org/jira/browse/SPARK-35978",
        "createdAt" : "2021-07-01T17:51:11Z",
        "updatedAt" : "2021-07-01T17:51:19Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "6c6bf3a04572a832ff9224c1a3cd61b81784c625",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +2503,2507 @@      case (\"double\", Nil) => DoubleType\n      case (\"date\", Nil) => DateType\n      case (\"timestamp\", Nil) => SQLConf.get.timestampType\n      case (\"string\", Nil) => StringType\n      case (\"character\" | \"char\", length :: Nil) => CharType(length.getText.toInt)"
  },
  {
    "id" : "47b5b052-c0ef-447f-9279-d79cbb6b578a",
    "prId" : 32922,
    "prUrl" : "https://github.com/apache/spark/pull/32922#pullrequestreview-690240933",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1d5c6bef-a57c-410a-ba76-89195dbe4fc7",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "shall we forbid `interval month to month`?",
        "createdAt" : "2021-06-23T03:12:04Z",
        "updatedAt" : "2021-06-23T03:12:29Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "b0d22082-a147-4fe0-93dd-ff0489d9a6c1",
        "parentId" : "1d5c6bef-a57c-410a-ba76-89195dbe4fc7",
        "authorId" : "3a227965-84e0-47cf-9974-11293764f028",
        "body" : "Sounds reasonable. I'll open a following-up PR for year-month/day-time intervals.",
        "createdAt" : "2021-06-23T06:02:23Z",
        "updatedAt" : "2021-06-23T06:02:23Z",
        "lastEditedBy" : "3a227965-84e0-47cf-9974-11293764f028",
        "tags" : [
        ]
      }
    ],
    "commit" : "d260bb1bc27d68f0cdf2287662413351e6b7b3c0",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +2517,2521 @@    val start = YearMonthIntervalType.stringToField(ctx.from.getText.toLowerCase(Locale.ROOT))\n    val end = if (ctx.to != null) {\n      YearMonthIntervalType.stringToField(ctx.to.getText.toLowerCase(Locale.ROOT))\n    } else {\n      start"
  },
  {
    "id" : "ad2f24db-2307-4f17-ab7a-2e9d384364e8",
    "prId" : 32892,
    "prUrl" : "https://github.com/apache/spark/pull/32892#pullrequestreview-682442151",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "468c20e8-9a5a-4d3e-9643-5ad55fe39795",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "I have seen this already in other PR. Does it make sense to put it to a common place `case object DayTimeIntervalType`?",
        "createdAt" : "2021-06-13T19:54:01Z",
        "updatedAt" : "2021-06-13T19:54:01Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "cc67ff97-77f3-489d-bae1-3dd0500debdd",
        "parentId" : "468c20e8-9a5a-4d3e-9643-5ad55fe39795",
        "authorId" : "3a227965-84e0-47cf-9974-11293764f028",
        "body" : "Yeah, it seems a common logic. After this and #32893 are merged, I'll open a followup PR.",
        "createdAt" : "2021-06-13T20:20:42Z",
        "updatedAt" : "2021-06-13T20:20:42Z",
        "lastEditedBy" : "3a227965-84e0-47cf-9974-11293764f028",
        "tags" : [
        ]
      }
    ],
    "commit" : "eef88fcbe7770d942fd8cf1177791eebf0ad79e7",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +2358,2362 @@      } else {\n        assert(calendarInterval.months == 0)\n        val strToFieldIndex = DayTimeIntervalType.dayTimeFields.map(i =>\n          DayTimeIntervalType.fieldToString(i) -> i).toMap\n        val fromUnit ="
  },
  {
    "id" : "56c537db-a6f2-437e-af38-1775c313f062",
    "prId" : 32303,
    "prUrl" : "https://github.com/apache/spark/pull/32303#pullrequestreview-678799840",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9c2dfc3e-ea7e-4ea1-a831-a69fb571012d",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "We do not handle join hints for the lateral case?",
        "createdAt" : "2021-06-08T04:00:20Z",
        "updatedAt" : "2021-06-08T04:39:01Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "85153080-1127-4185-b343-2a7c40486203",
        "parentId" : "9c2dfc3e-ea7e-4ea1-a831-a69fb571012d",
        "authorId" : "b3ba992a-312c-46eb-b3c3-8d861d15ac40",
        "body" : "Good point. I will add it.",
        "createdAt" : "2021-06-08T05:06:02Z",
        "updatedAt" : "2021-06-08T05:06:02Z",
        "lastEditedBy" : "b3ba992a-312c-46eb-b3c3-8d861d15ac40",
        "tags" : [
        ]
      },
      {
        "id" : "16732917-5652-4fa6-91e2-4f56916e8d11",
        "parentId" : "9c2dfc3e-ea7e-4ea1-a831-a69fb571012d",
        "authorId" : "b3ba992a-312c-46eb-b3c3-8d861d15ac40",
        "body" : "Actually, does it make sense to have join hints for lateral joins? A lateral join is essentially a nested loop join. Ideally, the evaluation logic should be for each row in the left, plug the outer query attribute values into the outer references and evaluate the subquery. So it should only be planned as a (correlated) nested loop join. But since Spark doesn't support such execution, it first decorrelates the subquery and then rewrites the lateral join as a normal join. This seems to be an implementation detail and it doesn't make sense to add join hints here.",
        "createdAt" : "2021-06-08T06:18:37Z",
        "updatedAt" : "2021-06-08T06:18:37Z",
        "lastEditedBy" : "b3ba992a-312c-46eb-b3c3-8d861d15ac40",
        "tags" : [
        ]
      },
      {
        "id" : "ecef3641-218e-4406-8b0e-664fee6df5bc",
        "parentId" : "9c2dfc3e-ea7e-4ea1-a831-a69fb571012d",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Hints exist for the physical planner and the planner does not tell a difference between normal joins and lateral joins (cuz the optimizer rewrites lateral join into normal ones), so adding hints for this case makes sense to me. In fact, I think users will eventually want to control physical joins types for lateral joins, too, to tune user's queries for their workloads.ã€€Anyway, since this is an improvement, I think it is okay to fix it in follow-up.",
        "createdAt" : "2021-06-08T08:28:07Z",
        "updatedAt" : "2021-06-08T08:28:07Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "1bd22cb9-034a-4e01-b593-7249b9b90222",
        "parentId" : "9c2dfc3e-ea7e-4ea1-a831-a69fb571012d",
        "authorId" : "b3ba992a-312c-46eb-b3c3-8d861d15ac40",
        "body" : "Sounds good.",
        "createdAt" : "2021-06-08T17:29:46Z",
        "updatedAt" : "2021-06-08T17:29:46Z",
        "lastEditedBy" : "b3ba992a-312c-46eb-b3c3-8d861d15ac40",
        "tags" : [
        ]
      }
    ],
    "commit" : "01fa70ec156ad5540b862c4b13dd1257062f31fb",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +874,878 @@      val join = right.optionalMap(left) { (left, right) =>\n        if (relation.LATERAL != null) {\n          LateralJoin(left, LateralSubquery(right), Inner, None)\n        } else {\n          Join(left, right, Inner, None, JoinHint.NONE)"
  },
  {
    "id" : "70c306e0-ea7d-4690-87c0-584a1cc9861a",
    "prId" : 32209,
    "prUrl" : "https://github.com/apache/spark/pull/32209#pullrequestreview-638653318",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bff317e5-dbd8-414f-bcd4-e3ff8a10472e",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "A little bit of opinion. Why not create a new `parseIntervalLiteral` for ansi interval, but reuse `parseIntervalLiteral`?",
        "createdAt" : "2021-04-19T09:13:17Z",
        "updatedAt" : "2021-04-19T09:13:55Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      },
      {
        "id" : "5041b4f1-4e13-42ed-b79b-cd23e22c8f76",
        "parentId" : "bff317e5-dbd8-414f-bcd4-e3ff8a10472e",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "I think we will have to do that at the end to support all combinations of `to` and `from` + precision ( `like INTERVAL '1 2:3:4.123' DAY(6) TO SECOND(3)`). But for now, we implement simplest possible solution to unblock other sub-tasks needed for Milestone 1 in SPARK-27790. The Milestone 1 supposes feature parity of new ANSI types with `CalendarIntervalType`. ",
        "createdAt" : "2021-04-19T09:25:57Z",
        "updatedAt" : "2021-04-19T09:25:58Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "9609d563-8621-4026-9afa-bbe75d298948",
        "parentId" : "bff317e5-dbd8-414f-bcd4-e3ff8a10472e",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "I got it. @MaxGekk Thans for your explanation.",
        "createdAt" : "2021-04-19T09:28:41Z",
        "updatedAt" : "2021-04-19T09:28:41Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "790815029a4072346ac38c57737b98a619080aa0",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +2316,2320 @@    val calendarInterval = parseIntervalLiteral(ctx)\n    if (ctx.errorCapturingUnitToUnitInterval != null && !conf.legacyIntervalEnabled) {\n      // Check the `to` unit to distinguish year-month and day-time intervals because\n      // `CalendarInterval` doesn't have enough info. For instance, new CalendarInterval(0, 0, 0)\n      // can be derived from INTERVAL '0-0' YEAR TO MONTH as well as from"
  },
  {
    "id" : "b239a73d-5ba7-4839-9140-2e682d8e8e1f",
    "prId" : 31421,
    "prUrl" : "https://github.com/apache/spark/pull/31421#pullrequestreview-581055792",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c5ef10e8-6f9a-49b4-b107-84624c9c36d8",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "nit: 4 spaces indentation",
        "createdAt" : "2021-02-02T07:39:19Z",
        "updatedAt" : "2021-02-02T07:39:20Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "971ccbd5947924992a0c24acd40d40f1587d7741",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +504,508 @@   */\n  protected def visitStringConstant(\n    ctx: ConstantContext,\n    processNullLiteral: Boolean): String = withOrigin(ctx) {\n    ctx match {"
  },
  {
    "id" : "7741d79b-0a9c-493c-954c-32ceab21f16a",
    "prId" : 31421,
    "prUrl" : "https://github.com/apache/spark/pull/31421#pullrequestreview-581056073",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fe29d78d-700c-4abd-a8e6-12ae05e8c5ef",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "this name is weird, how about `legacyNullAsString`?",
        "createdAt" : "2021-02-02T07:39:50Z",
        "updatedAt" : "2021-02-02T07:39:51Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "971ccbd5947924992a0c24acd40d40f1587d7741",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +473,477 @@  override def visitPartitionSpec(\n      ctx: PartitionSpecContext): Map[String, Option[String]] = withOrigin(ctx) {\n    val processNullLiteral =\n      !conf.getConf(SQLConf.LEGACY_PARSE_NULL_PARTITION_SPEC_AS_STRING_LITERAL)\n    val parts = ctx.partitionVal.asScala.map { pVal =>"
  },
  {
    "id" : "740fdd56-8005-4339-9323-a362f19a2c54",
    "prId" : 30943,
    "prUrl" : "https://github.com/apache/spark/pull/30943#pullrequestreview-559864763",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6d78cca9-031c-4909-8353-69ac05b3635c",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "`val ignoreNulls = Option(ctx.nullsOption).map(_.getType == SqlBaseParser.IGNORE).getOrElse(false)`",
        "createdAt" : "2020-12-30T05:15:38Z",
        "updatedAt" : "2020-12-30T06:46:08Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "f437e157-3c6f-4ddc-8817-e49b9e37dcdd",
        "parentId" : "6d78cca9-031c-4909-8353-69ac05b3635c",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "OK",
        "createdAt" : "2020-12-30T06:02:53Z",
        "updatedAt" : "2020-12-30T06:46:08Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "4cc5eb1c6c4ee31186dca059154621753b0ee62f",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +1698,1702 @@    }\n    val filter = Option(ctx.where).map(expression(_))\n    val ignoreNulls =\n      Option(ctx.nullsOption).map(_.getType == SqlBaseParser.IGNORE).getOrElse(false)\n    val function = UnresolvedFunction("
  },
  {
    "id" : "3ba68c02-ddc9-4fcf-b962-7f433574e51a",
    "prId" : 30813,
    "prUrl" : "https://github.com/apache/spark/pull/30813#pullrequestreview-554250192",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a13a3664-5e41-40c6-8907-02fdaabe29dc",
        "parentId" : null,
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "`partitionSpec` is not using `UnresolvedPartitionSpec` since v2 command is not supported.",
        "createdAt" : "2020-12-17T03:15:22Z",
        "updatedAt" : "2020-12-17T03:15:36Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "b59a7d7b510f6597a673431157b1835486faf7b6",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +3817,3821 @@   */\n  override def visitSetTableSerDe(ctx: SetTableSerDeContext): LogicalPlan = withOrigin(ctx) {\n    AlterTableSerDeProperties(\n      UnresolvedTable(\n        visitMultipartIdentifier(ctx.multipartIdentifier),"
  }
]