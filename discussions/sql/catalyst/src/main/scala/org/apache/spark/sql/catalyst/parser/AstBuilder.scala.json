[
  {
    "id" : "cddf2148-679a-4056-903e-ea8e7c8a5340",
    "prId" : 33200,
    "prUrl" : "https://github.com/apache/spark/pull/33200#pullrequestreview-713087207",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a203ecd9-5a4d-4be1-a290-10a88bd903bd",
        "parentId" : null,
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "New requirement for the replace columns.",
        "createdAt" : "2021-07-22T17:59:54Z",
        "updatedAt" : "2021-07-22T18:04:42Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "d2e691086b80fb7f64d1bce144bd1bdd03428b8d",
    "line" : 54,
    "diffHunk" : "@@ -1,1 +3745,3749 @@        if (col.path.isDefined) {\n          throw QueryParsingErrors.operationInHiveStyleCommandUnsupportedError(\n            \"Replacing with a nested column\", \"REPLACE COLUMNS\", ctx)\n        }\n        col"
  },
  {
    "id" : "dd29ba44-b899-4f2e-9684-dd9959c6ba2f",
    "prId" : 33176,
    "prUrl" : "https://github.com/apache/spark/pull/33176#pullrequestreview-697511898",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "646965fc-b5e7-493b-8235-3206163bec66",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "We need to support parsing type string for LTZ and NTZ specifically so that people can avoid relying on config in their queries.",
        "createdAt" : "2021-07-01T17:29:49Z",
        "updatedAt" : "2021-07-01T17:30:10Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "c97ce11f-145e-488c-aaf9-3c3266c388fc",
        "parentId" : "646965fc-b5e7-493b-8235-3206163bec66",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "I would prefer doing that in separate PRs to keep this one clean. We should support the new keyword both on data type parsing and the type literals.\r\nhttps://issues.apache.org/jira/browse/SPARK-35977\r\nhttps://issues.apache.org/jira/browse/SPARK-35978",
        "createdAt" : "2021-07-01T17:51:11Z",
        "updatedAt" : "2021-07-01T17:51:19Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "6c6bf3a04572a832ff9224c1a3cd61b81784c625",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +2503,2507 @@      case (\"double\", Nil) => DoubleType\n      case (\"date\", Nil) => DateType\n      case (\"timestamp\", Nil) => SQLConf.get.timestampType\n      case (\"string\", Nil) => StringType\n      case (\"character\" | \"char\", length :: Nil) => CharType(length.getText.toInt)"
  },
  {
    "id" : "47b5b052-c0ef-447f-9279-d79cbb6b578a",
    "prId" : 32922,
    "prUrl" : "https://github.com/apache/spark/pull/32922#pullrequestreview-690240933",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1d5c6bef-a57c-410a-ba76-89195dbe4fc7",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "shall we forbid `interval month to month`?",
        "createdAt" : "2021-06-23T03:12:04Z",
        "updatedAt" : "2021-06-23T03:12:29Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "b0d22082-a147-4fe0-93dd-ff0489d9a6c1",
        "parentId" : "1d5c6bef-a57c-410a-ba76-89195dbe4fc7",
        "authorId" : "3a227965-84e0-47cf-9974-11293764f028",
        "body" : "Sounds reasonable. I'll open a following-up PR for year-month/day-time intervals.",
        "createdAt" : "2021-06-23T06:02:23Z",
        "updatedAt" : "2021-06-23T06:02:23Z",
        "lastEditedBy" : "3a227965-84e0-47cf-9974-11293764f028",
        "tags" : [
        ]
      }
    ],
    "commit" : "d260bb1bc27d68f0cdf2287662413351e6b7b3c0",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +2517,2521 @@    val start = YearMonthIntervalType.stringToField(ctx.from.getText.toLowerCase(Locale.ROOT))\n    val end = if (ctx.to != null) {\n      YearMonthIntervalType.stringToField(ctx.to.getText.toLowerCase(Locale.ROOT))\n    } else {\n      start"
  },
  {
    "id" : "ad2f24db-2307-4f17-ab7a-2e9d384364e8",
    "prId" : 32892,
    "prUrl" : "https://github.com/apache/spark/pull/32892#pullrequestreview-682442151",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "468c20e8-9a5a-4d3e-9643-5ad55fe39795",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "I have seen this already in other PR. Does it make sense to put it to a common place `case object DayTimeIntervalType`?",
        "createdAt" : "2021-06-13T19:54:01Z",
        "updatedAt" : "2021-06-13T19:54:01Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "cc67ff97-77f3-489d-bae1-3dd0500debdd",
        "parentId" : "468c20e8-9a5a-4d3e-9643-5ad55fe39795",
        "authorId" : "3a227965-84e0-47cf-9974-11293764f028",
        "body" : "Yeah, it seems a common logic. After this and #32893 are merged, I'll open a followup PR.",
        "createdAt" : "2021-06-13T20:20:42Z",
        "updatedAt" : "2021-06-13T20:20:42Z",
        "lastEditedBy" : "3a227965-84e0-47cf-9974-11293764f028",
        "tags" : [
        ]
      }
    ],
    "commit" : "eef88fcbe7770d942fd8cf1177791eebf0ad79e7",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +2358,2362 @@      } else {\n        assert(calendarInterval.months == 0)\n        val strToFieldIndex = DayTimeIntervalType.dayTimeFields.map(i =>\n          DayTimeIntervalType.fieldToString(i) -> i).toMap\n        val fromUnit ="
  },
  {
    "id" : "56c537db-a6f2-437e-af38-1775c313f062",
    "prId" : 32303,
    "prUrl" : "https://github.com/apache/spark/pull/32303#pullrequestreview-678799840",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9c2dfc3e-ea7e-4ea1-a831-a69fb571012d",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "We do not handle join hints for the lateral case?",
        "createdAt" : "2021-06-08T04:00:20Z",
        "updatedAt" : "2021-06-08T04:39:01Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "85153080-1127-4185-b343-2a7c40486203",
        "parentId" : "9c2dfc3e-ea7e-4ea1-a831-a69fb571012d",
        "authorId" : "b3ba992a-312c-46eb-b3c3-8d861d15ac40",
        "body" : "Good point. I will add it.",
        "createdAt" : "2021-06-08T05:06:02Z",
        "updatedAt" : "2021-06-08T05:06:02Z",
        "lastEditedBy" : "b3ba992a-312c-46eb-b3c3-8d861d15ac40",
        "tags" : [
        ]
      },
      {
        "id" : "16732917-5652-4fa6-91e2-4f56916e8d11",
        "parentId" : "9c2dfc3e-ea7e-4ea1-a831-a69fb571012d",
        "authorId" : "b3ba992a-312c-46eb-b3c3-8d861d15ac40",
        "body" : "Actually, does it make sense to have join hints for lateral joins? A lateral join is essentially a nested loop join. Ideally, the evaluation logic should be for each row in the left, plug the outer query attribute values into the outer references and evaluate the subquery. So it should only be planned as a (correlated) nested loop join. But since Spark doesn't support such execution, it first decorrelates the subquery and then rewrites the lateral join as a normal join. This seems to be an implementation detail and it doesn't make sense to add join hints here.",
        "createdAt" : "2021-06-08T06:18:37Z",
        "updatedAt" : "2021-06-08T06:18:37Z",
        "lastEditedBy" : "b3ba992a-312c-46eb-b3c3-8d861d15ac40",
        "tags" : [
        ]
      },
      {
        "id" : "ecef3641-218e-4406-8b0e-664fee6df5bc",
        "parentId" : "9c2dfc3e-ea7e-4ea1-a831-a69fb571012d",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Hints exist for the physical planner and the planner does not tell a difference between normal joins and lateral joins (cuz the optimizer rewrites lateral join into normal ones), so adding hints for this case makes sense to me. In fact, I think users will eventually want to control physical joins types for lateral joins, too, to tune user's queries for their workloads.ã€€Anyway, since this is an improvement, I think it is okay to fix it in follow-up.",
        "createdAt" : "2021-06-08T08:28:07Z",
        "updatedAt" : "2021-06-08T08:28:07Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "1bd22cb9-034a-4e01-b593-7249b9b90222",
        "parentId" : "9c2dfc3e-ea7e-4ea1-a831-a69fb571012d",
        "authorId" : "b3ba992a-312c-46eb-b3c3-8d861d15ac40",
        "body" : "Sounds good.",
        "createdAt" : "2021-06-08T17:29:46Z",
        "updatedAt" : "2021-06-08T17:29:46Z",
        "lastEditedBy" : "b3ba992a-312c-46eb-b3c3-8d861d15ac40",
        "tags" : [
        ]
      }
    ],
    "commit" : "01fa70ec156ad5540b862c4b13dd1257062f31fb",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +874,878 @@      val join = right.optionalMap(left) { (left, right) =>\n        if (relation.LATERAL != null) {\n          LateralJoin(left, LateralSubquery(right), Inner, None)\n        } else {\n          Join(left, right, Inner, None, JoinHint.NONE)"
  },
  {
    "id" : "70c306e0-ea7d-4690-87c0-584a1cc9861a",
    "prId" : 32209,
    "prUrl" : "https://github.com/apache/spark/pull/32209#pullrequestreview-638653318",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bff317e5-dbd8-414f-bcd4-e3ff8a10472e",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "A little bit of opinion. Why not create a new `parseIntervalLiteral` for ansi interval, but reuse `parseIntervalLiteral`?",
        "createdAt" : "2021-04-19T09:13:17Z",
        "updatedAt" : "2021-04-19T09:13:55Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      },
      {
        "id" : "5041b4f1-4e13-42ed-b79b-cd23e22c8f76",
        "parentId" : "bff317e5-dbd8-414f-bcd4-e3ff8a10472e",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "I think we will have to do that at the end to support all combinations of `to` and `from` + precision ( `like INTERVAL '1 2:3:4.123' DAY(6) TO SECOND(3)`). But for now, we implement simplest possible solution to unblock other sub-tasks needed for Milestone 1 in SPARK-27790. The Milestone 1 supposes feature parity of new ANSI types with `CalendarIntervalType`. ",
        "createdAt" : "2021-04-19T09:25:57Z",
        "updatedAt" : "2021-04-19T09:25:58Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "9609d563-8621-4026-9afa-bbe75d298948",
        "parentId" : "bff317e5-dbd8-414f-bcd4-e3ff8a10472e",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "I got it. @MaxGekk Thans for your explanation.",
        "createdAt" : "2021-04-19T09:28:41Z",
        "updatedAt" : "2021-04-19T09:28:41Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "790815029a4072346ac38c57737b98a619080aa0",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +2316,2320 @@    val calendarInterval = parseIntervalLiteral(ctx)\n    if (ctx.errorCapturingUnitToUnitInterval != null && !conf.legacyIntervalEnabled) {\n      // Check the `to` unit to distinguish year-month and day-time intervals because\n      // `CalendarInterval` doesn't have enough info. For instance, new CalendarInterval(0, 0, 0)\n      // can be derived from INTERVAL '0-0' YEAR TO MONTH as well as from"
  },
  {
    "id" : "b239a73d-5ba7-4839-9140-2e682d8e8e1f",
    "prId" : 31421,
    "prUrl" : "https://github.com/apache/spark/pull/31421#pullrequestreview-581055792",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c5ef10e8-6f9a-49b4-b107-84624c9c36d8",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "nit: 4 spaces indentation",
        "createdAt" : "2021-02-02T07:39:19Z",
        "updatedAt" : "2021-02-02T07:39:20Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "971ccbd5947924992a0c24acd40d40f1587d7741",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +504,508 @@   */\n  protected def visitStringConstant(\n    ctx: ConstantContext,\n    processNullLiteral: Boolean): String = withOrigin(ctx) {\n    ctx match {"
  },
  {
    "id" : "7741d79b-0a9c-493c-954c-32ceab21f16a",
    "prId" : 31421,
    "prUrl" : "https://github.com/apache/spark/pull/31421#pullrequestreview-581056073",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fe29d78d-700c-4abd-a8e6-12ae05e8c5ef",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "this name is weird, how about `legacyNullAsString`?",
        "createdAt" : "2021-02-02T07:39:50Z",
        "updatedAt" : "2021-02-02T07:39:51Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "971ccbd5947924992a0c24acd40d40f1587d7741",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +473,477 @@  override def visitPartitionSpec(\n      ctx: PartitionSpecContext): Map[String, Option[String]] = withOrigin(ctx) {\n    val processNullLiteral =\n      !conf.getConf(SQLConf.LEGACY_PARSE_NULL_PARTITION_SPEC_AS_STRING_LITERAL)\n    val parts = ctx.partitionVal.asScala.map { pVal =>"
  },
  {
    "id" : "740fdd56-8005-4339-9323-a362f19a2c54",
    "prId" : 30943,
    "prUrl" : "https://github.com/apache/spark/pull/30943#pullrequestreview-559864763",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6d78cca9-031c-4909-8353-69ac05b3635c",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "`val ignoreNulls = Option(ctx.nullsOption).map(_.getType == SqlBaseParser.IGNORE).getOrElse(false)`",
        "createdAt" : "2020-12-30T05:15:38Z",
        "updatedAt" : "2020-12-30T06:46:08Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "f437e157-3c6f-4ddc-8817-e49b9e37dcdd",
        "parentId" : "6d78cca9-031c-4909-8353-69ac05b3635c",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "OK",
        "createdAt" : "2020-12-30T06:02:53Z",
        "updatedAt" : "2020-12-30T06:46:08Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "4cc5eb1c6c4ee31186dca059154621753b0ee62f",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +1698,1702 @@    }\n    val filter = Option(ctx.where).map(expression(_))\n    val ignoreNulls =\n      Option(ctx.nullsOption).map(_.getType == SqlBaseParser.IGNORE).getOrElse(false)\n    val function = UnresolvedFunction("
  },
  {
    "id" : "3ba68c02-ddc9-4fcf-b962-7f433574e51a",
    "prId" : 30813,
    "prUrl" : "https://github.com/apache/spark/pull/30813#pullrequestreview-554250192",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a13a3664-5e41-40c6-8907-02fdaabe29dc",
        "parentId" : null,
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "`partitionSpec` is not using `UnresolvedPartitionSpec` since v2 command is not supported.",
        "createdAt" : "2020-12-17T03:15:22Z",
        "updatedAt" : "2020-12-17T03:15:36Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "b59a7d7b510f6597a673431157b1835486faf7b6",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +3817,3821 @@   */\n  override def visitSetTableSerDe(ctx: SetTableSerDeContext): LogicalPlan = withOrigin(ctx) {\n    AlterTableSerDeProperties(\n      UnresolvedTable(\n        visitMultipartIdentifier(ctx.multipartIdentifier),"
  },
  {
    "id" : "15545733-0348-4e85-8b74-5d6524542570",
    "prId" : 30648,
    "prUrl" : "https://github.com/apache/spark/pull/30648#pullrequestreview-547885813",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "17abb4e8-bf5a-4cfe-b559-fac9034700d9",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "`ANALYZE TABLES IN default COMPUTE STATISTICS;` -> `ANALYZE TABLES IN multi_part_name COMPUTE STATISTICS [NOSCAN];`?",
        "createdAt" : "2020-12-07T23:38:34Z",
        "updatedAt" : "2021-02-25T03:06:41Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "754e7425-890a-47d2-9174-c777c1e8d51c",
        "parentId" : "17abb4e8-bf5a-4cfe-b559-fac9034700d9",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "This is example, similar to https://github.com/apache/spark/blob/29fed23ba16d580e6247b6e70e9c9eef0698aa95/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala#L3494-L3508",
        "createdAt" : "2020-12-09T06:54:20Z",
        "updatedAt" : "2021-02-25T03:06:41Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      }
    ],
    "commit" : "81efb9d1f0dc43b3053d03d3ea183e436a972576",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +3659,3663 @@   * Example SQL for analyzing all tables in default database:\n   * {{{\n   *   ANALYZE TABLES IN default COMPUTE STATISTICS;\n   * }}}\n   */"
  },
  {
    "id" : "e5aecca3-d867-4209-8168-773153d307af",
    "prId" : 30648,
    "prUrl" : "https://github.com/apache/spark/pull/30648#pullrequestreview-547883320",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a7de128c-0faa-46bc-b883-95538632c6e6",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "nit: `Seq.empty[String]` -> `Nil`?",
        "createdAt" : "2020-12-07T23:39:36Z",
        "updatedAt" : "2021-02-25T03:06:41Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "27be8c2e-39d8-4c89-9e8d-560ffbfc5f22",
        "parentId" : "a7de128c-0faa-46bc-b883-95538632c6e6",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "+1",
        "createdAt" : "2020-12-09T06:48:36Z",
        "updatedAt" : "2021-02-25T03:06:41Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      }
    ],
    "commit" : "81efb9d1f0dc43b3053d03d3ea183e436a972576",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +3670,3674 @@    val multiPart = Option(ctx.multipartIdentifier).map(visitMultipartIdentifier)\n    AnalyzeTables(\n      UnresolvedNamespace(multiPart.getOrElse(Seq.empty[String])),\n      noScan = ctx.identifier != null)\n  }"
  },
  {
    "id" : "51df87cd-a6e4-4a7d-a894-d695f1b87196",
    "prId" : 30610,
    "prUrl" : "https://github.com/apache/spark/pull/30610#pullrequestreview-546668403",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4e164ced-e040-4eb8-9bb0-d0e149e8bffb",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "if `isView=false`, shall we create `UnresolvedTable`?",
        "createdAt" : "2020-12-07T08:28:38Z",
        "updatedAt" : "2020-12-07T08:28:38Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "3f915a09-565c-4f1c-af71-e78905bc71c1",
        "parentId" : "4e164ced-e040-4eb8-9bb0-d0e149e8bffb",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "I can't because `ALTER TABLE ... RENAME TO` supports temp views as well.",
        "createdAt" : "2020-12-08T01:10:32Z",
        "updatedAt" : "2020-12-08T01:10:32Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "d1031753346babfe91f540a5d96ef433e5c04313",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +3856,3860 @@    val relationStr = if (isView) \"VIEW\" else \"TABLE\"\n    RenameTable(\n      UnresolvedTableOrView(\n        visitMultipartIdentifier(ctx.from),\n        s\"ALTER $relationStr ... RENAME TO\"),"
  },
  {
    "id" : "566c6ac1-13a1-4599-838b-2bcc76418f00",
    "prId" : 30465,
    "prUrl" : "https://github.com/apache/spark/pull/30465#pullrequestreview-537288660",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1e9d9da8-0a51-4257-95c5-8317f22b89be",
        "parentId" : null,
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "It seems we do not need empty check in Line#1387 now: https://github.com/apache/spark/pull/30465/files#diff-26fe6b511f4b8aedf5bdffae0d7c03aba69463c09fa4f881d77520125796e0faR1387",
        "createdAt" : "2020-11-23T13:37:23Z",
        "updatedAt" : "2020-11-30T02:17:53Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      },
      {
        "id" : "26f9095a-f512-4289-ae08-7b21611de645",
        "parentId" : "1e9d9da8-0a51-4257-95c5-8317f22b89be",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "Let's remove `getLikeQuantifierExprs`",
        "createdAt" : "2020-11-24T09:20:29Z",
        "updatedAt" : "2020-11-30T02:17:53Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "2be24d1455a3078da2021781bab97c22979913be",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +1397,1401 @@        Option(ctx.quantifier).map(_.getType) match {\n          case Some(SqlBaseParser.ANY) | Some(SqlBaseParser.SOME) =>\n            validate(!ctx.expression.isEmpty, \"Expected something between '(' and ')'.\", ctx)\n            val expressions = expressionList(ctx.expression)\n            if (expressions.forall(_.foldable) && expressions.forall(_.dataType == StringType)) {"
  },
  {
    "id" : "562ac1ab-2b59-436e-b573-9dabcca2fc08",
    "prId" : 30421,
    "prUrl" : "https://github.com/apache/spark/pull/30421#pullrequestreview-599381116",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "35e702a9-0b00-405c-9226-058de8b5e285",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "can we add a TODO here? For v2 commands, we will cast the string back to its actual value, which is a waste and can be improved in the future.",
        "createdAt" : "2021-02-26T08:33:52Z",
        "updatedAt" : "2021-03-02T13:10:58Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "f710bd84-2b0e-4144-9a23-9f5c0be3d7bb",
        "parentId" : "35e702a9-0b00-405c-9226-058de8b5e285",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> can we add a TODO here? For v2 commands, we will cast the string back to its actual value, which is a waste and can be improved in the future.\r\n\r\nDone",
        "createdAt" : "2021-02-26T09:10:52Z",
        "updatedAt" : "2021-03-02T13:10:58Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "08c55f66226a4cd3bd5f6389225d510e9c821b23",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +513,517 @@      case Literal(null, _) if !legacyNullAsString => null\n      case l @ Literal(null, _) => l.toString\n      case l: Literal =>\n        // TODO For v2 commands, we will cast the string back to its actual value,\n        //  which is a waste and can be improved in the future."
  },
  {
    "id" : "289983b9-82b8-420d-b473-b3428ad2fe16",
    "prId" : 30421,
    "prUrl" : "https://github.com/apache/spark/pull/30421#pullrequestreview-599380738",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d12d2cbf-f4a6-434a-972a-6a89ff8a133a",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "does it work for `ADD PARTITION(a=date'xxx')`, etc.?",
        "createdAt" : "2021-02-26T08:43:01Z",
        "updatedAt" : "2021-03-02T13:10:58Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "d4db2c46-80b2-428b-bcae-51ca81af4999",
        "parentId" : "d12d2cbf-f4a6-434a-972a-6a89ff8a133a",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> does it work for `ADD PARTITION(a=date'xxx')`, etc.?\r\n\r\nSupport and add test case  in UT",
        "createdAt" : "2021-02-26T09:10:28Z",
        "updatedAt" : "2021-03-02T13:10:58Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "08c55f66226a4cd3bd5f6389225d510e9c821b23",
    "line" : 1,
    "diffHunk" : "@@ -1,1 +507,511 @@   * String -> Literal -> String.\n   */\n  protected def visitStringConstant(\n      ctx: ConstantContext,\n      legacyNullAsString: Boolean): String = withOrigin(ctx) {"
  },
  {
    "id" : "dd7d5c20-3454-4511-a258-275d94b83fb3",
    "prId" : 30229,
    "prUrl" : "https://github.com/apache/spark/pull/30229#pullrequestreview-523079967",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cae319fb-9ad7-403b-af69-c900817c4f7b",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "`AnalyzeColumn` allows temp view but `AnalyzeTable`  does not?",
        "createdAt" : "2020-11-04T05:09:25Z",
        "updatedAt" : "2020-11-04T05:09:25Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "d767bea3-0aac-4ab6-b854-6c46353e752b",
        "parentId" : "cae319fb-9ad7-403b-af69-c900817c4f7b",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "Yes, that is what I see. The following is for `AnalyzeColumnCommand`:\r\nhttps://github.com/apache/spark/blob/1740b29b3f006abd08bc01b0ca807c3721d4bb0e/sql/core/src/main/scala/org/apache/spark/sql/execution/command/AnalyzeColumnCommand.scala#L53-L57\r\n\r\nOn the other hand, I don't see `getTempView` or `getTempViewOrPermanentTableMetadata` inside `AnalyzeTableCommand`. But looks like we can support temporary views similar to permanent views in `AnalyzeTableCommand`?",
        "createdAt" : "2020-11-04T05:47:42Z",
        "updatedAt" : "2020-11-04T05:47:42Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      },
      {
        "id" : "bc57162c-1da6-4311-9577-01b904966884",
        "parentId" : "cae319fb-9ad7-403b-af69-c900817c4f7b",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "OK,  let's keep the current behavior for now, and improve it later.",
        "createdAt" : "2020-11-04T06:49:58Z",
        "updatedAt" : "2020-11-04T06:49:58Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "c80e8eb5828ff68a1ebab4f9ee56b55b852ba7d0",
    "line" : 31,
    "diffHunk" : "@@ -1,1 +3264,3268 @@      checkPartitionSpec()\n      AnalyzeColumn(\n        UnresolvedTableOrView(tableName),\n        Option(visitIdentifierSeq(ctx.identifierSeq())),\n        allColumns = false)"
  },
  {
    "id" : "d141991c-c884-4ea2-bcf1-ca6bc2ca6d57",
    "prId" : 30212,
    "prUrl" : "https://github.com/apache/spark/pull/30212#pullrequestreview-555947622",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "755a2cbf-6627-4c26-85d2-d486b4886ffd",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "nit: indent",
        "createdAt" : "2020-12-19T13:29:33Z",
        "updatedAt" : "2021-03-30T05:55:50Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "e78ff9c5-232b-496f-8a23-01f691f58e66",
        "parentId" : "755a2cbf-6627-4c26-85d2-d486b4886ffd",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "Done",
        "createdAt" : "2020-12-19T15:10:09Z",
        "updatedAt" : "2021-03-30T05:55:50Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "bfc03afaf9fc0974cb218b3b3121829ae9e5c413",
    "line" : 55,
    "diffHunk" : "@@ -1,1 +944,948 @@            if (groupingAnalytics != null) {\n              val groupingSets = groupingAnalytics.groupingSet.asScala\n                .map(_.expression.asScala.map(e => expression(e)).toSeq)\n              if (groupingAnalytics.CUBE != null) {\n                // CUBE(A, B, (A, B), ()) is not supported."
  },
  {
    "id" : "65d5f537-85d9-4191-9690-34ed8750fdc7",
    "prId" : 30212,
    "prUrl" : "https://github.com/apache/spark/pull/30212#pullrequestreview-621959222",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "57c87db7-5c2d-4f1e-988d-01f389079b50",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Seems we don't distinguish `(a)` and `a`, then how do we handle `GROUPING SETS(a, b, c)` and `GROUPING SETS((a), (b), (c))`?\r\n\r\nAccording to the document, `GROUPING SETS(a, b, c)` is `GROUPING SETS((a, b), (a, c), (b, c), (a), (b), (c))`.",
        "createdAt" : "2021-03-26T09:20:35Z",
        "updatedAt" : "2021-03-30T05:55:50Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "ba4aa899-3068-4b85-b5b8-49154bf6eb9d",
        "parentId" : "57c87db7-5c2d-4f1e-988d-01f389079b50",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "or is my understanding totally wrong? `a` is the same as `(a)`? Then we should update the document in https://github.com/apache/spark/pull/30212/files#diff-e895c94a06560e4db8e99be99cc7864c065c2302402dcd158cd825f3b09f878fR63",
        "createdAt" : "2021-03-26T09:33:12Z",
        "updatedAt" : "2021-03-30T05:55:50Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "27f06022-fb83-47ca-bf97-b1ed8e4c7170",
        "parentId" : "57c87db7-5c2d-4f1e-988d-01f389079b50",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> Seems we don't distinguish `(a)` and `a`, then how do we handle `GROUPING SETS(a, b, c)` and `GROUPING SETS((a), (b), (c))`?\r\n> \r\n> According to the document, `GROUPING SETS(a, b, c)` is `GROUPING SETS((a, b), (a, c), (b, c), (a), (b), (c))`.\r\n\r\n`GROUPING SETS(a, b, c)`  is same as  `GROUPING SETS((a), (b), (c))`\r\n",
        "createdAt" : "2021-03-26T09:42:16Z",
        "updatedAt" : "2021-03-30T05:55:50Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "99bb6130-beec-458d-ad02-f14989d545f9",
        "parentId" : "57c87db7-5c2d-4f1e-988d-01f389079b50",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "here the group set  depending on the parser logic right?\r\nhttps://github.com/apache/spark/blob/8945e7c52f8d09282f6847c19bd926ac4c20e11a/sql/catalyst/src/main/antlr4/org/apache/spark/sql/catalyst/parser/SqlBase.g4#L604-L611\r\n\r\nAccording to the definition, `a` can be a grouping_set, `(a)` can be a grouping_set, `(a, b)` can be a grouping_set.\r\n\r\nYou may make a miss about how expand `cube/rollup` and origin `grouping set`",
        "createdAt" : "2021-03-26T09:57:32Z",
        "updatedAt" : "2021-03-30T05:55:50Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "a0751f19-8392-4ce7-bde2-bd35f4d4d3ba",
        "parentId" : "57c87db7-5c2d-4f1e-988d-01f389079b50",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "For SQL\r\n```\r\nselect id, city, car_model, sum(quantity)\r\nfrom dealer group by grouping sets((id), (city), (car_model))\r\nUNION all\r\nselect id, city, car_model, sum(quantity)\r\nfrom dealer group by grouping sets(id, city, car_model)\r\norder by id, city, car_model\r\n```\r\nResult in PGSQL\r\n![image](https://user-images.githubusercontent.com/46485123/112615257-3b203000-8e5d-11eb-958e-fb5196c9592c.png)\r\n\r\n",
        "createdAt" : "2021-03-26T10:01:17Z",
        "updatedAt" : "2021-03-30T05:55:50Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "bfc03afaf9fc0974cb218b3b3121829ae9e5c413",
    "line" : 33,
    "diffHunk" : "@@ -1,1 +923,927 @@        // subset of the columns appearing in group by expression.\n        val groupingSets =\n          ctx.groupingSet.asScala.map(_.expression.asScala.map(e => expression(e)).toSeq)\n        Aggregate(Seq(GroupingSets(groupingSets.toSeq, groupByExpressions)),\n          selectExpressions, query)"
  },
  {
    "id" : "92a1b74c-bbce-4e6a-971e-75cf6bc5c7b7",
    "prId" : 30212,
    "prUrl" : "https://github.com/apache/spark/pull/30212#pullrequestreview-623867728",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "09689e85-42c4-4012-8988-8156cba01563",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "could you handle the two error cases separately?",
        "createdAt" : "2021-03-30T05:13:03Z",
        "updatedAt" : "2021-03-30T05:55:50Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "11c4054f-b74c-4e5b-9b2f-e6f115bf5da1",
        "parentId" : "09689e85-42c4-4012-8988-8156cba01563",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> could you handle the two error cases separately?\r\n\r\nDone",
        "createdAt" : "2021-03-30T05:56:03Z",
        "updatedAt" : "2021-03-30T05:56:04Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "bfc03afaf9fc0974cb218b3b3121829ae9e5c413",
    "line" : 81,
    "diffHunk" : "@@ -1,1 +970,974 @@      val (groupingSet, expressions) = groupByExpressions.partition(_.isInstanceOf[GroupingSet])\n      if (expressions.nonEmpty && groupingSet.nonEmpty) {\n        throw new ParseException(\"Partial CUBE/ROLLUP/GROUPING SETS like \" +\n          \"`GROUP BY a, b, CUBE(a, b)` is not supported.\",\n          ctx)"
  },
  {
    "id" : "85efcc6d-95e8-4536-b802-360b3d92abc7",
    "prId" : 29728,
    "prUrl" : "https://github.com/apache/spark/pull/29728#pullrequestreview-487685289",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5c49af63-b030-472b-87b9-3f247b8de9d2",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "How about the case, `select * from t1 where (a > 3) = (b < 5) = (c > 3)` ?",
        "createdAt" : "2020-09-14T12:06:10Z",
        "updatedAt" : "2020-09-14T12:11:52Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "d1c08607-4ff6-455b-a918-bb676789f5e8",
        "parentId" : "5c49af63-b030-472b-87b9-3f247b8de9d2",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "btw, have you check if we could update the ANTLR parser rules to prevent this case?",
        "createdAt" : "2020-09-14T12:08:19Z",
        "updatedAt" : "2020-09-14T12:11:52Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "92f1272d16383046fdb260ca4fcafcbda104a6e0",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +1328,1332 @@    val left = expression(ctx.left)\n    val right = expression(ctx.right)\n    val isUnsupportedComparison = (left, right) match {\n      case (_: Attribute, _: Predicate) => true\n      case (_: Predicate, _: Attribute) => true"
  },
  {
    "id" : "6201ddd9-56d2-4040-abd7-9737e800c58a",
    "prId" : 29728,
    "prUrl" : "https://github.com/apache/spark/pull/29728#pullrequestreview-487685289",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e3c63eb9-ffba-4977-816d-064892c272b8",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Could we check this error case at the beginning of this function?\r\n```\r\n  override def visitComparison(ctx: ComparisonContext): Expression = withOrigin(ctx) {\r\n    val left = expression(ctx.left)\r\n    val right = expression(ctx.right)\r\n    if (<Error checks>) {\r\n      throw new ParseException(\"Syntax error at or near\", ctx)\r\n    }\r\n    val operator = ctx.comparisonOperator().getChild(0).asInstanceOf[TerminalNode]\r\n    operator.getSymbol.getType match {\r\n      case SqlBaseParser.EQ =>\r\n        EqualTo(left, right)\r\n      case SqlBaseParser.NSEQ =>\r\n        EqualNullSafe(left, right)\r\n      case SqlBaseParser.NEQ | SqlBaseParser.NEQJ =>\r\n        Not(EqualTo(left, right))\r\n        ...\r\n      case SqlBaseParser.GTE =>\r\n        GreaterThanOrEqual(left, right)\r\n    }\r\n```",
        "createdAt" : "2020-09-14T12:11:16Z",
        "updatedAt" : "2020-09-14T12:11:52Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "92f1272d16383046fdb260ca4fcafcbda104a6e0",
    "line" : 33,
    "diffHunk" : "@@ -1,1 +1350,1354 @@        GreaterThanOrEqual(left, right)\n      case _ =>\n        throw new ParseException(\"Syntax error at or near\", ctx)\n    }\n  }"
  },
  {
    "id" : "02f7c95b-ea58-4de7-9473-07803a929453",
    "prId" : 29728,
    "prUrl" : "https://github.com/apache/spark/pull/29728#pullrequestreview-487776547",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "31a3a67f-ef75-4789-b762-855c6eef3caa",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Total nit, but isn't it clearer to write `isSupportedComparison` and avoid inverting the logic everywhere?",
        "createdAt" : "2020-09-14T13:37:30Z",
        "updatedAt" : "2020-09-14T13:37:30Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      }
    ],
    "commit" : "92f1272d16383046fdb260ca4fcafcbda104a6e0",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +1328,1332 @@    val left = expression(ctx.left)\n    val right = expression(ctx.right)\n    val isUnsupportedComparison = (left, right) match {\n      case (_: Attribute, _: Predicate) => true\n      case (_: Predicate, _: Attribute) => true"
  },
  {
    "id" : "a834190a-2007-41ca-8ecc-83a5f0d1163e",
    "prId" : 29198,
    "prUrl" : "https://github.com/apache/spark/pull/29198#pullrequestreview-456497879",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a35b5421-ed2b-43ef-a98c-b493ec5cc1a3",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "will analyzer give a nice error if `UnresolvedFunc` is left to `CheckAnalysis`?",
        "createdAt" : "2020-07-28T10:18:09Z",
        "updatedAt" : "2020-07-28T16:26:35Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "674689de-cefe-412f-804b-4d61003be16d",
        "parentId" : "a35b5421-ed2b-43ef-a98c-b493ec5cc1a3",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "nvm, this won't happen for now.",
        "createdAt" : "2020-07-28T10:19:28Z",
        "updatedAt" : "2020-07-28T16:26:35Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "c662619bd00e74d4f683fcb565868bf72e4890d6",
    "line" : 29,
    "diffHunk" : "@@ -1,1 +3633,3637 @@    val functionName = visitMultipartIdentifier(ctx.multipartIdentifier)\n    DropFunction(\n      UnresolvedFunc(functionName),\n      ctx.EXISTS != null,\n      ctx.TEMPORARY != null)"
  },
  {
    "id" : "fc965b93-791e-478b-a364-c37ea4084e50",
    "prId" : 29087,
    "prUrl" : "https://github.com/apache/spark/pull/29087#pullrequestreview-561508268",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bea8da5b-c93b-46ad-aae8-f41776c8cff2",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "In hive, one cannot use `distinct` for TRANSFORM? e.g., `SELECT TRANSFORM(distinct a, b)`?",
        "createdAt" : "2021-01-05T00:31:17Z",
        "updatedAt" : "2021-03-25T08:21:50Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "9a0fa32d-02de-4cff-b372-2754a3201f2f",
        "parentId" : "bea8da5b-c93b-46ad-aae8-f41776c8cff2",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> In hive, one cannot use `distinct` for TRANSFORM? e.g., `SELECT TRANSFORM(distinct a, b)`?\r\n\r\nNot support. Do we need to support this?\r\n```\r\nhive>\r\n    >\r\n    > select transform(distinct a) using 'cat' from (select 1 as a) temp;\r\nNoViableAltException(96@[104:1: selectExpression : ( ( tableAllColumns )=> tableAllColumns | expression );])\r\n\tat org.apache.hadoop.hive.ql.parse.HiveParser_SelectClauseParser$DFA19.specialStateTransition(HiveParser_SelectClauseParser.java:5380)\r\n\tat org.antlr.runtime.DFA.predict(DFA.java:80)\r\n\tat org.apache.hadoop.hive.ql.parse.HiveParser_SelectClauseParser.selectExpression(HiveParser_SelectClauseParser.java:2215)\r\n\tat org.apache.hadoop.hive.ql.parse.HiveParser_SelectClauseParser.selectExpressionList(HiveParser_SelectClauseParser.java:2297)\r\n\tat org.apache.hadoop.hive.ql.parse.HiveParser_SelectClauseParser.selectTrfmClause(HiveParser_SelectClauseParser.java:1299)\r\n\tat org.apache.hadoop.hive.ql.parse.HiveParser_SelectClauseParser.selectClause(HiveParser_SelectClauseParser.java:968)\r\n\tat org.apache.hadoop.hive.ql.parse.HiveParser.selectClause(HiveParser.java:41964)\r\n\tat org.apache.hadoop.hive.ql.parse.HiveParser.atomSelectStatement(HiveParser.java:36720)\r\n\tat org.apache.hadoop.hive.ql.parse.HiveParser.selectStatement(HiveParser.java:36987)\r\n\tat org.apache.hadoop.hive.ql.parse.HiveParser.regularBody(HiveParser.java:36633)\r\n\tat org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpressionBody(HiveParser.java:35822)\r\n\tat org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpression(HiveParser.java:35710)\r\n\tat org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:2284)\r\n\tat org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1333)\r\n\tat org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:208)\r\n\tat org.apache.hadoop.hive.ql.parse.ParseUtils.parse(ParseUtils.java:77)\r\n\tat org.apache.hadoop.hive.ql.parse.ParseUtils.parse(ParseUtils.java:70)\r\n\tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:468)\r\n\tat org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1317)\r\n\tat org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1457)\r\n\tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:1237)\r\n\tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:1227)\r\n\tat org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:233)\r\n\tat org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:184)\r\n\tat org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:403)\r\n\tat org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:821)\r\n\tat org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:759)\r\n\tat org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:686)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:606)\r\n\tat org.apache.hadoop.util.RunJar.run(RunJar.java:221)\r\n\tat org.apache.hadoop.util.RunJar.main(RunJar.java:136)\r\nFAILED: ParseException line 1:17 cannot recognize input near 'distinct' 'a' ')' in select expression\r\n```",
        "createdAt" : "2021-01-05T02:12:12Z",
        "updatedAt" : "2021-03-25T08:21:51Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "28f7201a-fece-4b70-a2ab-c2b7138ee7dd",
        "parentId" : "bea8da5b-c93b-46ad-aae8-f41776c8cff2",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "I think we don't need to support it in this PR and its okay to just follow the hive behaviour.",
        "createdAt" : "2021-01-05T03:07:52Z",
        "updatedAt" : "2021-03-25T08:21:51Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "8c5b295a-ed3b-4526-b03c-528635a29050",
        "parentId" : "bea8da5b-c93b-46ad-aae8-f41776c8cff2",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> I think we don't need to support it in this PR and its okay to just follow the hive behaviour.\r\n\r\nYea",
        "createdAt" : "2021-01-05T04:34:00Z",
        "updatedAt" : "2021-03-25T08:21:51Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "12787053aec9d015506d5c59c58e91dd23d5bb82",
    "line" : 63,
    "diffHunk" : "@@ -1,1 +684,688 @@      havingClause,\n      windowClause,\n      isDistinct = false)\n\n    ScriptTransformation("
  },
  {
    "id" : "2f8bba72-6fb3-4a86-8c18-559f34ca5adb",
    "prId" : 29087,
    "prUrl" : "https://github.com/apache/spark/pull/29087#pullrequestreview-561580503",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2a63b793-4d54-44ad-8db3-08bd93cbeea2",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "I thinks its better to leave comments about how the analyzer handle this `UnresolvedStar` later.",
        "createdAt" : "2021-01-05T07:51:43Z",
        "updatedAt" : "2021-03-25T08:21:51Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "9b398acb-23b6-40ee-94d9-3b2f64bcb9f4",
        "parentId" : "2a63b793-4d54-44ad-8db3-08bd93cbeea2",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "Done",
        "createdAt" : "2021-01-05T08:03:11Z",
        "updatedAt" : "2021-03-25T08:21:51Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "12787053aec9d015506d5c59c58e91dd23d5bb82",
    "line" : 68,
    "diffHunk" : "@@ -1,1 +688,692 @@    ScriptTransformation(\n      // TODO: Remove this logic and see SPARK-34035\n      Seq(UnresolvedStar(None)),\n      string(transformClause.script),\n      attributes,"
  },
  {
    "id" : "24af0c9f-cce8-4e58-ae2c-5796b54044e0",
    "prId" : 29087,
    "prUrl" : "https://github.com/apache/spark/pull/29087#pullrequestreview-563169948",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "001e3602-2903-49db-81b0-370cb21bddf8",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "hm, on second thought, we cannot remove this param `input` from `ScriptTransformation` in this PR? Since the input exprs of the current `ScriptTransformation` implementaiton always coms from child's output, IIUC we don't need this param anymore?",
        "createdAt" : "2021-01-06T13:45:47Z",
        "updatedAt" : "2021-03-25T08:21:51Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "ac13bf02-9085-4b67-b768-8abe6d74d6e4",
        "parentId" : "001e3602-2903-49db-81b0-370cb21bddf8",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> hm, on second thought, we cannot remove this param `input` from `ScriptTransformation` in this PR? Since the input exprs of the current `ScriptTransformation` implementaiton always coms from child's output, IIUC we don't need this param anymore?\r\n\r\nIt looks like this. We can replace `input` with `child.output` directly. That's a really nice suggestion since current way(converting LogicalPlan it looks a little weird). If we remove this `input` parameter and add correct  comment.\r\nThe whole process looks more natural.  I have tried it in local, a new big diff. How about a new ticket about refactor this? ",
        "createdAt" : "2021-01-06T14:39:56Z",
        "updatedAt" : "2021-03-25T08:21:51Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "b17d758b-0d85-4dcb-be08-aa8626cd4b7a",
        "parentId" : "001e3602-2903-49db-81b0-370cb21bddf8",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Nice. Just removing the param causes a big diff? I'd like to remove[ the current the weird Analyzer code ](https://github.com/apache/spark/pull/29087/files#diff-ed19f376a63eba52eea59ca71f3355d4495fad4fad4db9a3324aade0d4986a47R1520-R1526) to handle the unresolved star in this PR though.",
        "createdAt" : "2021-01-06T23:44:51Z",
        "updatedAt" : "2021-03-25T08:21:51Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "07ca28c3-e633-40c7-b1b6-9583b7bc92d5",
        "parentId" : "001e3602-2903-49db-81b0-370cb21bddf8",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> Nice. Just removing the param causes a big diff? I'd like to remove[ the current the weird Analyzer code ](https://github.com/apache/spark/pull/29087/files#diff-ed19f376a63eba52eea59ca71f3355d4495fad4fad4db9a3324aade0d4986a47R1520-R1526) to handle the unresolved star in this PR though.\r\n\r\nYea, will change a lot of file.  Compared to the origin code here(the current the weird Analyzer code) ï¼Œcurrent change seem s not so weird, but  for the whole process, really weird.\r\n\r\nCreate a ticket for this https://issues.apache.org/jira/browse/SPARK-34035\r\n![image](https://user-images.githubusercontent.com/46485123/103833742-afe49900-50bc-11eb-979a-b610582383d0.png)\r\n",
        "createdAt" : "2021-01-06T23:53:41Z",
        "updatedAt" : "2021-03-25T08:21:51Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "325f29b9-cd6c-40c1-8a9b-e48cfb63f7aa",
        "parentId" : "001e3602-2903-49db-81b0-370cb21bddf8",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "> Yea, will change a lot of file. \r\n\r\nHow many changed lines of codes there?",
        "createdAt" : "2021-01-07T00:36:27Z",
        "updatedAt" : "2021-03-25T08:21:51Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "1f5f00df-e95b-4336-bb82-fd674b75e921",
        "parentId" : "001e3602-2903-49db-81b0-370cb21bddf8",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> > Yea, will change a lot of file.\r\n> \r\n> How many changed lines of codes there?\r\n\r\nNearly 100 lines, most of changes are about UT",
        "createdAt" : "2021-01-07T01:27:50Z",
        "updatedAt" : "2021-03-25T08:21:51Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "d2e5394b-46c7-44b1-9294-08e22359e74d",
        "parentId" : "001e3602-2903-49db-81b0-370cb21bddf8",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "okay, I don't have a strong opinion on it, so please follow other reviewer's comment.",
        "createdAt" : "2021-01-07T01:42:39Z",
        "updatedAt" : "2021-03-25T08:21:51Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "5c1d508f-f4cd-414c-871f-86f7567e8f74",
        "parentId" : "001e3602-2903-49db-81b0-370cb21bddf8",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> okay, I don't have a strong opinion on it, so please follow other reviewer's comment.\r\n\r\nYea",
        "createdAt" : "2021-01-07T02:24:07Z",
        "updatedAt" : "2021-03-25T08:21:51Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "12787053aec9d015506d5c59c58e91dd23d5bb82",
    "line" : 68,
    "diffHunk" : "@@ -1,1 +688,692 @@    ScriptTransformation(\n      // TODO: Remove this logic and see SPARK-34035\n      Seq(UnresolvedStar(None)),\n      string(transformClause.script),\n      attributes,"
  },
  {
    "id" : "1064ce60-f72b-4e1c-9289-95b7b0be0d13",
    "prId" : 29085,
    "prUrl" : "https://github.com/apache/spark/pull/29085#pullrequestreview-453357346",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4f6a20cb-a931-4465-83a4-d6d149784a1f",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Plz add tests in `PlanParserSuite`",
        "createdAt" : "2020-07-22T12:05:35Z",
        "updatedAt" : "2020-07-23T03:08:37Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "29bd62ab-3342-45fd-a6a2-e0becd8bcd74",
        "parentId" : "4f6a20cb-a931-4465-83a4-d6d149784a1f",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> Plz add tests in `PlanParserSuite`\r\n\r\nDone",
        "createdAt" : "2020-07-22T14:11:58Z",
        "updatedAt" : "2020-07-23T03:08:37Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "03d3409f6ca641a4f10fdc2ac71479445220f676",
    "line" : 30,
    "diffHunk" : "@@ -1,1 +770,774 @@   * Create a [[ScriptInputOutputSchema]].\n   */\n  protected def withScriptIOSchema(\n      ctx: ParserRuleContext,\n      inRowFormat: RowFormatContext,"
  },
  {
    "id" : "880db780-a8cf-4742-91d4-9f4f2f24588e",
    "prId" : 29022,
    "prUrl" : "https://github.com/apache/spark/pull/29022#pullrequestreview-444439479",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "62103494-8b8d-4cf0-b160-ad2f995327cc",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Could you add a function description like the other functions around here, @yaooqinn ?",
        "createdAt" : "2020-07-08T04:12:20Z",
        "updatedAt" : "2020-07-08T06:13:01Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "23fd096e-c4e4-4fbf-8fbc-640ee6e466bf",
        "parentId" : "62103494-8b8d-4cf0-b160-ad2f995327cc",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "done, thanks!",
        "createdAt" : "2020-07-08T06:13:26Z",
        "updatedAt" : "2020-07-08T06:13:27Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "6e422ff503d03191d47d44d6659fe9a6027a4fd9",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +2034,2038 @@   * Create a Float Literal expression.\n   */\n  override def visitFloatLiteral(ctx: FloatLiteralContext): Literal = {\n    val rawStrippedQualifier = ctx.getText.substring(0, ctx.getText.length - 1)\n    numericLiteral(ctx, rawStrippedQualifier,"
  },
  {
    "id" : "a33fde68-e5c7-471f-86be-c9912cca8a21",
    "prId" : 28935,
    "prUrl" : "https://github.com/apache/spark/pull/28935#pullrequestreview-438885052",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "90487e31-1e43-4d08-b8a9-cbf48550ae7d",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "We can just reuse `NullType`. We should forbid creating table with null type completely, including `spark.catalog.createTable`.",
        "createdAt" : "2020-06-29T04:53:29Z",
        "updatedAt" : "2020-06-30T01:45:25Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "3fa76cfaf7aaca5c5d2ca8ad2743bdefae208b61",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +2213,2217 @@        DecimalType(precision.getText.toInt, scale.getText.toInt)\n      case (\"interval\", Nil) => CalendarIntervalType\n      case (\"void\", Nil) => HiveVoidType\n      case (dt, params) =>\n        val dtStr = if (params.nonEmpty) s\"$dt(${params.mkString(\",\")})\" else dt"
  },
  {
    "id" : "b3279b34-a8ac-4557-94e1-94d50a76cfd7",
    "prId" : 28935,
    "prUrl" : "https://github.com/apache/spark/pull/28935#pullrequestreview-440486121",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "27400703-1d4c-4699-86a5-4c2ced96d9c6",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I don't get it why we need `HiveVoidType`. What happens if we just parse `void` to `NullType`?",
        "createdAt" : "2020-06-30T07:01:11Z",
        "updatedAt" : "2020-06-30T07:01:11Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "8fd7ccc7-5915-4af1-9e28-e97f553600c9",
        "parentId" : "27400703-1d4c-4699-86a5-4c2ced96d9c6",
        "authorId" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "body" : "Because that could indicate VOID is a Hive type, the handle processing is more unified. Or, we can just use the PR #28833 ",
        "createdAt" : "2020-06-30T08:23:15Z",
        "updatedAt" : "2020-06-30T08:23:15Z",
        "lastEditedBy" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "tags" : [
        ]
      },
      {
        "id" : "08d394fe-31df-43bc-b6de-b5ad56e6235d",
        "parentId" : "27400703-1d4c-4699-86a5-4c2ced96d9c6",
        "authorId" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "body" : "For example,  below function will point the failure is due to the legacy hive void type. If we mix VOID and NULL, I am not sure it would be better than separation.\r\n```scala\r\n  def failVoidType(dt: DataType): Unit = {\r\n    if (HiveVoidType.containsVoidType(dt)) {\r\n      throw new AnalysisException(\r\n        \"Cannot create tables with Hive VOID type.\")\r\n    }\r\n  }\r\n```",
        "createdAt" : "2020-06-30T09:35:01Z",
        "updatedAt" : "2020-06-30T09:35:01Z",
        "lastEditedBy" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "tags" : [
        ]
      },
      {
        "id" : "5e2c11ab-1400-4e44-b345-83b7736207b9",
        "parentId" : "27400703-1d4c-4699-86a5-4c2ced96d9c6",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "VOID and NULL are indeed the same type. We can just check null type and fail with error message: `Cannot create tables with VOID type`",
        "createdAt" : "2020-06-30T15:13:59Z",
        "updatedAt" : "2020-06-30T15:13:59Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "cc3c0595-ce3c-4a25-8a9d-dd8c46ce95cc",
        "parentId" : "27400703-1d4c-4699-86a5-4c2ced96d9c6",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "The point is consistency: The VOID type in SQL statement should be the same as `NullType` specified by Scala API in `spark.catalog.createTable`.",
        "createdAt" : "2020-06-30T15:18:26Z",
        "updatedAt" : "2020-06-30T15:18:26Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "eb3383be-0613-4d3f-a1f5-465d41cbbff8",
        "parentId" : "27400703-1d4c-4699-86a5-4c2ced96d9c6",
        "authorId" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "body" : "@cloud-fan , ok. I will follow your suggestion to fix it in #28833 , since this PR is a refator with new type `HiveVoidType`. Now we don't need it.",
        "createdAt" : "2020-07-01T01:18:23Z",
        "updatedAt" : "2020-07-01T01:18:23Z",
        "lastEditedBy" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "tags" : [
        ]
      }
    ],
    "commit" : "3fa76cfaf7aaca5c5d2ca8ad2743bdefae208b61",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +2185,2189 @@   */\n  private def visitSparkDataType(ctx: DataTypeContext): DataType = {\n    HiveVoidType.replaceVoidType(HiveStringType.replaceCharType(typedVisit(ctx)))\n  }\n"
  },
  {
    "id" : "de7b5ad2-8dee-40a8-b4b0-c571156eca71",
    "prId" : 28875,
    "prUrl" : "https://github.com/apache/spark/pull/28875#pullrequestreview-438913830",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cd5e17d6-b2c7-4b6e-98a3-62662c3737e2",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "we can still write `.children.isEmpty`, then we don't need to change `v2Commands.scala`",
        "createdAt" : "2020-06-23T04:17:12Z",
        "updatedAt" : "2020-06-28T03:03:06Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "defe3825-dbf7-4c6a-b886-e0494753aeba",
        "parentId" : "cd5e17d6-b2c7-4b6e-98a3-62662c3737e2",
        "authorId" : "8afcf008-6988-44a5-92fa-2bef54bb4058",
        "body" : "I don't think so, because the `children` of `InsertAction` and `UpdateAction` actually include `condition` and `assignments`. There may be cases where there're `assignments` and `condition` being ignored but `children` is nonEmpty.",
        "createdAt" : "2020-06-23T06:59:41Z",
        "updatedAt" : "2020-06-28T03:03:06Z",
        "lastEditedBy" : "8afcf008-6988-44a5-92fa-2bef54bb4058",
        "tags" : [
        ]
      },
      {
        "id" : "90afe381-e5a5-4448-80b8-f649a4c996b5",
        "parentId" : "cd5e17d6-b2c7-4b6e-98a3-62662c3737e2",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "then this is an existing bug?",
        "createdAt" : "2020-06-23T11:38:00Z",
        "updatedAt" : "2020-06-28T03:03:06Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "dc28e587-cf22-4380-8717-9fa90f21d925",
        "parentId" : "cd5e17d6-b2c7-4b6e-98a3-62662c3737e2",
        "authorId" : "8afcf008-6988-44a5-92fa-2bef54bb4058",
        "body" : "Yes, it was a bug.",
        "createdAt" : "2020-06-28T03:08:56Z",
        "updatedAt" : "2020-06-28T03:08:57Z",
        "lastEditedBy" : "8afcf008-6988-44a5-92fa-2bef54bb4058",
        "tags" : [
        ]
      },
      {
        "id" : "bb94071b-cc89-4710-9928-7e110ee02135",
        "parentId" : "cd5e17d6-b2c7-4b6e-98a3-62662c3737e2",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "can you send a new PR against branch 3.0 to fix this bug?",
        "createdAt" : "2020-06-29T02:24:04Z",
        "updatedAt" : "2020-06-29T02:26:19Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "819d5157-0f39-478c-bea1-3572751f5c72",
        "parentId" : "cd5e17d6-b2c7-4b6e-98a3-62662c3737e2",
        "authorId" : "8afcf008-6988-44a5-92fa-2bef54bb4058",
        "body" : "Submitted a pr at https://github.com/apache/spark/pull/28943.",
        "createdAt" : "2020-06-29T06:22:13Z",
        "updatedAt" : "2020-06-29T06:22:13Z",
        "lastEditedBy" : "8afcf008-6988-44a5-92fa-2bef54bb4058",
        "tags" : [
        ]
      }
    ],
    "commit" : "d5edef3c2b950440614fc5c9ee1e770bcd0b9884",
    "line" : 40,
    "diffHunk" : "@@ -1,1 +460,464 @@    // children being empty means that the condition is not set\n    val matchedActionSize = matchedActions.length\n    if (matchedActionSize >= 2 && !matchedActions.init.forall(_.condition.nonEmpty)) {\n      throw new ParseException(\"When there are more than one MATCHED clauses in a MERGE \" +\n          \"statement, only the last MATCHED clause can omit the condition.\", ctx)"
  },
  {
    "id" : "16b92101-c46a-48b3-a83b-f48cbf3b7a30",
    "prId" : 28833,
    "prUrl" : "https://github.com/apache/spark/pull/28833#pullrequestreview-430519164",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7be89ab7-72e0-4f8b-bace-039fd7454ece",
        "parentId" : null,
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "If change here and `NullType`, spark will also support `void` type for table. Is it needed ?",
        "createdAt" : "2020-06-15T09:42:52Z",
        "updatedAt" : "2020-07-08T00:27:04Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      },
      {
        "id" : "8672848c-d93e-439f-9610-59f2600391ab",
        "parentId" : "7be89ab7-72e0-4f8b-bace-039fd7454ece",
        "authorId" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "body" : "Just re-submit this PR since I don't know how to reopen. It's based on the latest discussions in #17953. Don't worry, we can close this if the community thinks the changes worthless to do.",
        "createdAt" : "2020-06-15T10:32:49Z",
        "updatedAt" : "2020-07-08T00:27:04Z",
        "lastEditedBy" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "tags" : [
        ]
      }
    ],
    "commit" : "9ad57d17bac47ea0f801004ec0aba9197e631bc7",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +2212,2216 @@      case (\"decimal\" | \"dec\" | \"numeric\", precision :: scale :: Nil) =>\n        DecimalType(precision.getText.toInt, scale.getText.toInt)\n      case (\"void\", Nil) => NullType\n      case (\"interval\", Nil) => CalendarIntervalType\n      case (dt, params) =>"
  },
  {
    "id" : "67616d52-c749-4379-a636-9012fcdde591",
    "prId" : 28501,
    "prUrl" : "https://github.com/apache/spark/pull/28501#pullrequestreview-412353658",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "309160e6-c7db-4a1f-989d-b4dfdb2970c8",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "does global aggregate still work? e.g. `UnresolvedHaving(Project(agg_func ...))`",
        "createdAt" : "2020-05-14T13:18:23Z",
        "updatedAt" : "2020-05-15T13:39:06Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "137c0b25-d60d-4c8c-994c-377f72df6663",
        "parentId" : "309160e6-c7db-4a1f-989d-b4dfdb2970c8",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "Yes, it still works, the `UnresolvedHaving` will be changed to Filter in rule `ResolveAggregateFunction`.",
        "createdAt" : "2020-05-15T05:18:54Z",
        "updatedAt" : "2020-05-15T13:39:07Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "1de0c759a90c7d868484b448289bef9c3865ab5d",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +630,634 @@      case e => Cast(e, BooleanType)\n    }\n    UnresolvedHaving(predicate, plan)\n  }\n"
  }
]