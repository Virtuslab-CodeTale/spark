[
  {
    "id" : "98429e3e-4bb2-482a-a107-0b5d7cd0ffbf",
    "prId" : 32761,
    "prUrl" : "https://github.com/apache/spark/pull/32761#pullrequestreview-676530119",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "675f2acd-9448-4a16-ad35-46e65f0a4b16",
        "parentId" : null,
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "Since this is only used in `TypeCoercion.scala`, shall we keep it there?",
        "createdAt" : "2021-06-04T08:42:35Z",
        "updatedAt" : "2021-06-04T08:42:36Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "bb4ebf44-cd23-45f4-b7f0-afadb619bb90",
        "parentId" : "675f2acd-9448-4a16-ad35-46e65f0a4b16",
        "authorId" : "8b518862-583d-43a2-a10e-b33ef1b6d824",
        "body" : "(1) we probably don't want to expose markRuleAsIneffective to the outside of trees? If it can be used arbitrarily in a rule, the behavior would be harder to reason.\r\n(2) resolveOperatorUpWithPruning can potentially call it.  I can do it in a separate PR.",
        "createdAt" : "2021-06-04T16:57:47Z",
        "updatedAt" : "2021-06-04T16:57:47Z",
        "lastEditedBy" : "8b518862-583d-43a2-a10e-b33ef1b6d824",
        "tags" : [
        ]
      },
      {
        "id" : "3961fcde-830e-4079-b0c8-434872e0d03e",
        "parentId" : "675f2acd-9448-4a16-ad35-46e65f0a4b16",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "ok",
        "createdAt" : "2021-06-04T17:43:40Z",
        "updatedAt" : "2021-06-04T17:43:40Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "6c9cd28ac10163cc4878a13b47e43b6c0af92465",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +567,571 @@   *               varying initial state for different invocations.\n   */\n  def transformUpWithBeforeAndAfterRuleOnChildren(\n      cond: BaseType => Boolean, ruleId: RuleId = UnknownRuleId)(\n    rule: PartialFunction[(BaseType, BaseType), BaseType]): BaseType = {"
  },
  {
    "id" : "35d89db7-0002-4b3a-a81c-36c6253dc92c",
    "prId" : 32557,
    "prUrl" : "https://github.com/apache/spark/pull/32557#pullrequestreview-661916719",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0d22e735-68dc-4acf-9e32-f1c7ba218fad",
        "parentId" : null,
        "authorId" : "6b53d819-334a-4038-a2b8-66582f8c76a7",
        "body" : "The `TreeNode` inherits `Product`, I'm wondering if only one check should be kept.\r\n\r\nMaybe this change is redundant, the following one which changes `case p if p.productIterator.exists(_.isInstanceOf[TreeNode[_]]) => true` is the key one.\r\n\r\n",
        "createdAt" : "2021-05-18T02:13:02Z",
        "updatedAt" : "2021-05-18T02:20:45Z",
        "lastEditedBy" : "6b53d819-334a-4038-a2b8-66582f8c76a7",
        "tags" : [
        ]
      },
      {
        "id" : "3dd0ca2a-68d8-480c-9534-b8c2491c6ed2",
        "parentId" : "0d22e735-68dc-4acf-9e32-f1c7ba218fad",
        "authorId" : "cd552757-c9c9-412a-9565-ff23341cdb66",
        "body" : "Nice catch, I think we can remove the TreeNode check. The code change is for Seq,  without this Seq[Product] will be written as null.",
        "createdAt" : "2021-05-18T10:37:11Z",
        "updatedAt" : "2021-05-18T10:37:11Z",
        "lastEditedBy" : "cd552757-c9c9-412a-9565-ff23341cdb66",
        "tags" : [
        ]
      }
    ],
    "commit" : "c307dd0c0c65ceefec69c9612ede4e8ee7670515",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +1001,1005 @@    case t: Seq[_] if t.forall(_.isInstanceOf[Partitioning]) ||\n      t.forall(_.isInstanceOf[DataType]) ||\n      t.forall(_.isInstanceOf[Product]) =>\n      JArray(t.map(parseToJson).toList)\n    case t: Seq[_] if t.length > 0 && t.head.isInstanceOf[String] =>"
  },
  {
    "id" : "fc0ff9bb-1459-49db-84c5-698d1d4bf16c",
    "prId" : 32249,
    "prUrl" : "https://github.com/apache/spark/pull/32249#pullrequestreview-639674506",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b248c448-728f-43fe-a457-0bf5fd28f5ee",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "is double \"this\" a typo?",
        "createdAt" : "2021-04-20T08:32:59Z",
        "updatedAt" : "2021-04-20T10:32:55Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "7c52bd01-ba0e-4edd-9871-5c238db9cf74",
        "parentId" : "b248c448-728f-43fe-a457-0bf5fd28f5ee",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "it's two sentences ðŸ˜‚",
        "createdAt" : "2021-04-20T08:43:38Z",
        "updatedAt" : "2021-04-20T10:32:55Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "b44429dd4e1e5b70bb922cb279b489337af12886",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +76,80 @@  def withOrigin[A](o: Origin)(f: => A): A = {\n    // remember the previous one so it can be reset to this\n    // this way withOrigin can be recursive\n    val previous = get\n    set(o)"
  },
  {
    "id" : "0db994d4-6343-4f67-af9b-77ee5618de79",
    "prId" : 32060,
    "prUrl" : "https://github.com/apache/spark/pull/32060#pullrequestreview-629692656",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ad7c6ff9-d147-443d-a4d9-9b153caf879a",
        "parentId" : null,
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "ditto",
        "createdAt" : "2021-04-07T06:50:24Z",
        "updatedAt" : "2021-04-12T00:32:43Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "431fa7a4-2f6b-4eb0-b8ad-454efaf35881",
        "parentId" : "ad7c6ff9-d147-443d-a4d9-9b153caf879a",
        "authorId" : "8b518862-583d-43a2-a10e-b33ef1b6d824",
        "body" : "The same reason as above.",
        "createdAt" : "2021-04-07T07:47:58Z",
        "updatedAt" : "2021-04-12T00:32:43Z",
        "lastEditedBy" : "8b518862-583d-43a2-a10e-b33ef1b6d824",
        "tags" : [
        ]
      }
    ],
    "commit" : "718a92a743e731a698473b9170ace55187755b55",
    "line" : 45,
    "diffHunk" : "@@ -1,1 +108,112 @@    val bits: BitSet = new BitSet(TreePattern.maxId)\n    // Propagate node pattern bits\n    val nodePatternIterator = nodePatterns.iterator\n    while (nodePatternIterator.hasNext) {\n      bits.set(nodePatternIterator.next().id)"
  },
  {
    "id" : "4d318cba-2b3c-4d78-bfb1-cfd0fce98387",
    "prId" : 32060,
    "prUrl" : "https://github.com/apache/spark/pull/32060#pullrequestreview-629693761",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "19e6228a-47e4-4f4d-b411-1f064d54711c",
        "parentId" : null,
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "ditto",
        "createdAt" : "2021-04-07T06:50:29Z",
        "updatedAt" : "2021-04-12T00:32:43Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "fea5c547-c970-498d-8fa7-cec0236b15e1",
        "parentId" : "19e6228a-47e4-4f4d-b411-1f064d54711c",
        "authorId" : "8b518862-583d-43a2-a10e-b33ef1b6d824",
        "body" : "The same reason as above.",
        "createdAt" : "2021-04-07T07:49:13Z",
        "updatedAt" : "2021-04-12T00:32:43Z",
        "lastEditedBy" : "8b518862-583d-43a2-a10e-b33ef1b6d824",
        "tags" : [
        ]
      }
    ],
    "commit" : "718a92a743e731a698473b9170ace55187755b55",
    "line" : 50,
    "diffHunk" : "@@ -1,1 +113,117 @@    }\n    // Propagate children's pattern bits\n    val childIterator = children.iterator\n    while (childIterator.hasNext) {\n      bits.union(childIterator.next().treePatternBits)"
  },
  {
    "id" : "83d3e8ae-4dbf-472e-b394-463107493c55",
    "prId" : 32030,
    "prUrl" : "https://github.com/apache/spark/pull/32030#pullrequestreview-631669375",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "43826742-daed-4238-b594-ac695841b7fc",
        "parentId" : null,
        "authorId" : "8b518862-583d-43a2-a10e-b33ef1b6d824",
        "body" : "Does it make sense to call legacyWithNewChildren or use its code in the default implementation so that subclasses do not have to call legacyWithNewChildren in withNewChildrenInternal?",
        "createdAt" : "2021-04-08T17:17:43Z",
        "updatedAt" : "2021-04-09T10:49:42Z",
        "lastEditedBy" : "8b518862-583d-43a2-a10e-b33ef1b6d824",
        "tags" : [
        ]
      },
      {
        "id" : "61cdf530-fca8-4442-9af8-f42319cdf73c",
        "parentId" : "43826742-daed-4238-b594-ac695841b7fc",
        "authorId" : "def2dd9f-ce27-41ce-8156-f8225ddc68a9",
        "body" : "Having a default implementation will lead to people who add new expressions don't implement `withNewChildrenInternal` and we again be back to the same situation having many slow `withNewChildren` implementations, so I prefer to make have it like this to enforce `withNewChildrenInternal` implementation. Actually, even now, there are two expressions added to the master and I need to update this PR to implement the `withNewChildrenInternal` for them. The `legacyWithNewChildren` is here for a transition period, we have some expressions that are a bit hard to write `withNewChildrenInternal` for and probably need some refactoring. The goal is to remove `legacyWithNewChildren` altogether at some point.",
        "createdAt" : "2021-04-08T17:29:16Z",
        "updatedAt" : "2021-04-09T10:49:42Z",
        "lastEditedBy" : "def2dd9f-ce27-41ce-8156-f8225ddc68a9",
        "tags" : [
        ]
      },
      {
        "id" : "38d52c4c-b642-4a0e-ba90-cf55f6f76d92",
        "parentId" : "43826742-daed-4238-b594-ac695841b7fc",
        "authorId" : "8b518862-583d-43a2-a10e-b33ef1b6d824",
        "body" : "Ok, makes sense. Thanks for the elaboration!",
        "createdAt" : "2021-04-08T17:36:26Z",
        "updatedAt" : "2021-04-09T10:49:42Z",
        "lastEditedBy" : "8b518862-583d-43a2-a10e-b33ef1b6d824",
        "tags" : [
        ]
      }
    ],
    "commit" : "5bc0ff7e88ec11caa06b3c786bcdea9a01ddfcaf",
    "line" : 41,
    "diffHunk" : "@@ -1,1 +284,288 @@  }\n\n  protected def withNewChildrenInternal(newChildren: IndexedSeq[BaseType]): BaseType\n\n  /**"
  },
  {
    "id" : "0e56da2a-5413-4392-b9f1-07aa8eab6479",
    "prId" : 29593,
    "prUrl" : "https://github.com/apache/spark/pull/29593#pullrequestreview-482062867",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4f4a886f-2aac-4b6b-926d-929cfabfe56b",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "thinking about it more, I think we missed one case of tree node tags. If the rule replaces one node with a new one, it makes sense to copy the tags. If the rule removes one or more nodes, it doesn't make sense to append the tags to an existing node.\r\n\r\nHowever, it's too expensive to detect the removal action (you need to traverse the tree and see if the new node is a descendant of the original node). I think a compromise is\r\n```\r\nif (tags.isEmpty) {\r\n  tags ++= other.tags\r\n}\r\n```\r\n\r\nIt's still possible that we copy tags to an existing node due to a node removal rule, but it's not a big issue if that node has no tags at all.",
        "createdAt" : "2020-09-02T11:58:32Z",
        "updatedAt" : "2020-09-04T07:00:39Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "d7ab2c02-9851-4339-b6c9-5b7af9c6a028",
        "parentId" : "4f4a886f-2aac-4b6b-926d-929cfabfe56b",
        "authorId" : "334a081c-cba1-403e-ac80-30e6124077d1",
        "body" : "looks good. @maryannxue any concerns with this approach ?",
        "createdAt" : "2020-09-03T12:52:17Z",
        "updatedAt" : "2020-09-04T07:00:39Z",
        "lastEditedBy" : "334a081c-cba1-403e-ac80-30e6124077d1",
        "tags" : [
        ]
      },
      {
        "id" : "ede395cb-2033-4e2e-a68b-e4ecf0debab8",
        "parentId" : "4f4a886f-2aac-4b6b-926d-929cfabfe56b",
        "authorId" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "body" : "Agree with @cloud-fan 's reasoning and proposed fix.\r\n\r\nThe only worry is that we're not 100% sure if we are breaking things somewhere else, but the tests don't say so yet. Can't think of an approach that would have less impact yet fix this problem perfectly. Let's go with this approach then.",
        "createdAt" : "2020-09-03T17:39:24Z",
        "updatedAt" : "2020-09-04T07:00:39Z",
        "lastEditedBy" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "tags" : [
        ]
      }
    ],
    "commit" : "7bd1ab0c5f898be28d9c18115daebff75eff36b7",
    "line" : 3,
    "diffHunk" : "@@ -1,1 +91,95 @@  private val tags: mutable.Map[TreeNodeTag[_], Any] = mutable.Map.empty\n\n  protected def copyTagsFrom(other: BaseType): Unit = {\n    // SPARK-32753: it only makes sense to copy tags to a new node\n    // but it's too expensive to detect other cases likes node removal"
  },
  {
    "id" : "66aa92cd-0d31-4a7c-82d6-402806f64ea0",
    "prId" : 29593,
    "prUrl" : "https://github.com/apache/spark/pull/29593#pullrequestreview-482406029",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "358017d7-801b-434c-bde4-d187365a241f",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "can we add some comments?",
        "createdAt" : "2020-09-04T05:34:16Z",
        "updatedAt" : "2020-09-04T07:00:39Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "fe98e013-758c-4666-8f5e-94feaeab2b01",
        "parentId" : "358017d7-801b-434c-bde4-d187365a241f",
        "authorId" : "334a081c-cba1-403e-ac80-30e6124077d1",
        "body" : "done",
        "createdAt" : "2020-09-04T07:01:02Z",
        "updatedAt" : "2020-09-04T07:01:03Z",
        "lastEditedBy" : "334a081c-cba1-403e-ac80-30e6124077d1",
        "tags" : [
        ]
      }
    ],
    "commit" : "7bd1ab0c5f898be28d9c18115daebff75eff36b7",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +95,99 @@    // but it's too expensive to detect other cases likes node removal\n    // so we make a compromise here to copy tags to node with no tags\n    if (tags.isEmpty) {\n      tags ++= other.tags\n    }"
  },
  {
    "id" : "e93a9606-34f0-4880-b0a3-334105a7d7ec",
    "prId" : 27601,
    "prUrl" : "https://github.com/apache/spark/pull/27601#pullrequestreview-359521501",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8659949e-94bc-4a79-bd98-89e8f244c672",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "So, only this line is the fix.",
        "createdAt" : "2020-02-17T07:05:41Z",
        "updatedAt" : "2020-02-17T08:40:11Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "c94fa84ab94a3c189ac6beca8e26f87a276f782d",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +128,132 @@    } else {\n      var h = seed\n      if (!ignorePrefix) h = scala.util.hashing.MurmurHash3.mix(h, x.productPrefix.hashCode)\n      var i = 0\n      while (i < arr) {"
  }
]