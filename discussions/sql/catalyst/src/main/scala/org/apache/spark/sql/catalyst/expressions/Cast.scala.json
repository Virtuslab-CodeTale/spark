[
  {
    "id" : "0448c95c-eaa6-4e24-9725-9e4206b400c2",
    "prId" : 33146,
    "prUrl" : "https://github.com/apache/spark/pull/33146#pullrequestreview-697161729",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a236b0e6-1f0b-4615-8902-92772097fddb",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "if `fields(0).nullable` is false, how can `row.isNullAt(0)` be true?",
        "createdAt" : "2021-06-30T13:11:33Z",
        "updatedAt" : "2021-06-30T13:11:33Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "0d476922-445e-4af8-ab0c-48659f924bb1",
        "parentId" : "a236b0e6-1f0b-4615-8902-92772097fddb",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "(I have the same question)",
        "createdAt" : "2021-07-01T02:03:20Z",
        "updatedAt" : "2021-07-01T02:03:27Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "35b515c3-1bd4-42f3-9113-4306cedb8dbd",
        "parentId" : "a236b0e6-1f0b-4615-8902-92772097fddb",
        "authorId" : "c12eb4aa-c39d-4fa9-8470-ed071c0be24a",
        "body" : "If user create dataframe from `spark.internalCreateDataFrame()`, the `row.isNullAt()` may be true even though the schema nullable is false.\r\nFor instance:\r\n```scala\r\n  val schema = StructType(Seq(\r\n    StructField(\"x\",\r\n      StructType(Seq(\r\n        StructField(\"y\", IntegerType, true),\r\n        StructField(\"z\", IntegerType, false)\r\n      )))))\r\n  val rdd = spark.sparkContext.parallelize(Seq(InternalRow(InternalRow(1, null))))\r\n  val df = spark.internalCreateDataFrame(rdd, schema)\r\n  df.show\r\n  // current master branch output\r\n  //  +---------+\r\n  //  |        x|\r\n  //  +---------+\r\n  //  |{1, null}|\r\n  //  +---------+\r\n```\r\n\r\nAlthough the `spark.internalCreateDataFrame()` is sql package private API, but `spark.read.json()` and `spark.read.csv()` call it without null value handled.(the example show in pr description)\r\n",
        "createdAt" : "2021-07-01T03:28:58Z",
        "updatedAt" : "2021-07-01T03:34:21Z",
        "lastEditedBy" : "c12eb4aa-c39d-4fa9-8470-ed071c0be24a",
        "tags" : [
        ]
      },
      {
        "id" : "992483e1-a9e1-40e7-90a8-fde414f35e1d",
        "parentId" : "a236b0e6-1f0b-4615-8902-92772097fddb",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Then we need to fix the nullability. There are so many places in the Spark codebase that relies on nullability to do optimizations. It's not possible to change all of them to not trust the nullability anymore.\r\n\r\nCan we fix `spark.read.json()` to set the nullability correctly?",
        "createdAt" : "2021-07-01T04:54:55Z",
        "updatedAt" : "2021-07-01T04:54:55Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "c637ef51-49df-40ec-bb82-6a4cbd7608ca",
        "parentId" : "a236b0e6-1f0b-4615-8902-92772097fddb",
        "authorId" : "c12eb4aa-c39d-4fa9-8470-ed071c0be24a",
        "body" : "Ok, let me try.",
        "createdAt" : "2021-07-01T12:05:41Z",
        "updatedAt" : "2021-07-01T12:05:41Z",
        "lastEditedBy" : "c12eb4aa-c39d-4fa9-8470-ed071c0be24a",
        "tags" : [
        ]
      }
    ],
    "commit" : "25c396c3e30679f11318d3a0c449306b12762af2",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +407,411 @@          val st = fields.map(_.dataType)\n          val toUTF8StringFuncs = st.map(castToString)\n          if (fields(0).nullable && row.isNullAt(0)) {\n            if (!legacyCastToStr) builder.append(\"null\")\n          } else {"
  },
  {
    "id" : "280b6a05-7062-4571-ae27-1d6c0830e972",
    "prId" : 33027,
    "prUrl" : "https://github.com/apache/spark/pull/33027#pullrequestreview-690205328",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7b107d8f-d520-4c9b-a1bd-bdcb16c56f72",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I checked `Add`, I think we should also add a new `def this` to allow omitting the `ansiEnabled` parameter, to get a bit more compatibility, such as `new Cast(...)`",
        "createdAt" : "2021-06-23T04:41:30Z",
        "updatedAt" : "2021-06-23T04:41:30Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "55891cfc9df412ae34dda7d5f3f6c98c832f4c01",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +1949,1953 @@    override val ansiEnabled: Boolean = SQLConf.get.ansiEnabled)\n  extends CastBase {\n\n  def this(child: Expression, dataType: DataType, timeZoneId: Option[String]) =\n    this(child, dataType, timeZoneId, ansiEnabled = SQLConf.get.ansiEnabled)"
  },
  {
    "id" : "6bff4828-50ff-498e-a993-a32d908d241e",
    "prId" : 32869,
    "prUrl" : "https://github.com/apache/spark/pull/32869#pullrequestreview-681084858",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f2f9c149-b314-456f-a022-6388f95f66d6",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "nit: we can add a `protected def dtUtilCls = classOf[DateTimeUtils].getClass.getName` in `CaseBase`, to shorten the code here and other places.",
        "createdAt" : "2021-06-10T17:47:30Z",
        "updatedAt" : "2021-06-10T17:47:30Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "dde4cd800a3d2e9aac55d05b5d99cc00fba0a2e6",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +1211,1215 @@        (c, evPrim, evNull) =>\n          // scalastyle:off line.size.limit\n          code\"$evPrim = org.apache.spark.sql.catalyst.util.DateTimeUtils.microsToDays($c, java.time.ZoneOffset.UTC);\"\n          // scalastyle:on line.size.limit\n      case _ =>"
  },
  {
    "id" : "098dcb66-4af0-4ab7-a427-f003989d7ff0",
    "prId" : 32864,
    "prUrl" : "https://github.com/apache/spark/pull/32864#pullrequestreview-680879169",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cf338e96-5ee6-4573-aaf9-b4944bbce0d9",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "how about the other way around?",
        "createdAt" : "2021-06-10T14:35:53Z",
        "updatedAt" : "2021-06-10T14:35:53Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "7d4e9f7e-8692-4ac1-8c9f-38eb6eb94965",
        "parentId" : "cf338e96-5ee6-4573-aaf9-b4944bbce0d9",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "It will be added in another PR",
        "createdAt" : "2021-06-10T14:43:46Z",
        "updatedAt" : "2021-06-10T14:43:46Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "b47219a0498192ae217ea18582bbc37cd5969bd7",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +69,73 @@    case (DateType, TimestampType) => true\n    case (_: NumericType, TimestampType) => true\n    case (TimestampWithoutTZType, TimestampType) => true\n\n    case (StringType, DateType) => true"
  }
]