[
  {
    "id" : "0448c95c-eaa6-4e24-9725-9e4206b400c2",
    "prId" : 33146,
    "prUrl" : "https://github.com/apache/spark/pull/33146#pullrequestreview-697161729",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a236b0e6-1f0b-4615-8902-92772097fddb",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "if `fields(0).nullable` is false, how can `row.isNullAt(0)` be true?",
        "createdAt" : "2021-06-30T13:11:33Z",
        "updatedAt" : "2021-06-30T13:11:33Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "0d476922-445e-4af8-ab0c-48659f924bb1",
        "parentId" : "a236b0e6-1f0b-4615-8902-92772097fddb",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "(I have the same question)",
        "createdAt" : "2021-07-01T02:03:20Z",
        "updatedAt" : "2021-07-01T02:03:27Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "35b515c3-1bd4-42f3-9113-4306cedb8dbd",
        "parentId" : "a236b0e6-1f0b-4615-8902-92772097fddb",
        "authorId" : "c12eb4aa-c39d-4fa9-8470-ed071c0be24a",
        "body" : "If user create dataframe from `spark.internalCreateDataFrame()`, the `row.isNullAt()` may be true even though the schema nullable is false.\r\nFor instance:\r\n```scala\r\n  val schema = StructType(Seq(\r\n    StructField(\"x\",\r\n      StructType(Seq(\r\n        StructField(\"y\", IntegerType, true),\r\n        StructField(\"z\", IntegerType, false)\r\n      )))))\r\n  val rdd = spark.sparkContext.parallelize(Seq(InternalRow(InternalRow(1, null))))\r\n  val df = spark.internalCreateDataFrame(rdd, schema)\r\n  df.show\r\n  // current master branch output\r\n  //  +---------+\r\n  //  |        x|\r\n  //  +---------+\r\n  //  |{1, null}|\r\n  //  +---------+\r\n```\r\n\r\nAlthough the `spark.internalCreateDataFrame()` is sql package private API, but `spark.read.json()` and `spark.read.csv()` call it without null value handled.(the example show in pr description)\r\n",
        "createdAt" : "2021-07-01T03:28:58Z",
        "updatedAt" : "2021-07-01T03:34:21Z",
        "lastEditedBy" : "c12eb4aa-c39d-4fa9-8470-ed071c0be24a",
        "tags" : [
        ]
      },
      {
        "id" : "992483e1-a9e1-40e7-90a8-fde414f35e1d",
        "parentId" : "a236b0e6-1f0b-4615-8902-92772097fddb",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Then we need to fix the nullability. There are so many places in the Spark codebase that relies on nullability to do optimizations. It's not possible to change all of them to not trust the nullability anymore.\r\n\r\nCan we fix `spark.read.json()` to set the nullability correctly?",
        "createdAt" : "2021-07-01T04:54:55Z",
        "updatedAt" : "2021-07-01T04:54:55Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "c637ef51-49df-40ec-bb82-6a4cbd7608ca",
        "parentId" : "a236b0e6-1f0b-4615-8902-92772097fddb",
        "authorId" : "c12eb4aa-c39d-4fa9-8470-ed071c0be24a",
        "body" : "Ok, let me try.",
        "createdAt" : "2021-07-01T12:05:41Z",
        "updatedAt" : "2021-07-01T12:05:41Z",
        "lastEditedBy" : "c12eb4aa-c39d-4fa9-8470-ed071c0be24a",
        "tags" : [
        ]
      }
    ],
    "commit" : "25c396c3e30679f11318d3a0c449306b12762af2",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +407,411 @@          val st = fields.map(_.dataType)\n          val toUTF8StringFuncs = st.map(castToString)\n          if (fields(0).nullable && row.isNullAt(0)) {\n            if (!legacyCastToStr) builder.append(\"null\")\n          } else {"
  },
  {
    "id" : "280b6a05-7062-4571-ae27-1d6c0830e972",
    "prId" : 33027,
    "prUrl" : "https://github.com/apache/spark/pull/33027#pullrequestreview-690205328",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7b107d8f-d520-4c9b-a1bd-bdcb16c56f72",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I checked `Add`, I think we should also add a new `def this` to allow omitting the `ansiEnabled` parameter, to get a bit more compatibility, such as `new Cast(...)`",
        "createdAt" : "2021-06-23T04:41:30Z",
        "updatedAt" : "2021-06-23T04:41:30Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "55891cfc9df412ae34dda7d5f3f6c98c832f4c01",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +1949,1953 @@    override val ansiEnabled: Boolean = SQLConf.get.ansiEnabled)\n  extends CastBase {\n\n  def this(child: Expression, dataType: DataType, timeZoneId: Option[String]) =\n    this(child, dataType, timeZoneId, ansiEnabled = SQLConf.get.ansiEnabled)"
  },
  {
    "id" : "6bff4828-50ff-498e-a993-a32d908d241e",
    "prId" : 32869,
    "prUrl" : "https://github.com/apache/spark/pull/32869#pullrequestreview-681084858",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f2f9c149-b314-456f-a022-6388f95f66d6",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "nit: we can add a `protected def dtUtilCls = classOf[DateTimeUtils].getClass.getName` in `CaseBase`, to shorten the code here and other places.",
        "createdAt" : "2021-06-10T17:47:30Z",
        "updatedAt" : "2021-06-10T17:47:30Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "dde4cd800a3d2e9aac55d05b5d99cc00fba0a2e6",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +1211,1215 @@        (c, evPrim, evNull) =>\n          // scalastyle:off line.size.limit\n          code\"$evPrim = org.apache.spark.sql.catalyst.util.DateTimeUtils.microsToDays($c, java.time.ZoneOffset.UTC);\"\n          // scalastyle:on line.size.limit\n      case _ =>"
  },
  {
    "id" : "098dcb66-4af0-4ab7-a427-f003989d7ff0",
    "prId" : 32864,
    "prUrl" : "https://github.com/apache/spark/pull/32864#pullrequestreview-680879169",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cf338e96-5ee6-4573-aaf9-b4944bbce0d9",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "how about the other way around?",
        "createdAt" : "2021-06-10T14:35:53Z",
        "updatedAt" : "2021-06-10T14:35:53Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "7d4e9f7e-8692-4ac1-8c9f-38eb6eb94965",
        "parentId" : "cf338e96-5ee6-4573-aaf9-b4944bbce0d9",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "It will be added in another PR",
        "createdAt" : "2021-06-10T14:43:46Z",
        "updatedAt" : "2021-06-10T14:43:46Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "b47219a0498192ae217ea18582bbc37cd5969bd7",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +69,73 @@    case (DateType, TimestampType) => true\n    case (_: NumericType, TimestampType) => true\n    case (TimestampWithoutTZType, TimestampType) => true\n\n    case (StringType, DateType) => true"
  },
  {
    "id" : "3786e456-6cea-466a-a342-c0721c886ecb",
    "prId" : 31954,
    "prUrl" : "https://github.com/apache/spark/pull/31954#pullrequestreview-621290840",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c96275e9-93fa-4b1f-8685-c95f5114d34f",
        "parentId" : null,
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "Does this affect the coming year-month and day-time interval?",
        "createdAt" : "2021-03-24T16:12:13Z",
        "updatedAt" : "2021-03-25T13:35:48Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "12697dbc-30aa-4972-b28b-464e78d0c3ae",
        "parentId" : "c96275e9-93fa-4b1f-8685-c95f5114d34f",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "Yes",
        "createdAt" : "2021-03-24T16:41:25Z",
        "updatedAt" : "2021-03-25T13:35:48Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "ab3948e9-2c2a-4050-b132-b5475b497c3c",
        "parentId" : "c96275e9-93fa-4b1f-8685-c95f5114d34f",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "@cloud-fan . Usually, we do the explicitly allowed-list approach in case of types. Is this change okay?\r\nIf this PR aims for `complex type` only, why don't we add them explicitly instead of doing this widely. ",
        "createdAt" : "2021-03-24T19:01:29Z",
        "updatedAt" : "2021-03-25T13:35:48Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "2e36f1c3-cc76-4030-83ae-3229a0f1e44b",
        "parentId" : "c96275e9-93fa-4b1f-8685-c95f5114d34f",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Also, cc @MaxGekk ",
        "createdAt" : "2021-03-24T19:02:01Z",
        "updatedAt" : "2021-03-25T13:35:48Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "d9b2e167-6747-41d7-82e2-50a915e86ade",
        "parentId" : "c96275e9-93fa-4b1f-8685-c95f5114d34f",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "`df.show` needs to cast the column to string, I think we need to support casting from all the data types here, otherwise `df.show` may still be broken under some cases.",
        "createdAt" : "2021-03-25T13:29:43Z",
        "updatedAt" : "2021-03-25T13:35:48Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "c878e638-c88a-42e5-b3a5-e3047e62ff65",
        "parentId" : "c96275e9-93fa-4b1f-8685-c95f5114d34f",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "So far, we don't support such casting. I opened the JIRAs for that: SPARK-34667 and SPARK-34668",
        "createdAt" : "2021-03-25T15:56:00Z",
        "updatedAt" : "2021-03-25T15:56:05Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "1707885814cf846d567fdbca65a2ded88643d679",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +1874,1878 @@    case (NullType, _) => true\n\n    case (_, StringType) => true\n\n    case (StringType, _: BinaryType) => true"
  },
  {
    "id" : "09820b11-6540-4755-8565-ad4127963dd0",
    "prId" : 31228,
    "prUrl" : "https://github.com/apache/spark/pull/31228#pullrequestreview-571326802",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f0a7ef35-d48b-420d-9bde-38ba06480885",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "we also need to change the codegen path.",
        "createdAt" : "2021-01-18T13:47:53Z",
        "updatedAt" : "2021-01-21T12:56:18Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "0353a190-8b95-4075-a7e4-53391c984663",
        "parentId" : "f0a7ef35-d48b-420d-9bde-38ba06480885",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "OK",
        "createdAt" : "2021-01-19T04:45:14Z",
        "updatedAt" : "2021-01-21T12:56:18Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      },
      {
        "id" : "e27d4916-700f-46a2-8190-c26bbffc6a05",
        "parentId" : "f0a7ef35-d48b-420d-9bde-38ba06480885",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "OK",
        "createdAt" : "2021-01-19T14:58:07Z",
        "updatedAt" : "2021-01-21T12:56:18Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "182d957ae72a108ea30032bb07368fbf90a72a61",
    "line" : 54,
    "diffHunk" : "@@ -1,1 +634,638 @@          longValue.toByte\n        } else {\n          throw QueryExecutionErrors.castingCauseOverflowError(t, Byte.getClass.getName)\n        }\n      })"
  },
  {
    "id" : "1c225765-da54-4cd4-9b29-d6b159d5a861",
    "prId" : 30874,
    "prUrl" : "https://github.com/apache/spark/pull/30874#pullrequestreview-556515068",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "058cb642-5b37-44fb-93d5-d75a2274606e",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "iff is 'if and only if'",
        "createdAt" : "2020-12-21T16:38:17Z",
        "updatedAt" : "2020-12-21T16:38:17Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "fd32bad5-3383-4a68-9430-c6dc02686869",
        "parentId" : "058cb642-5b37-44fb-93d5-d75a2274606e",
        "authorId" : "6acb4d7a-9c99-4a6f-a3ba-70ff202fef81",
        "body" : "Thank you!",
        "createdAt" : "2020-12-21T16:43:23Z",
        "updatedAt" : "2020-12-21T16:43:41Z",
        "lastEditedBy" : "6acb4d7a-9c99-4a6f-a3ba-70ff202fef81",
        "tags" : [
        ]
      }
    ],
    "commit" : "13e5c4dd49d2daecf337fa31c74312f4f633d680",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +40,44 @@\n  /**\n   * Returns true if we can cast `from` type to `to` type.\n   */\n  def canCast(from: DataType, to: DataType): Boolean = (from, to) match {"
  },
  {
    "id" : "4228df96-0a99-443a-914e-20791dfb027a",
    "prId" : 30329,
    "prUrl" : "https://github.com/apache/spark/pull/30329#pullrequestreview-529198208",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "19219201-947b-455d-b947-08c2da6393d6",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Why the case is not handled by the case above?\r\n```scala\r\ncase (TimestampType | DateType, StringType) => true\r\n```",
        "createdAt" : "2020-11-11T15:29:38Z",
        "updatedAt" : "2020-11-11T15:29:43Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "4d20c5c8-dafd-4b04-bd50-50dd6580a592",
        "parentId" : "19219201-947b-455d-b947-08c2da6393d6",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Yea it's already covered by L111. We should close this PR.",
        "createdAt" : "2020-11-12T15:24:13Z",
        "updatedAt" : "2020-11-12T15:24:13Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "ddd8f01aecf4aa8f6ae24a6ca16e5aa58b62f166",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +111,115 @@    case (TimestampType | DateType, StringType) => true\n    case (DateType, TimestampType) => true\n    case (DateType, StringType) => true\n    case (TimestampType, DateType) => true\n    case (ArrayType(fromType, _), ArrayType(toType, _)) => needsTimeZone(fromType, toType)"
  },
  {
    "id" : "7c8b38b6-7352-498f-838f-5cf30fb78d2c",
    "prId" : 30260,
    "prUrl" : "https://github.com/apache/spark/pull/30260#pullrequestreview-527965554",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f0b5f4d9-bd4d-42ca-901f-1bcfbafa9b4c",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "How about describing this new behaviour in the usage above of `ExpressionDescription`?",
        "createdAt" : "2020-11-10T01:57:50Z",
        "updatedAt" : "2020-11-18T13:36:51Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "344902d6-b549-4f84-b7d1-1e41f070721c",
        "parentId" : "f0b5f4d9-bd4d-42ca-901f-1bcfbafa9b4c",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "well, then we need to mention about the behavior of throwing overflow exceptions when ANSI flag enabled. I will add some content in the `sql-ref-ansi-compliance.md`",
        "createdAt" : "2020-11-11T08:38:55Z",
        "updatedAt" : "2020-11-18T13:36:51Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "6003bef62c59d623ca59e51532b4fa5006c56fa2",
    "line" : 33,
    "diffHunk" : "@@ -1,1 +1761,1765 @@  override protected val ansiEnabled: Boolean = SQLConf.get.ansiEnabled\n\n  override def canCast(from: DataType, to: DataType): Boolean = if (ansiEnabled) {\n    AnsiCast.canCast(from, to)\n  } else {"
  },
  {
    "id" : "8d75ac0d-08b9-4508-a0fa-1fd8b377d624",
    "prId" : 30260,
    "prUrl" : "https://github.com/apache/spark/pull/30260#pullrequestreview-526795322",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "938a7e7b-ac52-46f3-a112-0a5c29477f3c",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Could you leave some comments to summarize the current behaivour of the ANSI explicit cast as described in the PR description (references `section 6.13 of the ANSI SQL standard` and differences from the standard, e.g., `Numeric <=> Boolean`) ?",
        "createdAt" : "2020-11-10T02:02:13Z",
        "updatedAt" : "2020-11-18T13:36:51Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "6003bef62c59d623ca59e51532b4fa5006c56fa2",
    "line" : 49,
    "diffHunk" : "@@ -1,1 +1787,1791 @@}\n\nobject AnsiCast {\n  /**\n   * As per section 6.13 \"cast specification\" in \"Information technology — Database languages \" +"
  },
  {
    "id" : "3f3c5f4f-4b88-4fee-b910-dbc6b993cba0",
    "prId" : 30260,
    "prUrl" : "https://github.com/apache/spark/pull/30260#pullrequestreview-526800432",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "385c327b-f20f-432d-ab12-37af6a86281a",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "(Just a suggestion) For readability, could you reorder these entries according to `Cast.canCast` where possible? For example, the numeric entries for `Cast` seems to be placed just before complex types:\r\nhttps://github.com/apache/spark/blob/35ac314181129374b02f8f8c07341b43a734e1c7/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala#L70-L74",
        "createdAt" : "2020-11-10T02:12:51Z",
        "updatedAt" : "2020-11-18T13:36:51Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "6003bef62c59d623ca59e51532b4fa5006c56fa2",
    "line" : 116,
    "diffHunk" : "@@ -1,1 +1854,1858 @@    case (_: NumericType, _: NumericType) => true\n    case (StringType, _: NumericType) => true\n    case (BooleanType, _: NumericType) => true\n\n    case (_: NumericType, StringType) => true"
  },
  {
    "id" : "80a7878f-4812-46a7-b5ad-58c1a5d41997",
    "prId" : 30260,
    "prUrl" : "https://github.com/apache/spark/pull/30260#pullrequestreview-528998931",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "87a6b003-db69-4cb6-b65b-223bce16d1d7",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Nice update. Thanks!",
        "createdAt" : "2020-11-12T11:41:49Z",
        "updatedAt" : "2020-11-18T13:36:51Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "6003bef62c59d623ca59e51532b4fa5006c56fa2",
    "line" : 51,
    "diffHunk" : "@@ -1,1 +1789,1793 @@object AnsiCast {\n  /**\n   * As per section 6.13 \"cast specification\" in \"Information technology — Database languages \" +\n   * \"- SQL — Part 2: Foundation (SQL/Foundation)\":\n   * If the <cast operand> is a <value expression>, then the valid combinations of TD and SD"
  },
  {
    "id" : "65a3a4db-41ff-4dd9-a946-f73503b34d69",
    "prId" : 30260,
    "prUrl" : "https://github.com/apache/spark/pull/30260#pullrequestreview-528998931",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d99c316e-4155-4b63-a53b-cb30e3cb0ae5",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "How about describing the special case above in `sql-ref-ansi-compliance.md`, too?",
        "createdAt" : "2020-11-12T11:42:13Z",
        "updatedAt" : "2020-11-18T13:36:51Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "6003bef62c59d623ca59e51532b4fa5006c56fa2",
    "line" : 94,
    "diffHunk" : "@@ -1,1 +1832,1836 @@   * straightforward type conversions which are disallowed as per the SQL standard:\n   *   - Numeric <=> Boolean\n   *   - String <=> Binary\n   */\n  def canCast(from: DataType, to: DataType): Boolean = (from, to) match {"
  },
  {
    "id" : "0196aadf-1fd8-4585-9400-aa936ed25654",
    "prId" : 30213,
    "prUrl" : "https://github.com/apache/spark/pull/30213#pullrequestreview-521447487",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8861d41c-b022-4487-955a-82308622b8f1",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "It could be symmetric to line above:\r\n```scala\r\ncase (TimestampType | DateType, StringType) => true\r\n``` \r\nSo we could replace:\r\n```scala\r\ncase (TimestampType, StringType) => true\r\n```",
        "createdAt" : "2020-11-01T19:30:52Z",
        "updatedAt" : "2020-11-01T19:33:42Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "40f9b760-1a53-4f22-af4e-acd9f4107375",
        "parentId" : "8861d41c-b022-4487-955a-82308622b8f1",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "can you do a followup to clean this up? thanks!",
        "createdAt" : "2020-11-02T02:56:31Z",
        "updatedAt" : "2020-11-02T02:56:31Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "fc180750-ea39-42de-af73-a9b8c42b6cbc",
        "parentId" : "8861d41c-b022-4487-955a-82308622b8f1",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Here is it: https://github.com/apache/spark/pull/30223",
        "createdAt" : "2020-11-02T09:14:16Z",
        "updatedAt" : "2020-11-02T09:14:16Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "9e3b43b301e14c5b7e3e5d60b953efcc17ccc1b8",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +109,113 @@  def needsTimeZone(from: DataType, to: DataType): Boolean = (from, to) match {\n    case (StringType, TimestampType | DateType) => true\n    case (DateType, StringType) => true\n    case (DateType, TimestampType) => true\n    case (TimestampType, StringType) => true"
  },
  {
    "id" : "e2bb0b47-2595-4192-ad4e-ed755398ddba",
    "prId" : 30189,
    "prUrl" : "https://github.com/apache/spark/pull/30189#pullrequestreview-525429397",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f8cd7716-76ce-4846-bd6d-885509b7423e",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Is this for map type? I saw you only added tests for array and struct.",
        "createdAt" : "2020-11-05T22:51:55Z",
        "updatedAt" : "2020-11-06T14:58:35Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "56eae8f0-084b-4b5e-b802-5bb7cc2ca70e",
        "parentId" : "f8cd7716-76ce-4846-bd6d-885509b7423e",
        "authorId" : "51995e12-6440-47da-9a2d-ad3292ac10dd",
        "body" : "I replaced outNullElem (which always appended \" null\") with appendIfNotLegacyCastToStr (which allows you to pass in the string to be appended).\r\n\r\nBecause the map, struct, and array code all called the outNullElem function, I changed them all to call appendIfNotLegacyCastToStr, even though, in the case of map, I'm not actually changing its behavior.  IOW, the map code was already correct, so I'm not making any changes to its behavior, I only changed that function call because that function is shared with struct and array.\r\n\r\nThis is also why I didn't add any unit tests for map.  I didn't change its behavior.\r\n\r\nHow would you like me to resolve this?  I can put the outNullElem function back, and revert the map code to call it, which will revert all of my changes to the map code.  However, doing that will create what could be considered redundant code (both outNullElem and appendIfNotLegacyCastToStr do almost the same thing, and appendIfNotLegacyCastToStr supercedes outNullElem, rendering outNullElem redundant).\r\n\r\nI'm happy to make whatever change you recommend.  Please advise.",
        "createdAt" : "2020-11-06T15:11:39Z",
        "updatedAt" : "2020-11-06T15:11:40Z",
        "lastEditedBy" : "51995e12-6440-47da-9a2d-ad3292ac10dd",
        "tags" : [
        ]
      },
      {
        "id" : "4ea12d6d-7605-4d2b-98fa-7979923732f5",
        "parentId" : "f8cd7716-76ce-4846-bd6d-885509b7423e",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Oh, so map doesn't have the same a leading space? If so then it is okay. As you actually change map's code, it would be nice to add a test like array/struct to prevent anything wrong.",
        "createdAt" : "2020-11-06T17:05:04Z",
        "updatedAt" : "2020-11-06T17:05:05Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "53fcdc6f-7e89-4619-96f2-dda421c469b4",
        "parentId" : "f8cd7716-76ce-4846-bd6d-885509b7423e",
        "authorId" : "51995e12-6440-47da-9a2d-ad3292ac10dd",
        "body" : "map's behavior is already correct.  For example, when printing a map, it looks like this:\r\n\r\n{A -> B, C -> D}\r\n\r\nSo, the values are already printed with a leading space, as they should be.  Therefore, I didn't need to change map's behavior.\r\n\r\nRegarding a unit test, one already exists.  In CastSuite.scala, beginning on line 731 (in my branch), there's a test named \"SPARK-22973 Cast map to string\" which already exists.  So, yes, I changed map's code, but no, I didn't change map's behavior, and the correct map behavior is already being unit tested.  My code changes didn't change the behavior, and the existing unit test passes successfully after my changes.\r\n\r\nI could add another unit test, but it will literally just be the same as the existing unit test (because I didn't change any behavior).  Do you want me to go ahead and add another unit test?\r\n",
        "createdAt" : "2020-11-06T17:37:28Z",
        "updatedAt" : "2020-11-06T17:37:29Z",
        "lastEditedBy" : "51995e12-6440-47da-9a2d-ad3292ac10dd",
        "tags" : [
        ]
      },
      {
        "id" : "66f3c878-8c22-4fad-b7c6-8e9cf8bcf749",
        "parentId" : "f8cd7716-76ce-4846-bd6d-885509b7423e",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Okay it sounds good to me if we already have test for it. Thanks for clarifying.",
        "createdAt" : "2020-11-06T18:39:59Z",
        "updatedAt" : "2020-11-06T18:39:59Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "50807713-8e09-4f49-9efc-a55db9f503b6",
        "parentId" : "f8cd7716-76ce-4846-bd6d-885509b7423e",
        "authorId" : "51995e12-6440-47da-9a2d-ad3292ac10dd",
        "body" : "Sounds good.  Thanks for the feedback.",
        "createdAt" : "2020-11-06T19:22:33Z",
        "updatedAt" : "2020-11-06T19:22:33Z",
        "lastEditedBy" : "51995e12-6440-47da-9a2d-ad3292ac10dd",
        "tags" : [
        ]
      }
    ],
    "commit" : "601af5e9fb034703f002aa39ce3e695118ab9ffb",
    "line" : 53,
    "diffHunk" : "@@ -1,1 +985,989 @@       |  $buffer.append(\" ->\");\n       |  if ($map.valueArray().isNullAt(0)) {\n       |    ${appendIfNotLegacyCastToStr(buffer, \" null\")}\n       |  } else {\n       |    $buffer.append(\" \");"
  },
  {
    "id" : "019f68fc-6283-4958-aa9a-212bb4757fb1",
    "prId" : 29311,
    "prUrl" : "https://github.com/apache/spark/pull/29311#pullrequestreview-461371870",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f707c8db-1db2-4745-8e90-93a05a3c07f4",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "`if (array.isNullAt(i) && !legacyCastToStr)`",
        "createdAt" : "2020-08-05T05:53:18Z",
        "updatedAt" : "2020-08-05T07:36:14Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "750993a3-e581-4ac1-a746-5116068085d9",
        "parentId" : "f707c8db-1db2-4745-8e90-93a05a3c07f4",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Here, we have 3 branches. Don't think your proposal is applicable.",
        "createdAt" : "2020-08-05T06:24:45Z",
        "updatedAt" : "2020-08-05T07:36:14Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "39daa90e49645d04d13789eb284b46f69eaad6ee",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +322,326 @@          while (i < array.numElements) {\n            builder.append(\",\")\n            if (array.isNullAt(i)) {\n              if (!legacyCastToStr) builder.append(\" null\")\n            } else {"
  },
  {
    "id" : "64789f04-9007-4e75-902a-ed5a5337cd14",
    "prId" : 29311,
    "prUrl" : "https://github.com/apache/spark/pull/29311#pullrequestreview-461358999",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "db1d4656-003f-4714-8d65-682fcdefb5b4",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "ditto",
        "createdAt" : "2020-08-05T05:53:38Z",
        "updatedAt" : "2020-08-05T07:36:14Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "39daa90e49645d04d13789eb284b46f69eaad6ee",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +345,349 @@          builder.append(keyToUTF8String(keyArray.get(0, kt)).asInstanceOf[UTF8String])\n          builder.append(\" ->\")\n          if (valueArray.isNullAt(0)) {\n            if (!legacyCastToStr) builder.append(\" null\")\n          } else {"
  },
  {
    "id" : "84e8c583-0bbf-449a-a3dd-0f0a33e9c291",
    "prId" : 29311,
    "prUrl" : "https://github.com/apache/spark/pull/29311#pullrequestreview-461359081",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ca3ab4ab-f2c1-4525-b910-ff9d126320ea",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "ditto",
        "createdAt" : "2020-08-05T05:53:49Z",
        "updatedAt" : "2020-08-05T07:36:14Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "39daa90e49645d04d13789eb284b46f69eaad6ee",
    "line" : 39,
    "diffHunk" : "@@ -1,1 +356,360 @@            builder.append(keyToUTF8String(keyArray.get(i, kt)).asInstanceOf[UTF8String])\n            builder.append(\" ->\")\n            if (valueArray.isNullAt(i)) {\n              if (!legacyCastToStr) builder.append(\" null\")\n            } else {"
  },
  {
    "id" : "8ba5c2f0-0887-413b-861d-a69300df7cfe",
    "prId" : 29311,
    "prUrl" : "https://github.com/apache/spark/pull/29311#pullrequestreview-461359129",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dfa9d887-7c57-4a4c-bb7a-0ed7dece7584",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "ditto",
        "createdAt" : "2020-08-05T05:53:56Z",
        "updatedAt" : "2020-08-05T07:36:14Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "39daa90e49645d04d13789eb284b46f69eaad6ee",
    "line" : 50,
    "diffHunk" : "@@ -1,1 +376,380 @@          val st = fields.map(_.dataType)\n          val toUTF8StringFuncs = st.map(castToString)\n          if (row.isNullAt(0)) {\n            if (!legacyCastToStr) builder.append(\" null\")\n          } else {"
  },
  {
    "id" : "d736b8c9-387a-4536-9169-d54aeb3f6e08",
    "prId" : 28593,
    "prUrl" : "https://github.com/apache/spark/pull/28593#pullrequestreview-418725274",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a2f5d408-0e16-438d-82aa-0af4f7e66458",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "nit: `  case (_: NumericType, TimestampType) => SQLConf.get.allowCastNumericToTimestamp`?",
        "createdAt" : "2020-05-26T22:35:27Z",
        "updatedAt" : "2020-06-15T17:23:35Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "12b42396f058569354040d466962904794fa5c5e",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +60,64 @@    case (BooleanType, TimestampType) => true\n    case (DateType, TimestampType) => true\n    case (_: NumericType, TimestampType) =>\n      SQLConf.get.getConf(SQLConf.LEGACY_ALLOW_CAST_NUMERIC_TO_TIMESTAMP)\n"
  },
  {
    "id" : "c42d1269-e074-4a55-8fb6-0b80a3d273b3",
    "prId" : 28593,
    "prUrl" : "https://github.com/apache/spark/pull/28593#pullrequestreview-418957662",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4400c34b-ff80-43bf-9b28-12b698fb4055",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "btw, we need the check here? I think its okay just to update our migration guide.",
        "createdAt" : "2020-05-26T22:40:00Z",
        "updatedAt" : "2020-06-15T17:23:35Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "195768bd-1ca7-41be-af59-634447b06420",
        "parentId" : "4400c34b-ff80-43bf-9b28-12b698fb4055",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "it's better to let users know why it's wrong and how to fix, in the error message.",
        "createdAt" : "2020-05-27T08:14:51Z",
        "updatedAt" : "2020-06-15T17:23:35Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "12b42396f058569354040d466962904794fa5c5e",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +268,272 @@    } else {\n      TypeCheckResult.TypeCheckFailure(\n        if (child.dataType.isInstanceOf[NumericType] && dataType.isInstanceOf[TimestampType]) {\n          s\"cannot cast ${child.dataType.catalogString} to ${dataType.catalogString},\" +\n            \"you can enable the casting by setting \" +"
  },
  {
    "id" : "13843540-f04f-43b5-b912-56635ebdeec6",
    "prId" : 28571,
    "prUrl" : "https://github.com/apache/spark/pull/28571#pullrequestreview-414063601",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "23f127a8-a017-44e3-8802-7c61c44f6941",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Let's don't do this. It will require to add the compatibility across all the date time functionality in Spark, e.g.) `from_unixtime`.",
        "createdAt" : "2020-05-19T02:43:58Z",
        "updatedAt" : "2020-05-19T02:43:58Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "4aedd372f778a0b2ef3af21ff22c5765ad4027e0",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +394,398 @@  // converting seconds to us\n  private[this] def longToTimestamp(t: Long): Long = {\n    if (SQLConf.get.getConf(SQLConf.LONG_TIMESTAMP_CONVERSION_IN_SECONDS)) t * 1000000L\n    else t * 1000L\n  }"
  },
  {
    "id" : "fcd002f5-3d5a-4c49-ba27-ef8daa9a027b",
    "prId" : 28570,
    "prUrl" : "https://github.com/apache/spark/pull/28570#pullrequestreview-413566567",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "60b52fc2-1e1b-4105-8105-5303b50b5806",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "I guess checking the flag per each value brings significant performance drop. Could you show by a benchmark that performance is not sacrificed by your changes.",
        "createdAt" : "2020-05-18T12:27:13Z",
        "updatedAt" : "2020-05-18T12:33:28Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "029e131b-e33d-43b2-84f9-932290a4d57d",
        "parentId" : "60b52fc2-1e1b-4105-8105-5303b50b5806",
        "authorId" : "a67e5600-3f69-4b96-9c53-73795f48a0f6",
        "body" : "Hi MaxGekk \r\n i will add new benchmark later",
        "createdAt" : "2020-05-18T12:56:33Z",
        "updatedAt" : "2020-05-18T12:56:33Z",
        "lastEditedBy" : "a67e5600-3f69-4b96-9c53-73795f48a0f6",
        "tags" : [
        ]
      }
    ],
    "commit" : "0557f7397328398b80236fed56ed5a6cb3ca3164",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +456,460 @@  // SPARK-31710 converting seconds to us,Add compatibility flag\n  private[this] def longToTimestamp(t: Long): Long = {\n    if ( SQLConf.get.getConf( SQLConf.LONG_TIMESTAMP_CONVERSION_IN_SECONDS )) t * 1000000L\n    else t * 1000L\n  }"
  },
  {
    "id" : "3ea8931d-46ff-4e73-8114-db8c7823710b",
    "prId" : 28570,
    "prUrl" : "https://github.com/apache/spark/pull/28570#pullrequestreview-413566837",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e7138215-37a8-4f16-b37f-13661cfa724c",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "There are constants in `DateTimeConstants`. Please, use them.",
        "createdAt" : "2020-05-18T12:27:58Z",
        "updatedAt" : "2020-05-18T12:33:28Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "57a2f99c-b57d-4c72-9e4d-cecc5a1c5162",
        "parentId" : "e7138215-37a8-4f16-b37f-13661cfa724c",
        "authorId" : "a67e5600-3f69-4b96-9c53-73795f48a0f6",
        "body" : "Nice suggestion, i will recheck my code",
        "createdAt" : "2020-05-18T12:56:54Z",
        "updatedAt" : "2020-05-18T12:56:54Z",
        "lastEditedBy" : "a67e5600-3f69-4b96-9c53-73795f48a0f6",
        "tags" : [
        ]
      }
    ],
    "commit" : "0557f7397328398b80236fed56ed5a6cb3ca3164",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +457,461 @@  private[this] def longToTimestamp(t: Long): Long = {\n    if ( SQLConf.get.getConf( SQLConf.LONG_TIMESTAMP_CONVERSION_IN_SECONDS )) t * 1000000L\n    else t * 1000L\n  }\n  // converting us to seconds"
  }
]