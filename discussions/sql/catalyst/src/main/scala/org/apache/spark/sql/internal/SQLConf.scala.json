[
  {
    "id" : "1006393a-1dcd-46cb-9648-f462f3c15105",
    "prId" : 33655,
    "prUrl" : "https://github.com/apache/spark/pull/33655#pullrequestreview-724508457",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6f771c2c-d596-4ca9-a339-a39044e93897",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Could you add this check into `spark.sql.adaptive.advisoryPartitionSizeInBytes`, too?",
        "createdAt" : "2021-08-05T19:11:39Z",
        "updatedAt" : "2021-08-05T19:11:39Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "2f3635cd-b39d-4b85-bc81-36325b1b97f7",
        "parentId" : "6f771c2c-d596-4ca9-a339-a39044e93897",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "We can't add check in `ADVISORY_PARTITION_SIZE_IN_BYTES`, because it falls back to `SHUFFLE_TARGET_POSTSHUFFLE_INPUT_SIZE`, which is this conf.",
        "createdAt" : "2021-08-06T03:21:40Z",
        "updatedAt" : "2021-08-06T03:21:41Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "7d5b09ab-ea22-4004-b963-7a431f6b59bc",
        "parentId" : "6f771c2c-d596-4ca9-a339-a39044e93897",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "@cloud-fan . If we set negative values to `advisoryPartitionSizeInBytes` explicitly, it will not fallback to here. If we want to say, `advisoryPartitionSizeInBytes must be positive`, we should check both places, shouldn't we?",
        "createdAt" : "2021-08-06T16:06:34Z",
        "updatedAt" : "2021-08-06T16:06:34Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "eeeff3ae6bb62eedeb09011cc99514d240a8b7d0",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +481,485 @@      .version(\"1.6.0\")\n      .bytesConf(ByteUnit.BYTE)\n      .checkValue(_ > 0, \"advisoryPartitionSizeInBytes must be positive\")\n      .createWithDefaultString(\"64MB\")\n"
  },
  {
    "id" : "98d04720-367d-4e9e-8c61-371061b7bdc3",
    "prId" : 33444,
    "prUrl" : "https://github.com/apache/spark/pull/33444#pullrequestreview-711555608",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "23cb04d3-961d-4052-92fa-d13cc850bf24",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "indentation ?",
        "createdAt" : "2021-07-21T10:35:50Z",
        "updatedAt" : "2021-07-21T10:38:09Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      },
      {
        "id" : "ec7c2243-c1ee-4f0d-a674-6cd9958f62a0",
        "parentId" : "23cb04d3-961d-4052-92fa-d13cc850bf24",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "Oh, this is on purpose.",
        "createdAt" : "2021-07-21T11:40:24Z",
        "updatedAt" : "2021-07-21T11:40:25Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "221e62227f09d7e29fbab5c674af29b1f829b91e",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +3978,3982 @@  def timestampType: AtomicType = getConf(TIMESTAMP_TYPE) match {\n    // SPARK-36227: Remove TimestampNTZ type support in Spark 3.2 with minimal code changes.\n    //              The configuration `TIMESTAMP_TYPE` is only effective for testing in Spark 3.2.\n    case \"TIMESTAMP_NTZ\" if Utils.isTesting =>\n      TimestampNTZType"
  },
  {
    "id" : "28a8d916-3fd7-42f4-91a2-f8c55daf94f6",
    "prId" : 33348,
    "prUrl" : "https://github.com/apache/spark/pull/33348#pullrequestreview-706916663",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a27a7d03-c265-47e0-85f8-c44ef4ed3fb8",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Is it okay to reuse the hive config for the purpose? Sounds more like `spark.sql.metastorePartitionPruning` now?",
        "createdAt" : "2021-07-15T02:47:06Z",
        "updatedAt" : "2021-07-15T02:47:06Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "da1fbcbd-7551-4e1f-be7a-3b36855d6bd5",
        "parentId" : "a27a7d03-c265-47e0-85f8-c44ef4ed3fb8",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "yeah I agree, that's the exactly same point I mentioned in the JIRA ..",
        "createdAt" : "2021-07-15T02:52:06Z",
        "updatedAt" : "2021-07-15T02:52:07Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "98cca643-0333-477b-a4b6-e1c7fb947bb2",
        "parentId" : "a27a7d03-c265-47e0-85f8-c44ef4ed3fb8",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "The reason I reused the config is because it is not about Hive tables, but HMS-based catalog which is shared by both Hive tables and non-Hive tables in Spark. This is similar to `spark.sql.hive.manageFilesourcePartitions` which is used by data source tables even though it has `hive` in its name.\r\n\r\n",
        "createdAt" : "2021-07-15T04:38:10Z",
        "updatedAt" : "2021-07-15T04:38:10Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      }
    ],
    "commit" : "f686496a2a21b1075ff182dc936643d71e1e85e3",
    "line" : 1,
    "diffHunk" : "@@ -1,1 +977,981 @@    .createWithDefault(false)\n\n  val HIVE_METASTORE_PARTITION_PRUNING =\n    buildConf(\"spark.sql.hive.metastorePartitionPruning\")\n      .doc(\"When true, some predicates will be pushed down into the Hive metastore so that \" +"
  },
  {
    "id" : "7d2b99d9-a95b-4050-a90c-08628d93a8cb",
    "prId" : 33348,
    "prUrl" : "https://github.com/apache/spark/pull/33348#pullrequestreview-708603463",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "64a55233-e16f-4f4e-8d91-7013e680f0f9",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "`HIVE_MANAGE_FILESOURCE_PARTITIONS` mentions that when it is enabled, it uses the metastore to prune partitions during query planning. Maybe we should update the doc too.",
        "createdAt" : "2021-07-15T02:58:37Z",
        "updatedAt" : "2021-07-15T02:58:37Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "f69dd937-592d-4590-98f6-e49e3dc0e4a9",
        "parentId" : "64a55233-e16f-4f4e-8d91-7013e680f0f9",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Yea, we should mention that it only does so when `spark.sql.hive.metastorePartitionPruning` is turned on.",
        "createdAt" : "2021-07-15T04:39:19Z",
        "updatedAt" : "2021-07-15T04:39:19Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "790de5fb-4048-4338-87c5-c90c046b8e95",
        "parentId" : "64a55233-e16f-4f4e-8d91-7013e680f0f9",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Seems not update yet?",
        "createdAt" : "2021-07-16T08:11:40Z",
        "updatedAt" : "2021-07-16T08:11:41Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "e128d840-a738-4425-9328-e8d2df9fb4be",
        "parentId" : "64a55233-e16f-4f4e-8d91-7013e680f0f9",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Hmm I thought this needs to be updated if we don't have this PR, since Spark doesn't prune partitions for file data source tables on metastore side. But with this PR the description becomes correct?",
        "createdAt" : "2021-07-16T17:21:19Z",
        "updatedAt" : "2021-07-16T17:21:19Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "0ec79b21-30b3-4fb4-bc1a-6a217e38ff43",
        "parentId" : "64a55233-e16f-4f4e-8d91-7013e680f0f9",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Oh, I see. I should mention `spark.sql.hive.metastorePartitionPruning` there. Will update.",
        "createdAt" : "2021-07-16T17:22:15Z",
        "updatedAt" : "2021-07-16T17:22:15Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      }
    ],
    "commit" : "f686496a2a21b1075ff182dc936643d71e1e85e3",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +980,984 @@    buildConf(\"spark.sql.hive.metastorePartitionPruning\")\n      .doc(\"When true, some predicates will be pushed down into the Hive metastore so that \" +\n           \"unmatching partitions can be eliminated earlier.\")\n      .version(\"1.5.0\")\n      .booleanConf"
  }
]