[
  {
    "id" : "1006393a-1dcd-46cb-9648-f462f3c15105",
    "prId" : 33655,
    "prUrl" : "https://github.com/apache/spark/pull/33655#pullrequestreview-724508457",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6f771c2c-d596-4ca9-a339-a39044e93897",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Could you add this check into `spark.sql.adaptive.advisoryPartitionSizeInBytes`, too?",
        "createdAt" : "2021-08-05T19:11:39Z",
        "updatedAt" : "2021-08-05T19:11:39Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "2f3635cd-b39d-4b85-bc81-36325b1b97f7",
        "parentId" : "6f771c2c-d596-4ca9-a339-a39044e93897",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "We can't add check in `ADVISORY_PARTITION_SIZE_IN_BYTES`, because it falls back to `SHUFFLE_TARGET_POSTSHUFFLE_INPUT_SIZE`, which is this conf.",
        "createdAt" : "2021-08-06T03:21:40Z",
        "updatedAt" : "2021-08-06T03:21:41Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "7d5b09ab-ea22-4004-b963-7a431f6b59bc",
        "parentId" : "6f771c2c-d596-4ca9-a339-a39044e93897",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "@cloud-fan . If we set negative values to `advisoryPartitionSizeInBytes` explicitly, it will not fallback to here. If we want to say, `advisoryPartitionSizeInBytes must be positive`, we should check both places, shouldn't we?",
        "createdAt" : "2021-08-06T16:06:34Z",
        "updatedAt" : "2021-08-06T16:06:34Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "eeeff3ae6bb62eedeb09011cc99514d240a8b7d0",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +481,485 @@      .version(\"1.6.0\")\n      .bytesConf(ByteUnit.BYTE)\n      .checkValue(_ > 0, \"advisoryPartitionSizeInBytes must be positive\")\n      .createWithDefaultString(\"64MB\")\n"
  },
  {
    "id" : "98d04720-367d-4e9e-8c61-371061b7bdc3",
    "prId" : 33444,
    "prUrl" : "https://github.com/apache/spark/pull/33444#pullrequestreview-711555608",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "23cb04d3-961d-4052-92fa-d13cc850bf24",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "indentation ?",
        "createdAt" : "2021-07-21T10:35:50Z",
        "updatedAt" : "2021-07-21T10:38:09Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      },
      {
        "id" : "ec7c2243-c1ee-4f0d-a674-6cd9958f62a0",
        "parentId" : "23cb04d3-961d-4052-92fa-d13cc850bf24",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "Oh, this is on purpose.",
        "createdAt" : "2021-07-21T11:40:24Z",
        "updatedAt" : "2021-07-21T11:40:25Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "221e62227f09d7e29fbab5c674af29b1f829b91e",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +3978,3982 @@  def timestampType: AtomicType = getConf(TIMESTAMP_TYPE) match {\n    // SPARK-36227: Remove TimestampNTZ type support in Spark 3.2 with minimal code changes.\n    //              The configuration `TIMESTAMP_TYPE` is only effective for testing in Spark 3.2.\n    case \"TIMESTAMP_NTZ\" if Utils.isTesting =>\n      TimestampNTZType"
  },
  {
    "id" : "28a8d916-3fd7-42f4-91a2-f8c55daf94f6",
    "prId" : 33348,
    "prUrl" : "https://github.com/apache/spark/pull/33348#pullrequestreview-706916663",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a27a7d03-c265-47e0-85f8-c44ef4ed3fb8",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Is it okay to reuse the hive config for the purpose? Sounds more like `spark.sql.metastorePartitionPruning` now?",
        "createdAt" : "2021-07-15T02:47:06Z",
        "updatedAt" : "2021-07-15T02:47:06Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "da1fbcbd-7551-4e1f-be7a-3b36855d6bd5",
        "parentId" : "a27a7d03-c265-47e0-85f8-c44ef4ed3fb8",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "yeah I agree, that's the exactly same point I mentioned in the JIRA ..",
        "createdAt" : "2021-07-15T02:52:06Z",
        "updatedAt" : "2021-07-15T02:52:07Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "98cca643-0333-477b-a4b6-e1c7fb947bb2",
        "parentId" : "a27a7d03-c265-47e0-85f8-c44ef4ed3fb8",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "The reason I reused the config is because it is not about Hive tables, but HMS-based catalog which is shared by both Hive tables and non-Hive tables in Spark. This is similar to `spark.sql.hive.manageFilesourcePartitions` which is used by data source tables even though it has `hive` in its name.\r\n\r\n",
        "createdAt" : "2021-07-15T04:38:10Z",
        "updatedAt" : "2021-07-15T04:38:10Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      }
    ],
    "commit" : "f686496a2a21b1075ff182dc936643d71e1e85e3",
    "line" : 1,
    "diffHunk" : "@@ -1,1 +977,981 @@    .createWithDefault(false)\n\n  val HIVE_METASTORE_PARTITION_PRUNING =\n    buildConf(\"spark.sql.hive.metastorePartitionPruning\")\n      .doc(\"When true, some predicates will be pushed down into the Hive metastore so that \" +"
  },
  {
    "id" : "7d2b99d9-a95b-4050-a90c-08628d93a8cb",
    "prId" : 33348,
    "prUrl" : "https://github.com/apache/spark/pull/33348#pullrequestreview-708603463",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "64a55233-e16f-4f4e-8d91-7013e680f0f9",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "`HIVE_MANAGE_FILESOURCE_PARTITIONS` mentions that when it is enabled, it uses the metastore to prune partitions during query planning. Maybe we should update the doc too.",
        "createdAt" : "2021-07-15T02:58:37Z",
        "updatedAt" : "2021-07-15T02:58:37Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "f69dd937-592d-4590-98f6-e49e3dc0e4a9",
        "parentId" : "64a55233-e16f-4f4e-8d91-7013e680f0f9",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Yea, we should mention that it only does so when `spark.sql.hive.metastorePartitionPruning` is turned on.",
        "createdAt" : "2021-07-15T04:39:19Z",
        "updatedAt" : "2021-07-15T04:39:19Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "790de5fb-4048-4338-87c5-c90c046b8e95",
        "parentId" : "64a55233-e16f-4f4e-8d91-7013e680f0f9",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Seems not update yet?",
        "createdAt" : "2021-07-16T08:11:40Z",
        "updatedAt" : "2021-07-16T08:11:41Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "e128d840-a738-4425-9328-e8d2df9fb4be",
        "parentId" : "64a55233-e16f-4f4e-8d91-7013e680f0f9",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Hmm I thought this needs to be updated if we don't have this PR, since Spark doesn't prune partitions for file data source tables on metastore side. But with this PR the description becomes correct?",
        "createdAt" : "2021-07-16T17:21:19Z",
        "updatedAt" : "2021-07-16T17:21:19Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "0ec79b21-30b3-4fb4-bc1a-6a217e38ff43",
        "parentId" : "64a55233-e16f-4f4e-8d91-7013e680f0f9",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Oh, I see. I should mention `spark.sql.hive.metastorePartitionPruning` there. Will update.",
        "createdAt" : "2021-07-16T17:22:15Z",
        "updatedAt" : "2021-07-16T17:22:15Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      }
    ],
    "commit" : "f686496a2a21b1075ff182dc936643d71e1e85e3",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +980,984 @@    buildConf(\"spark.sql.hive.metastorePartitionPruning\")\n      .doc(\"When true, some predicates will be pushed down into the Hive metastore so that \" +\n           \"unmatching partitions can be eliminated earlier.\")\n      .version(\"1.5.0\")\n      .booleanConf"
  },
  {
    "id" : "7238b7ae-65a9-47c1-a00d-f2a76aea8b8b",
    "prId" : 33176,
    "prUrl" : "https://github.com/apache/spark/pull/33176#pullrequestreview-697432760",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9478feea-b649-41dc-a071-782cb70ba33e",
        "parentId" : null,
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "The scope is bigger than this. It will affect timestamp literal, function `to_timestamp` and data source io, etc. We will get to that later.",
        "createdAt" : "2021-07-01T16:15:44Z",
        "updatedAt" : "2021-07-01T16:15:44Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "6c6bf3a04572a832ff9224c1a3cd61b81784c625",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +2828,2832 @@  val TIMESTAMP_TYPE =\n    buildConf(\"spark.sql.timestampType\")\n      .doc(\"Configures the default timestamp type of Spark SQL, including SQL DDL and Cast \" +\n        s\"clause. Setting the configuration as ${TimestampTypes.TIMESTAMP_NTZ.toString} will \" +\n        \"use TIMESTAMP WITHOUT TIME ZONE as the default type while putting it as \" +"
  },
  {
    "id" : "664cb507-9819-4442-8415-2a6f9270ddf1",
    "prId" : 33176,
    "prUrl" : "https://github.com/apache/spark/pull/33176#pullrequestreview-697488439",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "17cb3762-17e7-40ee-bcef-26125f26516e",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "I wonder about types literals, for instance `timestamp'2021-07-01 01:02:03'`. Should the config influence on it too?",
        "createdAt" : "2021-07-01T16:20:01Z",
        "updatedAt" : "2021-07-01T16:26:02Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "32a32ad8-5877-4026-b429-fbc3ae13a782",
        "parentId" : "17cb3762-17e7-40ee-bcef-26125f26516e",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "Yes, I mentioned in https://github.com/apache/spark/pull/33176#discussion_r662426271. The type literal logic with TIMESTAMP_NTZ:\r\n1. if there is no time zone part, return timestamp without time zone literal\r\n2. otherwise, return timestamp with local time zone. ",
        "createdAt" : "2021-07-01T17:21:11Z",
        "updatedAt" : "2021-07-01T17:21:11Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "6c6bf3a04572a832ff9224c1a3cd61b81784c625",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +2828,2832 @@  val TIMESTAMP_TYPE =\n    buildConf(\"spark.sql.timestampType\")\n      .doc(\"Configures the default timestamp type of Spark SQL, including SQL DDL and Cast \" +\n        s\"clause. Setting the configuration as ${TimestampTypes.TIMESTAMP_NTZ.toString} will \" +\n        \"use TIMESTAMP WITHOUT TIME ZONE as the default type while putting it as \" +"
  },
  {
    "id" : "5a76519c-a40b-470c-b393-53a2c2e24100",
    "prId" : 33176,
    "prUrl" : "https://github.com/apache/spark/pull/33176#pullrequestreview-697536637",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "734df2b1-6200-47bb-b792-9c6f74924bc9",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "I thought the compiler automatically calls the `toString` method in string interpolations, so, you can skip it. Or I am wrong?",
        "createdAt" : "2021-07-01T18:17:53Z",
        "updatedAt" : "2021-07-01T18:17:54Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "6c6bf3a04572a832ff9224c1a3cd61b81784c625",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +2829,2833 @@    buildConf(\"spark.sql.timestampType\")\n      .doc(\"Configures the default timestamp type of Spark SQL, including SQL DDL and Cast \" +\n        s\"clause. Setting the configuration as ${TimestampTypes.TIMESTAMP_NTZ.toString} will \" +\n        \"use TIMESTAMP WITHOUT TIME ZONE as the default type while putting it as \" +\n        s\"${TimestampTypes.TIMESTAMP_LTZ.toString} will use TIMESTAMP WITH LOCAL TIME ZONE. \" +"
  },
  {
    "id" : "67ddb6c1-31ff-442e-8fdf-ce60be0b2def",
    "prId" : 33172,
    "prUrl" : "https://github.com/apache/spark/pull/33172#pullrequestreview-697345900",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "163d76ce-a571-4e21-aa67-dc79b3707e30",
        "parentId" : null,
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "better to update the doc of\r\nADVISORY_PARTITION_SIZE_IN_BYTES",
        "createdAt" : "2021-07-01T14:21:12Z",
        "updatedAt" : "2021-07-01T14:21:50Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "edc1a263-a317-407b-9471-da04309929b1",
        "parentId" : "163d76ce-a571-4e21-aa67-dc79b3707e30",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "`ADVISORY_PARTITION_SIZE_IN_BYTES` doesn't mention `COALESCE_PARTITIONS_MIN_PARTITION_NUM` either. I think it's fine to only refer to \"big\" confs in the \"small\" confs, not from the other side.",
        "createdAt" : "2021-07-01T14:54:02Z",
        "updatedAt" : "2021-07-01T14:54:02Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "51546e87c33064c80f902d4eecff1f640222dfb2",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +529,533 @@\n  val COALESCE_PARTITIONS_PARALLELISM_FIRST =\n    buildConf(\"spark.sql.adaptive.coalescePartitions.parallelismFirst\")\n      .doc(\"When true, Spark ignores the target size specified by \" +\n        s\"'${ADVISORY_PARTITION_SIZE_IN_BYTES.key}' (default 64MB) when coalescing contiguous \" +"
  },
  {
    "id" : "0871aeb8-ff77-4d83-93d2-f707dd159d71",
    "prId" : 33172,
    "prUrl" : "https://github.com/apache/spark/pull/33172#pullrequestreview-697522081",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4382901c-1208-4067-8e57-761fa2ec15e2",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "@viirya is it clearer to you now?",
        "createdAt" : "2021-07-01T17:22:59Z",
        "updatedAt" : "2021-07-01T17:23:05Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "56dc5122-5762-4883-bd39-4554eb66ac2c",
        "parentId" : "4382901c-1208-4067-8e57-761fa2ec15e2",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "okay",
        "createdAt" : "2021-07-01T18:03:21Z",
        "updatedAt" : "2021-07-01T18:03:21Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "51546e87c33064c80f902d4eecff1f640222dfb2",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +536,540 @@        \"This is to avoid performance regression when enabling adaptive query execution. \" +\n        \"It's recommended to set this config to false and respect the target size specified by \" +\n        s\"'${ADVISORY_PARTITION_SIZE_IN_BYTES.key}'.\")\n      .version(\"3.2.0\")\n      .booleanConf"
  },
  {
    "id" : "9a073cec-0a03-4dab-a0cb-ce1e0fb379b7",
    "prId" : 33081,
    "prUrl" : "https://github.com/apache/spark/pull/33081#pullrequestreview-706975183",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a29f8726-aefb-4347-8e58-64c8514b0fcf",
        "parentId" : null,
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "How about `spark.sql.streaming.sessionWindow.localMerge.enabled` or `spark.sql.streaming.sessionWindow. mergeSessionsInLocalPartition.enabled`?",
        "createdAt" : "2021-07-15T04:16:35Z",
        "updatedAt" : "2021-07-15T04:51:30Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      },
      {
        "id" : "382323ca-d659-4518-a500-4261e80a4a94",
        "parentId" : "a29f8726-aefb-4347-8e58-64c8514b0fcf",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "Comparing with the similar logic of AggUtils, maybe we can also remove this config? Just always do the local merge?",
        "createdAt" : "2021-07-15T04:22:12Z",
        "updatedAt" : "2021-07-15T04:51:30Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      },
      {
        "id" : "0c1f4b70-b3aa-4cac-bf22-b9e60a90ff91",
        "parentId" : "a29f8726-aefb-4347-8e58-64c8514b0fcf",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Yea, is it necessary to have this config? Seems we can always do it.",
        "createdAt" : "2021-07-15T05:17:31Z",
        "updatedAt" : "2021-07-15T05:39:10Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "87937ab3-25cf-4ae6-81d3-ad2a7cf3d594",
        "parentId" : "a29f8726-aefb-4347-8e58-64c8514b0fcf",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "As I explained on the doc method, this would incur additional \"logical sort\", so only useful when there're lots of input rows which are going to be consolidated into same session. The benefit is dependent on the characteristic of data.\r\n\r\nIf we want to pick one between two to simplify, it would be probably safer to remove local aggregation.",
        "createdAt" : "2021-07-15T06:34:59Z",
        "updatedAt" : "2021-07-15T06:34:59Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "bbade3501f16e9437ba9af4feca3e2029785d273",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1612,1616 @@\n  val STREAMING_SESSION_WINDOW_MERGE_SESSIONS_IN_LOCAL_PARTITION =\n    buildConf(\"spark.sql.streaming.sessionWindow.merge.sessions.in.local.partition\")\n      .internal()\n      .doc(\"When true, streaming session window sorts and merge sessions in local partition \" +"
  },
  {
    "id" : "67e1a57b-38af-4853-a248-57634c7f8a8d",
    "prId" : 33081,
    "prUrl" : "https://github.com/apache/spark/pull/33081#pullrequestreview-706904781",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "990650e9-93f6-4dfc-8638-327536180809",
        "parentId" : null,
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "nit: version",
        "createdAt" : "2021-07-15T04:22:21Z",
        "updatedAt" : "2021-07-15T04:51:30Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "bbade3501f16e9437ba9af4feca3e2029785d273",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +1618,1622 @@        \"there're lots of rows in a batch being assigned to same sessions.\")\n      .version(\"3.2.0\")\n      .booleanConf\n      .createWithDefault(false)\n"
  },
  {
    "id" : "6ae99fdc-36f9-41be-b878-99b083b7819b",
    "prId" : 32944,
    "prUrl" : "https://github.com/apache/spark/pull/32944#pullrequestreview-698959883",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d6bc2330-84cb-4b5d-956f-ea1ef838a53d",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "the only think is that the version has to be 3.3.0 since we cut the branch now. Since this PR won't likely affect anything in the main code, I am okay with merging to 3.2.0 either tho. I will leave it to @cloud-fan and you.",
        "createdAt" : "2021-07-05T00:31:06Z",
        "updatedAt" : "2021-07-05T00:31:25Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "540b702e-dae5-43da-ac24-5af339be85dd",
        "parentId" : "d6bc2330-84cb-4b5d-956f-ea1ef838a53d",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "3.2 is the first version that enables AQE by default, and this seems to be a useful extension. Let's include it in 3.2.",
        "createdAt" : "2021-07-05T09:05:55Z",
        "updatedAt" : "2021-07-05T09:05:55Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "ac5c12186035a47bbe7c0b1891564066db676354",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +683,687 @@      .doc(\"The custom cost evaluator class to be used for adaptive execution. If not being set,\" +\n        \" Spark will use its own SimpleCostEvaluator by default.\")\n      .version(\"3.2.0\")\n      .stringConf\n      .createOptional"
  },
  {
    "id" : "349c325b-92af-4409-b86f-3b3eaf20ba32",
    "prId" : 32865,
    "prUrl" : "https://github.com/apache/spark/pull/32865#pullrequestreview-681525932",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6438bf5a-e566-443d-9155-99eabe3d6f30",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Q: we need the two separate locks for the static/non-static configs?",
        "createdAt" : "2021-06-11T01:34:16Z",
        "updatedAt" : "2021-06-11T01:34:16Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "b6e0a028-f58e-4b0f-a06d-c0740d96782f",
        "parentId" : "6438bf5a-e566-443d-9155-99eabe3d6f30",
        "authorId" : "56f79eae-eef9-4cac-8f0c-c788c9a86e3a",
        "body" : "Thank you for your review! I think these are two different sets of configs, so using two separate locks should be better.",
        "createdAt" : "2021-06-11T07:22:21Z",
        "updatedAt" : "2021-06-11T07:23:43Z",
        "lastEditedBy" : "56f79eae-eef9-4cac-8f0c-c788c9a86e3a",
        "tags" : [
        ]
      }
    ],
    "commit" : "994cab24f91d681f187b3062da0de62a45b0ba6a",
    "line" : 30,
    "diffHunk" : "@@ -1,1 +60,64 @@  private[this] var sqlConfEntries: util.Map[String, ConfigEntry[_]] = util.Collections.emptyMap()\n\n  private[this] val staticConfKeysUpdateLock = new Object\n\n  @volatile"
  },
  {
    "id" : "666dfd56-fa92-4bc9-bbfa-5e1fbe5c3881",
    "prId" : 32865,
    "prUrl" : "https://github.com/apache/spark/pull/32865#pullrequestreview-681413755",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a1e8dfea-0a9d-4e29-ba06-b9595b1e7d61",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "~Is `private` enough?~ Never mind~",
        "createdAt" : "2021-06-11T03:19:55Z",
        "updatedAt" : "2021-06-11T03:22:22Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "994cab24f91d681f187b3062da0de62a45b0ba6a",
    "line" : 51,
    "diffHunk" : "@@ -1,1 +80,84 @@  }\n\n  private[internal] def getConfigEntry(key: String): ConfigEntry[_] = {\n    sqlConfEntries.get(key)\n  }"
  },
  {
    "id" : "8fb0e826-7178-41c4-839f-d9139027d4ac",
    "prId" : 32550,
    "prUrl" : "https://github.com/apache/spark/pull/32550#pullrequestreview-670156365",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1f734a2c-db32-43bc-8e73-8de7ee83e8df",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I would say:\r\n- \"If this value is not smaller than\" => \"If this value is larger than\"\r\n- \"all the partition size are not larger than this config\" => \"all the partition size are smaller than this config\"",
        "createdAt" : "2021-05-27T10:40:23Z",
        "updatedAt" : "2021-05-27T10:40:23Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "394d2736-4b78-4192-8b09-b2480488de9d",
        "parentId" : "1f734a2c-db32-43bc-8e73-8de7ee83e8df",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "I thought about this but the formula used equal, that said it would be `If this value is larger than or equal to xxx`. Do you think it's better ?",
        "createdAt" : "2021-05-27T13:05:50Z",
        "updatedAt" : "2021-05-27T13:05:50Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      }
    ],
    "commit" : "957215452e9df57f5e0424f63556897ea928744f",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +602,606 @@    buildConf(\"spark.sql.adaptive.maxShuffledHashJoinLocalMapThreshold\")\n      .doc(\"Configures the maximum size in bytes per partition that can be allowed to build \" +\n        \"local hash map. If this value is not smaller than \" +\n        s\"${ADVISORY_PARTITION_SIZE_IN_BYTES.key} and all the partition size are not larger \" +\n        \"than this config, join selection prefer to use shuffled hash join instead of \" +"
  },
  {
    "id" : "fe8b4a90-7333-4727-bc6c-9cd97deb7921",
    "prId" : 32129,
    "prUrl" : "https://github.com/apache/spark/pull/32129#pullrequestreview-633469192",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7a8df72d-8cd7-4900-ac20-42737ba417d8",
        "parentId" : null,
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "Moving `ANSI_ENABLED` to the front so that other configurations can refer to it without compiling errors.",
        "createdAt" : "2021-04-12T12:20:33Z",
        "updatedAt" : "2021-04-12T17:31:41Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "62cee4f24ed49d9c8f967f82267dbd3f3180c32c",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +207,211 @@    .createWithDefault(100)\n\n  val ANSI_ENABLED = buildConf(\"spark.sql.ansi.enabled\")\n    .doc(\"When true, Spark SQL uses an ANSI compliant dialect instead of being Hive compliant. \" +\n      \"For example, Spark will throw an exception at runtime instead of returning null results \" +"
  },
  {
    "id" : "3a2baac8-d3ae-4330-8fd3-16c76a499250",
    "prId" : 32049,
    "prUrl" : "https://github.com/apache/spark/pull/32049#pullrequestreview-627553202",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0ec09a71-2e6d-436b-9565-6a026ba7a0fa",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Just curious - would anyone ever _not_ want to push it down?\r\nI'm surprised, I thought we already did this!\r\nCC @cloud-fan ",
        "createdAt" : "2021-04-04T15:01:56Z",
        "updatedAt" : "2021-04-28T06:21:16Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "3efb61fd-870a-420f-b2fd-2ece2693ba71",
        "parentId" : "0ec09a71-2e6d-436b-9565-6a026ba7a0fa",
        "authorId" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "body" : "@srowen Hello Sean :)\r\nActually we only have filter push down for parquet, not aggregate push down yet. I will probably change the default to true after this PR gets reviewed and fully tested.",
        "createdAt" : "2021-04-04T16:12:52Z",
        "updatedAt" : "2021-04-28T06:21:16Z",
        "lastEditedBy" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "tags" : [
        ]
      }
    ],
    "commit" : "5c2b630a61d8ea9263116c37e154884a5cabd2ca",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +773,777 @@      .createWithDefault(10)\n\n  val PARQUET_AGGREGATE_PUSHDOWN_ENABLED = buildConf(\"spark.sql.parquet.aggregatePushdown\")\n    .doc(\"Enables Parquet aggregate push-down optimization when set to true.\")\n    .version(\"3.2.0\")"
  },
  {
    "id" : "432792bf-a654-4dd4-8558-d66da8540216",
    "prId" : 32049,
    "prUrl" : "https://github.com/apache/spark/pull/32049#pullrequestreview-628362844",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3fd0db59-3c93-4deb-8a96-aeb7ad09116a",
        "parentId" : null,
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "Shall it be an internal config or not? Do we expect this one to be user-facing and tune it frequently?",
        "createdAt" : "2021-04-05T07:47:49Z",
        "updatedAt" : "2021-04-28T06:21:16Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      },
      {
        "id" : "868bb4bc-c3ca-415d-9a1c-69bbaf17d6ac",
        "parentId" : "3fd0db59-3c93-4deb-8a96-aeb7ad09116a",
        "authorId" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "body" : "Thanks for reviewing!\r\nI think this should be similar to `PARQUET_FILTER_PUSHDOWN_ENABLED` and be a user-facing config. I guess we can default it to true in the future after we have more testing.",
        "createdAt" : "2021-04-06T01:10:13Z",
        "updatedAt" : "2021-04-28T06:21:16Z",
        "lastEditedBy" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "tags" : [
        ]
      }
    ],
    "commit" : "5c2b630a61d8ea9263116c37e154884a5cabd2ca",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +774,778 @@\n  val PARQUET_AGGREGATE_PUSHDOWN_ENABLED = buildConf(\"spark.sql.parquet.aggregatePushdown\")\n    .doc(\"Enables Parquet aggregate push-down optimization when set to true.\")\n    .version(\"3.2.0\")\n    .booleanConf"
  },
  {
    "id" : "c05d7789-3578-419c-9318-e0f52643bbf8",
    "prId" : 31986,
    "prUrl" : "https://github.com/apache/spark/pull/31986#pullrequestreview-656212437",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2ddbf876-4228-417f-88c1-779a697b1801",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "`in.memory` looks weird to me. Maybe just `inMemory`?",
        "createdAt" : "2021-05-11T00:52:31Z",
        "updatedAt" : "2021-05-11T04:07:22Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "275ee6db-50f4-45e4-a179-68d19d8fef3f",
        "parentId" : "2ddbf876-4228-417f-88c1-779a697b1801",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "I agree with you, but the name semantic follows existing configurations. Please search `_IN_MEMORY_THRESHOLD` into codebase.",
        "createdAt" : "2021-05-11T01:34:56Z",
        "updatedAt" : "2021-05-11T04:07:22Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "8c2eed82d553ddacb36088954232ae704a855094",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +2002,2006 @@\n  val SESSION_WINDOW_BUFFER_IN_MEMORY_THRESHOLD =\n    buildConf(\"spark.sql.sessionWindow.buffer.in.memory.threshold\")\n      .internal()\n      .doc(\"Threshold for number of windows guaranteed to be held in memory by the \" +"
  },
  {
    "id" : "02585522-117f-4689-a4b7-0377b1e7f4b4",
    "prId" : 31689,
    "prUrl" : "https://github.com/apache/spark/pull/31689#pullrequestreview-601362429",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "695bd7d7-5aaf-4850-9168-259943cbe925",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Seems a reasonable fix cc: @srowen @cloud-fan @viirya ",
        "createdAt" : "2021-03-02T02:57:07Z",
        "updatedAt" : "2021-03-02T02:57:07Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "4ffa2576782e410cef4d5cb3d8865362679ff11c",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +55,59 @@\n  private[sql] val sqlConfEntries =\n    new ConcurrentHashMap[String, ConfigEntry[_]]()\n\n  val staticConfKeys: java.util.Set[String] ="
  },
  {
    "id" : "a4769220-e41b-4012-a257-c72def286e7c",
    "prId" : 31578,
    "prUrl" : "https://github.com/apache/spark/pull/31578#pullrequestreview-592871935",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c43cccb4-5c6f-448d-9bda-920f914986bd",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Ur, is this guide correct? I guess the users are using `.format(\"avro\")` already.",
        "createdAt" : "2021-02-17T22:19:47Z",
        "updatedAt" : "2021-02-17T22:20:10Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "e27a2ca7-f0b2-4e6b-99fb-663c9f627c17",
        "parentId" : "c43cccb4-5c6f-448d-9bda-920f914986bd",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "The configs is about automatically mapping of `.format(\"com.databricks.spark.avro\")` to `.format(\"avro\")`, right? If we remove the config in the future, \"com.databricks.spark.avro\" will be not mapped to built in `avro`. So, in the guide, we recommend to change users code, and use the `avro` format directly.",
        "createdAt" : "2021-02-18T05:48:41Z",
        "updatedAt" : "2021-02-18T05:48:42Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "a2c2cfeb-41c8-49d0-8006-d1d38a49b6c9",
        "parentId" : "c43cccb4-5c6f-448d-9bda-920f914986bd",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Thanks. I was confused at that.",
        "createdAt" : "2021-02-18T05:53:29Z",
        "updatedAt" : "2021-02-18T05:53:30Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "660db04807559077244a31b916339a2876aeb0db",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +3148,3152 @@        s\"Use '${AVRO_REBASE_MODE_IN_READ.key}' instead.\"),\n      DeprecatedConfig(LEGACY_REPLACE_DATABRICKS_SPARK_AVRO_ENABLED.key, \"3.2\",\n        \"\"\"Use `.format(\"avro\")` in `DataFrameWriter` or `DataFrameReader` instead.\"\"\")\n    )\n"
  },
  {
    "id" : "bf78c885-f580-4fe8-96f5-6d3b01a4a5fa",
    "prId" : 31571,
    "prUrl" : "https://github.com/apache/spark/pull/31571#pullrequestreview-592680008",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f3f2f118-6858-414a-82e8-69197c7182df",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@gengliangwang do we need this conf? We do the same thing in `com.databricks.spark.csv` already by default internally. We could just make it exposed and deprecate this config.",
        "createdAt" : "2021-02-17T01:44:38Z",
        "updatedAt" : "2021-02-17T01:45:32Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "0decfaa4-6818-4e04-a3c7-37a831bfac6b",
        "parentId" : "f3f2f118-6858-414a-82e8-69197c7182df",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "In this way, I think it will address most of concerns raised.",
        "createdAt" : "2021-02-17T01:45:06Z",
        "updatedAt" : "2021-02-17T01:45:07Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "503c2e3f-6878-4840-9fba-89aa23159455",
        "parentId" : "f3f2f118-6858-414a-82e8-69197c7182df",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "And then we could remove both `com.databricks.spark.csv` and `com.databricks.spark.avro` fallbacks together in the future, of course (Spark 4.0?). I don't think it makes sense to keep both mapping forever.",
        "createdAt" : "2021-02-17T01:49:55Z",
        "updatedAt" : "2021-02-17T01:49:55Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "8bae371c-27c9-4541-8611-bdacce720505",
        "parentId" : "f3f2f118-6858-414a-82e8-69197c7182df",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Let's deprecate it and then remove it in later versions. It's OK if a deprecated internal config is mentioned in public docs for migration purpose.",
        "createdAt" : "2021-02-17T15:59:14Z",
        "updatedAt" : "2021-02-17T15:59:14Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "1b48cdcc-cf23-4d85-b3b8-34d7084f1420",
        "parentId" : "f3f2f118-6858-414a-82e8-69197c7182df",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Here is the PR https://github.com/apache/spark/pull/31578 to deprecate the config.",
        "createdAt" : "2021-02-17T18:29:52Z",
        "updatedAt" : "2021-02-17T18:29:52Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "1c55e9f4-584f-4073-a4e9-148e27dedbd7",
        "parentId" : "f3f2f118-6858-414a-82e8-69197c7182df",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Thank you all!",
        "createdAt" : "2021-02-17T22:21:28Z",
        "updatedAt" : "2021-02-17T22:21:28Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "c43224e894971ac9e500a0d59b1715f8f922caff",
    "line" : 3,
    "diffHunk" : "@@ -1,1 +2505,2509 @@\n  val LEGACY_REPLACE_DATABRICKS_SPARK_AVRO_ENABLED =\n    buildConf(\"spark.sql.legacy.replaceDatabricksSparkAvro.enabled\")\n      .doc(\"If it is set to true, the data source provider com.databricks.spark.avro is mapped \" +\n        \"to the built-in but external Avro data source module for backward compatibility.\")"
  },
  {
    "id" : "59e46a3b-9064-42bd-b86a-1edb93204b24",
    "prId" : 31421,
    "prUrl" : "https://github.com/apache/spark/pull/31421#pullrequestreview-581056259",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fef6313d-7d16-4b9b-b634-e728b5a49830",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "ditto, make it more specific.",
        "createdAt" : "2021-02-02T07:40:12Z",
        "updatedAt" : "2021-02-02T07:40:12Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "971ccbd5947924992a0c24acd40d40f1587d7741",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +2476,2480 @@    buildConf(\"spark.sql.legacy.parseNullPartitionSpecAsStringLiteral\")\n      .internal()\n      .doc(\"If it is set to true, a null partition value is parsed as a string literal of its \" +\n        \"text representation, e.g., string 'null'. Otherwise, null partition values are parsed \" +\n        \"as they are.\")"
  },
  {
    "id" : "d95b8dbd-fc0b-4832-8198-bddc8024c1ac",
    "prId" : 31421,
    "prUrl" : "https://github.com/apache/spark/pull/31421#pullrequestreview-581062903",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9e1b8ae8-f046-4888-92ec-e7ed6d12af47",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "the version should be 3.1.1, as we are going to backport this.",
        "createdAt" : "2021-02-02T07:40:33Z",
        "updatedAt" : "2021-02-02T07:40:33Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "aeeb1c7f-a5fe-44bd-8d2a-96c15e321097",
        "parentId" : "9e1b8ae8-f046-4888-92ec-e7ed6d12af47",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "should be 3.0.2 actually.",
        "createdAt" : "2021-02-02T07:42:52Z",
        "updatedAt" : "2021-02-02T07:42:52Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "7010947b-a6cf-4dfc-8df8-ec32d01cd91c",
        "parentId" : "9e1b8ae8-f046-4888-92ec-e7ed6d12af47",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Ah I see, then how should we place it in the migration guide? Repeat it 3 times in 3.0.2, 3.1.1 and 3.2.0 sections?",
        "createdAt" : "2021-02-02T07:46:40Z",
        "updatedAt" : "2021-02-02T07:46:40Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "935f5e60-073c-4953-abc4-d7b73f8a7ae3",
        "parentId" : "9e1b8ae8-f046-4888-92ec-e7ed6d12af47",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I think we have documented only in 3.0.2 so far in such cases ..",
        "createdAt" : "2021-02-02T07:50:42Z",
        "updatedAt" : "2021-02-02T07:50:42Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "971ccbd5947924992a0c24acd40d40f1587d7741",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +2479,2483 @@        \"text representation, e.g., string 'null'. Otherwise, null partition values are parsed \" +\n        \"as they are.\")\n      .version(\"3.2.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "1ab4d35f-d105-4f2e-958a-b486e6befd0a",
    "prId" : 31173,
    "prUrl" : "https://github.com/apache/spark/pull/31173#pullrequestreview-567895812",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2b9dd652-a11c-4863-8069-e650d2c942c3",
        "parentId" : null,
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "This change add a new configuration. cc @cloud-fan",
        "createdAt" : "2021-01-14T06:52:53Z",
        "updatedAt" : "2021-01-14T06:52:53Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      }
    ],
    "commit" : "5c8fa18aa9e1e0ff1436361a7b67055647f39d5c",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +3050,3054 @@     .version(\"3.2.0\")\n    .booleanConf\n    .createWithDefault(false)\n\n  /**"
  },
  {
    "id" : "6195228a-6811-4225-a90c-0d68a33dec01",
    "prId" : 31173,
    "prUrl" : "https://github.com/apache/spark/pull/31173#pullrequestreview-571894946",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "705e73a9-8036-4632-911f-7dc4b129545a",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "nit: Looks wrong indents.",
        "createdAt" : "2021-01-20T06:05:38Z",
        "updatedAt" : "2021-01-20T06:05:38Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "5c8fa18aa9e1e0ff1436361a7b67055647f39d5c",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +3047,3051 @@  val CLI_PRINT_HEADER =\n    buildConf(\"spark.sql.cli.print.header\")\n     .doc(\"When set to true, spark-sql CLI prints the names of the columns in query output.\")\n     .version(\"3.2.0\")\n    .booleanConf"
  },
  {
    "id" : "867990aa-fabf-45ae-8b06-a1ac2f1b4a4d",
    "prId" : 31002,
    "prUrl" : "https://github.com/apache/spark/pull/31002#pullrequestreview-560810087",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "288d0512-9ff0-43a5-8e76-7c923a8b44dc",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Maybe we should also fix in `DataFrameWriter.scala` and `readwriter.py` too.",
        "createdAt" : "2021-01-04T06:57:59Z",
        "updatedAt" : "2021-01-04T07:01:55Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "f6b7bfe7-13be-4729-a083-216806fd3195",
        "parentId" : "288d0512-9ff0-43a5-8e76-7c923a8b44dc",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Sure, thanks!",
        "createdAt" : "2021-01-04T06:58:17Z",
        "updatedAt" : "2021-01-04T07:01:55Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "ca1aa2c80e38147f87e339463258654c81827bdd",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +797,801 @@      \"`orc.compress` is specified in the table-specific options/properties, the precedence \" +\n      \"would be `compression`, `orc.compress`, `spark.sql.orc.compression.codec`.\" +\n      \"Acceptable values include: none, uncompressed, snappy, zlib, lzo, zstd.\")\n    .version(\"2.3.0\")\n    .stringConf"
  },
  {
    "id" : "6361a479-e057-43af-8540-f0d7ab70efb5",
    "prId" : 31002,
    "prUrl" : "https://github.com/apache/spark/pull/31002#pullrequestreview-560817427",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "519e4954-ef39-46e9-a395-04d65e4b8845",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Btw, we cannot use `lz4` for the orc format?",
        "createdAt" : "2021-01-04T07:11:18Z",
        "updatedAt" : "2021-01-04T07:11:19Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "5aeedc5d-a6ae-4152-91ee-f1089a4c4cd5",
        "parentId" : "519e4954-ef39-46e9-a395-04d65e4b8845",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Is it supported before, @maropu ?",
        "createdAt" : "2021-01-04T07:12:36Z",
        "updatedAt" : "2021-01-04T07:12:36Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "1d83a4d4-e0b2-4cfd-9e38-ed2de4331cf9",
        "parentId" : "519e4954-ef39-46e9-a395-04d65e4b8845",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "I'm not familiar with this part though, I saw the ORC code: \r\nhttps://github.com/apache/orc/blob/f6b6b2ea70f1b74e2ffd897e8985ffcd1c082582/java/core/src/java/org/apache/orc/CompressionKind.java#L25-L27",
        "createdAt" : "2021-01-04T07:15:47Z",
        "updatedAt" : "2021-01-04T07:15:47Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "35863d3d-aa5c-4d83-a563-a3c31eddb89e",
        "parentId" : "519e4954-ef39-46e9-a395-04d65e4b8845",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "ORC does, but what I mean is that in Apache Spark.",
        "createdAt" : "2021-01-04T07:17:26Z",
        "updatedAt" : "2021-01-04T07:17:26Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "c7417f7b-ed51-4094-836b-2cd3775ffa6a",
        "parentId" : "519e4954-ef39-46e9-a395-04d65e4b8845",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Ah, ok. nvm. Just a question.",
        "createdAt" : "2021-01-04T07:20:00Z",
        "updatedAt" : "2021-01-04T07:20:01Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "ef744109-effb-45cd-bb27-2dd6df41415b",
        "parentId" : "519e4954-ef39-46e9-a395-04d65e4b8845",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "I just didn't use it before. Feel free to make a PR if you want to add it, @maropu .",
        "createdAt" : "2021-01-04T07:20:11Z",
        "updatedAt" : "2021-01-04T07:20:11Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "ca1aa2c80e38147f87e339463258654c81827bdd",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +801,805 @@    .stringConf\n    .transform(_.toLowerCase(Locale.ROOT))\n    .checkValues(Set(\"none\", \"uncompressed\", \"snappy\", \"zlib\", \"lzo\", \"zstd\"))\n    .createWithDefault(\"snappy\")\n"
  },
  {
    "id" : "b9eb8a75-54c3-408a-82b5-5a73dded4548",
    "prId" : 30897,
    "prUrl" : "https://github.com/apache/spark/pull/30897#pullrequestreview-557467159",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cfca4882-0aea-413d-95de-1a2cc1ccfc47",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Please mention this on the PR title, @ulysses-you . ",
        "createdAt" : "2020-12-23T02:02:30Z",
        "updatedAt" : "2020-12-23T03:25:56Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "a7a66a75-73fe-46a4-8854-549dcc94c47e",
        "parentId" : "cfca4882-0aea-413d-95de-1a2cc1ccfc47",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "yes, update it.",
        "createdAt" : "2020-12-23T02:03:42Z",
        "updatedAt" : "2020-12-23T03:25:56Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      }
    ],
    "commit" : "8701feb11173deb82c8bb8070e13652a76abe38e",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +972,976 @@        \"set for each statement via `java.sql.Statement.setQueryTimeout` and they are smaller \" +\n        \"than this configuration value, they take precedence. If you set this timeout and prefer \" +\n        \"to cancel the queries right away without waiting task to finish, consider enabling \" +\n        s\"${THRIFTSERVER_FORCE_CANCEL.key} together.\")\n      .version(\"3.1.0\")"
  },
  {
    "id" : "b7e43f00-3307-4b35-86ec-53a059b25556",
    "prId" : 30554,
    "prUrl" : "https://github.com/apache/spark/pull/30554#pullrequestreview-541181246",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c26c59ee-4eca-4641-8297-163414880747",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "This seems to be inconsistent because the other code expects `LEGACY_CREATE_HIVE_TABLE_BY_DEFAULT_ENABLED`.",
        "createdAt" : "2020-11-30T18:29:34Z",
        "updatedAt" : "2020-12-02T08:50:59Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "bff924de21b8af2da17f31591768ce02c784bd43",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +2923,2927 @@    .createWithDefault(\"\")\n\n  val LEGACY_CREATE_HIVE_TABLE_BY_DEFAULT =\n    buildConf(\"spark.sql.legacy.createHiveTableByDefault\")\n      .internal()"
  },
  {
    "id" : "ca7d75aa-52f3-4dff-a72c-44ed661710b2",
    "prId" : 30554,
    "prUrl" : "https://github.com/apache/spark/pull/30554#pullrequestreview-542416388",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ce57e948-4f3b-434e-bfa5-387fa1a74bcc",
        "parentId" : null,
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "If this is a \"legacy\" key mention how long folks can depend on it.",
        "createdAt" : "2020-12-02T00:25:31Z",
        "updatedAt" : "2020-12-02T08:50:59Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "47ae3ac5-5ddf-4a89-85dd-a81c2cdf88bc",
        "parentId" : "ce57e948-4f3b-434e-bfa5-387fa1a74bcc",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "There are already a lot of legacy configurations, and there's no plan for how long we'll keep it. That's what I know as far as I have followed in the community. It would be a separate issue to decide lifetime of legacy configurations but ideally the legacy configurations will be removed in the major release bumpup I guess. I remember I discussed this with Sean as well somewhere. cc @srowen FYI",
        "createdAt" : "2020-12-02T00:31:23Z",
        "updatedAt" : "2020-12-02T08:50:59Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "57433591-9d80-49ac-929c-74d9c208509a",
        "parentId" : "ce57e948-4f3b-434e-bfa5-387fa1a74bcc",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "If we have a general policy for legacy configs that means this remains the same until Spark 4 by default I guess there is no need to document it here (I don't remember that conversation but I was out for a few months last/this year).",
        "createdAt" : "2020-12-02T00:53:06Z",
        "updatedAt" : "2020-12-02T08:50:59Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "bff924de21b8af2da17f31591768ce02c784bd43",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +2927,2931 @@      .internal()\n      .doc(\"When set to true, CREATE TABLE syntax without USING or STORED AS will use Hive \" +\n        s\"instead of the value of ${DEFAULT_DATA_SOURCE_NAME.key} as the table provider.\")\n      .version(\"3.1.0\")\n      .booleanConf"
  },
  {
    "id" : "ba83befb-d0f9-4957-9adb-df914c60e4b5",
    "prId" : 30341,
    "prUrl" : "https://github.com/apache/spark/pull/30341#pullrequestreview-528781095",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "58a71ca8-e373-4d0a-aa99-c3ccc69469ba",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "`100` is enough?",
        "createdAt" : "2020-11-12T06:14:34Z",
        "updatedAt" : "2020-11-17T01:00:33Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "079b4d46-b8ed-47e8-aa40-ac4e2523ebc2",
        "parentId" : "58a71ca8-e373-4d0a-aa99-c3ccc69469ba",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "This default value is copied from code generator's cache setting. I guess this should be enough.",
        "createdAt" : "2020-11-12T06:28:42Z",
        "updatedAt" : "2020-11-17T01:00:33Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "db115d6d850942eca5dc6fac80896b1038561e51",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +547,551 @@      .intConf\n      .checkValue(_ >= 0, \"The maximum must not be negative\")\n      .createWithDefault(100)\n\n  val CASE_SENSITIVE = buildConf(\"spark.sql.caseSensitive\")"
  },
  {
    "id" : "b75dccb4-3846-4736-9a5c-2e0353c46271",
    "prId" : 30325,
    "prUrl" : "https://github.com/apache/spark/pull/30325#pullrequestreview-531259095",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d821e514-3b60-48af-981e-a4010a23cbf5",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "static conf should go to `StaticSQLConf`",
        "createdAt" : "2020-11-16T12:04:34Z",
        "updatedAt" : "2020-11-17T02:11:32Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "231cd6f15f3fa71d677aae7170e1e404b82cef2a",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +816,820 @@      .createWithDefault(true)\n\n  val HIVE_METASTORE_PARTITION_PRUNING_INSET_THRESHOLD =\n    buildConf(\"spark.sql.hive.metastorePartitionPruningInSetThreshold\")\n      .doc(\"The threshold of set size for InSet predicate when pruning partitions through Hive \" +"
  },
  {
    "id" : "a31fde97-2b50-4668-ac61-d457b19409e9",
    "prId" : 30225,
    "prUrl" : "https://github.com/apache/spark/pull/30225#pullrequestreview-527880909",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "121b6659-3eec-4cd1-85a3-8ff907f07c3b",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "`-1` to follow the Hive config?",
        "createdAt" : "2020-11-11T05:55:47Z",
        "updatedAt" : "2020-12-07T05:36:46Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "020d7dc8c2eb299540ead4be974ba45e90d81a9f",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +845,849 @@      .intConf\n      .checkValue(_ >= -1, \"The maximum must be a positive integer, -1 to follow the Hive config.\")\n      .createWithDefault(100000)\n\n  val HIVE_MANAGE_FILESOURCE_PARTITIONS ="
  },
  {
    "id" : "a76c5048-3376-4af1-8bad-34b5cfd73faf",
    "prId" : 30225,
    "prUrl" : "https://github.com/apache/spark/pull/30225#pullrequestreview-544187074",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5646c47d-f55c-4d0c-8356-fffffe587c10",
        "parentId" : null,
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "have you considered to keep the default behavior as it is but allow it to be configurable? changing it means now we'll need to make two HMS calls (one additional `getNumPartitionsByFilter`) which I'm not sure is desirable (have seen HMS perform very badly in production before).",
        "createdAt" : "2020-11-20T00:43:52Z",
        "updatedAt" : "2020-12-07T05:36:46Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "34294955-5d9c-4f54-a563-0a496cb101af",
        "parentId" : "5646c47d-f55c-4d0c-8356-fffffe587c10",
        "authorId" : "a74459ae-fcfb-427d-b810-b8c75c10717d",
        "body" : "@sunchao Thank you for your response. I think this is a reasonable maximum, and Presto also has a parameter to limit the number of partitions in HiveMetadata#getPartitionsAsList, default value is 100_000",
        "createdAt" : "2020-12-03T08:25:10Z",
        "updatedAt" : "2020-12-07T05:36:46Z",
        "lastEditedBy" : "a74459ae-fcfb-427d-b810-b8c75c10717d",
        "tags" : [
        ]
      },
      {
        "id" : "2727c972-b13a-411b-975a-9b31fe19127b",
        "parentId" : "5646c47d-f55c-4d0c-8356-fffffe587c10",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Yea the default value 100_000 looks fine to me. My main question is whether we need to make the default value to be that and double the HMS calls. Presto doesn't call `getNumPartitionsByFilter` it seems as it streams through a partition iterator and stops once the threshold is reached.",
        "createdAt" : "2020-12-03T17:09:08Z",
        "updatedAt" : "2020-12-07T05:36:46Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      }
    ],
    "commit" : "020d7dc8c2eb299540ead4be974ba45e90d81a9f",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +845,849 @@      .intConf\n      .checkValue(_ >= -1, \"The maximum must be a positive integer, -1 to follow the Hive config.\")\n      .createWithDefault(100000)\n\n  val HIVE_MANAGE_FILESOURCE_PARTITIONS ="
  },
  {
    "id" : "de81b2f9-ec22-4333-9a0f-d2b6bbd02243",
    "prId" : 30210,
    "prUrl" : "https://github.com/apache/spark/pull/30210#pullrequestreview-528798476",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b220168f-27c2-4b76-bd59-11f7fe961c80",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "For the naming, cc @cloud-fan .",
        "createdAt" : "2020-11-12T07:08:48Z",
        "updatedAt" : "2020-11-12T07:08:48Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "5dff48f5c3d60801e425951cbb0b30175e54f2be",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1384,1388 @@\n  val STATEFUL_OPERATOR_CHECK_CORRECTNESS_ENABLED =\n    buildConf(\"spark.sql.streaming.statefulOperator.checkCorrectness.enabled\")\n      .internal()\n      .doc(\"When true, the stateful operators for streaming query will be checked for possible \" +"
  },
  {
    "id" : "c084c094-bcf5-465a-aeec-cff8995f9279",
    "prId" : 30162,
    "prUrl" : "https://github.com/apache/spark/pull/30162#pullrequestreview-518310359",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7036595e-f355-4d01-bcf5-fd815a2f488d",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "This doesn't change the default value. I believe we can add the benchmark result as a follow-up, @HeartSaVioR .",
        "createdAt" : "2020-10-28T04:08:28Z",
        "updatedAt" : "2020-10-29T02:49:20Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "4dc153db5eeea904e0c2208726cbd41bd514b62f",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +1333,1337 @@      .version(\"3.1.0\")\n      .stringConf\n      .createWithDefault(\"lz4\")\n\n  val STREAMING_AGGREGATION_STATE_FORMAT_VERSION ="
  },
  {
    "id" : "059fee83-7d67-4785-b3c3-86d145f443a1",
    "prId" : 30162,
    "prUrl" : "https://github.com/apache/spark/pull/30162#pullrequestreview-519193915",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7da4e3f9-3d1e-4b7c-aa83-b7026e0658e0",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Shall we add a test case for this assertion, `fully qualified class names`? In the new test case, it seems that we always use `getShortName`.",
        "createdAt" : "2020-10-28T23:52:14Z",
        "updatedAt" : "2020-10-29T02:49:20Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "4b2e7533-15c9-4289-b396-d05d852d1cb9",
        "parentId" : "7da4e3f9-3d1e-4b7c-aa83-b7026e0658e0",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Also, I'm wondering what happens when the invalid class name. It would be great if we can add a negative test case.",
        "createdAt" : "2020-10-28T23:53:24Z",
        "updatedAt" : "2020-10-29T02:49:20Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "6f7891ee-66b6-44a2-897d-281902ee93d2",
        "parentId" : "7da4e3f9-3d1e-4b7c-aa83-b7026e0658e0",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Okay.",
        "createdAt" : "2020-10-29T00:00:41Z",
        "updatedAt" : "2020-10-29T02:49:20Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "38309fe4-c61e-47ff-86ba-972c57b1fc0b",
        "parentId" : "7da4e3f9-3d1e-4b7c-aa83-b7026e0658e0",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "For invalid codec name, `CompressionCodec.createCodec` will throw an `IllegalArgumentException`.",
        "createdAt" : "2020-10-29T00:13:40Z",
        "updatedAt" : "2020-10-29T02:49:20Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "4dc153db5eeea904e0c2208726cbd41bd514b62f",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +1330,1334 @@      .doc(\"The codec used to compress delta and snapshot files generated by StateStore. \" +\n        \"By default, Spark provides four codecs: lz4, lzf, snappy, and zstd. You can also \" +\n        \"use fully qualified class names to specify the codec. Default codec is lz4.\")\n      .version(\"3.1.0\")\n      .stringConf"
  },
  {
    "id" : "a9a6ef68-f6f2-4120-adf8-22bda7e71492",
    "prId" : 30146,
    "prUrl" : "https://github.com/apache/spark/pull/30146#pullrequestreview-518774942",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "77164161-c191-435d-8780-31d19d4be935",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "`DYNAMIC_FILTER_PRUNING_ENABLED` -> `DYNAMIC_COLUMN_PRUNING_ENABLED`?",
        "createdAt" : "2020-10-27T19:16:54Z",
        "updatedAt" : "2020-10-29T10:11:50Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "afdb7b72-b24f-4b7a-b340-c598e9713979",
        "parentId" : "77164161-c191-435d-8780-31d19d4be935",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Ur, BTW, is this used in this PR?",
        "createdAt" : "2020-10-27T19:18:19Z",
        "updatedAt" : "2020-10-29T10:11:50Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "cdfb653d-dbb1-4730-80a6-3e173a9fbd00",
        "parentId" : "77164161-c191-435d-8780-31d19d4be935",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "Yes. This PR introduces a new configuration: `spark.sql.optimizer.dynamicFilterPruning.enabled`, which is the master switch of Dynamic Filter Pruning, `spark.sql.optimizer.dynamicPartitionPruning.enabled` is the switch of the partition column, `spark.sql.optimizer.dynamicDataPruning.enabled` is the switch of the data column.",
        "createdAt" : "2020-10-28T15:13:37Z",
        "updatedAt" : "2020-10-29T10:11:50Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      }
    ],
    "commit" : "9bdb86005112443c48cb4965bc82406129e23f66",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +247,251 @@\n  val DYNAMIC_FILTER_PRUNING_ENABLED =\n    buildConf(\"spark.sql.optimizer.dynamicFilterPruning.enabled\")\n      .doc(\"When true, we will generate predicate when it's used as join key and has benefit.\")\n      .version(\"3.1.0\")"
  },
  {
    "id" : "81c5bded-f25f-493e-8be6-9cd34b797638",
    "prId" : 30146,
    "prUrl" : "https://github.com/apache/spark/pull/30146#pullrequestreview-518779727",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0e9e6e28-c2fb-43e3-bb50-7ace67f42a14",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "This looks like the real config in this PR.\r\n`DYNAMIC_DATA_PRUNING_ENABLED` -> `DYNAMIC_COLUMN_PRUNING_ENABLED`?",
        "createdAt" : "2020-10-27T19:19:11Z",
        "updatedAt" : "2020-10-29T10:11:50Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "6b624aba-7740-4992-a86f-1d8e4d50175c",
        "parentId" : "0e9e6e28-c2fb-43e3-bb50-7ace67f42a14",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "This config is the switch of is the switch of the data column. May be `DYNAMIC_DATA_COLUMN_PRUNING_ENABLED`?\r\nBut this is inconsistent with `DYNAMIC_PARTITION_PRUNING_ENABLED`.",
        "createdAt" : "2020-10-28T15:18:00Z",
        "updatedAt" : "2020-10-29T10:11:50Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      }
    ],
    "commit" : "9bdb86005112443c48cb4965bc82406129e23f66",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +290,294 @@      .createWithDefault(true)\n\n  val DYNAMIC_DATA_PRUNING_ENABLED =\n    buildConf(\"spark.sql.optimizer.dynamicDataPruning.enabled\")\n      .doc(\"When true, we will generate predicate for data column when it's used as join key \" +"
  },
  {
    "id" : "a53e8534-8193-421d-97e7-64e6007c895f",
    "prId" : 30093,
    "prUrl" : "https://github.com/apache/spark/pull/30093#pullrequestreview-516734999",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "21c10eec-829b-4fd0-b8fd-91ad2c5ca13e",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "We need this config? cc: @cloud-fan ",
        "createdAt" : "2020-10-26T11:28:33Z",
        "updatedAt" : "2020-10-27T17:40:08Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "4d633b8e-b53d-4128-8b35-021f468f94e3",
        "parentId" : "21c10eec-829b-4fd0-b8fd-91ad2c5ca13e",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "If we want to backport this, it's safer to have a config for the new rule, in case it has bugs.",
        "createdAt" : "2020-10-26T12:40:04Z",
        "updatedAt" : "2020-10-27T17:40:08Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "cdc7dbe1f69d2cec912c536d388eb68756107d61",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +1254,1258 @@    .createWithDefault(true)\n\n  val REMOVE_REDUNDANT_SORTS_ENABLED = buildConf(\"spark.sql.execution.removeRedundantSorts\")\n    .internal()\n    .doc(\"Whether to remove redundant physical sort node\")"
  },
  {
    "id" : "839c7dc5-1129-4203-8320-3430664c1b90",
    "prId" : 30093,
    "prUrl" : "https://github.com/apache/spark/pull/30093#pullrequestreview-517041492",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2ceb7a00-9a60-4e9f-9c5b-163717989cd5",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "If we backport this into branch-2.4, `3.1.0` -> `2.4.8`?",
        "createdAt" : "2020-10-26T13:29:03Z",
        "updatedAt" : "2020-10-27T17:40:08Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "2210c2c4-39c5-496f-9961-87d18b7fbc54",
        "parentId" : "2ceb7a00-9a60-4e9f-9c5b-163717989cd5",
        "authorId" : "b3ba992a-312c-46eb-b3c3-8d861d15ac40",
        "body" : "I am not exactly sure if it's better to change it in this PR or to change it when this PR is backported to 2.4.8 (in case the current change does not work in 2.4.8)",
        "createdAt" : "2020-10-26T17:59:50Z",
        "updatedAt" : "2020-10-27T17:40:08Z",
        "lastEditedBy" : "b3ba992a-312c-46eb-b3c3-8d861d15ac40",
        "tags" : [
        ]
      }
    ],
    "commit" : "cdc7dbe1f69d2cec912c536d388eb68756107d61",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +1257,1261 @@    .internal()\n    .doc(\"Whether to remove redundant physical sort node\")\n    .version(\"3.1.0\")\n    .booleanConf\n    .createWithDefault(true)"
  },
  {
    "id" : "58e81fdd-ee2d-42ae-9fb2-b2fa3dc64674",
    "prId" : 30056,
    "prUrl" : "https://github.com/apache/spark/pull/30056#pullrequestreview-513991740",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ec6cb872-875f-4093-927d-b8fff6da940d",
        "parentId" : null,
        "authorId" : "ff1f962a-1f58-4eea-92c9-0f188dc7dd78",
        "body" : "Could the default be made `LegacyBehaviorPolicy.EXCEPTION` instead? Could also do this in a follow-up PR if this is controversial.",
        "createdAt" : "2020-10-19T10:28:10Z",
        "updatedAt" : "2020-10-19T10:34:31Z",
        "lastEditedBy" : "ff1f962a-1f58-4eea-92c9-0f188dc7dd78",
        "tags" : [
        ]
      },
      {
        "id" : "3bd16862-1b17-4fda-b5cb-67ec8dce3584",
        "parentId" : "ec6cb872-875f-4093-927d-b8fff6da940d",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "I am not sure that we can do that in the minor release 3.1. This can break existing apps. @cloud-fan @HyukjinKwon WDYT?",
        "createdAt" : "2020-10-19T11:24:40Z",
        "updatedAt" : "2020-10-19T11:24:40Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "f3213e06-a354-43e0-a61a-789d34ee9189",
        "parentId" : "ec6cb872-875f-4093-927d-b8fff6da940d",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "It's a breaking change, but probably is acceptable as it doesn't lead to silent results changing. We need an item in the migration guide though.",
        "createdAt" : "2020-10-21T15:10:14Z",
        "updatedAt" : "2020-10-21T15:10:15Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "f0781521-3515-4aba-b08d-5548de56dc03",
        "parentId" : "ec6cb872-875f-4093-927d-b8fff6da940d",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Here is the PR https://github.com/apache/spark/pull/30121 . Will see how many tests this will affect.",
        "createdAt" : "2020-10-21T17:18:03Z",
        "updatedAt" : "2020-10-21T17:18:03Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "fab3a8690413f4bc13d2f61a28eb769faff76cef",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +2653,2657 @@      .transform(_.toUpperCase(Locale.ROOT))\n      .checkValues(LegacyBehaviorPolicy.values.map(_.toString))\n      .createWithDefault(LegacyBehaviorPolicy.LEGACY.toString)\n\n  val LEGACY_PARQUET_REBASE_MODE_IN_READ ="
  },
  {
    "id" : "b0d45344-ef37-4eb2-9cff-6a9fdb86a8bb",
    "prId" : 30047,
    "prUrl" : "https://github.com/apache/spark/pull/30047#pullrequestreview-508934669",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "850349d1-ea83-45b0-b561-59d4c8c93acf",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Thank you for adding this, @viirya .",
        "createdAt" : "2020-10-15T04:50:56Z",
        "updatedAt" : "2020-10-15T16:18:18Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "e88cc54354a4e0ed7ed7a651766cdeb732fc390f",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +1466,1470 @@      .version(\"3.1.0\")\n      .booleanConf\n      .createWithDefault(true)\n\n  val FILE_SINK_LOG_DELETION = buildConf(\"spark.sql.streaming.fileSink.log.deletion\")"
  },
  {
    "id" : "dd0aad68-a24e-407a-ad8b-841df6cb9ea3",
    "prId" : 30047,
    "prUrl" : "https://github.com/apache/spark/pull/30047#pullrequestreview-509063355",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "00c36e65-cdeb-4f07-8f01-0672bfa67741",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "hmm, I think users can already disable any optimizer rule via `spark.sql.optimizer.excludedRules`?",
        "createdAt" : "2020-10-15T06:33:50Z",
        "updatedAt" : "2020-10-15T16:18:19Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "2ffc72aa-6745-4f66-94e4-30f41ca33c08",
        "parentId" : "00c36e65-cdeb-4f07-8f01-0672bfa67741",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "`excludedRules` should work, but I'm thinking to add CSV expression optimization similar to JSON in same rule. If so, a separate config for JSON/CSV expression seems good. If we have two rules for JSON, CSV individually, then `excludedRules` might be enough.",
        "createdAt" : "2020-10-15T06:42:38Z",
        "updatedAt" : "2020-10-15T16:18:19Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "929bb87a-a8ef-400e-b97a-a7ea59b47343",
        "parentId" : "00c36e65-cdeb-4f07-8f01-0672bfa67741",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I see, makes sense.",
        "createdAt" : "2020-10-15T06:46:04Z",
        "updatedAt" : "2020-10-15T16:18:19Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "e88cc54354a4e0ed7ed7a651766cdeb732fc390f",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1460,1464 @@\n  val JSON_EXPRESSION_OPTIMIZATION =\n    buildConf(\"spark.sql.optimizer.enableJsonExpressionOptimization\")\n      .doc(\"Whether to optimize JSON expressions in SQL optimizer. It includes pruning \" +\n        \"unnecessary columns from from_json, simplifing from_json + to_json, to_json + \" +"
  },
  {
    "id" : "e7f9d2e8-a5a4-487c-bd5b-bb4df2b41da5",
    "prId" : 29999,
    "prUrl" : "https://github.com/apache/spark/pull/29999#pullrequestreview-554807435",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "057d0f07-6922-4552-b337-6d33533131a9",
        "parentId" : null,
        "authorId" : "e5beb795-ac0e-4bd6-ad66-708215b8ae58",
        "body" : "A tree of 200 And-reduced expressions is already a huge expr tree.\r\nI think this could be useful and helpful with a default threshold of 5 or so already.",
        "createdAt" : "2020-12-17T15:41:52Z",
        "updatedAt" : "2020-12-17T15:41:52Z",
        "lastEditedBy" : "e5beb795-ac0e-4bd6-ad66-708215b8ae58",
        "tags" : [
        ]
      },
      {
        "id" : "7ccb5c7f-33d4-4b7a-9547-7007e6f0186a",
        "parentId" : "057d0f07-6922-4552-b337-6d33533131a9",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "We have removed this config: https://github.com/beliefer/spark/commit/9273d4250ddd5e011487a5a942c1b4d0f0412f78#diff-13c5b65678b327277c68d17910ae93629801af00117a0e3da007afd95b6c6764L219\r\n\r\nWe will always use the new expression for LIKE ALL if values are all literal.",
        "createdAt" : "2020-12-17T17:06:54Z",
        "updatedAt" : "2020-12-17T17:07:05Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "001eb38f603267c6a6f4e1c25430b8900644f5b7",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +227,231 @@      .checkValue(threshold => threshold >= 0, \"The maximum size of pattern sequence \" +\n        \"in like all must be non-negative\")\n      .createWithDefault(200)\n\n  val PLAN_CHANGE_LOG_LEVEL = buildConf(\"spark.sql.planChangeLog.level\")"
  },
  {
    "id" : "c3e3385b-908a-4ae1-bcf6-78679b8fc636",
    "prId" : 29983,
    "prUrl" : "https://github.com/apache/spark/pull/29983#pullrequestreview-506261555",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c7ea7d9b-65e3-44e9-8460-1b6992692942",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Shall we update migration guide (https://github.com/apache/spark/blob/master/docs/sql-migration-guide.md) too? Looks like it's more a behaviour change instead of a big fix.",
        "createdAt" : "2020-10-12T02:45:09Z",
        "updatedAt" : "2020-10-13T08:29:12Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "1fd3e1da-a69e-4bf0-b518-dd4440c20a07",
        "parentId" : "c7ea7d9b-65e3-44e9-8460-1b6992692942",
        "authorId" : "cce1c782-0596-44b6-8b99-6b77d2cca53c",
        "body" : "Ok, will update.",
        "createdAt" : "2020-10-12T02:48:22Z",
        "updatedAt" : "2020-10-13T08:29:12Z",
        "lastEditedBy" : "cce1c782-0596-44b6-8b99-6b77d2cca53c",
        "tags" : [
        ]
      }
    ],
    "commit" : "ddc522cd2a455a08e3151c363645cc3f001465d5",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +2348,2352 @@      .doc(\"When set to true, statistical aggregate function returns Double.NaN \" +\n        \"if divide by zero occurred during expression evaluation, otherwise, it returns null. \" +\n        \"Before version 3.1.0, it returns NaN in divideByZero case by default.\")\n      .version(\"3.1.0\")\n      .booleanConf"
  },
  {
    "id" : "c5055a95-0940-4c16-b7e4-fd7eb8e24425",
    "prId" : 29950,
    "prUrl" : "https://github.com/apache/spark/pull/29950#pullrequestreview-507068962",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ecfa7b45-911a-455d-8059-41309683efa1",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Could you add `checkValue`?",
        "createdAt" : "2020-10-12T02:45:03Z",
        "updatedAt" : "2020-11-13T00:06:09Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "c6ed6bbb-1faa-4e1a-ade9-4fa1db6d0e05",
        "parentId" : "ecfa7b45-911a-455d-8059-41309683efa1",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Added, thanks.",
        "createdAt" : "2020-10-13T04:57:35Z",
        "updatedAt" : "2020-11-13T00:06:09Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "bbaae3ed9b96ca517ec5435838ac37feb578c959",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +1981,1985 @@        \"the physical planning.\")\n      .version(\"3.1.0\")\n      .intConf\n      .checkValue(_ > 0, \"The value of maxCommonExprsInCollapseProject must be larger than zero.\")\n      .createWithDefault(Int.MaxValue)"
  },
  {
    "id" : "46414558-33f9-46f3-9927-e625696bec57",
    "prId" : 29950,
    "prUrl" : "https://github.com/apache/spark/pull/29950#pullrequestreview-513435611",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5d730403-4b3d-48d5-98c3-cbccf53fc9a3",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "(Just a comment) Even if we set `spark.sql.optimizer.excludedRules` to `CollapseProject`, it seems like Spark still respects this value in `ScanOperation`? That behaviour might be okay, but it looks a bit weird to me.",
        "createdAt" : "2020-10-21T01:18:50Z",
        "updatedAt" : "2020-11-13T00:06:09Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "911845da-cb27-4245-a3c7-a3205a8c1898",
        "parentId" : "5d730403-4b3d-48d5-98c3-cbccf53fc9a3",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Yeah, but currently if we exclude `CollapseProject`, `ScanOperation` will work and collapse projections. Maybe update this doc?",
        "createdAt" : "2020-10-21T07:04:28Z",
        "updatedAt" : "2020-11-13T00:06:09Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "6c12bba7-4fa4-4eaf-b15d-0c76c22ce570",
        "parentId" : "5d730403-4b3d-48d5-98c3-cbccf53fc9a3",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "hm I see. Yea, updating the doc sounds nice to me.",
        "createdAt" : "2020-10-21T08:35:56Z",
        "updatedAt" : "2020-11-13T00:06:09Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "bbaae3ed9b96ca517ec5435838ac37feb578c959",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +1968,1972 @@      .doc(\"An integer number indicates the maximum allowed number of common input expression \" +\n        \"from lower Project when being collapsed into upper Project by optimizer rule \" +\n        \"`CollapseProject`. Normally `CollapseProject` will collapse adjacent Project \" +\n        \"and merge expressions. But in some edge cases, expensive expressions might be \" +\n        \"duplicated many times in merged Project by this optimization. This config sets \" +"
  },
  {
    "id" : "0fa91c23-21cf-44d2-a1f0-fb7ab3e8ef58",
    "prId" : 29950,
    "prUrl" : "https://github.com/apache/spark/pull/29950#pullrequestreview-514980946",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a4e355ad-3afc-4ad3-8108-55bf13c8a03a",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "If we set this value to 1, all the existing tests can pass?",
        "createdAt" : "2020-10-21T01:20:08Z",
        "updatedAt" : "2020-11-13T00:06:09Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "a1f3e114-9c7c-4e42-8ace-24ba16a594ba",
        "parentId" : "a4e355ad-3afc-4ad3-8108-55bf13c8a03a",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "I guess not. We might have at lease few common expressions in collapsed projection. If set to 1, any duplicated expression is not allowed.",
        "createdAt" : "2020-10-22T17:53:55Z",
        "updatedAt" : "2020-11-13T00:06:09Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "bbaae3ed9b96ca517ec5435838ac37feb578c959",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1965,1969 @@\n  val MAX_COMMON_EXPRS_IN_COLLAPSE_PROJECT =\n    buildConf(\"spark.sql.optimizer.maxCommonExprsInCollapseProject\")\n      .doc(\"An integer number indicates the maximum allowed number of common input expression \" +\n        \"from lower Project when being collapsed into upper Project by optimizer rule \" +"
  },
  {
    "id" : "80bcb9c1-4ec5-44a0-9661-f065b2695acd",
    "prId" : 29950,
    "prUrl" : "https://github.com/apache/spark/pull/29950#pullrequestreview-529751693",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9138bb91-9fb1-4aa0-8c6a-df18bc08185b",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "`larger than zero` -> `positive`.",
        "createdAt" : "2020-11-13T05:47:38Z",
        "updatedAt" : "2020-11-13T05:47:38Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "bbaae3ed9b96ca517ec5435838ac37feb578c959",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +1982,1986 @@      .version(\"3.1.0\")\n      .intConf\n      .checkValue(_ > 0, \"The value of maxCommonExprsInCollapseProject must be larger than zero.\")\n      .createWithDefault(Int.MaxValue)\n"
  },
  {
    "id" : "ee5a6969-588e-42c4-8127-8968bb613ab2",
    "prId" : 29933,
    "prUrl" : "https://github.com/apache/spark/pull/29933#pullrequestreview-507360012",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "739440d4-071f-4990-926b-325183a1029e",
        "parentId" : null,
        "authorId" : "2821208d-43c5-4475-94ad-36cbf84c14d8",
        "body" : "Can you rewrite this as:\r\n```\r\nSet a query duration timeout in seconds in ThriftServer. If the timeout is set to a positive value, a running query will be cancelled automatically when the timeout is exceeded, otherwise the query continues to run till completion. Timeout values that are set for each statement via`java.sql.Statement.setQueryTimeout` take precedence.\r\n```",
        "createdAt" : "2020-10-13T11:34:18Z",
        "updatedAt" : "2020-10-14T23:22:41Z",
        "lastEditedBy" : "2821208d-43c5-4475-94ad-36cbf84c14d8",
        "tags" : [
        ]
      },
      {
        "id" : "a3527249-0417-4545-bb79-6a3bbce195cb",
        "parentId" : "739440d4-071f-4990-926b-325183a1029e",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Updated.",
        "createdAt" : "2020-10-13T12:04:58Z",
        "updatedAt" : "2020-10-14T23:22:41Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "5e3c44060567d2234dd79d222f77bc58983bb805",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +895,899 @@\n  val THRIFTSERVER_QUERY_TIMEOUT =\n    buildConf(\"spark.sql.thriftServer.queryTimeout\")\n      .doc(\"Set a query duration timeout in seconds in Thrift Server. If the timeout is set to \" +\n        \"a positive value, a running query will be cancelled automatically when the timeout is \" +"
  },
  {
    "id" : "09b216ae-b122-45a4-acc7-5deec0aeb3d7",
    "prId" : 29933,
    "prUrl" : "https://github.com/apache/spark/pull/29933#pullrequestreview-507360312",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7bd836f4-e249-40a3-9ab1-d3cfc8f77eb9",
        "parentId" : null,
        "authorId" : "2821208d-43c5-4475-94ad-36cbf84c14d8",
        "body" : "is this version correct?",
        "createdAt" : "2020-10-13T11:34:46Z",
        "updatedAt" : "2020-10-14T23:22:41Z",
        "lastEditedBy" : "2821208d-43c5-4475-94ad-36cbf84c14d8",
        "tags" : [
        ]
      },
      {
        "id" : "9ed7be71-7d34-4e05-b790-de00595e5ca0",
        "parentId" : "7bd836f4-e249-40a3-9ab1-d3cfc8f77eb9",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Yea, I think so.",
        "createdAt" : "2020-10-13T12:05:21Z",
        "updatedAt" : "2020-10-14T23:22:41Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "5e3c44060567d2234dd79d222f77bc58983bb805",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +901,905 @@        \"set for each statement via `java.sql.Statement.setQueryTimeout` and they are smaller \" +\n        \"than this configuration value, they take precedence.\")\n      .version(\"3.1.0\")\n      .timeConf(TimeUnit.SECONDS)\n      .createWithDefault(0L)"
  },
  {
    "id" : "b3bc3ce5-a1dd-47c1-9d19-efe158454575",
    "prId" : 29804,
    "prUrl" : "https://github.com/apache/spark/pull/29804#pullrequestreview-497272760",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "50809d9d-ffca-49bb-85c6-24f1ff2a8728",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Since our user documents are generated based on this statement, could you describe a bit more about how to decide whether to do bucketed scans or not?",
        "createdAt" : "2020-09-28T05:40:16Z",
        "updatedAt" : "2020-10-01T01:11:06Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "feb2b435-5e13-4770-8e3d-6958938cf66e",
        "parentId" : "50809d9d-ffca-49bb-85c6-24f1ff2a8728",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "@maropu - sure, wondering what do you think of below?\r\n\r\n```\r\nWhen true, decide whether to do bucketed scan on input tables\r\nbased on query plan automatically. Do not use bucketed scan if\r\n(1).query does not have operators to utilize bucketing (e.g. join, group-by, etc),\r\nor (2).there's an exchange operator between these operators and table scan.\r\n\r\n```",
        "createdAt" : "2020-09-28T05:51:07Z",
        "updatedAt" : "2020-10-01T01:11:06Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      },
      {
        "id" : "7becaafe-66e1-4a83-9079-fd7be3f45df8",
        "parentId" : "50809d9d-ffca-49bb-85c6-24f1ff2a8728",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "It looks okay, but I have the same suggestion with https://github.com/apache/spark/pull/29804#discussion_r494852129",
        "createdAt" : "2020-09-28T06:09:30Z",
        "updatedAt" : "2020-10-01T01:11:06Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "048ad1a5-5def-45ac-82a6-6ed03813f283",
        "parentId" : "50809d9d-ffca-49bb-85c6-24f1ff2a8728",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "@maropu - sure, updated.",
        "createdAt" : "2020-09-28T08:01:37Z",
        "updatedAt" : "2020-10-01T01:11:06Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      }
    ],
    "commit" : "b29f688fc4a06bc35effa6d24632d2a64501c9fd",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +954,958 @@  val AUTO_BUCKETED_SCAN_ENABLED =\n    buildConf(\"spark.sql.sources.bucketing.autoBucketedScan.enabled\")\n      .doc(\"When true, decide whether to do bucketed scan on input tables based on query plan \" +\n        \"automatically. Do not use bucketed scan if 1. query does not have operators to utilize \" +\n        \"bucketing (e.g. join, group-by, etc), or 2. there's an exchange operator between these \" +"
  },
  {
    "id" : "a52546db-04ba-4288-b9a2-d23310a22ed4",
    "prId" : 29804,
    "prUrl" : "https://github.com/apache/spark/pull/29804#pullrequestreview-497272760",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a90162d4-fd51-44b5-b13d-c3eb85a11e87",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "btw, we need to make this config external? If we just add this config for keeping the current behaviour, is it okay to add it as internal one?",
        "createdAt" : "2020-09-28T05:43:03Z",
        "updatedAt" : "2020-10-01T01:11:06Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "8edf562f-47f3-4ed2-88b9-48adc82384b6",
        "parentId" : "a90162d4-fd51-44b5-b13d-c3eb85a11e87",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "@maropu - sure, just for my own education, what does it indicate to make a config internal/external?",
        "createdAt" : "2020-09-28T05:52:09Z",
        "updatedAt" : "2020-10-01T01:11:06Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      },
      {
        "id" : "6fdf795f-8b91-4c92-a253-5758949058a8",
        "parentId" : "a90162d4-fd51-44b5-b13d-c3eb85a11e87",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "IIUC we don't have any strict rule for that. But, I think this new rule works well in most queries, so adding this as external looks less meaning because I think most users don't turn this feature off.",
        "createdAt" : "2020-09-28T06:07:50Z",
        "updatedAt" : "2020-10-01T01:11:06Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "32256f13-9b38-4ac9-8d3f-d9fe51a0cf7a",
        "parentId" : "a90162d4-fd51-44b5-b13d-c3eb85a11e87",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "@maropu - sure, updated.",
        "createdAt" : "2020-09-28T08:01:48Z",
        "updatedAt" : "2020-10-01T01:11:06Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      }
    ],
    "commit" : "b29f688fc4a06bc35effa6d24632d2a64501c9fd",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +959,963 @@        s\"operators and table scan. Note when '${BUCKETING_ENABLED.key}' is set to \" +\n        \"false, this configuration does not take any effect.\")\n      .version(\"3.1.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "126a829d-0736-468e-ad63-0964985d948d",
    "prId" : 29804,
    "prUrl" : "https://github.com/apache/spark/pull/29804#pullrequestreview-502706267",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6d6f1206-061c-4e95-8280-051e98fe6e85",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "we can follow AQE and only disable it for table cache. See https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/CacheManager.scala#L82",
        "createdAt" : "2020-10-06T06:29:34Z",
        "updatedAt" : "2020-10-06T06:29:35Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "841ec2da-9de7-4ffb-95f8-991510386fef",
        "parentId" : "6d6f1206-061c-4e95-8280-051e98fe6e85",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "@cloud-fan - thanks for pointing it out. Created https://issues.apache.org/jira/browse/SPARK-33075 for followup, cc @viirya in case there's any other regression for enabling auto bucketed scan, except cached query.",
        "createdAt" : "2020-10-06T08:35:14Z",
        "updatedAt" : "2020-10-06T08:35:14Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      }
    ],
    "commit" : "b29f688fc4a06bc35effa6d24632d2a64501c9fd",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +961,965 @@      .version(\"3.1.0\")\n      .booleanConf\n      .createWithDefault(false)\n\n  val CROSS_JOINS_ENABLED = buildConf(\"spark.sql.crossJoin.enabled\")"
  },
  {
    "id" : "fe046f68-60d0-46c4-9851-f1d8ff7ec1ee",
    "prId" : 29729,
    "prUrl" : "https://github.com/apache/spark/pull/29729#pullrequestreview-541567851",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e2626454-ebf3-4d87-bd4e-6b1a316bfcc5",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "As we only describe the details in migration doc which is not something end users always read as a guide doc or manual doc, we'd be better to write explanation either here or sql+kafka integration doc.\r\n(IMO it worths to add the comparison in the integration doc, and let here refer the doc.)",
        "createdAt" : "2020-11-30T21:11:18Z",
        "updatedAt" : "2020-12-01T07:55:04Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "158e5d3f-1036-4e89-8913-db6a07362781",
        "parentId" : "e2626454-ebf3-4d87-bd4e-6b1a316bfcc5",
        "authorId" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "body" : "Fixed. Updating the description because I've double checked the generated HTML files too.",
        "createdAt" : "2020-12-01T07:56:04Z",
        "updatedAt" : "2020-12-01T07:56:05Z",
        "lastEditedBy" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "tags" : [
        ]
      }
    ],
    "commit" : "203e60c9b7abeac0464ad3acc256121a5f900e40",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +1419,1423 @@    buildConf(\"spark.sql.streaming.kafka.useDeprecatedOffsetFetching\")\n      .internal()\n      .doc(\"When true, the deprecated Consumer based offset fetching used which could cause \" +\n        \"infinite wait in Spark queries. Such cases query restart is the only workaround. \" +\n        \"For further details please see Offset Fetching chapter of Structured Streaming Kafka \" +"
  },
  {
    "id" : "adc5fd8b-9f1f-403e-b997-f86d1646bcae",
    "prId" : 29726,
    "prUrl" : "https://github.com/apache/spark/pull/29726#pullrequestreview-621803131",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3ac10d00-9c81-4e8b-94b1-36984910ecb5",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "`PARTITON` -> `PARTITION`?",
        "createdAt" : "2021-03-26T05:53:22Z",
        "updatedAt" : "2021-03-26T05:53:22Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "bb579c655dfc1538bb682b6e03cecda948ca42e6",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +308,312 @@      .createWithDefault(true)\n\n  val DYNAMIC_PARTITON_PRUNING_PRUNING_SIDE_EXTRA_FILTER_RATIO =\n    buildConf(\"spark.sql.optimizer.dynamicPartitionPruning.pruningSideExtraFilterRatio\")\n    .internal()"
  },
  {
    "id" : "3eef7546-0640-4dd8-8f59-1ed69e967592",
    "prId" : 29726,
    "prUrl" : "https://github.com/apache/spark/pull/29726#pullrequestreview-621803568",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "804a5c9d-0702-42f8-bd33-020df1059cf3",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Indentation: two more spaces?",
        "createdAt" : "2021-03-26T05:54:32Z",
        "updatedAt" : "2021-03-26T05:54:32Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "bb579c655dfc1538bb682b6e03cecda948ca42e6",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +310,314 @@  val DYNAMIC_PARTITON_PRUNING_PRUNING_SIDE_EXTRA_FILTER_RATIO =\n    buildConf(\"spark.sql.optimizer.dynamicPartitionPruning.pruningSideExtraFilterRatio\")\n    .internal()\n    .doc(\"When filtering side doesn't support broadcast by join type, and doing DPP means \" +\n      \"running an extra query that may have significant overhead. This config will be used \" +"
  },
  {
    "id" : "5448804a-5475-422d-a203-32c007f41e6e",
    "prId" : 29726,
    "prUrl" : "https://github.com/apache/spark/pull/29726#pullrequestreview-621806551",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9b92c283-4cda-4643-9bfd-bbd44c0358b4",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Can we have `.checkValue` for the expected range, `0.0 <=` and `<= 1.0`? Or, are we allowing more possibility like `2.0`?",
        "createdAt" : "2021-03-26T06:03:08Z",
        "updatedAt" : "2021-03-26T06:03:08Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "bb579c655dfc1538bb682b6e03cecda948ca42e6",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +316,320 @@      \"in order to evaluate if it is worth adding an extra subquery as the pruning filter.\")\n    .version(\"3.2.0\")\n    .doubleConf\n    .createWithDefault(0.04)\n"
  },
  {
    "id" : "c15c23cb-4796-4ca4-8b50-15131e2219a6",
    "prId" : 29688,
    "prUrl" : "https://github.com/apache/spark/pull/29688#pullrequestreview-484747277",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "43b3380d-a68f-4f02-89df-95b2b453414b",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "plz add `checkValue`",
        "createdAt" : "2020-09-09T07:50:50Z",
        "updatedAt" : "2020-09-14T00:02:55Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "e83bf958828cd7b893c17e6962f2d63725632eec",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +2375,2379 @@      \"file location in `DataSourceScanExec`, every value will be abbreviated if exceed length.\")\n    .version(\"3.1.0\")\n    .intConf\n    .checkValue(_ > 3, \"This value must be bigger than 3.\")\n    .createWithDefault(100)"
  },
  {
    "id" : "74fca576-0795-49e3-8828-182adbddb7e9",
    "prId" : 29688,
    "prUrl" : "https://github.com/apache/spark/pull/29688#pullrequestreview-484804025",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "06211dc7-d9fd-44e2-9161-196e25763840",
        "parentId" : null,
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "This value from `org.apache.commons.lang3.StringUtils.abbreviate` whose marker is `...`.",
        "createdAt" : "2020-09-09T09:00:32Z",
        "updatedAt" : "2020-09-14T00:02:55Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      }
    ],
    "commit" : "e83bf958828cd7b893c17e6962f2d63725632eec",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +2376,2380 @@    .version(\"3.1.0\")\n    .intConf\n    .checkValue(_ > 3, \"This value must be bigger than 3.\")\n    .createWithDefault(100)\n"
  },
  {
    "id" : "4cb84bb4-eb63-4fe3-87dd-349eb7261941",
    "prId" : 29559,
    "prUrl" : "https://github.com/apache/spark/pull/29559#pullrequestreview-477234367",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "636ca112-f63f-4c75-a997-6edc10f0f3bd",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Since the PR description says nothing about adding this, I think its better to add it or separating this from this PR.  Also, could you add some tests for this?",
        "createdAt" : "2020-08-28T04:35:44Z",
        "updatedAt" : "2020-08-28T07:40:28Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "cfd348e643e61b51125ae1d375ee5830204c1acf",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +523,527 @@      .createWithDefault(0.2)\n\n  val ADAPTIVE_OPTIMIZER_EXCLUDED_RULES =\n    buildConf(\"spark.sql.adaptive.optimizer.excludedRules\")\n      .doc(\"Configures a list of rules to be disabled in the adaptive optimizer, in which the \" +"
  },
  {
    "id" : "9b989d87-9d07-4162-b3e7-165a07411551",
    "prId" : 29544,
    "prUrl" : "https://github.com/apache/spark/pull/29544#pullrequestreview-477233811",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e68d0b2d-3ebc-44f5-a14d-c5c8a6c5a2ee",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "can we put the old config name in `removedSQLConfigs`?",
        "createdAt" : "2020-08-28T04:33:40Z",
        "updatedAt" : "2020-08-28T06:21:08Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "40c99a21ae19f4c1e50029e4b10c768339336cc6",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +217,221 @@      .createWithDefault(400)\n\n  val PLAN_CHANGE_LOG_LEVEL = buildConf(\"spark.sql.planChangeLog.level\")\n    .internal()\n    .doc(\"Configures the log level for logging the change from the original plan to the new \" +"
  },
  {
    "id" : "a57b6cc5-5833-4d55-abf5-0346ff5c5081",
    "prId" : 29544,
    "prUrl" : "https://github.com/apache/spark/pull/29544#pullrequestreview-506307247",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "892e1dc3-fd97-441d-8267-0e29f345e275",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "The version of these three configurations needs to be updated to 3.1.0? ",
        "createdAt" : "2020-10-12T05:41:03Z",
        "updatedAt" : "2020-10-12T05:41:03Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "665ea48a-30a2-4fc3-8447-927ba6cd64e4",
        "parentId" : "892e1dc3-fd97-441d-8267-0e29f345e275",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Ah, I missed it. I'll update it in followup. Thanks!",
        "createdAt" : "2020-10-12T05:47:32Z",
        "updatedAt" : "2020-10-12T05:47:42Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "40c99a21ae19f4c1e50029e4b10c768339336cc6",
    "line" : 29,
    "diffHunk" : "@@ -1,1 +242,246 @@    .doc(\"Configures a list of batches for logging plan changes, in which the batches \" +\n      \"are specified by their batch names and separated by comma.\")\n    .version(\"3.0.0\")\n    .stringConf\n    .createOptional"
  },
  {
    "id" : "19109651-c7a1-457c-b828-133076f42ee8",
    "prId" : 29387,
    "prUrl" : "https://github.com/apache/spark/pull/29387#pullrequestreview-472940677",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "22d19104-aae7-4a2b-85e9-b08f05eaefff",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Just a question. Is the following still true?\r\n```\r\nNote that if fs.trash.interval is non-positive, this will be a no-op and log a warning message.\r\n```",
        "createdAt" : "2020-08-22T16:07:57Z",
        "updatedAt" : "2020-08-24T22:01:57Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "6d1cec03-79f5-4c06-b579-225c7a53a554",
        "parentId" : "22d19104-aae7-4a2b-85e9-b08f05eaefff",
        "authorId" : "a88f50e2-6440-4a6c-8883-3130888d05bc",
        "body" : "yes, this is still true.",
        "createdAt" : "2020-08-22T16:58:00Z",
        "updatedAt" : "2020-08-24T22:01:57Z",
        "lastEditedBy" : "a88f50e2-6440-4a6c-8883-3130888d05bc",
        "tags" : [
        ]
      },
      {
        "id" : "0fd4643d-e94b-4f1e-9d21-7f9188ca4186",
        "parentId" : "22d19104-aae7-4a2b-85e9-b08f05eaefff",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Yes. This is because when `fs.trash.interval` is not positive, Hadoop side will consider trash as disabled and will not delete the data. See [here](https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/TrashPolicyDefault.java#L125). Currently this just logs a warning but we could consider another flag to hard delete the data instead.",
        "createdAt" : "2020-08-22T17:38:42Z",
        "updatedAt" : "2020-08-24T22:01:57Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      }
    ],
    "commit" : "a2df53b48db372ed8f9a303cd8c0c499e33f3adf",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +2729,2733 @@        \"fs.trash.interval, and in default, the server side configuration value takes \" +\n        \"precedence over the client-side one. Note that if fs.trash.interval is non-positive, \" +\n        \"this will be a no-op and log a warning message.\")\n      .version(\"3.1.0\")\n      .booleanConf"
  },
  {
    "id" : "87a01f9e-3e9b-4613-85d5-c9c8d947c821",
    "prId" : 29387,
    "prUrl" : "https://github.com/apache/spark/pull/29387#pullrequestreview-472947585",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d91eff26-3a9f-4608-8852-21e8984fa24e",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "Does Spark SQL have the trash directory ?",
        "createdAt" : "2020-08-22T19:06:38Z",
        "updatedAt" : "2020-08-24T22:01:57Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "1a4f5ed4-7532-4608-bc40-582ce2757e8f",
        "parentId" : "d91eff26-3a9f-4608-8852-21e8984fa24e",
        "authorId" : "a88f50e2-6440-4a6c-8883-3130888d05bc",
        "body" : "Hi, It is hdfs trash directory",
        "createdAt" : "2020-08-22T19:41:04Z",
        "updatedAt" : "2020-08-24T22:01:57Z",
        "lastEditedBy" : "a88f50e2-6440-4a6c-8883-3130888d05bc",
        "tags" : [
        ]
      }
    ],
    "commit" : "a2df53b48db372ed8f9a303cd8c0c499e33f3adf",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +2726,2730 @@    buildConf(\"spark.sql.truncate.trash.enabled\")\n      .doc(\"This configuration decides when truncating table, whether data files will be moved \" +\n        \"to trash directory or deleted permanently. The trash retention time is controlled by \" +\n        \"fs.trash.interval, and in default, the server side configuration value takes \" +\n        \"precedence over the client-side one. Note that if fs.trash.interval is non-positive, \" +"
  },
  {
    "id" : "266b4e2a-6678-450e-90fc-0fc3ba1b2fa9",
    "prId" : 29387,
    "prUrl" : "https://github.com/apache/spark/pull/29387#pullrequestreview-473863014",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1e8b6942-cdbb-44a8-899d-5c28631c7495",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Please add version here.\r\n```scala\r\n.version(\"3.1.0\")\r\n```",
        "createdAt" : "2020-08-24T21:17:09Z",
        "updatedAt" : "2020-08-24T22:01:57Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "a2df53b48db372ed8f9a303cd8c0c499e33f3adf",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +2731,2735 @@        \"this will be a no-op and log a warning message.\")\n      .version(\"3.1.0\")\n      .booleanConf\n      .createWithDefault(false)\n"
  },
  {
    "id" : "15e15d0b-9374-4f4a-967b-0c5cdfd19c44",
    "prId" : 29387,
    "prUrl" : "https://github.com/apache/spark/pull/29387#pullrequestreview-475563285",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c6733787-9698-46c5-85af-04145d7c98b6",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "quick question, do we want to have each configuration for each operation? Looks like https://github.com/apache/spark/pull/29319 targets similar stuff. Maybe it'd make more sense to have a global configuration.",
        "createdAt" : "2020-08-26T06:44:31Z",
        "updatedAt" : "2020-08-26T06:44:31Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "0db65727-7e71-40b0-87ce-2a235aa24349",
        "parentId" : "c6733787-9698-46c5-85af-04145d7c98b6",
        "authorId" : "a88f50e2-6440-4a6c-8883-3130888d05bc",
        "body" : "I will rework on  #29319 and make it a global configuration.",
        "createdAt" : "2020-08-26T07:38:58Z",
        "updatedAt" : "2020-08-26T07:38:58Z",
        "lastEditedBy" : "a88f50e2-6440-4a6c-8883-3130888d05bc",
        "tags" : [
        ]
      },
      {
        "id" : "346d0915-1411-4847-a6d9-152144fbf67a",
        "parentId" : "c6733787-9698-46c5-85af-04145d7c98b6",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Yep. It's too early to make it a global configuration.",
        "createdAt" : "2020-08-26T14:44:41Z",
        "updatedAt" : "2020-08-26T14:44:41Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "a2df53b48db372ed8f9a303cd8c0c499e33f3adf",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +2724,2728 @@\n  val TRUNCATE_TRASH_ENABLED =\n    buildConf(\"spark.sql.truncate.trash.enabled\")\n      .doc(\"This configuration decides when truncating table, whether data files will be moved \" +\n        \"to trash directory or deleted permanently. The trash retention time is controlled by \" +"
  },
  {
    "id" : "927a67e8-a8ff-478a-a683-66239f09d84b",
    "prId" : 29304,
    "prUrl" : "https://github.com/apache/spark/pull/29304#pullrequestreview-459614009",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d0c90284-86fe-4cdc-9260-6431ca614f9c",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Plz add `checkValue`. I think only a positive value seems reasonable.",
        "createdAt" : "2020-08-01T14:02:26Z",
        "updatedAt" : "2020-08-04T22:55:52Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "511818af-232c-4005-b91f-e255fb5c48fb",
        "parentId" : "d0c90284-86fe-4cdc-9260-6431ca614f9c",
        "authorId" : "cce1c782-0596-44b6-8b99-6b77d2cca53c",
        "body" : "done",
        "createdAt" : "2020-08-02T01:48:23Z",
        "updatedAt" : "2020-08-04T22:55:52Z",
        "lastEditedBy" : "cce1c782-0596-44b6-8b99-6b77d2cca53c",
        "tags" : [
        ]
      }
    ],
    "commit" : "054581ae1d8f6148db7ec952ac23e5668ff9dc75",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +2686,2690 @@        \"it might cause Driver OOM if NAAJ numKeys increased, since it is exponential growth.\")\n      .version(\"3.1.0\")\n      .intConf\n      .checkValue(_ > 0, \"The value must be positive.\")\n      .createWithDefault(3)"
  },
  {
    "id" : "415c341a-c702-41e1-b0b3-0b9d9a77406f",
    "prId" : 29304,
    "prUrl" : "https://github.com/apache/spark/pull/29304#pullrequestreview-459952624",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0771148d-8e79-4c2b-9ff9-2ddd7039257b",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "add `.version(\"3.1.0\")`: https://github.com/apache/spark/pull/29335",
        "createdAt" : "2020-08-03T11:30:11Z",
        "updatedAt" : "2020-08-04T22:55:52Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "054581ae1d8f6148db7ec952ac23e5668ff9dc75",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +2686,2690 @@        \"it might cause Driver OOM if NAAJ numKeys increased, since it is exponential growth.\")\n      .version(\"3.1.0\")\n      .intConf\n      .checkValue(_ > 0, \"The value must be positive.\")\n      .createWithDefault(3)"
  },
  {
    "id" : "e627ea25-d512-460c-ae35-b7a22e4f7e04",
    "prId" : 29104,
    "prUrl" : "https://github.com/apache/spark/pull/29104#pullrequestreview-469897324",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b271ade3-88d5-4510-8dfa-66ae3e2afcb7",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "I think this is only possible when we can buildSide data is small. Can you add to this doc?",
        "createdAt" : "2020-07-21T23:06:06Z",
        "updatedAt" : "2020-07-27T22:06:38Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "4a251550-7d0b-4dc8-9ddb-31f040c984e2",
        "parentId" : "b271ade3-88d5-4510-8dfa-66ae3e2afcb7",
        "authorId" : "cce1c782-0596-44b6-8b99-6b77d2cca53c",
        "body" : "if it's single column NAAj, even if the buildSide is bigger than the autoBroadcaseThreshold, I think it's better to just using the optimize, because BroadcastNestedLoopJoin will be still worst than this optimization.",
        "createdAt" : "2020-07-22T01:54:01Z",
        "updatedAt" : "2020-07-27T22:06:38Z",
        "lastEditedBy" : "cce1c782-0596-44b6-8b99-6b77d2cca53c",
        "tags" : [
        ]
      },
      {
        "id" : "0554a91b-8ee7-4626-88f3-a861c96be7b1",
        "parentId" : "b271ade3-88d5-4510-8dfa-66ae3e2afcb7",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "I saw SPARK-32644, so it is not always able to use this optimization, isn't?",
        "createdAt" : "2020-08-18T23:30:19Z",
        "updatedAt" : "2020-08-18T23:30:19Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "233eff6549377d6c7c850fe8d1990fcd58fe0ea0",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +2676,2680 @@    buildConf(\"spark.sql.optimizeNullAwareAntiJoin\")\n      .internal()\n      .doc(\"When true, NULL-aware anti join execution will be planed into \" +\n        \"BroadcastHashJoinExec with flag isNullAwareAntiJoin enabled, \" +\n        \"optimized from O(M*N) calculation into O(M) calculation \" +"
  },
  {
    "id" : "8380d230-3dcf-41f5-a3d2-94053df2fa9a",
    "prId" : 29079,
    "prUrl" : "https://github.com/apache/spark/pull/29079#pullrequestreview-448788378",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "173f1ef6-300a-4347-b2b2-ff7fc36135e6",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Is it okay to share this parameter between sort-merge/hash joins? As @viirya suggested, we have some risk of OOM. So, I think we need a different threshold policy for the hash-join case.",
        "createdAt" : "2020-07-15T06:45:02Z",
        "updatedAt" : "2020-07-21T16:58:06Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "6bfeb979-d2a0-4544-b735-5eb90908bbe1",
        "parentId" : "173f1ef6-300a-4347-b2b2-ff7fc36135e6",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Also, I think we need to describe the risk clearly in `.doc`.",
        "createdAt" : "2020-07-15T06:45:34Z",
        "updatedAt" : "2020-07-21T16:58:06Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "1f8ed5a3-b3cc-4286-bfa6-812701eca717",
        "parentId" : "173f1ef6-300a-4347-b2b2-ff7fc36135e6",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "@maropu Make sense to me. Separated max bucket ratio configs for SHJ and SMJ for now, and added OOM related documentation to warn users.",
        "createdAt" : "2020-07-15T09:34:41Z",
        "updatedAt" : "2020-07-21T16:58:06Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      }
    ],
    "commit" : "b1a8a927f283b095716a638a26f9c5d5e9cc380c",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +2653,2657 @@\n  val COALESCE_BUCKETS_IN_JOIN_MAX_BUCKET_RATIO =\n    buildConf(\"spark.sql.bucketing.coalesceBucketsInJoin.maxBucketRatio\")\n      .doc(\"The ratio of the number of two buckets being coalesced should be less than or \" +\n        \"equal to this value for bucket coalescing to be applied. This configuration only \" +"
  },
  {
    "id" : "c8e32bff-b55b-4b67-a228-1700644b26ef",
    "prId" : 29079,
    "prUrl" : "https://github.com/apache/spark/pull/29079#pullrequestreview-451099862",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bd61630d-2bb1-4478-b01d-338851792ddc",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Can we add more doc like \"Coalescing bucketed table can avoid unnecessary shuffling during joining but it also reduces parallelism and could possibly cause OOM  for shuffled hash join\"?",
        "createdAt" : "2020-07-18T23:51:40Z",
        "updatedAt" : "2020-07-21T16:58:06Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "ef1e6ed1-c682-40b0-a3a3-71682391b9b9",
        "parentId" : "bd61630d-2bb1-4478-b01d-338851792ddc",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "@viirya - sure. added.",
        "createdAt" : "2020-07-19T05:23:54Z",
        "updatedAt" : "2020-07-21T16:58:06Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      }
    ],
    "commit" : "b1a8a927f283b095716a638a26f9c5d5e9cc380c",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +2647,2651 @@        \"shuffled hash join. Note: Coalescing bucketed table can avoid unnecessary shuffling \" +\n        \"in join, but it also reduces parallelism and could possibly cause OOM for \" +\n        \"shuffled hash join.\")\n      .version(\"3.1.0\")\n      .booleanConf"
  },
  {
    "id" : "e88722ff-ad0a-4fc2-8b9d-934f542fa177",
    "prId" : 29031,
    "prUrl" : "https://github.com/apache/spark/pull/29031#pullrequestreview-446172959",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e031a638-0786-4956-9af8-b246f259467d",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "nit: indentation is wrong",
        "createdAt" : "2020-07-10T07:15:13Z",
        "updatedAt" : "2020-08-10T18:02:22Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "394126da654c15c59a58bd906f0ca4564ce1c4e5",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1213,1217 @@\n  val REMOVE_REDUNDANT_PROJECTS_ENABLED = buildConf(\"spark.sql.execution.removeRedundantProjects\")\n    .internal()\n    .doc(\"Whether to remove redundant project exec node based on children's output and \" +\n      \"ordering requirement.\")"
  },
  {
    "id" : "4335edf2-ef53-407c-8c15-15c52aaab5c9",
    "prId" : 28853,
    "prUrl" : "https://github.com/apache/spark/pull/28853#pullrequestreview-433029054",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "aa5852cb-0465-4ec9-b2e3-35e164c3d218",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Could you add `checkValue`?",
        "createdAt" : "2020-06-18T07:55:13Z",
        "updatedAt" : "2020-06-20T03:56:22Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "1fb9dc651d5e1041fef8612bdbd3299dcea494a5",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +1182,1186 @@      \"effective only when using file-based sources such as Parquet, JSON and ORC.\")\n    .version(\"3.1.0\")\n    .intConf\n    .checkValue(v => v > 0, \"The min partition number must be a positive integer.\")\n    .createOptional"
  },
  {
    "id" : "e018f0f5-d775-4ffe-8e75-877e45d2aca5",
    "prId" : 28778,
    "prUrl" : "https://github.com/apache/spark/pull/28778#pullrequestreview-427925753",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "36b20081-9bae-4c38-800f-4f71d56b8186",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "`spark.sql.default.parallelism` -> `spark.sql.sessionLocalDefaultParallelism`?",
        "createdAt" : "2020-06-10T09:48:16Z",
        "updatedAt" : "2020-06-10T10:46:04Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "26382ce4-aac3-455c-b4c0-2aa3e59bd054",
        "parentId" : "36b20081-9bae-4c38-800f-4f71d56b8186",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "Em.. is it better to keep similar with `spark.default.parallelism`? so we can set this config easy. `sessionLocalDefaultParallelism ` seems complex.",
        "createdAt" : "2020-06-10T10:34:13Z",
        "updatedAt" : "2020-06-10T10:46:04Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      }
    ],
    "commit" : "55d367f6a7a277c83eff1ac83dd7708cfca608e9",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +372,376 @@      .createWithDefault(true)\n\n  val DEFAULT_PARALLELISM = buildConf(\"spark.sql.default.parallelism\")\n    .doc(\"The session-local default number of partitions and this value is widely used \" +\n      \"inside physical plans. If not set, the physical plans refer to \" +"
  },
  {
    "id" : "1fcbe04f-8935-41da-bb67-983363b8a881",
    "prId" : 28707,
    "prUrl" : "https://github.com/apache/spark/pull/28707#pullrequestreview-458941008",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ddf08b00-dc76-4a05-bb2f-68e88d94f9a7",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "Change `UnsafeRow` to `checkpoint` ? Most end users do not know what are `UnsafeRow`",
        "createdAt" : "2020-07-27T18:58:41Z",
        "updatedAt" : "2020-07-27T18:58:41Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "189a6265-0379-4fdf-b331-5e06c8712e7c",
        "parentId" : "ddf08b00-dc76-4a05-bb2f-68e88d94f9a7",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "Sure, will submit a follow-up PR today.",
        "createdAt" : "2020-07-31T06:21:18Z",
        "updatedAt" : "2020-07-31T06:21:18Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "557eb3099b3d0abe1fd2d7d91754fa747e05d200",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +1241,1245 @@    buildConf(\"spark.sql.streaming.stateStore.formatValidation.enabled\")\n      .internal()\n      .doc(\"When true, check if the UnsafeRow from the state store is valid or not when running \" +\n        \"streaming queries. This can happen if the state store format has been changed. Note, \" +\n        \"the feature is only effective in the build-in HDFS state store provider now.\")"
  },
  {
    "id" : "d360c2bc-fc1a-4f9b-ad5b-48622977c453",
    "prId" : 28676,
    "prUrl" : "https://github.com/apache/spark/pull/28676#pullrequestreview-450232532",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fa6f23aa-7920-4ebe-bdbc-2043c16f1f3e",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "can we make it an internal conf?",
        "createdAt" : "2020-07-17T15:09:43Z",
        "updatedAt" : "2020-07-17T20:59:39Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "ec3fa5d7-d222-4c47-aace-eec08dad89e7",
        "parentId" : "fa6f23aa-7920-4ebe-bdbc-2043c16f1f3e",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "Updated",
        "createdAt" : "2020-07-17T17:35:39Z",
        "updatedAt" : "2020-07-17T20:59:39Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "9caeecddaa07ef825b73835a3666502df468f881",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +2667,2671 @@\n  val BROADCAST_HASH_JOIN_OUTPUT_PARTITIONING_EXPAND_LIMIT =\n    buildConf(\"spark.sql.execution.broadcastHashJoin.outputPartitioningExpandLimit\")\n      .internal()\n      .doc(\"The maximum number of partitionings that a HashPartitioning can be expanded to. \" +"
  },
  {
    "id" : "4d453211-9543-4882-b3af-ec2b8aa90e93",
    "prId" : 28661,
    "prUrl" : "https://github.com/apache/spark/pull/28661#pullrequestreview-421486300",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "660ac2f4-2094-4439-99c5-d911bd224c00",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Is this targeting 3.0.0?",
        "createdAt" : "2020-05-28T06:17:37Z",
        "updatedAt" : "2020-05-29T07:58:03Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "1be7b9a8-99e7-44bb-a32f-c0bd993ed351",
        "parentId" : "660ac2f4-2094-4439-99c5-d911bd224c00",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Can be arguable .. but it virtually changes the exception message only at the core. I personally think it's okay/good to have it in 3.0. But I am okay to retarget if there's any concern about it.",
        "createdAt" : "2020-05-28T06:37:04Z",
        "updatedAt" : "2020-05-29T07:58:03Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "3ec44a10-da26-4f12-b213-3c5029cf751c",
        "parentId" : "660ac2f4-2094-4439-99c5-d911bd224c00",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "@gatorsmile . Are you okay with 3.0.0 targeting here?\r\nAlthough we are in RC stage, this PR looks worth for backporting. (Also the default is `false`.)\r\n\r\nOne question for @HyukjinKwon . Do we want this as a dynamic configuration instead of static configuration? I mean do we want to switch on/off during runtime?",
        "createdAt" : "2020-05-29T10:13:15Z",
        "updatedAt" : "2020-05-29T10:13:39Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "c1579f5c-a561-42bf-b951-da17e847bcf1",
        "parentId" : "660ac2f4-2094-4439-99c5-d911bd224c00",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "It doesn't strictly have to be in Spark 3.0. I just wanted to have some feedback quicker from users, and thought it's good to try this in Spark 3.0 as technically we just touch the error messages only here.\r\n\r\nI don't super strongly feel it has to land to Spark 3.0 - it's okay to retarget 3.1 if anyone feels strongly it has to be only in the master. Let me know :-)",
        "createdAt" : "2020-05-31T11:42:43Z",
        "updatedAt" : "2020-05-31T11:42:43Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "28c2a516eec5ffbd527b9e62869b279a162458e4",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +1790,1794 @@        \"together with Python stacktrace. By default, it is disabled and hides JVM stacktrace \" +\n        \"and shows a Python-friendly exception only.\")\n      .version(\"3.0.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "a0436c31-aef2-4d78-bae9-25486efaf560",
    "prId" : 28500,
    "prUrl" : "https://github.com/apache/spark/pull/28500#pullrequestreview-409236015",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "363bffe2-42c4-4d23-aa85-75d38ed899c3",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "Also check the migration guide. We need to remove one line/",
        "createdAt" : "2020-05-11T14:39:07Z",
        "updatedAt" : "2020-05-11T14:43:06Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      }
    ],
    "commit" : "0fc974fca87be4f7b6521de4e5de2eec596f4761",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +2236,2240 @@      .version(\"3.0.0\")\n      .booleanConf\n      .createWithDefault(true)\n\n  val LEGACY_BUCKETED_TABLE_SCAN_OUTPUT_ORDERING ="
  },
  {
    "id" : "1c5c1980-7090-4246-bd9b-3d10df22f0c4",
    "prId" : 28459,
    "prUrl" : "https://github.com/apache/spark/pull/28459#pullrequestreview-406291654",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "325bb975-bc37-4dce-a51f-0e5cbc27b658",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Technically we shouldn't necessarily go through deprecation to remove as it's an internal configuration but let me stay conservative here by deprecating first.",
        "createdAt" : "2020-05-06T04:04:17Z",
        "updatedAt" : "2020-05-06T05:25:18Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "34420fb12fcfd64e345c8487beef9c05d39b10bc",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +2609,2613 @@      DeprecatedConfig(SHUFFLE_TARGET_POSTSHUFFLE_INPUT_SIZE.key, \"3.0\",\n        s\"Use '${ADVISORY_PARTITION_SIZE_IN_BYTES.key}' instead of it.\"),\n      DeprecatedConfig(OPTIMIZER_METADATA_ONLY.key, \"3.0\",\n        \"Avoid to depend on this optimization to prevent a potential correctness issue. \" +\n          \"If you must use, use 'SparkSessionExtensions' instead to inject it as a custom rule.\")"
  },
  {
    "id" : "61d8dca6-24f9-46dc-8a29-df790666638e",
    "prId" : 28459,
    "prUrl" : "https://github.com/apache/spark/pull/28459#pullrequestreview-406303920",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a5dc7e90-27ce-45f1-9fe2-13f676530a21",
        "parentId" : null,
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "nit: could you add this note in the comment of the rule as well?",
        "createdAt" : "2020-05-06T04:57:24Z",
        "updatedAt" : "2020-05-06T05:25:18Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "34420fb12fcfd64e345c8487beef9c05d39b10bc",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +846,850 @@      \"to produce the partition columns instead of table scans. It applies when all the columns \" +\n      \"scanned are partition columns and the query has an aggregate operator that satisfies \" +\n      \"distinct semantics. By default the optimization is disabled, and deprecated as of Spark \" +\n      \"3.0 since it may return incorrect results when the files are empty, see also SPARK-26709.\" +\n      \"It will be removed in the future releases. If you must use, use 'SparkSessionExtensions' \" +"
  },
  {
    "id" : "b55ffd6f-fbe8-4511-a15a-1da22c4cec42",
    "prId" : 28424,
    "prUrl" : "https://github.com/apache/spark/pull/28424#pullrequestreview-404566788",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "389435c2-dd5f-4a2b-adda-c25c1f421f18",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "How about merging the two options above into one by using `-1` for deactivating the rule?",
        "createdAt" : "2020-05-01T06:49:53Z",
        "updatedAt" : "2020-05-02T06:42:37Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "54451e21-7bb8-4e23-b979-7861da0c38e4",
        "parentId" : "389435c2-dd5f-4a2b-adda-c25c1f421f18",
        "authorId" : "ad4cb313-47b2-4cfa-9c1f-0a3596794a67",
        "body" : "@maropu We also want to control whether pushdown should happen when right side is broadcastable.",
        "createdAt" : "2020-05-02T06:18:12Z",
        "updatedAt" : "2020-05-02T06:42:37Z",
        "lastEditedBy" : "ad4cb313-47b2-4cfa-9c1f-0a3596794a67",
        "tags" : [
        ]
      },
      {
        "id" : "d05a3b79-4224-4a97-b26d-39b7cb60259f",
        "parentId" : "389435c2-dd5f-4a2b-adda-c25c1f421f18",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Rather, we need an new option to turn on/off this rule? Any negative impact on performance?",
        "createdAt" : "2020-05-02T08:50:47Z",
        "updatedAt" : "2020-05-02T08:50:47Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "94935663-79f3-4b70-8afb-dc297b4ed568",
        "parentId" : "389435c2-dd5f-4a2b-adda-c25c1f421f18",
        "authorId" : "ad4cb313-47b2-4cfa-9c1f-0a3596794a67",
        "body" : "No I didn't find any performance regression with this stats based triggers. I kept everything behind config based on other configs used in similar features (Ex - costBasedjoinReorder).\r\n\r\nShould we remove the \"spark.sql.cbo.optimizeIntersect.enabled\" config here?",
        "createdAt" : "2020-05-02T10:09:20Z",
        "updatedAt" : "2020-05-02T10:09:29Z",
        "lastEditedBy" : "ad4cb313-47b2-4cfa-9c1f-0a3596794a67",
        "tags" : [
        ]
      },
      {
        "id" : "cc3c963c-2fce-461e-878b-648e4e684886",
        "parentId" : "389435c2-dd5f-4a2b-adda-c25c1f421f18",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Yea, could you remove it now? If we need to revert it back in the result of following reviews, I think that it should be added again at that time.\r\n\r\nbtw, I think you don't need to add new configs under the cbo namespace. We already have some optimizer rules depending on stats, e.g., `spark.sql.optimizer.dynamicPartitionPruning.useStats`. Could you refer the implementation?",
        "createdAt" : "2020-05-02T10:37:45Z",
        "updatedAt" : "2020-05-02T10:41:57Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "6ba042d3-5d51-44eb-9c2b-740c92780dc9",
        "parentId" : "389435c2-dd5f-4a2b-adda-c25c1f421f18",
        "authorId" : "ad4cb313-47b2-4cfa-9c1f-0a3596794a67",
        "body" : "@maropu I have removed cbo namespace from the config names.\r\n\r\nBut I see that all stats based optimization like costBasedJoinReordering, dynamicPartitionPruining are behind a configuration property. Shouldn't we follow same trend here?",
        "createdAt" : "2020-05-02T11:52:49Z",
        "updatedAt" : "2020-05-02T11:55:23Z",
        "lastEditedBy" : "ad4cb313-47b2-4cfa-9c1f-0a3596794a67",
        "tags" : [
        ]
      },
      {
        "id" : "894a0462-29cf-473b-9d84-63089922a40b",
        "parentId" : "389435c2-dd5f-4a2b-adda-c25c1f421f18",
        "authorId" : "a2fcc15e-a51a-42f0-87a6-137048a28e30",
        "body" : "@prakharjain09 Hello, if the stats are wrong or incomplete (like the cardinality changed drastically after that last time we performed the analyze on the table) , we could possibly slow down especially when we push the distinct to the right leg , no ? @maropu isn't it better to keep this under a flag ?",
        "createdAt" : "2020-05-02T22:29:08Z",
        "updatedAt" : "2020-05-02T22:29:09Z",
        "lastEditedBy" : "a2fcc15e-a51a-42f0-87a6-137048a28e30",
        "tags" : [
        ]
      },
      {
        "id" : "bfd16127-0730-4ccf-b2dc-02651af77924",
        "parentId" : "389435c2-dd5f-4a2b-adda-c25c1f421f18",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Yea, if we have the negative impact.",
        "createdAt" : "2020-05-02T22:59:14Z",
        "updatedAt" : "2020-05-02T22:59:15Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "29719f9de51e7026584b7882755d3cd7806ae8f5",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +1697,1701 @@        \" in Intersect optimization\")\n      .intConf\n      .createWithDefault(100)\n\n  private def isValidTimezone(zone: String): Boolean = {"
  },
  {
    "id" : "f59d6996-0c1e-45f0-88ac-5fc041fca6b5",
    "prId" : 28366,
    "prUrl" : "https://github.com/apache/spark/pull/28366#pullrequestreview-404433772",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dea91d51-b1c6-4a08-895f-0bbc209f34ed",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "We need `.transform(_.toUpperCase(Locale.ROOT))`? Also, could we validate input by `checkValues`? btw, is this feature expected to cover custom data sources except for the prebuilt ones (parquet, orc, ...)?",
        "createdAt" : "2020-04-29T23:52:19Z",
        "updatedAt" : "2020-05-05T06:13:39Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "72bb9c24-777b-4e23-b8c1-0410e49590ac",
        "parentId" : "dea91d51-b1c6-4a08-895f-0bbc209f34ed",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "We compare this list with `toLowerCase` when we need it. So seems to be fine to leave it here. Another similar example is `spark.sql.sources.useV1SourceList`. And as `useV1SourceList` too, seems `checkValues` is not needed.\r\n\r\nCurrently I think it is safer to assume custom data sources don't support this feature. I actually also think if custom data source wants to support it, it is better to adapt data source v2.\r\n\r\nWe don't have a common API for v1 data sources that tells if it supports nested predicate pushdown. If we really want to allow custom v1 data sources have that, we can consider adding one common v1 API for the purpose. But, again, seems to me that we will encourage adapting v2 instead adding new things to v1.",
        "createdAt" : "2020-05-01T06:03:51Z",
        "updatedAt" : "2020-05-05T06:13:39Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "34211d65-07f7-44bd-8388-22d88d263cce",
        "parentId" : "dea91d51-b1c6-4a08-895f-0bbc209f34ed",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "> Currently I think it is safer to assume custom data sources don't support this feature.\r\n\r\nYea, +1 on your thought.",
        "createdAt" : "2020-05-01T06:08:04Z",
        "updatedAt" : "2020-05-05T06:13:39Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "2bd4fe57-9821-4f25-a0b0-ecd5f1d307cf",
        "parentId" : "dea91d51-b1c6-4a08-895f-0bbc209f34ed",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "> Currently I think it is safer to assume custom data sources don't support this feature.\r\n\r\nLooks fine. @dbtsai are you good with it? Do you have use cases that need nested predicate pushdown for non-file-source?",
        "createdAt" : "2020-05-01T07:04:17Z",
        "updatedAt" : "2020-05-05T06:13:39Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "f19884c5-46a2-4bb0-812f-29e84820f4be",
        "parentId" : "dea91d51-b1c6-4a08-895f-0bbc209f34ed",
        "authorId" : "677aa336-324b-4b93-8300-21f8bd378f26",
        "body" : "In v1, we don't have any use-case for supporting it in custom data source. I'm good with it.",
        "createdAt" : "2020-05-01T22:37:36Z",
        "updatedAt" : "2020-05-05T06:13:39Z",
        "lastEditedBy" : "677aa336-324b-4b93-8300-21f8bd378f26",
        "tags" : [
        ]
      }
    ],
    "commit" : "00b9d47702ae76fca3c7246155175cb42f75136f",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +2073,2077 @@        \"other data sources don't support this feature yet. So the default value is 'parquet,orc'.\")\n      .version(\"3.0.0\")\n      .stringConf\n      .createWithDefault(\"parquet,orc\")\n"
  },
  {
    "id" : "e601c69a-ef57-419e-9f88-e275d8984329",
    "prId" : 28366,
    "prUrl" : "https://github.com/apache/spark/pull/28366#pullrequestreview-406392374",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9f5cee65-926b-4b8a-998c-849dfab43883",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Seems we decided to only make this configuration effective against DSv1, which seems okay because only DSv1 will have compatibility issues.\r\n\r\nBut shell we at least explicitly mention that this configuration is only effective with DSv1, (or make this configuration effective against DSv2)? Seems like it's going to be confusing to both end users or developers.\r\n\r\n",
        "createdAt" : "2020-05-06T08:04:49Z",
        "updatedAt" : "2020-05-06T08:05:03Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "0e3a1f64-133f-4921-979e-57dd324e5a69",
        "parentId" : "9f5cee65-926b-4b8a-998c-849dfab43883",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "I think DSv2 API supposes nested column capacity like pushdown and pruning, so we only need to deal with DSv1 compatibility issues here. Precisely, file source.\r\n\r\nI will create a simple followup to refine the doc of this configuration for this point. Thanks.\r\n\r\n",
        "createdAt" : "2020-05-06T08:12:51Z",
        "updatedAt" : "2020-05-06T08:12:52Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "a9ad2a16-50f2-40c3-b84b-d461f3585dbf",
        "parentId" : "9f5cee65-926b-4b8a-998c-849dfab43883",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Thanks!",
        "createdAt" : "2020-05-06T08:16:57Z",
        "updatedAt" : "2020-05-06T08:16:58Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "00b9d47702ae76fca3c7246155175cb42f75136f",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +2071,2075 @@        \"columns and/or names containing `dots` to data sources. Currently, Parquet implements \" +\n        \"both optimizations while ORC only supports predicates for names containing `dots`. The \" +\n        \"other data sources don't support this feature yet. So the default value is 'parquet,orc'.\")\n      .version(\"3.0.0\")\n      .stringConf"
  },
  {
    "id" : "8c852df8-3cad-4bb7-8648-90c979aefaec",
    "prId" : 28233,
    "prUrl" : "https://github.com/apache/spark/pull/28233#pullrequestreview-394648594",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5efc9c81-1a76-46e7-b04e-ddc311aaac83",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-30279, commit ID: 71c73d58f6e88d2558ed2e696897767d93bac60f#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-04-16T13:46:44Z",
        "updatedAt" : "2020-04-16T13:46:44Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "108396425097f420fb04615fe3068f03972f5062",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +2520,2524 @@      .internal()\n      .doc(\"When true, grouping_id() returns int values instead of long values.\")\n      .version(\"3.1.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "7a3f4723-27a7-4483-8f77-df8015e24bf4",
    "prId" : 28116,
    "prUrl" : "https://github.com/apache/spark/pull/28116#pullrequestreview-387697750",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "690264a3-41a7-4ca5-a761-091b4afcb3d7",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-27870, commit ID: 26998b8#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-04-04T13:03:59Z",
        "updatedAt" : "2020-04-04T13:03:59Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "dc0a44dbd0c01002358ff8f2d77036c21d01e459",
    "line" : 1428,
    "diffHunk" : "@@ -1,1 +1826,1830 @@        \"than 4 bytes. Lowering this value could make small Pandas UDF batch iterated and \" +\n        \"pipelined; however, it might degrade performance. See SPARK-27870.\")\n      .version(\"3.0.0\")\n      .fallbackConf(BUFFER_SIZE)\n"
  },
  {
    "id" : "d14b540d-c885-48ef-9f15-87c5f8078733",
    "prId" : 27936,
    "prUrl" : "https://github.com/apache/spark/pull/27936#pullrequestreview-376302288",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a508e180-9d03-47f9-a5f6-faa2eac230f7",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Thanks, `ansi` conf overrides `legacy` conf ? Do we have another `legacy` conf like this? Is it consistent with the other `legacy`? Could you update the migration guide and conf document to be consistent, too?\r\n\r\ncc @gatorsmile ",
        "createdAt" : "2020-03-17T16:06:31Z",
        "updatedAt" : "2020-03-17T16:54:43Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "f9451e3b-55e9-4b51-b32a-d16a95e133ba",
        "parentId" : "a508e180-9d03-47f9-a5f6-faa2eac230f7",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "This is the first one, and I don't think we need a migration guide as the behavior doesn't change by default.\r\n\r\nI'll update the config doc.",
        "createdAt" : "2020-03-17T16:38:45Z",
        "updatedAt" : "2020-03-17T16:54:43Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "e14def93-b050-45cb-a859-85706168daa9",
        "parentId" : "a508e180-9d03-47f9-a5f6-faa2eac230f7",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Thank you!",
        "createdAt" : "2020-03-17T18:32:42Z",
        "updatedAt" : "2020-03-17T18:32:42Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "8459d63ddc5f14ef7f91a5d9ebf07a2cdb2090ba",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +3017,3021 @@  def legacySizeOfNull: Boolean = {\n    // size(null) should return null under ansi mode.\n    getConf(SQLConf.LEGACY_SIZE_OF_NULL) && !getConf(ANSI_ENABLED)\n  }\n"
  },
  {
    "id" : "8da4bb91-6ffa-4599-b0b5-77fb1c94e314",
    "prId" : 27915,
    "prUrl" : "https://github.com/apache/spark/pull/27915#pullrequestreview-376297008",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1ffc0515-0c03-4b9a-90e1-45e5f0220979",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "It's not recommended to add a method to read a conf if it's only used once.",
        "createdAt" : "2020-03-16T16:44:10Z",
        "updatedAt" : "2020-03-18T14:27:24Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "f5989e80-4ba8-4d7f-b2ca-457307a09bff",
        "parentId" : "1ffc0515-0c03-4b9a-90e1-45e5f0220979",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "It has been already used in 9 places.",
        "createdAt" : "2020-03-16T18:50:31Z",
        "updatedAt" : "2020-03-18T14:27:24Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "addd7b2f-6a3d-44fd-8f2d-8afc12c909b4",
        "parentId" : "1ffc0515-0c03-4b9a-90e1-45e5f0220979",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Yea I know, but we should stop it. Let's follow https://github.com/apache/spark/commit/e736c62764137b2c3af90d2dc8a77e391891200a#diff-d07d3850a0ad8635a0c5562ed1e8bf26R184",
        "createdAt" : "2020-03-17T08:29:39Z",
        "updatedAt" : "2020-03-18T14:27:24Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "ec8cfffe-cbbc-4aac-9384-7830b650d899",
        "parentId" : "1ffc0515-0c03-4b9a-90e1-45e5f0220979",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Is it a correct link?",
        "createdAt" : "2020-03-17T09:46:54Z",
        "updatedAt" : "2020-03-18T14:27:24Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "b3247fa5-3bbc-43b8-a2a6-6e01c546b521",
        "parentId" : "1ffc0515-0c03-4b9a-90e1-45e5f0220979",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "I need to read the config in Java. The obvious replacement doesn't work:\r\n```java\r\nthis.rebaseDateTime = SQLConf.get().getConf(SQLConf.LEGACY_PARQUET_REBASE_DATETIME());\r\n```\r\n```\r\nError:(134, 48) java: incompatible types: inference variable T has incompatible bounds\r\n    equality constraints: java.lang.Object\r\n    upper bounds: java.lang.Boolean,java.lang.Object\r\n```\r\nIf you have strong opinion about removing the method `def parquetRebaseDateTimeEnabled`, could you show me how to read a SQL config in Java. I haven't found any example, unfortunately:\r\n```\r\n$ find . -name \"*.java\" -print0|xargs -0 grep 'SQLConf.get'\r\n./sql/core/src/test/java/test/org/apache/spark/sql/JavaBeanDeserializationSuite.java:      DateTimeUtils.getZoneId(SQLConf.get().sessionLocalTimeZone()));\r\n./sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader.java:    this.rebaseDateTime = SQLConf.get().parquetRebaseDateTimeEnabled();\r\n```",
        "createdAt" : "2020-03-17T18:25:41Z",
        "updatedAt" : "2020-03-18T14:27:24Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "184fcd8a9d6d177352aa67732f974e7ebf8eaf20",
    "line" : 25,
    "diffHunk" : "@@ -1,1 +3079,3083 @@  def integerGroupingIdEnabled: Boolean = getConf(SQLConf.LEGACY_INTEGER_GROUPING_ID)\n\n  def parquetRebaseDateTimeEnabled: Boolean = getConf(SQLConf.LEGACY_PARQUET_REBASE_DATETIME)\n\n  /** ********************** SQLConf functionality methods ************ */"
  },
  {
    "id" : "5e1f3c8d-d20b-4b10-95f1-2e1f7ef98d9e",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370142181",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "943529ba-67f6-4d00-8081-94a7f600de63",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-23850, commit ID: 6a55d8b03053e616dcacb79cd2c29a06d219dc32#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:09:51Z",
        "updatedAt" : "2020-03-06T08:09:51Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1877,1881 @@      \"in the explain output. This redaction is applied on top of the global redaction \" +\n      s\"configuration defined by ${SECRET_REDACTION_PATTERN.key}.\")\n    .version(\"2.2.2\")\n    .regexConf\n    .createWithDefault(\"(?i)url\".r)"
  },
  {
    "id" : "234cbcee-05f5-4382-b1c6-e04ee55ab654",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370142417",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6a05a705-9069-4350-9784-becb0ce61428",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-22791, commit ID: 28315714ddef3ddcc192375e98dd5207cf4ecc98#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:10:21Z",
        "updatedAt" : "2020-03-06T08:10:22Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +1887,1891 @@        \"dummy value. This is currently used to redact the output of SQL explain commands. \" +\n        \"When this conf is not set, the value from `spark.redaction.string.regex` is used.\")\n      .version(\"2.3.0\")\n      .fallbackConf(org.apache.spark.internal.config.STRING_REDACTION_PATTERN)\n"
  },
  {
    "id" : "acee5e2e-b87f-42bd-aa5d-a959c59ad14a",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370142568",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "08ff5909-cea7-464d-b88c-fc9139423297",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-22771, commit ID: f2b3525c17d660cf6f082bbafea8632615b4f58e#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:10:44Z",
        "updatedAt" : "2020-03-06T08:10:44Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +1893,1897 @@    .doc(\"When this option is set to false and all inputs are binary, `functions.concat` returns \" +\n      \"an output as binary. Otherwise, it returns as a string.\")\n    .version(\"2.3.0\")\n    .booleanConf\n    .createWithDefault(false)"
  },
  {
    "id" : "ae1e80fa-a24d-4e61-b606-d0d93087b1ff",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370142738",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "597c7a9c-e111-4301-96c5-97ba3168e6b8",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-22937, commit ID: bf853018cabcd3b3abf84bfe534d2981020b4a71#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:11:06Z",
        "updatedAt" : "2020-03-06T08:11:06Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 29,
    "diffHunk" : "@@ -1,1 +1900,1904 @@    .doc(\"When this option is set to false and all inputs are binary, `elt` returns \" +\n      \"an output as binary. Otherwise, it returns as a string.\")\n    .version(\"2.3.0\")\n    .booleanConf\n    .createWithDefault(false)"
  },
  {
    "id" : "f6b6f1c4-a721-499f-9714-a287a23042f5",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370142956",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c5dc6b47-302d-49be-a1ab-3c4f8edddc42",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-26263, commit ID: 5a140b7844936cf2b65f08853b8cfd8c499d4f13#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:11:32Z",
        "updatedAt" : "2020-03-06T08:11:32Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 38,
    "diffHunk" : "@@ -1,1 +1911,1915 @@        \"When this option is set to false, the partition column value will be converted to null \" +\n        \"if it can not be casted to corresponding user-specified schema.\")\n      .version(\"3.0.0\")\n      .booleanConf\n      .createWithDefault(true)"
  },
  {
    "id" : "f416436f-1540-4cbd-8f10-3e9296bc6640",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370143170",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f678c1e6-c904-4f73-8cd1-2bd2383a8fce",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-24063, commit ID: c4bbfd177b4e7cb46f47b39df9fd71d2d9a12c6d#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:12:00Z",
        "updatedAt" : "2020-03-06T08:12:00Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 47,
    "diffHunk" : "@@ -1,1 +1919,1923 @@      .doc(\"The max number of entries to be stored in queue to wait for late epochs. \" +\n        \"If this parameter is exceeded by the size of the queue, stream will stop with an error.\")\n      .version(\"3.0.0\")\n      .intConf\n      .createWithDefault(10000)"
  },
  {
    "id" : "b9459994-bc9e-4ec4-b409-65a1369b38fd",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370143369",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a33a65fd-20cf-45ea-9622-ca5d454fd446",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-22789, commit ID: 8941a4abcada873c26af924e129173dc33d66d71#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:12:27Z",
        "updatedAt" : "2020-03-06T08:12:27Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 56,
    "diffHunk" : "@@ -1,1 +1928,1932 @@      .doc(\"The size (measured in number of rows) of the queue used in continuous execution to\" +\n        \" buffer the results of a ContinuousDataReader.\")\n      .version(\"2.3.0\")\n      .intConf\n      .createWithDefault(1024)"
  },
  {
    "id" : "2ec404b1-6c2a-4e3d-b164-8f17ae62f81b",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370143547",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cfececfe-e00a-45f1-9bef-647ff97fdce4",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-22789, commit ID: 8941a4abcada873c26af924e129173dc33d66d71#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:12:48Z",
        "updatedAt" : "2020-03-06T08:12:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 65,
    "diffHunk" : "@@ -1,1 +1937,1941 @@      .doc(\"The interval at which continuous execution readers will poll to check whether\" +\n        \" the epoch has advanced on the driver.\")\n      .version(\"2.3.0\")\n      .timeConf(TimeUnit.MILLISECONDS)\n      .createWithDefault(100)"
  },
  {
    "id" : "a1ba5e28-b4e2-40e3-9b74-b21950c538da",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370143714",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2369d66a-c695-4164-af3a-26f46d1c635d",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-28747, commit ID: cb06209fc908bac6ce6a8f20653865489773cbc3#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:13:12Z",
        "updatedAt" : "2020-03-06T08:13:12Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 74,
    "diffHunk" : "@@ -1,1 +1946,1950 @@      \"implementation class names for which Data Source V2 code path is disabled. These data \" +\n      \"sources will fallback to Data Source V1 code path.\")\n    .version(\"3.0.0\")\n    .stringConf\n    .createWithDefault(\"avro,csv,json,kafka,orc,parquet,text\")"
  },
  {
    "id" : "7192eff4-e8b0-4cb9-8f5e-7479ac9fb3c6",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370143893",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cc47143a-7059-45f1-83a0-14418bbf9bb3",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-23196, commit ID: 588b9694c1967ff45774431441e84081ee6eb515#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:13:35Z",
        "updatedAt" : "2020-03-06T08:13:36Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 82,
    "diffHunk" : "@@ -1,1 +1953,1957 @@    .doc(\"A comma-separated list of fully qualified data source register class names for which\" +\n      \" StreamWriteSupport is disabled. Writes to these sources will fall back to the V1 Sinks.\")\n    .version(\"2.3.1\")\n    .stringConf\n    .createWithDefault(\"\")"
  },
  {
    "id" : "2c91ea76-d30a-454a-9eab-ba7545bfae59",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370144048",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0f0fa159-5ada-496a-94d7-323c810a1db0",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-23362, commit ID: 0a73aa31f41c83503d5d99eff3c9d7b406014ab3#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:13:57Z",
        "updatedAt" : "2020-03-06T08:13:57Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 91,
    "diffHunk" : "@@ -1,1 +1964,1968 @@          \"MicroBatchReadSupport is disabled. Reads from these sources will fall back to the \" +\n          \"V1 Sources.\")\n      .version(\"2.4.0\")\n      .stringConf\n      .createWithDefault(\"\")"
  },
  {
    "id" : "ae14eff6-b7c3-4728-9e62-80d6b1a3db8f",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370144201",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cb91eb7a-fc67-41bc-9ff3-2ae7b5d27bb3",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-20236, commit ID: b96248862589bae1ddcdb14ce4c802789a001306#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:14:20Z",
        "updatedAt" : "2020-03-06T08:14:20Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 100,
    "diffHunk" : "@@ -1,1 +1985,1989 @@        \"dataframe.write.option(\\\"partitionOverwriteMode\\\", \\\"dynamic\\\").save(path).\"\n      )\n      .version(\"2.3.0\")\n      .stringConf\n      .transform(_.toUpperCase(Locale.ROOT))"
  },
  {
    "id" : "e109d930-3d7c-4ae9-8a64-488e8f9d1937",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370144375",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3ab6bd91-4b0e-4f90-8476-26cc63653d71",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-28730, commit ID: 895c90b582cc2b2667241f66d5b733852aeef9eb#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:14:42Z",
        "updatedAt" : "2020-03-06T08:14:43Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 109,
    "diffHunk" : "@@ -1,1 +2010,2014 @@        \"not allowed.\"\n      )\n      .version(\"3.0.0\")\n      .stringConf\n      .transform(_.toUpperCase(Locale.ROOT))"
  },
  {
    "id" : "4af7069f-112c-4f8c-a752-49ecef1c0581",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370144568",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "52a276dd-d40f-4ada-8024-1ddac71310c0",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-30125, commit ID: d9b30694122f8716d3acb448638ef1e2b96ebc7a#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:15:06Z",
        "updatedAt" : "2020-03-06T08:15:06Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 118,
    "diffHunk" : "@@ -1,1 +2021,2025 @@      \"field. 2. Spark will forbid using the reserved keywords of ANSI SQL as identifiers in \" +\n      \"the SQL parser.\")\n    .version(\"3.0.0\")\n    .booleanConf\n    .createWithDefault(false)"
  },
  {
    "id" : "c9d5724a-569b-4301-a4f9-605972a2290e",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370144745",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ede90af9-ce1f-4270-8ae0-dc2d84efada1",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-23207 and SPARK-22905 and SPARK-24564 and SPARK-25114, commit ID: 4d2d3d47e00e78893b1ecd5a9a9070adc5243ac9#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:15:29Z",
        "updatedAt" : "2020-03-06T08:15:29Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 127,
    "diffHunk" : "@@ -1,1 +2034,2038 @@        \"to generate consistent repartition results. The performance of repartition() may go \" +\n        \"down since we insert extra local sort before it.\")\n      .version(\"2.1.4\")\n      .booleanConf\n      .createWithDefault(true)"
  },
  {
    "id" : "c547b17e-37ad-4cbd-876c-18a5fba7dae0",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370144927",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4bbb1294-0d2a-4096-b507-b701f3f3aa86",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-4502, commit ID: dfcff38394929970fee454c69864d0e10d59f8d4#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:15:52Z",
        "updatedAt" : "2020-03-06T08:15:52Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 136,
    "diffHunk" : "@@ -1,1 +2045,2049 @@        \"reading unnecessary nested column data. Currently Parquet and ORC are the \" +\n        \"data sources that implement this optimization.\")\n      .version(\"2.4.1\")\n      .booleanConf\n      .createWithDefault(true)"
  },
  {
    "id" : "ece6f02b-1ac3-4db1-abd7-5814cc55b40f",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370145089",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "35938f0f-908f-44c8-a070-bb8cf63b6f34",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-26837, commit ID: 0f2c0b53e8fb18c86c67b5dd679c006db93f94a5#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:16:15Z",
        "updatedAt" : "2020-03-06T08:16:15Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 145,
    "diffHunk" : "@@ -1,1 +2055,2059 @@        \"satisfying a query. This optimization allows object serializers to avoid \" +\n        \"executing unnecessary nested expressions.\")\n      .version(\"3.0.0\")\n      .booleanConf\n      .createWithDefault(true)"
  },
  {
    "id" : "a655f6d7-6fef-4fae-ba32-7fdf82d7df59",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370145309",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6adc0579-b12f-490a-afe3-97ab1d4a3edf",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-27707, commit ID: 127bc899ae78d73332a87f0972b5db3c9936c1f1#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:16:40Z",
        "updatedAt" : "2020-03-06T08:16:40Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 154,
    "diffHunk" : "@@ -1,1 +2066,2070 @@        \"physical data source scanning. For pruning nested fields from scanning, please use \" +\n        \"`spark.sql.optimizer.nestedSchemaPruning.enabled` config.\")\n      .version(\"3.0.0\")\n      .booleanConf\n      .createWithDefault(true)"
  },
  {
    "id" : "4e8c3200-63e2-42ee-90ad-43062ca8996d",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370145457",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "42d51712-1e7b-4385-8a0c-804c4076becb",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-24193, commit ID: 8a837bf4f3f2758f7825d2362cf9de209026651a#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:17:02Z",
        "updatedAt" : "2020-03-06T08:17:03Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 163,
    "diffHunk" : "@@ -1,1 +2076,2080 @@          \"'SELECT x FROM t ORDER BY y LIMIT m', if m is under this threshold, do a top-K sort\" +\n          \" in memory, otherwise do a global sort which spills to disk if necessary.\")\n      .version(\"2.4.0\")\n      .intConf\n      .createWithDefault(ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH)"
  },
  {
    "id" : "a0d92eeb-36a4-4422-8b8b-5e20d9899764",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370145649",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "49ad60b9-8402-4756-a572-bb40382d45e4",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-24244 and SPARK-24368, commit ID: 64fad0b519cf35b8c0a0dec18dd3df9488a5ed25#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:17:26Z",
        "updatedAt" : "2020-03-06T08:17:27Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 172,
    "diffHunk" : "@@ -1,1 +2092,2096 @@    .doc(\"If it is set to true, column names of the requested schema are passed to CSV parser. \" +\n      \"Other column values can be ignored during parsing even if they are malformed.\")\n    .version(\"2.4.0\")\n    .booleanConf\n    .createWithDefault(true)"
  },
  {
    "id" : "1c52f12c-2fa8-4595-a784-919cbfa79484",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370145894",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "810680e1-7fcd-45f8-a15e-63bccbec24ac",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-24215, commit ID: 6a0b77a55d53e74ac0a0892556c3a7a933474948#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:17:57Z",
        "updatedAt" : "2020-03-06T08:17:57Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 181,
    "diffHunk" : "@@ -1,1 +2103,2107 @@      \"REPL, the returned outputs are formatted like dataframe.show(). In SparkR, the returned \" +\n      \"outputs are showed similar to R data.frame would.\")\n    .version(\"2.4.0\")\n    .booleanConf\n    .createWithDefault(false)"
  },
  {
    "id" : "32588e3e-562b-4ff0-a9d3-67b2f4a7f8f2",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370146067",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cce1731c-0dbe-4663-8a15-79cc9cdbd0fe",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-24215, commit ID: 6a0b77a55d53e74ac0a0892556c3a7a933474948#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:18:20Z",
        "updatedAt" : "2020-03-06T08:18:21Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 190,
    "diffHunk" : "@@ -1,1 +2112,2116 @@      \"config is from 0 to (Int.MaxValue - 1), so the invalid config like negative and \" +\n      \"greater than (Int.MaxValue - 1) will be normalized to 0 and (Int.MaxValue - 1).\")\n    .version(\"2.4.0\")\n    .intConf\n    .createWithDefault(20)"
  },
  {
    "id" : "fb6d6f1e-4527-479c-a5db-ae6922419f91",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370146265",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fc47a8d7-58b2-415b-9ad9-69918295aafe",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-24215, commit ID: 6a0b77a55d53e74ac0a0892556c3a7a933474948#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:18:44Z",
        "updatedAt" : "2020-03-06T08:18:45Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 198,
    "diffHunk" : "@@ -1,1 +2119,2123 @@    .doc(\"The max number of characters for each cell that is returned by eager evaluation. \" +\n      s\"This only takes effect when ${REPL_EAGER_EVAL_ENABLED.key} is set to true.\")\n    .version(\"2.4.0\")\n    .intConf\n    .createWithDefault(20)"
  },
  {
    "id" : "121a3e5b-275a-4685-8aca-0f3e1779f1c6",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370146483",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c46ea0a6-c96b-491c-a398-10b7f0496717",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-24978, commit ID: 6193a202aab0271b4532ee4b740318290f2c44a1#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:19:15Z",
        "updatedAt" : "2020-03-06T08:19:15Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 207,
    "diffHunk" : "@@ -1,1 +2130,2134 @@        \"but the actual numBuckets is determined by loadFactor \" +\n        \"(e.g: default bit value 16 , the actual numBuckets is ((1 << 16) / 0.5).\")\n      .version(\"2.4.0\")\n      .intConf\n      .checkValue(bit => bit >= 10 && bit <= 30, \"The bit value must be in [10, 30].\")"
  },
  {
    "id" : "1a54cebd-2cb6-4c6b-a8ae-eba8102821d7",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370148725",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8db158ea-c7a0-4a8d-bd49-c04da4327eda",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-24881, commit ID: 0a0f68bae6c0a1bf30184b1e9ac6bf3805bd7511#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:23:56Z",
        "updatedAt" : "2020-03-06T08:23:57Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 216,
    "diffHunk" : "@@ -1,1 +2138,2142 @@    .doc(\"Compression codec used in writing of AVRO files. Supported codecs: \" +\n      \"uncompressed, deflate, snappy, bzip2 and xz. Default codec is snappy.\")\n    .version(\"2.4.0\")\n    .stringConf\n    .checkValues(Set(\"uncompressed\", \"deflate\", \"snappy\", \"bzip2\", \"xz\"))"
  },
  {
    "id" : "3d27cd61-5835-418e-beb5-32e70bc6452c",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370148946",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5267e9fb-07d9-4dfa-8610-9451764709f3",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-24881, commit ID: 0a0f68bae6c0a1bf30184b1e9ac6bf3805bd7511#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:24:28Z",
        "updatedAt" : "2020-03-06T08:24:28Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 225,
    "diffHunk" : "@@ -1,1 +2147,2151 @@      \"Valid value must be in the range of from 1 to 9 inclusive or -1. \" +\n      \"The default value is -1 which corresponds to 6 level in the current implementation.\")\n    .version(\"2.4.0\")\n    .intConf\n    .checkValues((1 to 9).toSet + Deflater.DEFAULT_COMPRESSION)"
  },
  {
    "id" : "439917c8-d4ff-44bd-b504-4bf60e3eacb5",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370149136",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9a42b9d7-1671-45b0-bf82-3399cff9084e",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-24605, commit ID: d08f53dc61f662f5291f71bcbe1a7b9f531a34d2#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:24:53Z",
        "updatedAt" : "2020-03-06T08:24:54Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 234,
    "diffHunk" : "@@ -1,1 +2156,2160 @@    .doc(\"If it is set to true, size of null returns -1. This behavior was inherited from Hive. \" +\n      \"The size function returns null for null input if the flag is disabled.\")\n    .version(\"2.4.0\")\n    .booleanConf\n    .createWithDefault(false)"
  },
  {
    "id" : "a97fc580-4bdf-4669-ae89-04abbb318ea1",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370149372",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "aba20790-0938-4130-9a74-c0000697cac7",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-25129, commit ID: ac0174e55af2e935d41545721e9f430c942b3a0c#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:25:23Z",
        "updatedAt" : "2020-03-06T08:25:23Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 243,
    "diffHunk" : "@@ -1,1 +2165,2169 @@      .doc(\"If it is set to true, the data source provider com.databricks.spark.avro is mapped \" +\n        \"to the built-in but external Avro data source module for backward compatibility.\")\n      .version(\"2.4.0\")\n      .booleanConf\n      .createWithDefault(true)"
  },
  {
    "id" : "f30baa6b-ad17-4eee-958e-eb2b7e418477",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370149575",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c990c425-b901-4202-8819-7ef6df5df6a0",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-24966, commit ID: 73dd6cf9b558f9d752e1f3c13584344257ad7863#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:25:49Z",
        "updatedAt" : "2020-03-06T08:25:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 252,
    "diffHunk" : "@@ -1,1 +2176,2180 @@        \"to false and order of evaluation is not specified by parentheses, INTERSECT operations \" +\n        \"are performed before any UNION, EXCEPT and MINUS operations.\")\n      .version(\"2.4.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "e3baa24b-f30c-47e5-9717-87f3dc2ee2ae",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370149790",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "134c4d6f-3b95-41dd-952e-414117cd5612",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-29956, commit ID: 87ebfaf003fcd05a7f6d23b3ecd4661409ce5f2f#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:26:13Z",
        "updatedAt" : "2020-03-06T08:26:14Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 261,
    "diffHunk" : "@@ -1,1 +2185,2189 @@      .doc(\"When set to true, a literal with an exponent (e.g. 1E-30) would be parsed \" +\n        \"as Decimal rather than Double.\")\n      .version(\"3.0.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "9f72646e-56cb-485c-83ae-2d98e743c5ee",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370150073",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8efe0486-d789-40a6-bfce-e8140ea92a16",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-30812, commit ID: b76bc0b1b8b2abd00a84f805af90ca4c5925faaa#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:26:48Z",
        "updatedAt" : "2020-03-06T08:26:48Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 270,
    "diffHunk" : "@@ -1,1 +2195,2199 @@        \"the type of number 1E10BD under legacy mode is DecimalType(2, -9), but is \" +\n        \"Decimal(11, 0) in non legacy mode.\")\n      .version(\"3.0.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "3fb28c43-de3c-4a09-b11d-d6ee1331a47a",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370150328",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e38e25bc-71bc-4743-9374-8fe3573ca9ba",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-30098, commit ID: 58be82ad4b98fc17e821e916e69e77a6aa36209d#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:27:14Z",
        "updatedAt" : "2020-03-06T08:27:15Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 279,
    "diffHunk" : "@@ -1,1 +2204,2208 @@      .doc(\"When set to true, CREATE TABLE syntax without a provider will use hive \" +\n        s\"instead of the value of ${DEFAULT_DATA_SOURCE_NAME.key}.\")\n      .version(\"3.0.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "520f2be7-1185-4480-b912-fbf4ad7af3e7",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370150589",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ece30089-2e1a-4eea-94e8-687cc5db01e7",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-25457, commit ID: 47d6e80a2e64823fabb596503fb6a6cc6f51f713#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:27:41Z",
        "updatedAt" : "2020-03-06T08:27:41Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 288,
    "diffHunk" : "@@ -1,1 +2212,2216 @@    .doc(\"If it is set to true, the div operator returns always a bigint. This behavior was \" +\n      \"inherited from Hive. Otherwise, the return type is the data type of the operands.\")\n    .version(\"3.0.0\")\n    .internal()\n    .booleanConf"
  },
  {
    "id" : "8fc4aa0e-96b6-47d9-a4f0-083ff70cd174",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370150858",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "195247dd-4803-4ff0-b545-72359ba614c2",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-28595, commit ID: 469423f33887a966aaa33eb75f5e7974a0a97beb#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:28:12Z",
        "updatedAt" : "2020-03-06T08:28:13Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 297,
    "diffHunk" : "@@ -1,1 +2222,2226 @@      .doc(\"When true, the bucketed table scan will list files during planning to figure out the \" +\n        \"output ordering, which is expensive and may make the planning quite slow.\")\n      .version(\"3.0.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "e5b0bc63-8b36-4ee5-b29e-2eb45b5545bc",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370151071",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0c3a3747-42d7-4d93-b214-4bb73b1e813b",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-25708, commit ID: 3dba5d41f1a66ae5eb08404d103284110c45a351#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:28:38Z",
        "updatedAt" : "2020-03-06T08:28:39Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 306,
    "diffHunk" : "@@ -1,1 +2231,2235 @@      .doc(\"If it is set to true, the parser will treat HAVING without GROUP BY as a normal \" +\n        \"WHERE, which does not follow SQL standard.\")\n      .version(\"2.4.1\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "fd914168-da6b-41d3-84b6-fd52227a7f74",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370151269",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b5fab42b-6bc5-4be8-bfe5-37cde8630ae6",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-25040, commit ID: d3de7568f32e298442f07b0a28b2c906de72c797#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:29:03Z",
        "updatedAt" : "2020-03-06T08:29:04Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 315,
    "diffHunk" : "@@ -1,1 +2240,2244 @@      .doc(\"When set to true, the parser of JSON data source treats empty strings as null for \" +\n        \"some data types such as `IntegerType`.\")\n      .version(\"3.0.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "a7709072-8f06-4564-a044-6dd56dcaf8c2",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370151468",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8cfc2ca6-06fb-4564-9364-626572fcd43f",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-30790, commit ID: 8ab6ae3ede96adb093347470a5cbbf17fe8c04e9#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:29:26Z",
        "updatedAt" : "2020-03-06T08:29:26Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 324,
    "diffHunk" : "@@ -1,1 +2250,2254 @@        \"type if the `array`/`map` function is called without any parameters. Otherwise, Spark \" +\n        \"returns an empty collection with `NullType` as element type.\")\n      .version(\"3.0.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "3f213c21-4605-4351-805f-f2f1bb415b46",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370151746",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6c2cc566-872b-4cd4-a08b-350a8c703b97",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-26580, commit ID: bc30a07ce262840c99a752db4fbd3a423f652017#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:29:54Z",
        "updatedAt" : "2020-03-06T08:29:54Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 333,
    "diffHunk" : "@@ -1,1 +2259,2263 @@      .doc(\"When set to true, user is allowed to use org.apache.spark.sql.functions.\" +\n        \"udf(f: AnyRef, dataType: DataType). Otherwise, exception will be throw.\")\n      .version(\"3.0.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "1c5b6383-dd77-438e-bfdb-508b078f7739",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370151993",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bcb67142-1654-47d1-976d-4fd88f1b9b4b",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-30312, commit ID: 830a4ec59b86253f18eb7dfd6ed0bbe0d7920e5b#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:30:22Z",
        "updatedAt" : "2020-03-06T08:30:23Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 342,
    "diffHunk" : "@@ -1,1 +2268,2272 @@      .doc(\"When set to true, TRUNCATE TABLE command will not try to set back original \" +\n        \"permission and ACLs when re-creating the table/partition paths.\")\n      .version(\"2.4.6\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "979e50ae-7dfc-4cfb-b3d6-66ecc82384fa",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370152209",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "29984611-1f42-476c-ad13-53155525ef4f",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-26085, commit ID: ab2eafb3cdc7631452650c6cac03a92629255347#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:30:48Z",
        "updatedAt" : "2020-03-06T08:30:48Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 351,
    "diffHunk" : "@@ -1,1 +2278,2282 @@        \"for non-struct key type, will be named as `value`, following the behavior of Spark \" +\n        \"version 2.4 and earlier.\")\n      .version(\"3.0.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "cd74bcf3-80e5-4a4f-8d83-eb18a2c0a3ac",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370152416",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9e475cae-60a8-4518-8b7b-3e02c70c9e01",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-26066, commit ID: 81550b38e43fb20f89f529d2127575c71a54a538#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:31:09Z",
        "updatedAt" : "2020-03-06T08:31:09Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 360,
    "diffHunk" : "@@ -1,1 +2286,2290 @@      \"in debug output. Any elements beyond the limit will be dropped and replaced by a\" +\n      \"\"\" \"... N more fields\" placeholder.\"\"\")\n    .version(\"3.0.0\")\n    .intConf\n    .createWithDefault(25)"
  },
  {
    "id" : "2e7027d9-fcd4-455d-bdea-525de8e7813b",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370152623",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "84d6010a-3eb7-468e-97a1-b00f0ba67023",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-26103, commit ID: 812ad5546148d2194ab0e4230ee85b8f6a5be2fb#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:31:34Z",
        "updatedAt" : "2020-03-06T08:31:34Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 369,
    "diffHunk" : "@@ -1,1 +2295,2299 @@      \"plan.  Set this to a lower value such as 8k if plan strings are taking up too much \" +\n      \"memory or are causing OutOfMemory errors in the driver or UI processes.\")\n    .version(\"3.0.0\")\n    .bytesConf(ByteUnit.BYTE)\n    .checkValue(i => i >= 0 && i <= ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH, \"Invalid \" +"
  },
  {
    "id" : "d2c0de91-e328-42c6-993e-ad5008e7d247",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370152969",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2bbecaf7-f141-4b95-b4c0-b7f31f757932",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-26060, commit ID: 1ab3d3e474ce2e36d58aea8ad09fb61f0c73e5c5#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:32:17Z",
        "updatedAt" : "2020-03-06T08:32:17Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 378,
    "diffHunk" : "@@ -1,1 +2307,2311 @@      .doc(\"If it is set to true, SET command will fail when the key is registered as \" +\n        \"a SparkConf entry.\")\n      .version(\"3.0.0\")\n      .booleanConf\n      .createWithDefault(true)"
  },
  {
    "id" : "bdac5c6e-3cdc-432f-820c-0fbedacef3d4",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370153237",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e88a6602-cf3e-4d9d-8c9c-514c0c34c288",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-27008, commit ID: 52671d631d2a64ed1cfa0c6e01168908faf92df8#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:32:46Z",
        "updatedAt" : "2020-03-06T08:32:46Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 387,
    "diffHunk" : "@@ -1,1 +2316,2320 @@      \"Catalyst's TimestampType and DateType. If it is set to false, java.sql.Timestamp \" +\n      \"and java.sql.Date are used for the same purpose.\")\n    .version(\"3.0.0\")\n    .booleanConf\n    .createWithDefault(false)"
  },
  {
    "id" : "a1c24c7c-5c90-45e2-b9bd-b7998c9f89b7",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370153438",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2d28952b-1ac2-497d-9643-369a1ba95c22",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-27588, commit ID: 618d6bff71073c8c93501ab7392c3cc579730f0b#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:33:10Z",
        "updatedAt" : "2020-03-06T08:33:10Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 396,
    "diffHunk" : "@@ -1,1 +2324,2328 @@      \"Spark will fail fast and not attempt to read the file if its length exceeds this value. \" +\n      \"The theoretical max is Int.MaxValue, though VMs might implement a smaller max.\")\n    .version(\"3.0.0\")\n    .internal()\n    .intConf"
  },
  {
    "id" : "2bf494d9-8119-4e76-9efb-f3f06a4660f5",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370171927",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bda7ce1d-6a53-4fb9-8984-b49a54014b3b",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-27638, commit ID: 83d289eef492de8c7f3e5145f9bd75431608b500#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T09:06:52Z",
        "updatedAt" : "2020-03-06T09:06:53Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 405,
    "diffHunk" : "@@ -1,1 +2334,2338 @@      .doc(\"If it is set to true, date/timestamp will cast to string in binary comparisons \" +\n        \"with String\")\n      .version(\"3.0.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "8679a37e-bd12-445f-b55a-d97a78c4a415",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370172150",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8dbfa246-d276-4461-b7b8-3830bb1476df",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-29753, commit ID: 942753a44beeae5f0142ceefa307e90cbc1234c5#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T09:07:16Z",
        "updatedAt" : "2020-03-06T09:07:17Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 413,
    "diffHunk" : "@@ -1,1 +2341,2345 @@    .doc(\"Name of the default catalog. This will be the current catalog if users have not \" +\n      \"explicitly set the current catalog yet.\")\n    .version(\"3.0.0\")\n    .stringConf\n    .createWithDefault(SESSION_CATALOG_NAME)"
  },
  {
    "id" : "2eadac5a-c508-4155-b20d-ed2c44198ff2",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370172406",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f5589e5d-796a-436a-b914-27388d45290a",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-29412, commit ID: 9407fba0375675d6ee6461253f3b8230e8d67509#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T09:07:41Z",
        "updatedAt" : "2020-03-06T09:07:41Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 422,
    "diffHunk" : "@@ -1,1 +2353,2357 @@        s\"metadata. To delegate operations to the $SESSION_CATALOG_NAME, implementations can \" +\n        \"extend 'CatalogExtension'.\")\n      .version(\"3.0.0\")\n      .stringConf\n      .createOptional"
  },
  {
    "id" : "c190316b-d5df-4fc2-bcd1-69116ed11cae",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370258749",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "72ed9b6e-1b70-45d7-b566-00e349c60494",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-30812, commit ID: b76bc0b1b8b2abd00a84f805af90ca4c5925faaa#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T11:33:04Z",
        "updatedAt" : "2020-03-06T11:33:05Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 431,
    "diffHunk" : "@@ -1,1 +2375,2379 @@    .internal()\n    .doc(\"When true, the upcast will be loose and allows string to atomic types.\")\n    .version(\"3.0.0\")\n    .booleanConf\n    .createWithDefault(false)"
  },
  {
    "id" : "162f840e-84c9-4b3b-adbd-8ce1cd78f5d1",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370258953",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "92b4d594-de46-483f-b474-5419c22e2871",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-30829, commit ID: 00943be81afbca6be13e1e72b24536cd98a788d6#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T11:33:29Z",
        "updatedAt" : "2020-03-06T11:33:30Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 440,
    "diffHunk" : "@@ -1,1 +2389,2393 @@      \"AnalysisException is thrown while name conflict is detected in nested CTE. This config \" +\n      \"will be removed in future versions and CORRECTED will be the only behavior.\")\n    .version(\"3.0.0\")\n    .stringConf\n    .transform(_.toUpperCase(Locale.ROOT))"
  },
  {
    "id" : "761dbe74-1d03-4275-b3d9-2ca350aea1d3",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370259222",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c4ae3778-7e6a-457e-8215-2104ccef4809",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-30668, commit ID: 7db0af578585ecaeee9fd23f8189292289b52a97#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T11:33:59Z",
        "updatedAt" : "2020-03-06T11:33:59Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 448,
    "diffHunk" : "@@ -1,1 +2402,2406 @@      \"The default value is EXCEPTION, RuntimeException is thrown when we will get different \" +\n      \"results.\")\n    .version(\"3.1.0\")\n    .stringConf\n    .transform(_.toUpperCase(Locale.ROOT))"
  },
  {
    "id" : "615ffd40-8237-4049-b7bf-91a96f590dc0",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370259440",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6e5e65e0-ba07-468c-866f-1d4288f98a0a",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-30812, commit ID: b76bc0b1b8b2abd00a84f805af90ca4c5925faaa#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T11:34:23Z",
        "updatedAt" : "2020-03-06T11:34:23Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 457,
    "diffHunk" : "@@ -1,1 +2412,2416 @@      .internal()\n      .doc(\"When true, the ArrayExists will follow the three-valued boolean logic.\")\n      .version(\"3.0.0\")\n      .booleanConf\n      .createWithDefault(true)"
  },
  {
    "id" : "9afb064f-4baa-493b-85f7-2beaee1f838d",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370259681",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "afbce64d-c277-479f-9cb7-da06bc3eef99",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-29175, commit ID: 3d7359ad4202067b26a199657b6a3e1f38be0e4d#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T11:34:50Z",
        "updatedAt" : "2020-03-06T11:34:51Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 466,
    "diffHunk" : "@@ -1,1 +2421,2425 @@        \"repositories. This is only used for downloading Hive jars in IsolatedClientLoader \" +\n        \"if the default Maven Central repo is unreachable.\")\n      .version(\"3.0.0\")\n      .stringConf\n      .createWithDefault("
  },
  {
    "id" : "a3078e1b-9278-42b5-9443-e932dd1efad8",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370259881",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e15003fa-5282-4d56-abb8-1a654e927d66",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-29864 and SPARK-29920, commit ID: e933539cdd557297daf97ff5e532a3f098896979#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T11:35:15Z",
        "updatedAt" : "2020-03-06T11:35:16Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 475,
    "diffHunk" : "@@ -1,1 +2434,2438 @@        \"`ParseException` is thrown if the input does not match to the pattern \" +\n        \"defined by `from` and `to`.\")\n      .version(\"3.0.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "11c5bf7d-6d20-4190-93ab-da9b47541730",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370260090",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "76dea0db-c173-490a-9180-be5fbc465062",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-30812, commit ID: b76bc0b1b8b2abd00a84f805af90ca4c5925faaa#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T11:35:39Z",
        "updatedAt" : "2020-03-06T11:35:40Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 484,
    "diffHunk" : "@@ -1,1 +2444,2448 @@        \"create/alter syntaxes. But please be aware that the reserved properties will be \" +\n        \"silently removed.\")\n      .version(\"3.0.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "e6f3454e-9f8b-401b-828f-3b6286cd58b9",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370260297",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f18f5304-be4a-499b-972c-3b3045f6aea1",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-30234, commit ID: 8a8d1fbb10af6da481f26831cd519ef46ccbce6c#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T11:36:03Z",
        "updatedAt" : "2020-03-06T11:36:04Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 493,
    "diffHunk" : "@@ -1,1 +2453,2457 @@      .doc(\"When true, only a single file can be added using ADD FILE. If false, then users \" +\n        \"can add directory by passing directory path to ADD FILE.\")\n      .version(\"3.0.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "1b34a23c-004e-4b4e-b161-d58939d4f7cf",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370260473",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "246fa3a2-b51c-4561-acf1-6557bea58f40",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-28152, commit ID: 69de7f31c37a7e0298e66cc814afc1b0aa948bbb#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T11:36:26Z",
        "updatedAt" : "2020-03-06T11:36:27Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 502,
    "diffHunk" : "@@ -1,1 +2461,2465 @@      .internal()\n      .doc(\"When true, use legacy MySqlServer SMALLINT and REAL type mapping.\")\n      .version(\"2.4.5\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "fa5e494d-43dd-4951-88ed-0fd77b0db660",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370260639",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "29a893b7-595e-404a-91a5-7c0c3022a998",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-30323, commit ID: 4e50f0291f032b4a5c0b46ed01fdef14e4cbb050#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T11:36:46Z",
        "updatedAt" : "2020-03-06T11:36:57Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 509,
    "diffHunk" : "@@ -1,1 +2467,2471 @@  val CSV_FILTER_PUSHDOWN_ENABLED = buildConf(\"spark.sql.csv.filterPushdown.enabled\")\n    .doc(\"When true, enable filter pushdown to CSV datasource.\")\n    .version(\"3.0.0\")\n    .booleanConf\n    .createWithDefault(true)"
  },
  {
    "id" : "37af3530-f909-4c14-b8ad-685d2fb5a244",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370261056",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a8928b7b-7015-46ac-b52c-da546a2f9bd9",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-29938, commit ID: 5ccbb38a71890b114c707279e7395d1f6284ebfd#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T11:37:35Z",
        "updatedAt" : "2020-03-06T11:37:35Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 518,
    "diffHunk" : "@@ -1,1 +2477,2481 @@        \"`AlterTableAddPartitionCommand` to add partitions into table. The smaller \" +\n        \"batch size is, the less memory is required for the real handler, e.g. Hive Metastore.\")\n      .version(\"3.0.0\")\n      .intConf\n      .checkValue(_ > 0, \"The value of spark.sql.addPartitionInBatch.size must be positive\")"
  },
  {
    "id" : "1e4f3a67-e5e8-40e9-a5c0-91858f95ab49",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370261196",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "aa1d6b66-4e9f-4560-a535-f9f581033b20",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-30668, commit ID: 92f57237871400ab9d499e1174af22a867c01988#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T11:37:55Z",
        "updatedAt" : "2020-03-06T11:37:55Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 527,
    "diffHunk" : "@@ -1,1 +2487,2491 @@      \"dates/timestamps in a locale-sensitive manner. When set to false, classes from \" +\n      \"java.time.* packages are used for the same purpose.\")\n    .version(\"3.0.0\")\n    .booleanConf\n    .createWithDefault(false)"
  },
  {
    "id" : "eb97805e-85db-44de-bb2c-7a985853c592",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370261367",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7d043379-3209-424e-90f6-45e66956f67c",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-25829, commit ID: 33329caa81827a245b84158b13234b88a4746e56#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T11:38:16Z",
        "updatedAt" : "2020-03-06T11:38:16Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 535,
    "diffHunk" : "@@ -1,1 +2494,2498 @@    .doc(\"When set to true, hash expressions can be applied on elements of MapType. Otherwise, \" +\n      \"an analysis exception will be thrown.\")\n    .version(\"3.0.0\")\n    .booleanConf\n    .createWithDefault(false)"
  },
  {
    "id" : "281a06fe-25dc-4d57-890d-db725f56d533",
    "prId" : 27798,
    "prUrl" : "https://github.com/apache/spark/pull/27798#pullrequestreview-369250498",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d9987e45-e3c6-4a7a-8ffb-51c3e864abd8",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "It's still based on global log level?",
        "createdAt" : "2020-03-05T02:01:20Z",
        "updatedAt" : "2020-03-05T19:36:19Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "3151b019ff1025a41334eca78464d4baa5eb8c9c",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +406,410 @@    .createWithDefault(false)\n\n  val ADAPTIVE_EXECUTION_LOG_LEVEL = buildConf(\"spark.sql.adaptive.logLevel\")\n    .internal()\n    .doc(\"Configures the log level for adaptive execution logging of plan changes. The value \" +"
  },
  {
    "id" : "850368c0-bdd0-4e7c-840f-930b5822c1bd",
    "prId" : 27798,
    "prUrl" : "https://github.com/apache/spark/pull/27798#pullrequestreview-370082340",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d45f71ae-468b-44f0-a599-7f3d432213e5",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Hi, @cloud-fan . This seems to break `branch-3.0`.\r\n```\r\n[error] /home/jenkins/workspace/spark-branch-3.0-test-sbt-hadoop-2.7-hive-2.3/sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala:385: value version is not a member of org.apache.spark.internal.config.ConfigBuilder\r\n[error] possible cause: maybe a semicolon is missing before `value version'?\r\n[error]     .version(\"3.0.0\")\r\n[error]      ^\r\n[error] one error found\r\n```",
        "createdAt" : "2020-03-06T04:57:09Z",
        "updatedAt" : "2020-03-06T04:57:09Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "3151b019ff1025a41334eca78464d4baa5eb8c9c",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +410,414 @@    .doc(\"Configures the log level for adaptive execution logging of plan changes. The value \" +\n      \"can be 'trace', 'debug', 'info', 'warn', or 'error'. The default log level is 'debug'.\")\n    .version(\"3.0.0\")\n    .stringConf\n    .transform(_.toUpperCase(Locale.ROOT))"
  },
  {
    "id" : "cef6d78f-237c-4d39-93bb-3254aab57a16",
    "prId" : 27793,
    "prUrl" : "https://github.com/apache/spark/pull/27793#pullrequestreview-369398399",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "76acf138-e537-4297-9071-b5b3d39b7d61",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "Also tell users what is the new conf that replaces it? ",
        "createdAt" : "2020-03-05T08:14:54Z",
        "updatedAt" : "2020-03-05T10:45:48Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "c2ac48ba-c8a6-4d9c-9486-94faacfad66a",
        "parentId" : "76acf138-e537-4297-9071-b5b3d39b7d61",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "The doc here is not user-facing. End users will see a message to suggest the new config by https://github.com/apache/spark/pull/27793/files#diff-9a6b543db706f1a90f790783d6930a13R2494",
        "createdAt" : "2020-03-05T09:10:58Z",
        "updatedAt" : "2020-03-05T10:45:48Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "8d18db825d9fb8843c641f1c25d54bef962c7fa7",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +382,386 @@    buildConf(\"spark.sql.adaptive.shuffle.targetPostShuffleInputSize\")\n      .internal()\n      .doc(\"(Deprecated since Spark 3.0)\")\n      .version(\"1.6.0\")\n      .bytesConf(ByteUnit.BYTE)"
  },
  {
    "id" : "fa53efae-5d8c-44cc-99f3-06878d171e60",
    "prId" : 27775,
    "prUrl" : "https://github.com/apache/spark/pull/27775#pullrequestreview-368460224",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0bd4e64d-afec-477b-8b43-9987fe50752d",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "nit: conf -> configuration",
        "createdAt" : "2020-03-04T02:12:18Z",
        "updatedAt" : "2020-03-04T02:12:19Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "1317477cd2b96633b0343fac0bce68a127d45579",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +2456,2460 @@  val LEGACY_ALLOW_AMBIGUOUS_GROUP_BY_ALIAS =\n    buildConf(\"spark.sql.legacy.allowAmbiguousGroupByAlias\")\n      .doc(s\"When ${GROUP_BY_ALIASES.key} is enabled and this conf is true, Spark will resolve \" +\n        \"the GROUP BY column using child's output, even though there is an ambiguous alias in \" +\n        \"the SELECT clause. Id false, Spark fails the query.\")"
  },
  {
    "id" : "e57439a9-5207-40dc-ba89-4055ca6b5d72",
    "prId" : 27775,
    "prUrl" : "https://github.com/apache/spark/pull/27775#pullrequestreview-368460304",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ce971ae4-4e79-4115-9440-2c1540d3f6fa",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "nit Id -> if",
        "createdAt" : "2020-03-04T02:12:39Z",
        "updatedAt" : "2020-03-04T02:12:40Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "1317477cd2b96633b0343fac0bce68a127d45579",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +2458,2462 @@      .doc(s\"When ${GROUP_BY_ALIASES.key} is enabled and this conf is true, Spark will resolve \" +\n        \"the GROUP BY column using child's output, even though there is an ambiguous alias in \" +\n        \"the SELECT clause. Id false, Spark fails the query.\")\n      .version(\"3.0.0\")\n      .booleanConf"
  },
  {
    "id" : "2191d361-1a7a-4949-a3d1-4c44b4a1f7b9",
    "prId" : 27770,
    "prUrl" : "https://github.com/apache/spark/pull/27770#pullrequestreview-367887814",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bb820e7c-09d3-419e-a206-3019f1a23ad6",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-19944, commit ID: 0ee38a39e43dd7ad9d50457e446ae36f64621a1b#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-03T11:05:44Z",
        "updatedAt" : "2020-03-03T11:05:45Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "7a6be9fce2973ff7df6631c4dbc61ec9945e4005",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1361,1365 @@    .internal()\n    .doc(\"Decides if we use ObjectHashAggregateExec\")\n    .version(\"2.2.0\")\n    .booleanConf\n    .createWithDefault(true)"
  },
  {
    "id" : "4e4201fe-21d8-4773-86b4-1883ee963ac6",
    "prId" : 27770,
    "prUrl" : "https://github.com/apache/spark/pull/27770#pullrequestreview-367888103",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "625c739f-1ebb-4d7c-9c7b-e8aec4564788",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-29444, commit ID: 78b0cbe265c4e8cc3d4d8bf5d734f2998c04d376#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-03T11:06:13Z",
        "updatedAt" : "2020-03-03T11:06:13Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "7a6be9fce2973ff7df6631c4dbc61ec9945e4005",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +1370,1374 @@        \"JSON functions such as to_json. \" +\n        \"If false, it generates null for null fields in JSON objects.\")\n      .version(\"3.0.0\")\n      .booleanConf\n      .createWithDefault(true)"
  },
  {
    "id" : "57a6c6cc-6dc9-4d6a-9550-d1dda5c1c14c",
    "prId" : 27770,
    "prUrl" : "https://github.com/apache/spark/pull/27770#pullrequestreview-367888355",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2c702373-cd55-4272-aa5a-2a3a68da734a",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-14678, commit ID: 7bc948557bb6169cbeec335f8400af09375a62d3#diff-32bb9518401c0948c5ea19377b5069ab",
        "createdAt" : "2020-03-03T11:06:38Z",
        "updatedAt" : "2020-03-03T11:06:38Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "7a6be9fce2973ff7df6631c4dbc61ec9945e4005",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +1377,1381 @@    .internal()\n    .doc(\"Whether to delete the expired log files in file stream sink.\")\n    .version(\"2.0.0\")\n    .booleanConf\n    .createWithDefault(true)"
  },
  {
    "id" : "fb28b68a-9294-4c17-8e62-15855ca15f5a",
    "prId" : 27770,
    "prUrl" : "https://github.com/apache/spark/pull/27770#pullrequestreview-367888481",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fdf1fd86-b548-4837-8f8d-859eeb9b2890",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-14678, commit ID: 7bc948557bb6169cbeec335f8400af09375a62d3#diff-32bb9518401c0948c5ea19377b5069ab",
        "createdAt" : "2020-03-03T11:06:48Z",
        "updatedAt" : "2020-03-03T11:06:48Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "7a6be9fce2973ff7df6631c4dbc61ec9945e4005",
    "line" : 31,
    "diffHunk" : "@@ -1,1 +1386,1390 @@      .doc(\"Number of log files after which all the previous files \" +\n        \"are compacted into the next log file.\")\n      .version(\"2.0.0\")\n      .intConf\n      .createWithDefault(10)"
  },
  {
    "id" : "3cf37baf-cc11-4c24-a4ae-c5901277ae3e",
    "prId" : 27770,
    "prUrl" : "https://github.com/apache/spark/pull/27770#pullrequestreview-367888579",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7547ce75-a73f-4425-949a-e09f9b6691a3",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-14678, commit ID: 7bc948557bb6169cbeec335f8400af09375a62d3#diff-32bb9518401c0948c5ea19377b5069ab",
        "createdAt" : "2020-03-03T11:06:56Z",
        "updatedAt" : "2020-03-03T11:06:56Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "7a6be9fce2973ff7df6631c4dbc61ec9945e4005",
    "line" : 40,
    "diffHunk" : "@@ -1,1 +1394,1398 @@      .internal()\n      .doc(\"How long that a file is guaranteed to be visible for all readers.\")\n      .version(\"2.0.0\")\n      .timeConf(TimeUnit.MILLISECONDS)\n      .createWithDefault(TimeUnit.MINUTES.toMillis(10)) // 10 minutes"
  },
  {
    "id" : "6c5d94ab-ef56-4c67-a0f7-8985b64354aa",
    "prId" : 27770,
    "prUrl" : "https://github.com/apache/spark/pull/27770#pullrequestreview-367888862",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a4ca2f1c-ca4d-489a-be7b-823ce2fddf65",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-15698, commit ID: 8d8e2332ca12067817de45a8d3812928150975d0#diff-32bb9518401c0948c5ea19377b5069ab",
        "createdAt" : "2020-03-03T11:07:21Z",
        "updatedAt" : "2020-03-03T11:07:21Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "7a6be9fce2973ff7df6631c4dbc61ec9945e4005",
    "line" : 48,
    "diffHunk" : "@@ -1,1 +1401,1405 @@    .internal()\n    .doc(\"Whether to delete the expired log files in file stream source.\")\n    .version(\"2.0.1\")\n    .booleanConf\n    .createWithDefault(true)"
  },
  {
    "id" : "173540c1-b6f6-4caf-bb77-fa7abe22a7d0",
    "prId" : 27770,
    "prUrl" : "https://github.com/apache/spark/pull/27770#pullrequestreview-367888974",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bb4ec43d-476f-4a17-9f98-15ee8bb37c37",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-15698, commit ID: 8d8e2332ca12067817de45a8d3812928150975d0#diff-32bb9518401c0948c5ea19377b5069ab",
        "createdAt" : "2020-03-03T11:07:32Z",
        "updatedAt" : "2020-03-03T11:07:32Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "7a6be9fce2973ff7df6631c4dbc61ec9945e4005",
    "line" : 57,
    "diffHunk" : "@@ -1,1 +1410,1414 @@      .doc(\"Number of log files after which all the previous files \" +\n        \"are compacted into the next log file.\")\n      .version(\"2.0.1\")\n      .intConf\n      .createWithDefault(10)"
  },
  {
    "id" : "10259377-dcb9-4633-9c6d-0e9233699c5e",
    "prId" : 27770,
    "prUrl" : "https://github.com/apache/spark/pull/27770#pullrequestreview-367889054",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "10c2f812-4ea7-49d3-82fe-e9c49d53ed32",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-15698, commit ID: 8d8e2332ca12067817de45a8d3812928150975d0#diff-32bb9518401c0948c5ea19377b5069ab",
        "createdAt" : "2020-03-03T11:07:40Z",
        "updatedAt" : "2020-03-03T11:07:40Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "7a6be9fce2973ff7df6631c4dbc61ec9945e4005",
    "line" : 66,
    "diffHunk" : "@@ -1,1 +1418,1422 @@      .internal()\n      .doc(\"How long in milliseconds a file is guaranteed to be visible for all readers.\")\n      .version(\"2.0.1\")\n      .timeConf(TimeUnit.MILLISECONDS)\n      .createWithDefault(TimeUnit.MINUTES.toMillis(10)) // 10 minutes"
  },
  {
    "id" : "0f729118-0356-46fe-bb6e-56585fca9a4f",
    "prId" : 27770,
    "prUrl" : "https://github.com/apache/spark/pull/27770#pullrequestreview-367889356",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "aa2cab0b-d08c-470e-99d3-acfbd2429e60",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-28651, commit ID: 5bb69945e4aaf519cd10a5c5083332f618039af0#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-03T11:08:11Z",
        "updatedAt" : "2020-03-03T11:08:11Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "7a6be9fce2973ff7df6631c4dbc61ec9945e4005",
    "line" : 75,
    "diffHunk" : "@@ -1,1 +1428,1432 @@        \"the fields). Otherwise, the schema might not be compatible with actual data, which \" +\n        \"leads to corruptions.\")\n      .version(\"3.0.0\")\n      .booleanConf\n      .createWithDefault(true)"
  },
  {
    "id" : "ac411fb8-b8cd-4a6a-8547-55bc00db5b28",
    "prId" : 27770,
    "prUrl" : "https://github.com/apache/spark/pull/27770#pullrequestreview-367889681",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cbae4150-d4cc-48d0-bf0b-a67a92510288",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-29876, commit ID: abf759a91e01497586b8bb6b7a314dd28fd6cff1#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-03T11:08:40Z",
        "updatedAt" : "2020-03-03T11:08:41Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "7a6be9fce2973ff7df6631c4dbc61ec9945e4005",
    "line" : 83,
    "diffHunk" : "@@ -1,1 +1435,1439 @@    buildConf(\"spark.sql.streaming.fileSource.cleaner.numThreads\")\n      .doc(\"Number of threads used in the file source completed file cleaner.\")\n      .version(\"3.0.0\")\n      .intConf\n      .createWithDefault(1)"
  },
  {
    "id" : "adbe9d21-06f9-4dad-bd55-43493a7d9662",
    "prId" : 27770,
    "prUrl" : "https://github.com/apache/spark/pull/27770#pullrequestreview-367889904",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fec90ca0-1036-4f04-844c-f3b8fa992b5f",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-15458, commit ID: 1fb7b3a0a2e3a5c5f784aab662df93fcc1449c36#diff-32bb9518401c0948c5ea19377b5069ab",
        "createdAt" : "2020-03-03T11:09:04Z",
        "updatedAt" : "2020-03-03T11:09:05Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "7a6be9fce2973ff7df6631c4dbc61ec9945e4005",
    "line" : 92,
    "diffHunk" : "@@ -1,1 +1443,1447 @@      .internal()\n      .doc(\"Whether file-based streaming sources will infer its own schema\")\n      .version(\"2.0.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "af210015-9ed0-4d8a-b9b4-461552c6bcab",
    "prId" : 27770,
    "prUrl" : "https://github.com/apache/spark/pull/27770#pullrequestreview-367890098",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e815f472-9284-4121-9a1b-c7474b1545be",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-16002, commit ID: afa14b71b28d788c53816bd2616ccff0c3967f40#diff-32bb9518401c0948c5ea19377b5069ab",
        "createdAt" : "2020-03-03T11:09:25Z",
        "updatedAt" : "2020-03-03T11:09:25Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "7a6be9fce2973ff7df6631c4dbc61ec9945e4005",
    "line" : 101,
    "diffHunk" : "@@ -1,1 +1451,1455 @@      .internal()\n      .doc(\"How long to delay polling new data when no data is available\")\n      .version(\"2.0.0\")\n      .timeConf(TimeUnit.MILLISECONDS)\n      .createWithDefault(10L)"
  },
  {
    "id" : "b7e15927-a828-4c27-9e67-7aa278388ce2",
    "prId" : 27770,
    "prUrl" : "https://github.com/apache/spark/pull/27770#pullrequestreview-367890323",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "eb58a67d-edcb-4164-b8c4-86ce95ccf0dd",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-30143, commit ID: 4c37a8a3f4a489b52f1919d2db84f6e32c6a05cd#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-03T11:09:48Z",
        "updatedAt" : "2020-03-03T11:09:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "7a6be9fce2973ff7df6631c4dbc61ec9945e4005",
    "line" : 110,
    "diffHunk" : "@@ -1,1 +1459,1463 @@      .doc(\"How long to wait in milliseconds for the streaming execution thread to stop when \" +\n        \"calling the streaming query's stop() method. 0 or negative values wait indefinitely.\")\n      .version(\"3.0.0\")\n      .timeConf(TimeUnit.MILLISECONDS)\n      .createWithDefaultString(\"0\")"
  },
  {
    "id" : "6e87f967-6f5c-4e36-a775-2e790fc1c566",
    "prId" : 27770,
    "prUrl" : "https://github.com/apache/spark/pull/27770#pullrequestreview-367890576",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "04577179-b00e-402f-9067-981411d97c46",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-19944, commit ID: 80ebca62cbdb7d5c8606e95a944164ab1a943694#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-03T11:10:12Z",
        "updatedAt" : "2020-03-03T11:10:12Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "7a6be9fce2973ff7df6631c4dbc61ec9945e4005",
    "line" : 119,
    "diffHunk" : "@@ -1,1 +1467,1471 @@      .internal()\n      .doc(\"How long to wait between two progress events when there is no data\")\n      .version(\"2.1.1\")\n      .timeConf(TimeUnit.MILLISECONDS)\n      .createWithDefault(10000L)"
  },
  {
    "id" : "fefb47f2-f488-4cda-963f-44a84d58f717",
    "prId" : 27770,
    "prUrl" : "https://github.com/apache/spark/pull/27770#pullrequestreview-367890855",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b95f1888-c88f-4d40-82e5-2be81d44a2f3",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-24157, commit ID: 535bf1cc9e6b54df7059ac3109b8cba30057d040#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-03T11:10:39Z",
        "updatedAt" : "2020-03-03T11:10:39Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "7a6be9fce2973ff7df6631c4dbc61ec9945e4005",
    "line" : 128,
    "diffHunk" : "@@ -1,1 +1476,1480 @@        \"Whether streaming micro-batch engine will execute batches without data \" +\n          \"for eager state management for stateful streaming queries.\")\n      .version(\"2.4.1\")\n      .booleanConf\n      .createWithDefault(true)"
  },
  {
    "id" : "05f4ab23-23cd-448d-87bf-69a44ff384e1",
    "prId" : 27770,
    "prUrl" : "https://github.com/apache/spark/pull/27770#pullrequestreview-367891108",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "754e948d-84e6-49ee-b21d-739cf03f4221",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-17731, commit ID: 881e0eb05782ea74cf92a62954466b14ea9e05b6#diff-32bb9518401c0948c5ea19377b5069ab",
        "createdAt" : "2020-03-03T11:11:02Z",
        "updatedAt" : "2020-03-03T11:11:02Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "7a6be9fce2973ff7df6631c4dbc61ec9945e4005",
    "line" : 136,
    "diffHunk" : "@@ -1,1 +1483,1487 @@    buildConf(\"spark.sql.streaming.metricsEnabled\")\n      .doc(\"Whether Dropwizard/Codahale metrics will be reported for active streaming queries.\")\n      .version(\"2.0.2\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "d9a05b29-bcb9-472f-a492-e2c8fc7e5219",
    "prId" : 27770,
    "prUrl" : "https://github.com/apache/spark/pull/27770#pullrequestreview-367891344",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9d02de11-25df-4043-af31-ff14b4267681",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-19944, commit ID: 80ebca62cbdb7d5c8606e95a944164ab1a943694#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-03T11:11:27Z",
        "updatedAt" : "2020-03-03T11:11:28Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "7a6be9fce2973ff7df6631c4dbc61ec9945e4005",
    "line" : 144,
    "diffHunk" : "@@ -1,1 +1490,1494 @@    buildConf(\"spark.sql.streaming.numRecentProgressUpdates\")\n      .doc(\"The number of progress updates to retain for a streaming query\")\n      .version(\"2.1.1\")\n      .intConf\n      .createWithDefault(100)"
  },
  {
    "id" : "3095efa9-67dc-439c-9ebe-7733e8a14d36",
    "prId" : 27770,
    "prUrl" : "https://github.com/apache/spark/pull/27770#pullrequestreview-367891614",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6e3ffb03-fc6c-4b36-9204-c8c6857d0b58",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-23966, commit ID: cbb41a0c5b01579c85f06ef42cc0585fbef216c5#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-03T11:11:52Z",
        "updatedAt" : "2020-03-03T11:11:52Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "7a6be9fce2973ff7df6631c4dbc61ec9945e4005",
    "line" : 153,
    "diffHunk" : "@@ -1,1 +1498,1502 @@      .doc(\"The class used to write checkpoint files atomically. This class must be a subclass \" +\n        \"of the interface CheckpointFileManager.\")\n      .version(\"2.4.0\")\n      .internal()\n      .stringConf"
  },
  {
    "id" : "e500c3ce-d7e8-4e11-bdf7-2b47a34d3b4b",
    "prId" : 27770,
    "prUrl" : "https://github.com/apache/spark/pull/27770#pullrequestreview-367891854",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1a474f6a-b2d2-4c6a-aaf3-30a3e75ff4c1",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-26824, commit ID: 77b99af57330cf2e5016a6acc69642d54041b041#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-03T11:12:14Z",
        "updatedAt" : "2020-03-03T11:12:15Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "7a6be9fce2973ff7df6631c4dbc61ec9945e4005",
    "line" : 162,
    "diffHunk" : "@@ -1,1 +1506,1510 @@      .doc(\"Whether to detect a streaming query may pick up an incorrect checkpoint path due \" +\n        \"to SPARK-26824.\")\n      .version(\"3.0.0\")\n      .internal()\n      .booleanConf"
  },
  {
    "id" : "dfee1a4a-b9f5-408f-a5a4-dc263e360688",
    "prId" : 27770,
    "prUrl" : "https://github.com/apache/spark/pull/27770#pullrequestreview-367892153",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bc374f66-4864-467d-8dbe-737b345a7969",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-24626, commit ID: f11f44548903bbab7ab764574d6bed326cf4cd8d#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-03T11:12:41Z",
        "updatedAt" : "2020-03-03T11:12:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "7a6be9fce2973ff7df6631c4dbc61ec9945e4005",
    "line" : 171,
    "diffHunk" : "@@ -1,1 +1517,1521 @@        \"as opposed to single thread listing. \" +\n        \"This usually speeds up commands that need to list many directories.\")\n      .version(\"2.4.1\")\n      .booleanConf\n      .createWithDefault(true)"
  },
  {
    "id" : "20c6fcaf-2707-472d-b901-0e68f5676859",
    "prId" : 27770,
    "prUrl" : "https://github.com/apache/spark/pull/27770#pullrequestreview-367892425",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "26ead50d-7fbf-4768-9ea6-6f08f36ebf4b",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-2393, commit ID: c7db274be79f448fda566208946cb50958ea9b1a#diff-41ef65b9ef5b518f77e2a03559893f4d",
        "createdAt" : "2020-03-03T11:13:07Z",
        "updatedAt" : "2020-03-03T11:13:08Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "7a6be9fce2973ff7df6631c4dbc61ec9945e4005",
    "line" : 180,
    "diffHunk" : "@@ -1,1 +1527,1531 @@      \"That is to say by default the optimizer will not choose to broadcast a table unless it \" +\n      \"knows for sure its size is small enough.\")\n    .version(\"1.1.0\")\n    .bytesConf(ByteUnit.BYTE)\n    .createWithDefault(Long.MaxValue)"
  },
  {
    "id" : "ca88d1d5-e61b-4766-aaab-c0b670a2ba23",
    "prId" : 27770,
    "prUrl" : "https://github.com/apache/spark/pull/27770#pullrequestreview-367892669",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8cec46e8-30ed-4d77-801c-7b403fc420eb",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-15960, commit ID: 5c53442cc098dd618ba1430962727c74b2de2e68#diff-32bb9518401c0948c5ea19377b5069ab",
        "createdAt" : "2020-03-03T11:13:33Z",
        "updatedAt" : "2020-03-03T11:13:34Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "7a6be9fce2973ff7df6631c4dbc61ec9945e4005",
    "line" : 189,
    "diffHunk" : "@@ -1,1 +1538,1542 @@      \"statistics are not available. For partitioned data source and partitioned Hive tables, \" +\n      s\"It is '${DEFAULT_SIZE_IN_BYTES.key}' if table statistics are not available.\")\n    .version(\"2.0.0\")\n    .booleanConf\n    .createWithDefault(false)"
  },
  {
    "id" : "289b8b14-e971-41ab-a0ed-2065a0014025",
    "prId" : 27770,
    "prUrl" : "https://github.com/apache/spark/pull/27770#pullrequestreview-367892924",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a1658037-eeb6-4f27-95fc-ec4203366abb",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-19944, commit ID: 80ebca62cbdb7d5c8606e95a944164ab1a943694#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-03T11:14:01Z",
        "updatedAt" : "2020-03-03T11:14:02Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "7a6be9fce2973ff7df6631c4dbc61ec9945e4005",
    "line" : 198,
    "diffHunk" : "@@ -1,1 +1547,1551 @@      .doc(\"The maximum estimation error allowed in HyperLogLog++ algorithm when generating \" +\n        \"column level statistics.\")\n      .version(\"2.1.1\")\n      .doubleConf\n      .createWithDefault(0.05)"
  },
  {
    "id" : "7d18221b-025b-4cae-ad3f-80fb86883fb2",
    "prId" : 27770,
    "prUrl" : "https://github.com/apache/spark/pull/27770#pullrequestreview-367893245",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e0e39212-d203-43ac-96c5-d6393901b23c",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-17074, commit ID: 11b60af737a04d931356aa74ebf3c6cf4a6b08d6#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-03T11:14:31Z",
        "updatedAt" : "2020-03-03T11:14:31Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "7a6be9fce2973ff7df6631c4dbc61ec9945e4005",
    "line" : 207,
    "diffHunk" : "@@ -1,1 +1558,1562 @@        \"column statistics usually takes only one table scan, but generating equi-height \" +\n        \"histogram will cause an extra table scan.\")\n      .version(\"2.3.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "fd15d587-8fee-4871-9f6d-a1e01cf308ed",
    "prId" : 27770,
    "prUrl" : "https://github.com/apache/spark/pull/27770#pullrequestreview-367893324",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e10d746a-fa3a-44f4-97d1-c0e600577536",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-17074, commit ID: 11b60af737a04d931356aa74ebf3c6cf4a6b08d6#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-03T11:14:40Z",
        "updatedAt" : "2020-03-03T11:14:40Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "7a6be9fce2973ff7df6631c4dbc61ec9945e4005",
    "line" : 216,
    "diffHunk" : "@@ -1,1 +1566,1570 @@      .internal()\n      .doc(\"The number of bins when generating histograms.\")\n      .version(\"2.3.0\")\n      .intConf\n      .checkValue(num => num > 1, \"The number of bins must be greater than 1.\")"
  },
  {
    "id" : "d76c7dd4-ec0a-4e45-8bca-f8ab295c73f9",
    "prId" : 27770,
    "prUrl" : "https://github.com/apache/spark/pull/27770#pullrequestreview-367893616",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dc9f3340-a7b5-4acd-8246-d2c30f94a54f",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-17074, commit ID: 11b60af737a04d931356aa74ebf3c6cf4a6b08d6#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-03T11:15:07Z",
        "updatedAt" : "2020-03-03T11:15:08Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "7a6be9fce2973ff7df6631c4dbc61ec9945e4005",
    "line" : 225,
    "diffHunk" : "@@ -1,1 +1577,1581 @@        \"Larger value means better accuracy. The relative error can be deduced by \" +\n        \"1.0 / PERCENTILE_ACCURACY.\")\n      .version(\"2.3.0\")\n      .intConf\n      .createWithDefault(10000)"
  },
  {
    "id" : "225fcd22-6757-42a7-b41c-4509b7da317b",
    "prId" : 27770,
    "prUrl" : "https://github.com/apache/spark/pull/27770#pullrequestreview-367894008",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c15dd552-eb43-41a9-b39a-50db515bc5af",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-21127, commit ID: d5202259d9aa9ad95d572af253bf4a722b7b437a#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-03T11:15:49Z",
        "updatedAt" : "2020-03-03T11:15:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "7a6be9fce2973ff7df6631c4dbc61ec9945e4005",
    "line" : 234,
    "diffHunk" : "@@ -1,1 +1586,1590 @@        \"the total number of files of the table is very large, this can be expensive and slow \" +\n        \"down data change commands.\")\n      .version(\"2.3.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "645679fa-f9d3-41a7-87f8-168e185efc52",
    "prId" : 27770,
    "prUrl" : "https://github.com/apache/spark/pull/27770#pullrequestreview-367894416",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "959bb7c6-c3b7-4211-b766-d1cb2c828a4b",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-19944, commit ID: 0ee38a39e43dd7ad9d50457e446ae36f64621a1b#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-03T11:16:31Z",
        "updatedAt" : "2020-03-03T11:16:31Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "7a6be9fce2973ff7df6631c4dbc61ec9945e4005",
    "line" : 242,
    "diffHunk" : "@@ -1,1 +1593,1597 @@    buildConf(\"spark.sql.cbo.enabled\")\n      .doc(\"Enables CBO for estimation of plan statistics when set true.\")\n      .version(\"2.2.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "5464d2dd-0437-49ee-a210-85965897de90",
    "prId" : 27770,
    "prUrl" : "https://github.com/apache/spark/pull/27770#pullrequestreview-367894598",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b6042435-e399-458b-af33-cf0e518aea62",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-19944, commit ID: 0ee38a39e43dd7ad9d50457e446ae36f64621a1b#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-03T11:16:49Z",
        "updatedAt" : "2020-03-03T11:16:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "7a6be9fce2973ff7df6631c4dbc61ec9945e4005",
    "line" : 258,
    "diffHunk" : "@@ -1,1 +1607,1611 @@    buildConf(\"spark.sql.cbo.joinReorder.enabled\")\n      .doc(\"Enables join reorder in CBO.\")\n      .version(\"2.2.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "5458ea53-2e43-498b-90cb-b949ff2bed3d",
    "prId" : 27770,
    "prUrl" : "https://github.com/apache/spark/pull/27770#pullrequestreview-367894655",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9eff520e-b569-4c07-9781-52b62131290f",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-19944, commit ID: 0ee38a39e43dd7ad9d50457e446ae36f64621a1b#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-03T11:16:55Z",
        "updatedAt" : "2020-03-03T11:16:56Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "7a6be9fce2973ff7df6631c4dbc61ec9945e4005",
    "line" : 266,
    "diffHunk" : "@@ -1,1 +1614,1618 @@    buildConf(\"spark.sql.cbo.joinReorder.dp.threshold\")\n      .doc(\"The maximum number of joined nodes allowed in the dynamic programming algorithm.\")\n      .version(\"2.2.0\")\n      .intConf\n      .checkValue(number => number > 0, \"The maximum number must be a positive integer.\")"
  },
  {
    "id" : "132880cd-89a4-43f1-ba83-d9ee4076c029",
    "prId" : 27770,
    "prUrl" : "https://github.com/apache/spark/pull/27770#pullrequestreview-367894966",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "41c442bd-38b4-486c-9485-b260abbd1ba1",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-24690, commit ID: 3f3a18fff116a02ff7996d45a1061f48a2de3102#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-03T11:17:28Z",
        "updatedAt" : "2020-03-03T11:17:28Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "7a6be9fce2973ff7df6631c4dbc61ec9945e4005",
    "line" : 250,
    "diffHunk" : "@@ -1,1 +1600,1604 @@    buildConf(\"spark.sql.cbo.planStats.enabled\")\n      .doc(\"When true, the logical plan will fetch row counts and column statistics from catalog.\")\n      .version(\"3.0.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "30ef9d6c-cd8a-400c-ab74-ba86aa01ae2c",
    "prId" : 27770,
    "prUrl" : "https://github.com/apache/spark/pull/27770#pullrequestreview-367895213",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e1732bcb-4c6d-4955-9999-349819dafd39",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-19915, commit ID: c083b6b7dec337d680b54dabeaa40e7a0f69ae69#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-03T11:17:51Z",
        "updatedAt" : "2020-03-03T11:17:51Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "7a6be9fce2973ff7df6631c4dbc61ec9945e4005",
    "line" : 275,
    "diffHunk" : "@@ -1,1 +1624,1628 @@      .doc(\"The weight of cardinality (number of rows) for plan cost comparison in join reorder: \" +\n        \"rows * weight + size * (1 - weight).\")\n      .version(\"2.2.0\")\n      .doubleConf\n      .checkValue(weight => weight >= 0 && weight <= 1, \"The weight value must be in [0, 1].\")"
  },
  {
    "id" : "7d4d3d01-a666-4536-bc44-fa4801050f2f",
    "prId" : 27770,
    "prUrl" : "https://github.com/apache/spark/pull/27770#pullrequestreview-367895413",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "64306ea2-40c5-4086-9c50-383d796b4a7f",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-20233, commit ID: fbe4216e1e83d243a7f0521b76bfb20c25278281#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-03T11:18:12Z",
        "updatedAt" : "2020-03-03T11:18:13Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "7a6be9fce2973ff7df6631c4dbc61ec9945e4005",
    "line" : 284,
    "diffHunk" : "@@ -1,1 +1632,1636 @@    buildConf(\"spark.sql.cbo.joinReorder.dp.star.filter\")\n      .doc(\"Applies star-join filter heuristics to cost based join enumeration.\")\n      .version(\"2.2.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "0b6452f1-37f8-4a1c-964b-5230c34f0dcc",
    "prId" : 27770,
    "prUrl" : "https://github.com/apache/spark/pull/27770#pullrequestreview-367895692",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b9a80e78-47ef-4d1e-8051-decb8138e408",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-17791, commit ID: 81639115947a13017d1637549a8f66ba599b27b8#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-03T11:18:44Z",
        "updatedAt" : "2020-03-03T11:18:45Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "7a6be9fce2973ff7df6631c4dbc61ec9945e4005",
    "line" : 291,
    "diffHunk" : "@@ -1,1 +1638,1642 @@  val STARSCHEMA_DETECTION = buildConf(\"spark.sql.cbo.starSchemaDetection\")\n    .doc(\"When true, it enables join reordering based on star schema detection. \")\n    .version(\"2.2.0\")\n    .booleanConf\n    .createWithDefault(false)"
  },
  {
    "id" : "0eb17015-dbaa-4e14-946e-971be45f2121",
    "prId" : 27770,
    "prUrl" : "https://github.com/apache/spark/pull/27770#pullrequestreview-367895946",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d158b88c-3c87-4c6a-9b02-08906a06d514",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-17791, commit ID: 81639115947a13017d1637549a8f66ba599b27b8#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-03T11:19:10Z",
        "updatedAt" : "2020-03-03T11:19:11Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "7a6be9fce2973ff7df6631c4dbc61ec9945e4005",
    "line" : 300,
    "diffHunk" : "@@ -1,1 +1646,1650 @@    .doc(\"Specifies the upper limit of the ratio between the largest fact tables\" +\n      \" for a star join to be considered. \")\n    .version(\"2.2.0\")\n    .doubleConf\n    .createWithDefault(0.9)"
  },
  {
    "id" : "ee860ef0-db86-4027-8f2a-de2af4680aad",
    "prId" : 27770,
    "prUrl" : "https://github.com/apache/spark/pull/27770#pullrequestreview-367896153",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9f329e48-dfd6-4e44-90f0-734453c79a4f",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-19944, commit ID: 0ee38a39e43dd7ad9d50457e446ae36f64621a1b#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-03T11:19:31Z",
        "updatedAt" : "2020-03-03T11:19:32Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "7a6be9fce2973ff7df6631c4dbc61ec9945e4005",
    "line" : 308,
    "diffHunk" : "@@ -1,1 +1653,1657 @@    buildConf(\"spark.sql.session.timeZone\")\n      .doc(\"\"\"The ID of session local timezone, e.g. \"GMT\", \"America/Los_Angeles\", etc.\"\"\")\n      .version(\"2.2.0\")\n      .stringConf\n      .createWithDefaultFunction(() => TimeZone.getDefault.getID)"
  },
  {
    "id" : "29151a72-8ca9-437b-b22e-0b376e01c4af",
    "prId" : 27770,
    "prUrl" : "https://github.com/apache/spark/pull/27770#pullrequestreview-367896380",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ddc02678-f354-43be-b56f-db192c6ba825",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-21595, commit ID: 406eb1c2ee670c2f14f2737c32c9aa0b8d35bf7c#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-03T11:19:56Z",
        "updatedAt" : "2020-03-03T11:19:56Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "7a6be9fce2973ff7df6631c4dbc61ec9945e4005",
    "line" : 317,
    "diffHunk" : "@@ -1,1 +1661,1665 @@      .internal()\n      .doc(\"Threshold for number of rows guaranteed to be held in memory by the window operator\")\n      .version(\"2.2.1\")\n      .intConf\n      .createWithDefault(4096)"
  },
  {
    "id" : "c15127da-8e01-42d1-8529-2950f341ef76",
    "prId" : 27770,
    "prUrl" : "https://github.com/apache/spark/pull/27770#pullrequestreview-367896595",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "956da240-7955-443f-8a83-5ab6b1e7b1ec",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-13450, commit ID: 02c274eaba0a8e7611226e0d4e93d3c36253f4ce#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-03T11:20:22Z",
        "updatedAt" : "2020-03-03T11:20:23Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "7a6be9fce2973ff7df6631c4dbc61ec9945e4005",
    "line" : 326,
    "diffHunk" : "@@ -1,1 +1669,1673 @@      .internal()\n      .doc(\"Threshold for number of rows to be spilled by window operator\")\n      .version(\"2.2.0\")\n      .intConf\n      .createWithDefault(SHUFFLE_SPILL_NUM_ELEMENTS_FORCE_SPILL_THRESHOLD.defaultValue.get)"
  },
  {
    "id" : "99ad1e94-02c4-4319-bd0c-9178073b376e",
    "prId" : 27770,
    "prUrl" : "https://github.com/apache/spark/pull/27770#pullrequestreview-367896771",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ea5cbe1d-840d-4c81-a01e-9bb9e0d5d48d",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-21595, commit ID: 406eb1c2ee670c2f14f2737c32c9aa0b8d35bf7c#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-03T11:20:43Z",
        "updatedAt" : "2020-03-03T11:20:44Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "7a6be9fce2973ff7df6631c4dbc61ec9945e4005",
    "line" : 335,
    "diffHunk" : "@@ -1,1 +1678,1682 @@      .doc(\"Threshold for number of rows guaranteed to be held in memory by the sort merge \" +\n        \"join operator\")\n      .version(\"2.2.1\")\n      .intConf\n      .createWithDefault(ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH)"
  },
  {
    "id" : "a1ccd1c6-4f89-4024-b6d0-c90b424ee40a",
    "prId" : 27770,
    "prUrl" : "https://github.com/apache/spark/pull/27770#pullrequestreview-367897050",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0cca878a-8e52-4152-85d9-ab2916f421db",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-13450, commit ID: 02c274eaba0a8e7611226e0d4e93d3c36253f4ce#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-03T11:21:10Z",
        "updatedAt" : "2020-03-03T11:21:11Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "7a6be9fce2973ff7df6631c4dbc61ec9945e4005",
    "line" : 344,
    "diffHunk" : "@@ -1,1 +1686,1690 @@      .internal()\n      .doc(\"Threshold for number of rows to be spilled by sort merge join operator\")\n      .version(\"2.2.0\")\n      .intConf\n      .createWithDefault(SHUFFLE_SPILL_NUM_ELEMENTS_FORCE_SPILL_THRESHOLD.defaultValue.get)"
  },
  {
    "id" : "856617f8-d219-49bb-a6de-b26ee9042dda",
    "prId" : 27770,
    "prUrl" : "https://github.com/apache/spark/pull/27770#pullrequestreview-367897282",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d31a3588-6545-41d0-873f-4b5ede966c19",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-21595, commit ID: 406eb1c2ee670c2f14f2737c32c9aa0b8d35bf7c#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-03T11:21:34Z",
        "updatedAt" : "2020-03-03T11:21:35Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "7a6be9fce2973ff7df6631c4dbc61ec9945e4005",
    "line" : 353,
    "diffHunk" : "@@ -1,1 +1695,1699 @@      .doc(\"Threshold for number of rows guaranteed to be held in memory by the cartesian \" +\n        \"product operator\")\n      .version(\"2.2.1\")\n      .intConf\n      .createWithDefault(4096)"
  },
  {
    "id" : "f134d3bd-0f0a-443e-bbd4-cc7e4e990691",
    "prId" : 27770,
    "prUrl" : "https://github.com/apache/spark/pull/27770#pullrequestreview-367897505",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dd6a337b-7ebc-4664-9d6e-e81fddf8edf9",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-13450, commit ID: 02c274eaba0a8e7611226e0d4e93d3c36253f4ce#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-03T11:21:57Z",
        "updatedAt" : "2020-03-03T11:21:58Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "7a6be9fce2973ff7df6631c4dbc61ec9945e4005",
    "line" : 362,
    "diffHunk" : "@@ -1,1 +1703,1707 @@      .internal()\n      .doc(\"Threshold for number of rows to be spilled by cartesian product operator\")\n      .version(\"2.2.0\")\n      .intConf\n      .createWithDefault(SHUFFLE_SPILL_NUM_ELEMENTS_FORCE_SPILL_THRESHOLD.defaultValue.get)"
  },
  {
    "id" : "ffa2476f-9c03-4596-b72e-aa9408119cab",
    "prId" : 27770,
    "prUrl" : "https://github.com/apache/spark/pull/27770#pullrequestreview-367897758",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "25e2c60a-b6aa-46a8-9ef2-656e673e12c4",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-12139, commit ID: 2cbfc975ba937a4eb761de7a6473b7747941f386#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-03T11:22:20Z",
        "updatedAt" : "2020-03-03T11:22:21Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "7a6be9fce2973ff7df6631c4dbc61ec9945e4005",
    "line" : 370,
    "diffHunk" : "@@ -1,1 +1710,1714 @@    .doc(\"When true, quoted Identifiers (using backticks) in SELECT statement are interpreted\" +\n      \" as regular expressions.\")\n    .version(\"2.3.0\")\n    .booleanConf\n    .createWithDefault(false)"
  },
  {
    "id" : "e78b0c28-315e-4b00-959b-7c7b36e96845",
    "prId" : 27770,
    "prUrl" : "https://github.com/apache/spark/pull/27770#pullrequestreview-367898038",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "aa3452a5-6009-4963-8ebb-c570b3fadcb5",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-22160, commit ID: 323806e68f91f3c7521327186a37ddd1436267d0#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-03T11:22:51Z",
        "updatedAt" : "2020-03-03T11:22:51Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "7a6be9fce2973ff7df6631c4dbc61ec9945e4005",
    "line" : 379,
    "diffHunk" : "@@ -1,1 +1719,1723 @@      .doc(\"Number of points to sample per partition in order to determine the range boundaries\" +\n          \" for range partitioning, typically used in global sorting (without limit).\")\n      .version(\"2.3.0\")\n      .intConf\n      .createWithDefault(100)"
  },
  {
    "id" : "a97e2d07-642e-4673-915f-519c33579b9c",
    "prId" : 27770,
    "prUrl" : "https://github.com/apache/spark/pull/27770#pullrequestreview-367898268",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bd6cb3dd-aa0c-40c5-b062-3fffb15c6022",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-22159, commit ID: d29d1e87995e02cb57ba3026c945c3cd66bb06e2#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-03T11:23:14Z",
        "updatedAt" : "2020-03-03T11:23:14Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "7a6be9fce2973ff7df6631c4dbc61ec9945e4005",
    "line" : 387,
    "diffHunk" : "@@ -1,1 +1726,1730 @@    buildConf(\"spark.sql.execution.arrow.enabled\")\n      .doc(\"(Deprecated since Spark 3.0, please set 'spark.sql.execution.arrow.pyspark.enabled'.)\")\n      .version(\"2.3.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "c3188ebc-4536-46e8-ac72-d9a483e3cbaa",
    "prId" : 27770,
    "prUrl" : "https://github.com/apache/spark/pull/27770#pullrequestreview-367898561",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "898deea8-0bd4-42bf-b7cf-bce124edb331",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-27834, commit ID: db48da87f02e2e89710ba65fab8b07e9c85b9e74#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-03T11:23:41Z",
        "updatedAt" : "2020-03-03T11:23:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "7a6be9fce2973ff7df6631c4dbc61ec9945e4005",
    "line" : 396,
    "diffHunk" : "@@ -1,1 +1738,1742 @@        \"The following data types are unsupported: \" +\n        \"BinaryType, MapType, ArrayType of TimestampType, and nested StructType.\")\n      .version(\"3.0.0\")\n      .fallbackConf(ARROW_EXECUTION_ENABLED)\n"
  },
  {
    "id" : "45afb034-ab51-4c23-a1f9-92ff45b73893",
    "prId" : 27770,
    "prUrl" : "https://github.com/apache/spark/pull/27770#pullrequestreview-367898985",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5ba18a3a-66f8-4558-ad94-bb7a1ffc6052",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-27834, commit ID: db48da87f02e2e89710ba65fab8b07e9c85b9e74#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-03T11:24:23Z",
        "updatedAt" : "2020-03-03T11:24:23Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "7a6be9fce2973ff7df6631c4dbc61ec9945e4005",
    "line" : 405,
    "diffHunk" : "@@ -1,1 +1751,1755 @@        \"The following data types are unsupported: \" +\n        \"FloatType, BinaryType, ArrayType, StructType and MapType.\")\n      .version(\"3.0.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "4ffc156a-40eb-4c6c-9dff-f78e7f6d37d1",
    "prId" : 27770,
    "prUrl" : "https://github.com/apache/spark/pull/27770#pullrequestreview-367899207",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2446c04f-59df-4fa4-8e72-51f45e329dd1",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-23380, commit ID: d6632d185e147fcbe6724545488ad80dce20277e#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-03T11:24:46Z",
        "updatedAt" : "2020-03-03T11:24:46Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "7a6be9fce2973ff7df6631c4dbc61ec9945e4005",
    "line" : 414,
    "diffHunk" : "@@ -1,1 +1759,1763 @@      .doc(\"(Deprecated since Spark 3.0, please set \" +\n        \"'spark.sql.execution.arrow.pyspark.fallback.enabled'.)\")\n      .version(\"2.4.0\")\n      .booleanConf\n      .createWithDefault(true)"
  },
  {
    "id" : "ec247ac5-c8af-4151-8e0c-4f45c7dfb128",
    "prId" : 27770,
    "prUrl" : "https://github.com/apache/spark/pull/27770#pullrequestreview-367899466",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4319f82e-53f2-49d2-9349-6ce04e9f07c9",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-27834, commit ID: db48da87f02e2e89710ba65fab8b07e9c85b9e74#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-03T11:25:13Z",
        "updatedAt" : "2020-03-03T11:25:13Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "7a6be9fce2973ff7df6631c4dbc61ec9945e4005",
    "line" : 423,
    "diffHunk" : "@@ -1,1 +1767,1771 @@      .doc(s\"When true, optimizations enabled by '${ARROW_PYSPARK_EXECUTION_ENABLED.key}' will \" +\n        \"fallback automatically to non-optimized implementations if an error occurs.\")\n      .version(\"3.0.0\")\n      .fallbackConf(ARROW_FALLBACK_ENABLED)\n"
  },
  {
    "id" : "6e03e6b9-1eeb-4a90-9754-784f616c1743",
    "prId" : 27770,
    "prUrl" : "https://github.com/apache/spark/pull/27770#pullrequestreview-367899729",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a27b8595-c168-46c1-aeed-ba7cf6adc33b",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-13534, commit ID: d03aebbe6508ba441dc87f9546f27aeb27553d77#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-03T11:25:41Z",
        "updatedAt" : "2020-03-03T11:25:41Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "7a6be9fce2973ff7df6631c4dbc61ec9945e4005",
    "line" : 431,
    "diffHunk" : "@@ -1,1 +1774,1778 @@      .doc(\"When using Apache Arrow, limit the maximum number of records that can be written \" +\n        \"to a single ArrowRecordBatch in memory. If set to zero or negative there is no limit.\")\n      .version(\"2.3.0\")\n      .intConf\n      .createWithDefault(10000)"
  },
  {
    "id" : "d15cf8ed-1351-4c78-aace-212fdefe9d2b",
    "prId" : 27770,
    "prUrl" : "https://github.com/apache/spark/pull/27770#pullrequestreview-367900426",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fa6c47f7-09ca-4b0c-90a9-a64f09fe3ac4",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-27870, commit ID: 692e3ddb4e517638156f7427ade8b62fb37634a7#diff-9a6b543db706f1a90f790783d6930a13\r\nExists in master, not branch-3.0",
        "createdAt" : "2020-03-03T11:26:53Z",
        "updatedAt" : "2020-03-03T11:26:54Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "7a6be9fce2973ff7df6631c4dbc61ec9945e4005",
    "line" : 440,
    "diffHunk" : "@@ -1,1 +1785,1789 @@        \"than 4 bytes. Lowering this value could make small Pandas UDF batch iterated and \" +\n        \"pipelined; however, it might degrade performance. See SPARK-27870.\")\n      .version(\"3.1.0\")\n      .fallbackConf(BUFFER_SIZE)\n"
  },
  {
    "id" : "96ba6da5-4674-4f29-8604-076db8926458",
    "prId" : 27770,
    "prUrl" : "https://github.com/apache/spark/pull/27770#pullrequestreview-367900756",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "49892fb1-8410-4588-a5d7-2643f3248915",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-24324, commit ID: 3f203050ac764516e68fb43628bba0df5963e44d#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-03T11:27:29Z",
        "updatedAt" : "2020-03-03T11:27:29Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "7a6be9fce2973ff7df6631c4dbc61ec9945e4005",
    "line" : 449,
    "diffHunk" : "@@ -1,1 +1795,1799 @@        \"the returned Pandas DataFrame based on position, regardless of column label type. \" +\n        \"This configuration will be deprecated in future releases.\")\n      .version(\"2.4.1\")\n      .booleanConf\n      .createWithDefault(true)"
  },
  {
    "id" : "f91621fc-44ad-4b5d-ab79-683827fd586c",
    "prId" : 27770,
    "prUrl" : "https://github.com/apache/spark/pull/27770#pullrequestreview-367901139",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b201538e-960d-4b75-ae6b-45251a8966c7",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-30812, commit ID: b76bc0b1b8b2abd00a84f805af90ca4c5925faaa#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-03T11:28:08Z",
        "updatedAt" : "2020-03-03T11:28:08Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "7a6be9fce2973ff7df6631c4dbc61ec9945e4005",
    "line" : 458,
    "diffHunk" : "@@ -1,1 +1806,1810 @@        \"when detecting unsafe type conversion like overflow. When false, disabling Arrow's type \" +\n        \"check and do type conversions anyway. This config only works for Arrow 0.11.0+.\")\n      .version(\"3.0.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "2d7c344c-8f58-4ef1-b933-3957d59b6d97",
    "prId" : 27770,
    "prUrl" : "https://github.com/apache/spark/pull/27770#pullrequestreview-367901351",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "454d2dc3-b52b-465c-a6c9-4d1a5b03cb06",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-22181, commit ID: 01f6ba0e7a12ef818d56e7d5b1bd889b79f2b57c#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-03T11:28:30Z",
        "updatedAt" : "2020-03-03T11:28:30Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "7a6be9fce2973ff7df6631c4dbc61ec9945e4005",
    "line" : 467,
    "diffHunk" : "@@ -1,1 +1820,1824 @@      \" rule will replace the except operation with a Filter by flipping the filter\" +\n      \" condition(s) of the right node.\")\n    .version(\"2.3.0\")\n    .booleanConf\n    .createWithDefault(true)"
  },
  {
    "id" : "d1570c20-3413-4df1-a2e6-7262fd52ddf1",
    "prId" : 27770,
    "prUrl" : "https://github.com/apache/spark/pull/27770#pullrequestreview-367901562",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "55e61240-ae3c-4120-8525-f4db7624df0b",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-22036, commit ID: 8a98274823a4671cee85081dd19f40146e736325#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-03T11:28:52Z",
        "updatedAt" : "2020-03-03T11:28:53Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "7a6be9fce2973ff7df6631c4dbc61ec9945e4005",
    "line" : 476,
    "diffHunk" : "@@ -1,1 +1831,1835 @@        \"decimal part of the result if an exact representation is not possible. Otherwise, NULL \" +\n        \"is returned in those cases, as previously.\")\n      .version(\"2.3.1\")\n      .booleanConf\n      .createWithDefault(true)"
  },
  {
    "id" : "79d2a833-2ee8-4019-b6c8-9979d5f2d070",
    "prId" : 27770,
    "prUrl" : "https://github.com/apache/spark/pull/27770#pullrequestreview-367901771",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cf0cc0c4-b1d1-4ae6-baf2-b90a021a5175",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-25454, commit ID: 26d893a4f64de18222942568f7735114447a6ab7#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-03T11:29:16Z",
        "updatedAt" : "2020-03-03T11:29:16Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "7a6be9fce2973ff7df6631c4dbc61ec9945e4005",
    "line" : 485,
    "diffHunk" : "@@ -1,1 +1841,1845 @@        \"required by the literal if this config is true, to make the resulting precision and/or \" +\n        \"scale smaller. This can reduce the possibility of precision lose and/or overflow.\")\n      .version(\"2.3.3\")\n      .booleanConf\n      .createWithDefault(true)"
  },
  {
    "id" : "f8dfe590-83d6-4d43-8f47-7aa7f9527d63",
    "prId" : 27730,
    "prUrl" : "https://github.com/apache/spark/pull/27730#pullrequestreview-366192644",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "aebadb4e-4726-4e97-be6b-bac6d06f63d7",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-21839, commit ID: d8f45408635d4fccac557cb1e877dfe9267fb326#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-02-28T07:01:54Z",
        "updatedAt" : "2020-03-01T05:24:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "a32304876ba552d31b8562d9562e0f0772851563",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +712,716 @@      \"would be `compression`, `orc.compress`, `spark.sql.orc.compression.codec`.\" +\n      \"Acceptable values include: none, uncompressed, snappy, zlib, lzo.\")\n    .version(\"2.3.0\")\n    .stringConf\n    .transform(_.toLowerCase(Locale.ROOT))"
  },
  {
    "id" : "ee8b723c-f775-488f-8325-98fb802372e8",
    "prId" : 27730,
    "prUrl" : "https://github.com/apache/spark/pull/27730#pullrequestreview-366192798",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "39347a4e-a4f5-41e1-91e6-4e52272ace6b",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-20728, commit ID: 326f1d6728a7734c228d8bfaa69442a1c7b92e9b#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-02-28T07:02:21Z",
        "updatedAt" : "2020-03-01T05:24:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "a32304876ba552d31b8562d9562e0f0772851563",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +721,725 @@    .doc(\"When native, use the native version of ORC support instead of the ORC library in Hive. \" +\n      \"It is 'hive' by default prior to Spark 2.4.\")\n    .version(\"2.3.0\")\n    .internal()\n    .stringConf"
  },
  {
    "id" : "d7b9855d-4373-4448-bc37-0dd2b1289220",
    "prId" : 27730,
    "prUrl" : "https://github.com/apache/spark/pull/27730#pullrequestreview-366192955",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e33927e6-ad47-4878-aa2f-901d630ef04d",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-16060, commit ID: 60f6b994505e3f82091a04eed2dc0a9e8bd523ce#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-02-28T07:02:46Z",
        "updatedAt" : "2020-03-01T05:24:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "a32304876ba552d31b8562d9562e0f0772851563",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +729,733 @@  val ORC_VECTORIZED_READER_ENABLED = buildConf(\"spark.sql.orc.enableVectorizedReader\")\n    .doc(\"Enables vectorized orc decoding.\")\n    .version(\"2.3.0\")\n    .booleanConf\n    .createWithDefault(true)"
  },
  {
    "id" : "7e1ca13d-2836-4739-b2ec-fd41748ba4d3",
    "prId" : 27730,
    "prUrl" : "https://github.com/apache/spark/pull/27730#pullrequestreview-366193118",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3603e028-bbb0-41a5-9755-005d7894dfc5",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-23188, commit ID: cc41245fa3f954f961541bf4b4275c28473042b8#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-02-28T07:03:14Z",
        "updatedAt" : "2020-03-01T05:24:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "a32304876ba552d31b8562d9562e0f0772851563",
    "line" : 27,
    "diffHunk" : "@@ -1,1 +736,740 @@    .doc(\"The number of rows to include in a orc vectorized reader batch. The number should \" +\n      \"be carefully chosen to minimize overhead and avoid OOMs in reading data.\")\n    .version(\"2.4.0\")\n    .intConf\n    .createWithDefault(4096)"
  },
  {
    "id" : "29ebb9cc-b39e-4180-a973-98d6bd129cd7",
    "prId" : 27730,
    "prUrl" : "https://github.com/apache/spark/pull/27730#pullrequestreview-366193266",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ce27169b-1580-4474-bb23-5d7eb447395a",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-2883, commit ID: 65d71bd9fbfe6fe1b741c80fed72d6ae3d22b028#diff-41ef65b9ef5b518f77e2a03559893f4d",
        "createdAt" : "2020-02-28T07:03:39Z",
        "updatedAt" : "2020-03-01T05:24:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "a32304876ba552d31b8562d9562e0f0772851563",
    "line" : 33,
    "diffHunk" : "@@ -1,1 +742,746 @@  val ORC_FILTER_PUSHDOWN_ENABLED = buildConf(\"spark.sql.orc.filterPushdown\")\n    .doc(\"When true, enable filter pushdown for ORC files.\")\n    .version(\"1.4.0\")\n    .booleanConf\n    .createWithDefault(true)"
  },
  {
    "id" : "f5458471-b08f-4ca4-bcf4-93bc7c6d3b09",
    "prId" : 27730,
    "prUrl" : "https://github.com/apache/spark/pull/27730#pullrequestreview-366193427",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1a3c9c19-4818-49fc-b14e-11f4a0a3dac0",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-11412, commit ID: 73183b3c8c2022846587f08e8dea5c387ed3b8d5#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-02-28T07:04:03Z",
        "updatedAt" : "2020-03-01T05:24:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "a32304876ba552d31b8562d9562e0f0772851563",
    "line" : 40,
    "diffHunk" : "@@ -1,1 +749,753 @@    .doc(\"When true, the Orc data source merges schemas collected from all data files, \" +\n      \"otherwise the schema is picked from a random data file.\")\n    .version(\"3.0.0\")\n    .booleanConf\n    .createWithDefault(false)"
  },
  {
    "id" : "84a5de9b-cd05-4551-bff6-4b397212149a",
    "prId" : 27730,
    "prUrl" : "https://github.com/apache/spark/pull/27730#pullrequestreview-366193556",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cbd35547-3343-4698-acb4-3fba4f72b0f3",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "Spark-5068, commit ID: 1f39a61118184e136f38381a9f3ba0b2d5d589d9#diff-41ef65b9ef5b518f77e2a03559893f4d",
        "createdAt" : "2020-02-28T07:04:27Z",
        "updatedAt" : "2020-03-01T05:24:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "a32304876ba552d31b8562d9562e0f0772851563",
    "line" : 48,
    "diffHunk" : "@@ -1,1 +757,761 @@         \"when reading data stored in HDFS. This configuration will be deprecated in the future \" +\n         s\"releases and replaced by ${SPARK_IGNORE_MISSING_FILES.key}.\")\n    .version(\"1.4.0\")\n    .booleanConf\n    .createWithDefault(false)"
  },
  {
    "id" : "0c94c59e-fec1-4074-a516-aeba27e41991",
    "prId" : 27730,
    "prUrl" : "https://github.com/apache/spark/pull/27730#pullrequestreview-366193717",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "eb548ec9-fa69-4770-abf4-968249bd5542",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-9386, commit ID: ce89ff477aea6def68265ed218f6105680755c9a#diff-41ef65b9ef5b518f77e2a03559893f4d",
        "createdAt" : "2020-02-28T07:04:59Z",
        "updatedAt" : "2020-03-01T05:24:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "a32304876ba552d31b8562d9562e0f0772851563",
    "line" : 56,
    "diffHunk" : "@@ -1,1 +767,771 @@           \"not converted to filesource relations (see HiveUtils.CONVERT_METASTORE_PARQUET and \" +\n           \"HiveUtils.CONVERT_METASTORE_ORC for more information).\")\n      .version(\"1.5.0\")\n      .booleanConf\n      .createWithDefault(true)"
  },
  {
    "id" : "57e48cb9-a3c3-47c2-81ce-82faf2f87638",
    "prId" : 27730,
    "prUrl" : "https://github.com/apache/spark/pull/27730#pullrequestreview-366193905",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b640ff24-543f-4419-a4ca-67061adc7fb6",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-19944, commit ID: 80ebca62cbdb7d5c8606e95a944164ab1a943694#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-02-28T07:05:33Z",
        "updatedAt" : "2020-03-01T05:24:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "a32304876ba552d31b8562d9562e0f0772851563",
    "line" : 64,
    "diffHunk" : "@@ -1,1 +777,781 @@           \"is enabled, datasource tables store partition in the Hive metastore, and use the \" +\n           \"metastore to prune partitions during query planning.\")\n      .version(\"2.1.1\")\n      .booleanConf\n      .createWithDefault(true)"
  },
  {
    "id" : "af784f46-c4aa-4b0a-891b-99bf2b258487",
    "prId" : 27730,
    "prUrl" : "https://github.com/apache/spark/pull/27730#pullrequestreview-366194015",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b65e20a9-e245-4408-ad2e-bdfde13585d4",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-19944, commit ID: 80ebca62cbdb7d5c8606e95a944164ab1a943694#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-02-28T07:05:57Z",
        "updatedAt" : "2020-03-01T05:24:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "a32304876ba552d31b8562d9562e0f0772851563",
    "line" : 72,
    "diffHunk" : "@@ -1,1 +786,790 @@           \"a cache that can use up to specified num bytes for file metadata. This conf only \" +\n           \"has an effect when hive filesource partition management is enabled.\")\n      .version(\"2.1.1\")\n      .longConf\n      .createWithDefault(250 * 1024 * 1024)"
  },
  {
    "id" : "74db876f-e75f-4e1e-bc1b-2e8c825ef157",
    "prId" : 27730,
    "prUrl" : "https://github.com/apache/spark/pull/27730#pullrequestreview-366194105",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8a118f11-cb89-4be3-96d7-f401b7178004",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-19944, commit ID: 80ebca62cbdb7d5c8606e95a944164ab1a943694#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-02-28T07:06:13Z",
        "updatedAt" : "2020-03-01T05:24:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "a32304876ba552d31b8562d9562e0f0772851563",
    "line" : 80,
    "diffHunk" : "@@ -1,1 +802,806 @@      \"attempt to write it to the table properties) and NEVER_INFER (the default mode-- fallback \" +\n      \"to using the case-insensitive metastore schema instead of inferring).\")\n    .version(\"2.1.1\")\n    .stringConf\n    .transform(_.toUpperCase(Locale.ROOT))"
  },
  {
    "id" : "86c8b818-c4d5-4cf4-a12f-ed361b2eddcf",
    "prId" : 27730,
    "prUrl" : "https://github.com/apache/spark/pull/27730#pullrequestreview-366194148",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c947f6d1-fdac-4852-a0af-d18d23adb332",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-19944, commit ID: 80ebca62cbdb7d5c8606e95a944164ab1a943694#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-02-28T07:06:21Z",
        "updatedAt" : "2020-03-01T05:24:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "a32304876ba552d31b8562d9562e0f0772851563",
    "line" : 88,
    "diffHunk" : "@@ -1,1 +815,819 @@      \"distinct semantics. By default the optimization is disabled, since it may return \" +\n      \"incorrect results when the files are empty.\")\n    .version(\"2.1.1\")\n    .booleanConf\n    .createWithDefault(false)"
  },
  {
    "id" : "0ce3f0f3-d760-4341-8cc8-c295da13e7c0",
    "prId" : 27730,
    "prUrl" : "https://github.com/apache/spark/pull/27730#pullrequestreview-366194292",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2924cfd9-260a-46f9-8945-df4a384b1909",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-3339, commit ID: 1c7f0ab302de9f82b1bd6da852d133823bc67c66#diff-41ef65b9ef5b518f77e2a03559893f4d",
        "createdAt" : "2020-02-28T07:06:51Z",
        "updatedAt" : "2020-03-01T05:24:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "a32304876ba552d31b8562d9562e0f0772851563",
    "line" : 95,
    "diffHunk" : "@@ -1,1 +822,826 @@    .doc(\"The name of internal column for storing raw/un-parsed JSON and CSV records that fail \" +\n      \"to parse.\")\n    .version(\"1.2.0\")\n    .stringConf\n    .createWithDefault(\"_corrupt_record\")"
  },
  {
    "id" : "a6163bb7-db5b-46ec-9413-ba44f0fd61d5",
    "prId" : 27730,
    "prUrl" : "https://github.com/apache/spark/pull/27730#pullrequestreview-366194436",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "66ad2af2-5ec9-474b-8986-43654c03f1f6",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-4269, commit ID: fa66ef6c97e87c9255b67b03836a4ba50598ebae#diff-41ef65b9ef5b518f77e2a03559893f4d",
        "createdAt" : "2020-02-28T07:07:19Z",
        "updatedAt" : "2020-03-01T05:24:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "a32304876ba552d31b8562d9562e0f0772851563",
    "line" : 101,
    "diffHunk" : "@@ -1,1 +828,832 @@  val BROADCAST_TIMEOUT = buildConf(\"spark.sql.broadcastTimeout\")\n    .doc(\"Timeout in seconds for the broadcast wait time in broadcast joins.\")\n    .version(\"1.3.0\")\n    .timeConf(TimeUnit.SECONDS)\n    .createWithDefaultString(s\"${5 * 60}\")"
  },
  {
    "id" : "2583de4b-c7c5-481e-a9a6-6ebbcf458fec",
    "prId" : 27730,
    "prUrl" : "https://github.com/apache/spark/pull/27730#pullrequestreview-366194630",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8439077e-53b1-4863-b271-e8485ff7eacf",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-3025, commit ID: 496f62d9a98067256d8a51fd1e7a485ff6492fa8#diff-41ef65b9ef5b518f77e2a03559893f4d",
        "createdAt" : "2020-02-28T07:07:45Z",
        "updatedAt" : "2020-03-01T05:24:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "a32304876ba552d31b8562d9562e0f0772851563",
    "line" : 108,
    "diffHunk" : "@@ -1,1 +835,839 @@  val THRIFTSERVER_POOL = buildConf(\"spark.sql.thriftserver.scheduler.pool\")\n    .doc(\"Set a Fair Scheduler pool for a JDBC client session.\")\n    .version(\"1.1.1\")\n    .stringConf\n    .createOptional"
  },
  {
    "id" : "cc3683ad-66e6-4c85-89c7-ef3e2584949b",
    "prId" : 27730,
    "prUrl" : "https://github.com/apache/spark/pull/27730#pullrequestreview-366194775",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "16e42c7d-6834-43bd-8227-d6e29a7442f0",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-18857, commit ID: c94288b57b5ce2232e58e35cada558d8d5b8ec6e#diff-32bb9518401c0948c5ea19377b5069ab",
        "createdAt" : "2020-02-28T07:08:10Z",
        "updatedAt" : "2020-03-01T05:24:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "a32304876ba552d31b8562d9562e0f0772851563",
    "line" : 116,
    "diffHunk" : "@@ -1,1 +843,847 @@      .internal()\n      .doc(\"When true, enable incremental collection for execution in Thrift Server.\")\n      .version(\"2.0.3\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "41c00453-2227-4b72-8ddb-d37c2aa0c796",
    "prId" : 27730,
    "prUrl" : "https://github.com/apache/spark/pull/27730#pullrequestreview-366194950",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8bfb400d-0bc9-4580-a7a2-04a4d9a62943",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-5100, commit ID: 343d3bfafd449a0371feb6a88f78e07302fa7143#diff-41ef65b9ef5b518f77e2a03559893f4d",
        "createdAt" : "2020-02-28T07:08:39Z",
        "updatedAt" : "2020-03-01T05:24:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "a32304876ba552d31b8562d9562e0f0772851563",
    "line" : 123,
    "diffHunk" : "@@ -1,1 +850,854 @@    buildConf(\"spark.sql.thriftserver.ui.retainedStatements\")\n      .doc(\"The number of SQL statements kept in the JDBC/ODBC web UI history.\")\n      .version(\"1.4.0\")\n      .intConf\n      .createWithDefault(200)"
  },
  {
    "id" : "d5ed4e34-6379-40f0-a943-c56bde414f55",
    "prId" : 27730,
    "prUrl" : "https://github.com/apache/spark/pull/27730#pullrequestreview-366195019",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d02a9771-78f8-4fbf-8568-8db53b35a279",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-5100, commit ID: 343d3bfafd449a0371feb6a88f78e07302fa7143#diff-41ef65b9ef5b518f77e2a03559893f4d",
        "createdAt" : "2020-02-28T07:08:50Z",
        "updatedAt" : "2020-03-01T05:24:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "a32304876ba552d31b8562d9562e0f0772851563",
    "line" : 129,
    "diffHunk" : "@@ -1,1 +856,860 @@  val THRIFTSERVER_UI_SESSION_LIMIT = buildConf(\"spark.sql.thriftserver.ui.retainedSessions\")\n    .doc(\"The number of SQL client sessions kept in the JDBC/ODBC web UI history.\")\n    .version(\"1.4.0\")\n    .intConf\n    .createWithDefault(200)"
  },
  {
    "id" : "4dcca33a-7b2a-48e0-8f0b-c80e5b5accc6",
    "prId" : 27730,
    "prUrl" : "https://github.com/apache/spark/pull/27730#pullrequestreview-366195173",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a20a21ec-88e2-4f09-9285-40d68ed24411",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-5658, commit ID: a21090ebe1ef7a709709300712de7d928a923244#diff-41ef65b9ef5b518f77e2a03559893f4d",
        "createdAt" : "2020-02-28T07:09:17Z",
        "updatedAt" : "2020-03-01T05:24:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "a32304876ba552d31b8562d9562e0f0772851563",
    "line" : 136,
    "diffHunk" : "@@ -1,1 +863,867 @@  val DEFAULT_DATA_SOURCE_NAME = buildConf(\"spark.sql.sources.default\")\n    .doc(\"The default data source to use in input/output.\")\n    .version(\"1.3.0\")\n    .stringConf\n    .createWithDefault(\"parquet\")"
  },
  {
    "id" : "d0e62d6b-61f3-44b7-bbf8-67aa67d6a000",
    "prId" : 27730,
    "prUrl" : "https://github.com/apache/spark/pull/27730#pullrequestreview-366195337",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cadfd9c2-5b23-41c4-9d05-741e42febe05",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-15646, commit ID: 5a835b99f9852b0c2a35f9c75a51d493474994ea#diff-32bb9518401c0948c5ea19377b5069ab",
        "createdAt" : "2020-02-28T07:09:43Z",
        "updatedAt" : "2020-03-01T05:24:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "a32304876ba552d31b8562d9562e0f0772851563",
    "line" : 144,
    "diffHunk" : "@@ -1,1 +872,876 @@      \"without specifying any storage property will be converted to a data source table, \" +\n      s\"using the data source set by ${DEFAULT_DATA_SOURCE_NAME.key}.\")\n    .version(\"2.0.0\")\n    .booleanConf\n    .createWithDefault(false)"
  },
  {
    "id" : "5a2ffcf6-405c-4b68-a851-36b53c37ae91",
    "prId" : 27730,
    "prUrl" : "https://github.com/apache/spark/pull/27730#pullrequestreview-366195483",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c8d68c20-3bcc-4b88-980b-e57ea03f6ab9",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-17063, commit ID: 3d283f6c9d9daef53fa4e90b0ead2a94710a37a7#diff-32bb9518401c0948c5ea19377b5069ab",
        "createdAt" : "2020-02-28T07:10:10Z",
        "updatedAt" : "2020-03-01T05:24:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "a32304876ba552d31b8562d9562e0f0772851563",
    "line" : 152,
    "diffHunk" : "@@ -1,1 +881,885 @@        \" in parallel while repairing table partitions to avoid the sequential listing in Hive\" +\n        \" metastore.\")\n      .version(\"2.0.1\")\n      .booleanConf\n      .createWithDefault(true)"
  },
  {
    "id" : "ff21691e-143b-4e44-bd32-05a06f0d9ea9",
    "prId" : 27730,
    "prUrl" : "https://github.com/apache/spark/pull/27730#pullrequestreview-366195626",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "255e678a-fe9b-479d-aa31-99cadfdd28c2",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-7939, commit ID: 03ef6be9ce61a13dcd9d8c71298fb4be39119411#diff-41ef65b9ef5b518f77e2a03559893f4d",
        "createdAt" : "2020-02-28T07:10:34Z",
        "updatedAt" : "2020-03-01T05:24:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "a32304876ba552d31b8562d9562e0f0772851563",
    "line" : 159,
    "diffHunk" : "@@ -1,1 +888,892 @@    buildConf(\"spark.sql.sources.partitionColumnTypeInference.enabled\")\n      .doc(\"When true, automatically infer the data types for partitioned columns.\")\n      .version(\"1.5.0\")\n      .booleanConf\n      .createWithDefault(true)"
  },
  {
    "id" : "b039f1e3-9e52-496c-82a3-594cc7ca694f",
    "prId" : 27730,
    "prUrl" : "https://github.com/apache/spark/pull/27730#pullrequestreview-366195786",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "18bf4b2b-9139-4779-838b-f911772b49ec",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-13486, commit ID: 2b2c8c33236677c916541f956f7b94bba014a9ce#diff-32bb9518401c0948c5ea19377b5069ab",
        "createdAt" : "2020-02-28T07:11:00Z",
        "updatedAt" : "2020-03-01T05:24:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "a32304876ba552d31b8562d9562e0f0772851563",
    "line" : 165,
    "diffHunk" : "@@ -1,1 +894,898 @@  val BUCKETING_ENABLED = buildConf(\"spark.sql.sources.bucketing.enabled\")\n    .doc(\"When false, we will treat bucketed table as normal table\")\n    .version(\"2.0.0\")\n    .booleanConf\n    .createWithDefault(true)"
  },
  {
    "id" : "969a9ff9-c8f9-4ce5-9fd0-207a5aa6687f",
    "prId" : 27730,
    "prUrl" : "https://github.com/apache/spark/pull/27730#pullrequestreview-366195920",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "298fb259-1e45-42bf-aa47-fec6c53b9015",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-23997, commit ID: de46df549acee7fda56bb0871f444d2f3b49e582#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-02-28T07:11:25Z",
        "updatedAt" : "2020-03-01T05:24:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "a32304876ba552d31b8562d9562e0f0772851563",
    "line" : 171,
    "diffHunk" : "@@ -1,1 +900,904 @@  val BUCKETING_MAX_BUCKETS = buildConf(\"spark.sql.sources.bucketing.maxBuckets\")\n    .doc(\"The maximum number of buckets allowed.\")\n    .version(\"2.4.0\")\n    .intConf\n    .checkValue(_ > 0, \"the value of spark.sql.sources.bucketing.maxBuckets must be greater than 0\")"
  },
  {
    "id" : "614eec16-a438-4ecd-b61a-97720578ce3e",
    "prId" : 27730,
    "prUrl" : "https://github.com/apache/spark/pull/27730#pullrequestreview-366196063",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a1ba92e5-721d-4ea0-bd56-bd57f6621e40",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-15425, commit ID: 4462da7071462084c5b55cc414c7faa0e1396a18#diff-32bb9518401c0948c5ea19377b5069ab",
        "createdAt" : "2020-02-28T07:11:48Z",
        "updatedAt" : "2020-03-01T05:24:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "a32304876ba552d31b8562d9562e0f0772851563",
    "line" : 179,
    "diffHunk" : "@@ -1,1 +909,913 @@    .doc(\"When false, we will throw an error if a query contains a cartesian product without \" +\n        \"explicit CROSS JOIN syntax.\")\n    .version(\"2.0.0\")\n    .booleanConf\n    .createWithDefault(true)"
  },
  {
    "id" : "0acb75d9-4bcb-4550-9196-275a526327a3",
    "prId" : 27730,
    "prUrl" : "https://github.com/apache/spark/pull/27730#pullrequestreview-366196179",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e0caf3cd-e1a6-454c-9816-10a74fe4b740",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-12789, commit ID: 2c5b18fb0fdeabd378dd97e91f72d1eac4e21cc7#diff-32bb9518401c0948c5ea19377b5069ab",
        "createdAt" : "2020-02-28T07:12:12Z",
        "updatedAt" : "2020-03-01T05:24:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "a32304876ba552d31b8562d9562e0f0772851563",
    "line" : 186,
    "diffHunk" : "@@ -1,1 +916,920 @@    .doc(\"When true, the ordinal numbers are treated as the position in the select list. \" +\n         \"When false, the ordinal numbers in order/sort by clause are ignored.\")\n    .version(\"2.0.0\")\n    .booleanConf\n    .createWithDefault(true)"
  },
  {
    "id" : "489c0aa6-2410-4b16-89e4-0065079ba28d",
    "prId" : 27730,
    "prUrl" : "https://github.com/apache/spark/pull/27730#pullrequestreview-366196326",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "aa752a5c-6018-4276-bbff-5c462ac5c0c4",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-13957, commit ID: 05f652d6c2bbd764a1dd5a45301811e14519486f#diff-32bb9518401c0948c5ea19377b5069ab",
        "createdAt" : "2020-02-28T07:12:38Z",
        "updatedAt" : "2020-03-01T05:24:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "a32304876ba552d31b8562d9562e0f0772851563",
    "line" : 193,
    "diffHunk" : "@@ -1,1 +923,927 @@    .doc(\"When true, the ordinal numbers in group by clauses are treated as the position \" +\n      \"in the select list. When false, the ordinal numbers are ignored.\")\n    .version(\"2.0.0\")\n    .booleanConf\n    .createWithDefault(true)"
  },
  {
    "id" : "65d2d711-80e8-41ae-a345-1f032d78af4a",
    "prId" : 27730,
    "prUrl" : "https://github.com/apache/spark/pull/27730#pullrequestreview-366196472",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8c377f11-39df-4ce1-9cc7-ba2279eda3f3",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-14471, commit ID: af3a1411a28796d4d9a100eefb093b1d91532754#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-02-28T07:13:04Z",
        "updatedAt" : "2020-03-01T05:24:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "a32304876ba552d31b8562d9562e0f0772851563",
    "line" : 200,
    "diffHunk" : "@@ -1,1 +930,934 @@    .doc(\"When true, aliases in a select list can be used in group by clauses. When false, \" +\n      \"an analysis exception is thrown in the case.\")\n    .version(\"2.2.0\")\n    .booleanConf\n    .createWithDefault(true)"
  },
  {
    "id" : "0d679ed7-a97c-47d9-a0f5-1554dded75a6",
    "prId" : 27730,
    "prUrl" : "https://github.com/apache/spark/pull/27730#pullrequestreview-366196719",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8f3614f7-8afe-4113-8b32-e1382b7150ea",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-7567, commit ID: a385f4b8dd22e0e056569cffc4fa63047cb7c8f2#diff-41ef65b9ef5b518f77e2a03559893f4d",
        "createdAt" : "2020-02-28T07:13:46Z",
        "updatedAt" : "2020-03-01T05:24:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "a32304876ba552d31b8562d9562e0f0772851563",
    "line" : 207,
    "diffHunk" : "@@ -1,1 +937,941 @@  // subclass of org.apache.hadoop.mapreduce.OutputCommitter.\n  val OUTPUT_COMMITTER_CLASS = buildConf(\"spark.sql.sources.outputCommitterClass\")\n    .version(\"1.4.0\")\n    .internal()\n    .stringConf"
  },
  {
    "id" : "f6387b67-415f-4624-ad4f-2e90cd5f9edc",
    "prId" : 27730,
    "prUrl" : "https://github.com/apache/spark/pull/27730#pullrequestreview-366196855",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4a2ce541-28e0-4b51-90e1-73eac601125f",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-19944, commit ID: 80ebca62cbdb7d5c8606e95a944164ab1a943694#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-02-28T07:14:09Z",
        "updatedAt" : "2020-03-01T05:24:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "a32304876ba552d31b8562d9562e0f0772851563",
    "line" : 214,
    "diffHunk" : "@@ -1,1 +944,948 @@  val FILE_COMMIT_PROTOCOL_CLASS =\n    buildConf(\"spark.sql.sources.commitProtocolClass\")\n      .version(\"2.1.1\")\n      .internal()\n      .stringConf"
  },
  {
    "id" : "d0c23054-6998-43e5-af93-bf6afb1377f0",
    "prId" : 27730,
    "prUrl" : "https://github.com/apache/spark/pull/27730#pullrequestreview-366197000",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a42b90ed-b28e-4f0d-8790-d2dfff46ede4",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-8125, commit ID: a1064df0ee3daf496800be84293345a10e1497d9#diff-41ef65b9ef5b518f77e2a03559893f4d",
        "createdAt" : "2020-02-28T07:14:36Z",
        "updatedAt" : "2020-03-01T05:24:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "a32304876ba552d31b8562d9562e0f0772851563",
    "line" : 222,
    "diffHunk" : "@@ -1,1 +956,960 @@        \"files with another Spark distributed job. This configuration is effective only when \" +\n        \"using file-based sources such as Parquet, JSON and ORC.\")\n      .version(\"1.5.0\")\n      .intConf\n      .checkValue(parallel => parallel >= 0, \"The maximum number of paths allowed for listing \" +"
  },
  {
    "id" : "f29665d1-58c1-41e2-9e82-dfe9854972cf",
    "prId" : 27730,
    "prUrl" : "https://github.com/apache/spark/pull/27730#pullrequestreview-366197223",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0994c710-53f2-4695-8792-e8860fa59314",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-19944, commit ID: 80ebca62cbdb7d5c8606e95a944164ab1a943694#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-02-28T07:15:08Z",
        "updatedAt" : "2020-03-01T05:24:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "a32304876ba552d31b8562d9562e0f0772851563",
    "line" : 230,
    "diffHunk" : "@@ -1,1 +966,970 @@      .doc(\"The number of parallelism to list a collection of path recursively, Set the \" +\n        \"number to prevent file listing from generating too many tasks.\")\n      .version(\"2.1.1\")\n      .internal()\n      .intConf"
  },
  {
    "id" : "6834956a-8856-4e86-821d-4ff0346ba5ac",
    "prId" : 27730,
    "prUrl" : "https://github.com/apache/spark/pull/27730#pullrequestreview-366197383",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a37529f8-47a7-4f53-b9d3-3d1343ca8435",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-30812, commit ID: b76bc0b1b8b2abd00a84f805af90ca4c5925faaa#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-02-28T07:15:33Z",
        "updatedAt" : "2020-03-01T05:24:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "a32304876ba552d31b8562d9562e0f0772851563",
    "line" : 238,
    "diffHunk" : "@@ -1,1 +978,982 @@        \"useful if data is read from a remote cluster so the scheduler could never \" +\n        \"take advantage of locality anyway.\")\n      .version(\"3.0.0\")\n      .internal()\n      .booleanConf"
  },
  {
    "id" : "9419d2a4-2d1c-421f-9b1b-246c8daee2af",
    "prId" : 27730,
    "prUrl" : "https://github.com/apache/spark/pull/27730#pullrequestreview-366197521",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4fd8acf4-cb74-4f27-993b-713f9b69c742",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-6231, commit ID: e61083ccab7764d1929248490a3d2e83987241e0#diff-41ef65b9ef5b518f77e2a03559893f4d",
        "createdAt" : "2020-02-28T07:16:00Z",
        "updatedAt" : "2020-03-01T05:24:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "a32304876ba552d31b8562d9562e0f0772851563",
    "line" : 246,
    "diffHunk" : "@@ -1,1 +987,991 @@  val DATAFRAME_SELF_JOIN_AUTO_RESOLVE_AMBIGUITY =\n    buildConf(\"spark.sql.selfJoinAutoResolveAmbiguity\")\n      .version(\"1.4.0\")\n      .internal()\n      .booleanConf"
  },
  {
    "id" : "5d29c9b6-c016-436d-bdd3-42d5519a1bb2",
    "prId" : 27730,
    "prUrl" : "https://github.com/apache/spark/pull/27730#pullrequestreview-366197642",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4a815d4c-713c-431a-9294-f2db58fc7855",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-30812, commit ID: b76bc0b1b8b2abd00a84f805af90ca4c5925faaa#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-02-28T07:16:22Z",
        "updatedAt" : "2020-03-01T05:24:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "a32304876ba552d31b8562d9562e0f0772851563",
    "line" : 254,
    "diffHunk" : "@@ -1,1 +995,999 @@    buildConf(\"spark.sql.analyzer.failAmbiguousSelfJoin\")\n      .doc(\"When true, fail the Dataset query if it contains ambiguous self-join.\")\n      .version(\"3.0.0\")\n      .internal()\n      .booleanConf"
  },
  {
    "id" : "fe4ba039-adbf-4811-a4f9-83f56d827a83",
    "prId" : 27730,
    "prUrl" : "https://github.com/apache/spark/pull/27730#pullrequestreview-366197774",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "238ecda9-c71e-4ce5-a128-4b9387ff743c",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-7462, commit ID: 9c35f02b35fda80d6558573466735e79b3dd9124#diff-41ef65b9ef5b518f77e2a03559893f4d",
        "createdAt" : "2020-02-28T07:16:48Z",
        "updatedAt" : "2020-03-01T05:24:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "a32304876ba552d31b8562d9562e0f0772851563",
    "line" : 261,
    "diffHunk" : "@@ -1,1 +1002,1006 @@  // Whether to retain group by columns or not in GroupedData.agg.\n  val DATAFRAME_RETAIN_GROUP_COLUMNS = buildConf(\"spark.sql.retainGroupColumns\")\n    .version(\"1.4.0\")\n    .internal()\n    .booleanConf"
  },
  {
    "id" : "add0a9cb-c9d9-42c2-a92f-8ef308d7fb40",
    "prId" : 27730,
    "prUrl" : "https://github.com/apache/spark/pull/27730#pullrequestreview-366197966",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ba22e9ac-6817-4bd3-9c6c-035771d97a1c",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-8992, commit ID: 5940fc71d2a245cc6e50edb455c3dd3dbb8de43a#diff-41ef65b9ef5b518f77e2a03559893f4d",
        "createdAt" : "2020-02-28T07:17:25Z",
        "updatedAt" : "2020-03-01T05:24:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "a32304876ba552d31b8562d9562e0f0772851563",
    "line" : 269,
    "diffHunk" : "@@ -1,1 +1010,1014 @@    .doc(\"When doing a pivot without specifying values for the pivot column this is the maximum \" +\n      \"number of (distinct) values that will be collected without error.\")\n    .version(\"1.6.0\")\n    .intConf\n    .createWithDefault(10000)"
  },
  {
    "id" : "8d258ea6-bc9d-4b92-a17c-4d190b0a7210",
    "prId" : 27730,
    "prUrl" : "https://github.com/apache/spark/pull/27730#pullrequestreview-366198087",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1839e44c-a29d-4d6a-9932-8c01def77544",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-11197, commit ID: f8c6bec65784de89b47e96a367d3f9790c1b3115#diff-41ef65b9ef5b518f77e2a03559893f4d",
        "createdAt" : "2020-02-28T07:17:48Z",
        "updatedAt" : "2020-03-01T05:24:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "a32304876ba552d31b8562d9562e0f0772851563",
    "line" : 276,
    "diffHunk" : "@@ -1,1 +1017,1021 @@    .internal()\n    .doc(\"When true, we could use `datasource`.`path` as table in SQL query.\")\n    .version(\"1.6.0\")\n    .booleanConf\n    .createWithDefault(true)"
  },
  {
    "id" : "f22ab937-cfd8-42a1-ad6f-26c77a878e54",
    "prId" : 27730,
    "prUrl" : "https://github.com/apache/spark/pull/27730#pullrequestreview-366198234",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6fceb728-8858-4a6c-b867-bea9447d4ab7",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-13486, commit ID: 2b2c8c33236677c916541f956f7b94bba014a9ce#diff-32bb9518401c0948c5ea19377b5069ab",
        "createdAt" : "2020-02-28T07:18:16Z",
        "updatedAt" : "2020-03-01T05:24:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "a32304876ba552d31b8562d9562e0f0772851563",
    "line" : 284,
    "diffHunk" : "@@ -1,1 +1025,1029 @@    .doc(\"When true, the whole stage (of multiple operators) will be compiled into single java\" +\n      \" method.\")\n    .version(\"2.0.0\")\n    .booleanConf\n    .createWithDefault(true)"
  },
  {
    "id" : "1b9660de-9008-4e10-903d-788dcd5bd07b",
    "prId" : 27730,
    "prUrl" : "https://github.com/apache/spark/pull/27730#pullrequestreview-366198408",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9992b296-7ac1-42a4-979c-7cb44f06aeda",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-23032, commit ID: 26a8b4e398ee6d1de06a5f3ac1d6d342c9b67d78#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-02-28T07:18:41Z",
        "updatedAt" : "2020-03-01T05:24:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "a32304876ba552d31b8562d9562e0f0772851563",
    "line" : 292,
    "diffHunk" : "@@ -1,1 +1034,1038 @@    .doc(\"When true, embed the (whole-stage) codegen stage ID into \" +\n      \"the class name of the generated class as a suffix\")\n    .version(\"2.3.1\")\n    .booleanConf\n    .createWithDefault(true)"
  },
  {
    "id" : "58927edf-49ab-4577-b058-788399a8105d",
    "prId" : 27730,
    "prUrl" : "https://github.com/apache/spark/pull/27730#pullrequestreview-366198651",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "941f330e-f487-4c26-9f7a-5b8f3f912cf6",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-14224 and SPARK-14223 and SPARK-14310, commit ID: 5a4b11a901703464b9261dea0642d80cf8d4856c#diff-32bb9518401c0948c5ea19377b5069ab",
        "createdAt" : "2020-02-28T07:19:27Z",
        "updatedAt" : "2020-03-01T05:24:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "a32304876ba552d31b8562d9562e0f0772851563",
    "line" : 300,
    "diffHunk" : "@@ -1,1 +1042,1046 @@    .doc(\"The maximum number of fields (including nested fields) that will be supported before\" +\n      \" deactivating whole-stage codegen.\")\n    .version(\"2.0.0\")\n    .intConf\n    .createWithDefault(100)"
  },
  {
    "id" : "646b9e9f-0c9e-4e53-a26f-69535ba9e2c4",
    "prId" : 27730,
    "prUrl" : "https://github.com/apache/spark/pull/27730#pullrequestreview-366198828",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "28b111f7-9e0e-47f0-8459-dac926c3167c",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-23711, commit ID: a40ffc656d62372da85e0fa932b67207839e7fde#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-02-28T07:19:53Z",
        "updatedAt" : "2020-03-01T05:24:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "a32304876ba552d31b8562d9562e0f0772851563",
    "line" : 308,
    "diffHunk" : "@@ -1,1 +1052,1056 @@      \"`NO_CODEGEN` skips codegen and goes interpreted path always. Note that \" +\n      \"this config works only for tests.\")\n    .version(\"2.4.0\")\n    .internal()\n    .stringConf"
  },
  {
    "id" : "a85448c4-e451-4f4d-9a6e-0b1e9d5d1848",
    "prId" : 27730,
    "prUrl" : "https://github.com/apache/spark/pull/27730#pullrequestreview-366198950",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dcde5a26-752a-4e0d-8890-4ffc6ef41e2c",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-15759, commit ID: f0fa0a8946fb4bdf0f4697a8e389f49e98422871#diff-32bb9518401c0948c5ea19377b5069ab",
        "createdAt" : "2020-02-28T07:20:14Z",
        "updatedAt" : "2020-03-01T05:24:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "a32304876ba552d31b8562d9562e0f0772851563",
    "line" : 316,
    "diffHunk" : "@@ -1,1 +1062,1066 @@    .doc(\"When true, (whole stage) codegen could be temporary disabled for the part of query that\" +\n      \" fail to compile generated code\")\n    .version(\"2.0.0\")\n    .booleanConf\n    .createWithDefault(true)"
  },
  {
    "id" : "75631568-950e-4d89-800f-084487d0ac3b",
    "prId" : 27730,
    "prUrl" : "https://github.com/apache/spark/pull/27730#pullrequestreview-366199129",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "af5b0aed-ecaa-4c2a-beb5-8daca4ab246e",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-20871, commit ID: 2a53fbfce72b3faef020e39a1e8628d68bc95beb#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-02-28T07:20:44Z",
        "updatedAt" : "2020-03-01T05:24:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "a32304876ba552d31b8562d9562e0f0772851563",
    "line" : 323,
    "diffHunk" : "@@ -1,1 +1069,1073 @@    .internal()\n    .doc(\"The maximum number of codegen lines to log when errors occur. Use -1 for unlimited.\")\n    .version(\"2.3.0\")\n    .intConf\n    .checkValue(maxLines => maxLines >= -1, \"The maximum must be a positive integer, 0 to \" +"
  },
  {
    "id" : "09be59fb-cc3c-477a-945f-a080e0a5891f",
    "prId" : 27730,
    "prUrl" : "https://github.com/apache/spark/pull/27730#pullrequestreview-366199282",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9eb1fa9a-18e0-4f4d-bca4-a72d18b0ac21",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-21871, commit ID: 4a779bdac3e75c17b7d36c5a009ba6c948fa9fb6#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-02-28T07:21:09Z",
        "updatedAt" : "2020-03-01T05:24:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "a32304876ba552d31b8562d9562e0f0772851563",
    "line" : 331,
    "diffHunk" : "@@ -1,1 +1083,1087 @@      s\"it may be preferable to set the value to ${CodeGenerator.DEFAULT_JVM_HUGE_METHOD_LIMIT} \" +\n      \"to match HotSpot's implementation.\")\n    .version(\"2.3.0\")\n    .intConf\n    .createWithDefault(65535)"
  },
  {
    "id" : "fa3bd5c9-afa8-498b-abd0-5e3f8c6a8ed7",
    "prId" : 27730,
    "prUrl" : "https://github.com/apache/spark/pull/27730#pullrequestreview-366199473",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "676b840c-1ac6-42f2-8400-43e546d74bd0",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-25850, commit ID: e017cb39642a5039abd8ce8127ad41712901bdbc#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-02-28T07:21:39Z",
        "updatedAt" : "2020-03-01T05:24:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "a32304876ba552d31b8562d9562e0f0772851563",
    "line" : 339,
    "diffHunk" : "@@ -1,1 +1095,1099 @@      \"bytecode should not go beyond 8KB, otherwise it will not be JITted; it also should not \" +\n      \"be too small, otherwise there will be many function calls.\")\n    .version(\"3.0.0\")\n    .intConf\n    .checkValue(threshold => threshold > 0, \"The threshold must be a positive integer.\")"
  },
  {
    "id" : "5f58cd77-813b-4b5a-8aa2-2e31fb3c2a43",
    "prId" : 27730,
    "prUrl" : "https://github.com/apache/spark/pull/27730#pullrequestreview-366199597",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "354b78dd-73a3-414f-b692-edfb101a6d6a",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-21717, commit ID: c79e771f8952e6773c3a84cc617145216feddbcf#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-02-28T07:22:03Z",
        "updatedAt" : "2020-03-01T05:24:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "a32304876ba552d31b8562d9562e0f0772851563",
    "line" : 347,
    "diffHunk" : "@@ -1,1 +1106,1110 @@        \"physical operator into individual methods, instead of a single big method. This can be \" +\n        \"used to avoid oversized function that can miss the opportunity of JIT optimization.\")\n      .version(\"2.3.1\")\n      .booleanConf\n      .createWithDefault(true)"
  },
  {
    "id" : "6aabb76f-6d22-46a0-91f8-a780fdb1eafc",
    "prId" : 27730,
    "prUrl" : "https://github.com/apache/spark/pull/27730#pullrequestreview-366199755",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4170a09b-cf24-46ab-9757-2ffe4f019032",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-13664, commit ID: 17eec0a71ba8713c559d641e3f43a1be726b037c#diff-32bb9518401c0948c5ea19377b5069ab",
        "createdAt" : "2020-02-28T07:22:28Z",
        "updatedAt" : "2020-03-01T05:24:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "a32304876ba552d31b8562d9562e0f0772851563",
    "line" : 355,
    "diffHunk" : "@@ -1,1 +1114,1118 @@      \"This configuration is effective only when using file-based sources such as Parquet, JSON \" +\n      \"and ORC.\")\n    .version(\"2.0.0\")\n    .bytesConf(ByteUnit.BYTE)\n    .createWithDefaultString(\"128MB\") // parquet.block.size"
  },
  {
    "id" : "b050898d-f5fc-4173-8bc7-c0e6ebae8a94",
    "prId" : 27730,
    "prUrl" : "https://github.com/apache/spark/pull/27730#pullrequestreview-366199928",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "82bfd227-10ff-4ce6-bd89-5211b574612e",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-14259, commit ID: 400b2f863ffaa01a34a8dae1541c61526fef908b#diff-32bb9518401c0948c5ea19377b5069ab",
        "createdAt" : "2020-02-28T07:22:57Z",
        "updatedAt" : "2020-03-01T05:24:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "a32304876ba552d31b8562d9562e0f0772851563",
    "line" : 363,
    "diffHunk" : "@@ -1,1 +1125,1129 @@      \" bigger files (which is scheduled first). This configuration is effective only when using\" +\n      \" file-based sources such as Parquet, JSON and ORC.\")\n    .version(\"2.0.0\")\n    .longConf\n    .createWithDefault(4 * 1024 * 1024)"
  },
  {
    "id" : "69cacac6-37fc-455a-9178-1f2e9f52cc67",
    "prId" : 27730,
    "prUrl" : "https://github.com/apache/spark/pull/27730#pullrequestreview-366200091",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d1681575-3f74-4881-8d0b-577247934d5c",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-19944, commit ID: 80ebca62cbdb7d5c8606e95a944164ab1a943694#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-02-28T07:23:22Z",
        "updatedAt" : "2020-03-01T05:24:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "a32304876ba552d31b8562d9562e0f0772851563",
    "line" : 371,
    "diffHunk" : "@@ -1,1 +1134,1138 @@      \"This configuration is effective only when using file-based sources such as Parquet, JSON \" +\n      \"and ORC.\")\n    .version(\"2.1.1\")\n    .booleanConf\n    .createWithDefault(false)"
  },
  {
    "id" : "d3240fed-4617-481d-b3a3-5948889612a4",
    "prId" : 27730,
    "prUrl" : "https://github.com/apache/spark/pull/27730#pullrequestreview-366200404",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0a7096d8-4c15-460e-b986-5549017c9157",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-22366, commit ID: 8e9863531bebbd4d83eafcbc2b359b8bd0ac5734#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-02-28T07:24:13Z",
        "updatedAt" : "2020-03-01T05:24:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "a32304876ba552d31b8562d9562e0f0772851563",
    "line" : 379,
    "diffHunk" : "@@ -1,1 +1143,1147 @@      \"This configuration is effective only when using file-based sources such as Parquet, JSON \" +\n      \"and ORC.\")\n    .version(\"2.3.0\")\n    .booleanConf\n    .createWithDefault(false)"
  },
  {
    "id" : "4e9b9eb1-b45a-4fe3-a3b0-5615105e40bf",
    "prId" : 27730,
    "prUrl" : "https://github.com/apache/spark/pull/27730#pullrequestreview-366200696",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "364e6786-7485-4b78-bbba-9507854aefe6",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-19944, commit ID: 0ee38a39e43dd7ad9d50457e446ae36f64621a1b#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-02-28T07:25:03Z",
        "updatedAt" : "2020-03-01T05:24:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "a32304876ba552d31b8562d9562e0f0772851563",
    "line" : 386,
    "diffHunk" : "@@ -1,1 +1150,1154 @@    .doc(\"Maximum number of records to write out to a single file. \" +\n      \"If this value is zero or negative, there is no limit.\")\n    .version(\"2.2.0\")\n    .longConf\n    .createWithDefault(0)"
  },
  {
    "id" : "dc6462e3-8ac2-48ec-b8ee-1872892d9f0b",
    "prId" : 27730,
    "prUrl" : "https://github.com/apache/spark/pull/27730#pullrequestreview-366200838",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1bc3d82e-a438-4b45-84f1-cdd4069ca16a",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-13523, commit ID: 3dc9ae2e158e5b51df6f799767946fe1d190156b#diff-32bb9518401c0948c5ea19377b5069ab",
        "createdAt" : "2020-02-28T07:25:28Z",
        "updatedAt" : "2020-03-01T05:24:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "a32304876ba552d31b8562d9562e0f0772851563",
    "line" : 393,
    "diffHunk" : "@@ -1,1 +1157,1161 @@    .internal()\n    .doc(\"When true, the planner will try to find out duplicated exchanges and re-use them.\")\n    .version(\"2.0.0\")\n    .booleanConf\n    .createWithDefault(true)"
  },
  {
    "id" : "958656b6-8352-483e-820e-375a46177ba3",
    "prId" : 27730,
    "prUrl" : "https://github.com/apache/spark/pull/27730#pullrequestreview-366201006",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "19d4edc9-cb5c-4571-bc1c-11e5b77a34b4",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-30812, commit ID: b76bc0b1b8b2abd00a84f805af90ca4c5925faaa#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-02-28T07:25:58Z",
        "updatedAt" : "2020-03-01T05:24:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "a32304876ba552d31b8562d9562e0f0772851563",
    "line" : 400,
    "diffHunk" : "@@ -1,1 +1164,1168 @@    .internal()\n    .doc(\"When true, the planner will try to find out duplicated subqueries and re-use them.\")\n    .version(\"3.0.0\")\n    .booleanConf\n    .createWithDefault(true)"
  },
  {
    "id" : "7c5f6657-6f8b-4e4a-9ab0-904bc2c5de79",
    "prId" : 27730,
    "prUrl" : "https://github.com/apache/spark/pull/27730#pullrequestreview-366201174",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cdf27e0b-df96-4d88-8102-b236476ffd81",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-20883 and SPARK-20376, commit ID: fa757ee1d41396ad8734a3f2dd045bb09bc82a2e#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-02-28T07:26:28Z",
        "updatedAt" : "2020-03-01T05:24:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "a32304876ba552d31b8562d9562e0f0772851563",
    "line" : 408,
    "diffHunk" : "@@ -1,1 +1176,1180 @@          \"Note: For structured streaming, this configuration cannot be changed between query \" +\n          \"restarts from the same checkpoint location.\")\n      .version(\"2.3.0\")\n      .stringConf\n      .createWithDefault("
  },
  {
    "id" : "6400a0a2-a6b2-42d1-9543-ed4e5d77f72a",
    "prId" : 27730,
    "prUrl" : "https://github.com/apache/spark/pull/27730#pullrequestreview-366201330",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "11a01ffc-4821-46f4-a911-252bf575ca62",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-13809, commit ID: 8c826880f5eaa3221c4e9e7d3fece54e821a0b98#diff-32bb9518401c0948c5ea19377b5069ab",
        "createdAt" : "2020-02-28T07:26:51Z",
        "updatedAt" : "2020-03-01T05:24:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "a32304876ba552d31b8562d9562e0f0772851563",
    "line" : 416,
    "diffHunk" : "@@ -1,1 +1186,1190 @@      .doc(\"Minimum number of state store delta files that needs to be generated before they \" +\n        \"consolidated into snapshots.\")\n      .version(\"2.0.0\")\n      .intConf\n      .createWithDefault(10)"
  },
  {
    "id" : "9ab5c4a1-6c44-4854-b242-bf7805f0647e",
    "prId" : 27730,
    "prUrl" : "https://github.com/apache/spark/pull/27730#pullrequestreview-366201552",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5a979e16-b9a6-4c8d-8062-1bebf1863a0b",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-22187, commit ID: b3d88ac02940eff4c867d3acb79fe5ff9d724e83#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-02-28T07:27:31Z",
        "updatedAt" : "2020-03-01T05:24:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "a32304876ba552d31b8562d9562e0f0772851563",
    "line" : 424,
    "diffHunk" : "@@ -1,1 +1194,1198 @@      .internal()\n      .doc(\"State format version used by flatMapGroupsWithState operation in a streaming query\")\n      .version(\"2.4.0\")\n      .intConf\n      .checkValue(v => Set(1, 2).contains(v), \"Valid versions are 1 and 2\")"
  },
  {
    "id" : "8afb3190-2f3b-4c45-9065-9b3997f113e3",
    "prId" : 27730,
    "prUrl" : "https://github.com/apache/spark/pull/27730#pullrequestreview-366201784",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5f64e9b2-2f29-4f75-9f67-1b8ec71772c4",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-13985, commit ID: caea15214571d9b12dcf1553e5c1cc8b83a8ba5b#diff-32bb9518401c0948c5ea19377b5069ab",
        "createdAt" : "2020-02-28T07:28:09Z",
        "updatedAt" : "2020-03-01T05:24:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "a32304876ba552d31b8562d9562e0f0772851563",
    "line" : 431,
    "diffHunk" : "@@ -1,1 +1201,1205 @@  val CHECKPOINT_LOCATION = buildConf(\"spark.sql.streaming.checkpointLocation\")\n    .doc(\"The default location for storing checkpoint data for streaming queries.\")\n    .version(\"2.0.0\")\n    .stringConf\n    .createOptional"
  },
  {
    "id" : "e1162107-c28d-4823-8e8b-dc02537ec99d",
    "prId" : 27730,
    "prUrl" : "https://github.com/apache/spark/pull/27730#pullrequestreview-366201932",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6006b133-a41b-45b6-8e6d-e2d9fa764923",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-30812, commit ID: b76bc0b1b8b2abd00a84f805af90ca4c5925faaa#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-02-28T07:28:33Z",
        "updatedAt" : "2020-03-01T05:24:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "a32304876ba552d31b8562d9562e0f0772851563",
    "line" : 438,
    "diffHunk" : "@@ -1,1 +1208,1212 @@    buildConf(\"spark.sql.streaming.forceDeleteTempCheckpointLocation\")\n      .doc(\"When true, enable temporary checkpoint locations force delete.\")\n      .version(\"3.0.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "963c919d-6a25-4de8-ae96-e1bb384ae963",
    "prId" : 27730,
    "prUrl" : "https://github.com/apache/spark/pull/27730#pullrequestreview-366202088",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ab2e479f-9157-4960-bc5b-cfef334b6872",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-19944, commit ID: 80ebca62cbdb7d5c8606e95a944164ab1a943694#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-02-28T07:28:58Z",
        "updatedAt" : "2020-03-01T05:24:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "a32304876ba552d31b8562d9562e0f0772851563",
    "line" : 445,
    "diffHunk" : "@@ -1,1 +1215,1219 @@    .internal()\n    .doc(\"The minimum number of batches that must be retained and made recoverable.\")\n    .version(\"2.1.1\")\n    .intConf\n    .createWithDefault(100)"
  },
  {
    "id" : "6ccf6c02-d2bc-4809-b5f5-f6e29e5f001b",
    "prId" : 27730,
    "prUrl" : "https://github.com/apache/spark/pull/27730#pullrequestreview-366202226",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2d04cad0-4c51-47ff-adbb-2224283af094",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-24717, commit ID: 8b7d4f842fdc90b8d1c37080bdd9b5e1d070f5c0#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-02-28T07:29:20Z",
        "updatedAt" : "2020-03-01T05:24:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "a32304876ba552d31b8562d9562e0f0772851563",
    "line" : 453,
    "diffHunk" : "@@ -1,1 +1225,1229 @@      \"'2' covers both success and direct failure cases, '1' covers only success case, \" +\n      \"and '0' covers extreme case - disable cache to maximize memory size of executors.\")\n    .version(\"2.4.0\")\n    .intConf\n    .createWithDefault(2)"
  },
  {
    "id" : "dd0175c4-96fa-408a-a428-2531db8bdba4",
    "prId" : 27730,
    "prUrl" : "https://github.com/apache/spark/pull/27730#pullrequestreview-366202393",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "df7d5c15-0de3-40f2-879b-213e5c714d39",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-24763, commit ID: 6c5cb85856235efd464b109558896f81ae2c4c75#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-02-28T07:29:47Z",
        "updatedAt" : "2020-03-01T05:24:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "a32304876ba552d31b8562d9562e0f0772851563",
    "line" : 461,
    "diffHunk" : "@@ -1,1 +1235,1239 @@        \"State between versions are tend to be incompatible, so state format version shouldn't \" +\n        \"be modified after running.\")\n      .version(\"2.4.0\")\n      .intConf\n      .checkValue(v => Set(1, 2).contains(v), \"Valid versions are 1 and 2\")"
  },
  {
    "id" : "eba00f1b-f8ea-4234-a81d-100b694c12d5",
    "prId" : 27730,
    "prUrl" : "https://github.com/apache/spark/pull/27730#pullrequestreview-366202532",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bceed37f-a930-4d31-9737-ae7401123ffa",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-29568, commit ID: 363af16c72abe19fc5cc5b5bdf9d8dc34975f2ba#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-02-28T07:30:11Z",
        "updatedAt" : "2020-03-01T05:24:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "a32304876ba552d31b8562d9562e0f0772851563",
    "line" : 469,
    "diffHunk" : "@@ -1,1 +1246,1250 @@      \"SparkSessions on the same cluster) and this flag is true, we will stop the old streaming \" +\n      \"query run to start the new one.\")\n    .version(\"3.0.0\")\n    .booleanConf\n    .createWithDefault(true)"
  },
  {
    "id" : "434f87d6-00de-42d5-912e-913688c0f71f",
    "prId" : 27730,
    "prUrl" : "https://github.com/apache/spark/pull/27730#pullrequestreview-366202726",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ec9b3ff5-48c1-4ae5-9d1f-a9fe697f65fa",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-26154, commit ID: c941362cb94b24bdf48d4928a1a4dff1b13a1484#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-02-28T07:30:39Z",
        "updatedAt" : "2020-03-01T05:24:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "a32304876ba552d31b8562d9562e0f0772851563",
    "line" : 477,
    "diffHunk" : "@@ -1,1 +1256,1260 @@        \"State between versions are tend to be incompatible, so state format version shouldn't \" +\n        \"be modified after running.\")\n      .version(\"3.0.0\")\n      .intConf\n      .checkValue(v => Set(1, 2).contains(v), \"Valid versions are 1 and 2\")"
  },
  {
    "id" : "50fd3a49-cae8-45d2-b874-a2233e92549f",
    "prId" : 27730,
    "prUrl" : "https://github.com/apache/spark/pull/27730#pullrequestreview-366202910",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6eaacaed-d4f4-4989-9431-2b9fb07c4b22",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-14473, commit ID: 775cf17eaaae1a38efe47b282b1d6bbdb99bd759#diff-32bb9518401c0948c5ea19377b5069ab",
        "createdAt" : "2020-02-28T07:31:06Z",
        "updatedAt" : "2020-03-01T05:24:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "a32304876ba552d31b8562d9562e0f0772851563",
    "line" : 485,
    "diffHunk" : "@@ -1,1 +1266,1270 @@      .doc(\"When true, the logical plan for streaming query will be checked for unsupported\" +\n        \" operations.\")\n      .version(\"2.0.0\")\n      .booleanConf\n      .createWithDefault(true)"
  },
  {
    "id" : "8922b3e7-b72e-4c58-8d57-5c13ab11b2c9",
    "prId" : 27730,
    "prUrl" : "https://github.com/apache/spark/pull/27730#pullrequestreview-366203072",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "047aefb9-0bf7-4e96-a321-32ac006f1569",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-14769, commit ID: 334c293ec0bcc2195d502c574ca40dbc4769d666#diff-32bb9518401c0948c5ea19377b5069ab",
        "createdAt" : "2020-02-28T07:31:31Z",
        "updatedAt" : "2020-03-01T05:24:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "a32304876ba552d31b8562d9562e0f0772851563",
    "line" : 493,
    "diffHunk" : "@@ -1,1 +1274,1278 @@      .doc(\"This enables substitution using syntax like `${var}`, `${system:var}`, \" +\n        \"and `${env:var}`.\")\n      .version(\"2.0.0\")\n      .booleanConf\n      .createWithDefault(true)"
  },
  {
    "id" : "fe1526df-59d4-4086-871a-937e8cc5a406",
    "prId" : 27730,
    "prUrl" : "https://github.com/apache/spark/pull/27730#pullrequestreview-366203322",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4ddab5f5-ecce-4c32-bc23-c88e1fbd6589",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-22159, commit ID: d29d1e87995e02cb57ba3026c945c3cd66bb06e2#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-02-28T07:32:05Z",
        "updatedAt" : "2020-03-01T05:24:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "a32304876ba552d31b8562d9562e0f0772851563",
    "line" : 501,
    "diffHunk" : "@@ -1,1 +1285,1289 @@        \"2nd-level, larger, slower map when 1st level is full or keys cannot be found. \" +\n        \"When disabled, records go directly to the 2nd level.\")\n      .version(\"2.3.0\")\n      .booleanConf\n      .createWithDefault(true)"
  },
  {
    "id" : "23a1d018-dead-4cc0-9434-99b3b448ec74",
    "prId" : 27730,
    "prUrl" : "https://github.com/apache/spark/pull/27730#pullrequestreview-366203524",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2e4a1d4a-7599-42bf-89fa-c1700e49273a",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-28257, commit ID: 42b80ae128ab1aa8a87c1376fe88e2cde52e6e4f#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-02-28T07:32:38Z",
        "updatedAt" : "2020-03-01T05:24:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "a32304876ba552d31b8562d9562e0f0772851563",
    "line" : 509,
    "diffHunk" : "@@ -1,1 +1293,1297 @@      .internal()\n      .doc(\"Enable vectorized aggregate hash map. This is for testing/benchmarking only.\")\n      .version(\"3.0.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "2b565392-ad7f-49b1-b75f-36d2ca66bd41",
    "prId" : 27730,
    "prUrl" : "https://github.com/apache/spark/pull/27730#pullrequestreview-366203680",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f0655d93-fea8-4da1-9028-c524efe615b4",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-21870, commit ID: cb0cddffe9452937033e0e6b1fc0e600d2c787ad#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-02-28T07:33:05Z",
        "updatedAt" : "2020-03-01T05:24:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "a32304876ba552d31b8562d9562e0f0772851563",
    "line" : 517,
    "diffHunk" : "@@ -1,1 +1303,1307 @@        \"instead of a single big method. This can be used to avoid oversized function that \" +\n        \"can miss the opportunity of JIT optimization.\")\n      .version(\"3.0.0\")\n      .booleanConf\n      .createWithDefault(true)"
  },
  {
    "id" : "91a356bf-31c6-450a-ad86-641ca65c391e",
    "prId" : 27730,
    "prUrl" : "https://github.com/apache/spark/pull/27730#pullrequestreview-366204015",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d2717211-b980-4f00-a72e-bc3c8e3eaddf",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-19877, commit ID: ee36bc1c9043ead3c3ba4fba7e68c6c47ad7ae7a#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-02-28T07:33:58Z",
        "updatedAt" : "2020-03-01T05:24:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "a32304876ba552d31b8562d9562e0f0772851563",
    "line" : 525,
    "diffHunk" : "@@ -1,1 +1315,1319 @@        \"configuration puts a limit on this: when the depth of a view exceeds this value during \" +\n        \"analysis, we terminate the resolution to avoid potential errors.\")\n      .version(\"2.2.0\")\n      .intConf\n      .checkValue(depth => depth > 0, \"The maximum depth of a view reference in a nested view \" +"
  },
  {
    "id" : "e44f75f3-93be-4190-aff3-2c7403328ff3",
    "prId" : 27730,
    "prUrl" : "https://github.com/apache/spark/pull/27730#pullrequestreview-366204302",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3059bbbb-d000-4dc7-8366-5ce34cb269bd",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-19944, commit ID: 80ebca62cbdb7d5c8606e95a944164ab1a943694#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-02-28T07:34:43Z",
        "updatedAt" : "2020-03-01T05:24:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "a32304876ba552d31b8562d9562e0f0772851563",
    "line" : 533,
    "diffHunk" : "@@ -1,1 +1323,1327 @@  val STREAMING_FILE_COMMIT_PROTOCOL_CLASS =\n    buildConf(\"spark.sql.streaming.commitProtocolClass\")\n      .version(\"2.1.0\")\n      .internal()\n      .stringConf"
  },
  {
    "id" : "2946837e-8d9e-4676-b3f7-5f0fe7bc7eae",
    "prId" : 27730,
    "prUrl" : "https://github.com/apache/spark/pull/27730#pullrequestreview-366204610",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "611ff2bf-1cb3-4c5b-88db-ce3349d1e31d",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-24730, commit ID: 6078b891da8fe7fc36579699473168ae7443284c#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-02-28T07:35:38Z",
        "updatedAt" : "2020-03-01T05:24:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "a32304876ba552d31b8562d9562e0f0772851563",
    "line" : 541,
    "diffHunk" : "@@ -1,1 +1336,1340 @@        \"Note: This configuration cannot be changed between query restarts from the same \" +\n        \"checkpoint location.\")\n      .version(\"2.4.0\")\n      .stringConf\n      .transform(_.toLowerCase(Locale.ROOT))"
  },
  {
    "id" : "b8b56bf3-7c09-422d-ab9f-48cd7c8ccd3e",
    "prId" : 27730,
    "prUrl" : "https://github.com/apache/spark/pull/27730#pullrequestreview-366204807",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "120ecf54-5826-4b25-8a4f-c5573d9e9758",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-19944, commit ID: 0ee38a39e43dd7ad9d50457e446ae36f64621a1b#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-02-28T07:36:09Z",
        "updatedAt" : "2020-03-01T05:24:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "a32304876ba552d31b8562d9562e0f0772851563",
    "line" : 549,
    "diffHunk" : "@@ -1,1 +1351,1355 @@        \"grows too large, we will fall back to sort-based aggregation. This option sets a row \" +\n        \"count threshold for the size of the hash map.\")\n      .version(\"2.2.0\")\n      .intConf\n      // We are trying to be conservative and use a relatively small default count threshold here"
  },
  {
    "id" : "9575edb5-47e7-4507-a579-6307d858e0e5",
    "prId" : 27728,
    "prUrl" : "https://github.com/apache/spark/pull/27728#pullrequestreview-382687787",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d7ad3480-e71a-4861-a4b5-9b9bb164d152",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Will we port this back to branch-3.0?",
        "createdAt" : "2020-03-25T09:27:25Z",
        "updatedAt" : "2020-03-26T08:00:03Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "76b5087c-939b-4e14-93f8-0f46aa3b2479",
        "parentId" : "d7ad3480-e71a-4861-a4b5-9b9bb164d152",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Besides of https://github.com/apache/spark/pull/27728#discussion_r397742247, one more concern about enabling by default is, after this is enabled, we will push down `a.b` as `b` in `a` by default.\r\n\r\nI don't think DSv1 pushed down non-existent columns before; however, now DSv1 implementations should understand non-existent column  named `a.b`.\r\n\r\nI think we shouldn't assume the DSv1 downstream sources handle non-existent column handling by default. Think about constructing query strings from filters like JDBC - it will fail and the implementations have to be fixed, or this configuration has to be disabled. However, this configuration is none or all.",
        "createdAt" : "2020-03-25T10:21:03Z",
        "updatedAt" : "2020-03-26T08:00:03Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "2ebb9eb2-dcce-41da-9863-5417984cb594",
        "parentId" : "d7ad3480-e71a-4861-a4b5-9b9bb164d152",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Making a configuration that takes a list of sources could be done separately in a separate PR to make this PR smaller.",
        "createdAt" : "2020-03-25T10:36:43Z",
        "updatedAt" : "2020-03-26T08:00:03Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "a66aa6bf-3662-4aa7-abc0-f5dca4202c03",
        "parentId" : "d7ad3480-e71a-4861-a4b5-9b9bb164d152",
        "authorId" : "677aa336-324b-4b93-8300-21f8bd378f26",
        "body" : "Since the filter apis will be enhanced to support nested columns and column name containing `dots`, it will be nice to introduce it in a major release. \r\n\r\nIt's a good idea! We can make another PR to turn this feature on for specific data sources in a separate PR. This PR already grows too big.\r\n\r\nThanks!",
        "createdAt" : "2020-03-26T04:41:14Z",
        "updatedAt" : "2020-03-26T08:00:03Z",
        "lastEditedBy" : "677aa336-324b-4b93-8300-21f8bd378f26",
        "tags" : [
        ]
      },
      {
        "id" : "6f41cda1-9e99-4a94-bcca-8f48a2bf9144",
        "parentId" : "d7ad3480-e71a-4861-a4b5-9b9bb164d152",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "FYI @HeartSaVioR, are you interested in this followup out of curiosity?",
        "createdAt" : "2020-03-27T01:40:06Z",
        "updatedAt" : "2020-03-27T01:40:07Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "995db7c0-99a9-4311-a5ff-9fbd154de4ce",
        "parentId" : "d7ad3480-e71a-4861-a4b5-9b9bb164d152",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Thanks for pinging me!\r\n\r\nI might not be able to do this very soon, so please go ahead if someone is also interested or planning to. If that would be OK I deal with this in a week (or maybe even a couple of weeks), yeah I'm interested.",
        "createdAt" : "2020-03-27T06:32:02Z",
        "updatedAt" : "2020-03-27T06:32:02Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "32dc6bb9-10e8-4111-8668-358f0649bbdb",
        "parentId" : "d7ad3480-e71a-4861-a4b5-9b9bb164d152",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Sure, I think it should be fine.",
        "createdAt" : "2020-03-27T09:15:28Z",
        "updatedAt" : "2020-03-27T09:15:28Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "5fd97c0a90eb1885a93fffb9d04a262b35f62bc3",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +2059,2063 @@      .version(\"3.0.0\")\n      .booleanConf\n      .createWithDefault(true)\n\n  val SERIALIZER_NESTED_SCHEMA_PRUNING_ENABLED ="
  },
  {
    "id" : "6fc29c89-dff6-4c63-a039-22c6e49b725b",
    "prId" : 27725,
    "prUrl" : "https://github.com/apache/spark/pull/27725#pullrequestreview-366205385",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "302daee2-d944-43ab-924c-014a086bebd6",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Ur, it's because of the reversing name?",
        "createdAt" : "2020-02-28T07:08:15Z",
        "updatedAt" : "2020-02-28T07:09:07Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "eeb6c551-b786-45e2-931e-caebb8cdca36",
        "parentId" : "302daee2-d944-43ab-924c-014a086bebd6",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Yea, I think this comes from the comment: https://github.com/apache/spark/pull/26863#discussion_r378715586",
        "createdAt" : "2020-02-28T07:25:38Z",
        "updatedAt" : "2020-02-28T07:25:39Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "6d86bd44-15c8-4161-b888-9001be8bb7ec",
        "parentId" : "302daee2-d944-43ab-924c-014a086bebd6",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Thanks, @maropu .",
        "createdAt" : "2020-02-28T07:37:38Z",
        "updatedAt" : "2020-02-28T07:37:38Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "346b3e87e92d36860116996c09445d2fbd0c9ea0",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +2217,2221 @@        \"can add directory by passing directory path to ADD FILE.\")\n      .booleanConf\n      .createWithDefault(false)\n\n  val LEGACY_MSSQLSERVER_NUMERIC_MAPPING_ENABLED ="
  },
  {
    "id" : "57c87142-80c7-42c9-958d-d48b9cc5cfb1",
    "prId" : 27691,
    "prUrl" : "https://github.com/apache/spark/pull/27691#pullrequestreview-363970012",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ce48a45a-e043-4967-9bc1-ad85d6d26b91",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-30138, commit ID: c2f29d5ea58eb4565cc5602937d6d0bb75558513#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-02-25T09:06:23Z",
        "updatedAt" : "2020-02-26T03:08:20Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "c67da839bab020e9a4da845412ec989ef862655b",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +176,180 @@    .internal()\n    .doc(\"The max number of iterations the analyzer runs.\")\n    .version(\"3.0.0\")\n    .intConf\n    .createWithDefault(100)"
  },
  {
    "id" : "5812f57c-cbf7-482a-9c11-285d41fc5ec2",
    "prId" : 27691,
    "prUrl" : "https://github.com/apache/spark/pull/27691#pullrequestreview-363970321",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "01e1a578-825d-4339-b512-e53d4e70a58b",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-24802, commit ID: 434319e73f8cb6e080671bdde42a72228bd814ef#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-02-25T09:06:54Z",
        "updatedAt" : "2020-02-26T03:08:20Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "c67da839bab020e9a4da845412ec989ef862655b",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +185,189 @@      \"rules in this configuration will eventually be excluded, as some rules are necessary \" +\n      \"for correctness. The optimizer will log the rules that have indeed been excluded.\")\n    .version(\"2.4.0\")\n    .stringConf\n    .createOptional"
  },
  {
    "id" : "24641462-8aec-4c9e-9bf7-4b165d99ab34",
    "prId" : 27691,
    "prUrl" : "https://github.com/apache/spark/pull/27691#pullrequestreview-363970501",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3f01d595-2960-4782-9f53-1ca1aa20c75c",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-14677, commit ID: f4be0946af219379fb2476e6f80b2e50463adeb2#diff-32bb9518401c0948c5ea19377b5069ab",
        "createdAt" : "2020-02-25T09:07:12Z",
        "updatedAt" : "2020-02-26T03:08:20Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "c67da839bab020e9a4da845412ec989ef862655b",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +192,196 @@    .internal()\n    .doc(\"The max number of iterations the optimizer runs.\")\n    .version(\"2.0.0\")\n    .intConf\n    .createWithDefault(100)"
  },
  {
    "id" : "3a5cdbb1-09b0-44fa-8a90-e11ff8490517",
    "prId" : 27691,
    "prUrl" : "https://github.com/apache/spark/pull/27691#pullrequestreview-363970588",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dcf86536-dbf7-4eb0-9776-83bd9915c8ae",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-14796, commit ID: 3647120a5a879edf3a96a5fd68fb7aa849ad57ef#diff-32bb9518401c0948c5ea19377b5069ab",
        "createdAt" : "2020-02-25T09:07:19Z",
        "updatedAt" : "2020-02-26T03:08:20Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "c67da839bab020e9a4da845412ec989ef862655b",
    "line" : 27,
    "diffHunk" : "@@ -1,1 +200,204 @@      .internal()\n      .doc(\"The threshold of set size for InSet conversion.\")\n      .version(\"2.0.0\")\n      .intConf\n      .createWithDefault(10)"
  },
  {
    "id" : "c7270f95-6e71-4cfc-a05b-69522ea98dff",
    "prId" : 27691,
    "prUrl" : "https://github.com/apache/spark/pull/27691#pullrequestreview-363970678",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7ab2ad0c-3ce2-47c2-8473-6742ab2381c4",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-26205, commit ID: 0c23a39384b7ae5fb4aeb4f7f6fe72007b84bbd2#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-02-25T09:07:27Z",
        "updatedAt" : "2020-02-26T03:08:20Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "c67da839bab020e9a4da845412ec989ef862655b",
    "line" : 35,
    "diffHunk" : "@@ -1,1 +209,213 @@      .doc(\"Configures the max set size in InSet for which Spark will generate code with \" +\n        \"switch statements. This is applicable only to bytes, shorts, ints, dates.\")\n      .version(\"3.0.0\")\n      .intConf\n      .checkValue(threshold => threshold >= 0 && threshold <= 600, \"The max set size \" +"
  },
  {
    "id" : "d77e6caf-1e0c-4609-b349-3861a6877c1c",
    "prId" : 27691,
    "prUrl" : "https://github.com/apache/spark/pull/27691#pullrequestreview-363970781",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3438181c-69a1-485f-a7b6-4517a453cbd9",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-25415, commit ID: 8b702e1e0aba1d3e4b0aa582f20cf99f80a44a09#diff-9a6b543db706f1a90f790783d6930a13\r\nThis configuration does not exist in branch-2.4 branch, but from the branch-3.0 git log, it is found that the version number of the pom.xml file is 2.4.0-SNAPSHOT",
        "createdAt" : "2020-02-25T09:07:35Z",
        "updatedAt" : "2020-02-26T03:08:20Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "c67da839bab020e9a4da845412ec989ef862655b",
    "line" : 43,
    "diffHunk" : "@@ -1,1 +220,224 @@      \"plan after a rule or batch is applied. The value can be 'trace', 'debug', 'info', \" +\n      \"'warn', or 'error'. The default log level is 'trace'.\")\n    .version(\"3.0.0\")\n    .stringConf\n    .transform(_.toUpperCase(Locale.ROOT))"
  },
  {
    "id" : "ae1a6f34-c53f-4b0a-8a73-08f94a757fe2",
    "prId" : 27691,
    "prUrl" : "https://github.com/apache/spark/pull/27691#pullrequestreview-363970876",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fedb760e-58dd-4216-a192-ac9f7955c475",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-25415, commit ID: 8b702e1e0aba1d3e4b0aa582f20cf99f80a44a09#diff-9a6b543db706f1a90f790783d6930a13\r\nThis configuration does not exist in branch-2.4 branch, but from the branch-3.0 git log, it is found that the version number of the pom.xml file is 2.4.0-SNAPSHOT",
        "createdAt" : "2020-02-25T09:07:42Z",
        "updatedAt" : "2020-02-26T03:08:20Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "c67da839bab020e9a4da845412ec989ef862655b",
    "line" : 51,
    "diffHunk" : "@@ -1,1 +232,236 @@    .doc(\"Configures a list of rules to be logged in the optimizer, in which the rules are \" +\n      \"specified by their rule names and separated by comma.\")\n    .version(\"3.0.0\")\n    .stringConf\n    .createOptional"
  },
  {
    "id" : "c515bd55-4b5b-4516-84b2-d0f7fad39c6e",
    "prId" : 27691,
    "prUrl" : "https://github.com/apache/spark/pull/27691#pullrequestreview-363970964",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a9ce51f4-6c68-4e67-a35c-6deea90d3c0b",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-27088, commit ID: 074533334d01afdd7862a1ac6c5a7a672bcce3f8#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-02-25T09:07:50Z",
        "updatedAt" : "2020-02-26T03:08:20Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "c67da839bab020e9a4da845412ec989ef862655b",
    "line" : 59,
    "diffHunk" : "@@ -1,1 +240,244 @@    .doc(\"Configures a list of batches to be logged in the optimizer, in which the batches \" +\n      \"are specified by their batch names and separated by comma.\")\n    .version(\"3.0.0\")\n    .stringConf\n    .createOptional"
  },
  {
    "id" : "6a1d691d-ac9e-47fb-a5e4-70566f7496bd",
    "prId" : 27691,
    "prUrl" : "https://github.com/apache/spark/pull/27691#pullrequestreview-363971049",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "09d0482a-c336-4ffc-8422-e01f99bcec12",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-11150, commit ID: a7a3935c97d1fe6060cae42bbc9229c087b648ab#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-02-25T09:07:57Z",
        "updatedAt" : "2020-02-26T03:08:20Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "c67da839bab020e9a4da845412ec989ef862655b",
    "line" : 66,
    "diffHunk" : "@@ -1,1 +247,251 @@    buildConf(\"spark.sql.optimizer.dynamicPartitionPruning.enabled\")\n      .doc(\"When true, we will generate predicate for partition column when it's used as join key\")\n      .version(\"3.0.0\")\n      .booleanConf\n      .createWithDefault(true)"
  },
  {
    "id" : "a7df97a9-913d-4353-81d9-0de5b19cf09e",
    "prId" : 27691,
    "prUrl" : "https://github.com/apache/spark/pull/27691#pullrequestreview-363971135",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "717ecb07-2011-455a-a204-325100b1449b",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-11150, commit ID: a7a3935c97d1fe6060cae42bbc9229c087b648ab#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-02-25T09:08:04Z",
        "updatedAt" : "2020-02-26T03:08:20Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "c67da839bab020e9a4da845412ec989ef862655b",
    "line" : 74,
    "diffHunk" : "@@ -1,1 +257,261 @@        \"partitioned table after dynamic partition pruning, in order to evaluate if it is worth \" +\n        \"adding an extra subquery as the pruning filter if broadcast reuse is not applicable.\")\n      .version(\"3.0.0\")\n      .booleanConf\n      .createWithDefault(true)"
  },
  {
    "id" : "4de1f952-7d14-4fdf-81e9-159bc4f036e0",
    "prId" : 27691,
    "prUrl" : "https://github.com/apache/spark/pull/27691#pullrequestreview-363971183",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "72f156ed-68dd-447c-b815-f0f4c0c3ffa5",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-11150, commit ID: a7a3935c97d1fe6060cae42bbc9229c087b648ab#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-02-25T09:08:10Z",
        "updatedAt" : "2020-02-26T03:08:20Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "c67da839bab020e9a4da845412ec989ef862655b",
    "line" : 82,
    "diffHunk" : "@@ -1,1 +268,272 @@      \"after dynamic partition pruning, in order to evaluate if it is worth adding an extra \" +\n      \"subquery as the pruning filter if broadcast reuse is not applicable.\")\n    .version(\"3.0.0\")\n    .doubleConf\n    .createWithDefault(0.5)"
  },
  {
    "id" : "74b35921-e5d1-4dca-ae96-d1b4e7f38308",
    "prId" : 27691,
    "prUrl" : "https://github.com/apache/spark/pull/27691#pullrequestreview-363971271",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "eb6a9278-fb3e-4b96-80d5-1def24be8b9e",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-30528, commit ID: 59a13c9b7bc3b3aa5b5bc30a60344f849c0f8012#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-02-25T09:08:18Z",
        "updatedAt" : "2020-02-26T03:08:20Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "c67da839bab020e9a4da845412ec989ef862655b",
    "line" : 90,
    "diffHunk" : "@@ -1,1 +277,281 @@      .doc(\"When true, dynamic partition pruning will only apply when the broadcast exchange of \" +\n        \"a broadcast hash join operation can be reused as the dynamic pruning filter.\")\n      .version(\"3.0.0\")\n      .booleanConf\n      .createWithDefault(true)"
  },
  {
    "id" : "ec4039f7-00f8-4a80-93ea-98112ba34a81",
    "prId" : 27691,
    "prUrl" : "https://github.com/apache/spark/pull/27691#pullrequestreview-363971369",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "95b25433-0bb8-4616-bd17-03cf5c13169b",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-2631, commit ID: 86534d0f5255362618c05a07b0171ec35c915822#diff-41ef65b9ef5b518f77e2a03559893f4d",
        "createdAt" : "2020-02-25T09:08:26Z",
        "updatedAt" : "2020-02-26T03:08:20Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "c67da839bab020e9a4da845412ec989ef862655b",
    "line" : 97,
    "diffHunk" : "@@ -1,1 +284,288 @@    .doc(\"When set to true Spark SQL will automatically select a compression codec for each \" +\n      \"column based on statistics of the data.\")\n    .version(\"1.0.1\")\n    .booleanConf\n    .createWithDefault(true)"
  },
  {
    "id" : "a8f365ea-d035-47be-bd0d-489ff651c50f",
    "prId" : 27691,
    "prUrl" : "https://github.com/apache/spark/pull/27691#pullrequestreview-363971460",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "db7ad208-20bd-424d-9aeb-2eeed647d3e0",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-2650, commit ID: 779d1eb26d0f031791e93c908d51a59c3b422a55#diff-41ef65b9ef5b518f77e2a03559893f4d",
        "createdAt" : "2020-02-25T09:08:33Z",
        "updatedAt" : "2020-02-26T03:08:20Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "c67da839bab020e9a4da845412ec989ef862655b",
    "line" : 104,
    "diffHunk" : "@@ -1,1 +291,295 @@    .doc(\"Controls the size of batches for columnar caching.  Larger batch sizes can improve \" +\n      \"memory utilization and compression, but risk OOMs when caching data.\")\n    .version(\"1.1.1\")\n    .intConf\n    .createWithDefault(10000)"
  },
  {
    "id" : "e4715232-cf4c-4f15-b8e1-c7673359c6cf",
    "prId" : 27691,
    "prUrl" : "https://github.com/apache/spark/pull/27691#pullrequestreview-363971538",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "495ae750-fd96-4990-be3d-a09d645a26b8",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-2961, commit ID: 248067adbe90f93c7d5e23aa61b3072dfdf48a8a#diff-41ef65b9ef5b518f77e2a03559893f4d",
        "createdAt" : "2020-02-25T09:08:40Z",
        "updatedAt" : "2020-02-26T03:08:20Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "c67da839bab020e9a4da845412ec989ef862655b",
    "line" : 112,
    "diffHunk" : "@@ -1,1 +299,303 @@      .internal()\n      .doc(\"When true, enable partition pruning for in-memory columnar tables.\")\n      .version(\"1.2.0\")\n      .booleanConf\n      .createWithDefault(true)"
  },
  {
    "id" : "7e839915-ea9c-4b62-8b10-196c4378558d",
    "prId" : 27691,
    "prUrl" : "https://github.com/apache/spark/pull/27691#pullrequestreview-363971604",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2c8aa7f2-217e-4d95-8067-fcd5bc92659d",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-28257, commit ID: 42b80ae128ab1aa8a87c1376fe88e2cde52e6e4f#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-02-25T09:08:47Z",
        "updatedAt" : "2020-02-26T03:08:20Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "c67da839bab020e9a4da845412ec989ef862655b",
    "line" : 120,
    "diffHunk" : "@@ -1,1 +307,311 @@      .internal()\n      .doc(\"When true, enable in-memory table scan accumulators.\")\n      .version(\"3.0.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "ee86b29f-7570-4b1a-a59b-7d0800c13b48",
    "prId" : 27691,
    "prUrl" : "https://github.com/apache/spark/pull/27691#pullrequestreview-363971672",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f1dfec02-23f6-47e5-8482-bdf6ad8ee924",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-23312, commit ID: e5e9f9a430c827669ecfe9d5c13cc555fc89c980#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-02-25T09:08:54Z",
        "updatedAt" : "2020-02-26T03:08:20Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "c67da839bab020e9a4da845412ec989ef862655b",
    "line" : 127,
    "diffHunk" : "@@ -1,1 +314,318 @@    buildConf(\"spark.sql.inMemoryColumnarStorage.enableVectorizedReader\")\n      .doc(\"Enables vectorized reader for columnar caching.\")\n      .version(\"2.3.1\")\n      .booleanConf\n      .createWithDefault(true)"
  },
  {
    "id" : "15402a53-1e21-464f-a90b-c8980032a721",
    "prId" : 27691,
    "prUrl" : "https://github.com/apache/spark/pull/27691#pullrequestreview-363971746",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1c99dd88-b8a1-4185-865e-13f553fd0a71",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-20101, commit ID: 572af5027e45ca96e0d283a8bf7c84dcf476f9bc#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-02-25T09:09:00Z",
        "updatedAt" : "2020-02-26T03:08:20Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "c67da839bab020e9a4da845412ec989ef862655b",
    "line" : 135,
    "diffHunk" : "@@ -1,1 +322,326 @@      .internal()\n      .doc(\"When true, use OffHeapColumnVector in ColumnarBatch.\")\n      .version(\"2.3.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "114e50ac-9efa-4bf3-9c7e-8cd7b9087e46",
    "prId" : 27691,
    "prUrl" : "https://github.com/apache/spark/pull/27691#pullrequestreview-363971814",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "755f7ded-cbe6-4010-ba23-0fb28562b008",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-13977, commit ID: 9c23c818ca0175c8f2a4a66eac261ec251d27c97#diff-32bb9518401c0948c5ea19377b5069ab",
        "createdAt" : "2020-02-25T09:09:07Z",
        "updatedAt" : "2020-02-26T03:08:20Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "c67da839bab020e9a4da845412ec989ef862655b",
    "line" : 142,
    "diffHunk" : "@@ -1,1 +329,333 @@    .internal()\n    .doc(\"When true, prefer sort merge join over shuffle hash join.\")\n    .version(\"2.0.0\")\n    .booleanConf\n    .createWithDefault(true)"
  },
  {
    "id" : "2e63b55f-4e0d-48ac-93cb-227f991f41ce",
    "prId" : 27691,
    "prUrl" : "https://github.com/apache/spark/pull/27691#pullrequestreview-363971886",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3c4415cb-5c6b-41ae-9008-9f1344efd8d9",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-14724, commit ID: e2b5647ab92eb478b3f7b36a0ce6faf83e24c0e5#diff-32bb9518401c0948c5ea19377b5069ab",
        "createdAt" : "2020-02-25T09:09:14Z",
        "updatedAt" : "2020-02-26T03:08:20Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "c67da839bab020e9a4da845412ec989ef862655b",
    "line" : 150,
    "diffHunk" : "@@ -1,1 +338,342 @@      \"requires additional memory to be reserved up-front. The memory overhead may be \" +\n      \"significant when sorting very small rows (up to 50% more in this case).\")\n    .version(\"2.0.0\")\n    .booleanConf\n    .createWithDefault(true)"
  },
  {
    "id" : "a8c6e4e2-184f-437a-b34f-6ab2049db604",
    "prId" : 27691,
    "prUrl" : "https://github.com/apache/spark/pull/27691#pullrequestreview-363972030",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7a1c6831-cb84-4099-9f04-7a4ae5e67ae6",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-2393, commit ID: c7db274be79f448fda566208946cb50958ea9b1a#diff-41ef65b9ef5b518f77e2a03559893f4d",
        "createdAt" : "2020-02-25T09:09:25Z",
        "updatedAt" : "2020-02-26T03:08:20Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "c67da839bab020e9a4da845412ec989ef862655b",
    "line" : 158,
    "diffHunk" : "@@ -1,1 +349,353 @@      \"run, and file-based data source tables where the statistics are computed directly on \" +\n      \"the files of data.\")\n    .version(\"1.1.0\")\n    .bytesConf(ByteUnit.BYTE)\n    .createWithDefaultString(\"10MB\")"
  },
  {
    "id" : "eafd0e58-9551-4252-8c0b-de12025c28a7",
    "prId" : 27691,
    "prUrl" : "https://github.com/apache/spark/pull/27691#pullrequestreview-363972103",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "61f52c65-066a-4327-8667-074f9af3a3a7",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-19944, commit ID: 80ebca62cbdb7d5c8606e95a944164ab1a943694#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-02-25T09:09:32Z",
        "updatedAt" : "2020-02-26T03:08:20Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "c67da839bab020e9a4da845412ec989ef862655b",
    "line" : 166,
    "diffHunk" : "@@ -1,1 +358,362 @@      \"on a query. Higher values lead to more partitions read. Lower values might lead to \" +\n      \"longer execution times as more jobs will be run\")\n    .version(\"2.1.1\")\n    .intConf\n    .createWithDefault(4)"
  },
  {
    "id" : "f61bc5a0-201c-42fa-9813-12a7c0c2ccde",
    "prId" : 27691,
    "prUrl" : "https://github.com/apache/spark/pull/27691#pullrequestreview-363972170",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e13340fd-f07e-4de9-8973-d23b0be190c3",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-20331, commit ID: d8cada8d1d3fce979a4bc1f9879593206722a3b9#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-02-25T09:09:39Z",
        "updatedAt" : "2020-02-26T03:08:20Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "c67da839bab020e9a4da845412ec989ef862655b",
    "line" : 174,
    "diffHunk" : "@@ -1,1 +366,370 @@      .internal()\n      .doc(\"When true, advanced partition predicate pushdown into Hive metastore is enabled.\")\n      .version(\"2.3.0\")\n      .booleanConf\n      .createWithDefault(true)"
  },
  {
    "id" : "04e5f518-6b43-483e-9b8d-798e36501ceb",
    "prId" : 27691,
    "prUrl" : "https://github.com/apache/spark/pull/27691#pullrequestreview-363972252",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b12cbc2c-ab1f-433f-9e98-44a0abbd4882",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-1508, commit ID: 08ed9ad81397b71206c4dc903bfb94b6105691ed#diff-41ef65b9ef5b518f77e2a03559893f4d",
        "createdAt" : "2020-02-25T09:09:46Z",
        "updatedAt" : "2020-02-26T03:08:20Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "c67da839bab020e9a4da845412ec989ef862655b",
    "line" : 182,
    "diffHunk" : "@@ -1,1 +374,378 @@      \"Note: For structured streaming, this configuration cannot be changed between query \" +\n      \"restarts from the same checkpoint location.\")\n    .version(\"1.1.0\")\n    .intConf\n    .checkValue(_ > 0, \"The value of spark.sql.shuffle.partitions must be positive\")"
  },
  {
    "id" : "beba0134-ba78-485e-a800-d9bc2a2b32af",
    "prId" : 27691,
    "prUrl" : "https://github.com/apache/spark/pull/27691#pullrequestreview-363972340",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "45750d47-9252-482d-8a86-ab6762f2696a",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-9858 and SPARK-9859 and SPARK-9861, commit ID: d728d5c98658c44ed2949b55d36edeaa46f8c980#diff-41ef65b9ef5b518f77e2a03559893f4d",
        "createdAt" : "2020-02-25T09:09:54Z",
        "updatedAt" : "2020-02-26T03:08:20Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "c67da839bab020e9a4da845412ec989ef862655b",
    "line" : 189,
    "diffHunk" : "@@ -1,1 +381,385 @@  val ADAPTIVE_EXECUTION_ENABLED = buildConf(\"spark.sql.adaptive.enabled\")\n    .doc(\"When true, enable adaptive query execution.\")\n    .version(\"1.6.0\")\n    .booleanConf\n    .createWithDefault(false)"
  },
  {
    "id" : "35185219-4486-4cef-b412-36235c770e02",
    "prId" : 27691,
    "prUrl" : "https://github.com/apache/spark/pull/27691#pullrequestreview-363972429",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "401249ec-f84d-43e2-9673-4929b56b01da",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-30719, commit ID: b29cb1a82b1a1facf1dd040025db93d998dad4cd#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-02-25T09:10:01Z",
        "updatedAt" : "2020-02-26T03:08:20Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "c67da839bab020e9a4da845412ec989ef862655b",
    "line" : 197,
    "diffHunk" : "@@ -1,1 +391,395 @@      s\"'${ADAPTIVE_EXECUTION_ENABLED.key}' enabled), Spark will force apply adaptive query \" +\n      \"execution for all supported queries.\")\n    .version(\"3.0.0\")\n    .booleanConf\n    .createWithDefault(false)"
  },
  {
    "id" : "3ba27793-fcdb-4fce-bfa3-e89fa59b43f8",
    "prId" : 27691,
    "prUrl" : "https://github.com/apache/spark/pull/27691#pullrequestreview-363972526",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c28feb02-87e3-4b9c-9d69-e731d071b6a7",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-30812, commit ID: b76bc0b1b8b2abd00a84f805af90ca4c5925faaa#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-02-25T09:10:09Z",
        "updatedAt" : "2020-02-26T03:08:20Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "c67da839bab020e9a4da845412ec989ef862655b",
    "line" : 205,
    "diffHunk" : "@@ -1,1 +399,403 @@      .doc(s\"When true and '${ADAPTIVE_EXECUTION_ENABLED.key}' is enabled, this enables reducing \" +\n        \"the number of post-shuffle partitions based on map output statistics.\")\n      .version(\"3.0.0\")\n      .booleanConf\n      .createWithDefault(true)"
  },
  {
    "id" : "f5a4de6d-6a6a-4818-8fd8-dfcf2423f71c",
    "prId" : 27691,
    "prUrl" : "https://github.com/apache/spark/pull/27691#pullrequestreview-363972617",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ab1c1db3-b772-4216-8215-86cd0b445e97",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-30812, commit ID: b76bc0b1b8b2abd00a84f805af90ca4c5925faaa#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-02-25T09:10:16Z",
        "updatedAt" : "2020-02-26T03:08:20Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "c67da839bab020e9a4da845412ec989ef862655b",
    "line" : 213,
    "diffHunk" : "@@ -1,1 +412,416 @@        \"on a relocatable serializer, the concatenation support codec in use and the new version \" +\n        \"shuffle fetch protocol.\")\n      .version(\"3.0.0\")\n      .booleanConf\n      .createWithDefault(true)"
  },
  {
    "id" : "ea9023af-9e17-4301-bccf-0494f5b4df6e",
    "prId" : 27691,
    "prUrl" : "https://github.com/apache/spark/pull/27691#pullrequestreview-363972698",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "00426eb8-3fc5-4411-9770-9365c98c30fe",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-9853, commit ID: 8616109061efc5b23b24bb9ec4a3c0f2745903c1#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-02-25T09:10:24Z",
        "updatedAt" : "2020-02-26T03:08:20Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "c67da839bab020e9a4da845412ec989ef862655b",
    "line" : 221,
    "diffHunk" : "@@ -1,1 +421,425 @@        s\"'${ADAPTIVE_EXECUTION_ENABLED.key}' and \" +\n        s\"'${REDUCE_POST_SHUFFLE_PARTITIONS_ENABLED.key}' is enabled.\")\n      .version(\"3.0.0\")\n      .intConf\n      .checkValue(_ > 0, \"The minimum shuffle partition number \" +"
  },
  {
    "id" : "3ac714c9-4b3e-46a5-b490-0c89759523ea",
    "prId" : 27691,
    "prUrl" : "https://github.com/apache/spark/pull/27691#pullrequestreview-363972776",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "112fb1b7-d4d7-4434-858a-c280535be484",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-9858 and SPARK-9859 and SPARK-9861, commit ID: d728d5c98658c44ed2949b55d36edeaa46f8c980#diff-41ef65b9ef5b518f77e2a03559893f4d",
        "createdAt" : "2020-02-25T09:10:31Z",
        "updatedAt" : "2020-02-26T03:08:20Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "c67da839bab020e9a4da845412ec989ef862655b",
    "line" : 229,
    "diffHunk" : "@@ -1,1 +432,436 @@        s\"an effect when '${ADAPTIVE_EXECUTION_ENABLED.key}' and \" +\n        s\"'${REDUCE_POST_SHUFFLE_PARTITIONS_ENABLED.key}' is enabled.\")\n      .version(\"1.6.0\")\n      .bytesConf(ByteUnit.BYTE)\n      .createWithDefaultString(\"64MB\")"
  },
  {
    "id" : "6253c0b1-0c1e-4595-9f1c-f37ae0bf5adf",
    "prId" : 27691,
    "prUrl" : "https://github.com/apache/spark/pull/27691#pullrequestreview-363972862",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "721d80d4-8bfa-4029-8053-c31ae9b768f0",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-9853, commit ID: 8616109061efc5b23b24bb9ec4a3c0f2745903c1#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-02-25T09:10:38Z",
        "updatedAt" : "2020-02-26T03:08:20Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "c67da839bab020e9a4da845412ec989ef862655b",
    "line" : 237,
    "diffHunk" : "@@ -1,1 +443,447 @@        s\"'${ADAPTIVE_EXECUTION_ENABLED.key}' and \" +\n        s\"'${REDUCE_POST_SHUFFLE_PARTITIONS_ENABLED.key}' is enabled.\")\n      .version(\"3.0.0\")\n      .intConf\n      .checkValue(_ > 0, \"The maximum shuffle partition number \" +"
  },
  {
    "id" : "d15bade4-6fe5-4546-b31a-596cb1c70d5e",
    "prId" : 27691,
    "prUrl" : "https://github.com/apache/spark/pull/27691#pullrequestreview-363972942",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7d86d488-85e3-4318-8d0a-83d7cfe80aa6",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-29893, commit ID: 6e581cf164c3a2930966b270ac1406dc1195c942#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-02-25T09:10:45Z",
        "updatedAt" : "2020-02-26T03:08:20Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "c67da839bab020e9a4da845412ec989ef862655b",
    "line" : 245,
    "diffHunk" : "@@ -1,1 +454,458 @@      \"optimization of converting the shuffle reader to local shuffle reader for the shuffle \" +\n      \"exchange of the broadcast hash join in probe side.\")\n    .version(\"3.0.0\")\n    .booleanConf\n    .createWithDefault(true)"
  },
  {
    "id" : "0d56535a-32e0-4ddd-832a-7600e3ae62b4",
    "prId" : 27691,
    "prUrl" : "https://github.com/apache/spark/pull/27691#pullrequestreview-363973029",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "256d322e-a971-4921-8597-8412e261474a",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-30812, commit ID: b76bc0b1b8b2abd00a84f805af90ca4c5925faaa#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-02-25T09:10:53Z",
        "updatedAt" : "2020-02-26T03:08:20Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "c67da839bab020e9a4da845412ec989ef862655b",
    "line" : 253,
    "diffHunk" : "@@ -1,1 +462,466 @@    .doc(\"When true and adaptive execution is enabled, a skewed join is automatically handled at \" +\n      \"runtime.\")\n    .version(\"3.0.0\")\n    .booleanConf\n    .createWithDefault(true)"
  },
  {
    "id" : "70eef9fd-bb4b-4249-8950-8f0aa81ea1fc",
    "prId" : 27691,
    "prUrl" : "https://github.com/apache/spark/pull/27691#pullrequestreview-363973178",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cc72c37f-2551-4bfb-beae-4910ecd094c8",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-30812, commit ID: 5b36cdbbfef147e93b35eaa4f8e0bea9690b6d06#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-02-25T09:11:06Z",
        "updatedAt" : "2020-02-26T03:08:20Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "c67da839bab020e9a4da845412ec989ef862655b",
    "line" : 261,
    "diffHunk" : "@@ -1,1 +471,475 @@        \" this factor multiple the median partition size and also larger than \" +\n        s\" ${SHUFFLE_TARGET_POSTSHUFFLE_INPUT_SIZE.key}\")\n      .version(\"3.0.0\")\n      .intConf\n      .checkValue(_ > 0, \"The skew factor must be positive.\")"
  },
  {
    "id" : "81bc1df1-feeb-4dfd-94d0-15f8e3e794b7",
    "prId" : 27691,
    "prUrl" : "https://github.com/apache/spark/pull/27691#pullrequestreview-363973258",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f8ef0436-4c91-4a3d-a186-65d7877344f5",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-29002, commit ID: b2f06608b785f577999318c00f2c315f39d90889#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-02-25T09:11:13Z",
        "updatedAt" : "2020-02-26T03:08:20Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "c67da839bab020e9a4da845412ec989ef862655b",
    "line" : 269,
    "diffHunk" : "@@ -1,1 +482,486 @@        \"of its size.This configuration only has an effect when \" +\n        s\"'${ADAPTIVE_EXECUTION_ENABLED.key}' is enabled.\")\n      .version(\"3.0.0\")\n      .doubleConf\n      .checkValue(_ >= 0, \"The non-empty partition ratio must be positive number.\")"
  },
  {
    "id" : "bd835321-05f3-4bbf-b657-c53e12074e5c",
    "prId" : 27691,
    "prUrl" : "https://github.com/apache/spark/pull/27691#pullrequestreview-363973351",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6bc184e1-8bf2-45bb-bcb8-d823e2b23825",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-10371, commit ID: f38509a763816f43a224653fe65e4645894c9fc4#diff-41ef65b9ef5b518f77e2a03559893f4d",
        "createdAt" : "2020-02-25T09:11:20Z",
        "updatedAt" : "2020-02-26T03:08:20Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "c67da839bab020e9a4da845412ec989ef862655b",
    "line" : 277,
    "diffHunk" : "@@ -1,1 +491,495 @@      .internal()\n      .doc(\"When true, common subexpressions will be eliminated.\")\n      .version(\"1.6.0\")\n      .booleanConf\n      .createWithDefault(true)"
  },
  {
    "id" : "02f3a30b-f2a4-41e7-a989-3aa4aac8dcc6",
    "prId" : 27691,
    "prUrl" : "https://github.com/apache/spark/pull/27691#pullrequestreview-363973426",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "730a672d-f84f-40cd-ab07-590ed9e0c2d0",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-4699, commit ID: 21bd7222e55b9cf684c072141998a0623a69f514#diff-41ef65b9ef5b518f77e2a03559893f4d",
        "createdAt" : "2020-02-25T09:11:27Z",
        "updatedAt" : "2020-02-26T03:08:20Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "c67da839bab020e9a4da845412ec989ef862655b",
    "line" : 285,
    "diffHunk" : "@@ -1,1 +499,503 @@    .doc(\"Whether the query analyzer should be case sensitive or not. \" +\n      \"Default to case insensitive. It is highly discouraged to turn on case sensitive mode.\")\n    .version(\"1.4.0\")\n    .booleanConf\n    .createWithDefault(false)"
  },
  {
    "id" : "49880e42-8ffd-46bd-9b23-8446dbe0e5ab",
    "prId" : 27691,
    "prUrl" : "https://github.com/apache/spark/pull/27691#pullrequestreview-363973487",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "302c8322-cc02-4bc7-88df-15497ccaadc4",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-19846, commit ID: e011004bedca47be998a0c14fe22a6f9bb5090cd#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-02-25T09:11:34Z",
        "updatedAt" : "2020-02-26T03:08:20Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "c67da839bab020e9a4da845412ec989ef862655b",
    "line" : 293,
    "diffHunk" : "@@ -1,1 +509,513 @@      \"for certain kinds of query plans (such as those with a large number of predicates and \" +\n      \"aliases) which might negatively impact overall runtime.\")\n    .version(\"2.2.0\")\n    .booleanConf\n    .createWithDefault(true)"
  },
  {
    "id" : "5c99890f-3504-400c-9e61-ec786a9f3f78",
    "prId" : 27691,
    "prUrl" : "https://github.com/apache/spark/pull/27691#pullrequestreview-363973562",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2695e4de-1ef2-42f3-ae24-f85834cba7d6",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-20399, commit ID: 3d1908fd58fd9b1970cbffebdb731bfe4c776ad9#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-02-25T09:11:40Z",
        "updatedAt" : "2020-02-26T03:08:20Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "c67da839bab020e9a4da845412ec989ef862655b",
    "line" : 301,
    "diffHunk" : "@@ -1,1 +518,522 @@      \"parser. The default is false since Spark 2.0. Setting it to true can restore the behavior \" +\n      \"prior to Spark 2.0.\")\n    .version(\"2.2.1\")\n    .booleanConf\n    .createWithDefault(false)"
  },
  {
    "id" : "6d586726-ac20-4e83-9862-634843cb89b2",
    "prId" : 27691,
    "prUrl" : "https://github.com/apache/spark/pull/27691#pullrequestreview-363973640",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5cd08973-4c3c-46c8-91e7-e1e744798237",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-22790, commit ID: 0fc5533e53ad03eb67590ddd231f40c2713150c3#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-02-25T09:11:48Z",
        "updatedAt" : "2020-02-26T03:08:20Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "c67da839bab020e9a4da845412ec989ef862655b",
    "line" : 309,
    "diffHunk" : "@@ -1,1 +527,531 @@      \"factor as the estimated data size, in case the data is compressed in the file and lead to\" +\n      \" a heavily underestimated result.\")\n    .version(\"2.3.1\")\n    .doubleConf\n    .checkValue(_ > 0, \"the value of fileDataSizeFactor must be greater than 0\")"
  },
  {
    "id" : "10e0ff33-cd0a-4612-ba49-bec8cce2eb24",
    "prId" : 27691,
    "prUrl" : "https://github.com/apache/spark/pull/27691#pullrequestreview-363973738",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ca503cdb-aec0-459c-b82a-15b6ba35dd19",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-8690, commit ID: 246265f2bb056d5e9011d3331b809471a24ff8d7#diff-41ef65b9ef5b518f77e2a03559893f4d",
        "createdAt" : "2020-02-25T09:11:56Z",
        "updatedAt" : "2020-02-26T03:08:20Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "c67da839bab020e9a4da845412ec989ef862655b",
    "line" : 317,
    "diffHunk" : "@@ -1,1 +536,540 @@         \"otherwise the schema is picked from the summary file or a random data file \" +\n         \"if no summary file is available.\")\n    .version(\"1.5.0\")\n    .booleanConf\n    .createWithDefault(false)"
  },
  {
    "id" : "8cd3c036-1383-4577-a38e-4e827fa4a93b",
    "prId" : 27691,
    "prUrl" : "https://github.com/apache/spark/pull/27691#pullrequestreview-363973829",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a5c4c4da-5abe-4c89-8448-b5d6d9f61aba",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-8838, commit ID: 6175d6cfe795fbd88e3ee713fac375038a3993a8#diff-41ef65b9ef5b518f77e2a03559893f4d",
        "createdAt" : "2020-02-25T09:12:04Z",
        "updatedAt" : "2020-02-26T03:08:20Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "c67da839bab020e9a4da845412ec989ef862655b",
    "line" : 325,
    "diffHunk" : "@@ -1,1 +545,549 @@         \"false, which is the default, we will merge all part-files. This should be considered \" +\n         \"as expert-only option, and shouldn't be enabled before knowing what it means exactly.\")\n    .version(\"1.5.0\")\n    .booleanConf\n    .createWithDefault(false)"
  },
  {
    "id" : "7913e8f7-14cd-42e9-9eb3-95ad617b2d7f",
    "prId" : 27691,
    "prUrl" : "https://github.com/apache/spark/pull/27691#pullrequestreview-363973913",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e1737f9e-0b2d-43ce-8a3a-34f0f5ba7efa",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-2927, commit ID: de501e169f24e4573747aec85b7651c98633c028#diff-41ef65b9ef5b518f77e2a03559893f4d",
        "createdAt" : "2020-02-25T09:12:12Z",
        "updatedAt" : "2020-02-26T03:08:20Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "c67da839bab020e9a4da845412ec989ef862655b",
    "line" : 333,
    "diffHunk" : "@@ -1,1 +554,558 @@      \"Parquet schema. This flag tells Spark SQL to interpret binary data as a string to provide \" +\n      \"compatibility with these systems.\")\n    .version(\"1.1.1\")\n    .booleanConf\n    .createWithDefault(false)"
  },
  {
    "id" : "ae8724bf-c7c5-4b63-b486-2d28ae6ce149",
    "prId" : 27691,
    "prUrl" : "https://github.com/apache/spark/pull/27691#pullrequestreview-363974006",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6723fe1e-399d-42ca-acb2-4258ed068307",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-4987, commit ID: 67d52207b5cf2df37ca70daff2a160117510f55e#diff-41ef65b9ef5b518f77e2a03559893f4d",
        "createdAt" : "2020-02-25T09:12:21Z",
        "updatedAt" : "2020-02-26T03:08:20Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "c67da839bab020e9a4da845412ec989ef862655b",
    "line" : 341,
    "diffHunk" : "@@ -1,1 +563,567 @@      \"nanoseconds field. This flag tells Spark SQL to interpret INT96 data as a timestamp to \" +\n      \"provide compatibility with these systems.\")\n    .version(\"1.3.0\")\n    .booleanConf\n    .createWithDefault(true)"
  },
  {
    "id" : "1575ebd8-eaa7-40a7-94a2-8d17d4a45937",
    "prId" : 27691,
    "prUrl" : "https://github.com/apache/spark/pull/27691#pullrequestreview-363974078",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "efe95503-6354-4df0-9e3f-5f9626bcda89",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-12297, commit ID: acf7ef3154e094875fa89f30a78ab111b267db91#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-02-25T09:12:29Z",
        "updatedAt" : "2020-02-26T03:08:20Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "c67da839bab020e9a4da845412ec989ef862655b",
    "line" : 349,
    "diffHunk" : "@@ -1,1 +571,575 @@      \"converting to timestamps, for data written by Impala.  This is necessary because Impala \" +\n      \"stores INT96 data with a different timezone offset than Hive & Spark.\")\n    .version(\"2.3.0\")\n    .booleanConf\n    .createWithDefault(false)"
  },
  {
    "id" : "2b8e3a98-a86e-4484-b451-44228291ad4b",
    "prId" : 27691,
    "prUrl" : "https://github.com/apache/spark/pull/27691#pullrequestreview-363974157",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "700d543c-a60e-4140-a234-e711c214f024",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-10365, commit ID: 21a7bfd5c324e6c82152229f1394f26afeae771c#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-02-25T09:12:36Z",
        "updatedAt" : "2020-02-26T03:08:20Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "c67da839bab020e9a4da845412ec989ef862655b",
    "line" : 357,
    "diffHunk" : "@@ -1,1 +585,589 @@      \"Unix epoch. TIMESTAMP_MILLIS is also standard, but with millisecond precision, which \" +\n      \"means Spark has to truncate the microsecond portion of its timestamp value.\")\n    .version(\"2.3.0\")\n    .stringConf\n    .transform(_.toUpperCase(Locale.ROOT))"
  },
  {
    "id" : "b1622377-0efc-4439-a93b-9f06fffd79ac",
    "prId" : 27691,
    "prUrl" : "https://github.com/apache/spark/pull/27691#pullrequestreview-363974264",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b6fd267e-4fba-436d-997f-848a5603ba9a",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-3131, commit ID: 3a9d874d7a46ab8b015631d91ba479d9a0ba827f#diff-41ef65b9ef5b518f77e2a03559893f4d",
        "createdAt" : "2020-02-25T09:12:44Z",
        "updatedAt" : "2020-02-26T03:08:20Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "c67da839bab020e9a4da845412ec989ef862655b",
    "line" : 365,
    "diffHunk" : "@@ -1,1 +597,601 @@      \"`spark.sql.parquet.compression.codec`. Acceptable values include: none, uncompressed, \" +\n      \"snappy, gzip, lzo, brotli, lz4, zstd.\")\n    .version(\"1.1.1\")\n    .stringConf\n    .transform(_.toLowerCase(Locale.ROOT))"
  },
  {
    "id" : "fa35fd84-b4db-403e-af80-b81ba112c967",
    "prId" : 27691,
    "prUrl" : "https://github.com/apache/spark/pull/27691#pullrequestreview-363974346",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a5e2086d-3828-4a8c-a935-fe03d122984a",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-4391, commit ID: 576688aa2a19bd4ba239a2b93af7947f983e5124#diff-41ef65b9ef5b518f77e2a03559893f4d",
        "createdAt" : "2020-02-25T09:12:51Z",
        "updatedAt" : "2020-02-26T03:08:20Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "c67da839bab020e9a4da845412ec989ef862655b",
    "line" : 373,
    "diffHunk" : "@@ -1,1 +605,609 @@  val PARQUET_FILTER_PUSHDOWN_ENABLED = buildConf(\"spark.sql.parquet.filterPushdown\")\n    .doc(\"Enables Parquet filter push-down optimization when set to true.\")\n    .version(\"1.2.0\")\n    .booleanConf\n    .createWithDefault(true)"
  },
  {
    "id" : "69ee4c26-debe-4c96-b892-24e7ec62de4c",
    "prId" : 27691,
    "prUrl" : "https://github.com/apache/spark/pull/27691#pullrequestreview-363974421",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "063bc9a5-7497-41bb-8166-253c2cbb136a",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-23727, commit ID: b02e76cbffe9e589b7a4e60f91250ca12a4420b2#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-02-25T09:12:58Z",
        "updatedAt" : "2020-02-26T03:08:20Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "c67da839bab020e9a4da845412ec989ef862655b",
    "line" : 381,
    "diffHunk" : "@@ -1,1 +613,617 @@      s\"This configuration only has an effect when '${PARQUET_FILTER_PUSHDOWN_ENABLED.key}' is \" +\n      \"enabled.\")\n    .version(\"2.4.0\")\n    .internal()\n    .booleanConf"
  },
  {
    "id" : "661ded6d-3795-48a2-86ee-9c094c0f1631",
    "prId" : 27691,
    "prUrl" : "https://github.com/apache/spark/pull/27691#pullrequestreview-363974507",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "049e1e15-4735-4496-a6df-28a9d2b3adbe",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-24718, commit ID: 43e4e851b642bbee535d22e1b9e72ec6b99f6ed4#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-02-25T09:13:08Z",
        "updatedAt" : "2020-02-26T03:08:20Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "c67da839bab020e9a4da845412ec989ef862655b",
    "line" : 392,
    "diffHunk" : "@@ -1,1 +623,627 @@        s\"This configuration only has an effect when '${PARQUET_FILTER_PUSHDOWN_ENABLED.key}' is \" +\n        \"enabled and Timestamp stored as TIMESTAMP_MICROS or TIMESTAMP_MILLIS type.\")\n      .version(\"2.4.0\")\n      .internal()\n      .booleanConf"
  },
  {
    "id" : "935de36b-aaaf-4b72-9ee7-498b28b169ef",
    "prId" : 27691,
    "prUrl" : "https://github.com/apache/spark/pull/27691#pullrequestreview-363974626",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e4ba4332-4668-4f20-b9e0-21fb5c425633",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-24549, commit ID: 9549a2814951f9ba969955d78ac4bd2240f85989#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-02-25T09:13:16Z",
        "updatedAt" : "2020-02-26T03:08:20Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "c67da839bab020e9a4da845412ec989ef862655b",
    "line" : 402,
    "diffHunk" : "@@ -1,1 +633,637 @@        s\"This configuration only has an effect when '${PARQUET_FILTER_PUSHDOWN_ENABLED.key}' is \" +\n        \"enabled.\")\n      .version(\"2.4.0\")\n      .internal()\n      .booleanConf"
  },
  {
    "id" : "88e51063-09f5-4307-b864-4e71ad064513",
    "prId" : 27691,
    "prUrl" : "https://github.com/apache/spark/pull/27691#pullrequestreview-363974856",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5e0620f2-acd7-4111-9ae3-2cae26d7bc31",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-24638, commit ID: 03545ce6de08bd0ad685c5f59b73bc22dfc40887#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-02-25T09:13:23Z",
        "updatedAt" : "2020-02-26T03:08:20Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "c67da839bab020e9a4da845412ec989ef862655b",
    "line" : 410,
    "diffHunk" : "@@ -1,1 +643,647 @@      s\"This configuration only has an effect when '${PARQUET_FILTER_PUSHDOWN_ENABLED.key}' is \" +\n      \"enabled.\")\n    .version(\"2.4.0\")\n    .internal()\n    .booleanConf"
  },
  {
    "id" : "f18142d5-9d9d-4e91-8fc2-7e7736a57e7c",
    "prId" : 27691,
    "prUrl" : "https://github.com/apache/spark/pull/27691#pullrequestreview-363975023",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "93dc4706-4972-4c48-843e-1a8cf8449566",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-17091, commit ID: e1de34113e057707dfc5ff54a8109b3ec7c16dfb#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-02-25T09:13:33Z",
        "updatedAt" : "2020-02-26T03:08:20Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "c67da839bab020e9a4da845412ec989ef862655b",
    "line" : 418,
    "diffHunk" : "@@ -1,1 +656,660 @@        s\"This configuration only has an effect when '${PARQUET_FILTER_PUSHDOWN_ENABLED.key}' is \" +\n        \"enabled.\")\n      .version(\"2.4.0\")\n      .internal()\n      .intConf"
  },
  {
    "id" : "0225518b-f08e-4b6e-a16e-122e5a9b56e4",
    "prId" : 27691,
    "prUrl" : "https://github.com/apache/spark/pull/27691#pullrequestreview-363975105",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "637ab0d5-9fc0-4cbe-ac1b-261d46f91104",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-10400, commit ID: 01cd688f5245cbb752863100b399b525b31c3510#diff-41ef65b9ef5b518f77e2a03559893f4d",
        "createdAt" : "2020-02-25T09:13:40Z",
        "updatedAt" : "2020-02-26T03:08:20Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "c67da839bab020e9a4da845412ec989ef862655b",
    "line" : 426,
    "diffHunk" : "@@ -1,1 +668,672 @@      \"will be used. For example, decimals will be written in int-based format. If Parquet \" +\n      \"output is intended for use with systems that do not support this newer format, set to true.\")\n    .version(\"1.6.0\")\n    .booleanConf\n    .createWithDefault(false)"
  },
  {
    "id" : "33d7b261-9223-4097-8555-1ec8098c8ee6",
    "prId" : 27691,
    "prUrl" : "https://github.com/apache/spark/pull/27691#pullrequestreview-363975202",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2fe21bee-2174-4a81-a345-edbf380e1600",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-8139, commit ID: 111d6b9b8a584b962b6ae80c7aa8c45845ce0099#diff-41ef65b9ef5b518f77e2a03559893f4d",
        "createdAt" : "2020-02-25T09:13:48Z",
        "updatedAt" : "2020-02-26T03:08:20Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "c67da839bab020e9a4da845412ec989ef862655b",
    "line" : 434,
    "diffHunk" : "@@ -1,1 +678,682 @@      \"summaries will never be created, irrespective of the value of \" +\n      \"parquet.summary.metadata.level\")\n    .version(\"1.5.0\")\n    .internal()\n    .stringConf"
  },
  {
    "id" : "c221a192-0175-413b-99d9-499795b00ed7",
    "prId" : 27691,
    "prUrl" : "https://github.com/apache/spark/pull/27691#pullrequestreview-363975306",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fd2a11ff-82bb-4401-96b4-492ec85b5e83",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-13486, commit ID: 2b2c8c33236677c916541f956f7b94bba014a9ce#diff-32bb9518401c0948c5ea19377b5069ab",
        "createdAt" : "2020-02-25T09:13:56Z",
        "updatedAt" : "2020-02-26T03:08:20Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "c67da839bab020e9a4da845412ec989ef862655b",
    "line" : 442,
    "diffHunk" : "@@ -1,1 +686,690 @@    buildConf(\"spark.sql.parquet.enableVectorizedReader\")\n      .doc(\"Enables vectorized parquet decoding.\")\n      .version(\"2.0.0\")\n      .booleanConf\n      .createWithDefault(true)"
  },
  {
    "id" : "cc7b7b44-e602-4a31-b388-4c562a975247",
    "prId" : 27691,
    "prUrl" : "https://github.com/apache/spark/pull/27691#pullrequestreview-363975411",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2aeca926-5930-4057-af01-61b4abd3bf06",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-17310, commit ID: 673c67046598d33b9ecf864024ca7a937c1998d6#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-02-25T09:14:05Z",
        "updatedAt" : "2020-02-26T03:08:20Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "c67da839bab020e9a4da845412ec989ef862655b",
    "line" : 450,
    "diffHunk" : "@@ -1,1 +696,700 @@      \"is enabled and the vectorized reader is not used. You can ensure the vectorized reader \" +\n      s\"is not used by setting '${PARQUET_VECTORIZED_READER_ENABLED.key}' to false.\")\n    .version(\"2.3.0\")\n    .booleanConf\n    .createWithDefault(false)"
  },
  {
    "id" : "42d524ec-7667-4b07-9e54-5b9bcaf2d6f2",
    "prId" : 27691,
    "prUrl" : "https://github.com/apache/spark/pull/27691#pullrequestreview-363975520",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9266dfee-ec06-423a-8bc9-d8d4e88e46fb",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-23188, commit ID: cc41245fa3f954f961541bf4b4275c28473042b8#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-02-25T09:14:13Z",
        "updatedAt" : "2020-02-26T03:08:20Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "c67da839bab020e9a4da845412ec989ef862655b",
    "line" : 457,
    "diffHunk" : "@@ -1,1 +703,707 @@    .doc(\"The number of rows to include in a parquet vectorized reader batch. The number should \" +\n      \"be carefully chosen to minimize overhead and avoid OOMs in reading data.\")\n    .version(\"2.4.0\")\n    .intConf\n    .createWithDefault(4096)"
  },
  {
    "id" : "bade7f1c-be70-4e20-a67e-976531ef6102",
    "prId" : 27690,
    "prUrl" : "https://github.com/apache/spark/pull/27690#pullrequestreview-433798796",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0d70773f-2e0f-42cf-a285-6cb6d02ffa86",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "The supported schemes are determined by Hive? If so, is it meaningful to config the supported schemes here?",
        "createdAt" : "2020-06-13T07:37:45Z",
        "updatedAt" : "2020-07-17T08:22:50Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "dc2d8593-647b-4877-90e8-ff5fe2cb64d7",
        "parentId" : "0d70773f-2e0f-42cf-a285-6cb6d02ffa86",
        "authorId" : "8c4501b6-dbfc-491f-80a5-f615a2311f95",
        "body" : "Users can specify any blob storage schema like following. If copy operation is expensive in the storage system, this option will be effective.\r\n- Amazon S3: `s3`, `s3a`, `s3n`\r\n- Azure Blob Storage: `wasb`, `wasbs`\r\n- Google Cloud Storage: `gs`\r\n- Databricks: `dbfs`\r\n- OpenStack: `swift`\r\n\r\nSince any schemes are possible to be used, I believe we cannot define specific supported schemes here. That's why I just listed samples in SQLConf.scala.",
        "createdAt" : "2020-06-15T03:28:53Z",
        "updatedAt" : "2020-07-17T08:22:50Z",
        "lastEditedBy" : "8c4501b6-dbfc-491f-80a5-f615a2311f95",
        "tags" : [
        ]
      },
      {
        "id" : "dedf17be-c5e9-4cdc-ae54-a584f3e3778d",
        "parentId" : "0d70773f-2e0f-42cf-a285-6cb6d02ffa86",
        "authorId" : "8c4501b6-dbfc-491f-80a5-f615a2311f95",
        "body" : "Note: I am not 100% sure whether all these blob storage systems have similar characteristics and not sure if this option is effective. At least, this option is effective for Amazon S3.",
        "createdAt" : "2020-06-15T03:31:22Z",
        "updatedAt" : "2020-07-17T08:22:50Z",
        "lastEditedBy" : "8c4501b6-dbfc-491f-80a5-f615a2311f95",
        "tags" : [
        ]
      },
      {
        "id" : "0a59b107-a492-468c-b360-88ac524e772c",
        "parentId" : "0d70773f-2e0f-42cf-a285-6cb6d02ffa86",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "If users can specify any blob storage scheme, I'm not sure if it is meaningful to have a list config of schemes. It sounds like we just need a boolean config to enable/disable this feature.",
        "createdAt" : "2020-06-15T17:17:45Z",
        "updatedAt" : "2020-07-17T08:22:50Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "63d3c9e3-76be-4b39-97d0-3d22d66a9786",
        "parentId" : "0d70773f-2e0f-42cf-a285-6cb6d02ffa86",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Ah, nice. +1 on the @viirya idea.",
        "createdAt" : "2020-06-15T23:29:55Z",
        "updatedAt" : "2020-07-17T08:22:50Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "ab8b30fc-c533-4b3c-94ff-1a2878c0d901",
        "parentId" : "0d70773f-2e0f-42cf-a285-6cb6d02ffa86",
        "authorId" : "8c4501b6-dbfc-491f-80a5-f615a2311f95",
        "body" : "If all the blob storage services have the same characteristics, then it will be possible. However, actually we cannot guarantee it.\r\nI believe that it is better to allow users to enable/disable this feature per scheme.",
        "createdAt" : "2020-06-17T00:21:26Z",
        "updatedAt" : "2020-07-17T08:22:50Z",
        "lastEditedBy" : "8c4501b6-dbfc-491f-80a5-f615a2311f95",
        "tags" : [
        ]
      },
      {
        "id" : "7fd11347-168d-4670-813a-376bcaa45362",
        "parentId" : "0d70773f-2e0f-42cf-a285-6cb6d02ffa86",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Ah, if we use a boolean config for this, we cannot apply this optimization only at a specific schema... I see.",
        "createdAt" : "2020-06-17T00:46:17Z",
        "updatedAt" : "2020-07-17T08:22:50Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "e766d2f7-f915-4581-8824-3f1fecac9554",
        "parentId" : "0d70773f-2e0f-42cf-a285-6cb6d02ffa86",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Okay, it sounds making sense.",
        "createdAt" : "2020-06-19T04:45:13Z",
        "updatedAt" : "2020-07-17T08:22:50Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "44ce61eb4cc129c285131525b99b20f82774d107",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +850,854 @@      .version(\"3.1.0\")\n      .stringConf\n      .createWithDefault(\"\")\n\n  val OPTIMIZER_METADATA_ONLY = buildConf(\"spark.sql.optimizer.metadataOnly\")"
  },
  {
    "id" : "7d2c9b0e-fb61-4833-a6ae-2726edb26735",
    "prId" : 27669,
    "prUrl" : "https://github.com/apache/spark/pull/27669#pullrequestreview-363005888",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "263cb2ac-8184-45a9-a83d-eba9643c2cf2",
        "parentId" : null,
        "authorId" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "body" : "We no longer use `ADAPTIVE_EXECUTION_SKEWED_PARTITION_SIZE_THRESHOLD`, right? Let's remove it.",
        "createdAt" : "2020-02-21T16:30:39Z",
        "updatedAt" : "2020-02-25T06:55:56Z",
        "lastEditedBy" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "tags" : [
        ]
      },
      {
        "id" : "5ebdc02e-8d15-49a4-b01a-6be531f56188",
        "parentId" : "263cb2ac-8184-45a9-a83d-eba9643c2cf2",
        "authorId" : "1b84a7ff-6bf9-4417-bf9f-e46e997e5974",
        "body" : "When the `nonSkewSizes ` is very small. The `targetSize ` will be small without this config. Then it will split more small task when handling skewed partition? So we may need this config.",
        "createdAt" : "2020-02-22T02:33:11Z",
        "updatedAt" : "2020-02-25T06:55:56Z",
        "lastEditedBy" : "1b84a7ff-6bf9-4417-bf9f-e46e997e5974",
        "tags" : [
        ]
      },
      {
        "id" : "89475636-0aab-44b8-9eb4-7aee39507fda",
        "parentId" : "263cb2ac-8184-45a9-a83d-eba9643c2cf2",
        "authorId" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "body" : "We are using https://github.com/apache/spark/pull/27669/files#diff-2d6bea6eed43ca6f37fe3531cb574069R93 now, as we are trying to make the same target partition size for both coalesced non-skew partitions and skew partitions after split if the avg non-skew size is small.",
        "createdAt" : "2020-02-22T03:14:24Z",
        "updatedAt" : "2020-02-25T06:55:56Z",
        "lastEditedBy" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "tags" : [
        ]
      }
    ],
    "commit" : "4755526bd20f44aa24f91a120c98518330081398",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +439,443 @@        s\" ${SHUFFLE_TARGET_POSTSHUFFLE_INPUT_SIZE.key}\")\n      .intConf\n      .checkValue(_ > 0, \"The skew factor must be positive.\")\n      .createWithDefault(10)\n"
  },
  {
    "id" : "6eb135a7-5a90-4679-929e-1d3bb7a719e6",
    "prId" : 27639,
    "prUrl" : "https://github.com/apache/spark/pull/27639#pullrequestreview-361460935",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "115e6528-c4f7-4214-9bfd-1a6eb35bc7fe",
        "parentId" : null,
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "This follows the doc of `spark.sql.files.maxPartitionBytes` and `spark.sql.files.openCostInBytes`",
        "createdAt" : "2020-02-19T21:46:03Z",
        "updatedAt" : "2020-02-19T21:46:03Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "4adae1553282ff3037e521539cfc16ffc55328d7",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +880,884 @@        \"of detected paths exceeds this value during partition discovery, it tries to list the \" +\n        \"files with another Spark distributed job. This configuration is effective only when \" +\n        \"using file-based sources such as Parquet, JSON and ORC.\")\n      .intConf\n      .checkValue(parallel => parallel >= 0, \"The maximum number of paths allowed for listing \" +"
  },
  {
    "id" : "2e581c8a-9cba-40b1-b369-81f451950d0f",
    "prId" : 27580,
    "prUrl" : "https://github.com/apache/spark/pull/27580#pullrequestreview-365423160",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2b00bdf0-67ae-4c2a-a1fc-277e33f0c4c9",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Maybe, `allowHashOnMapType`?",
        "createdAt" : "2020-02-27T05:07:13Z",
        "updatedAt" : "2020-02-27T05:07:13Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "fc032d50-a56f-4497-92c6-dc482f1d6ef8",
        "parentId" : "2b00bdf0-67ae-4c2a-a1fc-277e33f0c4c9",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "ah \"allow\" is more precise. @iRakson feel free to send a followup to address all the comments.",
        "createdAt" : "2020-02-27T05:31:11Z",
        "updatedAt" : "2020-02-27T05:31:11Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "8dfee675f6bfaf04068b856fc15158cea307f188",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +2206,2210 @@      .createWithDefault(false)\n\n  val LEGACY_USE_HASH_ON_MAPTYPE = buildConf(\"spark.sql.legacy.useHashOnMapType\")\n    .doc(\"When set to true, hash expressions can be applied on elements of MapType. Otherwise, \" +\n      \"an analysis exception will be thrown.\")"
  },
  {
    "id" : "37c96dc0-fb66-43d0-83c8-a9faf98fb7f9",
    "prId" : 27563,
    "prUrl" : "https://github.com/apache/spark/pull/27563#pullrequestreview-360791187",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "464317d4-8fcc-4244-a128-a5410c6f72c9",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "ah I missed this one. We should update other skew join related configs to use the `skewedJoinOptimization` prefix.\r\n\r\n@JkSelf can you help to fix it?",
        "createdAt" : "2020-02-18T12:41:23Z",
        "updatedAt" : "2020-02-18T12:41:23Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "87e6f59d-48b9-40d2-ae39-f85dd3234e69",
        "parentId" : "464317d4-8fcc-4244-a128-a5410c6f72c9",
        "authorId" : "1b84a7ff-6bf9-4417-bf9f-e46e997e5974",
        "body" : "fix it in [PR#27630](https://github.com/apache/spark/pull/27630).",
        "createdAt" : "2020-02-19T01:37:37Z",
        "updatedAt" : "2020-02-19T01:37:38Z",
        "lastEditedBy" : "1b84a7ff-6bf9-4417-bf9f-e46e997e5974",
        "tags" : [
        ]
      }
    ],
    "commit" : "83fda3c90dd7cea7db4353be881965c8aa9e12ac",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +426,430 @@\n  val ADAPTIVE_EXECUTION_SKEWED_JOIN_ENABLED =\n    buildConf(\"spark.sql.adaptive.skewedJoinOptimization.enabled\")\n    .doc(\"When true and adaptive execution is enabled, a skewed join is automatically handled at \" +\n      \"runtime.\")"
  },
  {
    "id" : "7540b1ce-756a-46d7-b3e4-e65a2c614984",
    "prId" : 27488,
    "prUrl" : "https://github.com/apache/spark/pull/27488#pullrequestreview-371610620",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "aa50f6ef-c7c7-46ef-93b5-f478e12973e7",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "> exception will be throw\r\n->\r\n> an exception will be thrown at runtime. ",
        "createdAt" : "2020-03-10T00:24:32Z",
        "updatedAt" : "2020-03-10T00:24:32Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      }
    ],
    "commit" : "0d1601bde2fc4ab39ed6b3c82e486e9c1174b915",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +2021,2025 @@      .internal()\n      .doc(\"When set to true, user is allowed to use org.apache.spark.sql.functions.\" +\n        \"udf(f: AnyRef, dataType: DataType). Otherwise, exception will be throw.\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "8d88c75e-1c7f-4966-8740-50a6bc723445",
    "prId" : 27459,
    "prUrl" : "https://github.com/apache/spark/pull/27459#pullrequestreview-354230770",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "703578e0-ba80-4cd9-bdef-aee7a41a0a5f",
        "parentId" : null,
        "authorId" : "fd6ebc48-7da4-490b-8d41-2e8530d92720",
        "body" : "This is an example of some of the follow-up we'll need to do to clean the docstrings so they are suitable for rendering in Markdown. (The docstrings escape some config names with backticks, but not consistently.)",
        "createdAt" : "2020-02-04T21:52:21Z",
        "updatedAt" : "2020-02-07T23:13:31Z",
        "lastEditedBy" : "fd6ebc48-7da4-490b-8d41-2e8530d92720",
        "tags" : [
        ]
      },
      {
        "id" : "27c6e367-a49d-4c70-be34-323ca92a03b8",
        "parentId" : "703578e0-ba80-4cd9-bdef-aee7a41a0a5f",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I think this is fine.",
        "createdAt" : "2020-02-06T06:51:59Z",
        "updatedAt" : "2020-02-07T23:13:31Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "b08bac43a844376bc870cb99c776d50157c39a01",
    "line" : 65,
    "diffHunk" : "@@ -1,1 +1154,1158 @@    buildConf(\"spark.sql.variable.substitute\")\n      .doc(\"This enables substitution using syntax like `${var}`, `${system:var}`, \" +\n        \"and `${env:var}`.\")\n      .booleanConf\n      .createWithDefault(true)"
  },
  {
    "id" : "4dcb2164-cdff-466f-95c3-c73161201f8e",
    "prId" : 27452,
    "prUrl" : "https://github.com/apache/spark/pull/27452#pullrequestreview-356888017",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9f479d94-022b-49ba-a260-bd69efb05cad",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "`spark.sql.adaptive.forceApply.enabled`?\r\ncc @gatorsmile ",
        "createdAt" : "2020-02-05T19:22:29Z",
        "updatedAt" : "2020-02-06T06:36:02Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "090c5555-bee6-4a6a-b2a4-75efe34e8756",
        "parentId" : "9f479d94-022b-49ba-a260-bd69efb05cad",
        "authorId" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "body" : "No... that sounds weird.",
        "createdAt" : "2020-02-05T19:32:47Z",
        "updatedAt" : "2020-02-06T06:36:02Z",
        "lastEditedBy" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "tags" : [
        ]
      },
      {
        "id" : "2de80be8-c60f-439d-bd32-b6d0637bbf76",
        "parentId" : "9f479d94-022b-49ba-a260-bd69efb05cad",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "@maryannxue . `.enabled` is a general guideline for boolean flag from @gatorsmile .\r\n> No... that sounds weird.",
        "createdAt" : "2020-02-05T19:35:56Z",
        "updatedAt" : "2020-02-06T06:36:02Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "d64b65e5-4c38-4bff-a6ce-b8876e6cefc2",
        "parentId" : "9f479d94-022b-49ba-a260-bd69efb05cad",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "it's usually `xxx.featureName.enabled`, but `forceApply` is a verb. For example, `spark.sql.join.preferSortMergeJoin`",
        "createdAt" : "2020-02-06T05:47:40Z",
        "updatedAt" : "2020-02-06T06:36:02Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "2b52820f-8875-4516-9d03-2dcbdf888c83",
        "parentId" : "9f479d94-022b-49ba-a260-bd69efb05cad",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Oh. Got it. I misunderstood the rule at that part until now. My bad. Thank you, @cloud-fan and @maryannxue .",
        "createdAt" : "2020-02-06T23:00:16Z",
        "updatedAt" : "2020-02-06T23:00:16Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "b6aa5e4b-9a76-4cc3-8b57-e8a5c87624eb",
        "parentId" : "9f479d94-022b-49ba-a260-bd69efb05cad",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Then why don't we make it a feature name and add `.enabled` to be consistent?",
        "createdAt" : "2020-02-10T09:06:19Z",
        "updatedAt" : "2020-02-10T09:06:20Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "a89b5ea2-e07b-478a-89c6-f3a0891dc832",
        "parentId" : "9f479d94-022b-49ba-a260-bd69efb05cad",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Such configurations `spark.sql.join.preferSortMergeJoin` were added long ago but this is new where it's free to change.",
        "createdAt" : "2020-02-10T09:12:33Z",
        "updatedAt" : "2020-02-10T09:12:33Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "d6f2d1e2-42f6-4e54-8ec8-95f1afea4ce5",
        "parentId" : "9f479d94-022b-49ba-a260-bd69efb05cad",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "do we have a policy that config name must be `xxx.featureName.enabled`? At least for internal configs we follow PR author's personal preference AFAIK.",
        "createdAt" : "2020-02-10T09:43:50Z",
        "updatedAt" : "2020-02-10T09:43:50Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "bfeeb711-3fc3-46a4-b290-5132805f8767",
        "parentId" : "9f479d94-022b-49ba-a260-bd69efb05cad",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "We should then stop renaming the configurations to add `.enabled` (e.g., https://github.com/apache/spark/pull/27346, https://github.com/apache/spark/pull/27210, https://github.com/apache/spark/pull/26694). `reuse`, `ignore`, and `fail`  can be a verb either.",
        "createdAt" : "2020-02-10T09:58:25Z",
        "updatedAt" : "2020-02-10T09:58:25Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "54a605ee-3dda-4ea1-97b5-2b000e535fe1",
        "parentId" : "9f479d94-022b-49ba-a260-bd69efb05cad",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "@gatorsmile do we need to add \".enabled\" post-fix to all boolean configs?",
        "createdAt" : "2020-02-10T10:12:17Z",
        "updatedAt" : "2020-02-10T10:12:17Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "da1e6e87-bd79-4196-82b5-f3b651ef1d8a",
        "parentId" : "9f479d94-022b-49ba-a260-bd69efb05cad",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "After this PR, it would be great if we have a documented policy for this, @gatorsmile and @cloud-fan . ",
        "createdAt" : "2020-02-11T18:23:23Z",
        "updatedAt" : "2020-02-11T18:23:23Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "86e310e7-5e7c-4bce-9c21-0b47cb89c97f",
        "parentId" : "9f479d94-022b-49ba-a260-bd69efb05cad",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Thank you for the references, @HyukjinKwon . ",
        "createdAt" : "2020-02-11T18:23:42Z",
        "updatedAt" : "2020-02-11T18:23:43Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "a796d191013a1dee296b92539e489c8ab471b21d",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +359,363 @@    .createWithDefault(false)\n\n  val ADAPTIVE_EXECUTION_FORCE_APPLY = buildConf(\"spark.sql.adaptive.forceApply\")\n    .internal()\n    .doc(\"Adaptive query execution is skipped when the query does not have exchanges or \" +"
  },
  {
    "id" : "55712beb-85dd-44a1-a440-87c12c4dc297",
    "prId" : 27452,
    "prUrl" : "https://github.com/apache/spark/pull/27452#pullrequestreview-353986569",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "093db5be-9754-45db-a774-a4db0f6fe825",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "To be clear, shall we mention like `By setting both `spark.sql.adaptive.enabled` and this config to true,`?",
        "createdAt" : "2020-02-05T19:25:32Z",
        "updatedAt" : "2020-02-06T06:36:02Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "ecb8b7d5-666e-499b-8193-cfef14b53900",
        "parentId" : "093db5be-9754-45db-a774-a4db0f6fe825",
        "authorId" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "body" : "By setting this config (together with spark.sql.adaptive.enabled) to true",
        "createdAt" : "2020-02-05T19:33:42Z",
        "updatedAt" : "2020-02-06T06:36:02Z",
        "lastEditedBy" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "tags" : [
        ]
      },
      {
        "id" : "ca783646-c614-4a85-886f-c846fd372192",
        "parentId" : "093db5be-9754-45db-a774-a4db0f6fe825",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "That sounds good.",
        "createdAt" : "2020-02-05T19:36:21Z",
        "updatedAt" : "2020-02-06T06:36:02Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "a796d191013a1dee296b92539e489c8ab471b21d",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +364,368 @@      \"sub-queries. By setting this config to true (together with \" +\n      s\"'${ADAPTIVE_EXECUTION_ENABLED.key}' enabled), Spark will force apply adaptive query \" +\n      \"execution for all supported queries.\")\n    .booleanConf\n    .createWithDefault(false)"
  },
  {
    "id" : "d0cebe87-9443-4560-97b3-1d6ed8610ec4",
    "prId" : 27413,
    "prUrl" : "https://github.com/apache/spark/pull/27413#pullrequestreview-395088267",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a5a4d3e4-e623-45ee-bf45-b284d34dcc31",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "We should avoid using the internal class name in the conf description and log/exception messages, since most readers are external end users who have no idea about the internal code. ",
        "createdAt" : "2020-02-08T00:53:13Z",
        "updatedAt" : "2020-02-08T00:53:13Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "6d528974-2e6d-4537-ba03-a2b7b0d1f2c4",
        "parentId" : "a5a4d3e4-e623-45ee-bf45-b284d34dcc31",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "This is an internal config, only Spark developers would read it.",
        "createdAt" : "2020-02-10T02:16:21Z",
        "updatedAt" : "2020-02-10T02:16:21Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "66207d80-68e8-4d64-b80d-dec8955932b0",
        "parentId" : "a5a4d3e4-e623-45ee-bf45-b284d34dcc31",
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "Most Spark developers do not know INSERT also can add partitions if the partitions do not exist. The current description is not clear.   ",
        "createdAt" : "2020-04-17T00:34:08Z",
        "updatedAt" : "2020-04-17T00:34:08Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      }
    ],
    "commit" : "ac8b771c1264fc8069a61a3cdb338f4ac6c406af",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +2153,2157 @@      .internal()\n      .doc(\"The number of partitions to be handled in one turn when use \" +\n        \"`AlterTableAddPartitionCommand` to add partitions into table. The smaller \" +\n        \"batch size is, the less memory is required for the real handler, e.g. Hive Metastore.\")\n      .intConf"
  },
  {
    "id" : "2da01db5-cd4f-48b2-9e7b-fdf84f46f031",
    "prId" : 27378,
    "prUrl" : "https://github.com/apache/spark/pull/27378#pullrequestreview-352509623",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4bd6e325-b53f-4061-9a83-38bcaece8e78",
        "parentId" : null,
        "authorId" : "39fe625f-8c54-48bc-ac6f-7279921adf02",
        "body" : "if possible, make those constant values in alphabeta order",
        "createdAt" : "2020-01-29T18:26:14Z",
        "updatedAt" : "2020-02-03T19:37:43Z",
        "lastEditedBy" : "39fe625f-8c54-48bc-ac6f-7279921adf02",
        "tags" : [
        ]
      },
      {
        "id" : "287e7e65-6b3a-46cc-940e-40f3b7712aed",
        "parentId" : "4bd6e325-b53f-4061-9a83-38bcaece8e78",
        "authorId" : "96666b11-a040-4d89-aef7-093f2f8f0d32",
        "body" : "Changed the ordering of the added constants to be in alphabeta order with respect to one another.",
        "createdAt" : "2020-02-03T19:38:00Z",
        "updatedAt" : "2020-02-03T19:38:01Z",
        "lastEditedBy" : "96666b11-a040-4d89-aef7-093f2f8f0d32",
        "tags" : [
        ]
      }
    ],
    "commit" : "c992eda9856a66a5a7fc907a95d148ad07dd5f08",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +992,996 @@      .createWithDefault(true)\n\n  val EXTENDED_EVENT_INFO = buildConf(\"spark.sql.extendedEventInfo\")\n    .doc(\"Enables extended SQL event information, for example storing additional information\" +\n      \" about certain operators such as InMemoryTableScan in SparkPlanInfo\")"
  },
  {
    "id" : "deb0c310-72b0-420a-8ff8-11bd2422abc8",
    "prId" : 27372,
    "prUrl" : "https://github.com/apache/spark/pull/27372#pullrequestreview-349192595",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b21e5bb5-9a00-4293-af4e-57dfcb18f87c",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "We need `internal()` for legacy options? Currently, some of the legacy options have `internal()` and the others have not. Any consistent policy for that?",
        "createdAt" : "2020-01-28T08:00:31Z",
        "updatedAt" : "2020-01-28T23:16:25Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "0ba477f0-5856-4469-884f-cefeff38f6b6",
        "parentId" : "b21e5bb5-9a00-4293-af4e-57dfcb18f87c",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I don't think we have such thing. Seems like it has been just case-by-case.",
        "createdAt" : "2020-01-28T08:24:16Z",
        "updatedAt" : "2020-01-28T23:16:25Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "bbfde53aaf2dc0a9b17e96760c48b97cb1be9d54",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +2119,2123 @@  val LEGACY_ADD_DIRECTORY_USING_RECURSIVE =\n    buildConf(\"spark.sql.legacy.addDirectory.recursive.enabled\")\n      .internal()\n      .doc(\"When true, users can add directory by passing path of a directory to ADD FILE \" +\n        \"command of SQL. If false, then only a single file can be added.\")"
  },
  {
    "id" : "832632d6-4837-4eac-991b-d6b829fc8db9",
    "prId" : 27346,
    "prUrl" : "https://github.com/apache/spark/pull/27346#pullrequestreview-355871275",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e05e68b2-4b92-43b0-b399-816d00bb5efc",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "now this is inconsistent with the exchange reuse: `spark.sql.exchange.reuse`\r\n\r\ncc @rxin @maryannxue ",
        "createdAt" : "2020-02-10T10:09:03Z",
        "updatedAt" : "2020-02-10T10:09:04Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "fe88cc7c-d7fd-4a6a-9171-77a88e4e6056",
        "parentId" : "e05e68b2-4b92-43b0-b399-816d00bb5efc",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "It seems we currently have the three patterns for boolean configs;\r\n - spark.sql.xxx.enabled, e.g., `spark.sql.optimizer.dynamicPartitionPruning.enabled`\r\n - spark.sql.enableXXX, e.g., `spark.sql.inMemoryColumnarStorage.enableVectorizedReader`\r\n - the names without `enabled`, e.g., `spark.sql.exchange.reuse`\r\n\r\nWe need a consistent rule for these boolean configs?\r\n",
        "createdAt" : "2020-02-10T11:40:18Z",
        "updatedAt" : "2020-02-10T11:40:18Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "cebb770b8663023a511ff7de23d55fc238e447bb",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1054,1058 @@    .createWithDefault(true)\n\n  val SUBQUERY_REUSE_ENABLED = buildConf(\"spark.sql.execution.subquery.reuse.enabled\")\n    .internal()\n    .doc(\"When true, the planner will try to find out duplicated subqueries and re-use them.\")"
  },
  {
    "id" : "81a6f504-8aa8-46a1-aeda-06351ad08530",
    "prId" : 27293,
    "prUrl" : "https://github.com/apache/spark/pull/27293#pullrequestreview-351335286",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2e4f43ce-babd-4f72-9a45-4dbdc54323d5",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Since this become a variable from now, we need to add `.checkValue(` to prevent accidental user mistakes.",
        "createdAt" : "2020-01-22T18:25:58Z",
        "updatedAt" : "2020-01-22T18:28:21Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "99c2358d-943c-4420-a6f0-b48e60bccaca",
        "parentId" : "2e4f43ce-babd-4f72-9a45-4dbdc54323d5",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Maybe, `> 0`? Do you think we have a reasonable upper bound for this, too?",
        "createdAt" : "2020-01-22T18:29:03Z",
        "updatedAt" : "2020-01-22T18:29:04Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "06e1c6be-d48f-4f79-9611-45dc6487c5e3",
        "parentId" : "2e4f43ce-babd-4f72-9a45-4dbdc54323d5",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "Lower bound checking might be enough, done in https://github.com/apache/spark/pull/27413/commits/ac8b771c1264fc8069a61a3cdb338f4ac6c406af.",
        "createdAt" : "2020-01-31T07:45:39Z",
        "updatedAt" : "2020-01-31T07:45:40Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "06b3cb8109ea3e68aae54141d1d91ebb87f479c5",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +2162,2166 @@        \"`AlterTableAddPartitionCommand` to add partitions into table. The smaller \" +\n        \"batch size is, the less memory is required for the real handler, e.g. Hive Metastore.\")\n      .intConf\n      .createWithDefault(100)\n"
  },
  {
    "id" : "c56a0413-8326-4b84-b26e-033c4fc8b0bf",
    "prId" : 27184,
    "prUrl" : "https://github.com/apache/spark/pull/27184#pullrequestreview-343671253",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e7a8bb5c-5a39-427e-9129-caab6584410c",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "For such a change, could we have an item in the migration guide? ",
        "createdAt" : "2020-01-16T00:50:46Z",
        "updatedAt" : "2020-01-16T00:50:46Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "5a48bb96-d5a0-4646-95ce-e9f873fe0cc6",
        "parentId" : "e7a8bb5c-5a39-427e-9129-caab6584410c",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Sure. ~Can we handle that after releasing 2.4.5 since It's a documentation change?~",
        "createdAt" : "2020-01-16T03:41:02Z",
        "updatedAt" : "2020-01-16T04:04:35Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "b0006d01-9e16-46dd-a524-112fafeeedc4",
        "parentId" : "e7a8bb5c-5a39-427e-9129-caab6584410c",
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "Thank you!",
        "createdAt" : "2020-01-16T04:46:14Z",
        "updatedAt" : "2020-01-16T04:46:15Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      }
    ],
    "commit" : "6b92a9f67e4a17ce5451580685ca9c11cf08516b",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +2163,2167 @@\n  val LEGACY_MSSQLSERVER_NUMERIC_MAPPING_ENABLED =\n    buildConf(\"spark.sql.legacy.mssqlserver.numericMapping.enabled\")\n      .internal()\n      .doc(\"When true, use legacy MySqlServer SMALLINT and REAL type mapping.\")"
  },
  {
    "id" : "3e618d9f-5cee-407e-9f63-abc5a9e17263",
    "prId" : 27184,
    "prUrl" : "https://github.com/apache/spark/pull/27184#pullrequestreview-343658398",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8eeaae51-2749-416d-9d44-f1ff4a38c874",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "It is not very clear what is the legacy MySqlServer SMALLINT and REAL type mapping if they do not read the PR. Could we make it clear? The behavior when the value is set to true and the behavior when the value is false",
        "createdAt" : "2020-01-16T00:55:51Z",
        "updatedAt" : "2020-01-16T00:55:51Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "2eaca55f-efc7-4cf5-b2a9-75238dbd2eb8",
        "parentId" : "8eeaae51-2749-416d-9d44-f1ff4a38c874",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Of course, I'd like to mention it at the migration guide fully.",
        "createdAt" : "2020-01-16T03:41:57Z",
        "updatedAt" : "2020-01-16T03:41:57Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "6b92a9f67e4a17ce5451580685ca9c11cf08516b",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +2165,2169 @@    buildConf(\"spark.sql.legacy.mssqlserver.numericMapping.enabled\")\n      .internal()\n      .doc(\"When true, use legacy MySqlServer SMALLINT and REAL type mapping.\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "f135640e-5557-4c05-be31-20b3dae45b8e",
    "prId" : 27092,
    "prUrl" : "https://github.com/apache/spark/pull/27092#pullrequestreview-339804911",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "15b51f9d-a83b-430e-9b33-79e653e9c767",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "I haven't found where this config is used. We can remove it, I think.",
        "createdAt" : "2020-01-08T11:31:20Z",
        "updatedAt" : "2020-01-09T05:42:10Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "faf88e8f461842e1b7a549088ebcacf0ec8e4cae",
    "line" : 40,
    "diffHunk" : "@@ -1,1 +2166,2170 @@  val deprecatedSQLConfigs: Map[String, DeprecatedConfig] = {\n    val configs = Seq(\n      DeprecatedConfig(VARIABLE_SUBSTITUTE_DEPTH.key, \"2.1\",\n        \"The SQL config is not used by Spark anymore.\"),\n      DeprecatedConfig(PANDAS_RESPECT_SESSION_LOCAL_TIMEZONE.key, \"2.3\","
  },
  {
    "id" : "4c0aa01d-a639-4b23-80d3-08466e18084a",
    "prId" : 27092,
    "prUrl" : "https://github.com/apache/spark/pull/27092#pullrequestreview-339816586",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "94bca81a-1e40-49db-ba35-c9c5502707ef",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "@cloud-fan Regarding to your comment https://github.com/apache/spark/pull/19868#issuecomment-571957075, `spark.sql.hive.verifyPartitionPath` can be changed at runtime but `spark.files.ignoreMissingFiles` cannot be. Is it fair replacement?",
        "createdAt" : "2020-01-08T11:36:57Z",
        "updatedAt" : "2020-01-09T05:42:10Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "264816c8-2eab-499f-8b42-a452ffd457cb",
        "parentId" : "94bca81a-1e40-49db-ba35-c9c5502707ef",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "We can just slightly reword that users can use `spark.files.ignoreMissingFiles` instead of saying it's a replacement if you're concerned.  I don't believe this configuration is commonly used enough, and it should be fine.\r\n\r\nIf users find this is unreasonable, we can un-deprecate it later given the feedback.",
        "createdAt" : "2020-01-08T11:56:09Z",
        "updatedAt" : "2020-01-09T05:42:10Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "faf88e8f461842e1b7a549088ebcacf0ec8e4cae",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +711,715 @@    .doc(\"When true, check all the partition paths under the table\\'s root directory \" +\n         \"when reading data stored in HDFS. This configuration will be deprecated in the future \" +\n         s\"releases and replaced by ${SPARK_IGNORE_MISSING_FILES.key}.\")\n    .booleanConf\n    .createWithDefault(false)"
  },
  {
    "id" : "ed4fbd63-ddfa-4da7-9c67-000c022d7d4a",
    "prId" : 27057,
    "prUrl" : "https://github.com/apache/spark/pull/27057#pullrequestreview-337926558",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4f0d6709-a1ef-4281-85f2-ab81eb5b8e72",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Looks a nice feature. Just a question; any policy about when these entries will be removed in this list? We need to keep them forever, or we can remove them sometime?",
        "createdAt" : "2020-01-02T23:32:23Z",
        "updatedAt" : "2020-01-02T23:32:23Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "ffd09113-48d3-4641-a4b2-ee4226075150",
        "parentId" : "4f0d6709-a1ef-4281-85f2-ab81eb5b8e72",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I think we can remove those after few minor releases or when we bump up the major release.",
        "createdAt" : "2020-01-03T01:33:09Z",
        "updatedAt" : "2020-01-03T01:39:13Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "e9e63ae1adcb43ee404f6ebb1be9c86fac9c67e8",
    "line" : 29,
    "diffHunk" : "@@ -1,1 +196,200 @@      RemovedConfig(\"spark.sql.legacy.compareDateTimestampInTimestamp\", \"3.0.0\", \"true\",\n        \"It was removed to prevent errors like SPARK-23549 for non-default value.\")\n    )\n\n    Map(configs.map { cfg => cfg.key -> cfg } : _*)"
  }
]