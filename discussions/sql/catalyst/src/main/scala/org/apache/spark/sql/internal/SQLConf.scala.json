[
  {
    "id" : "1006393a-1dcd-46cb-9648-f462f3c15105",
    "prId" : 33655,
    "prUrl" : "https://github.com/apache/spark/pull/33655#pullrequestreview-724508457",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6f771c2c-d596-4ca9-a339-a39044e93897",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Could you add this check into `spark.sql.adaptive.advisoryPartitionSizeInBytes`, too?",
        "createdAt" : "2021-08-05T19:11:39Z",
        "updatedAt" : "2021-08-05T19:11:39Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "2f3635cd-b39d-4b85-bc81-36325b1b97f7",
        "parentId" : "6f771c2c-d596-4ca9-a339-a39044e93897",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "We can't add check in `ADVISORY_PARTITION_SIZE_IN_BYTES`, because it falls back to `SHUFFLE_TARGET_POSTSHUFFLE_INPUT_SIZE`, which is this conf.",
        "createdAt" : "2021-08-06T03:21:40Z",
        "updatedAt" : "2021-08-06T03:21:41Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "7d5b09ab-ea22-4004-b963-7a431f6b59bc",
        "parentId" : "6f771c2c-d596-4ca9-a339-a39044e93897",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "@cloud-fan . If we set negative values to `advisoryPartitionSizeInBytes` explicitly, it will not fallback to here. If we want to say, `advisoryPartitionSizeInBytes must be positive`, we should check both places, shouldn't we?",
        "createdAt" : "2021-08-06T16:06:34Z",
        "updatedAt" : "2021-08-06T16:06:34Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "eeeff3ae6bb62eedeb09011cc99514d240a8b7d0",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +481,485 @@      .version(\"1.6.0\")\n      .bytesConf(ByteUnit.BYTE)\n      .checkValue(_ > 0, \"advisoryPartitionSizeInBytes must be positive\")\n      .createWithDefaultString(\"64MB\")\n"
  },
  {
    "id" : "98d04720-367d-4e9e-8c61-371061b7bdc3",
    "prId" : 33444,
    "prUrl" : "https://github.com/apache/spark/pull/33444#pullrequestreview-711555608",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "23cb04d3-961d-4052-92fa-d13cc850bf24",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "indentation ?",
        "createdAt" : "2021-07-21T10:35:50Z",
        "updatedAt" : "2021-07-21T10:38:09Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      },
      {
        "id" : "ec7c2243-c1ee-4f0d-a674-6cd9958f62a0",
        "parentId" : "23cb04d3-961d-4052-92fa-d13cc850bf24",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "Oh, this is on purpose.",
        "createdAt" : "2021-07-21T11:40:24Z",
        "updatedAt" : "2021-07-21T11:40:25Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "221e62227f09d7e29fbab5c674af29b1f829b91e",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +3978,3982 @@  def timestampType: AtomicType = getConf(TIMESTAMP_TYPE) match {\n    // SPARK-36227: Remove TimestampNTZ type support in Spark 3.2 with minimal code changes.\n    //              The configuration `TIMESTAMP_TYPE` is only effective for testing in Spark 3.2.\n    case \"TIMESTAMP_NTZ\" if Utils.isTesting =>\n      TimestampNTZType"
  },
  {
    "id" : "28a8d916-3fd7-42f4-91a2-f8c55daf94f6",
    "prId" : 33348,
    "prUrl" : "https://github.com/apache/spark/pull/33348#pullrequestreview-706916663",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a27a7d03-c265-47e0-85f8-c44ef4ed3fb8",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Is it okay to reuse the hive config for the purpose? Sounds more like `spark.sql.metastorePartitionPruning` now?",
        "createdAt" : "2021-07-15T02:47:06Z",
        "updatedAt" : "2021-07-15T02:47:06Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "da1fbcbd-7551-4e1f-be7a-3b36855d6bd5",
        "parentId" : "a27a7d03-c265-47e0-85f8-c44ef4ed3fb8",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "yeah I agree, that's the exactly same point I mentioned in the JIRA ..",
        "createdAt" : "2021-07-15T02:52:06Z",
        "updatedAt" : "2021-07-15T02:52:07Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "98cca643-0333-477b-a4b6-e1c7fb947bb2",
        "parentId" : "a27a7d03-c265-47e0-85f8-c44ef4ed3fb8",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "The reason I reused the config is because it is not about Hive tables, but HMS-based catalog which is shared by both Hive tables and non-Hive tables in Spark. This is similar to `spark.sql.hive.manageFilesourcePartitions` which is used by data source tables even though it has `hive` in its name.\r\n\r\n",
        "createdAt" : "2021-07-15T04:38:10Z",
        "updatedAt" : "2021-07-15T04:38:10Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      }
    ],
    "commit" : "f686496a2a21b1075ff182dc936643d71e1e85e3",
    "line" : 1,
    "diffHunk" : "@@ -1,1 +977,981 @@    .createWithDefault(false)\n\n  val HIVE_METASTORE_PARTITION_PRUNING =\n    buildConf(\"spark.sql.hive.metastorePartitionPruning\")\n      .doc(\"When true, some predicates will be pushed down into the Hive metastore so that \" +"
  },
  {
    "id" : "7d2b99d9-a95b-4050-a90c-08628d93a8cb",
    "prId" : 33348,
    "prUrl" : "https://github.com/apache/spark/pull/33348#pullrequestreview-708603463",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "64a55233-e16f-4f4e-8d91-7013e680f0f9",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "`HIVE_MANAGE_FILESOURCE_PARTITIONS` mentions that when it is enabled, it uses the metastore to prune partitions during query planning. Maybe we should update the doc too.",
        "createdAt" : "2021-07-15T02:58:37Z",
        "updatedAt" : "2021-07-15T02:58:37Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "f69dd937-592d-4590-98f6-e49e3dc0e4a9",
        "parentId" : "64a55233-e16f-4f4e-8d91-7013e680f0f9",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Yea, we should mention that it only does so when `spark.sql.hive.metastorePartitionPruning` is turned on.",
        "createdAt" : "2021-07-15T04:39:19Z",
        "updatedAt" : "2021-07-15T04:39:19Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "790de5fb-4048-4338-87c5-c90c046b8e95",
        "parentId" : "64a55233-e16f-4f4e-8d91-7013e680f0f9",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Seems not update yet?",
        "createdAt" : "2021-07-16T08:11:40Z",
        "updatedAt" : "2021-07-16T08:11:41Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "e128d840-a738-4425-9328-e8d2df9fb4be",
        "parentId" : "64a55233-e16f-4f4e-8d91-7013e680f0f9",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Hmm I thought this needs to be updated if we don't have this PR, since Spark doesn't prune partitions for file data source tables on metastore side. But with this PR the description becomes correct?",
        "createdAt" : "2021-07-16T17:21:19Z",
        "updatedAt" : "2021-07-16T17:21:19Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "0ec79b21-30b3-4fb4-bc1a-6a217e38ff43",
        "parentId" : "64a55233-e16f-4f4e-8d91-7013e680f0f9",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Oh, I see. I should mention `spark.sql.hive.metastorePartitionPruning` there. Will update.",
        "createdAt" : "2021-07-16T17:22:15Z",
        "updatedAt" : "2021-07-16T17:22:15Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      }
    ],
    "commit" : "f686496a2a21b1075ff182dc936643d71e1e85e3",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +980,984 @@    buildConf(\"spark.sql.hive.metastorePartitionPruning\")\n      .doc(\"When true, some predicates will be pushed down into the Hive metastore so that \" +\n           \"unmatching partitions can be eliminated earlier.\")\n      .version(\"1.5.0\")\n      .booleanConf"
  },
  {
    "id" : "7238b7ae-65a9-47c1-a00d-f2a76aea8b8b",
    "prId" : 33176,
    "prUrl" : "https://github.com/apache/spark/pull/33176#pullrequestreview-697432760",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9478feea-b649-41dc-a071-782cb70ba33e",
        "parentId" : null,
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "The scope is bigger than this. It will affect timestamp literal, function `to_timestamp` and data source io, etc. We will get to that later.",
        "createdAt" : "2021-07-01T16:15:44Z",
        "updatedAt" : "2021-07-01T16:15:44Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "6c6bf3a04572a832ff9224c1a3cd61b81784c625",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +2828,2832 @@  val TIMESTAMP_TYPE =\n    buildConf(\"spark.sql.timestampType\")\n      .doc(\"Configures the default timestamp type of Spark SQL, including SQL DDL and Cast \" +\n        s\"clause. Setting the configuration as ${TimestampTypes.TIMESTAMP_NTZ.toString} will \" +\n        \"use TIMESTAMP WITHOUT TIME ZONE as the default type while putting it as \" +"
  },
  {
    "id" : "664cb507-9819-4442-8415-2a6f9270ddf1",
    "prId" : 33176,
    "prUrl" : "https://github.com/apache/spark/pull/33176#pullrequestreview-697488439",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "17cb3762-17e7-40ee-bcef-26125f26516e",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "I wonder about types literals, for instance `timestamp'2021-07-01 01:02:03'`. Should the config influence on it too?",
        "createdAt" : "2021-07-01T16:20:01Z",
        "updatedAt" : "2021-07-01T16:26:02Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "32a32ad8-5877-4026-b429-fbc3ae13a782",
        "parentId" : "17cb3762-17e7-40ee-bcef-26125f26516e",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "Yes, I mentioned in https://github.com/apache/spark/pull/33176#discussion_r662426271. The type literal logic with TIMESTAMP_NTZ:\r\n1. if there is no time zone part, return timestamp without time zone literal\r\n2. otherwise, return timestamp with local time zone. ",
        "createdAt" : "2021-07-01T17:21:11Z",
        "updatedAt" : "2021-07-01T17:21:11Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "6c6bf3a04572a832ff9224c1a3cd61b81784c625",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +2828,2832 @@  val TIMESTAMP_TYPE =\n    buildConf(\"spark.sql.timestampType\")\n      .doc(\"Configures the default timestamp type of Spark SQL, including SQL DDL and Cast \" +\n        s\"clause. Setting the configuration as ${TimestampTypes.TIMESTAMP_NTZ.toString} will \" +\n        \"use TIMESTAMP WITHOUT TIME ZONE as the default type while putting it as \" +"
  },
  {
    "id" : "5a76519c-a40b-470c-b393-53a2c2e24100",
    "prId" : 33176,
    "prUrl" : "https://github.com/apache/spark/pull/33176#pullrequestreview-697536637",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "734df2b1-6200-47bb-b792-9c6f74924bc9",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "I thought the compiler automatically calls the `toString` method in string interpolations, so, you can skip it. Or I am wrong?",
        "createdAt" : "2021-07-01T18:17:53Z",
        "updatedAt" : "2021-07-01T18:17:54Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "6c6bf3a04572a832ff9224c1a3cd61b81784c625",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +2829,2833 @@    buildConf(\"spark.sql.timestampType\")\n      .doc(\"Configures the default timestamp type of Spark SQL, including SQL DDL and Cast \" +\n        s\"clause. Setting the configuration as ${TimestampTypes.TIMESTAMP_NTZ.toString} will \" +\n        \"use TIMESTAMP WITHOUT TIME ZONE as the default type while putting it as \" +\n        s\"${TimestampTypes.TIMESTAMP_LTZ.toString} will use TIMESTAMP WITH LOCAL TIME ZONE. \" +"
  },
  {
    "id" : "67ddb6c1-31ff-442e-8fdf-ce60be0b2def",
    "prId" : 33172,
    "prUrl" : "https://github.com/apache/spark/pull/33172#pullrequestreview-697345900",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "163d76ce-a571-4e21-aa67-dc79b3707e30",
        "parentId" : null,
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "better to update the doc of\r\nADVISORY_PARTITION_SIZE_IN_BYTES",
        "createdAt" : "2021-07-01T14:21:12Z",
        "updatedAt" : "2021-07-01T14:21:50Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "edc1a263-a317-407b-9471-da04309929b1",
        "parentId" : "163d76ce-a571-4e21-aa67-dc79b3707e30",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "`ADVISORY_PARTITION_SIZE_IN_BYTES` doesn't mention `COALESCE_PARTITIONS_MIN_PARTITION_NUM` either. I think it's fine to only refer to \"big\" confs in the \"small\" confs, not from the other side.",
        "createdAt" : "2021-07-01T14:54:02Z",
        "updatedAt" : "2021-07-01T14:54:02Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "51546e87c33064c80f902d4eecff1f640222dfb2",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +529,533 @@\n  val COALESCE_PARTITIONS_PARALLELISM_FIRST =\n    buildConf(\"spark.sql.adaptive.coalescePartitions.parallelismFirst\")\n      .doc(\"When true, Spark ignores the target size specified by \" +\n        s\"'${ADVISORY_PARTITION_SIZE_IN_BYTES.key}' (default 64MB) when coalescing contiguous \" +"
  },
  {
    "id" : "0871aeb8-ff77-4d83-93d2-f707dd159d71",
    "prId" : 33172,
    "prUrl" : "https://github.com/apache/spark/pull/33172#pullrequestreview-697522081",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4382901c-1208-4067-8e57-761fa2ec15e2",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "@viirya is it clearer to you now?",
        "createdAt" : "2021-07-01T17:22:59Z",
        "updatedAt" : "2021-07-01T17:23:05Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "56dc5122-5762-4883-bd39-4554eb66ac2c",
        "parentId" : "4382901c-1208-4067-8e57-761fa2ec15e2",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "okay",
        "createdAt" : "2021-07-01T18:03:21Z",
        "updatedAt" : "2021-07-01T18:03:21Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "51546e87c33064c80f902d4eecff1f640222dfb2",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +536,540 @@        \"This is to avoid performance regression when enabling adaptive query execution. \" +\n        \"It's recommended to set this config to false and respect the target size specified by \" +\n        s\"'${ADVISORY_PARTITION_SIZE_IN_BYTES.key}'.\")\n      .version(\"3.2.0\")\n      .booleanConf"
  },
  {
    "id" : "9a073cec-0a03-4dab-a0cb-ce1e0fb379b7",
    "prId" : 33081,
    "prUrl" : "https://github.com/apache/spark/pull/33081#pullrequestreview-706975183",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a29f8726-aefb-4347-8e58-64c8514b0fcf",
        "parentId" : null,
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "How about `spark.sql.streaming.sessionWindow.localMerge.enabled` or `spark.sql.streaming.sessionWindow. mergeSessionsInLocalPartition.enabled`?",
        "createdAt" : "2021-07-15T04:16:35Z",
        "updatedAt" : "2021-07-15T04:51:30Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      },
      {
        "id" : "382323ca-d659-4518-a500-4261e80a4a94",
        "parentId" : "a29f8726-aefb-4347-8e58-64c8514b0fcf",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "Comparing with the similar logic of AggUtils, maybe we can also remove this config? Just always do the local merge?",
        "createdAt" : "2021-07-15T04:22:12Z",
        "updatedAt" : "2021-07-15T04:51:30Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      },
      {
        "id" : "0c1f4b70-b3aa-4cac-bf22-b9e60a90ff91",
        "parentId" : "a29f8726-aefb-4347-8e58-64c8514b0fcf",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Yea, is it necessary to have this config? Seems we can always do it.",
        "createdAt" : "2021-07-15T05:17:31Z",
        "updatedAt" : "2021-07-15T05:39:10Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "87937ab3-25cf-4ae6-81d3-ad2a7cf3d594",
        "parentId" : "a29f8726-aefb-4347-8e58-64c8514b0fcf",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "As I explained on the doc method, this would incur additional \"logical sort\", so only useful when there're lots of input rows which are going to be consolidated into same session. The benefit is dependent on the characteristic of data.\r\n\r\nIf we want to pick one between two to simplify, it would be probably safer to remove local aggregation.",
        "createdAt" : "2021-07-15T06:34:59Z",
        "updatedAt" : "2021-07-15T06:34:59Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "bbade3501f16e9437ba9af4feca3e2029785d273",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1612,1616 @@\n  val STREAMING_SESSION_WINDOW_MERGE_SESSIONS_IN_LOCAL_PARTITION =\n    buildConf(\"spark.sql.streaming.sessionWindow.merge.sessions.in.local.partition\")\n      .internal()\n      .doc(\"When true, streaming session window sorts and merge sessions in local partition \" +"
  },
  {
    "id" : "67e1a57b-38af-4853-a248-57634c7f8a8d",
    "prId" : 33081,
    "prUrl" : "https://github.com/apache/spark/pull/33081#pullrequestreview-706904781",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "990650e9-93f6-4dfc-8638-327536180809",
        "parentId" : null,
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "nit: version",
        "createdAt" : "2021-07-15T04:22:21Z",
        "updatedAt" : "2021-07-15T04:51:30Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "bbade3501f16e9437ba9af4feca3e2029785d273",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +1618,1622 @@        \"there're lots of rows in a batch being assigned to same sessions.\")\n      .version(\"3.2.0\")\n      .booleanConf\n      .createWithDefault(false)\n"
  },
  {
    "id" : "6ae99fdc-36f9-41be-b878-99b083b7819b",
    "prId" : 32944,
    "prUrl" : "https://github.com/apache/spark/pull/32944#pullrequestreview-698959883",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d6bc2330-84cb-4b5d-956f-ea1ef838a53d",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "the only think is that the version has to be 3.3.0 since we cut the branch now. Since this PR won't likely affect anything in the main code, I am okay with merging to 3.2.0 either tho. I will leave it to @cloud-fan and you.",
        "createdAt" : "2021-07-05T00:31:06Z",
        "updatedAt" : "2021-07-05T00:31:25Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "540b702e-dae5-43da-ac24-5af339be85dd",
        "parentId" : "d6bc2330-84cb-4b5d-956f-ea1ef838a53d",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "3.2 is the first version that enables AQE by default, and this seems to be a useful extension. Let's include it in 3.2.",
        "createdAt" : "2021-07-05T09:05:55Z",
        "updatedAt" : "2021-07-05T09:05:55Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "ac5c12186035a47bbe7c0b1891564066db676354",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +683,687 @@      .doc(\"The custom cost evaluator class to be used for adaptive execution. If not being set,\" +\n        \" Spark will use its own SimpleCostEvaluator by default.\")\n      .version(\"3.2.0\")\n      .stringConf\n      .createOptional"
  },
  {
    "id" : "349c325b-92af-4409-b86f-3b3eaf20ba32",
    "prId" : 32865,
    "prUrl" : "https://github.com/apache/spark/pull/32865#pullrequestreview-681525932",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6438bf5a-e566-443d-9155-99eabe3d6f30",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Q: we need the two separate locks for the static/non-static configs?",
        "createdAt" : "2021-06-11T01:34:16Z",
        "updatedAt" : "2021-06-11T01:34:16Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "b6e0a028-f58e-4b0f-a06d-c0740d96782f",
        "parentId" : "6438bf5a-e566-443d-9155-99eabe3d6f30",
        "authorId" : "56f79eae-eef9-4cac-8f0c-c788c9a86e3a",
        "body" : "Thank you for your review! I think these are two different sets of configs, so using two separate locks should be better.",
        "createdAt" : "2021-06-11T07:22:21Z",
        "updatedAt" : "2021-06-11T07:23:43Z",
        "lastEditedBy" : "56f79eae-eef9-4cac-8f0c-c788c9a86e3a",
        "tags" : [
        ]
      }
    ],
    "commit" : "994cab24f91d681f187b3062da0de62a45b0ba6a",
    "line" : 30,
    "diffHunk" : "@@ -1,1 +60,64 @@  private[this] var sqlConfEntries: util.Map[String, ConfigEntry[_]] = util.Collections.emptyMap()\n\n  private[this] val staticConfKeysUpdateLock = new Object\n\n  @volatile"
  },
  {
    "id" : "666dfd56-fa92-4bc9-bbfa-5e1fbe5c3881",
    "prId" : 32865,
    "prUrl" : "https://github.com/apache/spark/pull/32865#pullrequestreview-681413755",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a1e8dfea-0a9d-4e29-ba06-b9595b1e7d61",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "~Is `private` enough?~ Never mind~",
        "createdAt" : "2021-06-11T03:19:55Z",
        "updatedAt" : "2021-06-11T03:22:22Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "994cab24f91d681f187b3062da0de62a45b0ba6a",
    "line" : 51,
    "diffHunk" : "@@ -1,1 +80,84 @@  }\n\n  private[internal] def getConfigEntry(key: String): ConfigEntry[_] = {\n    sqlConfEntries.get(key)\n  }"
  },
  {
    "id" : "8fb0e826-7178-41c4-839f-d9139027d4ac",
    "prId" : 32550,
    "prUrl" : "https://github.com/apache/spark/pull/32550#pullrequestreview-670156365",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1f734a2c-db32-43bc-8e73-8de7ee83e8df",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I would say:\r\n- \"If this value is not smaller than\" => \"If this value is larger than\"\r\n- \"all the partition size are not larger than this config\" => \"all the partition size are smaller than this config\"",
        "createdAt" : "2021-05-27T10:40:23Z",
        "updatedAt" : "2021-05-27T10:40:23Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "394d2736-4b78-4192-8b09-b2480488de9d",
        "parentId" : "1f734a2c-db32-43bc-8e73-8de7ee83e8df",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "I thought about this but the formula used equal, that said it would be `If this value is larger than or equal to xxx`. Do you think it's better ?",
        "createdAt" : "2021-05-27T13:05:50Z",
        "updatedAt" : "2021-05-27T13:05:50Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      }
    ],
    "commit" : "957215452e9df57f5e0424f63556897ea928744f",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +602,606 @@    buildConf(\"spark.sql.adaptive.maxShuffledHashJoinLocalMapThreshold\")\n      .doc(\"Configures the maximum size in bytes per partition that can be allowed to build \" +\n        \"local hash map. If this value is not smaller than \" +\n        s\"${ADVISORY_PARTITION_SIZE_IN_BYTES.key} and all the partition size are not larger \" +\n        \"than this config, join selection prefer to use shuffled hash join instead of \" +"
  },
  {
    "id" : "fe8b4a90-7333-4727-bc6c-9cd97deb7921",
    "prId" : 32129,
    "prUrl" : "https://github.com/apache/spark/pull/32129#pullrequestreview-633469192",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7a8df72d-8cd7-4900-ac20-42737ba417d8",
        "parentId" : null,
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "Moving `ANSI_ENABLED` to the front so that other configurations can refer to it without compiling errors.",
        "createdAt" : "2021-04-12T12:20:33Z",
        "updatedAt" : "2021-04-12T17:31:41Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "62cee4f24ed49d9c8f967f82267dbd3f3180c32c",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +207,211 @@    .createWithDefault(100)\n\n  val ANSI_ENABLED = buildConf(\"spark.sql.ansi.enabled\")\n    .doc(\"When true, Spark SQL uses an ANSI compliant dialect instead of being Hive compliant. \" +\n      \"For example, Spark will throw an exception at runtime instead of returning null results \" +"
  },
  {
    "id" : "3a2baac8-d3ae-4330-8fd3-16c76a499250",
    "prId" : 32049,
    "prUrl" : "https://github.com/apache/spark/pull/32049#pullrequestreview-627553202",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0ec09a71-2e6d-436b-9565-6a026ba7a0fa",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Just curious - would anyone ever _not_ want to push it down?\r\nI'm surprised, I thought we already did this!\r\nCC @cloud-fan ",
        "createdAt" : "2021-04-04T15:01:56Z",
        "updatedAt" : "2021-04-28T06:21:16Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "3efb61fd-870a-420f-b2fd-2ece2693ba71",
        "parentId" : "0ec09a71-2e6d-436b-9565-6a026ba7a0fa",
        "authorId" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "body" : "@srowen Hello Sean :)\r\nActually we only have filter push down for parquet, not aggregate push down yet. I will probably change the default to true after this PR gets reviewed and fully tested.",
        "createdAt" : "2021-04-04T16:12:52Z",
        "updatedAt" : "2021-04-28T06:21:16Z",
        "lastEditedBy" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "tags" : [
        ]
      }
    ],
    "commit" : "5c2b630a61d8ea9263116c37e154884a5cabd2ca",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +773,777 @@      .createWithDefault(10)\n\n  val PARQUET_AGGREGATE_PUSHDOWN_ENABLED = buildConf(\"spark.sql.parquet.aggregatePushdown\")\n    .doc(\"Enables Parquet aggregate push-down optimization when set to true.\")\n    .version(\"3.2.0\")"
  },
  {
    "id" : "432792bf-a654-4dd4-8558-d66da8540216",
    "prId" : 32049,
    "prUrl" : "https://github.com/apache/spark/pull/32049#pullrequestreview-628362844",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3fd0db59-3c93-4deb-8a96-aeb7ad09116a",
        "parentId" : null,
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "Shall it be an internal config or not? Do we expect this one to be user-facing and tune it frequently?",
        "createdAt" : "2021-04-05T07:47:49Z",
        "updatedAt" : "2021-04-28T06:21:16Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      },
      {
        "id" : "868bb4bc-c3ca-415d-9a1c-69bbaf17d6ac",
        "parentId" : "3fd0db59-3c93-4deb-8a96-aeb7ad09116a",
        "authorId" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "body" : "Thanks for reviewing!\r\nI think this should be similar to `PARQUET_FILTER_PUSHDOWN_ENABLED` and be a user-facing config. I guess we can default it to true in the future after we have more testing.",
        "createdAt" : "2021-04-06T01:10:13Z",
        "updatedAt" : "2021-04-28T06:21:16Z",
        "lastEditedBy" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "tags" : [
        ]
      }
    ],
    "commit" : "5c2b630a61d8ea9263116c37e154884a5cabd2ca",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +774,778 @@\n  val PARQUET_AGGREGATE_PUSHDOWN_ENABLED = buildConf(\"spark.sql.parquet.aggregatePushdown\")\n    .doc(\"Enables Parquet aggregate push-down optimization when set to true.\")\n    .version(\"3.2.0\")\n    .booleanConf"
  },
  {
    "id" : "c05d7789-3578-419c-9318-e0f52643bbf8",
    "prId" : 31986,
    "prUrl" : "https://github.com/apache/spark/pull/31986#pullrequestreview-656212437",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2ddbf876-4228-417f-88c1-779a697b1801",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "`in.memory` looks weird to me. Maybe just `inMemory`?",
        "createdAt" : "2021-05-11T00:52:31Z",
        "updatedAt" : "2021-05-11T04:07:22Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "275ee6db-50f4-45e4-a179-68d19d8fef3f",
        "parentId" : "2ddbf876-4228-417f-88c1-779a697b1801",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "I agree with you, but the name semantic follows existing configurations. Please search `_IN_MEMORY_THRESHOLD` into codebase.",
        "createdAt" : "2021-05-11T01:34:56Z",
        "updatedAt" : "2021-05-11T04:07:22Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "8c2eed82d553ddacb36088954232ae704a855094",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +2002,2006 @@\n  val SESSION_WINDOW_BUFFER_IN_MEMORY_THRESHOLD =\n    buildConf(\"spark.sql.sessionWindow.buffer.in.memory.threshold\")\n      .internal()\n      .doc(\"Threshold for number of windows guaranteed to be held in memory by the \" +"
  },
  {
    "id" : "02585522-117f-4689-a4b7-0377b1e7f4b4",
    "prId" : 31689,
    "prUrl" : "https://github.com/apache/spark/pull/31689#pullrequestreview-601362429",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "695bd7d7-5aaf-4850-9168-259943cbe925",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Seems a reasonable fix cc: @srowen @cloud-fan @viirya ",
        "createdAt" : "2021-03-02T02:57:07Z",
        "updatedAt" : "2021-03-02T02:57:07Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "4ffa2576782e410cef4d5cb3d8865362679ff11c",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +55,59 @@\n  private[sql] val sqlConfEntries =\n    new ConcurrentHashMap[String, ConfigEntry[_]]()\n\n  val staticConfKeys: java.util.Set[String] ="
  },
  {
    "id" : "a4769220-e41b-4012-a257-c72def286e7c",
    "prId" : 31578,
    "prUrl" : "https://github.com/apache/spark/pull/31578#pullrequestreview-592871935",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c43cccb4-5c6f-448d-9bda-920f914986bd",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Ur, is this guide correct? I guess the users are using `.format(\"avro\")` already.",
        "createdAt" : "2021-02-17T22:19:47Z",
        "updatedAt" : "2021-02-17T22:20:10Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "e27a2ca7-f0b2-4e6b-99fb-663c9f627c17",
        "parentId" : "c43cccb4-5c6f-448d-9bda-920f914986bd",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "The configs is about automatically mapping of `.format(\"com.databricks.spark.avro\")` to `.format(\"avro\")`, right? If we remove the config in the future, \"com.databricks.spark.avro\" will be not mapped to built in `avro`. So, in the guide, we recommend to change users code, and use the `avro` format directly.",
        "createdAt" : "2021-02-18T05:48:41Z",
        "updatedAt" : "2021-02-18T05:48:42Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "a2c2cfeb-41c8-49d0-8006-d1d38a49b6c9",
        "parentId" : "c43cccb4-5c6f-448d-9bda-920f914986bd",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Thanks. I was confused at that.",
        "createdAt" : "2021-02-18T05:53:29Z",
        "updatedAt" : "2021-02-18T05:53:30Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "660db04807559077244a31b916339a2876aeb0db",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +3148,3152 @@        s\"Use '${AVRO_REBASE_MODE_IN_READ.key}' instead.\"),\n      DeprecatedConfig(LEGACY_REPLACE_DATABRICKS_SPARK_AVRO_ENABLED.key, \"3.2\",\n        \"\"\"Use `.format(\"avro\")` in `DataFrameWriter` or `DataFrameReader` instead.\"\"\")\n    )\n"
  },
  {
    "id" : "bf78c885-f580-4fe8-96f5-6d3b01a4a5fa",
    "prId" : 31571,
    "prUrl" : "https://github.com/apache/spark/pull/31571#pullrequestreview-592680008",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f3f2f118-6858-414a-82e8-69197c7182df",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@gengliangwang do we need this conf? We do the same thing in `com.databricks.spark.csv` already by default internally. We could just make it exposed and deprecate this config.",
        "createdAt" : "2021-02-17T01:44:38Z",
        "updatedAt" : "2021-02-17T01:45:32Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "0decfaa4-6818-4e04-a3c7-37a831bfac6b",
        "parentId" : "f3f2f118-6858-414a-82e8-69197c7182df",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "In this way, I think it will address most of concerns raised.",
        "createdAt" : "2021-02-17T01:45:06Z",
        "updatedAt" : "2021-02-17T01:45:07Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "503c2e3f-6878-4840-9fba-89aa23159455",
        "parentId" : "f3f2f118-6858-414a-82e8-69197c7182df",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "And then we could remove both `com.databricks.spark.csv` and `com.databricks.spark.avro` fallbacks together in the future, of course (Spark 4.0?). I don't think it makes sense to keep both mapping forever.",
        "createdAt" : "2021-02-17T01:49:55Z",
        "updatedAt" : "2021-02-17T01:49:55Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "8bae371c-27c9-4541-8611-bdacce720505",
        "parentId" : "f3f2f118-6858-414a-82e8-69197c7182df",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Let's deprecate it and then remove it in later versions. It's OK if a deprecated internal config is mentioned in public docs for migration purpose.",
        "createdAt" : "2021-02-17T15:59:14Z",
        "updatedAt" : "2021-02-17T15:59:14Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "1b48cdcc-cf23-4d85-b3b8-34d7084f1420",
        "parentId" : "f3f2f118-6858-414a-82e8-69197c7182df",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Here is the PR https://github.com/apache/spark/pull/31578 to deprecate the config.",
        "createdAt" : "2021-02-17T18:29:52Z",
        "updatedAt" : "2021-02-17T18:29:52Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "1c55e9f4-584f-4073-a4e9-148e27dedbd7",
        "parentId" : "f3f2f118-6858-414a-82e8-69197c7182df",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Thank you all!",
        "createdAt" : "2021-02-17T22:21:28Z",
        "updatedAt" : "2021-02-17T22:21:28Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "c43224e894971ac9e500a0d59b1715f8f922caff",
    "line" : 3,
    "diffHunk" : "@@ -1,1 +2505,2509 @@\n  val LEGACY_REPLACE_DATABRICKS_SPARK_AVRO_ENABLED =\n    buildConf(\"spark.sql.legacy.replaceDatabricksSparkAvro.enabled\")\n      .doc(\"If it is set to true, the data source provider com.databricks.spark.avro is mapped \" +\n        \"to the built-in but external Avro data source module for backward compatibility.\")"
  },
  {
    "id" : "59e46a3b-9064-42bd-b86a-1edb93204b24",
    "prId" : 31421,
    "prUrl" : "https://github.com/apache/spark/pull/31421#pullrequestreview-581056259",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fef6313d-7d16-4b9b-b634-e728b5a49830",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "ditto, make it more specific.",
        "createdAt" : "2021-02-02T07:40:12Z",
        "updatedAt" : "2021-02-02T07:40:12Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "971ccbd5947924992a0c24acd40d40f1587d7741",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +2476,2480 @@    buildConf(\"spark.sql.legacy.parseNullPartitionSpecAsStringLiteral\")\n      .internal()\n      .doc(\"If it is set to true, a null partition value is parsed as a string literal of its \" +\n        \"text representation, e.g., string 'null'. Otherwise, null partition values are parsed \" +\n        \"as they are.\")"
  },
  {
    "id" : "d95b8dbd-fc0b-4832-8198-bddc8024c1ac",
    "prId" : 31421,
    "prUrl" : "https://github.com/apache/spark/pull/31421#pullrequestreview-581062903",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9e1b8ae8-f046-4888-92ec-e7ed6d12af47",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "the version should be 3.1.1, as we are going to backport this.",
        "createdAt" : "2021-02-02T07:40:33Z",
        "updatedAt" : "2021-02-02T07:40:33Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "aeeb1c7f-a5fe-44bd-8d2a-96c15e321097",
        "parentId" : "9e1b8ae8-f046-4888-92ec-e7ed6d12af47",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "should be 3.0.2 actually.",
        "createdAt" : "2021-02-02T07:42:52Z",
        "updatedAt" : "2021-02-02T07:42:52Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "7010947b-a6cf-4dfc-8df8-ec32d01cd91c",
        "parentId" : "9e1b8ae8-f046-4888-92ec-e7ed6d12af47",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Ah I see, then how should we place it in the migration guide? Repeat it 3 times in 3.0.2, 3.1.1 and 3.2.0 sections?",
        "createdAt" : "2021-02-02T07:46:40Z",
        "updatedAt" : "2021-02-02T07:46:40Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "935f5e60-073c-4953-abc4-d7b73f8a7ae3",
        "parentId" : "9e1b8ae8-f046-4888-92ec-e7ed6d12af47",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I think we have documented only in 3.0.2 so far in such cases ..",
        "createdAt" : "2021-02-02T07:50:42Z",
        "updatedAt" : "2021-02-02T07:50:42Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "971ccbd5947924992a0c24acd40d40f1587d7741",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +2479,2483 @@        \"text representation, e.g., string 'null'. Otherwise, null partition values are parsed \" +\n        \"as they are.\")\n      .version(\"3.2.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "1ab4d35f-d105-4f2e-958a-b486e6befd0a",
    "prId" : 31173,
    "prUrl" : "https://github.com/apache/spark/pull/31173#pullrequestreview-567895812",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2b9dd652-a11c-4863-8069-e650d2c942c3",
        "parentId" : null,
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "This change add a new configuration. cc @cloud-fan",
        "createdAt" : "2021-01-14T06:52:53Z",
        "updatedAt" : "2021-01-14T06:52:53Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      }
    ],
    "commit" : "5c8fa18aa9e1e0ff1436361a7b67055647f39d5c",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +3050,3054 @@     .version(\"3.2.0\")\n    .booleanConf\n    .createWithDefault(false)\n\n  /**"
  },
  {
    "id" : "6195228a-6811-4225-a90c-0d68a33dec01",
    "prId" : 31173,
    "prUrl" : "https://github.com/apache/spark/pull/31173#pullrequestreview-571894946",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "705e73a9-8036-4632-911f-7dc4b129545a",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "nit: Looks wrong indents.",
        "createdAt" : "2021-01-20T06:05:38Z",
        "updatedAt" : "2021-01-20T06:05:38Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "5c8fa18aa9e1e0ff1436361a7b67055647f39d5c",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +3047,3051 @@  val CLI_PRINT_HEADER =\n    buildConf(\"spark.sql.cli.print.header\")\n     .doc(\"When set to true, spark-sql CLI prints the names of the columns in query output.\")\n     .version(\"3.2.0\")\n    .booleanConf"
  },
  {
    "id" : "867990aa-fabf-45ae-8b06-a1ac2f1b4a4d",
    "prId" : 31002,
    "prUrl" : "https://github.com/apache/spark/pull/31002#pullrequestreview-560810087",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "288d0512-9ff0-43a5-8e76-7c923a8b44dc",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Maybe we should also fix in `DataFrameWriter.scala` and `readwriter.py` too.",
        "createdAt" : "2021-01-04T06:57:59Z",
        "updatedAt" : "2021-01-04T07:01:55Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "f6b7bfe7-13be-4729-a083-216806fd3195",
        "parentId" : "288d0512-9ff0-43a5-8e76-7c923a8b44dc",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Sure, thanks!",
        "createdAt" : "2021-01-04T06:58:17Z",
        "updatedAt" : "2021-01-04T07:01:55Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "ca1aa2c80e38147f87e339463258654c81827bdd",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +797,801 @@      \"`orc.compress` is specified in the table-specific options/properties, the precedence \" +\n      \"would be `compression`, `orc.compress`, `spark.sql.orc.compression.codec`.\" +\n      \"Acceptable values include: none, uncompressed, snappy, zlib, lzo, zstd.\")\n    .version(\"2.3.0\")\n    .stringConf"
  },
  {
    "id" : "6361a479-e057-43af-8540-f0d7ab70efb5",
    "prId" : 31002,
    "prUrl" : "https://github.com/apache/spark/pull/31002#pullrequestreview-560817427",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "519e4954-ef39-46e9-a395-04d65e4b8845",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Btw, we cannot use `lz4` for the orc format?",
        "createdAt" : "2021-01-04T07:11:18Z",
        "updatedAt" : "2021-01-04T07:11:19Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "5aeedc5d-a6ae-4152-91ee-f1089a4c4cd5",
        "parentId" : "519e4954-ef39-46e9-a395-04d65e4b8845",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Is it supported before, @maropu ?",
        "createdAt" : "2021-01-04T07:12:36Z",
        "updatedAt" : "2021-01-04T07:12:36Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "1d83a4d4-e0b2-4cfd-9e38-ed2de4331cf9",
        "parentId" : "519e4954-ef39-46e9-a395-04d65e4b8845",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "I'm not familiar with this part though, I saw the ORC code: \r\nhttps://github.com/apache/orc/blob/f6b6b2ea70f1b74e2ffd897e8985ffcd1c082582/java/core/src/java/org/apache/orc/CompressionKind.java#L25-L27",
        "createdAt" : "2021-01-04T07:15:47Z",
        "updatedAt" : "2021-01-04T07:15:47Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "35863d3d-aa5c-4d83-a563-a3c31eddb89e",
        "parentId" : "519e4954-ef39-46e9-a395-04d65e4b8845",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "ORC does, but what I mean is that in Apache Spark.",
        "createdAt" : "2021-01-04T07:17:26Z",
        "updatedAt" : "2021-01-04T07:17:26Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "c7417f7b-ed51-4094-836b-2cd3775ffa6a",
        "parentId" : "519e4954-ef39-46e9-a395-04d65e4b8845",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Ah, ok. nvm. Just a question.",
        "createdAt" : "2021-01-04T07:20:00Z",
        "updatedAt" : "2021-01-04T07:20:01Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "ef744109-effb-45cd-bb27-2dd6df41415b",
        "parentId" : "519e4954-ef39-46e9-a395-04d65e4b8845",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "I just didn't use it before. Feel free to make a PR if you want to add it, @maropu .",
        "createdAt" : "2021-01-04T07:20:11Z",
        "updatedAt" : "2021-01-04T07:20:11Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "ca1aa2c80e38147f87e339463258654c81827bdd",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +801,805 @@    .stringConf\n    .transform(_.toLowerCase(Locale.ROOT))\n    .checkValues(Set(\"none\", \"uncompressed\", \"snappy\", \"zlib\", \"lzo\", \"zstd\"))\n    .createWithDefault(\"snappy\")\n"
  },
  {
    "id" : "b9eb8a75-54c3-408a-82b5-5a73dded4548",
    "prId" : 30897,
    "prUrl" : "https://github.com/apache/spark/pull/30897#pullrequestreview-557467159",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cfca4882-0aea-413d-95de-1a2cc1ccfc47",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Please mention this on the PR title, @ulysses-you . ",
        "createdAt" : "2020-12-23T02:02:30Z",
        "updatedAt" : "2020-12-23T03:25:56Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "a7a66a75-73fe-46a4-8854-549dcc94c47e",
        "parentId" : "cfca4882-0aea-413d-95de-1a2cc1ccfc47",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "yes, update it.",
        "createdAt" : "2020-12-23T02:03:42Z",
        "updatedAt" : "2020-12-23T03:25:56Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      }
    ],
    "commit" : "8701feb11173deb82c8bb8070e13652a76abe38e",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +972,976 @@        \"set for each statement via `java.sql.Statement.setQueryTimeout` and they are smaller \" +\n        \"than this configuration value, they take precedence. If you set this timeout and prefer \" +\n        \"to cancel the queries right away without waiting task to finish, consider enabling \" +\n        s\"${THRIFTSERVER_FORCE_CANCEL.key} together.\")\n      .version(\"3.1.0\")"
  },
  {
    "id" : "b7e43f00-3307-4b35-86ec-53a059b25556",
    "prId" : 30554,
    "prUrl" : "https://github.com/apache/spark/pull/30554#pullrequestreview-541181246",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c26c59ee-4eca-4641-8297-163414880747",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "This seems to be inconsistent because the other code expects `LEGACY_CREATE_HIVE_TABLE_BY_DEFAULT_ENABLED`.",
        "createdAt" : "2020-11-30T18:29:34Z",
        "updatedAt" : "2020-12-02T08:50:59Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "bff924de21b8af2da17f31591768ce02c784bd43",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +2923,2927 @@    .createWithDefault(\"\")\n\n  val LEGACY_CREATE_HIVE_TABLE_BY_DEFAULT =\n    buildConf(\"spark.sql.legacy.createHiveTableByDefault\")\n      .internal()"
  },
  {
    "id" : "ca7d75aa-52f3-4dff-a72c-44ed661710b2",
    "prId" : 30554,
    "prUrl" : "https://github.com/apache/spark/pull/30554#pullrequestreview-542416388",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ce57e948-4f3b-434e-bfa5-387fa1a74bcc",
        "parentId" : null,
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "If this is a \"legacy\" key mention how long folks can depend on it.",
        "createdAt" : "2020-12-02T00:25:31Z",
        "updatedAt" : "2020-12-02T08:50:59Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "47ae3ac5-5ddf-4a89-85dd-a81c2cdf88bc",
        "parentId" : "ce57e948-4f3b-434e-bfa5-387fa1a74bcc",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "There are already a lot of legacy configurations, and there's no plan for how long we'll keep it. That's what I know as far as I have followed in the community. It would be a separate issue to decide lifetime of legacy configurations but ideally the legacy configurations will be removed in the major release bumpup I guess. I remember I discussed this with Sean as well somewhere. cc @srowen FYI",
        "createdAt" : "2020-12-02T00:31:23Z",
        "updatedAt" : "2020-12-02T08:50:59Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "57433591-9d80-49ac-929c-74d9c208509a",
        "parentId" : "ce57e948-4f3b-434e-bfa5-387fa1a74bcc",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "If we have a general policy for legacy configs that means this remains the same until Spark 4 by default I guess there is no need to document it here (I don't remember that conversation but I was out for a few months last/this year).",
        "createdAt" : "2020-12-02T00:53:06Z",
        "updatedAt" : "2020-12-02T08:50:59Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "bff924de21b8af2da17f31591768ce02c784bd43",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +2927,2931 @@      .internal()\n      .doc(\"When set to true, CREATE TABLE syntax without USING or STORED AS will use Hive \" +\n        s\"instead of the value of ${DEFAULT_DATA_SOURCE_NAME.key} as the table provider.\")\n      .version(\"3.1.0\")\n      .booleanConf"
  },
  {
    "id" : "ba83befb-d0f9-4957-9adb-df914c60e4b5",
    "prId" : 30341,
    "prUrl" : "https://github.com/apache/spark/pull/30341#pullrequestreview-528781095",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "58a71ca8-e373-4d0a-aa99-c3ccc69469ba",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "`100` is enough?",
        "createdAt" : "2020-11-12T06:14:34Z",
        "updatedAt" : "2020-11-17T01:00:33Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "079b4d46-b8ed-47e8-aa40-ac4e2523ebc2",
        "parentId" : "58a71ca8-e373-4d0a-aa99-c3ccc69469ba",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "This default value is copied from code generator's cache setting. I guess this should be enough.",
        "createdAt" : "2020-11-12T06:28:42Z",
        "updatedAt" : "2020-11-17T01:00:33Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "db115d6d850942eca5dc6fac80896b1038561e51",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +547,551 @@      .intConf\n      .checkValue(_ >= 0, \"The maximum must not be negative\")\n      .createWithDefault(100)\n\n  val CASE_SENSITIVE = buildConf(\"spark.sql.caseSensitive\")"
  },
  {
    "id" : "b75dccb4-3846-4736-9a5c-2e0353c46271",
    "prId" : 30325,
    "prUrl" : "https://github.com/apache/spark/pull/30325#pullrequestreview-531259095",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d821e514-3b60-48af-981e-a4010a23cbf5",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "static conf should go to `StaticSQLConf`",
        "createdAt" : "2020-11-16T12:04:34Z",
        "updatedAt" : "2020-11-17T02:11:32Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "231cd6f15f3fa71d677aae7170e1e404b82cef2a",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +816,820 @@      .createWithDefault(true)\n\n  val HIVE_METASTORE_PARTITION_PRUNING_INSET_THRESHOLD =\n    buildConf(\"spark.sql.hive.metastorePartitionPruningInSetThreshold\")\n      .doc(\"The threshold of set size for InSet predicate when pruning partitions through Hive \" +"
  },
  {
    "id" : "a31fde97-2b50-4668-ac61-d457b19409e9",
    "prId" : 30225,
    "prUrl" : "https://github.com/apache/spark/pull/30225#pullrequestreview-527880909",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "121b6659-3eec-4cd1-85a3-8ff907f07c3b",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "`-1` to follow the Hive config?",
        "createdAt" : "2020-11-11T05:55:47Z",
        "updatedAt" : "2020-12-07T05:36:46Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "020d7dc8c2eb299540ead4be974ba45e90d81a9f",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +845,849 @@      .intConf\n      .checkValue(_ >= -1, \"The maximum must be a positive integer, -1 to follow the Hive config.\")\n      .createWithDefault(100000)\n\n  val HIVE_MANAGE_FILESOURCE_PARTITIONS ="
  },
  {
    "id" : "a76c5048-3376-4af1-8bad-34b5cfd73faf",
    "prId" : 30225,
    "prUrl" : "https://github.com/apache/spark/pull/30225#pullrequestreview-544187074",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5646c47d-f55c-4d0c-8356-fffffe587c10",
        "parentId" : null,
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "have you considered to keep the default behavior as it is but allow it to be configurable? changing it means now we'll need to make two HMS calls (one additional `getNumPartitionsByFilter`) which I'm not sure is desirable (have seen HMS perform very badly in production before).",
        "createdAt" : "2020-11-20T00:43:52Z",
        "updatedAt" : "2020-12-07T05:36:46Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "34294955-5d9c-4f54-a563-0a496cb101af",
        "parentId" : "5646c47d-f55c-4d0c-8356-fffffe587c10",
        "authorId" : "a74459ae-fcfb-427d-b810-b8c75c10717d",
        "body" : "@sunchao Thank you for your response. I think this is a reasonable maximum, and Presto also has a parameter to limit the number of partitions in HiveMetadata#getPartitionsAsList, default value is 100_000",
        "createdAt" : "2020-12-03T08:25:10Z",
        "updatedAt" : "2020-12-07T05:36:46Z",
        "lastEditedBy" : "a74459ae-fcfb-427d-b810-b8c75c10717d",
        "tags" : [
        ]
      },
      {
        "id" : "2727c972-b13a-411b-975a-9b31fe19127b",
        "parentId" : "5646c47d-f55c-4d0c-8356-fffffe587c10",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Yea the default value 100_000 looks fine to me. My main question is whether we need to make the default value to be that and double the HMS calls. Presto doesn't call `getNumPartitionsByFilter` it seems as it streams through a partition iterator and stops once the threshold is reached.",
        "createdAt" : "2020-12-03T17:09:08Z",
        "updatedAt" : "2020-12-07T05:36:46Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      }
    ],
    "commit" : "020d7dc8c2eb299540ead4be974ba45e90d81a9f",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +845,849 @@      .intConf\n      .checkValue(_ >= -1, \"The maximum must be a positive integer, -1 to follow the Hive config.\")\n      .createWithDefault(100000)\n\n  val HIVE_MANAGE_FILESOURCE_PARTITIONS ="
  },
  {
    "id" : "de81b2f9-ec22-4333-9a0f-d2b6bbd02243",
    "prId" : 30210,
    "prUrl" : "https://github.com/apache/spark/pull/30210#pullrequestreview-528798476",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b220168f-27c2-4b76-bd59-11f7fe961c80",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "For the naming, cc @cloud-fan .",
        "createdAt" : "2020-11-12T07:08:48Z",
        "updatedAt" : "2020-11-12T07:08:48Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "5dff48f5c3d60801e425951cbb0b30175e54f2be",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1384,1388 @@\n  val STATEFUL_OPERATOR_CHECK_CORRECTNESS_ENABLED =\n    buildConf(\"spark.sql.streaming.statefulOperator.checkCorrectness.enabled\")\n      .internal()\n      .doc(\"When true, the stateful operators for streaming query will be checked for possible \" +"
  },
  {
    "id" : "c084c094-bcf5-465a-aeec-cff8995f9279",
    "prId" : 30162,
    "prUrl" : "https://github.com/apache/spark/pull/30162#pullrequestreview-518310359",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7036595e-f355-4d01-bcf5-fd815a2f488d",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "This doesn't change the default value. I believe we can add the benchmark result as a follow-up, @HeartSaVioR .",
        "createdAt" : "2020-10-28T04:08:28Z",
        "updatedAt" : "2020-10-29T02:49:20Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "4dc153db5eeea904e0c2208726cbd41bd514b62f",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +1333,1337 @@      .version(\"3.1.0\")\n      .stringConf\n      .createWithDefault(\"lz4\")\n\n  val STREAMING_AGGREGATION_STATE_FORMAT_VERSION ="
  },
  {
    "id" : "059fee83-7d67-4785-b3c3-86d145f443a1",
    "prId" : 30162,
    "prUrl" : "https://github.com/apache/spark/pull/30162#pullrequestreview-519193915",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7da4e3f9-3d1e-4b7c-aa83-b7026e0658e0",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Shall we add a test case for this assertion, `fully qualified class names`? In the new test case, it seems that we always use `getShortName`.",
        "createdAt" : "2020-10-28T23:52:14Z",
        "updatedAt" : "2020-10-29T02:49:20Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "4b2e7533-15c9-4289-b396-d05d852d1cb9",
        "parentId" : "7da4e3f9-3d1e-4b7c-aa83-b7026e0658e0",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Also, I'm wondering what happens when the invalid class name. It would be great if we can add a negative test case.",
        "createdAt" : "2020-10-28T23:53:24Z",
        "updatedAt" : "2020-10-29T02:49:20Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "6f7891ee-66b6-44a2-897d-281902ee93d2",
        "parentId" : "7da4e3f9-3d1e-4b7c-aa83-b7026e0658e0",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Okay.",
        "createdAt" : "2020-10-29T00:00:41Z",
        "updatedAt" : "2020-10-29T02:49:20Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "38309fe4-c61e-47ff-86ba-972c57b1fc0b",
        "parentId" : "7da4e3f9-3d1e-4b7c-aa83-b7026e0658e0",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "For invalid codec name, `CompressionCodec.createCodec` will throw an `IllegalArgumentException`.",
        "createdAt" : "2020-10-29T00:13:40Z",
        "updatedAt" : "2020-10-29T02:49:20Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "4dc153db5eeea904e0c2208726cbd41bd514b62f",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +1330,1334 @@      .doc(\"The codec used to compress delta and snapshot files generated by StateStore. \" +\n        \"By default, Spark provides four codecs: lz4, lzf, snappy, and zstd. You can also \" +\n        \"use fully qualified class names to specify the codec. Default codec is lz4.\")\n      .version(\"3.1.0\")\n      .stringConf"
  },
  {
    "id" : "a9a6ef68-f6f2-4120-adf8-22bda7e71492",
    "prId" : 30146,
    "prUrl" : "https://github.com/apache/spark/pull/30146#pullrequestreview-518774942",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "77164161-c191-435d-8780-31d19d4be935",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "`DYNAMIC_FILTER_PRUNING_ENABLED` -> `DYNAMIC_COLUMN_PRUNING_ENABLED`?",
        "createdAt" : "2020-10-27T19:16:54Z",
        "updatedAt" : "2020-10-29T10:11:50Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "afdb7b72-b24f-4b7a-b340-c598e9713979",
        "parentId" : "77164161-c191-435d-8780-31d19d4be935",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Ur, BTW, is this used in this PR?",
        "createdAt" : "2020-10-27T19:18:19Z",
        "updatedAt" : "2020-10-29T10:11:50Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "cdfb653d-dbb1-4730-80a6-3e173a9fbd00",
        "parentId" : "77164161-c191-435d-8780-31d19d4be935",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "Yes. This PR introduces a new configuration: `spark.sql.optimizer.dynamicFilterPruning.enabled`, which is the master switch of Dynamic Filter Pruning, `spark.sql.optimizer.dynamicPartitionPruning.enabled` is the switch of the partition column, `spark.sql.optimizer.dynamicDataPruning.enabled` is the switch of the data column.",
        "createdAt" : "2020-10-28T15:13:37Z",
        "updatedAt" : "2020-10-29T10:11:50Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      }
    ],
    "commit" : "9bdb86005112443c48cb4965bc82406129e23f66",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +247,251 @@\n  val DYNAMIC_FILTER_PRUNING_ENABLED =\n    buildConf(\"spark.sql.optimizer.dynamicFilterPruning.enabled\")\n      .doc(\"When true, we will generate predicate when it's used as join key and has benefit.\")\n      .version(\"3.1.0\")"
  },
  {
    "id" : "81c5bded-f25f-493e-8be6-9cd34b797638",
    "prId" : 30146,
    "prUrl" : "https://github.com/apache/spark/pull/30146#pullrequestreview-518779727",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0e9e6e28-c2fb-43e3-bb50-7ace67f42a14",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "This looks like the real config in this PR.\r\n`DYNAMIC_DATA_PRUNING_ENABLED` -> `DYNAMIC_COLUMN_PRUNING_ENABLED`?",
        "createdAt" : "2020-10-27T19:19:11Z",
        "updatedAt" : "2020-10-29T10:11:50Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "6b624aba-7740-4992-a86f-1d8e4d50175c",
        "parentId" : "0e9e6e28-c2fb-43e3-bb50-7ace67f42a14",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "This config is the switch of is the switch of the data column. May be `DYNAMIC_DATA_COLUMN_PRUNING_ENABLED`?\r\nBut this is inconsistent with `DYNAMIC_PARTITION_PRUNING_ENABLED`.",
        "createdAt" : "2020-10-28T15:18:00Z",
        "updatedAt" : "2020-10-29T10:11:50Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      }
    ],
    "commit" : "9bdb86005112443c48cb4965bc82406129e23f66",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +290,294 @@      .createWithDefault(true)\n\n  val DYNAMIC_DATA_PRUNING_ENABLED =\n    buildConf(\"spark.sql.optimizer.dynamicDataPruning.enabled\")\n      .doc(\"When true, we will generate predicate for data column when it's used as join key \" +"
  },
  {
    "id" : "a53e8534-8193-421d-97e7-64e6007c895f",
    "prId" : 30093,
    "prUrl" : "https://github.com/apache/spark/pull/30093#pullrequestreview-516734999",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "21c10eec-829b-4fd0-b8fd-91ad2c5ca13e",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "We need this config? cc: @cloud-fan ",
        "createdAt" : "2020-10-26T11:28:33Z",
        "updatedAt" : "2020-10-27T17:40:08Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "4d633b8e-b53d-4128-8b35-021f468f94e3",
        "parentId" : "21c10eec-829b-4fd0-b8fd-91ad2c5ca13e",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "If we want to backport this, it's safer to have a config for the new rule, in case it has bugs.",
        "createdAt" : "2020-10-26T12:40:04Z",
        "updatedAt" : "2020-10-27T17:40:08Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "cdc7dbe1f69d2cec912c536d388eb68756107d61",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +1254,1258 @@    .createWithDefault(true)\n\n  val REMOVE_REDUNDANT_SORTS_ENABLED = buildConf(\"spark.sql.execution.removeRedundantSorts\")\n    .internal()\n    .doc(\"Whether to remove redundant physical sort node\")"
  },
  {
    "id" : "839c7dc5-1129-4203-8320-3430664c1b90",
    "prId" : 30093,
    "prUrl" : "https://github.com/apache/spark/pull/30093#pullrequestreview-517041492",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2ceb7a00-9a60-4e9f-9c5b-163717989cd5",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "If we backport this into branch-2.4, `3.1.0` -> `2.4.8`?",
        "createdAt" : "2020-10-26T13:29:03Z",
        "updatedAt" : "2020-10-27T17:40:08Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "2210c2c4-39c5-496f-9961-87d18b7fbc54",
        "parentId" : "2ceb7a00-9a60-4e9f-9c5b-163717989cd5",
        "authorId" : "b3ba992a-312c-46eb-b3c3-8d861d15ac40",
        "body" : "I am not exactly sure if it's better to change it in this PR or to change it when this PR is backported to 2.4.8 (in case the current change does not work in 2.4.8)",
        "createdAt" : "2020-10-26T17:59:50Z",
        "updatedAt" : "2020-10-27T17:40:08Z",
        "lastEditedBy" : "b3ba992a-312c-46eb-b3c3-8d861d15ac40",
        "tags" : [
        ]
      }
    ],
    "commit" : "cdc7dbe1f69d2cec912c536d388eb68756107d61",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +1257,1261 @@    .internal()\n    .doc(\"Whether to remove redundant physical sort node\")\n    .version(\"3.1.0\")\n    .booleanConf\n    .createWithDefault(true)"
  },
  {
    "id" : "58e81fdd-ee2d-42ae-9fb2-b2fa3dc64674",
    "prId" : 30056,
    "prUrl" : "https://github.com/apache/spark/pull/30056#pullrequestreview-513991740",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ec6cb872-875f-4093-927d-b8fff6da940d",
        "parentId" : null,
        "authorId" : "ff1f962a-1f58-4eea-92c9-0f188dc7dd78",
        "body" : "Could the default be made `LegacyBehaviorPolicy.EXCEPTION` instead? Could also do this in a follow-up PR if this is controversial.",
        "createdAt" : "2020-10-19T10:28:10Z",
        "updatedAt" : "2020-10-19T10:34:31Z",
        "lastEditedBy" : "ff1f962a-1f58-4eea-92c9-0f188dc7dd78",
        "tags" : [
        ]
      },
      {
        "id" : "3bd16862-1b17-4fda-b5cb-67ec8dce3584",
        "parentId" : "ec6cb872-875f-4093-927d-b8fff6da940d",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "I am not sure that we can do that in the minor release 3.1. This can break existing apps. @cloud-fan @HyukjinKwon WDYT?",
        "createdAt" : "2020-10-19T11:24:40Z",
        "updatedAt" : "2020-10-19T11:24:40Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "f3213e06-a354-43e0-a61a-789d34ee9189",
        "parentId" : "ec6cb872-875f-4093-927d-b8fff6da940d",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "It's a breaking change, but probably is acceptable as it doesn't lead to silent results changing. We need an item in the migration guide though.",
        "createdAt" : "2020-10-21T15:10:14Z",
        "updatedAt" : "2020-10-21T15:10:15Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "f0781521-3515-4aba-b08d-5548de56dc03",
        "parentId" : "ec6cb872-875f-4093-927d-b8fff6da940d",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Here is the PR https://github.com/apache/spark/pull/30121 . Will see how many tests this will affect.",
        "createdAt" : "2020-10-21T17:18:03Z",
        "updatedAt" : "2020-10-21T17:18:03Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "fab3a8690413f4bc13d2f61a28eb769faff76cef",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +2653,2657 @@      .transform(_.toUpperCase(Locale.ROOT))\n      .checkValues(LegacyBehaviorPolicy.values.map(_.toString))\n      .createWithDefault(LegacyBehaviorPolicy.LEGACY.toString)\n\n  val LEGACY_PARQUET_REBASE_MODE_IN_READ ="
  },
  {
    "id" : "b0d45344-ef37-4eb2-9cff-6a9fdb86a8bb",
    "prId" : 30047,
    "prUrl" : "https://github.com/apache/spark/pull/30047#pullrequestreview-508934669",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "850349d1-ea83-45b0-b561-59d4c8c93acf",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Thank you for adding this, @viirya .",
        "createdAt" : "2020-10-15T04:50:56Z",
        "updatedAt" : "2020-10-15T16:18:18Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "e88cc54354a4e0ed7ed7a651766cdeb732fc390f",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +1466,1470 @@      .version(\"3.1.0\")\n      .booleanConf\n      .createWithDefault(true)\n\n  val FILE_SINK_LOG_DELETION = buildConf(\"spark.sql.streaming.fileSink.log.deletion\")"
  },
  {
    "id" : "dd0aad68-a24e-407a-ad8b-841df6cb9ea3",
    "prId" : 30047,
    "prUrl" : "https://github.com/apache/spark/pull/30047#pullrequestreview-509063355",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "00c36e65-cdeb-4f07-8f01-0672bfa67741",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "hmm, I think users can already disable any optimizer rule via `spark.sql.optimizer.excludedRules`?",
        "createdAt" : "2020-10-15T06:33:50Z",
        "updatedAt" : "2020-10-15T16:18:19Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "2ffc72aa-6745-4f66-94e4-30f41ca33c08",
        "parentId" : "00c36e65-cdeb-4f07-8f01-0672bfa67741",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "`excludedRules` should work, but I'm thinking to add CSV expression optimization similar to JSON in same rule. If so, a separate config for JSON/CSV expression seems good. If we have two rules for JSON, CSV individually, then `excludedRules` might be enough.",
        "createdAt" : "2020-10-15T06:42:38Z",
        "updatedAt" : "2020-10-15T16:18:19Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "929bb87a-a8ef-400e-b97a-a7ea59b47343",
        "parentId" : "00c36e65-cdeb-4f07-8f01-0672bfa67741",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I see, makes sense.",
        "createdAt" : "2020-10-15T06:46:04Z",
        "updatedAt" : "2020-10-15T16:18:19Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "e88cc54354a4e0ed7ed7a651766cdeb732fc390f",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1460,1464 @@\n  val JSON_EXPRESSION_OPTIMIZATION =\n    buildConf(\"spark.sql.optimizer.enableJsonExpressionOptimization\")\n      .doc(\"Whether to optimize JSON expressions in SQL optimizer. It includes pruning \" +\n        \"unnecessary columns from from_json, simplifing from_json + to_json, to_json + \" +"
  },
  {
    "id" : "e7f9d2e8-a5a4-487c-bd5b-bb4df2b41da5",
    "prId" : 29999,
    "prUrl" : "https://github.com/apache/spark/pull/29999#pullrequestreview-554807435",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "057d0f07-6922-4552-b337-6d33533131a9",
        "parentId" : null,
        "authorId" : "e5beb795-ac0e-4bd6-ad66-708215b8ae58",
        "body" : "A tree of 200 And-reduced expressions is already a huge expr tree.\r\nI think this could be useful and helpful with a default threshold of 5 or so already.",
        "createdAt" : "2020-12-17T15:41:52Z",
        "updatedAt" : "2020-12-17T15:41:52Z",
        "lastEditedBy" : "e5beb795-ac0e-4bd6-ad66-708215b8ae58",
        "tags" : [
        ]
      },
      {
        "id" : "7ccb5c7f-33d4-4b7a-9547-7007e6f0186a",
        "parentId" : "057d0f07-6922-4552-b337-6d33533131a9",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "We have removed this config: https://github.com/beliefer/spark/commit/9273d4250ddd5e011487a5a942c1b4d0f0412f78#diff-13c5b65678b327277c68d17910ae93629801af00117a0e3da007afd95b6c6764L219\r\n\r\nWe will always use the new expression for LIKE ALL if values are all literal.",
        "createdAt" : "2020-12-17T17:06:54Z",
        "updatedAt" : "2020-12-17T17:07:05Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "001eb38f603267c6a6f4e1c25430b8900644f5b7",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +227,231 @@      .checkValue(threshold => threshold >= 0, \"The maximum size of pattern sequence \" +\n        \"in like all must be non-negative\")\n      .createWithDefault(200)\n\n  val PLAN_CHANGE_LOG_LEVEL = buildConf(\"spark.sql.planChangeLog.level\")"
  },
  {
    "id" : "c3e3385b-908a-4ae1-bcf6-78679b8fc636",
    "prId" : 29983,
    "prUrl" : "https://github.com/apache/spark/pull/29983#pullrequestreview-506261555",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c7ea7d9b-65e3-44e9-8460-1b6992692942",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Shall we update migration guide (https://github.com/apache/spark/blob/master/docs/sql-migration-guide.md) too? Looks like it's more a behaviour change instead of a big fix.",
        "createdAt" : "2020-10-12T02:45:09Z",
        "updatedAt" : "2020-10-13T08:29:12Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "1fd3e1da-a69e-4bf0-b518-dd4440c20a07",
        "parentId" : "c7ea7d9b-65e3-44e9-8460-1b6992692942",
        "authorId" : "cce1c782-0596-44b6-8b99-6b77d2cca53c",
        "body" : "Ok, will update.",
        "createdAt" : "2020-10-12T02:48:22Z",
        "updatedAt" : "2020-10-13T08:29:12Z",
        "lastEditedBy" : "cce1c782-0596-44b6-8b99-6b77d2cca53c",
        "tags" : [
        ]
      }
    ],
    "commit" : "ddc522cd2a455a08e3151c363645cc3f001465d5",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +2348,2352 @@      .doc(\"When set to true, statistical aggregate function returns Double.NaN \" +\n        \"if divide by zero occurred during expression evaluation, otherwise, it returns null. \" +\n        \"Before version 3.1.0, it returns NaN in divideByZero case by default.\")\n      .version(\"3.1.0\")\n      .booleanConf"
  },
  {
    "id" : "c5055a95-0940-4c16-b7e4-fd7eb8e24425",
    "prId" : 29950,
    "prUrl" : "https://github.com/apache/spark/pull/29950#pullrequestreview-507068962",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ecfa7b45-911a-455d-8059-41309683efa1",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Could you add `checkValue`?",
        "createdAt" : "2020-10-12T02:45:03Z",
        "updatedAt" : "2020-11-13T00:06:09Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "c6ed6bbb-1faa-4e1a-ade9-4fa1db6d0e05",
        "parentId" : "ecfa7b45-911a-455d-8059-41309683efa1",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Added, thanks.",
        "createdAt" : "2020-10-13T04:57:35Z",
        "updatedAt" : "2020-11-13T00:06:09Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "bbaae3ed9b96ca517ec5435838ac37feb578c959",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +1981,1985 @@        \"the physical planning.\")\n      .version(\"3.1.0\")\n      .intConf\n      .checkValue(_ > 0, \"The value of maxCommonExprsInCollapseProject must be larger than zero.\")\n      .createWithDefault(Int.MaxValue)"
  },
  {
    "id" : "46414558-33f9-46f3-9927-e625696bec57",
    "prId" : 29950,
    "prUrl" : "https://github.com/apache/spark/pull/29950#pullrequestreview-513435611",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5d730403-4b3d-48d5-98c3-cbccf53fc9a3",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "(Just a comment) Even if we set `spark.sql.optimizer.excludedRules` to `CollapseProject`, it seems like Spark still respects this value in `ScanOperation`? That behaviour might be okay, but it looks a bit weird to me.",
        "createdAt" : "2020-10-21T01:18:50Z",
        "updatedAt" : "2020-11-13T00:06:09Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "911845da-cb27-4245-a3c7-a3205a8c1898",
        "parentId" : "5d730403-4b3d-48d5-98c3-cbccf53fc9a3",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Yeah, but currently if we exclude `CollapseProject`, `ScanOperation` will work and collapse projections. Maybe update this doc?",
        "createdAt" : "2020-10-21T07:04:28Z",
        "updatedAt" : "2020-11-13T00:06:09Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "6c12bba7-4fa4-4eaf-b15d-0c76c22ce570",
        "parentId" : "5d730403-4b3d-48d5-98c3-cbccf53fc9a3",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "hm I see. Yea, updating the doc sounds nice to me.",
        "createdAt" : "2020-10-21T08:35:56Z",
        "updatedAt" : "2020-11-13T00:06:09Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "bbaae3ed9b96ca517ec5435838ac37feb578c959",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +1968,1972 @@      .doc(\"An integer number indicates the maximum allowed number of common input expression \" +\n        \"from lower Project when being collapsed into upper Project by optimizer rule \" +\n        \"`CollapseProject`. Normally `CollapseProject` will collapse adjacent Project \" +\n        \"and merge expressions. But in some edge cases, expensive expressions might be \" +\n        \"duplicated many times in merged Project by this optimization. This config sets \" +"
  },
  {
    "id" : "0fa91c23-21cf-44d2-a1f0-fb7ab3e8ef58",
    "prId" : 29950,
    "prUrl" : "https://github.com/apache/spark/pull/29950#pullrequestreview-514980946",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a4e355ad-3afc-4ad3-8108-55bf13c8a03a",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "If we set this value to 1, all the existing tests can pass?",
        "createdAt" : "2020-10-21T01:20:08Z",
        "updatedAt" : "2020-11-13T00:06:09Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "a1f3e114-9c7c-4e42-8ace-24ba16a594ba",
        "parentId" : "a4e355ad-3afc-4ad3-8108-55bf13c8a03a",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "I guess not. We might have at lease few common expressions in collapsed projection. If set to 1, any duplicated expression is not allowed.",
        "createdAt" : "2020-10-22T17:53:55Z",
        "updatedAt" : "2020-11-13T00:06:09Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "bbaae3ed9b96ca517ec5435838ac37feb578c959",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1965,1969 @@\n  val MAX_COMMON_EXPRS_IN_COLLAPSE_PROJECT =\n    buildConf(\"spark.sql.optimizer.maxCommonExprsInCollapseProject\")\n      .doc(\"An integer number indicates the maximum allowed number of common input expression \" +\n        \"from lower Project when being collapsed into upper Project by optimizer rule \" +"
  },
  {
    "id" : "80bcb9c1-4ec5-44a0-9661-f065b2695acd",
    "prId" : 29950,
    "prUrl" : "https://github.com/apache/spark/pull/29950#pullrequestreview-529751693",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9138bb91-9fb1-4aa0-8c6a-df18bc08185b",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "`larger than zero` -> `positive`.",
        "createdAt" : "2020-11-13T05:47:38Z",
        "updatedAt" : "2020-11-13T05:47:38Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "bbaae3ed9b96ca517ec5435838ac37feb578c959",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +1982,1986 @@      .version(\"3.1.0\")\n      .intConf\n      .checkValue(_ > 0, \"The value of maxCommonExprsInCollapseProject must be larger than zero.\")\n      .createWithDefault(Int.MaxValue)\n"
  },
  {
    "id" : "ee5a6969-588e-42c4-8127-8968bb613ab2",
    "prId" : 29933,
    "prUrl" : "https://github.com/apache/spark/pull/29933#pullrequestreview-507360012",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "739440d4-071f-4990-926b-325183a1029e",
        "parentId" : null,
        "authorId" : "2821208d-43c5-4475-94ad-36cbf84c14d8",
        "body" : "Can you rewrite this as:\r\n```\r\nSet a query duration timeout in seconds in ThriftServer. If the timeout is set to a positive value, a running query will be cancelled automatically when the timeout is exceeded, otherwise the query continues to run till completion. Timeout values that are set for each statement via`java.sql.Statement.setQueryTimeout` take precedence.\r\n```",
        "createdAt" : "2020-10-13T11:34:18Z",
        "updatedAt" : "2020-10-14T23:22:41Z",
        "lastEditedBy" : "2821208d-43c5-4475-94ad-36cbf84c14d8",
        "tags" : [
        ]
      },
      {
        "id" : "a3527249-0417-4545-bb79-6a3bbce195cb",
        "parentId" : "739440d4-071f-4990-926b-325183a1029e",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Updated.",
        "createdAt" : "2020-10-13T12:04:58Z",
        "updatedAt" : "2020-10-14T23:22:41Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "5e3c44060567d2234dd79d222f77bc58983bb805",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +895,899 @@\n  val THRIFTSERVER_QUERY_TIMEOUT =\n    buildConf(\"spark.sql.thriftServer.queryTimeout\")\n      .doc(\"Set a query duration timeout in seconds in Thrift Server. If the timeout is set to \" +\n        \"a positive value, a running query will be cancelled automatically when the timeout is \" +"
  },
  {
    "id" : "09b216ae-b122-45a4-acc7-5deec0aeb3d7",
    "prId" : 29933,
    "prUrl" : "https://github.com/apache/spark/pull/29933#pullrequestreview-507360312",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7bd836f4-e249-40a3-9ab1-d3cfc8f77eb9",
        "parentId" : null,
        "authorId" : "2821208d-43c5-4475-94ad-36cbf84c14d8",
        "body" : "is this version correct?",
        "createdAt" : "2020-10-13T11:34:46Z",
        "updatedAt" : "2020-10-14T23:22:41Z",
        "lastEditedBy" : "2821208d-43c5-4475-94ad-36cbf84c14d8",
        "tags" : [
        ]
      },
      {
        "id" : "9ed7be71-7d34-4e05-b790-de00595e5ca0",
        "parentId" : "7bd836f4-e249-40a3-9ab1-d3cfc8f77eb9",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Yea, I think so.",
        "createdAt" : "2020-10-13T12:05:21Z",
        "updatedAt" : "2020-10-14T23:22:41Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "5e3c44060567d2234dd79d222f77bc58983bb805",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +901,905 @@        \"set for each statement via `java.sql.Statement.setQueryTimeout` and they are smaller \" +\n        \"than this configuration value, they take precedence.\")\n      .version(\"3.1.0\")\n      .timeConf(TimeUnit.SECONDS)\n      .createWithDefault(0L)"
  },
  {
    "id" : "b3bc3ce5-a1dd-47c1-9d19-efe158454575",
    "prId" : 29804,
    "prUrl" : "https://github.com/apache/spark/pull/29804#pullrequestreview-497272760",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "50809d9d-ffca-49bb-85c6-24f1ff2a8728",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Since our user documents are generated based on this statement, could you describe a bit more about how to decide whether to do bucketed scans or not?",
        "createdAt" : "2020-09-28T05:40:16Z",
        "updatedAt" : "2020-10-01T01:11:06Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "feb2b435-5e13-4770-8e3d-6958938cf66e",
        "parentId" : "50809d9d-ffca-49bb-85c6-24f1ff2a8728",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "@maropu - sure, wondering what do you think of below?\r\n\r\n```\r\nWhen true, decide whether to do bucketed scan on input tables\r\nbased on query plan automatically. Do not use bucketed scan if\r\n(1).query does not have operators to utilize bucketing (e.g. join, group-by, etc),\r\nor (2).there's an exchange operator between these operators and table scan.\r\n\r\n```",
        "createdAt" : "2020-09-28T05:51:07Z",
        "updatedAt" : "2020-10-01T01:11:06Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      },
      {
        "id" : "7becaafe-66e1-4a83-9079-fd7be3f45df8",
        "parentId" : "50809d9d-ffca-49bb-85c6-24f1ff2a8728",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "It looks okay, but I have the same suggestion with https://github.com/apache/spark/pull/29804#discussion_r494852129",
        "createdAt" : "2020-09-28T06:09:30Z",
        "updatedAt" : "2020-10-01T01:11:06Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "048ad1a5-5def-45ac-82a6-6ed03813f283",
        "parentId" : "50809d9d-ffca-49bb-85c6-24f1ff2a8728",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "@maropu - sure, updated.",
        "createdAt" : "2020-09-28T08:01:37Z",
        "updatedAt" : "2020-10-01T01:11:06Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      }
    ],
    "commit" : "b29f688fc4a06bc35effa6d24632d2a64501c9fd",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +954,958 @@  val AUTO_BUCKETED_SCAN_ENABLED =\n    buildConf(\"spark.sql.sources.bucketing.autoBucketedScan.enabled\")\n      .doc(\"When true, decide whether to do bucketed scan on input tables based on query plan \" +\n        \"automatically. Do not use bucketed scan if 1. query does not have operators to utilize \" +\n        \"bucketing (e.g. join, group-by, etc), or 2. there's an exchange operator between these \" +"
  },
  {
    "id" : "a52546db-04ba-4288-b9a2-d23310a22ed4",
    "prId" : 29804,
    "prUrl" : "https://github.com/apache/spark/pull/29804#pullrequestreview-497272760",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a90162d4-fd51-44b5-b13d-c3eb85a11e87",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "btw, we need to make this config external? If we just add this config for keeping the current behaviour, is it okay to add it as internal one?",
        "createdAt" : "2020-09-28T05:43:03Z",
        "updatedAt" : "2020-10-01T01:11:06Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "8edf562f-47f3-4ed2-88b9-48adc82384b6",
        "parentId" : "a90162d4-fd51-44b5-b13d-c3eb85a11e87",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "@maropu - sure, just for my own education, what does it indicate to make a config internal/external?",
        "createdAt" : "2020-09-28T05:52:09Z",
        "updatedAt" : "2020-10-01T01:11:06Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      },
      {
        "id" : "6fdf795f-8b91-4c92-a253-5758949058a8",
        "parentId" : "a90162d4-fd51-44b5-b13d-c3eb85a11e87",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "IIUC we don't have any strict rule for that. But, I think this new rule works well in most queries, so adding this as external looks less meaning because I think most users don't turn this feature off.",
        "createdAt" : "2020-09-28T06:07:50Z",
        "updatedAt" : "2020-10-01T01:11:06Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "32256f13-9b38-4ac9-8d3f-d9fe51a0cf7a",
        "parentId" : "a90162d4-fd51-44b5-b13d-c3eb85a11e87",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "@maropu - sure, updated.",
        "createdAt" : "2020-09-28T08:01:48Z",
        "updatedAt" : "2020-10-01T01:11:06Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      }
    ],
    "commit" : "b29f688fc4a06bc35effa6d24632d2a64501c9fd",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +959,963 @@        s\"operators and table scan. Note when '${BUCKETING_ENABLED.key}' is set to \" +\n        \"false, this configuration does not take any effect.\")\n      .version(\"3.1.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "126a829d-0736-468e-ad63-0964985d948d",
    "prId" : 29804,
    "prUrl" : "https://github.com/apache/spark/pull/29804#pullrequestreview-502706267",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6d6f1206-061c-4e95-8280-051e98fe6e85",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "we can follow AQE and only disable it for table cache. See https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/CacheManager.scala#L82",
        "createdAt" : "2020-10-06T06:29:34Z",
        "updatedAt" : "2020-10-06T06:29:35Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "841ec2da-9de7-4ffb-95f8-991510386fef",
        "parentId" : "6d6f1206-061c-4e95-8280-051e98fe6e85",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "@cloud-fan - thanks for pointing it out. Created https://issues.apache.org/jira/browse/SPARK-33075 for followup, cc @viirya in case there's any other regression for enabling auto bucketed scan, except cached query.",
        "createdAt" : "2020-10-06T08:35:14Z",
        "updatedAt" : "2020-10-06T08:35:14Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      }
    ],
    "commit" : "b29f688fc4a06bc35effa6d24632d2a64501c9fd",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +961,965 @@      .version(\"3.1.0\")\n      .booleanConf\n      .createWithDefault(false)\n\n  val CROSS_JOINS_ENABLED = buildConf(\"spark.sql.crossJoin.enabled\")"
  },
  {
    "id" : "fe046f68-60d0-46c4-9851-f1d8ff7ec1ee",
    "prId" : 29729,
    "prUrl" : "https://github.com/apache/spark/pull/29729#pullrequestreview-541567851",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e2626454-ebf3-4d87-bd4e-6b1a316bfcc5",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "As we only describe the details in migration doc which is not something end users always read as a guide doc or manual doc, we'd be better to write explanation either here or sql+kafka integration doc.\r\n(IMO it worths to add the comparison in the integration doc, and let here refer the doc.)",
        "createdAt" : "2020-11-30T21:11:18Z",
        "updatedAt" : "2020-12-01T07:55:04Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "158e5d3f-1036-4e89-8913-db6a07362781",
        "parentId" : "e2626454-ebf3-4d87-bd4e-6b1a316bfcc5",
        "authorId" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "body" : "Fixed. Updating the description because I've double checked the generated HTML files too.",
        "createdAt" : "2020-12-01T07:56:04Z",
        "updatedAt" : "2020-12-01T07:56:05Z",
        "lastEditedBy" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "tags" : [
        ]
      }
    ],
    "commit" : "203e60c9b7abeac0464ad3acc256121a5f900e40",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +1419,1423 @@    buildConf(\"spark.sql.streaming.kafka.useDeprecatedOffsetFetching\")\n      .internal()\n      .doc(\"When true, the deprecated Consumer based offset fetching used which could cause \" +\n        \"infinite wait in Spark queries. Such cases query restart is the only workaround. \" +\n        \"For further details please see Offset Fetching chapter of Structured Streaming Kafka \" +"
  },
  {
    "id" : "adc5fd8b-9f1f-403e-b997-f86d1646bcae",
    "prId" : 29726,
    "prUrl" : "https://github.com/apache/spark/pull/29726#pullrequestreview-621803131",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3ac10d00-9c81-4e8b-94b1-36984910ecb5",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "`PARTITON` -> `PARTITION`?",
        "createdAt" : "2021-03-26T05:53:22Z",
        "updatedAt" : "2021-03-26T05:53:22Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "bb579c655dfc1538bb682b6e03cecda948ca42e6",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +308,312 @@      .createWithDefault(true)\n\n  val DYNAMIC_PARTITON_PRUNING_PRUNING_SIDE_EXTRA_FILTER_RATIO =\n    buildConf(\"spark.sql.optimizer.dynamicPartitionPruning.pruningSideExtraFilterRatio\")\n    .internal()"
  },
  {
    "id" : "3eef7546-0640-4dd8-8f59-1ed69e967592",
    "prId" : 29726,
    "prUrl" : "https://github.com/apache/spark/pull/29726#pullrequestreview-621803568",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "804a5c9d-0702-42f8-bd33-020df1059cf3",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Indentation: two more spaces?",
        "createdAt" : "2021-03-26T05:54:32Z",
        "updatedAt" : "2021-03-26T05:54:32Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "bb579c655dfc1538bb682b6e03cecda948ca42e6",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +310,314 @@  val DYNAMIC_PARTITON_PRUNING_PRUNING_SIDE_EXTRA_FILTER_RATIO =\n    buildConf(\"spark.sql.optimizer.dynamicPartitionPruning.pruningSideExtraFilterRatio\")\n    .internal()\n    .doc(\"When filtering side doesn't support broadcast by join type, and doing DPP means \" +\n      \"running an extra query that may have significant overhead. This config will be used \" +"
  },
  {
    "id" : "5448804a-5475-422d-a203-32c007f41e6e",
    "prId" : 29726,
    "prUrl" : "https://github.com/apache/spark/pull/29726#pullrequestreview-621806551",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9b92c283-4cda-4643-9bfd-bbd44c0358b4",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Can we have `.checkValue` for the expected range, `0.0 <=` and `<= 1.0`? Or, are we allowing more possibility like `2.0`?",
        "createdAt" : "2021-03-26T06:03:08Z",
        "updatedAt" : "2021-03-26T06:03:08Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "bb579c655dfc1538bb682b6e03cecda948ca42e6",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +316,320 @@      \"in order to evaluate if it is worth adding an extra subquery as the pruning filter.\")\n    .version(\"3.2.0\")\n    .doubleConf\n    .createWithDefault(0.04)\n"
  },
  {
    "id" : "c15c23cb-4796-4ca4-8b50-15131e2219a6",
    "prId" : 29688,
    "prUrl" : "https://github.com/apache/spark/pull/29688#pullrequestreview-484747277",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "43b3380d-a68f-4f02-89df-95b2b453414b",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "plz add `checkValue`",
        "createdAt" : "2020-09-09T07:50:50Z",
        "updatedAt" : "2020-09-14T00:02:55Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "e83bf958828cd7b893c17e6962f2d63725632eec",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +2375,2379 @@      \"file location in `DataSourceScanExec`, every value will be abbreviated if exceed length.\")\n    .version(\"3.1.0\")\n    .intConf\n    .checkValue(_ > 3, \"This value must be bigger than 3.\")\n    .createWithDefault(100)"
  },
  {
    "id" : "74fca576-0795-49e3-8828-182adbddb7e9",
    "prId" : 29688,
    "prUrl" : "https://github.com/apache/spark/pull/29688#pullrequestreview-484804025",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "06211dc7-d9fd-44e2-9161-196e25763840",
        "parentId" : null,
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "This value from `org.apache.commons.lang3.StringUtils.abbreviate` whose marker is `...`.",
        "createdAt" : "2020-09-09T09:00:32Z",
        "updatedAt" : "2020-09-14T00:02:55Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      }
    ],
    "commit" : "e83bf958828cd7b893c17e6962f2d63725632eec",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +2376,2380 @@    .version(\"3.1.0\")\n    .intConf\n    .checkValue(_ > 3, \"This value must be bigger than 3.\")\n    .createWithDefault(100)\n"
  },
  {
    "id" : "4cb84bb4-eb63-4fe3-87dd-349eb7261941",
    "prId" : 29559,
    "prUrl" : "https://github.com/apache/spark/pull/29559#pullrequestreview-477234367",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "636ca112-f63f-4c75-a997-6edc10f0f3bd",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Since the PR description says nothing about adding this, I think its better to add it or separating this from this PR.  Also, could you add some tests for this?",
        "createdAt" : "2020-08-28T04:35:44Z",
        "updatedAt" : "2020-08-28T07:40:28Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "cfd348e643e61b51125ae1d375ee5830204c1acf",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +523,527 @@      .createWithDefault(0.2)\n\n  val ADAPTIVE_OPTIMIZER_EXCLUDED_RULES =\n    buildConf(\"spark.sql.adaptive.optimizer.excludedRules\")\n      .doc(\"Configures a list of rules to be disabled in the adaptive optimizer, in which the \" +"
  },
  {
    "id" : "9b989d87-9d07-4162-b3e7-165a07411551",
    "prId" : 29544,
    "prUrl" : "https://github.com/apache/spark/pull/29544#pullrequestreview-477233811",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e68d0b2d-3ebc-44f5-a14d-c5c8a6c5a2ee",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "can we put the old config name in `removedSQLConfigs`?",
        "createdAt" : "2020-08-28T04:33:40Z",
        "updatedAt" : "2020-08-28T06:21:08Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "40c99a21ae19f4c1e50029e4b10c768339336cc6",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +217,221 @@      .createWithDefault(400)\n\n  val PLAN_CHANGE_LOG_LEVEL = buildConf(\"spark.sql.planChangeLog.level\")\n    .internal()\n    .doc(\"Configures the log level for logging the change from the original plan to the new \" +"
  },
  {
    "id" : "a57b6cc5-5833-4d55-abf5-0346ff5c5081",
    "prId" : 29544,
    "prUrl" : "https://github.com/apache/spark/pull/29544#pullrequestreview-506307247",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "892e1dc3-fd97-441d-8267-0e29f345e275",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "The version of these three configurations needs to be updated to 3.1.0? ",
        "createdAt" : "2020-10-12T05:41:03Z",
        "updatedAt" : "2020-10-12T05:41:03Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "665ea48a-30a2-4fc3-8447-927ba6cd64e4",
        "parentId" : "892e1dc3-fd97-441d-8267-0e29f345e275",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Ah, I missed it. I'll update it in followup. Thanks!",
        "createdAt" : "2020-10-12T05:47:32Z",
        "updatedAt" : "2020-10-12T05:47:42Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "40c99a21ae19f4c1e50029e4b10c768339336cc6",
    "line" : 29,
    "diffHunk" : "@@ -1,1 +242,246 @@    .doc(\"Configures a list of batches for logging plan changes, in which the batches \" +\n      \"are specified by their batch names and separated by comma.\")\n    .version(\"3.0.0\")\n    .stringConf\n    .createOptional"
  },
  {
    "id" : "19109651-c7a1-457c-b828-133076f42ee8",
    "prId" : 29387,
    "prUrl" : "https://github.com/apache/spark/pull/29387#pullrequestreview-472940677",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "22d19104-aae7-4a2b-85e9-b08f05eaefff",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Just a question. Is the following still true?\r\n```\r\nNote that if fs.trash.interval is non-positive, this will be a no-op and log a warning message.\r\n```",
        "createdAt" : "2020-08-22T16:07:57Z",
        "updatedAt" : "2020-08-24T22:01:57Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "6d1cec03-79f5-4c06-b579-225c7a53a554",
        "parentId" : "22d19104-aae7-4a2b-85e9-b08f05eaefff",
        "authorId" : "a88f50e2-6440-4a6c-8883-3130888d05bc",
        "body" : "yes, this is still true.",
        "createdAt" : "2020-08-22T16:58:00Z",
        "updatedAt" : "2020-08-24T22:01:57Z",
        "lastEditedBy" : "a88f50e2-6440-4a6c-8883-3130888d05bc",
        "tags" : [
        ]
      },
      {
        "id" : "0fd4643d-e94b-4f1e-9d21-7f9188ca4186",
        "parentId" : "22d19104-aae7-4a2b-85e9-b08f05eaefff",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Yes. This is because when `fs.trash.interval` is not positive, Hadoop side will consider trash as disabled and will not delete the data. See [here](https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/TrashPolicyDefault.java#L125). Currently this just logs a warning but we could consider another flag to hard delete the data instead.",
        "createdAt" : "2020-08-22T17:38:42Z",
        "updatedAt" : "2020-08-24T22:01:57Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      }
    ],
    "commit" : "a2df53b48db372ed8f9a303cd8c0c499e33f3adf",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +2729,2733 @@        \"fs.trash.interval, and in default, the server side configuration value takes \" +\n        \"precedence over the client-side one. Note that if fs.trash.interval is non-positive, \" +\n        \"this will be a no-op and log a warning message.\")\n      .version(\"3.1.0\")\n      .booleanConf"
  },
  {
    "id" : "87a01f9e-3e9b-4613-85d5-c9c8d947c821",
    "prId" : 29387,
    "prUrl" : "https://github.com/apache/spark/pull/29387#pullrequestreview-472947585",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d91eff26-3a9f-4608-8852-21e8984fa24e",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "Does Spark SQL have the trash directory ?",
        "createdAt" : "2020-08-22T19:06:38Z",
        "updatedAt" : "2020-08-24T22:01:57Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "1a4f5ed4-7532-4608-bc40-582ce2757e8f",
        "parentId" : "d91eff26-3a9f-4608-8852-21e8984fa24e",
        "authorId" : "a88f50e2-6440-4a6c-8883-3130888d05bc",
        "body" : "Hi, It is hdfs trash directory",
        "createdAt" : "2020-08-22T19:41:04Z",
        "updatedAt" : "2020-08-24T22:01:57Z",
        "lastEditedBy" : "a88f50e2-6440-4a6c-8883-3130888d05bc",
        "tags" : [
        ]
      }
    ],
    "commit" : "a2df53b48db372ed8f9a303cd8c0c499e33f3adf",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +2726,2730 @@    buildConf(\"spark.sql.truncate.trash.enabled\")\n      .doc(\"This configuration decides when truncating table, whether data files will be moved \" +\n        \"to trash directory or deleted permanently. The trash retention time is controlled by \" +\n        \"fs.trash.interval, and in default, the server side configuration value takes \" +\n        \"precedence over the client-side one. Note that if fs.trash.interval is non-positive, \" +"
  },
  {
    "id" : "266b4e2a-6678-450e-90fc-0fc3ba1b2fa9",
    "prId" : 29387,
    "prUrl" : "https://github.com/apache/spark/pull/29387#pullrequestreview-473863014",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1e8b6942-cdbb-44a8-899d-5c28631c7495",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Please add version here.\r\n```scala\r\n.version(\"3.1.0\")\r\n```",
        "createdAt" : "2020-08-24T21:17:09Z",
        "updatedAt" : "2020-08-24T22:01:57Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "a2df53b48db372ed8f9a303cd8c0c499e33f3adf",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +2731,2735 @@        \"this will be a no-op and log a warning message.\")\n      .version(\"3.1.0\")\n      .booleanConf\n      .createWithDefault(false)\n"
  },
  {
    "id" : "15e15d0b-9374-4f4a-967b-0c5cdfd19c44",
    "prId" : 29387,
    "prUrl" : "https://github.com/apache/spark/pull/29387#pullrequestreview-475563285",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c6733787-9698-46c5-85af-04145d7c98b6",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "quick question, do we want to have each configuration for each operation? Looks like https://github.com/apache/spark/pull/29319 targets similar stuff. Maybe it'd make more sense to have a global configuration.",
        "createdAt" : "2020-08-26T06:44:31Z",
        "updatedAt" : "2020-08-26T06:44:31Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "0db65727-7e71-40b0-87ce-2a235aa24349",
        "parentId" : "c6733787-9698-46c5-85af-04145d7c98b6",
        "authorId" : "a88f50e2-6440-4a6c-8883-3130888d05bc",
        "body" : "I will rework on  #29319 and make it a global configuration.",
        "createdAt" : "2020-08-26T07:38:58Z",
        "updatedAt" : "2020-08-26T07:38:58Z",
        "lastEditedBy" : "a88f50e2-6440-4a6c-8883-3130888d05bc",
        "tags" : [
        ]
      },
      {
        "id" : "346d0915-1411-4847-a6d9-152144fbf67a",
        "parentId" : "c6733787-9698-46c5-85af-04145d7c98b6",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Yep. It's too early to make it a global configuration.",
        "createdAt" : "2020-08-26T14:44:41Z",
        "updatedAt" : "2020-08-26T14:44:41Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "a2df53b48db372ed8f9a303cd8c0c499e33f3adf",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +2724,2728 @@\n  val TRUNCATE_TRASH_ENABLED =\n    buildConf(\"spark.sql.truncate.trash.enabled\")\n      .doc(\"This configuration decides when truncating table, whether data files will be moved \" +\n        \"to trash directory or deleted permanently. The trash retention time is controlled by \" +"
  },
  {
    "id" : "927a67e8-a8ff-478a-a683-66239f09d84b",
    "prId" : 29304,
    "prUrl" : "https://github.com/apache/spark/pull/29304#pullrequestreview-459614009",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d0c90284-86fe-4cdc-9260-6431ca614f9c",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Plz add `checkValue`. I think only a positive value seems reasonable.",
        "createdAt" : "2020-08-01T14:02:26Z",
        "updatedAt" : "2020-08-04T22:55:52Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "511818af-232c-4005-b91f-e255fb5c48fb",
        "parentId" : "d0c90284-86fe-4cdc-9260-6431ca614f9c",
        "authorId" : "cce1c782-0596-44b6-8b99-6b77d2cca53c",
        "body" : "done",
        "createdAt" : "2020-08-02T01:48:23Z",
        "updatedAt" : "2020-08-04T22:55:52Z",
        "lastEditedBy" : "cce1c782-0596-44b6-8b99-6b77d2cca53c",
        "tags" : [
        ]
      }
    ],
    "commit" : "054581ae1d8f6148db7ec952ac23e5668ff9dc75",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +2686,2690 @@        \"it might cause Driver OOM if NAAJ numKeys increased, since it is exponential growth.\")\n      .version(\"3.1.0\")\n      .intConf\n      .checkValue(_ > 0, \"The value must be positive.\")\n      .createWithDefault(3)"
  },
  {
    "id" : "415c341a-c702-41e1-b0b3-0b9d9a77406f",
    "prId" : 29304,
    "prUrl" : "https://github.com/apache/spark/pull/29304#pullrequestreview-459952624",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0771148d-8e79-4c2b-9ff9-2ddd7039257b",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "add `.version(\"3.1.0\")`: https://github.com/apache/spark/pull/29335",
        "createdAt" : "2020-08-03T11:30:11Z",
        "updatedAt" : "2020-08-04T22:55:52Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "054581ae1d8f6148db7ec952ac23e5668ff9dc75",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +2686,2690 @@        \"it might cause Driver OOM if NAAJ numKeys increased, since it is exponential growth.\")\n      .version(\"3.1.0\")\n      .intConf\n      .checkValue(_ > 0, \"The value must be positive.\")\n      .createWithDefault(3)"
  },
  {
    "id" : "e627ea25-d512-460c-ae35-b7a22e4f7e04",
    "prId" : 29104,
    "prUrl" : "https://github.com/apache/spark/pull/29104#pullrequestreview-469897324",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b271ade3-88d5-4510-8dfa-66ae3e2afcb7",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "I think this is only possible when we can buildSide data is small. Can you add to this doc?",
        "createdAt" : "2020-07-21T23:06:06Z",
        "updatedAt" : "2020-07-27T22:06:38Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "4a251550-7d0b-4dc8-9ddb-31f040c984e2",
        "parentId" : "b271ade3-88d5-4510-8dfa-66ae3e2afcb7",
        "authorId" : "cce1c782-0596-44b6-8b99-6b77d2cca53c",
        "body" : "if it's single column NAAj, even if the buildSide is bigger than the autoBroadcaseThreshold, I think it's better to just using the optimize, because BroadcastNestedLoopJoin will be still worst than this optimization.",
        "createdAt" : "2020-07-22T01:54:01Z",
        "updatedAt" : "2020-07-27T22:06:38Z",
        "lastEditedBy" : "cce1c782-0596-44b6-8b99-6b77d2cca53c",
        "tags" : [
        ]
      },
      {
        "id" : "0554a91b-8ee7-4626-88f3-a861c96be7b1",
        "parentId" : "b271ade3-88d5-4510-8dfa-66ae3e2afcb7",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "I saw SPARK-32644, so it is not always able to use this optimization, isn't?",
        "createdAt" : "2020-08-18T23:30:19Z",
        "updatedAt" : "2020-08-18T23:30:19Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "233eff6549377d6c7c850fe8d1990fcd58fe0ea0",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +2676,2680 @@    buildConf(\"spark.sql.optimizeNullAwareAntiJoin\")\n      .internal()\n      .doc(\"When true, NULL-aware anti join execution will be planed into \" +\n        \"BroadcastHashJoinExec with flag isNullAwareAntiJoin enabled, \" +\n        \"optimized from O(M*N) calculation into O(M) calculation \" +"
  },
  {
    "id" : "8380d230-3dcf-41f5-a3d2-94053df2fa9a",
    "prId" : 29079,
    "prUrl" : "https://github.com/apache/spark/pull/29079#pullrequestreview-448788378",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "173f1ef6-300a-4347-b2b2-ff7fc36135e6",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Is it okay to share this parameter between sort-merge/hash joins? As @viirya suggested, we have some risk of OOM. So, I think we need a different threshold policy for the hash-join case.",
        "createdAt" : "2020-07-15T06:45:02Z",
        "updatedAt" : "2020-07-21T16:58:06Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "6bfeb979-d2a0-4544-b735-5eb90908bbe1",
        "parentId" : "173f1ef6-300a-4347-b2b2-ff7fc36135e6",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Also, I think we need to describe the risk clearly in `.doc`.",
        "createdAt" : "2020-07-15T06:45:34Z",
        "updatedAt" : "2020-07-21T16:58:06Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "1f8ed5a3-b3cc-4286-bfa6-812701eca717",
        "parentId" : "173f1ef6-300a-4347-b2b2-ff7fc36135e6",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "@maropu Make sense to me. Separated max bucket ratio configs for SHJ and SMJ for now, and added OOM related documentation to warn users.",
        "createdAt" : "2020-07-15T09:34:41Z",
        "updatedAt" : "2020-07-21T16:58:06Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      }
    ],
    "commit" : "b1a8a927f283b095716a638a26f9c5d5e9cc380c",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +2653,2657 @@\n  val COALESCE_BUCKETS_IN_JOIN_MAX_BUCKET_RATIO =\n    buildConf(\"spark.sql.bucketing.coalesceBucketsInJoin.maxBucketRatio\")\n      .doc(\"The ratio of the number of two buckets being coalesced should be less than or \" +\n        \"equal to this value for bucket coalescing to be applied. This configuration only \" +"
  },
  {
    "id" : "c8e32bff-b55b-4b67-a228-1700644b26ef",
    "prId" : 29079,
    "prUrl" : "https://github.com/apache/spark/pull/29079#pullrequestreview-451099862",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bd61630d-2bb1-4478-b01d-338851792ddc",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Can we add more doc like \"Coalescing bucketed table can avoid unnecessary shuffling during joining but it also reduces parallelism and could possibly cause OOM  for shuffled hash join\"?",
        "createdAt" : "2020-07-18T23:51:40Z",
        "updatedAt" : "2020-07-21T16:58:06Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "ef1e6ed1-c682-40b0-a3a3-71682391b9b9",
        "parentId" : "bd61630d-2bb1-4478-b01d-338851792ddc",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "@viirya - sure. added.",
        "createdAt" : "2020-07-19T05:23:54Z",
        "updatedAt" : "2020-07-21T16:58:06Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      }
    ],
    "commit" : "b1a8a927f283b095716a638a26f9c5d5e9cc380c",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +2647,2651 @@        \"shuffled hash join. Note: Coalescing bucketed table can avoid unnecessary shuffling \" +\n        \"in join, but it also reduces parallelism and could possibly cause OOM for \" +\n        \"shuffled hash join.\")\n      .version(\"3.1.0\")\n      .booleanConf"
  },
  {
    "id" : "e88722ff-ad0a-4fc2-8b9d-934f542fa177",
    "prId" : 29031,
    "prUrl" : "https://github.com/apache/spark/pull/29031#pullrequestreview-446172959",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e031a638-0786-4956-9af8-b246f259467d",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "nit: indentation is wrong",
        "createdAt" : "2020-07-10T07:15:13Z",
        "updatedAt" : "2020-08-10T18:02:22Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "394126da654c15c59a58bd906f0ca4564ce1c4e5",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1213,1217 @@\n  val REMOVE_REDUNDANT_PROJECTS_ENABLED = buildConf(\"spark.sql.execution.removeRedundantProjects\")\n    .internal()\n    .doc(\"Whether to remove redundant project exec node based on children's output and \" +\n      \"ordering requirement.\")"
  },
  {
    "id" : "4335edf2-ef53-407c-8c15-15c52aaab5c9",
    "prId" : 28853,
    "prUrl" : "https://github.com/apache/spark/pull/28853#pullrequestreview-433029054",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "aa5852cb-0465-4ec9-b2e3-35e164c3d218",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Could you add `checkValue`?",
        "createdAt" : "2020-06-18T07:55:13Z",
        "updatedAt" : "2020-06-20T03:56:22Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "1fb9dc651d5e1041fef8612bdbd3299dcea494a5",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +1182,1186 @@      \"effective only when using file-based sources such as Parquet, JSON and ORC.\")\n    .version(\"3.1.0\")\n    .intConf\n    .checkValue(v => v > 0, \"The min partition number must be a positive integer.\")\n    .createOptional"
  },
  {
    "id" : "e018f0f5-d775-4ffe-8e75-877e45d2aca5",
    "prId" : 28778,
    "prUrl" : "https://github.com/apache/spark/pull/28778#pullrequestreview-427925753",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "36b20081-9bae-4c38-800f-4f71d56b8186",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "`spark.sql.default.parallelism` -> `spark.sql.sessionLocalDefaultParallelism`?",
        "createdAt" : "2020-06-10T09:48:16Z",
        "updatedAt" : "2020-06-10T10:46:04Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "26382ce4-aac3-455c-b4c0-2aa3e59bd054",
        "parentId" : "36b20081-9bae-4c38-800f-4f71d56b8186",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "Em.. is it better to keep similar with `spark.default.parallelism`? so we can set this config easy. `sessionLocalDefaultParallelism ` seems complex.",
        "createdAt" : "2020-06-10T10:34:13Z",
        "updatedAt" : "2020-06-10T10:46:04Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      }
    ],
    "commit" : "55d367f6a7a277c83eff1ac83dd7708cfca608e9",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +372,376 @@      .createWithDefault(true)\n\n  val DEFAULT_PARALLELISM = buildConf(\"spark.sql.default.parallelism\")\n    .doc(\"The session-local default number of partitions and this value is widely used \" +\n      \"inside physical plans. If not set, the physical plans refer to \" +"
  },
  {
    "id" : "1fcbe04f-8935-41da-bb67-983363b8a881",
    "prId" : 28707,
    "prUrl" : "https://github.com/apache/spark/pull/28707#pullrequestreview-458941008",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ddf08b00-dc76-4a05-bb2f-68e88d94f9a7",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "Change `UnsafeRow` to `checkpoint` ? Most end users do not know what are `UnsafeRow`",
        "createdAt" : "2020-07-27T18:58:41Z",
        "updatedAt" : "2020-07-27T18:58:41Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "189a6265-0379-4fdf-b331-5e06c8712e7c",
        "parentId" : "ddf08b00-dc76-4a05-bb2f-68e88d94f9a7",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "Sure, will submit a follow-up PR today.",
        "createdAt" : "2020-07-31T06:21:18Z",
        "updatedAt" : "2020-07-31T06:21:18Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "557eb3099b3d0abe1fd2d7d91754fa747e05d200",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +1241,1245 @@    buildConf(\"spark.sql.streaming.stateStore.formatValidation.enabled\")\n      .internal()\n      .doc(\"When true, check if the UnsafeRow from the state store is valid or not when running \" +\n        \"streaming queries. This can happen if the state store format has been changed. Note, \" +\n        \"the feature is only effective in the build-in HDFS state store provider now.\")"
  },
  {
    "id" : "d360c2bc-fc1a-4f9b-ad5b-48622977c453",
    "prId" : 28676,
    "prUrl" : "https://github.com/apache/spark/pull/28676#pullrequestreview-450232532",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fa6f23aa-7920-4ebe-bdbc-2043c16f1f3e",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "can we make it an internal conf?",
        "createdAt" : "2020-07-17T15:09:43Z",
        "updatedAt" : "2020-07-17T20:59:39Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "ec3fa5d7-d222-4c47-aace-eec08dad89e7",
        "parentId" : "fa6f23aa-7920-4ebe-bdbc-2043c16f1f3e",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "Updated",
        "createdAt" : "2020-07-17T17:35:39Z",
        "updatedAt" : "2020-07-17T20:59:39Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "9caeecddaa07ef825b73835a3666502df468f881",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +2667,2671 @@\n  val BROADCAST_HASH_JOIN_OUTPUT_PARTITIONING_EXPAND_LIMIT =\n    buildConf(\"spark.sql.execution.broadcastHashJoin.outputPartitioningExpandLimit\")\n      .internal()\n      .doc(\"The maximum number of partitionings that a HashPartitioning can be expanded to. \" +"
  },
  {
    "id" : "4d453211-9543-4882-b3af-ec2b8aa90e93",
    "prId" : 28661,
    "prUrl" : "https://github.com/apache/spark/pull/28661#pullrequestreview-421486300",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "660ac2f4-2094-4439-99c5-d911bd224c00",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Is this targeting 3.0.0?",
        "createdAt" : "2020-05-28T06:17:37Z",
        "updatedAt" : "2020-05-29T07:58:03Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "1be7b9a8-99e7-44bb-a32f-c0bd993ed351",
        "parentId" : "660ac2f4-2094-4439-99c5-d911bd224c00",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Can be arguable .. but it virtually changes the exception message only at the core. I personally think it's okay/good to have it in 3.0. But I am okay to retarget if there's any concern about it.",
        "createdAt" : "2020-05-28T06:37:04Z",
        "updatedAt" : "2020-05-29T07:58:03Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "3ec44a10-da26-4f12-b213-3c5029cf751c",
        "parentId" : "660ac2f4-2094-4439-99c5-d911bd224c00",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "@gatorsmile . Are you okay with 3.0.0 targeting here?\r\nAlthough we are in RC stage, this PR looks worth for backporting. (Also the default is `false`.)\r\n\r\nOne question for @HyukjinKwon . Do we want this as a dynamic configuration instead of static configuration? I mean do we want to switch on/off during runtime?",
        "createdAt" : "2020-05-29T10:13:15Z",
        "updatedAt" : "2020-05-29T10:13:39Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "c1579f5c-a561-42bf-b951-da17e847bcf1",
        "parentId" : "660ac2f4-2094-4439-99c5-d911bd224c00",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "It doesn't strictly have to be in Spark 3.0. I just wanted to have some feedback quicker from users, and thought it's good to try this in Spark 3.0 as technically we just touch the error messages only here.\r\n\r\nI don't super strongly feel it has to land to Spark 3.0 - it's okay to retarget 3.1 if anyone feels strongly it has to be only in the master. Let me know :-)",
        "createdAt" : "2020-05-31T11:42:43Z",
        "updatedAt" : "2020-05-31T11:42:43Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "28c2a516eec5ffbd527b9e62869b279a162458e4",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +1790,1794 @@        \"together with Python stacktrace. By default, it is disabled and hides JVM stacktrace \" +\n        \"and shows a Python-friendly exception only.\")\n      .version(\"3.0.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "a0436c31-aef2-4d78-bae9-25486efaf560",
    "prId" : 28500,
    "prUrl" : "https://github.com/apache/spark/pull/28500#pullrequestreview-409236015",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "363bffe2-42c4-4d23-aa85-75d38ed899c3",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "Also check the migration guide. We need to remove one line/",
        "createdAt" : "2020-05-11T14:39:07Z",
        "updatedAt" : "2020-05-11T14:43:06Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      }
    ],
    "commit" : "0fc974fca87be4f7b6521de4e5de2eec596f4761",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +2236,2240 @@      .version(\"3.0.0\")\n      .booleanConf\n      .createWithDefault(true)\n\n  val LEGACY_BUCKETED_TABLE_SCAN_OUTPUT_ORDERING ="
  },
  {
    "id" : "1c5c1980-7090-4246-bd9b-3d10df22f0c4",
    "prId" : 28459,
    "prUrl" : "https://github.com/apache/spark/pull/28459#pullrequestreview-406291654",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "325bb975-bc37-4dce-a51f-0e5cbc27b658",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Technically we shouldn't necessarily go through deprecation to remove as it's an internal configuration but let me stay conservative here by deprecating first.",
        "createdAt" : "2020-05-06T04:04:17Z",
        "updatedAt" : "2020-05-06T05:25:18Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "34420fb12fcfd64e345c8487beef9c05d39b10bc",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +2609,2613 @@      DeprecatedConfig(SHUFFLE_TARGET_POSTSHUFFLE_INPUT_SIZE.key, \"3.0\",\n        s\"Use '${ADVISORY_PARTITION_SIZE_IN_BYTES.key}' instead of it.\"),\n      DeprecatedConfig(OPTIMIZER_METADATA_ONLY.key, \"3.0\",\n        \"Avoid to depend on this optimization to prevent a potential correctness issue. \" +\n          \"If you must use, use 'SparkSessionExtensions' instead to inject it as a custom rule.\")"
  },
  {
    "id" : "61d8dca6-24f9-46dc-8a29-df790666638e",
    "prId" : 28459,
    "prUrl" : "https://github.com/apache/spark/pull/28459#pullrequestreview-406303920",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a5dc7e90-27ce-45f1-9fe2-13f676530a21",
        "parentId" : null,
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "nit: could you add this note in the comment of the rule as well?",
        "createdAt" : "2020-05-06T04:57:24Z",
        "updatedAt" : "2020-05-06T05:25:18Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "34420fb12fcfd64e345c8487beef9c05d39b10bc",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +846,850 @@      \"to produce the partition columns instead of table scans. It applies when all the columns \" +\n      \"scanned are partition columns and the query has an aggregate operator that satisfies \" +\n      \"distinct semantics. By default the optimization is disabled, and deprecated as of Spark \" +\n      \"3.0 since it may return incorrect results when the files are empty, see also SPARK-26709.\" +\n      \"It will be removed in the future releases. If you must use, use 'SparkSessionExtensions' \" +"
  },
  {
    "id" : "b55ffd6f-fbe8-4511-a15a-1da22c4cec42",
    "prId" : 28424,
    "prUrl" : "https://github.com/apache/spark/pull/28424#pullrequestreview-404566788",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "389435c2-dd5f-4a2b-adda-c25c1f421f18",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "How about merging the two options above into one by using `-1` for deactivating the rule?",
        "createdAt" : "2020-05-01T06:49:53Z",
        "updatedAt" : "2020-05-02T06:42:37Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "54451e21-7bb8-4e23-b979-7861da0c38e4",
        "parentId" : "389435c2-dd5f-4a2b-adda-c25c1f421f18",
        "authorId" : "ad4cb313-47b2-4cfa-9c1f-0a3596794a67",
        "body" : "@maropu We also want to control whether pushdown should happen when right side is broadcastable.",
        "createdAt" : "2020-05-02T06:18:12Z",
        "updatedAt" : "2020-05-02T06:42:37Z",
        "lastEditedBy" : "ad4cb313-47b2-4cfa-9c1f-0a3596794a67",
        "tags" : [
        ]
      },
      {
        "id" : "d05a3b79-4224-4a97-b26d-39b7cb60259f",
        "parentId" : "389435c2-dd5f-4a2b-adda-c25c1f421f18",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Rather, we need an new option to turn on/off this rule? Any negative impact on performance?",
        "createdAt" : "2020-05-02T08:50:47Z",
        "updatedAt" : "2020-05-02T08:50:47Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "94935663-79f3-4b70-8afb-dc297b4ed568",
        "parentId" : "389435c2-dd5f-4a2b-adda-c25c1f421f18",
        "authorId" : "ad4cb313-47b2-4cfa-9c1f-0a3596794a67",
        "body" : "No I didn't find any performance regression with this stats based triggers. I kept everything behind config based on other configs used in similar features (Ex - costBasedjoinReorder).\r\n\r\nShould we remove the \"spark.sql.cbo.optimizeIntersect.enabled\" config here?",
        "createdAt" : "2020-05-02T10:09:20Z",
        "updatedAt" : "2020-05-02T10:09:29Z",
        "lastEditedBy" : "ad4cb313-47b2-4cfa-9c1f-0a3596794a67",
        "tags" : [
        ]
      },
      {
        "id" : "cc3c963c-2fce-461e-878b-648e4e684886",
        "parentId" : "389435c2-dd5f-4a2b-adda-c25c1f421f18",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Yea, could you remove it now? If we need to revert it back in the result of following reviews, I think that it should be added again at that time.\r\n\r\nbtw, I think you don't need to add new configs under the cbo namespace. We already have some optimizer rules depending on stats, e.g., `spark.sql.optimizer.dynamicPartitionPruning.useStats`. Could you refer the implementation?",
        "createdAt" : "2020-05-02T10:37:45Z",
        "updatedAt" : "2020-05-02T10:41:57Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "6ba042d3-5d51-44eb-9c2b-740c92780dc9",
        "parentId" : "389435c2-dd5f-4a2b-adda-c25c1f421f18",
        "authorId" : "ad4cb313-47b2-4cfa-9c1f-0a3596794a67",
        "body" : "@maropu I have removed cbo namespace from the config names.\r\n\r\nBut I see that all stats based optimization like costBasedJoinReordering, dynamicPartitionPruining are behind a configuration property. Shouldn't we follow same trend here?",
        "createdAt" : "2020-05-02T11:52:49Z",
        "updatedAt" : "2020-05-02T11:55:23Z",
        "lastEditedBy" : "ad4cb313-47b2-4cfa-9c1f-0a3596794a67",
        "tags" : [
        ]
      },
      {
        "id" : "894a0462-29cf-473b-9d84-63089922a40b",
        "parentId" : "389435c2-dd5f-4a2b-adda-c25c1f421f18",
        "authorId" : "a2fcc15e-a51a-42f0-87a6-137048a28e30",
        "body" : "@prakharjain09 Hello, if the stats are wrong or incomplete (like the cardinality changed drastically after that last time we performed the analyze on the table) , we could possibly slow down especially when we push the distinct to the right leg , no ? @maropu isn't it better to keep this under a flag ?",
        "createdAt" : "2020-05-02T22:29:08Z",
        "updatedAt" : "2020-05-02T22:29:09Z",
        "lastEditedBy" : "a2fcc15e-a51a-42f0-87a6-137048a28e30",
        "tags" : [
        ]
      },
      {
        "id" : "bfd16127-0730-4ccf-b2dc-02651af77924",
        "parentId" : "389435c2-dd5f-4a2b-adda-c25c1f421f18",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Yea, if we have the negative impact.",
        "createdAt" : "2020-05-02T22:59:14Z",
        "updatedAt" : "2020-05-02T22:59:15Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "29719f9de51e7026584b7882755d3cd7806ae8f5",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +1697,1701 @@        \" in Intersect optimization\")\n      .intConf\n      .createWithDefault(100)\n\n  private def isValidTimezone(zone: String): Boolean = {"
  },
  {
    "id" : "f59d6996-0c1e-45f0-88ac-5fc041fca6b5",
    "prId" : 28366,
    "prUrl" : "https://github.com/apache/spark/pull/28366#pullrequestreview-404433772",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dea91d51-b1c6-4a08-895f-0bbc209f34ed",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "We need `.transform(_.toUpperCase(Locale.ROOT))`? Also, could we validate input by `checkValues`? btw, is this feature expected to cover custom data sources except for the prebuilt ones (parquet, orc, ...)?",
        "createdAt" : "2020-04-29T23:52:19Z",
        "updatedAt" : "2020-05-05T06:13:39Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "72bb9c24-777b-4e23-b8c1-0410e49590ac",
        "parentId" : "dea91d51-b1c6-4a08-895f-0bbc209f34ed",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "We compare this list with `toLowerCase` when we need it. So seems to be fine to leave it here. Another similar example is `spark.sql.sources.useV1SourceList`. And as `useV1SourceList` too, seems `checkValues` is not needed.\r\n\r\nCurrently I think it is safer to assume custom data sources don't support this feature. I actually also think if custom data source wants to support it, it is better to adapt data source v2.\r\n\r\nWe don't have a common API for v1 data sources that tells if it supports nested predicate pushdown. If we really want to allow custom v1 data sources have that, we can consider adding one common v1 API for the purpose. But, again, seems to me that we will encourage adapting v2 instead adding new things to v1.",
        "createdAt" : "2020-05-01T06:03:51Z",
        "updatedAt" : "2020-05-05T06:13:39Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "34211d65-07f7-44bd-8388-22d88d263cce",
        "parentId" : "dea91d51-b1c6-4a08-895f-0bbc209f34ed",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "> Currently I think it is safer to assume custom data sources don't support this feature.\r\n\r\nYea, +1 on your thought.",
        "createdAt" : "2020-05-01T06:08:04Z",
        "updatedAt" : "2020-05-05T06:13:39Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "2bd4fe57-9821-4f25-a0b0-ecd5f1d307cf",
        "parentId" : "dea91d51-b1c6-4a08-895f-0bbc209f34ed",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "> Currently I think it is safer to assume custom data sources don't support this feature.\r\n\r\nLooks fine. @dbtsai are you good with it? Do you have use cases that need nested predicate pushdown for non-file-source?",
        "createdAt" : "2020-05-01T07:04:17Z",
        "updatedAt" : "2020-05-05T06:13:39Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "f19884c5-46a2-4bb0-812f-29e84820f4be",
        "parentId" : "dea91d51-b1c6-4a08-895f-0bbc209f34ed",
        "authorId" : "677aa336-324b-4b93-8300-21f8bd378f26",
        "body" : "In v1, we don't have any use-case for supporting it in custom data source. I'm good with it.",
        "createdAt" : "2020-05-01T22:37:36Z",
        "updatedAt" : "2020-05-05T06:13:39Z",
        "lastEditedBy" : "677aa336-324b-4b93-8300-21f8bd378f26",
        "tags" : [
        ]
      }
    ],
    "commit" : "00b9d47702ae76fca3c7246155175cb42f75136f",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +2073,2077 @@        \"other data sources don't support this feature yet. So the default value is 'parquet,orc'.\")\n      .version(\"3.0.0\")\n      .stringConf\n      .createWithDefault(\"parquet,orc\")\n"
  },
  {
    "id" : "e601c69a-ef57-419e-9f88-e275d8984329",
    "prId" : 28366,
    "prUrl" : "https://github.com/apache/spark/pull/28366#pullrequestreview-406392374",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9f5cee65-926b-4b8a-998c-849dfab43883",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Seems we decided to only make this configuration effective against DSv1, which seems okay because only DSv1 will have compatibility issues.\r\n\r\nBut shell we at least explicitly mention that this configuration is only effective with DSv1, (or make this configuration effective against DSv2)? Seems like it's going to be confusing to both end users or developers.\r\n\r\n",
        "createdAt" : "2020-05-06T08:04:49Z",
        "updatedAt" : "2020-05-06T08:05:03Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "0e3a1f64-133f-4921-979e-57dd324e5a69",
        "parentId" : "9f5cee65-926b-4b8a-998c-849dfab43883",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "I think DSv2 API supposes nested column capacity like pushdown and pruning, so we only need to deal with DSv1 compatibility issues here. Precisely, file source.\r\n\r\nI will create a simple followup to refine the doc of this configuration for this point. Thanks.\r\n\r\n",
        "createdAt" : "2020-05-06T08:12:51Z",
        "updatedAt" : "2020-05-06T08:12:52Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "a9ad2a16-50f2-40c3-b84b-d461f3585dbf",
        "parentId" : "9f5cee65-926b-4b8a-998c-849dfab43883",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Thanks!",
        "createdAt" : "2020-05-06T08:16:57Z",
        "updatedAt" : "2020-05-06T08:16:58Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "00b9d47702ae76fca3c7246155175cb42f75136f",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +2071,2075 @@        \"columns and/or names containing `dots` to data sources. Currently, Parquet implements \" +\n        \"both optimizations while ORC only supports predicates for names containing `dots`. The \" +\n        \"other data sources don't support this feature yet. So the default value is 'parquet,orc'.\")\n      .version(\"3.0.0\")\n      .stringConf"
  },
  {
    "id" : "8c852df8-3cad-4bb7-8648-90c979aefaec",
    "prId" : 28233,
    "prUrl" : "https://github.com/apache/spark/pull/28233#pullrequestreview-394648594",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5efc9c81-1a76-46e7-b04e-ddc311aaac83",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-30279, commit ID: 71c73d58f6e88d2558ed2e696897767d93bac60f#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-04-16T13:46:44Z",
        "updatedAt" : "2020-04-16T13:46:44Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "108396425097f420fb04615fe3068f03972f5062",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +2520,2524 @@      .internal()\n      .doc(\"When true, grouping_id() returns int values instead of long values.\")\n      .version(\"3.1.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "7a3f4723-27a7-4483-8f77-df8015e24bf4",
    "prId" : 28116,
    "prUrl" : "https://github.com/apache/spark/pull/28116#pullrequestreview-387697750",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "690264a3-41a7-4ca5-a761-091b4afcb3d7",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-27870, commit ID: 26998b8#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-04-04T13:03:59Z",
        "updatedAt" : "2020-04-04T13:03:59Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "dc0a44dbd0c01002358ff8f2d77036c21d01e459",
    "line" : 1428,
    "diffHunk" : "@@ -1,1 +1826,1830 @@        \"than 4 bytes. Lowering this value could make small Pandas UDF batch iterated and \" +\n        \"pipelined; however, it might degrade performance. See SPARK-27870.\")\n      .version(\"3.0.0\")\n      .fallbackConf(BUFFER_SIZE)\n"
  },
  {
    "id" : "d14b540d-c885-48ef-9f15-87c5f8078733",
    "prId" : 27936,
    "prUrl" : "https://github.com/apache/spark/pull/27936#pullrequestreview-376302288",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a508e180-9d03-47f9-a5f6-faa2eac230f7",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Thanks, `ansi` conf overrides `legacy` conf ? Do we have another `legacy` conf like this? Is it consistent with the other `legacy`? Could you update the migration guide and conf document to be consistent, too?\r\n\r\ncc @gatorsmile ",
        "createdAt" : "2020-03-17T16:06:31Z",
        "updatedAt" : "2020-03-17T16:54:43Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "f9451e3b-55e9-4b51-b32a-d16a95e133ba",
        "parentId" : "a508e180-9d03-47f9-a5f6-faa2eac230f7",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "This is the first one, and I don't think we need a migration guide as the behavior doesn't change by default.\r\n\r\nI'll update the config doc.",
        "createdAt" : "2020-03-17T16:38:45Z",
        "updatedAt" : "2020-03-17T16:54:43Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "e14def93-b050-45cb-a859-85706168daa9",
        "parentId" : "a508e180-9d03-47f9-a5f6-faa2eac230f7",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Thank you!",
        "createdAt" : "2020-03-17T18:32:42Z",
        "updatedAt" : "2020-03-17T18:32:42Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "8459d63ddc5f14ef7f91a5d9ebf07a2cdb2090ba",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +3017,3021 @@  def legacySizeOfNull: Boolean = {\n    // size(null) should return null under ansi mode.\n    getConf(SQLConf.LEGACY_SIZE_OF_NULL) && !getConf(ANSI_ENABLED)\n  }\n"
  },
  {
    "id" : "8da4bb91-6ffa-4599-b0b5-77fb1c94e314",
    "prId" : 27915,
    "prUrl" : "https://github.com/apache/spark/pull/27915#pullrequestreview-376297008",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1ffc0515-0c03-4b9a-90e1-45e5f0220979",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "It's not recommended to add a method to read a conf if it's only used once.",
        "createdAt" : "2020-03-16T16:44:10Z",
        "updatedAt" : "2020-03-18T14:27:24Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "f5989e80-4ba8-4d7f-b2ca-457307a09bff",
        "parentId" : "1ffc0515-0c03-4b9a-90e1-45e5f0220979",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "It has been already used in 9 places.",
        "createdAt" : "2020-03-16T18:50:31Z",
        "updatedAt" : "2020-03-18T14:27:24Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "addd7b2f-6a3d-44fd-8f2d-8afc12c909b4",
        "parentId" : "1ffc0515-0c03-4b9a-90e1-45e5f0220979",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Yea I know, but we should stop it. Let's follow https://github.com/apache/spark/commit/e736c62764137b2c3af90d2dc8a77e391891200a#diff-d07d3850a0ad8635a0c5562ed1e8bf26R184",
        "createdAt" : "2020-03-17T08:29:39Z",
        "updatedAt" : "2020-03-18T14:27:24Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "ec8cfffe-cbbc-4aac-9384-7830b650d899",
        "parentId" : "1ffc0515-0c03-4b9a-90e1-45e5f0220979",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Is it a correct link?",
        "createdAt" : "2020-03-17T09:46:54Z",
        "updatedAt" : "2020-03-18T14:27:24Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "b3247fa5-3bbc-43b8-a2a6-6e01c546b521",
        "parentId" : "1ffc0515-0c03-4b9a-90e1-45e5f0220979",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "I need to read the config in Java. The obvious replacement doesn't work:\r\n```java\r\nthis.rebaseDateTime = SQLConf.get().getConf(SQLConf.LEGACY_PARQUET_REBASE_DATETIME());\r\n```\r\n```\r\nError:(134, 48) java: incompatible types: inference variable T has incompatible bounds\r\n    equality constraints: java.lang.Object\r\n    upper bounds: java.lang.Boolean,java.lang.Object\r\n```\r\nIf you have strong opinion about removing the method `def parquetRebaseDateTimeEnabled`, could you show me how to read a SQL config in Java. I haven't found any example, unfortunately:\r\n```\r\n$ find . -name \"*.java\" -print0|xargs -0 grep 'SQLConf.get'\r\n./sql/core/src/test/java/test/org/apache/spark/sql/JavaBeanDeserializationSuite.java:      DateTimeUtils.getZoneId(SQLConf.get().sessionLocalTimeZone()));\r\n./sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader.java:    this.rebaseDateTime = SQLConf.get().parquetRebaseDateTimeEnabled();\r\n```",
        "createdAt" : "2020-03-17T18:25:41Z",
        "updatedAt" : "2020-03-18T14:27:24Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "184fcd8a9d6d177352aa67732f974e7ebf8eaf20",
    "line" : 25,
    "diffHunk" : "@@ -1,1 +3079,3083 @@  def integerGroupingIdEnabled: Boolean = getConf(SQLConf.LEGACY_INTEGER_GROUPING_ID)\n\n  def parquetRebaseDateTimeEnabled: Boolean = getConf(SQLConf.LEGACY_PARQUET_REBASE_DATETIME)\n\n  /** ********************** SQLConf functionality methods ************ */"
  },
  {
    "id" : "5e1f3c8d-d20b-4b10-95f1-2e1f7ef98d9e",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370142181",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "943529ba-67f6-4d00-8081-94a7f600de63",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-23850, commit ID: 6a55d8b03053e616dcacb79cd2c29a06d219dc32#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:09:51Z",
        "updatedAt" : "2020-03-06T08:09:51Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1877,1881 @@      \"in the explain output. This redaction is applied on top of the global redaction \" +\n      s\"configuration defined by ${SECRET_REDACTION_PATTERN.key}.\")\n    .version(\"2.2.2\")\n    .regexConf\n    .createWithDefault(\"(?i)url\".r)"
  },
  {
    "id" : "234cbcee-05f5-4382-b1c6-e04ee55ab654",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370142417",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6a05a705-9069-4350-9784-becb0ce61428",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-22791, commit ID: 28315714ddef3ddcc192375e98dd5207cf4ecc98#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:10:21Z",
        "updatedAt" : "2020-03-06T08:10:22Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +1887,1891 @@        \"dummy value. This is currently used to redact the output of SQL explain commands. \" +\n        \"When this conf is not set, the value from `spark.redaction.string.regex` is used.\")\n      .version(\"2.3.0\")\n      .fallbackConf(org.apache.spark.internal.config.STRING_REDACTION_PATTERN)\n"
  },
  {
    "id" : "acee5e2e-b87f-42bd-aa5d-a959c59ad14a",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370142568",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "08ff5909-cea7-464d-b88c-fc9139423297",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-22771, commit ID: f2b3525c17d660cf6f082bbafea8632615b4f58e#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:10:44Z",
        "updatedAt" : "2020-03-06T08:10:44Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +1893,1897 @@    .doc(\"When this option is set to false and all inputs are binary, `functions.concat` returns \" +\n      \"an output as binary. Otherwise, it returns as a string.\")\n    .version(\"2.3.0\")\n    .booleanConf\n    .createWithDefault(false)"
  },
  {
    "id" : "ae1e80fa-a24d-4e61-b606-d0d93087b1ff",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370142738",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "597c7a9c-e111-4301-96c5-97ba3168e6b8",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-22937, commit ID: bf853018cabcd3b3abf84bfe534d2981020b4a71#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:11:06Z",
        "updatedAt" : "2020-03-06T08:11:06Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 29,
    "diffHunk" : "@@ -1,1 +1900,1904 @@    .doc(\"When this option is set to false and all inputs are binary, `elt` returns \" +\n      \"an output as binary. Otherwise, it returns as a string.\")\n    .version(\"2.3.0\")\n    .booleanConf\n    .createWithDefault(false)"
  },
  {
    "id" : "f6b6f1c4-a721-499f-9714-a287a23042f5",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370142956",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c5dc6b47-302d-49be-a1ab-3c4f8edddc42",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-26263, commit ID: 5a140b7844936cf2b65f08853b8cfd8c499d4f13#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:11:32Z",
        "updatedAt" : "2020-03-06T08:11:32Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 38,
    "diffHunk" : "@@ -1,1 +1911,1915 @@        \"When this option is set to false, the partition column value will be converted to null \" +\n        \"if it can not be casted to corresponding user-specified schema.\")\n      .version(\"3.0.0\")\n      .booleanConf\n      .createWithDefault(true)"
  },
  {
    "id" : "f416436f-1540-4cbd-8f10-3e9296bc6640",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370143170",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f678c1e6-c904-4f73-8cd1-2bd2383a8fce",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-24063, commit ID: c4bbfd177b4e7cb46f47b39df9fd71d2d9a12c6d#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:12:00Z",
        "updatedAt" : "2020-03-06T08:12:00Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 47,
    "diffHunk" : "@@ -1,1 +1919,1923 @@      .doc(\"The max number of entries to be stored in queue to wait for late epochs. \" +\n        \"If this parameter is exceeded by the size of the queue, stream will stop with an error.\")\n      .version(\"3.0.0\")\n      .intConf\n      .createWithDefault(10000)"
  },
  {
    "id" : "b9459994-bc9e-4ec4-b409-65a1369b38fd",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370143369",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a33a65fd-20cf-45ea-9622-ca5d454fd446",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-22789, commit ID: 8941a4abcada873c26af924e129173dc33d66d71#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:12:27Z",
        "updatedAt" : "2020-03-06T08:12:27Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 56,
    "diffHunk" : "@@ -1,1 +1928,1932 @@      .doc(\"The size (measured in number of rows) of the queue used in continuous execution to\" +\n        \" buffer the results of a ContinuousDataReader.\")\n      .version(\"2.3.0\")\n      .intConf\n      .createWithDefault(1024)"
  },
  {
    "id" : "2ec404b1-6c2a-4e3d-b164-8f17ae62f81b",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370143547",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cfececfe-e00a-45f1-9bef-647ff97fdce4",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-22789, commit ID: 8941a4abcada873c26af924e129173dc33d66d71#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:12:48Z",
        "updatedAt" : "2020-03-06T08:12:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 65,
    "diffHunk" : "@@ -1,1 +1937,1941 @@      .doc(\"The interval at which continuous execution readers will poll to check whether\" +\n        \" the epoch has advanced on the driver.\")\n      .version(\"2.3.0\")\n      .timeConf(TimeUnit.MILLISECONDS)\n      .createWithDefault(100)"
  },
  {
    "id" : "a1ba5e28-b4e2-40e3-9b74-b21950c538da",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370143714",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2369d66a-c695-4164-af3a-26f46d1c635d",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-28747, commit ID: cb06209fc908bac6ce6a8f20653865489773cbc3#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:13:12Z",
        "updatedAt" : "2020-03-06T08:13:12Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 74,
    "diffHunk" : "@@ -1,1 +1946,1950 @@      \"implementation class names for which Data Source V2 code path is disabled. These data \" +\n      \"sources will fallback to Data Source V1 code path.\")\n    .version(\"3.0.0\")\n    .stringConf\n    .createWithDefault(\"avro,csv,json,kafka,orc,parquet,text\")"
  },
  {
    "id" : "7192eff4-e8b0-4cb9-8f5e-7479ac9fb3c6",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370143893",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cc47143a-7059-45f1-83a0-14418bbf9bb3",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-23196, commit ID: 588b9694c1967ff45774431441e84081ee6eb515#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:13:35Z",
        "updatedAt" : "2020-03-06T08:13:36Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 82,
    "diffHunk" : "@@ -1,1 +1953,1957 @@    .doc(\"A comma-separated list of fully qualified data source register class names for which\" +\n      \" StreamWriteSupport is disabled. Writes to these sources will fall back to the V1 Sinks.\")\n    .version(\"2.3.1\")\n    .stringConf\n    .createWithDefault(\"\")"
  },
  {
    "id" : "2c91ea76-d30a-454a-9eab-ba7545bfae59",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370144048",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0f0fa159-5ada-496a-94d7-323c810a1db0",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-23362, commit ID: 0a73aa31f41c83503d5d99eff3c9d7b406014ab3#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:13:57Z",
        "updatedAt" : "2020-03-06T08:13:57Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 91,
    "diffHunk" : "@@ -1,1 +1964,1968 @@          \"MicroBatchReadSupport is disabled. Reads from these sources will fall back to the \" +\n          \"V1 Sources.\")\n      .version(\"2.4.0\")\n      .stringConf\n      .createWithDefault(\"\")"
  },
  {
    "id" : "ae14eff6-b7c3-4728-9e62-80d6b1a3db8f",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370144201",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cb91eb7a-fc67-41bc-9ff3-2ae7b5d27bb3",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-20236, commit ID: b96248862589bae1ddcdb14ce4c802789a001306#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:14:20Z",
        "updatedAt" : "2020-03-06T08:14:20Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 100,
    "diffHunk" : "@@ -1,1 +1985,1989 @@        \"dataframe.write.option(\\\"partitionOverwriteMode\\\", \\\"dynamic\\\").save(path).\"\n      )\n      .version(\"2.3.0\")\n      .stringConf\n      .transform(_.toUpperCase(Locale.ROOT))"
  },
  {
    "id" : "e109d930-3d7c-4ae9-8a64-488e8f9d1937",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370144375",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3ab6bd91-4b0e-4f90-8476-26cc63653d71",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-28730, commit ID: 895c90b582cc2b2667241f66d5b733852aeef9eb#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:14:42Z",
        "updatedAt" : "2020-03-06T08:14:43Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 109,
    "diffHunk" : "@@ -1,1 +2010,2014 @@        \"not allowed.\"\n      )\n      .version(\"3.0.0\")\n      .stringConf\n      .transform(_.toUpperCase(Locale.ROOT))"
  },
  {
    "id" : "4af7069f-112c-4f8c-a752-49ecef1c0581",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370144568",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "52a276dd-d40f-4ada-8024-1ddac71310c0",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-30125, commit ID: d9b30694122f8716d3acb448638ef1e2b96ebc7a#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:15:06Z",
        "updatedAt" : "2020-03-06T08:15:06Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 118,
    "diffHunk" : "@@ -1,1 +2021,2025 @@      \"field. 2. Spark will forbid using the reserved keywords of ANSI SQL as identifiers in \" +\n      \"the SQL parser.\")\n    .version(\"3.0.0\")\n    .booleanConf\n    .createWithDefault(false)"
  },
  {
    "id" : "c9d5724a-569b-4301-a4f9-605972a2290e",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370144745",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ede90af9-ce1f-4270-8ae0-dc2d84efada1",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-23207 and SPARK-22905 and SPARK-24564 and SPARK-25114, commit ID: 4d2d3d47e00e78893b1ecd5a9a9070adc5243ac9#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:15:29Z",
        "updatedAt" : "2020-03-06T08:15:29Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 127,
    "diffHunk" : "@@ -1,1 +2034,2038 @@        \"to generate consistent repartition results. The performance of repartition() may go \" +\n        \"down since we insert extra local sort before it.\")\n      .version(\"2.1.4\")\n      .booleanConf\n      .createWithDefault(true)"
  },
  {
    "id" : "c547b17e-37ad-4cbd-876c-18a5fba7dae0",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370144927",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4bbb1294-0d2a-4096-b507-b701f3f3aa86",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-4502, commit ID: dfcff38394929970fee454c69864d0e10d59f8d4#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:15:52Z",
        "updatedAt" : "2020-03-06T08:15:52Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 136,
    "diffHunk" : "@@ -1,1 +2045,2049 @@        \"reading unnecessary nested column data. Currently Parquet and ORC are the \" +\n        \"data sources that implement this optimization.\")\n      .version(\"2.4.1\")\n      .booleanConf\n      .createWithDefault(true)"
  },
  {
    "id" : "ece6f02b-1ac3-4db1-abd7-5814cc55b40f",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370145089",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "35938f0f-908f-44c8-a070-bb8cf63b6f34",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-26837, commit ID: 0f2c0b53e8fb18c86c67b5dd679c006db93f94a5#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:16:15Z",
        "updatedAt" : "2020-03-06T08:16:15Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 145,
    "diffHunk" : "@@ -1,1 +2055,2059 @@        \"satisfying a query. This optimization allows object serializers to avoid \" +\n        \"executing unnecessary nested expressions.\")\n      .version(\"3.0.0\")\n      .booleanConf\n      .createWithDefault(true)"
  },
  {
    "id" : "a655f6d7-6fef-4fae-ba32-7fdf82d7df59",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370145309",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6adc0579-b12f-490a-afe3-97ab1d4a3edf",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-27707, commit ID: 127bc899ae78d73332a87f0972b5db3c9936c1f1#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:16:40Z",
        "updatedAt" : "2020-03-06T08:16:40Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 154,
    "diffHunk" : "@@ -1,1 +2066,2070 @@        \"physical data source scanning. For pruning nested fields from scanning, please use \" +\n        \"`spark.sql.optimizer.nestedSchemaPruning.enabled` config.\")\n      .version(\"3.0.0\")\n      .booleanConf\n      .createWithDefault(true)"
  },
  {
    "id" : "4e8c3200-63e2-42ee-90ad-43062ca8996d",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370145457",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "42d51712-1e7b-4385-8a0c-804c4076becb",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-24193, commit ID: 8a837bf4f3f2758f7825d2362cf9de209026651a#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:17:02Z",
        "updatedAt" : "2020-03-06T08:17:03Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 163,
    "diffHunk" : "@@ -1,1 +2076,2080 @@          \"'SELECT x FROM t ORDER BY y LIMIT m', if m is under this threshold, do a top-K sort\" +\n          \" in memory, otherwise do a global sort which spills to disk if necessary.\")\n      .version(\"2.4.0\")\n      .intConf\n      .createWithDefault(ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH)"
  },
  {
    "id" : "a0d92eeb-36a4-4422-8b8b-5e20d9899764",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370145649",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "49ad60b9-8402-4756-a572-bb40382d45e4",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-24244 and SPARK-24368, commit ID: 64fad0b519cf35b8c0a0dec18dd3df9488a5ed25#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:17:26Z",
        "updatedAt" : "2020-03-06T08:17:27Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 172,
    "diffHunk" : "@@ -1,1 +2092,2096 @@    .doc(\"If it is set to true, column names of the requested schema are passed to CSV parser. \" +\n      \"Other column values can be ignored during parsing even if they are malformed.\")\n    .version(\"2.4.0\")\n    .booleanConf\n    .createWithDefault(true)"
  },
  {
    "id" : "1c52f12c-2fa8-4595-a784-919cbfa79484",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370145894",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "810680e1-7fcd-45f8-a15e-63bccbec24ac",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-24215, commit ID: 6a0b77a55d53e74ac0a0892556c3a7a933474948#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:17:57Z",
        "updatedAt" : "2020-03-06T08:17:57Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 181,
    "diffHunk" : "@@ -1,1 +2103,2107 @@      \"REPL, the returned outputs are formatted like dataframe.show(). In SparkR, the returned \" +\n      \"outputs are showed similar to R data.frame would.\")\n    .version(\"2.4.0\")\n    .booleanConf\n    .createWithDefault(false)"
  },
  {
    "id" : "32588e3e-562b-4ff0-a9d3-67b2f4a7f8f2",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370146067",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cce1731c-0dbe-4663-8a15-79cc9cdbd0fe",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-24215, commit ID: 6a0b77a55d53e74ac0a0892556c3a7a933474948#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:18:20Z",
        "updatedAt" : "2020-03-06T08:18:21Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 190,
    "diffHunk" : "@@ -1,1 +2112,2116 @@      \"config is from 0 to (Int.MaxValue - 1), so the invalid config like negative and \" +\n      \"greater than (Int.MaxValue - 1) will be normalized to 0 and (Int.MaxValue - 1).\")\n    .version(\"2.4.0\")\n    .intConf\n    .createWithDefault(20)"
  },
  {
    "id" : "fb6d6f1e-4527-479c-a5db-ae6922419f91",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370146265",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fc47a8d7-58b2-415b-9ad9-69918295aafe",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-24215, commit ID: 6a0b77a55d53e74ac0a0892556c3a7a933474948#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:18:44Z",
        "updatedAt" : "2020-03-06T08:18:45Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 198,
    "diffHunk" : "@@ -1,1 +2119,2123 @@    .doc(\"The max number of characters for each cell that is returned by eager evaluation. \" +\n      s\"This only takes effect when ${REPL_EAGER_EVAL_ENABLED.key} is set to true.\")\n    .version(\"2.4.0\")\n    .intConf\n    .createWithDefault(20)"
  },
  {
    "id" : "121a3e5b-275a-4685-8aca-0f3e1779f1c6",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370146483",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c46ea0a6-c96b-491c-a398-10b7f0496717",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-24978, commit ID: 6193a202aab0271b4532ee4b740318290f2c44a1#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:19:15Z",
        "updatedAt" : "2020-03-06T08:19:15Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 207,
    "diffHunk" : "@@ -1,1 +2130,2134 @@        \"but the actual numBuckets is determined by loadFactor \" +\n        \"(e.g: default bit value 16 , the actual numBuckets is ((1 << 16) / 0.5).\")\n      .version(\"2.4.0\")\n      .intConf\n      .checkValue(bit => bit >= 10 && bit <= 30, \"The bit value must be in [10, 30].\")"
  },
  {
    "id" : "1a54cebd-2cb6-4c6b-a8ae-eba8102821d7",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370148725",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8db158ea-c7a0-4a8d-bd49-c04da4327eda",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-24881, commit ID: 0a0f68bae6c0a1bf30184b1e9ac6bf3805bd7511#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:23:56Z",
        "updatedAt" : "2020-03-06T08:23:57Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 216,
    "diffHunk" : "@@ -1,1 +2138,2142 @@    .doc(\"Compression codec used in writing of AVRO files. Supported codecs: \" +\n      \"uncompressed, deflate, snappy, bzip2 and xz. Default codec is snappy.\")\n    .version(\"2.4.0\")\n    .stringConf\n    .checkValues(Set(\"uncompressed\", \"deflate\", \"snappy\", \"bzip2\", \"xz\"))"
  },
  {
    "id" : "3d27cd61-5835-418e-beb5-32e70bc6452c",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370148946",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5267e9fb-07d9-4dfa-8610-9451764709f3",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-24881, commit ID: 0a0f68bae6c0a1bf30184b1e9ac6bf3805bd7511#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:24:28Z",
        "updatedAt" : "2020-03-06T08:24:28Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 225,
    "diffHunk" : "@@ -1,1 +2147,2151 @@      \"Valid value must be in the range of from 1 to 9 inclusive or -1. \" +\n      \"The default value is -1 which corresponds to 6 level in the current implementation.\")\n    .version(\"2.4.0\")\n    .intConf\n    .checkValues((1 to 9).toSet + Deflater.DEFAULT_COMPRESSION)"
  },
  {
    "id" : "439917c8-d4ff-44bd-b504-4bf60e3eacb5",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370149136",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9a42b9d7-1671-45b0-bf82-3399cff9084e",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-24605, commit ID: d08f53dc61f662f5291f71bcbe1a7b9f531a34d2#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:24:53Z",
        "updatedAt" : "2020-03-06T08:24:54Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 234,
    "diffHunk" : "@@ -1,1 +2156,2160 @@    .doc(\"If it is set to true, size of null returns -1. This behavior was inherited from Hive. \" +\n      \"The size function returns null for null input if the flag is disabled.\")\n    .version(\"2.4.0\")\n    .booleanConf\n    .createWithDefault(false)"
  },
  {
    "id" : "a97fc580-4bdf-4669-ae89-04abbb318ea1",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370149372",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "aba20790-0938-4130-9a74-c0000697cac7",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-25129, commit ID: ac0174e55af2e935d41545721e9f430c942b3a0c#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:25:23Z",
        "updatedAt" : "2020-03-06T08:25:23Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 243,
    "diffHunk" : "@@ -1,1 +2165,2169 @@      .doc(\"If it is set to true, the data source provider com.databricks.spark.avro is mapped \" +\n        \"to the built-in but external Avro data source module for backward compatibility.\")\n      .version(\"2.4.0\")\n      .booleanConf\n      .createWithDefault(true)"
  },
  {
    "id" : "f30baa6b-ad17-4eee-958e-eb2b7e418477",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370149575",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c990c425-b901-4202-8819-7ef6df5df6a0",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-24966, commit ID: 73dd6cf9b558f9d752e1f3c13584344257ad7863#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:25:49Z",
        "updatedAt" : "2020-03-06T08:25:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 252,
    "diffHunk" : "@@ -1,1 +2176,2180 @@        \"to false and order of evaluation is not specified by parentheses, INTERSECT operations \" +\n        \"are performed before any UNION, EXCEPT and MINUS operations.\")\n      .version(\"2.4.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "e3baa24b-f30c-47e5-9717-87f3dc2ee2ae",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370149790",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "134c4d6f-3b95-41dd-952e-414117cd5612",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-29956, commit ID: 87ebfaf003fcd05a7f6d23b3ecd4661409ce5f2f#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:26:13Z",
        "updatedAt" : "2020-03-06T08:26:14Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 261,
    "diffHunk" : "@@ -1,1 +2185,2189 @@      .doc(\"When set to true, a literal with an exponent (e.g. 1E-30) would be parsed \" +\n        \"as Decimal rather than Double.\")\n      .version(\"3.0.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "9f72646e-56cb-485c-83ae-2d98e743c5ee",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370150073",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8efe0486-d789-40a6-bfce-e8140ea92a16",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-30812, commit ID: b76bc0b1b8b2abd00a84f805af90ca4c5925faaa#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:26:48Z",
        "updatedAt" : "2020-03-06T08:26:48Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 270,
    "diffHunk" : "@@ -1,1 +2195,2199 @@        \"the type of number 1E10BD under legacy mode is DecimalType(2, -9), but is \" +\n        \"Decimal(11, 0) in non legacy mode.\")\n      .version(\"3.0.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "3fb28c43-de3c-4a09-b11d-d6ee1331a47a",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370150328",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e38e25bc-71bc-4743-9374-8fe3573ca9ba",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-30098, commit ID: 58be82ad4b98fc17e821e916e69e77a6aa36209d#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:27:14Z",
        "updatedAt" : "2020-03-06T08:27:15Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 279,
    "diffHunk" : "@@ -1,1 +2204,2208 @@      .doc(\"When set to true, CREATE TABLE syntax without a provider will use hive \" +\n        s\"instead of the value of ${DEFAULT_DATA_SOURCE_NAME.key}.\")\n      .version(\"3.0.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "520f2be7-1185-4480-b912-fbf4ad7af3e7",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370150589",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ece30089-2e1a-4eea-94e8-687cc5db01e7",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-25457, commit ID: 47d6e80a2e64823fabb596503fb6a6cc6f51f713#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:27:41Z",
        "updatedAt" : "2020-03-06T08:27:41Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 288,
    "diffHunk" : "@@ -1,1 +2212,2216 @@    .doc(\"If it is set to true, the div operator returns always a bigint. This behavior was \" +\n      \"inherited from Hive. Otherwise, the return type is the data type of the operands.\")\n    .version(\"3.0.0\")\n    .internal()\n    .booleanConf"
  },
  {
    "id" : "8fc4aa0e-96b6-47d9-a4f0-083ff70cd174",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370150858",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "195247dd-4803-4ff0-b545-72359ba614c2",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-28595, commit ID: 469423f33887a966aaa33eb75f5e7974a0a97beb#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:28:12Z",
        "updatedAt" : "2020-03-06T08:28:13Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 297,
    "diffHunk" : "@@ -1,1 +2222,2226 @@      .doc(\"When true, the bucketed table scan will list files during planning to figure out the \" +\n        \"output ordering, which is expensive and may make the planning quite slow.\")\n      .version(\"3.0.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "e5b0bc63-8b36-4ee5-b29e-2eb45b5545bc",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370151071",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0c3a3747-42d7-4d93-b214-4bb73b1e813b",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-25708, commit ID: 3dba5d41f1a66ae5eb08404d103284110c45a351#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:28:38Z",
        "updatedAt" : "2020-03-06T08:28:39Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 306,
    "diffHunk" : "@@ -1,1 +2231,2235 @@      .doc(\"If it is set to true, the parser will treat HAVING without GROUP BY as a normal \" +\n        \"WHERE, which does not follow SQL standard.\")\n      .version(\"2.4.1\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "fd914168-da6b-41d3-84b6-fd52227a7f74",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370151269",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b5fab42b-6bc5-4be8-bfe5-37cde8630ae6",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-25040, commit ID: d3de7568f32e298442f07b0a28b2c906de72c797#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:29:03Z",
        "updatedAt" : "2020-03-06T08:29:04Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 315,
    "diffHunk" : "@@ -1,1 +2240,2244 @@      .doc(\"When set to true, the parser of JSON data source treats empty strings as null for \" +\n        \"some data types such as `IntegerType`.\")\n      .version(\"3.0.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "a7709072-8f06-4564-a044-6dd56dcaf8c2",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370151468",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8cfc2ca6-06fb-4564-9364-626572fcd43f",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-30790, commit ID: 8ab6ae3ede96adb093347470a5cbbf17fe8c04e9#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:29:26Z",
        "updatedAt" : "2020-03-06T08:29:26Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 324,
    "diffHunk" : "@@ -1,1 +2250,2254 @@        \"type if the `array`/`map` function is called without any parameters. Otherwise, Spark \" +\n        \"returns an empty collection with `NullType` as element type.\")\n      .version(\"3.0.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "3f213c21-4605-4351-805f-f2f1bb415b46",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370151746",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6c2cc566-872b-4cd4-a08b-350a8c703b97",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-26580, commit ID: bc30a07ce262840c99a752db4fbd3a423f652017#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:29:54Z",
        "updatedAt" : "2020-03-06T08:29:54Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 333,
    "diffHunk" : "@@ -1,1 +2259,2263 @@      .doc(\"When set to true, user is allowed to use org.apache.spark.sql.functions.\" +\n        \"udf(f: AnyRef, dataType: DataType). Otherwise, exception will be throw.\")\n      .version(\"3.0.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "1c5b6383-dd77-438e-bfdb-508b078f7739",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370151993",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bcb67142-1654-47d1-976d-4fd88f1b9b4b",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-30312, commit ID: 830a4ec59b86253f18eb7dfd6ed0bbe0d7920e5b#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:30:22Z",
        "updatedAt" : "2020-03-06T08:30:23Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 342,
    "diffHunk" : "@@ -1,1 +2268,2272 @@      .doc(\"When set to true, TRUNCATE TABLE command will not try to set back original \" +\n        \"permission and ACLs when re-creating the table/partition paths.\")\n      .version(\"2.4.6\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "979e50ae-7dfc-4cfb-b3d6-66ecc82384fa",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370152209",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "29984611-1f42-476c-ad13-53155525ef4f",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-26085, commit ID: ab2eafb3cdc7631452650c6cac03a92629255347#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:30:48Z",
        "updatedAt" : "2020-03-06T08:30:48Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 351,
    "diffHunk" : "@@ -1,1 +2278,2282 @@        \"for non-struct key type, will be named as `value`, following the behavior of Spark \" +\n        \"version 2.4 and earlier.\")\n      .version(\"3.0.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "cd74bcf3-80e5-4a4f-8d83-eb18a2c0a3ac",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370152416",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9e475cae-60a8-4518-8b7b-3e02c70c9e01",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-26066, commit ID: 81550b38e43fb20f89f529d2127575c71a54a538#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:31:09Z",
        "updatedAt" : "2020-03-06T08:31:09Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 360,
    "diffHunk" : "@@ -1,1 +2286,2290 @@      \"in debug output. Any elements beyond the limit will be dropped and replaced by a\" +\n      \"\"\" \"... N more fields\" placeholder.\"\"\")\n    .version(\"3.0.0\")\n    .intConf\n    .createWithDefault(25)"
  },
  {
    "id" : "2e7027d9-fcd4-455d-bdea-525de8e7813b",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370152623",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "84d6010a-3eb7-468e-97a1-b00f0ba67023",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-26103, commit ID: 812ad5546148d2194ab0e4230ee85b8f6a5be2fb#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:31:34Z",
        "updatedAt" : "2020-03-06T08:31:34Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 369,
    "diffHunk" : "@@ -1,1 +2295,2299 @@      \"plan.  Set this to a lower value such as 8k if plan strings are taking up too much \" +\n      \"memory or are causing OutOfMemory errors in the driver or UI processes.\")\n    .version(\"3.0.0\")\n    .bytesConf(ByteUnit.BYTE)\n    .checkValue(i => i >= 0 && i <= ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH, \"Invalid \" +"
  },
  {
    "id" : "d2c0de91-e328-42c6-993e-ad5008e7d247",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370152969",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2bbecaf7-f141-4b95-b4c0-b7f31f757932",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-26060, commit ID: 1ab3d3e474ce2e36d58aea8ad09fb61f0c73e5c5#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:32:17Z",
        "updatedAt" : "2020-03-06T08:32:17Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 378,
    "diffHunk" : "@@ -1,1 +2307,2311 @@      .doc(\"If it is set to true, SET command will fail when the key is registered as \" +\n        \"a SparkConf entry.\")\n      .version(\"3.0.0\")\n      .booleanConf\n      .createWithDefault(true)"
  },
  {
    "id" : "bdac5c6e-3cdc-432f-820c-0fbedacef3d4",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370153237",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e88a6602-cf3e-4d9d-8c9c-514c0c34c288",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-27008, commit ID: 52671d631d2a64ed1cfa0c6e01168908faf92df8#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:32:46Z",
        "updatedAt" : "2020-03-06T08:32:46Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 387,
    "diffHunk" : "@@ -1,1 +2316,2320 @@      \"Catalyst's TimestampType and DateType. If it is set to false, java.sql.Timestamp \" +\n      \"and java.sql.Date are used for the same purpose.\")\n    .version(\"3.0.0\")\n    .booleanConf\n    .createWithDefault(false)"
  },
  {
    "id" : "a1c24c7c-5c90-45e2-b9bd-b7998c9f89b7",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370153438",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2d28952b-1ac2-497d-9643-369a1ba95c22",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-27588, commit ID: 618d6bff71073c8c93501ab7392c3cc579730f0b#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T08:33:10Z",
        "updatedAt" : "2020-03-06T08:33:10Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 396,
    "diffHunk" : "@@ -1,1 +2324,2328 @@      \"Spark will fail fast and not attempt to read the file if its length exceeds this value. \" +\n      \"The theoretical max is Int.MaxValue, though VMs might implement a smaller max.\")\n    .version(\"3.0.0\")\n    .internal()\n    .intConf"
  },
  {
    "id" : "2bf494d9-8119-4e76-9efb-f3f06a4660f5",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370171927",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bda7ce1d-6a53-4fb9-8984-b49a54014b3b",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-27638, commit ID: 83d289eef492de8c7f3e5145f9bd75431608b500#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T09:06:52Z",
        "updatedAt" : "2020-03-06T09:06:53Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 405,
    "diffHunk" : "@@ -1,1 +2334,2338 @@      .doc(\"If it is set to true, date/timestamp will cast to string in binary comparisons \" +\n        \"with String\")\n      .version(\"3.0.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "8679a37e-bd12-445f-b55a-d97a78c4a415",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370172150",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8dbfa246-d276-4461-b7b8-3830bb1476df",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-29753, commit ID: 942753a44beeae5f0142ceefa307e90cbc1234c5#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T09:07:16Z",
        "updatedAt" : "2020-03-06T09:07:17Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 413,
    "diffHunk" : "@@ -1,1 +2341,2345 @@    .doc(\"Name of the default catalog. This will be the current catalog if users have not \" +\n      \"explicitly set the current catalog yet.\")\n    .version(\"3.0.0\")\n    .stringConf\n    .createWithDefault(SESSION_CATALOG_NAME)"
  },
  {
    "id" : "2eadac5a-c508-4155-b20d-ed2c44198ff2",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370172406",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f5589e5d-796a-436a-b914-27388d45290a",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-29412, commit ID: 9407fba0375675d6ee6461253f3b8230e8d67509#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T09:07:41Z",
        "updatedAt" : "2020-03-06T09:07:41Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 422,
    "diffHunk" : "@@ -1,1 +2353,2357 @@        s\"metadata. To delegate operations to the $SESSION_CATALOG_NAME, implementations can \" +\n        \"extend 'CatalogExtension'.\")\n      .version(\"3.0.0\")\n      .stringConf\n      .createOptional"
  },
  {
    "id" : "c190316b-d5df-4fc2-bcd1-69116ed11cae",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370258749",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "72ed9b6e-1b70-45d7-b566-00e349c60494",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-30812, commit ID: b76bc0b1b8b2abd00a84f805af90ca4c5925faaa#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T11:33:04Z",
        "updatedAt" : "2020-03-06T11:33:05Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 431,
    "diffHunk" : "@@ -1,1 +2375,2379 @@    .internal()\n    .doc(\"When true, the upcast will be loose and allows string to atomic types.\")\n    .version(\"3.0.0\")\n    .booleanConf\n    .createWithDefault(false)"
  },
  {
    "id" : "162f840e-84c9-4b3b-adbd-8ce1cd78f5d1",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370258953",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "92b4d594-de46-483f-b474-5419c22e2871",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-30829, commit ID: 00943be81afbca6be13e1e72b24536cd98a788d6#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T11:33:29Z",
        "updatedAt" : "2020-03-06T11:33:30Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 440,
    "diffHunk" : "@@ -1,1 +2389,2393 @@      \"AnalysisException is thrown while name conflict is detected in nested CTE. This config \" +\n      \"will be removed in future versions and CORRECTED will be the only behavior.\")\n    .version(\"3.0.0\")\n    .stringConf\n    .transform(_.toUpperCase(Locale.ROOT))"
  },
  {
    "id" : "761dbe74-1d03-4275-b3d9-2ca350aea1d3",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370259222",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c4ae3778-7e6a-457e-8215-2104ccef4809",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-30668, commit ID: 7db0af578585ecaeee9fd23f8189292289b52a97#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T11:33:59Z",
        "updatedAt" : "2020-03-06T11:33:59Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 448,
    "diffHunk" : "@@ -1,1 +2402,2406 @@      \"The default value is EXCEPTION, RuntimeException is thrown when we will get different \" +\n      \"results.\")\n    .version(\"3.1.0\")\n    .stringConf\n    .transform(_.toUpperCase(Locale.ROOT))"
  },
  {
    "id" : "615ffd40-8237-4049-b7bf-91a96f590dc0",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370259440",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6e5e65e0-ba07-468c-866f-1d4288f98a0a",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-30812, commit ID: b76bc0b1b8b2abd00a84f805af90ca4c5925faaa#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T11:34:23Z",
        "updatedAt" : "2020-03-06T11:34:23Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 457,
    "diffHunk" : "@@ -1,1 +2412,2416 @@      .internal()\n      .doc(\"When true, the ArrayExists will follow the three-valued boolean logic.\")\n      .version(\"3.0.0\")\n      .booleanConf\n      .createWithDefault(true)"
  },
  {
    "id" : "9afb064f-4baa-493b-85f7-2beaee1f838d",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370259681",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "afbce64d-c277-479f-9cb7-da06bc3eef99",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-29175, commit ID: 3d7359ad4202067b26a199657b6a3e1f38be0e4d#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T11:34:50Z",
        "updatedAt" : "2020-03-06T11:34:51Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 466,
    "diffHunk" : "@@ -1,1 +2421,2425 @@        \"repositories. This is only used for downloading Hive jars in IsolatedClientLoader \" +\n        \"if the default Maven Central repo is unreachable.\")\n      .version(\"3.0.0\")\n      .stringConf\n      .createWithDefault("
  },
  {
    "id" : "a3078e1b-9278-42b5-9443-e932dd1efad8",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370259881",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e15003fa-5282-4d56-abb8-1a654e927d66",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-29864 and SPARK-29920, commit ID: e933539cdd557297daf97ff5e532a3f098896979#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T11:35:15Z",
        "updatedAt" : "2020-03-06T11:35:16Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 475,
    "diffHunk" : "@@ -1,1 +2434,2438 @@        \"`ParseException` is thrown if the input does not match to the pattern \" +\n        \"defined by `from` and `to`.\")\n      .version(\"3.0.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "11c5bf7d-6d20-4190-93ab-da9b47541730",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370260090",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "76dea0db-c173-490a-9180-be5fbc465062",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-30812, commit ID: b76bc0b1b8b2abd00a84f805af90ca4c5925faaa#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T11:35:39Z",
        "updatedAt" : "2020-03-06T11:35:40Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 484,
    "diffHunk" : "@@ -1,1 +2444,2448 @@        \"create/alter syntaxes. But please be aware that the reserved properties will be \" +\n        \"silently removed.\")\n      .version(\"3.0.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "e6f3454e-9f8b-401b-828f-3b6286cd58b9",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370260297",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f18f5304-be4a-499b-972c-3b3045f6aea1",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-30234, commit ID: 8a8d1fbb10af6da481f26831cd519ef46ccbce6c#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T11:36:03Z",
        "updatedAt" : "2020-03-06T11:36:04Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 493,
    "diffHunk" : "@@ -1,1 +2453,2457 @@      .doc(\"When true, only a single file can be added using ADD FILE. If false, then users \" +\n        \"can add directory by passing directory path to ADD FILE.\")\n      .version(\"3.0.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "1b34a23c-004e-4b4e-b161-d58939d4f7cf",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370260473",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "246fa3a2-b51c-4561-acf1-6557bea58f40",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-28152, commit ID: 69de7f31c37a7e0298e66cc814afc1b0aa948bbb#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T11:36:26Z",
        "updatedAt" : "2020-03-06T11:36:27Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 502,
    "diffHunk" : "@@ -1,1 +2461,2465 @@      .internal()\n      .doc(\"When true, use legacy MySqlServer SMALLINT and REAL type mapping.\")\n      .version(\"2.4.5\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "fa5e494d-43dd-4951-88ed-0fd77b0db660",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370260639",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "29a893b7-595e-404a-91a5-7c0c3022a998",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-30323, commit ID: 4e50f0291f032b4a5c0b46ed01fdef14e4cbb050#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T11:36:46Z",
        "updatedAt" : "2020-03-06T11:36:57Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 509,
    "diffHunk" : "@@ -1,1 +2467,2471 @@  val CSV_FILTER_PUSHDOWN_ENABLED = buildConf(\"spark.sql.csv.filterPushdown.enabled\")\n    .doc(\"When true, enable filter pushdown to CSV datasource.\")\n    .version(\"3.0.0\")\n    .booleanConf\n    .createWithDefault(true)"
  },
  {
    "id" : "37af3530-f909-4c14-b8ad-685d2fb5a244",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370261056",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a8928b7b-7015-46ac-b52c-da546a2f9bd9",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-29938, commit ID: 5ccbb38a71890b114c707279e7395d1f6284ebfd#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T11:37:35Z",
        "updatedAt" : "2020-03-06T11:37:35Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 518,
    "diffHunk" : "@@ -1,1 +2477,2481 @@        \"`AlterTableAddPartitionCommand` to add partitions into table. The smaller \" +\n        \"batch size is, the less memory is required for the real handler, e.g. Hive Metastore.\")\n      .version(\"3.0.0\")\n      .intConf\n      .checkValue(_ > 0, \"The value of spark.sql.addPartitionInBatch.size must be positive\")"
  },
  {
    "id" : "1e4f3a67-e5e8-40e9-a5c0-91858f95ab49",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370261196",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "aa1d6b66-4e9f-4560-a535-f9f581033b20",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-30668, commit ID: 92f57237871400ab9d499e1174af22a867c01988#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T11:37:55Z",
        "updatedAt" : "2020-03-06T11:37:55Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 527,
    "diffHunk" : "@@ -1,1 +2487,2491 @@      \"dates/timestamps in a locale-sensitive manner. When set to false, classes from \" +\n      \"java.time.* packages are used for the same purpose.\")\n    .version(\"3.0.0\")\n    .booleanConf\n    .createWithDefault(false)"
  },
  {
    "id" : "eb97805e-85db-44de-bb2c-7a985853c592",
    "prId" : 27829,
    "prUrl" : "https://github.com/apache/spark/pull/27829#pullrequestreview-370261367",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7d043379-3209-424e-90f6-45e66956f67c",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-25829, commit ID: 33329caa81827a245b84158b13234b88a4746e56#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-03-06T11:38:16Z",
        "updatedAt" : "2020-03-06T11:38:16Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8ea7e04435510242751b28765b50e1076108833",
    "line" : 535,
    "diffHunk" : "@@ -1,1 +2494,2498 @@    .doc(\"When set to true, hash expressions can be applied on elements of MapType. Otherwise, \" +\n      \"an analysis exception will be thrown.\")\n    .version(\"3.0.0\")\n    .booleanConf\n    .createWithDefault(false)"
  },
  {
    "id" : "281a06fe-25dc-4d57-890d-db725f56d533",
    "prId" : 27798,
    "prUrl" : "https://github.com/apache/spark/pull/27798#pullrequestreview-369250498",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d9987e45-e3c6-4a7a-8ffb-51c3e864abd8",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "It's still based on global log level?",
        "createdAt" : "2020-03-05T02:01:20Z",
        "updatedAt" : "2020-03-05T19:36:19Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "3151b019ff1025a41334eca78464d4baa5eb8c9c",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +406,410 @@    .createWithDefault(false)\n\n  val ADAPTIVE_EXECUTION_LOG_LEVEL = buildConf(\"spark.sql.adaptive.logLevel\")\n    .internal()\n    .doc(\"Configures the log level for adaptive execution logging of plan changes. The value \" +"
  },
  {
    "id" : "850368c0-bdd0-4e7c-840f-930b5822c1bd",
    "prId" : 27798,
    "prUrl" : "https://github.com/apache/spark/pull/27798#pullrequestreview-370082340",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d45f71ae-468b-44f0-a599-7f3d432213e5",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Hi, @cloud-fan . This seems to break `branch-3.0`.\r\n```\r\n[error] /home/jenkins/workspace/spark-branch-3.0-test-sbt-hadoop-2.7-hive-2.3/sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala:385: value version is not a member of org.apache.spark.internal.config.ConfigBuilder\r\n[error] possible cause: maybe a semicolon is missing before `value version'?\r\n[error]     .version(\"3.0.0\")\r\n[error]      ^\r\n[error] one error found\r\n```",
        "createdAt" : "2020-03-06T04:57:09Z",
        "updatedAt" : "2020-03-06T04:57:09Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "3151b019ff1025a41334eca78464d4baa5eb8c9c",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +410,414 @@    .doc(\"Configures the log level for adaptive execution logging of plan changes. The value \" +\n      \"can be 'trace', 'debug', 'info', 'warn', or 'error'. The default log level is 'debug'.\")\n    .version(\"3.0.0\")\n    .stringConf\n    .transform(_.toUpperCase(Locale.ROOT))"
  },
  {
    "id" : "cef6d78f-237c-4d39-93bb-3254aab57a16",
    "prId" : 27793,
    "prUrl" : "https://github.com/apache/spark/pull/27793#pullrequestreview-369398399",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "76acf138-e537-4297-9071-b5b3d39b7d61",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "Also tell users what is the new conf that replaces it? ",
        "createdAt" : "2020-03-05T08:14:54Z",
        "updatedAt" : "2020-03-05T10:45:48Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "c2ac48ba-c8a6-4d9c-9486-94faacfad66a",
        "parentId" : "76acf138-e537-4297-9071-b5b3d39b7d61",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "The doc here is not user-facing. End users will see a message to suggest the new config by https://github.com/apache/spark/pull/27793/files#diff-9a6b543db706f1a90f790783d6930a13R2494",
        "createdAt" : "2020-03-05T09:10:58Z",
        "updatedAt" : "2020-03-05T10:45:48Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "8d18db825d9fb8843c641f1c25d54bef962c7fa7",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +382,386 @@    buildConf(\"spark.sql.adaptive.shuffle.targetPostShuffleInputSize\")\n      .internal()\n      .doc(\"(Deprecated since Spark 3.0)\")\n      .version(\"1.6.0\")\n      .bytesConf(ByteUnit.BYTE)"
  }
]