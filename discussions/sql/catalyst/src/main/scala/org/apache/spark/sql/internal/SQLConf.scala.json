[
  {
    "id" : "1006393a-1dcd-46cb-9648-f462f3c15105",
    "prId" : 33655,
    "prUrl" : "https://github.com/apache/spark/pull/33655#pullrequestreview-724508457",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6f771c2c-d596-4ca9-a339-a39044e93897",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Could you add this check into `spark.sql.adaptive.advisoryPartitionSizeInBytes`, too?",
        "createdAt" : "2021-08-05T19:11:39Z",
        "updatedAt" : "2021-08-05T19:11:39Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "2f3635cd-b39d-4b85-bc81-36325b1b97f7",
        "parentId" : "6f771c2c-d596-4ca9-a339-a39044e93897",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "We can't add check in `ADVISORY_PARTITION_SIZE_IN_BYTES`, because it falls back to `SHUFFLE_TARGET_POSTSHUFFLE_INPUT_SIZE`, which is this conf.",
        "createdAt" : "2021-08-06T03:21:40Z",
        "updatedAt" : "2021-08-06T03:21:41Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "7d5b09ab-ea22-4004-b963-7a431f6b59bc",
        "parentId" : "6f771c2c-d596-4ca9-a339-a39044e93897",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "@cloud-fan . If we set negative values to `advisoryPartitionSizeInBytes` explicitly, it will not fallback to here. If we want to say, `advisoryPartitionSizeInBytes must be positive`, we should check both places, shouldn't we?",
        "createdAt" : "2021-08-06T16:06:34Z",
        "updatedAt" : "2021-08-06T16:06:34Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "eeeff3ae6bb62eedeb09011cc99514d240a8b7d0",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +481,485 @@      .version(\"1.6.0\")\n      .bytesConf(ByteUnit.BYTE)\n      .checkValue(_ > 0, \"advisoryPartitionSizeInBytes must be positive\")\n      .createWithDefaultString(\"64MB\")\n"
  },
  {
    "id" : "98d04720-367d-4e9e-8c61-371061b7bdc3",
    "prId" : 33444,
    "prUrl" : "https://github.com/apache/spark/pull/33444#pullrequestreview-711555608",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "23cb04d3-961d-4052-92fa-d13cc850bf24",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "indentation ?",
        "createdAt" : "2021-07-21T10:35:50Z",
        "updatedAt" : "2021-07-21T10:38:09Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      },
      {
        "id" : "ec7c2243-c1ee-4f0d-a674-6cd9958f62a0",
        "parentId" : "23cb04d3-961d-4052-92fa-d13cc850bf24",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "Oh, this is on purpose.",
        "createdAt" : "2021-07-21T11:40:24Z",
        "updatedAt" : "2021-07-21T11:40:25Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "221e62227f09d7e29fbab5c674af29b1f829b91e",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +3978,3982 @@  def timestampType: AtomicType = getConf(TIMESTAMP_TYPE) match {\n    // SPARK-36227: Remove TimestampNTZ type support in Spark 3.2 with minimal code changes.\n    //              The configuration `TIMESTAMP_TYPE` is only effective for testing in Spark 3.2.\n    case \"TIMESTAMP_NTZ\" if Utils.isTesting =>\n      TimestampNTZType"
  },
  {
    "id" : "28a8d916-3fd7-42f4-91a2-f8c55daf94f6",
    "prId" : 33348,
    "prUrl" : "https://github.com/apache/spark/pull/33348#pullrequestreview-706916663",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a27a7d03-c265-47e0-85f8-c44ef4ed3fb8",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Is it okay to reuse the hive config for the purpose? Sounds more like `spark.sql.metastorePartitionPruning` now?",
        "createdAt" : "2021-07-15T02:47:06Z",
        "updatedAt" : "2021-07-15T02:47:06Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "da1fbcbd-7551-4e1f-be7a-3b36855d6bd5",
        "parentId" : "a27a7d03-c265-47e0-85f8-c44ef4ed3fb8",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "yeah I agree, that's the exactly same point I mentioned in the JIRA ..",
        "createdAt" : "2021-07-15T02:52:06Z",
        "updatedAt" : "2021-07-15T02:52:07Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "98cca643-0333-477b-a4b6-e1c7fb947bb2",
        "parentId" : "a27a7d03-c265-47e0-85f8-c44ef4ed3fb8",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "The reason I reused the config is because it is not about Hive tables, but HMS-based catalog which is shared by both Hive tables and non-Hive tables in Spark. This is similar to `spark.sql.hive.manageFilesourcePartitions` which is used by data source tables even though it has `hive` in its name.\r\n\r\n",
        "createdAt" : "2021-07-15T04:38:10Z",
        "updatedAt" : "2021-07-15T04:38:10Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      }
    ],
    "commit" : "f686496a2a21b1075ff182dc936643d71e1e85e3",
    "line" : 1,
    "diffHunk" : "@@ -1,1 +977,981 @@    .createWithDefault(false)\n\n  val HIVE_METASTORE_PARTITION_PRUNING =\n    buildConf(\"spark.sql.hive.metastorePartitionPruning\")\n      .doc(\"When true, some predicates will be pushed down into the Hive metastore so that \" +"
  },
  {
    "id" : "7d2b99d9-a95b-4050-a90c-08628d93a8cb",
    "prId" : 33348,
    "prUrl" : "https://github.com/apache/spark/pull/33348#pullrequestreview-708603463",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "64a55233-e16f-4f4e-8d91-7013e680f0f9",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "`HIVE_MANAGE_FILESOURCE_PARTITIONS` mentions that when it is enabled, it uses the metastore to prune partitions during query planning. Maybe we should update the doc too.",
        "createdAt" : "2021-07-15T02:58:37Z",
        "updatedAt" : "2021-07-15T02:58:37Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "f69dd937-592d-4590-98f6-e49e3dc0e4a9",
        "parentId" : "64a55233-e16f-4f4e-8d91-7013e680f0f9",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Yea, we should mention that it only does so when `spark.sql.hive.metastorePartitionPruning` is turned on.",
        "createdAt" : "2021-07-15T04:39:19Z",
        "updatedAt" : "2021-07-15T04:39:19Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "790de5fb-4048-4338-87c5-c90c046b8e95",
        "parentId" : "64a55233-e16f-4f4e-8d91-7013e680f0f9",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Seems not update yet?",
        "createdAt" : "2021-07-16T08:11:40Z",
        "updatedAt" : "2021-07-16T08:11:41Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "e128d840-a738-4425-9328-e8d2df9fb4be",
        "parentId" : "64a55233-e16f-4f4e-8d91-7013e680f0f9",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Hmm I thought this needs to be updated if we don't have this PR, since Spark doesn't prune partitions for file data source tables on metastore side. But with this PR the description becomes correct?",
        "createdAt" : "2021-07-16T17:21:19Z",
        "updatedAt" : "2021-07-16T17:21:19Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "0ec79b21-30b3-4fb4-bc1a-6a217e38ff43",
        "parentId" : "64a55233-e16f-4f4e-8d91-7013e680f0f9",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Oh, I see. I should mention `spark.sql.hive.metastorePartitionPruning` there. Will update.",
        "createdAt" : "2021-07-16T17:22:15Z",
        "updatedAt" : "2021-07-16T17:22:15Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      }
    ],
    "commit" : "f686496a2a21b1075ff182dc936643d71e1e85e3",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +980,984 @@    buildConf(\"spark.sql.hive.metastorePartitionPruning\")\n      .doc(\"When true, some predicates will be pushed down into the Hive metastore so that \" +\n           \"unmatching partitions can be eliminated earlier.\")\n      .version(\"1.5.0\")\n      .booleanConf"
  },
  {
    "id" : "7238b7ae-65a9-47c1-a00d-f2a76aea8b8b",
    "prId" : 33176,
    "prUrl" : "https://github.com/apache/spark/pull/33176#pullrequestreview-697432760",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9478feea-b649-41dc-a071-782cb70ba33e",
        "parentId" : null,
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "The scope is bigger than this. It will affect timestamp literal, function `to_timestamp` and data source io, etc. We will get to that later.",
        "createdAt" : "2021-07-01T16:15:44Z",
        "updatedAt" : "2021-07-01T16:15:44Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "6c6bf3a04572a832ff9224c1a3cd61b81784c625",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +2828,2832 @@  val TIMESTAMP_TYPE =\n    buildConf(\"spark.sql.timestampType\")\n      .doc(\"Configures the default timestamp type of Spark SQL, including SQL DDL and Cast \" +\n        s\"clause. Setting the configuration as ${TimestampTypes.TIMESTAMP_NTZ.toString} will \" +\n        \"use TIMESTAMP WITHOUT TIME ZONE as the default type while putting it as \" +"
  },
  {
    "id" : "664cb507-9819-4442-8415-2a6f9270ddf1",
    "prId" : 33176,
    "prUrl" : "https://github.com/apache/spark/pull/33176#pullrequestreview-697488439",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "17cb3762-17e7-40ee-bcef-26125f26516e",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "I wonder about types literals, for instance `timestamp'2021-07-01 01:02:03'`. Should the config influence on it too?",
        "createdAt" : "2021-07-01T16:20:01Z",
        "updatedAt" : "2021-07-01T16:26:02Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "32a32ad8-5877-4026-b429-fbc3ae13a782",
        "parentId" : "17cb3762-17e7-40ee-bcef-26125f26516e",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "Yes, I mentioned in https://github.com/apache/spark/pull/33176#discussion_r662426271. The type literal logic with TIMESTAMP_NTZ:\r\n1. if there is no time zone part, return timestamp without time zone literal\r\n2. otherwise, return timestamp with local time zone. ",
        "createdAt" : "2021-07-01T17:21:11Z",
        "updatedAt" : "2021-07-01T17:21:11Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "6c6bf3a04572a832ff9224c1a3cd61b81784c625",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +2828,2832 @@  val TIMESTAMP_TYPE =\n    buildConf(\"spark.sql.timestampType\")\n      .doc(\"Configures the default timestamp type of Spark SQL, including SQL DDL and Cast \" +\n        s\"clause. Setting the configuration as ${TimestampTypes.TIMESTAMP_NTZ.toString} will \" +\n        \"use TIMESTAMP WITHOUT TIME ZONE as the default type while putting it as \" +"
  },
  {
    "id" : "5a76519c-a40b-470c-b393-53a2c2e24100",
    "prId" : 33176,
    "prUrl" : "https://github.com/apache/spark/pull/33176#pullrequestreview-697536637",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "734df2b1-6200-47bb-b792-9c6f74924bc9",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "I thought the compiler automatically calls the `toString` method in string interpolations, so, you can skip it. Or I am wrong?",
        "createdAt" : "2021-07-01T18:17:53Z",
        "updatedAt" : "2021-07-01T18:17:54Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "6c6bf3a04572a832ff9224c1a3cd61b81784c625",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +2829,2833 @@    buildConf(\"spark.sql.timestampType\")\n      .doc(\"Configures the default timestamp type of Spark SQL, including SQL DDL and Cast \" +\n        s\"clause. Setting the configuration as ${TimestampTypes.TIMESTAMP_NTZ.toString} will \" +\n        \"use TIMESTAMP WITHOUT TIME ZONE as the default type while putting it as \" +\n        s\"${TimestampTypes.TIMESTAMP_LTZ.toString} will use TIMESTAMP WITH LOCAL TIME ZONE. \" +"
  },
  {
    "id" : "67ddb6c1-31ff-442e-8fdf-ce60be0b2def",
    "prId" : 33172,
    "prUrl" : "https://github.com/apache/spark/pull/33172#pullrequestreview-697345900",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "163d76ce-a571-4e21-aa67-dc79b3707e30",
        "parentId" : null,
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "better to update the doc of\r\nADVISORY_PARTITION_SIZE_IN_BYTES",
        "createdAt" : "2021-07-01T14:21:12Z",
        "updatedAt" : "2021-07-01T14:21:50Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "edc1a263-a317-407b-9471-da04309929b1",
        "parentId" : "163d76ce-a571-4e21-aa67-dc79b3707e30",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "`ADVISORY_PARTITION_SIZE_IN_BYTES` doesn't mention `COALESCE_PARTITIONS_MIN_PARTITION_NUM` either. I think it's fine to only refer to \"big\" confs in the \"small\" confs, not from the other side.",
        "createdAt" : "2021-07-01T14:54:02Z",
        "updatedAt" : "2021-07-01T14:54:02Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "51546e87c33064c80f902d4eecff1f640222dfb2",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +529,533 @@\n  val COALESCE_PARTITIONS_PARALLELISM_FIRST =\n    buildConf(\"spark.sql.adaptive.coalescePartitions.parallelismFirst\")\n      .doc(\"When true, Spark ignores the target size specified by \" +\n        s\"'${ADVISORY_PARTITION_SIZE_IN_BYTES.key}' (default 64MB) when coalescing contiguous \" +"
  },
  {
    "id" : "0871aeb8-ff77-4d83-93d2-f707dd159d71",
    "prId" : 33172,
    "prUrl" : "https://github.com/apache/spark/pull/33172#pullrequestreview-697522081",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4382901c-1208-4067-8e57-761fa2ec15e2",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "@viirya is it clearer to you now?",
        "createdAt" : "2021-07-01T17:22:59Z",
        "updatedAt" : "2021-07-01T17:23:05Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "56dc5122-5762-4883-bd39-4554eb66ac2c",
        "parentId" : "4382901c-1208-4067-8e57-761fa2ec15e2",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "okay",
        "createdAt" : "2021-07-01T18:03:21Z",
        "updatedAt" : "2021-07-01T18:03:21Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "51546e87c33064c80f902d4eecff1f640222dfb2",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +536,540 @@        \"This is to avoid performance regression when enabling adaptive query execution. \" +\n        \"It's recommended to set this config to false and respect the target size specified by \" +\n        s\"'${ADVISORY_PARTITION_SIZE_IN_BYTES.key}'.\")\n      .version(\"3.2.0\")\n      .booleanConf"
  },
  {
    "id" : "9a073cec-0a03-4dab-a0cb-ce1e0fb379b7",
    "prId" : 33081,
    "prUrl" : "https://github.com/apache/spark/pull/33081#pullrequestreview-706975183",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a29f8726-aefb-4347-8e58-64c8514b0fcf",
        "parentId" : null,
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "How about `spark.sql.streaming.sessionWindow.localMerge.enabled` or `spark.sql.streaming.sessionWindow. mergeSessionsInLocalPartition.enabled`?",
        "createdAt" : "2021-07-15T04:16:35Z",
        "updatedAt" : "2021-07-15T04:51:30Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      },
      {
        "id" : "382323ca-d659-4518-a500-4261e80a4a94",
        "parentId" : "a29f8726-aefb-4347-8e58-64c8514b0fcf",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "Comparing with the similar logic of AggUtils, maybe we can also remove this config? Just always do the local merge?",
        "createdAt" : "2021-07-15T04:22:12Z",
        "updatedAt" : "2021-07-15T04:51:30Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      },
      {
        "id" : "0c1f4b70-b3aa-4cac-bf22-b9e60a90ff91",
        "parentId" : "a29f8726-aefb-4347-8e58-64c8514b0fcf",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Yea, is it necessary to have this config? Seems we can always do it.",
        "createdAt" : "2021-07-15T05:17:31Z",
        "updatedAt" : "2021-07-15T05:39:10Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "87937ab3-25cf-4ae6-81d3-ad2a7cf3d594",
        "parentId" : "a29f8726-aefb-4347-8e58-64c8514b0fcf",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "As I explained on the doc method, this would incur additional \"logical sort\", so only useful when there're lots of input rows which are going to be consolidated into same session. The benefit is dependent on the characteristic of data.\r\n\r\nIf we want to pick one between two to simplify, it would be probably safer to remove local aggregation.",
        "createdAt" : "2021-07-15T06:34:59Z",
        "updatedAt" : "2021-07-15T06:34:59Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "bbade3501f16e9437ba9af4feca3e2029785d273",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1612,1616 @@\n  val STREAMING_SESSION_WINDOW_MERGE_SESSIONS_IN_LOCAL_PARTITION =\n    buildConf(\"spark.sql.streaming.sessionWindow.merge.sessions.in.local.partition\")\n      .internal()\n      .doc(\"When true, streaming session window sorts and merge sessions in local partition \" +"
  },
  {
    "id" : "67e1a57b-38af-4853-a248-57634c7f8a8d",
    "prId" : 33081,
    "prUrl" : "https://github.com/apache/spark/pull/33081#pullrequestreview-706904781",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "990650e9-93f6-4dfc-8638-327536180809",
        "parentId" : null,
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "nit: version",
        "createdAt" : "2021-07-15T04:22:21Z",
        "updatedAt" : "2021-07-15T04:51:30Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "bbade3501f16e9437ba9af4feca3e2029785d273",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +1618,1622 @@        \"there're lots of rows in a batch being assigned to same sessions.\")\n      .version(\"3.2.0\")\n      .booleanConf\n      .createWithDefault(false)\n"
  },
  {
    "id" : "6ae99fdc-36f9-41be-b878-99b083b7819b",
    "prId" : 32944,
    "prUrl" : "https://github.com/apache/spark/pull/32944#pullrequestreview-698959883",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d6bc2330-84cb-4b5d-956f-ea1ef838a53d",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "the only think is that the version has to be 3.3.0 since we cut the branch now. Since this PR won't likely affect anything in the main code, I am okay with merging to 3.2.0 either tho. I will leave it to @cloud-fan and you.",
        "createdAt" : "2021-07-05T00:31:06Z",
        "updatedAt" : "2021-07-05T00:31:25Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "540b702e-dae5-43da-ac24-5af339be85dd",
        "parentId" : "d6bc2330-84cb-4b5d-956f-ea1ef838a53d",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "3.2 is the first version that enables AQE by default, and this seems to be a useful extension. Let's include it in 3.2.",
        "createdAt" : "2021-07-05T09:05:55Z",
        "updatedAt" : "2021-07-05T09:05:55Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "ac5c12186035a47bbe7c0b1891564066db676354",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +683,687 @@      .doc(\"The custom cost evaluator class to be used for adaptive execution. If not being set,\" +\n        \" Spark will use its own SimpleCostEvaluator by default.\")\n      .version(\"3.2.0\")\n      .stringConf\n      .createOptional"
  }
]