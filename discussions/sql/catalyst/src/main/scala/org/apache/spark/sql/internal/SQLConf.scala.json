[
  {
    "id" : "1006393a-1dcd-46cb-9648-f462f3c15105",
    "prId" : 33655,
    "prUrl" : "https://github.com/apache/spark/pull/33655#pullrequestreview-724508457",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6f771c2c-d596-4ca9-a339-a39044e93897",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Could you add this check into `spark.sql.adaptive.advisoryPartitionSizeInBytes`, too?",
        "createdAt" : "2021-08-05T19:11:39Z",
        "updatedAt" : "2021-08-05T19:11:39Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "2f3635cd-b39d-4b85-bc81-36325b1b97f7",
        "parentId" : "6f771c2c-d596-4ca9-a339-a39044e93897",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "We can't add check in `ADVISORY_PARTITION_SIZE_IN_BYTES`, because it falls back to `SHUFFLE_TARGET_POSTSHUFFLE_INPUT_SIZE`, which is this conf.",
        "createdAt" : "2021-08-06T03:21:40Z",
        "updatedAt" : "2021-08-06T03:21:41Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "7d5b09ab-ea22-4004-b963-7a431f6b59bc",
        "parentId" : "6f771c2c-d596-4ca9-a339-a39044e93897",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "@cloud-fan . If we set negative values to `advisoryPartitionSizeInBytes` explicitly, it will not fallback to here. If we want to say, `advisoryPartitionSizeInBytes must be positive`, we should check both places, shouldn't we?",
        "createdAt" : "2021-08-06T16:06:34Z",
        "updatedAt" : "2021-08-06T16:06:34Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "eeeff3ae6bb62eedeb09011cc99514d240a8b7d0",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +481,485 @@      .version(\"1.6.0\")\n      .bytesConf(ByteUnit.BYTE)\n      .checkValue(_ > 0, \"advisoryPartitionSizeInBytes must be positive\")\n      .createWithDefaultString(\"64MB\")\n"
  },
  {
    "id" : "98d04720-367d-4e9e-8c61-371061b7bdc3",
    "prId" : 33444,
    "prUrl" : "https://github.com/apache/spark/pull/33444#pullrequestreview-711555608",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "23cb04d3-961d-4052-92fa-d13cc850bf24",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "indentation ?",
        "createdAt" : "2021-07-21T10:35:50Z",
        "updatedAt" : "2021-07-21T10:38:09Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      },
      {
        "id" : "ec7c2243-c1ee-4f0d-a674-6cd9958f62a0",
        "parentId" : "23cb04d3-961d-4052-92fa-d13cc850bf24",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "Oh, this is on purpose.",
        "createdAt" : "2021-07-21T11:40:24Z",
        "updatedAt" : "2021-07-21T11:40:25Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "221e62227f09d7e29fbab5c674af29b1f829b91e",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +3978,3982 @@  def timestampType: AtomicType = getConf(TIMESTAMP_TYPE) match {\n    // SPARK-36227: Remove TimestampNTZ type support in Spark 3.2 with minimal code changes.\n    //              The configuration `TIMESTAMP_TYPE` is only effective for testing in Spark 3.2.\n    case \"TIMESTAMP_NTZ\" if Utils.isTesting =>\n      TimestampNTZType"
  },
  {
    "id" : "28a8d916-3fd7-42f4-91a2-f8c55daf94f6",
    "prId" : 33348,
    "prUrl" : "https://github.com/apache/spark/pull/33348#pullrequestreview-706916663",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a27a7d03-c265-47e0-85f8-c44ef4ed3fb8",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Is it okay to reuse the hive config for the purpose? Sounds more like `spark.sql.metastorePartitionPruning` now?",
        "createdAt" : "2021-07-15T02:47:06Z",
        "updatedAt" : "2021-07-15T02:47:06Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "da1fbcbd-7551-4e1f-be7a-3b36855d6bd5",
        "parentId" : "a27a7d03-c265-47e0-85f8-c44ef4ed3fb8",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "yeah I agree, that's the exactly same point I mentioned in the JIRA ..",
        "createdAt" : "2021-07-15T02:52:06Z",
        "updatedAt" : "2021-07-15T02:52:07Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "98cca643-0333-477b-a4b6-e1c7fb947bb2",
        "parentId" : "a27a7d03-c265-47e0-85f8-c44ef4ed3fb8",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "The reason I reused the config is because it is not about Hive tables, but HMS-based catalog which is shared by both Hive tables and non-Hive tables in Spark. This is similar to `spark.sql.hive.manageFilesourcePartitions` which is used by data source tables even though it has `hive` in its name.\r\n\r\n",
        "createdAt" : "2021-07-15T04:38:10Z",
        "updatedAt" : "2021-07-15T04:38:10Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      }
    ],
    "commit" : "f686496a2a21b1075ff182dc936643d71e1e85e3",
    "line" : 1,
    "diffHunk" : "@@ -1,1 +977,981 @@    .createWithDefault(false)\n\n  val HIVE_METASTORE_PARTITION_PRUNING =\n    buildConf(\"spark.sql.hive.metastorePartitionPruning\")\n      .doc(\"When true, some predicates will be pushed down into the Hive metastore so that \" +"
  },
  {
    "id" : "7d2b99d9-a95b-4050-a90c-08628d93a8cb",
    "prId" : 33348,
    "prUrl" : "https://github.com/apache/spark/pull/33348#pullrequestreview-708603463",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "64a55233-e16f-4f4e-8d91-7013e680f0f9",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "`HIVE_MANAGE_FILESOURCE_PARTITIONS` mentions that when it is enabled, it uses the metastore to prune partitions during query planning. Maybe we should update the doc too.",
        "createdAt" : "2021-07-15T02:58:37Z",
        "updatedAt" : "2021-07-15T02:58:37Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "f69dd937-592d-4590-98f6-e49e3dc0e4a9",
        "parentId" : "64a55233-e16f-4f4e-8d91-7013e680f0f9",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Yea, we should mention that it only does so when `spark.sql.hive.metastorePartitionPruning` is turned on.",
        "createdAt" : "2021-07-15T04:39:19Z",
        "updatedAt" : "2021-07-15T04:39:19Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "790de5fb-4048-4338-87c5-c90c046b8e95",
        "parentId" : "64a55233-e16f-4f4e-8d91-7013e680f0f9",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Seems not update yet?",
        "createdAt" : "2021-07-16T08:11:40Z",
        "updatedAt" : "2021-07-16T08:11:41Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "e128d840-a738-4425-9328-e8d2df9fb4be",
        "parentId" : "64a55233-e16f-4f4e-8d91-7013e680f0f9",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Hmm I thought this needs to be updated if we don't have this PR, since Spark doesn't prune partitions for file data source tables on metastore side. But with this PR the description becomes correct?",
        "createdAt" : "2021-07-16T17:21:19Z",
        "updatedAt" : "2021-07-16T17:21:19Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "0ec79b21-30b3-4fb4-bc1a-6a217e38ff43",
        "parentId" : "64a55233-e16f-4f4e-8d91-7013e680f0f9",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Oh, I see. I should mention `spark.sql.hive.metastorePartitionPruning` there. Will update.",
        "createdAt" : "2021-07-16T17:22:15Z",
        "updatedAt" : "2021-07-16T17:22:15Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      }
    ],
    "commit" : "f686496a2a21b1075ff182dc936643d71e1e85e3",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +980,984 @@    buildConf(\"spark.sql.hive.metastorePartitionPruning\")\n      .doc(\"When true, some predicates will be pushed down into the Hive metastore so that \" +\n           \"unmatching partitions can be eliminated earlier.\")\n      .version(\"1.5.0\")\n      .booleanConf"
  },
  {
    "id" : "7238b7ae-65a9-47c1-a00d-f2a76aea8b8b",
    "prId" : 33176,
    "prUrl" : "https://github.com/apache/spark/pull/33176#pullrequestreview-697432760",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9478feea-b649-41dc-a071-782cb70ba33e",
        "parentId" : null,
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "The scope is bigger than this. It will affect timestamp literal, function `to_timestamp` and data source io, etc. We will get to that later.",
        "createdAt" : "2021-07-01T16:15:44Z",
        "updatedAt" : "2021-07-01T16:15:44Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "6c6bf3a04572a832ff9224c1a3cd61b81784c625",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +2828,2832 @@  val TIMESTAMP_TYPE =\n    buildConf(\"spark.sql.timestampType\")\n      .doc(\"Configures the default timestamp type of Spark SQL, including SQL DDL and Cast \" +\n        s\"clause. Setting the configuration as ${TimestampTypes.TIMESTAMP_NTZ.toString} will \" +\n        \"use TIMESTAMP WITHOUT TIME ZONE as the default type while putting it as \" +"
  },
  {
    "id" : "664cb507-9819-4442-8415-2a6f9270ddf1",
    "prId" : 33176,
    "prUrl" : "https://github.com/apache/spark/pull/33176#pullrequestreview-697488439",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "17cb3762-17e7-40ee-bcef-26125f26516e",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "I wonder about types literals, for instance `timestamp'2021-07-01 01:02:03'`. Should the config influence on it too?",
        "createdAt" : "2021-07-01T16:20:01Z",
        "updatedAt" : "2021-07-01T16:26:02Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "32a32ad8-5877-4026-b429-fbc3ae13a782",
        "parentId" : "17cb3762-17e7-40ee-bcef-26125f26516e",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "Yes, I mentioned in https://github.com/apache/spark/pull/33176#discussion_r662426271. The type literal logic with TIMESTAMP_NTZ:\r\n1. if there is no time zone part, return timestamp without time zone literal\r\n2. otherwise, return timestamp with local time zone. ",
        "createdAt" : "2021-07-01T17:21:11Z",
        "updatedAt" : "2021-07-01T17:21:11Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "6c6bf3a04572a832ff9224c1a3cd61b81784c625",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +2828,2832 @@  val TIMESTAMP_TYPE =\n    buildConf(\"spark.sql.timestampType\")\n      .doc(\"Configures the default timestamp type of Spark SQL, including SQL DDL and Cast \" +\n        s\"clause. Setting the configuration as ${TimestampTypes.TIMESTAMP_NTZ.toString} will \" +\n        \"use TIMESTAMP WITHOUT TIME ZONE as the default type while putting it as \" +"
  },
  {
    "id" : "5a76519c-a40b-470c-b393-53a2c2e24100",
    "prId" : 33176,
    "prUrl" : "https://github.com/apache/spark/pull/33176#pullrequestreview-697536637",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "734df2b1-6200-47bb-b792-9c6f74924bc9",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "I thought the compiler automatically calls the `toString` method in string interpolations, so, you can skip it. Or I am wrong?",
        "createdAt" : "2021-07-01T18:17:53Z",
        "updatedAt" : "2021-07-01T18:17:54Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "6c6bf3a04572a832ff9224c1a3cd61b81784c625",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +2829,2833 @@    buildConf(\"spark.sql.timestampType\")\n      .doc(\"Configures the default timestamp type of Spark SQL, including SQL DDL and Cast \" +\n        s\"clause. Setting the configuration as ${TimestampTypes.TIMESTAMP_NTZ.toString} will \" +\n        \"use TIMESTAMP WITHOUT TIME ZONE as the default type while putting it as \" +\n        s\"${TimestampTypes.TIMESTAMP_LTZ.toString} will use TIMESTAMP WITH LOCAL TIME ZONE. \" +"
  },
  {
    "id" : "67ddb6c1-31ff-442e-8fdf-ce60be0b2def",
    "prId" : 33172,
    "prUrl" : "https://github.com/apache/spark/pull/33172#pullrequestreview-697345900",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "163d76ce-a571-4e21-aa67-dc79b3707e30",
        "parentId" : null,
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "better to update the doc of\r\nADVISORY_PARTITION_SIZE_IN_BYTES",
        "createdAt" : "2021-07-01T14:21:12Z",
        "updatedAt" : "2021-07-01T14:21:50Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "edc1a263-a317-407b-9471-da04309929b1",
        "parentId" : "163d76ce-a571-4e21-aa67-dc79b3707e30",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "`ADVISORY_PARTITION_SIZE_IN_BYTES` doesn't mention `COALESCE_PARTITIONS_MIN_PARTITION_NUM` either. I think it's fine to only refer to \"big\" confs in the \"small\" confs, not from the other side.",
        "createdAt" : "2021-07-01T14:54:02Z",
        "updatedAt" : "2021-07-01T14:54:02Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "51546e87c33064c80f902d4eecff1f640222dfb2",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +529,533 @@\n  val COALESCE_PARTITIONS_PARALLELISM_FIRST =\n    buildConf(\"spark.sql.adaptive.coalescePartitions.parallelismFirst\")\n      .doc(\"When true, Spark ignores the target size specified by \" +\n        s\"'${ADVISORY_PARTITION_SIZE_IN_BYTES.key}' (default 64MB) when coalescing contiguous \" +"
  },
  {
    "id" : "0871aeb8-ff77-4d83-93d2-f707dd159d71",
    "prId" : 33172,
    "prUrl" : "https://github.com/apache/spark/pull/33172#pullrequestreview-697522081",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4382901c-1208-4067-8e57-761fa2ec15e2",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "@viirya is it clearer to you now?",
        "createdAt" : "2021-07-01T17:22:59Z",
        "updatedAt" : "2021-07-01T17:23:05Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "56dc5122-5762-4883-bd39-4554eb66ac2c",
        "parentId" : "4382901c-1208-4067-8e57-761fa2ec15e2",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "okay",
        "createdAt" : "2021-07-01T18:03:21Z",
        "updatedAt" : "2021-07-01T18:03:21Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "51546e87c33064c80f902d4eecff1f640222dfb2",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +536,540 @@        \"This is to avoid performance regression when enabling adaptive query execution. \" +\n        \"It's recommended to set this config to false and respect the target size specified by \" +\n        s\"'${ADVISORY_PARTITION_SIZE_IN_BYTES.key}'.\")\n      .version(\"3.2.0\")\n      .booleanConf"
  },
  {
    "id" : "9a073cec-0a03-4dab-a0cb-ce1e0fb379b7",
    "prId" : 33081,
    "prUrl" : "https://github.com/apache/spark/pull/33081#pullrequestreview-706975183",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a29f8726-aefb-4347-8e58-64c8514b0fcf",
        "parentId" : null,
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "How about `spark.sql.streaming.sessionWindow.localMerge.enabled` or `spark.sql.streaming.sessionWindow. mergeSessionsInLocalPartition.enabled`?",
        "createdAt" : "2021-07-15T04:16:35Z",
        "updatedAt" : "2021-07-15T04:51:30Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      },
      {
        "id" : "382323ca-d659-4518-a500-4261e80a4a94",
        "parentId" : "a29f8726-aefb-4347-8e58-64c8514b0fcf",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "Comparing with the similar logic of AggUtils, maybe we can also remove this config? Just always do the local merge?",
        "createdAt" : "2021-07-15T04:22:12Z",
        "updatedAt" : "2021-07-15T04:51:30Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      },
      {
        "id" : "0c1f4b70-b3aa-4cac-bf22-b9e60a90ff91",
        "parentId" : "a29f8726-aefb-4347-8e58-64c8514b0fcf",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Yea, is it necessary to have this config? Seems we can always do it.",
        "createdAt" : "2021-07-15T05:17:31Z",
        "updatedAt" : "2021-07-15T05:39:10Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "87937ab3-25cf-4ae6-81d3-ad2a7cf3d594",
        "parentId" : "a29f8726-aefb-4347-8e58-64c8514b0fcf",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "As I explained on the doc method, this would incur additional \"logical sort\", so only useful when there're lots of input rows which are going to be consolidated into same session. The benefit is dependent on the characteristic of data.\r\n\r\nIf we want to pick one between two to simplify, it would be probably safer to remove local aggregation.",
        "createdAt" : "2021-07-15T06:34:59Z",
        "updatedAt" : "2021-07-15T06:34:59Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "bbade3501f16e9437ba9af4feca3e2029785d273",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1612,1616 @@\n  val STREAMING_SESSION_WINDOW_MERGE_SESSIONS_IN_LOCAL_PARTITION =\n    buildConf(\"spark.sql.streaming.sessionWindow.merge.sessions.in.local.partition\")\n      .internal()\n      .doc(\"When true, streaming session window sorts and merge sessions in local partition \" +"
  },
  {
    "id" : "67e1a57b-38af-4853-a248-57634c7f8a8d",
    "prId" : 33081,
    "prUrl" : "https://github.com/apache/spark/pull/33081#pullrequestreview-706904781",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "990650e9-93f6-4dfc-8638-327536180809",
        "parentId" : null,
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "nit: version",
        "createdAt" : "2021-07-15T04:22:21Z",
        "updatedAt" : "2021-07-15T04:51:30Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "bbade3501f16e9437ba9af4feca3e2029785d273",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +1618,1622 @@        \"there're lots of rows in a batch being assigned to same sessions.\")\n      .version(\"3.2.0\")\n      .booleanConf\n      .createWithDefault(false)\n"
  },
  {
    "id" : "6ae99fdc-36f9-41be-b878-99b083b7819b",
    "prId" : 32944,
    "prUrl" : "https://github.com/apache/spark/pull/32944#pullrequestreview-698959883",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d6bc2330-84cb-4b5d-956f-ea1ef838a53d",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "the only think is that the version has to be 3.3.0 since we cut the branch now. Since this PR won't likely affect anything in the main code, I am okay with merging to 3.2.0 either tho. I will leave it to @cloud-fan and you.",
        "createdAt" : "2021-07-05T00:31:06Z",
        "updatedAt" : "2021-07-05T00:31:25Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "540b702e-dae5-43da-ac24-5af339be85dd",
        "parentId" : "d6bc2330-84cb-4b5d-956f-ea1ef838a53d",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "3.2 is the first version that enables AQE by default, and this seems to be a useful extension. Let's include it in 3.2.",
        "createdAt" : "2021-07-05T09:05:55Z",
        "updatedAt" : "2021-07-05T09:05:55Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "ac5c12186035a47bbe7c0b1891564066db676354",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +683,687 @@      .doc(\"The custom cost evaluator class to be used for adaptive execution. If not being set,\" +\n        \" Spark will use its own SimpleCostEvaluator by default.\")\n      .version(\"3.2.0\")\n      .stringConf\n      .createOptional"
  },
  {
    "id" : "349c325b-92af-4409-b86f-3b3eaf20ba32",
    "prId" : 32865,
    "prUrl" : "https://github.com/apache/spark/pull/32865#pullrequestreview-681525932",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6438bf5a-e566-443d-9155-99eabe3d6f30",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Q: we need the two separate locks for the static/non-static configs?",
        "createdAt" : "2021-06-11T01:34:16Z",
        "updatedAt" : "2021-06-11T01:34:16Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "b6e0a028-f58e-4b0f-a06d-c0740d96782f",
        "parentId" : "6438bf5a-e566-443d-9155-99eabe3d6f30",
        "authorId" : "56f79eae-eef9-4cac-8f0c-c788c9a86e3a",
        "body" : "Thank you for your review! I think these are two different sets of configs, so using two separate locks should be better.",
        "createdAt" : "2021-06-11T07:22:21Z",
        "updatedAt" : "2021-06-11T07:23:43Z",
        "lastEditedBy" : "56f79eae-eef9-4cac-8f0c-c788c9a86e3a",
        "tags" : [
        ]
      }
    ],
    "commit" : "994cab24f91d681f187b3062da0de62a45b0ba6a",
    "line" : 30,
    "diffHunk" : "@@ -1,1 +60,64 @@  private[this] var sqlConfEntries: util.Map[String, ConfigEntry[_]] = util.Collections.emptyMap()\n\n  private[this] val staticConfKeysUpdateLock = new Object\n\n  @volatile"
  },
  {
    "id" : "666dfd56-fa92-4bc9-bbfa-5e1fbe5c3881",
    "prId" : 32865,
    "prUrl" : "https://github.com/apache/spark/pull/32865#pullrequestreview-681413755",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a1e8dfea-0a9d-4e29-ba06-b9595b1e7d61",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "~Is `private` enough?~ Never mind~",
        "createdAt" : "2021-06-11T03:19:55Z",
        "updatedAt" : "2021-06-11T03:22:22Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "994cab24f91d681f187b3062da0de62a45b0ba6a",
    "line" : 51,
    "diffHunk" : "@@ -1,1 +80,84 @@  }\n\n  private[internal] def getConfigEntry(key: String): ConfigEntry[_] = {\n    sqlConfEntries.get(key)\n  }"
  },
  {
    "id" : "8fb0e826-7178-41c4-839f-d9139027d4ac",
    "prId" : 32550,
    "prUrl" : "https://github.com/apache/spark/pull/32550#pullrequestreview-670156365",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1f734a2c-db32-43bc-8e73-8de7ee83e8df",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I would say:\r\n- \"If this value is not smaller than\" => \"If this value is larger than\"\r\n- \"all the partition size are not larger than this config\" => \"all the partition size are smaller than this config\"",
        "createdAt" : "2021-05-27T10:40:23Z",
        "updatedAt" : "2021-05-27T10:40:23Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "394d2736-4b78-4192-8b09-b2480488de9d",
        "parentId" : "1f734a2c-db32-43bc-8e73-8de7ee83e8df",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "I thought about this but the formula used equal, that said it would be `If this value is larger than or equal to xxx`. Do you think it's better ?",
        "createdAt" : "2021-05-27T13:05:50Z",
        "updatedAt" : "2021-05-27T13:05:50Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      }
    ],
    "commit" : "957215452e9df57f5e0424f63556897ea928744f",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +602,606 @@    buildConf(\"spark.sql.adaptive.maxShuffledHashJoinLocalMapThreshold\")\n      .doc(\"Configures the maximum size in bytes per partition that can be allowed to build \" +\n        \"local hash map. If this value is not smaller than \" +\n        s\"${ADVISORY_PARTITION_SIZE_IN_BYTES.key} and all the partition size are not larger \" +\n        \"than this config, join selection prefer to use shuffled hash join instead of \" +"
  },
  {
    "id" : "fe8b4a90-7333-4727-bc6c-9cd97deb7921",
    "prId" : 32129,
    "prUrl" : "https://github.com/apache/spark/pull/32129#pullrequestreview-633469192",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7a8df72d-8cd7-4900-ac20-42737ba417d8",
        "parentId" : null,
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "Moving `ANSI_ENABLED` to the front so that other configurations can refer to it without compiling errors.",
        "createdAt" : "2021-04-12T12:20:33Z",
        "updatedAt" : "2021-04-12T17:31:41Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "62cee4f24ed49d9c8f967f82267dbd3f3180c32c",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +207,211 @@    .createWithDefault(100)\n\n  val ANSI_ENABLED = buildConf(\"spark.sql.ansi.enabled\")\n    .doc(\"When true, Spark SQL uses an ANSI compliant dialect instead of being Hive compliant. \" +\n      \"For example, Spark will throw an exception at runtime instead of returning null results \" +"
  },
  {
    "id" : "3a2baac8-d3ae-4330-8fd3-16c76a499250",
    "prId" : 32049,
    "prUrl" : "https://github.com/apache/spark/pull/32049#pullrequestreview-627553202",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0ec09a71-2e6d-436b-9565-6a026ba7a0fa",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Just curious - would anyone ever _not_ want to push it down?\r\nI'm surprised, I thought we already did this!\r\nCC @cloud-fan ",
        "createdAt" : "2021-04-04T15:01:56Z",
        "updatedAt" : "2021-04-28T06:21:16Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "3efb61fd-870a-420f-b2fd-2ece2693ba71",
        "parentId" : "0ec09a71-2e6d-436b-9565-6a026ba7a0fa",
        "authorId" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "body" : "@srowen Hello Sean :)\r\nActually we only have filter push down for parquet, not aggregate push down yet. I will probably change the default to true after this PR gets reviewed and fully tested.",
        "createdAt" : "2021-04-04T16:12:52Z",
        "updatedAt" : "2021-04-28T06:21:16Z",
        "lastEditedBy" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "tags" : [
        ]
      }
    ],
    "commit" : "5c2b630a61d8ea9263116c37e154884a5cabd2ca",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +773,777 @@      .createWithDefault(10)\n\n  val PARQUET_AGGREGATE_PUSHDOWN_ENABLED = buildConf(\"spark.sql.parquet.aggregatePushdown\")\n    .doc(\"Enables Parquet aggregate push-down optimization when set to true.\")\n    .version(\"3.2.0\")"
  },
  {
    "id" : "432792bf-a654-4dd4-8558-d66da8540216",
    "prId" : 32049,
    "prUrl" : "https://github.com/apache/spark/pull/32049#pullrequestreview-628362844",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3fd0db59-3c93-4deb-8a96-aeb7ad09116a",
        "parentId" : null,
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "Shall it be an internal config or not? Do we expect this one to be user-facing and tune it frequently?",
        "createdAt" : "2021-04-05T07:47:49Z",
        "updatedAt" : "2021-04-28T06:21:16Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      },
      {
        "id" : "868bb4bc-c3ca-415d-9a1c-69bbaf17d6ac",
        "parentId" : "3fd0db59-3c93-4deb-8a96-aeb7ad09116a",
        "authorId" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "body" : "Thanks for reviewing!\r\nI think this should be similar to `PARQUET_FILTER_PUSHDOWN_ENABLED` and be a user-facing config. I guess we can default it to true in the future after we have more testing.",
        "createdAt" : "2021-04-06T01:10:13Z",
        "updatedAt" : "2021-04-28T06:21:16Z",
        "lastEditedBy" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "tags" : [
        ]
      }
    ],
    "commit" : "5c2b630a61d8ea9263116c37e154884a5cabd2ca",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +774,778 @@\n  val PARQUET_AGGREGATE_PUSHDOWN_ENABLED = buildConf(\"spark.sql.parquet.aggregatePushdown\")\n    .doc(\"Enables Parquet aggregate push-down optimization when set to true.\")\n    .version(\"3.2.0\")\n    .booleanConf"
  },
  {
    "id" : "c05d7789-3578-419c-9318-e0f52643bbf8",
    "prId" : 31986,
    "prUrl" : "https://github.com/apache/spark/pull/31986#pullrequestreview-656212437",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2ddbf876-4228-417f-88c1-779a697b1801",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "`in.memory` looks weird to me. Maybe just `inMemory`?",
        "createdAt" : "2021-05-11T00:52:31Z",
        "updatedAt" : "2021-05-11T04:07:22Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "275ee6db-50f4-45e4-a179-68d19d8fef3f",
        "parentId" : "2ddbf876-4228-417f-88c1-779a697b1801",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "I agree with you, but the name semantic follows existing configurations. Please search `_IN_MEMORY_THRESHOLD` into codebase.",
        "createdAt" : "2021-05-11T01:34:56Z",
        "updatedAt" : "2021-05-11T04:07:22Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "8c2eed82d553ddacb36088954232ae704a855094",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +2002,2006 @@\n  val SESSION_WINDOW_BUFFER_IN_MEMORY_THRESHOLD =\n    buildConf(\"spark.sql.sessionWindow.buffer.in.memory.threshold\")\n      .internal()\n      .doc(\"Threshold for number of windows guaranteed to be held in memory by the \" +"
  },
  {
    "id" : "02585522-117f-4689-a4b7-0377b1e7f4b4",
    "prId" : 31689,
    "prUrl" : "https://github.com/apache/spark/pull/31689#pullrequestreview-601362429",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "695bd7d7-5aaf-4850-9168-259943cbe925",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Seems a reasonable fix cc: @srowen @cloud-fan @viirya ",
        "createdAt" : "2021-03-02T02:57:07Z",
        "updatedAt" : "2021-03-02T02:57:07Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "4ffa2576782e410cef4d5cb3d8865362679ff11c",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +55,59 @@\n  private[sql] val sqlConfEntries =\n    new ConcurrentHashMap[String, ConfigEntry[_]]()\n\n  val staticConfKeys: java.util.Set[String] ="
  },
  {
    "id" : "a4769220-e41b-4012-a257-c72def286e7c",
    "prId" : 31578,
    "prUrl" : "https://github.com/apache/spark/pull/31578#pullrequestreview-592871935",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c43cccb4-5c6f-448d-9bda-920f914986bd",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Ur, is this guide correct? I guess the users are using `.format(\"avro\")` already.",
        "createdAt" : "2021-02-17T22:19:47Z",
        "updatedAt" : "2021-02-17T22:20:10Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "e27a2ca7-f0b2-4e6b-99fb-663c9f627c17",
        "parentId" : "c43cccb4-5c6f-448d-9bda-920f914986bd",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "The configs is about automatically mapping of `.format(\"com.databricks.spark.avro\")` to `.format(\"avro\")`, right? If we remove the config in the future, \"com.databricks.spark.avro\" will be not mapped to built in `avro`. So, in the guide, we recommend to change users code, and use the `avro` format directly.",
        "createdAt" : "2021-02-18T05:48:41Z",
        "updatedAt" : "2021-02-18T05:48:42Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "a2c2cfeb-41c8-49d0-8006-d1d38a49b6c9",
        "parentId" : "c43cccb4-5c6f-448d-9bda-920f914986bd",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Thanks. I was confused at that.",
        "createdAt" : "2021-02-18T05:53:29Z",
        "updatedAt" : "2021-02-18T05:53:30Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "660db04807559077244a31b916339a2876aeb0db",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +3148,3152 @@        s\"Use '${AVRO_REBASE_MODE_IN_READ.key}' instead.\"),\n      DeprecatedConfig(LEGACY_REPLACE_DATABRICKS_SPARK_AVRO_ENABLED.key, \"3.2\",\n        \"\"\"Use `.format(\"avro\")` in `DataFrameWriter` or `DataFrameReader` instead.\"\"\")\n    )\n"
  },
  {
    "id" : "bf78c885-f580-4fe8-96f5-6d3b01a4a5fa",
    "prId" : 31571,
    "prUrl" : "https://github.com/apache/spark/pull/31571#pullrequestreview-592680008",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f3f2f118-6858-414a-82e8-69197c7182df",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@gengliangwang do we need this conf? We do the same thing in `com.databricks.spark.csv` already by default internally. We could just make it exposed and deprecate this config.",
        "createdAt" : "2021-02-17T01:44:38Z",
        "updatedAt" : "2021-02-17T01:45:32Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "0decfaa4-6818-4e04-a3c7-37a831bfac6b",
        "parentId" : "f3f2f118-6858-414a-82e8-69197c7182df",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "In this way, I think it will address most of concerns raised.",
        "createdAt" : "2021-02-17T01:45:06Z",
        "updatedAt" : "2021-02-17T01:45:07Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "503c2e3f-6878-4840-9fba-89aa23159455",
        "parentId" : "f3f2f118-6858-414a-82e8-69197c7182df",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "And then we could remove both `com.databricks.spark.csv` and `com.databricks.spark.avro` fallbacks together in the future, of course (Spark 4.0?). I don't think it makes sense to keep both mapping forever.",
        "createdAt" : "2021-02-17T01:49:55Z",
        "updatedAt" : "2021-02-17T01:49:55Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "8bae371c-27c9-4541-8611-bdacce720505",
        "parentId" : "f3f2f118-6858-414a-82e8-69197c7182df",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Let's deprecate it and then remove it in later versions. It's OK if a deprecated internal config is mentioned in public docs for migration purpose.",
        "createdAt" : "2021-02-17T15:59:14Z",
        "updatedAt" : "2021-02-17T15:59:14Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "1b48cdcc-cf23-4d85-b3b8-34d7084f1420",
        "parentId" : "f3f2f118-6858-414a-82e8-69197c7182df",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Here is the PR https://github.com/apache/spark/pull/31578 to deprecate the config.",
        "createdAt" : "2021-02-17T18:29:52Z",
        "updatedAt" : "2021-02-17T18:29:52Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "1c55e9f4-584f-4073-a4e9-148e27dedbd7",
        "parentId" : "f3f2f118-6858-414a-82e8-69197c7182df",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Thank you all!",
        "createdAt" : "2021-02-17T22:21:28Z",
        "updatedAt" : "2021-02-17T22:21:28Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "c43224e894971ac9e500a0d59b1715f8f922caff",
    "line" : 3,
    "diffHunk" : "@@ -1,1 +2505,2509 @@\n  val LEGACY_REPLACE_DATABRICKS_SPARK_AVRO_ENABLED =\n    buildConf(\"spark.sql.legacy.replaceDatabricksSparkAvro.enabled\")\n      .doc(\"If it is set to true, the data source provider com.databricks.spark.avro is mapped \" +\n        \"to the built-in but external Avro data source module for backward compatibility.\")"
  },
  {
    "id" : "59e46a3b-9064-42bd-b86a-1edb93204b24",
    "prId" : 31421,
    "prUrl" : "https://github.com/apache/spark/pull/31421#pullrequestreview-581056259",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fef6313d-7d16-4b9b-b634-e728b5a49830",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "ditto, make it more specific.",
        "createdAt" : "2021-02-02T07:40:12Z",
        "updatedAt" : "2021-02-02T07:40:12Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "971ccbd5947924992a0c24acd40d40f1587d7741",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +2476,2480 @@    buildConf(\"spark.sql.legacy.parseNullPartitionSpecAsStringLiteral\")\n      .internal()\n      .doc(\"If it is set to true, a null partition value is parsed as a string literal of its \" +\n        \"text representation, e.g., string 'null'. Otherwise, null partition values are parsed \" +\n        \"as they are.\")"
  },
  {
    "id" : "d95b8dbd-fc0b-4832-8198-bddc8024c1ac",
    "prId" : 31421,
    "prUrl" : "https://github.com/apache/spark/pull/31421#pullrequestreview-581062903",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9e1b8ae8-f046-4888-92ec-e7ed6d12af47",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "the version should be 3.1.1, as we are going to backport this.",
        "createdAt" : "2021-02-02T07:40:33Z",
        "updatedAt" : "2021-02-02T07:40:33Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "aeeb1c7f-a5fe-44bd-8d2a-96c15e321097",
        "parentId" : "9e1b8ae8-f046-4888-92ec-e7ed6d12af47",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "should be 3.0.2 actually.",
        "createdAt" : "2021-02-02T07:42:52Z",
        "updatedAt" : "2021-02-02T07:42:52Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "7010947b-a6cf-4dfc-8df8-ec32d01cd91c",
        "parentId" : "9e1b8ae8-f046-4888-92ec-e7ed6d12af47",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Ah I see, then how should we place it in the migration guide? Repeat it 3 times in 3.0.2, 3.1.1 and 3.2.0 sections?",
        "createdAt" : "2021-02-02T07:46:40Z",
        "updatedAt" : "2021-02-02T07:46:40Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "935f5e60-073c-4953-abc4-d7b73f8a7ae3",
        "parentId" : "9e1b8ae8-f046-4888-92ec-e7ed6d12af47",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I think we have documented only in 3.0.2 so far in such cases ..",
        "createdAt" : "2021-02-02T07:50:42Z",
        "updatedAt" : "2021-02-02T07:50:42Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "971ccbd5947924992a0c24acd40d40f1587d7741",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +2479,2483 @@        \"text representation, e.g., string 'null'. Otherwise, null partition values are parsed \" +\n        \"as they are.\")\n      .version(\"3.2.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "1ab4d35f-d105-4f2e-958a-b486e6befd0a",
    "prId" : 31173,
    "prUrl" : "https://github.com/apache/spark/pull/31173#pullrequestreview-567895812",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2b9dd652-a11c-4863-8069-e650d2c942c3",
        "parentId" : null,
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "This change add a new configuration. cc @cloud-fan",
        "createdAt" : "2021-01-14T06:52:53Z",
        "updatedAt" : "2021-01-14T06:52:53Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      }
    ],
    "commit" : "5c8fa18aa9e1e0ff1436361a7b67055647f39d5c",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +3050,3054 @@     .version(\"3.2.0\")\n    .booleanConf\n    .createWithDefault(false)\n\n  /**"
  },
  {
    "id" : "6195228a-6811-4225-a90c-0d68a33dec01",
    "prId" : 31173,
    "prUrl" : "https://github.com/apache/spark/pull/31173#pullrequestreview-571894946",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "705e73a9-8036-4632-911f-7dc4b129545a",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "nit: Looks wrong indents.",
        "createdAt" : "2021-01-20T06:05:38Z",
        "updatedAt" : "2021-01-20T06:05:38Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "5c8fa18aa9e1e0ff1436361a7b67055647f39d5c",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +3047,3051 @@  val CLI_PRINT_HEADER =\n    buildConf(\"spark.sql.cli.print.header\")\n     .doc(\"When set to true, spark-sql CLI prints the names of the columns in query output.\")\n     .version(\"3.2.0\")\n    .booleanConf"
  },
  {
    "id" : "867990aa-fabf-45ae-8b06-a1ac2f1b4a4d",
    "prId" : 31002,
    "prUrl" : "https://github.com/apache/spark/pull/31002#pullrequestreview-560810087",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "288d0512-9ff0-43a5-8e76-7c923a8b44dc",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Maybe we should also fix in `DataFrameWriter.scala` and `readwriter.py` too.",
        "createdAt" : "2021-01-04T06:57:59Z",
        "updatedAt" : "2021-01-04T07:01:55Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "f6b7bfe7-13be-4729-a083-216806fd3195",
        "parentId" : "288d0512-9ff0-43a5-8e76-7c923a8b44dc",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Sure, thanks!",
        "createdAt" : "2021-01-04T06:58:17Z",
        "updatedAt" : "2021-01-04T07:01:55Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "ca1aa2c80e38147f87e339463258654c81827bdd",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +797,801 @@      \"`orc.compress` is specified in the table-specific options/properties, the precedence \" +\n      \"would be `compression`, `orc.compress`, `spark.sql.orc.compression.codec`.\" +\n      \"Acceptable values include: none, uncompressed, snappy, zlib, lzo, zstd.\")\n    .version(\"2.3.0\")\n    .stringConf"
  },
  {
    "id" : "6361a479-e057-43af-8540-f0d7ab70efb5",
    "prId" : 31002,
    "prUrl" : "https://github.com/apache/spark/pull/31002#pullrequestreview-560817427",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "519e4954-ef39-46e9-a395-04d65e4b8845",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Btw, we cannot use `lz4` for the orc format?",
        "createdAt" : "2021-01-04T07:11:18Z",
        "updatedAt" : "2021-01-04T07:11:19Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "5aeedc5d-a6ae-4152-91ee-f1089a4c4cd5",
        "parentId" : "519e4954-ef39-46e9-a395-04d65e4b8845",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Is it supported before, @maropu ?",
        "createdAt" : "2021-01-04T07:12:36Z",
        "updatedAt" : "2021-01-04T07:12:36Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "1d83a4d4-e0b2-4cfd-9e38-ed2de4331cf9",
        "parentId" : "519e4954-ef39-46e9-a395-04d65e4b8845",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "I'm not familiar with this part though, I saw the ORC code: \r\nhttps://github.com/apache/orc/blob/f6b6b2ea70f1b74e2ffd897e8985ffcd1c082582/java/core/src/java/org/apache/orc/CompressionKind.java#L25-L27",
        "createdAt" : "2021-01-04T07:15:47Z",
        "updatedAt" : "2021-01-04T07:15:47Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "35863d3d-aa5c-4d83-a563-a3c31eddb89e",
        "parentId" : "519e4954-ef39-46e9-a395-04d65e4b8845",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "ORC does, but what I mean is that in Apache Spark.",
        "createdAt" : "2021-01-04T07:17:26Z",
        "updatedAt" : "2021-01-04T07:17:26Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "c7417f7b-ed51-4094-836b-2cd3775ffa6a",
        "parentId" : "519e4954-ef39-46e9-a395-04d65e4b8845",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Ah, ok. nvm. Just a question.",
        "createdAt" : "2021-01-04T07:20:00Z",
        "updatedAt" : "2021-01-04T07:20:01Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "ef744109-effb-45cd-bb27-2dd6df41415b",
        "parentId" : "519e4954-ef39-46e9-a395-04d65e4b8845",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "I just didn't use it before. Feel free to make a PR if you want to add it, @maropu .",
        "createdAt" : "2021-01-04T07:20:11Z",
        "updatedAt" : "2021-01-04T07:20:11Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "ca1aa2c80e38147f87e339463258654c81827bdd",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +801,805 @@    .stringConf\n    .transform(_.toLowerCase(Locale.ROOT))\n    .checkValues(Set(\"none\", \"uncompressed\", \"snappy\", \"zlib\", \"lzo\", \"zstd\"))\n    .createWithDefault(\"snappy\")\n"
  },
  {
    "id" : "b9eb8a75-54c3-408a-82b5-5a73dded4548",
    "prId" : 30897,
    "prUrl" : "https://github.com/apache/spark/pull/30897#pullrequestreview-557467159",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cfca4882-0aea-413d-95de-1a2cc1ccfc47",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Please mention this on the PR title, @ulysses-you . ",
        "createdAt" : "2020-12-23T02:02:30Z",
        "updatedAt" : "2020-12-23T03:25:56Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "a7a66a75-73fe-46a4-8854-549dcc94c47e",
        "parentId" : "cfca4882-0aea-413d-95de-1a2cc1ccfc47",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "yes, update it.",
        "createdAt" : "2020-12-23T02:03:42Z",
        "updatedAt" : "2020-12-23T03:25:56Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      }
    ],
    "commit" : "8701feb11173deb82c8bb8070e13652a76abe38e",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +972,976 @@        \"set for each statement via `java.sql.Statement.setQueryTimeout` and they are smaller \" +\n        \"than this configuration value, they take precedence. If you set this timeout and prefer \" +\n        \"to cancel the queries right away without waiting task to finish, consider enabling \" +\n        s\"${THRIFTSERVER_FORCE_CANCEL.key} together.\")\n      .version(\"3.1.0\")"
  },
  {
    "id" : "b7e43f00-3307-4b35-86ec-53a059b25556",
    "prId" : 30554,
    "prUrl" : "https://github.com/apache/spark/pull/30554#pullrequestreview-541181246",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c26c59ee-4eca-4641-8297-163414880747",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "This seems to be inconsistent because the other code expects `LEGACY_CREATE_HIVE_TABLE_BY_DEFAULT_ENABLED`.",
        "createdAt" : "2020-11-30T18:29:34Z",
        "updatedAt" : "2020-12-02T08:50:59Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "bff924de21b8af2da17f31591768ce02c784bd43",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +2923,2927 @@    .createWithDefault(\"\")\n\n  val LEGACY_CREATE_HIVE_TABLE_BY_DEFAULT =\n    buildConf(\"spark.sql.legacy.createHiveTableByDefault\")\n      .internal()"
  },
  {
    "id" : "ca7d75aa-52f3-4dff-a72c-44ed661710b2",
    "prId" : 30554,
    "prUrl" : "https://github.com/apache/spark/pull/30554#pullrequestreview-542416388",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ce57e948-4f3b-434e-bfa5-387fa1a74bcc",
        "parentId" : null,
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "If this is a \"legacy\" key mention how long folks can depend on it.",
        "createdAt" : "2020-12-02T00:25:31Z",
        "updatedAt" : "2020-12-02T08:50:59Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "47ae3ac5-5ddf-4a89-85dd-a81c2cdf88bc",
        "parentId" : "ce57e948-4f3b-434e-bfa5-387fa1a74bcc",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "There are already a lot of legacy configurations, and there's no plan for how long we'll keep it. That's what I know as far as I have followed in the community. It would be a separate issue to decide lifetime of legacy configurations but ideally the legacy configurations will be removed in the major release bumpup I guess. I remember I discussed this with Sean as well somewhere. cc @srowen FYI",
        "createdAt" : "2020-12-02T00:31:23Z",
        "updatedAt" : "2020-12-02T08:50:59Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "57433591-9d80-49ac-929c-74d9c208509a",
        "parentId" : "ce57e948-4f3b-434e-bfa5-387fa1a74bcc",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "If we have a general policy for legacy configs that means this remains the same until Spark 4 by default I guess there is no need to document it here (I don't remember that conversation but I was out for a few months last/this year).",
        "createdAt" : "2020-12-02T00:53:06Z",
        "updatedAt" : "2020-12-02T08:50:59Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "bff924de21b8af2da17f31591768ce02c784bd43",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +2927,2931 @@      .internal()\n      .doc(\"When set to true, CREATE TABLE syntax without USING or STORED AS will use Hive \" +\n        s\"instead of the value of ${DEFAULT_DATA_SOURCE_NAME.key} as the table provider.\")\n      .version(\"3.1.0\")\n      .booleanConf"
  },
  {
    "id" : "ba83befb-d0f9-4957-9adb-df914c60e4b5",
    "prId" : 30341,
    "prUrl" : "https://github.com/apache/spark/pull/30341#pullrequestreview-528781095",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "58a71ca8-e373-4d0a-aa99-c3ccc69469ba",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "`100` is enough?",
        "createdAt" : "2020-11-12T06:14:34Z",
        "updatedAt" : "2020-11-17T01:00:33Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "079b4d46-b8ed-47e8-aa40-ac4e2523ebc2",
        "parentId" : "58a71ca8-e373-4d0a-aa99-c3ccc69469ba",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "This default value is copied from code generator's cache setting. I guess this should be enough.",
        "createdAt" : "2020-11-12T06:28:42Z",
        "updatedAt" : "2020-11-17T01:00:33Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "db115d6d850942eca5dc6fac80896b1038561e51",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +547,551 @@      .intConf\n      .checkValue(_ >= 0, \"The maximum must not be negative\")\n      .createWithDefault(100)\n\n  val CASE_SENSITIVE = buildConf(\"spark.sql.caseSensitive\")"
  },
  {
    "id" : "b75dccb4-3846-4736-9a5c-2e0353c46271",
    "prId" : 30325,
    "prUrl" : "https://github.com/apache/spark/pull/30325#pullrequestreview-531259095",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d821e514-3b60-48af-981e-a4010a23cbf5",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "static conf should go to `StaticSQLConf`",
        "createdAt" : "2020-11-16T12:04:34Z",
        "updatedAt" : "2020-11-17T02:11:32Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "231cd6f15f3fa71d677aae7170e1e404b82cef2a",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +816,820 @@      .createWithDefault(true)\n\n  val HIVE_METASTORE_PARTITION_PRUNING_INSET_THRESHOLD =\n    buildConf(\"spark.sql.hive.metastorePartitionPruningInSetThreshold\")\n      .doc(\"The threshold of set size for InSet predicate when pruning partitions through Hive \" +"
  },
  {
    "id" : "a31fde97-2b50-4668-ac61-d457b19409e9",
    "prId" : 30225,
    "prUrl" : "https://github.com/apache/spark/pull/30225#pullrequestreview-527880909",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "121b6659-3eec-4cd1-85a3-8ff907f07c3b",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "`-1` to follow the Hive config?",
        "createdAt" : "2020-11-11T05:55:47Z",
        "updatedAt" : "2020-12-07T05:36:46Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "020d7dc8c2eb299540ead4be974ba45e90d81a9f",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +845,849 @@      .intConf\n      .checkValue(_ >= -1, \"The maximum must be a positive integer, -1 to follow the Hive config.\")\n      .createWithDefault(100000)\n\n  val HIVE_MANAGE_FILESOURCE_PARTITIONS ="
  },
  {
    "id" : "a76c5048-3376-4af1-8bad-34b5cfd73faf",
    "prId" : 30225,
    "prUrl" : "https://github.com/apache/spark/pull/30225#pullrequestreview-544187074",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5646c47d-f55c-4d0c-8356-fffffe587c10",
        "parentId" : null,
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "have you considered to keep the default behavior as it is but allow it to be configurable? changing it means now we'll need to make two HMS calls (one additional `getNumPartitionsByFilter`) which I'm not sure is desirable (have seen HMS perform very badly in production before).",
        "createdAt" : "2020-11-20T00:43:52Z",
        "updatedAt" : "2020-12-07T05:36:46Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "34294955-5d9c-4f54-a563-0a496cb101af",
        "parentId" : "5646c47d-f55c-4d0c-8356-fffffe587c10",
        "authorId" : "a74459ae-fcfb-427d-b810-b8c75c10717d",
        "body" : "@sunchao Thank you for your response. I think this is a reasonable maximum, and Presto also has a parameter to limit the number of partitions in HiveMetadata#getPartitionsAsList, default value is 100_000",
        "createdAt" : "2020-12-03T08:25:10Z",
        "updatedAt" : "2020-12-07T05:36:46Z",
        "lastEditedBy" : "a74459ae-fcfb-427d-b810-b8c75c10717d",
        "tags" : [
        ]
      },
      {
        "id" : "2727c972-b13a-411b-975a-9b31fe19127b",
        "parentId" : "5646c47d-f55c-4d0c-8356-fffffe587c10",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Yea the default value 100_000 looks fine to me. My main question is whether we need to make the default value to be that and double the HMS calls. Presto doesn't call `getNumPartitionsByFilter` it seems as it streams through a partition iterator and stops once the threshold is reached.",
        "createdAt" : "2020-12-03T17:09:08Z",
        "updatedAt" : "2020-12-07T05:36:46Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      }
    ],
    "commit" : "020d7dc8c2eb299540ead4be974ba45e90d81a9f",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +845,849 @@      .intConf\n      .checkValue(_ >= -1, \"The maximum must be a positive integer, -1 to follow the Hive config.\")\n      .createWithDefault(100000)\n\n  val HIVE_MANAGE_FILESOURCE_PARTITIONS ="
  },
  {
    "id" : "de81b2f9-ec22-4333-9a0f-d2b6bbd02243",
    "prId" : 30210,
    "prUrl" : "https://github.com/apache/spark/pull/30210#pullrequestreview-528798476",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b220168f-27c2-4b76-bd59-11f7fe961c80",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "For the naming, cc @cloud-fan .",
        "createdAt" : "2020-11-12T07:08:48Z",
        "updatedAt" : "2020-11-12T07:08:48Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "5dff48f5c3d60801e425951cbb0b30175e54f2be",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1384,1388 @@\n  val STATEFUL_OPERATOR_CHECK_CORRECTNESS_ENABLED =\n    buildConf(\"spark.sql.streaming.statefulOperator.checkCorrectness.enabled\")\n      .internal()\n      .doc(\"When true, the stateful operators for streaming query will be checked for possible \" +"
  },
  {
    "id" : "c084c094-bcf5-465a-aeec-cff8995f9279",
    "prId" : 30162,
    "prUrl" : "https://github.com/apache/spark/pull/30162#pullrequestreview-518310359",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7036595e-f355-4d01-bcf5-fd815a2f488d",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "This doesn't change the default value. I believe we can add the benchmark result as a follow-up, @HeartSaVioR .",
        "createdAt" : "2020-10-28T04:08:28Z",
        "updatedAt" : "2020-10-29T02:49:20Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "4dc153db5eeea904e0c2208726cbd41bd514b62f",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +1333,1337 @@      .version(\"3.1.0\")\n      .stringConf\n      .createWithDefault(\"lz4\")\n\n  val STREAMING_AGGREGATION_STATE_FORMAT_VERSION ="
  },
  {
    "id" : "059fee83-7d67-4785-b3c3-86d145f443a1",
    "prId" : 30162,
    "prUrl" : "https://github.com/apache/spark/pull/30162#pullrequestreview-519193915",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7da4e3f9-3d1e-4b7c-aa83-b7026e0658e0",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Shall we add a test case for this assertion, `fully qualified class names`? In the new test case, it seems that we always use `getShortName`.",
        "createdAt" : "2020-10-28T23:52:14Z",
        "updatedAt" : "2020-10-29T02:49:20Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "4b2e7533-15c9-4289-b396-d05d852d1cb9",
        "parentId" : "7da4e3f9-3d1e-4b7c-aa83-b7026e0658e0",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Also, I'm wondering what happens when the invalid class name. It would be great if we can add a negative test case.",
        "createdAt" : "2020-10-28T23:53:24Z",
        "updatedAt" : "2020-10-29T02:49:20Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "6f7891ee-66b6-44a2-897d-281902ee93d2",
        "parentId" : "7da4e3f9-3d1e-4b7c-aa83-b7026e0658e0",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Okay.",
        "createdAt" : "2020-10-29T00:00:41Z",
        "updatedAt" : "2020-10-29T02:49:20Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "38309fe4-c61e-47ff-86ba-972c57b1fc0b",
        "parentId" : "7da4e3f9-3d1e-4b7c-aa83-b7026e0658e0",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "For invalid codec name, `CompressionCodec.createCodec` will throw an `IllegalArgumentException`.",
        "createdAt" : "2020-10-29T00:13:40Z",
        "updatedAt" : "2020-10-29T02:49:20Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "4dc153db5eeea904e0c2208726cbd41bd514b62f",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +1330,1334 @@      .doc(\"The codec used to compress delta and snapshot files generated by StateStore. \" +\n        \"By default, Spark provides four codecs: lz4, lzf, snappy, and zstd. You can also \" +\n        \"use fully qualified class names to specify the codec. Default codec is lz4.\")\n      .version(\"3.1.0\")\n      .stringConf"
  },
  {
    "id" : "a9a6ef68-f6f2-4120-adf8-22bda7e71492",
    "prId" : 30146,
    "prUrl" : "https://github.com/apache/spark/pull/30146#pullrequestreview-518774942",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "77164161-c191-435d-8780-31d19d4be935",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "`DYNAMIC_FILTER_PRUNING_ENABLED` -> `DYNAMIC_COLUMN_PRUNING_ENABLED`?",
        "createdAt" : "2020-10-27T19:16:54Z",
        "updatedAt" : "2020-10-29T10:11:50Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "afdb7b72-b24f-4b7a-b340-c598e9713979",
        "parentId" : "77164161-c191-435d-8780-31d19d4be935",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Ur, BTW, is this used in this PR?",
        "createdAt" : "2020-10-27T19:18:19Z",
        "updatedAt" : "2020-10-29T10:11:50Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "cdfb653d-dbb1-4730-80a6-3e173a9fbd00",
        "parentId" : "77164161-c191-435d-8780-31d19d4be935",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "Yes. This PR introduces a new configuration: `spark.sql.optimizer.dynamicFilterPruning.enabled`, which is the master switch of Dynamic Filter Pruning, `spark.sql.optimizer.dynamicPartitionPruning.enabled` is the switch of the partition column, `spark.sql.optimizer.dynamicDataPruning.enabled` is the switch of the data column.",
        "createdAt" : "2020-10-28T15:13:37Z",
        "updatedAt" : "2020-10-29T10:11:50Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      }
    ],
    "commit" : "9bdb86005112443c48cb4965bc82406129e23f66",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +247,251 @@\n  val DYNAMIC_FILTER_PRUNING_ENABLED =\n    buildConf(\"spark.sql.optimizer.dynamicFilterPruning.enabled\")\n      .doc(\"When true, we will generate predicate when it's used as join key and has benefit.\")\n      .version(\"3.1.0\")"
  },
  {
    "id" : "81c5bded-f25f-493e-8be6-9cd34b797638",
    "prId" : 30146,
    "prUrl" : "https://github.com/apache/spark/pull/30146#pullrequestreview-518779727",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0e9e6e28-c2fb-43e3-bb50-7ace67f42a14",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "This looks like the real config in this PR.\r\n`DYNAMIC_DATA_PRUNING_ENABLED` -> `DYNAMIC_COLUMN_PRUNING_ENABLED`?",
        "createdAt" : "2020-10-27T19:19:11Z",
        "updatedAt" : "2020-10-29T10:11:50Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "6b624aba-7740-4992-a86f-1d8e4d50175c",
        "parentId" : "0e9e6e28-c2fb-43e3-bb50-7ace67f42a14",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "This config is the switch of is the switch of the data column. May be `DYNAMIC_DATA_COLUMN_PRUNING_ENABLED`?\r\nBut this is inconsistent with `DYNAMIC_PARTITION_PRUNING_ENABLED`.",
        "createdAt" : "2020-10-28T15:18:00Z",
        "updatedAt" : "2020-10-29T10:11:50Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      }
    ],
    "commit" : "9bdb86005112443c48cb4965bc82406129e23f66",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +290,294 @@      .createWithDefault(true)\n\n  val DYNAMIC_DATA_PRUNING_ENABLED =\n    buildConf(\"spark.sql.optimizer.dynamicDataPruning.enabled\")\n      .doc(\"When true, we will generate predicate for data column when it's used as join key \" +"
  },
  {
    "id" : "a53e8534-8193-421d-97e7-64e6007c895f",
    "prId" : 30093,
    "prUrl" : "https://github.com/apache/spark/pull/30093#pullrequestreview-516734999",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "21c10eec-829b-4fd0-b8fd-91ad2c5ca13e",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "We need this config? cc: @cloud-fan ",
        "createdAt" : "2020-10-26T11:28:33Z",
        "updatedAt" : "2020-10-27T17:40:08Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "4d633b8e-b53d-4128-8b35-021f468f94e3",
        "parentId" : "21c10eec-829b-4fd0-b8fd-91ad2c5ca13e",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "If we want to backport this, it's safer to have a config for the new rule, in case it has bugs.",
        "createdAt" : "2020-10-26T12:40:04Z",
        "updatedAt" : "2020-10-27T17:40:08Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "cdc7dbe1f69d2cec912c536d388eb68756107d61",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +1254,1258 @@    .createWithDefault(true)\n\n  val REMOVE_REDUNDANT_SORTS_ENABLED = buildConf(\"spark.sql.execution.removeRedundantSorts\")\n    .internal()\n    .doc(\"Whether to remove redundant physical sort node\")"
  },
  {
    "id" : "839c7dc5-1129-4203-8320-3430664c1b90",
    "prId" : 30093,
    "prUrl" : "https://github.com/apache/spark/pull/30093#pullrequestreview-517041492",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2ceb7a00-9a60-4e9f-9c5b-163717989cd5",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "If we backport this into branch-2.4, `3.1.0` -> `2.4.8`?",
        "createdAt" : "2020-10-26T13:29:03Z",
        "updatedAt" : "2020-10-27T17:40:08Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "2210c2c4-39c5-496f-9961-87d18b7fbc54",
        "parentId" : "2ceb7a00-9a60-4e9f-9c5b-163717989cd5",
        "authorId" : "b3ba992a-312c-46eb-b3c3-8d861d15ac40",
        "body" : "I am not exactly sure if it's better to change it in this PR or to change it when this PR is backported to 2.4.8 (in case the current change does not work in 2.4.8)",
        "createdAt" : "2020-10-26T17:59:50Z",
        "updatedAt" : "2020-10-27T17:40:08Z",
        "lastEditedBy" : "b3ba992a-312c-46eb-b3c3-8d861d15ac40",
        "tags" : [
        ]
      }
    ],
    "commit" : "cdc7dbe1f69d2cec912c536d388eb68756107d61",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +1257,1261 @@    .internal()\n    .doc(\"Whether to remove redundant physical sort node\")\n    .version(\"3.1.0\")\n    .booleanConf\n    .createWithDefault(true)"
  }
]