[
  {
    "id" : "e10a8896-c56e-4a3b-87cf-fb3667862c84",
    "prId" : 33019,
    "prUrl" : "https://github.com/apache/spark/pull/33019#pullrequestreview-689383820",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2d069f44-af46-48e9-8f17-1b29ee85b109",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "what's the difference between this one and `QueryExecutionErrors.failToRecognizePatternInDateTimeFormatterError`?",
        "createdAt" : "2021-06-22T10:30:21Z",
        "updatedAt" : "2021-06-22T10:30:21Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "1c4b67e0-27fb-4f6b-9703-b259e5cfa324",
        "parentId" : "2d069f44-af46-48e9-8f17-1b29ee85b109",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "I just renamed `failToRecognizePatternInDateTimeFormatterError` as `failToRecognizePatternAfterUpgradeError`. WDYT?",
        "createdAt" : "2021-06-22T11:27:51Z",
        "updatedAt" : "2021-06-22T11:27:52Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "ea317a0c5620367921bb80ff6b451dc5fc8cf083",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +190,194 @@  protected def checkInvalidPattern(pattern: String): PartialFunction[Throwable, Nothing] = {\n    case e: IllegalArgumentException =>\n      throw QueryExecutionErrors.failToRecognizePatternError(pattern, e)\n  }\n}"
  },
  {
    "id" : "8d7d31d4-9771-4355-945d-5c6d09a813ed",
    "prId" : 28766,
    "prUrl" : "https://github.com/apache/spark/pull/28766#pullrequestreview-427725353",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "efb9b33c-9d1a-4e6b-9d5f-df034ece1d79",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "can you show an example of this error message? let's see if it looks good.",
        "createdAt" : "2020-06-10T03:05:51Z",
        "updatedAt" : "2020-06-10T14:24:59Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "b073a10c-d5ee-4cc1-a5c3-09c7038b0fc8",
        "parentId" : "efb9b33c-9d1a-4e6b-9d5f-df034ece1d79",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "OK",
        "createdAt" : "2020-06-10T05:41:41Z",
        "updatedAt" : "2020-06-10T14:24:59Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "78c676e1d60a5c9c785b40d3b42ac756b72c54e4",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +47,51 @@      if (actual != expected) {\n        throw new DateTimeException(s\"Conflict found: Field $field $actual differs from\" +\n          s\" $field $expected derived from $candidate\")\n      }\n    }"
  },
  {
    "id" : "c6237fee-3a15-45ba-902a-8b33e70f3100",
    "prId" : 28736,
    "prUrl" : "https://github.com/apache/spark/pull/28736#pullrequestreview-442092401",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cef8ac98-11cb-4ac4-9d8d-f496dd35fbe5",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "@HyukjinKwon Should we combine the JVM stacktrace for SparkUpgradeException in the python side?",
        "createdAt" : "2020-07-02T18:01:19Z",
        "updatedAt" : "2020-07-02T18:01:36Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "be9374bd-048a-48d0-af79-565bf697b17f",
        "parentId" : "cef8ac98-11cb-4ac4-9d8d-f496dd35fbe5",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "+1!",
        "createdAt" : "2020-07-03T01:06:27Z",
        "updatedAt" : "2020-07-03T01:06:27Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "54e70b617a2fdfe898af1b425044725bc4c698af",
    "line" : 46,
    "diffHunk" : "@@ -1,1 +144,148 @@        case _: Throwable => throw e\n      }\n      throw new SparkUpgradeException(\"3.0\", s\"Fail to format it to '$resultCandidate' in the new\" +\n        s\" formatter. You can set ${SQLConf.LEGACY_TIME_PARSER_POLICY.key} to LEGACY to restore\" +\n        \" the behavior before Spark 3.0, or set to CORRECTED and treat it as an invalid\" +"
  },
  {
    "id" : "a11bfcff-f039-46fe-a781-60e9fb8fb658",
    "prId" : 28727,
    "prUrl" : "https://github.com/apache/spark/pull/28727#pullrequestreview-424448063",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b15a34f5-2e7c-48ca-909a-fb17301af047",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Yikes, isn't this going to change the Locale's settings for the whole JVM?",
        "createdAt" : "2020-06-04T12:42:47Z",
        "updatedAt" : "2020-06-04T12:43:00Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "bc661204-8344-49a9-ac75-2e871bff2f04",
        "parentId" : "b15a34f5-2e7c-48ca-909a-fb17301af047",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "Only for those trying to use the original value of `SUNDAY FISRT and 1 mininalDay`. This seems not to be a good way though.",
        "createdAt" : "2020-06-04T12:59:32Z",
        "updatedAt" : "2020-06-04T12:59:32Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "7111cf84-69a5-4e2e-a803-e1a190aaeefe",
        "parentId" : "b15a34f5-2e7c-48ca-909a-fb17301af047",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "yea, we shouldn't change a global cache, which is shared in the entire JVM, including user code.",
        "createdAt" : "2020-06-04T13:31:58Z",
        "updatedAt" : "2020-06-04T13:31:58Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "590b40af70bebfe9176d37d6ebd0ad71ecbff9e7",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +162,166 @@\n  def presetSundayStartToMondayStart(): Unit = {\n    val CACHE: Field = classOf[WeekFields].getDeclaredField(\"CACHE\")\n    CACHE.setAccessible(true)\n    val modifiers: Field = CACHE.getClass.getDeclaredField(\"modifiers\")"
  },
  {
    "id" : "648d83f2-349b-425b-a0c3-cf3e2674f697",
    "prId" : 28674,
    "prUrl" : "https://github.com/apache/spark/pull/28674#pullrequestreview-421932813",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "998026d4-da3b-4232-bb11-f634299592b5",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "should this happen when we create the formatter?",
        "createdAt" : "2020-06-01T15:17:37Z",
        "updatedAt" : "2020-06-01T15:17:37Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "0dcd5dffb8c7b74d3570c67aae2639332cebb687",
    "line" : 52,
    "diffHunk" : "@@ -1,1 +61,65 @@    if (weekBased && mayNonWeekBased(accessor)) {\n      throw new DateTimeException(\n        s\"Can not mix week-based and non-week-based date fields together for parsing dates\")\n    } else if (weekBased) {\n"
  },
  {
    "id" : "e35e38ca-03aa-4c7b-8fb6-a5eb5c88879c",
    "prId" : 28673,
    "prUrl" : "https://github.com/apache/spark/pull/28673#pullrequestreview-421406616",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "360d1642-de4b-44e4-a371-058b717c9791",
        "parentId" : null,
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "Shall we use Nonfatal here?",
        "createdAt" : "2020-05-29T08:28:59Z",
        "updatedAt" : "2020-05-30T03:56:59Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "bab2a7cb-ca56-4bfb-b719-2319ffd4818e",
        "parentId" : "360d1642-de4b-44e4-a371-058b717c9791",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "This seems like a Java bug to me to throw `ArrayIndexOutOfBoundsException`.\r\n\r\nShall we fail the formatter creation when there are more than 10 `y` in the pattern string? And document this limitation as well.",
        "createdAt" : "2020-05-30T06:54:01Z",
        "updatedAt" : "2020-05-30T06:54:02Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "981a2db0-3b62-473f-a507-8d3a9c01162c",
        "parentId" : "360d1642-de4b-44e4-a371-058b717c9791",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "okay. Do we still need to check the runtime exceptions or `DateTimeException `  for the formatting phase?",
        "createdAt" : "2020-05-30T13:50:29Z",
        "updatedAt" : "2020-05-30T13:54:47Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "a5da8c3db37e19427c8729b645c354bce707bb28",
    "line" : 54,
    "diffHunk" : "@@ -1,1 +144,148 @@   */\n  private def needConvertToSparkUpgradeException(e: Throwable): Boolean = e match {\n    case _: DateTimeException | _: ArrayIndexOutOfBoundsException\n      if SQLConf.get.legacyTimeParserPolicy == EXCEPTION => true\n    case _ => false"
  },
  {
    "id" : "8304594b-468a-45cb-89df-966b5d6db888",
    "prId" : 28673,
    "prUrl" : "https://github.com/apache/spark/pull/28673#pullrequestreview-421375401",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9f916533-2288-4b2a-81bd-42592e5e2009",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "The parser case can get`ArrayIndexOutOfBoundsException`, too?",
        "createdAt" : "2020-05-30T03:06:49Z",
        "updatedAt" : "2020-05-30T03:56:59Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "5f4d93e2-eaf8-4c0d-be7b-5ff70efcc2f2",
        "parentId" : "9f916533-2288-4b2a-81bd-42592e5e2009",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "There could be such a risk for the parser to get it after roughly checked the JCL.",
        "createdAt" : "2020-05-30T03:28:14Z",
        "updatedAt" : "2020-05-30T03:56:59Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "32e5b578-bbec-4c55-aff5-31dfab7b8771",
        "parentId" : "9f916533-2288-4b2a-81bd-42592e5e2009",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "Maybe RuntimeException is more appropriate",
        "createdAt" : "2020-05-30T03:57:38Z",
        "updatedAt" : "2020-05-30T03:57:38Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "a5da8c3db37e19427c8729b645c354bce707bb28",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +108,112 @@  protected def checkDiffParserResult[T](\n      s: String, legacyParseFunc: String => T): PartialFunction[Throwable, T] = {\n    case e if needConvertToSparkUpgradeException(e) =>\n      try {\n        legacyParseFunc(s)"
  },
  {
    "id" : "109cdf6c-80e3-4bcb-847e-34cfd46621a7",
    "prId" : 28673,
    "prUrl" : "https://github.com/apache/spark/pull/28673#pullrequestreview-421372589",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f5fff2c2-ab7c-4d93-bce4-fe013a65dcf3",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Could you leave some comments about what's a condition for `ArrayIndexOutOfBoundsException` to be thrown?",
        "createdAt" : "2020-05-30T03:14:53Z",
        "updatedAt" : "2020-05-30T03:56:59Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "a5da8c3db37e19427c8729b645c354bce707bb28",
    "line" : 55,
    "diffHunk" : "@@ -1,1 +145,149 @@  private def needConvertToSparkUpgradeException(e: Throwable): Boolean = e match {\n    case _: DateTimeException | _: ArrayIndexOutOfBoundsException\n      if SQLConf.get.legacyTimeParserPolicy == EXCEPTION => true\n    case _ => false\n  }"
  },
  {
    "id" : "d24f7ec7-3f57-4677-b53d-7224ba4212a3",
    "prId" : 28646,
    "prUrl" : "https://github.com/apache/spark/pull/28646#pullrequestreview-419088383",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0d9993f6-b08c-4dd5-ad87-0532c7e6c616",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "You checked only 'LLL' pattern in `bugInStandAloneForm()` but throws exception for 'qqq' as well. Are you sure they are directly related?",
        "createdAt" : "2020-05-26T20:51:33Z",
        "updatedAt" : "2020-05-27T12:43:10Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "d9d72fc1-6516-45d8-b316-6e2cc7e2b52a",
        "parentId" : "0d9993f6-b08c-4dd5-ad87-0532c7e6c616",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "In the java doc:\r\n```\r\nPattern letters 'L', 'c', and 'q' specify the stand-alone form of the text styles.\r\n```\r\n\r\nI think they are directly related. And I tested `q` locally as well. `c` is already forbidden in 3.0.",
        "createdAt" : "2020-05-27T07:02:59Z",
        "updatedAt" : "2020-05-27T12:43:10Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "b2e5b4f9-2cf6-4b2a-bb9d-e55ec3f88bbd",
        "parentId" : "0d9993f6-b08c-4dd5-ad87-0532c7e6c616",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "ok. I have double checked the pattern 'qqq'. It has the same problem\r\n\r\nJDK 11:\r\n```sql\r\nspark-sql> select to_csv(named_struct('date', date '1970-01-01'), map('dateFormat', 'qqq', 'locale', 'RU'));\r\n1-Ð¹ ÐºÐ².\r\nspark-sql> select to_csv(named_struct('date', date '1970-01-01'), map('dateFormat', 'qqq', 'locale', 'EN'));\r\nQ1\r\n```\r\n\r\nJDK 8\r\n```sql\r\nspark-sql> select to_csv(named_struct('date', date '1970-01-01'), map('dateFormat', 'qqq', 'locale', 'RU'));\r\n1\r\nspark-sql> select to_csv(named_struct('date', date '1970-01-01'), map('dateFormat', 'qqq', 'locale', 'EN'));\r\n1\r\n```",
        "createdAt" : "2020-05-27T11:10:42Z",
        "updatedAt" : "2020-05-27T12:43:10Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "b7cbc8bea2064449174fc2e2c9664e97ee12da20",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +254,258 @@            throw new IllegalArgumentException(s\"Too many pattern letters: ${style.head}\")\n          }\n          if (bugInStandAloneForm && (patternPart.contains(\"LLL\") || patternPart.contains(\"qqq\"))) {\n            throw new IllegalArgumentException(\"Java 8 has a bug to support stand-alone \" +\n              \"form (3 or more 'L' or 'q' in the pattern string). Please use 'M' or 'Q' instead, \" +"
  },
  {
    "id" : "d547347f-424e-4764-907c-cd528677952e",
    "prId" : 28576,
    "prUrl" : "https://github.com/apache/spark/pull/28576#pullrequestreview-414131191",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5618664e-0ccb-4cf1-8e67-0e266f64104f",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "when the field value exceeds the valid range, the JDK throws `DateTimeException`, and we should catch it too. `DateTimeParseException` extends `DateTimeException`",
        "createdAt" : "2020-05-19T06:18:41Z",
        "updatedAt" : "2020-05-21T12:48:25Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "161223b5de92064a4cc3672702e09f868bd68870",
    "line" : 78,
    "diffHunk" : "@@ -1,1 +108,112 @@  protected def checkDiffResult[T](\n      s: String, legacyParseFunc: String => T): PartialFunction[Throwable, T] = {\n    case e: DateTimeException if SQLConf.get.legacyTimeParserPolicy == EXCEPTION =>\n      try {\n        legacyParseFunc(s)"
  },
  {
    "id" : "93b66731-667c-46c9-9744-ebd5b6c986a9",
    "prId" : 28576,
    "prUrl" : "https://github.com/apache/spark/pull/28576#pullrequestreview-415455689",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "faf6a007-84c2-46e2-9f05-c4e41329d4f9",
        "parentId" : null,
        "authorId" : "a80e0991-f7b1-415a-8d2e-ab615cc6ef4a",
        "body" : "Are we also going to error out if they specify the day but not the month? Really, the only formats that make sense are the ones where a full prefix is given in the y-m-d h-m-s sequence, and all others are likely to be a case where they made a mistake (e.g. asked for \"mm\" twice where they meant MM).",
        "createdAt" : "2020-05-19T15:41:08Z",
        "updatedAt" : "2020-05-21T12:48:25Z",
        "lastEditedBy" : "a80e0991-f7b1-415a-8d2e-ab615cc6ef4a",
        "tags" : [
        ]
      },
      {
        "id" : "c3c1f5aa-a3e9-44a0-8b57-4c7739c63178",
        "parentId" : "faf6a007-84c2-46e2-9f05-c4e41329d4f9",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "January should be fine? It accepts any DAY_OF_MONTH.",
        "createdAt" : "2020-05-20T03:53:22Z",
        "updatedAt" : "2020-05-21T12:48:25Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "22b60ebb-5b92-4b27-9385-381ab53ebd5d",
        "parentId" : "faf6a007-84c2-46e2-9f05-c4e41329d4f9",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "> asked for \"mm\" twice where they meant MM\r\n\r\nThis will error out unless the 2 mm parse to the same value, e.g. `mm mm yyyy` with value `10 10 1990`.",
        "createdAt" : "2020-05-20T03:54:26Z",
        "updatedAt" : "2020-05-21T12:48:25Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "2cd7dfd4-d149-4844-a606-9c3e9d6f65f1",
        "parentId" : "faf6a007-84c2-46e2-9f05-c4e41329d4f9",
        "authorId" : "a80e0991-f7b1-415a-8d2e-ab615cc6ef4a",
        "body" : "I'm saying it works (i.e., it accepts all valid values) but it's nonsensical; it'll never return the complete timestamp that people intended.\r\n\r\nThat said, I can see cases where people want to parse contiguous ranges (e.g. just a time, or just a month, or just a month + day) and then extract the parsed components using `EXTRACT` / `date_part`, or just to format them differently.\r\n\r\nSo I guess the most permissive but still safe thing here would be to forbid missing \"year\" only if someone also requests \"day\" and \"month\", because that's the situation where \"year\" affects the valid range of \"day\". But if they don't request year *and* they don't request month, then month would default to 1 and there would be no impact on the valid range of days, it would always go up to 32. I.e.:\r\n\r\n`\"yyyy MM dd\"` -> OK\r\n`\"MM dd\"` -> fail, need year to know valid range of `dd`\r\n`\"MM\"` -> OK\r\n`\"dd\"` -> OK\r\n`\"hh:mm:ss\"` -> OK\r\n`\"MM dd hh:mm:ss\"` -> fail, need year to know valid range of `dd`",
        "createdAt" : "2020-05-20T08:37:06Z",
        "updatedAt" : "2020-05-21T12:48:25Z",
        "lastEditedBy" : "a80e0991-f7b1-415a-8d2e-ab615cc6ef4a",
        "tags" : [
        ]
      },
      {
        "id" : "e4133b56-de4a-48f8-9d74-e75b27a3ad14",
        "parentId" : "faf6a007-84c2-46e2-9f05-c4e41329d4f9",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "ah this looks better!\r\n\r\nI'm wondering if we still need the legacy config though. It's very hard to hit the missing year exception, and if it happens it's very likely to be a data error. Users should fix the data or change the pattern and provide the default year.",
        "createdAt" : "2020-05-20T08:53:23Z",
        "updatedAt" : "2020-05-21T12:48:25Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "e3cbdd56-d248-4392-844e-d7fd93b331a0",
        "parentId" : "faf6a007-84c2-46e2-9f05-c4e41329d4f9",
        "authorId" : "a80e0991-f7b1-415a-8d2e-ab615cc6ef4a",
        "body" : "I guess we could even allow the default year, and if people hit Feb 29 then they'll get an error at runtime. No need for a legacy config in that case...",
        "createdAt" : "2020-05-20T08:57:58Z",
        "updatedAt" : "2020-05-21T12:48:25Z",
        "lastEditedBy" : "a80e0991-f7b1-415a-8d2e-ab615cc6ef4a",
        "tags" : [
        ]
      },
      {
        "id" : "c89a9007-b945-46fd-8a39-c0a9e8f11806",
        "parentId" : "faf6a007-84c2-46e2-9f05-c4e41329d4f9",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "ok so we should go back to the first version, which just uses 1970 as the default value and fails for invalid dates at runtime ðŸ˜‚",
        "createdAt" : "2020-05-20T09:28:32Z",
        "updatedAt" : "2020-05-21T12:48:25Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "2ac692e2-4739-4b3d-88da-9aefb375c234",
        "parentId" : "faf6a007-84c2-46e2-9f05-c4e41329d4f9",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "BTW the current framework just returns null if the parser fails. Shall we treat this case specially and throw an error that fails the query?",
        "createdAt" : "2020-05-20T09:29:55Z",
        "updatedAt" : "2020-05-21T12:48:25Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "ff5c78fe-5db0-43a3-bd1e-1a3763d705bf",
        "parentId" : "faf6a007-84c2-46e2-9f05-c4e41329d4f9",
        "authorId" : "a80e0991-f7b1-415a-8d2e-ab615cc6ef4a",
        "body" : "Error out in ANSI mode only, I'd say, just like with all other parse failures (which should fail in ANSI mode -- I hope they already do).\r\n\r\nFWIW, going back to the first version is not a shame now that we know it's the right thing to do. :) Maybe we can leave some comments in the code to explain the reasoning why we don't error out if things are missing.",
        "createdAt" : "2020-05-20T10:04:47Z",
        "updatedAt" : "2020-05-21T12:48:25Z",
        "lastEditedBy" : "a80e0991-f7b1-415a-8d2e-ab615cc6ef4a",
        "tags" : [
        ]
      },
      {
        "id" : "365bdb88-b0c2-4b9e-a2f6-e672dceda54b",
        "parentId" : "faf6a007-84c2-46e2-9f05-c4e41329d4f9",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "unfortunately datetime functions don't respect the ANSI flag at all. Maybe we should keep everything the same as Spark 2.4 (return null when parsing Feb 29 because 1970-02-29 is invalid), and implement ANSI behavior in Spark 3.1?",
        "createdAt" : "2020-05-20T12:47:46Z",
        "updatedAt" : "2020-05-21T12:48:25Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "bfd71575-3dd5-4eea-b8be-936002a12339",
        "parentId" : "faf6a007-84c2-46e2-9f05-c4e41329d4f9",
        "authorId" : "a80e0991-f7b1-415a-8d2e-ab615cc6ef4a",
        "body" : "Yikes! It seems terrible that we have to do that later, because then it can break existing workloads. Launching the ANSI behavior was our chance to do the right thing here. If we can still fix this in 3.0 then that would of course be preferable, but that seems like a stretch...",
        "createdAt" : "2020-05-20T15:27:45Z",
        "updatedAt" : "2020-05-21T12:48:25Z",
        "lastEditedBy" : "a80e0991-f7b1-415a-8d2e-ab615cc6ef4a",
        "tags" : [
        ]
      }
    ],
    "commit" : "161223b5de92064a4cc3672702e09f868bd68870",
    "line" : 30,
    "diffHunk" : "@@ -1,1 +49,53 @@    // To be compatible with Spark 2.4, we pick 1970 as the default value of year.\n    val year = getOrDefault(accessor, ChronoField.YEAR, 1970)\n    val month = getOrDefault(accessor, ChronoField.MONTH_OF_YEAR, 1)\n    val day = getOrDefault(accessor, ChronoField.DAY_OF_MONTH, 1)\n    LocalDate.of(year, month, day)"
  },
  {
    "id" : "d6fb7665-1ac1-4a8e-a1d9-76b848edd807",
    "prId" : 28576,
    "prUrl" : "https://github.com/apache/spark/pull/28576#pullrequestreview-416689519",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9d8fbd0a-ee1d-41de-972b-de3298824786",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "nit: I would follow the naming of `getOrElse` though ..",
        "createdAt" : "2020-05-22T07:00:19Z",
        "updatedAt" : "2020-05-22T07:09:39Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "161223b5de92064a4cc3672702e09f868bd68870",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +32,36 @@\ntrait DateTimeFormatterHelper {\n  private def getOrDefault(accessor: TemporalAccessor, field: ChronoField, default: Int): Int = {\n    if (accessor.isSupported(field)) {\n      accessor.get(field)"
  },
  {
    "id" : "22c68736-e4fc-4a9e-8424-4a34a5a3d47f",
    "prId" : 27949,
    "prUrl" : "https://github.com/apache/spark/pull/27949#pullrequestreview-376881589",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d858a528-f4ee-4467-95c6-b1089e16f1e3",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "can we add some comment to explain why `idx != 0`?",
        "createdAt" : "2020-03-18T13:16:54Z",
        "updatedAt" : "2020-03-18T13:51:38Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "777e828a-80a6-4f2f-be4c-3352b5780da2",
        "parentId" : "d858a528-f4ee-4467-95c6-b1089e16f1e3",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "comment added, thanks.",
        "createdAt" : "2020-03-18T13:51:42Z",
        "updatedAt" : "2020-03-18T13:52:38Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "3e2b2c206bc2eb60a5e65bad64b92f49121a877c",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +121,125 @@      // string at res(0). So when the first element here is empty string we do not need append `'`\n      // literal to the DateTimeFormatterBuilder.\n      case (\"\", idx) if idx != 0 => builder.appendLiteral(\"'\")\n      case (pattenPart, idx) if idx % 2 == 0 =>\n        var rest = pattenPart"
  },
  {
    "id" : "b5a39e3b-0333-4162-8688-7a89bec2d04a",
    "prId" : 27906,
    "prUrl" : "https://github.com/apache/spark/pull/27906#pullrequestreview-375784237",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1fe88d7f-d6db-400d-ada2-420ac3c34be4",
        "parentId" : null,
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "How about move this method into `DateTimeUtils`, together with `convertIncompatiblePattern`?\r\nI think we need to collect all these kind of util functions analyzing pattern string together.",
        "createdAt" : "2020-03-17T06:32:59Z",
        "updatedAt" : "2020-03-17T07:11:18Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      },
      {
        "id" : "a856747b-2875-40ba-8c25-fd2c41bad24e",
        "parentId" : "1fe88d7f-d6db-400d-ada2-420ac3c34be4",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "This is very specific to datetime parsing. How about we move `convertIncompatiblePattern` to `object DateTimeFormatterHelper`?",
        "createdAt" : "2020-03-17T06:43:43Z",
        "updatedAt" : "2020-03-17T07:11:18Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "bb22832e-626f-42e2-b173-249af06d4103",
        "parentId" : "1fe88d7f-d6db-400d-ada2-420ac3c34be4",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "Then, I will not do this refactoring in this PR, not related",
        "createdAt" : "2020-03-17T07:12:19Z",
        "updatedAt" : "2020-03-17T07:12:19Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "ec4408341fc8589becdc67e8fa7231cd07ec4d7f",
    "line" : 40,
    "diffHunk" : "@@ -1,1 +114,118 @@   * Building a formatter for parsing seconds fraction with variable length\n   */\n  def createBuilderWithVarLengthSecondFraction(\n      pattern: String): DateTimeFormatterBuilder = {\n    val builder = createBuilder()"
  }
]