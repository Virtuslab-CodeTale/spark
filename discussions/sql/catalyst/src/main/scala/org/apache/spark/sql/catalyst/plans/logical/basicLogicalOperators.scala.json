[
  {
    "id" : "93448fda-b31d-4007-b5bc-ac6340132a02",
    "prId" : 33099,
    "prUrl" : "https://github.com/apache/spark/pull/33099#pullrequestreview-693417704",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "11aa06d2-4757-41b7-884f-49c8a3e24ce8",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "This looks like a nice refactoring in order to make consistent and simplify.",
        "createdAt" : "2021-06-27T04:19:03Z",
        "updatedAt" : "2021-06-27T04:19:03Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "ba266562-8e34-4465-81fd-49cfbcb911e8",
        "parentId" : "11aa06d2-4757-41b7-884f-49c8a3e24ce8",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "sorry for the late, I prefer not to do this. Since `RepartitionOperation` can affect the behavior of AQE, the semantics of `RepartitionOperation` have a little different from previous.\r\n\r\nCurrently, the `RepartitionOperation` behavior with AQE framwork follows:\r\n* `Repartition` does not use AQE stage optimizer rules\r\n* `RepartitionByExpression` depends on `optNumPartitions` and `partitionExpressions`\r\n*  `RebalancePartitions`(if extend)  depends on `partitionExpressions`\r\n\r\nAnd the rule `CollapseRepartition` may not effective in all use case.",
        "createdAt" : "2021-06-27T13:32:39Z",
        "updatedAt" : "2021-06-27T14:15:03Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      }
    ],
    "commit" : "67642d87f7d1799e97f925444484f52354e6af72",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +1374,1378 @@case class RebalancePartitions(\n    partitionExpressions: Seq[Expression],\n    child: LogicalPlan) extends RepartitionOperation {\n\n  override def numPartitions: Int = conf.numShufflePartitions"
  },
  {
    "id" : "0b5071e5-92b6-4a49-aa1d-74522d1e4df1",
    "prId" : 32932,
    "prUrl" : "https://github.com/apache/spark/pull/32932#pullrequestreview-693412190",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "599094f1-e305-4fe3-97fa-18b7f1e1b655",
        "parentId" : null,
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "Make `RebalancePartitions` extends `RepartitionOperation`?",
        "createdAt" : "2021-06-26T01:47:40Z",
        "updatedAt" : "2021-06-26T01:47:40Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      },
      {
        "id" : "11dc3ee2-0672-447e-a290-2baf03b5926d",
        "parentId" : "599094f1-e305-4fe3-97fa-18b7f1e1b655",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "Since it's a special one. For conservative, this PR does not extend `RepartitionOperation`, see the comment  https://github.com/apache/spark/pull/32932#discussion_r656249153.",
        "createdAt" : "2021-06-27T12:34:15Z",
        "updatedAt" : "2021-06-27T12:34:15Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      }
    ],
    "commit" : "0fe14d0001adedef66696699d478dd4d4e9c392c",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +1363,1367 @@case class RebalancePartitions(\n    partitionExpressions: Seq[Expression],\n    child: LogicalPlan) extends UnaryNode {\n  override def maxRows: Option[Long] = child.maxRows\n  override def output: Seq[Attribute] = child.output"
  },
  {
    "id" : "fa54bb2b-19df-48f6-a0b1-d42ef7aa6051",
    "prId" : 32498,
    "prUrl" : "https://github.com/apache/spark/pull/32498#pullrequestreview-658585331",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9e111b49-ffe9-48d4-b24e-ea94c08018a5",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "```\r\n  private def getRangeValue(index: Int): Long = {\r\n    assert(index >= 0, \"index must be greater than and equal to 0\")\r\n    if (step == 0) {\r\n      start + (numElements.toLong - index - 1) * step\r\n    } else {\r\n      start + index * step\r\n    }\r\n  }\r\n```\r\n?",
        "createdAt" : "2021-05-13T04:11:54Z",
        "updatedAt" : "2021-05-13T04:11:54Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "03c04694-9d5b-4e3b-81eb-e70db22fe6c0",
        "parentId" : "9e111b49-ffe9-48d4-b24e-ea94c08018a5",
        "authorId" : "d0d56018-475d-4da3-93a6-5e843402b7f2",
        "body" : "Done",
        "createdAt" : "2021-05-13T05:02:42Z",
        "updatedAt" : "2021-05-13T05:02:43Z",
        "lastEditedBy" : "d0d56018-475d-4da3-93a6-5e843402b7f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "bb69f84a3d05acec239db571afbfe2f41007d9ce",
    "line" : 50,
    "diffHunk" : "@@ -1,1 +821,825 @@\n  // Utility method to compute histogram\n  private def getRangeValue(index: Int): Long = {\n    assert(index >= 0, \"index must be greater than and equal to 0\")\n    if (step < 0) {"
  },
  {
    "id" : "fe00874a-5504-42de-a6be-940c23bba6a7",
    "prId" : 32498,
    "prUrl" : "https://github.com/apache/spark/pull/32498#pullrequestreview-661462423",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b5a57242-1d23-47f7-a54c-9d9d4d746d7f",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "EDIT: Fixed my formula\r\nI'm probably missing something, but isn't this more like:\r\n```\r\nstart + (numElements.toLong - 1) * (-step) + index * step\r\n```\r\nThis tries to step backwards from the end, right? `step` is negative.\r\n\r\nAlso I assume step = 0 is an error actually, maybe it's caught earlier, but the result should be `start` always in that case anyway, so `step < 0` as a condition?",
        "createdAt" : "2021-05-17T14:26:36Z",
        "updatedAt" : "2021-05-17T14:37:16Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "1a12600e-5704-4a07-83ed-b42342792790",
        "parentId" : "b5a57242-1d23-47f7-a54c-9d9d4d746d7f",
        "authorId" : "d0d56018-475d-4da3-93a6-5e843402b7f2",
        "body" : ">This tries to step backwards from the end, right? step is negative\r\n\r\nYes, we reverse the range values if the step is negative to compute histogram statistics.\r\n\r\n>I'm probably missing something, but isn't this more like:\r\n    start + (numElements.toLong - 1) * (-step) + index * step\r\n\r\nI am not sure this gives the right range values if the step is negative. \r\nFor eg: start = 1 , step = -2, and numElements = 4, Range values  = [1, -1, -3, -5]\r\nBut for computing histogram we need values like this [-5, -3, -1, 1]. So if the index = 0, it should return -5.\r\n\r\nWith the formula in the PR => 1 + (4 - 0 -1 )* -2= -5\r\n With the formula you have given above  => 1 + (4 -1) *-(-2) + 0 = 7\r\n\r\nSo I think the formula in the PR seems correct?\r\n\r\n",
        "createdAt" : "2021-05-17T20:42:18Z",
        "updatedAt" : "2021-05-17T20:42:18Z",
        "lastEditedBy" : "d0d56018-475d-4da3-93a6-5e843402b7f2",
        "tags" : [
        ]
      },
      {
        "id" : "ebd5f341-2122-47a9-95e0-4c35f73eccab",
        "parentId" : "b5a57242-1d23-47f7-a54c-9d9d4d746d7f",
        "authorId" : "d0d56018-475d-4da3-93a6-5e843402b7f2",
        "body" : ">Also I assume step = 0 is an error actually, maybe it's caught earlier, but the result should be start always in that case anyway, so step < 0 as a condition?\r\n\r\nYes, if step=0 both the conditions will return same start element. I have updated the condition to step < 0.",
        "createdAt" : "2021-05-17T20:44:47Z",
        "updatedAt" : "2021-05-17T20:44:47Z",
        "lastEditedBy" : "d0d56018-475d-4da3-93a6-5e843402b7f2",
        "tags" : [
        ]
      },
      {
        "id" : "4eb48290-7ba0-4a31-812d-3a4c54656a70",
        "parentId" : "b5a57242-1d23-47f7-a54c-9d9d4d746d7f",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "OK, I thought it might mean the desired result was [7, 5, 3, 1] in this case. You step back from the start. But it has to be reversed? OK if it needs to be [-5, -3, -1, 1] yeah that looks right. A comment to this effect might be helpful.",
        "createdAt" : "2021-05-17T21:20:11Z",
        "updatedAt" : "2021-05-17T21:20:11Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "abcf5ed9-7925-4ddc-8828-1f4a6d3d4083",
        "parentId" : "b5a57242-1d23-47f7-a54c-9d9d4d746d7f",
        "authorId" : "d0d56018-475d-4da3-93a6-5e843402b7f2",
        "body" : "Yes. I added a comment.",
        "createdAt" : "2021-05-17T22:05:29Z",
        "updatedAt" : "2021-05-17T22:16:05Z",
        "lastEditedBy" : "d0d56018-475d-4da3-93a6-5e843402b7f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "bb69f84a3d05acec239db571afbfe2f41007d9ce",
    "line" : 54,
    "diffHunk" : "@@ -1,1 +825,829 @@    if (step < 0) {\n      // Reverse the range values for computing histogram, if the step size is negative.\n      start + (numElements.toLong - index - 1) * step\n    } else {\n      start + index * step"
  },
  {
    "id" : "eada1773-1e1e-4e58-89df-f9ada07770d2",
    "prId" : 32350,
    "prUrl" : "https://github.com/apache/spark/pull/32350#pullrequestreview-647776353",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2319c3ed-d30b-4808-b81d-696d2266bb1b",
        "parentId" : null,
        "authorId" : "685f805b-e4fa-4f21-b066-58afcecf9ce6",
        "body" : "this override is needed for the added test",
        "createdAt" : "2021-04-29T02:29:50Z",
        "updatedAt" : "2021-05-06T06:35:40Z",
        "lastEditedBy" : "685f805b-e4fa-4f21-b066-58afcecf9ce6",
        "tags" : [
        ]
      }
    ],
    "commit" : "a3e26c79de9f46aeee82dc4e5bc11cfe2ab69668",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +70,74 @@  override def output: Seq[Attribute] = projectList.map(_.toAttribute)\n  override def maxRows: Option[Long] = child.maxRows\n  override def maxRowsPerPartition: Option[Long] = child.maxRowsPerPartition\n\n  override lazy val resolved: Boolean = {"
  },
  {
    "id" : "1f3a50ec-df93-4968-bcf7-44c9c1ec6038",
    "prId" : 31913,
    "prUrl" : "https://github.com/apache/spark/pull/31913#pullrequestreview-635309221",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "51267cd6-bd39-4208-956e-73a4bd304bc5",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I agree that it's not necessary to replace `Attribute` with `GroupingExprRef`, but it makes the framework more consistent if we always use `GroupingExprRef`. We can make `GroupingExprRef` a named expression to fix issues.\r\n\r\nThis is just my thought though, I'm happy to hear more opinions.",
        "createdAt" : "2021-04-13T15:37:02Z",
        "updatedAt" : "2021-04-17T16:23:39Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "d2ba6a43-06e4-4696-bfd5-c97467bb0d66",
        "parentId" : "51267cd6-bd39-4208-956e-73a4bd304bc5",
        "authorId" : "3d4870da-39a8-4406-b00d-131930d14cd8",
        "body" : "I also see the pros of making this framework more general, and I'm happy to do it, but it is not required for this bugfix. How about doing it in a follow-up ticket?",
        "createdAt" : "2021-04-13T18:35:36Z",
        "updatedAt" : "2021-04-17T16:23:39Z",
        "lastEditedBy" : "3d4870da-39a8-4406-b00d-131930d14cd8",
        "tags" : [
        ]
      },
      {
        "id" : "a5d99cdc-866d-444f-8b70-53e5d4a56bbf",
        "parentId" : "51267cd6-bd39-4208-956e-73a4bd304bc5",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "SGTM",
        "createdAt" : "2021-04-14T07:45:25Z",
        "updatedAt" : "2021-04-17T16:23:39Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "fb3a19dad5ac4448b8fe9d1b12ed1c3f6a0369a7",
    "line" : 103,
    "diffHunk" : "@@ -1,1 +883,887 @@    val complexGroupingExpressions = collectComplexGroupingExpressions(groupingExpressions)\n    val aggrExprWithGroupingReferences = if (complexGroupingExpressions.nonEmpty) {\n      insertGroupingReferences(aggregateExpressions, complexGroupingExpressions)\n    } else {\n      aggregateExpressions"
  },
  {
    "id" : "5aa026df-782e-4d05-9465-a671fc0f6b32",
    "prId" : 31908,
    "prUrl" : "https://github.com/apache/spark/pull/31908#pullrequestreview-617096013",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "343c0042-5baa-4062-8369-69b096bd87dc",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Ur, is this description correct? Technically, `semanticEquals` returns `false` for in-deterministic expressions. So, it seems that we miss some `Distinct` operators theoretically. It's not a problem for this optimizer's functionality, but maybe could you revise this description a little?\r\n```scala\r\n  def semanticEquals(other: Expression): Boolean =\r\n    deterministic && other.deterministic && canonicalized == other.canonicalized\r\n```",
        "createdAt" : "2021-03-22T01:35:09Z",
        "updatedAt" : "2021-04-22T02:59:22Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "aae4efea461af0da9f6ddfec4fb7a073ce191a67",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +846,850 @@    copy(child = newChild)\n\n  // Whether this Aggregate operator is equally the Distinct operator.\n  private[sql] def isDistinct: Boolean = {\n    aggregateExpressions.forall(a => groupingExpressions.exists(g => a.semanticEquals(g)))"
  },
  {
    "id" : "4ab3150b-e553-4c52-b8c1-3f50e5e9cfb5",
    "prId" : 31897,
    "prUrl" : "https://github.com/apache/spark/pull/31897#pullrequestreview-616616940",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "da6bc2e9-18d0-42c1-a0e2-822e2bf53ae9",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "BTW, isn't this a bug of `Window` class implementation itself, @viirya ?",
        "createdAt" : "2021-03-19T16:39:17Z",
        "updatedAt" : "2021-03-19T16:39:17Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "f84b6371-8d5a-4526-877f-75437d62aa59",
        "parentId" : "da6bc2e9-18d0-42c1-a0e2-822e2bf53ae9",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "I'm wondering if we can have this line for all the other applicable branches.",
        "createdAt" : "2021-03-19T16:44:34Z",
        "updatedAt" : "2021-03-19T16:44:34Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "2ee865df-4d37-4208-8d04-f50b8e6e1a5c",
        "parentId" : "da6bc2e9-18d0-42c1-a0e2-822e2bf53ae9",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Yea, I think so, although I changed it here for nested column pruning.\r\n\r\nCurrently it seems not causing other problem, although it is wrong. Looks good to have it in other branches (3.0/2.4) too.\r\n",
        "createdAt" : "2021-03-19T17:02:15Z",
        "updatedAt" : "2021-03-19T17:02:15Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "7eab1ace51a5174f347eec239279b5e7b3ed98dd",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +671,675 @@    child.output ++ windowExpressions.map(_.toAttribute)\n\n  override def producedAttributes: AttributeSet = windowOutputSet\n\n  def windowOutputSet: AttributeSet = AttributeSet(windowExpressions.map(_.toAttribute))"
  },
  {
    "id" : "73463f9c-c644-4207-8ef8-827f449bc5ac",
    "prId" : 31791,
    "prUrl" : "https://github.com/apache/spark/pull/31791#pullrequestreview-629459499",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ea7dbc79-b919-462e-b03d-9459b037f94a",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "What was the behavior before this PR? Only allow integral literals?",
        "createdAt" : "2021-04-06T09:13:43Z",
        "updatedAt" : "2021-04-06T22:56:35Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "16ac2418-c18c-4e12-8efe-8eceef1ce21e",
        "parentId" : "ea7dbc79-b919-462e-b03d-9459b037f94a",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Can you point me to the original code that does implicit cast for range?",
        "createdAt" : "2021-04-06T09:15:54Z",
        "updatedAt" : "2021-04-06T22:56:35Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "5f5b9e2f-378e-41b8-b26b-baca9500688b",
        "parentId" : "ea7dbc79-b919-462e-b03d-9459b037f94a",
        "authorId" : "b3ba992a-312c-46eb-b3c3-8d861d15ac40",
        "body" : "The original behavior is to implicit cast the given value into the expected type:\r\nhttps://github.com/apache/spark/blob/390d5bde81ce282a016c6cf36fb0d57012515388/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveTableValuedFunctions.scala#L40-L50\r\n\r\nWhere the expected types are hardcoded, for example `\"end\" -> LongType`\r\nhttps://github.com/apache/spark/blob/390d5bde81ce282a016c6cf36fb0d57012515388/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveTableValuedFunctions.scala#L86-L88\r\n\r\nIt uses `TypeCoercion.implicitCast(value, expectedType)`. So here `castAndEval` is trying to follow the previous behavior.",
        "createdAt" : "2021-04-06T22:31:37Z",
        "updatedAt" : "2021-04-06T22:56:35Z",
        "lastEditedBy" : "b3ba992a-312c-46eb-b3c3-8d861d15ac40",
        "tags" : [
        ]
      }
    ],
    "commit" : "3e41b618b1d01e1e36db3fa3b324834718ce38e0",
    "line" : 47,
    "diffHunk" : "@@ -1,1 +574,578 @@  }\n\n  def toLong(expression: Expression): Long = castAndEval[Long](expression, LongType)\n\n  def toInt(expression: Expression): Int = castAndEval[Int](expression, IntegerType)"
  },
  {
    "id" : "b778467d-ac57-4c57-a225-155bf08a5646",
    "prId" : 31570,
    "prUrl" : "https://github.com/apache/spark/pull/31570#pullrequestreview-614932111",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "654efc7e-13ac-459e-b091-1ffb7aa24f1f",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Ah OK this is conflicting with SessionWindowExpression when renaming. OK to leave as it is.",
        "createdAt" : "2021-03-18T03:07:56Z",
        "updatedAt" : "2021-03-18T03:37:02Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "04ca039bdf74c3b71be5fe53574906f426f25256",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +674,678 @@}\n\ncase class SessionWindow(\n    windowExpressions: NamedExpression,\n    timeColumn: Expression,"
  },
  {
    "id" : "06c7b477-2c88-463a-b55e-4e9fe841d0d8",
    "prId" : 30567,
    "prUrl" : "https://github.com/apache/spark/pull/30567#pullrequestreview-544684927",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "60c56031-6bdd-4fb9-bf2e-1a7839b7ef8e",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "This can be put in `SessionCatalog`",
        "createdAt" : "2020-12-04T06:23:36Z",
        "updatedAt" : "2020-12-04T06:23:37Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "db9f0baffd424beba5f9efd19620d781cfb2b22b",
    "line" : 60,
    "diffHunk" : "@@ -1,1 +490,494 @@  }\n\n  def fromCatalogTable(\n      metadata: CatalogTable, isTempView: Boolean, parser: ParserInterface): View = {\n    val viewText = metadata.viewText.getOrElse(sys.error(\"Invalid view without text.\"))"
  },
  {
    "id" : "2270814e-313d-4388-95f3-0f98a68722c0",
    "prId" : 30455,
    "prUrl" : "https://github.com/apache/spark/pull/30455#pullrequestreview-536946737",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "67a3832e-cd58-43de-9496-5330e66ee29b",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "nit: for readability, how about leaving a comment like this?\r\n```\r\ncase _ => // FullOuter\r\n```",
        "createdAt" : "2020-11-23T23:29:48Z",
        "updatedAt" : "2020-11-23T23:41:29Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "59be79be-f796-490a-9e42-c1fbec92bce4",
        "parentId" : "67a3832e-cd58-43de-9496-5330e66ee29b",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Ya. I thought like that, but actually, there are many missing patterns.",
        "createdAt" : "2020-11-24T00:21:16Z",
        "updatedAt" : "2020-11-24T00:21:17Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "e4bd0d2f-264e-4485-89ff-bedb4972d862",
        "parentId" : "67a3832e-cd58-43de-9496-5330e66ee29b",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "It was difficult to track one-by-one. Logically, this is the rest of the previous patterns. So, I decided to skip that comment.",
        "createdAt" : "2020-11-24T00:22:01Z",
        "updatedAt" : "2020-11-24T00:22:02Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "721439826beee1b4c8e58108ab7091e44db4cf9f",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +363,367 @@      case RightOuter =>\n        right.constraints\n      case _ =>\n        ExpressionSet()\n    }"
  },
  {
    "id" : "465497f9-b2dc-4b9a-9579-bc00b5c56237",
    "prId" : 30443,
    "prUrl" : "https://github.com/apache/spark/pull/30443#pullrequestreview-555230211",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c82f470a-45cd-40cf-b01b-7c615a0487bf",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Could you add tests for this code path?",
        "createdAt" : "2020-12-18T06:54:31Z",
        "updatedAt" : "2020-12-22T15:00:45Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "f27f44af-a6ba-4a1c-b2ff-75efe4133ea4",
        "parentId" : "c82f470a-45cd-40cf-b01b-7c615a0487bf",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "Add the test.",
        "createdAt" : "2020-12-18T07:46:15Z",
        "updatedAt" : "2020-12-22T15:00:45Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      }
    ],
    "commit" : "5d46a4c26a193e7ddea7788510d52376325cb99f",
    "line" : 42,
    "diffHunk" : "@@ -1,1 +566,570 @@      Some(numElements.toLong)\n    } else {\n      None\n    }\n  }"
  },
  {
    "id" : "2c4e3707-138e-40bd-84ff-71c7c6b71659",
    "prId" : 30443,
    "prUrl" : "https://github.com/apache/spark/pull/30443#pullrequestreview-555941994",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0600c20a-a1aa-46fb-89c7-44aa07783c09",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Could you add tests for this case in `test(\"SPARK-33497: Eliminate Limit if Join max rows not larger than Limit\")`?",
        "createdAt" : "2020-12-19T12:34:50Z",
        "updatedAt" : "2020-12-22T15:00:45Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "650230dc-fb0e-4b09-8c77-9a5f584fa284",
        "parentId" : "0600c20a-a1aa-46fb-89c7-44aa07783c09",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "Added by\r\n```\r\n  // maxRow is not valid long\r\n  checkPlanAndMaxRow(\r\n    testRelation.join(testRelation4, joinType).limit(100),\r\n    testRelation.join(testRelation4, joinType).limit(100),\r\n    100\r\n  )\r\n```",
        "createdAt" : "2020-12-19T13:34:14Z",
        "updatedAt" : "2020-12-22T15:00:45Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      }
    ],
    "commit" : "5d46a4c26a193e7ddea7788510d52376325cb99f",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +336,340 @@          Some(maxRows.toLong)\n        } else {\n          None\n        }\n"
  },
  {
    "id" : "51879394-9d43-49e1-ae77-be7eb5486d27",
    "prId" : 30334,
    "prUrl" : "https://github.com/apache/spark/pull/30334#pullrequestreview-633095217",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "76e94a04-654e-4380-9c70-bcd8faffe66a",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "cc @wangyum who touched Range rowCount recently.",
        "createdAt" : "2021-02-17T09:16:26Z",
        "updatedAt" : "2021-03-12T16:39:52Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "586abf59-9ca0-43af-a866-9ff5443dee06",
        "parentId" : "76e94a04-654e-4380-9c70-bcd8faffe66a",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "cc @AngersZhuuuu too FYI",
        "createdAt" : "2021-04-07T07:28:45Z",
        "updatedAt" : "2021-04-07T07:28:46Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "befd0b42-cbc1-451d-a42f-42f9c8eb02f5",
        "parentId" : "76e94a04-654e-4380-9c70-bcd8faffe66a",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "LGTM",
        "createdAt" : "2021-04-12T02:17:05Z",
        "updatedAt" : "2021-04-12T02:17:06Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "9eb4b7054c7185cd7dd665797188ad236ff78e43",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +607,611 @@\n  override def computeStats(): Statistics = {\n    if (numElements == 0) {\n      Statistics(sizeInBytes = 0, rowCount = Some(0))\n    } else {"
  },
  {
    "id" : "2df0bf7f-ebb3-4ac4-802e-53354c3872b0",
    "prId" : 30018,
    "prUrl" : "https://github.com/apache/spark/pull/30018#pullrequestreview-506897133",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f29efaec-065a-4ec2-9d56-dd0f13033067",
        "parentId" : null,
        "authorId" : "a8e23d47-3ae4-4385-848c-38a216d1bd08",
        "body" : "This caused some confusion while making this PR",
        "createdAt" : "2020-10-12T20:32:12Z",
        "updatedAt" : "2021-03-19T04:24:38Z",
        "lastEditedBy" : "a8e23d47-3ae4-4385-848c-38a216d1bd08",
        "tags" : [
        ]
      }
    ],
    "commit" : "07e758d25e0840bf40129a36ad5bdb90a005058a",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +622,626 @@ * @param groupingExpressions expressions for grouping keys\n * @param aggregateExpressions expressions for a project list, which could contain\n *                             [[AggregateExpression]]s.\n *\n * Note: Currently, aggregateExpressions is the project list of this Group by operator. Before"
  },
  {
    "id" : "cb7cd601-d084-426b-9ebb-65076f076e0e",
    "prId" : 29107,
    "prUrl" : "https://github.com/apache/spark/pull/29107#pullrequestreview-452805731",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "22ab76f0-0869-4a84-8349-2dc1ccf04eb5",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "we should add an assert: `allowMissingCol` can only be true of `byName` is true.",
        "createdAt" : "2020-07-21T12:04:24Z",
        "updatedAt" : "2020-07-23T05:50:46Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "545c2be6-24d8-41af-a9d9-b461aae04bec",
        "parentId" : "22ab76f0-0869-4a84-8349-2dc1ccf04eb5",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Added.",
        "createdAt" : "2020-07-21T20:30:41Z",
        "updatedAt" : "2020-07-23T05:50:46Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "2a9e1e44e03ce0551d619aec52d60cd2757c422b",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +230,234 @@    children: Seq[LogicalPlan],\n    byName: Boolean = false,\n    allowMissingCol: Boolean = false) extends LogicalPlan {\n  assert(!allowMissingCol || byName, \"`allowMissingCol` can be true only if `byName` is true.\")\n"
  },
  {
    "id" : "765d4002-e777-4a5c-9d01-932f339ed8e2",
    "prId" : 28027,
    "prUrl" : "https://github.com/apache/spark/pull/28027#pullrequestreview-518056858",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "009f401e-dd74-41bb-a62b-fd060a58224e",
        "parentId" : null,
        "authorId" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "body" : "why is this differentiation needed? Won't the metadata columns be part of `output`?",
        "createdAt" : "2020-10-27T01:34:24Z",
        "updatedAt" : "2020-11-18T00:41:10Z",
        "lastEditedBy" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "tags" : [
        ]
      },
      {
        "id" : "c930ec58-5888-4cec-a5b2-926f03b69689",
        "parentId" : "009f401e-dd74-41bb-a62b-fd060a58224e",
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "They are _eventually_ part of the output, but they can't be at first because `*` expansion uses all of `output`. If we added them immediately, we would get metadata columns in a `select *`.\r\n\r\nInstead, we add the metadata columns to this and then update column resolution to look up columns here. The result is that we can resolve everything just like normal, including `*`, but the columns are missing from output. Then the new analyzer rule adds the columns to the output if they are resolved, but missing. Since the parent node is already resolved, we know that this is safe and happens after `*` expansion.",
        "createdAt" : "2020-10-27T19:17:53Z",
        "updatedAt" : "2020-11-18T00:41:10Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      }
    ],
    "commit" : "6e72f4d68e6935efba32ba7a5f8b48adb66fe145",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +891,895 @@    val qualifierList = identifier.qualifier :+ alias\n    child.metadataOutput.map(_.withQualifier(qualifierList))\n  }\n\n  override def doCanonicalize(): LogicalPlan = child.canonicalized"
  },
  {
    "id" : "7640db3f-5874-45ab-8d10-d5e96042132d",
    "prId" : 25601,
    "prUrl" : "https://github.com/apache/spark/pull/25601#pullrequestreview-283732839",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "752f172f-1f60-42fb-896f-bc12eaad9841",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "After reading the code, it's actually `catalogAndNamespace`, right?",
        "createdAt" : "2019-09-04T13:00:37Z",
        "updatedAt" : "2019-09-09T23:58:04Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "29935c22-5361-4d34-9260-31d2a04ec0ab",
        "parentId" : "752f172f-1f60-42fb-896f-bc12eaad9841",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "This is just `namespace` since catalog is already resolved in `DataSourceResolution.scala`. ",
        "createdAt" : "2019-09-04T16:12:36Z",
        "updatedAt" : "2019-09-09T23:58:04Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "9a55a03acf84fe67595817c8fef409a9e4912a51",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +566,570 @@case class ShowNamespaces(\n    catalog: SupportsNamespaces,\n    namespace: Option[Seq[String]],\n    pattern: Option[String]) extends Command {\n  override val output: Seq[Attribute] = Seq("
  },
  {
    "id" : "3a7e8e1e-4519-4d8a-899a-d8265191c716",
    "prId" : 25354,
    "prUrl" : "https://github.com/apache/spark/pull/25354#pullrequestreview-277936119",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1a5925f6-56b9-4ee4-91c9-5c491fb4455d",
        "parentId" : null,
        "authorId" : "1aba4faa-ea5e-4487-baf9-d552ca566126",
        "body" : "just curious, is it coding style to have boolean parameter as the last one like isByName?\r\n\r\nhere `writeOptions` 2nd from last, and `writeOptions` is the last parameter in `OverwritePartitionsDynamic`",
        "createdAt" : "2019-08-21T05:30:28Z",
        "updatedAt" : "2019-08-31T02:18:57Z",
        "lastEditedBy" : "1aba4faa-ea5e-4487-baf9-d552ca566126",
        "tags" : [
        ]
      },
      {
        "id" : "446075bd-31c3-42cc-af9d-e39cbdfbc2ae",
        "parentId" : "1a5925f6-56b9-4ee4-91c9-5c491fb4455d",
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "Yes, it is for style. Boolean parameters should be passed by name, like `isByName = false`. Although you can pass positional parameters after a named parameter, the expectation is usually that named parameters are not necessarily in the correct position and can be omitted or reordered.",
        "createdAt" : "2019-08-21T16:12:06Z",
        "updatedAt" : "2019-08-31T02:18:57Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      },
      {
        "id" : "03db22d6-6f7d-4163-98f3-ca87e9c556ff",
        "parentId" : "1a5925f6-56b9-4ee4-91c9-5c491fb4455d",
        "authorId" : "1aba4faa-ea5e-4487-baf9-d552ca566126",
        "body" : "thanks Ryan üëç ",
        "createdAt" : "2019-08-21T16:59:48Z",
        "updatedAt" : "2019-08-31T02:18:57Z",
        "lastEditedBy" : "1aba4faa-ea5e-4487-baf9-d552ca566126",
        "tags" : [
        ]
      }
    ],
    "commit" : "9864d42501feff1acd01e11aedd2a7dc84a88bd5",
    "line" : 71,
    "diffHunk" : "@@ -1,1 +563,567 @@    table: NamedRelation,\n    query: LogicalPlan,\n    writeOptions: Map[String, String],\n    isByName: Boolean) extends V2WriteCommand\n"
  }
]