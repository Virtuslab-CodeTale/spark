[
  {
    "id" : "93448fda-b31d-4007-b5bc-ac6340132a02",
    "prId" : 33099,
    "prUrl" : "https://github.com/apache/spark/pull/33099#pullrequestreview-693417704",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "11aa06d2-4757-41b7-884f-49c8a3e24ce8",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "This looks like a nice refactoring in order to make consistent and simplify.",
        "createdAt" : "2021-06-27T04:19:03Z",
        "updatedAt" : "2021-06-27T04:19:03Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "ba266562-8e34-4465-81fd-49cfbcb911e8",
        "parentId" : "11aa06d2-4757-41b7-884f-49c8a3e24ce8",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "sorry for the late, I prefer not to do this. Since `RepartitionOperation` can affect the behavior of AQE, the semantics of `RepartitionOperation` have a little different from previous.\r\n\r\nCurrently, the `RepartitionOperation` behavior with AQE framwork follows:\r\n* `Repartition` does not use AQE stage optimizer rules\r\n* `RepartitionByExpression` depends on `optNumPartitions` and `partitionExpressions`\r\n*  `RebalancePartitions`(if extend)  depends on `partitionExpressions`\r\n\r\nAnd the rule `CollapseRepartition` may not effective in all use case.",
        "createdAt" : "2021-06-27T13:32:39Z",
        "updatedAt" : "2021-06-27T14:15:03Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      }
    ],
    "commit" : "67642d87f7d1799e97f925444484f52354e6af72",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +1374,1378 @@case class RebalancePartitions(\n    partitionExpressions: Seq[Expression],\n    child: LogicalPlan) extends RepartitionOperation {\n\n  override def numPartitions: Int = conf.numShufflePartitions"
  },
  {
    "id" : "0b5071e5-92b6-4a49-aa1d-74522d1e4df1",
    "prId" : 32932,
    "prUrl" : "https://github.com/apache/spark/pull/32932#pullrequestreview-693412190",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "599094f1-e305-4fe3-97fa-18b7f1e1b655",
        "parentId" : null,
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "Make `RebalancePartitions` extends `RepartitionOperation`?",
        "createdAt" : "2021-06-26T01:47:40Z",
        "updatedAt" : "2021-06-26T01:47:40Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      },
      {
        "id" : "11dc3ee2-0672-447e-a290-2baf03b5926d",
        "parentId" : "599094f1-e305-4fe3-97fa-18b7f1e1b655",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "Since it's a special one. For conservative, this PR does not extend `RepartitionOperation`, see the comment  https://github.com/apache/spark/pull/32932#discussion_r656249153.",
        "createdAt" : "2021-06-27T12:34:15Z",
        "updatedAt" : "2021-06-27T12:34:15Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      }
    ],
    "commit" : "0fe14d0001adedef66696699d478dd4d4e9c392c",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +1363,1367 @@case class RebalancePartitions(\n    partitionExpressions: Seq[Expression],\n    child: LogicalPlan) extends UnaryNode {\n  override def maxRows: Option[Long] = child.maxRows\n  override def output: Seq[Attribute] = child.output"
  },
  {
    "id" : "fa54bb2b-19df-48f6-a0b1-d42ef7aa6051",
    "prId" : 32498,
    "prUrl" : "https://github.com/apache/spark/pull/32498#pullrequestreview-658585331",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9e111b49-ffe9-48d4-b24e-ea94c08018a5",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "```\r\n  private def getRangeValue(index: Int): Long = {\r\n    assert(index >= 0, \"index must be greater than and equal to 0\")\r\n    if (step == 0) {\r\n      start + (numElements.toLong - index - 1) * step\r\n    } else {\r\n      start + index * step\r\n    }\r\n  }\r\n```\r\n?",
        "createdAt" : "2021-05-13T04:11:54Z",
        "updatedAt" : "2021-05-13T04:11:54Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "03c04694-9d5b-4e3b-81eb-e70db22fe6c0",
        "parentId" : "9e111b49-ffe9-48d4-b24e-ea94c08018a5",
        "authorId" : "d0d56018-475d-4da3-93a6-5e843402b7f2",
        "body" : "Done",
        "createdAt" : "2021-05-13T05:02:42Z",
        "updatedAt" : "2021-05-13T05:02:43Z",
        "lastEditedBy" : "d0d56018-475d-4da3-93a6-5e843402b7f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "bb69f84a3d05acec239db571afbfe2f41007d9ce",
    "line" : 50,
    "diffHunk" : "@@ -1,1 +821,825 @@\n  // Utility method to compute histogram\n  private def getRangeValue(index: Int): Long = {\n    assert(index >= 0, \"index must be greater than and equal to 0\")\n    if (step < 0) {"
  },
  {
    "id" : "fe00874a-5504-42de-a6be-940c23bba6a7",
    "prId" : 32498,
    "prUrl" : "https://github.com/apache/spark/pull/32498#pullrequestreview-661462423",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b5a57242-1d23-47f7-a54c-9d9d4d746d7f",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "EDIT: Fixed my formula\r\nI'm probably missing something, but isn't this more like:\r\n```\r\nstart + (numElements.toLong - 1) * (-step) + index * step\r\n```\r\nThis tries to step backwards from the end, right? `step` is negative.\r\n\r\nAlso I assume step = 0 is an error actually, maybe it's caught earlier, but the result should be `start` always in that case anyway, so `step < 0` as a condition?",
        "createdAt" : "2021-05-17T14:26:36Z",
        "updatedAt" : "2021-05-17T14:37:16Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "1a12600e-5704-4a07-83ed-b42342792790",
        "parentId" : "b5a57242-1d23-47f7-a54c-9d9d4d746d7f",
        "authorId" : "d0d56018-475d-4da3-93a6-5e843402b7f2",
        "body" : ">This tries to step backwards from the end, right? step is negative\r\n\r\nYes, we reverse the range values if the step is negative to compute histogram statistics.\r\n\r\n>I'm probably missing something, but isn't this more like:\r\n    start + (numElements.toLong - 1) * (-step) + index * step\r\n\r\nI am not sure this gives the right range values if the step is negative. \r\nFor eg: start = 1 , step = -2, and numElements = 4, Range values  = [1, -1, -3, -5]\r\nBut for computing histogram we need values like this [-5, -3, -1, 1]. So if the index = 0, it should return -5.\r\n\r\nWith the formula in the PR => 1 + (4 - 0 -1 )* -2= -5\r\n With the formula you have given above  => 1 + (4 -1) *-(-2) + 0 = 7\r\n\r\nSo I think the formula in the PR seems correct?\r\n\r\n",
        "createdAt" : "2021-05-17T20:42:18Z",
        "updatedAt" : "2021-05-17T20:42:18Z",
        "lastEditedBy" : "d0d56018-475d-4da3-93a6-5e843402b7f2",
        "tags" : [
        ]
      },
      {
        "id" : "ebd5f341-2122-47a9-95e0-4c35f73eccab",
        "parentId" : "b5a57242-1d23-47f7-a54c-9d9d4d746d7f",
        "authorId" : "d0d56018-475d-4da3-93a6-5e843402b7f2",
        "body" : ">Also I assume step = 0 is an error actually, maybe it's caught earlier, but the result should be start always in that case anyway, so step < 0 as a condition?\r\n\r\nYes, if step=0 both the conditions will return same start element. I have updated the condition to step < 0.",
        "createdAt" : "2021-05-17T20:44:47Z",
        "updatedAt" : "2021-05-17T20:44:47Z",
        "lastEditedBy" : "d0d56018-475d-4da3-93a6-5e843402b7f2",
        "tags" : [
        ]
      },
      {
        "id" : "4eb48290-7ba0-4a31-812d-3a4c54656a70",
        "parentId" : "b5a57242-1d23-47f7-a54c-9d9d4d746d7f",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "OK, I thought it might mean the desired result was [7, 5, 3, 1] in this case. You step back from the start. But it has to be reversed? OK if it needs to be [-5, -3, -1, 1] yeah that looks right. A comment to this effect might be helpful.",
        "createdAt" : "2021-05-17T21:20:11Z",
        "updatedAt" : "2021-05-17T21:20:11Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "abcf5ed9-7925-4ddc-8828-1f4a6d3d4083",
        "parentId" : "b5a57242-1d23-47f7-a54c-9d9d4d746d7f",
        "authorId" : "d0d56018-475d-4da3-93a6-5e843402b7f2",
        "body" : "Yes. I added a comment.",
        "createdAt" : "2021-05-17T22:05:29Z",
        "updatedAt" : "2021-05-17T22:16:05Z",
        "lastEditedBy" : "d0d56018-475d-4da3-93a6-5e843402b7f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "bb69f84a3d05acec239db571afbfe2f41007d9ce",
    "line" : 54,
    "diffHunk" : "@@ -1,1 +825,829 @@    if (step < 0) {\n      // Reverse the range values for computing histogram, if the step size is negative.\n      start + (numElements.toLong - index - 1) * step\n    } else {\n      start + index * step"
  },
  {
    "id" : "eada1773-1e1e-4e58-89df-f9ada07770d2",
    "prId" : 32350,
    "prUrl" : "https://github.com/apache/spark/pull/32350#pullrequestreview-647776353",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2319c3ed-d30b-4808-b81d-696d2266bb1b",
        "parentId" : null,
        "authorId" : "685f805b-e4fa-4f21-b066-58afcecf9ce6",
        "body" : "this override is needed for the added test",
        "createdAt" : "2021-04-29T02:29:50Z",
        "updatedAt" : "2021-05-06T06:35:40Z",
        "lastEditedBy" : "685f805b-e4fa-4f21-b066-58afcecf9ce6",
        "tags" : [
        ]
      }
    ],
    "commit" : "a3e26c79de9f46aeee82dc4e5bc11cfe2ab69668",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +70,74 @@  override def output: Seq[Attribute] = projectList.map(_.toAttribute)\n  override def maxRows: Option[Long] = child.maxRows\n  override def maxRowsPerPartition: Option[Long] = child.maxRowsPerPartition\n\n  override lazy val resolved: Boolean = {"
  },
  {
    "id" : "1f3a50ec-df93-4968-bcf7-44c9c1ec6038",
    "prId" : 31913,
    "prUrl" : "https://github.com/apache/spark/pull/31913#pullrequestreview-635309221",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "51267cd6-bd39-4208-956e-73a4bd304bc5",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I agree that it's not necessary to replace `Attribute` with `GroupingExprRef`, but it makes the framework more consistent if we always use `GroupingExprRef`. We can make `GroupingExprRef` a named expression to fix issues.\r\n\r\nThis is just my thought though, I'm happy to hear more opinions.",
        "createdAt" : "2021-04-13T15:37:02Z",
        "updatedAt" : "2021-04-17T16:23:39Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "d2ba6a43-06e4-4696-bfd5-c97467bb0d66",
        "parentId" : "51267cd6-bd39-4208-956e-73a4bd304bc5",
        "authorId" : "3d4870da-39a8-4406-b00d-131930d14cd8",
        "body" : "I also see the pros of making this framework more general, and I'm happy to do it, but it is not required for this bugfix. How about doing it in a follow-up ticket?",
        "createdAt" : "2021-04-13T18:35:36Z",
        "updatedAt" : "2021-04-17T16:23:39Z",
        "lastEditedBy" : "3d4870da-39a8-4406-b00d-131930d14cd8",
        "tags" : [
        ]
      },
      {
        "id" : "a5d99cdc-866d-444f-8b70-53e5d4a56bbf",
        "parentId" : "51267cd6-bd39-4208-956e-73a4bd304bc5",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "SGTM",
        "createdAt" : "2021-04-14T07:45:25Z",
        "updatedAt" : "2021-04-17T16:23:39Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "fb3a19dad5ac4448b8fe9d1b12ed1c3f6a0369a7",
    "line" : 103,
    "diffHunk" : "@@ -1,1 +883,887 @@    val complexGroupingExpressions = collectComplexGroupingExpressions(groupingExpressions)\n    val aggrExprWithGroupingReferences = if (complexGroupingExpressions.nonEmpty) {\n      insertGroupingReferences(aggregateExpressions, complexGroupingExpressions)\n    } else {\n      aggregateExpressions"
  },
  {
    "id" : "5aa026df-782e-4d05-9465-a671fc0f6b32",
    "prId" : 31908,
    "prUrl" : "https://github.com/apache/spark/pull/31908#pullrequestreview-617096013",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "343c0042-5baa-4062-8369-69b096bd87dc",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Ur, is this description correct? Technically, `semanticEquals` returns `false` for in-deterministic expressions. So, it seems that we miss some `Distinct` operators theoretically. It's not a problem for this optimizer's functionality, but maybe could you revise this description a little?\r\n```scala\r\n  def semanticEquals(other: Expression): Boolean =\r\n    deterministic && other.deterministic && canonicalized == other.canonicalized\r\n```",
        "createdAt" : "2021-03-22T01:35:09Z",
        "updatedAt" : "2021-04-22T02:59:22Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "aae4efea461af0da9f6ddfec4fb7a073ce191a67",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +846,850 @@    copy(child = newChild)\n\n  // Whether this Aggregate operator is equally the Distinct operator.\n  private[sql] def isDistinct: Boolean = {\n    aggregateExpressions.forall(a => groupingExpressions.exists(g => a.semanticEquals(g)))"
  },
  {
    "id" : "4ab3150b-e553-4c52-b8c1-3f50e5e9cfb5",
    "prId" : 31897,
    "prUrl" : "https://github.com/apache/spark/pull/31897#pullrequestreview-616616940",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "da6bc2e9-18d0-42c1-a0e2-822e2bf53ae9",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "BTW, isn't this a bug of `Window` class implementation itself, @viirya ?",
        "createdAt" : "2021-03-19T16:39:17Z",
        "updatedAt" : "2021-03-19T16:39:17Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "f84b6371-8d5a-4526-877f-75437d62aa59",
        "parentId" : "da6bc2e9-18d0-42c1-a0e2-822e2bf53ae9",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "I'm wondering if we can have this line for all the other applicable branches.",
        "createdAt" : "2021-03-19T16:44:34Z",
        "updatedAt" : "2021-03-19T16:44:34Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "2ee865df-4d37-4208-8d04-f50b8e6e1a5c",
        "parentId" : "da6bc2e9-18d0-42c1-a0e2-822e2bf53ae9",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Yea, I think so, although I changed it here for nested column pruning.\r\n\r\nCurrently it seems not causing other problem, although it is wrong. Looks good to have it in other branches (3.0/2.4) too.\r\n",
        "createdAt" : "2021-03-19T17:02:15Z",
        "updatedAt" : "2021-03-19T17:02:15Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "7eab1ace51a5174f347eec239279b5e7b3ed98dd",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +671,675 @@    child.output ++ windowExpressions.map(_.toAttribute)\n\n  override def producedAttributes: AttributeSet = windowOutputSet\n\n  def windowOutputSet: AttributeSet = AttributeSet(windowExpressions.map(_.toAttribute))"
  },
  {
    "id" : "73463f9c-c644-4207-8ef8-827f449bc5ac",
    "prId" : 31791,
    "prUrl" : "https://github.com/apache/spark/pull/31791#pullrequestreview-629459499",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ea7dbc79-b919-462e-b03d-9459b037f94a",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "What was the behavior before this PR? Only allow integral literals?",
        "createdAt" : "2021-04-06T09:13:43Z",
        "updatedAt" : "2021-04-06T22:56:35Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "16ac2418-c18c-4e12-8efe-8eceef1ce21e",
        "parentId" : "ea7dbc79-b919-462e-b03d-9459b037f94a",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Can you point me to the original code that does implicit cast for range?",
        "createdAt" : "2021-04-06T09:15:54Z",
        "updatedAt" : "2021-04-06T22:56:35Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "5f5b9e2f-378e-41b8-b26b-baca9500688b",
        "parentId" : "ea7dbc79-b919-462e-b03d-9459b037f94a",
        "authorId" : "b3ba992a-312c-46eb-b3c3-8d861d15ac40",
        "body" : "The original behavior is to implicit cast the given value into the expected type:\r\nhttps://github.com/apache/spark/blob/390d5bde81ce282a016c6cf36fb0d57012515388/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveTableValuedFunctions.scala#L40-L50\r\n\r\nWhere the expected types are hardcoded, for example `\"end\" -> LongType`\r\nhttps://github.com/apache/spark/blob/390d5bde81ce282a016c6cf36fb0d57012515388/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveTableValuedFunctions.scala#L86-L88\r\n\r\nIt uses `TypeCoercion.implicitCast(value, expectedType)`. So here `castAndEval` is trying to follow the previous behavior.",
        "createdAt" : "2021-04-06T22:31:37Z",
        "updatedAt" : "2021-04-06T22:56:35Z",
        "lastEditedBy" : "b3ba992a-312c-46eb-b3c3-8d861d15ac40",
        "tags" : [
        ]
      }
    ],
    "commit" : "3e41b618b1d01e1e36db3fa3b324834718ce38e0",
    "line" : 47,
    "diffHunk" : "@@ -1,1 +574,578 @@  }\n\n  def toLong(expression: Expression): Long = castAndEval[Long](expression, LongType)\n\n  def toInt(expression: Expression): Int = castAndEval[Int](expression, IntegerType)"
  },
  {
    "id" : "b778467d-ac57-4c57-a225-155bf08a5646",
    "prId" : 31570,
    "prUrl" : "https://github.com/apache/spark/pull/31570#pullrequestreview-614932111",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "654efc7e-13ac-459e-b091-1ffb7aa24f1f",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Ah OK this is conflicting with SessionWindowExpression when renaming. OK to leave as it is.",
        "createdAt" : "2021-03-18T03:07:56Z",
        "updatedAt" : "2021-03-18T03:37:02Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "04ca039bdf74c3b71be5fe53574906f426f25256",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +674,678 @@}\n\ncase class SessionWindow(\n    windowExpressions: NamedExpression,\n    timeColumn: Expression,"
  },
  {
    "id" : "06c7b477-2c88-463a-b55e-4e9fe841d0d8",
    "prId" : 30567,
    "prUrl" : "https://github.com/apache/spark/pull/30567#pullrequestreview-544684927",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "60c56031-6bdd-4fb9-bf2e-1a7839b7ef8e",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "This can be put in `SessionCatalog`",
        "createdAt" : "2020-12-04T06:23:36Z",
        "updatedAt" : "2020-12-04T06:23:37Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "db9f0baffd424beba5f9efd19620d781cfb2b22b",
    "line" : 60,
    "diffHunk" : "@@ -1,1 +490,494 @@  }\n\n  def fromCatalogTable(\n      metadata: CatalogTable, isTempView: Boolean, parser: ParserInterface): View = {\n    val viewText = metadata.viewText.getOrElse(sys.error(\"Invalid view without text.\"))"
  },
  {
    "id" : "2270814e-313d-4388-95f3-0f98a68722c0",
    "prId" : 30455,
    "prUrl" : "https://github.com/apache/spark/pull/30455#pullrequestreview-536946737",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "67a3832e-cd58-43de-9496-5330e66ee29b",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "nit: for readability, how about leaving a comment like this?\r\n```\r\ncase _ => // FullOuter\r\n```",
        "createdAt" : "2020-11-23T23:29:48Z",
        "updatedAt" : "2020-11-23T23:41:29Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "59be79be-f796-490a-9e42-c1fbec92bce4",
        "parentId" : "67a3832e-cd58-43de-9496-5330e66ee29b",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Ya. I thought like that, but actually, there are many missing patterns.",
        "createdAt" : "2020-11-24T00:21:16Z",
        "updatedAt" : "2020-11-24T00:21:17Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "e4bd0d2f-264e-4485-89ff-bedb4972d862",
        "parentId" : "67a3832e-cd58-43de-9496-5330e66ee29b",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "It was difficult to track one-by-one. Logically, this is the rest of the previous patterns. So, I decided to skip that comment.",
        "createdAt" : "2020-11-24T00:22:01Z",
        "updatedAt" : "2020-11-24T00:22:02Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "721439826beee1b4c8e58108ab7091e44db4cf9f",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +363,367 @@      case RightOuter =>\n        right.constraints\n      case _ =>\n        ExpressionSet()\n    }"
  },
  {
    "id" : "465497f9-b2dc-4b9a-9579-bc00b5c56237",
    "prId" : 30443,
    "prUrl" : "https://github.com/apache/spark/pull/30443#pullrequestreview-555230211",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c82f470a-45cd-40cf-b01b-7c615a0487bf",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Could you add tests for this code path?",
        "createdAt" : "2020-12-18T06:54:31Z",
        "updatedAt" : "2020-12-22T15:00:45Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "f27f44af-a6ba-4a1c-b2ff-75efe4133ea4",
        "parentId" : "c82f470a-45cd-40cf-b01b-7c615a0487bf",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "Add the test.",
        "createdAt" : "2020-12-18T07:46:15Z",
        "updatedAt" : "2020-12-22T15:00:45Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      }
    ],
    "commit" : "5d46a4c26a193e7ddea7788510d52376325cb99f",
    "line" : 42,
    "diffHunk" : "@@ -1,1 +566,570 @@      Some(numElements.toLong)\n    } else {\n      None\n    }\n  }"
  },
  {
    "id" : "2c4e3707-138e-40bd-84ff-71c7c6b71659",
    "prId" : 30443,
    "prUrl" : "https://github.com/apache/spark/pull/30443#pullrequestreview-555941994",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0600c20a-a1aa-46fb-89c7-44aa07783c09",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Could you add tests for this case in `test(\"SPARK-33497: Eliminate Limit if Join max rows not larger than Limit\")`?",
        "createdAt" : "2020-12-19T12:34:50Z",
        "updatedAt" : "2020-12-22T15:00:45Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "650230dc-fb0e-4b09-8c77-9a5f584fa284",
        "parentId" : "0600c20a-a1aa-46fb-89c7-44aa07783c09",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "Added by\r\n```\r\n  // maxRow is not valid long\r\n  checkPlanAndMaxRow(\r\n    testRelation.join(testRelation4, joinType).limit(100),\r\n    testRelation.join(testRelation4, joinType).limit(100),\r\n    100\r\n  )\r\n```",
        "createdAt" : "2020-12-19T13:34:14Z",
        "updatedAt" : "2020-12-22T15:00:45Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      }
    ],
    "commit" : "5d46a4c26a193e7ddea7788510d52376325cb99f",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +336,340 @@          Some(maxRows.toLong)\n        } else {\n          None\n        }\n"
  },
  {
    "id" : "51879394-9d43-49e1-ae77-be7eb5486d27",
    "prId" : 30334,
    "prUrl" : "https://github.com/apache/spark/pull/30334#pullrequestreview-633095217",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "76e94a04-654e-4380-9c70-bcd8faffe66a",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "cc @wangyum who touched Range rowCount recently.",
        "createdAt" : "2021-02-17T09:16:26Z",
        "updatedAt" : "2021-03-12T16:39:52Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "586abf59-9ca0-43af-a866-9ff5443dee06",
        "parentId" : "76e94a04-654e-4380-9c70-bcd8faffe66a",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "cc @AngersZhuuuu too FYI",
        "createdAt" : "2021-04-07T07:28:45Z",
        "updatedAt" : "2021-04-07T07:28:46Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "befd0b42-cbc1-451d-a42f-42f9c8eb02f5",
        "parentId" : "76e94a04-654e-4380-9c70-bcd8faffe66a",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "LGTM",
        "createdAt" : "2021-04-12T02:17:05Z",
        "updatedAt" : "2021-04-12T02:17:06Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "9eb4b7054c7185cd7dd665797188ad236ff78e43",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +607,611 @@\n  override def computeStats(): Statistics = {\n    if (numElements == 0) {\n      Statistics(sizeInBytes = 0, rowCount = Some(0))\n    } else {"
  }
]