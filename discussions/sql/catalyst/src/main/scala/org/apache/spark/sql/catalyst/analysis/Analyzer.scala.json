[
  {
    "id" : "01a73ee2-5d29-4bf4-9936-e9f4212784ab",
    "prId" : 33574,
    "prUrl" : "https://github.com/apache/spark/pull/33574#pullrequestreview-718331844",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c27663bb-b0d6-4922-b886-3d4b151d5581",
        "parentId" : null,
        "authorId" : "c12eb4aa-c39d-4fa9-8470-ed071c0be24a",
        "body" : "What's the difference between underscore?",
        "createdAt" : "2021-07-29T09:54:52Z",
        "updatedAt" : "2021-07-29T09:56:06Z",
        "lastEditedBy" : "c12eb4aa-c39d-4fa9-8470-ed071c0be24a",
        "tags" : [
        ]
      },
      {
        "id" : "67cc877e-1473-41fa-8991-865f9251a6de",
        "parentId" : "c27663bb-b0d6-4922-b886-3d4b151d5581",
        "authorId" : "f850781e-b750-4a78-9a80-1e68e9b4f9b7",
        "body" : "Using the underscore, the aggsBuffer is outside the scope of the map function at runtime and it will save the results of all elements.\r\nusing normal parameters, the aggsBuffer will only be recreated each time inside the map function loop.\r\n\r\nI suspect that Scala syntactic sugar in the conversion of the code made changes to cause, I also debugged this code many times before I found this difference, here is a simplified code to test separately.\r\n\r\n```\r\n    def testMap(seq: Seq[Int]): Seq[Int] = {\r\n      seq.map {\r\n        val buf = ArrayBuffer[Int]()\r\n        _ match {\r\n          case e: Int if e < 1 =>\r\n            val r = e + 1\r\n            println(s\"add to buf: $r\")\r\n            buf += r\r\n            r\r\n          case e: Int if buf.contains(e) =>\r\n            println(\"already in buf\")\r\n            0\r\n          case e =>\r\n            println(\"not in buf\")\r\n            e\r\n        }\r\n      }\r\n    }\r\n\r\n    testMap(Seq(0, 1))\r\n\r\n```",
        "createdAt" : "2021-07-29T12:36:51Z",
        "updatedAt" : "2021-07-29T12:51:53Z",
        "lastEditedBy" : "f850781e-b750-4a78-9a80-1e68e9b4f9b7",
        "tags" : [
        ]
      },
      {
        "id" : "39338740-00a7-46fa-9edd-d57c54915570",
        "parentId" : "c27663bb-b0d6-4922-b886-3d4b151d5581",
        "authorId" : "c12eb4aa-c39d-4fa9-8470-ed071c0be24a",
        "body" : "Thank you for the detailed explanation.",
        "createdAt" : "2021-07-29T17:00:21Z",
        "updatedAt" : "2021-07-29T17:00:21Z",
        "lastEditedBy" : "c12eb4aa-c39d-4fa9-8470-ed071c0be24a",
        "tags" : [
        ]
      }
    ],
    "commit" : "321374d87a1f50e458be42e55b08472befa846e3",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +589,593 @@        aggsBuffer.exists(a => a.find(_ eq e).isDefined)\n      }\n      replaceGroupingFunc(agg, groupByExprs, gid).transformDown {\n        // AggregateExpression should be computed on the unmodified value of its argument\n        // expressions, so we should not replace any references to grouping expression"
  },
  {
    "id" : "0ba97497-c1fe-4957-a185-3a359639f69c",
    "prId" : 33498,
    "prUrl" : "https://github.com/apache/spark/pull/33498#pullrequestreview-729193442",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f32c3e3a-753d-4efb-965f-1937d8405652",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Let's use a single brace next time for `map` in this case: https://github.com/databricks/scala-style-guide#anonymous-methods",
        "createdAt" : "2021-08-13T01:24:55Z",
        "updatedAt" : "2021-08-13T01:24:55Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "fb3931c6149d0d1d5962c5475f3d67b8430db07e",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +2554,2558 @@              // GROUP BY c1 HAVING c2 = 0`, even though we can resolve column `c2` here, we\n              // should undo it later and fail with \"Column c2 not found\".\n              agg.child.resolve(u.nameParts, resolver).map({\n                case a: Alias => TempResolvedColumn(a.child, u.nameParts)\n                case o => TempResolvedColumn(o, u.nameParts)"
  },
  {
    "id" : "c33c8016-5be7-44a1-9131-b1e815d5d99d",
    "prId" : 33468,
    "prUrl" : "https://github.com/apache/spark/pull/33468#pullrequestreview-712437184",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "55fc3b7f-928c-4de9-8d6c-a5c43b58325d",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "since it's a new code path, I didn't take care of the legacy mode and use ansi behavior directly.",
        "createdAt" : "2021-07-22T07:30:15Z",
        "updatedAt" : "2021-07-22T07:30:22Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "984b511adb2ca5b60f6acf1a84e1bbacc6effc30",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +3318,3322 @@          }\n          val casted = if (assignment.key.dataType != nullHandled.dataType) {\n            AnsiCast(nullHandled, assignment.key.dataType)\n          } else {\n            nullHandled"
  },
  {
    "id" : "943361ec-06d7-4ee9-b343-534112a2b7c0",
    "prId" : 33113,
    "prUrl" : "https://github.com/apache/spark/pull/33113#pullrequestreview-698906121",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bb9ab4d3-d602-4950-acfe-f6a66e588deb",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Do we need to put `fieldName` in `UnresolvedFieldPosition`? We can easily get it via `AlterTableAlterColumn.column.name`",
        "createdAt" : "2021-07-05T05:17:43Z",
        "updatedAt" : "2021-07-05T05:17:43Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "4e74bca2-f5aa-4e5b-913d-c460047d1d53",
        "parentId" : "bb9ab4d3-d602-4950-acfe-f6a66e588deb",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "https://github.com/apache/spark/pull/33213",
        "createdAt" : "2021-07-05T08:07:36Z",
        "updatedAt" : "2021-07-05T08:07:36Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "6a75c920df97a13ac547056eb7eb286b3accf973",
    "line" : 35,
    "diffHunk" : "@@ -1,1 +3537,3541 @@          case u: UnresolvedFieldPosition => u.position match {\n            case after: After =>\n              resolveFieldNames(table.schema, u.fieldName.init :+ after.column())\n                .map { resolved =>\n                  ResolvedFieldPosition(ColumnPosition.after(resolved.field.name))"
  },
  {
    "id" : "1c587e8f-a9b7-4fd2-bf38-8ea860264a63",
    "prId" : 32854,
    "prUrl" : "https://github.com/apache/spark/pull/32854#pullrequestreview-680375089",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0c52a681-96ea-466d-8b5a-78747e0b53c2",
        "parentId" : null,
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "This is a copied/modified version of https://github.com/apache/spark/blob/cadd3a0588eeed42c6742ae1b7a2eaa85bd8a3af/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala#L3687",
        "createdAt" : "2021-06-10T06:07:41Z",
        "updatedAt" : "2021-06-10T06:11:41Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "c9c545b559c69c3ddee408588b1ada341f7b3ae9",
    "line" : 30,
    "diffHunk" : "@@ -1,1 +3535,3539 @@     * not found. An error will be thrown in CheckAnalysis for columns that can't be resolved.\n     */\n    private def resolveFieldNames(\n        schema: StructType,\n        fieldNames: Seq[String]): Option[Seq[String]] = {"
  },
  {
    "id" : "746ab13f-2b8d-4bed-9047-faef78b7001f",
    "prId" : 32854,
    "prUrl" : "https://github.com/apache/spark/pull/32854#pullrequestreview-680375089",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "83679b7a-ef66-4cc9-9d21-d61f13e6b5cb",
        "parentId" : null,
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "We can remove `ResolveAlterTableChanges` once all the alter table commands are migrated to `ResolveAlterTableCommands`.",
        "createdAt" : "2021-06-10T06:11:36Z",
        "updatedAt" : "2021-06-10T06:11:41Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "c9c545b559c69c3ddee408588b1ada341f7b3ae9",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +300,304 @@      postHocResolutionRules: _*),\n    Batch(\"Normalize Alter Table Field Names\", Once, ResolveFieldNames),\n    Batch(\"Normalize Alter Table\", Once, ResolveAlterTableChanges),\n    Batch(\"Remove Unresolved Hints\", Once,\n      new ResolveHints.RemoveAllHints),"
  },
  {
    "id" : "fb39b0ef-b5ac-4245-96f9-809f63eb473e",
    "prId" : 32854,
    "prUrl" : "https://github.com/apache/spark/pull/32854#pullrequestreview-691073052",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5ff03d52-5c32-412c-a468-39d40f93bb1a",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Shall we fail here instead of in `CheckAnalysis`? Then we can avoid repeating the field name resolution logic in `CheckAnalysis`.",
        "createdAt" : "2021-06-10T17:54:28Z",
        "updatedAt" : "2021-06-10T17:54:28Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "b19e5d01-719e-4b49-946f-6132efef5e5f",
        "parentId" : "5ff03d52-5c32-412c-a468-39d40f93bb1a",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Another idea is to follow `UnresolvedPartitionSpec`/`ResolvedPartitionSpec`. We add `UnresolvedFieldName` and `ResolvedFieldName`, to do the resolution in a general way like `ResolvePartitionSpec`",
        "createdAt" : "2021-06-10T17:56:35Z",
        "updatedAt" : "2021-06-10T17:56:35Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "0b647f89-2928-4b0f-ae5e-a7d650ad941a",
        "parentId" : "5ff03d52-5c32-412c-a468-39d40f93bb1a",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "I updated the PR with the `UnresolvedFieldName`/`ResolvedFieldName` approach. Please let me know what you think.",
        "createdAt" : "2021-06-23T18:05:16Z",
        "updatedAt" : "2021-06-23T18:05:20Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      },
      {
        "id" : "3841f9db-7a5d-442e-b951-3abb0c5fac43",
        "parentId" : "5ff03d52-5c32-412c-a468-39d40f93bb1a",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "Also, please let me know if you want to see more commands migrated in this PR.",
        "createdAt" : "2021-06-23T19:41:25Z",
        "updatedAt" : "2021-06-23T19:41:28Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "c9c545b559c69c3ddee408588b1ada341f7b3ae9",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +3533,3537 @@    /**\n     * Returns the resolved field name if the field can be resolved, returns None if the column is\n     * not found. An error will be thrown in CheckAnalysis for columns that can't be resolved.\n     */\n    private def resolveFieldNames("
  },
  {
    "id" : "ac55f5ac-617d-4172-a657-b1fd26f8a6ab",
    "prId" : 32832,
    "prUrl" : "https://github.com/apache/spark/pull/32832#pullrequestreview-688058198",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fdd1db32-a392-4762-b7d5-15232f113b5d",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "We can allow an expression tree that contains only `ExtractValue` and attributes",
        "createdAt" : "2021-06-21T06:10:16Z",
        "updatedAt" : "2021-06-21T06:10:16Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "751a55e65033e9bb2c279acee6a300e6d61c6317",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +463,467 @@            case g: Generator => MultiAlias(g, Nil)\n            case c @ Cast(ne: NamedExpression, _, _, _) => Alias(c, ne.name)()\n            case e: ExtractValue =>\n              if (extractOnly(e)) {\n                Alias(e, toPrettySQL(e))()"
  },
  {
    "id" : "0e46698e-30a5-4a09-bbfb-5bd2cfab3660",
    "prId" : 32832,
    "prUrl" : "https://github.com/apache/spark/pull/32832#pullrequestreview-696773027",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "616eb6aa-cfa5-4d6a-9570-ba01a361c252",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "since we allow literal here, maybe we should allow it in the non `ExtractValue` case as well",
        "createdAt" : "2021-06-28T16:44:31Z",
        "updatedAt" : "2021-06-28T16:44:31Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "9cdea9a0-3600-4003-8217-5f61f02f6b34",
        "parentId" : "616eb6aa-cfa5-4d6a-9570-ba01a361c252",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "or maybe we shouldn't allow literal here either",
        "createdAt" : "2021-06-28T16:45:47Z",
        "updatedAt" : "2021-06-28T16:45:47Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "4f34a1a5-b44c-46e7-946e-838e95963a74",
        "parentId" : "616eb6aa-cfa5-4d6a-9570-ba01a361c252",
        "authorId" : "29c89154-22f7-40ee-b295-115a55734d6c",
        "body" : "@cloud-fan I don't think the alias of literal will be changed. And in order to keep current test cases passing. I prefer allowing the literal for non ExtractValue as well. Is this OK?",
        "createdAt" : "2021-06-30T17:03:24Z",
        "updatedAt" : "2021-06-30T17:03:25Z",
        "lastEditedBy" : "29c89154-22f7-40ee-b295-115a55734d6c",
        "tags" : [
        ]
      },
      {
        "id" : "d7a8480d-cce9-4b5c-ab5b-9d9719d7cc36",
        "parentId" : "616eb6aa-cfa5-4d6a-9570-ba01a361c252",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "sure",
        "createdAt" : "2021-07-01T03:13:45Z",
        "updatedAt" : "2021-07-01T03:13:45Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "751a55e65033e9bb2c279acee6a300e6d61c6317",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +446,450 @@      def extractOnly(e: Expression): Boolean = e match {\n        case _: ExtractValue => e.children.forall(extractOnly)\n        case _: Literal => true\n        case _: Attribute => true\n        case _ => false"
  },
  {
    "id" : "2d94179a-03ec-423d-ae78-ed69ec456e55",
    "prId" : 32686,
    "prUrl" : "https://github.com/apache/spark/pull/32686#pullrequestreview-672573756",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1e150735-9d81-4b14-a0a7-eda5a36e1484",
        "parentId" : null,
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "Let's add comment saying that `mayResolveAttrByAggregateExprs` requires the TreePattern `UNRESOLVED_ATTRIBUTE`",
        "createdAt" : "2021-05-31T13:51:20Z",
        "updatedAt" : "2021-05-31T14:42:46Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "a6edff95-5786-47ae-b688-f877f3ae7478",
        "parentId" : "1e150735-9d81-4b14-a0a7-eda5a36e1484",
        "authorId" : "8b518862-583d-43a2-a10e-b33ef1b6d824",
        "body" : "Done.",
        "createdAt" : "2021-06-01T00:12:19Z",
        "updatedAt" : "2021-06-01T00:12:20Z",
        "lastEditedBy" : "8b518862-583d-43a2-a10e-b33ef1b6d824",
        "tags" : [
        ]
      }
    ],
    "commit" : "8252a6a93a05c97ed47e3174be76fe1aeb3f6567",
    "line" : 82,
    "diffHunk" : "@@ -1,1 +1880,1884 @@    override def apply(plan: LogicalPlan): LogicalPlan = plan.resolveOperatorsUpWithPruning(\n      // mayResolveAttrByAggregateExprs requires the TreePattern UNRESOLVED_ATTRIBUTE.\n      _.containsAllPatterns(AGGREGATE, UNRESOLVED_ATTRIBUTE), ruleId) {\n      case agg @ Aggregate(groups, aggs, child)\n          if allowGroupByAlias && child.resolved && aggs.forall(_.resolved) &&"
  },
  {
    "id" : "8903730d-d110-49ae-b7e2-7f55d68b05cb",
    "prId" : 32619,
    "prUrl" : "https://github.com/apache/spark/pull/32619#pullrequestreview-665297120",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2c8d308f-5ca9-48e7-967d-b575327aaa63",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "So, previously the percentage and accuracy was wrapped by non-foldable if-else which caused analysis failures:\r\n\r\n```\r\nif ((type#7 <=> cast(b as string))) 10000 else cast(null as int)\r\n```",
        "createdAt" : "2021-05-21T10:00:03Z",
        "updatedAt" : "2021-05-21T10:00:03Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "0ecab9f41dcd5a669ae105a7af9db592d4800f40",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +774,778 @@                  // ApproximatePercentile takes two literals for accuracy and percentage which\n                  // should not be wrapped by if-else.\n                  a.withNewChildren(ifExpr(a.first) :: a.second :: a.third :: Nil)\n                case a: AggregateFunction =>\n                  a.withNewChildren(a.children.map(ifExpr))"
  },
  {
    "id" : "e094b0bd-c2c5-46d6-aae9-803aef558e02",
    "prId" : 32553,
    "prUrl" : "https://github.com/apache/spark/pull/32553#pullrequestreview-661335351",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a1002da9-37e9-438f-a6e3-8a97a8d81d59",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "how about primitive type parameters?",
        "createdAt" : "2021-05-17T14:54:05Z",
        "updatedAt" : "2021-05-17T14:54:06Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "c7bd8795-c436-4391-8270-cb7287e36d15",
        "parentId" : "a1002da9-37e9-438f-a6e3-8a97a8d81d59",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "I think it shouldn't matter. Here we're making sure that the magic method will always be invoked **regardless** of null-ness of the arguments. \r\n\r\nFor primitive types this has less significant meaning because even if `progagateNull` is true, `needNullCheck`\r\n```\r\n  protected lazy val needNullCheck: Boolean = propagateNull && arguments.exists(_.nullable)\r\n```\r\nin `InvokeLike` also considers null-ness of arguments.",
        "createdAt" : "2021-05-17T18:14:30Z",
        "updatedAt" : "2021-05-17T18:14:31Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "35f6812f-a484-4ee0-b994-774169295b1a",
        "parentId" : "a1002da9-37e9-438f-a6e3-8a97a8d81d59",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "How can users handle nullable int values if the UDF is something like `isPositive(int i)` which can't accept null argument?",
        "createdAt" : "2021-05-17T18:51:06Z",
        "updatedAt" : "2021-05-17T18:51:07Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "3f71f8e0-f1b2-4e7b-8de7-2d884777b9e7",
        "parentId" : "a1002da9-37e9-438f-a6e3-8a97a8d81d59",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Ah I see what you mean, thanks. Seems we should allow users to define magic method with boxed primitive types for this case? We could also follow the behavior of [ScalaUDF](https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/ScalaUDF.scala#L32) and returns null if any of the primitive type parameter is nullable and the input is null, however currently `InvokeLike` cannot handle the case where a subset of the input types are of primitive nullable type.",
        "createdAt" : "2021-05-17T19:22:26Z",
        "updatedAt" : "2021-05-17T19:22:26Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      }
    ],
    "commit" : "fb0d959fa2ba7ced2370840e3bb9fd53321b921b",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +2205,2209 @@          case Some(m) if Modifier.isStatic(m.getModifiers) =>\n            StaticInvoke(scalarFunc.getClass, scalarFunc.resultType(),\n              MAGIC_METHOD_NAME, arguments, propagateNull = false,\n              returnNullable = scalarFunc.isResultNullable)\n          case Some(_) =>"
  },
  {
    "id" : "48420f45-9f54-409b-bfdf-ac6c4ecba605",
    "prId" : 32553,
    "prUrl" : "https://github.com/apache/spark/pull/32553#pullrequestreview-661729604",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8680767a-36f7-42b6-ab17-4a35a2c94d40",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "So I think we need `propagateNull` as true, instead of false?",
        "createdAt" : "2021-05-18T06:22:50Z",
        "updatedAt" : "2021-05-18T06:22:50Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "29295584-9452-4822-9674-83eebe7d78b4",
        "parentId" : "8680767a-36f7-42b6-ab17-4a35a2c94d40",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "If `propagateNull` is true, we'd return null directly even if input arguments are of non-primitive type, which is not what we want.",
        "createdAt" : "2021-05-18T06:43:31Z",
        "updatedAt" : "2021-05-18T06:43:31Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "cfa7ecb3-3898-4718-b27f-e3d95c9455a6",
        "parentId" : "8680767a-36f7-42b6-ab17-4a35a2c94d40",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Got it. Thanks.",
        "createdAt" : "2021-05-18T07:26:39Z",
        "updatedAt" : "2021-05-18T07:26:39Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "fb0d959fa2ba7ced2370840e3bb9fd53321b921b",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +2205,2209 @@          case Some(m) if Modifier.isStatic(m.getModifiers) =>\n            StaticInvoke(scalarFunc.getClass, scalarFunc.resultType(),\n              MAGIC_METHOD_NAME, arguments, propagateNull = false,\n              returnNullable = scalarFunc.isResultNullable)\n          case Some(_) =>"
  },
  {
    "id" : "86361c83-554b-41f5-b926-9c02bd40785f",
    "prId" : 32542,
    "prUrl" : "https://github.com/apache/spark/pull/32542#pullrequestreview-659567253",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "937bdf70-e11c-4dfc-ae28-74b98fdcc748",
        "parentId" : null,
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "This is moved from `ResolveSessionCatalog.scala`.",
        "createdAt" : "2021-05-14T06:48:56Z",
        "updatedAt" : "2021-05-14T06:49:00Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "9e058b10a3e61cfe702cab8a4b15726660e3da78",
    "line" : 31,
    "diffHunk" : "@@ -1,1 +3518,3522 @@        val normalizedChanges = a.changes.flatMap {\n          case add: AddColumn =>\n            CatalogV2Util.failNullType(add.dataType)\n            def addColumn(\n                parentSchema: StructType,"
  },
  {
    "id" : "4c6bbe32-0bee-4084-8945-8ff712da97d5",
    "prId" : 32470,
    "prUrl" : "https://github.com/apache/spark/pull/32470#pullrequestreview-681368939",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "94ce2c08-59e8-464b-b0c6-c3595c551be2",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "How about leaving some comments about what are the returned two values?",
        "createdAt" : "2021-06-11T01:06:22Z",
        "updatedAt" : "2021-06-11T01:14:46Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "b362a098978be65ab1fc033fe0213a78a467b6ae",
    "line" : 173,
    "diffHunk" : "@@ -1,1 +2485,2489 @@    def resolveExprsWithAggregate(\n        exprs: Seq[Expression],\n        agg: Aggregate): (Seq[NamedExpression], Seq[Expression]) = {\n      def resolveCol(input: Expression): Expression = {\n        input.transform {"
  },
  {
    "id" : "b98ab40d-3e13-433f-b474-5eb5a153166e",
    "prId" : 32470,
    "prUrl" : "https://github.com/apache/spark/pull/32470#pullrequestreview-682103224",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "425810fd-adb3-450c-890c-7595c508b9ba",
        "parentId" : null,
        "authorId" : "b3ba992a-312c-46eb-b3c3-8d861d15ac40",
        "body" : "Should we also take into account the aggregate expressions already added in `aggExprList`? For example\r\n```sql\r\nselect c1 from t1 group by c1 having sum(c2) > 0 and sum(c2) < 10\r\n```\r\nHere `sum(c2)` will be added to the exprList and we don't need to create another alias for the second `sum(c2)`. Maybe use a aggExpr map instead of list?\r\n",
        "createdAt" : "2021-06-11T17:45:54Z",
        "updatedAt" : "2021-06-11T17:47:31Z",
        "lastEditedBy" : "b3ba992a-312c-46eb-b3c3-8d861d15ac40",
        "tags" : [
        ]
      },
      {
        "id" : "98fddd94-fcbf-4cb4-8b9d-96db2c6f100e",
        "parentId" : "425810fd-adb3-450c-890c-7595c508b9ba",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Makes sense. The current code follows the previous one in https://github.com/apache/spark/pull/32470/files#diff-ed19f376a63eba52eea59ca71f3355d4495fad4fad4db9a3324aade0d4986a47L2502\r\n\r\nI'll do it in a follow-up as this is an improvement, and update TPCDS query golden files if needed.",
        "createdAt" : "2021-06-11T18:10:13Z",
        "updatedAt" : "2021-06-11T18:10:13Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "b362a098978be65ab1fc033fe0213a78a467b6ae",
    "line" : 290,
    "diffHunk" : "@@ -1,1 +2536,2540 @@      // Avoid adding an extra aggregate expression if it's already present in\n      // `agg.aggregateExpressions`.\n      val index = agg.aggregateExpressions.indexWhere {\n        case Alias(child, _) => child semanticEquals expr\n        case other => other semanticEquals expr"
  },
  {
    "id" : "fcd01a54-0e0a-4cb7-9595-6974ddac05a6",
    "prId" : 32470,
    "prUrl" : "https://github.com/apache/spark/pull/32470#pullrequestreview-682626763",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1b34ce8b-7b43-4981-9e19-1dd1313c2db9",
        "parentId" : null,
        "authorId" : "b3ba992a-312c-46eb-b3c3-8d861d15ac40",
        "body" : "The children of the resolved subquery that are not aggregate expressions should also be wrapped in `TempResolvedColumn` (but it seems the current logic can't handle this case with proper error messages either. This can be a follow-up in the future)\r\n```sql\r\nselect c1 from t1 group by c1 having (select sum(c2) from t2 where t1.c2 = t2.c1) > 0\r\n-- org.apache.spark.sql.AnalysisException: Resolved attribute(s) c2#10 missing from c1#9 in operator\r\n-- !Filter (scalar-subquery#8 [c2#10] > cast(0 as bigint)).;\r\n```\r\n",
        "createdAt" : "2021-06-14T08:13:35Z",
        "updatedAt" : "2021-06-14T08:14:03Z",
        "lastEditedBy" : "b3ba992a-312c-46eb-b3c3-8d861d15ac40",
        "tags" : [
        ]
      }
    ],
    "commit" : "b362a098978be65ab1fc033fe0213a78a467b6ae",
    "line" : 208,
    "diffHunk" : "@@ -1,1 +2507,2511 @@        if (SubqueryExpression.hasSubquery(input)) {\n          val fake = Project(Alias(input, \"fake\")() :: Nil, agg.child)\n          ResolveSubquery(fake).asInstanceOf[Project].projectList.head.asInstanceOf[Alias].child\n        } else {\n          input"
  },
  {
    "id" : "23976fe2-35ec-4933-a5f3-c1045ad5dc33",
    "prId" : 32407,
    "prUrl" : "https://github.com/apache/spark/pull/32407#pullrequestreview-649505451",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d52e88b0-3e2f-42ce-9b86-21a6e736ef68",
        "parentId" : null,
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Ideally we'd want to check if this method is actually static, otherwise there could be runtime error. However this only works for methods defined in Java; for Scala seems there is no easy way.",
        "createdAt" : "2021-04-30T07:23:08Z",
        "updatedAt" : "2021-05-08T01:02:45Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "3b994c6c-bf0e-490e-a87d-080018a81af5",
        "parentId" : "d52e88b0-3e2f-42ce-9b86-21a6e736ef68",
        "authorId" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "body" : "Scala doesn't have the concept of truly static methods, right? The equivalent (`object` methods) are actually just instance methods on a singleton.",
        "createdAt" : "2021-04-30T15:05:16Z",
        "updatedAt" : "2021-05-08T01:02:45Z",
        "lastEditedBy" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "tags" : [
        ]
      },
      {
        "id" : "43229227-e34d-4c1a-8e4a-a72617a6b863",
        "parentId" : "d52e88b0-3e2f-42ce-9b86-21a6e736ef68",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Yes that's correct. The `StaticInvoke` calls the static method on the non-anonymous class which just forward to the non-static method defined in anonymous/singleton Java class (i.e., the class with `$` at the end of its name). \r\n\r\nFor instance, for the `LongAddWithStatic` class, this is the method defined in `LongAddWithStaticMagic.class`:\r\n```\r\n  public static long staticInvoke(long, long);\r\n    Code:\r\n       0: getstatic     #16                 // Field org/apache/spark/sql/connector/functions/LongAddWithStaticMagic$.MODULE$:Lorg/apache/spark/sql/connector/functions/LongAddWithStaticMagic$;\r\n       3: lload_0\r\n       4: lload_2\r\n       5: invokevirtual #51                 // Method org/apache/spark/sql/connector/functions/LongAddWithStaticMagic$.staticInvoke:(JJ)J\r\n       8: lreturn\r\n```\r\n\r\nand the same method defined in the singleton class `LongAddWithStaticMagic$`:\r\n```\r\n  public long staticInvoke(long, long);\r\n    Code:\r\n       0: lload_1\r\n       1: lload_3\r\n       2: ladd\r\n       3: lreturn\r\n```\r\n\r\nSo I was expecting worse performance from Scala since it calls `invokevirtual` underneath while Java uses `invokestatic`, but the result doesn't look so. It could be that the performance is dominated by other factors.",
        "createdAt" : "2021-04-30T18:39:07Z",
        "updatedAt" : "2021-05-08T01:02:45Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "56515b10-bcc5-4d6c-bc9d-9e3e44d4623c",
        "parentId" : "d52e88b0-3e2f-42ce-9b86-21a6e736ef68",
        "authorId" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "body" : "Very interesting, thanks for the explanation!",
        "createdAt" : "2021-04-30T18:42:05Z",
        "updatedAt" : "2021-05-08T01:02:45Z",
        "lastEditedBy" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "tags" : [
        ]
      }
    ],
    "commit" : "5096fac302554f88723777ed15583ee8b4b9a4f4",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +2183,2187 @@        findMethod(scalarFunc, MAGIC_METHOD_NAME, argClasses) match {\n          case Some(m) if Modifier.isStatic(m.getModifiers) =>\n            StaticInvoke(scalarFunc.getClass, scalarFunc.resultType(),\n              MAGIC_METHOD_NAME, arguments, returnNullable = scalarFunc.isResultNullable)\n          case Some(_) =>"
  },
  {
    "id" : "f5ee18fa-62a7-4371-a256-b4e0324d37e9",
    "prId" : 32247,
    "prUrl" : "https://github.com/apache/spark/pull/32247#pullrequestreview-641802167",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5e6a6842-ac19-4cf8-ae18-7cb28eb5d2a0",
        "parentId" : null,
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "Quick question: why don't we have pattern `SubqueryExpression` ",
        "createdAt" : "2021-04-22T04:55:07Z",
        "updatedAt" : "2021-04-22T05:57:06Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "4d367747-ff52-4c99-b9ec-5a0defb15bf3",
        "parentId" : "5e6a6842-ac19-4cf8-ae18-7cb28eb5d2a0",
        "authorId" : "8b518862-583d-43a2-a10e-b33ef1b6d824",
        "body" : "PlanExpression covers both logical and physical, so that we can use it in a few physical rules too in later PRs without adding new bits.",
        "createdAt" : "2021-04-22T05:59:03Z",
        "updatedAt" : "2021-04-22T06:00:00Z",
        "lastEditedBy" : "8b518862-583d-43a2-a10e-b33ef1b6d824",
        "tags" : [
        ]
      }
    ],
    "commit" : "41270b60f210e392eb90984a000a66d9fe9e5b2a",
    "line" : 39,
    "diffHunk" : "@@ -1,1 +3793,3797 @@    plan.resolveOperatorsWithPruning(_.containsAllPatterns(PLAN_EXPRESSION, FILTER), ruleId) {\n      case f @ Filter(_, a: Aggregate) if f.resolved =>\n        f.transformExpressionsWithPruning(_.containsPattern(PLAN_EXPRESSION), ruleId) {\n          case s: SubqueryExpression if s.children.nonEmpty =>\n            // Collect the aliases from output of aggregate."
  },
  {
    "id" : "5f90b95f-a1c7-46a5-9a07-14a52b32ae31",
    "prId" : 32192,
    "prUrl" : "https://github.com/apache/spark/pull/32192#pullrequestreview-636851042",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d8bfa187-cd17-48e3-bf7e-39513ba80b5e",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "To avoid resolving `UpdateAction` again with `resolveValuesWithSourceOnly = false` in the next analysis round, I added `resolveMergeExprOrFail` to fail earlier if an attribute can't be resolved.",
        "createdAt" : "2021-04-15T15:50:04Z",
        "updatedAt" : "2021-04-15T15:50:04Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "19d77eadfb176a77c110d654b6ca9aa78ee4d6a9",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +1481,1485 @@                  updateCondition.map(resolveExpressionByPlanChildren(_, m)),\n                  // For UPDATE *, the value must from source table.\n                  resolveAssignments(assignments, m, resolveValuesWithSourceOnly = true))\n              case o => o\n            }"
  },
  {
    "id" : "9a19d56e-655b-41d5-868b-e5530ff85e3c",
    "prId" : 32170,
    "prUrl" : "https://github.com/apache/spark/pull/32170#pullrequestreview-694112028",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e338141f-3599-441f-9350-03e7572dba8f",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I doubt this. For old interval types, `Date + Interval` always returns Date:\r\n```\r\ncase (DateType, CalendarIntervalType) => DateAddInterval(l, r, ansiEnabled = f)\r\ncase (CalendarIntervalType, DateType) => DateAddInterval(r, l, ansiEnabled = f)\r\n```\r\n\r\nI think we should do this for new interval types as well, and fail at runtime if the interval has hour, minute, second fields.",
        "createdAt" : "2021-06-28T15:41:33Z",
        "updatedAt" : "2021-06-28T15:41:33Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "4a425e1452d4834c645e3627dec469fd14e7c322",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +347,351 @@        case a @ Add(l, r, f) if a.childrenResolved => (l.dataType, r.dataType) match {\n          case (DateType, DayTimeIntervalType) => TimeAdd(Cast(l, TimestampType), r)\n          case (DayTimeIntervalType, DateType) => TimeAdd(Cast(r, TimestampType), l)\n          case (DateType, YearMonthIntervalType) => DateAddYMInterval(l, r)\n          case (YearMonthIntervalType, DateType) => DateAddYMInterval(r, l)"
  },
  {
    "id" : "bd36f9ac-6608-4459-a26d-a44c02ed3a05",
    "prId" : 32135,
    "prUrl" : "https://github.com/apache/spark/pull/32135#pullrequestreview-634479988",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2112804b-c0c1-41bd-8d8b-18d2ecf11838",
        "parentId" : null,
        "authorId" : "80d631a6-73e8-46a6-a01b-a80d1f1cc6cc",
        "body" : "Does this pass scalastyle?",
        "createdAt" : "2021-04-13T07:42:34Z",
        "updatedAt" : "2021-04-13T07:42:34Z",
        "lastEditedBy" : "80d631a6-73e8-46a6-a01b-a80d1f1cc6cc",
        "tags" : [
        ]
      },
      {
        "id" : "f6077644-090c-4ab6-aa9a-a9975b1789ee",
        "parentId" : "2112804b-c0c1-41bd-8d8b-18d2ecf11838",
        "authorId" : "8b518862-583d-43a2-a10e-b33ef1b6d824",
        "body" : "It seemed ok -- I didn't see scalastyle complained anything. \r\nBut let me know if there's a convention in the Spark codebase.",
        "createdAt" : "2021-04-13T08:03:48Z",
        "updatedAt" : "2021-04-13T08:03:49Z",
        "lastEditedBy" : "8b518862-583d-43a2-a10e-b33ef1b6d824",
        "tags" : [
        ]
      },
      {
        "id" : "fb486f00-f0ba-4878-a1f6-ba835092def6",
        "parentId" : "2112804b-c0c1-41bd-8d8b-18d2ecf11838",
        "authorId" : "80d631a6-73e8-46a6-a01b-a80d1f1cc6cc",
        "body" : "Normally we put it all on a single line. If it passes then I am happy.",
        "createdAt" : "2021-04-13T08:37:52Z",
        "updatedAt" : "2021-04-13T08:37:52Z",
        "lastEditedBy" : "80d631a6-73e8-46a6-a01b-a80d1f1cc6cc",
        "tags" : [
        ]
      },
      {
        "id" : "a3c947ed-65f3-46ef-b8b9-f288f09660c6",
        "parentId" : "2112804b-c0c1-41bd-8d8b-18d2ecf11838",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "It passes..",
        "createdAt" : "2021-04-13T11:33:26Z",
        "updatedAt" : "2021-04-13T11:33:27Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "9db15f0ee51bca66e5ca66a47fd93506a421f664",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +40,44 @@import org.apache.spark.sql.catalyst.streaming.StreamingRelationV2\nimport org.apache.spark.sql.catalyst.trees.TreeNodeRef\nimport org.apache.spark.sql.catalyst.trees.TreePattern.{\n  EXPRESSION_WITH_RANDOM_SEED, NATURAL_LIKE_JOIN, WINDOW_EXPRESSION\n}"
  },
  {
    "id" : "91362692-da52-40f9-8e2d-dee26b113f30",
    "prId" : 32082,
    "prUrl" : "https://github.com/apache/spark/pull/32082#pullrequestreview-645869174",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1a375362-eba2-4825-afa4-6c91a58a8842",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "ditto, put into a new method.",
        "createdAt" : "2021-04-27T13:40:42Z",
        "updatedAt" : "2021-04-27T19:27:48Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "c18715fcc304f4a805d420e193d4a3884b3f7e4f",
    "line" : 163,
    "diffHunk" : "@@ -1,1 +2130,2134 @@                      processV2ScalarFunction(scalarFunc, inputType, arguments, isDistinct,\n                        filter, ignoreNulls)\n                    case aggFunc: V2AggregateFunction[_, _] =>\n                      processV2AggregateFunction(aggFunc, arguments, isDistinct, filter,\n                        ignoreNulls)"
  },
  {
    "id" : "68f3c0b9-cd56-4d4c-ab9e-653be9557748",
    "prId" : 31940,
    "prUrl" : "https://github.com/apache/spark/pull/31940#pullrequestreview-618579807",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5082beb8-adc7-48f1-953a-c58de324940e",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "shall we handle `case b @ BinaryComparison(OuterReference(attr: Attribute), lit) if lit.foldable` as well.",
        "createdAt" : "2021-03-23T07:35:17Z",
        "updatedAt" : "2021-03-24T13:11:14Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "a813d2f1-89fd-44cf-93e5-75af81ccf8a4",
        "parentId" : "5082beb8-adc7-48f1-953a-c58de324940e",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "done",
        "createdAt" : "2021-03-23T13:22:13Z",
        "updatedAt" : "2021-03-24T13:11:14Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "9142bfc7ea082133bd60f7dc1090c64f7835199d",
    "line" : 36,
    "diffHunk" : "@@ -1,1 +4040,4044 @@          b.withNewChildren(CharVarcharUtils.addPaddingInStringComparison(Seq(left, right)))\n\n        case b @ BinaryComparison(OuterReference(left: Attribute), right: Attribute) =>\n          b.withNewChildren(padOuterRefAttrCmp(left, right))\n"
  }
]