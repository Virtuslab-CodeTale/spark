[
  {
    "id" : "01a73ee2-5d29-4bf4-9936-e9f4212784ab",
    "prId" : 33574,
    "prUrl" : "https://github.com/apache/spark/pull/33574#pullrequestreview-718331844",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c27663bb-b0d6-4922-b886-3d4b151d5581",
        "parentId" : null,
        "authorId" : "c12eb4aa-c39d-4fa9-8470-ed071c0be24a",
        "body" : "What's the difference between underscore?",
        "createdAt" : "2021-07-29T09:54:52Z",
        "updatedAt" : "2021-07-29T09:56:06Z",
        "lastEditedBy" : "c12eb4aa-c39d-4fa9-8470-ed071c0be24a",
        "tags" : [
        ]
      },
      {
        "id" : "67cc877e-1473-41fa-8991-865f9251a6de",
        "parentId" : "c27663bb-b0d6-4922-b886-3d4b151d5581",
        "authorId" : "f850781e-b750-4a78-9a80-1e68e9b4f9b7",
        "body" : "Using the underscore, the aggsBuffer is outside the scope of the map function at runtime and it will save the results of all elements.\r\nusing normal parameters, the aggsBuffer will only be recreated each time inside the map function loop.\r\n\r\nI suspect that Scala syntactic sugar in the conversion of the code made changes to cause, I also debugged this code many times before I found this difference, here is a simplified code to test separately.\r\n\r\n```\r\n    def testMap(seq: Seq[Int]): Seq[Int] = {\r\n      seq.map {\r\n        val buf = ArrayBuffer[Int]()\r\n        _ match {\r\n          case e: Int if e < 1 =>\r\n            val r = e + 1\r\n            println(s\"add to buf: $r\")\r\n            buf += r\r\n            r\r\n          case e: Int if buf.contains(e) =>\r\n            println(\"already in buf\")\r\n            0\r\n          case e =>\r\n            println(\"not in buf\")\r\n            e\r\n        }\r\n      }\r\n    }\r\n\r\n    testMap(Seq(0, 1))\r\n\r\n```",
        "createdAt" : "2021-07-29T12:36:51Z",
        "updatedAt" : "2021-07-29T12:51:53Z",
        "lastEditedBy" : "f850781e-b750-4a78-9a80-1e68e9b4f9b7",
        "tags" : [
        ]
      },
      {
        "id" : "39338740-00a7-46fa-9edd-d57c54915570",
        "parentId" : "c27663bb-b0d6-4922-b886-3d4b151d5581",
        "authorId" : "c12eb4aa-c39d-4fa9-8470-ed071c0be24a",
        "body" : "Thank you for the detailed explanation.",
        "createdAt" : "2021-07-29T17:00:21Z",
        "updatedAt" : "2021-07-29T17:00:21Z",
        "lastEditedBy" : "c12eb4aa-c39d-4fa9-8470-ed071c0be24a",
        "tags" : [
        ]
      }
    ],
    "commit" : "321374d87a1f50e458be42e55b08472befa846e3",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +589,593 @@        aggsBuffer.exists(a => a.find(_ eq e).isDefined)\n      }\n      replaceGroupingFunc(agg, groupByExprs, gid).transformDown {\n        // AggregateExpression should be computed on the unmodified value of its argument\n        // expressions, so we should not replace any references to grouping expression"
  },
  {
    "id" : "0ba97497-c1fe-4957-a185-3a359639f69c",
    "prId" : 33498,
    "prUrl" : "https://github.com/apache/spark/pull/33498#pullrequestreview-729193442",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f32c3e3a-753d-4efb-965f-1937d8405652",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Let's use a single brace next time for `map` in this case: https://github.com/databricks/scala-style-guide#anonymous-methods",
        "createdAt" : "2021-08-13T01:24:55Z",
        "updatedAt" : "2021-08-13T01:24:55Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "fb3931c6149d0d1d5962c5475f3d67b8430db07e",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +2554,2558 @@              // GROUP BY c1 HAVING c2 = 0`, even though we can resolve column `c2` here, we\n              // should undo it later and fail with \"Column c2 not found\".\n              agg.child.resolve(u.nameParts, resolver).map({\n                case a: Alias => TempResolvedColumn(a.child, u.nameParts)\n                case o => TempResolvedColumn(o, u.nameParts)"
  },
  {
    "id" : "c33c8016-5be7-44a1-9131-b1e815d5d99d",
    "prId" : 33468,
    "prUrl" : "https://github.com/apache/spark/pull/33468#pullrequestreview-712437184",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "55fc3b7f-928c-4de9-8d6c-a5c43b58325d",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "since it's a new code path, I didn't take care of the legacy mode and use ansi behavior directly.",
        "createdAt" : "2021-07-22T07:30:15Z",
        "updatedAt" : "2021-07-22T07:30:22Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "984b511adb2ca5b60f6acf1a84e1bbacc6effc30",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +3318,3322 @@          }\n          val casted = if (assignment.key.dataType != nullHandled.dataType) {\n            AnsiCast(nullHandled, assignment.key.dataType)\n          } else {\n            nullHandled"
  },
  {
    "id" : "943361ec-06d7-4ee9-b343-534112a2b7c0",
    "prId" : 33113,
    "prUrl" : "https://github.com/apache/spark/pull/33113#pullrequestreview-698906121",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bb9ab4d3-d602-4950-acfe-f6a66e588deb",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Do we need to put `fieldName` in `UnresolvedFieldPosition`? We can easily get it via `AlterTableAlterColumn.column.name`",
        "createdAt" : "2021-07-05T05:17:43Z",
        "updatedAt" : "2021-07-05T05:17:43Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "4e74bca2-f5aa-4e5b-913d-c460047d1d53",
        "parentId" : "bb9ab4d3-d602-4950-acfe-f6a66e588deb",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "https://github.com/apache/spark/pull/33213",
        "createdAt" : "2021-07-05T08:07:36Z",
        "updatedAt" : "2021-07-05T08:07:36Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "6a75c920df97a13ac547056eb7eb286b3accf973",
    "line" : 35,
    "diffHunk" : "@@ -1,1 +3537,3541 @@          case u: UnresolvedFieldPosition => u.position match {\n            case after: After =>\n              resolveFieldNames(table.schema, u.fieldName.init :+ after.column())\n                .map { resolved =>\n                  ResolvedFieldPosition(ColumnPosition.after(resolved.field.name))"
  },
  {
    "id" : "1c587e8f-a9b7-4fd2-bf38-8ea860264a63",
    "prId" : 32854,
    "prUrl" : "https://github.com/apache/spark/pull/32854#pullrequestreview-680375089",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0c52a681-96ea-466d-8b5a-78747e0b53c2",
        "parentId" : null,
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "This is a copied/modified version of https://github.com/apache/spark/blob/cadd3a0588eeed42c6742ae1b7a2eaa85bd8a3af/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala#L3687",
        "createdAt" : "2021-06-10T06:07:41Z",
        "updatedAt" : "2021-06-10T06:11:41Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "c9c545b559c69c3ddee408588b1ada341f7b3ae9",
    "line" : 30,
    "diffHunk" : "@@ -1,1 +3535,3539 @@     * not found. An error will be thrown in CheckAnalysis for columns that can't be resolved.\n     */\n    private def resolveFieldNames(\n        schema: StructType,\n        fieldNames: Seq[String]): Option[Seq[String]] = {"
  },
  {
    "id" : "746ab13f-2b8d-4bed-9047-faef78b7001f",
    "prId" : 32854,
    "prUrl" : "https://github.com/apache/spark/pull/32854#pullrequestreview-680375089",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "83679b7a-ef66-4cc9-9d21-d61f13e6b5cb",
        "parentId" : null,
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "We can remove `ResolveAlterTableChanges` once all the alter table commands are migrated to `ResolveAlterTableCommands`.",
        "createdAt" : "2021-06-10T06:11:36Z",
        "updatedAt" : "2021-06-10T06:11:41Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "c9c545b559c69c3ddee408588b1ada341f7b3ae9",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +300,304 @@      postHocResolutionRules: _*),\n    Batch(\"Normalize Alter Table Field Names\", Once, ResolveFieldNames),\n    Batch(\"Normalize Alter Table\", Once, ResolveAlterTableChanges),\n    Batch(\"Remove Unresolved Hints\", Once,\n      new ResolveHints.RemoveAllHints),"
  },
  {
    "id" : "fb39b0ef-b5ac-4245-96f9-809f63eb473e",
    "prId" : 32854,
    "prUrl" : "https://github.com/apache/spark/pull/32854#pullrequestreview-691073052",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5ff03d52-5c32-412c-a468-39d40f93bb1a",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Shall we fail here instead of in `CheckAnalysis`? Then we can avoid repeating the field name resolution logic in `CheckAnalysis`.",
        "createdAt" : "2021-06-10T17:54:28Z",
        "updatedAt" : "2021-06-10T17:54:28Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "b19e5d01-719e-4b49-946f-6132efef5e5f",
        "parentId" : "5ff03d52-5c32-412c-a468-39d40f93bb1a",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Another idea is to follow `UnresolvedPartitionSpec`/`ResolvedPartitionSpec`. We add `UnresolvedFieldName` and `ResolvedFieldName`, to do the resolution in a general way like `ResolvePartitionSpec`",
        "createdAt" : "2021-06-10T17:56:35Z",
        "updatedAt" : "2021-06-10T17:56:35Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "0b647f89-2928-4b0f-ae5e-a7d650ad941a",
        "parentId" : "5ff03d52-5c32-412c-a468-39d40f93bb1a",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "I updated the PR with the `UnresolvedFieldName`/`ResolvedFieldName` approach. Please let me know what you think.",
        "createdAt" : "2021-06-23T18:05:16Z",
        "updatedAt" : "2021-06-23T18:05:20Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      },
      {
        "id" : "3841f9db-7a5d-442e-b951-3abb0c5fac43",
        "parentId" : "5ff03d52-5c32-412c-a468-39d40f93bb1a",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "Also, please let me know if you want to see more commands migrated in this PR.",
        "createdAt" : "2021-06-23T19:41:25Z",
        "updatedAt" : "2021-06-23T19:41:28Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "c9c545b559c69c3ddee408588b1ada341f7b3ae9",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +3533,3537 @@    /**\n     * Returns the resolved field name if the field can be resolved, returns None if the column is\n     * not found. An error will be thrown in CheckAnalysis for columns that can't be resolved.\n     */\n    private def resolveFieldNames("
  },
  {
    "id" : "ac55f5ac-617d-4172-a657-b1fd26f8a6ab",
    "prId" : 32832,
    "prUrl" : "https://github.com/apache/spark/pull/32832#pullrequestreview-688058198",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fdd1db32-a392-4762-b7d5-15232f113b5d",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "We can allow an expression tree that contains only `ExtractValue` and attributes",
        "createdAt" : "2021-06-21T06:10:16Z",
        "updatedAt" : "2021-06-21T06:10:16Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "751a55e65033e9bb2c279acee6a300e6d61c6317",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +463,467 @@            case g: Generator => MultiAlias(g, Nil)\n            case c @ Cast(ne: NamedExpression, _, _, _) => Alias(c, ne.name)()\n            case e: ExtractValue =>\n              if (extractOnly(e)) {\n                Alias(e, toPrettySQL(e))()"
  },
  {
    "id" : "0e46698e-30a5-4a09-bbfb-5bd2cfab3660",
    "prId" : 32832,
    "prUrl" : "https://github.com/apache/spark/pull/32832#pullrequestreview-696773027",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "616eb6aa-cfa5-4d6a-9570-ba01a361c252",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "since we allow literal here, maybe we should allow it in the non `ExtractValue` case as well",
        "createdAt" : "2021-06-28T16:44:31Z",
        "updatedAt" : "2021-06-28T16:44:31Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "9cdea9a0-3600-4003-8217-5f61f02f6b34",
        "parentId" : "616eb6aa-cfa5-4d6a-9570-ba01a361c252",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "or maybe we shouldn't allow literal here either",
        "createdAt" : "2021-06-28T16:45:47Z",
        "updatedAt" : "2021-06-28T16:45:47Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "4f34a1a5-b44c-46e7-946e-838e95963a74",
        "parentId" : "616eb6aa-cfa5-4d6a-9570-ba01a361c252",
        "authorId" : "29c89154-22f7-40ee-b295-115a55734d6c",
        "body" : "@cloud-fan I don't think the alias of literal will be changed. And in order to keep current test cases passing. I prefer allowing the literal for non ExtractValue as well. Is this OK?",
        "createdAt" : "2021-06-30T17:03:24Z",
        "updatedAt" : "2021-06-30T17:03:25Z",
        "lastEditedBy" : "29c89154-22f7-40ee-b295-115a55734d6c",
        "tags" : [
        ]
      },
      {
        "id" : "d7a8480d-cce9-4b5c-ab5b-9d9719d7cc36",
        "parentId" : "616eb6aa-cfa5-4d6a-9570-ba01a361c252",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "sure",
        "createdAt" : "2021-07-01T03:13:45Z",
        "updatedAt" : "2021-07-01T03:13:45Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "751a55e65033e9bb2c279acee6a300e6d61c6317",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +446,450 @@      def extractOnly(e: Expression): Boolean = e match {\n        case _: ExtractValue => e.children.forall(extractOnly)\n        case _: Literal => true\n        case _: Attribute => true\n        case _ => false"
  },
  {
    "id" : "2d94179a-03ec-423d-ae78-ed69ec456e55",
    "prId" : 32686,
    "prUrl" : "https://github.com/apache/spark/pull/32686#pullrequestreview-672573756",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1e150735-9d81-4b14-a0a7-eda5a36e1484",
        "parentId" : null,
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "Let's add comment saying that `mayResolveAttrByAggregateExprs` requires the TreePattern `UNRESOLVED_ATTRIBUTE`",
        "createdAt" : "2021-05-31T13:51:20Z",
        "updatedAt" : "2021-05-31T14:42:46Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "a6edff95-5786-47ae-b688-f877f3ae7478",
        "parentId" : "1e150735-9d81-4b14-a0a7-eda5a36e1484",
        "authorId" : "8b518862-583d-43a2-a10e-b33ef1b6d824",
        "body" : "Done.",
        "createdAt" : "2021-06-01T00:12:19Z",
        "updatedAt" : "2021-06-01T00:12:20Z",
        "lastEditedBy" : "8b518862-583d-43a2-a10e-b33ef1b6d824",
        "tags" : [
        ]
      }
    ],
    "commit" : "8252a6a93a05c97ed47e3174be76fe1aeb3f6567",
    "line" : 82,
    "diffHunk" : "@@ -1,1 +1880,1884 @@    override def apply(plan: LogicalPlan): LogicalPlan = plan.resolveOperatorsUpWithPruning(\n      // mayResolveAttrByAggregateExprs requires the TreePattern UNRESOLVED_ATTRIBUTE.\n      _.containsAllPatterns(AGGREGATE, UNRESOLVED_ATTRIBUTE), ruleId) {\n      case agg @ Aggregate(groups, aggs, child)\n          if allowGroupByAlias && child.resolved && aggs.forall(_.resolved) &&"
  },
  {
    "id" : "8903730d-d110-49ae-b7e2-7f55d68b05cb",
    "prId" : 32619,
    "prUrl" : "https://github.com/apache/spark/pull/32619#pullrequestreview-665297120",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2c8d308f-5ca9-48e7-967d-b575327aaa63",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "So, previously the percentage and accuracy was wrapped by non-foldable if-else which caused analysis failures:\r\n\r\n```\r\nif ((type#7 <=> cast(b as string))) 10000 else cast(null as int)\r\n```",
        "createdAt" : "2021-05-21T10:00:03Z",
        "updatedAt" : "2021-05-21T10:00:03Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "0ecab9f41dcd5a669ae105a7af9db592d4800f40",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +774,778 @@                  // ApproximatePercentile takes two literals for accuracy and percentage which\n                  // should not be wrapped by if-else.\n                  a.withNewChildren(ifExpr(a.first) :: a.second :: a.third :: Nil)\n                case a: AggregateFunction =>\n                  a.withNewChildren(a.children.map(ifExpr))"
  },
  {
    "id" : "e094b0bd-c2c5-46d6-aae9-803aef558e02",
    "prId" : 32553,
    "prUrl" : "https://github.com/apache/spark/pull/32553#pullrequestreview-661335351",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a1002da9-37e9-438f-a6e3-8a97a8d81d59",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "how about primitive type parameters?",
        "createdAt" : "2021-05-17T14:54:05Z",
        "updatedAt" : "2021-05-17T14:54:06Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "c7bd8795-c436-4391-8270-cb7287e36d15",
        "parentId" : "a1002da9-37e9-438f-a6e3-8a97a8d81d59",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "I think it shouldn't matter. Here we're making sure that the magic method will always be invoked **regardless** of null-ness of the arguments. \r\n\r\nFor primitive types this has less significant meaning because even if `progagateNull` is true, `needNullCheck`\r\n```\r\n  protected lazy val needNullCheck: Boolean = propagateNull && arguments.exists(_.nullable)\r\n```\r\nin `InvokeLike` also considers null-ness of arguments.",
        "createdAt" : "2021-05-17T18:14:30Z",
        "updatedAt" : "2021-05-17T18:14:31Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "35f6812f-a484-4ee0-b994-774169295b1a",
        "parentId" : "a1002da9-37e9-438f-a6e3-8a97a8d81d59",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "How can users handle nullable int values if the UDF is something like `isPositive(int i)` which can't accept null argument?",
        "createdAt" : "2021-05-17T18:51:06Z",
        "updatedAt" : "2021-05-17T18:51:07Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "3f71f8e0-f1b2-4e7b-8de7-2d884777b9e7",
        "parentId" : "a1002da9-37e9-438f-a6e3-8a97a8d81d59",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Ah I see what you mean, thanks. Seems we should allow users to define magic method with boxed primitive types for this case? We could also follow the behavior of [ScalaUDF](https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/ScalaUDF.scala#L32) and returns null if any of the primitive type parameter is nullable and the input is null, however currently `InvokeLike` cannot handle the case where a subset of the input types are of primitive nullable type.",
        "createdAt" : "2021-05-17T19:22:26Z",
        "updatedAt" : "2021-05-17T19:22:26Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      }
    ],
    "commit" : "fb0d959fa2ba7ced2370840e3bb9fd53321b921b",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +2205,2209 @@          case Some(m) if Modifier.isStatic(m.getModifiers) =>\n            StaticInvoke(scalarFunc.getClass, scalarFunc.resultType(),\n              MAGIC_METHOD_NAME, arguments, propagateNull = false,\n              returnNullable = scalarFunc.isResultNullable)\n          case Some(_) =>"
  },
  {
    "id" : "48420f45-9f54-409b-bfdf-ac6c4ecba605",
    "prId" : 32553,
    "prUrl" : "https://github.com/apache/spark/pull/32553#pullrequestreview-661729604",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8680767a-36f7-42b6-ab17-4a35a2c94d40",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "So I think we need `propagateNull` as true, instead of false?",
        "createdAt" : "2021-05-18T06:22:50Z",
        "updatedAt" : "2021-05-18T06:22:50Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "29295584-9452-4822-9674-83eebe7d78b4",
        "parentId" : "8680767a-36f7-42b6-ab17-4a35a2c94d40",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "If `propagateNull` is true, we'd return null directly even if input arguments are of non-primitive type, which is not what we want.",
        "createdAt" : "2021-05-18T06:43:31Z",
        "updatedAt" : "2021-05-18T06:43:31Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "cfa7ecb3-3898-4718-b27f-e3d95c9455a6",
        "parentId" : "8680767a-36f7-42b6-ab17-4a35a2c94d40",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Got it. Thanks.",
        "createdAt" : "2021-05-18T07:26:39Z",
        "updatedAt" : "2021-05-18T07:26:39Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "fb0d959fa2ba7ced2370840e3bb9fd53321b921b",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +2205,2209 @@          case Some(m) if Modifier.isStatic(m.getModifiers) =>\n            StaticInvoke(scalarFunc.getClass, scalarFunc.resultType(),\n              MAGIC_METHOD_NAME, arguments, propagateNull = false,\n              returnNullable = scalarFunc.isResultNullable)\n          case Some(_) =>"
  },
  {
    "id" : "86361c83-554b-41f5-b926-9c02bd40785f",
    "prId" : 32542,
    "prUrl" : "https://github.com/apache/spark/pull/32542#pullrequestreview-659567253",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "937bdf70-e11c-4dfc-ae28-74b98fdcc748",
        "parentId" : null,
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "This is moved from `ResolveSessionCatalog.scala`.",
        "createdAt" : "2021-05-14T06:48:56Z",
        "updatedAt" : "2021-05-14T06:49:00Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "9e058b10a3e61cfe702cab8a4b15726660e3da78",
    "line" : 31,
    "diffHunk" : "@@ -1,1 +3518,3522 @@        val normalizedChanges = a.changes.flatMap {\n          case add: AddColumn =>\n            CatalogV2Util.failNullType(add.dataType)\n            def addColumn(\n                parentSchema: StructType,"
  },
  {
    "id" : "4c6bbe32-0bee-4084-8945-8ff712da97d5",
    "prId" : 32470,
    "prUrl" : "https://github.com/apache/spark/pull/32470#pullrequestreview-681368939",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "94ce2c08-59e8-464b-b0c6-c3595c551be2",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "How about leaving some comments about what are the returned two values?",
        "createdAt" : "2021-06-11T01:06:22Z",
        "updatedAt" : "2021-06-11T01:14:46Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "b362a098978be65ab1fc033fe0213a78a467b6ae",
    "line" : 173,
    "diffHunk" : "@@ -1,1 +2485,2489 @@    def resolveExprsWithAggregate(\n        exprs: Seq[Expression],\n        agg: Aggregate): (Seq[NamedExpression], Seq[Expression]) = {\n      def resolveCol(input: Expression): Expression = {\n        input.transform {"
  },
  {
    "id" : "b98ab40d-3e13-433f-b474-5eb5a153166e",
    "prId" : 32470,
    "prUrl" : "https://github.com/apache/spark/pull/32470#pullrequestreview-682103224",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "425810fd-adb3-450c-890c-7595c508b9ba",
        "parentId" : null,
        "authorId" : "b3ba992a-312c-46eb-b3c3-8d861d15ac40",
        "body" : "Should we also take into account the aggregate expressions already added in `aggExprList`? For example\r\n```sql\r\nselect c1 from t1 group by c1 having sum(c2) > 0 and sum(c2) < 10\r\n```\r\nHere `sum(c2)` will be added to the exprList and we don't need to create another alias for the second `sum(c2)`. Maybe use a aggExpr map instead of list?\r\n",
        "createdAt" : "2021-06-11T17:45:54Z",
        "updatedAt" : "2021-06-11T17:47:31Z",
        "lastEditedBy" : "b3ba992a-312c-46eb-b3c3-8d861d15ac40",
        "tags" : [
        ]
      },
      {
        "id" : "98fddd94-fcbf-4cb4-8b9d-96db2c6f100e",
        "parentId" : "425810fd-adb3-450c-890c-7595c508b9ba",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Makes sense. The current code follows the previous one in https://github.com/apache/spark/pull/32470/files#diff-ed19f376a63eba52eea59ca71f3355d4495fad4fad4db9a3324aade0d4986a47L2502\r\n\r\nI'll do it in a follow-up as this is an improvement, and update TPCDS query golden files if needed.",
        "createdAt" : "2021-06-11T18:10:13Z",
        "updatedAt" : "2021-06-11T18:10:13Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "b362a098978be65ab1fc033fe0213a78a467b6ae",
    "line" : 290,
    "diffHunk" : "@@ -1,1 +2536,2540 @@      // Avoid adding an extra aggregate expression if it's already present in\n      // `agg.aggregateExpressions`.\n      val index = agg.aggregateExpressions.indexWhere {\n        case Alias(child, _) => child semanticEquals expr\n        case other => other semanticEquals expr"
  },
  {
    "id" : "fcd01a54-0e0a-4cb7-9595-6974ddac05a6",
    "prId" : 32470,
    "prUrl" : "https://github.com/apache/spark/pull/32470#pullrequestreview-682626763",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1b34ce8b-7b43-4981-9e19-1dd1313c2db9",
        "parentId" : null,
        "authorId" : "b3ba992a-312c-46eb-b3c3-8d861d15ac40",
        "body" : "The children of the resolved subquery that are not aggregate expressions should also be wrapped in `TempResolvedColumn` (but it seems the current logic can't handle this case with proper error messages either. This can be a follow-up in the future)\r\n```sql\r\nselect c1 from t1 group by c1 having (select sum(c2) from t2 where t1.c2 = t2.c1) > 0\r\n-- org.apache.spark.sql.AnalysisException: Resolved attribute(s) c2#10 missing from c1#9 in operator\r\n-- !Filter (scalar-subquery#8 [c2#10] > cast(0 as bigint)).;\r\n```\r\n",
        "createdAt" : "2021-06-14T08:13:35Z",
        "updatedAt" : "2021-06-14T08:14:03Z",
        "lastEditedBy" : "b3ba992a-312c-46eb-b3c3-8d861d15ac40",
        "tags" : [
        ]
      }
    ],
    "commit" : "b362a098978be65ab1fc033fe0213a78a467b6ae",
    "line" : 208,
    "diffHunk" : "@@ -1,1 +2507,2511 @@        if (SubqueryExpression.hasSubquery(input)) {\n          val fake = Project(Alias(input, \"fake\")() :: Nil, agg.child)\n          ResolveSubquery(fake).asInstanceOf[Project].projectList.head.asInstanceOf[Alias].child\n        } else {\n          input"
  },
  {
    "id" : "23976fe2-35ec-4933-a5f3-c1045ad5dc33",
    "prId" : 32407,
    "prUrl" : "https://github.com/apache/spark/pull/32407#pullrequestreview-649505451",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d52e88b0-3e2f-42ce-9b86-21a6e736ef68",
        "parentId" : null,
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Ideally we'd want to check if this method is actually static, otherwise there could be runtime error. However this only works for methods defined in Java; for Scala seems there is no easy way.",
        "createdAt" : "2021-04-30T07:23:08Z",
        "updatedAt" : "2021-05-08T01:02:45Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "3b994c6c-bf0e-490e-a87d-080018a81af5",
        "parentId" : "d52e88b0-3e2f-42ce-9b86-21a6e736ef68",
        "authorId" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "body" : "Scala doesn't have the concept of truly static methods, right? The equivalent (`object` methods) are actually just instance methods on a singleton.",
        "createdAt" : "2021-04-30T15:05:16Z",
        "updatedAt" : "2021-05-08T01:02:45Z",
        "lastEditedBy" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "tags" : [
        ]
      },
      {
        "id" : "43229227-e34d-4c1a-8e4a-a72617a6b863",
        "parentId" : "d52e88b0-3e2f-42ce-9b86-21a6e736ef68",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Yes that's correct. The `StaticInvoke` calls the static method on the non-anonymous class which just forward to the non-static method defined in anonymous/singleton Java class (i.e., the class with `$` at the end of its name). \r\n\r\nFor instance, for the `LongAddWithStatic` class, this is the method defined in `LongAddWithStaticMagic.class`:\r\n```\r\n  public static long staticInvoke(long, long);\r\n    Code:\r\n       0: getstatic     #16                 // Field org/apache/spark/sql/connector/functions/LongAddWithStaticMagic$.MODULE$:Lorg/apache/spark/sql/connector/functions/LongAddWithStaticMagic$;\r\n       3: lload_0\r\n       4: lload_2\r\n       5: invokevirtual #51                 // Method org/apache/spark/sql/connector/functions/LongAddWithStaticMagic$.staticInvoke:(JJ)J\r\n       8: lreturn\r\n```\r\n\r\nand the same method defined in the singleton class `LongAddWithStaticMagic$`:\r\n```\r\n  public long staticInvoke(long, long);\r\n    Code:\r\n       0: lload_1\r\n       1: lload_3\r\n       2: ladd\r\n       3: lreturn\r\n```\r\n\r\nSo I was expecting worse performance from Scala since it calls `invokevirtual` underneath while Java uses `invokestatic`, but the result doesn't look so. It could be that the performance is dominated by other factors.",
        "createdAt" : "2021-04-30T18:39:07Z",
        "updatedAt" : "2021-05-08T01:02:45Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "56515b10-bcc5-4d6c-bc9d-9e3e44d4623c",
        "parentId" : "d52e88b0-3e2f-42ce-9b86-21a6e736ef68",
        "authorId" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "body" : "Very interesting, thanks for the explanation!",
        "createdAt" : "2021-04-30T18:42:05Z",
        "updatedAt" : "2021-05-08T01:02:45Z",
        "lastEditedBy" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "tags" : [
        ]
      }
    ],
    "commit" : "5096fac302554f88723777ed15583ee8b4b9a4f4",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +2183,2187 @@        findMethod(scalarFunc, MAGIC_METHOD_NAME, argClasses) match {\n          case Some(m) if Modifier.isStatic(m.getModifiers) =>\n            StaticInvoke(scalarFunc.getClass, scalarFunc.resultType(),\n              MAGIC_METHOD_NAME, arguments, returnNullable = scalarFunc.isResultNullable)\n          case Some(_) =>"
  },
  {
    "id" : "f5ee18fa-62a7-4371-a256-b4e0324d37e9",
    "prId" : 32247,
    "prUrl" : "https://github.com/apache/spark/pull/32247#pullrequestreview-641802167",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5e6a6842-ac19-4cf8-ae18-7cb28eb5d2a0",
        "parentId" : null,
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "Quick question: why don't we have pattern `SubqueryExpression` ",
        "createdAt" : "2021-04-22T04:55:07Z",
        "updatedAt" : "2021-04-22T05:57:06Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "4d367747-ff52-4c99-b9ec-5a0defb15bf3",
        "parentId" : "5e6a6842-ac19-4cf8-ae18-7cb28eb5d2a0",
        "authorId" : "8b518862-583d-43a2-a10e-b33ef1b6d824",
        "body" : "PlanExpression covers both logical and physical, so that we can use it in a few physical rules too in later PRs without adding new bits.",
        "createdAt" : "2021-04-22T05:59:03Z",
        "updatedAt" : "2021-04-22T06:00:00Z",
        "lastEditedBy" : "8b518862-583d-43a2-a10e-b33ef1b6d824",
        "tags" : [
        ]
      }
    ],
    "commit" : "41270b60f210e392eb90984a000a66d9fe9e5b2a",
    "line" : 39,
    "diffHunk" : "@@ -1,1 +3793,3797 @@    plan.resolveOperatorsWithPruning(_.containsAllPatterns(PLAN_EXPRESSION, FILTER), ruleId) {\n      case f @ Filter(_, a: Aggregate) if f.resolved =>\n        f.transformExpressionsWithPruning(_.containsPattern(PLAN_EXPRESSION), ruleId) {\n          case s: SubqueryExpression if s.children.nonEmpty =>\n            // Collect the aliases from output of aggregate."
  },
  {
    "id" : "5f90b95f-a1c7-46a5-9a07-14a52b32ae31",
    "prId" : 32192,
    "prUrl" : "https://github.com/apache/spark/pull/32192#pullrequestreview-636851042",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d8bfa187-cd17-48e3-bf7e-39513ba80b5e",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "To avoid resolving `UpdateAction` again with `resolveValuesWithSourceOnly = false` in the next analysis round, I added `resolveMergeExprOrFail` to fail earlier if an attribute can't be resolved.",
        "createdAt" : "2021-04-15T15:50:04Z",
        "updatedAt" : "2021-04-15T15:50:04Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "19d77eadfb176a77c110d654b6ca9aa78ee4d6a9",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +1481,1485 @@                  updateCondition.map(resolveExpressionByPlanChildren(_, m)),\n                  // For UPDATE *, the value must from source table.\n                  resolveAssignments(assignments, m, resolveValuesWithSourceOnly = true))\n              case o => o\n            }"
  },
  {
    "id" : "9a19d56e-655b-41d5-868b-e5530ff85e3c",
    "prId" : 32170,
    "prUrl" : "https://github.com/apache/spark/pull/32170#pullrequestreview-694112028",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e338141f-3599-441f-9350-03e7572dba8f",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I doubt this. For old interval types, `Date + Interval` always returns Date:\r\n```\r\ncase (DateType, CalendarIntervalType) => DateAddInterval(l, r, ansiEnabled = f)\r\ncase (CalendarIntervalType, DateType) => DateAddInterval(r, l, ansiEnabled = f)\r\n```\r\n\r\nI think we should do this for new interval types as well, and fail at runtime if the interval has hour, minute, second fields.",
        "createdAt" : "2021-06-28T15:41:33Z",
        "updatedAt" : "2021-06-28T15:41:33Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "4a425e1452d4834c645e3627dec469fd14e7c322",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +347,351 @@        case a @ Add(l, r, f) if a.childrenResolved => (l.dataType, r.dataType) match {\n          case (DateType, DayTimeIntervalType) => TimeAdd(Cast(l, TimestampType), r)\n          case (DayTimeIntervalType, DateType) => TimeAdd(Cast(r, TimestampType), l)\n          case (DateType, YearMonthIntervalType) => DateAddYMInterval(l, r)\n          case (YearMonthIntervalType, DateType) => DateAddYMInterval(r, l)"
  },
  {
    "id" : "bd36f9ac-6608-4459-a26d-a44c02ed3a05",
    "prId" : 32135,
    "prUrl" : "https://github.com/apache/spark/pull/32135#pullrequestreview-634479988",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2112804b-c0c1-41bd-8d8b-18d2ecf11838",
        "parentId" : null,
        "authorId" : "80d631a6-73e8-46a6-a01b-a80d1f1cc6cc",
        "body" : "Does this pass scalastyle?",
        "createdAt" : "2021-04-13T07:42:34Z",
        "updatedAt" : "2021-04-13T07:42:34Z",
        "lastEditedBy" : "80d631a6-73e8-46a6-a01b-a80d1f1cc6cc",
        "tags" : [
        ]
      },
      {
        "id" : "f6077644-090c-4ab6-aa9a-a9975b1789ee",
        "parentId" : "2112804b-c0c1-41bd-8d8b-18d2ecf11838",
        "authorId" : "8b518862-583d-43a2-a10e-b33ef1b6d824",
        "body" : "It seemed ok -- I didn't see scalastyle complained anything. \r\nBut let me know if there's a convention in the Spark codebase.",
        "createdAt" : "2021-04-13T08:03:48Z",
        "updatedAt" : "2021-04-13T08:03:49Z",
        "lastEditedBy" : "8b518862-583d-43a2-a10e-b33ef1b6d824",
        "tags" : [
        ]
      },
      {
        "id" : "fb486f00-f0ba-4878-a1f6-ba835092def6",
        "parentId" : "2112804b-c0c1-41bd-8d8b-18d2ecf11838",
        "authorId" : "80d631a6-73e8-46a6-a01b-a80d1f1cc6cc",
        "body" : "Normally we put it all on a single line. If it passes then I am happy.",
        "createdAt" : "2021-04-13T08:37:52Z",
        "updatedAt" : "2021-04-13T08:37:52Z",
        "lastEditedBy" : "80d631a6-73e8-46a6-a01b-a80d1f1cc6cc",
        "tags" : [
        ]
      },
      {
        "id" : "a3c947ed-65f3-46ef-b8b9-f288f09660c6",
        "parentId" : "2112804b-c0c1-41bd-8d8b-18d2ecf11838",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "It passes..",
        "createdAt" : "2021-04-13T11:33:26Z",
        "updatedAt" : "2021-04-13T11:33:27Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "9db15f0ee51bca66e5ca66a47fd93506a421f664",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +40,44 @@import org.apache.spark.sql.catalyst.streaming.StreamingRelationV2\nimport org.apache.spark.sql.catalyst.trees.TreeNodeRef\nimport org.apache.spark.sql.catalyst.trees.TreePattern.{\n  EXPRESSION_WITH_RANDOM_SEED, NATURAL_LIKE_JOIN, WINDOW_EXPRESSION\n}"
  },
  {
    "id" : "91362692-da52-40f9-8e2d-dee26b113f30",
    "prId" : 32082,
    "prUrl" : "https://github.com/apache/spark/pull/32082#pullrequestreview-645869174",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1a375362-eba2-4825-afa4-6c91a58a8842",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "ditto, put into a new method.",
        "createdAt" : "2021-04-27T13:40:42Z",
        "updatedAt" : "2021-04-27T19:27:48Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "c18715fcc304f4a805d420e193d4a3884b3f7e4f",
    "line" : 163,
    "diffHunk" : "@@ -1,1 +2130,2134 @@                      processV2ScalarFunction(scalarFunc, inputType, arguments, isDistinct,\n                        filter, ignoreNulls)\n                    case aggFunc: V2AggregateFunction[_, _] =>\n                      processV2AggregateFunction(aggFunc, arguments, isDistinct, filter,\n                        ignoreNulls)"
  },
  {
    "id" : "68f3c0b9-cd56-4d4c-ab9e-653be9557748",
    "prId" : 31940,
    "prUrl" : "https://github.com/apache/spark/pull/31940#pullrequestreview-618579807",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5082beb8-adc7-48f1-953a-c58de324940e",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "shall we handle `case b @ BinaryComparison(OuterReference(attr: Attribute), lit) if lit.foldable` as well.",
        "createdAt" : "2021-03-23T07:35:17Z",
        "updatedAt" : "2021-03-24T13:11:14Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "a813d2f1-89fd-44cf-93e5-75af81ccf8a4",
        "parentId" : "5082beb8-adc7-48f1-953a-c58de324940e",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "done",
        "createdAt" : "2021-03-23T13:22:13Z",
        "updatedAt" : "2021-03-24T13:11:14Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "9142bfc7ea082133bd60f7dc1090c64f7835199d",
    "line" : 36,
    "diffHunk" : "@@ -1,1 +4040,4044 @@          b.withNewChildren(CharVarcharUtils.addPaddingInStringComparison(Seq(left, right)))\n\n        case b @ BinaryComparison(OuterReference(left: Attribute), right: Attribute) =>\n          b.withNewChildren(padOuterRefAttrCmp(left, right))\n"
  },
  {
    "id" : "7ff98299-bd97-4b25-aa8c-d7fdf7ef443e",
    "prId" : 31855,
    "prUrl" : "https://github.com/apache/spark/pull/31855#pullrequestreview-615655467",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2e9ae0e9-415d-4345-ab41-aa45c8df7ed7",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "shall we allow adding year-month interval as well?",
        "createdAt" : "2021-03-18T16:45:00Z",
        "updatedAt" : "2021-03-18T17:15:31Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "eec5595d-e3f3-4506-b247-ff15f5720d45",
        "parentId" : "2e9ae0e9-415d-4345-ab41-aa45c8df7ed7",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "The op is allowed already, see the test https://github.com/apache/spark/pull/31789/files#diff-bff10c8a3182aa943d8927135e0c14b02a338d9bcca94ddcd77670ee01fea0f3R2380 . It should still pass after the changes.\r\n\r\nI have to add this cases because I reuse the `TimeAdd` expression, and associated rules here.\r\n\r\nThe `year-month interval` +/- `year-month interval` expr is handled by the default case `case _ => a`/`case _ => s`.",
        "createdAt" : "2021-03-18T17:03:33Z",
        "updatedAt" : "2021-03-18T17:15:31Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "c851449fd6b76079a0e8694366e892c1833135c9",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +344,348 @@          case (YearMonthIntervalType, TimestampType) => TimestampAddYMInterval(r, l)\n          case (CalendarIntervalType, CalendarIntervalType) |\n               (DayTimeIntervalType, DayTimeIntervalType) => a\n          case (DateType, CalendarIntervalType) => DateAddInterval(l, r, ansiEnabled = f)\n          case (_, CalendarIntervalType | DayTimeIntervalType) => Cast(TimeAdd(l, r), l.dataType)"
  },
  {
    "id" : "8be0dae7-ad8a-4c52-aec3-488aadc0d98c",
    "prId" : 31855,
    "prUrl" : "https://github.com/apache/spark/pull/31855#pullrequestreview-615667004",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0f7df5c4-d997-4ef0-b72e-e3c13593b28c",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "ditto",
        "createdAt" : "2021-03-18T16:45:41Z",
        "updatedAt" : "2021-03-18T17:15:31Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "cfe6fc26-7a46-4394-a1a2-3817580b593d",
        "parentId" : "0f7df5c4-d997-4ef0-b72e-e3c13593b28c",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "The `YearMonthIntervalType` is handled by the default case `case _ => s`. We need to handle `CalendarIntervalType` and `DayTimeIntervalType` especially otherwise they will be casted to some unexpected type in the base arithmetic ops `+`/`-`",
        "createdAt" : "2021-03-18T17:14:37Z",
        "updatedAt" : "2021-03-18T17:15:31Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "c851449fd6b76079a0e8694366e892c1833135c9",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +359,363 @@            DatetimeSub(l, r, TimestampAddYMInterval(l, UnaryMinus(r, f)))\n          case (CalendarIntervalType, CalendarIntervalType) |\n               (DayTimeIntervalType, DayTimeIntervalType) => s\n          case (DateType, CalendarIntervalType) =>\n            DatetimeSub(l, r, DateAddInterval(l, UnaryMinus(r, f), ansiEnabled = f))"
  },
  {
    "id" : "92e884a5-64ba-4a4c-ac40-11910b5148c9",
    "prId" : 31758,
    "prUrl" : "https://github.com/apache/spark/pull/31758#pullrequestreview-604951965",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c8d0079a-1011-47f3-a304-f5603f8e2588",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "This change is not related to this PR, but a small followup from https://github.com/apache/spark/pull/31728#discussion_r588049629",
        "createdAt" : "2021-03-05T09:27:42Z",
        "updatedAt" : "2021-03-12T15:09:42Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "505476540dc20c480a7d7e540391c01fcf1912f7",
    "line" : 102,
    "diffHunk" : "@@ -1,1 +1976,1980 @@      },\n      resolveColumnByOrdinal = ordinal => {\n        assert(q.children.length == 1)\n        assert(ordinal >= 0 && ordinal < q.children.head.output.length)\n        q.children.head.output(ordinal)"
  },
  {
    "id" : "2b04fc93-91fa-4af5-9dbe-268752835047",
    "prId" : 31728,
    "prUrl" : "https://github.com/apache/spark/pull/31728#pullrequestreview-603779731",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a24932cf-3d59-422b-a087-e231b52b251e",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "The doc is mostly from the old `resolveExpressionTopDown`",
        "createdAt" : "2021-03-03T16:07:30Z",
        "updatedAt" : "2021-03-04T09:15:52Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "b9677cad-cb8b-4601-9e44-db55a2d99589",
        "parentId" : "a24932cf-3d59-422b-a087-e231b52b251e",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "ditto: `private`?",
        "createdAt" : "2021-03-04T07:27:18Z",
        "updatedAt" : "2021-03-04T09:15:52Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "69065d06-f748-472e-a3df-b65920dfe82a",
        "parentId" : "a24932cf-3d59-422b-a087-e231b52b251e",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "`resolveExpressionsBottomUp` was `protected[sql]`, so I make this public in case we need it in other places (in Spark extensions)",
        "createdAt" : "2021-03-04T07:33:26Z",
        "updatedAt" : "2021-03-04T09:15:52Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "7c2de54303d41c9400e8765116f53c7c863f978f",
    "line" : 331,
    "diffHunk" : "@@ -1,1 +1956,1960 @@   * @return resolved Expression.\n   */\n  def resolveExpressionByPlanChildren(\n      e: Expression,\n      q: LogicalPlan,"
  },
  {
    "id" : "ece97dc8-2a89-40ed-afd1-093483905bd2",
    "prId" : 31728,
    "prUrl" : "https://github.com/apache/spark/pull/31728#pullrequestreview-603775765",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "87b8e054-3997-4faf-8810-a79e15951a3b",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "nit: `private`?",
        "createdAt" : "2021-03-04T07:27:01Z",
        "updatedAt" : "2021-03-04T09:15:52Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "7c2de54303d41c9400e8765116f53c7c863f978f",
    "line" : 301,
    "diffHunk" : "@@ -1,1 +1926,1930 @@   * resolve it if possible.\n   */\n  def resolveExpressionByPlanOutput(\n      expr: Expression,\n      plan: LogicalPlan,"
  },
  {
    "id" : "5ccfcdeb-1c43-4dc3-80b5-3fb2411ca3bf",
    "prId" : 31728,
    "prUrl" : "https://github.com/apache/spark/pull/31728#pullrequestreview-604825231",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e0c9308b-898d-433d-ba09-148d9becdb2f",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Does `resolveExpressionTopDown` have this logic originally? Seems I cannot find it.",
        "createdAt" : "2021-03-04T08:32:13Z",
        "updatedAt" : "2021-03-04T09:15:52Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "c520d91e-bbd9-493a-b7d5-9b253262a97b",
        "parentId" : "e0c9308b-898d-433d-ba09-148d9becdb2f",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "No it doesn't, but I can't find a reason why we shouldn't have it. This doesn't hurt anyway.\r\n\r\nThe only difference should be where to resolve the columns: plan output vs plan children output. So that it's easy for developers to decide which one to call.",
        "createdAt" : "2021-03-04T09:18:14Z",
        "updatedAt" : "2021-03-04T09:18:38Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "5177831c-0d4d-4451-9a79-038b7c3967c5",
        "parentId" : "e0c9308b-898d-433d-ba09-148d9becdb2f",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Because here looks like we flatten all children outputs and let `GetColumnByOrdinal` resolve to ordinal in the flattened outputs. It doesn't like any other `GetColumnByOrdinal` usage so I have a question here. It works if the expr/query plan of `GetColumnByOrdinal` considers the ordinal correctly. \r\n\r\n",
        "createdAt" : "2021-03-04T18:35:01Z",
        "updatedAt" : "2021-03-04T18:35:02Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "6d483f99-9359-4c8a-a602-7e99ff88a885",
        "parentId" : "e0c9308b-898d-433d-ba09-148d9becdb2f",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "This is a good point. Since this logic is not really needed anywhere, maybe we can use a stricter definition first. We can say that it only works if the plan has one child and we look up the attribute from output attributes of that child.\r\n\r\nI'll update this in the followup PR (I have some other related code cleanup in mind), to avoid waiting for the QA job.",
        "createdAt" : "2021-03-05T05:52:18Z",
        "updatedAt" : "2021-03-05T05:52:19Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "7c2de54303d41c9400e8765116f53c7c863f978f",
    "line" : 345,
    "diffHunk" : "@@ -1,1 +1970,1974 @@        assert(ordinal >= 0 && ordinal < candidates.length)\n        candidates.apply(ordinal)\n      },\n      trimAlias = trimAlias,\n      throws = true)"
  },
  {
    "id" : "99c84186-cf33-448c-9847-340b0f44f0c4",
    "prId" : 31713,
    "prUrl" : "https://github.com/apache/spark/pull/31713#pullrequestreview-603711703",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "788cb01a-080c-4fb3-87bb-7a773290595d",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "good catch!",
        "createdAt" : "2021-03-04T05:18:25Z",
        "updatedAt" : "2021-03-04T05:18:25Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "8468b7f76932b917282766ee6302d54fd9b0686f",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +1401,1405 @@              // an UnresolvedAttribute.\n              EqualNullSafe(\n                UnresolvedAttribute.quoted(attr.name),\n                Cast(Literal(value), attr.dataType))\n            case None =>"
  },
  {
    "id" : "62bfc387-cdf2-4b91-a1cf-30c25808cefe",
    "prId" : 31652,
    "prUrl" : "https://github.com/apache/spark/pull/31652#pullrequestreview-605903903",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f08e8d4e-06b0-443b-a95c-82c68b320e28",
        "parentId" : null,
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "I renamed because this type of view can also be created not from a dataframe - e.g., `ALTER VIEW ... AS` with `spark.sql.legacy.storeAnalyzedPlanForView` set to true.",
        "createdAt" : "2021-03-08T00:28:16Z",
        "updatedAt" : "2021-03-10T20:37:43Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "7e4815a03d0d423ac377b6c3f7ce05f102ab0087",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +880,884 @@  private def unwrapRelationPlan(plan: LogicalPlan): LogicalPlan = {\n    EliminateSubqueryAliases(plan) match {\n      case v: View if v.isTempViewStoringAnalyzedPlan => v.child\n      case other => other\n    }"
  },
  {
    "id" : "168d0798-62d3-49ad-8647-93a2cc3e167e",
    "prId" : 31606,
    "prUrl" : "https://github.com/apache/spark/pull/31606#pullrequestreview-596662998",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "61c28140-9240-4308-91dc-30783b80e349",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "when shall we skip the check?",
        "createdAt" : "2021-02-22T16:28:33Z",
        "updatedAt" : "2021-02-23T05:51:59Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "f175a0a8-c103-4cc0-a1b3-29eaaf29cade",
        "parentId" : "61c28140-9240-4308-91dc-30783b80e349",
        "authorId" : "29c89154-22f7-40ee-b295-115a55734d6c",
        "body" : "For `UnresolvedView`, `UnresolvedTable`, `UnresolvedTableOrView`, we use `lookupTempView` to check whether the temp view exists, but it's ok to skip the checkAnalysis. For example:\r\n1. In DropView, the UnresolvedView will change to ResolvedView even the referred table is dropped.\r\n2. When resolving `UnresolvedTable`, if it is a view, should analyzer should throw `expectTableNotViewError` rather than `table or view not found`",
        "createdAt" : "2021-02-23T02:42:38Z",
        "updatedAt" : "2021-02-23T05:51:59Z",
        "lastEditedBy" : "29c89154-22f7-40ee-b295-115a55734d6c",
        "tags" : [
        ]
      },
      {
        "id" : "14d6a95c-c1da-4056-b4fd-d82ff09af950",
        "parentId" : "61c28140-9240-4308-91dc-30783b80e349",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "@imback82 This is something we should improve. For `SELECT ... FROM view` and `DROP VIEW view`, the way to lookup the view should be different.\r\n\r\nFor `SELECT ... FROM view`, we must fully resolve the view, as we need to execute it.\r\n\r\nFor `DROP VIEW view`, we only need to get the view metadata entry. The same to `ALTER VIEW ...`.\r\n\r\nI think we should change how we resolve `UnresolvedTableOrView` and `UnresolvedView`, to only get the view metadata.",
        "createdAt" : "2021-02-23T09:38:00Z",
        "updatedAt" : "2021-02-23T09:38:00Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "d234971e-40b7-4521-96c5-8aff5e435e1e",
        "parentId" : "61c28140-9240-4308-91dc-30783b80e349",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "OK, I will think about this.",
        "createdAt" : "2021-02-23T18:23:54Z",
        "updatedAt" : "2021-02-23T18:23:54Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "37c6b9714f9499608807a39a5e646630dff8c330",
    "line" : 37,
    "diffHunk" : "@@ -1,1 +924,928 @@        identifier: Seq[String],\n        isStreaming: Boolean = false,\n        performCheck: Boolean = false): Option[LogicalPlan] = {\n      // Permanent View can't refer to temp views, no need to lookup at all.\n      if (isResolvingView && !referredTempViewNames.contains(identifier)) return None"
  },
  {
    "id" : "8fbd8eea-ac74-43b7-a742-ea532e200c99",
    "prId" : 31570,
    "prUrl" : "https://github.com/apache/spark/pull/31570#pullrequestreview-614932111",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6c19e874-4611-444e-8456-8ecd9eea39e4",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "I know the logic in TimeWindowing is a bit long and you may not want to add additional complexity there, but I feel this still has to be consolidated with TimeWindowing (that said TimeWindowing needs to refactor a bit after adding logic on session window). Because there're lots of similarities and the limitation is applied altogether, like only allowing a time window should include session window, and vice versa.",
        "createdAt" : "2021-03-18T02:29:54Z",
        "updatedAt" : "2021-03-18T03:37:02Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "28c07ca0-236d-4020-aa97-1480a405f4b4",
        "parentId" : "6c19e874-4611-444e-8456-8ecd9eea39e4",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Or, at least counting windows should consider both.",
        "createdAt" : "2021-03-18T02:34:06Z",
        "updatedAt" : "2021-03-18T03:37:03Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "04ca039bdf74c3b71be5fe53574906f426f25256",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +3971,3975 @@ * [[SessionWindowExpression]] is the only column in group by.\n */\nobject ResolveSessionWindow extends Rule[LogicalPlan] {\n\n  private def hasWindowFunction(groupList: Seq[Expression]): Boolean ="
  },
  {
    "id" : "92b17f33-053c-4282-8a20-cfefedefb409",
    "prId" : 31570,
    "prUrl" : "https://github.com/apache/spark/pull/31570#pullrequestreview-614932111",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "37d90217-d92c-4325-8239-099b4cc44ea9",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "I see how to deal with meta fields (start, end) is slightly different from time window. Would the difference be bugging when we consolidate twos into one? I have no specific preference, but would like to be consistent with both so that we don't deal with differences in maintenance.\r\n\r\nThat said, let's change both (OK to do it on follow-up PR), or follow the current approach.",
        "createdAt" : "2021-03-18T02:40:25Z",
        "updatedAt" : "2021-03-18T03:37:02Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "04ca039bdf74c3b71be5fe53574906f426f25256",
    "line" : 29,
    "diffHunk" : "@@ -1,1 +3983,3987 @@  }\n\n  private final val WINDOW_COL_NAME = \"session_window\"\n\n  def apply(plan: LogicalPlan): LogicalPlan = plan.resolveOperators {"
  },
  {
    "id" : "0b28f9a7-4474-4e2b-88ca-e9d78af70b3c",
    "prId" : 31570,
    "prUrl" : "https://github.com/apache/spark/pull/31570#pullrequestreview-614932111",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "73e4a2b1-7479-4ebe-8832-587a2ff8f8c3",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "`hasWindowFunction` and `windowExpressions` should count time window as well. I guess it'll be simpler to resolve when we consolidate, but it would depend on how more complicated it will become.",
        "createdAt" : "2021-03-18T02:45:52Z",
        "updatedAt" : "2021-03-18T03:37:02Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "04ca039bdf74c3b71be5fe53574906f426f25256",
    "line" : 32,
    "diffHunk" : "@@ -1,1 +3986,3990 @@\n  def apply(plan: LogicalPlan): LogicalPlan = plan.resolveOperators {\n    case p @ Aggregate(groupingExpr, aggregateExpr, _) if hasWindowFunction(groupingExpr) =>\n      val child = p.child\n      val windowExpressions ="
  },
  {
    "id" : "bc77ff8a-4555-45de-987c-02984ca25431",
    "prId" : 31570,
    "prUrl" : "https://github.com/apache/spark/pull/31570#pullrequestreview-614932111",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "312d2a77-f9bc-48d7-b418-6422f4598d57",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "I feel this is less clearer, probably better to mention another key column(s) are required? Like \"Cannot use session_window without additional key column(s)\", or if we assume end users know about the concept of \"global aggregation\", \"Cannot apply session_window on global aggregation\".",
        "createdAt" : "2021-03-18T02:50:31Z",
        "updatedAt" : "2021-03-18T03:37:02Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "04ca039bdf74c3b71be5fe53574906f426f25256",
    "line" : 56,
    "diffHunk" : "@@ -1,1 +4010,4014 @@        val partitionExpression = groupingExpr.filterNot(hasWindowFunction)\n        if (partitionExpression.isEmpty) {\n          p.failAnalysis(\"Cannot use session_window as the only group by column.\")\n        }\n"
  },
  {
    "id" : "709e1714-9f96-462f-b7c7-b872f247ddc9",
    "prId" : 31440,
    "prUrl" : "https://github.com/apache/spark/pull/31440#pullrequestreview-584116228",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dbe10cf2-b620-4736-bffa-3d82c53bda9e",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "No matter how many meta cols we actually refer, we always add all meta cols, right?",
        "createdAt" : "2021-02-05T03:33:29Z",
        "updatedAt" : "2021-02-05T03:33:29Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "27bfe244-6c6c-4373-bda4-f608d256c380",
        "parentId" : "dbe10cf2-b620-4736-bffa-3d82c53bda9e",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Yea, it's good enough because:\r\n1. We guarantee that the outer plan will project out extra columns, so the final output schema won't change\r\n2. Column pruning will work and eventually the data source doesn't need to produce un-referenced columns.",
        "createdAt" : "2021-02-05T08:29:22Z",
        "updatedAt" : "2021-02-05T08:29:22Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "be4aefefd402637e77e50936b442ac619e14bf1d",
    "line" : 31,
    "diffHunk" : "@@ -1,1 +988,992 @@          node\n        } else {\n          val newNode = addMetadataCol(node)\n          // We should not change the output schema of the plan. We should project away the extr\n          // metadata columns if necessary."
  },
  {
    "id" : "f393a000-feee-4aad-8b49-7e80f6fc94ab",
    "prId" : 31440,
    "prUrl" : "https://github.com/apache/spark/pull/31440#pullrequestreview-584542275",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b5b7e2ae-625d-49e9-a53e-787c7ba0798c",
        "parentId" : null,
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "nit: extr -> extra",
        "createdAt" : "2021-02-05T17:15:52Z",
        "updatedAt" : "2021-02-05T17:28:53Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "be4aefefd402637e77e50936b442ac619e14bf1d",
    "line" : 32,
    "diffHunk" : "@@ -1,1 +989,993 @@        } else {\n          val newNode = addMetadataCol(node)\n          // We should not change the output schema of the plan. We should project away the extr\n          // metadata columns if necessary.\n          if (newNode.sameOutput(node)) {"
  },
  {
    "id" : "792a42d6-e92c-4a9d-9099-238dfde149ae",
    "prId" : 31429,
    "prUrl" : "https://github.com/apache/spark/pull/31429#pullrequestreview-580980878",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7f3c674d-3768-4823-a4d7-57ca97426cc8",
        "parentId" : null,
        "authorId" : "43998e22-6c2f-401d-9914-8cecf6fad929",
        "body" : "I'm wondering `MapInPandas` is fine?",
        "createdAt" : "2021-02-02T03:36:39Z",
        "updatedAt" : "2021-02-02T05:36:37Z",
        "lastEditedBy" : "43998e22-6c2f-401d-9914-8cecf6fad929",
        "tags" : [
        ]
      },
      {
        "id" : "801bf43d-6513-4d01-b8c0-4bd5294974a4",
        "parentId" : "7f3c674d-3768-4823-a4d7-57ca97426cc8",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Ah..`MapInPandas` has the same issue..I'll fix it together. Thanks!",
        "createdAt" : "2021-02-02T04:43:40Z",
        "updatedAt" : "2021-02-02T05:36:37Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "5758c7731f51b1c3ed18f8183fea19284d0f92bc",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +1406,1410 @@          Seq((oldVersion, oldVersion.copy(output = output.map(_.newInstance()))))\n\n        case oldVersion @ FlatMapCoGroupsInPandas(_, _, _, output, _, _)\n            if oldVersion.outputSet.intersect(conflictingAttributes).nonEmpty =>\n          Seq((oldVersion, oldVersion.copy(output = output.map(_.newInstance()))))"
  },
  {
    "id" : "93e7caa6-5386-4b59-8ff4-0e208391db84",
    "prId" : 31424,
    "prUrl" : "https://github.com/apache/spark/pull/31424#pullrequestreview-582028878",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7c3ba3ce-330c-4764-ab69-f5b003409d95",
        "parentId" : null,
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "@cloud-fan looks like I missed handling this in `ResolveTempViews`. I will create a follow up PR. Sorry about that.",
        "createdAt" : "2021-02-03T06:06:09Z",
        "updatedAt" : "2021-02-03T06:06:09Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "64bf918043925a792c24cae203a0a192b13d6593",
    "line" : 26,
    "diffHunk" : "@@ -1,1 +1147,1151 @@      case u @ UnresolvedTable(identifier, cmd, relationTypeMismatchHint) =>\n        lookupTableOrView(identifier).map {\n          case v: ResolvedView =>\n            throw QueryCompilationErrors.expectTableNotViewError(\n              v, cmd, relationTypeMismatchHint, u)"
  },
  {
    "id" : "dc7c5db3-6fb5-4a1d-94a0-1ec4b27837d3",
    "prId" : 31402,
    "prUrl" : "https://github.com/apache/spark/pull/31402#pullrequestreview-581512396",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2f185445-e516-4248-88ab-54782ed5ce02",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "It seems duplicate with https://github.com/apache/spark/blob/a5af58bde544985d152bd974aa18cf0bdbfad2d5/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala#L3171",
        "createdAt" : "2021-02-02T10:15:11Z",
        "updatedAt" : "2021-02-02T10:15:12Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      },
      {
        "id" : "d4dc6bfd-1a81-4399-abdf-0e0b75108912",
        "parentId" : "2f185445-e516-4248-88ab-54782ed5ce02",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> It seems duplicate with\r\n> \r\n> https://github.com/apache/spark/blob/a5af58bde544985d152bd974aa18cf0bdbfad2d5/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala#L3171\r\n\r\nYea, but logically they are not same.\r\nThis behavior should be earlier before throw exception.",
        "createdAt" : "2021-02-02T10:16:57Z",
        "updatedAt" : "2021-02-02T10:16:57Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "a15c01fe-1c51-404d-9f37-892a428b11c4",
        "parentId" : "2f185445-e516-4248-88ab-54782ed5ce02",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "This block is to use the default frame regardless of the specified frame ?",
        "createdAt" : "2021-02-02T10:21:57Z",
        "updatedAt" : "2021-02-02T10:22:27Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      },
      {
        "id" : "a281860c-f0b1-4aea-a1d8-cf68902fed6d",
        "parentId" : "2f185445-e516-4248-88ab-54782ed5ce02",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> This block is to use the default frame regardless of the specified frame ?\r\n\r\nYea..PG do like this ==",
        "createdAt" : "2021-02-02T10:45:42Z",
        "updatedAt" : "2021-02-02T10:45:43Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "a9ac20b4-bb01-4705-8767-3850c1eea57c",
        "parentId" : "2f185445-e516-4248-88ab-54782ed5ce02",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "PG is not the golden standard Spark has to follow. To me, the PG behavior looks unreasonable as it silently ignores the user input. Can we investigate more databases? I'd expect that we should fail if a user specifies window frame but the window function can only use its default frame.",
        "createdAt" : "2021-02-02T16:11:02Z",
        "updatedAt" : "2021-02-02T16:11:20Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "a5af58bde544985d152bd974aa18cf0bdbfad2d5",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +3165,3169 @@      case WindowExpression(wf: AggregateWindowFunction, s: WindowSpecDefinition)\n        if wf.frame != UnspecifiedFrame =>\n        WindowExpression(wf, s.copy(frameSpecification = wf.frame))\n      case WindowExpression(wf: WindowFunction, WindowSpecDefinition(_, _, f: SpecifiedWindowFrame))\n          if wf.frame != UnspecifiedFrame && wf.frame != f =>"
  },
  {
    "id" : "df568288-5e1b-437e-a9ac-62b3c00e60a9",
    "prId" : 31339,
    "prUrl" : "https://github.com/apache/spark/pull/31339#pullrequestreview-576123965",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4daf0939-47a6-4ba3-b529-ddcae3080d21",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "for `INSERT OVERWRITE t PARTITION (c=null) ...`, we do need to drop data if the column `c` is null, so `EqualNullSafe` is corrected here.",
        "createdAt" : "2021-01-26T08:38:13Z",
        "updatedAt" : "2021-01-26T08:38:13Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "987051d7-eb2b-4ddf-91a2-492d11904570",
        "parentId" : "4daf0939-47a6-4ba3-b529-ddcae3080d21",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "@aokolnychyi  can you also take a look?",
        "createdAt" : "2021-01-26T08:39:15Z",
        "updatedAt" : "2021-01-26T08:39:15Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "6a40120137ea6140e196873015cc5f020624f853",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1336,1340 @@              // table names at that point. because resolution happens after a future rule, create\n              // an UnresolvedAttribute.\n              EqualNullSafe(UnresolvedAttribute(attr.name), Cast(Literal(value), attr.dataType))\n            case None =>\n              throw QueryCompilationErrors.unknownStaticPartitionColError(name)"
  },
  {
    "id" : "fc51c80e-27d6-4b53-836a-0acb6ef0b951",
    "prId" : 31286,
    "prUrl" : "https://github.com/apache/spark/pull/31286#pullrequestreview-577991364",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0c4cc40a-5677-4e20-8588-9397252336cd",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Why `count(t1.*, t2.*)` is well-defined but `count(table.*)` is not? Isn't `count(t1.*, t2.*)` also expended like `count(table.*)` too?",
        "createdAt" : "2021-01-27T18:00:43Z",
        "updatedAt" : "2021-02-02T03:04:18Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "06fd018a-7ebe-49d9-b59f-8c546ccdb31e",
        "parentId" : "0c4cc40a-5677-4e20-8588-9397252336cd",
        "authorId" : "29c89154-22f7-40ee-b295-115a55734d6c",
        "body" : "Thanks @viirya for reviewing.\r\nbecause expand `count(t1.*, t2.*)` is not ambiguous, I think no one will argue that `count(t1.*, t2.*)` should equal `count(1)`. Also this usage is not supported by the common database (MySQL, oracle, pgsql), so we are not in conflict with others.\r\nbut `count(table.*)` is not same. spark SQL expands the columns, pgsql converts it to `count(1)`, MySQL, Oracle (as well as ANSI) thinks this should be disallowed.\r\nSo I think block `count(table.*)` can avoid ambiguous while keeping `count(t1.*, t2.*)` can reduce the side effects.",
        "createdAt" : "2021-01-28T01:59:58Z",
        "updatedAt" : "2021-02-02T03:04:18Z",
        "lastEditedBy" : "29c89154-22f7-40ee-b295-115a55734d6c",
        "tags" : [
        ]
      },
      {
        "id" : "e19eb5de-d1ef-4013-8863-605897387a74",
        "parentId" : "0c4cc40a-5677-4e20-8588-9397252336cd",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "How about the case `select count(t.*, t.*) from values (1, null) t(a, b)`? Btw, If the other databases don't support the syntax, `count(t1.*, t2.*)`, we still need to keep supporting it?",
        "createdAt" : "2021-01-28T02:35:16Z",
        "updatedAt" : "2021-02-02T03:04:18Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "31479a2b-9a8f-4f61-83f5-f62eeeec966d",
        "parentId" : "0c4cc40a-5677-4e20-8588-9397252336cd",
        "authorId" : "29c89154-22f7-40ee-b295-115a55734d6c",
        "body" : "Thanks @maropu  for reviewing.\r\n\r\n`select count(t.*, t.*) from values (1, null) t(a, b)` will output 0.\r\n\r\nI'm fine with blocking `count(t1.*, t2.*)` as well but since in spark, we are allowing other similar cases that other databases don't support (and not follow ANSI), it's not harmful to keep `count(t1.*, t2.*)` as one more case. After all, introducing unnecessary behavior change (blocking `count(t1.*, t2.*)`) doesn't benefit users.\r\nThe similar usages:\r\n`count(col_a, col_b)` - count multiple columns is not supported by pgsql, oracel. MySQL only support with distinct\r\n`count(struct_col.*)` - expand columns in struct data type is not supported by the mentioned databases.",
        "createdAt" : "2021-01-28T04:53:40Z",
        "updatedAt" : "2021-02-02T03:04:18Z",
        "lastEditedBy" : "29c89154-22f7-40ee-b295-115a55734d6c",
        "tags" : [
        ]
      },
      {
        "id" : "30d3ba47-8b81-4c8e-8f34-b9a51615418a",
        "parentId" : "0c4cc40a-5677-4e20-8588-9397252336cd",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Let's minimize the change for now. We can open a new PR to discuss if we should forbid other cases.",
        "createdAt" : "2021-01-28T05:09:05Z",
        "updatedAt" : "2021-02-02T03:04:18Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "19340d4fadffc4efe9d4adc823b381c2ec76d590",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +1766,1770 @@          // SPECIAL CASE: We want to block count(tblName.*) because in spark, count(tblName.*) will\n          // be expanded while count(*) will be converted to count(1). They will produce different\n          // results and confuse users if there is any null values. For count(t1.*, t2.*), it is\n          // still allowed, since it's well-defined in spark.\n          if (!conf.allowStarWithSingleTableIdentifierInCount &&"
  },
  {
    "id" : "7000430b-f59b-4e3b-9c85-b5e78ff790eb",
    "prId" : 31273,
    "prUrl" : "https://github.com/apache/spark/pull/31273#pullrequestreview-595163348",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7d0d5525-7322-4117-b5f7-8baa210d6cd5",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "nit: `...forall(conf.resolver)`",
        "createdAt" : "2021-02-19T08:07:19Z",
        "updatedAt" : "2021-02-24T04:37:19Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "f7ef99cf-d663-4ccb-b9c9-355b9f41ac4f",
        "parentId" : "7d0d5525-7322-4117-b5f7-8baa210d6cd5",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "That doesn't compile?\r\n```\r\n[error]  found   : org.apache.spark.sql.catalyst.analysis.Resolver\r\n[error]     (which expands to)  (String, String) => Boolean\r\n[error]  required: ((String, String)) => Boolean\r\n[error]       (n.length == nameParts.length) && n.zip(nameParts).forall(resolver)\r\n```",
        "createdAt" : "2021-02-19T21:46:17Z",
        "updatedAt" : "2021-02-24T04:37:19Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      },
      {
        "id" : "c265e89d-047e-41a9-96ea-0dda91daedcb",
        "parentId" : "7d0d5525-7322-4117-b5f7-8baa210d6cd5",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "ok let's leave it for now.",
        "createdAt" : "2021-02-22T09:41:08Z",
        "updatedAt" : "2021-02-24T04:37:19Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "82d58baa98235f912141b2b40cdcd3bf0bdbe744",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +864,868 @@  private def isReferredTempViewName(nameParts: Seq[String]): Boolean = {\n    AnalysisContext.get.referredTempViewNames.exists { n =>\n      (n.length == nameParts.length) && n.zip(nameParts).forall {\n        case (a, b) => resolver(a, b)\n      }"
  },
  {
    "id" : "621a1e12-cb4f-4a4d-a3cd-85b0a13992e9",
    "prId" : 30943,
    "prUrl" : "https://github.com/apache/spark/pull/30943#pullrequestreview-559860505",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7f768f8d-8d32-41eb-8905-ed2b32453906",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "we need to add a catch-all case at the end.",
        "createdAt" : "2020-12-30T05:10:53Z",
        "updatedAt" : "2020-12-30T06:46:08Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "1ae39b51-5611-4eaf-8a97-3854b0c46457",
        "parentId" : "7f768f8d-8d32-41eb-8905-ed2b32453906",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "OK",
        "createdAt" : "2020-12-30T06:00:05Z",
        "updatedAt" : "2020-12-30T06:46:08Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "4cc5eb1c6c4ee31186dca059154621753b0ee62f",
    "line" : 42,
    "diffHunk" : "@@ -1,1 +2144,2148 @@                        lead.copy(ignoreNulls = ignoreNulls)\n                      case lag: Lag =>\n                        lag.copy(ignoreNulls = ignoreNulls)\n                      case _ =>\n                        throw QueryCompilationErrors.ignoreNullsWithUnsupportedFunctionError("
  },
  {
    "id" : "d0decaea-c4d4-47a3-99e4-fc8e994c33dd",
    "prId" : 30943,
    "prUrl" : "https://github.com/apache/spark/pull/30943#pullrequestreview-559927009",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0abea857-61ee-4210-a9ce-739c4bbf9f35",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "not related to this PR, but seems better to report error separately for DISTINCT and FILTER, like IGNORE NULLS",
        "createdAt" : "2020-12-30T06:13:23Z",
        "updatedAt" : "2020-12-30T06:46:08Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "7aa08493-42a9-4a39-a10a-63868bb11e35",
        "parentId" : "0abea857-61ee-4210-a9ce-739c4bbf9f35",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "I got it.",
        "createdAt" : "2020-12-30T06:46:52Z",
        "updatedAt" : "2020-12-30T06:46:52Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "4cc5eb1c6c4ee31186dca059154621753b0ee62f",
    "line" : 69,
    "diffHunk" : "@@ -1,1 +2170,2174 @@                  }\n                // This function is not an aggregate function, just return the resolved one.\n                case other if (isDistinct || filter.isDefined) =>\n                  throw QueryCompilationErrors.distinctOrFilterOnlyWithAggregateFunctionError(\n                    other.prettyName)"
  },
  {
    "id" : "f5e92ebe-7aab-44ef-a7b5-0a4c80327f17",
    "prId" : 30781,
    "prUrl" : "https://github.com/apache/spark/pull/30781#pullrequestreview-604800572",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "459a92f4-605e-4dfc-8d59-4184b34818b8",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "what if `grouping__id` refers to an actual column (e.g. from a table) in these operators?",
        "createdAt" : "2021-03-02T13:32:55Z",
        "updatedAt" : "2021-03-02T13:32:55Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "bc989881-6ffb-4c83-8d20-4e91548d6fb2",
        "parentId" : "459a92f4-605e-4dfc-8d59-4184b34818b8",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "It will throw an analysis exception both after/before this PR, e.g.,\r\n```\r\nscala> sql(\"select * from t3\").show()\r\n+---+---+------------+\r\n|  a|  b|grouping__id|\r\n+---+---+------------+\r\n|  1|  2|           3|\r\n+---+---+------------+\r\n\r\n// with/without this PR\r\nscala> sql(\"select * from t3 where grouping__id > 0\").show()\r\norg.apache.spark.sql.AnalysisException: grouping()/grouping_id() can only be used with GroupingSets/Cube/Rollup;\r\n```\r\nOn the other hand, this PR will change behaviours other than these operators above, e.g.,\r\n```\r\n// The current behaviour\r\nscala> sql(\"select grouping__id from t3\").show()\r\norg.apache.spark.sql.AnalysisException: grouping_id() can only be used with GroupingSets/Cube/Rollup;;\r\n\r\n// w/ this PR\r\nscala> sql(\"select grouping__id from t3\").show()\r\n+------------+\r\n|grouping__id|\r\n+------------+\r\n|           3|\r\n+------------+\r\n```",
        "createdAt" : "2021-03-03T23:51:36Z",
        "updatedAt" : "2021-03-03T23:51:36Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "13297229-4439-420b-aa2f-27b68a10e65b",
        "parentId" : "459a92f4-605e-4dfc-8d59-4184b34818b8",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "This reminds me of `Analyzer.resolveLiteralFunction`. For these identifier-like functions, we should try to resolve them as columns first.\r\n\r\nPersonally, I find the identifier-like functions a very tricky syntax. `CURRENT_DATE` etc. is from the SQL standard so I'm fine with it, plus we treat `CURRENT_DATE` as a reserved keyword under ANSI mode. `grouping__id` is not standard and I'm wondering if we should just forbid it (with a legacy config). Users must use `grouping__id()`.",
        "createdAt" : "2021-03-04T07:30:06Z",
        "updatedAt" : "2021-03-04T07:30:07Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "1cc69ace-69de-46d0-b9a8-d3bc3afc3398",
        "parentId" : "459a92f4-605e-4dfc-8d59-4184b34818b8",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "IIUC `grouping__id` exists for hive interoperability? Anyway, the idea looks reasonable. I'll make a PR to follow it.",
        "createdAt" : "2021-03-05T04:37:50Z",
        "updatedAt" : "2021-03-05T04:37:50Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "36a23057-b474-4736-bbbe-197f250cf0b6",
        "parentId" : "459a92f4-605e-4dfc-8d59-4184b34818b8",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "https://github.com/apache/spark/pull/31751",
        "createdAt" : "2021-03-05T04:38:13Z",
        "updatedAt" : "2021-03-05T04:38:13Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "ebddbb219270a3d4df652f0942145b88e0c000dc",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +2101,2105 @@      case q: LogicalPlan =>\n        val isGroupingIdAllowed = q match {\n          case _: Aggregate | _: GroupingSets | _: Filter | _: Sort => true\n          case _ => false\n        }"
  },
  {
    "id" : "dcebde93-8fc5-40c2-ac07-bfd093a2c098",
    "prId" : 30703,
    "prUrl" : "https://github.com/apache/spark/pull/30703#pullrequestreview-549978470",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "244da3fd-6bcc-40e9-b8d0-314110a104b3",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "shall we just write `case e if !e.deterministic => e`?",
        "createdAt" : "2020-12-11T06:52:26Z",
        "updatedAt" : "2020-12-11T10:30:05Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "aa8e0842-5699-40d5-be4f-5eea9dde853b",
        "parentId" : "244da3fd-6bcc-40e9-b8d0-314110a104b3",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "First time, I tried this [33eda3c](https://github.com/apache/spark/pull/30703/commits/33eda3cb2575f1d11f013b87cd4196b7668db8ce), but the behavior is unexpected.\r\n\r\nLet's say we have a sql `select * from t order by rand()`, then it will pull out `SortOrder(rand())` into `Project`.\r\n",
        "createdAt" : "2020-12-11T07:25:32Z",
        "updatedAt" : "2020-12-11T10:30:05Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      },
      {
        "id" : "51c5f958-5447-4206-bee6-abeeb9de19f6",
        "parentId" : "244da3fd-6bcc-40e9-b8d0-314110a104b3",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "ah, so we need to collect leaf non-deterministic nodes here. Shall we create a trait for them?",
        "createdAt" : "2020-12-11T09:16:13Z",
        "updatedAt" : "2020-12-11T10:30:05Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "6b30f304-bb96-4783-86e0-9b4deb048c2c",
        "parentId" : "244da3fd-6bcc-40e9-b8d0-314110a104b3",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "I'd like to add a `LeafNondeterministic`, but seems there is not only leaf node, like `Rand(child)`.",
        "createdAt" : "2020-12-11T09:34:37Z",
        "updatedAt" : "2020-12-11T10:30:05Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      },
      {
        "id" : "c1853124-74ec-4f3c-a6d1-253686b4c8df",
        "parentId" : "244da3fd-6bcc-40e9-b8d0-314110a104b3",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Can we make `CallMethodViaReflection` to extend `Nondeterministic`? I think `UserDefinedExpression` is the only exception that its deterministic is a flag.",
        "createdAt" : "2020-12-11T10:23:25Z",
        "updatedAt" : "2020-12-11T10:30:05Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "83758db4-1cf5-4ebe-b55a-d9f7aa20e698",
        "parentId" : "244da3fd-6bcc-40e9-b8d0-314110a104b3",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "ok. try it.",
        "createdAt" : "2020-12-11T10:30:51Z",
        "updatedAt" : "2020-12-11T10:30:51Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      }
    ],
    "commit" : "9ec0654af4795d74dbd594cd02c921b727433abf",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +2966,2970 @@        val leafNondeterministic = expr.collect {\n          case n: Nondeterministic => n\n          case udf: UserDefinedExpression if !udf.deterministic => udf\n        }\n        leafNondeterministic.distinct.map { e =>"
  },
  {
    "id" : "a4d8810b-45ac-4d97-9ca6-85378c309db5",
    "prId" : 30598,
    "prUrl" : "https://github.com/apache/spark/pull/30598#pullrequestreview-550674000",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3bb4d110-8493-4a8c-a8ed-e7f3d0fe9727",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "nit: the name should be `relation` instead of  `v2Relation`",
        "createdAt" : "2020-12-11T12:37:02Z",
        "updatedAt" : "2020-12-11T12:37:02Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "e5458694-8045-4c5e-b0c6-34d7c3fad49f",
        "parentId" : "3bb4d110-8493-4a8c-a8ed-e7f3d0fe9727",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "Thanks, will address in the next PR.",
        "createdAt" : "2020-12-12T01:29:17Z",
        "updatedAt" : "2020-12-12T01:29:18Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "7baa81d3e0b22cb853c07b67511b42301f06a6f1",
    "line" : 29,
    "diffHunk" : "@@ -1,1 +1103,1107 @@      case c @ CacheTable(u @ UnresolvedRelation(_, _, false), _, _, _) =>\n        lookupRelation(u.multipartIdentifier, u.options, false)\n          .map(v2Relation => c.copy(table = v2Relation))\n          .getOrElse(c)\n"
  },
  {
    "id" : "7422d6c9-6d0f-4f87-92ce-4fcb1f386382",
    "prId" : 30461,
    "prUrl" : "https://github.com/apache/spark/pull/30461#pullrequestreview-540413328",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bb92f22e-9af5-4c59-a73e-4111a2d32a7d",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "`'` at the end is not needed. I will remove it in this PR: https://github.com/apache/spark/pull/30398",
        "createdAt" : "2020-11-28T08:45:34Z",
        "updatedAt" : "2020-11-28T08:45:48Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "0d69059b-222f-43fb-85df-8aabbe7bca41",
        "parentId" : "bb92f22e-9af5-4c59-a73e-4111a2d32a7d",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "Thanks @MaxGekk!",
        "createdAt" : "2020-11-28T19:43:36Z",
        "updatedAt" : "2020-11-28T19:43:36Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "45223a8446354ed99e888796768afdf20862ad66",
    "line" : 31,
    "diffHunk" : "@@ -1,1 +1082,1086 @@          case v: ResolvedView =>\n            val viewStr = if (v.isTemp) \"temp view\" else \"view\"\n            u.failAnalysis(s\"${v.identifier.quoted} is a $viewStr. '$cmd' expects a table.'\")\n          case table => table\n        }.getOrElse(u)"
  },
  {
    "id" : "b7bfee21-114b-4b7c-9c23-65e3d301de06",
    "prId" : 30412,
    "prUrl" : "https://github.com/apache/spark/pull/30412#pullrequestreview-534135444",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "39fa304b-11a7-4d33-b6cd-487f1443e7e5",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "We remove the char/varchar metadata after length check expressions are added, so that we don't do it repeatedly and this rule is idempotent.",
        "createdAt" : "2020-11-18T16:30:30Z",
        "updatedAt" : "2020-11-27T10:45:20Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "475fb8f2-7d66-4249-aefb-ad882625b49c",
        "parentId" : "39fa304b-11a7-4d33-b6cd-487f1443e7e5",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Does the current implementation assume the analyzer removes the metadata in plans before the optimizer phase? If so, how about checking if plans don't have the metadata in `CheckAnalysis`?",
        "createdAt" : "2020-11-19T05:49:33Z",
        "updatedAt" : "2020-11-27T10:45:20Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "c0fd50b8-4f5a-44fc-889a-0571a479a847",
        "parentId" : "39fa304b-11a7-4d33-b6cd-487f1443e7e5",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "No it doesn't. Metadata is fine as it's harmless. We only need to watch out for some specific rules that look at the char/varchar metadata, and make sure they are idempotent.\r\n\r\nAs a fact, the added cast and length check expression is wrapped by an `Alias` which retains char/varchar metadata. So the output attributes of `Project` above the v2 relation still have metadata. It's necessary as we need to rely on it later to do padding for char type column comparison.",
        "createdAt" : "2020-11-19T07:16:27Z",
        "updatedAt" : "2020-11-27T10:45:20Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "73b99dc7a98f5d0673d309adba457cd19144be92",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +3100,3104 @@          val cleanedTable = v2Write.table match {\n            case r: DataSourceV2Relation =>\n              r.copy(output = r.output.map(CharVarcharUtils.cleanAttrMetadata))\n            case other => other\n          }"
  },
  {
    "id" : "d1ce5782-d801-4464-ae28-d838f282c0d0",
    "prId" : 30212,
    "prUrl" : "https://github.com/apache/spark/pull/30212#pullrequestreview-623326039",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3552c254-46a6-41ba-8d85-31e8770612b3",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "where do we fail if there are more than one `GroupingSet`? do we have test for it?",
        "createdAt" : "2021-03-29T14:21:14Z",
        "updatedAt" : "2021-03-30T05:55:50Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "e3364d96-e2f0-4cfa-bcf5-22e084ba38f5",
        "parentId" : "3552c254-46a6-41ba-8d85-31e8770612b3",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> where do we fail if there are more than one `GroupingSet`? do we have test for it?\r\n\r\nRefer to https://issues.apache.org/jira/browse/SPARK-33229\r\nAdd UT in end-to-end test",
        "createdAt" : "2021-03-29T14:56:39Z",
        "updatedAt" : "2021-03-30T05:55:50Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "bfc03afaf9fc0974cb218b3b3121829ae9e5c413",
    "line" : 63,
    "diffHunk" : "@@ -1,1 +617,621 @@        // For CUBE/ROLLUP expressions, to avoid resolving repeatedly, here we delete them from\n        // groupingExpressions for condition resolving.\n        case a @ Aggregate(Seq(gs: GroupingSet), _, _) =>\n          a.copy(groupingExpressions =\n            getFinalGroupByExpressions(gs.groupingSets, gs.groupByExprs))"
  },
  {
    "id" : "1c432284-f8d2-4ccb-a6c6-37976d9cc6e9",
    "prId" : 30145,
    "prUrl" : "https://github.com/apache/spark/pull/30145#pullrequestreview-631002302",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1b08bc25-3e91-4739-a432-f2fd96953b27",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Could you add tests for this code path?",
        "createdAt" : "2021-04-08T02:38:00Z",
        "updatedAt" : "2021-04-08T05:59:11Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "7bd4a84f-8ce2-4003-af6b-01e552f1624c",
        "parentId" : "1b08bc25-3e91-4739-a432-f2fd96953b27",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> Could you add tests for this code path?\r\n\r\nYea",
        "createdAt" : "2021-04-08T05:58:58Z",
        "updatedAt" : "2021-04-08T05:59:11Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "8d9a3928aaee57aeafb979f41dba4b0fe6c85d40",
    "line" : 30,
    "diffHunk" : "@@ -1,1 +1807,1811 @@          aggs(index - 1)\n        } else {\n          throw QueryCompilationErrors.groupByPositionRangeError(index, aggs.size, ordinal)\n        }\n      case gs: BaseGroupingSets =>"
  },
  {
    "id" : "bea7d53e-5eb4-416b-9838-dd5002c7cfbf",
    "prId" : 30029,
    "prUrl" : "https://github.com/apache/spark/pull/30029#pullrequestreview-509103370",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "488b8062-da83-432b-ba82-19878306ad6b",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "ah this is a good catch! do we have this bug in 3.0 and 2.4?",
        "createdAt" : "2020-10-15T06:38:59Z",
        "updatedAt" : "2020-10-15T06:39:00Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "cd8ee702-b2bb-41a5-bc9e-8dfe8fc7d2f0",
        "parentId" : "488b8062-da83-432b-ba82-19878306ad6b",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "Yeah, seems it's a long time bug.",
        "createdAt" : "2020-10-15T07:45:31Z",
        "updatedAt" : "2020-10-15T07:45:31Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      }
    ],
    "commit" : "a4c2028cf1fa7a785e9a8f728fbbd2e5d0d09801",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +471,475 @@    private def constructGroupByAlias(groupByExprs: Seq[Expression]): Seq[Alias] = {\n      groupByExprs.map {\n        case e: NamedExpression => Alias(e, e.name)(qualifier = e.qualifier)\n        case other => Alias(other, other.toString)()\n      }"
  },
  {
    "id" : "78e7b815-ce81-4424-b7e9-62eb6a22d2ea",
    "prId" : 29970,
    "prUrl" : "https://github.com/apache/spark/pull/29970#pullrequestreview-504492987",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a6b8009b-74bf-4c26-8a6e-22916160f8ec",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Probably we can leave TODO to handle streaming writer case later, with mentioning SPARK-27484.",
        "createdAt" : "2020-10-08T07:22:24Z",
        "updatedAt" : "2020-11-06T15:45:25Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "be50b21524cf5b96f1c53561370f18b1490b8338",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +866,870 @@          case UnresolvedRelation(ident, _, false) =>\n            lookupTempView(ident).map(EliminateSubqueryAliases(_)).map {\n              case r: DataSourceV2Relation => write.withNewTable(r)\n              case _ => throw new AnalysisException(\"Cannot write into temp view \" +\n                s\"${ident.quoted} as it's not a data source v2 relation.\")"
  },
  {
    "id" : "07dd5267-05bf-4756-9596-ef2905694a88",
    "prId" : 29970,
    "prUrl" : "https://github.com/apache/spark/pull/29970#pullrequestreview-504492987",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5195f3c0-f8fc-4458-91ff-8900fefa995a",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "ditto",
        "createdAt" : "2020-10-08T07:22:33Z",
        "updatedAt" : "2020-11-06T15:45:25Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "be50b21524cf5b96f1c53561370f18b1490b8338",
    "line" : 25,
    "diffHunk" : "@@ -1,1 +957,961 @@      case write: V2WriteCommand =>\n        write.table match {\n          case u: UnresolvedRelation if !u.isStreaming =>\n            lookupV2Relation(u.multipartIdentifier, u.options, false).map {\n              case r: DataSourceV2Relation => write.withNewTable(r)"
  },
  {
    "id" : "3d65c39f-9b02-4761-8493-7c3cbb6cc303",
    "prId" : 29970,
    "prUrl" : "https://github.com/apache/spark/pull/29970#pullrequestreview-504492987",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "277c07ff-9f1d-4889-a5ef-4be4a9e5273b",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "ditto",
        "createdAt" : "2020-10-08T07:22:45Z",
        "updatedAt" : "2020-11-06T15:45:25Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "be50b21524cf5b96f1c53561370f18b1490b8338",
    "line" : 44,
    "diffHunk" : "@@ -1,1 +1046,1050 @@      case write: V2WriteCommand =>\n        write.table match {\n          case u: UnresolvedRelation if !u.isStreaming =>\n            lookupRelation(u.multipartIdentifier, u.options, false)\n              .map(EliminateSubqueryAliases(_))"
  },
  {
    "id" : "3c019519-1ad0-44ab-9513-97f308b6a2eb",
    "prId" : 29756,
    "prUrl" : "https://github.com/apache/spark/pull/29756#pullrequestreview-494397853",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a7fd4391-c390-48dc-8da0-97e755a2ebbb",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "ditto",
        "createdAt" : "2020-09-21T15:48:53Z",
        "updatedAt" : "2020-09-24T07:05:24Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "b26cf780-dfd0-42bb-b89e-4c52d9e609a4",
        "parentId" : "a7fd4391-c390-48dc-8da0-97e755a2ebbb",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "Done in fad1976",
        "createdAt" : "2020-09-23T08:00:13Z",
        "updatedAt" : "2020-09-24T07:05:24Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "d2eb23fedc024b2e88b35baa777363348f9fffb0",
    "line" : 157,
    "diffHunk" : "@@ -1,1 +1070,1074 @@                    Some(catalog), Some(ident), v1Fallback))\n              } else {\n                SubqueryAlias(\n                  catalog.name +: ident.asMultipartIdentifier,\n                  DataSourceV2Relation.create(table, Some(catalog), Some(ident), options))"
  },
  {
    "id" : "38408209-ec46-416c-83da-bb5dacbeb3ce",
    "prId" : 29756,
    "prUrl" : "https://github.com/apache/spark/pull/29756#pullrequestreview-495206683",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "904b588a-43cd-4bbf-9c0f-8aff17d2a9cc",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "what happens for views? shall we fail here if `v1Table` is a view before we have a good story?",
        "createdAt" : "2020-09-23T11:12:44Z",
        "updatedAt" : "2020-09-24T07:05:24Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "5348374f-cead-43c2-87d8-8ba4876c906c",
        "parentId" : "904b588a-43cd-4bbf-9c0f-8aff17d2a9cc",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "Sure, done in 97761d2.",
        "createdAt" : "2020-09-24T04:14:13Z",
        "updatedAt" : "2020-09-24T07:05:24Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "d2eb23fedc024b2e88b35baa777363348f9fffb0",
    "line" : 136,
    "diffHunk" : "@@ -1,1 +1052,1056 @@                    \"`DataStreamReader.table` yet.\")\n                }\n                SubqueryAlias(\n                  catalog.name +: ident.asMultipartIdentifier,\n                  UnresolvedCatalogRelation(v1Table.v1Table, options, isStreaming = true))"
  },
  {
    "id" : "afe3ed71-667f-4b89-8269-23b91cc94b09",
    "prId" : 29485,
    "prUrl" : "https://github.com/apache/spark/pull/29485#pullrequestreview-477170906",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2734e081-4255-46be-8d01-362bbdc3f54d",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "I rewrote the existing `rewritePlan` a bit, then just reused it for `WidenSetOperationTypes `. Does this udpate satisfy your intention? https://github.com/apache/spark/pull/29485#issuecomment-680673887",
        "createdAt" : "2020-08-28T00:42:29Z",
        "updatedAt" : "2020-08-28T06:25:43Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "6372515769c1d3c299b865d4ec4b8378bdc5e84c",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +135,139 @@   *         the given `plan` for rewriting it.\n   */\n  def rewritePlan(plan: LogicalPlan, rewritePlanMap: Map[LogicalPlan, LogicalPlan])\n    : (LogicalPlan, Seq[(Attribute, Attribute)]) = {\n    if (plan.resolved) {"
  },
  {
    "id" : "2d283dc7-316d-4b27-b485-cf9d16215151",
    "prId" : 29198,
    "prUrl" : "https://github.com/apache/spark/pull/29198#pullrequestreview-455103722",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8a225689-6eb2-48fc-8ba0-3d9323843169",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "If this method is only used here, can we define it here?",
        "createdAt" : "2020-07-24T04:40:25Z",
        "updatedAt" : "2020-07-28T16:26:35Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "9aa60dff-96b0-4bce-8320-1ed4381ac48d",
        "parentId" : "8a225689-6eb2-48fc-8ba0-3d9323843169",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "This will be used by `CREATE FUNCTION` when I revert the change.",
        "createdAt" : "2020-07-24T18:34:43Z",
        "updatedAt" : "2020-07-28T16:26:35Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "c662619bd00e74d4f683fcb565868bf72e4890d6",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1940,1944 @@      // Resolve functions with concrete relations from v2 catalog.\n      case UnresolvedFunc(multipartIdent) =>\n        val funcIdent = parseSessionCatalogFunctionIdentifier(multipartIdent)\n        ResolvedFunc(Identifier.of(funcIdent.database.toArray, funcIdent.funcName))\n"
  },
  {
    "id" : "f92e574e-9572-4fa5-bd67-07824eb83a15",
    "prId" : 29166,
    "prUrl" : "https://github.com/apache/spark/pull/29166#pullrequestreview-452126183",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6a077a3a-a4c6-45da-b10c-c01d6915fd33",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "This rewriting is much more surgical than before, +1",
        "createdAt" : "2020-07-21T04:57:51Z",
        "updatedAt" : "2020-07-23T14:11:53Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "0f3a6c448403df3c5d57b99d5675fdeca0a26729",
    "line" : 42,
    "diffHunk" : "@@ -1,1 +1255,1259 @@    }\n\n    private def rewritePlan(plan: LogicalPlan, conflictPlanMap: Map[LogicalPlan, LogicalPlan])\n      : (LogicalPlan, Seq[(Attribute, Attribute)]) = {\n      if (conflictPlanMap.contains(plan)) {"
  },
  {
    "id" : "26a6f1b4-18a7-4369-94f6-2bcab43d019b",
    "prId" : 29166,
    "prUrl" : "https://github.com/apache/spark/pull/29166#pullrequestreview-452222765",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d4039251-760b-40ff-9ddb-6fbd854da476",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Same reason as above? Add a simple comment too?",
        "createdAt" : "2020-07-21T06:53:24Z",
        "updatedAt" : "2020-07-23T14:11:53Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "cf2d46c8-4c06-4966-9c97-3709a319f522",
        "parentId" : "d4039251-760b-40ff-9ddb-6fbd854da476",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Yes, sure.",
        "createdAt" : "2020-07-21T08:09:20Z",
        "updatedAt" : "2020-07-23T14:11:53Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "0f3a6c448403df3c5d57b99d5675fdeca0a26729",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +1207,1211 @@\n        // We don't search the child plan recursively for the same reason as the above Project.\n        case _ @ Aggregate(_, aggregateExpressions, _)\n          if findAliases(aggregateExpressions).size == aggregateExpressions.size =>\n          Nil"
  },
  {
    "id" : "48851bf1-55a4-4d43-8ced-0b95194e35dc",
    "prId" : 29166,
    "prUrl" : "https://github.com/apache/spark/pull/29166#pullrequestreview-452227496",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "84396a3e-5afc-4d98-93e9-ff3b81583799",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Don't we need to put this before previous `Project` pattern?",
        "createdAt" : "2020-07-21T06:54:04Z",
        "updatedAt" : "2020-07-23T14:11:53Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "22b1320e-c463-4d5a-8d61-df7edbc3884f",
        "parentId" : "84396a3e-5afc-4d98-93e9-ff3b81583799",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "I think we don't. It's mutually exclusive with the previous one as it has an implicit requirement ( `findAliases(projectList).intersect(conflictingAttributes).isEmpty`). Therefore, the actual requirement here is:\r\n`findAliases(projectList).intersect(conflictingAttributes).isEmpty && findAliases(projectList).size == projectList.size`",
        "createdAt" : "2020-07-21T08:15:40Z",
        "updatedAt" : "2020-07-23T14:11:53Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "0f3a6c448403df3c5d57b99d5675fdeca0a26729",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +1199,1203 @@        case _ @ Project(projectList, _)\n          if findAliases(projectList).size == projectList.size =>\n          Nil\n\n        case oldVersion @ Aggregate(_, aggregateExpressions, _)"
  },
  {
    "id" : "d8513116-d331-483e-b886-07cda9a7b430",
    "prId" : 29166,
    "prUrl" : "https://github.com/apache/spark/pull/29166#pullrequestreview-453104989",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f9e907b0-a68f-4b0a-aaa8-ceedb766ac9d",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "nit: `conflictPlanMap(plan) -> plan.output.zip(newRelation.output)`",
        "createdAt" : "2020-07-21T10:45:36Z",
        "updatedAt" : "2020-07-23T14:11:53Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "f94ecc08-f571-4d1c-b83a-2c4601dfdb46",
        "parentId" : "f9e907b0-a68f-4b0a-aaa8-ceedb766ac9d",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Do you mean `conflictPlanMap(plan) -> plan.output.zip(conflictPlanMap(plan).output)`?",
        "createdAt" : "2020-07-21T16:38:55Z",
        "updatedAt" : "2020-07-23T14:11:53Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "f4dd630e-5b57-4407-ab71-d867511c4ca1",
        "parentId" : "f9e907b0-a68f-4b0a-aaa8-ceedb766ac9d",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "ah i see, nvm.",
        "createdAt" : "2020-07-22T08:30:14Z",
        "updatedAt" : "2020-07-23T14:11:53Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "0f3a6c448403df3c5d57b99d5675fdeca0a26729",
    "line" : 49,
    "diffHunk" : "@@ -1,1 +1262,1266 @@        // attributes for the parent node.\n        val newRelation = conflictPlanMap(plan)\n        newRelation -> plan.output.zip(newRelation.output)\n      } else {\n        val attrMapping = new mutable.ArrayBuffer[(Attribute, Attribute)]()"
  },
  {
    "id" : "c6e1f0be-a5a8-4f4b-89a5-c5fd81c5a58d",
    "prId" : 29166,
    "prUrl" : "https://github.com/apache/spark/pull/29166#pullrequestreview-453085376",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e3b36cd3-8806-45c0-ac89-93bae4efb5d9",
        "parentId" : null,
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "When the query from [SPARK-32280](https://issues.apache.org/jira/browse/SPARK-32280) is run, I believe this assert is hit. Should this be an assert or should we throw an AnalysisException?",
        "createdAt" : "2020-07-21T18:15:15Z",
        "updatedAt" : "2020-07-23T14:11:53Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      },
      {
        "id" : "032dd15c-6f09-4691-a977-a590609ffcf0",
        "parentId" : "e3b36cd3-8806-45c0-ac89-93bae4efb5d9",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Thanks for the reference. Yeah, it hits the assert. I'm looking into it.",
        "createdAt" : "2020-07-22T08:03:31Z",
        "updatedAt" : "2020-07-23T14:11:53Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "0f3a6c448403df3c5d57b99d5675fdeca0a26729",
    "line" : 72,
    "diffHunk" : "@@ -1,1 +1285,1289 @@          assert(!attrMapping.groupBy(_._1.exprId)\n            .exists(_._2.map(_._2.exprId).distinct.length > 1),\n            \"Found duplicate rewrite attributes\")\n          val attributeRewrites = AttributeMap(attrMapping)\n          // Using attrMapping from the children plans to rewrite their parent node."
  },
  {
    "id" : "8fd50c3f-2116-43bc-a3b4-288bb9e35b07",
    "prId" : 29166,
    "prUrl" : "https://github.com/apache/spark/pull/29166#pullrequestreview-453819175",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d33febb7-6fbd-416e-871b-f0f3458450bd",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "the parents of `plan` can only reference `plan.outputSet`, that's why I think only `plan.outputSet` is necessary.",
        "createdAt" : "2020-07-23T03:37:06Z",
        "updatedAt" : "2020-07-23T14:11:53Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "5ccf8921-a3cb-4897-b46b-76243a8ec147",
        "parentId" : "d33febb7-6fbd-416e-871b-f0f3458450bd",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "ah sorry, you are right. `attrMapping` is not only propagated to parents, but is also used to rewrite attributes of the currennt `plan`, so `plan.references` is necessary. Can we make the comment clearer about it?",
        "createdAt" : "2020-07-23T03:38:27Z",
        "updatedAt" : "2020-07-23T14:11:53Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "4033dabc-9a26-45d8-9300-f9041a090019",
        "parentId" : "d33febb7-6fbd-416e-871b-f0f3458450bd",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "maybe something like\r\n```\r\n// `attrMapping` is used to replace the attributes of the current `plan`, and also to be propagated to\r\n// parent plans, so the `oldAttr` must either be part of `plan.references` (so that it can be used to\r\n// replace attributes of current plan) or `plan.outputSet` (so that it can be used by parent plans).\r\n```",
        "createdAt" : "2020-07-23T03:42:18Z",
        "updatedAt" : "2020-07-23T14:11:53Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "0f3a6c448403df3c5d57b99d5675fdeca0a26729",
    "line" : 62,
    "diffHunk" : "@@ -1,1 +1275,1279 @@            // replace attributes of the current `plan`) or `plan.outputSet` (so that it can be\n            // used by those parent plans).\n            (plan.outputSet ++ plan.references).contains(oldAttr)\n          }\n          newChild"
  },
  {
    "id" : "a4d635cd-b153-4c5f-8cee-cb7c92fd8b30",
    "prId" : 29166,
    "prUrl" : "https://github.com/apache/spark/pull/29166#pullrequestreview-455377391",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "08fe8238-b800-4d00-b837-fc7ab80a462b",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "cc @srowen and @HyukjinKwon  . `catalyst` module compilation on `Scala 2.13` is broken here.\r\n```\r\n[ERROR] [Error] /Users/dongjoon/PRS/SPARK-testCommandAvailable/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala:1284: type mismatch;\r\n found   : scala.collection.mutable.ArrayBuffer[(org.apache.spark.sql.catalyst.expressions.Attribute, org.apache.spark.sql.catalyst.expressions.Attribute)]\r\n required: Seq[(org.apache.spark.sql.catalyst.expressions.Attribute, org.apache.spark.sql.catalyst.expressions.Attribute)]\r\n```",
        "createdAt" : "2020-07-26T06:19:38Z",
        "updatedAt" : "2020-07-26T06:19:39Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "d61b87a8-9acb-4536-a40a-cbec91ed55ef",
        "parentId" : "08fe8238-b800-4d00-b837-fc7ab80a462b",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Yep, I'm sure we're going to have to go back and add fixes for new code that's added while we're working on 2.13, which has similar issues. This just requires a `.toSeq`. I think you can tack this on to other fixes you make in the module along the way -- just fix it when it blocks progress.",
        "createdAt" : "2020-07-26T13:53:17Z",
        "updatedAt" : "2020-07-26T13:53:17Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "86bb1d68-4b74-4516-a191-b0c29f4b1212",
        "parentId" : "08fe8238-b800-4d00-b837-fc7ab80a462b",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "+1 for that approach. Thanks.",
        "createdAt" : "2020-07-26T15:29:19Z",
        "updatedAt" : "2020-07-26T15:29:19Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "71b8fe3e-33c7-4c05-869d-42dc65e7e5fe",
        "parentId" : "08fe8238-b800-4d00-b837-fc7ab80a462b",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Thanks for letting me know. +1",
        "createdAt" : "2020-07-26T16:19:35Z",
        "updatedAt" : "2020-07-26T16:19:36Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "0f3a6c448403df3c5d57b99d5675fdeca0a26729",
    "line" : 68,
    "diffHunk" : "@@ -1,1 +1281,1285 @@\n        if (attrMapping.isEmpty) {\n          newPlan -> attrMapping\n        } else {\n          assert(!attrMapping.groupBy(_._1.exprId)"
  },
  {
    "id" : "cd38efdf-e02d-4207-a0f7-6edd109e1509",
    "prId" : 29087,
    "prUrl" : "https://github.com/apache/spark/pull/29087#pullrequestreview-561466133",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1b0a230b-04a7-41d7-8ad1-c9c12d8140c1",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Could you add tests for `UnresolvedRegex` (`spark.sql.parser.quotedRegexColumnNames`=true case)?",
        "createdAt" : "2021-01-05T00:24:27Z",
        "updatedAt" : "2021-03-25T08:21:50Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "0b6fe441-fc15-4eae-b38e-1c3267f938cf",
        "parentId" : "1b0a230b-04a7-41d7-8ad1-c9c12d8140c1",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "Done can be supported.",
        "createdAt" : "2021-01-05T01:55:29Z",
        "updatedAt" : "2021-03-25T08:21:51Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "12787053aec9d015506d5c59c58e91dd23d5bb82",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +1561,1565 @@      // TODO: Remove this logic and see SPARK-34035\n      case t: ScriptTransformation if containsStar(t.input) =>\n        t.copy(input = t.child.output)\n      case g: Generate if containsStar(g.generator.children) =>\n        throw QueryCompilationErrors.invalidStarUsageError(\"explode/json_tuple/UDTF\")"
  },
  {
    "id" : "c58d936d-2c42-4a40-8497-482e015244eb",
    "prId" : 29087,
    "prUrl" : "https://github.com/apache/spark/pull/29087#pullrequestreview-638402620",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9566e1fe-b7a6-4b10-9aa0-715159907e67",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Please do not forget to fix this, @AngersZhuuuu ",
        "createdAt" : "2021-04-19T01:18:12Z",
        "updatedAt" : "2021-04-19T01:18:13Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "12787053aec9d015506d5c59c58e91dd23d5bb82",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1559,1563 @@          a.copy(aggregateExpressions = buildExpandedProjectList(a.aggregateExpressions, a.child))\n        }\n      // TODO: Remove this logic and see SPARK-34035\n      case t: ScriptTransformation if containsStar(t.input) =>\n        t.copy(input = t.child.output)"
  },
  {
    "id" : "1a64256b-7629-4958-b79f-7197817db162",
    "prId" : 29062,
    "prUrl" : "https://github.com/apache/spark/pull/29062#pullrequestreview-446953404",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ce9a5143-2e01-4929-ab7a-84d34678f270",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "I checked the order of analysis rules in branch-2.4. `Substitution` batch is after `Hints` too, like branch-3.0. Is there any change causing this issue?",
        "createdAt" : "2020-07-12T06:25:39Z",
        "updatedAt" : "2020-07-12T06:25:39Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "24cee31a-70ed-4194-9bd3-ca045899e3a6",
        "parentId" : "ce9a5143-2e01-4929-ab7a-84d34678f270",
        "authorId" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "body" : "It's hard to find which change causing it for me since 2.4 and 3.0 have many differences that I don't know.",
        "createdAt" : "2020-07-13T02:59:08Z",
        "updatedAt" : "2020-07-13T02:59:08Z",
        "lastEditedBy" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "tags" : [
        ]
      }
    ],
    "commit" : "e361401596693557877d750d195c7555fa99cd7c",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +205,209 @@      WindowsSubstitution,\n      EliminateUnions,\n      new SubstituteUnresolvedOrdinals(conf)),\n    Batch(\"Disable Hints\", Once,\n      new ResolveHints.DisableHints(conf)),"
  },
  {
    "id" : "c313ec11-5551-48ff-a76d-d264330a4cc3",
    "prId" : 29062,
    "prUrl" : "https://github.com/apache/spark/pull/29062#pullrequestreview-461400976",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7898e662-99c4-48a7-b1c2-b4466db63c49",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "Why we have to execute the batch Substitution before the batch ResolveHints? What is the root cause? I am unable to tell it from either PR description or the code comments. Could any of you explain it? \r\n\r\ncc @maryannxue  \r\n",
        "createdAt" : "2020-07-30T20:13:50Z",
        "updatedAt" : "2020-07-30T20:13:51Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "a7a9b717-4885-47aa-abd7-56b7f04db0ac",
        "parentId" : "7898e662-99c4-48a7-b1c2-b4466db63c49",
        "authorId" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "body" : "I don't know which PR causes this regression problem since there is a big gap between 2.4 and 3.0 about CTE. I am not familiar with this part.\r\nI fixed this by debuging. In 3.0, I found only the `child of CTE` could enter into the rule of `ResolveCoalesceHints`. For the above test case:\r\n```\r\nCTE [cte]\r\n:  +- 'SubqueryAlias `cte`\r\n:     +- 'UnresolvedHint REPARTITION, [3]\r\n:        +- 'Project [*]\r\n:           +- 'UnresolvedRelation `t`\r\n+- 'Project [*]\r\n   +- 'UnresolvedRelation `cte`\r\n```\r\nOnly this child `Project` branch could enter into the rule of `ResolveCoalesceHints`\r\n```\r\n+- 'Project [*]\r\n   +- 'UnresolvedRelation `cte`\r\n```",
        "createdAt" : "2020-08-05T07:18:34Z",
        "updatedAt" : "2020-08-05T07:19:16Z",
        "lastEditedBy" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "tags" : [
        ]
      }
    ],
    "commit" : "e361401596693557877d750d195c7555fa99cd7c",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +201,205 @@\n  lazy val batches: Seq[Batch] = Seq(\n    Batch(\"Substitution\", fixedPoint,\n      CTESubstitution,\n      WindowsSubstitution,"
  },
  {
    "id" : "fb685f45-548d-4232-9ba6-2fd1d93af4f5",
    "prId" : 28979,
    "prUrl" : "https://github.com/apache/spark/pull/28979#pullrequestreview-443138858",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "00c3e1f3-4d35-4e3d-be11-9e4669993d62",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "(This is not related to this PR though) To avoid recomputation, could we use `val` for `inputPrimitives` instead of `def`?\r\nhttps://github.com/apache/spark/blob/42f01e314b4874236544cc8b94bef766269385ee/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/ScalaUDF.scala#L65",
        "createdAt" : "2020-07-04T10:14:15Z",
        "updatedAt" : "2020-07-09T13:18:13Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "d5dc4acb-7359-4f96-90ad-8faf8e5a38a5",
        "parentId" : "00c3e1f3-4d35-4e3d-be11-9e4669993d62",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "yea, sounds good.",
        "createdAt" : "2020-07-06T14:40:57Z",
        "updatedAt" : "2020-07-09T13:18:13Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "871c35ffd3d71cef6040849561ae07a8d4e6b370",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +2820,2824 @@      case p => p transformExpressionsUp {\n\n        case udf: ScalaUDF if udf.inputPrimitives.contains(true) =>\n          // Otherwise, add special handling of null for fields that can't accept null.\n          // The result of operations like this, when passed null, is generally to return null."
  },
  {
    "id" : "969f17e7-af43-42be-bdfd-4f65c6384be1",
    "prId" : 28745,
    "prUrl" : "https://github.com/apache/spark/pull/28745#pullrequestreview-427134285",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6bc6dc12-efc3-4bef-832c-c07111e36a72",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "So, basically this is for the case when grouping expressions are non-deterministic:\r\n\r\n```bash\r\n== Physical Plan ==\r\nFlatMapGroupsInPandas [_nondeterministic#14], my_pandas_udf(column#4L, score#6), [column#12, score#13]\r\n+- *(2) Sort [_nondeterministic#14 ASC NULLS FIRST], false, 0\r\n   +- Exchange hashpartitioning(_nondeterministic#14, 200), true, [id=#19]\r\n      +- *(1) Project [column#4L, score#6, rand(42) AS _nondeterministic#14]  # <--- here to evaluate non-deterministic expression only once.\r\n...\r\n```",
        "createdAt" : "2020-06-09T13:18:39Z",
        "updatedAt" : "2020-06-09T13:18:39Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "2bf09c0b2b009ddea5d9fe1b08c52bcee705e2f7",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +2756,2760 @@        projectGroupingExprs(a, a.groupingExpressions)\n\n      case f: FlatMapGroupsInPandas if f.groupingExprs.exists(!_.deterministic) =>\n        projectGroupingExprs(f, f.groupingExprs)\n"
  },
  {
    "id" : "8f1abd19-13b1-4b78-b589-7a4743f529ae",
    "prId" : 28645,
    "prUrl" : "https://github.com/apache/spark/pull/28645#pullrequestreview-426754138",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0f5ccb31-41d3-4953-9c12-65a040b224a9",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Can you add some doc for this rule?",
        "createdAt" : "2020-06-05T07:02:23Z",
        "updatedAt" : "2020-06-19T06:05:17Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "4b759e07-2227-446f-a650-b5c2eaa37637",
        "parentId" : "0f5ccb31-41d3-4953-9c12-65a040b224a9",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "sure",
        "createdAt" : "2020-06-09T02:52:25Z",
        "updatedAt" : "2020-06-19T06:05:17Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "f29a62ac985c6714f0cd6ff5bdd919b2fce89723",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +2858,2862 @@   * The resolved encoders then will be used to deserialize the internal row to Scala value.\n   */\n  object ResolveEncodersInUDF extends Rule[LogicalPlan] {\n    override def apply(plan: LogicalPlan): LogicalPlan = plan.resolveOperatorsUp {\n      case p if !p.resolved => p // Skip unresolved nodes."
  },
  {
    "id" : "31d9fbd0-1f9a-47b5-8e90-6cb0a9848bf0",
    "prId" : 28645,
    "prUrl" : "https://github.com/apache/spark/pull/28645#pullrequestreview-426754053",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c6021e8a-70db-4934-8d10-7cfe8ae113d7",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Do we need to give attrs explicitly? Cannot the encoder use `schema` internally to resolve?",
        "createdAt" : "2020-06-05T16:01:36Z",
        "updatedAt" : "2020-06-19T06:05:17Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "7799bee5-c522-4465-8579-0df9f8a2b70b",
        "parentId" : "c6021e8a-70db-4934-8d10-7cfe8ae113d7",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "By using internal `schema`, `ExpressionEncoder` would always use `Decimal(38, 18)` as the data type for the input decimal value when deserializing an internal row to external value. However, it could cause data loss when the data type of the input decimal value is not compatible with `Decimal(38, 18)`. For example, we should not cast the input value to `Deciaml(38, 18)` if the actual data type is `Decimal(30, 0)`. Therefore, we give the attrs explicitly here in order to let the encoder uses the actual data type from the input value when doing deserialzation.",
        "createdAt" : "2020-06-09T02:52:11Z",
        "updatedAt" : "2020-06-19T06:05:17Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "f29a62ac985c6714f0cd6ff5bdd919b2fce89723",
    "line" : 43,
    "diffHunk" : "@@ -1,1 +2878,2882 @@                  // a simple literal to avoid any overhead\n                  new StructType().add(\"input\", dataType).toAttributes\n                }\n                enc.resolveAndBind(attrs)\n              }"
  },
  {
    "id" : "4e776ca4-7fad-45ab-895d-f927104ade48",
    "prId" : 28645,
    "prUrl" : "https://github.com/apache/spark/pull/28645#pullrequestreview-433225445",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f09f468d-b940-4e4a-907e-977b452585c1",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "encoder does support UDT. We can figure it out later.",
        "createdAt" : "2020-06-18T11:48:14Z",
        "updatedAt" : "2020-06-19T06:05:17Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "ee8ebda1-83a3-4fc1-afb2-4bc30f824e3c",
        "parentId" : "f09f468d-b940-4e4a-907e-977b452585c1",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "It does, but just doesn't support upcast from the subclass to the parent class. So, when the input data type from the child is the subclass of the input parameter data type of the udf, `resolveAndBind` can fail. \r\n\r\nI think this may need a separate fix.",
        "createdAt" : "2020-06-18T12:20:53Z",
        "updatedAt" : "2020-06-19T06:05:17Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "f29a62ac985c6714f0cd6ff5bdd919b2fce89723",
    "line" : 33,
    "diffHunk" : "@@ -1,1 +2868,2872 @@            val dataType = udf.children(i).dataType\n            if (dataType.existsRecursively(_.isInstanceOf[UserDefinedType[_]])) {\n              // for UDT, we use `CatalystTypeConverters`\n              None\n            } else {"
  },
  {
    "id" : "55944dda-84a9-476a-be43-1e53208fef17",
    "prId" : 28490,
    "prUrl" : "https://github.com/apache/spark/pull/28490#pullrequestreview-479435342",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "eaac8b6d-232c-4856-955b-582356645869",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "nit: move the original comment `// SPARK-25942: ....` above this line.",
        "createdAt" : "2020-08-31T15:16:43Z",
        "updatedAt" : "2020-09-02T04:42:25Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "07c10372-217b-4cca-b8c7-4ecbb54b5aaf",
        "parentId" : "eaac8b6d-232c-4856-955b-582356645869",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> nit: move the original comment `// [SPARK-25942](https://issues.apache.org/jira/browse/SPARK-25942): ....` above this line.\r\n\r\nDone",
        "createdAt" : "2020-08-31T15:33:30Z",
        "updatedAt" : "2020-09-02T04:42:25Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "c67a4d71-5098-405b-a518-b3f33a309b34",
        "parentId" : "eaac8b6d-232c-4856-955b-582356645869",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "The comment is lost?",
        "createdAt" : "2020-09-01T04:48:57Z",
        "updatedAt" : "2020-09-02T04:42:25Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "67e72791-62a9-4b24-b776-4d9a37483b9e",
        "parentId" : "eaac8b6d-232c-4856-955b-582356645869",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> The comment is lost?\r\n\r\nOh make a mistake, add back",
        "createdAt" : "2020-09-01T06:31:14Z",
        "updatedAt" : "2020-09-02T04:42:25Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "e6fb91fc8124c60e27899846b675853c22537791",
    "line" : 82,
    "diffHunk" : "@@ -1,1 +1457,1461 @@      // unnecessary alias of GetStructField here.\n      case a: Aggregate =>\n        val planForResolve = a.child match {\n          // SPARK-25942: Resolves aggregate expressions with `AppendColumns`'s children, instead of\n          // `AppendColumns`, because `AppendColumns`'s serializer might produce conflict attribute"
  },
  {
    "id" : "f0b31bb6-67ff-45f4-bf51-8a09d019f779",
    "prId" : 28490,
    "prUrl" : "https://github.com/apache/spark/pull/28490#pullrequestreview-479297205",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d4b4e3ab-db85-4913-82ba-102b48223c8d",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I'd expect something simpler\r\n```\r\n// If `trimAlias` is true, trim Aliases over non-top-level GetStructField.\r\ndef resolveExpressionTopDown(..., trimAlias: Boolean = false, isTopLevel: Boolean = true) = {\r\n  ...\r\n  result match {\r\n    case Alias(s: GetStructField, _) if trimAlias && !isTopLevel => s\r\n    case _ => result\r\n  }\r\n  ...\r\n  case _ => e.mapChildren(resolveExpressionTopDown(_, q, trimAlias, isTopLevel = false))\r\n}\r\n```\r\n\r\nand here\r\n```\r\nval resolvedGroupingExprs = a.groupingExpressions\r\n  .map(resolveExpressionTopDown(_, planForResolve, trimAlias = true))\r\n  .map {\r\n    // trim Alias over top-level GetStructField\r\n    case Alias(s: GetStructField, _) => s\r\n    case other => other\r\n  }\r\n\r\nval resolvedAggExprs = a.aggregateExpressions\r\n  .map(resolveExpressionTopDown(_, planForResolve, trimAlias = true))\r\n```",
        "createdAt" : "2020-08-31T17:04:10Z",
        "updatedAt" : "2020-09-02T04:42:25Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "3df480a7-28be-4fce-8f2a-4b82ffcb2ebc",
        "parentId" : "d4b4e3ab-db85-4913-82ba-102b48223c8d",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "Updated",
        "createdAt" : "2020-09-01T01:40:14Z",
        "updatedAt" : "2020-09-02T04:42:25Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "e6fb91fc8124c60e27899846b675853c22537791",
    "line" : 96,
    "diffHunk" : "@@ -1,1 +1471,1475 @@        val resolvedAggExprs = a.aggregateExpressions\n          .map(resolveExpressionTopDown(_, planForResolve, trimAlias = true))\n            .map(_.asInstanceOf[NamedExpression])\n\n        a.copy(resolvedGroupingExprs, resolvedAggExprs, a.child)"
  },
  {
    "id" : "86f1896f-195a-42ba-9cc1-e90c77ebbcd0",
    "prId" : 28490,
    "prUrl" : "https://github.com/apache/spark/pull/28490#pullrequestreview-479413416",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0471c255-0395-4eef-a5dc-a8d4850b6972",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "let's add comments to explain what the new parameters do",
        "createdAt" : "2020-09-01T04:50:21Z",
        "updatedAt" : "2020-09-02T04:42:25Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "2bde3c11-429b-4c94-a07d-b4fa5bd2c39d",
        "parentId" : "0471c255-0395-4eef-a5dc-a8d4850b6972",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> let's add comments to explain what the new parameters do\r\n\r\nDone",
        "createdAt" : "2020-09-01T05:38:24Z",
        "updatedAt" : "2020-09-02T04:42:25Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "e6fb91fc8124c60e27899846b675853c22537791",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +1335,1339 @@     * @return resolved Expression.\n     */\n    private def resolveExpressionTopDown(\n        e: Expression,\n        q: LogicalPlan,"
  },
  {
    "id" : "6ebe9305-d58b-4400-a9a2-1300191ef356",
    "prId" : 28490,
    "prUrl" : "https://github.com/apache/spark/pull/28490#pullrequestreview-480474524",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fb207baf-aec5-4744-bed0-0e97b61e22f1",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "We need a high-level comment to explain why we trim alias here, e.g.\r\n```\r\n// SPARK-31670: ...\r\n```",
        "createdAt" : "2020-09-02T04:15:13Z",
        "updatedAt" : "2020-09-02T04:42:25Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "86c6768c-f932-43df-8cfe-bb443e8c861d",
        "parentId" : "fb207baf-aec5-4744-bed0-0e97b61e22f1",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> We need a high-level comment to explain why we trim alias here, e.g.\r\n> \r\n> ```\r\n> // [SPARK-31670](https://issues.apache.org/jira/browse/SPARK-31670): ...\r\n> ```\r\n\r\nDone",
        "createdAt" : "2020-09-02T04:25:59Z",
        "updatedAt" : "2020-09-02T04:42:25Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "e6fb91fc8124c60e27899846b675853c22537791",
    "line" : 81,
    "diffHunk" : "@@ -1,1 +1456,1460 @@      // groupByExpressions in `ResolveGroupingAnalytics.constructAggregateExprs()`, we trim\n      // unnecessary alias of GetStructField here.\n      case a: Aggregate =>\n        val planForResolve = a.child match {\n          // SPARK-25942: Resolves aggregate expressions with `AppendColumns`'s children, instead of"
  },
  {
    "id" : "95395f1d-1df8-4358-95a1-2d8e4d9b76b2",
    "prId" : 28490,
    "prUrl" : "https://github.com/apache/spark/pull/28490#pullrequestreview-524804191",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8476d19d-9bfd-4d5d-aa7e-f878e0535553",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "In the comment, SPARK-31607 seems to be a typo of SPARK-31670 because SPARK-31607 is `Improve the perf of CTESubstitution`.",
        "createdAt" : "2020-11-05T23:57:55Z",
        "updatedAt" : "2020-11-05T23:57:56Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "3aed0b61-dc50-44ba-af22-7cdcacbfdec7",
        "parentId" : "8476d19d-9bfd-4d5d-aa7e-f878e0535553",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> In the comment, [SPARK-31607](https://issues.apache.org/jira/browse/SPARK-31607) seems to be a typo of [SPARK-31670](https://issues.apache.org/jira/browse/SPARK-31670) because [SPARK-31607](https://issues.apache.org/jira/browse/SPARK-31607) is `Improve the perf of CTESubstitution`.\r\n\r\nYeasorry for my mistake",
        "createdAt" : "2020-11-06T01:27:15Z",
        "updatedAt" : "2020-11-06T01:27:16Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "12c2d2b5-f997-4d95-9891-267d3e61956f",
        "parentId" : "8476d19d-9bfd-4d5d-aa7e-f878e0535553",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Never mind~, @AngersZhuuuu . :)",
        "createdAt" : "2020-11-06T01:53:28Z",
        "updatedAt" : "2020-11-06T01:53:28Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "e6fb91fc8124c60e27899846b675853c22537791",
    "line" : 76,
    "diffHunk" : "@@ -1,1 +1451,1455 @@      case plan if containsDeserializer(plan.expressions) => plan\n\n      // SPARK-31607: Resolve Struct field in groupByExpressions and aggregateExpressions\n      // with CUBE/ROLLUP will be wrapped with alias like Alias(GetStructField, name) with\n      // different ExprId. This cause aggregateExpressions can't be replaced by expanded"
  },
  {
    "id" : "d09aff4a-f796-446e-becd-389f1617df80",
    "prId" : 28310,
    "prUrl" : "https://github.com/apache/spark/pull/28310#pullrequestreview-399617356",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1b394703-eb42-4c29-8e51-714cdd780524",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "This is a good idea, maybe we can remove `TimeSub` later.",
        "createdAt" : "2020-04-24T03:09:03Z",
        "updatedAt" : "2020-04-25T07:30:28Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "635edf60-e7bd-466b-9c18-f41e307ed82f",
        "parentId" : "1b394703-eb42-4c29-8e51-714cdd780524",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "yea, we can clean it up later",
        "createdAt" : "2020-04-24T03:34:07Z",
        "updatedAt" : "2020-04-25T07:30:28Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "c64d27ea81f73ab2bd67dbae4c88d6704639063d",
    "line" : 59,
    "diffHunk" : "@@ -1,1 +308,312 @@        case s @ Subtract(l, r) if s.childrenResolved => (l.dataType, r.dataType) match {\n          case (CalendarIntervalType, CalendarIntervalType) => s\n          case (DateType, CalendarIntervalType) => DateAddInterval(l, UnaryMinus(r))\n          case (_, CalendarIntervalType) => Cast(TimeSub(l, r), l.dataType)\n          case (TimestampType, _) => SubtractTimestamps(l, r)"
  },
  {
    "id" : "67bc6407-2331-4560-b2d4-477dba86c759",
    "prId" : 28294,
    "prUrl" : "https://github.com/apache/spark/pull/28294#pullrequestreview-398817152",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5571b885-ddce-450b-916d-1944ffa49758",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Could you leave some comments here about why we need this special handling for aggregate with having?",
        "createdAt" : "2020-04-23T00:52:02Z",
        "updatedAt" : "2020-04-27T12:56:29Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "df4c783c-6594-4aad-bf1d-7c91125de8d7",
        "parentId" : "5571b885-ddce-450b-916d-1944ffa49758",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "Sure, done in c75fe08.",
        "createdAt" : "2020-04-23T07:11:30Z",
        "updatedAt" : "2020-04-27T12:56:29Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "18d857fa993e2acd7abcb837700e6bc4f3511dd5",
    "line" : 34,
    "diffHunk" : "@@ -1,1 +2040,2044 @@      // resolve the having condition expression, here we skip resolving it in ResolveReferences\n      // and transform it to Filter after aggregate is resolved. See more details in SPARK-31519.\n      case AggregateWithHaving(cond, agg: Aggregate) if agg.resolved =>\n        resolveHaving(Filter(cond, agg), agg)\n"
  },
  {
    "id" : "283f3739-532b-4976-94e7-20d9e16ca20d",
    "prId" : 28107,
    "prUrl" : "https://github.com/apache/spark/pull/28107#pullrequestreview-387325730",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "937afdf8-b81b-4855-8eff-b16330e8cd5b",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "can we just add a case above it?\r\n```\r\ncase f: Filter if ResolveAggregateFunctions.containsAggregate(f.condition) => f\r\n\r\ncase q: LogicalPlan =>\r\n  ...\r\n```",
        "createdAt" : "2020-04-03T14:34:53Z",
        "updatedAt" : "2020-04-06T09:35:32Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "0c94376d-1030-4c0f-b5da-1041fca39a63",
        "parentId" : "937afdf8-b81b-4855-8eff-b16330e8cd5b",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> can we just add a case above it?\r\n> \r\n> ```\r\n> case f: Filter if ResolveAggregateFunctions.containsAggregate(f.condition) => f\r\n> \r\n> case q: LogicalPlan =>\r\n>   ...\r\n> ```\r\n\r\nChanged, for containsAggregate() method, where I put it is better? or add a common interface for Resolve Reference",
        "createdAt" : "2020-04-03T14:48:47Z",
        "updatedAt" : "2020-04-06T09:35:32Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "20febcefcf88f02c6188a119722be70085a0a17c",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +1399,1403 @@      case f @ Filter(cond, agg @ Aggregate(_, _, _)) if containsAggregate(cond) => f\n\n      case q: LogicalPlan =>\n        logTrace(s\"Attempting to resolve ${q.simpleString(SQLConf.get.maxToStringFields)}\")\n        q.mapExpressions(resolveExpressionTopDown(_, q))"
  },
  {
    "id" : "be9991c7-25fa-4a52-beda-3510bd06a884",
    "prId" : 28107,
    "prUrl" : "https://github.com/apache/spark/pull/28107#pullrequestreview-387327667",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "16dd15c9-d189-457a-9b8a-be6fb3f4c16f",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "why can't we reuse `ResolveAggregateFunctions.containsAggregate`?",
        "createdAt" : "2020-04-03T14:48:30Z",
        "updatedAt" : "2020-04-06T09:35:32Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "394d00ab-2333-4cbf-8236-a32d2c1d6b35",
        "parentId" : "16dd15c9-d189-457a-9b8a-be6fb3f4c16f",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> why can't we reuse `ResolveAggregateFunctions.containsAggregate`?\r\n\r\nSince here function is still UnresolvedFunction, we can't just reuse this.",
        "createdAt" : "2020-04-03T14:51:01Z",
        "updatedAt" : "2020-04-06T09:35:32Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "20febcefcf88f02c6188a119722be70085a0a17c",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +1404,1408 @@    }\n\n    def containsAggregate(e: Expression): Boolean = {\n      e.find {\n        // In current loop, functions maybe unresolved,"
  },
  {
    "id" : "120c2076-ac66-4db5-be01-f0354146fbb5",
    "prId" : 27803,
    "prUrl" : "https://github.com/apache/spark/pull/27803#pullrequestreview-401503467",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b972f982-e788-4933-a3a0-49ba9598d1f1",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Could you describe the output as a function description, please? For example, the purpose of the second boolean in the result pair?",
        "createdAt" : "2020-04-28T05:16:53Z",
        "updatedAt" : "2020-04-28T07:05:51Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "fc63d40304b1584282fc64edee925ce335ef3626",
    "line" : 73,
    "diffHunk" : "@@ -1,1 +2246,2250 @@    // This method extracts all generators from given expressions and then returns a sequence of\n    // pairs (a found generator, the generator is outer or not).\n    private def collectAdjacentGenerators(children: Seq[Expression]): Seq[(Generator, Boolean)] = {\n      // We've already checked that all the `Generator` has unary nodes only\n      // in `hasUnsupportedNestedGenerator`."
  },
  {
    "id" : "73200de5-1d95-43f7-a1d3-1d3c4ad51739",
    "prId" : 27643,
    "prUrl" : "https://github.com/apache/spark/pull/27643#pullrequestreview-361705478",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a797a617-ed25-4c94-98af-981b352c53f6",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Is this a suitable place for the warning? How about putting this in `CheckAnalysis` (this is not an error issue though)?",
        "createdAt" : "2020-02-20T08:03:02Z",
        "updatedAt" : "2020-03-06T04:04:24Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "427f4906-13ff-4252-a7af-add63bff99d4",
        "parentId" : "a797a617-ed25-4c94-98af-981b352c53f6",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "If we put it in `CheckAnalysis`, we can't distinguish `trim(a, b)` and `trim(a from b)`. They are both `String2TrimExpression`",
        "createdAt" : "2020-02-20T08:14:10Z",
        "updatedAt" : "2020-03-06T04:04:24Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "97ca99f1-6afd-447d-a683-17e088153b6b",
        "parentId" : "a797a617-ed25-4c94-98af-981b352c53f6",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Hi, @maropu . Since `CheckAnalysis` received already resolved expressions, there is no way to distinguish `SQL syntax TRIM(trimStr FROM str)` from Parser and `function invocation TRIM(trimStr, str)` from here.\r\n\r\nAlso, `failAnalysis(\"DISTINCT or FILTER specified` also happens here instead of `CheckAnalysis`.",
        "createdAt" : "2020-02-20T08:18:41Z",
        "updatedAt" : "2020-03-06T04:04:24Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "45a7194c-2a4d-4103-bdaf-b2b9301dd029",
        "parentId" : "a797a617-ed25-4c94-98af-981b352c53f6",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Oops. I was late. Yes. @cloud-fan 's comment is correct.",
        "createdAt" : "2020-02-20T08:21:05Z",
        "updatedAt" : "2020-03-06T04:04:24Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "2fe0e76d-efc0-43d6-a428-6848ee144218",
        "parentId" : "a797a617-ed25-4c94-98af-981b352c53f6",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Ah, I see. Thanks for the explanation, guys!",
        "createdAt" : "2020-02-20T08:23:13Z",
        "updatedAt" : "2020-03-06T04:04:24Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "89b2b040113e7715d7441d3ae9974c38bc6c81f5",
    "line" : 34,
    "diffHunk" : "@@ -1,1 +1850,1854 @@                      \" Use SQL syntax `TRIM((BOTH | LEADING | TRAILING)? trimStr FROM str)`\" +\n                      \" instead.\")\n                    trimWarningEnabled.set(false)\n                  }\n                  e"
  },
  {
    "id" : "a99dedca-6e82-4106-9d61-7a4e02142c2b",
    "prId" : 27643,
    "prUrl" : "https://github.com/apache/spark/pull/27643#pullrequestreview-362082781",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d4f810d2-b86e-4d6f-a0e5-141e904556c0",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "TRIM is an operator which can do the work of LTRIM and RTRIM functions as well, e.g. `TRIM(LEADING trimStr FROM srcStr)` is the same as LTRIM.\r\n\r\nThere is no FROM syntax for LTRIM and RTRIM as they are just functions. Shall we deprecate LTRIM and RTRIM completely?",
        "createdAt" : "2020-02-20T10:00:20Z",
        "updatedAt" : "2020-03-06T04:04:24Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "029a516c-9532-4e32-ad68-dd3fc05521e2",
        "parentId" : "d4f810d2-b86e-4d6f-a0e5-141e904556c0",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Oops. Sure. Let me think about that.",
        "createdAt" : "2020-02-20T17:13:23Z",
        "updatedAt" : "2020-03-06T04:04:24Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "89b2b040113e7715d7441d3ae9974c38bc6c81f5",
    "line" : 31,
    "diffHunk" : "@@ -1,1 +1847,1851 @@                case e: String2TrimExpression if arguments.size == 2 =>\n                  if (trimWarningEnabled.get) {\n                    log.warn(\"Two-parameter TRIM/LTRIM/RTRIM function signatures are deprecated.\" +\n                      \" Use SQL syntax `TRIM((BOTH | LEADING | TRAILING)? trimStr FROM str)`\" +\n                      \" instead.\")"
  },
  {
    "id" : "d4ffa518-da5c-4cbd-9f28-3f306c1ae976",
    "prId" : 27643,
    "prUrl" : "https://github.com/apache/spark/pull/27643#pullrequestreview-362075442",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6e25c4d3-08ef-4c6d-8032-5230cbfb1fc4",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "When do you set it back to `true`? The `ResolveFunctions` object can live forever. By setting it to `false` only once, you disable warnings for all apps and users.\r\n\r\nLet's image multiple notebooks connected to the same cluster - they will share the same `trimWarningEnabled` if I am not wrong.",
        "createdAt" : "2020-02-20T16:34:46Z",
        "updatedAt" : "2020-03-06T04:04:24Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "9d7e29f8-288a-4319-9eb0-a06920127242",
        "parentId" : "6e25c4d3-08ef-4c6d-8032-5230cbfb1fc4",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "This is an inner object of `Analyzer`, and we will have a new instance for a new session which creates an `Analyzer`.",
        "createdAt" : "2020-02-20T17:03:11Z",
        "updatedAt" : "2020-03-06T04:04:24Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "89b2b040113e7715d7441d3ae9974c38bc6c81f5",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +1797,1801 @@   */\n  object ResolveFunctions extends Rule[LogicalPlan] {\n    val trimWarningEnabled = new AtomicBoolean(true)\n    def apply(plan: LogicalPlan): LogicalPlan = plan.resolveOperatorsUp {\n      case q: LogicalPlan =>"
  },
  {
    "id" : "686a687e-5b19-4daf-af82-5b8965644236",
    "prId" : 27532,
    "prUrl" : "https://github.com/apache/spark/pull/27532#pullrequestreview-364627332",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "26f3db49-ed8f-4f5a-9fc0-8debac9aa2bb",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "shall we add catalog name too? to support cases like `select spark_catalog.default.t.i from t`.",
        "createdAt" : "2020-02-26T02:59:33Z",
        "updatedAt" : "2020-02-26T05:40:36Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "bee5cb36-8696-4f7a-a275-ce8960027425",
        "parentId" : "26f3db49-ed8f-4f5a-9fc0-8debac9aa2bb",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "We could, but it would not be consistent with v1 table behavior. I was thinking about adding this support when I update the resolution rule for session catalogs: https://github.com/apache/spark/pull/27391#discussion_r383091921. What do you think, should I do it now?",
        "createdAt" : "2020-02-26T03:17:03Z",
        "updatedAt" : "2020-02-26T05:40:36Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      },
      {
        "id" : "febebfb6-1c84-4a12-b944-0530c8bec5f7",
        "parentId" : "26f3db49-ed8f-4f5a-9fc0-8debac9aa2bb",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "ah ok. Let's leave it for now.",
        "createdAt" : "2020-02-26T05:14:24Z",
        "updatedAt" : "2020-02-26T05:40:36Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "f23ded6c07d00091ae21f0b04641743a3e6acb86",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +936,940 @@            case table =>\n              SubqueryAlias(\n                ident.asMultipartIdentifier,\n                DataSourceV2Relation.create(table, Some(catalog), Some(ident)))\n          }"
  },
  {
    "id" : "c3248cf5-9524-43d9-9f6e-ce8d2c1f6781",
    "prId" : 27350,
    "prUrl" : "https://github.com/apache/spark/pull/27350#pullrequestreview-351032492",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "74b0c72d-8080-4031-9a7a-95029d61d6e8",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "shall we check that the new name doesn't conflict with the existing field names?",
        "createdAt" : "2020-01-30T12:37:24Z",
        "updatedAt" : "2020-01-30T18:27:08Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "4ae68257-7e06-4834-a360-cff8630e666e",
        "parentId" : "74b0c72d-8080-4031-9a7a-95029d61d6e8",
        "authorId" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "body" : "good idea",
        "createdAt" : "2020-01-30T17:44:21Z",
        "updatedAt" : "2020-01-30T18:27:08Z",
        "lastEditedBy" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "tags" : [
        ]
      },
      {
        "id" : "f56069cc-2b2e-4709-8a99-a564bd181a6f",
        "parentId" : "74b0c72d-8080-4031-9a7a-95029d61d6e8",
        "authorId" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "body" : "left to CheckAnalysis",
        "createdAt" : "2020-01-30T18:08:49Z",
        "updatedAt" : "2020-01-30T18:27:08Z",
        "lastEditedBy" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "tags" : [
        ]
      }
    ],
    "commit" : "2b364e23927fbcdd8a6d219c9ae6bd861dbe3f17",
    "line" : 123,
    "diffHunk" : "@@ -1,1 +3108,3112 @@              TableChange.updateColumnComment(_, comment.newComment())).orElse(Some(comment))\n\n          case rename: RenameColumn =>\n            resolveFieldNames(\n              schema,"
  },
  {
    "id" : "3de7284c-ae97-4c82-8b52-8b7c2168027c",
    "prId" : 27350,
    "prUrl" : "https://github.com/apache/spark/pull/27350#pullrequestreview-351013717",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c6618ae6-45af-4ebe-b9d5-880c08331036",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "do we need this case? `case column: ColumnChange` is catch-all already.",
        "createdAt" : "2020-01-30T12:38:12Z",
        "updatedAt" : "2020-01-30T18:27:08Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "1f3d82df-502a-4bf2-bab8-19af8e766d52",
        "parentId" : "c6618ae6-45af-4ebe-b9d5-880c08331036",
        "authorId" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "body" : "This is for other changes like SetProperties",
        "createdAt" : "2020-01-30T17:38:55Z",
        "updatedAt" : "2020-01-30T18:27:08Z",
        "lastEditedBy" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "tags" : [
        ]
      }
    ],
    "commit" : "2b364e23927fbcdd8a6d219c9ae6bd861dbe3f17",
    "line" : 137,
    "diffHunk" : "@@ -1,1 +3122,3126 @@            throw new UnsupportedOperationException(\n              \"Please add an implementation for a column change here\")\n          case other => Some(other)\n        }\n"
  },
  {
    "id" : "291c23db-6e6a-44f0-9cb6-3016b418c6f3",
    "prId" : 27350,
    "prUrl" : "https://github.com/apache/spark/pull/27350#pullrequestreview-351014030",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "31fec004-2f44-4b54-9c67-e32c83e5107b",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "when will `ColumnPosition` be null?",
        "createdAt" : "2020-01-30T12:38:50Z",
        "updatedAt" : "2020-01-30T18:27:08Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "9919e9a5-b009-4f35-b0ed-eb8da016c447",
        "parentId" : "31fec004-2f44-4b54-9c67-e32c83e5107b",
        "authorId" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "body" : "when you're adding a column without a position (so to the end)",
        "createdAt" : "2020-01-30T17:39:30Z",
        "updatedAt" : "2020-01-30T18:27:08Z",
        "lastEditedBy" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "tags" : [
        ]
      }
    ],
    "commit" : "2b364e23927fbcdd8a6d219c9ae6bd861dbe3f17",
    "line" : 161,
    "diffHunk" : "@@ -1,1 +3146,3150 @@        struct: StructType): ColumnPosition = {\n      position match {\n        case null => null\n        case after: After =>\n          struct.fieldNames.find(n => conf.resolver(n, after.column())) match {"
  },
  {
    "id" : "61c05fd2-23e4-4835-aeaa-eae712dfa558",
    "prId" : 27350,
    "prUrl" : "https://github.com/apache/spark/pull/27350#pullrequestreview-351335381",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "30840972-59e6-48ad-80be-5d8ac4846698",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I think we can remove other column changes if they are noop, e.g. `UpdateColumnNullability` without changing nullability. We can address it in a followup.",
        "createdAt" : "2020-01-31T07:45:57Z",
        "updatedAt" : "2020-01-31T07:45:57Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "2b364e23927fbcdd8a6d219c9ae6bd861dbe3f17",
    "line" : 73,
    "diffHunk" : "@@ -1,1 +3058,3062 @@              val (fieldNames, field) = fieldOpt.get\n              if (field.dataType == typeChange.newDataType()) {\n                // The user didn't want the field to change, so remove this change\n                None\n              } else {"
  },
  {
    "id" : "d1e57db2-986c-4be9-a43b-b7ff8ce89e64",
    "prId" : 27263,
    "prUrl" : "https://github.com/apache/spark/pull/27263#pullrequestreview-345103358",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2dd8ab63-da2c-4839-8a51-c592eb1431dc",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "For better understanding, can you leave some comments here about the conflict described in the PR descirption?",
        "createdAt" : "2020-01-20T02:21:45Z",
        "updatedAt" : "2020-01-20T06:22:22Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "d2bf3a9a-f32c-4fe0-8e71-21d9385efe7b",
        "parentId" : "2dd8ab63-da2c-4839-8a51-c592eb1431dc",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Added(on the top of `collectConflictPlans`).",
        "createdAt" : "2020-01-20T06:23:26Z",
        "updatedAt" : "2020-01-20T06:23:26Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "37f90112-32e0-41d4-bc63-f327d20c9aca",
        "parentId" : "2dd8ab63-da2c-4839-8a51-c592eb1431dc",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Thanks!",
        "createdAt" : "2020-01-20T06:45:34Z",
        "updatedAt" : "2020-01-20T06:45:35Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "fbbf78542b2ceeb4c98e5416d60641fbcac6d0dd",
    "line" : 68,
    "diffHunk" : "@@ -1,1 +1126,1130 @@      }\n\n      val conflictPlans = collectConflictPlans(right)\n\n      /*"
  },
  {
    "id" : "32bd8429-c7f5-4b88-a144-22ac229bdeb0",
    "prId" : 27095,
    "prUrl" : "https://github.com/apache/spark/pull/27095#pullrequestreview-338482685",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3250ab2f-0817-43ad-a66c-f64d64c4ddc8",
        "parentId" : null,
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "@cloud-fan this was introduced to handle commands where namespace is used as optional  (`SHOW NAMESPACES` for example). Is this approach (which treats empty `Seq` as `None`) reasonable?",
        "createdAt" : "2020-01-04T08:03:03Z",
        "updatedAt" : "2020-01-07T05:54:46Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      },
      {
        "id" : "94966e6f-a4ba-4085-921a-49b21e0bf3a4",
        "parentId" : "3250ab2f-0817-43ad-a66c-f64d64c4ddc8",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "This may not be a good idea since empty `Seq` can mean `None` or `Nil`.\r\n\r\nHow about we add `Option` as following since namespace can be optional in a command:\r\n```scala\r\ncase class ResolvedNamespace(catalog: SupportsNamespaces, namespace: Option[Seq[String]])\r\n  extends LeafNode {\r\n  override def output: Seq[Attribute] = Nil\r\n}\r\n\r\ncase class UnresolvedNamespace(multipartIdentifier: Option[Seq[String]]) extends LeafNode {\r\n  override lazy val resolved: Boolean = false\r\n  override def output: Seq[Attribute] = Nil\r\n}\r\n```",
        "createdAt" : "2020-01-04T08:26:31Z",
        "updatedAt" : "2020-01-07T05:54:46Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      },
      {
        "id" : "adc55bf7-5133-4ba0-a534-9b8d70be19d1",
        "parentId" : "3250ab2f-0817-43ad-a66c-f64d64c4ddc8",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "`CatalogAndNamespace` doesn't look up the namespace, but look up the catalog. I think it can handle Nil, which resolves catalog to the current catalog, and return Nil as the namespace identifier.",
        "createdAt" : "2020-01-05T06:00:36Z",
        "updatedAt" : "2020-01-07T05:54:46Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "59ae31f4-bccc-4d73-98c1-88ee38b4e2e7",
        "parentId" : "3250ab2f-0817-43ad-a66c-f64d64c4ddc8",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "The conflict here is that `SHOW NAMESPACES` treats `None` as `Nil`, but `SHOW TABLES` treats `None` as `current namespace`, thus causing the ambiguity. To make `SHOW TABLES` work with `Nil` approach (the current one), I have to do the following:\r\n```scala\r\n    case ShowTablesStatement(NonSessionCatalogAndNamespace(catalog, ns), pattern) =>\r\n      val namespace = if (ns.isEmpty && currentCatalog.name.equals(catalog.name)) {\r\n        catalogManager.currentNamespace.toSeq\r\n      } else { ns }\r\n      ShowTables(catalog.asTableCatalog, namespace, pattern)\r\n```",
        "createdAt" : "2020-01-05T07:03:43Z",
        "updatedAt" : "2020-01-07T05:54:46Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      },
      {
        "id" : "56b4e1c4-238b-4ca6-aac6-2621eee4e2df",
        "parentId" : "3250ab2f-0817-43ad-a66c-f64d64c4ddc8",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I think `SHOW TABLES` is a special case, can we manually handle it in `ResolveNamespace`?",
        "createdAt" : "2020-01-06T04:03:48Z",
        "updatedAt" : "2020-01-07T05:54:46Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "f5b7fb27-6a45-49ef-8e15-da92b392b9ae",
        "parentId" : "3250ab2f-0817-43ad-a66c-f64d64c4ddc8",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "OK, will do.",
        "createdAt" : "2020-01-06T06:19:08Z",
        "updatedAt" : "2020-01-07T05:54:46Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "b31f07bbf1ed0ae0e19498eea70e8c316e340262",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +735,739 @@          ResolvedNamespace(currentCatalog.asNamespaceCatalog, catalogManager.currentNamespace))\n      case UnresolvedNamespace(Seq()) =>\n        ResolvedNamespace(currentCatalog.asNamespaceCatalog, Seq.empty[String])\n      case UnresolvedNamespace(CatalogAndNamespace(catalog, ns)) =>\n        ResolvedNamespace(catalog.asNamespaceCatalog, ns)"
  },
  {
    "id" : "b9b291d6-1058-4e43-b538-8398e352ceea",
    "prId" : 27095,
    "prUrl" : "https://github.com/apache/spark/pull/27095#pullrequestreview-339018463",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bc887912-2ae8-4378-a0d2-bae655eb5220",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "We can move this logic to `CatalogAndNamespace`, but I'm fine with what it is now.",
        "createdAt" : "2020-01-07T03:48:53Z",
        "updatedAt" : "2020-01-07T05:54:46Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "b31f07bbf1ed0ae0e19498eea70e8c316e340262",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +734,738 @@        s.copy(namespace =\n          ResolvedNamespace(currentCatalog.asNamespaceCatalog, catalogManager.currentNamespace))\n      case UnresolvedNamespace(Seq()) =>\n        ResolvedNamespace(currentCatalog.asNamespaceCatalog, Seq.empty[String])\n      case UnresolvedNamespace(CatalogAndNamespace(catalog, ns)) =>"
  },
  {
    "id" : "9ffc5923-fa47-44da-bade-42c6ee98ef01",
    "prId" : 26847,
    "prUrl" : "https://github.com/apache/spark/pull/26847#pullrequestreview-337284783",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "85e2c6ba-3c74-4c7f-bf70-ef21267f5146",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Does it mean the `ParsedStatement` from parser will turn to use `UnresolvedNamespace`? Currently, the catalogs in statements are resolved at `ResolveCatalogs`. Will we need to refactor `ResolveCatalogs` due to this change?",
        "createdAt" : "2019-12-30T20:04:48Z",
        "updatedAt" : "2020-01-03T03:50:51Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "5b71cb5e-dacd-4d40-b209-32e3891e4089",
        "parentId" : "85e2c6ba-3c74-4c7f-bf70-ef21267f5146",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "Yes, see\r\nhttps://github.com/apache/spark/pull/26847#issuecomment-567516386 \r\n> We can have an extra rule to catch commands with v1 relation, and convert them to v1 commands. This can help us get rid of the duplicated code between ResolveCatalogs and ResolveSessionCatalog",
        "createdAt" : "2019-12-31T02:26:36Z",
        "updatedAt" : "2020-01-03T03:50:51Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "d20b1b2a2beb7cd395b92b4330724bc2c1a2d6bd",
    "line" : 27,
    "diffHunk" : "@@ -1,1 +729,733 @@      case UnresolvedNamespace(CatalogAndNamespace(catalog, ns)) =>\n        ResolvedNamespace(catalog.asNamespaceCatalog, ns)\n    }\n  }\n"
  },
  {
    "id" : "ed52014c-e2bb-4676-9267-a71fa12e65f0",
    "prId" : 26684,
    "prUrl" : "https://github.com/apache/spark/pull/26684#pullrequestreview-324001145",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "626e9ea7-60ef-41c6-b19c-24f9b836a643",
        "parentId" : null,
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "If `ResolveRelations` is going to be completely rewritten before 3.0, then we should fix it to separate view resolution from table resolution and to use multiple executions instead of recursion. The only reason why I don't think we should do that is to avoid too many changes to `ResolveRelations` just before a release.",
        "createdAt" : "2019-11-27T16:59:31Z",
        "updatedAt" : "2019-12-06T05:48:45Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      },
      {
        "id" : "8c3d0fd0-6699-49db-ba7d-ee004016d8be",
        "parentId" : "626e9ea7-60ef-41c6-b19c-24f9b836a643",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "Got it. I will also explore removing the recursion separately.",
        "createdAt" : "2019-11-28T00:20:37Z",
        "updatedAt" : "2019-12-06T05:48:45Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "23cc9d3894fe39f989ea7511433213dc44a60e5c",
    "line" : 147,
    "diffHunk" : "@@ -1,1 +818,822 @@        catalog: CatalogPlugin,\n        ident: Identifier,\n        recurse: Boolean): Option[LogicalPlan] = {\n      val newIdent = withNewNamespace(ident)\n      assert(newIdent.namespace.size == 1)"
  },
  {
    "id" : "490c82a9-9df9-475d-bc97-09cf6fed891c",
    "prId" : 26684,
    "prUrl" : "https://github.com/apache/spark/pull/26684#pullrequestreview-333023634",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a4242b31-5a8c-4a56-8c5c-89ca24f0935f",
        "parentId" : null,
        "authorId" : "f06b291a-bc5c-4fd8-9bac-e5a6d035891b",
        "body" : "'tableIdent' is not used at all.",
        "createdAt" : "2019-12-17T04:44:27Z",
        "updatedAt" : "2019-12-17T04:44:28Z",
        "lastEditedBy" : "f06b291a-bc5c-4fd8-9bac-e5a6d035891b",
        "tags" : [
        ]
      }
    ],
    "commit" : "23cc9d3894fe39f989ea7511433213dc44a60e5c",
    "line" : 153,
    "diffHunk" : "@@ -1,1 +824,828 @@      CatalogV2Util.loadTable(catalog, newIdent) match {\n        case Some(v1Table: V1Table) =>\n          val tableIdent = TableIdentifier(newIdent.name, newIdent.namespace.headOption)\n          val relation = v1SessionCatalog.getRelation(v1Table.v1Table)\n          if (recurse) {"
  },
  {
    "id" : "9aa542ea-d80a-41f7-8cf7-975184395595",
    "prId" : 26656,
    "prUrl" : "https://github.com/apache/spark/pull/26656#pullrequestreview-335151329",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "469d6d66-d475-4456-8445-a8a85c60c716",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "when we are here, the expression is not resolved yet, and `deterministic` may be incorrect.\r\n\r\nwe should check it in `CheckAnalysis`",
        "createdAt" : "2019-12-19T10:59:33Z",
        "updatedAt" : "2019-12-24T03:01:16Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "069aa68c-a50b-451f-85f2-0e9bd7d64168",
        "parentId" : "469d6d66-d475-4456-8445-a8a85c60c716",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "It seems children of function parsed here.\r\nhttps://github.com/apache/spark/blob/000ae721a643736b5ba47962267552438e9a4862/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala#L1723",
        "createdAt" : "2019-12-20T02:24:30Z",
        "updatedAt" : "2019-12-24T03:01:16Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      },
      {
        "id" : "023e1884-7a27-4e3d-9d97-d7f158bc244c",
        "parentId" : "469d6d66-d475-4456-8445-a8a85c60c716",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "ah then it's fine.",
        "createdAt" : "2019-12-20T09:34:38Z",
        "updatedAt" : "2019-12-24T03:01:16Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "d98ea4139454e3346e220aa0e1df8a2e4da18cea",
    "line" : 41,
    "diffHunk" : "@@ -1,1 +1755,1759 @@                      failAnalysis(\"DISTINCT and FILTER cannot be used in aggregate functions \" +\n                        \"at the same time\")\n                    } else if (!filter.get.deterministic) {\n                      failAnalysis(\"FILTER expression is non-deterministic, \" +\n                        \"it cannot be used in aggregate functions\")"
  },
  {
    "id" : "1d5a4f03-c921-40ba-b72f-cccd16368e03",
    "prId" : 26589,
    "prUrl" : "https://github.com/apache/spark/pull/26589#pullrequestreview-337619173",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d2e94f40-82d1-4945-b464-d5d4d8ea20d1",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "We should also update `withAnalysisContext` in this object.",
        "createdAt" : "2020-01-02T11:23:16Z",
        "updatedAt" : "2020-01-03T05:19:38Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "8e7b66612dd0e90d0f183edcf733c9d5a99edcec",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +102,106 @@    relationCache: mutable.Map[Seq[String], LogicalPlan] = mutable.Map.empty)\n\nobject AnalysisContext {\n  private val value = new ThreadLocal[AnalysisContext]() {\n    override def initialValue: AnalysisContext = AnalysisContext()"
  },
  {
    "id" : "36dfdf2e-aad3-4e54-a8e6-68a878b51af5",
    "prId" : 26589,
    "prUrl" : "https://github.com/apache/spark/pull/26589#pullrequestreview-347255592",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4c5c70dd-f522-4595-9765-c7fb01ccac58",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "This doesn't work. The table is already loaded and it's too late to use the cache. I've sent https://github.com/apache/spark/pull/27341 to fix it.",
        "createdAt" : "2020-01-23T12:21:44Z",
        "updatedAt" : "2020-01-23T12:21:44Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "8e7b66612dd0e90d0f183edcf733c9d5a99edcec",
    "line" : 31,
    "diffHunk" : "@@ -1,1 +876,880 @@            case v1Table: V1Table =>\n              val key = catalog.name +: ident.namespace :+ ident.name\n              AnalysisContext.get.relationCache.getOrElseUpdate(\n                key, v1SessionCatalog.getRelation(v1Table.v1Table))\n            case table =>"
  },
  {
    "id" : "330ad349-33e5-41ad-aab3-886e9fa44dcd",
    "prId" : 26589,
    "prUrl" : "https://github.com/apache/spark/pull/26589#pullrequestreview-423334406",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "34230d6b-beea-435b-8899-6a73a87edd11",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "This is the anti-pattern from the style guide ... https://github.com/databricks/scala-style-guide#case-classes-and-immutability",
        "createdAt" : "2020-06-03T08:33:40Z",
        "updatedAt" : "2020-06-03T08:33:40Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "8e7b66612dd0e90d0f183edcf733c9d5a99edcec",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +100,104 @@    catalogAndNamespace: Seq[String] = Nil,\n    nestedViewDepth: Int = 0,\n    relationCache: mutable.Map[Seq[String], LogicalPlan] = mutable.Map.empty)\n\nobject AnalysisContext {"
  },
  {
    "id" : "64a77a7b-e4ad-4861-94f1-952f4440534b",
    "prId" : 26412,
    "prUrl" : "https://github.com/apache/spark/pull/26412#pullrequestreview-325906275",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d8628cef-4a7e-4fbd-b15a-a22853487c74",
        "parentId" : null,
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "Hi @cloud-fan, when I was implementing this, I got a bit confused.\r\nOur date_sub/date_add actually support implicit type cast, that is\r\n```\r\nselect date_add(date'1900-01-01', 1);\r\nselect date_add(date'1900-01-01', 1.2);\r\nselect date_add(date'1900-01-01', \"1.2\");\r\n```\r\nare supported, and result the same.\r\n\r\nbut while using `date'1900-01-01' + 1`, we only support int. Is this nonconforming",
        "createdAt" : "2019-12-02T09:04:16Z",
        "updatedAt" : "2019-12-05T08:14:49Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "08b5362c-53ec-49e8-871c-601bd57e1f96",
        "parentId" : "d8628cef-4a7e-4fbd-b15a-a22853487c74",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "This part is really messy. @maropu  AFAIK there is a proposal to update the type coercion rules to follow SQL standard, is it still active?",
        "createdAt" : "2019-12-02T12:30:01Z",
        "updatedAt" : "2019-12-05T08:14:49Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "4be39b44-77e3-4007-bacb-86a6505f7867",
        "parentId" : "d8628cef-4a7e-4fbd-b15a-a22853487c74",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "Em.. I guess that you are pinging @maropu?",
        "createdAt" : "2019-12-02T12:55:44Z",
        "updatedAt" : "2019-12-05T08:14:49Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "3ca97ca0-83b5-461b-af1d-2e81a7b6d239",
        "parentId" : "d8628cef-4a7e-4fbd-b15a-a22853487c74",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Ur, I think there is currently no active work about that... cc: @gengliangwang \r\n@yaooqinn Cannot we easily prohibit the implicit cast for date adds? That's confusing regardless of the standard.",
        "createdAt" : "2019-12-02T22:45:14Z",
        "updatedAt" : "2019-12-05T08:14:49Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "428421cb-1b09-4017-9aac-efa43f99eca9",
        "parentId" : "d8628cef-4a7e-4fbd-b15a-a22853487c74",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "From hive\r\n```\r\nDATE_ADD() only takes TINYINT/SMALLINT/INT types as second argument, got DOUBLE\r\n```",
        "createdAt" : "2019-12-03T02:54:52Z",
        "updatedAt" : "2019-12-05T08:14:49Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "50be112b-b232-41f0-879a-8782e02e24d2",
        "parentId" : "d8628cef-4a7e-4fbd-b15a-a22853487c74",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "@maropu It's true that there no active work about that. We should revisit and try creating a full plan next Q1/Q2.",
        "createdAt" : "2019-12-03T05:45:58Z",
        "updatedAt" : "2019-12-05T08:14:49Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "a44948eccea4ac3f033248babe84c8ab6792d5e0",
    "line" : 42,
    "diffHunk" : "@@ -1,1 +278,282 @@          case (_, CalendarIntervalType) => Cast(TimeAdd(l, r), l.dataType)\n          case (CalendarIntervalType, _) => Cast(TimeAdd(r, l), r.dataType)\n          case (DateType, _) => DateAdd(l, r)\n          case (_, DateType) => DateAdd(r, l)\n          case _ => a"
  },
  {
    "id" : "ade03702-c02b-4d95-b166-70638d4d6774",
    "prId" : 26214,
    "prUrl" : "https://github.com/apache/spark/pull/26214#pullrequestreview-316840622",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f1c5506f-84df-428e-9c3b-9ab0a1607819",
        "parentId" : null,
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "I like that this is refactored into a separate rule. Can we move it to an earlier batch? If metastore views can't contain temp views, then there's no reason to do this in the same batch as table and view resolution from catalogs.",
        "createdAt" : "2019-11-13T23:51:50Z",
        "updatedAt" : "2019-11-21T08:11:47Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      },
      {
        "id" : "36a4e68f-8e58-49f4-ac6c-a75cbc19267c",
        "parentId" : "f1c5506f-84df-428e-9c3b-9ab0a1607819",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "as discussed in the sync, we decide to keep it in the current batch for safety, in case some user-supplied analyzers rules need Spark to resolve unresolved temp views.",
        "createdAt" : "2019-11-14T10:11:50Z",
        "updatedAt" : "2019-11-21T08:11:47Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "c10f28896b20e124532aa36d586a23d2c7bc2810",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +669,673 @@   * [[ResolveTables]].\n   */\n  object ResolveTempViews extends Rule[LogicalPlan] {\n    def apply(plan: LogicalPlan): LogicalPlan = plan.resolveOperatorsUp {\n      case u @ UnresolvedRelation(Seq(part1)) =>"
  },
  {
    "id" : "ba6e4f56-37eb-4a00-a214-1ead3efa5b61",
    "prId" : 26214,
    "prUrl" : "https://github.com/apache/spark/pull/26214#pullrequestreview-321717022",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a816bdc6-529b-4185-be08-70cedc57574f",
        "parentId" : null,
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "This needs to check whether `part1` is a known catalog. If it is a catalog, then it isn't a temp view reference because catalog resolution happens first.\r\n\r\nNot needing to remember that is the purpose of the extractors. I think it would be better to continue using the extractor:\r\n\r\n```scala\r\n  case u @ UnresolvedRelation(AsTemporaryTableIdentifier(ident)) =>\r\n    ident.database match {\r\n      case Some(db) =>\r\n        v1SessionCatalog.lookupGlobalTempView(db, ident.table).getOrElse(u)\r\n      case None =>\r\n        v1SessionCatalog.lookupTempView(ident.table).getOrElse(u)\r\n    }\r\n```",
        "createdAt" : "2019-11-14T00:02:07Z",
        "updatedAt" : "2019-11-21T08:11:47Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      },
      {
        "id" : "612cd4c7-115a-4357-ae07-b59e89bedc35",
        "parentId" : "a816bdc6-529b-4185-be08-70cedc57574f",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "as discussed in the sync, we decide to treat the global temp view name prefix `global_temp` as a special catalog, so that it won't be masked by user-supplied catalog.",
        "createdAt" : "2019-11-14T12:06:45Z",
        "updatedAt" : "2019-11-21T08:11:47Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "73f71071-c8f4-4c2a-90a2-666724ef6b16",
        "parentId" : "a816bdc6-529b-4185-be08-70cedc57574f",
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "If this is intended to only match `global_temp` then it should match `Seq(GLOBAL_TEMP_NAMESPACE, name)` instead of matching `part1`.",
        "createdAt" : "2019-11-21T17:42:39Z",
        "updatedAt" : "2019-11-21T17:42:40Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      },
      {
        "id" : "b6d7a279-6e48-42de-a468-310488c04992",
        "parentId" : "a816bdc6-529b-4185-be08-70cedc57574f",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "unfortunately, `global_temp` is not a constant, it's a static SQL conf that users can set before starting a Spark application(not at runtime)",
        "createdAt" : "2019-11-22T02:28:20Z",
        "updatedAt" : "2019-11-22T02:28:20Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "b9c01819-3c59-4afd-b59a-add89d72e0e2",
        "parentId" : "a816bdc6-529b-4185-be08-70cedc57574f",
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "Yeah, that's why I went ahead with the merge. I think the code is currently correct.\r\n\r\nStill, it would be nice to use the runtime setting here in the matcher instead.",
        "createdAt" : "2019-11-22T18:05:43Z",
        "updatedAt" : "2019-11-22T18:05:44Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      }
    ],
    "commit" : "c10f28896b20e124532aa36d586a23d2c7bc2810",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +673,677 @@      case u @ UnresolvedRelation(Seq(part1)) =>\n        v1SessionCatalog.lookupTempView(part1).getOrElse(u)\n      case u @ UnresolvedRelation(Seq(part1, part2)) =>\n        v1SessionCatalog.lookupGlobalTempView(part1, part2).getOrElse(u)\n    }"
  },
  {
    "id" : "da4cd336-4c90-4fec-87ee-0eacc795fa7e",
    "prId" : 26176,
    "prUrl" : "https://github.com/apache/spark/pull/26176#pullrequestreview-315107330",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bd2403b0-2932-4924-8ddc-81b247f58506",
        "parentId" : null,
        "authorId" : "f06b291a-bc5c-4fd8-9bac-e5a6d035891b",
        "body" : "Why is it '.getOrElse(u)' instead of '.getOrElse(show)' here ?",
        "createdAt" : "2019-11-11T03:01:54Z",
        "updatedAt" : "2019-11-11T19:52:51Z",
        "lastEditedBy" : "f06b291a-bc5c-4fd8-9bac-e5a6d035891b",
        "tags" : [
        ]
      },
      {
        "id" : "3105aecb-920b-4fe3-ae01-2ee7c59514cc",
        "parentId" : "bd2403b0-2932-4924-8ddc-81b247f58506",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "good catch!",
        "createdAt" : "2019-11-11T04:00:04Z",
        "updatedAt" : "2019-11-11T19:52:51Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "7936a661-97c3-4458-a2b3-598badf1745e",
        "parentId" : "bd2403b0-2932-4924-8ddc-81b247f58506",
        "authorId" : "51be1be0-7c72-4f32-a21f-91b1707ec363",
        "body" : "oh, yes good catch!!",
        "createdAt" : "2019-11-11T19:27:21Z",
        "updatedAt" : "2019-11-11T19:52:51Z",
        "lastEditedBy" : "51be1be0-7c72-4f32-a21f-91b1707ec363",
        "tags" : [
        ]
      }
    ],
    "commit" : "846e21a96e106f44fa0b88ab71d07095995b3790",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +696,700 @@          .map(rel => show.copy(table = rel))\n          .getOrElse(show)\n\n      case u: UnresolvedV2Relation =>\n        CatalogV2Util.loadRelation(u.catalog, u.tableName).getOrElse(u)"
  },
  {
    "id" : "ade8b744-bd98-4e74-a3e1-e7481f490c0e",
    "prId" : 26167,
    "prUrl" : "https://github.com/apache/spark/pull/26167#pullrequestreview-313881829",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3fa161af-248d-49e2-88a6-4525b80e6311",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "nit:\r\n```\r\ndef func(\r\n    para1: T,\r\n    para2: T)...\r\n```",
        "createdAt" : "2019-11-08T05:36:45Z",
        "updatedAt" : "2019-11-08T10:56:38Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "953a0003-02f5-4e47-9711-f6774c9d9c49",
        "parentId" : "3fa161af-248d-49e2-88a6-4525b80e6311",
        "authorId" : "8afcf008-6988-44a5-92fa-2bef54bb4058",
        "body" : "done.",
        "createdAt" : "2019-11-08T06:56:12Z",
        "updatedAt" : "2019-11-08T10:56:38Z",
        "lastEditedBy" : "8afcf008-6988-44a5-92fa-2bef54bb4058",
        "tags" : [
        ]
      }
    ],
    "commit" : "391559d9457678eee74f40cba52b09a935b9beb9",
    "line" : 33,
    "diffHunk" : "@@ -1,1 +1208,1212 @@    def resolveAssignments(\n        assignments: Seq[Assignment],\n        mergeInto: MergeIntoTable): Seq[Assignment] = {\n      if (assignments.isEmpty) {\n        val expandedColumns = mergeInto.targetTable.output"
  },
  {
    "id" : "da5c5201-06ae-4639-a752-a6a4f690bea6",
    "prId" : 25955,
    "prUrl" : "https://github.com/apache/spark/pull/25955#pullrequestreview-303607355",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8840e60e-735e-47fd-96b0-cfe4e1255348",
        "parentId" : null,
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "I tested out a trait that worked for all of the plans that need to be resolved here, but the code was longer with the trait and implementations. If we need it later because we have more cases in this rule, it should be easy to add. I don't think we need it right now.",
        "createdAt" : "2019-10-17T23:57:49Z",
        "updatedAt" : "2019-10-30T18:21:04Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      }
    ],
    "commit" : "c223e0537796c1e0a52bc5a655660f89ab539192",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +687,691 @@            .getOrElse(desc)\n\n      case alter @ AlterTable(_, _, u: UnresolvedV2Relation, _) =>\n        CatalogV2Util.loadRelation(u.catalog, u.tableName)\n            .map(rel => alter.copy(table = rel))"
  },
  {
    "id" : "c15cf3db-1351-4626-8745-f8e957a19d4d",
    "prId" : 25955,
    "prUrl" : "https://github.com/apache/spark/pull/25955#pullrequestreview-306083412",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a520c98f-6048-412d-8b3e-c5ee741bdfbc",
        "parentId" : null,
        "authorId" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "body" : "Thanks for incorporating these!",
        "createdAt" : "2019-10-23T18:00:48Z",
        "updatedAt" : "2019-10-30T18:21:04Z",
        "lastEditedBy" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "tags" : [
        ]
      }
    ],
    "commit" : "c223e0537796c1e0a52bc5a655660f89ab539192",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +682,686 @@          .getOrElse(i)\n\n      case desc @ DescribeTable(u: UnresolvedV2Relation, _) =>\n        CatalogV2Util.loadRelation(u.catalog, u.tableName)\n            .map(rel => desc.copy(table = rel))"
  },
  {
    "id" : "e5c547b6-6055-4e90-aea1-85efdc0dfc93",
    "prId" : 25854,
    "prUrl" : "https://github.com/apache/spark/pull/25854#pullrequestreview-656377476",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d88e4464-b00a-4812-959e-8c292bf1abd7",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Can't recall the details, but why it's not `Seq(j.left, j.right)`?",
        "createdAt" : "2021-05-10T15:19:15Z",
        "updatedAt" : "2021-05-10T15:19:15Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "c03893d3-d40d-4139-a2bb-0bd235ad05a5",
        "parentId" : "d88e4464-b00a-4812-959e-8c292bf1abd7",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> Can't recall the details, but why it's not `Seq(j.left, j.right)`?\r\n\r\nShould be a mistake, raise a pr and remove this?",
        "createdAt" : "2021-05-11T07:37:06Z",
        "updatedAt" : "2021-05-11T07:37:06Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "307802a5e599fadfafc9fb4ffe2ccbd10d60f6ba",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1699,1703 @@        resolveSubQueries(q, q.children)\n      case j: Join if j.childrenResolved =>\n        resolveSubQueries(j, Seq(j, j.left, j.right))\n      case s: SupportsSubquery if s.childrenResolved =>\n        resolveSubQueries(s, s.children)"
  },
  {
    "id" : "8eca01f6-1962-413a-a7cc-dad7f1f68fdc",
    "prId" : 25774,
    "prUrl" : "https://github.com/apache/spark/pull/25774#pullrequestreview-287469160",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c6ccce79-bfbe-4a9b-a52d-520635e13e7a",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "simpler to `ResolveRelations`, `ResolveTables` should handle both `UnresolvedRelation` and `InsertIntoStatement(UnresolvedRelation, ...)`.",
        "createdAt" : "2019-09-12T14:24:52Z",
        "updatedAt" : "2019-09-12T14:24:52Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "2753af5c2adbbb0c27decda3afb7a06e8ff1a31f",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +673,677 @@        v2TableOpt.map(DataSourceV2Relation.create).getOrElse(u)\n\n      case i @ InsertIntoStatement(u: UnresolvedRelation, _, _, _, _) if i.query.resolved =>\n        val v2TableOpt = lookupV2Relation(u.multipartIdentifier) match {\n          case scala.Left((_, _, tableOpt)) => tableOpt"
  },
  {
    "id" : "9366c475-dd5e-46cb-873e-5584a80fe667",
    "prId" : 25774,
    "prUrl" : "https://github.com/apache/spark/pull/25774#pullrequestreview-287469330",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "26c7dbab-9a00-47af-ac9d-c3c06a9ab2a7",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "just indentation changes.",
        "createdAt" : "2019-09-12T14:25:05Z",
        "updatedAt" : "2019-09-12T14:25:05Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "2753af5c2adbbb0c27decda3afb7a06e8ff1a31f",
    "line" : 29,
    "diffHunk" : "@@ -1,1 +796,800 @@    override def apply(plan: LogicalPlan): LogicalPlan = plan resolveOperators {\n      case i @ InsertIntoStatement(r: DataSourceV2Relation, _, _, _, _) if i.query.resolved =>\n        // ifPartitionNotExists is append with validation, but validation is not supported\n        if (i.ifPartitionNotExists) {\n          throw new AnalysisException("
  },
  {
    "id" : "0aa67ebd-a54a-4cce-b12e-2cc4b7f9721b",
    "prId" : 25512,
    "prUrl" : "https://github.com/apache/spark/pull/25512#pullrequestreview-282590587",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4a847d54-abf2-4b78-be9f-ec09a8345d13",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "nit: can we keep it close to other Project cases?",
        "createdAt" : "2019-08-30T07:57:18Z",
        "updatedAt" : "2019-09-04T10:00:56Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "fc872a99-2037-4119-a81f-dbd42c592abf",
        "parentId" : "4a847d54-abf2-4b78-be9f-ec09a8345d13",
        "authorId" : "7e665d8b-d739-4edf-88c8-7379ff8585c2",
        "body" : "I prefer current switch case order.\r\nThe first four switch case only do some check so put them in front.",
        "createdAt" : "2019-09-02T13:20:15Z",
        "updatedAt" : "2019-09-04T10:00:56Z",
        "lastEditedBy" : "7e665d8b-d739-4edf-88c8-7379ff8585c2",
        "tags" : [
        ]
      }
    ],
    "commit" : "a839035ec424af67fe12c917a1bab2782e0053cb",
    "line" : 56,
    "diffHunk" : "@@ -1,1 +2071,2075 @@        Project(projectExprs.toList, newAgg)\n\n      case p @ Project(projectList, child) =>\n        // Holds the resolved generator, if one exists in the project list.\n        var resolvedGenerator: Generate = null"
  },
  {
    "id" : "cac0f25b-4c51-4226-b2ba-64dd91167d9a",
    "prId" : 25507,
    "prUrl" : "https://github.com/apache/spark/pull/25507#pullrequestreview-279676554",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3c3225cd-7721-4f10-a4a1-e33f7b8895db",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "`lookupV2Relation` needs to return `Either` because we need to catch this case and create v1 insert command. I have a simpler idea:\r\n1. `ResolveInsertInto` only handles v2 insert command, and leave `InsertIntoStatement(UnresolvedRelation, ...)` unchanged if we can't load a v2 table.\r\n2. `ResolveRelations` catches the `InsertIntoStatement(UnresolvedRelation, ...)` and convert it to v1 insert command if we can load a v1 table.\r\n\r\nBy doing this, `lookupV2Relation` doesn't need to distinguish between normal v2 catalog and v2 session catalog during table lookup, and can return `Option[Table]`.",
        "createdAt" : "2019-08-26T12:07:54Z",
        "updatedAt" : "2019-08-26T18:57:39Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "a01e0f37-8d8f-4ce1-b68d-20b255bd0cb8",
        "parentId" : "3c3225cd-7721-4f10-a4a1-e33f7b8895db",
        "authorId" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "body" : "I was looking at ALTER TABLE as well, and it seems like `lookupV2Relation` will need to support Either in cases where we do have a catalog name. I was thinking that:\r\n```scala\r\nEither[(CatalogPlugin, Ident, Option[Table]), Option[(Ident, Table)]] match {\r\n  case Left => // Catalog found, table may or may not be found\r\n  case Right => // Table may be resolved as V2 or left for V1\r\n}\r\n```",
        "createdAt" : "2019-08-26T15:54:25Z",
        "updatedAt" : "2019-08-26T18:57:39Z",
        "lastEditedBy" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "tags" : [
        ]
      }
    ],
    "commit" : "f1d86fffdafaac99a040ecd718f9e16aa49d164c",
    "line" : 70,
    "diffHunk" : "@@ -1,1 +769,773 @@            resolveV2Insert(i, v2Table)\n          case _ =>\n            InsertIntoTable(i.table, i.partitionSpec, i.query, i.overwrite, i.ifPartitionNotExists)\n        }\n    }"
  },
  {
    "id" : "56e09513-ebe5-49c1-9641-675981108b2c",
    "prId" : 25502,
    "prUrl" : "https://github.com/apache/spark/pull/25502#pullrequestreview-281841032",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a3b66dc7-d9cc-440a-94a3-01e8cab08a7e",
        "parentId" : null,
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "I understand the motivation to create the `lookupV2Relation` method, but I don't quite like the effects that it has on these rules because it mixes identifier resolution (which catalog is responsible?) with table resolution (look up the table in a catalog). This leads to some strange changes, like the addition of `AlterTableStatement` to `checkAnalysis` that throws \"Table or view not found\" when the real problem is that the statement wasn't converted to a v1 or v2 plan.\r\n\r\nBefore this change, only identifier resolution is done. Table resolution is done by the `ResolveTables` rule so it is fairly well contained. Plans were created with a place-holder `UnresolvedRelation`, which avoids the need to catch `AlterTableStatement` in `checkAnalysis`\r\n\r\nBut, to add the fallback to v1 for tables that are loaded by the v2 session catalog, we have to look up the table and only convert to the v2 plan if it isn't an `UnresolvedTable`.\r\n\r\nI'm not sure what the _right_ solution is. Maybe instead of avoiding conversion to the v2 plan, we should convert from v2 to v1 for the fallback case. I think that would make the rules cleaner and more orthogonal to one another.",
        "createdAt" : "2019-08-29T22:45:32Z",
        "updatedAt" : "2019-08-29T23:18:17Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      },
      {
        "id" : "016b8fb6-6eb7-47f3-b0ac-d6de76592485",
        "parentId" : "a3b66dc7-d9cc-440a-94a3-01e8cab08a7e",
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "I don't think this is a reason to block this commit. I would just like to follow up with a reasonable solution if there isn't a quick fix that can go in this PR. The important thing is getting this PR in so this makes it into 3.0.",
        "createdAt" : "2019-08-29T23:00:54Z",
        "updatedAt" : "2019-08-29T23:18:17Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      },
      {
        "id" : "a154f846-29b3-47ca-b979-56c69d36328b",
        "parentId" : "a3b66dc7-d9cc-440a-94a3-01e8cab08a7e",
        "authorId" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "body" : "> mixes identifier resolution (which catalog is responsible?) with table resolution (look up the table in a catalog)\r\nthat's fair and makes a lot of sense to keep these separate. I'm in favor of merging this as a first step (which will allow the tests to be there), and then cleaning up the logic, since the logic will look similar for INSERT INTO as well. Since this is already a 900loc PR, I'd like to that in a follow up.",
        "createdAt" : "2019-08-29T23:17:18Z",
        "updatedAt" : "2019-08-29T23:18:17Z",
        "lastEditedBy" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "tags" : [
        ]
      },
      {
        "id" : "33a97efc-772d-4a2c-9e16-c6a6f77e772f",
        "parentId" : "a3b66dc7-d9cc-440a-94a3-01e8cab08a7e",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I also thought about it before. I think the ideal resolution process is:\r\n1. rules like `ResolveAlterTable` are only responsible for converting XYZStatement to v1 or v2 command\r\n2. `ResolveTables` and `ResolveRelations` are responsible for resolving `UnresolvedRelation` to v1 or v2 relations\r\n\r\nHowever, some commands like ALTER TABLE also need to get the catalog instance, which can't be done by `ResolveTables` or `ResolveRelations`. Unlike table resolution which replaces `UnresolvedRelation` with v1/v2 relation and can be done by a rule separately. Catalog resolution needs to be done during the converting from XYZStatement to v1/v2 command and we can't do it in a separated rule.\r\n\r\nI don't have a good idea now but we should definitely revisit it later.\r\n\r\n",
        "createdAt" : "2019-08-30T06:02:16Z",
        "updatedAt" : "2019-08-30T06:02:17Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "1f821986e7458ca4b87c898d6c3c6d73b0ac78ad",
    "line" : 93,
    "diffHunk" : "@@ -1,1 +943,947 @@    private def resolveV2Alter(\n        tableName: Seq[String],\n        changes: Seq[TableChange]): Option[AlterTable] = {\n      lookupV2Relation(tableName) match {\n        case scala.Left((v2Catalog, ident, tableOpt)) =>"
  },
  {
    "id" : "1ea3f24d-6355-49e5-b40e-7053bf132a35",
    "prId" : 25453,
    "prUrl" : "https://github.com/apache/spark/pull/25453#pullrequestreview-276895508",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8074330f-7241-4e51-acac-0c94779caae2",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "we don't need this now",
        "createdAt" : "2019-08-20T01:52:47Z",
        "updatedAt" : "2019-08-22T08:27:31Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "eb9442cdeadd2ab2232c82a3dfb65055489e4e06",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +43,47 @@import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation\nimport org.apache.spark.sql.internal.SQLConf\nimport org.apache.spark.sql.internal.SQLConf.{PartitionOverwriteMode, StoreAssignmentPolicy}\nimport org.apache.spark.sql.sources.v2.Table\nimport org.apache.spark.sql.sources.v2.internal.UnresolvedTable"
  },
  {
    "id" : "53ce6109-c6e1-4ed2-957c-35782ea57f11",
    "prId" : 25453,
    "prUrl" : "https://github.com/apache/spark/pull/25453#pullrequestreview-276959101",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "51cf7979-5cc9-4a0d-9225-d7c0701d74e4",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "The code in line2528-2535, line2539-2546, and line2550-2557 is duplicate, so can we merge them into one rule by defining an extractor(unapply)?",
        "createdAt" : "2019-08-20T01:56:52Z",
        "updatedAt" : "2019-08-22T08:27:31Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "b31abcc5-de9e-4236-a06c-8588dcb4c434",
        "parentId" : "51cf7979-5cc9-4a0d-9225-d7c0701d74e4",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "btw, `ResolveOutputRelation` currently seems to be a thin wrapper for `TableOutputResolver`. Do we need to separate `TableOutputResolver` from `ResolveOutputRelation`?",
        "createdAt" : "2019-08-20T01:59:05Z",
        "updatedAt" : "2019-08-22T08:27:31Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "e42e0daf-35c6-4627-a3a8-4a2d10c3ab09",
        "parentId" : "51cf7979-5cc9-4a0d-9225-d7c0701d74e4",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "> The code in line2528-2535, line2539-2546, and line2550-2557 is duplicate, so can we merge them into one rule by defining an extractor(unapply)?\r\n\r\nEventually we need to call `append.copy(query = projection)` and `overwrite.copy(query = projection)`, we need to match all the `V2WriteCommand` anyway. I think it is fine to keep the current code.",
        "createdAt" : "2019-08-20T06:45:01Z",
        "updatedAt" : "2019-08-22T08:27:31Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "bcaa645e-1a2c-4790-bd7b-8c1eaf6c112c",
        "parentId" : "51cf7979-5cc9-4a0d-9225-d7c0701d74e4",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "> btw, ResolveOutputRelation currently seems to be a thin wrapper for TableOutputResolver. Do we need to separate TableOutputResolver from ResolveOutputRelation?\r\n\r\nOtherwise, we can't access the method `resolveOutputColumns` in `PreprocessTableInsertion`",
        "createdAt" : "2019-08-20T06:45:57Z",
        "updatedAt" : "2019-08-22T08:27:31Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "eb9442cdeadd2ab2232c82a3dfb65055489e4e06",
    "line" : 26,
    "diffHunk" : "@@ -1,1 +2529,2533 @@          TableOutputResolver.resolveOutputColumns(\n            table.name, table.output, query, isByName, conf, storeAssignmentPolicy)\n\n        if (projection != query) {\n          append.copy(query = projection)"
  },
  {
    "id" : "56d3d198-202e-4843-9eb7-82fe52e433ac",
    "prId" : 25453,
    "prUrl" : "https://github.com/apache/spark/pull/25453#pullrequestreview-277574194",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "355b9d03-a3d7-4452-a6ff-4122d0c85d19",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "We might need to update the description for `ResolveOutputRelation` and refine \"what's a safe cast?\".",
        "createdAt" : "2019-08-21T00:15:42Z",
        "updatedAt" : "2019-08-22T08:27:31Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "01605e23-2733-4962-9640-22a05dcba84a",
        "parentId" : "355b9d03-a3d7-4452-a6ff-4122d0c85d19",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "btw, we might need to move some parts of this description to `TableOutputResolver`.",
        "createdAt" : "2019-08-21T00:20:20Z",
        "updatedAt" : "2019-08-22T08:27:31Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "1e284edd-d98f-4231-9d87-b55871f1b89f",
        "parentId" : "355b9d03-a3d7-4452-a6ff-4122d0c85d19",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "I have removed the word \"safe\" in the comment. I think the comment \"Detect plans that are not compatible with the output table and throw AnalysisException\" already states that there is type coercion check in the rule.",
        "createdAt" : "2019-08-21T06:09:41Z",
        "updatedAt" : "2019-08-22T08:27:31Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "eb9442cdeadd2ab2232c82a3dfb65055489e4e06",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +2523,2527 @@   */\n  object ResolveOutputRelation extends Rule[LogicalPlan] {\n    override def apply(plan: LogicalPlan): LogicalPlan = plan.resolveOperators {\n      case append @ AppendData(table, query, isByName)\n          if table.resolved && query.resolved && !append.outputResolved =>"
  },
  {
    "id" : "f65b7eaf-9d34-4e84-97c7-0cdcef549cae",
    "prId" : 25402,
    "prUrl" : "https://github.com/apache/spark/pull/25402#pullrequestreview-274652987",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "639db2ba-31a2-4ee0-9f1a-ff04fff22882",
        "parentId" : null,
        "authorId" : "8afcf008-6988-44a5-92fa-2bef54bb4058",
        "body" : "A +1 on this.",
        "createdAt" : "2019-08-14T03:27:52Z",
        "updatedAt" : "2019-08-14T16:08:07Z",
        "lastEditedBy" : "8afcf008-6988-44a5-92fa-2bef54bb4058",
        "tags" : [
        ]
      }
    ],
    "commit" : "a70e72676da6442a45e3a358c734b6f161c615d0",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +659,663 @@            case resolved => DataSourceV2Relation.create(resolved)\n          }\n          .getOrElse(u)\n    }\n  }"
  }
]