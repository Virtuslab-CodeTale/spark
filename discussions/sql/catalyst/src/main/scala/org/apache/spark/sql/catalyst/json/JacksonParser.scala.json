[
  {
    "id" : "8ad659d8-7177-4c8f-9632-a4dc476efa8c",
    "prId" : 33742,
    "prUrl" : "https://github.com/apache/spark/pull/33742#pullrequestreview-730075292",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "68c82d71-b347-4f64-b563-b92a3738ecfa",
        "parentId" : null,
        "authorId" : "3a227965-84e0-47cf-9974-11293764f028",
        "body" : "Casting from numerical types to `timestamp_ntz` seems not supported so I don't care `VALUE_NUMBER_INT` here.",
        "createdAt" : "2021-08-14T07:57:08Z",
        "updatedAt" : "2021-08-14T07:57:08Z",
        "lastEditedBy" : "3a227965-84e0-47cf-9974-11293764f028",
        "tags" : [
        ]
      }
    ],
    "commit" : "31c9851d11b606970918d8d87899419f6e92bedd",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +263,267 @@      (parser: JsonParser) => parseJsonToken[java.lang.Long](parser, dataType) {\n        case VALUE_STRING if parser.getTextLength >= 1 =>\n          timestampNTZFormatter.parseWithoutTimeZone(parser.getText)\n      }\n"
  },
  {
    "id" : "a6644eb0-3bf9-4dde-86b8-521ebbaacf38",
    "prId" : 33654,
    "prUrl" : "https://github.com/apache/spark/pull/33654#pullrequestreview-723323184",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4194ea6c-7173-4fdc-831b-3150b4e48fb9",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "unsupported data type is kind of a fatal error. I think throwing an exception makes more sense, as this should not happen. We should finish the TIMESTAMP NTZ support before Spark 3.3.",
        "createdAt" : "2021-08-05T13:00:19Z",
        "updatedAt" : "2021-08-05T13:00:46Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "39d7c26aaef799b6901c4b88982ca3c9cf8f531d",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +332,336 @@\n    // We don't actually hit this exception though, we keep it for understandability\n    case _ => throw QueryExecutionErrors.unsupportedTypeError(dataType)\n  }\n"
  },
  {
    "id" : "f5790698-229c-4a7a-986f-06212cb89f25",
    "prId" : 33212,
    "prUrl" : "https://github.com/apache/spark/pull/33212#pullrequestreview-706214688",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e37e84e4-2cf0-4451-9977-6d3685cdce9a",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Does this additional check cause no performance regression? For example, if a schema does not have non-nullable fields at all, we don't need this check, right?",
        "createdAt" : "2021-07-12T06:44:12Z",
        "updatedAt" : "2021-07-12T06:44:13Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "7ae27084-005a-417a-b216-d53769565043",
        "parentId" : "e37e84e4-2cf0-4451-9977-6d3685cdce9a",
        "authorId" : "c12eb4aa-c39d-4fa9-8470-ed071c0be24a",
        "body" : "Yes, you are right. this check has two purposes.\r\n1. check the non-nullable field. \r\n2. skipping row using the missing field.\r\n\r\nDo you have a better idea to implement that without performance regression?",
        "createdAt" : "2021-07-12T07:12:41Z",
        "updatedAt" : "2021-07-12T07:12:41Z",
        "lastEditedBy" : "c12eb4aa-c39d-4fa9-8470-ed071c0be24a",
        "tags" : [
        ]
      },
      {
        "id" : "91d8dc43-a49e-4ce2-9071-02640ee7b12f",
        "parentId" : "e37e84e4-2cf0-4451-9977-6d3685cdce9a",
        "authorId" : "c12eb4aa-c39d-4fa9-8470-ed071c0be24a",
        "body" : "skip this additional check when the mode is `PERMISSIVE`.",
        "createdAt" : "2021-07-14T12:30:58Z",
        "updatedAt" : "2021-07-14T12:30:59Z",
        "lastEditedBy" : "c12eb4aa-c39d-4fa9-8470-ed071c0be24a",
        "tags" : [
        ]
      }
    ],
    "commit" : "9fb8fbd5598eb0d80c11bf7e129ad5c9b146f837",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +427,431 @@    }\n\n    // When the input schema is setting to `nullable = false`, make sure the field is not null.\n    checkNotNullableInRow(row, schema, skipRow, badRecordException)\n"
  },
  {
    "id" : "902f3d76-9f1b-4ee5-b9f5-41c170be1993",
    "prId" : 33212,
    "prUrl" : "https://github.com/apache/spark/pull/33212#pullrequestreview-710156125",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2507f75f-cea5-47c2-826c-3cad53f9222c",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I .. am not sure if we really need these complicated fix to address nullability mismatch (which is rather a corner case) to be honest. I wonder if there's a simpler approach, e.g.) simply warning on non-nullable columns?\r\nJust to be clear, I don't mind if other committers prefer to fix it.",
        "createdAt" : "2021-07-20T00:44:07Z",
        "updatedAt" : "2021-07-20T00:44:07Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "483ed5ee-04ac-4570-be75-d625139d1600",
        "parentId" : "2507f75f-cea5-47c2-826c-3cad53f9222c",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "That's another proposal I mentioned earlier: if the user-given schema is not-nullable, we just turn it into nullable schema and don't fail.",
        "createdAt" : "2021-07-20T03:12:07Z",
        "updatedAt" : "2021-07-20T03:12:07Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "7425fa55-b97e-4827-b684-f9ef4756acc1",
        "parentId" : "2507f75f-cea5-47c2-826c-3cad53f9222c",
        "authorId" : "c12eb4aa-c39d-4fa9-8470-ed071c0be24a",
        "body" : "Thank you for your suggestions, I'll raise a new PR.",
        "createdAt" : "2021-07-20T03:55:32Z",
        "updatedAt" : "2021-07-20T03:55:32Z",
        "lastEditedBy" : "c12eb4aa-c39d-4fa9-8470-ed071c0be24a",
        "tags" : [
        ]
      }
    ],
    "commit" : "9fb8fbd5598eb0d80c11bf7e129ad5c9b146f837",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +406,410 @@        case Some(index) =>\n          try {\n            val fieldValue = fieldConverters(index).apply(parser)\n            val isIllegal =\n              options.parseMode != PermissiveMode && !schema(index).nullable && fieldValue == null"
  },
  {
    "id" : "eee2118a-c3c2-4d26-8c55-c48a97ee66bf",
    "prId" : 32599,
    "prUrl" : "https://github.com/apache/spark/pull/32599#pullrequestreview-667167168",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5d1b7081-2c95-4444-a06e-e964884d94fc",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Can we reuse `makeConverter`?",
        "createdAt" : "2021-05-24T00:46:15Z",
        "updatedAt" : "2021-05-24T00:46:15Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "4006ab97-4b14-4c54-b28d-1f4248441002",
        "parentId" : "5d1b7081-2c95-4444-a06e-e964884d94fc",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I think it's different from `makeConverter`, as the JSON key is always string: `{\"key\": value}`.\r\n\r\nThat said, I'm not sure if this feature makes sense. If we turn a JSON string into a map, the key type should always be string type. Users can run `transform_keys` function to cast the key string to whatever data type they want.\r\n\r\nWe should probably improve the error message though, if a user specifies a MapType with non-string key type. cc @MaxGekk ",
        "createdAt" : "2021-05-24T16:18:47Z",
        "updatedAt" : "2021-05-24T16:18:47Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "93ab3da8-4b66-4d4f-8a4a-2e5139f922e6",
        "parentId" : "5d1b7081-2c95-4444-a06e-e964884d94fc",
        "authorId" : "51be1be0-7c72-4f32-a21f-91b1707ec363",
        "body" : "> Can we reuse `makeConverter`?\r\n\r\nThe procesing of the keys is different as Cloud-fan said so I think we cant reuse it.",
        "createdAt" : "2021-05-24T20:23:23Z",
        "updatedAt" : "2021-05-24T20:23:23Z",
        "lastEditedBy" : "51be1be0-7c72-4f32-a21f-91b1707ec363",
        "tags" : [
        ]
      },
      {
        "id" : "3c611caf-d234-4930-b310-e796c6d689c1",
        "parentId" : "5d1b7081-2c95-4444-a06e-e964884d94fc",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "+1 for @cloud-fan's idea",
        "createdAt" : "2021-05-24T21:05:14Z",
        "updatedAt" : "2021-05-24T21:05:14Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "b8a2bfea30d9d1750362588d5455557642d0dcad",
    "line" : 42,
    "diffHunk" : "@@ -1,1 +330,334 @@  }\n\n  def makeKeyConverter(dataType: DataType): KeyConverter = dataType match {\n    case BooleanType => (key: String) => key.toBoolean\n    case ByteType => (key: String) => key.toByte"
  },
  {
    "id" : "f327208c-7390-494f-b055-872e3832d681",
    "prId" : 27710,
    "prUrl" : "https://github.com/apache/spark/pull/27710#pullrequestreview-405616486",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fd5ef363-622e-4555-88ee-c3121006c492",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Should we rebase this?",
        "createdAt" : "2020-05-01T21:53:38Z",
        "updatedAt" : "2020-05-01T21:53:38Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "1a37b614-bc9a-4c9b-98ca-7a235e8de641",
        "parentId" : "fd5ef363-622e-4555-88ee-c3121006c492",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "good catch! I think we should.",
        "createdAt" : "2020-05-05T05:35:11Z",
        "updatedAt" : "2020-05-05T05:35:12Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "79345625-c54c-421d-905b-728a8e5dd6e4",
        "parentId" : "fd5ef363-622e-4555-88ee-c3121006c492",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Here is the PR https://github.com/apache/spark/pull/28453",
        "createdAt" : "2020-05-05T09:13:42Z",
        "updatedAt" : "2020-05-05T09:13:43Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "16db32f50449bb4bb721101c709ea56cc305d324",
    "line" : 41,
    "diffHunk" : "@@ -1,1 +258,262 @@                // So, we just convert it to Int.\n                try {\n                  parser.getText.toInt\n                } catch {\n                  case _: NumberFormatException => throw e"
  }
]