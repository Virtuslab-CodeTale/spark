[
  {
    "id" : "8ad659d8-7177-4c8f-9632-a4dc476efa8c",
    "prId" : 33742,
    "prUrl" : "https://github.com/apache/spark/pull/33742#pullrequestreview-730075292",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "68c82d71-b347-4f64-b563-b92a3738ecfa",
        "parentId" : null,
        "authorId" : "3a227965-84e0-47cf-9974-11293764f028",
        "body" : "Casting from numerical types to `timestamp_ntz` seems not supported so I don't care `VALUE_NUMBER_INT` here.",
        "createdAt" : "2021-08-14T07:57:08Z",
        "updatedAt" : "2021-08-14T07:57:08Z",
        "lastEditedBy" : "3a227965-84e0-47cf-9974-11293764f028",
        "tags" : [
        ]
      }
    ],
    "commit" : "31c9851d11b606970918d8d87899419f6e92bedd",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +263,267 @@      (parser: JsonParser) => parseJsonToken[java.lang.Long](parser, dataType) {\n        case VALUE_STRING if parser.getTextLength >= 1 =>\n          timestampNTZFormatter.parseWithoutTimeZone(parser.getText)\n      }\n"
  },
  {
    "id" : "a6644eb0-3bf9-4dde-86b8-521ebbaacf38",
    "prId" : 33654,
    "prUrl" : "https://github.com/apache/spark/pull/33654#pullrequestreview-723323184",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4194ea6c-7173-4fdc-831b-3150b4e48fb9",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "unsupported data type is kind of a fatal error. I think throwing an exception makes more sense, as this should not happen. We should finish the TIMESTAMP NTZ support before Spark 3.3.",
        "createdAt" : "2021-08-05T13:00:19Z",
        "updatedAt" : "2021-08-05T13:00:46Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "39d7c26aaef799b6901c4b88982ca3c9cf8f531d",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +332,336 @@\n    // We don't actually hit this exception though, we keep it for understandability\n    case _ => throw QueryExecutionErrors.unsupportedTypeError(dataType)\n  }\n"
  },
  {
    "id" : "f5790698-229c-4a7a-986f-06212cb89f25",
    "prId" : 33212,
    "prUrl" : "https://github.com/apache/spark/pull/33212#pullrequestreview-706214688",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e37e84e4-2cf0-4451-9977-6d3685cdce9a",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Does this additional check cause no performance regression? For example, if a schema does not have non-nullable fields at all, we don't need this check, right?",
        "createdAt" : "2021-07-12T06:44:12Z",
        "updatedAt" : "2021-07-12T06:44:13Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "7ae27084-005a-417a-b216-d53769565043",
        "parentId" : "e37e84e4-2cf0-4451-9977-6d3685cdce9a",
        "authorId" : "c12eb4aa-c39d-4fa9-8470-ed071c0be24a",
        "body" : "Yes, you are right. this check has two purposes.\r\n1. check the non-nullable field. \r\n2. skipping row using the missing field.\r\n\r\nDo you have a better idea to implement that without performance regression?",
        "createdAt" : "2021-07-12T07:12:41Z",
        "updatedAt" : "2021-07-12T07:12:41Z",
        "lastEditedBy" : "c12eb4aa-c39d-4fa9-8470-ed071c0be24a",
        "tags" : [
        ]
      },
      {
        "id" : "91d8dc43-a49e-4ce2-9071-02640ee7b12f",
        "parentId" : "e37e84e4-2cf0-4451-9977-6d3685cdce9a",
        "authorId" : "c12eb4aa-c39d-4fa9-8470-ed071c0be24a",
        "body" : "skip this additional check when the mode is `PERMISSIVE`.",
        "createdAt" : "2021-07-14T12:30:58Z",
        "updatedAt" : "2021-07-14T12:30:59Z",
        "lastEditedBy" : "c12eb4aa-c39d-4fa9-8470-ed071c0be24a",
        "tags" : [
        ]
      }
    ],
    "commit" : "9fb8fbd5598eb0d80c11bf7e129ad5c9b146f837",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +427,431 @@    }\n\n    // When the input schema is setting to `nullable = false`, make sure the field is not null.\n    checkNotNullableInRow(row, schema, skipRow, badRecordException)\n"
  },
  {
    "id" : "902f3d76-9f1b-4ee5-b9f5-41c170be1993",
    "prId" : 33212,
    "prUrl" : "https://github.com/apache/spark/pull/33212#pullrequestreview-710156125",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2507f75f-cea5-47c2-826c-3cad53f9222c",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I .. am not sure if we really need these complicated fix to address nullability mismatch (which is rather a corner case) to be honest. I wonder if there's a simpler approach, e.g.) simply warning on non-nullable columns?\r\nJust to be clear, I don't mind if other committers prefer to fix it.",
        "createdAt" : "2021-07-20T00:44:07Z",
        "updatedAt" : "2021-07-20T00:44:07Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "483ed5ee-04ac-4570-be75-d625139d1600",
        "parentId" : "2507f75f-cea5-47c2-826c-3cad53f9222c",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "That's another proposal I mentioned earlier: if the user-given schema is not-nullable, we just turn it into nullable schema and don't fail.",
        "createdAt" : "2021-07-20T03:12:07Z",
        "updatedAt" : "2021-07-20T03:12:07Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "7425fa55-b97e-4827-b684-f9ef4756acc1",
        "parentId" : "2507f75f-cea5-47c2-826c-3cad53f9222c",
        "authorId" : "c12eb4aa-c39d-4fa9-8470-ed071c0be24a",
        "body" : "Thank you for your suggestions, I'll raise a new PR.",
        "createdAt" : "2021-07-20T03:55:32Z",
        "updatedAt" : "2021-07-20T03:55:32Z",
        "lastEditedBy" : "c12eb4aa-c39d-4fa9-8470-ed071c0be24a",
        "tags" : [
        ]
      }
    ],
    "commit" : "9fb8fbd5598eb0d80c11bf7e129ad5c9b146f837",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +406,410 @@        case Some(index) =>\n          try {\n            val fieldValue = fieldConverters(index).apply(parser)\n            val isIllegal =\n              options.parseMode != PermissiveMode && !schema(index).nullable && fieldValue == null"
  },
  {
    "id" : "eee2118a-c3c2-4d26-8c55-c48a97ee66bf",
    "prId" : 32599,
    "prUrl" : "https://github.com/apache/spark/pull/32599#pullrequestreview-667167168",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5d1b7081-2c95-4444-a06e-e964884d94fc",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Can we reuse `makeConverter`?",
        "createdAt" : "2021-05-24T00:46:15Z",
        "updatedAt" : "2021-05-24T00:46:15Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "4006ab97-4b14-4c54-b28d-1f4248441002",
        "parentId" : "5d1b7081-2c95-4444-a06e-e964884d94fc",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I think it's different from `makeConverter`, as the JSON key is always string: `{\"key\": value}`.\r\n\r\nThat said, I'm not sure if this feature makes sense. If we turn a JSON string into a map, the key type should always be string type. Users can run `transform_keys` function to cast the key string to whatever data type they want.\r\n\r\nWe should probably improve the error message though, if a user specifies a MapType with non-string key type. cc @MaxGekk ",
        "createdAt" : "2021-05-24T16:18:47Z",
        "updatedAt" : "2021-05-24T16:18:47Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "93ab3da8-4b66-4d4f-8a4a-2e5139f922e6",
        "parentId" : "5d1b7081-2c95-4444-a06e-e964884d94fc",
        "authorId" : "51be1be0-7c72-4f32-a21f-91b1707ec363",
        "body" : "> Can we reuse `makeConverter`?\r\n\r\nThe procesing of the keys is different as Cloud-fan said so I think we cant reuse it.",
        "createdAt" : "2021-05-24T20:23:23Z",
        "updatedAt" : "2021-05-24T20:23:23Z",
        "lastEditedBy" : "51be1be0-7c72-4f32-a21f-91b1707ec363",
        "tags" : [
        ]
      },
      {
        "id" : "3c611caf-d234-4930-b310-e796c6d689c1",
        "parentId" : "5d1b7081-2c95-4444-a06e-e964884d94fc",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "+1 for @cloud-fan's idea",
        "createdAt" : "2021-05-24T21:05:14Z",
        "updatedAt" : "2021-05-24T21:05:14Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "b8a2bfea30d9d1750362588d5455557642d0dcad",
    "line" : 42,
    "diffHunk" : "@@ -1,1 +330,334 @@  }\n\n  def makeKeyConverter(dataType: DataType): KeyConverter = dataType match {\n    case BooleanType => (key: String) => key.toBoolean\n    case ByteType => (key: String) => key.toByte"
  },
  {
    "id" : "f327208c-7390-494f-b055-872e3832d681",
    "prId" : 27710,
    "prUrl" : "https://github.com/apache/spark/pull/27710#pullrequestreview-405616486",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fd5ef363-622e-4555-88ee-c3121006c492",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Should we rebase this?",
        "createdAt" : "2020-05-01T21:53:38Z",
        "updatedAt" : "2020-05-01T21:53:38Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "1a37b614-bc9a-4c9b-98ca-7a235e8de641",
        "parentId" : "fd5ef363-622e-4555-88ee-c3121006c492",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "good catch! I think we should.",
        "createdAt" : "2020-05-05T05:35:11Z",
        "updatedAt" : "2020-05-05T05:35:12Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "79345625-c54c-421d-905b-728a8e5dd6e4",
        "parentId" : "fd5ef363-622e-4555-88ee-c3121006c492",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Here is the PR https://github.com/apache/spark/pull/28453",
        "createdAt" : "2020-05-05T09:13:42Z",
        "updatedAt" : "2020-05-05T09:13:43Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "16db32f50449bb4bb721101c709ea56cc305d324",
    "line" : 41,
    "diffHunk" : "@@ -1,1 +258,262 @@                // So, we just convert it to Int.\n                try {\n                  parser.getText.toInt\n                } catch {\n                  case _: NumberFormatException => throw e"
  },
  {
    "id" : "c8c9cb05-6344-48af-89db-c710385500b8",
    "prId" : 27366,
    "prUrl" : "https://github.com/apache/spark/pull/27366#pullrequestreview-351499867",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "238209f9-a7a2-4a70-8cd1-d7f3de515063",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Hm, if I didn't misread, this wouldn't work properly (with https://github.com/apache/spark/pull/27366/files#diff-635e02b2d1ce4ad1675b0350ccac0c10R266) if we happen to push down predicates that refer nested columns. \r\n\r\nSeems a bit odd because the previous code didn't have such assumption before and all codes were valid for both root and nested rows. Now this method works differently for root row and nested row. I wonder if we can decouple the filtering logic out of this method ...",
        "createdAt" : "2020-01-31T12:55:23Z",
        "updatedAt" : "2020-07-15T20:10:16Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "4e7cf428-f20a-410c-895c-aac4eff4f224",
        "parentId" : "238209f9-a7a2-4a70-8cd1-d7f3de515063",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "> if we happen to push down predicates that refer nested columns.\r\n\r\nI did some experiments, and found out that filters with nested attributes are not pushed down.",
        "createdAt" : "2020-01-31T13:13:25Z",
        "updatedAt" : "2020-07-15T20:10:16Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "a217aef3-ce5c-4d6c-84dd-b8df10dc0646",
        "parentId" : "238209f9-a7a2-4a70-8cd1-d7f3de515063",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "@dbtsai is trying to support them https://github.com/dbtsai/spark/pull/10",
        "createdAt" : "2020-01-31T13:16:32Z",
        "updatedAt" : "2020-07-15T20:10:16Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "06d31cc8-f4b8-4413-8a4b-2aadf4864617",
        "parentId" : "238209f9-a7a2-4a70-8cd1-d7f3de515063",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "> Now this method works differently for root row and nested row. I wonder if we can decouple the filtering logic out of this method ...\r\n\r\nInitially, I just duplicate the code but in the commit collapsed it to one method: https://github.com/apache/spark/pull/27366/commits/94a22e1cbbc3fc5ca5d87f6b08c8b5adc121a7b8",
        "createdAt" : "2020-01-31T13:22:07Z",
        "updatedAt" : "2020-07-15T20:10:16Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "57524d615680ac6126495ea8fc2e51000156f8ff",
    "line" : 71,
    "diffHunk" : "@@ -1,1 +391,395 @@          try {\n            row.update(index, fieldConverters(index).apply(parser))\n            skipRow = structFilters.skipRow(row, index)\n          } catch {\n            case e: SparkUpgradeException => throw e"
  },
  {
    "id" : "03a10782-3b3d-40e9-860e-1a4cc2a9818d",
    "prId" : 27366,
    "prUrl" : "https://github.com/apache/spark/pull/27366#pullrequestreview-351847540",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5e39367d-64a3-47ac-94d7-dfa7f196e7e7",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Is it safe to use simply `get`? Sometime, this can cause `java.util.NoSuchElementException: None.get` situation.",
        "createdAt" : "2020-01-31T20:53:46Z",
        "updatedAt" : "2020-07-15T20:10:16Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "c5580b59-3a5d-4e8f-a303-319f5fc21130",
        "parentId" : "5e39367d-64a3-47ac-94d7-dfa7f196e7e7",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "`None` can be returned when a row is skipped while applying filters. That could happen for root struct only which is not the case here.",
        "createdAt" : "2020-01-31T20:59:35Z",
        "updatedAt" : "2020-07-15T20:10:16Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "e1f9c71a-3740-41a3-a719-bb96be1de41f",
        "parentId" : "5e39367d-64a3-47ac-94d7-dfa7f196e7e7",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Got it.",
        "createdAt" : "2020-02-01T00:36:12Z",
        "updatedAt" : "2020-07-15T20:10:16Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "57524d615680ac6126495ea8fc2e51000156f8ff",
    "line" : 49,
    "diffHunk" : "@@ -1,1 +295,299 @@      val fieldConverters = st.map(_.dataType).map(makeConverter).toArray\n      (parser: JsonParser) => parseJsonToken[InternalRow](parser, dataType) {\n        case START_OBJECT => convertObject(parser, st, fieldConverters).get\n      }\n"
  },
  {
    "id" : "46c6d741-c592-42f5-8bf4-f17a40f4a069",
    "prId" : 27366,
    "prUrl" : "https://github.com/apache/spark/pull/27366#pullrequestreview-435064879",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1aff467f-886f-4f70-af39-157fdd3e8725",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "If any badRecordException is happened, `structFilters.skipRow` is not ran for that index and some predicates might not be with all references set in the row. That's said we could fail to filter out some rows in the case. I guess that it is fine because we should have Filter on top of Json scan operator to catch them. <del>Seems we can skip `structFilters.skipRow` once any badRecordException is happened?</del> Nvm, we still need to evaluate predicates on other indexes.",
        "createdAt" : "2020-06-22T15:43:49Z",
        "updatedAt" : "2020-07-15T20:10:16Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "2a230637-de20-4a12-b87e-fd7559468180",
        "parentId" : "1aff467f-886f-4f70-af39-157fdd3e8725",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "> Do we always have Filter on top of Json scan operator to catch them?\r\n\r\nAs far as I know yes, we do.",
        "createdAt" : "2020-06-22T15:59:34Z",
        "updatedAt" : "2020-07-15T20:10:16Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "57524d615680ac6126495ea8fc2e51000156f8ff",
    "line" : 74,
    "diffHunk" : "@@ -1,1 +394,398 @@          } catch {\n            case e: SparkUpgradeException => throw e\n            case NonFatal(e) =>\n              badRecordException = badRecordException.orElse(Some(e))\n              parser.skipChildren()"
  },
  {
    "id" : "06762c81-f32b-4772-b6ec-09112bd3440f",
    "prId" : 27366,
    "prUrl" : "https://github.com/apache/spark/pull/27366#pullrequestreview-448816877",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2eb7634b-a488-4dbb-9798-8c133d936892",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "No big deal but wouldn't it better to just use `NoopFilters` when the switch is off? For example,\r\n\r\n```scala\r\nval jsonFilters = if (SQLConf.get.jsonFilterPushDown) {\r\n  new JsonFilters(filters, st)\r\n} else {\r\n  new NoopFilters()\r\n}\r\n```",
        "createdAt" : "2020-07-15T09:45:26Z",
        "updatedAt" : "2020-07-15T20:10:16Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "30c89e99-e169-4369-b79c-b7189186b0fe",
        "parentId" : "2eb7634b-a488-4dbb-9798-8c133d936892",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "I believe the flag check should be incapsulated in JsonFilters, and not visible at the caller side.",
        "createdAt" : "2020-07-15T09:53:39Z",
        "updatedAt" : "2020-07-15T20:10:16Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "7be9ede1-b4e9-4284-9784-8b9982a2d2f3",
        "parentId" : "2eb7634b-a488-4dbb-9798-8c133d936892",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "ðŸ‘Œ  that's fine",
        "createdAt" : "2020-07-15T10:13:31Z",
        "updatedAt" : "2020-07-15T20:10:16Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "57524d615680ac6126495ea8fc2e51000156f8ff",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +88,92 @@    val elementConverter = makeConverter(st)\n    val fieldConverters = st.map(_.dataType).map(makeConverter).toArray\n    val jsonFilters = new JsonFilters(filters, st)\n    (parser: JsonParser) => parseJsonToken[Iterable[InternalRow]](parser, st) {\n      case START_OBJECT => convertObject(parser, st, fieldConverters, jsonFilters)"
  }
]