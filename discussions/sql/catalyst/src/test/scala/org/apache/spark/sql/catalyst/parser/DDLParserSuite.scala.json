[
  {
    "id" : "c5ac414a-e0ba-4e46-bb12-9be81549cd8b",
    "prId" : 28875,
    "prUrl" : "https://github.com/apache/spark/pull/28875#pullrequestreview-438739375",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "985a351f-ff13-4ec3-9084-2d8199389ab2",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "shall we test the same thing for not matched clause?",
        "createdAt" : "2020-06-23T11:41:32Z",
        "updatedAt" : "2020-06-28T03:03:06Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "4ac8371e-2185-4188-b82a-6eac967615be",
        "parentId" : "985a351f-ff13-4ec3-9084-2d8199389ab2",
        "authorId" : "8afcf008-6988-44a5-92fa-2bef54bb4058",
        "body" : "done.",
        "createdAt" : "2020-06-28T03:09:05Z",
        "updatedAt" : "2020-06-28T03:09:06Z",
        "lastEditedBy" : "8afcf008-6988-44a5-92fa-2bef54bb4058",
        "tags" : [
        ]
      }
    ],
    "commit" : "d5edef3c2b950440614fc5c9ee1e770bcd0b9884",
    "line" : 52,
    "diffHunk" : "@@ -1,1 +1166,1170 @@  }\n\n  test(\"merge into table: only the last matched clause can omit the condition\") {\n    val exc = intercept[ParseException] {\n      parsePlan("
  },
  {
    "id" : "58354390-8e3e-4021-977d-62ae5af27d97",
    "prId" : 27677,
    "prUrl" : "https://github.com/apache/spark/pull/27677#pullrequestreview-363182576",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a8546412-b701-4fe8-89f2-b53f3b69e394",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "This is a bit hard to justify. For example, we may want to update different rows with different new values according to the condition.",
        "createdAt" : "2020-02-24T06:14:19Z",
        "updatedAt" : "2020-02-24T06:14:20Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "beaa8be7-d0e8-4f58-b10f-ba3adbcfdc64",
        "parentId" : "a8546412-b701-4fe8-89f2-b53f3b69e394",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "seems better to fail at the implementation side if it's not supported.",
        "createdAt" : "2020-02-24T06:14:48Z",
        "updatedAt" : "2020-02-24T06:14:49Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "a7614ac7014f6547c20c5303bf05bdc10a15f9d2",
    "line" : 35,
    "diffHunk" : "@@ -1,1 +1222,1226 @@\n  test(\"merge into table: there can be only a single use DELETE or UPDATE\") {\n    Seq(\"UPDATE SET *\", \"DELETE\").foreach { op =>\n      val exc = intercept[ParseException] {\n        parsePlan("
  },
  {
    "id" : "0d4a54f8-aa90-4ec1-90c3-afc363a4d47d",
    "prId" : 26374,
    "prUrl" : "https://github.com/apache/spark/pull/26374#pullrequestreview-311337841",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "92bf4ada-be69-40cf-9a1a-eaac2a5d232b",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Although this is migrated from the other test case, instead of comparing at the end, shall we group them by query like the following?\r\n\r\n```scala\r\nval sql1 = \"ALTER TABLE table_name SET SERDE 'org.apache.class'\"\r\nval parsed1 = parsePlan(sql1)\r\nval expected1 = AlterTableSerDePropertiesStatement(\r\n  Seq(\"table_name\"), Some(\"org.apache.class\"), None, None)\r\ncomparePlans(parsed1, expected1)\r\n```\r\n\r\nThen, it will become more maintainable when we add a new test case and backport.",
        "createdAt" : "2019-11-04T20:28:16Z",
        "updatedAt" : "2019-11-05T02:24:09Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "7c2078f442bcda5ee0a22a2273a80282ea13663b",
    "line" : 85,
    "diffHunk" : "@@ -1,1 +1402,1406 @@      Some(Map(\"columns\" -> \"foo,bar\", \"field.delim\" -> \",\")),\n      Some(Map(\"test\" -> \"1\", \"dt\" -> \"2008-08-08\", \"country\" -> \"us\")))\n    comparePlans(parsed7, expected7)\n  }\n"
  },
  {
    "id" : "27ac25e1-6d3b-4e04-ae6a-0872551ab448",
    "prId" : 26198,
    "prUrl" : "https://github.com/apache/spark/pull/26198#pullrequestreview-304958300",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "018d2cb6-030d-4507-83ec-82c96975898a",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Let's add a case of `a.b.c`.",
        "createdAt" : "2019-10-22T01:50:51Z",
        "updatedAt" : "2019-10-22T16:52:04Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "f8a4d09d-3b1d-4097-81b2-7c831aac654c",
        "parentId" : "018d2cb6-030d-4507-83ec-82c96975898a",
        "authorId" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "body" : "Thanks @viirya for your review. The comment has been addressed. ",
        "createdAt" : "2019-10-22T04:23:58Z",
        "updatedAt" : "2019-10-22T16:52:04Z",
        "lastEditedBy" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "tags" : [
        ]
      }
    ],
    "commit" : "248513ca8edc7b83a03c44a145554c0c36474a00",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +974,978 @@  test(\"SHOW PARTITIONS\") {\n    val sql1 = \"SHOW PARTITIONS t1\"\n    val sql2 = \"SHOW PARTITIONS db1.t1\"\n    val sql3 = \"SHOW PARTITIONS t1 PARTITION(partcol1='partvalue', partcol2='partvalue')\"\n    val sql4 = \"SHOW PARTITIONS a.b.c\""
  },
  {
    "id" : "7e711fab-1c3c-4884-a933-72b1063d9be6",
    "prId" : 25626,
    "prUrl" : "https://github.com/apache/spark/pull/25626#pullrequestreview-283471128",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dc529a5e-f949-4908-96ac-d4c15b948423",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "let's add some simple positive tests as well. You can take a look at how other commands are tested in this suite.",
        "createdAt" : "2019-09-04T08:53:19Z",
        "updatedAt" : "2019-09-23T06:56:27Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "b348402a-b5d7-40db-bec2-dc9d4030dd11",
        "parentId" : "dc529a5e-f949-4908-96ac-d4c15b948423",
        "authorId" : "8afcf008-6988-44a5-92fa-2bef54bb4058",
        "body" : "Yea, thanks for advising. Will add more cases.",
        "createdAt" : "2019-09-04T08:57:50Z",
        "updatedAt" : "2019-09-23T06:56:27Z",
        "lastEditedBy" : "8afcf008-6988-44a5-92fa-2bef54bb4058",
        "tags" : [
        ]
      }
    ],
    "commit" : "3732a18e3012e0dccc28663f9a37ca570e8b13d1",
    "line" : 42,
    "diffHunk" : "@@ -1,1 +819,823 @@  }\n\n  test(\"update table: columns aliases is not allowed\") {\n    val exc = intercept[ParseException] {\n      parsePlan("
  },
  {
    "id" : "bf1c15ff-e8d3-4016-bdc7-96327f6d176a",
    "prId" : 25601,
    "prUrl" : "https://github.com/apache/spark/pull/25601#pullrequestreview-287304611",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "eca9c08e-95fd-4a90-aa4a-47559d92a077",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "cc @xianyinxin can you add similar parser tests for DELETE/UPDATE as well?",
        "createdAt" : "2019-09-04T05:55:35Z",
        "updatedAt" : "2019-09-09T23:58:04Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "a9566d9f-b1c6-45b1-9784-53919006d4c8",
        "parentId" : "eca9c08e-95fd-4a90-aa4a-47559d92a077",
        "authorId" : "8afcf008-6988-44a5-92fa-2bef54bb4058",
        "body" : "For update, https://github.com/apache/spark/pull/25626 has added some parser cases. For delete, will done in https://github.com/apache/spark/pull/25652.",
        "createdAt" : "2019-09-12T09:33:46Z",
        "updatedAt" : "2019-09-12T09:33:46Z",
        "lastEditedBy" : "8afcf008-6988-44a5-92fa-2bef54bb4058",
        "tags" : [
        ]
      }
    ],
    "commit" : "9a55a03acf84fe67595817c8fef409a9e4912a51",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +780,784 @@  }\n\n  test(\"show namespaces\") {\n    comparePlans(\n      parsePlan(\"SHOW NAMESPACES\"),"
  },
  {
    "id" : "d93283c0-8e1e-4d36-ac4c-0b1cd310607c",
    "prId" : 25247,
    "prUrl" : "https://github.com/apache/spark/pull/25247#pullrequestreview-269803565",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c8da613f-73fb-417f-a10d-4c76e94f6da2",
        "parentId" : null,
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "Nit: is `*` or `%` the wildcard?",
        "createdAt" : "2019-07-31T23:09:20Z",
        "updatedAt" : "2019-08-23T00:03:45Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      },
      {
        "id" : "b6084fcf-2be7-4dde-9388-dac0df0fbdac",
        "parentId" : "c8da613f-73fb-417f-a10d-4c76e94f6da2",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "I think it's `*`: https://github.com/apache/spark/blob/607fb879060f8e620d74017ff3fd8c8c737cdbab/sql/core/src/test/resources/sql-tests/inputs/show-tables.sql#L16",
        "createdAt" : "2019-08-01T18:32:54Z",
        "updatedAt" : "2019-08-23T00:03:45Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "6f2fa4e7b8572a6b9dd243f1253e4962af27d40a",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +776,780 @@      ShowTablesStatement(Some(Seq(\"testcat\", \"ns1\", \"ns2\", \"tbl\")), None))\n    comparePlans(\n      parsePlan(\"SHOW TABLES IN tbl LIKE '*dog*'\"),\n      ShowTablesStatement(Some(Seq(\"tbl\")), Some(\"*dog*\")))\n  }"
  },
  {
    "id" : "db565aa7-f647-4e5b-a0ac-e7459600a1fc",
    "prId" : 25040,
    "prUrl" : "https://github.com/apache/spark/pull/25040#pullrequestreview-271546475",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1ec54588-8372-445f-9a99-f0c61938fb2c",
        "parentId" : null,
        "authorId" : "806aa501-9ea7-4cec-b017-a84d9a1602d8",
        "body" : "These are getting really really long, and in particular the merge conflicts are a bit tedious to resolve. I'm normally very averse to wildcard imports, but there might come a point where we'll have to do that. Or I wonder if we could have a helper `object` that bundles all of these, or factory methods for these, or matchers... somehow.",
        "createdAt" : "2019-07-30T01:45:29Z",
        "updatedAt" : "2019-08-06T18:59:07Z",
        "lastEditedBy" : "806aa501-9ea7-4cec-b017-a84d9a1602d8",
        "tags" : [
        ]
      },
      {
        "id" : "e11aa56f-b5a6-4857-a837-afa2b96509b4",
        "parentId" : "1ec54588-8372-445f-9a99-f0c61938fb2c",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "+1 for wildcard here.",
        "createdAt" : "2019-07-30T03:19:03Z",
        "updatedAt" : "2019-08-06T18:59:07Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "3fd8f5a1-7a70-4616-95ef-d30747836267",
        "parentId" : "1ec54588-8372-445f-9a99-f0c61938fb2c",
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "This package may make sense for a wildcard import because it has no sub-packages and is unlikely to in the future. Still, because Scala will import sub-packages, I think it's probably best to keep avoiding wildcard imports, even here.",
        "createdAt" : "2019-08-06T18:51:27Z",
        "updatedAt" : "2019-08-06T18:59:07Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      }
    ],
    "commit" : "cff78a16e691917e812b4cd63bf7544a54af4742",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +25,29 @@import org.apache.spark.sql.catalyst.catalog.BucketSpec\nimport org.apache.spark.sql.catalyst.plans.logical.{LogicalPlan, Project}\nimport org.apache.spark.sql.catalyst.plans.logical.sql.{AlterTableAddColumnsStatement, AlterTableAlterColumnStatement, AlterTableDropColumnsStatement, AlterTableRenameColumnStatement, AlterTableSetLocationStatement, AlterTableSetPropertiesStatement, AlterTableUnsetPropertiesStatement, AlterViewSetPropertiesStatement, AlterViewUnsetPropertiesStatement, CreateTableAsSelectStatement, CreateTableStatement, DescribeColumnStatement, DescribeTableStatement, DropTableStatement, DropViewStatement, InsertIntoStatement, QualifiedColType, ReplaceTableAsSelectStatement, ReplaceTableStatement}\nimport org.apache.spark.sql.types.{IntegerType, LongType, StringType, StructType, TimestampType}\nimport org.apache.spark.unsafe.types.UTF8String"
  },
  {
    "id" : "90b7333c-1e84-47eb-97ab-da41b220a583",
    "prId" : 24832,
    "prUrl" : "https://github.com/apache/spark/pull/24832#pullrequestreview-266730518",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "00701697-ecfc-4dfd-8101-6bc121ae78a1",
        "parentId" : null,
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "@cloud-fan, @brkyvz, this is the test that required adding the `IF NOT EXISTS` to `INSERT INTO`. I think it is better to have a good error message instead of relying on not being able to parse the statement.",
        "createdAt" : "2019-07-25T16:02:57Z",
        "updatedAt" : "2019-07-25T16:05:47Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      }
    ],
    "commit" : "7f193ca074b7fd0dea9f7b28b4e1776819e6d512",
    "line" : 113,
    "diffHunk" : "@@ -1,1 +711,715 @@  }\n\n  test(\"insert table: if not exists without overwrite fails\") {\n    val exc = intercept[AnalysisException] {\n      parsePlan("
  },
  {
    "id" : "960129c0-0b2e-47fd-bd5b-6374ef672133",
    "prId" : 24723,
    "prUrl" : "https://github.com/apache/spark/pull/24723#pullrequestreview-244647798",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3792f794-9711-421c-8b7c-c62d664845ad",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "~Is it inevitable to duplicate this utility functions during refactoring?~ Sorry, I realized that this is not the same at line 35.",
        "createdAt" : "2019-06-02T23:41:14Z",
        "updatedAt" : "2019-06-04T19:04:01Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "bd7d8c8f7c6a2c378deba5927c96a8eb4dc6ef54",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +31,35 @@  import CatalystSqlParser._\n\n  private def assertUnsupported(sql: String, containsThesePhrases: Seq[String] = Seq()): Unit = {\n    val e = intercept[ParseException] {\n      parsePlan(sql)"
  }
]