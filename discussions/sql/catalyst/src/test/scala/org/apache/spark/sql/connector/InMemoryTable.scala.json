[
  {
    "id" : "fec762e1-f624-47e2-8933-f9599edf9897",
    "prId" : 31440,
    "prUrl" : "https://github.com/apache/spark/pull/31440#pullrequestreview-582506296",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "56b84797-0cf7-42b6-8f22-1f71d263070c",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "The actual data is int.",
        "createdAt" : "2021-02-03T15:43:57Z",
        "updatedAt" : "2021-02-03T15:56:34Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "be4aefefd402637e77e50936b442ac619e14bf1d",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +62,66 @@  private object IndexColumn extends MetadataColumn {\n    override def name: String = \"index\"\n    override def dataType: DataType = IntegerType\n    override def comment: String = \"Metadata column used to conflict with a data column\"\n  }"
  },
  {
    "id" : "348c173c-4c2a-4b0a-9d97-439d90f4cc51",
    "prId" : 31083,
    "prUrl" : "https://github.com/apache/spark/pull/31083#pullrequestreview-563392953",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "30063d12-f945-4866-b54a-08fbfc14a3d4",
        "parentId" : null,
        "authorId" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "body" : "@rdblue, I think I addressed all comments around tests on the original PR but let me know if I missed something.",
        "createdAt" : "2021-01-07T10:55:25Z",
        "updatedAt" : "2021-01-25T20:45:29Z",
        "lastEditedBy" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "tags" : [
        ]
      }
    ],
    "commit" : "5aa31c93e212576caa0b0d6986dfd20bf971310f",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +48,52 @@    val schema: StructType,\n    override val partitioning: Array[Transform],\n    override val properties: util.Map[String, String],\n    val distribution: Distribution = Distributions.unspecified(),\n    val ordering: Array[SortOrder] = Array.empty)"
  },
  {
    "id" : "e76c5a7f-f473-4ad7-86dc-5ebbc3410a75",
    "prId" : 28993,
    "prUrl" : "https://github.com/apache/spark/pull/28993#pullrequestreview-443310722",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "53b84003-616c-43d9-9d3e-53bd506c1ea6",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "nit: `extractor(ref.fieldNames, schema, row)._1.hashCode()`?",
        "createdAt" : "2020-07-06T18:09:03Z",
        "updatedAt" : "2020-07-07T20:51:44Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "cd3ef19a-9eef-421c-974f-63db6de26ad8",
        "parentId" : "53b84003-616c-43d9-9d3e-53bd506c1ea6",
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "If we use the `Tuple2` instance, we know it won't be null and will always get a hash code.",
        "createdAt" : "2020-07-06T18:23:41Z",
        "updatedAt" : "2020-07-07T20:51:44Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      }
    ],
    "commit" : "2efb84cf1abab6a67718501e79d00c4f63ecc8aa",
    "line" : 103,
    "diffHunk" : "@@ -1,1 +129,133 @@        }\n      case BucketTransform(numBuckets, ref) =>\n        (extractor(ref.fieldNames, schema, row).hashCode() & Integer.MAX_VALUE) % numBuckets\n    }\n  }"
  },
  {
    "id" : "8b86cc8b-d44c-4e2a-98e1-f74cc19267db",
    "prId" : 28993,
    "prUrl" : "https://github.com/apache/spark/pull/28993#pullrequestreview-444197636",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "24390a7a-d5bd-488c-82cd-bc528ccb15cc",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Shall we remove line 46 and 47 since this is not used after this PR?",
        "createdAt" : "2020-07-07T18:29:07Z",
        "updatedAt" : "2020-07-07T20:51:44Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "4793b284-585d-43f0-bfdd-90b76de73d27",
        "parentId" : "24390a7a-d5bd-488c-82cd-bc528ccb15cc",
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "I considered it, but we may still want to test creating a table with a different transform that is not implemented. Since we already support it, I thought it wasn't worth removing this. I can if you feel strongly about it.",
        "createdAt" : "2020-07-07T18:38:50Z",
        "updatedAt" : "2020-07-07T20:51:44Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      },
      {
        "id" : "5dfcf7fc-1e0a-45f6-963d-13b127b1e81e",
        "parentId" : "24390a7a-d5bd-488c-82cd-bc528ccb15cc",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Got it. Then, no problem.",
        "createdAt" : "2020-07-07T19:45:05Z",
        "updatedAt" : "2020-07-07T20:51:44Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "2efb84cf1abab6a67718501e79d00c4f63ecc8aa",
    "line" : 27,
    "diffHunk" : "@@ -1,1 +48,52 @@\n  private val allowUnsupportedTransforms =\n    properties.getOrDefault(\"allow-unsupported-transforms\", \"false\").toBoolean\n\n  partitioning.foreach {"
  },
  {
    "id" : "3bbedda6-2a97-4106-9eb8-fa9a25b2556e",
    "prId" : 28993,
    "prUrl" : "https://github.com/apache/spark/pull/28993#pullrequestreview-444148231",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2fb7511b-700c-4352-b75d-5f7f41208b7e",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "`allowUnsupportedTransforms` seems to be used only here and not by other places (in InMemoryTable or in test suites). I'm wondering if we need this.",
        "createdAt" : "2020-07-07T18:30:50Z",
        "updatedAt" : "2020-07-07T20:51:44Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "2efb84cf1abab6a67718501e79d00c4f63ecc8aa",
    "line" : 40,
    "diffHunk" : "@@ -1,1 +57,61 @@    case _: HoursTransform =>\n    case _: BucketTransform =>\n    case t if !allowUnsupportedTransforms =>\n      throw new IllegalArgumentException(s\"Transform $t is not a supported transform\")\n  }"
  },
  {
    "id" : "8806671e-35d0-45a1-aa10-25fed464fa78",
    "prId" : 28993,
    "prUrl" : "https://github.com/apache/spark/pull/28993#pullrequestreview-445215820",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2984352e-d4b5-4dfe-b25d-4b6fa49f2cf4",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Hm .. @rdblue, will this be the default behaviour for these partitioning expressions? I wonder what happens if other datasources implement these in a different way, for example, some expressions might respect Spark session timezone.",
        "createdAt" : "2020-07-08T01:43:35Z",
        "updatedAt" : "2020-07-08T01:44:21Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "02de16e1-d7b3-4f22-9ca6-e3f215fed125",
        "parentId" : "2984352e-d4b5-4dfe-b25d-4b6fa49f2cf4",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "It's a testing implementation. There is no default behavior. Partitioning expression just indicates how the scan can be faster with specific pushed filters. ",
        "createdAt" : "2020-07-08T04:47:48Z",
        "updatedAt" : "2020-07-08T04:47:49Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "57a17e45-9fd8-4923-9087-0b0d4a850fae",
        "parentId" : "2984352e-d4b5-4dfe-b25d-4b6fa49f2cf4",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Oh! It was in the test!",
        "createdAt" : "2020-07-08T05:30:35Z",
        "updatedAt" : "2020-07-08T05:30:35Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "aa9dda51-9471-45c5-826e-a71d7e324a40",
        "parentId" : "2984352e-d4b5-4dfe-b25d-4b6fa49f2cf4",
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "Yes, @cloud-fan is right.\r\n\r\nSpark also doesn't require specific behavior for these. The `days` function indicates that data should be broken down into day-sized partitions, but doesn't require a specific boundary. It is up to the source to decide where those boundaries are.\r\n\r\nBy the time timestamps get to the source, they are already normalized values in microseconds from epoch in UTC. Because we have consistent values, the source can divide values into partitions however it wants. A filter like `ts >= t1 and ts < t2` gets converted to `part_day >= day(t1) and part_day <= day(t2)` no matter what the specific implementation of `day` is.",
        "createdAt" : "2020-07-08T17:13:06Z",
        "updatedAt" : "2020-07-08T17:13:07Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      },
      {
        "id" : "5efe907d-a298-47fc-927b-61c4e65c463f",
        "parentId" : "2984352e-d4b5-4dfe-b25d-4b6fa49f2cf4",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Sure, sorry it was clearly my misunderstanding. LGTM.",
        "createdAt" : "2020-07-09T00:35:29Z",
        "updatedAt" : "2020-07-09T00:35:30Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "2efb84cf1abab6a67718501e79d00c4f63ecc8aa",
    "line" : 80,
    "diffHunk" : "@@ -1,1 +106,110 @@          case (micros: Long, TimestampType) =>\n            val localDate = DateTimeUtils.microsToInstant(micros).atZone(UTC).toLocalDate\n            ChronoUnit.YEARS.between(EPOCH_LOCAL_DATE, localDate)\n        }\n      case MonthsTransform(ref) =>"
  }
]