[
  {
    "id" : "69276e0e-30c5-40a9-ab26-b093f5ff8d89",
    "prId" : 32921,
    "prUrl" : "https://github.com/apache/spark/pull/32921#pullrequestreview-684528855",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "63c35b94-750e-463e-80a8-0a51ed229cd2",
        "parentId" : null,
        "authorId" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "body" : "I had to implement stats as tests rely on them.",
        "createdAt" : "2021-06-15T21:59:23Z",
        "updatedAt" : "2021-06-15T21:59:23Z",
        "lastEditedBy" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "tags" : [
        ]
      }
    ],
    "commit" : "881d2b2b7246d9453bfa7e074ab19334bf8d9876",
    "line" : 27,
    "diffHunk" : "@@ -1,1 +257,261 @@    override def toBatch: Batch = this\n\n    override def estimateStatistics(): Statistics = {\n      if (data.isEmpty) {\n        return InMemoryStats(OptionalLong.of(0L), OptionalLong.of(0L))"
  },
  {
    "id" : "b01827a4-d5ed-4383-be9c-29ceb41ec32c",
    "prId" : 32921,
    "prUrl" : "https://github.com/apache/spark/pull/32921#pullrequestreview-684529127",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ff5a729f-8e4a-42cf-aee6-9e2e852404c3",
        "parentId" : null,
        "authorId" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "body" : "It is just a temp trivial implementation.",
        "createdAt" : "2021-06-15T21:59:50Z",
        "updatedAt" : "2021-06-15T21:59:50Z",
        "lastEditedBy" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "tags" : [
        ]
      }
    ],
    "commit" : "881d2b2b7246d9453bfa7e074ab19334bf8d9876",
    "line" : 56,
    "diffHunk" : "@@ -1,1 +286,290 @@\n    override def filter(filters: Array[Filter]): Unit = {\n      if (partitioning.length == 1) {\n        filters.foreach {\n          case In(attrName, values) if attrName == partitioning.head.name =>"
  },
  {
    "id" : "74f08767-220b-498c-b013-20a540e5e70e",
    "prId" : 32921,
    "prUrl" : "https://github.com/apache/spark/pull/32921#pullrequestreview-697589829",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e0ed06f2-de47-4f1e-8129-a1a81f96e6e6",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Do we need an accurate size here? If not we can simply do `dataType.defaultSize * numRows`.",
        "createdAt" : "2021-07-01T18:29:26Z",
        "updatedAt" : "2021-07-01T18:29:26Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "459f7aae-f2e9-4e10-bc40-108aec8b1548",
        "parentId" : "e0ed06f2-de47-4f1e-8129-a1a81f96e6e6",
        "authorId" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "body" : "Tests were sensitive to stats but let me try to simplify it.",
        "createdAt" : "2021-07-01T18:53:56Z",
        "updatedAt" : "2021-07-01T18:53:56Z",
        "lastEditedBy" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "tags" : [
        ]
      },
      {
        "id" : "e50a9a01-2c53-43dc-8ba4-5a339c7a41f0",
        "parentId" : "e0ed06f2-de47-4f1e-8129-a1a81f96e6e6",
        "authorId" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "body" : "Unfortunately, this triggers a set of failures as `PartitionPruning` is sensitive to stats.",
        "createdAt" : "2021-07-01T19:11:16Z",
        "updatedAt" : "2021-07-01T19:11:16Z",
        "lastEditedBy" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "tags" : [
        ]
      },
      {
        "id" : "5bacfbce-eeee-499f-922b-e2c9784f0286",
        "parentId" : "e0ed06f2-de47-4f1e-8129-a1a81f96e6e6",
        "authorId" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "body" : "```\r\n[info] Run completed in 29 seconds, 324 milliseconds.\r\n[info] Total number of tests run: 30\r\n[info] Suites: completed 1, aborted 0\r\n[info] Tests: succeeded 24, failed 6, canceled 0, ignored 0, pending 0\r\n[info] *** 6 TESTS FAILED ***\r\n[error] Failed tests:\r\n[error]         org.apache.spark.sql.DynamicPartitionPruningV2SuiteAEOff\r\n[error] (Test / testOnly) sbt.TestsFailedException: Tests unsuccessful\r\n```",
        "createdAt" : "2021-07-01T19:15:17Z",
        "updatedAt" : "2021-07-01T19:15:17Z",
        "lastEditedBy" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "tags" : [
        ]
      },
      {
        "id" : "d3c95971-fca9-434e-a0c8-0a4c6cef7a04",
        "parentId" : "e0ed06f2-de47-4f1e-8129-a1a81f96e6e6",
        "authorId" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "body" : "Kept as is for now.",
        "createdAt" : "2021-07-01T19:19:07Z",
        "updatedAt" : "2021-07-01T19:19:07Z",
        "lastEditedBy" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "tags" : [
        ]
      },
      {
        "id" : "9d7ea67b-3fd6-4ce3-bbad-c11628258a5f",
        "parentId" : "e0ed06f2-de47-4f1e-8129-a1a81f96e6e6",
        "authorId" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "body" : "Let me try with object header.",
        "createdAt" : "2021-07-01T19:26:24Z",
        "updatedAt" : "2021-07-01T19:26:24Z",
        "lastEditedBy" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "tags" : [
        ]
      },
      {
        "id" : "0cc8ac8a-98d2-4717-a453-7eeb903345f1",
        "parentId" : "e0ed06f2-de47-4f1e-8129-a1a81f96e6e6",
        "authorId" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "body" : "Works with object header. Updated.",
        "createdAt" : "2021-07-01T19:33:26Z",
        "updatedAt" : "2021-07-01T19:33:26Z",
        "lastEditedBy" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "tags" : [
        ]
      }
    ],
    "commit" : "881d2b2b7246d9453bfa7e074ab19334bf8d9876",
    "line" : 34,
    "diffHunk" : "@@ -1,1 +264,268 @@      val inputPartitions = data.map(_.asInstanceOf[BufferedRows])\n      val numRows = inputPartitions.map(_.rows.size).sum\n      // we assume an average object header is 12 bytes\n      val objectHeaderSizeInBytes = 12L\n      val rowSizeInBytes = objectHeaderSizeInBytes + schema.defaultSize"
  },
  {
    "id" : "5455cce8-8382-4348-af18-5d5f6f0b2a86",
    "prId" : 32354,
    "prUrl" : "https://github.com/apache/spark/pull/32354#pullrequestreview-651518900",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "93def622-6711-4254-a37b-d848ef11dfd2",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Is case-sensitivity a problem here?",
        "createdAt" : "2021-05-04T06:19:13Z",
        "updatedAt" : "2021-05-04T18:00:49Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "3beaeb4a-d179-427b-8970-cc7d94be7b6a",
        "parentId" : "93def622-6711-4254-a37b-d848ef11dfd2",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Good question. Looking at `PushdownUtils.pruneColumns`, I see that we apply `SQLConf.resolver` when nested column pruning is enabled, but seems not so when it is disabled. IMO perhaps we should have better contract between Spark and data source implementors w.r.t `SupportsPushDownRequiredColumns.pruneColumns`, and Spark should guarantee that the `requiredSchema` passed in to the method should be a \"subset\" of the relation's schema (e.g., table schema).",
        "createdAt" : "2021-05-04T17:38:50Z",
        "updatedAt" : "2021-05-04T18:00:49Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "110f62f0-2b0f-48cb-af52-ecfa422cc54d",
        "parentId" : "93def622-6711-4254-a37b-d848ef11dfd2",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Oh actually, in both cases the `requiredSchema` are normalized according to case-sensitivity. This is done in `V2ScanRelationPushDown` (see `normalizedProjects` there.) ",
        "createdAt" : "2021-05-04T18:00:20Z",
        "updatedAt" : "2021-05-04T18:00:49Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      }
    ],
    "commit" : "a67dde0b2ddea64bf1eb5bb67e0b1cd938146ee6",
    "line" : 88,
    "diffHunk" : "@@ -1,1 +524,528 @@      schema: StructType,\n      row: InternalRow): Any = {\n    val index = schema.fieldIndex(field.name)\n    field.dataType match {\n      case StructType(fields) =>"
  }
]