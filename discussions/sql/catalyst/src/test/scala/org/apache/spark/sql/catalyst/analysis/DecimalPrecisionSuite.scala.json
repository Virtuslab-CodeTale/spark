[
  {
    "id" : "6306afde-11be-4fb1-9644-0268bda126f2",
    "prId" : 25035,
    "prUrl" : "https://github.com/apache/spark/pull/25035#pullrequestreview-257881666",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b4a6d0ee-0fa0-4f9c-8b5a-f8fcdf6425e9",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Could you fix the relevant UT failures in `AnalysisSuite` and `DecimalAggregatesSuite`, too?",
        "createdAt" : "2019-07-03T03:03:31Z",
        "updatedAt" : "2019-07-04T15:06:32Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "b700af53-0919-49e8-8b6e-23df3b9cd886",
        "parentId" : "b4a6d0ee-0fa0-4f9c-8b5a-f8fcdf6425e9",
        "authorId" : "24e1dd39-ae3f-4bbb-a391-f60afb62d075",
        "body" : "Actually the UT failures pointed out an issue. The `DecimalAggregates` optimization works in case the resulting type has a precision lower than the maximum number of precision digits for a long. Keeping the rule as is, just returning a wider data type means that applying that rule can cause a difference in the output of the query, which means that it is not a valid optimization rule anymore.\r\n\r\nIt is true, though, that we wouldn't have regressions, just some cases which were returning `null` would still return `null` unless that rule is excluded. What do you think on this? Thanks.",
        "createdAt" : "2019-07-03T17:38:25Z",
        "updatedAt" : "2019-07-04T15:06:32Z",
        "lastEditedBy" : "24e1dd39-ae3f-4bbb-a391-f60afb62d075",
        "tags" : [
        ]
      },
      {
        "id" : "bf1dddee-7908-4662-8bd8-23b40543f801",
        "parentId" : "b4a6d0ee-0fa0-4f9c-8b5a-f8fcdf6425e9",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "First, this PR doesn't provide the evidence of the claim, `overflows`. We need to add that specific test case.\r\nSecond, you may want to change `DecimalAggregates` optimizer together to pass the UTs.\r\nWithout passing UTs, it's difficult for some PR to get reviewers' attention.\r\n\r\n",
        "createdAt" : "2019-07-03T21:54:19Z",
        "updatedAt" : "2019-07-04T15:06:32Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "205e9da3-1c36-458e-8551-03baa1930b37",
        "parentId" : "b4a6d0ee-0fa0-4f9c-8b5a-f8fcdf6425e9",
        "authorId" : "24e1dd39-ae3f-4bbb-a391-f60afb62d075",
        "body" : "> First, this PR doesn't provide the evidence of the claim, overflows. We need to add that specific test case.\r\n\r\nIt is not easy to provide that case @dongjoon-hyun as a UT. you need a `DataFrame` with more than bilion of rows. If you have a column which is a `DECIMAL(2,1)`, for instance, the result now is a `DECIMAL(12,1)`. To overflow it with a sum, you need ~ 11.000.000.000 `9,9`. So it is not feasible to add it as a UT.\r\n\r\n> Second, you may want to change DecimalAggregates optimizer together to pass the UTs.\r\n\r\nAbout this point, I was asking your opinion, because I can fix it in order to have it passing UTs, but I am not sure about the semantic of the fix, because after the change the rule would not be a valid optimizer rule anymore, as in certain cases like the one mentioned above, this rule can cause the overflow to happen again (while without that rule the correct value is returned). Anyway, I'll go ahead and fix it, but I'd love to get your opinion on this topic. Thanks.",
        "createdAt" : "2019-07-04T07:53:14Z",
        "updatedAt" : "2019-07-04T15:06:32Z",
        "lastEditedBy" : "24e1dd39-ae3f-4bbb-a391-f60afb62d075",
        "tags" : [
        ]
      }
    ],
    "commit" : "683741a8be406db775dc43bc6093f177ccc0ed2e",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +87,91 @@    checkType(Remainder(d1, d2), DecimalType(3, 2))\n    checkType(Remainder(d2, d1), DecimalType(3, 2))\n    checkType(Sum(d1), DecimalType(DecimalType.MAX_PRECISION, 1))\n    checkType(Average(d1), DecimalType(6, 5))\n"
  }
]