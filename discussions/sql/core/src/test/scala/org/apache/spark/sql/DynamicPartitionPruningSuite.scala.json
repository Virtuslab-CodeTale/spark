[
  {
    "id" : "c3c2d5d3-8641-4c9a-8992-996896de8e8e",
    "prId" : 31984,
    "prUrl" : "https://github.com/apache/spark/pull/31984#pullrequestreview-628264393",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "eb0ab16c-3493-4879-999e-5245438fafb1",
        "parentId" : null,
        "authorId" : "b3ba992a-312c-46eb-b3c3-8d861d15ac40",
        "body" : "@wangyum Why would this PR change this test? This originally can be planned as a subquery duplicate but now it cannot.",
        "createdAt" : "2021-04-05T20:17:26Z",
        "updatedAt" : "2021-04-05T20:17:26Z",
        "lastEditedBy" : "b3ba992a-312c-46eb-b3c3-8d861d15ac40",
        "tags" : [
        ]
      },
      {
        "id" : "497e5887-8748-4430-b5a3-1457e200aa2f",
        "parentId" : "eb0ab16c-3493-4879-999e-5245438fafb1",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "This test `set SQLConf.AUTO_BROADCASTJOIN_THRESHOLD.key -> \"-1\"`.",
        "createdAt" : "2021-04-06T00:05:02Z",
        "updatedAt" : "2021-04-06T00:05:02Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      },
      {
        "id" : "b01e71e2-20ae-401d-a0c6-fedf2e0d9127",
        "parentId" : "eb0ab16c-3493-4879-999e-5245438fafb1",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "Please see https://github.com/apache/spark/pull/31984/commits/b510d7da21f8a92af69b1485b72aef6ad5901448#r603143204 for more details.",
        "createdAt" : "2021-04-06T00:11:50Z",
        "updatedAt" : "2021-04-06T00:11:51Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      },
      {
        "id" : "e57b4f56-5fd8-4973-9b01-c0b3651514df",
        "parentId" : "eb0ab16c-3493-4879-999e-5245438fafb1",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "We can add hint to enable DPP:\r\n```scala\r\n    Given(\"disable broadcast hash join and enable query duplication\")\r\n    withSQLConf(SQLConf.DYNAMIC_PARTITION_PRUNING_REUSE_BROADCAST_ONLY.key -> \"false\",\r\n      SQLConf.AUTO_BROADCASTJOIN_THRESHOLD.key -> \"-1\",\r\n      SQLConf.DYNAMIC_PARTITION_PRUNING_USE_STATS.key -> \"true\") {\r\n      val df = sql(\r\n        \"\"\"\r\n          |SELECT /*+ BROADCAST(s) */ f.date_id, f.product_id, f.units_sold, f.store_id FROM fact_stats f\r\n          |JOIN dim_stats s\r\n          |ON f.store_id = s.store_id WHERE s.country = 'DE'\r\n        \"\"\".stripMargin)\r\n\r\n      checkPartitionPruningPredicate(df, false, true)\r\n\r\n      checkAnswer(df,\r\n        Row(1030, 2, 10, 3) ::\r\n        Row(1040, 2, 50, 3) ::\r\n        Row(1050, 2, 50, 3) ::\r\n        Row(1060, 2, 50, 3) :: Nil\r\n      )\r\n    }\r\n```",
        "createdAt" : "2021-04-06T00:12:25Z",
        "updatedAt" : "2021-04-06T00:12:26Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      }
    ],
    "commit" : "7edd25c82e3919f4d7f738f39e3f147aa5a0a849",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +869,873 @@        \"\"\".stripMargin)\n\n      checkPartitionPruningPredicate(df, false, false)\n\n      checkAnswer(df,"
  },
  {
    "id" : "a9d923bd-5d5b-4f60-80e5-a064b5ea78b9",
    "prId" : 31563,
    "prUrl" : "https://github.com/apache/spark/pull/31563#pullrequestreview-597536517",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6fcf2cf4-64a6-49ed-9ee4-d0cea5085b85",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "How about adding more tests for `LIKE ALL`, `NOT LIKE ANY`, and `NOT LIKE ALL`?",
        "createdAt" : "2021-02-24T05:25:01Z",
        "updatedAt" : "2021-02-24T05:25:02Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "0232a25e-dc37-40bb-bf78-c8ed62b7d308",
        "parentId" : "6fcf2cf4-64a6-49ed-9ee4-d0cea5085b85",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I don't have a strong opinion. It's caused by the multi like optimization and any of these 3 can trigger this bug.",
        "createdAt" : "2021-02-24T14:27:54Z",
        "updatedAt" : "2021-02-24T14:27:55Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "a1592d3261b837709c87f632e647465bb2c60745",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +1396,1400 @@          |SELECT date_id, product_id FROM fact_sk f\n          |JOIN dim_store s\n          |ON f.store_id = s.store_id WHERE s.country LIKE ANY ('%D%E%', '%A%B%')\n        \"\"\".stripMargin)\n"
  },
  {
    "id" : "82c47542-7901-4699-909c-371e109ee969",
    "prId" : 29641,
    "prUrl" : "https://github.com/apache/spark/pull/29641#pullrequestreview-483061691",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2cbc23c9-defc-4dd2-af50-2452f3893daf",
        "parentId" : null,
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "what is the difference between `!=` and `!==`?",
        "createdAt" : "2020-09-04T14:26:46Z",
        "updatedAt" : "2020-09-04T14:27:06Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "0dec7904-5486-42af-9667-778566c356fa",
        "parentId" : "2cbc23c9-defc-4dd2-af50-2452f3893daf",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "Using `!==`can give more informative error message:\r\nhttps://www.scalatest.org/getting_started_with_fun_suite",
        "createdAt" : "2020-09-05T15:28:10Z",
        "updatedAt" : "2020-09-05T15:29:15Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      }
    ],
    "commit" : "216f225bf8f2d92ba26cf02b83ed7f199f2a62a9",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +387,391 @@        assert(scan3.metrics(\"filesSize\").value == partFilesSize)\n        assert(scan3.metrics(\"numPartitions\").value === 1)\n        assert(scan3.metrics(\"pruningTime\").value !== -1)\n      }\n    }"
  },
  {
    "id" : "b86c6c4c-f650-4803-b46c-929dde25144b",
    "prId" : 29636,
    "prUrl" : "https://github.com/apache/spark/pull/29636#pullrequestreview-481689416",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "368e25e7-c929-41b1-8cce-9e772b2714f7",
        "parentId" : null,
        "authorId" : "78cb7df6-a1c6-4bcb-99c1-16fd18fb32f7",
        "body" : "We don't have two reasonable join keys between available tables, so I just reduce the number of records (partitions) here. It's unnecessary to create so many partitions in this test.",
        "createdAt" : "2020-09-03T10:04:01Z",
        "updatedAt" : "2020-09-03T14:06:08Z",
        "lastEditedBy" : "78cb7df6-a1c6-4bcb-99c1-16fd18fb32f7",
        "tags" : [
        ]
      }
    ],
    "commit" : "c49376a921e2d3aa1c412feb62cc40cb6b5d760c",
    "line" : 42,
    "diffHunk" : "@@ -1,1 +1144,1148 @@    withSQLConf(SQLConf.DYNAMIC_PARTITION_PRUNING_REUSE_BROADCAST_ONLY.key -> \"true\") {\n      withTable(\"fact\", \"dim\") {\n        spark.range(20).select($\"id\".as(\"A\"), $\"id\".as(\"AA\"))\n          .write.partitionBy(\"A\").format(tableFormat).mode(\"overwrite\").saveAsTable(\"fact\")\n        spark.range(10).select($\"id\".as(\"B\"), $\"id\".as(\"BB\"))"
  },
  {
    "id" : "6b9325c0-4e10-4c9c-b6c5-d5b0a25079f5",
    "prId" : 29475,
    "prUrl" : "https://github.com/apache/spark/pull/29475#pullrequestreview-475111468",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e9d0b511-6c3a-46bd-aadb-ca7ba3a31136",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Does this issue only happen in this struct case? Looks like the title `non-atomic type` suggests the other non-atomic cases can happen, too?",
        "createdAt" : "2020-08-26T02:03:38Z",
        "updatedAt" : "2020-08-26T02:42:32Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "8d3bb8b2-9cf9-4c84-b8eb-b53a09413076",
        "parentId" : "e9d0b511-6c3a-46bd-aadb-ca7ba3a31136",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "This will increase test time:\r\n\r\n```\r\n[info] - SPARK-32659: Fix the data issue when pruning DPP on non-atomic type (48 seconds, 170 milliseconds)\r\n```\r\n\r\nThis test tests `InSubqueryExec` use `InSet`.\r\n`InSet` itself has these tests:\r\nhttps://github.com/apache/spark/blob/1515d45b8db69de67cf61100f7cf4fa5c7cadbd4/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/PredicateSuite.scala#L321-L353",
        "createdAt" : "2020-08-26T02:41:14Z",
        "updatedAt" : "2020-08-26T02:42:32Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      }
    ],
    "commit" : "f92a594deafb98d36e5001937d6dc781891be7fe",
    "line" : 46,
    "diffHunk" : "@@ -1,1 +1338,1342 @@                  |FROM df1\n                  |  JOIN df2\n                  |  ON struct(df1.k) = struct(df2.k)\n                  |    AND df2.id < 2\n                  |\"\"\".stripMargin)"
  },
  {
    "id" : "a5704557-47bb-4fcf-a19e-44683aa3e493",
    "prId" : 26744,
    "prUrl" : "https://github.com/apache/spark/pull/26744#pullrequestreview-326491579",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "880dbf2c-30f1-417e-8213-eb15868cb5d6",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Why does this change match the description \"no predicate on the dimension table\"?",
        "createdAt" : "2019-12-03T08:57:30Z",
        "updatedAt" : "2019-12-03T08:57:31Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "fb3d400e-bcfe-4501-8fbf-d975fcb5f1d5",
        "parentId" : "880dbf2c-30f1-417e-8213-eb15868cb5d6",
        "authorId" : "cb93ab63-5789-483a-864c-9467d1506bf5",
        "body" : "If we don't change it. `partScan` will always be None. `hasPartitionPruningFilter` will be false after we change it. It will match \"no predicate on the dimension table\"\r\n```\r\nvar partScan = getPartitionTableScan(l, left)\r\nif (partScan.isDefined && canPruneLeft(joinType) &&\r\n    hasPartitionPruningFilter(right)) {\r\n    val hasBenefit = pruningHasBenefit(l, partScan.get, r, right)\r\n    newLeft = insertPredicate(l, newLeft, r, right, rightKeys, hasBenefit)\r\n} else {\r\n    partScan = getPartitionTableScan(r, right)\r\n    if (partScan.isDefined && canPruneRight(joinType) &&\r\n        hasPartitionPruningFilter(left) ) {\r\n        val hasBenefit = pruningHasBenefit(r, partScan.get, l, left)\r\n        newRight = insertPredicate(r, newRight, l, left, leftKeys, hasBenefit)\r\n    }\r\n}\r\n```",
        "createdAt" : "2019-12-03T09:22:23Z",
        "updatedAt" : "2019-12-03T09:27:17Z",
        "lastEditedBy" : "cb93ab63-5789-483a-864c-9467d1506bf5",
        "tags" : [
        ]
      },
      {
        "id" : "7baaf953-8f95-4be7-a614-fb2e54c64214",
        "parentId" : "880dbf2c-30f1-417e-8213-eb15868cb5d6",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Yes. This should be a join on the partitioned column, `fact_sk.store_id`, because the previous test covers `not a partition column` case. And, `no predicate on the dimension table` should mean `WHERE s.country = 'NL'`.",
        "createdAt" : "2019-12-03T22:26:39Z",
        "updatedAt" : "2019-12-03T22:26:39Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "b3de7630146d21778c72ed27c94ad8824ad7dde2",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +424,428 @@            |SELECT * FROM fact_sk f\n            |JOIN dim_store s\n            |ON f.store_id = s.store_id\n          \"\"\".stripMargin)\n"
  },
  {
    "id" : "4fc5b247-ddae-44ba-a5eb-7b6c6993115e",
    "prId" : 25644,
    "prUrl" : "https://github.com/apache/spark/pull/25644#pullrequestreview-284769484",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9d7ed69c-5573-4303-9876-7e1730021faa",
        "parentId" : null,
        "authorId" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "body" : "Nit: you could also do\r\n```\r\n      val countSubqueryBroadcasts =\r\n        plan.collectInPlanAndSubqueries({ case _: SubqueryBroadcastExec => 1 }).sum\r\n      val countReusedSubqueryBroadcasts =\r\n        plan.collectInPlanAndSubqueries({\r\n          case ReusedSubqueryExec(_: SubqueryBroadcastExec) => 1\r\n        }).sum\r\n\r\n      assert(countSubqueryBroadcasts == 1)\r\n      assert(countReusedSubqueryBroadcasts == 1)\r\n```",
        "createdAt" : "2019-09-05T14:43:16Z",
        "updatedAt" : "2019-10-28T15:44:42Z",
        "lastEditedBy" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "tags" : [
        ]
      },
      {
        "id" : "106410f3-3385-4c89-b5c9-809f9bef65a8",
        "parentId" : "9d7ed69c-5573-4303-9876-7e1730021faa",
        "authorId" : "3d4870da-39a8-4406-b00d-131930d14cd8",
        "body" : "Ok, changed it.\r\nWhat do you think about removing `expressions.InSubquery` from `PlanAdaptiveSubqueries`? I don't see why it is there, but not sure that it should be removed in this PR.",
        "createdAt" : "2019-09-05T15:26:51Z",
        "updatedAt" : "2019-10-28T15:44:42Z",
        "lastEditedBy" : "3d4870da-39a8-4406-b00d-131930d14cd8",
        "tags" : [
        ]
      },
      {
        "id" : "d20fff0b-4c83-4468-b9cc-03193eabcf0e",
        "parentId" : "9d7ed69c-5573-4303-9876-7e1730021faa",
        "authorId" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "body" : "We have `InSubqueryExec` now in DPP, so better keep it.",
        "createdAt" : "2019-09-05T19:37:19Z",
        "updatedAt" : "2019-10-28T15:44:42Z",
        "lastEditedBy" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "tags" : [
        ]
      },
      {
        "id" : "349ca08a-4c3e-4f78-83d6-a8f0ba0519c4",
        "parentId" : "9d7ed69c-5573-4303-9876-7e1730021faa",
        "authorId" : "3d4870da-39a8-4406-b00d-131930d14cd8",
        "body" : "Ok, I've reverted the change.",
        "createdAt" : "2019-09-06T09:59:08Z",
        "updatedAt" : "2019-10-28T15:44:42Z",
        "lastEditedBy" : "3d4870da-39a8-4406-b00d-131930d14cd8",
        "tags" : [
        ]
      }
    ],
    "commit" : "f86b8c3142fba8f4ed9d2e628c4eb085f26e8f8e",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1256,1260 @@        plan.collectInPlanAndSubqueries({ case _: SubqueryBroadcastExec => 1 }).sum\n      val countReusedSubqueryBroadcasts =\n        plan.collectInPlanAndSubqueries({\n          case ReusedSubqueryExec(_: SubqueryBroadcastExec) => 1\n        }).sum"
  }
]