[
  {
    "id" : "833fd0c4-99a2-4e06-9ad2-0ab1daf4d0e5",
    "prId" : 32953,
    "prUrl" : "https://github.com/apache/spark/pull/32953#pullrequestreview-687505318",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "23749fed-7aaf-4faf-b012-cb908c5c7fa5",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "We should support watermarks as ANSI intervals in the future. @sarutak Could you open an JIRA for that, so, we won't forget to implement it.",
        "createdAt" : "2021-06-17T19:45:50Z",
        "updatedAt" : "2021-06-17T19:46:53Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "944b4168-ac99-4426-9ba1-402afd59a637",
        "parentId" : "23749fed-7aaf-4faf-b012-cb908c5c7fa5",
        "authorId" : "3a227965-84e0-47cf-9974-11293764f028",
        "body" : "O.K, I'll open it.",
        "createdAt" : "2021-06-18T15:01:46Z",
        "updatedAt" : "2021-06-18T15:01:46Z",
        "lastEditedBy" : "3a227965-84e0-47cf-9974-11293764f028",
        "tags" : [
        ]
      }
    ],
    "commit" : "5de87360d98867cd7b16a86e02a592d7a6835796",
    "line" : 43,
    "diffHunk" : "@@ -1,1 +786,790 @@        (\"10 minutes\",\n          \"interval 3 minutes 30 seconds\"),\n        (\"10 minutes\",\n          \"interval '3:30' minute to second\")).foreach { case (watermark, bound) =>\n        val (leftInput2, rightInput2, joined2) ="
  },
  {
    "id" : "b6aeaa93-19ce-4f27-9d46-193e6ead76f5",
    "prId" : 32176,
    "prUrl" : "https://github.com/apache/spark/pull/32176#pullrequestreview-637012916",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a24ca9d0-9003-4c6f-83bb-801f08f6821f",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "For some reasons, `CalendarInterval` is used in the streaming code. For instance at\r\nhttps://github.com/apache/spark/blob/0945baf90660a101ae0f86a39d4c91ca74ae5ee3/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/EventTimeWatermarkExec.scala#L89-L91\r\n\r\nNeed to revise the code. The streaming tests fails on new ANSI intervals at the moment.",
        "createdAt" : "2021-04-15T18:34:30Z",
        "updatedAt" : "2021-04-16T11:24:15Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "7668405ebd902cea698b76c373d0c06b1e759426",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +50,54 @@\n  override protected def sparkConf = super.sparkConf\n    // TODO(SPARK-35095): Use ANSI intervals in streaming join tests\n    .set(SQLConf.LEGACY_INTERVAL_ENABLED, true)\n"
  },
  {
    "id" : "0e91d086-568f-4d49-8b5b-544fe410348c",
    "prId" : 30563,
    "prUrl" : "https://github.com/apache/spark/pull/30563#pullrequestreview-542471694",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "38088b73-2c0b-4510-8fe4-10ba6078ddbe",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Fixing nits alone in a PR is not encouraged in general. They can be fixed together when you touch this codes or review other PRs to fix. It's okay since the PR is already open but let's avoid next time.",
        "createdAt" : "2020-12-02T02:59:55Z",
        "updatedAt" : "2020-12-02T02:59:55Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "de5a19ff-1ebe-429e-ad89-a16c74d66db4",
        "parentId" : "38088b73-2c0b-4510-8fe4-10ba6078ddbe",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "@HyukjinKwon - thanks for pointing it out. Just for my own understanding to better follow next time, is `[MINOR][FOLLOWUP]` PR discouraged, or `[MINOR][FOLLOWUP]` nits (renaming, fixing code style, etc) PR discouraged?\r\n\r\nAnd what's could be a MINOR PR besides nits (maybe refactoring)? Thanks.",
        "createdAt" : "2020-12-02T03:07:34Z",
        "updatedAt" : "2020-12-02T03:07:34Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      },
      {
        "id" : "dc1e92b8-687b-4ac8-9654-7050638044bc",
        "parentId" : "38088b73-2c0b-4510-8fe4-10ba6078ddbe",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "That's not just a nit, as it brought actual confusion. I actually requested a new PR as I don't like to mix this up with the origin PR, #30395",
        "createdAt" : "2020-12-02T03:08:53Z",
        "updatedAt" : "2020-12-02T03:09:06Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "28819e70-a937-4bf5-8668-e511cabb9629",
        "parentId" : "38088b73-2c0b-4510-8fe4-10ba6078ddbe",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Ah, okay. Gotya.",
        "createdAt" : "2020-12-02T03:27:03Z",
        "updatedAt" : "2020-12-02T03:27:03Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "fef24030b31ebdff15fe3ee003da8c8bc0e6d564",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +137,141 @@  }\n\n  protected def setupJoinWithRangeCondition(joinType: String)\n    : (MemoryStream[(Int, Int)], MemoryStream[(Int, Int)], DataFrame) = {\n"
  },
  {
    "id" : "dbbb6b93-59b4-41c2-ba17-cf10e0c75539",
    "prId" : 30076,
    "prUrl" : "https://github.com/apache/spark/pull/30076#pullrequestreview-512403571",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "db17160c-58a3-4e40-a338-3d18dd0a9fac",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "It'd be pretty much helpful to provide guide comments on tracking refactors.\r\ne.g. this is equivalent to `StreamingOuterJoinSuite.setupStream` with changing signature `private` to `protected` to co-use.",
        "createdAt" : "2020-10-20T07:25:55Z",
        "updatedAt" : "2020-10-21T22:41:42Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "14871d9d2be6b751687e78dd4d17c2e249b8f205",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +56,60 @@  }\n\n  protected def setupStream(prefix: String, multiplier: Int): (MemoryStream[Int], DataFrame) = {\n    val input = MemoryStream[Int]\n    val df = input.toDF"
  },
  {
    "id" : "4b9eff0e-50cf-4175-b782-54d2d8f7185b",
    "prId" : 30076,
    "prUrl" : "https://github.com/apache/spark/pull/30076#pullrequestreview-512403571",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "73c879a3-4ef7-4634-864f-878fcdd1d70a",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "For reviewers: this is equivalent to `StreamingOuterJoinSuite.setupWindowedJoin` with changing \r\n1) signature `private` to `protected`\r\n2) conditional select on left_semi vs others, as in left_semi only left side of columns are available",
        "createdAt" : "2020-10-20T07:27:53Z",
        "updatedAt" : "2020-10-21T22:41:42Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "14871d9d2be6b751687e78dd4d17c2e249b8f205",
    "line" : 41,
    "diffHunk" : "@@ -1,1 +76,80 @@    val windowed2 = df2.select('key, window('rightTime, \"10 second\"), 'rightValue)\n    val joined = windowed1.join(windowed2, Seq(\"key\", \"window\"), joinType)\n    val select = if (joinType == \"left_semi\") {\n      joined.select('key, $\"window.end\".cast(\"long\"), 'leftValue)\n    } else {"
  },
  {
    "id" : "42db3ca5-059e-4dd8-ac3d-22e5a413aa00",
    "prId" : 30076,
    "prUrl" : "https://github.com/apache/spark/pull/30076#pullrequestreview-512403571",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "122d01ed-a213-4047-a563-044bbdd33089",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "For reviewers: this is extracted from `test(\"left outer early state exclusion on left\")` / `test(\"right outer early state exclusion on left\")`, with adding select per join type.",
        "createdAt" : "2020-10-20T07:32:36Z",
        "updatedAt" : "2020-10-21T22:41:42Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "14871d9d2be6b751687e78dd4d17c2e249b8f205",
    "line" : 50,
    "diffHunk" : "@@ -1,1 +85,89 @@  }\n\n  protected def setupWindowedJoinWithLeftCondition(joinType: String)\n    : (MemoryStream[Int], MemoryStream[Int], DataFrame) = {\n"
  },
  {
    "id" : "95ab5145-276c-4a8b-b996-51553ffbf87b",
    "prId" : 30076,
    "prUrl" : "https://github.com/apache/spark/pull/30076#pullrequestreview-512403571",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5371ce8c-edfb-45c8-a5a5-38695028d925",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "For reviewers: this is extracted from `test(\"left outer early state exclusion on right\")` / `test(\"right outer early state exclusion on right\")`, with adding select per join type.",
        "createdAt" : "2020-10-20T07:34:21Z",
        "updatedAt" : "2020-10-21T22:41:42Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "14871d9d2be6b751687e78dd4d17c2e249b8f205",
    "line" : 79,
    "diffHunk" : "@@ -1,1 +114,118 @@  }\n\n  protected def setupWindowedJoinWithRightCondition(joinType: String)\n    : (MemoryStream[Int], MemoryStream[Int], DataFrame) = {\n"
  },
  {
    "id" : "c62668ca-29b9-4bc8-8109-9f07fe6466af",
    "prId" : 30076,
    "prUrl" : "https://github.com/apache/spark/pull/30076#pullrequestreview-512403571",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e5bba441-bc13-4f80-956a-654a232dd6d5",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "For reviewers: this is extracted from `test(s\"${joinType.replaceAllLiterally(\"_\", \" \")} with watermark range condition\")`, with conditional select on left_semi vs others, as in left_semi only left side of columns are available.",
        "createdAt" : "2020-10-20T07:38:50Z",
        "updatedAt" : "2020-10-21T22:41:42Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "14871d9d2be6b751687e78dd4d17c2e249b8f205",
    "line" : 108,
    "diffHunk" : "@@ -1,1 +143,147 @@  }\n\n  protected def setupWindowedJoinWithRangeCondition(joinType: String)\n    : (MemoryStream[(Int, Int)], MemoryStream[(Int, Int)], DataFrame) = {\n"
  },
  {
    "id" : "c8c47286-59fe-4a62-9a75-c31effb40367",
    "prId" : 30076,
    "prUrl" : "https://github.com/apache/spark/pull/30076#pullrequestreview-512403571",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1ab75718-f527-4296-a2a8-b260d75a365d",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "For reviewers: this is extracted from `test(\"SPARK-26187 self left outer join should not return outer nulls for already matched rows\")`, with conditional select on left_semi vs others, as in left_semi only left side of columns are available.",
        "createdAt" : "2020-10-20T07:41:01Z",
        "updatedAt" : "2020-10-21T22:41:42Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "14871d9d2be6b751687e78dd4d17c2e249b8f205",
    "line" : 139,
    "diffHunk" : "@@ -1,1 +174,178 @@  }\n\n  protected def setupWindowedSelfJoin(joinType: String)\n    : (MemoryStream[(Int, Long)], DataFrame) = {\n"
  },
  {
    "id" : "452ec4ee-0548-4a76-b704-07662a4344eb",
    "prId" : 30076,
    "prUrl" : "https://github.com/apache/spark/pull/30076#pullrequestreview-512403571",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9529e8aa-690f-4250-971e-f7c576808456",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "It might be better to add a code comment why there's no change on state.\r\n(e.g. Unlike inner/outer joins, given left input row matches with right input row, we don't buffer the left input row to the state.)",
        "createdAt" : "2020-10-20T07:47:23Z",
        "updatedAt" : "2020-10-21T22:41:42Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "14871d9d2be6b751687e78dd4d17c2e249b8f205",
    "line" : 408,
    "diffHunk" : "@@ -1,1 +1112,1116 @@      // left: 21\n      // right: 22\n      assertNumStateRows(total = 2, updated = 0),\n      StopStream,\n      StartStream(),"
  },
  {
    "id" : "2bd9a6c9-314e-4c7e-8787-f0ec61d0149c",
    "prId" : 30076,
    "prUrl" : "https://github.com/apache/spark/pull/30076#pullrequestreview-512403571",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3e5c99a1-eb43-4259-be63-263c1364060f",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "I see you've referred the existing test code:\r\n\r\n```\r\n      AddData(input2, 1),\r\n      CheckNewAnswer(),                             // Should not join as < 15 removed\r\n      assertNumStateRows(total = 2, updated = 0),   // row not add as 1 < state key watermark = 15\r\n\r\n      AddData(input1, 5),\r\n      CheckNewAnswer(),                             // Same reason as above\r\n      assertNumStateRows(total = 2, updated = 0, droppedByWatermark = 1)\r\n```\r\n\r\ncould you please add `assertNumStateRows` per case here as well?",
        "createdAt" : "2020-10-20T07:54:33Z",
        "updatedAt" : "2020-10-21T22:41:42Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "14871d9d2be6b751687e78dd4d17c2e249b8f205",
    "line" : 412,
    "diffHunk" : "@@ -1,1 +1116,1120 @@      StartStream(),\n\n      AddData(leftInput, 1),\n      // Row not add as 1 < state key watermark = 12.\n      CheckNewAnswer(),"
  },
  {
    "id" : "b33b5f86-cdeb-4b08-8e62-a5675c51e2d7",
    "prId" : 30076,
    "prUrl" : "https://github.com/apache/spark/pull/30076#pullrequestreview-512403571",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "513bc254-f7f7-493b-973b-f42d85602c95",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Can you describe the details on the state rows, and add the same num state rows verification on below after each CheckNewAnswer?",
        "createdAt" : "2020-10-20T08:40:04Z",
        "updatedAt" : "2020-10-21T22:41:42Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "14871d9d2be6b751687e78dd4d17c2e249b8f205",
    "line" : 440,
    "diffHunk" : "@@ -1,1 +1144,1148 @@      // left: 3\n      // right: 3, 4, 5\n      assertNumStateRows(total = 4, updated = 4),\n      // We shouldn't get more semi join rows when the watermark advances.\n      MultiAddData(leftInput, 20)(rightInput, 21),"
  },
  {
    "id" : "b1b63fbd-0d11-42d3-9a3b-77255ba9aa7a",
    "prId" : 30076,
    "prUrl" : "https://github.com/apache/spark/pull/30076#pullrequestreview-512403571",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "76b77528-06b1-4579-92f3-193196ef8556",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Same here.",
        "createdAt" : "2020-10-20T08:40:14Z",
        "updatedAt" : "2020-10-21T22:41:42Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "14871d9d2be6b751687e78dd4d17c2e249b8f205",
    "line" : 472,
    "diffHunk" : "@@ -1,1 +1176,1180 @@      // left: 3, 4, 5\n      // right: 3\n      assertNumStateRows(total = 4, updated = 4),\n      // We shouldn't get more semi join rows when the watermark advances.\n      MultiAddData(leftInput, 20)(rightInput, 21),"
  },
  {
    "id" : "0b1bbf85-24bc-4e29-a61d-736830ad0cd3",
    "prId" : 30076,
    "prUrl" : "https://github.com/apache/spark/pull/30076#pullrequestreview-512403571",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3ce15c1a-5b3d-46f1-9825-e4430a1d4c10",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Same here.",
        "createdAt" : "2020-10-20T08:40:50Z",
        "updatedAt" : "2020-10-21T22:41:42Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "14871d9d2be6b751687e78dd4d17c2e249b8f205",
    "line" : 543,
    "diffHunk" : "@@ -1,1 +1247,1251 @@    testStream(query)(\n      AddData(inputStream, (1, 1L), (2, 2L), (3, 3L), (4, 4L), (5, 5L)),\n      CheckNewAnswer((2, 2), (4, 4)),\n      // batch 1 - global watermark = 0\n      // states"
  },
  {
    "id" : "52e41ae3-6bb5-4cff-b3f4-b398322d34db",
    "prId" : 30076,
    "prUrl" : "https://github.com/apache/spark/pull/30076#pullrequestreview-513603480",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d7cc2584-7b3a-4410-ae94-e93edc02d14c",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "I see you exclude 1, 2 from right in this commit and nothing changed for both output and state rows. Could you please explain this, say, the reason 1, 2 in right side are not added in state?",
        "createdAt" : "2020-10-21T08:18:05Z",
        "updatedAt" : "2020-10-21T22:41:42Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "a6448d62-5453-4999-932e-e3b51e61b328",
        "parentId" : "d7cc2584-7b3a-4410-ae94-e93edc02d14c",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "The join condition has `leftValue > 4` on left side. For left semi join there's a logical plan optimization rule to push down the condition from join (`PushPredicateThroughJoin`), so there's a filter operator on right side as well to filter out the rows with 1,2 before join. So the join result and state store will be the same with/without 1,2 rows because they are filtered out before join.\r\n\r\nLeft semi join physical plan:\r\n\r\n```\r\n*(4) Project [key#3, cast(precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(leftTime#4-T10000ms, TimestampType, LongType) - 0) as double) / 1.0E7)) as double) = (cast((precisetimestampconversion(leftTime#4-T10000ms, TimestampType, LongType) - 0) as double) / 1.0E7)) THEN (CEIL((cast((precisetimestampconversion(leftTime#4-T10000ms, TimestampType, LongType) - 0) as double) / 1.0E7)) + 1) ELSE CEIL((cast((precisetimestampconversion(leftTime#4-T10000ms, TimestampType, LongType) - 0) as double) / 1.0E7)) END + 0) - 1) * 10000000) + 10000000), LongType, TimestampType) as bigint) AS end#46L, leftValue#5]\r\n+- StreamingSymmetricHashJoin [key#3, named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(leftTime#4-T10000ms, TimestampType, LongType) - 0) as double) / 1.0E7)) as double) = (cast((precisetimestampconversion(leftTime#4-T10000ms, TimestampType, LongType) - 0) as double) / 1.0E7)) THEN (CEIL((cast((precisetimestampconversion(leftTime#4-T10000ms, TimestampType, LongType) - 0) as double) / 1.0E7)) + 1) ELSE CEIL((cast((precisetimestampconversion(leftTime#4-T10000ms, TimestampType, LongType) - 0) as double) / 1.0E7)) END + 0) - 1) * 10000000) + 0), LongType, TimestampType), end, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(leftTime#4-T10000ms, TimestampType, LongType) - 0) as double) / 1.0E7)) as double) = (cast((precisetimestampconversion(leftTime#4-T10000ms, TimestampType, LongType) - 0) as double) / 1.0E7)) THEN (CEIL((cast((precisetimestampconversion(leftTime#4-T10000ms, TimestampType, LongType) - 0) as double) / 1.0E7)) + 1) ELSE CEIL((cast((precisetimestampconversion(leftTime#4-T10000ms, TimestampType, LongType) - 0) as double) / 1.0E7)) END + 0) - 1) * 10000000) + 10000000), LongType, TimestampType))], [key#12, window#23-T10000ms], LeftSemi, condition = [ leftOnly = null, rightOnly = null, both = null, full = null ], state info [ checkpoint = <unknown>, runId = 92a31d09-4275-4ee6-8bba-03e1973b4298, opId = 0, ver = 0, numPartitions = 5], 0, state cleanup [ left key predicate: (input[1, struct<start:timestamp,end:timestamp>, false].end <= 0), right key predicate: (input[1, struct<start:timestamp,end:timestamp>, false].end <= 0) ], 2\r\n   :- Exchange hashpartitioning(key#3, named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(leftTime#4-T10000ms, TimestampType, LongType) - 0) as double) / 1.0E7)) as double) = (cast((precisetimestampconversion(leftTime#4-T10000ms, TimestampType, LongType) - 0) as double) / 1.0E7)) THEN (CEIL((cast((precisetimestampconversion(leftTime#4-T10000ms, TimestampType, LongType) - 0) as double) / 1.0E7)) + 1) ELSE CEIL((cast((precisetimestampconversion(leftTime#4-T10000ms, TimestampType, LongType) - 0) as double) / 1.0E7)) END + 0) - 1) * 10000000) + 0), LongType, TimestampType), end, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(leftTime#4-T10000ms, TimestampType, LongType) - 0) as double) / 1.0E7)) as double) = (cast((precisetimestampconversion(leftTime#4-T10000ms, TimestampType, LongType) - 0) as double) / 1.0E7)) THEN (CEIL((cast((precisetimestampconversion(leftTime#4-T10000ms, TimestampType, LongType) - 0) as double) / 1.0E7)) + 1) ELSE CEIL((cast((precisetimestampconversion(leftTime#4-T10000ms, TimestampType, LongType) - 0) as double) / 1.0E7)) END + 0) - 1) * 10000000) + 10000000), LongType, TimestampType)), 5), true, [id=#54]\r\n   :  +- EventTimeWatermark leftTime#4: timestamp, 10 seconds\r\n   :     +- *(1) Project [value#1 AS key#3, timestamp_seconds(value#1) AS leftTime#4, (value#1 * 2) AS leftValue#5]\r\n   :        +- *(1) Filter ((value#1 * 2) > 4)\r\n   :           +- StreamingRelation memory, [value#1]\r\n   +- Exchange hashpartitioning(key#12, window#23-T10000ms, 5), true, [id=#63]\r\n      +- *(3) Project [key#12, named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(rightTime#13-T10000ms, TimestampType, LongType) - 0) as double) / 1.0E7)) as double) = (cast((precisetimestampconversion(rightTime#13-T10000ms, TimestampType, LongType) - 0) as double) / 1.0E7)) THEN (CEIL((cast((precisetimestampconversion(rightTime#13-T10000ms, TimestampType, LongType) - 0) as double) / 1.0E7)) + 1) ELSE CEIL((cast((precisetimestampconversion(rightTime#13-T10000ms, TimestampType, LongType) - 0) as double) / 1.0E7)) END + 0) - 1) * 10000000) + 0), LongType, TimestampType), end, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(rightTime#13-T10000ms, TimestampType, LongType) - 0) as double) / 1.0E7)) as double) = (cast((precisetimestampconversion(rightTime#13-T10000ms, TimestampType, LongType) - 0) as double) / 1.0E7)) THEN (CEIL((cast((precisetimestampconversion(rightTime#13-T10000ms, TimestampType, LongType) - 0) as double) / 1.0E7)) + 1) ELSE CEIL((cast((precisetimestampconversion(rightTime#13-T10000ms, TimestampType, LongType) - 0) as double) / 1.0E7)) END + 0) - 1) * 10000000) + 10000000), LongType, TimestampType)) AS window#23-T10000ms]\r\n         +- EventTimeWatermark rightTime#13: timestamp, 10 seconds\r\n            +- *(2) Project [value#10 AS key#12, timestamp_seconds(value#10) AS rightTime#13]\r\n               +- *(2) Filter ((value#10 * 2) > 4)\r\n                  +- StreamingRelation memory, [value#10]\r\n```",
        "createdAt" : "2020-10-21T08:30:37Z",
        "updatedAt" : "2020-10-21T22:41:42Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      },
      {
        "id" : "df11ed9b-fc3f-41ae-bfdf-884c163912b9",
        "parentId" : "d7cc2584-7b3a-4410-ae94-e93edc02d14c",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Ah OK thanks for explanation. I agree it's probably less confused to just remove them from right side instead of being affected by the optimizer.",
        "createdAt" : "2020-10-21T12:22:57Z",
        "updatedAt" : "2020-10-21T22:41:42Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "14871d9d2be6b751687e78dd4d17c2e249b8f205",
    "line" : 433,
    "diffHunk" : "@@ -1,1 +1137,1141 @@\n    testStream(joined)(\n      MultiAddData(leftInput, 1, 2, 3)(rightInput, 3, 4, 5),\n      // The left rows with leftValue <= 4 should not generate their semi join rows and\n      // not get added to the state."
  },
  {
    "id" : "54cdf968-a2ee-4507-97eb-0056c0df4c2a",
    "prId" : 30076,
    "prUrl" : "https://github.com/apache/spark/pull/30076#pullrequestreview-514125232",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "323f71f1-c94b-4884-8355-af3a087acab5",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Probably better to leave the similar explanation (like you do for left state) for right state.",
        "createdAt" : "2020-10-21T12:49:31Z",
        "updatedAt" : "2020-10-21T22:41:42Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "18242f88-2ba2-4264-b2b2-8b4acc98bdc9",
        "parentId" : "323f71f1-c94b-4884-8355-af3a087acab5",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "@HeartSaVioR - updated, I also figured the optimization rule should be `PushPredicateThroughJoin`, instead of `PushDownLeftSemiAntiJoin `, updated comment as well.",
        "createdAt" : "2020-10-21T20:08:08Z",
        "updatedAt" : "2020-10-21T22:41:42Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      }
    ],
    "commit" : "14871d9d2be6b751687e78dd4d17c2e249b8f205",
    "line" : 548,
    "diffHunk" : "@@ -1,1 +1252,1256 @@      // left: (2, 2L), (4, 4L)\n      //       (left rows with value % 2 != 0 is filtered per [[PushPredicateThroughJoin]])\n      // right: (2, 2L), (4, 4L)\n      //       (right rows with value % 2 != 0 is filtered per [[PushPredicateThroughJoin]])\n      assertNumStateRows(total = 4, updated = 4),"
  },
  {
    "id" : "8d70fe90-4673-470a-8106-66efb0e5224c",
    "prId" : 28975,
    "prUrl" : "https://github.com/apache/spark/pull/28975#pullrequestreview-445143453",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0c2ddcb0-63c2-4974-afae-3de9a0e66b84",
        "parentId" : null,
        "authorId" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "body" : "Any specific reason why not use `select`? I don't see any expression here.",
        "createdAt" : "2020-07-08T15:07:09Z",
        "updatedAt" : "2020-07-08T22:27:28Z",
        "lastEditedBy" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "tags" : [
        ]
      },
      {
        "id" : "1294ae07-f65a-4d48-8f13-3269c9621862",
        "parentId" : "0c2ddcb0-63c2-4974-afae-3de9a0e66b84",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "I guess it's pretty much simpler and more readable than `select('_1.as(\"eventTime\"), '_2.as(\"id\"), '_3.as(\"comment\"))` (or even with col(...) if ' notation doesn't work for _1, _2, _3).",
        "createdAt" : "2020-07-08T21:31:06Z",
        "updatedAt" : "2020-07-08T22:27:28Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "fb63d7ed604e9fc31a960e23e9b00298d5aa8760",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +1002,1006 @@    val input1 = MemoryStream[(Timestamp, String, String)]\n    val df1 = input1.toDF\n      .selectExpr(\"_1 as eventTime\", \"_2 as id\", \"_3 as comment\")\n      .withWatermark(s\"eventTime\", \"2 minutes\")\n"
  },
  {
    "id" : "e3768efa-bae7-4209-a6f2-aa1c620394ad",
    "prId" : 28975,
    "prUrl" : "https://github.com/apache/spark/pull/28975#pullrequestreview-445144088",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5e6bf87a-7ccc-4afc-93ac-7f4da404e698",
        "parentId" : null,
        "authorId" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "body" : "Same here.",
        "createdAt" : "2020-07-08T15:07:17Z",
        "updatedAt" : "2020-07-08T22:27:28Z",
        "lastEditedBy" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "tags" : [
        ]
      },
      {
        "id" : "b33b227b-c429-4efb-960c-8e4adefc3c9f",
        "parentId" : "5e6bf87a-7ccc-4afc-93ac-7f4da404e698",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Same here as well.",
        "createdAt" : "2020-07-08T21:32:15Z",
        "updatedAt" : "2020-07-08T22:27:28Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "fb63d7ed604e9fc31a960e23e9b00298d5aa8760",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +1007,1011 @@    val input2 = MemoryStream[(Timestamp, String, String)]\n    val df2 = input2.toDF\n      .selectExpr(\"_1 as eventTime\", \"_2 as id\", \"_3 as name\")\n      .withWatermark(s\"eventTime\", \"4 minutes\")\n"
  },
  {
    "id" : "50f547fc-8cd5-4442-9709-19dfe60d6475",
    "prId" : 28326,
    "prUrl" : "https://github.com/apache/spark/pull/28326#pullrequestreview-401668164",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4afac305-b487-4dd6-8f4b-b4831fe55aa4",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "super nit. `out` -> `outer`, Let's ignore for now.",
        "createdAt" : "2020-04-27T19:20:55Z",
        "updatedAt" : "2020-04-27T19:20:56Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "424560ce-334d-45c5-a76a-1a381d7d9797",
        "parentId" : "4afac305-b487-4dd6-8f4b-b4831fe55aa4",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "Thanks, delete this test in #28390.",
        "createdAt" : "2020-04-28T09:40:48Z",
        "updatedAt" : "2020-04-28T09:40:49Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "05ed338a23829875683fca1efafa32340bad271f",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +993,997 @@  }\n\n  test(\"SPARK-27340 Windowed left out join with Alias on TimeWindow\") {\n    val (leftInput, df1) = setupStream(\"left\", 2)\n    val (rightInput, df2) = setupStream(\"right\", 3)"
  },
  {
    "id" : "c3a0c0df-37e3-4cce-b154-362675c7df1f",
    "prId" : 28326,
    "prUrl" : "https://github.com/apache/spark/pull/28326#pullrequestreview-401667998",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "79bd1590-44f7-4f67-9445-7df795c5951b",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "I guess this is to retain the efforts of origin PR, but based on root cause, it should be pretty much easier to reproduce (and you actually did it in EventTimeWatermarkSuite).\r\n\r\nLet's remove this test in new commit (so that we can still retain the credit) and append more code on new UT to do E2E test. I'll comment there for code we need to add.",
        "createdAt" : "2020-04-27T20:47:41Z",
        "updatedAt" : "2020-04-27T20:52:41Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "17a0c36c-5ecb-4cbd-902b-17101df20b61",
        "parentId" : "79bd1590-44f7-4f67-9445-7df795c5951b",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "Thanks, addressed in #28390.",
        "createdAt" : "2020-04-28T09:40:35Z",
        "updatedAt" : "2020-04-28T09:40:35Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "05ed338a23829875683fca1efafa32340bad271f",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +993,997 @@  }\n\n  test(\"SPARK-27340 Windowed left out join with Alias on TimeWindow\") {\n    val (leftInput, df1) = setupStream(\"left\", 2)\n    val (rightInput, df2) = setupStream(\"right\", 3)"
  },
  {
    "id" : "1eda6354-85b8-46d0-a20f-25cb0340957a",
    "prId" : 26108,
    "prUrl" : "https://github.com/apache/spark/pull/26108#pullrequestreview-389689825",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "63287a56-c815-4832-b48e-99d1bbdc3ed5",
        "parentId" : null,
        "authorId" : "9cd657f0-37cd-41bd-908a-f2c996b95a7a",
        "body" : "@HeartSaVioR Question on this test. The point of this test is to ensure that Spark 3.x can read join state from Spark 2.4. But since the format was changed for stream-stream outer joins (that is, only outer joins use the new format), shouldn't we do this compatibility test for outer joins?",
        "createdAt" : "2020-03-27T21:57:45Z",
        "updatedAt" : "2020-03-27T21:57:45Z",
        "lastEditedBy" : "9cd657f0-37cd-41bd-908a-f2c996b95a7a",
        "tags" : [
        ]
      },
      {
        "id" : "23fd1e00-fe07-4eff-aaa6-1e8d4cb13b38",
        "parentId" : "63287a56-c815-4832-b48e-99d1bbdc3ed5",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Please find `SPARK-26187 restore the stream-stream outer join query from Spark 2.4` in this suite. (Yeah twos are unintentionally a bit far away, my bad. If moving test will help readability I'll do it as follow-up PR. Please let me know how you think.)\r\n\r\nOne thing, the format is changed for stream-stream joins instead of only outer joins, to allow changing the type of join inner <-> left/right outer, which is technically possible (with such correctness issue) in previous state and now possible without correctness issue.\r\n\r\nSpark will fail the query if end users try to read state version 1 for stream-stream outer join, but will still allow state version 1 for stream-stream inner join to reduce the impact. Once they try to change the join type to outer then they'll have error message as covered in the test I mentioned above.",
        "createdAt" : "2020-03-27T22:38:23Z",
        "updatedAt" : "2020-03-27T22:38:23Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "44c92861-92db-4eed-b143-a8b38ea98d84",
        "parentId" : "63287a56-c815-4832-b48e-99d1bbdc3ed5",
        "authorId" : "9cd657f0-37cd-41bd-908a-f2c996b95a7a",
        "body" : "Thank you for the clarification. ",
        "createdAt" : "2020-04-08T06:41:18Z",
        "updatedAt" : "2020-04-08T06:41:19Z",
        "lastEditedBy" : "9cd657f0-37cd-41bd-908a-f2c996b95a7a",
        "tags" : [
        ]
      }
    ],
    "commit" : "7213d4266b7bfa2bd05225c68e2184a7abb24469",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +422,426 @@  }\n\n  test(\"SPARK-26187 restore the stream-stream inner join query from Spark 2.4\") {\n    val inputStream = MemoryStream[(Int, Long)]\n    val df = inputStream.toDS()"
  }
]