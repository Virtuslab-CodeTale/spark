[
  {
    "id" : "87c52cad-3d78-4703-b310-21789f4b54ac",
    "prId" : 26768,
    "prUrl" : "https://github.com/apache/spark/pull/26768#pullrequestreview-328065934",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b0ff64d9-56aa-4bb3-b0f3-0661ca836532",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Can we .. \r\n\r\n1.  make `QueryTest.checkAnswer` private so that it can only be called in its companion class `QueryTest`\r\n2. and avoid to call `checkAnswer` in the `QueryTest` object in the tests\r\n\r\n?",
        "createdAt" : "2019-12-06T08:39:40Z",
        "updatedAt" : "2019-12-06T08:39:40Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "f7733eaf-607b-4471-8db8-7029c81e17cc",
        "parentId" : "b0ff64d9-56aa-4bb3-b0f3-0661ca836532",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Ah, basically similar comment with @HeartSaVioR's.",
        "createdAt" : "2019-12-06T08:40:21Z",
        "updatedAt" : "2019-12-06T08:40:22Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "b2d14506-83af-4e8c-8225-f44693b6ac55",
        "parentId" : "b0ff64d9-56aa-4bb3-b0f3-0661ca836532",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "I think @HyukjinKwon's suggestion would be better for future modification in point of preventing the cause, if there's no actual usage of `QueryTest.checkAnswer` from outside of `QueryTest`. (I guess it might have been misused like this PR fixes...)",
        "createdAt" : "2019-12-06T08:53:16Z",
        "updatedAt" : "2019-12-06T08:53:16Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "61eacc940381ef717f102c809ef80afd9b0facf7",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +367,371 @@      val caught = intercept[SparkException] {\n        withSQLConf(SOURCES_BINARY_FILE_MAX_LENGTH.key -> (content.length - 1).toString) {\n          QueryTest.checkAnswer(readContent(), expected) match {\n            case Some(errorMessage) => fail(errorMessage)\n            case None =>"
  },
  {
    "id" : "26030392-e2de-4168-be78-5ff2e9fef53a",
    "prId" : 25463,
    "prUrl" : "https://github.com/apache/spark/pull/25463#pullrequestreview-275497731",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f4691209-f816-4a7b-9675-a4b3e5290db7",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "nit\r\n```\r\n- import org.apache.spark.sql.test.{SharedSparkSession, SQLTestUtils}\r\n+ import org.apache.spark.sql.test.SharedSparkSession\r\n```",
        "createdAt" : "2019-08-15T15:24:42Z",
        "updatedAt" : "2019-08-19T08:00:35Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "76f199bce0780886f925d3963711044fc09a6b53",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +35,39 @@import org.apache.spark.sql.internal.SQLConf.SOURCES_BINARY_FILE_MAX_LENGTH\nimport org.apache.spark.sql.sources._\nimport org.apache.spark.sql.test.{SharedSparkSession, SQLTestUtils}\nimport org.apache.spark.sql.types._\nimport org.apache.spark.util.Utils"
  },
  {
    "id" : "cc434f50-87ee-4810-949f-836c1474d728",
    "prId" : 24855,
    "prUrl" : "https://github.com/apache/spark/pull/24855#pullrequestreview-248879255",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d4317394-9d03-4a3e-bbcc-5362efa5daac",
        "parentId" : null,
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Is it ok that we only have the space in the file name, or do we need it in the path were providing to trigger SPARK-28030?",
        "createdAt" : "2019-06-12T16:21:21Z",
        "updatedAt" : "2019-06-12T16:27:02Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "cae0d310-82b4-4b61-a590-7a83691385ac",
        "parentId" : "d4317394-9d03-4a3e-bbcc-5362efa5daac",
        "authorId" : "0c812942-02cb-4975-9748-394d1387affa",
        "body" : "this test will fail without the patch",
        "createdAt" : "2019-06-12T16:29:28Z",
        "updatedAt" : "2019-06-12T16:29:28Z",
        "lastEditedBy" : "0c812942-02cb-4975-9748-394d1387affa",
        "tags" : [
        ]
      }
    ],
    "commit" : "2e2b43f2fa1a0e62f92cc6b9c5d43fc5f4f450e5",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +371,375 @@\n  test(\"SPARK-28030: support chars in file names that require URL encoding\") {\n    withTempDir { dir =>\n      val file = new File(dir, \"test space.txt\")\n      val content = \"123\".getBytes"
  },
  {
    "id" : "6be96624-e77b-4ee9-9fa5-715a369cca5f",
    "prId" : 24855,
    "prUrl" : "https://github.com/apache/spark/pull/24855#pullrequestreview-248880606",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9655afcc-6673-4b68-9101-2244e453255c",
        "parentId" : null,
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "The change seems to impact not just the binary file format, maybe this belongs in one of our root datasource tests. What do you think?",
        "createdAt" : "2019-06-12T16:23:05Z",
        "updatedAt" : "2019-06-12T16:27:02Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "a81a08ca-97fe-47cf-a96b-74f8e64190fa",
        "parentId" : "9655afcc-6673-4b68-9101-2244e453255c",
        "authorId" : "0c812942-02cb-4975-9748-394d1387affa",
        "body" : "it still relies on each data source implementation to recognize `filePath` is actually an URI. I don't see how to test it in our root datasource test. Btw, this PR adds a regression test. So I want to keep the scope minimal.",
        "createdAt" : "2019-06-12T16:32:05Z",
        "updatedAt" : "2019-06-12T16:32:06Z",
        "lastEditedBy" : "0c812942-02cb-4975-9748-394d1387affa",
        "tags" : [
        ]
      }
    ],
    "commit" : "2e2b43f2fa1a0e62f92cc6b9c5d43fc5f4f450e5",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +370,374 @@  }\n\n  test(\"SPARK-28030: support chars in file names that require URL encoding\") {\n    withTempDir { dir =>\n      val file = new File(dir, \"test space.txt\")"
  },
  {
    "id" : "e244142b-d828-45a3-8c4c-bb54e79ed4f3",
    "prId" : 24483,
    "prUrl" : "https://github.com/apache/spark/pull/24483#pullrequestreview-231881307",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3b627e0d-11a5-4d60-bdf4-63cfba093287",
        "parentId" : null,
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "Seems the test can still pass without this line. Maybe we can remove it?",
        "createdAt" : "2019-04-29T20:02:56Z",
        "updatedAt" : "2019-04-29T20:16:28Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "511670ed-ed79-4643-8a83-3beb8e55d6e6",
        "parentId" : "3b627e0d-11a5-4d60-bdf4-63cfba093287",
        "authorId" : "0c812942-02cb-4975-9748-394d1387affa",
        "body" : "If we still set the max to `content.length`, the test will fail. This is to ensure we don't even attempt to read the file if the file is too big.",
        "createdAt" : "2019-04-29T20:19:14Z",
        "updatedAt" : "2019-04-29T20:19:14Z",
        "lastEditedBy" : "0c812942-02cb-4975-9748-394d1387affa",
        "tags" : [
        ]
      }
    ],
    "commit" : "0d6f92c4160c2b92d547e68f48280f440a65748f",
    "line" : 36,
    "diffHunk" : "@@ -1,1 +360,364 @@      }\n      // Disable read. If the implementation attempts to read, the exception would be different.\n      file.setReadable(false)\n      val caught = intercept[SparkException] {\n        withSQLConf(SOURCES_BINARY_FILE_MAX_LENGTH.key -> (content.length - 1).toString) {"
  },
  {
    "id" : "ab125b07-842f-48bc-be65-e7a013ca9eb6",
    "prId" : 24473,
    "prUrl" : "https://github.com/apache/spark/pull/24473#pullrequestreview-231451329",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "75251e67-edfe-4806-90b8-5469b6ec4baf",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Looks like both positive and negative cases are within one test. Can we split them?",
        "createdAt" : "2019-04-28T01:57:12Z",
        "updatedAt" : "2019-04-28T05:40:24Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "cd075a09-5de2-4114-a0b4-504304632c43",
        "parentId" : "75251e67-edfe-4806-90b8-5469b6ec4baf",
        "authorId" : "0c812942-02cb-4975-9748-394d1387affa",
        "body" : "Is it necessary? test is grouped by function/feature already.",
        "createdAt" : "2019-04-28T05:31:51Z",
        "updatedAt" : "2019-04-28T05:40:24Z",
        "lastEditedBy" : "0c812942-02cb-4975-9748-394d1387affa",
        "tags" : [
        ]
      },
      {
        "id" : "7151a295-3584-44bd-9167-fed04ded4629",
        "parentId" : "75251e67-edfe-4806-90b8-5469b6ec4baf",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Not neccesary but why don't we make the test case simple and separate :)",
        "createdAt" : "2019-04-28T05:44:32Z",
        "updatedAt" : "2019-04-28T05:45:28Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "8e5c95c6-e465-4d12-9f56-995cd461ce9c",
        "parentId" : "75251e67-edfe-4806-90b8-5469b6ec4baf",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "We can still retain the group too, for instance, `column pruning - positive` and `column pruning - negative`. Not a big deal but I don't think it's difficult or too demanding to fix.",
        "createdAt" : "2019-04-28T06:42:53Z",
        "updatedAt" : "2019-04-28T06:42:53Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "f4a64698805c768e37f081eefe2a5c8f06b8e4b0",
    "line" : 89,
    "diffHunk" : "@@ -1,1 +324,328 @@    }\n    file.setReadable(false)\n    withClue(\"cannot read content\") {\n      intercept[IOException] {\n        read(file, getRequiredSchema(CONTENT))"
  },
  {
    "id" : "be95222a-7448-4e1a-b29c-18c5f6300663",
    "prId" : 24387,
    "prUrl" : "https://github.com/apache/spark/pull/24387#pullrequestreview-228092634",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "904fd791-32a8-4ba9-9bf7-e2a003e9f32b",
        "parentId" : null,
        "authorId" : "0c812942-02cb-4975-9748-394d1387affa",
        "body" : "If we only unit test `buildReader`, we don't need to share those objects across tests.",
        "createdAt" : "2019-04-18T04:14:47Z",
        "updatedAt" : "2019-04-20T18:58:15Z",
        "lastEditedBy" : "0c812942-02cb-4975-9748-394d1387affa",
        "tags" : [
        ]
      }
    ],
    "commit" : "71db855468691b18f9365f70ca376225fc046989",
    "line" : 25,
    "diffHunk" : "@@ -1,1 +45,49 @@  private var fs: FileSystem = _\n\n  private var file1Status: FileStatus = _\n\n  override def beforeAll(): Unit = {"
  }
]