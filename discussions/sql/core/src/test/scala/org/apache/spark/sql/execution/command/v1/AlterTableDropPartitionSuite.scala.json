[
  {
    "id" : "ef4158ba-9c74-4d35-9061-0ac0b3557d9d",
    "prId" : 31131,
    "prUrl" : "https://github.com/apache/spark/pull/31131#pullrequestreview-565662654",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5a52b723-04c8-4b57-b1c7-537c3a382843",
        "parentId" : null,
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "hmm I'm not sure if this check is useful - seems it is unrelated to caching (does `getTableSize` look at cached table for stats?)",
        "createdAt" : "2021-01-11T18:37:07Z",
        "updatedAt" : "2021-01-12T17:21:06Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "377e43cc-c3d6-4937-87bf-712821a86c8c",
        "parentId" : "5a52b723-04c8-4b57-b1c7-537c3a382843",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "> does getTableSize look at cached table for stats\r\n\r\nNo, it doesn't but updating stats uncached table before the PR https://github.com/apache/spark/pull/31112\r\n\r\nActually, I moved the test from Hive specific test suite to the base test suite for v1 catalogs. Here, I made the test portable because table data has different sizes, so, instead of comparing exact numbers, I replaced that by this check (which is independent from table size).",
        "createdAt" : "2021-01-11T19:16:37Z",
        "updatedAt" : "2021-01-12T17:21:06Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "4153ba2b254756fb7d865e8746d48cc2defd1e91",
    "line" : 32,
    "diffHunk" : "@@ -1,1 +62,66 @@        assert(spark.catalog.isCached(t))\n        val onePartSize = getTableSize(t)\n        assert(0 < onePartSize && onePartSize < twoPartSize)\n        checkAnswer(sql(s\"SELECT * FROM $t\"), Seq(Row(1, 1)))\n      }"
  },
  {
    "id" : "70da9409-4453-40b8-9c8c-9ad733b6134e",
    "prId" : 30983,
    "prUrl" : "https://github.com/apache/spark/pull/30983#pullrequestreview-560537536",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1a65fe9c-7a68-441d-83ee-d504e2c38c98",
        "parentId" : null,
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : " nit: it will be good to verify that the table is cached after the file index issue is solved :)",
        "createdAt" : "2020-12-31T17:08:41Z",
        "updatedAt" : "2021-01-03T08:33:20Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "8cd3b198-8735-46f2-9da2-5c61f8091403",
        "parentId" : "1a65fe9c-7a68-441d-83ee-d504e2c38c98",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "I expected your suggestion ;-). Actually, when I added the check from previous PR:\r\n```scala\r\nassert(spark.sharedState.cacheManager.lookupCachedData(spark.table(\"t\")).isDefined)\r\n```\r\nit failed for v1 Hive external catalog. It even fails immediately after `sql(\"CACHE TABLE t\")`.\r\n\r\nSince this is an unified test for both In-Memory and Hive catalogs, I decided to skip the check because this fix is mostly about correctness, and `checkAnswer(sql(\"SELECT * FROM t\"), Seq(Row(1, 1)))` checks the correct result.",
        "createdAt" : "2020-12-31T18:30:55Z",
        "updatedAt" : "2021-01-03T08:33:20Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "638925ca-bb7e-4606-ae6f-85905706680e",
        "parentId" : "1a65fe9c-7a68-441d-83ee-d504e2c38c98",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "As an alternative solution, I can move this test from the base test suite to specific suites for v1 In-Memory and Hive catalogs, and add the check for In-Memory catalog.",
        "createdAt" : "2020-12-31T18:34:43Z",
        "updatedAt" : "2021-01-03T08:33:21Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "7e3e1512-9bfe-4009-8e86-b15120f1aa79",
        "parentId" : "1a65fe9c-7a68-441d-83ee-d504e2c38c98",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "For some reasons `lookupCachedData` cannot find the cached hive table by the dataset `spark.table(\"t\")`. I just checked the entire cache - is it empty or not.",
        "createdAt" : "2020-12-31T19:20:35Z",
        "updatedAt" : "2021-01-03T08:33:21Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "021c4ac89b4bfde874bdbaf814e3d7791cb88bc9",
    "line" : 25,
    "diffHunk" : "@@ -1,1 +55,59 @@      sql(\"ALTER TABLE t DROP PARTITION (part=0)\")\n      assert(spark.catalog.isCached(\"t\"))\n      checkAnswer(sql(\"SELECT * FROM t\"), Seq(Row(1, 1)))\n    }\n  }"
  },
  {
    "id" : "6a5c7477-c432-4c69-bf51-b989f3b84565",
    "prId" : 30979,
    "prUrl" : "https://github.com/apache/spark/pull/30979#pullrequestreview-560206157",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "75842900-e632-4d49-9093-d2c1bb67051a",
        "parentId" : null,
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "can we check that the cache still exist after the command? this will validate that the cache is actually refreshed rather than removed.",
        "createdAt" : "2020-12-30T19:03:22Z",
        "updatedAt" : "2020-12-30T20:43:26Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "b125a661-a696-4fe1-98b2-15d693d8ab9e",
        "parentId" : "75842900-e632-4d49-9093-d2c1bb67051a",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "I still use `df.isEmpty` instead of checking the content of the dataframe because `sparkSession.catalog.refreshTable` is not enough, it seems. It doesn't refresh the file indexes for the table.\r\n\r\nSo, test fails:\r\n```\r\n  test(\"SPARK-33941: refresh cache after partition dropping\") {\r\n    withNamespaceAndTable(\"ns\", \"tbl\") { t =>\r\n      sql(s\"CREATE TABLE $t (id int, part int) $defaultUsing PARTITIONED BY (part)\")\r\n      sql(s\"INSERT INTO $t PARTITION (part=0) SELECT 0\")\r\n      sql(s\"INSERT INTO $t PARTITION (part=1) SELECT 1\")\r\n      val df = spark.table(t)\r\n      df.cache()\r\n      df.collect()\r\n      sql(s\"ALTER TABLE $t DROP PARTITION (part=0)\")\r\n      df.collect()\r\n    }\r\n  }\r\n```\r\nwith the exception:\r\n```\r\njava.io.FileNotFoundException: File file:/Users/maximgekk/proj/drop-partition-invalidate-cache/spark-warehouse/org.apache.spark.sql.execution.command.v1.AlterTableDropPartitionSuite/ns.db/tbl/part=0/part-00000-4536ff40-a1ac-41f4-9ff5-7fbc7d67c7b9.c000.snappy.parquet does not exist\r\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:169)\r\n```\r\n\r\nAnd `REFRESH TABLE` after `ALTER TABLE .. DROP PARTITION` doesn't help since it doesn't refresh the file index too :-(",
        "createdAt" : "2020-12-30T20:57:35Z",
        "updatedAt" : "2020-12-30T20:57:36Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "5d403e23-60bb-4ad7-bcb9-b4919bc42e4c",
        "parentId" : "75842900-e632-4d49-9093-d2c1bb67051a",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "hmm, even simpler test without caching fails:\r\n```scala\r\n  test(\"SPARK-33941: refresh cache after partition dropping\") {\r\n    withNamespaceAndTable(\"ns\", \"tbl\") { t =>\r\n      sql(s\"CREATE TABLE $t (id int, part int) $defaultUsing PARTITIONED BY (part)\")\r\n      sql(s\"INSERT INTO $t PARTITION (part=0) SELECT 0\")\r\n      sql(s\"INSERT INTO $t PARTITION (part=1) SELECT 1\")\r\n      val df = spark.table(t)\r\n      // df.cache()\r\n      df.collect()\r\n      sql(s\"ALTER TABLE $t DROP PARTITION (part=0)\")\r\n      sql(s\"ALTER TABLE $t RECOVER PARTITIONS\")\r\n      sql(s\"MSCK REPAIR TABLE $t\")\r\n      df.collect()\r\n    }\r\n  }\r\n```\r\nwith:\r\n```java\r\nInput path does not exist: file:/private/var/folders/p3/dfs6mf655d7fnjrsjvldh0tc0000gn/T/warehouse-a3f5be00-8e30-43be-a14d-a17983f2cd2d/ns.db/tbl/part=0\r\norg.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/private/var/folders/p3/dfs6mf655d7fnjrsjvldh0tc0000gn/T/warehouse-a3f5be00-8e30-43be-a14d-a17983f2cd2d/ns.db/tbl/part=0\r\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:297)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:239)\r\n```\r\non v1 In-Memory and Hive external catalogs. The issue looks much more serious than I could expect. cc @cloud-fan @HyukjinKwon ",
        "createdAt" : "2020-12-30T21:09:55Z",
        "updatedAt" : "2020-12-30T21:09:55Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "4465a638-4277-4018-8a71-8e04c43251ba",
        "parentId" : "75842900-e632-4d49-9093-d2c1bb67051a",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Hmm interesting... I thought this should have been addressed via #30699 but maybe it's a different one. I'll check this when I have time too.",
        "createdAt" : "2020-12-30T21:29:04Z",
        "updatedAt" : "2020-12-30T21:29:04Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      }
    ],
    "commit" : "d951371958f1cedda624406705f12223986849aa",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +53,57 @@      sql(s\"ALTER TABLE $t DROP PARTITION (part=0)\")\n      assert(spark.sharedState.cacheManager.lookupCachedData(df).isDefined)\n      assert(df.isEmpty)\n    }\n  }"
  }
]