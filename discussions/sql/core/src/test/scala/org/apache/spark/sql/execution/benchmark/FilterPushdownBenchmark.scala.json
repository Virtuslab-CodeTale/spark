[
  {
    "id" : "34d9cdce-db6c-4354-8eaa-7a3731a16637",
    "prId" : 24783,
    "prUrl" : "https://github.com/apache/spark/pull/24783#pullrequestreview-245427294",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "21dc22de-618a-4f58-a69a-a0f4513bca44",
        "parentId" : null,
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "I tried with 5000 filters, and the execution becomes very slow. For end-to-end tests, we need to have a smaller size here, comparing to the benchmark `Convert filters to ORC filter`",
        "createdAt" : "2019-06-04T12:58:35Z",
        "updatedAt" : "2019-06-04T12:58:36Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "66d012bccfcb6ac70157519b19ded4038ee1195b",
    "line" : 114,
    "diffHunk" : "@@ -1,1 +451,455 @@          val benchmark =\n            new Benchmark(\"Select data with filters\", numRows, minNumIters = 5, output = output)\n          Seq(100, 500, 1000).foreach { numFilter =>\n            val whereColumn = (1 to numFilter)\n              .map(i => col(\"c1\") === lit(i))"
  }
]