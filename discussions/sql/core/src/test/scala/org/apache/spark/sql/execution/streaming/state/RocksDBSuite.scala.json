[
  {
    "id" : "abf33b71-cdba-4f1e-ab83-1dda807ede4a",
    "prId" : 32933,
    "prUrl" : "https://github.com/apache/spark/pull/32933#pullrequestreview-696205636",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0f5a3317-731c-4f52-9fa3-edd33a8a3165",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "This is interesting - we don't remove any keys but expect some SST files to be invalid. Would compaction chime in and compact several SSTs into bigger one?",
        "createdAt" : "2021-06-28T10:02:39Z",
        "updatedAt" : "2021-06-28T12:04:27Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "56bf5813-6bfc-4ba5-908d-cf4b50b3cf77",
        "parentId" : "0f5a3317-731c-4f52-9fa3-edd33a8a3165",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "Right. We'll do the RocksDB checkpoint for each commit operation, each checkpoint is a full snapshot and includes all data. In this UT we have 50 versions but only retain 10 versions, so the SST files for deleted versions(1 to 40) will be deleted.",
        "createdAt" : "2021-06-30T14:03:08Z",
        "updatedAt" : "2021-06-30T14:03:08Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "ea949836c26be0124e37df0eb641be806628d94d",
    "line" : 37,
    "diffHunk" : "@@ -1,1 +127,131 @@      db.cleanup()\n      assert(versionsPresent === (41L to 50L))\n      assert(listFiles(sstDir).length < numSstFiles)\n\n      // Verify data in retained vesions."
  },
  {
    "id" : "4ab999ee-3a61-4629-8337-877ce9819026",
    "prId" : 32933,
    "prUrl" : "https://github.com/apache/spark/pull/32933#pullrequestreview-696206469",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c5c06da5-74ee-472e-a860-3ef53eabac18",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "nit: double empty lines",
        "createdAt" : "2021-06-28T10:07:30Z",
        "updatedAt" : "2021-06-28T12:04:27Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "9bc7047d-c6c1-47e5-bff9-28d66706eff5",
        "parentId" : "c5c06da5-74ee-472e-a860-3ef53eabac18",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "Thanks, done in the next commit.",
        "createdAt" : "2021-06-30T14:03:44Z",
        "updatedAt" : "2021-06-30T14:03:45Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "ea949836c26be0124e37df0eb641be806628d94d",
    "line" : 220,
    "diffHunk" : "@@ -1,1 +498,502 @@  def listFiles(file: String): Seq[File] = listFiles(new File(file))\n}\n\nobject RocksDBSuite {\n  @volatile var singleton: RocksDB = _"
  },
  {
    "id" : "5f75cc1d-d98f-46de-bb5d-badcb51a6a87",
    "prId" : 32933,
    "prUrl" : "https://github.com/apache/spark/pull/32933#pullrequestreview-697848643",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b345c23d-610c-4d2a-b880-371b2e0e20c1",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "This test seems not related to clean up change here? Looks like more related to RocksDB instance PR.",
        "createdAt" : "2021-07-01T20:14:19Z",
        "updatedAt" : "2021-07-01T20:14:19Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "f18c918b-6941-4fe4-a668-67b6cc16c59e",
        "parentId" : "b345c23d-610c-4d2a-b880-371b2e0e20c1",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "Ah yea, this is the test for rollback.\r\nActually the original plan is expose `rollback` and `cleanup` in this PR. It should be a mistake for the last PR, I introduced the `rollback` without tests.",
        "createdAt" : "2021-07-02T05:57:43Z",
        "updatedAt" : "2021-07-02T05:57:43Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      },
      {
        "id" : "c0479589-84a3-4dd4-a0cb-0460ccfe7c34",
        "parentId" : "b345c23d-610c-4d2a-b880-371b2e0e20c1",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "okay",
        "createdAt" : "2021-07-02T06:02:13Z",
        "updatedAt" : "2021-07-02T06:02:13Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "ea949836c26be0124e37df0eb641be806628d94d",
    "line" : 86,
    "diffHunk" : "@@ -1,1 +274,278 @@  }\n\n  test(\"disallow concurrent updates to the same RocksDB instance\") {\n    quietly {\n      withDB("
  },
  {
    "id" : "09106d6b-5b17-4545-85d7-b643e02bd850",
    "prId" : 32933,
    "prUrl" : "https://github.com/apache/spark/pull/32933#pullrequestreview-697911568",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "13953faf-df00-4041-85cc-d2a3973adfeb",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Hm, what this test is used for? Each `RocksDB` in each thread uses the same remote root dir, won't they conflict?",
        "createdAt" : "2021-07-01T20:21:30Z",
        "updatedAt" : "2021-07-01T20:21:30Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "4fc990af-dcdf-42f5-b4b3-13415aa5f949",
        "parentId" : "13953faf-df00-4041-85cc-d2a3973adfeb",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "This is to simulate the multi-thread scenario of updating and cleaning old versions. It will not conflict since we call commit for each update thread and the version get updated for each commits.",
        "createdAt" : "2021-07-02T06:09:49Z",
        "updatedAt" : "2021-07-02T06:09:49Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      },
      {
        "id" : "dede41a6-9471-4042-9f2f-3fe91c98b0aa",
        "parentId" : "13953faf-df00-4041-85cc-d2a3973adfeb",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Hmm, will it happens? I think `RocksDB` is not thread-safe, and each state task only has one `RocksDB` instance. They should update and clean old versions individually as they are for different state store. ",
        "createdAt" : "2021-07-02T06:15:43Z",
        "updatedAt" : "2021-07-02T06:15:43Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "f3886748-b93f-4ff4-9d49-92637bed892a",
        "parentId" : "13953faf-df00-4041-85cc-d2a3973adfeb",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "This looks to be more likely simulating the case multiple streaming queries with same checkpoint run concurrently. \r\n\r\nSST files shouldn't conflict as we make the file name be unique, and for metadata files we use `overwriteIfPossible = true`, so won't throw error if the file already exists.",
        "createdAt" : "2021-07-02T06:54:07Z",
        "updatedAt" : "2021-07-02T06:54:08Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "49f4051e-daf9-4bc5-9a39-da2927f820e3",
        "parentId" : "13953faf-df00-4041-85cc-d2a3973adfeb",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "Yea, I think the purpose of this test is to make sure no error thrown and the result is correct in the end.\r\nAfter taking a further look, there's a small issue is that `exception` never used. I'll confirm it separately.",
        "createdAt" : "2021-07-02T07:40:19Z",
        "updatedAt" : "2021-07-02T07:40:19Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      },
      {
        "id" : "754a64ef-ca4c-47bc-8557-46621f2e14bb",
        "parentId" : "13953faf-df00-4041-85cc-d2a3973adfeb",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "@xuanyuanking and me discussed this test offline. Seems there is something wrong with `exception` usage. It doesn't look completely correct. @xuanyuanking will address it by fixing it or deleting the test later in a follow-up.",
        "createdAt" : "2021-07-02T07:42:13Z",
        "updatedAt" : "2021-07-02T07:42:13Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "ea949836c26be0124e37df0eb641be806628d94d",
    "line" : 161,
    "diffHunk" : "@@ -1,1 +349,353 @@            try {\n              for (version <- 0 to numUpdatesInEachThread) {\n                withDB(\n                  remoteDir,\n                  version = version) { db =>"
  },
  {
    "id" : "b94db314-53ef-4d78-9c1c-8743e1052c19",
    "prId" : 32928,
    "prUrl" : "https://github.com/apache/spark/pull/32928#pullrequestreview-693747977",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6fb99c7b-7936-4409-8be7-64584aff36f2",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Here the `conf` param is always provided - is there usage on using default value?",
        "createdAt" : "2021-06-28T08:03:35Z",
        "updatedAt" : "2021-06-28T08:17:12Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "d6897826-d109-4264-8527-196e8547e667",
        "parentId" : "6fb99c7b-7936-4409-8be7-64584aff36f2",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "No usage for the default value, delete the default value in the next commit.",
        "createdAt" : "2021-06-28T09:31:00Z",
        "updatedAt" : "2021-06-28T09:31:00Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "21e1728cddab0fcd39886d914190a899aaac6d59",
    "line" : 83,
    "diffHunk" : "@@ -1,1 +240,244 @@      remoteDir: String,\n      version: Int = 0,\n      conf: RocksDBConf = RocksDBConf().copy(compactOnCommit = false, minVersionsToRetain = 100),\n      hadoopConf: Configuration = new Configuration())(\n      func: RocksDB => T): T = {"
  },
  {
    "id" : "b759e4f7-238a-4c59-bbc0-da602aa2102b",
    "prId" : 32767,
    "prUrl" : "https://github.com/apache/spark/pull/32767#pullrequestreview-675147952",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1d9727a8-c3d3-4d27-83dd-24423223e6e0",
        "parentId" : null,
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "Per https://github.com/apache/spark/pull/32582#discussion_r642036286",
        "createdAt" : "2021-06-03T10:57:12Z",
        "updatedAt" : "2021-06-03T10:57:12Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "7279d434ddba27a93924577458f8dedf6bf340c5",
    "line" : 42,
    "diffHunk" : "@@ -1,1 +77,81 @@        \"archive/00007.log\" -> 101\n      ))\n      loadAndVerifyCheckpointFiles(fileManager, verificationDir, version = 1, cpFiles1, 101)\n\n      // Save SAME version again with different checkpoint files and load back again to verify"
  },
  {
    "id" : "a84e564a-2847-4b44-b1cb-1ff2cc1d87ed",
    "prId" : 32582,
    "prUrl" : "https://github.com/apache/spark/pull/32582#pullrequestreview-674054599",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a5f7ae16-f789-4b60-82aa-f057e3657713",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "nit: empty two lines",
        "createdAt" : "2021-05-30T07:18:03Z",
        "updatedAt" : "2021-05-30T07:54:27Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "bb96bd3f-e598-41d2-a76b-081999749beb",
        "parentId" : "a5f7ae16-f789-4b60-82aa-f057e3657713",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "Thanks, done in 3800c51",
        "createdAt" : "2021-06-02T11:04:55Z",
        "updatedAt" : "2021-06-02T11:04:55Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "c05b140f73f855a148ccedaf753841fb94a4ab1d",
    "line" : 92,
    "diffHunk" : "@@ -1,1 +130,134 @@    // scalastyle:on line.size.limit\n  }\n\n  def generateFiles(dir: String, fileToLengths: Seq[(String, Int)]): Unit = {\n    fileToLengths.foreach { case (fileName, length) =>"
  },
  {
    "id" : "4c625f1a-51ec-4a54-87fc-abc2a725be79",
    "prId" : 32582,
    "prUrl" : "https://github.com/apache/spark/pull/32582#pullrequestreview-674051843",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "64f66c71-2370-4900-86c3-cef50a723822",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "With this, `def listFiles(file: String)` is probably unnecessary, as I expect implicit conversion happening when calling `listFiles(file: File)` with String.",
        "createdAt" : "2021-05-30T07:36:40Z",
        "updatedAt" : "2021-05-30T07:54:27Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "338a1e74-fc34-444c-ad90-2e8ff754a3a6",
        "parentId" : "64f66c71-2370-4900-86c3-cef50a723822",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "That's right in this PR. In the further PRs, we will have more caller for the `listFiles(file: String)`. Maybe let's keep it for now.",
        "createdAt" : "2021-06-02T11:01:42Z",
        "updatedAt" : "2021-06-02T11:01:42Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "c05b140f73f855a148ccedaf753841fb94a4ab1d",
    "line" : 110,
    "diffHunk" : "@@ -1,1 +148,152 @@  }\n\n  implicit def toFile(path: String): File = new File(path)\n\n  def listFiles(file: File): Seq[File] = {"
  },
  {
    "id" : "88c9fc9a-4677-4d5c-947b-08519cc37d79",
    "prId" : 32582,
    "prUrl" : "https://github.com/apache/spark/pull/32582#pullrequestreview-678273540",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "76f0ce5f-72c8-4699-b5ef-e091f08efcb0",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Isn't `checkpointDir` already be defined in `RocksDBFileManager`?",
        "createdAt" : "2021-06-04T06:33:20Z",
        "updatedAt" : "2021-06-04T06:33:20Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "842d6f3b-8ad5-4fe5-829c-31bf2f9026c0",
        "parentId" : "76f0ce5f-72c8-4699-b5ef-e091f08efcb0",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "The `checkpointDir` here is a temp dir for a specific version of checkpointing. This checkpoint here corresponds to a RocksDB checkpoint. The method, `saveCheckpointToDfs` will be called in the `commit` logic for the RocksDBStateStoreProvider. The commit follows these steps:\r\n```\r\n   * - Write all the updates to the native RocksDB\r\n   * - Flush all changes to disk\r\n   * - Create a RocksDB checkpoint in a new local dir [Done in this method]\r\n   * - Sync the checkpoint dir files to DFS [Done in this method]\r\n```",
        "createdAt" : "2021-06-04T15:26:38Z",
        "updatedAt" : "2021-06-04T15:26:38Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      },
      {
        "id" : "eaf99462-3581-4087-861a-7819e604c51c",
        "parentId" : "76f0ce5f-72c8-4699-b5ef-e091f08efcb0",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Is it `localTempDir`?\r\n\r\n```scala\r\nclass RocksDBFileManager(\r\n    dfsRootDir: String,\r\n    localTempDir: File,\r\n    hadoopConf: Configuration,\r\n    loggingId: String = \"\")\r\n```",
        "createdAt" : "2021-06-04T16:01:04Z",
        "updatedAt" : "2021-06-04T16:01:05Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "c67cf033-ecb9-4a2e-921e-daaa837f55d9",
        "parentId" : "76f0ce5f-72c8-4699-b5ef-e091f08efcb0",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "Sorry just found I missed this comment.\r\nNot the same one. The `localTempDir` is controlled by RocksDBFileManager. The lifetime of temp dir for `checkpointDir` is controlled by `RocksDBStateStoreProvider `.",
        "createdAt" : "2021-06-08T09:00:50Z",
        "updatedAt" : "2021-06-08T09:00:51Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "c05b140f73f855a148ccedaf753841fb94a4ab1d",
    "line" : 105,
    "diffHunk" : "@@ -1,1 +143,147 @@      version: Int,\n      numKeys: Int): Unit = {\n    val checkpointDir = Utils.createTempDir().getAbsolutePath // local dir to create checkpoints\n    generateFiles(checkpointDir, fileToLengths)\n    fileManager.saveCheckpointToDfs(checkpointDir, version, numKeys)"
  },
  {
    "id" : "d203a2df-1e2c-49aa-8a1d-5acb6baa80a5",
    "prId" : 32582,
    "prUrl" : "https://github.com/apache/spark/pull/32582#pullrequestreview-676422212",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c535348e-afb9-4c03-b3e0-026916fd8405",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "`version = 1`? In practice, will we save checkpoint files for same version? I.e. for same micro-batch?",
        "createdAt" : "2021-06-04T06:36:55Z",
        "updatedAt" : "2021-06-04T06:36:55Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "54a7b7da-9a62-4324-bd8f-e05a51f68c22",
        "parentId" : "c535348e-afb9-4c03-b3e0-026916fd8405",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "For task failover, it's possible that the tasks failed during uploading. The new task uses the same version and may schedule to the same executor but with different checkpoint dir.",
        "createdAt" : "2021-06-04T15:34:37Z",
        "updatedAt" : "2021-06-04T15:34:37Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "c05b140f73f855a148ccedaf753841fb94a4ab1d",
    "line" : 52,
    "diffHunk" : "@@ -1,1 +69,73 @@        \"archive/00003.log\" -> 3000 // new log file\n      )\n      saveCheckpointFiles(fileManager, cpFiles1_, version = 1, numKeys = 1001)\n      assert(numRemoteSSTFiles === 4, \"shouldn't copy same files again\") // 2 old + 2 new SST files\n      assert(numRemoteLogFiles === 4, \"shouldn't copy same files again\") // 2 old + 2 new log files"
  },
  {
    "id" : "c8506f33-1f13-49dc-bbce-8d8cc689326c",
    "prId" : 32582,
    "prUrl" : "https://github.com/apache/spark/pull/32582#pullrequestreview-676423789",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6901fd1b-96b0-41a9-b02e-4ad46ff625a9",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Use `withTempDir` too?",
        "createdAt" : "2021-06-04T06:37:46Z",
        "updatedAt" : "2021-06-04T06:37:46Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "d7423a0b-d9dc-4fae-b913-fc293e423467",
        "parentId" : "6901fd1b-96b0-41a9-b02e-4ad46ff625a9",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "This test will be further extended and more temp dir will be added. So maybe we just have the root path using `withTempDir`? See the demo here: https://github.com/apache/spark/pull/32767/files#diff-dc6f9dfe11e76f890ff2986f866853bcac263027c82562f9a52f4672a5460826R37",
        "createdAt" : "2021-06-04T15:35:47Z",
        "updatedAt" : "2021-06-04T15:35:47Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "c05b140f73f855a148ccedaf753841fb94a4ab1d",
    "line" : 74,
    "diffHunk" : "@@ -1,1 +91,95 @@        SQLConf.STREAMING_CHECKPOINT_FILE_MANAGER_CLASS.parent.key,\n        classOf[CreateAtomicTestManager].getName)\n      val dfsRootDir = Utils.createTempDir().getAbsolutePath\n      val fileManager = new RocksDBFileManager(dfsRootDir, Utils.createTempDir(), hadoopConf)\n      val cpFiles = Seq(\"sst-file1.sst\" -> 10, \"sst-file2.sst\" -> 20, \"other-file1\" -> 100)"
  },
  {
    "id" : "9612f2ab-0352-42f2-ba99-37769d020450",
    "prId" : 32582,
    "prUrl" : "https://github.com/apache/spark/pull/32582#pullrequestreview-676442974",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4e42defb-d8f1-4f20-8895-f4de3f7a13ea",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "I'm not sure why this will fail?",
        "createdAt" : "2021-06-04T06:40:51Z",
        "updatedAt" : "2021-06-04T06:40:51Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "f9753054-53ba-4c99-9001-47efc67b9e6f",
        "parentId" : "4e42defb-d8f1-4f20-8895-f4de3f7a13ea",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "This is failed on purpose. \r\n```\r\n[info]   java.io.IOException: Copy failed intentionally\r\n[info]   at org.apache.spark.sql.execution.streaming.CreateAtomicTestManager$$anon$3.close(CheckpointFileManagerSuite.scala:169)\r\n```\r\n\r\nThis test aims to test the behavior that the `cancel` should be called no matter any error happens during `close`. See the comment and code here: https://github.com/apache/spark/pull/32582/files#diff-e3d3914d0398d61fdd299b1f8d3e869ec6a86e97606677c724969e421c9bf44eR222-R227",
        "createdAt" : "2021-06-04T15:55:04Z",
        "updatedAt" : "2021-06-04T15:55:05Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "c05b140f73f855a148ccedaf753841fb94a4ab1d",
    "line" : 80,
    "diffHunk" : "@@ -1,1 +97,101 @@      intercept[IOException] {\n        saveCheckpointFiles(fileManager, cpFiles, version = 1, numKeys = 101)\n      }\n      assert(CreateAtomicTestManager.cancelCalledInCreateAtomic)\n    }"
  }
]