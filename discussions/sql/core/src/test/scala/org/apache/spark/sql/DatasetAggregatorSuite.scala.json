[
  {
    "id" : "95dfe082-0db3-4db4-84d0-dc15aeafdd38",
    "prId" : 27499,
    "prUrl" : "https://github.com/apache/spark/pull/27499#pullrequestreview-362487131",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "597dfb1c-045a-4188-9cd5-e5d109815bdd",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "This case I'm a bit unsure. `TypedColumn` is also a `Colum` and why can't we use it like a `Column`? It's typed but it also has a corresponding catalyst schema and we should be able to use it in untyped operations.",
        "createdAt" : "2020-02-11T11:26:25Z",
        "updatedAt" : "2020-02-26T16:57:33Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "88843916-6707-407c-b24c-1b6f3c0950be",
        "parentId" : "597dfb1c-045a-4188-9cd5-e5d109815bdd",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "I think we can do it like @HyukjinKwon suggested https://github.com/apache/spark/pull/27499#discussion_r377444876, the result is we will lose typed info from typed columns.\r\n\r\n",
        "createdAt" : "2020-02-11T16:34:23Z",
        "updatedAt" : "2020-02-26T16:57:33Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "f354aa26-1b13-402d-80c8-336ca5e9abf7",
        "parentId" : "597dfb1c-045a-4188-9cd5-e5d109815bdd",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "This case, the users could be unaware of that they are using a untyped select with typed columns. The result might be surprised to the users as the typed info are lost.\r\n\r\n",
        "createdAt" : "2020-02-11T21:29:51Z",
        "updatedAt" : "2020-02-26T16:57:33Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "e0dfde03-2b0c-4d17-ab35-ecd3adfdde6e",
        "parentId" : "597dfb1c-045a-4188-9cd5-e5d109815bdd",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "OK I see the point. If this is supported, then we can't add new typed `select` overloads as it breaks compatibility (the return type changes). What's the current behavior without your fix?",
        "createdAt" : "2020-02-12T11:31:49Z",
        "updatedAt" : "2020-02-26T16:57:33Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "9f4f426a-86fc-4b03-8352-115da231d1eb",
        "parentId" : "597dfb1c-045a-4188-9cd5-e5d109815bdd",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Current behaviour seems throwing an exception (as described in the JIRA).\r\n\r\n```\r\nscala> df.select(fooAgg(1),fooAgg(2),fooAgg(3),fooAgg(4),fooAgg(5),fooAgg(6)).show\r\n\r\norg.apache.spark.sql.AnalysisException: unresolved operator 'Aggregate [fooagg(FooAgg(1), None, None, None, input[0, int, false] AS value#114, assertnotnull(cast(value#114 as int)), input[0, int, false] AS value#113, IntegerType, IntegerType, false) AS foo_agg_1#116, fooagg(FooAgg(2), None, None, None, input[0, int, false] AS value#119, assertnotnull(cast(value#119 as int)), input[0, int, false] AS value#118, IntegerType, IntegerType, false) AS foo_agg_2#121, fooagg(FooAgg(3), None, None, None, input[0, int, false] AS value#124, assertnotnull(cast(value#124 as int)), input[0, int, false] AS value#123, IntegerType, IntegerType, false) AS foo_agg_3#126, fooagg(FooAgg(4), None, None, None, input[0, int, false] AS value#129, assertnotnull(cast(value#129 as int)), input[0, int, false] AS value#128, IntegerType, IntegerType, false) AS foo_agg_4#131, fooagg(FooAgg(5), None, None, None, input[0, int, false] AS value#134, assertnotnull(cast(value#134 as int)), input[0, int, false] AS value#133, IntegerType, IntegerType, false) AS foo_agg_5#136, fooagg(FooAgg(6), None, None, None, input[0, int, false] AS value#139, assertnotnull(cast(value#139 as int)), input[0, int, false] AS value#138, IntegerType, IntegerType, false) AS foo_agg_6#141];;\r\n'Aggregate [fooagg(FooAgg(1), None, None, None, input[0, int, false] AS value#114, assertnotnull(cast(value#114 as int)), input[0, int, false] AS value#113, IntegerType, IntegerType, false) AS foo_agg_1#116, fooagg(FooAgg(2), None, None, None, input[0, int, false] AS value#119, assertnotnull(cast(value#119 as int)), input[0, int, false] AS value#118, IntegerType, IntegerType, false) AS foo_agg_2#121, fooagg(FooAgg(3), None, None, None, input[0, int, false] AS value#124, assertnotnull(cast(value#124 as int)), input[0, int, false] AS value#123, IntegerType, IntegerType, false) AS foo_agg_3#126, fooagg(FooAgg(4), None, None, None, input[0, int, false] AS value#129, assertnotnull(cast(value#129 as int)), input[0, int, false] AS value#128, IntegerType, IntegerType, false) AS foo_agg_4#131, fooagg(FooAgg(5), None, None, None, input[0, int, false] AS value#134, assertnotnull(cast(value#134 as int)), input[0, int, false] AS value#133, IntegerType, IntegerType, false) AS foo_agg_5#136, fooagg(FooAgg(6), None, None, None, input[0, int, false] AS value#139, assertnotnull(cast(value#139 as int)), input[0, int, false] AS value#138, IntegerType, IntegerType, false) AS foo_agg_6#141]\r\n+- Project [_1#6 AS a#13, _2#7 AS b#14, _3#8 AS c#15, _4#9 AS d#16, _5#10 AS e#17, _6#11 AS F#18]\r\n +- LocalRelation [_1#6, _2#7, _3#8, _4#9, _5#10, _6#11]\r\n\r\nat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.failAnalysis(CheckAnalysis.scala:43)\r\n at org.apache.spark.sql.catalyst.analysis.Analyzer.failAnalysis(Analyzer.scala:95)\r\n at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$3.apply(CheckAnalysis.scala:431)\r\n at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$3.apply(CheckAnalysis.scala:430)\r\n at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\r\n at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:430)\r\n at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\r\n at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\r\n at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\r\n at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\r\n at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\r\n at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\r\n at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\r\n at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\r\n at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\r\n at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3412)\r\n at org.apache.spark.sql.Dataset.select(Dataset.scala:1340)\r\n```\r\n\r\nThe Resolution of aggregation expr requires to set the encoder, class, etc (via `TypedColumn.withInputType`); however, it's not set. See `SimpleTypedAggregateExpression.withInputInfo`.",
        "createdAt" : "2020-02-18T07:58:56Z",
        "updatedAt" : "2020-02-26T16:57:33Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "abf7f331-ded1-4a5a-99fb-355a370c4bf1",
        "parentId" : "597dfb1c-045a-4188-9cd5-e5d109815bdd",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Sorry for late. I'm on a vacation now so will reply late.\r\n\r\nYea, currently it silently chooses untyped `select` if the number of  typed columns is more than 5. An analysis exception will be thrown. It is very confusing for users.\r\n",
        "createdAt" : "2020-02-18T12:48:22Z",
        "updatedAt" : "2020-02-26T16:57:33Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "cb15d927-4804-4a0f-88d5-6143433e45e1",
        "parentId" : "597dfb1c-045a-4188-9cd5-e5d109815bdd",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "We should at least make the error message clear as this PR does. But exposing `selectUntyped` needs more discussion.",
        "createdAt" : "2020-02-19T08:27:44Z",
        "updatedAt" : "2020-02-26T16:57:33Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "9a279701-86a5-4ac4-8bff-dcf1dce88760",
        "parentId" : "597dfb1c-045a-4188-9cd5-e5d109815bdd",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Ok. It sounds reasonable to me. Where is better to raise a discussion for this, dev maillist or JIRA?",
        "createdAt" : "2020-02-20T01:57:14Z",
        "updatedAt" : "2020-02-26T16:57:33Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "8f24b705-bacd-4532-8b32-709e8ffe4096",
        "parentId" : "597dfb1c-045a-4188-9cd5-e5d109815bdd",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "We can open a JIRA first.",
        "createdAt" : "2020-02-20T07:36:57Z",
        "updatedAt" : "2020-02-26T16:57:33Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "0463a80b-a4b7-4851-b44f-5618311c483f",
        "parentId" : "597dfb1c-045a-4188-9cd5-e5d109815bdd",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Ok. Let me remove `selectUntyped` change for now.",
        "createdAt" : "2020-02-21T09:10:14Z",
        "updatedAt" : "2020-02-26T16:57:33Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "7d045fbfb1b6bf35c5cc49e9948db2c5140bb15d",
    "line" : 30,
    "diffHunk" : "@@ -1,1 +414,418 @@    // Passes typed columns to untyped `Dataset.select` API.\n    val err = intercept[AnalysisException] {\n      df.select(fooAgg(1), fooAgg(2), fooAgg(3), fooAgg(4), fooAgg(5), fooAgg(6))\n    }.getMessage\n    assert(err.contains(\"cannot be passed in untyped `select` API. \" +"
  },
  {
    "id" : "b9891526-6c34-4012-9944-4fa91a219464",
    "prId" : 27499,
    "prUrl" : "https://github.com/apache/spark/pull/27499#pullrequestreview-364442673",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "66bc2056-96af-46ab-9938-8ee45a80051c",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Not related to this PR, just a note:\r\n\r\nWe have 5 overloads of typed `select`, and typed `count` is supported in both typed and untyped `select`. That said, if we add a 6th overload of typed `select`, it can break queries that call the untyped `select` with 6 typed `count`s.\r\n\r\nI'm not sure what's the best way to move forward. Maybe we should add new methods `typedSelect` to disambiguate the untyped version.",
        "createdAt" : "2020-02-25T08:32:30Z",
        "updatedAt" : "2020-02-26T16:57:33Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "6b96349c-4bdd-4182-ba85-f67d938ca1e1",
        "parentId" : "66bc2056-96af-46ab-9938-8ee45a80051c",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Yea, to be clear, if we add a 6th overload of typed `select`, a call to the untyped `select` with 6 typed `count` could return `Dataset[(Long, Long, ...)]` instead of `DataFrame`.\r\n\r\nI think you meant something like existing `selectUntyped`? Although its naming is confusing.",
        "createdAt" : "2020-02-25T20:58:22Z",
        "updatedAt" : "2020-02-26T16:57:33Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "7d045fbfb1b6bf35c5cc49e9948db2c5140bb15d",
    "line" : 30,
    "diffHunk" : "@@ -1,1 +414,418 @@    // Passes typed columns to untyped `Dataset.select` API.\n    val err = intercept[AnalysisException] {\n      df.select(fooAgg(1), fooAgg(2), fooAgg(3), fooAgg(4), fooAgg(5), fooAgg(6))\n    }.getMessage\n    assert(err.contains(\"cannot be passed in untyped `select` API. \" +"
  }
]