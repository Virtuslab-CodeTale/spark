[
  {
    "id" : "b0cb9d33-50b8-45f5-af49-4c7d95a70e2b",
    "prId" : 32448,
    "prUrl" : "https://github.com/apache/spark/pull/32448#pullrequestreview-656210591",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ed16a99f-47ba-496f-a19b-66a53b40c733",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Can we update migration guide (https://github.com/apache/spark/blob/master/docs/sql-migration-guide.md)?",
        "createdAt" : "2021-05-10T06:13:50Z",
        "updatedAt" : "2021-05-10T06:13:50Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "3994884e-f738-4d3a-b263-a493b99be052",
        "parentId" : "ed16a99f-47ba-496f-a19b-66a53b40c733",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "is this an expected behavior change? and why do we prefer the new behavior?",
        "createdAt" : "2021-05-10T06:57:15Z",
        "updatedAt" : "2021-05-10T06:57:23Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "4337ccba-e9f6-42d9-a9c0-a24535eb0c71",
        "parentId" : "ed16a99f-47ba-496f-a19b-66a53b40c733",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Yeah, the nested fields don't necessarily have to be sorted. The behaviour should be same or at least similar with the outermost schema. Sorting is sort of unexpected I think.",
        "createdAt" : "2021-05-10T08:21:24Z",
        "updatedAt" : "2021-05-10T08:21:25Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "1fc908bd-a7d7-45cd-87eb-ec4c20957ccc",
        "parentId" : "ed16a99f-47ba-496f-a19b-66a53b40c733",
        "authorId" : "92aa9573-b64a-4a86-aebf-54c827df3ed0",
        "body" : "You want a 3.1 -> 3.2 migration message saying fields are no longer sorted but instead kept in order with new fields added to the end?",
        "createdAt" : "2021-05-10T13:09:08Z",
        "updatedAt" : "2021-05-10T13:09:08Z",
        "lastEditedBy" : "92aa9573-b64a-4a86-aebf-54c827df3ed0",
        "tags" : [
        ]
      },
      {
        "id" : "234b7c5c-b4d6-4de5-ba40-b08aaf88e733",
        "parentId" : "ed16a99f-47ba-496f-a19b-66a53b40c733",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "yes",
        "createdAt" : "2021-05-10T19:37:48Z",
        "updatedAt" : "2021-05-10T19:37:48Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "dbe0afcf-7fca-4abc-a11e-6bfbcf79f673",
        "parentId" : "ed16a99f-47ba-496f-a19b-66a53b40c733",
        "authorId" : "92aa9573-b64a-4a86-aebf-54c827df3ed0",
        "body" : "Following on to that, how is it determined when things are backported to previous releases (3.1 in this case) versus saved for the next minor release? Is that up to submitters or do maintainers make that call? There's a little bit of a \"behavior change\" here, though it's mostly a bug fix. Similar with https://github.com/apache/spark/pull/32338, where it's a bug fix that could be useful in a 3.1 patch release.",
        "createdAt" : "2021-05-11T01:30:09Z",
        "updatedAt" : "2021-05-11T01:30:09Z",
        "lastEditedBy" : "92aa9573-b64a-4a86-aebf-54c827df3ed0",
        "tags" : [
        ]
      }
    ],
    "commit" : "5762ddc83e96b640ffdd4125c182646bf9fb03ff",
    "line" : 50,
    "diffHunk" : "@@ -1,1 +713,717 @@      assert(unionDf.schema.toDDL ==\n        \"`id` INT,`a` STRUCT<`a`: INT, `b`: BIGINT, \" +\n          \"`nested`: STRUCT<`a`: INT, `c`: STRING, `A`: INT, `b`: BIGINT>>\")\n      checkAnswer(unionDf,\n        Row(0, Row(0, 1, Row(1, \"2\", null, null))) ::"
  },
  {
    "id" : "849eef8b-3c36-4d22-9fed-e4d280a5f632",
    "prId" : 32448,
    "prUrl" : "https://github.com/apache/spark/pull/32448#pullrequestreview-662892407",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "90ca8fcf-ebc1-4609-9f30-2dd35ef0d596",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Can you clarify which step fails? I'm reading this test but can't think of how it can fail without this PR.",
        "createdAt" : "2021-05-17T16:29:55Z",
        "updatedAt" : "2021-05-17T16:29:55Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "b42b175d-2130-4332-b562-fbaadb621f23",
        "parentId" : "90ca8fcf-ebc1-4609-9f30-2dd35ef0d596",
        "authorId" : "92aa9573-b64a-4a86-aebf-54c827df3ed0",
        "body" : "The union throws an analysis exception because the right side gets corrupted. It's the same example I have in the issue: https://issues.apache.org/jira/browse/SPARK-35290",
        "createdAt" : "2021-05-17T16:52:10Z",
        "updatedAt" : "2021-05-17T16:52:10Z",
        "lastEditedBy" : "92aa9573-b64a-4a86-aebf-54c827df3ed0",
        "tags" : [
        ]
      },
      {
        "id" : "c33ebb53-8701-4e9d-9e8b-1e97b8ee0907",
        "parentId" : "90ca8fcf-ebc1-4609-9f30-2dd35ef0d596",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I see. From the JIRA, seems the problem is `b:struct<ba:string:bb:string>` becomes `b:struct<aa:string,bb:string>` due to some reasons. Can we fix the bug surgically without changing how the fields are ordered? Or do we have other reasons to change the behavior?",
        "createdAt" : "2021-05-17T17:21:45Z",
        "updatedAt" : "2021-05-17T17:21:45Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "e18952b0-be64-4c6e-ab0d-c031fb831d5a",
        "parentId" : "90ca8fcf-ebc1-4609-9f30-2dd35ef0d596",
        "authorId" : "92aa9573-b64a-4a86-aebf-54c827df3ed0",
        "body" : "I found one tweak that seemed to fix the issue but made the deeply nested case a lot slower to resolve. I went down this route because it sounded like this would have been the ideal behavior originally but sorting seemed like the only option. And it simplifies things a little bit and uses existing struct merging functionality.",
        "createdAt" : "2021-05-17T17:33:34Z",
        "updatedAt" : "2021-05-17T17:33:34Z",
        "lastEditedBy" : "92aa9573-b64a-4a86-aebf-54c827df3ed0",
        "tags" : [
        ]
      },
      {
        "id" : "c51ddb55-8a7b-457f-ace2-00e9524c8e48",
        "parentId" : "90ca8fcf-ebc1-4609-9f30-2dd35ef0d596",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Can you open a PR for that tweak? If possible it's better to fix a bug without changing the designed behavior.",
        "createdAt" : "2021-05-18T14:27:05Z",
        "updatedAt" : "2021-05-18T14:27:05Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "938f1bea-61f9-4963-8e39-a6820e10d172",
        "parentId" : "90ca8fcf-ebc1-4609-9f30-2dd35ef0d596",
        "authorId" : "92aa9573-b64a-4a86-aebf-54c827df3ed0",
        "body" : "It was \"designed\" because it was thought to be the only way. The author said this might be a better approach. I can see if I can remember what that update was, never thoroughly tested it",
        "createdAt" : "2021-05-18T14:59:04Z",
        "updatedAt" : "2021-05-18T14:59:04Z",
        "lastEditedBy" : "92aa9573-b64a-4a86-aebf-54c827df3ed0",
        "tags" : [
        ]
      },
      {
        "id" : "f2962fe7-ce8f-4165-9642-aa92a5dd8da3",
        "parentId" : "90ca8fcf-ebc1-4609-9f30-2dd35ef0d596",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I think sorting is reasonable as well. `df1.unionByName(df2)` can always have the same schema of `df2.unionByName(df1)`",
        "createdAt" : "2021-05-18T15:16:49Z",
        "updatedAt" : "2021-05-18T15:16:49Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "c23a3c24-61fe-465e-8a71-9ae4cb5d06b7",
        "parentId" : "90ca8fcf-ebc1-4609-9f30-2dd35ef0d596",
        "authorId" : "92aa9573-b64a-4a86-aebf-54c827df3ed0",
        "body" : "It's just weird because it's different behavior than every other part of spark. `withColumn` and `withField` both append to the end. And currently before this PR and if the I applied the tweak `df1.unionByName(df2, True)` would not always have the same schema as `df2.unionByName(df1, True)`. It only sorts nested struct fields, not the top level attributes.",
        "createdAt" : "2021-05-18T15:53:09Z",
        "updatedAt" : "2021-05-18T15:53:27Z",
        "lastEditedBy" : "92aa9573-b64a-4a86-aebf-54c827df3ed0",
        "tags" : [
        ]
      },
      {
        "id" : "7b48ee45-ca29-42fc-a557-1286e386404a",
        "parentId" : "90ca8fcf-ebc1-4609-9f30-2dd35ef0d596",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I see. So this PR mainly proposes to change the behavior, and fixing a bug is just a bonus.",
        "createdAt" : "2021-05-18T16:16:33Z",
        "updatedAt" : "2021-05-18T16:16:33Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "147afadc-a70e-4469-99e9-6579fd45094a",
        "parentId" : "90ca8fcf-ebc1-4609-9f30-2dd35ef0d596",
        "authorId" : "92aa9573-b64a-4a86-aebf-54c827df3ed0",
        "body" : "I guess you can look at it that way. It was the most effective way to fix the bug and simplify the code in the process (and not introduce a potential performance issue)",
        "createdAt" : "2021-05-18T17:03:16Z",
        "updatedAt" : "2021-05-18T17:03:16Z",
        "lastEditedBy" : "92aa9573-b64a-4a86-aebf-54c827df3ed0",
        "tags" : [
        ]
      },
      {
        "id" : "955b1786-fc49-4a55-83a2-568aa0314a8a",
        "parentId" : "90ca8fcf-ebc1-4609-9f30-2dd35ef0d596",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "I originally planed to work on to get rid of the sorting behavior but have no time on it. I think it is more consistent with top-level union by name behavior. Currently this approach looks much simpler than `withField` approach and can easily get rid of the sorting behavior. ",
        "createdAt" : "2021-05-19T08:11:56Z",
        "updatedAt" : "2021-05-19T08:11:56Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "5762ddc83e96b640ffdd4125c182646bf9fb03ff",
    "line" : 142,
    "diffHunk" : "@@ -1,1 +799,803 @@    checkAnswer(unionDf,\n      Row(Row(null, Row(null, \"ba\"))) ::\n      Row(Row(Row(\"aa\"), Row(\"bb\", null))) :: Nil)\n  }\n"
  },
  {
    "id" : "e79ee4d8-2296-4bed-8179-174e34aa766c",
    "prId" : 31595,
    "prUrl" : "https://github.com/apache/spark/pull/31595#pullrequestreview-596623779",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a27bea6f-f036-45f9-9f0a-3d53c6f223ea",
        "parentId" : null,
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "After checking the optimized logical plan of this query, I found the `collect` in `RemoveNoopUnion` relies on the fact that the query has one extra unnecessary `Project` on one child:\r\n\r\n```\r\nDistinct\r\n+- Union false, false\r\n   :- Project [key#3, (key#3 + 1) AS expr#143]\r\n   :  +- SerializeFromObject [knownnotnull(assertnotnull(input[0, org.apache.spark.sql.test.SQLTestData$TestData, true])).key AS key#3, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, org.apache.spark.sql.test.SQLTestData$TestData, true])).value, true, false) AS value#4]\r\n   :     +- ExternalRDD [obj#2]\r\n   +- Project [key#3 AS key#145, expr#144 AS expr#146]. <<-- this one\r\n      +- Project [key#3, (key#3 + 2) AS expr#144]\r\n         +- SerializeFromObject [knownnotnull(assertnotnull(input[0, org.apache.spark.sql.test.SQLTestData$TestData, true])).key AS key#3, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, org.apache.spark.sql.test.SQLTestData$TestData, true])).value, true, false) AS value#4]\r\n            +- ExternalRDD [obj#2]\r\n```\r\n\r\nSo the rules works because there's one unnecessary `Project` existed before the rule. And later on this `Project` is removed by `CollapseProject`. If this `Project` does not exist, the correctness of the `RemoveNoopUnion` rule cannot be guaranteed.\r\n\r\nI feel this is dangerous and may lead to wrong query plan later. But I am also fine that we guard this with a unit test here.\r\n\r\nbtw, because of the unnecessary `Project` in the middle, this query cannot be optimized by the rule:\r\n\r\n```\r\nselect distinct key, expr\r\nfrom\r\n(\r\n  select key, key as expr\r\n  from testData\r\n  union all\r\n  select key, key as expr\r\n  from testData\r\n)\r\n```",
        "createdAt" : "2021-02-23T07:50:12Z",
        "updatedAt" : "2021-02-25T04:01:53Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      },
      {
        "id" : "38a2bd1c-9ffb-49d7-9873-388662cc1dc9",
        "parentId" : "a27bea6f-f036-45f9-9f0a-3d53c6f223ea",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "You can verify it by excluding also `CollapseProject`. It doesn't affect the rule.",
        "createdAt" : "2021-02-23T08:27:11Z",
        "updatedAt" : "2021-02-25T04:01:53Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "e84e2841-f13d-439c-a010-b0a6b85ac21c",
        "parentId" : "a27bea6f-f036-45f9-9f0a-3d53c6f223ea",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "@viirya - I am not saying without `CollapseProject` this rule is wrong. You put the rule before `CollapseProject`, so it is not affected by that anyway. I am saying the rule depends on the unnecessary `Project` implicitly, and rule can go wrong / sub-optimal. E.g., above query cannot be optimized by the rule.",
        "createdAt" : "2021-02-23T08:33:51Z",
        "updatedAt" : "2021-02-25T04:01:53Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      },
      {
        "id" : "f4d9ca7e-5d0f-4d0a-9862-834c88f8f992",
        "parentId" : "a27bea6f-f036-45f9-9f0a-3d53c6f223ea",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Manually put `CollapseProject` ahead of this rule and the project is removed, but I don't see it is affected too and the query plan looks correct. Can you check it again?",
        "createdAt" : "2021-02-23T08:50:02Z",
        "updatedAt" : "2021-02-25T04:01:53Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "0156902b-27e3-499d-808d-9c8da9f7319a",
        "parentId" : "a27bea6f-f036-45f9-9f0a-3d53c6f223ea",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "@viirya - I am not sure if we are in the same page now. This is a counter-example to produce wrong query plan if we have `CollapseProject` ahead of the rule - https://github.com/apache/spark/compare/80b03dd64e317...c21:remove-union .",
        "createdAt" : "2021-02-23T09:26:05Z",
        "updatedAt" : "2021-02-25T04:01:53Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      },
      {
        "id" : "6436a1d0-1bc3-4c1c-9957-2b64935477e9",
        "parentId" : "a27bea6f-f036-45f9-9f0a-3d53c6f223ea",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "I posted the query plan with/without this rule for the query from the diff in the above link. I don't see the query plan is different. I'm not sure which query plan is wrong?\r\n\r\n```\r\nsql(\r\n  \"\"\"\r\n     |select key, key + 2 as expr\r\n     |from testData4\r\n     |\"\"\".stripMargin)\r\n```\r\n\r\n```\r\n== Physical Plan ==                                                                                                                                                                                                                    \r\n*(1) Project [key#2641, (key#2641 + 2) AS expr#2645]                                                               \r\n+- *(1) SerializeFromObject [knownnotnull(assertnotnull(input[0, org.apache.spark.sql.test.SQLTestData$TestData4, true])).key AS key#2641]                                                                                             \r\n   +- Scan[obj#2640]    \r\n```\r\n\r\n```\r\n== Physical Plan ==\r\n*(1) Project [key#2732, (key#2732 + 2) AS expr#2736]\r\n+- *(1) SerializeFromObject [knownnotnull(assertnotnull(input[0, org.apache.spark.sql.test.SQLTestData$TestData4, true])).key AS key#2732]\r\n   +- Scan[obj#2731]\r\n```",
        "createdAt" : "2021-02-23T10:15:19Z",
        "updatedAt" : "2021-02-25T04:01:53Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "263bc07b-6830-4d02-997b-9b89f03b6d30",
        "parentId" : "a27bea6f-f036-45f9-9f0a-3d53c6f223ea",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "@viirya - in https://github.com/apache/spark/compare/80b03dd64e317...c21:remove-union,  I am testing this query:\r\n\r\n```\r\nselect distinct key, expr\r\nfrom\r\n(\r\n  select key, key + 1 as expr\r\n  from testData\r\n  union all\r\n  select key, key + 2 as expr\r\n  from testData\r\n)\r\n```\r\n\r\nOptimized logical plan with the rule:\r\n\r\n```\r\n== Optimized Logical Plan ==\r\nAggregate [key#14, expr#18], [key#14, expr#18]\r\n+- Project [key#14, (key#14 + 1) AS expr#18]\r\n   +- SerializeFromObject [knownnotnull(assertnotnull(input[0, org.apache.spark.sql.test.SQLTestData$TestData4, true])).key AS key#14]\r\n      +- ExternalRDD [obj#13]\r\n```\r\n\r\nOptimized logical plan without the rule:\r\n\r\n```\r\n== Optimized Logical Plan ==\r\nAggregate [key#2, expr#6], [key#2, expr#6]\r\n+- Union false, false\r\n   :- Project [key#2, (key#2 + 1) AS expr#6]\r\n   :  +- SerializeFromObject [knownnotnull(assertnotnull(input[0, org.apache.spark.sql.test.SQLTestData$TestData4, true])).key AS key#2]\r\n   :     +- ExternalRDD [obj#1]\r\n   +- Project [key#2, (key#2 + 2) AS expr#9]\r\n      +- SerializeFromObject [knownnotnull(assertnotnull(input[0, org.apache.spark.sql.test.SQLTestData$TestData4, true])).key AS key#2]\r\n         +- ExternalRDD [obj#1]\r\n```",
        "createdAt" : "2021-02-23T17:10:29Z",
        "updatedAt" : "2021-02-25T04:01:53Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      },
      {
        "id" : "9278b01a-aff2-47bd-8464-95bb63803e2b",
        "parentId" : "a27bea6f-f036-45f9-9f0a-3d53c6f223ea",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Oh, thanks. I think it is clear now. Let me fix it.",
        "createdAt" : "2021-02-23T17:55:07Z",
        "updatedAt" : "2021-02-25T04:01:53Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "b5d704112549310e2592aad21bd8308386ff0cf8",
    "line" : 231,
    "diffHunk" : "@@ -1,1 +843,847 @@            |  select key, key + 2 as expr\n            |  from testData\n            |)\n            |\"\"\".stripMargin)\n        val expected = sql("
  },
  {
    "id" : "408ffb2b-47f2-4bea-9c70-d0908ae635da",
    "prId" : 31595,
    "prUrl" : "https://github.com/apache/spark/pull/31595#pullrequestreview-596689436",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "685adc7b-a4bf-4a12-a720-89ee43570cc4",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "what happens to this test if we don't disable the new rule?",
        "createdAt" : "2021-02-23T12:58:07Z",
        "updatedAt" : "2021-02-25T04:01:53Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "1cb59319-79b2-405f-b16b-73091874b99b",
        "parentId" : "685adc7b-a4bf-4a12-a720-89ee43570cc4",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "The test checks a Union with certain children. But the new rule will remove unnecessary Union.\r\n\r\nSo the check\r\n\r\n```\r\n// After optimizer, four 'union.deduplicate' operations should be assert(unionDF.queryExecution.optimizedPlan.collect {\r\n  case u: Union if u.children.size == 5 => u\r\n}.size === 1)\r\n```\r\n\r\nwill fail.",
        "createdAt" : "2021-02-23T18:54:54Z",
        "updatedAt" : "2021-02-25T04:01:53Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "b5d704112549310e2592aad21bd8308386ff0cf8",
    "line" : 29,
    "diffHunk" : "@@ -1,1 +389,393 @@  test(\"SPARK-34283: SQL-style union using Dataset, \" +\n    \"remove unnecessary deduplicate in multiple unions\") {\n    withSQLConf(SQLConf.OPTIMIZER_EXCLUDED_RULES.key -> RemoveNoopUnion.ruleName) {\n      val unionDF = testData.union(testData).distinct().union(testData).distinct()\n        .union(testData).distinct().union(testData).distinct()"
  },
  {
    "id" : "ed522ffd-418f-47b4-80e8-737fe77c2f42",
    "prId" : 31404,
    "prUrl" : "https://github.com/apache/spark/pull/31404#pullrequestreview-590023006",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "564dc3f6-0855-4f83-a831-8984caa97e09",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "We need this test? What's a purpose of this test?",
        "createdAt" : "2021-02-10T15:00:56Z",
        "updatedAt" : "2021-02-19T02:38:37Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "93457a76-dc81-484f-a8bd-bbf3e250c5d2",
        "parentId" : "564dc3f6-0855-4f83-a831-8984caa97e09",
        "authorId" : "ab8f521d-7cf6-437f-8ea5-922bd77ef56b",
        "body" : "The purpose is to check whether the results between Dataset-style and SQL-Style are the same.",
        "createdAt" : "2021-02-14T07:54:09Z",
        "updatedAt" : "2021-02-19T02:38:37Z",
        "lastEditedBy" : "ab8f521d-7cf6-437f-8ea5-922bd77ef56b",
        "tags" : [
        ]
      }
    ],
    "commit" : "1385f2b5963a6ec88dc9d9881f2a4bdd5c735e7a",
    "line" : 46,
    "diffHunk" : "@@ -1,1 +419,423 @@        | select key, value from testData\n        |\"\"\".stripMargin)\n    checkAnswer(unionDF, unionSQLResult)\n  }\n"
  },
  {
    "id" : "72743839-bfd4-43cc-8da7-3bf4901703e9",
    "prId" : 29587,
    "prUrl" : "https://github.com/apache/spark/pull/29587#pullrequestreview-487265161",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1e3a219e-46c9-4241-9c87-87077179a672",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Could you add some tests for case-sensitivity here, too?",
        "createdAt" : "2020-09-09T01:36:27Z",
        "updatedAt" : "2020-10-16T19:47:41Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "b200ed07-e1ff-4b36-bc68-74de98bbe6c2",
        "parentId" : "1e3a219e-46c9-4241-9c87-87077179a672",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Added.",
        "createdAt" : "2020-09-13T00:36:19Z",
        "updatedAt" : "2020-10-16T19:47:41Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "3d907d07233851d6169533d0475c5a53b02cb4a7",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +537,541 @@    }\n  }\n\n  test(\"SPARK-32376: Make unionByName null-filling behavior work with struct columns - simple\") {\n    val df1 = Seq(((1, 2, 3), 0), ((2, 3, 4), 1), ((3, 4, 5), 2)).toDF(\"a\", \"idx\")"
  },
  {
    "id" : "b864b03d-6c51-461c-b148-080784e682ca",
    "prId" : 29587,
    "prUrl" : "https://github.com/apache/spark/pull/29587#pullrequestreview-489402196",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "731ceba6-76e4-4ada-8ac9-47410145e9ba",
        "parentId" : null,
        "authorId" : "6fc88871-fca6-485c-a47e-5e17709c7b68",
        "body" : "What if there are missing nested columns only on one side and fields are not sorted by name (in ascending order) on the other side? \r\n\r\nFor example: \r\n```\r\ntest(\"failing test?\") {\r\n  withSQLConf(SQLConf.UNION_BYNAME_STRUCT_SUPPORT_ENABLED.key -> \"true\") {\r\n    val nestedStructType1 = StructType(Seq(\r\n      StructField(\"b\", StringType)))\r\n    val nestedStructValues1 = Row(\"b\")\r\n\r\n    val nestedStructType2 = StructType(Seq(\r\n      StructField(\"b\", StringType),\r\n      StructField(\"a\", StringType)))\r\n    val nestedStructValues2 = Row(\"b\", \"a\")\r\n\r\n    val df1: DataFrame = spark.createDataFrame(\r\n      sparkContext.parallelize(Row(nestedStructValues1) :: Nil),\r\n      StructType(Seq(StructField(\"topLevelCol\", nestedStructType1))))\r\n\r\n    val df2: DataFrame = spark.createDataFrame(\r\n      sparkContext.parallelize(Row(nestedStructValues2) :: Nil),\r\n      StructType(Seq(StructField(\"topLevelCol\", nestedStructType2))))\r\n\r\n    df1.unionByName(df2, allowMissingColumns = true).show(false)\r\n  }\r\n}\r\n```\r\nGives me the following error: \r\n```\r\n[info] - failing test? *** FAILED *** (3 seconds, 719 milliseconds)\r\n[info]   org.apache.spark.sql.AnalysisException: Union can only be performed on tables with the compatible column types. struct<b:string,a:string> <> struct<a:string,b:string> at the first column of the second table;;\r\n[info] 'Union false, false\r\n[info] :- Project [with_fields(topLevelCol#1, a, null, true) AS topLevelCol#6]\r\n[info] :  +- LogicalRDD [topLevelCol#1], false\r\n[info] +- Project [topLevelCol#4]\r\n[info]    +- LogicalRDD [topLevelCol#4], false\r\n```\r\nIs this expected behaviour? I feel this should be supported but CMIIW. \r\n",
        "createdAt" : "2020-09-14T17:23:05Z",
        "updatedAt" : "2020-10-16T19:47:41Z",
        "lastEditedBy" : "6fc88871-fca6-485c-a47e-5e17709c7b68",
        "tags" : [
        ]
      },
      {
        "id" : "ba5b8c3d-2abf-44fe-8855-1c51e863d08f",
        "parentId" : "731ceba6-76e4-4ada-8ac9-47410145e9ba",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Good catch! Thanks. I missed this kind of case. I will update this to fix it.",
        "createdAt" : "2020-09-14T17:39:36Z",
        "updatedAt" : "2020-10-16T19:47:41Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "f9ae1b69-8f2b-4051-8829-39956b122dc0",
        "parentId" : "731ceba6-76e4-4ada-8ac9-47410145e9ba",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Fixed this case. Thanks.",
        "createdAt" : "2020-09-15T01:17:28Z",
        "updatedAt" : "2020-10-16T19:47:41Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "4ac62447-b8fc-4d55-9e0b-8afe01b18fad",
        "parentId" : "731ceba6-76e4-4ada-8ac9-47410145e9ba",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Ah, I see and I missed the case. Nice catch.",
        "createdAt" : "2020-09-16T08:39:27Z",
        "updatedAt" : "2020-10-16T19:47:41Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "3d907d07233851d6169533d0475c5a53b02cb4a7",
    "line" : 185,
    "diffHunk" : "@@ -1,1 +718,722 @@case class UnionClass2(a: Int, c: String)\ncase class UnionClass3(a: Int, b: Long)\ncase class UnionClass4(A: Int, b: Long)"
  },
  {
    "id" : "c7f18d25-781b-46a8-887e-7609be40e8a5",
    "prId" : 28996,
    "prUrl" : "https://github.com/apache/spark/pull/28996#pullrequestreview-446816820",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e9708634-0a35-499e-a8c5-1a2d2018ddda",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "@viirya . Can we have both case-sensitive and case-insensitive test coverage?",
        "createdAt" : "2020-07-11T16:30:58Z",
        "updatedAt" : "2020-07-11T17:43:25Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "c74075e2-1fe5-42fd-888f-729e1228dd6d",
        "parentId" : "e9708634-0a35-499e-a8c5-1a2d2018ddda",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "sure.",
        "createdAt" : "2020-07-11T17:25:28Z",
        "updatedAt" : "2020-07-11T17:43:25Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "873498394eaf80b2b302b6fc7aeec410e7113415",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +524,528 @@      Row(1, 2, null) :: Row(3, 5, 4) :: Nil)\n    checkAnswer(df2.unionByName(df1, true),\n      Row(3, 4, 5) :: Row(1, null, 2) :: Nil)\n\n    withSQLConf(SQLConf.CASE_SENSITIVE.key -> \"true\") {"
  }
]