[
  {
    "id" : "77c91699-a5b5-42fa-bbf1-2c8b02e53fd2",
    "prId" : 31296,
    "prUrl" : "https://github.com/apache/spark/pull/31296#pullrequestreview-578798168",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ba65252e-777e-4a02-a6ca-2150583c8ef7",
        "parentId" : null,
        "authorId" : "8c82da5a-f351-4a37-a8a9-13809311b07b",
        "body" : "nit: Why `toDF`?",
        "createdAt" : "2021-01-28T22:34:28Z",
        "updatedAt" : "2021-01-28T22:35:03Z",
        "lastEditedBy" : "8c82da5a-f351-4a37-a8a9-13809311b07b",
        "tags" : [
        ]
      }
    ],
    "commit" : "ac7460b0431e7012f17c3ac293226bb4f428bd15",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +1271,1275 @@    val inputData = MemoryStream[Int]\n    val piped = inputData.toDS()\n      .pipe(\"cat\", (n, printFunc) => printFunc(n.toString)).toDF\n\n    testStream(piped)("
  },
  {
    "id" : "27fc6e53-8dee-48df-8414-a958aec6b6d7",
    "prId" : 30443,
    "prUrl" : "https://github.com/apache/spark/pull/30443#pullrequestreview-555923213",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f57d8330-7895-4f64-9d24-54d12a26bd62",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Why did you change the number?",
        "createdAt" : "2020-12-18T12:52:54Z",
        "updatedAt" : "2020-12-22T15:00:45Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "4e971da7-7faa-4b2e-a606-9d54549af8a2",
        "parentId" : "f57d8330-7895-4f64-9d24-54d12a26bd62",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "This case test the `LocalLimit`, and `range(1).limit(1)` will eliminate the limit. So here we change to range(2).",
        "createdAt" : "2020-12-19T07:09:34Z",
        "updatedAt" : "2020-12-22T15:00:45Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      }
    ],
    "commit" : "5d46a4c26a193e7ddea7788510d52376325cb99f",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1131,1135 @@\n    // Should be LocalLimitExec in the first place, not from optimization of StreamingLocalLimitExec\n    val staticDF = spark.range(2).toDF(\"value\").limit(1)\n    verifyLocalLimit(inputDF.toDF(\"value\").join(staticDF, \"value\"), expectStreamingLimit = false)\n"
  },
  {
    "id" : "757407ba-1261-4a55-b997-86c5e30701d8",
    "prId" : 29585,
    "prUrl" : "https://github.com/apache/spark/pull/29585#pullrequestreview-481376608",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "863f59a3-3bc6-400a-92e8-e46451900303",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "NOTE: This ill-formed definition was found by the integrity check: https://github.com/apache/spark/pull/29585/files#diff-27c76f96a7b2733ecfd6f46a1716e153R224",
        "createdAt" : "2020-09-02T23:17:31Z",
        "updatedAt" : "2020-09-29T12:28:36Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "e3c87429e7e6f463c4e7740c7a42e8b2def528b0",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +1268,1272 @@\nabstract class FakeSource extends StreamSourceProvider {\n  private val fakeSchema = StructType(StructField(\"a\", LongType) :: Nil)\n\n  override def sourceSchema("
  },
  {
    "id" : "a856c834-42c4-4648-b4df-3ab45f3bfef3",
    "prId" : 27373,
    "prUrl" : "https://github.com/apache/spark/pull/27373#pullrequestreview-351657381",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0c9a221c-80d5-4e6d-8b4e-a4fe95dadfef",
        "parentId" : null,
        "authorId" : "2d6b46ba-4100-4c4d-9341-fff5e39647ec",
        "body" : "nit: this is missing the jira id.",
        "createdAt" : "2020-01-31T17:24:16Z",
        "updatedAt" : "2020-01-31T17:24:50Z",
        "lastEditedBy" : "2d6b46ba-4100-4c4d-9341-fff5e39647ec",
        "tags" : [
        ]
      }
    ],
    "commit" : "b1275257357421d8148418b39b6547c99eedc563",
    "line" : 68,
    "diffHunk" : "@@ -1,1 +1061,1065 @@  }\n\n  test(\"streaming limit should not apply on limits on state subplans\") {\n    val streanData = MemoryStream[Int]\n    val streamingDF = streanData.toDF().toDF(\"value\")"
  },
  {
    "id" : "a98c7f2b-2bf4-4033-afc6-883df6e92cd1",
    "prId" : 26768,
    "prUrl" : "https://github.com/apache/spark/pull/26768#pullrequestreview-328027943",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2a5c3ce2-8d9d-4ee7-ae98-a512ad838296",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "This seems to fail like the following at `schema` comparison.\r\n```\r\n!== Correct Answer - 9 ==   == Spark Answer - 9 ==\r\n!struct<>                   struct<value:int,count(1):bigint>\r\n [1,1]                      [1,1]\r\n [2,1]                      [2,1]\r\n [3,2]                      [3,2]\r\n [4,2]                      [4,2]\r\n [5,2]                      [5,2]\r\n [6,2]                      [6,2]\r\n [7,1]                      [7,1]\r\n [8,1]                      [8,1]\r\n [9,1]                      [9,1]\r\n```",
        "createdAt" : "2019-12-05T19:18:10Z",
        "updatedAt" : "2019-12-06T02:10:52Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "dc757a85-8620-4026-9485-ca445d1ca29f",
        "parentId" : "2a5c3ce2-8d9d-4ee7-ae98-a512ad838296",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "This is because `expectedAnswer`'s data type do not match. https://github.com/apache/spark/pull/26768/commits/61eacc940381ef717f102c809ef80afd9b0facf7 fix this issue.",
        "createdAt" : "2019-12-06T02:12:12Z",
        "updatedAt" : "2019-12-06T02:12:13Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      },
      {
        "id" : "7d436d17-72bd-4693-b0ac-ef2620c105dc",
        "parentId" : "2a5c3ce2-8d9d-4ee7-ae98-a512ad838296",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "Before this PR. it will not fail.",
        "createdAt" : "2019-12-06T02:12:43Z",
        "updatedAt" : "2019-12-06T02:12:43Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      },
      {
        "id" : "1289bf4b-ef05-40c2-b4a9-c1f0ed766ade",
        "parentId" : "2a5c3ce2-8d9d-4ee7-ae98-a512ad838296",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Thanks!",
        "createdAt" : "2019-12-06T07:09:47Z",
        "updatedAt" : "2019-12-06T07:09:47Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "61eacc940381ef717f102c809ef80afd9b0facf7",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +761,765 @@          case Some(errorMessage) => fail(errorMessage)\n          case None =>\n        }\n      } finally {\n        if (streamingQuery ne null) {"
  },
  {
    "id" : "a8e28f92-c81d-4ec1-844f-ef6840abf6cb",
    "prId" : 25514,
    "prUrl" : "https://github.com/apache/spark/pull/25514#pullrequestreview-290768668",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e55be616-4ea2-4a4b-9226-549dbf447e7e",
        "parentId" : null,
        "authorId" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "body" : "Why do we need this (and similar changes)? The original test is running fine with temp checkpoint.",
        "createdAt" : "2019-09-19T18:44:51Z",
        "updatedAt" : "2019-09-19T18:45:13Z",
        "lastEditedBy" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "tags" : [
        ]
      }
    ],
    "commit" : "73ff29b057fad3980a35b82197a646a875512e7a",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +446,450 @@    // Test StreamingQuery.display\n    val dir1 = Utils.createTempDir().getCanonicalFile\n    val q = df.writeStream.queryName(\"memory_explain\").outputMode(\"complete\").format(\"memory\")\n      .option(\"checkpointLocation\", \"file://\" + dir1.getCanonicalPath)\n      .start()"
  },
  {
    "id" : "5fda294e-fd55-4b69-842e-f420e2f254c3",
    "prId" : 24327,
    "prUrl" : "https://github.com/apache/spark/pull/24327#pullrequestreview-250507832",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c664742a-b2d5-4c7c-be31-645bc2663b11",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "how is this related to parquet?",
        "createdAt" : "2019-06-17T07:14:36Z",
        "updatedAt" : "2019-06-17T07:14:37Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "aea7e9cb-1759-4a14-9ee4-7178ee609184",
        "parentId" : "c664742a-b2d5-4c7c-be31-645bc2663b11",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "```\r\n[info]   Decoded objects do not match expected objects:\r\n[info]   expected: WrappedArray(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\r\n[info]   actual:   WrappedArray(9, 0, 10, 1, 2, 8, 3, 6, 7, 5, 4)\r\n[info]   assertnotnull(upcast(getcolumnbyordinal(0, LongType), LongType, - root class: \"scala.Long\"))\r\n[info]   +- upcast(getcolumnbyordinal(0, LongType), LongType, - root class: \"scala.Long\")\r\n[info]      +- getcolumnbyordinal(0, LongType) (QueryTest.scala:70)\r\n```\r\nWe need to fix the read path for steaming output.\r\n",
        "createdAt" : "2019-06-17T13:51:25Z",
        "updatedAt" : "2019-06-17T13:51:39Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "f658e9265ba741922fc96eec76038addcb6491a1",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +221,225 @@    // TODO: fix file source V2 as well.\n    withSQLConf(SQLConf.USE_V1_SOURCE_READER_LIST.key -> \"parquet\") {\n      val df = spark.readStream.format(classOf[FakeDefaultSource].getName).load()\n      assertDF(df)\n      assertDF(df)"
  }
]