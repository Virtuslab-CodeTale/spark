[
  {
    "id" : "3af4f0e4-0bba-43f2-bf35-34af796b6210",
    "prId" : 25536,
    "prUrl" : "https://github.com/apache/spark/pull/25536#pullrequestreview-278038221",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a061f8d9-9b89-46b4-9c63-03ade4c4ebf4",
        "parentId" : null,
        "authorId" : "1aba4faa-ea5e-4487-baf9-d552ca566126",
        "body" : "minor and non-blocking... \"table_name\" repeated many times and is it better to make it a test class variable and each test case referencing it?\r\n\r\nsorry maybe I'm being too nitpick lol\r\n\r\nPR looks good to me :)",
        "createdAt" : "2019-08-21T18:40:52Z",
        "updatedAt" : "2019-08-21T18:43:14Z",
        "lastEditedBy" : "1aba4faa-ea5e-4487-baf9-d552ca566126",
        "tags" : [
        ]
      },
      {
        "id" : "0da21d8a-9ca7-4878-9546-aaa553d8f88d",
        "parentId" : "a061f8d9-9b89-46b4-9c63-03ade4c4ebf4",
        "authorId" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "body" : "It's better to me if you don't have to jump around too much when reading the code.",
        "createdAt" : "2019-08-21T19:14:15Z",
        "updatedAt" : "2019-08-21T19:14:16Z",
        "lastEditedBy" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "tags" : [
        ]
      },
      {
        "id" : "7bc603ae-d438-4cc2-9cd0-0a6a47341cc7",
        "parentId" : "a061f8d9-9b89-46b4-9c63-03ade4c4ebf4",
        "authorId" : "1aba4faa-ea5e-4487-baf9-d552ca566126",
        "body" : "in your PR https://github.com/apache/spark/pull/25507/files `DataSourceV2DataFrameSessionCatalogSuite.scala`, there is a class variable `v2Format` being referenced by all test cases\r\n\r\n```\r\nprotected val v2Format: String = classOf[InMemoryTableProvider].getName\r\n```\r\n\r\nso I thought it's a style convention...",
        "createdAt" : "2019-08-21T19:59:05Z",
        "updatedAt" : "2019-08-21T19:59:06Z",
        "lastEditedBy" : "1aba4faa-ea5e-4487-baf9-d552ca566126",
        "tags" : [
        ]
      }
    ],
    "commit" : "655d07ef22bd3b775906fe69e04e4f8beb82a1a5",
    "line" : 78,
    "diffHunk" : "@@ -1,1 +510,514 @@        spark.sql(s\"CREATE TABLE $identifier USING foo AS SELECT 1 i\")\n\n        val table = catalog.loadTable(Identifier.of(Array(), \"table_name\"))\n\n        assert(table.name == identifier)"
  },
  {
    "id" : "6afb51d4-ab7c-47d1-887e-fce1a26a548b",
    "prId" : 25402,
    "prUrl" : "https://github.com/apache/spark/pull/25402#pullrequestreview-274667429",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "074d5627-67e1-4d43-8b5c-70632a62d146",
        "parentId" : null,
        "authorId" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "body" : "We can maintain this behavior, but I'd rather not, as the V2SessionCatalog can't properly handle views and such",
        "createdAt" : "2019-08-14T04:57:14Z",
        "updatedAt" : "2019-08-14T16:08:07Z",
        "lastEditedBy" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "tags" : [
        ]
      }
    ],
    "commit" : "a70e72676da6442a45e3a358c734b6f161c615d0",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +495,499 @@    sparkSession.sql(s\"CREATE TABLE table_name USING parquet AS SELECT id, data FROM source\")\n\n    checkAnswer(sparkSession.sql(s\"TABLE default.table_name\"), sparkSession.table(\"source\"))\n    // The fact that the following line doesn't throw an exception means, the session catalog\n    // can load the table."
  },
  {
    "id" : "abf06536-037d-4809-a47f-6251b5d502cf",
    "prId" : 25368,
    "prUrl" : "https://github.com/apache/spark/pull/25368#pullrequestreview-273496646",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fa0bffff-2bad-461c-a9a0-c69c4c3ade36",
        "parentId" : null,
        "authorId" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "body" : "`withSQLConf`? Otherwise you affect all tests below",
        "createdAt" : "2019-08-09T22:18:12Z",
        "updatedAt" : "2019-08-20T03:00:48Z",
        "lastEditedBy" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "tags" : [
        ]
      },
      {
        "id" : "c4f6f426-d0bf-4eaf-948b-c293f6514e75",
        "parentId" : "fa0bffff-2bad-461c-a9a0-c69c4c3ade36",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "This test suite will clear all the configs after each test, see https://github.com/apache/spark/pull/25368/files#diff-b49f76fba19ee10a28e0e61c4b44e1a0R62",
        "createdAt" : "2019-08-12T02:31:06Z",
        "updatedAt" : "2019-08-20T03:00:48Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "4895a6eab6ec449238192715b78fea61217f7f54",
    "line" : 83,
    "diffHunk" : "@@ -1,1 +201,205 @@\n  test(\"CreateTable: use default catalog for v2 sources when default catalog is set\") {\n    spark.conf.set(\"spark.sql.default.catalog\", \"testcat\")\n    spark.sql(s\"CREATE TABLE table_name (id bigint, data string) USING foo\")\n"
  },
  {
    "id" : "784345a2-3b09-49d2-8c50-392689437308",
    "prId" : 25368,
    "prUrl" : "https://github.com/apache/spark/pull/25368#pullrequestreview-273381475",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "41abbb19-3421-46ad-a6be-9d09559553a5",
        "parentId" : null,
        "authorId" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "body" : "ditto",
        "createdAt" : "2019-08-09T22:18:54Z",
        "updatedAt" : "2019-08-20T03:00:48Z",
        "lastEditedBy" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "tags" : [
        ]
      }
    ],
    "commit" : "4895a6eab6ec449238192715b78fea61217f7f54",
    "line" : 206,
    "diffHunk" : "@@ -1,1 +462,466 @@\n  test(\"CreateTableAsSelect: use default catalog for v2 sources when default catalog is set\") {\n    spark.conf.set(\"spark.sql.default.catalog\", \"testcat\")\n\n    val df = spark.createDataFrame(Seq((1L, \"a\"), (2L, \"b\"), (3L, \"c\"))).toDF(\"id\", \"data\")"
  },
  {
    "id" : "2ef211e4-8c77-44a6-8280-8bb6e6c38512",
    "prId" : 25247,
    "prUrl" : "https://github.com/apache/spark/pull/25247#pullrequestreview-273841918",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0b4abdbf-f474-4d90-a559-1fc7db76fb57",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "In current Spark, `SHOW TABLES FROM non-existing-db` would fail, shall we follow it?",
        "createdAt" : "2019-08-12T03:31:15Z",
        "updatedAt" : "2019-08-23T00:03:45Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "622962f2-ae1f-4648-9a26-4f00ab7908d8",
        "parentId" : "0b4abdbf-f474-4d90-a559-1fc7db76fb57",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "@cloud-fan as far as I understand, throwing `NoSuchNamespaceException` is optional in v2:\r\n```\r\n  /**\r\n   * List the tables in a namespace from the catalog.\r\n   * <p>\r\n   * If the catalog supports views, this must return identifiers for only tables and not views.\r\n   *\r\n   * @param namespace a multi-part namespace\r\n   * @return an array of Identifiers for tables\r\n   * @throws NoSuchNamespaceException If the namespace does not exist (optional).\r\n   */\r\n  Identifier[] listTables(String[] namespace) throws NoSuchNamespaceException;\r\n```\r\n\r\nI can update `TestInMemoryTableCatalog` to throw NoSuchNamespaceException if there is no namespace existing for the tables created. However, I am not sure if this is the right approach since you could have created namespace without tables - in v1, you could have done `CREATE DATABASE db` without creating tables belonging to `db`, although I don't think this scenario is supported in v2 yet.\r\n\r\nPlease advise how this needs to be handled. Thanks!\r\n\r\n",
        "createdAt" : "2019-08-12T04:24:06Z",
        "updatedAt" : "2019-08-23T00:03:45Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      },
      {
        "id" : "169930ad-ef53-42c9-8408-ecafd212ac54",
        "parentId" : "0b4abdbf-f474-4d90-a559-1fc7db76fb57",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "v2 can support `CREATE NAMESPACE`, we have the APIs in `SupportNamespace`.\r\n\r\nI think this is another case we should discuss: how much should Spark restrict the semantic of a SQL command? e.g. `SHOW TABLE catalog.nonExistingNS`, should Spark guarantee that, this command fails if the namespace doesn't exist?\r\n\r\ncc @brkyvz @rdblue ",
        "createdAt" : "2019-08-12T11:34:02Z",
        "updatedAt" : "2019-08-23T00:03:45Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "d4342d68-afaf-4d6b-9d98-965b4c095aa9",
        "parentId" : "0b4abdbf-f474-4d90-a559-1fc7db76fb57",
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "If the catalog throws `NoSuchNamespaceException`, then the query should fail. If not, then this should return whatever the catalog returned. Throwing `NoSuchNamespaceException` is optional, so I think Spark should respect the catalog's choice of whether to throw it or not in this case.",
        "createdAt" : "2019-08-12T17:35:23Z",
        "updatedAt" : "2019-08-23T00:03:45Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      }
    ],
    "commit" : "6f2fa4e7b8572a6b9dd243f1253e4962af27d40a",
    "line" : 63,
    "diffHunk" : "@@ -1,1 +1739,1743 @@\n  test(\"ShowTables: using v2 catalog, namespace doesn't exist\") {\n    runShowTablesSql(\"SHOW TABLES FROM testcat.unknown\", Seq())\n  }\n"
  },
  {
    "id" : "ff4f17f5-23a0-47ed-8412-1425c4856f59",
    "prId" : 25247,
    "prUrl" : "https://github.com/apache/spark/pull/25247#pullrequestreview-277539124",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "282a17c7-0209-4aa7-9f31-2a25ca0092fb",
        "parentId" : null,
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "This test is currently failing and will be updated based on the resolution of https://github.com/apache/spark/pull/25368/files#r315980292.",
        "createdAt" : "2019-08-21T03:16:21Z",
        "updatedAt" : "2019-08-23T00:03:45Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "6f2fa4e7b8572a6b9dd243f1253e4962af27d40a",
    "line" : 97,
    "diffHunk" : "@@ -1,1 +1773,1777 @@\n  test(\"ShowTables: namespace is not specified and default v2 catalog is set\") {\n    spark.conf.set(\"spark.sql.default.catalog\", \"testcat\")\n    spark.sql(\"CREATE TABLE testcat.table (id bigint, data string) USING foo\")\n"
  },
  {
    "id" : "c21e4382-a0de-410e-bab4-960de02ca74a",
    "prId" : 25115,
    "prUrl" : "https://github.com/apache/spark/pull/25115#pullrequestreview-274416477",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "22ff8292-0bf7-408a-9492-bf50afbb05f8",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "can we also test correlated subquery? ",
        "createdAt" : "2019-08-09T10:45:34Z",
        "updatedAt" : "2019-08-14T01:57:20Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "267ccaa6-b13b-485a-8082-7239c2ceee16",
        "parentId" : "22ff8292-0bf7-408a-9492-bf50afbb05f8",
        "authorId" : "8afcf008-6988-44a5-92fa-2bef54bb4058",
        "body" : "Is that necessary to test correlated subquery? Because correlated subquery is a subset of subquery and we forbid subquery here, then correlated subquery is also forbidden.\r\nMy thought is later I want to add pre-execution subquery for DELETE, but correlated subquery is still forbidden, so we can modify the test cases at that time.",
        "createdAt" : "2019-08-13T06:53:06Z",
        "updatedAt" : "2019-08-14T01:57:20Z",
        "lastEditedBy" : "8afcf008-6988-44a5-92fa-2bef54bb4058",
        "tags" : [
        ]
      },
      {
        "id" : "2d3b7c7c-a533-4876-959b-ec35bccfbf2b",
        "parentId" : "22ff8292-0bf7-408a-9492-bf50afbb05f8",
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "This sounds reasonable to me.",
        "createdAt" : "2019-08-13T16:47:43Z",
        "updatedAt" : "2019-08-14T01:57:20Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      }
    ],
    "commit" : "bbf515666495cbf5f12731b3cdab4a23960f3d77",
    "line" : 32,
    "diffHunk" : "@@ -1,1 +1891,1895 @@      sql(s\"INSERT INTO $t VALUES (2L, 'a', 2), (2L, 'b', 3), (3L, 'c', 3)\")\n      val exc = intercept[AnalysisException] {\n        sql(s\"DELETE FROM $t WHERE id IN (SELECT id FROM $t)\")\n      }\n"
  },
  {
    "id" : "115102a0-85e6-4c1a-a565-31cd9ac2ec06",
    "prId" : 25077,
    "prUrl" : "https://github.com/apache/spark/pull/25077#pullrequestreview-258885986",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e28b63b2-53a2-4036-bdb9-444a0e5f4baf",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "This test well demonstrates the expected behavior after this fix.",
        "createdAt" : "2019-07-08T13:04:06Z",
        "updatedAt" : "2019-07-09T12:28:36Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "4ed4399ad558249ceb2c56e1d1c88837ae4602df",
    "line" : 49,
    "diffHunk" : "@@ -1,1 +52,56 @@  }\n\n  test(\"CreateTable: basic\") {\n    def checkTestCatalog(sql: String): Unit = {\n      spark.sql(sql)"
  },
  {
    "id" : "f54b383d-45a0-4ef5-b308-2df2c9240223",
    "prId" : 24937,
    "prUrl" : "https://github.com/apache/spark/pull/24937#pullrequestreview-257241515",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a31ff62a-cae5-47ad-8306-a041a932f5fa",
        "parentId" : null,
        "authorId" : "1f3c66ec-cc36-4a64-807a-476c6615e68c",
        "body" : "nit: I'd prefer to have a test for compatible-but-truncating types too, even if they currently have the same behavior.",
        "createdAt" : "2019-07-01T17:00:18Z",
        "updatedAt" : "2019-07-11T17:21:21Z",
        "lastEditedBy" : "1f3c66ec-cc36-4a64-807a-476c6615e68c",
        "tags" : [
        ]
      },
      {
        "id" : "d23f8fdf-504d-4b1f-8111-9c6220c4d885",
        "parentId" : "a31ff62a-cae5-47ad-8306-a041a932f5fa",
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "Can you give an example?",
        "createdAt" : "2019-07-02T17:13:03Z",
        "updatedAt" : "2019-07-11T17:21:21Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      },
      {
        "id" : "0cb4dd0d-c477-4e85-84a9-87571c53eee0",
        "parentId" : "a31ff62a-cae5-47ad-8306-a041a932f5fa",
        "authorId" : "1f3c66ec-cc36-4a64-807a-476c6615e68c",
        "body" : "I'm thinking cases like LONG to INT, but maybe I'm missing context on how they're abstracted.",
        "createdAt" : "2019-07-02T18:05:16Z",
        "updatedAt" : "2019-07-11T17:21:21Z",
        "lastEditedBy" : "1f3c66ec-cc36-4a64-807a-476c6615e68c",
        "tags" : [
        ]
      },
      {
        "id" : "ec47a207-4608-4716-8cc7-1b87f536d636",
        "parentId" : "a31ff62a-cae5-47ad-8306-a041a932f5fa",
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "There is a test case for this that rejects unsafe casts: AlterTable: update column type must be compatible. Long to int is not allowed.",
        "createdAt" : "2019-07-03T03:11:25Z",
        "updatedAt" : "2019-07-11T17:21:21Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      }
    ],
    "commit" : "5b10bd0164132bf296a07bcb794c3f7af262a72e",
    "line" : 437,
    "diffHunk" : "@@ -1,1 +784,788 @@  }\n\n  test(\"AlterTable: update column type must be compatible\") {\n    val t = \"testcat.ns1.table_name\"\n    withTable(t) {"
  },
  {
    "id" : "39cb10cc-e587-4168-b121-29e29671dc5e",
    "prId" : 24937,
    "prUrl" : "https://github.com/apache/spark/pull/24937#pullrequestreview-257061164",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "af215c4b-68e9-4ec1-9cac-8935110e8c75",
        "parentId" : null,
        "authorId" : "1f3c66ec-cc36-4a64-807a-476c6615e68c",
        "body" : "General question more related to the parser than here: are there tests ensuring that something dangerous doesn't happen if you try to rename/alter/whatever a special identifier like \"points.value\" directly?",
        "createdAt" : "2019-07-01T17:04:25Z",
        "updatedAt" : "2019-07-11T17:21:21Z",
        "lastEditedBy" : "1f3c66ec-cc36-4a64-807a-476c6615e68c",
        "tags" : [
        ]
      },
      {
        "id" : "88c04c3d-7782-4a48-8258-5e8ee9bbc7ef",
        "parentId" : "af215c4b-68e9-4ec1-9cac-8935110e8c75",
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "It is possible to alter `points.value`, so the parser and analysis checks allow it. For example, the \"AlterTable: update column map value type\" test updates a map's value type from `int` to `long`.\r\n\r\nImplementations may also support renaming here, since some formats, like Parquet, use an array of structs to store map data. (Similarly, Parquet has a struct to capture nullability of array elements.) So it may be valid to rename `points.value`, but most implementations would reject it.",
        "createdAt" : "2019-07-02T17:12:49Z",
        "updatedAt" : "2019-07-11T17:21:21Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      },
      {
        "id" : "575e5071-99e7-49f3-985e-4ad6e9fc65be",
        "parentId" : "af215c4b-68e9-4ec1-9cac-8935110e8c75",
        "authorId" : "1f3c66ec-cc36-4a64-807a-476c6615e68c",
        "body" : "Makes sense.",
        "createdAt" : "2019-07-02T18:06:27Z",
        "updatedAt" : "2019-07-11T17:21:21Z",
        "lastEditedBy" : "1f3c66ec-cc36-4a64-807a-476c6615e68c",
        "tags" : [
        ]
      }
    ],
    "commit" : "5b10bd0164132bf296a07bcb794c3f7af262a72e",
    "line" : 633,
    "diffHunk" : "@@ -1,1 +980,984 @@    withTable(t) {\n      sql(s\"CREATE TABLE $t (id int, points map<string, struct<x: double, y: double>>) USING foo\")\n      sql(s\"ALTER TABLE $t RENAME COLUMN points.value.y TO t\")\n\n      val testCatalog = spark.catalog(\"testcat\").asTableCatalog"
  },
  {
    "id" : "af3571b5-0e9a-4fe8-a42a-c4ad5251f18f",
    "prId" : 24937,
    "prUrl" : "https://github.com/apache/spark/pull/24937#pullrequestreview-260547519",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "081c21ea-5cc8-4464-afea-191bba2e8c5e",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "can we create a new test suite for alter table? I'm afraid this test suite will get too big if we put all the DDL tests here.",
        "createdAt" : "2019-07-11T08:38:18Z",
        "updatedAt" : "2019-07-11T17:21:21Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "5b10bd0164132bf296a07bcb794c3f7af262a72e",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +369,373 @@  }\n\n  test(\"AlterTable: table does not exist\") {\n    val exc = intercept[AnalysisException] {\n      sql(s\"ALTER TABLE testcat.ns1.table_name DROP COLUMN id\")"
  },
  {
    "id" : "8d078938-b884-4209-8cab-2c18b5c873f6",
    "prId" : 24832,
    "prUrl" : "https://github.com/apache/spark/pull/24832#pullrequestreview-258547312",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1ad371e1-a7f3-4ff2-a9cf-ceda1a46fe2c",
        "parentId" : null,
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "This is the behavior of existing file sources? With `static` partition overwrite mode, the entire table is overwritten unless you use a `PARTITION` clause?",
        "createdAt" : "2019-07-05T20:55:22Z",
        "updatedAt" : "2019-07-25T16:05:47Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      }
    ],
    "commit" : "7f193ca074b7fd0dea9f7b28b4e1776819e6d512",
    "line" : 135,
    "diffHunk" : "@@ -1,1 +1466,1470 @@    withSQLConf(PARTITION_OVERWRITE_MODE.key -> PartitionOverwriteMode.STATIC.toString) {\n      val t1 = \"testcat.ns1.ns2.tbl\"\n      withTable(t1) {\n        sql(s\"CREATE TABLE $t1 (id bigint, data string) USING foo PARTITIONED BY (id)\")\n        sql(s\"INSERT INTO $t1 VALUES (2L, 'dummy'), (4L, 'also-deleted')\")"
  },
  {
    "id" : "47bb10e1-cd96-4cd0-a732-3cac02e92246",
    "prId" : 24798,
    "prUrl" : "https://github.com/apache/spark/pull/24798#pullrequestreview-247345212",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "85a0cddb-b996-4f1f-ac38-7531a79f89d9",
        "parentId" : null,
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "I think this should also have test cases for `REPLACE TABLE` without `AS SELECT ...`. I also don't think there is a test for `REPLACE TABLE` when the original table doesn't exist. It would be good to add that as well.",
        "createdAt" : "2019-06-06T23:39:22Z",
        "updatedAt" : "2019-07-19T19:00:59Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      },
      {
        "id" : "cc27586f-8559-4ef6-93b5-ac3a23f5eea5",
        "parentId" : "85a0cddb-b996-4f1f-ac38-7531a79f89d9",
        "authorId" : "806aa501-9ea7-4cec-b017-a84d9a1602d8",
        "body" : "I added test cases.",
        "createdAt" : "2019-06-08T00:16:32Z",
        "updatedAt" : "2019-07-19T19:00:59Z",
        "lastEditedBy" : "806aa501-9ea7-4cec-b017-a84d9a1602d8",
        "tags" : [
        ]
      }
    ],
    "commit" : "05a827df7094b07a492add875c6e649df52db41f",
    "line" : 147,
    "diffHunk" : "@@ -1,1 +272,276 @@    assert(maybeReplacedTable === table, \"Table should not have changed.\")\n  }\n\n  test(\"ReplaceTable: Erases the table contents and changes the metadata.\") {\n    spark.sql(s\"CREATE TABLE testcat.table_name USING $orc2 AS SELECT id, data FROM source\")"
  }
]