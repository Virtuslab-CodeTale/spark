[
  {
    "id" : "3af4f0e4-0bba-43f2-bf35-34af796b6210",
    "prId" : 25536,
    "prUrl" : "https://github.com/apache/spark/pull/25536#pullrequestreview-278038221",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a061f8d9-9b89-46b4-9c63-03ade4c4ebf4",
        "parentId" : null,
        "authorId" : "1aba4faa-ea5e-4487-baf9-d552ca566126",
        "body" : "minor and non-blocking... \"table_name\" repeated many times and is it better to make it a test class variable and each test case referencing it?\r\n\r\nsorry maybe I'm being too nitpick lol\r\n\r\nPR looks good to me :)",
        "createdAt" : "2019-08-21T18:40:52Z",
        "updatedAt" : "2019-08-21T18:43:14Z",
        "lastEditedBy" : "1aba4faa-ea5e-4487-baf9-d552ca566126",
        "tags" : [
        ]
      },
      {
        "id" : "0da21d8a-9ca7-4878-9546-aaa553d8f88d",
        "parentId" : "a061f8d9-9b89-46b4-9c63-03ade4c4ebf4",
        "authorId" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "body" : "It's better to me if you don't have to jump around too much when reading the code.",
        "createdAt" : "2019-08-21T19:14:15Z",
        "updatedAt" : "2019-08-21T19:14:16Z",
        "lastEditedBy" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "tags" : [
        ]
      },
      {
        "id" : "7bc603ae-d438-4cc2-9cd0-0a6a47341cc7",
        "parentId" : "a061f8d9-9b89-46b4-9c63-03ade4c4ebf4",
        "authorId" : "1aba4faa-ea5e-4487-baf9-d552ca566126",
        "body" : "in your PR https://github.com/apache/spark/pull/25507/files `DataSourceV2DataFrameSessionCatalogSuite.scala`, there is a class variable `v2Format` being referenced by all test cases\r\n\r\n```\r\nprotected val v2Format: String = classOf[InMemoryTableProvider].getName\r\n```\r\n\r\nso I thought it's a style convention...",
        "createdAt" : "2019-08-21T19:59:05Z",
        "updatedAt" : "2019-08-21T19:59:06Z",
        "lastEditedBy" : "1aba4faa-ea5e-4487-baf9-d552ca566126",
        "tags" : [
        ]
      }
    ],
    "commit" : "655d07ef22bd3b775906fe69e04e4f8beb82a1a5",
    "line" : 78,
    "diffHunk" : "@@ -1,1 +510,514 @@        spark.sql(s\"CREATE TABLE $identifier USING foo AS SELECT 1 i\")\n\n        val table = catalog.loadTable(Identifier.of(Array(), \"table_name\"))\n\n        assert(table.name == identifier)"
  },
  {
    "id" : "6afb51d4-ab7c-47d1-887e-fce1a26a548b",
    "prId" : 25402,
    "prUrl" : "https://github.com/apache/spark/pull/25402#pullrequestreview-274667429",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "074d5627-67e1-4d43-8b5c-70632a62d146",
        "parentId" : null,
        "authorId" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "body" : "We can maintain this behavior, but I'd rather not, as the V2SessionCatalog can't properly handle views and such",
        "createdAt" : "2019-08-14T04:57:14Z",
        "updatedAt" : "2019-08-14T16:08:07Z",
        "lastEditedBy" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "tags" : [
        ]
      }
    ],
    "commit" : "a70e72676da6442a45e3a358c734b6f161c615d0",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +495,499 @@    sparkSession.sql(s\"CREATE TABLE table_name USING parquet AS SELECT id, data FROM source\")\n\n    checkAnswer(sparkSession.sql(s\"TABLE default.table_name\"), sparkSession.table(\"source\"))\n    // The fact that the following line doesn't throw an exception means, the session catalog\n    // can load the table."
  },
  {
    "id" : "abf06536-037d-4809-a47f-6251b5d502cf",
    "prId" : 25368,
    "prUrl" : "https://github.com/apache/spark/pull/25368#pullrequestreview-273496646",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fa0bffff-2bad-461c-a9a0-c69c4c3ade36",
        "parentId" : null,
        "authorId" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "body" : "`withSQLConf`? Otherwise you affect all tests below",
        "createdAt" : "2019-08-09T22:18:12Z",
        "updatedAt" : "2019-08-20T03:00:48Z",
        "lastEditedBy" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "tags" : [
        ]
      },
      {
        "id" : "c4f6f426-d0bf-4eaf-948b-c293f6514e75",
        "parentId" : "fa0bffff-2bad-461c-a9a0-c69c4c3ade36",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "This test suite will clear all the configs after each test, see https://github.com/apache/spark/pull/25368/files#diff-b49f76fba19ee10a28e0e61c4b44e1a0R62",
        "createdAt" : "2019-08-12T02:31:06Z",
        "updatedAt" : "2019-08-20T03:00:48Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "4895a6eab6ec449238192715b78fea61217f7f54",
    "line" : 83,
    "diffHunk" : "@@ -1,1 +201,205 @@\n  test(\"CreateTable: use default catalog for v2 sources when default catalog is set\") {\n    spark.conf.set(\"spark.sql.default.catalog\", \"testcat\")\n    spark.sql(s\"CREATE TABLE table_name (id bigint, data string) USING foo\")\n"
  },
  {
    "id" : "784345a2-3b09-49d2-8c50-392689437308",
    "prId" : 25368,
    "prUrl" : "https://github.com/apache/spark/pull/25368#pullrequestreview-273381475",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "41abbb19-3421-46ad-a6be-9d09559553a5",
        "parentId" : null,
        "authorId" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "body" : "ditto",
        "createdAt" : "2019-08-09T22:18:54Z",
        "updatedAt" : "2019-08-20T03:00:48Z",
        "lastEditedBy" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "tags" : [
        ]
      }
    ],
    "commit" : "4895a6eab6ec449238192715b78fea61217f7f54",
    "line" : 206,
    "diffHunk" : "@@ -1,1 +462,466 @@\n  test(\"CreateTableAsSelect: use default catalog for v2 sources when default catalog is set\") {\n    spark.conf.set(\"spark.sql.default.catalog\", \"testcat\")\n\n    val df = spark.createDataFrame(Seq((1L, \"a\"), (2L, \"b\"), (3L, \"c\"))).toDF(\"id\", \"data\")"
  },
  {
    "id" : "2ef211e4-8c77-44a6-8280-8bb6e6c38512",
    "prId" : 25247,
    "prUrl" : "https://github.com/apache/spark/pull/25247#pullrequestreview-273841918",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0b4abdbf-f474-4d90-a559-1fc7db76fb57",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "In current Spark, `SHOW TABLES FROM non-existing-db` would fail, shall we follow it?",
        "createdAt" : "2019-08-12T03:31:15Z",
        "updatedAt" : "2019-08-23T00:03:45Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "622962f2-ae1f-4648-9a26-4f00ab7908d8",
        "parentId" : "0b4abdbf-f474-4d90-a559-1fc7db76fb57",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "@cloud-fan as far as I understand, throwing `NoSuchNamespaceException` is optional in v2:\r\n```\r\n  /**\r\n   * List the tables in a namespace from the catalog.\r\n   * <p>\r\n   * If the catalog supports views, this must return identifiers for only tables and not views.\r\n   *\r\n   * @param namespace a multi-part namespace\r\n   * @return an array of Identifiers for tables\r\n   * @throws NoSuchNamespaceException If the namespace does not exist (optional).\r\n   */\r\n  Identifier[] listTables(String[] namespace) throws NoSuchNamespaceException;\r\n```\r\n\r\nI can update `TestInMemoryTableCatalog` to throw NoSuchNamespaceException if there is no namespace existing for the tables created. However, I am not sure if this is the right approach since you could have created namespace without tables - in v1, you could have done `CREATE DATABASE db` without creating tables belonging to `db`, although I don't think this scenario is supported in v2 yet.\r\n\r\nPlease advise how this needs to be handled. Thanks!\r\n\r\n",
        "createdAt" : "2019-08-12T04:24:06Z",
        "updatedAt" : "2019-08-23T00:03:45Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      },
      {
        "id" : "169930ad-ef53-42c9-8408-ecafd212ac54",
        "parentId" : "0b4abdbf-f474-4d90-a559-1fc7db76fb57",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "v2 can support `CREATE NAMESPACE`, we have the APIs in `SupportNamespace`.\r\n\r\nI think this is another case we should discuss: how much should Spark restrict the semantic of a SQL command? e.g. `SHOW TABLE catalog.nonExistingNS`, should Spark guarantee that, this command fails if the namespace doesn't exist?\r\n\r\ncc @brkyvz @rdblue ",
        "createdAt" : "2019-08-12T11:34:02Z",
        "updatedAt" : "2019-08-23T00:03:45Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "d4342d68-afaf-4d6b-9d98-965b4c095aa9",
        "parentId" : "0b4abdbf-f474-4d90-a559-1fc7db76fb57",
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "If the catalog throws `NoSuchNamespaceException`, then the query should fail. If not, then this should return whatever the catalog returned. Throwing `NoSuchNamespaceException` is optional, so I think Spark should respect the catalog's choice of whether to throw it or not in this case.",
        "createdAt" : "2019-08-12T17:35:23Z",
        "updatedAt" : "2019-08-23T00:03:45Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      }
    ],
    "commit" : "6f2fa4e7b8572a6b9dd243f1253e4962af27d40a",
    "line" : 63,
    "diffHunk" : "@@ -1,1 +1739,1743 @@\n  test(\"ShowTables: using v2 catalog, namespace doesn't exist\") {\n    runShowTablesSql(\"SHOW TABLES FROM testcat.unknown\", Seq())\n  }\n"
  },
  {
    "id" : "ff4f17f5-23a0-47ed-8412-1425c4856f59",
    "prId" : 25247,
    "prUrl" : "https://github.com/apache/spark/pull/25247#pullrequestreview-277539124",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "282a17c7-0209-4aa7-9f31-2a25ca0092fb",
        "parentId" : null,
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "This test is currently failing and will be updated based on the resolution of https://github.com/apache/spark/pull/25368/files#r315980292.",
        "createdAt" : "2019-08-21T03:16:21Z",
        "updatedAt" : "2019-08-23T00:03:45Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "6f2fa4e7b8572a6b9dd243f1253e4962af27d40a",
    "line" : 97,
    "diffHunk" : "@@ -1,1 +1773,1777 @@\n  test(\"ShowTables: namespace is not specified and default v2 catalog is set\") {\n    spark.conf.set(\"spark.sql.default.catalog\", \"testcat\")\n    spark.sql(\"CREATE TABLE testcat.table (id bigint, data string) USING foo\")\n"
  },
  {
    "id" : "c21e4382-a0de-410e-bab4-960de02ca74a",
    "prId" : 25115,
    "prUrl" : "https://github.com/apache/spark/pull/25115#pullrequestreview-274416477",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "22ff8292-0bf7-408a-9492-bf50afbb05f8",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "can we also test correlated subquery? ",
        "createdAt" : "2019-08-09T10:45:34Z",
        "updatedAt" : "2019-08-14T01:57:20Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "267ccaa6-b13b-485a-8082-7239c2ceee16",
        "parentId" : "22ff8292-0bf7-408a-9492-bf50afbb05f8",
        "authorId" : "8afcf008-6988-44a5-92fa-2bef54bb4058",
        "body" : "Is that necessary to test correlated subquery? Because correlated subquery is a subset of subquery and we forbid subquery here, then correlated subquery is also forbidden.\r\nMy thought is later I want to add pre-execution subquery for DELETE, but correlated subquery is still forbidden, so we can modify the test cases at that time.",
        "createdAt" : "2019-08-13T06:53:06Z",
        "updatedAt" : "2019-08-14T01:57:20Z",
        "lastEditedBy" : "8afcf008-6988-44a5-92fa-2bef54bb4058",
        "tags" : [
        ]
      },
      {
        "id" : "2d3b7c7c-a533-4876-959b-ec35bccfbf2b",
        "parentId" : "22ff8292-0bf7-408a-9492-bf50afbb05f8",
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "This sounds reasonable to me.",
        "createdAt" : "2019-08-13T16:47:43Z",
        "updatedAt" : "2019-08-14T01:57:20Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      }
    ],
    "commit" : "bbf515666495cbf5f12731b3cdab4a23960f3d77",
    "line" : 32,
    "diffHunk" : "@@ -1,1 +1891,1895 @@      sql(s\"INSERT INTO $t VALUES (2L, 'a', 2), (2L, 'b', 3), (3L, 'c', 3)\")\n      val exc = intercept[AnalysisException] {\n        sql(s\"DELETE FROM $t WHERE id IN (SELECT id FROM $t)\")\n      }\n"
  }
]