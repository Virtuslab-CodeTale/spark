[
  {
    "id" : "ef5b25b3-9f87-4584-b9be-7f3fa9548af0",
    "prId" : 30867,
    "prUrl" : "https://github.com/apache/spark/pull/30867#pullrequestreview-556090362",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e8c38dea-cb71-4f6e-a56b-f455d6ecaba0",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Thanks for enforcing this.",
        "createdAt" : "2020-12-21T01:51:16Z",
        "updatedAt" : "2020-12-21T07:20:26Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "4e82580cdc1b44a1680631ad54121e327f37b9e0",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +126,130 @@          assert(info.getExamples.endsWith(\"\\n  \"))\n          assert(info.getSince.matches(\"[0-9]+\\\\.[0-9]+\\\\.[0-9]+\"))\n          assert(info.getGroup.nonEmpty)\n\n          if (info.getArguments.nonEmpty) {"
  },
  {
    "id" : "0f169e33-88b2-40df-8745-5e5c2ff58f91",
    "prId" : 30400,
    "prUrl" : "https://github.com/apache/spark/pull/30400#pullrequestreview-535666051",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1f52d70e-efe1-4230-a4ad-1243982c4f57",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Shall we add this after `CurrentTimestamp` like the following?\r\n```scala\r\n\"org.apache.spark.sql.catalyst.expressions.CurrentTimestamp\",\r\n\"org.apache.spark.sql.catalyst.expressions.CurrentTimeZone\",\r\n\"org.apache.spark.sql.catalyst.expressions.Now\",\r\n```",
        "createdAt" : "2020-11-20T18:08:40Z",
        "updatedAt" : "2020-11-22T06:16:55Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "c83da46530ad1d4c5527a05e94c7fbad3eafba84",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +150,154 @@      \"org.apache.spark.sql.catalyst.expressions.CurrentDate\",\n      \"org.apache.spark.sql.catalyst.expressions.CurrentTimestamp\",\n      \"org.apache.spark.sql.catalyst.expressions.CurrentTimeZone\",\n      \"org.apache.spark.sql.catalyst.expressions.Now\",\n      // Random output without a seed"
  },
  {
    "id" : "ba0bb40a-67c5-4843-9fd1-6dfeb59547a3",
    "prId" : 29743,
    "prUrl" : "https://github.com/apache/spark/pull/29743#pullrequestreview-488436355",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9472e124-7596-40f9-899d-d7b8539f712b",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Looks these checks have already been done in the `ExpressionInfo` constructor?",
        "createdAt" : "2020-09-15T00:13:56Z",
        "updatedAt" : "2020-09-19T19:00:31Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "ed3bb42f-7185-438c-bb23-4fb14741012f",
        "parentId" : "9472e124-7596-40f9-899d-d7b8539f712b",
        "authorId" : "a8e23d47-3ae4-4385-848c-38a216d1bd08",
        "body" : "These ones are a bit more strict. \r\nFor example this extra space made the Example for `typeof` not appear in http://spark.apache.org/docs/latest/api/sql/#typeof\r\n\r\nhttps://github.com/apache/spark/pull/29743/files#diff-25282ab1377a3d87999d1d0d7a8ec270R208-R214\r\n\r\nI didn't want to add these checks to the constructor, because I'm afraid that they might break some UDFs and also I have no way to exclude some from the checks.",
        "createdAt" : "2020-09-15T03:11:54Z",
        "updatedAt" : "2020-09-19T19:00:31Z",
        "lastEditedBy" : "a8e23d47-3ae4-4385-848c-38a216d1bd08",
        "tags" : [
        ]
      },
      {
        "id" : "6c2961c6-6ff2-4eee-824d-b91ab5554481",
        "parentId" : "9472e124-7596-40f9-899d-d7b8539f712b",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "> I didn't want to add these checks to the constructor, because I'm afraid that they might break some UDFs and also I have no way to exclude some from the checks.\r\n\r\nIf we make the checks more strict in the ctor and we find existing expressions throwing exceptions, I think it is okay just to fix them. Any problem there? ",
        "createdAt" : "2020-09-15T07:58:50Z",
        "updatedAt" : "2020-09-19T19:00:31Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "1914b2bf-1fcb-48bc-9a70-ad3c531dcb49",
        "parentId" : "9472e124-7596-40f9-899d-d7b8539f712b",
        "authorId" : "a8e23d47-3ae4-4385-848c-38a216d1bd08",
        "body" : "No, problems with current ones - only some dummy functions in tests.",
        "createdAt" : "2020-09-15T08:16:04Z",
        "updatedAt" : "2020-09-19T19:00:31Z",
        "lastEditedBy" : "a8e23d47-3ae4-4385-848c-38a216d1bd08",
        "tags" : [
        ]
      }
    ],
    "commit" : "4162b41d325da3f9b732d835168a546916b50499",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +121,125 @@          assert(info.getExamples.startsWith(\"\\n    Examples:\\n\"))\n          assert(info.getExamples.endsWith(\"\\n  \"))\n          assert(info.getSince.matches(\"[0-9]+\\\\.[0-9]+\\\\.[0-9]+\"))\n\n          if (info.getArguments.nonEmpty) {"
  },
  {
    "id" : "87623518-07cd-4178-bd3a-0b49d9b7f4d0",
    "prId" : 29646,
    "prUrl" : "https://github.com/apache/spark/pull/29646#pullrequestreview-482460979",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "81acc58a-31a4-4a55-be5a-fc6909ed4fd2",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@maropu, I think the asserts in the constructor of `ExpressionInfo` would cover what this PR aims. About ensuring to have the `since` for new expressions, I think it's less likely that we miss adding `since` during the review of other PRs just given my experience.",
        "createdAt" : "2020-09-04T08:12:03Z",
        "updatedAt" : "2020-09-04T08:12:04Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "510ab184-b226-4a1e-b79d-3c7c68d54b14",
        "parentId" : "81acc58a-31a4-4a55-be5a-fc6909ed4fd2",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "okay, I will close this. Thanks for the check, @HyukjinKwon ",
        "createdAt" : "2020-09-04T08:28:50Z",
        "updatedAt" : "2020-09-04T08:28:50Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "21496cc98118d5377d776fa5058fc4a02f137c58",
    "line" : 79,
    "diffHunk" : "@@ -1,1 +267,271 @@      .map(spark.sessionState.catalog.lookupFunctionInfo)\n      .filterNot(e => ignoreSet.contains(e.getClassName))\n      .filter(funcInfo => !funcInfo.getSince.matches(\"[0-9]+\\\\.[0-9]+\\\\.[0-9]+\"))\n      .map(_.getClassName)\n      .distinct"
  },
  {
    "id" : "77b1dab7-b2f2-4295-acb8-86fdcb0da5f7",
    "prId" : 28626,
    "prUrl" : "https://github.com/apache/spark/pull/28626#pullrequestreview-419981654",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "489f328d-5041-4800-935d-7bccb8640beb",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "can we let `DivModLike` implement `nullSafeEval` instead?",
        "createdAt" : "2020-05-28T09:17:45Z",
        "updatedAt" : "2020-05-28T14:24:37Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "420db492-f61e-411f-a033-402dc4816424",
        "parentId" : "489f328d-5041-4800-935d-7bccb8640beb",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "Yes. This makes the code clear.",
        "createdAt" : "2020-05-28T10:35:42Z",
        "updatedAt" : "2020-05-28T14:24:37Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      }
    ],
    "commit" : "265abd3b8905613aa4b9878326f0004cae2baab7",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +167,171 @@    // Do not check these expressions, because these expressions extend NullIntolerant\n    // and override the eval method to avoid evaluating input1 if input2 is 0.\n    val ignoreSet = Set(classOf[IntegralDivide], classOf[Divide], classOf[Remainder], classOf[Pmod])\n\n    val candidateExprsToCheck = spark.sessionState.functionRegistry.listFunction()"
  },
  {
    "id" : "e3f091d3-cd95-422c-85eb-7d7de2dcbd0e",
    "prId" : 28308,
    "prUrl" : "https://github.com/apache/spark/pull/28308#pullrequestreview-398824559",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d65b98bf-29bd-4132-a9f0-bbbca4cd1445",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "To improve test coverage, a new test added here.",
        "createdAt" : "2020-04-23T07:23:14Z",
        "updatedAt" : "2020-04-23T07:42:46Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "15df98f11f1b73814d20044980cee196932d30e2",
    "line" : 60,
    "diffHunk" : "@@ -1,1 +58,62 @@  }\n\n  test(\"error handling in ExpressionInfo\") {\n    val errMsg1 = intercept[IllegalArgumentException] {\n      val invalidNote = \"  invalid note\""
  },
  {
    "id" : "560be378-e5e4-4ca5-8027-62c52026ee99",
    "prId" : 28308,
    "prUrl" : "https://github.com/apache/spark/pull/28308#pullrequestreview-398824724",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0f5c71b9-150a-42af-b0d9-3074c192830a",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "from `UDFSuite`",
        "createdAt" : "2020-04-23T07:23:28Z",
        "updatedAt" : "2020-04-23T07:42:46Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "15df98f11f1b73814d20044980cee196932d30e2",
    "line" : 31,
    "diffHunk" : "@@ -1,1 +29,33 @@class ExpressionInfoSuite extends SparkFunSuite with SharedSparkSession {\n\n  test(\"Replace _FUNC_ in ExpressionInfo\") {\n    val info = spark.sessionState.catalog.lookupFunctionInfo(FunctionIdentifier(\"upper\"))\n    assert(info.getName === \"upper\")"
  },
  {
    "id" : "3f07e176-f1cc-40bc-8103-089767c7e75c",
    "prId" : 28308,
    "prUrl" : "https://github.com/apache/spark/pull/28308#pullrequestreview-398824789",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "98144710-3699-4006-9545-4902bbcb0295",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "from `UDFSuite`",
        "createdAt" : "2020-04-23T07:23:34Z",
        "updatedAt" : "2020-04-23T07:42:46Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "15df98f11f1b73814d20044980cee196932d30e2",
    "line" : 42,
    "diffHunk" : "@@ -1,1 +40,44 @@  }\n\n  test(\"group info in ExpressionInfo\") {\n    val info = spark.sessionState.catalog.lookupFunctionInfo(FunctionIdentifier(\"sum\"))\n    assert(info.getGroup === \"agg_funcs\")"
  },
  {
    "id" : "49b08148-e5c4-4276-987b-f8ea98593e1a",
    "prId" : 28308,
    "prUrl" : "https://github.com/apache/spark/pull/28308#pullrequestreview-398824958",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3b70af36-c44a-4464-bdb7-249af1f63b2c",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "from `SQLQuerySuite`",
        "createdAt" : "2020-04-23T07:23:48Z",
        "updatedAt" : "2020-04-23T07:42:46Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "15df98f11f1b73814d20044980cee196932d30e2",
    "line" : 80,
    "diffHunk" : "@@ -1,1 +78,82 @@  }\n\n  test(\"using _FUNC_ instead of function names in examples\") {\n    val exampleRe = \"(>.*;)\".r\n    val setStmtRe = \"(?i)^(>\\\\s+set\\\\s+).+\".r"
  },
  {
    "id" : "81f38153-a0fc-4703-9da7-88e472e39041",
    "prId" : 28308,
    "prUrl" : "https://github.com/apache/spark/pull/28308#pullrequestreview-398825031",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7f225863-54a3-4751-bbd4-9757a0466074",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "from `SQLQuerySuite`",
        "createdAt" : "2020-04-23T07:23:54Z",
        "updatedAt" : "2020-04-23T07:42:46Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "15df98f11f1b73814d20044980cee196932d30e2",
    "line" : 107,
    "diffHunk" : "@@ -1,1 +105,109 @@  }\n\n  test(\"check outputs of expression examples\") {\n    def unindentAndTrim(s: String): String = {\n      s.replaceAll(\"\\n\\\\s+\", \"\\n\").trim"
  },
  {
    "id" : "7fc737b9-7f19-4f5b-b861-d06e82a59ab0",
    "prId" : 28308,
    "prUrl" : "https://github.com/apache/spark/pull/28308#pullrequestreview-398825537",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "aeec2de7-0e73-4fcd-8dda-6be59ea4d1cb",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "minor: I modified this line a bit as my IDE suggested.",
        "createdAt" : "2020-04-23T07:24:43Z",
        "updatedAt" : "2020-04-23T07:42:46Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "15df98f11f1b73814d20044980cee196932d30e2",
    "line" : 143,
    "diffHunk" : "@@ -1,1 +141,145 @@          val example = info.getExamples\n          checkExampleSyntax(example)\n          example.split(\"  > \").toList.foreach {\n            case exampleRe(sql, output) =>\n              val df = clonedSpark.sql(sql)"
  },
  {
    "id" : "c33b9309-6e35-4e33-946b-b50d6719d401",
    "prId" : 28308,
    "prUrl" : "https://github.com/apache/spark/pull/28308#pullrequestreview-422465569",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c75bbde1-797f-4c27-9dbd-2d4aaadd7e78",
        "parentId" : null,
        "authorId" : "8e87861a-4202-49a2-baf7-6d51f6aaa5a2",
        "body" : "Hi, @maropu this test case has a minor problem when running individually for machines in different timezones: expressions such as `FromUnixTime` are timezone aware. \r\nIn `SQLQuerySuite`, the timezone is set explicitly in `org.apache.spark.sql.QueryTest`, however `ExpressionInfoSuite` doesn't set timezone, thus fails this test case.",
        "createdAt" : "2020-06-02T08:10:39Z",
        "updatedAt" : "2020-06-02T08:10:40Z",
        "lastEditedBy" : "8e87861a-4202-49a2-baf7-6d51f6aaa5a2",
        "tags" : [
        ]
      },
      {
        "id" : "c2dc5b05-6e36-4cb7-aa99-858e6e35ebeb",
        "parentId" : "c75bbde1-797f-4c27-9dbd-2d4aaadd7e78",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@advancedxy, that was fixed SPARK-31725.",
        "createdAt" : "2020-06-02T08:35:31Z",
        "updatedAt" : "2020-06-02T08:35:31Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "2371bbff-99ca-4840-aa1f-843d3453f217",
        "parentId" : "c75bbde1-797f-4c27-9dbd-2d4aaadd7e78",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Have you checked the thread in https://github.com/apache/spark/pull/28538?",
        "createdAt" : "2020-06-02T08:36:15Z",
        "updatedAt" : "2020-06-02T08:36:16Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "5bf501f0-0d86-4a0a-9d67-402b67daf66e",
        "parentId" : "c75bbde1-797f-4c27-9dbd-2d4aaadd7e78",
        "authorId" : "8e87861a-4202-49a2-baf7-6d51f6aaa5a2",
        "body" : "@HyukjinKwon and @maropu thanks for the information, I was checking an older version of Spark 3.0",
        "createdAt" : "2020-06-02T08:42:57Z",
        "updatedAt" : "2020-06-02T08:42:57Z",
        "lastEditedBy" : "8e87861a-4202-49a2-baf7-6d51f6aaa5a2",
        "tags" : [
        ]
      }
    ],
    "commit" : "15df98f11f1b73814d20044980cee196932d30e2",
    "line" : 107,
    "diffHunk" : "@@ -1,1 +105,109 @@  }\n\n  test(\"check outputs of expression examples\") {\n    def unindentAndTrim(s: String): String = {\n      s.replaceAll(\"\\n\\\\s+\", \"\\n\").trim"
  }
]