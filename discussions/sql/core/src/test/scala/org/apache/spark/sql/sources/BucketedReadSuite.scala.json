[
  {
    "id" : "7e1a61ee-415e-4c65-b754-594a85f0fe66",
    "prId" : 31413,
    "prUrl" : "https://github.com/apache/spark/pull/31413#pullrequestreview-583056002",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d03a072f-58f7-499c-99a4-883e5f81934a",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "The code to check bucket pruning seems duplicated. Can we improve it?",
        "createdAt" : "2021-02-04T05:27:35Z",
        "updatedAt" : "2021-02-05T04:43:15Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "5e3d34ae-3309-49d4-9f93-c257c46817ab",
        "parentId" : "d03a072f-58f7-499c-99a4-883e5f81934a",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "@cloud-fan - sure, improved to remove several statements.",
        "createdAt" : "2021-02-04T05:52:43Z",
        "updatedAt" : "2021-02-05T04:43:15Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      }
    ],
    "commit" : "03120af6b31af89dbc9fb9aad05045e98d52c699",
    "line" : 29,
    "diffHunk" : "@@ -1,1 +161,165 @@\n          val bucketColumnType = bucketedDataFrame.schema.apply(bucketColumnIndex).dataType\n          val rowsWithInvalidBuckets = fileScan.execute().filter(row => {\n            // Return rows should have been pruned\n            val bucketColumnValue = row.get(bucketColumnIndex, bucketColumnType)"
  },
  {
    "id" : "9fc9ede8-5002-4f55-b0f4-e4ef2b20c960",
    "prId" : 30941,
    "prUrl" : "https://github.com/apache/spark/pull/30941#pullrequestreview-559018317",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e76ec2c6-8419-4bb8-b861-2a711c58081f",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@Ngone51, seems like we still disable it here though. Can you mention this in PR description?",
        "createdAt" : "2020-12-28T06:11:40Z",
        "updatedAt" : "2020-12-28T06:11:40Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "92d04979-042a-43d5-98bc-0642d2c83cec",
        "parentId" : "e76ec2c6-8419-4bb8-b861-2a711c58081f",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Sure!",
        "createdAt" : "2020-12-28T07:00:24Z",
        "updatedAt" : "2020-12-28T07:00:24Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "d6be7c4b577f5b47b9ad8f965eb46f5e48f4b36b",
    "line" : 43,
    "diffHunk" : "@@ -1,1 +933,937 @@    withSQLConf(\n      SQLConf.COALESCE_BUCKETS_IN_JOIN_ENABLED.key -> \"true\",\n      SQLConf.ADAPTIVE_EXECUTION_ENABLED.key -> \"false\") {\n      // The side with bucketedTableTestSpec1 will be coalesced to have 4 output partitions.\n      // Currently, sort will be introduced for the side that is coalesced."
  },
  {
    "id" : "b06d06b7-b163-4ca4-9d6f-a864ab555558",
    "prId" : 28123,
    "prUrl" : "https://github.com/apache/spark/pull/28123#pullrequestreview-399617707",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d9825ea1-a876-43b6-9bf8-f149316159ae",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "We need to add more exhaustive tests for this optimization by using `COALESCE_BUCKET_IN_JOIN_MAX_NUM_BUCKETS_DIFF `, e.g., boundary config value tests.",
        "createdAt" : "2020-04-14T01:31:02Z",
        "updatedAt" : "2020-06-19T04:52:18Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "5f33ca9f-9577-449e-8e61-d9c5634be7c1",
        "parentId" : "d9825ea1-a876-43b6-9bf8-f149316159ae",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "Still working on this, will update tests in the next iteration.",
        "createdAt" : "2020-04-21T02:37:33Z",
        "updatedAt" : "2020-06-19T04:52:18Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      },
      {
        "id" : "45fcd630-97b8-4299-8d63-d380c2ac8c8b",
        "parentId" : "d9825ea1-a876-43b6-9bf8-f149316159ae",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "Added more tests.",
        "createdAt" : "2020-04-24T03:35:30Z",
        "updatedAt" : "2020-06-19T04:52:18Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "62a04a3e4d94e63b3787533f619e368a7e8d59f6",
    "line" : 55,
    "diffHunk" : "@@ -1,1 +876,880 @@  }\n\n  test(\"bucket coalescing eliminates shuffle\") {\n    withSQLConf(SQLConf.COALESCE_BUCKETS_IN_SORT_MERGE_JOIN_ENABLED.key -> \"true\") {\n      // The side with bucketedTableTestSpec1 will be coalesced to have 4 output partitions."
  },
  {
    "id" : "23a60dd2-4554-4b19-be85-0d4e60703c1c",
    "prId" : 27842,
    "prUrl" : "https://github.com/apache/spark/pull/27842#pullrequestreview-371557294",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "46730124-1951-4c34-8765-90dd733f09fa",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Could you add more tests, e.g., orderBy cases?",
        "createdAt" : "2020-03-08T00:33:40Z",
        "updatedAt" : "2020-03-10T02:57:27Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "3ec7c6dd-4231-4560-a0f6-54ec61152bcb",
        "parentId" : "46730124-1951-4c34-8765-90dd733f09fa",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "Do you mean something like the following?\r\n```\r\nval t1 = spark.range(10).selectExpr(\"id as k1\").orderBy(\"k1\")\r\n  .selectExpr(\"k1 as k11\").orderBy(\"k11\")\r\nval t2 = spark.range(10).selectExpr(\"id as k2\").orderBy(\"k2\")\r\nt1.join(t2, t1(\"k11\") === t2(\"k2\")).explain(true)\r\n```\r\nExtra `Sort` is optimized away, so it doesn't affect the physical plan (no extra sort).",
        "createdAt" : "2020-03-08T03:31:09Z",
        "updatedAt" : "2020-03-10T02:57:27Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      },
      {
        "id" : "5c8c7ace-dd88-4821-9090-49af2b0f3b19",
        "parentId" : "46730124-1951-4c34-8765-90dd733f09fa",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Ah, I see. How about the aggregate case?",
        "createdAt" : "2020-03-08T07:11:45Z",
        "updatedAt" : "2020-03-10T02:57:27Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "b9b0682d-1bdc-4ff9-876b-900bb7fa78b6",
        "parentId" : "46730124-1951-4c34-8765-90dd733f09fa",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "I added a test for `SortAggregateExec`. `HashAggregateExec` and `ObjectAggregateExec` are not affected since ordering is not involved with them.",
        "createdAt" : "2020-03-09T21:57:55Z",
        "updatedAt" : "2020-03-10T02:57:27Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "fc6bc62356ef1a4bd23a69d5a73a5c7c78995c03",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +608,612 @@    withSQLConf(SQLConf.AUTO_BROADCASTJOIN_THRESHOLD.key -> \"0\") {\n      withTable(\"t\") {\n        df1.repartition(1).write.format(\"parquet\").bucketBy(8, \"i\").sortBy(\"i\").saveAsTable(\"t\")\n        val t1 = spark.table(\"t\")\n        val t2 = t1.selectExpr(\"i as ii\")"
  },
  {
    "id" : "fad7912a-bdd1-4f90-988f-d366ca5e74f0",
    "prId" : 25328,
    "prUrl" : "https://github.com/apache/spark/pull/25328#pullrequestreview-271399549",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8efbf312-0da5-482d-bfd3-948ca789e544",
        "parentId" : null,
        "authorId" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "body" : "Should we do a \"store and recover the old conf\" instead?",
        "createdAt" : "2019-08-06T14:32:43Z",
        "updatedAt" : "2019-08-07T05:56:48Z",
        "lastEditedBy" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "tags" : [
        ]
      },
      {
        "id" : "5d9ce5c7-6d83-4249-9c9d-0de35fb84379",
        "parentId" : "8efbf312-0da5-482d-bfd3-948ca789e544",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "In the test, we assume that every test suite will keep the shared `SparkSession` clean after tests are run. So the old conf should be the default conf here, and we only need to call `unsetConf` to restore the default config.",
        "createdAt" : "2019-08-06T14:45:35Z",
        "updatedAt" : "2019-08-07T05:56:48Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "bf9b2617f81dd778d0d1355a66c1e664e7bc1f3b",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +56,60 @@\n  protected override def afterAll(): Unit = {\n    spark.sessionState.conf.unsetConf(SQLConf.LEGACY_BUCKETED_TABLE_SCAN_OUTPUT_ORDERING)\n    super.afterAll()\n  }"
  },
  {
    "id" : "fa120ae8-1685-420f-9eed-27cb5f446042",
    "prId" : 24957,
    "prUrl" : "https://github.com/apache/spark/pull/24957#pullrequestreview-254305710",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "064d26c3-ee4a-4310-91c6-d16578a26e16",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "trigger the test and see whether it passes in 2.4?",
        "createdAt" : "2019-06-25T21:13:59Z",
        "updatedAt" : "2019-06-25T21:14:00Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "dba8c7e8-c3bd-4a4e-94ea-c0dd34061b0d",
        "parentId" : "064d26c3-ee4a-4310-91c6-d16578a26e16",
        "authorId" : "677aa336-324b-4b93-8300-21f8bd378f26",
        "body" : "@parthchandra can you confirm this? Thanks.",
        "createdAt" : "2019-06-25T22:30:41Z",
        "updatedAt" : "2019-06-25T22:30:42Z",
        "lastEditedBy" : "677aa336-324b-4b93-8300-21f8bd378f26",
        "tags" : [
        ]
      },
      {
        "id" : "308d0dff-3810-4fa8-b6d8-688ce59c44f8",
        "parentId" : "064d26c3-ee4a-4310-91c6-d16578a26e16",
        "authorId" : "8ecac3da-eead-4324-8801-d31743255784",
        "body" : "Yes. The test passes in 2.4 for both bucketing with/without hive.",
        "createdAt" : "2019-06-25T22:38:08Z",
        "updatedAt" : "2019-06-25T22:38:08Z",
        "lastEditedBy" : "8ecac3da-eead-4324-8801-d31743255784",
        "tags" : [
        ]
      }
    ],
    "commit" : "17e0e2a7c161f7192993367bb408a339e764c3b5",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +740,744 @@  //  overflow if the files list is stored in a recursive data structure\n  //  This test is ignored because it takes long to run (~3 min)\n  ignore(\"SPARK-27100 stack overflow: read data with large partitions\") {\n    val nCount = 20000\n    // reshuffle data so that many small files are created"
  },
  {
    "id" : "fff2809e-680e-4b5c-9048-52d4dbe91ab6",
    "prId" : 24865,
    "prUrl" : "https://github.com/apache/spark/pull/24865#pullrequestreview-252025133",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3ee18818-e16e-493a-b8e0-59102dd1f3cd",
        "parentId" : null,
        "authorId" : "677aa336-324b-4b93-8300-21f8bd378f26",
        "body" : "remove extra new line",
        "createdAt" : "2019-06-19T23:51:11Z",
        "updatedAt" : "2019-06-22T00:32:47Z",
        "lastEditedBy" : "677aa336-324b-4b93-8300-21f8bd378f26",
        "tags" : [
        ]
      }
    ],
    "commit" : "1cd73c375da4a72822ea3355170fd4af6ff622c5",
    "line" : 53,
    "diffHunk" : "@@ -1,1 +785,789 @@    }\n  }\n\n}"
  }
]