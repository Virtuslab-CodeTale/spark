[
  {
    "id" : "bcc7c038-83c6-4dd8-af17-bfc03b5969ec",
    "prId" : 31326,
    "prUrl" : "https://github.com/apache/spark/pull/31326#pullrequestreview-575706762",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "64cddf27-0983-4db8-8dc1-622a5ab9f384",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Now, the `In-Memory` catalog behaves similarly to Hive external catalog, so, we don't need to distinguish them in tests.",
        "createdAt" : "2021-01-25T18:30:25Z",
        "updatedAt" : "2021-01-25T18:30:26Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "b5e7763e-0125-4def-a332-faef2c7f4ee6",
        "parentId" : "64cddf27-0983-4db8-8dc1-622a5ab9f384",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "shall we do it in master branch as well?",
        "createdAt" : "2021-01-25T18:35:51Z",
        "updatedAt" : "2021-01-25T18:35:51Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "fc19f5d3-6f45-4bde-aded-f3341fdf8c78",
        "parentId" : "64cddf27-0983-4db8-8dc1-622a5ab9f384",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "In master, we already have common settings in unified tests:\r\nhttps://github.com/apache/spark/blob/861f8bb5fb82e53a223ae121737fd6d54ab8ba52/sql/core/src/test/scala/org/apache/spark/sql/execution/command/v1/AlterTableDropPartitionSuite.scala#L36",
        "createdAt" : "2021-01-25T18:49:01Z",
        "updatedAt" : "2021-01-25T18:49:01Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "9171b0b47f7e0ca0be655ff40e570c1e5572b2c8",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +1737,1741 @@    createTablePartition(catalog, Map(\"a\" -> null, \"b\" -> null), tableIdent)\n    assert(catalog.listPartitions(tableIdent).map(_.spec).toSet ==\n      Set(Map(\"a\" -> \"__HIVE_DEFAULT_PARTITION__\", \"b\" -> \"__HIVE_DEFAULT_PARTITION__\")))\n    sql(\"ALTER TABLE tab1 DROP PARTITION (a = null, b = null)\")\n    assert(catalog.listPartitions(tableIdent).isEmpty)"
  },
  {
    "id" : "5d35ec75-89f2-4751-860a-033db6ed9a6e",
    "prId" : 30862,
    "prUrl" : "https://github.com/apache/spark/pull/30862#pullrequestreview-556215347",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dc6f269c-6dbc-4ee7-9c1a-16fd3cfdb87c",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Why did you replace `NoSuchTableException `?",
        "createdAt" : "2020-12-20T16:27:37Z",
        "updatedAt" : "2020-12-21T02:12:30Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "65302b44-9fe7-4c2f-8dac-1ec6e7c72cb4",
        "parentId" : "dc6f269c-6dbc-4ee7-9c1a-16fd3cfdb87c",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "Unresolved table is now handled here:\r\nhttps://github.com/apache/spark/blob/8e2633962f789a6ba5eb9448596f6ac4b7b1c2ff/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala#L104-L105\r\n",
        "createdAt" : "2020-12-21T02:19:42Z",
        "updatedAt" : "2020-12-21T02:20:10Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      },
      {
        "id" : "0a7c9097-bcda-4670-a79a-2b637d95b76c",
        "parentId" : "dc6f269c-6dbc-4ee7-9c1a-16fd3cfdb87c",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "in that case, I believe we should throw `NoSuchTableException` here. ",
        "createdAt" : "2020-12-21T08:10:44Z",
        "updatedAt" : "2020-12-21T08:11:46Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "c8c7cc50-6ef3-46b0-97ec-57e53681edc0",
        "parentId" : "dc6f269c-6dbc-4ee7-9c1a-16fd3cfdb87c",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "otherwise with such approach, you will lift all specific exception to the general one - `AnalysisException`.",
        "createdAt" : "2020-12-21T08:13:23Z",
        "updatedAt" : "2020-12-21T08:13:23Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "bd4eeaa8-5cd5-4668-9eac-2e5b68f0a898",
        "parentId" : "dc6f269c-6dbc-4ee7-9c1a-16fd3cfdb87c",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "let's fix it separately.",
        "createdAt" : "2020-12-21T08:14:31Z",
        "updatedAt" : "2020-12-21T08:14:31Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "38947fb0-af81-4e0f-a926-e02bb2b58919",
        "parentId" : "dc6f269c-6dbc-4ee7-9c1a-16fd3cfdb87c",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "Sounds good. I will do a follow-up PR.",
        "createdAt" : "2020-12-21T08:46:28Z",
        "updatedAt" : "2020-12-21T08:46:39Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "9894f9eaf5806f4bfe5961ba3608a2344bef6942",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +1643,1647 @@\n    // table to alter does not exist\n    val e = intercept[AnalysisException] {\n      sql(\"ALTER TABLE does_not_exist PARTITION (c='3') RENAME TO PARTITION (c='333')\")\n    }"
  },
  {
    "id" : "598e71cf-16a4-47dd-b575-9928759293f2",
    "prId" : 29552,
    "prUrl" : "https://github.com/apache/spark/pull/29552#pullrequestreview-475893318",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e5e1b05d-66b4-4779-8338-491e76c8c37a",
        "parentId" : null,
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "can we use `Trash.getCurrentTrashDir` instead? semantically it is the same but hides details such as homedir and others.",
        "createdAt" : "2020-08-26T19:23:48Z",
        "updatedAt" : "2020-08-30T07:26:31Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "080ade7e-6117-4702-be83-71de2ecc342f",
        "parentId" : "e5e1b05d-66b4-4779-8338-491e76c8c37a",
        "authorId" : "a88f50e2-6440-4a6c-8883-3130888d05bc",
        "body" : "is `Trash.getCurrentTrashDir` available for haddop2.7? I cannot find it",
        "createdAt" : "2020-08-26T19:26:15Z",
        "updatedAt" : "2020-08-30T07:26:31Z",
        "lastEditedBy" : "a88f50e2-6440-4a6c-8883-3130888d05bc",
        "tags" : [
        ]
      },
      {
        "id" : "e50d9749-62b5-4ae5-907b-14d991f00609",
        "parentId" : "e5e1b05d-66b4-4779-8338-491e76c8c37a",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Eh it is in 2.7.4 and up. Maybe you can use `Trash.getTrashPolicy().getCurrentTrashDir()` instead.",
        "createdAt" : "2020-08-26T20:15:45Z",
        "updatedAt" : "2020-08-30T07:26:31Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "9c709dd3-d516-42e8-bab9-52074d57631a",
        "parentId" : "e5e1b05d-66b4-4779-8338-491e76c8c37a",
        "authorId" : "a88f50e2-6440-4a6c-8883-3130888d05bc",
        "body" : "I am afraid i couldn't find this as well :(  is it not recommended to use `getHomeDirectory` ? I see we have used it at some places before like [this](https://github.com/apache/spark/blob/2dee4352a0a3578818f1e364ee324fc012fbb91d/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/Client.scala#L187)",
        "createdAt" : "2020-08-26T20:30:57Z",
        "updatedAt" : "2020-08-30T07:26:31Z",
        "lastEditedBy" : "a88f50e2-6440-4a6c-8883-3130888d05bc",
        "tags" : [
        ]
      },
      {
        "id" : "1f3e640a-2980-4288-be50-fe72ccd7520a",
        "parentId" : "e5e1b05d-66b4-4779-8338-491e76c8c37a",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Hmm okay. This is mostly a nit as this is only used in UT, so it's fine from my side.",
        "createdAt" : "2020-08-26T22:00:30Z",
        "updatedAt" : "2020-08-30T07:26:31Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      }
    ],
    "commit" : "2dd78a4b2793356f2a66bec9f67e0b4aa80d3aa9",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +3117,3121 @@\n        val fs = tablePath.getFileSystem(hadoopConf)\n        val trashCurrent = new Path(fs.getHomeDirectory, \".Trash/Current\")\n        val trashPath = Path.mergePaths(trashCurrent, tablePath)\n        assert(!fs.exists(trashPath))"
  },
  {
    "id" : "0096b8e5-78aa-44eb-b5da-27a68f963b93",
    "prId" : 29550,
    "prUrl" : "https://github.com/apache/spark/pull/29550#pullrequestreview-475612812",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "894719a8-a72e-458a-bb79-ff0d6baad50d",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Can we make a fake file system (that extends `FileSystem`) and use a temporary directory?",
        "createdAt" : "2020-08-26T14:39:43Z",
        "updatedAt" : "2020-08-26T14:40:08Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "52ed43d6-8f34-485f-8ddd-893bb36e9db7",
        "parentId" : "894719a8-a72e-458a-bb79-ff0d6baad50d",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "+1 for @HyukjinKwon 's suggestion.",
        "createdAt" : "2020-08-26T14:51:57Z",
        "updatedAt" : "2020-08-26T14:51:58Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "3a175492-a971-4bf4-9b92-01b2a3eac245",
        "parentId" : "894719a8-a72e-458a-bb79-ff0d6baad50d",
        "authorId" : "a88f50e2-6440-4a6c-8883-3130888d05bc",
        "body" : "Hii @HyukjinKwon, but `Trash.moveToAppropriateTrash` will always move the data to `trashCurrent`. can you guide me on what you mean by using a temporary directory instead.",
        "createdAt" : "2020-08-26T15:35:55Z",
        "updatedAt" : "2020-08-26T15:35:55Z",
        "lastEditedBy" : "a88f50e2-6440-4a6c-8883-3130888d05bc",
        "tags" : [
        ]
      }
    ],
    "commit" : "7c9fc86414bc311dc00a6bd3b92133ddd96f97e7",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +3117,3121 @@\n        val fs = tablePath.getFileSystem(hadoopConf)\n        val trashCurrent = new Path(fs.getTrashRoot(tablePath), \"Current\")\n        val trashPath = Path.mergePaths(trashCurrent, tablePath)\n        assert(!fs.exists(trashPath))"
  },
  {
    "id" : "4cc3ede3-8370-481f-9804-38281b1cf22c",
    "prId" : 29387,
    "prUrl" : "https://github.com/apache/spark/pull/29387#pullrequestreview-472960445",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e6e75848-af18-4a17-b9c2-076fd5f921c6",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "@Udbhav30 . It's always good to have complete test coverage for your patch. Please add negative test cases.\r\n1. When  `spark.sql.truncate.trash.enabled` is `false`, this should be no-op.\r\n2. When `fs.trash.interval=-1`, this should be no-op. ",
        "createdAt" : "2020-08-22T16:06:23Z",
        "updatedAt" : "2020-08-24T22:01:57Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "0fee20e4-710c-4bf6-af61-a7ae37a8be1a",
        "parentId" : "e6e75848-af18-4a17-b9c2-076fd5f921c6",
        "authorId" : "a88f50e2-6440-4a6c-8883-3130888d05bc",
        "body" : "sure thanks, i will add more tests",
        "createdAt" : "2020-08-22T16:59:00Z",
        "updatedAt" : "2020-08-24T22:01:57Z",
        "lastEditedBy" : "a88f50e2-6440-4a6c-8883-3130888d05bc",
        "tags" : [
        ]
      },
      {
        "id" : "e78a740e-658b-48b5-a0b9-fe25e1c50514",
        "parentId" : "e6e75848-af18-4a17-b9c2-076fd5f921c6",
        "authorId" : "a88f50e2-6440-4a6c-8883-3130888d05bc",
        "body" : "Hi @dongjoon-hyun , I have added tests as you suggested.",
        "createdAt" : "2020-08-22T19:49:02Z",
        "updatedAt" : "2020-08-24T22:01:57Z",
        "lastEditedBy" : "a88f50e2-6440-4a6c-8883-3130888d05bc",
        "tags" : [
        ]
      },
      {
        "id" : "f95288a6-694e-454f-b8c9-d9cf16554f93",
        "parentId" : "e6e75848-af18-4a17-b9c2-076fd5f921c6",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Thanks!",
        "createdAt" : "2020-08-22T20:30:17Z",
        "updatedAt" : "2020-08-24T22:01:57Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "a2df53b48db372ed8f9a303cd8c0c499e33f3adf",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +3106,3110 @@    val trashIntervalKey = \"fs.trash.interval\"\n    withTable(\"tab1\") {\n      withSQLConf(SQLConf.TRUNCATE_TRASH_ENABLED.key -> \"true\") {\n        sql(\"CREATE TABLE tab1 (col INT) USING parquet\")\n        sql(\"INSERT INTO tab1 SELECT 1\")"
  },
  {
    "id" : "2ae0a411-f296-4670-80db-2082412de267",
    "prId" : 29387,
    "prUrl" : "https://github.com/apache/spark/pull/29387#pullrequestreview-472972410",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2eba8dd2-72ce-4c0a-8389-d9dda251a4f7",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "All the FS support this operation? ",
        "createdAt" : "2020-08-22T19:08:31Z",
        "updatedAt" : "2020-08-24T22:01:57Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "39f13e2a-1c07-42a2-8a43-32642ebe48a4",
        "parentId" : "2eba8dd2-72ce-4c0a-8389-d9dda251a4f7",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Yes a default impl is defined in `FileSystem` which calls `getHomeDirectory` implemented in the same class.\r\n\r\nEven though it is supported, it seems the trash mechanism is less useful in cloud object stores like S3, where renaming doesn't exist and therefore moving to trash is much more expensive. However user can disable that by the configs given here and in Hadoop itself.",
        "createdAt" : "2020-08-23T01:30:56Z",
        "updatedAt" : "2020-08-24T22:01:57Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      }
    ],
    "commit" : "a2df53b48db372ed8f9a303cd8c0c499e33f3adf",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +3117,3121 @@\n        val fs = tablePath.getFileSystem(hadoopConf)\n        val trashRoot = fs.getTrashRoot(tablePath)\n        assert(!fs.exists(trashRoot))\n        try {"
  },
  {
    "id" : "580c0185-754c-45f5-b6a3-31896635cc00",
    "prId" : 29387,
    "prUrl" : "https://github.com/apache/spark/pull/29387#pullrequestreview-473869562",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "44b0c630-55fa-4faf-820c-1a1cb3174cdc",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Can we use `spark.sessionState.newHadoopConf()`? Then, we can remove `scalastyle:off hadoopConfiguration`.",
        "createdAt" : "2020-08-24T21:16:02Z",
        "updatedAt" : "2020-08-24T22:01:57Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "ba6e1c30-a8f8-486d-8e16-d186a9b09b3d",
        "parentId" : "44b0c630-55fa-4faf-820c-1a1cb3174cdc",
        "authorId" : "a88f50e2-6440-4a6c-8883-3130888d05bc",
        "body" : "I tried to use 'spark.sessionState.newHadoopConf()' but the hadoop conf was not reflected, so ''moveToAppropriateTrash' returns false as 'fs.trash.interval' is 0",
        "createdAt" : "2020-08-24T21:21:20Z",
        "updatedAt" : "2020-08-24T22:01:57Z",
        "lastEditedBy" : "a88f50e2-6440-4a6c-8883-3130888d05bc",
        "tags" : [
        ]
      },
      {
        "id" : "d08569cb-555d-4753-85de-5c83bb8310fb",
        "parentId" : "44b0c630-55fa-4faf-820c-1a1cb3174cdc",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "The other places are the same. Let's try to following the Scalastyle warning if possible.",
        "createdAt" : "2020-08-24T21:25:22Z",
        "updatedAt" : "2020-08-24T22:01:57Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "e3d73d97-ed6f-4a9f-a869-d0c5ab3f7824",
        "parentId" : "44b0c630-55fa-4faf-820c-1a1cb3174cdc",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Oh, should we change this underlying one?",
        "createdAt" : "2020-08-24T21:28:27Z",
        "updatedAt" : "2020-08-24T22:01:57Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "a2df53b48db372ed8f9a303cd8c0c499e33f3adf",
    "line" : 40,
    "diffHunk" : "@@ -1,1 +3138,3142 @@        sql(\"INSERT INTO tab1 SELECT 1\")\n        // scalastyle:off hadoopconfiguration\n        val hadoopConf = spark.sparkContext.hadoopConfiguration\n        // scalastyle:on hadoopconfiguration\n        val originalValue = hadoopConf.get(trashIntervalKey, \"0\")"
  },
  {
    "id" : "9cf8c828-3140-4de3-9b6a-f0d25e6e0e62",
    "prId" : 29387,
    "prUrl" : "https://github.com/apache/spark/pull/29387#pullrequestreview-473868476",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5882778a-a8e8-47b3-997e-f553f2b064ab",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "`spark.sessionState.newHadoopConf()`?",
        "createdAt" : "2020-08-24T21:26:26Z",
        "updatedAt" : "2020-08-24T22:01:57Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "a2df53b48db372ed8f9a303cd8c0c499e33f3adf",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +3110,3114 @@        sql(\"INSERT INTO tab1 SELECT 1\")\n        // scalastyle:off hadoopconfiguration\n        val hadoopConf = spark.sparkContext.hadoopConfiguration\n        // scalastyle:on hadoopconfiguration\n        val originalValue = hadoopConf.get(trashIntervalKey, \"0\")"
  },
  {
    "id" : "591e886c-b84c-4331-aade-7349bf28261b",
    "prId" : 29387,
    "prUrl" : "https://github.com/apache/spark/pull/29387#pullrequestreview-473877908",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e35c8ed6-7843-48d1-905b-c740e9d78faf",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "In the other tests, this PR is using `spark.sparkContext.hadoopConfiguration`.\r\nIf that is required, this test case looks misleading, `withSQLConf(SQLConf.TRUNCATE_TRASH_ENABLED.key -> \"false\") {` is required here? I'm wondering if this test case passed with `withSQLConf(SQLConf.TRUNCATE_TRASH_ENABLED.key -> \"true\") {`, too.",
        "createdAt" : "2020-08-24T21:31:13Z",
        "updatedAt" : "2020-08-24T22:01:57Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "1f616617-a4a9-4a14-bd7a-c4c37dcd8ec3",
        "parentId" : "e35c8ed6-7843-48d1-905b-c740e9d78faf",
        "authorId" : "a88f50e2-6440-4a6c-8883-3130888d05bc",
        "body" : "Hii, In this test we did not update the `hadoopConf`, so using `spark.sessionState.newHadoopConf()` doesn't make any difference",
        "createdAt" : "2020-08-24T21:36:02Z",
        "updatedAt" : "2020-08-24T22:01:57Z",
        "lastEditedBy" : "a88f50e2-6440-4a6c-8883-3130888d05bc",
        "tags" : [
        ]
      },
      {
        "id" : "5129a41b-aec9-4dd0-8169-b18a40e5dd59",
        "parentId" : "e35c8ed6-7843-48d1-905b-c740e9d78faf",
        "authorId" : "a88f50e2-6440-4a6c-8883-3130888d05bc",
        "body" : "@dongjoon-hyun  See [here](https://github.com/apache/hadoop/blob/64f36b9543c011ce2f1f7d1e10da0eab88a0759d/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/TrashPolicyDefault.java#L125). If `fs.trash.interval` is non positive then `moveToAppropriateTrash ` function returns false. So to test this I have to add positive value to `fs.trash.interval`, but `spark.sessionState.newHadoopConf()` does not update the `hadoopConf` and so other testcase fails. And here this testcase is no-op so updating the `hadoopConf` is not required so I used `spark.sessionState.newHadoopConf()` ",
        "createdAt" : "2020-08-24T21:43:41Z",
        "updatedAt" : "2020-08-24T22:04:02Z",
        "lastEditedBy" : "a88f50e2-6440-4a6c-8883-3130888d05bc",
        "tags" : [
        ]
      }
    ],
    "commit" : "a2df53b48db372ed8f9a303cd8c0c499e33f3adf",
    "line" : 65,
    "diffHunk" : "@@ -1,1 +3163,3167 @@        sql(\"CREATE TABLE tab1 (col INT) USING parquet\")\n        sql(\"INSERT INTO tab1 SELECT 1\")\n        val hadoopConf = spark.sessionState.newHadoopConf()\n        val tablePath = new Path(spark.sessionState.catalog\n          .getTableMetadata(TableIdentifier(\"tab1\")).storage.locationUri.get)"
  },
  {
    "id" : "952f8677-243e-4398-8c51-d06ed40f4488",
    "prId" : 29387,
    "prUrl" : "https://github.com/apache/spark/pull/29387#pullrequestreview-475514237",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "21bff3fe-4180-48c2-8c80-986554ffdc32",
        "parentId" : null,
        "authorId" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "body" : "@Udbhav30 This line of code is not Mac os friendly, the `trashRoot` is `/Users/xxx/.Trash/`, it is the path to the trash can of Mac os. So normally, it exists...",
        "createdAt" : "2020-08-26T13:09:41Z",
        "updatedAt" : "2020-08-26T13:09:42Z",
        "lastEditedBy" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "tags" : [
        ]
      },
      {
        "id" : "9848bca9-c02b-494d-bd49-69ba956b08e2",
        "parentId" : "21bff3fe-4180-48c2-8c80-986554ffdc32",
        "authorId" : "a88f50e2-6440-4a6c-8883-3130888d05bc",
        "body" : "Thanks @LuciferYang for pointing it out, i will raise follow-up PR and assert the particular folder that is `trashRoot/pathToTable/tab1` in this case instead of `trashRoot`",
        "createdAt" : "2020-08-26T13:55:39Z",
        "updatedAt" : "2020-08-26T13:55:40Z",
        "lastEditedBy" : "a88f50e2-6440-4a6c-8883-3130888d05bc",
        "tags" : [
        ]
      }
    ],
    "commit" : "a2df53b48db372ed8f9a303cd8c0c499e33f3adf",
    "line" : 48,
    "diffHunk" : "@@ -1,1 +3146,3150 @@        val fs = tablePath.getFileSystem(hadoopConf)\n        val trashRoot = fs.getTrashRoot(tablePath)\n        assert(!fs.exists(trashRoot))\n        try {\n          hadoopConf.set(trashIntervalKey, \"0\")"
  },
  {
    "id" : "ad3d7547-2d95-4408-beb4-9a1eaf5f5052",
    "prId" : 28840,
    "prUrl" : "https://github.com/apache/spark/pull/28840#pullrequestreview-449417149",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "60d6a5d1-be14-48b2-a204-39d55a8ceb8d",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "It's a bit weird if we don't fail invalid functions when creating, but fail when refreshing it. How hard is it to make `REFRESH TABLE` lazy?",
        "createdAt" : "2020-07-14T12:57:57Z",
        "updatedAt" : "2020-07-21T12:12:06Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "e429f22a-b951-4100-a65e-8e8c363593d7",
        "parentId" : "60d6a5d1-be14-48b2-a204-39d55a8ceb8d",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "I don't know why we not check function during create. It seems no use to create a not exists function but can produce some problem like typo.\r\n\r\nThe same command, Hive failed directly `create function f1 as 'test.non.exists.udf'`.",
        "createdAt" : "2020-07-14T13:42:59Z",
        "updatedAt" : "2020-07-21T12:12:06Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      },
      {
        "id" : "7bb5e205-15fe-4fbd-81a6-f8553e581820",
        "parentId" : "60d6a5d1-be14-48b2-a204-39d55a8ceb8d",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "We can fix CREATE FUNCTION later and update this test.",
        "createdAt" : "2020-07-14T14:20:58Z",
        "updatedAt" : "2020-07-21T12:12:06Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "3ee2bcff-47cc-420f-8cf5-28bf376c8881",
        "parentId" : "60d6a5d1-be14-48b2-a204-39d55a8ceb8d",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "or is it because you are calling the internal API not the `CREATE FUNCTION` command?",
        "createdAt" : "2020-07-14T14:21:34Z",
        "updatedAt" : "2020-07-21T12:12:06Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "dd426386-8394-45bc-9f87-959801f5a6d7",
        "parentId" : "60d6a5d1-be14-48b2-a204-39d55a8ceb8d",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "If we make `REFRESH FUNCTION` lazy as `CREATE FUNCTION`, something like this\r\n```\r\nif (catalog.isRegisteredFunction(identifier)) {\r\n  catalog.unregisterFunction(identifier)\r\n}\r\nif (!catalog.isPersistentFunction(identifier)) {\r\n  throw new NoSuchFunctionException(identifier.database.get, functionName)\r\n}\r\n```\r\n\r\nThe different thing is we don't register/check function and the register/check action happened when user query with this function like `select func(f)`.\r\n\r\nI think it might be better to do the function check right now.",
        "createdAt" : "2020-07-14T23:58:52Z",
        "updatedAt" : "2020-07-21T12:12:06Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      },
      {
        "id" : "07e905f5-7174-465c-babe-41d12425677e",
        "parentId" : "60d6a5d1-be14-48b2-a204-39d55a8ceb8d",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Maybe we should make `CREATE FUNCTION` not lazy, in a new PR.",
        "createdAt" : "2020-07-15T18:45:51Z",
        "updatedAt" : "2020-07-21T12:12:06Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "ecf21d1f-e396-4bc3-a579-689f6661414d",
        "parentId" : "60d6a5d1-be14-48b2-a204-39d55a8ceb8d",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "ok, I will try it after this pr finished.",
        "createdAt" : "2020-07-15T23:49:31Z",
        "updatedAt" : "2020-07-21T12:12:06Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      }
    ],
    "commit" : "b18437c5998f5df50172732015f66b33c3908d2a",
    "line" : 49,
    "diffHunk" : "@@ -1,1 +3065,3069 @@\n      val function = CatalogFunction(func, \"test.non.exists.udf\", Seq.empty)\n      spark.sessionState.catalog.createFunction(function, false)\n      assert(!spark.sessionState.catalog.isRegisteredFunction(func))\n      val err = intercept[AnalysisException] {"
  },
  {
    "id" : "c0bb9ee5-58c0-4142-86a0-587516d348b4",
    "prId" : 28840,
    "prUrl" : "https://github.com/apache/spark/pull/28840#pullrequestreview-468103817",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5ee27f44-ae36-4a72-8e14-d0da54c4ebdc",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "This is the only positive test case. Could you think more and try to cover more cases?\r\n\r\n",
        "createdAt" : "2020-08-16T23:21:31Z",
        "updatedAt" : "2020-08-16T23:21:31Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      }
    ],
    "commit" : "b18437c5998f5df50172732015f66b33c3908d2a",
    "line" : 38,
    "diffHunk" : "@@ -1,1 +3054,3058 @@      sql(\"CREATE FUNCTION func1 AS 'test.org.apache.spark.sql.MyDoubleAvg'\")\n      assert(!spark.sessionState.catalog.isRegisteredFunction(func))\n      sql(\"REFRESH FUNCTION func1\")\n      assert(spark.sessionState.catalog.isRegisteredFunction(func))\n"
  },
  {
    "id" : "1c7bae49-ea92-429d-b553-d105158ec867",
    "prId" : 28634,
    "prUrl" : "https://github.com/apache/spark/pull/28634#pullrequestreview-417749392",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "780f45c9-9aa9-4c04-aced-29211507618a",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I have the impression that Spark conf is immutable. Are you sure this works?",
        "createdAt" : "2020-05-25T14:25:05Z",
        "updatedAt" : "2020-05-25T14:25:05Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "5eed683c-5a31-4ded-965c-2af62e155255",
        "parentId" : "780f45c9-9aa9-4c04-aced-29211507618a",
        "authorId" : "ad4cb313-47b2-4cfa-9c1f-0a3596794a67",
        "body" : "Yes this works. I have verified this by printing the value of this conf inside the main code in AlterTableRecoverPartitions class. Both of the tests prints different values - 0 and 10.\r\n\r\nAlso there are many tests which modify conf using \"sc.conf.set\". Ex - TaskSetManagerSuite, RDDSuite etc.",
        "createdAt" : "2020-05-25T14:32:41Z",
        "updatedAt" : "2020-05-25T14:32:41Z",
        "lastEditedBy" : "ad4cb313-47b2-4cfa-9c1f-0a3596794a67",
        "tags" : [
        ]
      }
    ],
    "commit" : "f6900b32b361777ec32e27ec6a8df33c068b1f4d",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +1207,1211 @@      RDD_PARALLEL_LISTING_THRESHOLD)\n    try {\n      spark.sparkContext.conf.set(RDD_PARALLEL_LISTING_THRESHOLD.key, \"10\")\n      testRecoverPartitions()\n    } finally {"
  },
  {
    "id" : "97d680ff-98d9-4836-9276-6b24e7a2195a",
    "prId" : 27923,
    "prUrl" : "https://github.com/apache/spark/pull/27923#pullrequestreview-374966840",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "844bb9b1-e388-4162-85ae-4f0834a9ddae",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "this only affects partition path, or table path as well?",
        "createdAt" : "2020-03-16T07:08:32Z",
        "updatedAt" : "2020-03-16T07:08:32Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "37fcd976-8fa0-4d2c-8f1f-6383935cb7a7",
        "parentId" : "844bb9b1-e388-4162-85ae-4f0834a9ddae",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Both, but can be more likely to happen on partition path since there's much more than table path.",
        "createdAt" : "2020-03-16T07:27:40Z",
        "updatedAt" : "2020-03-16T07:27:40Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "cd846270a2307ae07c3abecd0b476f678cb010bf",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +2100,2104 @@        // execute without java.io.FileNotFoundException\n        sql(\"TRUNCATE TABLE tab1\")\n        // partition path should be re-created\n        assert(path.exists())\n      }"
  },
  {
    "id" : "30133b70-c741-4972-a830-6fe861353de9",
    "prId" : 27548,
    "prUrl" : "https://github.com/apache/spark/pull/27548#pullrequestreview-357820153",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "08830662-6ecf-46c6-89fa-c8eb5efec406",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Why do you need to add a name?",
        "createdAt" : "2020-02-12T21:59:43Z",
        "updatedAt" : "2020-02-12T21:59:44Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "06142e82-d89b-47e1-91c9-e566a1faecef",
        "parentId" : "08830662-6ecf-46c6-89fa-c8eb5efec406",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Without a name it conflicts with the user ACL I added below.",
        "createdAt" : "2020-02-12T22:01:16Z",
        "updatedAt" : "2020-02-12T22:01:16Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "7568db7d05b6f873b4ee63c5324ebe70ba99a2cf",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +2043,2047 @@          val customAcl = new java.util.ArrayList[AclEntry]()\n          customAcl.add(new AclEntry.Builder()\n            .setName(\"test\")\n            .setType(AclEntryType.USER)\n            .setScope(AclEntryScope.ACCESS)"
  },
  {
    "id" : "bdbbe7e3-3882-4f5d-a820-ba246c3e61e0",
    "prId" : 27175,
    "prUrl" : "https://github.com/apache/spark/pull/27175#pullrequestreview-341535831",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "504c4953-8f47-40eb-afaa-740378c5da94",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "This is a loose condition. I think it is rarely to have default file permission as `rwxrwxrwx`. Another option is not to check permission when `ignore` is true.",
        "createdAt" : "2020-01-11T18:43:48Z",
        "updatedAt" : "2020-01-11T18:43:48Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "52067a37848b8e6d0ac1aa367c8bb19ff83c683c",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +2053,2057 @@          val fileStatus2 = fs.getFileStatus(tablePath)\n          if (ignore) {\n            assert(fileStatus2.getPermission().toString() != \"rwxrwxrwx\")\n          } else {\n            assert(fileStatus2.getPermission().toString() == \"rwxrwxrwx\")"
  }
]