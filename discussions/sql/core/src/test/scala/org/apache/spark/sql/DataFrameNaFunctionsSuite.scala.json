[
  {
    "id" : "d8ed72b6-1eb0-4945-9cad-574febda66de",
    "prId" : 31769,
    "prUrl" : "https://github.com/apache/spark/pull/31769#pullrequestreview-606812438",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2059c1fe-d275-45d2-85b3-ff1c3c5963e7",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Ah, I see. Thanks for the update.",
        "createdAt" : "2021-03-08T23:48:47Z",
        "updatedAt" : "2021-03-09T05:46:30Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "58679bd9e27bb376fd9e6ea2baa780fe3454f23a",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +462,466 @@  }\n\n  test(\"SPARK-34417: test fillMap() for column with a dot in the name\") {\n    val na = \"n/a\"\n    checkAnswer("
  },
  {
    "id" : "bad5897d-1238-4cb1-b422-c988d553f686",
    "prId" : 31545,
    "prUrl" : "https://github.com/apache/spark/pull/31545#pullrequestreview-588326886",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1dcc854f-dc41-4352-bab7-21bc341be6eb",
        "parentId" : null,
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "I think this will fail if you have `Map(\"Col\" -> na)` because of the same reason due to Line 424 in `DataFrameNaFunctions.scala`?",
        "createdAt" : "2021-02-11T03:44:26Z",
        "updatedAt" : "2021-03-02T01:41:51Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      },
      {
        "id" : "069e9834-c802-408b-af1e-8f0cb3e29094",
        "parentId" : "1dcc854f-dc41-4352-bab7-21bc341be6eb",
        "authorId" : "2aac12c0-5b21-45d8-99f0-6e7832629f6d",
        "body" : "No, it does not fail. Since column is successfully resolved at ```df.resolve(colName)```, it will not go to 424.\r\nOnly for the data frame columns not present in the na-fill-map, it will go to line 424.",
        "createdAt" : "2021-02-11T04:45:16Z",
        "updatedAt" : "2021-03-02T01:41:51Z",
        "lastEditedBy" : "2aac12c0-5b21-45d8-99f0-6e7832629f6d",
        "tags" : [
        ]
      },
      {
        "id" : "dafae276-cb65-4f86-acd7-6c95308568a2",
        "parentId" : "1dcc854f-dc41-4352-bab7-21bc341be6eb",
        "authorId" : "2aac12c0-5b21-45d8-99f0-6e7832629f6d",
        "body" : "@imback82 i think your point is that if data frame has a column having dot in the name, but it is not part of the null fill map then it will fail.\r\nYes, it will fail, example below.\r\n```import org.apache.spark.sql.SparkSession\r\n\r\nobject ColumnNameWithDot {\r\n\r\n  def main(args: Array[String]): Unit = {\r\n\r\n    val spark = SparkSession.builder.appName(\"Simple Application\")\r\n      .config(\"spark.master\", \"local\").getOrCreate()\r\n\r\n    spark.sparkContext.setLogLevel(\"OFF\")\r\n\r\n    import spark.implicits._\r\n    val df = Seq((\"abc\", 23), (\"def\", 44), (null, 0)).toDF(\"ColWith.Dot\", \"Col.2\")\r\n    df.na.fill(Map(\"`ColWith.Dot`\" -> \"na\"))\r\n      .show()\r\n  }\r\n}",
        "createdAt" : "2021-02-11T05:23:42Z",
        "updatedAt" : "2021-03-02T01:41:51Z",
        "lastEditedBy" : "2aac12c0-5b21-45d8-99f0-6e7832629f6d",
        "tags" : [
        ]
      },
      {
        "id" : "92f499c2-4605-4af6-a433-2bd0e666f6a2",
        "parentId" : "1dcc854f-dc41-4352-bab7-21bc341be6eb",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "Can you run this test after changing it to `Map(\"Col\" -> na)`? I see the following failure, which should be handled:\r\n```\r\nsbt:spark-sql> testOnly *DataFrameNaFunctionsSuite -- -z \"SPARK-34417\"\r\n[info] - SPARK-34417 - test fillMap() for column with a dot in the name *** FAILED *** (2 seconds, 274 milliseconds)\r\n[info]   org.apache.spark.sql.AnalysisException: Cannot resolve column name \"ColWith.Dot\" among (ColWith.Dot, Col); did you mean to quote the `ColWith.Dot` column?\r\n[info]   at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$resolveException(Dataset.scala:272)\r\n[info]   at org.apache.spark.sql.Dataset.$anonfun$resolve$1(Dataset.scala:263)\r\n...\r\n```",
        "createdAt" : "2021-02-11T05:25:54Z",
        "updatedAt" : "2021-03-02T01:41:51Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      },
      {
        "id" : "a1279d75-9174-4671-bcef-c50519839bf0",
        "parentId" : "1dcc854f-dc41-4352-bab7-21bc341be6eb",
        "authorId" : "2aac12c0-5b21-45d8-99f0-6e7832629f6d",
        "body" : "Handled the above failure.",
        "createdAt" : "2021-02-11T08:33:12Z",
        "updatedAt" : "2021-03-02T01:41:51Z",
        "lastEditedBy" : "2aac12c0-5b21-45d8-99f0-6e7832629f6d",
        "tags" : [
        ]
      }
    ],
    "commit" : "896652c34aaf8ecc9bcaaa4f81ccb149dd0c4a76",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +466,470 @@    checkAnswer(\n      Seq((\"abc\", 23L), (\"def\", 44L), (null, 0L)).toDF(\"ColWith.Dot\", \"Col\")\n        .na.fill(Map(\"`ColWith.Dot`\" -> na)),\n      Row(\"abc\", 23) :: Row(\"def\", 44L) :: Row(na, 0L) :: Nil)\n  }"
  },
  {
    "id" : "c80304a3-11ac-4f9e-99e1-5430e28d3037",
    "prId" : 28266,
    "prUrl" : "https://github.com/apache/spark/pull/28266#pullrequestreview-396064141",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "aa48521a-2d0b-4251-80a3-f0a172e6c91c",
        "parentId" : null,
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "Note that this was the behavior in Spark 2.4.4. We can handle this more gracefully (e.g., use `outputAttributes`) if we need to.\r\n\r\nOn a side note, for `fill`, `*` is ignored in Spark 2.4.4.",
        "createdAt" : "2020-04-19T21:30:13Z",
        "updatedAt" : "2020-04-19T21:31:12Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "283fee179186aa4cf7ad51e1f45149764139794c",
    "line" : 39,
    "diffHunk" : "@@ -1,1 +297,301 @@      df.na.drop(\"any\", Seq(\"*\"))\n    }\n    assert(exception.getMessage.contains(\"Cannot resolve column name \\\"*\\\"\"))\n  }\n"
  },
  {
    "id" : "3f4cd036-0c89-4198-ab31-d9a12b0308d2",
    "prId" : 28266,
    "prUrl" : "https://github.com/apache/spark/pull/28266#pullrequestreview-396064141",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7f8de42a-1b97-40e6-950e-bf990381501c",
        "parentId" : null,
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "Note that nested columns are ignored for `fill` in Spark 2.4.4.",
        "createdAt" : "2020-04-19T21:31:04Z",
        "updatedAt" : "2020-04-19T21:31:12Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "283fee179186aa4cf7ad51e1f45149764139794c",
    "line" : 52,
    "diffHunk" : "@@ -1,1 +304,308 @@\n    // Nested columns are ignored for fill().\n    checkAnswer(df.na.fill(\"a1\", Seq(\"c1.c1-1\")), df)\n  }\n"
  },
  {
    "id" : "670b14d2-6eee-4faa-89f4-e38f92e7cd39",
    "prId" : 28266,
    "prUrl" : "https://github.com/apache/spark/pull/28266#pullrequestreview-396113756",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "32f095cb-1db9-4c1a-9dd1-6bc0c90054b7",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "nit: This looks like a bug, so could you add the prefix: `SPARK-31256`.",
        "createdAt" : "2020-04-20T02:59:45Z",
        "updatedAt" : "2020-04-20T02:59:45Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "36fc9606-1851-463a-a4f4-68d314b0b129",
        "parentId" : "32f095cb-1db9-4c1a-9dd1-6bc0c90054b7",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "nvm, a bit late...",
        "createdAt" : "2020-04-20T03:00:06Z",
        "updatedAt" : "2020-04-20T03:00:06Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "a7b5f162-646c-4d99-b820-bea243d8509a",
        "parentId" : "32f095cb-1db9-4c1a-9dd1-6bc0c90054b7",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "ðŸ˜‚",
        "createdAt" : "2020-04-20T03:08:56Z",
        "updatedAt" : "2020-04-20T03:08:57Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "283fee179186aa4cf7ad51e1f45149764139794c",
    "line" : 57,
    "diffHunk" : "@@ -1,1 +307,311 @@  }\n\n  test(\"drop with nested columns\") {\n    val df = createDFWithNestedColumns\n"
  },
  {
    "id" : "55ef6fcc-53ee-43a7-a10b-456702ef1a69",
    "prId" : 26738,
    "prUrl" : "https://github.com/apache/spark/pull/26738#pullrequestreview-326119092",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7f83d2bd-3fbc-471d-ae2c-e22b2652076b",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Do we have to use boxed type? how about\r\n```\r\nSeq((1, 1L, 1.toShort, 1.toByte, 1.0f, 1.0), (0, 0L, ...)).toDF...\r\n```",
        "createdAt" : "2019-12-03T11:12:31Z",
        "updatedAt" : "2019-12-03T13:55:45Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "59d293a1-eccd-44e8-abd7-c5c557627617",
        "parentId" : "7f83d2bd-3fbc-471d-ae2c-e22b2652076b",
        "authorId" : "6aa4fe6e-d8b4-470d-9ebd-61d8303b117f",
        "body" : "I was following the rest of the file's conventions. Happy to update",
        "createdAt" : "2019-12-03T11:14:17Z",
        "updatedAt" : "2019-12-03T13:55:45Z",
        "lastEditedBy" : "6aa4fe6e-d8b4-470d-9ebd-61d8303b117f",
        "tags" : [
        ]
      },
      {
        "id" : "55c38d3b-f7b7-4d16-aec5-428c5a42e1cb",
        "parentId" : "7f83d2bd-3fbc-471d-ae2c-e22b2652076b",
        "authorId" : "6aa4fe6e-d8b4-470d-9ebd-61d8303b117f",
        "body" : "Just tried it this way and it fails with `sbt.ForkMain$ForkError: java.lang.NoClassDefFoundError: no Java class corresponding to Product with Serializable found`\r\nSo we do need to use boxed type",
        "createdAt" : "2019-12-03T11:22:26Z",
        "updatedAt" : "2019-12-03T13:55:45Z",
        "lastEditedBy" : "6aa4fe6e-d8b4-470d-9ebd-61d8303b117f",
        "tags" : [
        ]
      },
      {
        "id" : "c946a18d-45ec-47ce-9791-e43e8bb543e9",
        "parentId" : "7f83d2bd-3fbc-471d-ae2c-e22b2652076b",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "OK we can keep the type declaration, but can we use `1L`, `1.toShort`, etc. when writing the values?",
        "createdAt" : "2019-12-03T11:47:22Z",
        "updatedAt" : "2019-12-03T13:55:45Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "21a33cf9-9669-4aac-b6e7-28fa7bbe72cf",
        "parentId" : "7f83d2bd-3fbc-471d-ae2c-e22b2652076b",
        "authorId" : "6aa4fe6e-d8b4-470d-9ebd-61d8303b117f",
        "body" : "Definitely can do so for the values, updated.",
        "createdAt" : "2019-12-03T12:59:58Z",
        "updatedAt" : "2019-12-03T13:55:45Z",
        "lastEditedBy" : "6aa4fe6e-d8b4-470d-9ebd-61d8303b117f",
        "tags" : [
        ]
      }
    ],
    "commit" : "b3709a16e89284584922995f4877c31b5095aafe",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +40,44 @@  def createNaNDF(): DataFrame = {\n    Seq[(java.lang.Integer, java.lang.Long, java.lang.Short,\n      java.lang.Byte, java.lang.Float, java.lang.Double)](\n      (1, 1L, 1.toShort, 1.toByte, 1.0f, 1.0),\n      (0, 0L, 0.toShort, 0.toByte, Float.NaN, Double.NaN)"
  }
]