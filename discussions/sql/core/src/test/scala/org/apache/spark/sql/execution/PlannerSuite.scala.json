[
  {
    "id" : "0d4edbcb-f7c4-4ad8-9d1b-ebb976b6f526",
    "prId" : 30300,
    "prUrl" : "https://github.com/apache/spark/pull/30300#pullrequestreview-529257217",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1232ea56-e5de-4f42-bc48-30b303038244",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Ah, last my nit comment: could you check the query use `RangePartitioning` correctly in this test (I have the same comment for the other tests, too)?",
        "createdAt" : "2020-11-12T13:06:19Z",
        "updatedAt" : "2020-11-16T06:54:00Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "276301bd-c423-4810-8808-93c2c4973ab0",
        "parentId" : "1232ea56-e5de-4f42-bc48-30b303038244",
        "authorId" : "ad4cb313-47b2-4cfa-9c1f-0a3596794a67",
        "body" : "@maropu you mean to add assertions that the partitioning here is RangePartitioning and not something else?",
        "createdAt" : "2020-11-12T14:40:27Z",
        "updatedAt" : "2020-11-16T06:54:00Z",
        "lastEditedBy" : "ad4cb313-47b2-4cfa-9c1f-0a3596794a67",
        "tags" : [
        ]
      },
      {
        "id" : "e30f7fee-c7f4-42b3-9d4c-73aeab19db9a",
        "parentId" : "1232ea56-e5de-4f42-bc48-30b303038244",
        "authorId" : "ad4cb313-47b2-4cfa-9c1f-0a3596794a67",
        "body" : "Added checks to assert partitioning is what we expect.",
        "createdAt" : "2020-11-12T16:21:08Z",
        "updatedAt" : "2020-11-16T06:54:00Z",
        "lastEditedBy" : "ad4cb313-47b2-4cfa-9c1f-0a3596794a67",
        "tags" : [
        ]
      }
    ],
    "commit" : "16e1db202f3522ebe607894b93811bb54bdd9a0c",
    "line" : 70,
    "diffHunk" : "@@ -1,1 +962,966 @@      // if Project normalizes alias in its Range outputPartitioning, then no Exchange should come\n      // in between HashAggregates\n      val planned = df.queryExecution.executedPlan\n      val exchanges = planned.collect { case s: ShuffleExchangeExec => s }\n      assert(exchanges.isEmpty)"
  },
  {
    "id" : "94d84ef7-db4c-4c96-a3d2-59d3a28731ea",
    "prId" : 30300,
    "prUrl" : "https://github.com/apache/spark/pull/30300#pullrequestreview-550129516",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0206e27c-515f-49a3-96e7-e18fa5d4a504",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "not related to this PR: The `ProjectExec` only outputs `t1id` (after column pruning), and it's a bit redundant to return `PartitioningCollection` here, as `t1id` is the only output and other partitionings are just invalid.",
        "createdAt" : "2020-12-09T16:36:10Z",
        "updatedAt" : "2020-12-09T16:36:10Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "1c8a4845-a475-432d-b12c-fc40f8142460",
        "parentId" : "0206e27c-515f-49a3-96e7-e18fa5d4a504",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Oh, it looks interesting. Hi, @prakharjain09, are you interested in the improvement above?",
        "createdAt" : "2020-12-11T13:19:40Z",
        "updatedAt" : "2020-12-11T13:19:40Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "28f229f7-c4d3-4843-85ba-b1b8523b3cf9",
        "parentId" : "0206e27c-515f-49a3-96e7-e18fa5d4a504",
        "authorId" : "ad4cb313-47b2-4cfa-9c1f-0a3596794a67",
        "body" : "@maropu Sure. Basically the idea is to stop propagating partitionings and  sortOrders corresponding to attributes which are not part of outputset?\r\n\r\nWorking on this as part of https://issues.apache.org/jira/browse/SPARK-33758.",
        "createdAt" : "2020-12-11T14:10:50Z",
        "updatedAt" : "2020-12-11T14:10:50Z",
        "lastEditedBy" : "ad4cb313-47b2-4cfa-9c1f-0a3596794a67",
        "tags" : [
        ]
      }
    ],
    "commit" : "16e1db202f3522ebe607894b93811bb54bdd9a0c",
    "line" : 27,
    "diffHunk" : "@@ -1,1 +919,923 @@        assert(projects.exists(_.outputPartitioning match {\n          case PartitioningCollection(Seq(HashPartitioning(Seq(k1: AttributeReference), _),\n            HashPartitioning(Seq(k2: AttributeReference), _))) if k1.name == \"t1id\" =>\n            true\n          case _ => false"
  },
  {
    "id" : "4a675b2f-a5b8-431d-a6f7-63d3112ff9cd",
    "prId" : 28885,
    "prUrl" : "https://github.com/apache/spark/pull/28885#pullrequestreview-438794438",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a53ddff8-3f39-4430-a991-ef4b29091be7",
        "parentId" : null,
        "authorId" : "3d4870da-39a8-4406-b00d-131930d14cd8",
        "body" : "If this were the same as the other child of SortMergeJoinExec then no reuse would be required.",
        "createdAt" : "2020-06-21T08:45:56Z",
        "updatedAt" : "2021-03-19T12:14:59Z",
        "lastEditedBy" : "3d4870da-39a8-4406-b00d-131930d14cd8",
        "tags" : [
        ]
      },
      {
        "id" : "dad91612-7cab-4936-8d94-36c9b6c03beb",
        "parentId" : "a53ddff8-3f39-4430-a991-ef4b29091be7",
        "authorId" : "def2dd9f-ce27-41ce-8156-f8225ddc68a9",
        "body" : "I don't follow this part, could you please let me know how copying here makes it different? ",
        "createdAt" : "2020-06-27T11:34:11Z",
        "updatedAt" : "2021-03-19T12:14:59Z",
        "lastEditedBy" : "def2dd9f-ce27-41ce-8156-f8225ddc68a9",
        "tags" : [
        ]
      },
      {
        "id" : "25302c3f-7ba3-4cf3-9c0f-59a194fdec6a",
        "parentId" : "a53ddff8-3f39-4430-a991-ef4b29091be7",
        "authorId" : "3d4870da-39a8-4406-b00d-131930d14cd8",
        "body" : "Yes, so I think the whole point of exchange reuse is to find different exchange instances which result the same data and keep only one instance and reuse its output where we can (i.e. call `execute`/`executeBroadcast` multiple times on one exchange instance instead of call `execute`/`executeBroadcast` once on each instances). In this example the 2 children points to the same exchange instance (`shuffle`) so there is no point in reuse here.\r\n(We could wrap one of them in a `ReusedExcahngeExec` node, but it wouldn't make any difference from performance point of view.)",
        "createdAt" : "2020-06-28T10:28:36Z",
        "updatedAt" : "2021-03-19T12:14:59Z",
        "lastEditedBy" : "3d4870da-39a8-4406-b00d-131930d14cd8",
        "tags" : [
        ]
      },
      {
        "id" : "1d89fc7e-b997-4bdb-ae20-ad860b1e16fa",
        "parentId" : "a53ddff8-3f39-4430-a991-ef4b29091be7",
        "authorId" : "def2dd9f-ce27-41ce-8156-f8225ddc68a9",
        "body" : "Makes sense, although I think the reuse exchange logic that we have doesn't check if the instances are the same, and will replace it with a reuse exchange anyway, but good to have a different instance here. ",
        "createdAt" : "2020-06-28T16:28:16Z",
        "updatedAt" : "2021-03-19T12:14:59Z",
        "lastEditedBy" : "def2dd9f-ce27-41ce-8156-f8225ddc68a9",
        "tags" : [
        ]
      }
    ],
    "commit" : "7187ebd2e053570d92017100dba4a0738fa2f014",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +463,467 @@      None,\n      shuffle,\n      shuffle.copy())\n\n    val outputPlan = ReuseExchangeAndSubquery.apply(inputPlan)"
  },
  {
    "id" : "611145bc-a9a1-4cf7-ab39-8e1cc6b62d5a",
    "prId" : 27842,
    "prUrl" : "https://github.com/apache/spark/pull/27842#pullrequestreview-371691263",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8f5a3243-e018-4703-9ade-c353faba4f65",
        "parentId" : null,
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "FYI, this is 5 without the fix.",
        "createdAt" : "2020-03-10T00:25:55Z",
        "updatedAt" : "2020-03-10T02:57:27Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      },
      {
        "id" : "fc8b44e1-5d1c-4970-b5d6-f73dae649f18",
        "parentId" : "8f5a3243-e018-4703-9ade-c353faba4f65",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "why do we have sorts between `SortAggregateExec` and `SortMergeJoinExec`?",
        "createdAt" : "2020-03-10T05:25:33Z",
        "updatedAt" : "2020-03-10T05:25:33Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "594ba4f9-b11c-4a28-994f-9bdc21bccd8a",
        "parentId" : "8f5a3243-e018-4703-9ade-c353faba4f65",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "Oh, nothing between them. Two sorts under `SortAggregateExec`:\r\n```\r\n   +- SortAggregate(key=[k2#224L], functions=[collect_list(k2#224L, 0, 0)], output=[k2#224L, collect_list(k2)#236])\r\n      +- *(5) Sort [k2#224L ASC NULLS FIRST], false, 0\r\n         +- Exchange hashpartitioning(k2#224L, 5), true, [id=#151]\r\n            +- SortAggregate(key=[k2#224L], functions=[partial_collect_list(k2#224L, 0, 0)], output=[k2#224L, buf#254])\r\n               +- *(4) Sort [k2#224L ASC NULLS FIRST], false, 0\r\n                  +- *(4) Project [FLOOR((cast(id#222L as double) / 4.0)) AS k2#224L]\r\n                     +- *(4) Range (0, 20, step=1, splits=2)\r\n```",
        "createdAt" : "2020-03-10T05:34:31Z",
        "updatedAt" : "2020-03-10T05:34:32Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      },
      {
        "id" : "02be0123-5700-4776-96bf-4082a3a3e174",
        "parentId" : "8f5a3243-e018-4703-9ade-c353faba4f65",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "ah i see",
        "createdAt" : "2020-03-10T05:36:08Z",
        "updatedAt" : "2020-03-10T05:36:08Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "fc6bc62356ef1a4bd23a69d5a73a5c7c78995c03",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +991,995 @@        // We expect two SortExec nodes on each side of join.\n        val sorts = planned.collect { case s: SortExec => s }\n        assert(sorts.size == 4)\n      }\n    }"
  },
  {
    "id" : "0655ca3a-b6e2-4794-bdf3-1fba70e11506",
    "prId" : 26946,
    "prUrl" : "https://github.com/apache/spark/pull/26946#pullrequestreview-336127632",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c70663ad-312d-4a2d-80e8-9d2b56a91ad3",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I'm not very sure about this. `df.repartition` is a low-level API that allows users to hash-partition the data. There is also a `df.repartitionByRange` to do range partitioning. We shouldn't break users' expectations.",
        "createdAt" : "2019-12-24T07:52:17Z",
        "updatedAt" : "2019-12-24T08:48:46Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "5093247d-dec9-4d01-bcac-aa74baa48fe4",
        "parentId" : "c70663ad-312d-4a2d-80e8-9d2b56a91ad3",
        "authorId" : "0f4ef4e8-09be-436f-b743-bf8fbc343490",
        "body" : "HashPartitioning or RangePartitioning won't take effect if it is followed by OrderedDistribution. If user really want hashPartitioning, they should use sortWithinPartitions.",
        "createdAt" : "2019-12-24T08:29:09Z",
        "updatedAt" : "2019-12-24T08:48:46Z",
        "lastEditedBy" : "0f4ef4e8-09be-436f-b743-bf8fbc343490",
        "tags" : [
        ]
      },
      {
        "id" : "91390031-ac00-409e-93ba-3b29ad922a49",
        "parentId" : "c70663ad-312d-4a2d-80e8-9d2b56a91ad3",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "makes sense",
        "createdAt" : "2019-12-24T08:35:54Z",
        "updatedAt" : "2019-12-24T08:48:46Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "d2615b61a6e5f7ed849f826d25d45315953386c9",
    "line" : 45,
    "diffHunk" : "@@ -1,1 +463,467 @@      \"HashPartitioning should be changed to RangePartitioning\")\n\n    val query = testData.select('key, 'value).repartition(5, 'key).sort('key.asc)\n    assert(query.rdd.getNumPartitions == 5)\n    assert(query.rdd.collectPartitions()(0).map(_.get(0)).toSeq == (1 to 20))"
  },
  {
    "id" : "88128854-d002-4db1-acd7-fe99bfa1e90f",
    "prId" : 26943,
    "prUrl" : "https://github.com/apache/spark/pull/26943#pullrequestreview-346338651",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c8e2496d-5f96-4912-b403-1708a382bb47",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "For better test coverage, can you add tests for sort-aggregates and object-aggregates?",
        "createdAt" : "2020-01-08T11:47:37Z",
        "updatedAt" : "2020-01-23T18:57:53Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "4d64d588-04be-4dc1-b183-a34619ee7faa",
        "parentId" : "c8e2496d-5f96-4912-b403-1708a382bb47",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "Thanks, added.",
        "createdAt" : "2020-01-22T04:21:55Z",
        "updatedAt" : "2020-01-23T18:57:53Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "fbafedf8793554366135b4d41189e06feb6f875e",
    "line" : 58,
    "diffHunk" : "@@ -1,1 +985,989 @@  }\n\n  test(\"aliases in the aggregate expressions should not introduce extra shuffle\") {\n    withSQLConf(SQLConf.AUTO_BROADCASTJOIN_THRESHOLD.key -> \"-1\") {\n      val t1 = spark.range(10).selectExpr(\"floor(id/4) as k1\")"
  },
  {
    "id" : "e29d8574-2db8-44ff-808d-901054b5cb30",
    "prId" : 26943,
    "prUrl" : "https://github.com/apache/spark/pull/26943#pullrequestreview-347542228",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "611b075c-1e3a-4cfa-ac2f-e60ebbb46179",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I was confused about why only one shuffle, then realized it's exchange reuse.\r\n\r\nCan we join different data frames? e.g. `spark.range(10)` and `spark.range(20)`.",
        "createdAt" : "2020-01-23T08:17:37Z",
        "updatedAt" : "2020-01-23T18:57:53Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "9c159070-859f-4744-9cb6-3bbde58e1c61",
        "parentId" : "611b075c-1e3a-4cfa-ac2f-e60ebbb46179",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "Thanks for pointing that out. I updated it and it now generates two `ShuffleExchangeExec` instead of four.",
        "createdAt" : "2020-01-23T19:13:30Z",
        "updatedAt" : "2020-01-23T19:13:30Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "fbafedf8793554366135b4d41189e06feb6f875e",
    "line" : 26,
    "diffHunk" : "@@ -1,1 +953,957 @@            |ON t1.k = t2.k\n          \"\"\".stripMargin).queryExecution.executedPlan\n        val exchanges = planned.collect { case s: ShuffleExchangeExec => s }\n        assert(exchanges.size == 2)\n      }"
  },
  {
    "id" : "15a055c9-7d87-4873-a40d-2e3a70ef602a",
    "prId" : 26943,
    "prUrl" : "https://github.com/apache/spark/pull/26943#pullrequestreview-347118259",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "205c94ab-4f9e-4863-8487-9bef6fffaea3",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "ditto",
        "createdAt" : "2020-01-23T08:17:56Z",
        "updatedAt" : "2020-01-23T18:57:53Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "fbafedf8793554366135b4d41189e06feb6f875e",
    "line" : 45,
    "diffHunk" : "@@ -1,1 +972,976 @@            |ON t1.k1 = t2.k2\n            |\"\"\".stripMargin).queryExecution.executedPlan\n        val exchanges = planned.collect { case s: ShuffleExchangeExec => s }\n\n        // Make sure aliases to an expression (key + 1) are not replaced."
  }
]