[
  {
    "id" : "4953fa47-7a4d-40a0-ac45-75cfb6e54318",
    "prId" : 33651,
    "prUrl" : "https://github.com/apache/spark/pull/33651#pullrequestreview-723950350",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "08c3a499-06c1-430a-8f17-7a4b11ad44ae",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "we can use `spark.range` if you just want to control the number of partitions.",
        "createdAt" : "2021-08-06T03:12:40Z",
        "updatedAt" : "2021-08-06T03:12:41Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "84c8e488-8d5b-4b7a-afba-169d6a3194bf",
        "parentId" : "08c3a499-06c1-430a-8f17-7a4b11ad44ae",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "Actually I don't like use range here since it has output partitionging. If you don't mind I prefer use `parallelize`.",
        "createdAt" : "2021-08-06T03:23:24Z",
        "updatedAt" : "2021-08-06T03:23:24Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      }
    ],
    "commit" : "6f28a37f420f781486c3e8c102156e02d9f830d7",
    "line" : 30,
    "diffHunk" : "@@ -1,1 +2019,2023 @@    withTempView(\"v\") {\n      spark.sparkContext.parallelize(\n        (1 to 10).map(i => TestData(i, if (i > 2) \"2\" else i.toString)), 2)\n        .toDF(\"c1\", \"c2\").createOrReplaceTempView(\"v\")\n"
  },
  {
    "id" : "bad39635-9765-450f-9cf8-2f302a9e9bfb",
    "prId" : 33651,
    "prUrl" : "https://github.com/apache/spark/pull/33651#pullrequestreview-723982272",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "58f18fa7-b803-4371-94c2-2e6ac41aac8d",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Hm? What this test wants to tell? Both queries have no top level limit after AQE?",
        "createdAt" : "2021-08-06T04:58:14Z",
        "updatedAt" : "2021-08-06T04:58:33Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "860d797c-6858-4161-9859-9547676cbd52",
        "parentId" : "58f18fa7-b803-4371-94c2-2e6ac41aac8d",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Oh, I see.",
        "createdAt" : "2021-08-06T05:01:14Z",
        "updatedAt" : "2021-08-06T05:01:14Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "6f28a37f420f781486c3e8c102156e02d9f830d7",
    "line" : 48,
    "diffHunk" : "@@ -1,1 +2037,2041 @@          \"\"\".stripMargin)\n        assert(findTopLevelLimit(origin2).size == 1)\n        assert(findTopLevelLimit(adaptive2).isEmpty)\n      }\n    }"
  },
  {
    "id" : "2a1aea24-1646-4e76-b832-afb6ca02940a",
    "prId" : 33429,
    "prUrl" : "https://github.com/apache/spark/pull/33429#pullrequestreview-710298179",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0cab7749-b1e5-4859-96a6-8009262316b0",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "\"num readers\" seems more natural?",
        "createdAt" : "2021-07-20T06:55:53Z",
        "updatedAt" : "2021-07-20T06:55:53Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "4c8cb445-af82-4204-9882-c39413c04eb9",
        "parentId" : "0cab7749-b1e5-4859-96a6-8009262316b0",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "or \"num shuffles with local read\", but it's longer",
        "createdAt" : "2021-07-20T06:56:29Z",
        "updatedAt" : "2021-07-20T06:56:30Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "72409441-b62b-4124-bf77-7b30282aad10",
        "parentId" : "0cab7749-b1e5-4859-96a6-8009262316b0",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I think it's fine. The number of local shuffle reads that refers the number of (local) `AQEShuffleReadExec`, the number of AQE read plans.",
        "createdAt" : "2021-07-20T08:12:18Z",
        "updatedAt" : "2021-07-20T08:12:18Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "19d6deebc8476849ceb4f547f013954e9857caa6",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +140,144 @@  }\n\n  private def checkNumLocalShuffleReads(\n      plan: SparkPlan, numShufflesWithoutLocalRead: Int = 0): Unit = {\n    val numShuffles = collect(plan) {"
  },
  {
    "id" : "a712c777-12ea-4174-955b-19e3d7b88b67",
    "prId" : 33188,
    "prUrl" : "https://github.com/apache/spark/pull/33188#pullrequestreview-698176151",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "55641ec8-3c50-483c-af7e-40ae07efe6de",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Ideally, when we optimize out the user-specified repartition (with num partitions), we should mark the operator that produces the same output partitioning as \"not optimizable\", so that we don't break the semantic of user-specified repartition.\r\n\r\nHowever, this is super complicated and it doesn't seem to be worthwhile to optimize user-specified repartitions. I'm +1 to just not optimize out user-specified repartition in AQE.",
        "createdAt" : "2021-07-02T13:16:35Z",
        "updatedAt" : "2021-07-02T13:16:35Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "7b668bbd9cef4ca2f0cfa84d2e16870464db4f1b",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +1527,1531 @@        dfRepartitionWithNum.collect()\n        val planWithNum = dfRepartitionWithNum.queryExecution.executedPlan\n        // The top shuffle from repartition is not optimized out.\n        assert(hasRepartitionShuffle(planWithNum))\n        val bhjWithNum = findTopLevelBroadcastHashJoin(planWithNum)"
  },
  {
    "id" : "e4fc330c-4544-4767-afa7-9a4212497626",
    "prId" : 32944,
    "prUrl" : "https://github.com/apache/spark/pull/32944#pullrequestreview-697933547",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4e96a279-2565-41ac-b849-72e8a66a5a53",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "does this custom cost evaluator change the query plan? It seems to be the same with the builtin cost evaluator.",
        "createdAt" : "2021-07-02T03:18:55Z",
        "updatedAt" : "2021-07-02T03:18:55Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "f9e0dc30-0cc1-4da8-b06d-bcdfc21c2909",
        "parentId" : "4e96a279-2565-41ac-b849-72e8a66a5a53",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "@cloud-fan - this evaluator does not change plan, and to be the same with the builtin evaluator for this query. Do we want to come up a different one here? I think this just validates the custom evaluator works.",
        "createdAt" : "2021-07-02T07:59:03Z",
        "updatedAt" : "2021-07-02T07:59:03Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      },
      {
        "id" : "780bac9c-0321-4cb0-abb0-fe303531d136",
        "parentId" : "4e96a279-2565-41ac-b849-72e8a66a5a53",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "SGTM, let's leave it then",
        "createdAt" : "2021-07-02T08:09:12Z",
        "updatedAt" : "2021-07-02T08:09:24Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "ac5c12186035a47bbe7c0b1891564066db676354",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +1914,1918 @@\n      withSQLConf(SQLConf.ADAPTIVE_CUSTOM_COST_EVALUATOR_CLASS.key ->\n        \"org.apache.spark.sql.execution.adaptive.SimpleShuffleSortCostEvaluator\") {\n        val (plan, adaptivePlan) = runAdaptiveAndVerifyResult(query)\n        val smj = findTopLevelSortMergeJoin(plan)"
  },
  {
    "id" : "e01123cf-67e4-4cd2-a183-851922161d4e",
    "prId" : 32883,
    "prUrl" : "https://github.com/apache/spark/pull/32883#pullrequestreview-696204802",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "02ae8d95-be0a-402d-89f9-12ea6144d914",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "can we tune the size a little more, so that coalesce also applies?",
        "createdAt" : "2021-06-30T13:20:26Z",
        "updatedAt" : "2021-06-30T13:20:36Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "f71ccb53-b3ae-4a8d-bbe9-a6026f4ee087",
        "parentId" : "02ae8d95-be0a-402d-89f9-12ea6144d914",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "yea, tune up to 150",
        "createdAt" : "2021-06-30T14:02:29Z",
        "updatedAt" : "2021-06-30T14:02:29Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      }
    ],
    "commit" : "8e706ae5f289ce9d4f6a7ca624f1d58516a8a177",
    "line" : 41,
    "diffHunk" : "@@ -1,1 +1849,1853 @@\n        withSQLConf(SQLConf.ADVISORY_PARTITION_SIZE_IN_BYTES.key -> \"150\") {\n          // partition size [0,258,72,72,72]\n          checkPartitionNumber(\"SELECT /*+ REBALANCE(c1) */ * FROM v\", 2, 4)\n          // partition size [72,216,216,144,72]"
  },
  {
    "id" : "c605a678-ea1c-4fda-a939-aff15fcccc0c",
    "prId" : 32872,
    "prUrl" : "https://github.com/apache/spark/pull/32872#pullrequestreview-703444744",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f692f971-37de-4abc-ba5f-2301221251ef",
        "parentId" : null,
        "authorId" : "9acf210d-4935-4e03-8384-d944eb558b45",
        "body" : "Here, of the 2 CustomShuffleReaderExec nodes one has \r\n[0, 0, 0, 72, 0] in CustomShuffleReader.child._resultOption\r\nand the resulting partitions are\r\n0 = {CoalescedPartitionSpec@16752} \"CoalescedPartitionSpec(3,4,Some(72))\"\r\n1 = {CoalescedPartitionSpec@16752} \"CoalescedPartitionSpec(3,4,Some(72))\"\r\n2 = {CoalescedPartitionSpec@16753} \"CoalescedPartitionSpec(4,5,Some(0))\"\r\n3 = {CoalescedPartitionSpec@16753} \"CoalescedPartitionSpec(4,5,Some(0))\"\r\n\r\n\r\nand the other has [0, 0, 0, 138, 138]\r\nAnd resulting partitions\r\n0 = {PartialReducerPartitionSpec@16800} \"PartialReducerPartitionSpec(3,0,1,72)\"\r\n1 = {PartialReducerPartitionSpec@16801} \"PartialReducerPartitionSpec(3,1,2,66)\"\r\n2 = {PartialReducerPartitionSpec@16802} \"PartialReducerPartitionSpec(4,0,1,66)\"\r\n3 = {PartialReducerPartitionSpec@16803} \"PartialReducerPartitionSpec(4,1,2,72)\"\r\n\r\nso I don't think you can make it coalesce w/o \"disabling\" skew mitigation.  In any case since this fixes the definition of \"coalesced\" I think it's ok to have to adjust a test.",
        "createdAt" : "2021-07-09T23:40:37Z",
        "updatedAt" : "2021-07-09T23:40:37Z",
        "lastEditedBy" : "9acf210d-4935-4e03-8384-d944eb558b45",
        "tags" : [
        ]
      }
    ],
    "commit" : "4bed38168439d9afb83be89f16bcc20f817eaa45",
    "line" : 26,
    "diffHunk" : "@@ -1,1 +1583,1587 @@        assert(smjWithNum.head.isSkewJoin)\n        val customReadersWithNum = collect(smjWithNum.head) {\n          case c: CustomShuffleReaderExec => c\n        }\n        assert(customReadersWithNum.nonEmpty)"
  },
  {
    "id" : "53a6b4cd-7a6d-4258-9b6b-d5df96f03c36",
    "prId" : 32602,
    "prUrl" : "https://github.com/apache/spark/pull/32602#pullrequestreview-666662654",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cf3f2bb0-4a6b-491e-8aac-ffa847a4ba65",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "We can't optimize this before this PR?",
        "createdAt" : "2021-05-24T10:38:04Z",
        "updatedAt" : "2021-05-24T10:38:04Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "6c7eacc6-156e-4f36-99b2-145044fb54ab",
        "parentId" : "cf3f2bb0-4a6b-491e-8aac-ffa847a4ba65",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "yeah, we cann't. Before we only check right side with `LeftSemi/LeftAnti`.\r\n\r\nAnd the test should use different column to do filter and join  in case of `InferFiltersFromConstraints` make right side empty.  Updated it.",
        "createdAt" : "2021-05-24T11:53:03Z",
        "updatedAt" : "2021-05-24T11:53:03Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      }
    ],
    "commit" : "624e45e27513933e9fe5615601d0dddac341a258",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +1314,1318 @@      SQLConf.AUTO_BROADCASTJOIN_THRESHOLD.key -> \"-1\") {\n      Seq(\n        // left semi join and empty left side\n        (\"SELECT * FROM (SELECT * FROM testData WHERE value = '0')t1 LEFT SEMI JOIN \" +\n          \"testData2 t2 ON t1.key = t2.a\", true),"
  },
  {
    "id" : "7dfdda5d-69e3-4ca4-981b-1121eab5ab7d",
    "prId" : 31873,
    "prUrl" : "https://github.com/apache/spark/pull/31873#pullrequestreview-614820998",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e89029c8-f289-4a62-ad97-85870fddde28",
        "parentId" : null,
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "This is true now as we eliminate join to its left side.",
        "createdAt" : "2021-03-17T21:59:42Z",
        "updatedAt" : "2021-03-18T00:31:00Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      }
    ],
    "commit" : "7f7f3347efe267e9f75c166c8fe98ef09a204db7",
    "line" : 41,
    "diffHunk" : "@@ -1,1 +1260,1264 @@        // broadcast empty right side\n        (\"SELECT /*+ broadcast(emptyTestData) */ * FROM testData LEFT ANTI JOIN emptyTestData\",\n          true),\n        // broadcast left side\n        (\"SELECT /*+ broadcast(testData) */ * FROM testData LEFT ANTI JOIN testData3\", false)"
  },
  {
    "id" : "185d4eb3-8b74-4992-b9d6-9efa90db158f",
    "prId" : 31793,
    "prUrl" : "https://github.com/apache/spark/pull/31793#pullrequestreview-608354722",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e2c30997-9318-4036-9d24-7514b63eb142",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Thank you for making a test case, @andygrove .\r\n\r\nHowever, this doesn't match with your PR description. This should be `IllegalStateException`.\r\n```\r\nAfter fixing this regression I saw:\r\n> java.lang.IllegalStateException: operating on canonicalized plan\r\n```",
        "createdAt" : "2021-03-10T05:59:54Z",
        "updatedAt" : "2021-03-10T05:59:54Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "a54ba8729a3516e1dc2373da1eab82431eedf78e",
    "line" : 25,
    "diffHunk" : "@@ -1,1 +884,888 @@      val doExecute = c.getClass.getMethod(\"doExecute\")\n      doExecute.setAccessible(true)\n      val ex = intercept[InvocationTargetException] {\n        doExecute.invoke(c)\n      }"
  },
  {
    "id" : "088e5332-b97a-4044-8efd-5e77df7d8ce4",
    "prId" : 31793,
    "prUrl" : "https://github.com/apache/spark/pull/31793#pullrequestreview-608358081",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a0769e65-0297-4f38-bde7-7d743baa41c5",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Please remove this since this is not the expected exception.",
        "createdAt" : "2021-03-10T06:00:11Z",
        "updatedAt" : "2021-03-10T06:00:11Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "78dd7e57-c172-4a19-bc7a-c3178c04ffe3",
        "parentId" : "a0769e65-0297-4f38-bde7-7d743baa41c5",
        "authorId" : "7e060620-bf02-418e-832f-c68dddfa2611",
        "body" : "Thanks for the review. The InvocationTargetException is here because the test is using reflection to invoke doExecute. This exception is wrapping the IllegalStateException. I will update the test tomorrow to make this clearer.",
        "createdAt" : "2021-03-10T06:03:42Z",
        "updatedAt" : "2021-03-10T06:03:43Z",
        "lastEditedBy" : "7e060620-bf02-418e-832f-c68dddfa2611",
        "tags" : [
        ]
      },
      {
        "id" : "8f8590b9-af11-4039-837c-a50c8fefddae",
        "parentId" : "a0769e65-0297-4f38-bde7-7d743baa41c5",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Oh, got it. I missed that.",
        "createdAt" : "2021-03-10T06:07:56Z",
        "updatedAt" : "2021-03-10T06:07:56Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "a54ba8729a3516e1dc2373da1eab82431eedf78e",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +19,23 @@\nimport java.io.File\nimport java.lang.reflect.InvocationTargetException\nimport java.net.URI\n"
  },
  {
    "id" : "a772da83-ce23-4b13-ba62-ad07464dbaf7",
    "prId" : 31793,
    "prUrl" : "https://github.com/apache/spark/pull/31793#pullrequestreview-608356357",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c29aeb97-6136-4fcc-9a6b-2cbb4c2ba30b",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "For the following question, ScalaTest's `PrivateMethodTester.invokePrivate` is recommended.\r\n> The problem is that doExecute is protected, and I can't test by calling execute because it has separate checks for canonicalized plans. What would be the recommended approach here?\r\n\r\nSo, you can revise this test case like the following.\r\n\r\n```scala\r\n-      // we can't just call execute() because that has separate checks for canonicalized plans\r\n-      val doExecute = c.getClass.getMethod(\"doExecute\")\r\n-      doExecute.setAccessible(true)\r\n-      val ex = intercept[InvocationTargetException] {\r\n-        doExecute.invoke(c)\r\n+      val doExecute = PrivateMethod[Unit](Symbol(\"doExecute\"))\r\n+      val ex = intercept[IllegalStateException] {\r\n+        c.invokePrivate(doExecute())\r\n       }\r\n-      assert(ex.getCause.getMessage === \"operating on canonicalized plan\")\r\n+      assert(ex.getMessage === \"operating on canonicalized plan\")\r\n```",
        "createdAt" : "2021-03-10T06:03:53Z",
        "updatedAt" : "2021-03-10T06:03:53Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "a54ba8729a3516e1dc2373da1eab82431eedf78e",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +887,891 @@        doExecute.invoke(c)\n      }\n      assert(ex.getCause.getMessage === \"operating on canonicalized plan\")\n    }\n  }"
  },
  {
    "id" : "f94be8c4-2dfa-4210-99ef-a455db0dec50",
    "prId" : 30998,
    "prUrl" : "https://github.com/apache/spark/pull/30998#pullrequestreview-564687366",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9f0b2c6f-44af-48b5-8d07-4e8081e29a10",
        "parentId" : null,
        "authorId" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "body" : "It seems that this case fails a little frequentlyï¼š\r\n\r\n- [https://amplab.cs.berkeley.edu/jenkins/job/spark-branch-3.1-test-maven-hadoop-2.7-jdk-11/95/](https://amplab.cs.berkeley.edu/jenkins/job/spark-branch-3.1-test-maven-hadoop-2.7-jdk-11/95/)\r\n- [https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-maven-hadoop-3.2/1854/](https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-maven-hadoop-3.2/1854/)\r\n- [https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-maven-hadoop-2.7-jdk-11-scala-2.13/147/#showFailuresLink](https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-maven-hadoop-2.7-jdk-11-scala-2.13/147/#showFailuresLink)\r\n- [https://amplab.cs.berkeley.edu/jenkins/job/spark-branch-3.1-test-maven-hadoop-3.2/80/](https://amplab.cs.berkeley.edu/jenkins/job/spark-branch-3.1-test-maven-hadoop-3.2/80/)\r\n\r\n@zhongyu09 @cloud-fan  @viirya ",
        "createdAt" : "2021-01-08T06:23:31Z",
        "updatedAt" : "2021-01-08T06:23:32Z",
        "lastEditedBy" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "tags" : [
        ]
      },
      {
        "id" : "fb80c965-c204-4726-b0c2-6a4a3291bed9",
        "parentId" : "9f0b2c6f-44af-48b5-8d07-4e8081e29a10",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Hmm, so it is still flaky.",
        "createdAt" : "2021-01-08T07:08:02Z",
        "updatedAt" : "2021-01-08T07:08:02Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "944c6e6d-c62d-4538-94eb-b1926891b331",
        "parentId" : "9f0b2c6f-44af-48b5-8d07-4e8081e29a10",
        "authorId" : "21ccf86e-7d54-44b8-aa50-2ad0a352ad3b",
        "body" : "I try the test tens of times and the test failed twice. As we discussed, the guarantee is not strict since the submit of broadcast job and shuffle map job are in different thread. So there's still risk for the shuffle map job schedule earlier before broadcast job. I wonder should we need to remove the UT until we thorough resolve the issue.\r\n\r\nCC @cloud-fan @dongjoon-hyun ",
        "createdAt" : "2021-01-08T07:27:31Z",
        "updatedAt" : "2021-01-08T07:27:32Z",
        "lastEditedBy" : "21ccf86e-7d54-44b8-aa50-2ad0a352ad3b",
        "tags" : [
        ]
      },
      {
        "id" : "f1b75c63-b910-4ed2-9e98-1acec7a839a2",
        "parentId" : "9f0b2c6f-44af-48b5-8d07-4e8081e29a10",
        "authorId" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "body" : "I run `SPARK-33933: AQE broadcast should not timeout with slow map tasks`  in local 5 times and all failed as follow:\r\n\r\n```\r\n- SPARK-33933: AQE broadcast should not timeout with slow map tasks *** FAILED ***\r\n  1751 was not greater than 2000 (AdaptiveQueryExecSuite.scala:1454)\r\n```",
        "createdAt" : "2021-01-08T07:30:11Z",
        "updatedAt" : "2021-01-08T07:30:11Z",
        "lastEditedBy" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "tags" : [
        ]
      },
      {
        "id" : "872bf7cf-49e9-4abb-879e-110887773756",
        "parentId" : "9f0b2c6f-44af-48b5-8d07-4e8081e29a10",
        "authorId" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "body" : "> Hmm, so it is still flaky.\r\n\r\nyes",
        "createdAt" : "2021-01-08T07:32:25Z",
        "updatedAt" : "2021-01-08T07:32:25Z",
        "lastEditedBy" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "tags" : [
        ]
      },
      {
        "id" : "4a284b42-1db6-4d97-afdb-deb946c66d5d",
        "parentId" : "9f0b2c6f-44af-48b5-8d07-4e8081e29a10",
        "authorId" : "21ccf86e-7d54-44b8-aa50-2ad0a352ad3b",
        "body" : "For now, revert the PR or remove the UT, which one is better? ",
        "createdAt" : "2021-01-08T07:41:43Z",
        "updatedAt" : "2021-01-08T07:41:43Z",
        "lastEditedBy" : "21ccf86e-7d54-44b8-aa50-2ad0a352ad3b",
        "tags" : [
        ]
      },
      {
        "id" : "5db48ea6-378e-4ae7-b308-02db3bee9222",
        "parentId" : "9f0b2c6f-44af-48b5-8d07-4e8081e29a10",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I was worried about this and tested the PR twice...\r\n\r\nMaybe it's better to be more low-level and verify the stage submission order, which is more reliable. We can use SparkListener to watch the job start events, and see if the job with less tasks (the broadcast stage) is started first.\r\n\r\n@zhongyu09 what do you think?",
        "createdAt" : "2021-01-08T07:48:39Z",
        "updatedAt" : "2021-01-08T07:49:16Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "09c7c3c5-819d-4480-99c7-98a3ce4471b7",
        "parentId" : "9f0b2c6f-44af-48b5-8d07-4e8081e29a10",
        "authorId" : "21ccf86e-7d54-44b8-aa50-2ad0a352ad3b",
        "body" : "I am afraid that won't work. The real materialize work runs in different thread for broadcast and map tasks. So the job sequence of start events might not be strictly the same as calling materialize().",
        "createdAt" : "2021-01-08T08:02:49Z",
        "updatedAt" : "2021-01-08T08:02:49Z",
        "lastEditedBy" : "21ccf86e-7d54-44b8-aa50-2ad0a352ad3b",
        "tags" : [
        ]
      },
      {
        "id" : "c9edc33a-8d90-49c4-9568-3f9331bc4981",
        "parentId" : "9f0b2c6f-44af-48b5-8d07-4e8081e29a10",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "we can add retry logic to make it less likely to fail. It should be much more stable than checking the running time.\r\n\r\nAnother way is to check the actual stage submission order (not the job submission order). But seems there is no easy way to get the stage submission order. Maybe we can add logs and check the log?",
        "createdAt" : "2021-01-08T08:22:36Z",
        "updatedAt" : "2021-01-08T08:27:42Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "fd7964f2-e1f9-4ea6-9ef5-bb86e7acc5ff",
        "parentId" : "9f0b2c6f-44af-48b5-8d07-4e8081e29a10",
        "authorId" : "21ccf86e-7d54-44b8-aa50-2ad0a352ad3b",
        "body" : "@cloud-fan  You mean the error 1751 was not greater than 2000 (AdaptiveQueryExecSuite.scala:1454) ? That's my fault, by moving the UT from BroadcastJoinSuite to AdaptiveQueryExecSuite, the spark conf changed to local[2] and so the running times are faster than before.",
        "createdAt" : "2021-01-08T08:31:33Z",
        "updatedAt" : "2021-01-08T08:31:34Z",
        "lastEditedBy" : "21ccf86e-7d54-44b8-aa50-2ad0a352ad3b",
        "tags" : [
        ]
      },
      {
        "id" : "48073f49-6736-436e-9d78-1930eebe0524",
        "parentId" : "9f0b2c6f-44af-48b5-8d07-4e8081e29a10",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "> the spark conf changed to local[2] and so the running times are faster than before.\r\n\r\nThis shows the test is unreliable...\r\n\r\nChecking the Spark jobs submission order should be easy to do and fast to run, and with retry it should be unlikely to fail. It's better to check stage submission order directly, if we can figure out how to do it.",
        "createdAt" : "2021-01-08T08:34:27Z",
        "updatedAt" : "2021-01-08T08:34:27Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "c02b5c41-d033-4010-a539-fd1c8c1e0ae2",
        "parentId" : "9f0b2c6f-44af-48b5-8d07-4e8081e29a10",
        "authorId" : "21ccf86e-7d54-44b8-aa50-2ad0a352ad3b",
        "body" : "Yes I know, the failure reported by @LuciferYang is easy to solve. \r\nBut the question is, the jobs submission order may be not correct, like the test in https://github.com/apache/spark/pull/31084.",
        "createdAt" : "2021-01-08T08:39:08Z",
        "updatedAt" : "2021-01-08T08:39:08Z",
        "lastEditedBy" : "21ccf86e-7d54-44b8-aa50-2ad0a352ad3b",
        "tags" : [
        ]
      },
      {
        "id" : "82789f61-5eb5-4783-a34d-c3f52a4cb585",
        "parentId" : "9f0b2c6f-44af-48b5-8d07-4e8081e29a10",
        "authorId" : "21ccf86e-7d54-44b8-aa50-2ad0a352ad3b",
        "body" : "@cloud-fan  You mean `with retry it should be unlikely to fail` to solve the edge case? ",
        "createdAt" : "2021-01-08T08:43:08Z",
        "updatedAt" : "2021-01-08T08:43:08Z",
        "lastEditedBy" : "21ccf86e-7d54-44b8-aa50-2ad0a352ad3b",
        "tags" : [
        ]
      },
      {
        "id" : "201b297f-69db-45db-aa07-0c6e0d00a787",
        "parentId" : "9f0b2c6f-44af-48b5-8d07-4e8081e29a10",
        "authorId" : "21ccf86e-7d54-44b8-aa50-2ad0a352ad3b",
        "body" : "I am trying to create a new PR. Sorry for the inconvenience.",
        "createdAt" : "2021-01-08T09:00:19Z",
        "updatedAt" : "2021-01-08T09:00:19Z",
        "lastEditedBy" : "21ccf86e-7d54-44b8-aa50-2ad0a352ad3b",
        "tags" : [
        ]
      },
      {
        "id" : "142d4169-372e-465c-83a8-b8455b427922",
        "parentId" : "9f0b2c6f-44af-48b5-8d07-4e8081e29a10",
        "authorId" : "21ccf86e-7d54-44b8-aa50-2ad0a352ad3b",
        "body" : "We can get the stage submission time using SparkListeneer. But after trying serval times, the stage submission time is not stable thus the UT cannot always passed. I suggest to remove the UT before we completely solve the issue. https://github.com/apache/spark/pull/31099",
        "createdAt" : "2021-01-09T05:58:45Z",
        "updatedAt" : "2021-01-09T05:58:45Z",
        "lastEditedBy" : "21ccf86e-7d54-44b8-aa50-2ad0a352ad3b",
        "tags" : [
        ]
      }
    ],
    "commit" : "7cbeb14272833ac5d7e5aecc81f93c2c3a5cbadf",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1433,1437 @@  }\n\n  test(\"SPARK-33933: AQE broadcast should not timeout with slow map tasks\") {\n    val broadcastTimeoutInSec = 1\n    val df = spark.sparkContext.parallelize(Range(0, 100), 100)"
  }
]