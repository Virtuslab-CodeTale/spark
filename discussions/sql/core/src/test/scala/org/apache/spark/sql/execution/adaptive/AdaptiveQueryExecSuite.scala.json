[
  {
    "id" : "4953fa47-7a4d-40a0-ac45-75cfb6e54318",
    "prId" : 33651,
    "prUrl" : "https://github.com/apache/spark/pull/33651#pullrequestreview-723950350",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "08c3a499-06c1-430a-8f17-7a4b11ad44ae",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "we can use `spark.range` if you just want to control the number of partitions.",
        "createdAt" : "2021-08-06T03:12:40Z",
        "updatedAt" : "2021-08-06T03:12:41Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "84c8e488-8d5b-4b7a-afba-169d6a3194bf",
        "parentId" : "08c3a499-06c1-430a-8f17-7a4b11ad44ae",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "Actually I don't like use range here since it has output partitionging. If you don't mind I prefer use `parallelize`.",
        "createdAt" : "2021-08-06T03:23:24Z",
        "updatedAt" : "2021-08-06T03:23:24Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      }
    ],
    "commit" : "6f28a37f420f781486c3e8c102156e02d9f830d7",
    "line" : 30,
    "diffHunk" : "@@ -1,1 +2019,2023 @@    withTempView(\"v\") {\n      spark.sparkContext.parallelize(\n        (1 to 10).map(i => TestData(i, if (i > 2) \"2\" else i.toString)), 2)\n        .toDF(\"c1\", \"c2\").createOrReplaceTempView(\"v\")\n"
  },
  {
    "id" : "bad39635-9765-450f-9cf8-2f302a9e9bfb",
    "prId" : 33651,
    "prUrl" : "https://github.com/apache/spark/pull/33651#pullrequestreview-723982272",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "58f18fa7-b803-4371-94c2-2e6ac41aac8d",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Hm? What this test wants to tell? Both queries have no top level limit after AQE?",
        "createdAt" : "2021-08-06T04:58:14Z",
        "updatedAt" : "2021-08-06T04:58:33Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "860d797c-6858-4161-9859-9547676cbd52",
        "parentId" : "58f18fa7-b803-4371-94c2-2e6ac41aac8d",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Oh, I see.",
        "createdAt" : "2021-08-06T05:01:14Z",
        "updatedAt" : "2021-08-06T05:01:14Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "6f28a37f420f781486c3e8c102156e02d9f830d7",
    "line" : 48,
    "diffHunk" : "@@ -1,1 +2037,2041 @@          \"\"\".stripMargin)\n        assert(findTopLevelLimit(origin2).size == 1)\n        assert(findTopLevelLimit(adaptive2).isEmpty)\n      }\n    }"
  },
  {
    "id" : "2a1aea24-1646-4e76-b832-afb6ca02940a",
    "prId" : 33429,
    "prUrl" : "https://github.com/apache/spark/pull/33429#pullrequestreview-710298179",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0cab7749-b1e5-4859-96a6-8009262316b0",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "\"num readers\" seems more natural?",
        "createdAt" : "2021-07-20T06:55:53Z",
        "updatedAt" : "2021-07-20T06:55:53Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "4c8cb445-af82-4204-9882-c39413c04eb9",
        "parentId" : "0cab7749-b1e5-4859-96a6-8009262316b0",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "or \"num shuffles with local read\", but it's longer",
        "createdAt" : "2021-07-20T06:56:29Z",
        "updatedAt" : "2021-07-20T06:56:30Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "72409441-b62b-4124-bf77-7b30282aad10",
        "parentId" : "0cab7749-b1e5-4859-96a6-8009262316b0",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I think it's fine. The number of local shuffle reads that refers the number of (local) `AQEShuffleReadExec`, the number of AQE read plans.",
        "createdAt" : "2021-07-20T08:12:18Z",
        "updatedAt" : "2021-07-20T08:12:18Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "19d6deebc8476849ceb4f547f013954e9857caa6",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +140,144 @@  }\n\n  private def checkNumLocalShuffleReads(\n      plan: SparkPlan, numShufflesWithoutLocalRead: Int = 0): Unit = {\n    val numShuffles = collect(plan) {"
  },
  {
    "id" : "a712c777-12ea-4174-955b-19e3d7b88b67",
    "prId" : 33188,
    "prUrl" : "https://github.com/apache/spark/pull/33188#pullrequestreview-698176151",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "55641ec8-3c50-483c-af7e-40ae07efe6de",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Ideally, when we optimize out the user-specified repartition (with num partitions), we should mark the operator that produces the same output partitioning as \"not optimizable\", so that we don't break the semantic of user-specified repartition.\r\n\r\nHowever, this is super complicated and it doesn't seem to be worthwhile to optimize user-specified repartitions. I'm +1 to just not optimize out user-specified repartition in AQE.",
        "createdAt" : "2021-07-02T13:16:35Z",
        "updatedAt" : "2021-07-02T13:16:35Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "7b668bbd9cef4ca2f0cfa84d2e16870464db4f1b",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +1527,1531 @@        dfRepartitionWithNum.collect()\n        val planWithNum = dfRepartitionWithNum.queryExecution.executedPlan\n        // The top shuffle from repartition is not optimized out.\n        assert(hasRepartitionShuffle(planWithNum))\n        val bhjWithNum = findTopLevelBroadcastHashJoin(planWithNum)"
  },
  {
    "id" : "e4fc330c-4544-4767-afa7-9a4212497626",
    "prId" : 32944,
    "prUrl" : "https://github.com/apache/spark/pull/32944#pullrequestreview-697933547",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4e96a279-2565-41ac-b849-72e8a66a5a53",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "does this custom cost evaluator change the query plan? It seems to be the same with the builtin cost evaluator.",
        "createdAt" : "2021-07-02T03:18:55Z",
        "updatedAt" : "2021-07-02T03:18:55Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "f9e0dc30-0cc1-4da8-b06d-bcdfc21c2909",
        "parentId" : "4e96a279-2565-41ac-b849-72e8a66a5a53",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "@cloud-fan - this evaluator does not change plan, and to be the same with the builtin evaluator for this query. Do we want to come up a different one here? I think this just validates the custom evaluator works.",
        "createdAt" : "2021-07-02T07:59:03Z",
        "updatedAt" : "2021-07-02T07:59:03Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      },
      {
        "id" : "780bac9c-0321-4cb0-abb0-fe303531d136",
        "parentId" : "4e96a279-2565-41ac-b849-72e8a66a5a53",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "SGTM, let's leave it then",
        "createdAt" : "2021-07-02T08:09:12Z",
        "updatedAt" : "2021-07-02T08:09:24Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "ac5c12186035a47bbe7c0b1891564066db676354",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +1914,1918 @@\n      withSQLConf(SQLConf.ADAPTIVE_CUSTOM_COST_EVALUATOR_CLASS.key ->\n        \"org.apache.spark.sql.execution.adaptive.SimpleShuffleSortCostEvaluator\") {\n        val (plan, adaptivePlan) = runAdaptiveAndVerifyResult(query)\n        val smj = findTopLevelSortMergeJoin(plan)"
  },
  {
    "id" : "e01123cf-67e4-4cd2-a183-851922161d4e",
    "prId" : 32883,
    "prUrl" : "https://github.com/apache/spark/pull/32883#pullrequestreview-696204802",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "02ae8d95-be0a-402d-89f9-12ea6144d914",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "can we tune the size a little more, so that coalesce also applies?",
        "createdAt" : "2021-06-30T13:20:26Z",
        "updatedAt" : "2021-06-30T13:20:36Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "f71ccb53-b3ae-4a8d-bbe9-a6026f4ee087",
        "parentId" : "02ae8d95-be0a-402d-89f9-12ea6144d914",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "yea, tune up to 150",
        "createdAt" : "2021-06-30T14:02:29Z",
        "updatedAt" : "2021-06-30T14:02:29Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      }
    ],
    "commit" : "8e706ae5f289ce9d4f6a7ca624f1d58516a8a177",
    "line" : 41,
    "diffHunk" : "@@ -1,1 +1849,1853 @@\n        withSQLConf(SQLConf.ADVISORY_PARTITION_SIZE_IN_BYTES.key -> \"150\") {\n          // partition size [0,258,72,72,72]\n          checkPartitionNumber(\"SELECT /*+ REBALANCE(c1) */ * FROM v\", 2, 4)\n          // partition size [72,216,216,144,72]"
  },
  {
    "id" : "c605a678-ea1c-4fda-a939-aff15fcccc0c",
    "prId" : 32872,
    "prUrl" : "https://github.com/apache/spark/pull/32872#pullrequestreview-703444744",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f692f971-37de-4abc-ba5f-2301221251ef",
        "parentId" : null,
        "authorId" : "9acf210d-4935-4e03-8384-d944eb558b45",
        "body" : "Here, of the 2 CustomShuffleReaderExec nodes one has \r\n[0, 0, 0, 72, 0] in CustomShuffleReader.child._resultOption\r\nand the resulting partitions are\r\n0 = {CoalescedPartitionSpec@16752} \"CoalescedPartitionSpec(3,4,Some(72))\"\r\n1 = {CoalescedPartitionSpec@16752} \"CoalescedPartitionSpec(3,4,Some(72))\"\r\n2 = {CoalescedPartitionSpec@16753} \"CoalescedPartitionSpec(4,5,Some(0))\"\r\n3 = {CoalescedPartitionSpec@16753} \"CoalescedPartitionSpec(4,5,Some(0))\"\r\n\r\n\r\nand the other has [0, 0, 0, 138, 138]\r\nAnd resulting partitions\r\n0 = {PartialReducerPartitionSpec@16800} \"PartialReducerPartitionSpec(3,0,1,72)\"\r\n1 = {PartialReducerPartitionSpec@16801} \"PartialReducerPartitionSpec(3,1,2,66)\"\r\n2 = {PartialReducerPartitionSpec@16802} \"PartialReducerPartitionSpec(4,0,1,66)\"\r\n3 = {PartialReducerPartitionSpec@16803} \"PartialReducerPartitionSpec(4,1,2,72)\"\r\n\r\nso I don't think you can make it coalesce w/o \"disabling\" skew mitigation.  In any case since this fixes the definition of \"coalesced\" I think it's ok to have to adjust a test.",
        "createdAt" : "2021-07-09T23:40:37Z",
        "updatedAt" : "2021-07-09T23:40:37Z",
        "lastEditedBy" : "9acf210d-4935-4e03-8384-d944eb558b45",
        "tags" : [
        ]
      }
    ],
    "commit" : "4bed38168439d9afb83be89f16bcc20f817eaa45",
    "line" : 26,
    "diffHunk" : "@@ -1,1 +1583,1587 @@        assert(smjWithNum.head.isSkewJoin)\n        val customReadersWithNum = collect(smjWithNum.head) {\n          case c: CustomShuffleReaderExec => c\n        }\n        assert(customReadersWithNum.nonEmpty)"
  },
  {
    "id" : "53a6b4cd-7a6d-4258-9b6b-d5df96f03c36",
    "prId" : 32602,
    "prUrl" : "https://github.com/apache/spark/pull/32602#pullrequestreview-666662654",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cf3f2bb0-4a6b-491e-8aac-ffa847a4ba65",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "We can't optimize this before this PR?",
        "createdAt" : "2021-05-24T10:38:04Z",
        "updatedAt" : "2021-05-24T10:38:04Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "6c7eacc6-156e-4f36-99b2-145044fb54ab",
        "parentId" : "cf3f2bb0-4a6b-491e-8aac-ffa847a4ba65",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "yeah, we cann't. Before we only check right side with `LeftSemi/LeftAnti`.\r\n\r\nAnd the test should use different column to do filter and join  in case of `InferFiltersFromConstraints` make right side empty.  Updated it.",
        "createdAt" : "2021-05-24T11:53:03Z",
        "updatedAt" : "2021-05-24T11:53:03Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      }
    ],
    "commit" : "624e45e27513933e9fe5615601d0dddac341a258",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +1314,1318 @@      SQLConf.AUTO_BROADCASTJOIN_THRESHOLD.key -> \"-1\") {\n      Seq(\n        // left semi join and empty left side\n        (\"SELECT * FROM (SELECT * FROM testData WHERE value = '0')t1 LEFT SEMI JOIN \" +\n          \"testData2 t2 ON t1.key = t2.a\", true),"
  },
  {
    "id" : "7dfdda5d-69e3-4ca4-981b-1121eab5ab7d",
    "prId" : 31873,
    "prUrl" : "https://github.com/apache/spark/pull/31873#pullrequestreview-614820998",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e89029c8-f289-4a62-ad97-85870fddde28",
        "parentId" : null,
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "This is true now as we eliminate join to its left side.",
        "createdAt" : "2021-03-17T21:59:42Z",
        "updatedAt" : "2021-03-18T00:31:00Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      }
    ],
    "commit" : "7f7f3347efe267e9f75c166c8fe98ef09a204db7",
    "line" : 41,
    "diffHunk" : "@@ -1,1 +1260,1264 @@        // broadcast empty right side\n        (\"SELECT /*+ broadcast(emptyTestData) */ * FROM testData LEFT ANTI JOIN emptyTestData\",\n          true),\n        // broadcast left side\n        (\"SELECT /*+ broadcast(testData) */ * FROM testData LEFT ANTI JOIN testData3\", false)"
  },
  {
    "id" : "185d4eb3-8b74-4992-b9d6-9efa90db158f",
    "prId" : 31793,
    "prUrl" : "https://github.com/apache/spark/pull/31793#pullrequestreview-608354722",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e2c30997-9318-4036-9d24-7514b63eb142",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Thank you for making a test case, @andygrove .\r\n\r\nHowever, this doesn't match with your PR description. This should be `IllegalStateException`.\r\n```\r\nAfter fixing this regression I saw:\r\n> java.lang.IllegalStateException: operating on canonicalized plan\r\n```",
        "createdAt" : "2021-03-10T05:59:54Z",
        "updatedAt" : "2021-03-10T05:59:54Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "a54ba8729a3516e1dc2373da1eab82431eedf78e",
    "line" : 25,
    "diffHunk" : "@@ -1,1 +884,888 @@      val doExecute = c.getClass.getMethod(\"doExecute\")\n      doExecute.setAccessible(true)\n      val ex = intercept[InvocationTargetException] {\n        doExecute.invoke(c)\n      }"
  },
  {
    "id" : "088e5332-b97a-4044-8efd-5e77df7d8ce4",
    "prId" : 31793,
    "prUrl" : "https://github.com/apache/spark/pull/31793#pullrequestreview-608358081",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a0769e65-0297-4f38-bde7-7d743baa41c5",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Please remove this since this is not the expected exception.",
        "createdAt" : "2021-03-10T06:00:11Z",
        "updatedAt" : "2021-03-10T06:00:11Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "78dd7e57-c172-4a19-bc7a-c3178c04ffe3",
        "parentId" : "a0769e65-0297-4f38-bde7-7d743baa41c5",
        "authorId" : "7e060620-bf02-418e-832f-c68dddfa2611",
        "body" : "Thanks for the review. The InvocationTargetException is here because the test is using reflection to invoke doExecute. This exception is wrapping the IllegalStateException. I will update the test tomorrow to make this clearer.",
        "createdAt" : "2021-03-10T06:03:42Z",
        "updatedAt" : "2021-03-10T06:03:43Z",
        "lastEditedBy" : "7e060620-bf02-418e-832f-c68dddfa2611",
        "tags" : [
        ]
      },
      {
        "id" : "8f8590b9-af11-4039-837c-a50c8fefddae",
        "parentId" : "a0769e65-0297-4f38-bde7-7d743baa41c5",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Oh, got it. I missed that.",
        "createdAt" : "2021-03-10T06:07:56Z",
        "updatedAt" : "2021-03-10T06:07:56Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "a54ba8729a3516e1dc2373da1eab82431eedf78e",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +19,23 @@\nimport java.io.File\nimport java.lang.reflect.InvocationTargetException\nimport java.net.URI\n"
  },
  {
    "id" : "a772da83-ce23-4b13-ba62-ad07464dbaf7",
    "prId" : 31793,
    "prUrl" : "https://github.com/apache/spark/pull/31793#pullrequestreview-608356357",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c29aeb97-6136-4fcc-9a6b-2cbb4c2ba30b",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "For the following question, ScalaTest's `PrivateMethodTester.invokePrivate` is recommended.\r\n> The problem is that doExecute is protected, and I can't test by calling execute because it has separate checks for canonicalized plans. What would be the recommended approach here?\r\n\r\nSo, you can revise this test case like the following.\r\n\r\n```scala\r\n-      // we can't just call execute() because that has separate checks for canonicalized plans\r\n-      val doExecute = c.getClass.getMethod(\"doExecute\")\r\n-      doExecute.setAccessible(true)\r\n-      val ex = intercept[InvocationTargetException] {\r\n-        doExecute.invoke(c)\r\n+      val doExecute = PrivateMethod[Unit](Symbol(\"doExecute\"))\r\n+      val ex = intercept[IllegalStateException] {\r\n+        c.invokePrivate(doExecute())\r\n       }\r\n-      assert(ex.getCause.getMessage === \"operating on canonicalized plan\")\r\n+      assert(ex.getMessage === \"operating on canonicalized plan\")\r\n```",
        "createdAt" : "2021-03-10T06:03:53Z",
        "updatedAt" : "2021-03-10T06:03:53Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "a54ba8729a3516e1dc2373da1eab82431eedf78e",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +887,891 @@        doExecute.invoke(c)\n      }\n      assert(ex.getCause.getMessage === \"operating on canonicalized plan\")\n    }\n  }"
  },
  {
    "id" : "f94be8c4-2dfa-4210-99ef-a455db0dec50",
    "prId" : 30998,
    "prUrl" : "https://github.com/apache/spark/pull/30998#pullrequestreview-564687366",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9f0b2c6f-44af-48b5-8d07-4e8081e29a10",
        "parentId" : null,
        "authorId" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "body" : "It seems that this case fails a little frequentlyï¼š\r\n\r\n- [https://amplab.cs.berkeley.edu/jenkins/job/spark-branch-3.1-test-maven-hadoop-2.7-jdk-11/95/](https://amplab.cs.berkeley.edu/jenkins/job/spark-branch-3.1-test-maven-hadoop-2.7-jdk-11/95/)\r\n- [https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-maven-hadoop-3.2/1854/](https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-maven-hadoop-3.2/1854/)\r\n- [https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-maven-hadoop-2.7-jdk-11-scala-2.13/147/#showFailuresLink](https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-maven-hadoop-2.7-jdk-11-scala-2.13/147/#showFailuresLink)\r\n- [https://amplab.cs.berkeley.edu/jenkins/job/spark-branch-3.1-test-maven-hadoop-3.2/80/](https://amplab.cs.berkeley.edu/jenkins/job/spark-branch-3.1-test-maven-hadoop-3.2/80/)\r\n\r\n@zhongyu09 @cloud-fan  @viirya ",
        "createdAt" : "2021-01-08T06:23:31Z",
        "updatedAt" : "2021-01-08T06:23:32Z",
        "lastEditedBy" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "tags" : [
        ]
      },
      {
        "id" : "fb80c965-c204-4726-b0c2-6a4a3291bed9",
        "parentId" : "9f0b2c6f-44af-48b5-8d07-4e8081e29a10",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Hmm, so it is still flaky.",
        "createdAt" : "2021-01-08T07:08:02Z",
        "updatedAt" : "2021-01-08T07:08:02Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "944c6e6d-c62d-4538-94eb-b1926891b331",
        "parentId" : "9f0b2c6f-44af-48b5-8d07-4e8081e29a10",
        "authorId" : "21ccf86e-7d54-44b8-aa50-2ad0a352ad3b",
        "body" : "I try the test tens of times and the test failed twice. As we discussed, the guarantee is not strict since the submit of broadcast job and shuffle map job are in different thread. So there's still risk for the shuffle map job schedule earlier before broadcast job. I wonder should we need to remove the UT until we thorough resolve the issue.\r\n\r\nCC @cloud-fan @dongjoon-hyun ",
        "createdAt" : "2021-01-08T07:27:31Z",
        "updatedAt" : "2021-01-08T07:27:32Z",
        "lastEditedBy" : "21ccf86e-7d54-44b8-aa50-2ad0a352ad3b",
        "tags" : [
        ]
      },
      {
        "id" : "f1b75c63-b910-4ed2-9e98-1acec7a839a2",
        "parentId" : "9f0b2c6f-44af-48b5-8d07-4e8081e29a10",
        "authorId" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "body" : "I run `SPARK-33933: AQE broadcast should not timeout with slow map tasks`  in local 5 times and all failed as follow:\r\n\r\n```\r\n- SPARK-33933: AQE broadcast should not timeout with slow map tasks *** FAILED ***\r\n  1751 was not greater than 2000 (AdaptiveQueryExecSuite.scala:1454)\r\n```",
        "createdAt" : "2021-01-08T07:30:11Z",
        "updatedAt" : "2021-01-08T07:30:11Z",
        "lastEditedBy" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "tags" : [
        ]
      },
      {
        "id" : "872bf7cf-49e9-4abb-879e-110887773756",
        "parentId" : "9f0b2c6f-44af-48b5-8d07-4e8081e29a10",
        "authorId" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "body" : "> Hmm, so it is still flaky.\r\n\r\nyes",
        "createdAt" : "2021-01-08T07:32:25Z",
        "updatedAt" : "2021-01-08T07:32:25Z",
        "lastEditedBy" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "tags" : [
        ]
      },
      {
        "id" : "4a284b42-1db6-4d97-afdb-deb946c66d5d",
        "parentId" : "9f0b2c6f-44af-48b5-8d07-4e8081e29a10",
        "authorId" : "21ccf86e-7d54-44b8-aa50-2ad0a352ad3b",
        "body" : "For now, revert the PR or remove the UT, which one is better? ",
        "createdAt" : "2021-01-08T07:41:43Z",
        "updatedAt" : "2021-01-08T07:41:43Z",
        "lastEditedBy" : "21ccf86e-7d54-44b8-aa50-2ad0a352ad3b",
        "tags" : [
        ]
      },
      {
        "id" : "5db48ea6-378e-4ae7-b308-02db3bee9222",
        "parentId" : "9f0b2c6f-44af-48b5-8d07-4e8081e29a10",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I was worried about this and tested the PR twice...\r\n\r\nMaybe it's better to be more low-level and verify the stage submission order, which is more reliable. We can use SparkListener to watch the job start events, and see if the job with less tasks (the broadcast stage) is started first.\r\n\r\n@zhongyu09 what do you think?",
        "createdAt" : "2021-01-08T07:48:39Z",
        "updatedAt" : "2021-01-08T07:49:16Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "09c7c3c5-819d-4480-99c7-98a3ce4471b7",
        "parentId" : "9f0b2c6f-44af-48b5-8d07-4e8081e29a10",
        "authorId" : "21ccf86e-7d54-44b8-aa50-2ad0a352ad3b",
        "body" : "I am afraid that won't work. The real materialize work runs in different thread for broadcast and map tasks. So the job sequence of start events might not be strictly the same as calling materialize().",
        "createdAt" : "2021-01-08T08:02:49Z",
        "updatedAt" : "2021-01-08T08:02:49Z",
        "lastEditedBy" : "21ccf86e-7d54-44b8-aa50-2ad0a352ad3b",
        "tags" : [
        ]
      },
      {
        "id" : "c9edc33a-8d90-49c4-9568-3f9331bc4981",
        "parentId" : "9f0b2c6f-44af-48b5-8d07-4e8081e29a10",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "we can add retry logic to make it less likely to fail. It should be much more stable than checking the running time.\r\n\r\nAnother way is to check the actual stage submission order (not the job submission order). But seems there is no easy way to get the stage submission order. Maybe we can add logs and check the log?",
        "createdAt" : "2021-01-08T08:22:36Z",
        "updatedAt" : "2021-01-08T08:27:42Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "fd7964f2-e1f9-4ea6-9ef5-bb86e7acc5ff",
        "parentId" : "9f0b2c6f-44af-48b5-8d07-4e8081e29a10",
        "authorId" : "21ccf86e-7d54-44b8-aa50-2ad0a352ad3b",
        "body" : "@cloud-fan  You mean the error 1751 was not greater than 2000 (AdaptiveQueryExecSuite.scala:1454) ? That's my fault, by moving the UT from BroadcastJoinSuite to AdaptiveQueryExecSuite, the spark conf changed to local[2] and so the running times are faster than before.",
        "createdAt" : "2021-01-08T08:31:33Z",
        "updatedAt" : "2021-01-08T08:31:34Z",
        "lastEditedBy" : "21ccf86e-7d54-44b8-aa50-2ad0a352ad3b",
        "tags" : [
        ]
      },
      {
        "id" : "48073f49-6736-436e-9d78-1930eebe0524",
        "parentId" : "9f0b2c6f-44af-48b5-8d07-4e8081e29a10",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "> the spark conf changed to local[2] and so the running times are faster than before.\r\n\r\nThis shows the test is unreliable...\r\n\r\nChecking the Spark jobs submission order should be easy to do and fast to run, and with retry it should be unlikely to fail. It's better to check stage submission order directly, if we can figure out how to do it.",
        "createdAt" : "2021-01-08T08:34:27Z",
        "updatedAt" : "2021-01-08T08:34:27Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "c02b5c41-d033-4010-a539-fd1c8c1e0ae2",
        "parentId" : "9f0b2c6f-44af-48b5-8d07-4e8081e29a10",
        "authorId" : "21ccf86e-7d54-44b8-aa50-2ad0a352ad3b",
        "body" : "Yes I know, the failure reported by @LuciferYang is easy to solve. \r\nBut the question is, the jobs submission order may be not correct, like the test in https://github.com/apache/spark/pull/31084.",
        "createdAt" : "2021-01-08T08:39:08Z",
        "updatedAt" : "2021-01-08T08:39:08Z",
        "lastEditedBy" : "21ccf86e-7d54-44b8-aa50-2ad0a352ad3b",
        "tags" : [
        ]
      },
      {
        "id" : "82789f61-5eb5-4783-a34d-c3f52a4cb585",
        "parentId" : "9f0b2c6f-44af-48b5-8d07-4e8081e29a10",
        "authorId" : "21ccf86e-7d54-44b8-aa50-2ad0a352ad3b",
        "body" : "@cloud-fan  You mean `with retry it should be unlikely to fail` to solve the edge case? ",
        "createdAt" : "2021-01-08T08:43:08Z",
        "updatedAt" : "2021-01-08T08:43:08Z",
        "lastEditedBy" : "21ccf86e-7d54-44b8-aa50-2ad0a352ad3b",
        "tags" : [
        ]
      },
      {
        "id" : "201b297f-69db-45db-aa07-0c6e0d00a787",
        "parentId" : "9f0b2c6f-44af-48b5-8d07-4e8081e29a10",
        "authorId" : "21ccf86e-7d54-44b8-aa50-2ad0a352ad3b",
        "body" : "I am trying to create a new PR. Sorry for the inconvenience.",
        "createdAt" : "2021-01-08T09:00:19Z",
        "updatedAt" : "2021-01-08T09:00:19Z",
        "lastEditedBy" : "21ccf86e-7d54-44b8-aa50-2ad0a352ad3b",
        "tags" : [
        ]
      },
      {
        "id" : "142d4169-372e-465c-83a8-b8455b427922",
        "parentId" : "9f0b2c6f-44af-48b5-8d07-4e8081e29a10",
        "authorId" : "21ccf86e-7d54-44b8-aa50-2ad0a352ad3b",
        "body" : "We can get the stage submission time using SparkListeneer. But after trying serval times, the stage submission time is not stable thus the UT cannot always passed. I suggest to remove the UT before we completely solve the issue. https://github.com/apache/spark/pull/31099",
        "createdAt" : "2021-01-09T05:58:45Z",
        "updatedAt" : "2021-01-09T05:58:45Z",
        "lastEditedBy" : "21ccf86e-7d54-44b8-aa50-2ad0a352ad3b",
        "tags" : [
        ]
      }
    ],
    "commit" : "7cbeb14272833ac5d7e5aecc81f93c2c3a5cbadf",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1433,1437 @@  }\n\n  test(\"SPARK-33933: AQE broadcast should not timeout with slow map tasks\") {\n    val broadcastTimeoutInSec = 1\n    val df = spark.sparkContext.parallelize(Range(0, 100), 100)"
  },
  {
    "id" : "187fc89d-c1dc-42c1-8bd7-9b06100912fa",
    "prId" : 29614,
    "prUrl" : "https://github.com/apache/spark/pull/29614#pullrequestreview-481568022",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6fdd6474-bc3a-4e3d-8e47-90056388679c",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "We already exclude this in `SharedSparkSession`? https://github.com/apache/spark/blob/95f1e9549bb741db6c285390a71c609a9e5d3b02/sql/core/src/test/scala/org/apache/spark/sql/test/SharedSparkSession.scala#L70-L74",
        "createdAt" : "2020-09-03T07:31:08Z",
        "updatedAt" : "2020-09-03T07:31:08Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "82f6cc656ebc1f77ef736f07bd3e0dd71eea07fe",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +1231,1235 @@      SQLConf.ADAPTIVE_EXECUTION_ENABLED.key -> \"true\",\n      // exclude ConvertToLocalRelation rule make it easier for Test.\n      SQLConf.OPTIMIZER_EXCLUDED_RULES.key -> ConvertToLocalRelation.ruleName) {\n      withTempView(\"m\", \"s\", \"pm\", \"ps\") {\n        Seq((null: Integer, 1.0), (2: Integer, 3.0), (4: Integer, 5.0))"
  },
  {
    "id" : "7f0f8c15-2b6a-46b0-bb2d-b41a1acae095",
    "prId" : 29484,
    "prUrl" : "https://github.com/apache/spark/pull/29484#pullrequestreview-472244699",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3a951291-fd27-4697-9969-743bae218a5d",
        "parentId" : null,
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "@cloud-fan - just fyi. the change in this unit test is needed as `assert(find(plan)(_.isInstanceOf[BroadcastHashJoinExec]).isDefined)` no long true, because this is an inner join and the build side is empty. So with the change in this PR, the join operator is optimized into an empty relation operator (failure stack trace of unit test without change is [here](https://github.com/apache/spark/pull/29484/checks?check_run_id=1011186945)).\r\n\r\nChanged from inner join to left outer join, to help unit test pass. And I don't think changing from inner join to left outer join here can comprise any functionality of original unit test. Let me know if it's not the case. thanks.",
        "createdAt" : "2020-08-21T06:52:51Z",
        "updatedAt" : "2020-08-23T06:17:42Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      }
    ],
    "commit" : "496eda790c95b1c9895967b7181d582fa87da0ff",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +244,248 @@        checkAnswer(testDf, Seq())\n        val plan = testDf.queryExecution.executedPlan\n        assert(find(plan)(_.isInstanceOf[BroadcastHashJoinExec]).isDefined)\n        val coalescedReaders = collect(plan) {\n          case r: CustomShuffleReaderExec => r"
  },
  {
    "id" : "b8cd5d5b-b0a4-47cb-8971-db9082278fc4",
    "prId" : 29484,
    "prUrl" : "https://github.com/apache/spark/pull/29484#pullrequestreview-472860768",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "06fc5eb6-dc41-4a77-aaa2-a45e8279d557",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "SortMergeJoin? I think this targets BHJ and SHJ?",
        "createdAt" : "2020-08-21T21:45:45Z",
        "updatedAt" : "2020-08-23T06:17:42Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "9b1181f3-cf59-495c-9d3d-61980da13307",
        "parentId" : "06fc5eb6-dc41-4a77-aaa2-a45e8279d557",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "@viirya - similar to other test cases in this file - the input data stats is super large, and by default it uses SMJ. Per my comment above, SMJ/BHJ/SHJ will all turn into empty `LocalRelation` (where `SMJ/SHJ` first turned into `BHJ`).",
        "createdAt" : "2020-08-21T23:08:26Z",
        "updatedAt" : "2020-08-23T06:17:42Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      }
    ],
    "commit" : "496eda790c95b1c9895967b7181d582fa87da0ff",
    "line" : 38,
    "diffHunk" : "@@ -1,1 +1198,1202 @@      ).foreach(query => {\n        val (plan, adaptivePlan) = runAdaptiveAndVerifyResult(query)\n        val smj = findTopLevelSortMergeJoin(plan)\n        assert(smj.size == 1)\n        val join = findTopLevelBaseJoin(adaptivePlan)"
  },
  {
    "id" : "019bc341-3b6a-4a92-83dd-3f869c41a446",
    "prId" : 29021,
    "prUrl" : "https://github.com/apache/spark/pull/29021#pullrequestreview-443634434",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ea794308-1956-49a2-a9b2-38a60580f932",
        "parentId" : null,
        "authorId" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "body" : "It's hard to build a test case like the case in PR description. So I just want to give a simple one. But the rule is same. In this test case, we build a SMJ like:\r\n```\r\nSMJ\r\n     Sort\r\n       CustomShuffleReader(coalesced)\r\n         Shuffle\r\n     Sort\r\n       HashAggregate\r\n         CustomShuffleReader(coalesced)\r\n           Shuffle\r\n```\r\nIt has already breaked current matching pattern. The plan will be optimized to\r\n```\r\nSMJ\r\n     Sort\r\n       CustomShuffleReader(skewed)\r\n         Shuffle\r\n     Sort\r\n       HashAggregate\r\n         CustomShuffleReader(coalesced)\r\n           Shuffle\r\n```\r\nWe don't split the shuffle side with any AggregateExec.\r\n\r\nThe plan before patch is\r\n```\r\nAdaptiveSparkPlan(isFinalPlan=true)\r\n+- *(5) SortMergeJoin [key1#183L], [key2#189L], Inner, false\r\n   :- *(3) Sort [key1#183L ASC NULLS FIRST], false, 0\r\n   :  +- CustomShuffleReader coalesced\r\n   :     +- ShuffleQueryStage 0\r\n   :        +- Exchange hashpartitioning(key1#183L, 100), true\r\n   :           +- *(1) Project [CASE WHEN (id#181L < 250) THEN 249 WHEN (id#181L >= 750) THEN 1000 ELSE id#181L END AS key1#183L, id#181L AS value1#184L]\r\n   :              +- *(1) Range (0, 1000, step=1, splits=10)\r\n   +- *(4) Sort [key2#189L ASC NULLS FIRST], false, 0\r\n      +- *(4) HashAggregate(keys=[key2#189L], functions=[sum(value2#190L)], output=[key2#189L, sum2#193L])\r\n         +- CustomShuffleReader coalesced\r\n            +- ShuffleQueryStage 1\r\n               +- Exchange hashpartitioning(key2#189L, 100), true\r\n                  +- *(2) HashAggregate(keys=[key2#189L], functions=[partial_sum(value2#190L)], output=[key2#189L, sum#200L])\r\n                     +- *(2) Project [CASE WHEN (id#187L < 250) THEN 249 ELSE id#187L END AS key2#189L, id#187L AS value2#190L]\r\n                        +- *(2) Range (0, 1000, step=1, splits=10)\r\n```\r\nThe plan after is\r\n```\r\nAdaptiveSparkPlan(isFinalPlan=true)\r\n+- *(5) SortMergeJoin(skew=true) [key1#183L], [key2#189L], Inner, true\r\n   :- *(3) Sort [key1#183L ASC NULLS FIRST], false, 0\r\n   :  +- CustomShuffleReader coalesced and skewed\r\n   :     +- ShuffleQueryStage 0\r\n   :        +- Exchange hashpartitioning(key1#183L, 100), true\r\n   :           +- *(1) Project [CASE WHEN (id#181L < 250) THEN 249 WHEN (id#181L >= 750) THEN 1000 ELSE id#181L END AS key1#183L, id#181L AS value1#184L]\r\n   :              +- *(1) Range (0, 1000, step=1, splits=10)\r\n   +- *(4) Sort [key2#189L ASC NULLS FIRST], false, 0\r\n      +- *(4) HashAggregate(keys=[key2#189L], functions=[sum(value2#190L)], output=[key2#189L, sum2#193L])\r\n         +- CustomShuffleReader coalesced\r\n            +- ShuffleQueryStage 1\r\n               +- Exchange hashpartitioning(key2#189L, 100), true\r\n                  +- *(2) HashAggregate(keys=[key2#189L], functions=[partial_sum(value2#190L)], output=[key2#189L, sum#200L])\r\n                     +- *(2) Project [CASE WHEN (id#187L < 250) THEN 249 ELSE id#187L END AS key2#189L, id#187L AS value2#190L]\r\n                        +- *(2) Range (0, 1000, step=1, splits=10)\r\n```",
        "createdAt" : "2020-07-07T07:35:39Z",
        "updatedAt" : "2020-07-21T07:50:57Z",
        "lastEditedBy" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "tags" : [
        ]
      }
    ],
    "commit" : "5bed68ceed90c89457ff04af6da62f2222c795b7",
    "line" : 45,
    "diffHunk" : "@@ -1,1 +732,736 @@  }\n\n  test(\"SPARK-32201: handle general skew join pattern\") {\n    withSQLConf(\n      SQLConf.ADAPTIVE_EXECUTION_ENABLED.key -> \"true\","
  },
  {
    "id" : "5f67857d-dfd8-4446-adf7-2da5369d9c2a",
    "prId" : 28900,
    "prUrl" : "https://github.com/apache/spark/pull/28900#pullrequestreview-436358970",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "08d04b4f-1b53-441f-af5c-077495e0481b",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "we can merge this test case to your two newly added test cases.",
        "createdAt" : "2020-06-24T06:09:08Z",
        "updatedAt" : "2020-06-24T16:03:48Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "c376f675-446c-4e01-b254-e151899716c5",
        "parentId" : "08d04b4f-1b53-441f-af5c-077495e0481b",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "i.e. one test to test `repartition`, and it verifies both the initial partition number and the coalesced partition number. The other test tests the same thing but for `repartitionByRange`.",
        "createdAt" : "2020-06-24T06:12:55Z",
        "updatedAt" : "2020-06-24T16:03:48Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "5945c3a5-4818-4ff1-8568-1564adfa942d",
        "parentId" : "08d04b4f-1b53-441f-af5c-077495e0481b",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Yeah, merged them.",
        "createdAt" : "2020-06-24T06:30:42Z",
        "updatedAt" : "2020-06-24T16:03:48Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "1ae1a8799ecf0a485cb4617a54d8d68feea0eaf8",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +1024,1028 @@\n  test(\"SPARK-31220, SPARK-32056: repartition by expression with AQE\") {\n    Seq(true, false).foreach { enableAQE =>\n      withSQLConf(\n        SQLConf.ADAPTIVE_EXECUTION_ENABLED.key -> enableAQE.toString,"
  },
  {
    "id" : "b1f545fa-60d3-4659-85d2-432fc3d8d1fd",
    "prId" : 28900,
    "prUrl" : "https://github.com/apache/spark/pull/28900#pullrequestreview-436358873",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "db08852d-f661-4a8b-8839-6f7d092878dd",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "to make the test more robust, shall we set `SQLConf.COALESCE_PARTITIONS_INITIAL_PARTITION_NUM` to 10? then it's clear that once coalescing happens, the partition number will be smaller than 10.",
        "createdAt" : "2020-06-24T06:11:32Z",
        "updatedAt" : "2020-06-24T16:03:48Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "17ea090e-a1a9-42ab-b644-1adbff591c7e",
        "parentId" : "db08852d-f661-4a8b-8839-6f7d092878dd",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "ok.",
        "createdAt" : "2020-06-24T06:30:32Z",
        "updatedAt" : "2020-06-24T16:03:48Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "1ae1a8799ecf0a485cb4617a54d8d68feea0eaf8",
    "line" : 75,
    "diffHunk" : "@@ -1,1 +1079,1083 @@\n        if (enableAQE) {\n          assert(partitionsNum1 < 10)\n          assert(partitionsNum2 < 10)\n"
  },
  {
    "id" : "72cb05fe-be03-44a4-a982-66abee8027eb",
    "prId" : 28517,
    "prUrl" : "https://github.com/apache/spark/pull/28517#pullrequestreview-410498079",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "62e3709f-092a-444c-b00e-33bd25ae9918",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "This test fails with below error message:\r\n\r\n```\r\norg.apache.spark.sql.AnalysisException: Hive support is required to CREATE Hive TABLE (AS SELECT);; 'CreateTable `t1`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, ErrorIfExists +- Project [1 AS col#545614]    +- OneRowRelation \r\n```\r\n\r\nThis test initially assumed the create table without USING is following native syntax, and it's no longer valid. Added `USING parquet` explicitly.",
        "createdAt" : "2020-05-12T23:46:41Z",
        "updatedAt" : "2020-05-15T05:20:01Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "84c172fb20321b082fc98fa419f9756ec44a6a7e",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +945,949 @@      SQLConf.ADAPTIVE_EXECUTION_FORCE_APPLY.key -> \"true\") {\n      withTable(\"t1\") {\n        val plan = sql(\"CREATE TABLE t1 USING parquet AS SELECT 1 col\").queryExecution.executedPlan\n        assert(plan.isInstanceOf[DataWritingCommandExec])\n        assert(plan.asInstanceOf[DataWritingCommandExec].child.isInstanceOf[AdaptiveSparkPlanExec])"
  },
  {
    "id" : "7de3d91b-5512-4d90-9ab0-867d70dd1c49",
    "prId" : 28517,
    "prUrl" : "https://github.com/apache/spark/pull/28517#pullrequestreview-410498079",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5529f22a-b4aa-4d82-8f1c-b35134d661a5",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Same here.",
        "createdAt" : "2020-05-12T23:46:55Z",
        "updatedAt" : "2020-05-15T05:20:01Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "84c172fb20321b082fc98fa419f9756ec44a6a7e",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +1006,1010 @@        spark.sparkContext.addSparkListener(listener)\n        try {\n          sql(\"CREATE TABLE t1 USING parquet AS SELECT 1 col\").collect()\n          spark.sparkContext.listenerBus.waitUntilEmpty()\n          assert(checkDone)"
  },
  {
    "id" : "4958fe14-dc12-4c9b-aafc-8677f89d420c",
    "prId" : 28474,
    "prUrl" : "https://github.com/apache/spark/pull/28474#pullrequestreview-407424516",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1978e1e6-1abc-4169-91dc-87492a0f1d26",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "let's remove the listener at the end of the test",
        "createdAt" : "2020-05-07T12:32:52Z",
        "updatedAt" : "2020-05-08T17:06:17Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "9111ceb3d15edbeaa55ac2135bb53c955ae935c0",
    "line" : 35,
    "diffHunk" : "@@ -1,1 +1004,1008 @@          }\n        }\n        spark.sparkContext.addSparkListener(listener)\n        try {\n          sql(\"CREATE TABLE t1 AS SELECT 1 col\").collect()"
  },
  {
    "id" : "3e1e415a-4ff5-44f6-8257-afc820e9d303",
    "prId" : 28247,
    "prUrl" : "https://github.com/apache/spark/pull/28247#pullrequestreview-395731391",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d186646f-0459-4555-b99f-5156aa7d2cad",
        "parentId" : null,
        "authorId" : "80d631a6-73e8-46a6-a01b-a80d1f1cc6cc",
        "body" : "Undo this after the test? Or are we setting it in before()?",
        "createdAt" : "2020-04-17T19:37:34Z",
        "updatedAt" : "2020-04-17T22:57:35Z",
        "lastEditedBy" : "80d631a6-73e8-46a6-a01b-a80d1f1cc6cc",
        "tags" : [
        ]
      },
      {
        "id" : "afe4a78c-4751-4e83-b7e3-f81c40ffbfd4",
        "parentId" : "d186646f-0459-4555-b99f-5156aa7d2cad",
        "authorId" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "body" : "I believe it is somehow properly (secretly) set for each test. But you are right, I should undo it anyway.",
        "createdAt" : "2020-04-17T19:51:18Z",
        "updatedAt" : "2020-04-17T22:57:35Z",
        "lastEditedBy" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "tags" : [
        ]
      }
    ],
    "commit" : "877f6e13cb463f5e0e84c67964f0b89961b075b5",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +914,918 @@      val df = spark.range(10).select(sum('id))\n      assert(df.queryExecution.executedPlan.isInstanceOf[AdaptiveSparkPlanExec])\n      SparkSession.setActiveSession(null)\n      checkAnswer(df, Seq(Row(45)))\n      SparkSession.setActiveSession(spark) // recover the active session."
  },
  {
    "id" : "72a5e7a5-179a-4f05-9192-4300ddbed381",
    "prId" : 28247,
    "prUrl" : "https://github.com/apache/spark/pull/28247#pullrequestreview-395731536",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ecd935f5-72fa-4a6b-a369-b6831debd9b1",
        "parentId" : null,
        "authorId" : "80d631a6-73e8-46a6-a01b-a80d1f1cc6cc",
        "body" : "This throws an NPE right without setting the session right?",
        "createdAt" : "2020-04-17T19:37:58Z",
        "updatedAt" : "2020-04-17T22:57:35Z",
        "lastEditedBy" : "80d631a6-73e8-46a6-a01b-a80d1f1cc6cc",
        "tags" : [
        ]
      },
      {
        "id" : "99b872bf-8e35-44b5-8a1a-1d0d5808892c",
        "parentId" : "ecd935f5-72fa-4a6b-a369-b6831debd9b1",
        "authorId" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "body" : "Yep.",
        "createdAt" : "2020-04-17T19:51:33Z",
        "updatedAt" : "2020-04-17T22:57:35Z",
        "lastEditedBy" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "tags" : [
        ]
      }
    ],
    "commit" : "877f6e13cb463f5e0e84c67964f0b89961b075b5",
    "line" : 25,
    "diffHunk" : "@@ -1,1 +915,919 @@      assert(df.queryExecution.executedPlan.isInstanceOf[AdaptiveSparkPlanExec])\n      SparkSession.setActiveSession(null)\n      checkAnswer(df, Seq(Row(45)))\n      SparkSession.setActiveSession(spark) // recover the active session.\n    }"
  },
  {
    "id" : "2134a884-c5cb-4997-8716-f288a5436c15",
    "prId" : 27833,
    "prUrl" : "https://github.com/apache/spark/pull/27833#pullrequestreview-371352673",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4408741c-0ccd-42e2-8098-5e201eb21a0d",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "This is definitely better with 2 splits, as the target size is 2000 and the total size is 4014.",
        "createdAt" : "2020-03-09T16:49:28Z",
        "updatedAt" : "2020-03-10T11:30:06Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "54ceaed73e355289005f34ec872631aa13af010f",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +647,651 @@        // Partition 1, 2, 3: not skewed, and coalesced into 1 partition.\n        // Partition 4: only left side is skewed, and divide into 2 splits, so\n        //              2 sub-partitions.\n        // So total (8 + 1 + 3) partitions.\n        val innerSmj = findTopLevelSortMergeJoin(innerAdaptivePlan)"
  },
  {
    "id" : "cbec5159-6e99-497c-ab62-9ab75d253377",
    "prId" : 26516,
    "prUrl" : "https://github.com/apache/spark/pull/26516#pullrequestreview-318784370",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3a4987e2-79e6-4d4e-b3b3-0fb82d91fdfc",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "better to add `assert(localReaders.length == 2)`",
        "createdAt" : "2019-11-19T04:58:01Z",
        "updatedAt" : "2019-11-19T08:10:51Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "fff1115fba0e9d8f934263d6709631ed0110701f",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +117,121 @@      val localReaders = collect(adaptivePlan) {\n        case reader: LocalShuffleReaderExec => reader\n      }\n      assert(localReaders.length == 2)\n      // The pre-shuffle partition size is [0, 0, 0, 72, 0]"
  },
  {
    "id" : "2032df89-424d-4ab7-8847-7185d558d358",
    "prId" : 26516,
    "prUrl" : "https://github.com/apache/spark/pull/26516#pullrequestreview-318784735",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a26d050a-caf1-463c-92c5-ddaf79ac34ab",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "can we tune the configs so that we chang parallelism?  without this patch, the parallelism is numMappers, which is still 2.",
        "createdAt" : "2019-11-19T04:59:36Z",
        "updatedAt" : "2019-11-19T08:10:51Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "fff1115fba0e9d8f934263d6709631ed0110701f",
    "line" : 31,
    "diffHunk" : "@@ -1,1 +131,135 @@      // math.max(1, advisoryParallelism / numMappers): math.max(1, 3/2) = 1\n      // and the partitions length is 1 * numMappers = 2\n      assert(localShuffleRDD0.getPartitions.length == 2)\n      // the final parallelism is\n      // math.max(1, advisoryParallelism / numMappers): math.max(1, 5/2) = 2"
  },
  {
    "id" : "f154999b-7d1b-4872-8551-7ee3d8c8313b",
    "prId" : 26516,
    "prUrl" : "https://github.com/apache/spark/pull/26516#pullrequestreview-318824304",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "36102b7e-eaa9-4928-8a33-e5415fa84b05",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "nit: update the comment",
        "createdAt" : "2019-11-19T07:20:25Z",
        "updatedAt" : "2019-11-19T08:10:51Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "fff1115fba0e9d8f934263d6709631ed0110701f",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +119,123 @@      }\n      assert(localReaders.length == 2)\n      // The pre-shuffle partition size is [0, 0, 0, 72, 0]\n      // And the partitionStartIndices is [0, 3, 4]\n      assert(localReaders(0).advisoryParallelism.get == 3)"
  },
  {
    "id" : "ce2aaa03-87ca-40d3-b428-f63faab94881",
    "prId" : 26516,
    "prUrl" : "https://github.com/apache/spark/pull/26516#pullrequestreview-318824339",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f167175c-45cd-49dd-859b-8e955855bca6",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "ditoo",
        "createdAt" : "2019-11-19T07:20:32Z",
        "updatedAt" : "2019-11-19T08:10:51Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "fff1115fba0e9d8f934263d6709631ed0110701f",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +122,126 @@      // And the partitionStartIndices is [0, 3, 4]\n      assert(localReaders(0).advisoryParallelism.get == 3)\n      // The pre-shuffle partition size is [0, 72, 0, 72, 126]\n      // And the partitionStartIndices is [0, 1, 2, 3, 4]\n      assert(localReaders(1).advisoryParallelism.get == 5)"
  }
]