[
  {
    "id" : "4953fa47-7a4d-40a0-ac45-75cfb6e54318",
    "prId" : 33651,
    "prUrl" : "https://github.com/apache/spark/pull/33651#pullrequestreview-723950350",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "08c3a499-06c1-430a-8f17-7a4b11ad44ae",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "we can use `spark.range` if you just want to control the number of partitions.",
        "createdAt" : "2021-08-06T03:12:40Z",
        "updatedAt" : "2021-08-06T03:12:41Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "84c8e488-8d5b-4b7a-afba-169d6a3194bf",
        "parentId" : "08c3a499-06c1-430a-8f17-7a4b11ad44ae",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "Actually I don't like use range here since it has output partitionging. If you don't mind I prefer use `parallelize`.",
        "createdAt" : "2021-08-06T03:23:24Z",
        "updatedAt" : "2021-08-06T03:23:24Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      }
    ],
    "commit" : "6f28a37f420f781486c3e8c102156e02d9f830d7",
    "line" : 30,
    "diffHunk" : "@@ -1,1 +2019,2023 @@    withTempView(\"v\") {\n      spark.sparkContext.parallelize(\n        (1 to 10).map(i => TestData(i, if (i > 2) \"2\" else i.toString)), 2)\n        .toDF(\"c1\", \"c2\").createOrReplaceTempView(\"v\")\n"
  },
  {
    "id" : "bad39635-9765-450f-9cf8-2f302a9e9bfb",
    "prId" : 33651,
    "prUrl" : "https://github.com/apache/spark/pull/33651#pullrequestreview-723982272",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "58f18fa7-b803-4371-94c2-2e6ac41aac8d",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Hm? What this test wants to tell? Both queries have no top level limit after AQE?",
        "createdAt" : "2021-08-06T04:58:14Z",
        "updatedAt" : "2021-08-06T04:58:33Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "860d797c-6858-4161-9859-9547676cbd52",
        "parentId" : "58f18fa7-b803-4371-94c2-2e6ac41aac8d",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Oh, I see.",
        "createdAt" : "2021-08-06T05:01:14Z",
        "updatedAt" : "2021-08-06T05:01:14Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "6f28a37f420f781486c3e8c102156e02d9f830d7",
    "line" : 48,
    "diffHunk" : "@@ -1,1 +2037,2041 @@          \"\"\".stripMargin)\n        assert(findTopLevelLimit(origin2).size == 1)\n        assert(findTopLevelLimit(adaptive2).isEmpty)\n      }\n    }"
  },
  {
    "id" : "2a1aea24-1646-4e76-b832-afb6ca02940a",
    "prId" : 33429,
    "prUrl" : "https://github.com/apache/spark/pull/33429#pullrequestreview-710298179",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0cab7749-b1e5-4859-96a6-8009262316b0",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "\"num readers\" seems more natural?",
        "createdAt" : "2021-07-20T06:55:53Z",
        "updatedAt" : "2021-07-20T06:55:53Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "4c8cb445-af82-4204-9882-c39413c04eb9",
        "parentId" : "0cab7749-b1e5-4859-96a6-8009262316b0",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "or \"num shuffles with local read\", but it's longer",
        "createdAt" : "2021-07-20T06:56:29Z",
        "updatedAt" : "2021-07-20T06:56:30Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "72409441-b62b-4124-bf77-7b30282aad10",
        "parentId" : "0cab7749-b1e5-4859-96a6-8009262316b0",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I think it's fine. The number of local shuffle reads that refers the number of (local) `AQEShuffleReadExec`, the number of AQE read plans.",
        "createdAt" : "2021-07-20T08:12:18Z",
        "updatedAt" : "2021-07-20T08:12:18Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "19d6deebc8476849ceb4f547f013954e9857caa6",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +140,144 @@  }\n\n  private def checkNumLocalShuffleReads(\n      plan: SparkPlan, numShufflesWithoutLocalRead: Int = 0): Unit = {\n    val numShuffles = collect(plan) {"
  },
  {
    "id" : "a712c777-12ea-4174-955b-19e3d7b88b67",
    "prId" : 33188,
    "prUrl" : "https://github.com/apache/spark/pull/33188#pullrequestreview-698176151",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "55641ec8-3c50-483c-af7e-40ae07efe6de",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Ideally, when we optimize out the user-specified repartition (with num partitions), we should mark the operator that produces the same output partitioning as \"not optimizable\", so that we don't break the semantic of user-specified repartition.\r\n\r\nHowever, this is super complicated and it doesn't seem to be worthwhile to optimize user-specified repartitions. I'm +1 to just not optimize out user-specified repartition in AQE.",
        "createdAt" : "2021-07-02T13:16:35Z",
        "updatedAt" : "2021-07-02T13:16:35Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "7b668bbd9cef4ca2f0cfa84d2e16870464db4f1b",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +1527,1531 @@        dfRepartitionWithNum.collect()\n        val planWithNum = dfRepartitionWithNum.queryExecution.executedPlan\n        // The top shuffle from repartition is not optimized out.\n        assert(hasRepartitionShuffle(planWithNum))\n        val bhjWithNum = findTopLevelBroadcastHashJoin(planWithNum)"
  },
  {
    "id" : "e4fc330c-4544-4767-afa7-9a4212497626",
    "prId" : 32944,
    "prUrl" : "https://github.com/apache/spark/pull/32944#pullrequestreview-697933547",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4e96a279-2565-41ac-b849-72e8a66a5a53",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "does this custom cost evaluator change the query plan? It seems to be the same with the builtin cost evaluator.",
        "createdAt" : "2021-07-02T03:18:55Z",
        "updatedAt" : "2021-07-02T03:18:55Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "f9e0dc30-0cc1-4da8-b06d-bcdfc21c2909",
        "parentId" : "4e96a279-2565-41ac-b849-72e8a66a5a53",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "@cloud-fan - this evaluator does not change plan, and to be the same with the builtin evaluator for this query. Do we want to come up a different one here? I think this just validates the custom evaluator works.",
        "createdAt" : "2021-07-02T07:59:03Z",
        "updatedAt" : "2021-07-02T07:59:03Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      },
      {
        "id" : "780bac9c-0321-4cb0-abb0-fe303531d136",
        "parentId" : "4e96a279-2565-41ac-b849-72e8a66a5a53",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "SGTM, let's leave it then",
        "createdAt" : "2021-07-02T08:09:12Z",
        "updatedAt" : "2021-07-02T08:09:24Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "ac5c12186035a47bbe7c0b1891564066db676354",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +1914,1918 @@\n      withSQLConf(SQLConf.ADAPTIVE_CUSTOM_COST_EVALUATOR_CLASS.key ->\n        \"org.apache.spark.sql.execution.adaptive.SimpleShuffleSortCostEvaluator\") {\n        val (plan, adaptivePlan) = runAdaptiveAndVerifyResult(query)\n        val smj = findTopLevelSortMergeJoin(plan)"
  },
  {
    "id" : "e01123cf-67e4-4cd2-a183-851922161d4e",
    "prId" : 32883,
    "prUrl" : "https://github.com/apache/spark/pull/32883#pullrequestreview-696204802",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "02ae8d95-be0a-402d-89f9-12ea6144d914",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "can we tune the size a little more, so that coalesce also applies?",
        "createdAt" : "2021-06-30T13:20:26Z",
        "updatedAt" : "2021-06-30T13:20:36Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "f71ccb53-b3ae-4a8d-bbe9-a6026f4ee087",
        "parentId" : "02ae8d95-be0a-402d-89f9-12ea6144d914",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "yea, tune up to 150",
        "createdAt" : "2021-06-30T14:02:29Z",
        "updatedAt" : "2021-06-30T14:02:29Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      }
    ],
    "commit" : "8e706ae5f289ce9d4f6a7ca624f1d58516a8a177",
    "line" : 41,
    "diffHunk" : "@@ -1,1 +1849,1853 @@\n        withSQLConf(SQLConf.ADVISORY_PARTITION_SIZE_IN_BYTES.key -> \"150\") {\n          // partition size [0,258,72,72,72]\n          checkPartitionNumber(\"SELECT /*+ REBALANCE(c1) */ * FROM v\", 2, 4)\n          // partition size [72,216,216,144,72]"
  },
  {
    "id" : "c605a678-ea1c-4fda-a939-aff15fcccc0c",
    "prId" : 32872,
    "prUrl" : "https://github.com/apache/spark/pull/32872#pullrequestreview-703444744",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f692f971-37de-4abc-ba5f-2301221251ef",
        "parentId" : null,
        "authorId" : "9acf210d-4935-4e03-8384-d944eb558b45",
        "body" : "Here, of the 2 CustomShuffleReaderExec nodes one has \r\n[0, 0, 0, 72, 0] in CustomShuffleReader.child._resultOption\r\nand the resulting partitions are\r\n0 = {CoalescedPartitionSpec@16752} \"CoalescedPartitionSpec(3,4,Some(72))\"\r\n1 = {CoalescedPartitionSpec@16752} \"CoalescedPartitionSpec(3,4,Some(72))\"\r\n2 = {CoalescedPartitionSpec@16753} \"CoalescedPartitionSpec(4,5,Some(0))\"\r\n3 = {CoalescedPartitionSpec@16753} \"CoalescedPartitionSpec(4,5,Some(0))\"\r\n\r\n\r\nand the other has [0, 0, 0, 138, 138]\r\nAnd resulting partitions\r\n0 = {PartialReducerPartitionSpec@16800} \"PartialReducerPartitionSpec(3,0,1,72)\"\r\n1 = {PartialReducerPartitionSpec@16801} \"PartialReducerPartitionSpec(3,1,2,66)\"\r\n2 = {PartialReducerPartitionSpec@16802} \"PartialReducerPartitionSpec(4,0,1,66)\"\r\n3 = {PartialReducerPartitionSpec@16803} \"PartialReducerPartitionSpec(4,1,2,72)\"\r\n\r\nso I don't think you can make it coalesce w/o \"disabling\" skew mitigation.  In any case since this fixes the definition of \"coalesced\" I think it's ok to have to adjust a test.",
        "createdAt" : "2021-07-09T23:40:37Z",
        "updatedAt" : "2021-07-09T23:40:37Z",
        "lastEditedBy" : "9acf210d-4935-4e03-8384-d944eb558b45",
        "tags" : [
        ]
      }
    ],
    "commit" : "4bed38168439d9afb83be89f16bcc20f817eaa45",
    "line" : 26,
    "diffHunk" : "@@ -1,1 +1583,1587 @@        assert(smjWithNum.head.isSkewJoin)\n        val customReadersWithNum = collect(smjWithNum.head) {\n          case c: CustomShuffleReaderExec => c\n        }\n        assert(customReadersWithNum.nonEmpty)"
  },
  {
    "id" : "53a6b4cd-7a6d-4258-9b6b-d5df96f03c36",
    "prId" : 32602,
    "prUrl" : "https://github.com/apache/spark/pull/32602#pullrequestreview-666662654",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cf3f2bb0-4a6b-491e-8aac-ffa847a4ba65",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "We can't optimize this before this PR?",
        "createdAt" : "2021-05-24T10:38:04Z",
        "updatedAt" : "2021-05-24T10:38:04Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "6c7eacc6-156e-4f36-99b2-145044fb54ab",
        "parentId" : "cf3f2bb0-4a6b-491e-8aac-ffa847a4ba65",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "yeah, we cann't. Before we only check right side with `LeftSemi/LeftAnti`.\r\n\r\nAnd the test should use different column to do filter and join  in case of `InferFiltersFromConstraints` make right side empty.  Updated it.",
        "createdAt" : "2021-05-24T11:53:03Z",
        "updatedAt" : "2021-05-24T11:53:03Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      }
    ],
    "commit" : "624e45e27513933e9fe5615601d0dddac341a258",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +1314,1318 @@      SQLConf.AUTO_BROADCASTJOIN_THRESHOLD.key -> \"-1\") {\n      Seq(\n        // left semi join and empty left side\n        (\"SELECT * FROM (SELECT * FROM testData WHERE value = '0')t1 LEFT SEMI JOIN \" +\n          \"testData2 t2 ON t1.key = t2.a\", true),"
  },
  {
    "id" : "7dfdda5d-69e3-4ca4-981b-1121eab5ab7d",
    "prId" : 31873,
    "prUrl" : "https://github.com/apache/spark/pull/31873#pullrequestreview-614820998",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e89029c8-f289-4a62-ad97-85870fddde28",
        "parentId" : null,
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "This is true now as we eliminate join to its left side.",
        "createdAt" : "2021-03-17T21:59:42Z",
        "updatedAt" : "2021-03-18T00:31:00Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      }
    ],
    "commit" : "7f7f3347efe267e9f75c166c8fe98ef09a204db7",
    "line" : 41,
    "diffHunk" : "@@ -1,1 +1260,1264 @@        // broadcast empty right side\n        (\"SELECT /*+ broadcast(emptyTestData) */ * FROM testData LEFT ANTI JOIN emptyTestData\",\n          true),\n        // broadcast left side\n        (\"SELECT /*+ broadcast(testData) */ * FROM testData LEFT ANTI JOIN testData3\", false)"
  },
  {
    "id" : "185d4eb3-8b74-4992-b9d6-9efa90db158f",
    "prId" : 31793,
    "prUrl" : "https://github.com/apache/spark/pull/31793#pullrequestreview-608354722",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e2c30997-9318-4036-9d24-7514b63eb142",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Thank you for making a test case, @andygrove .\r\n\r\nHowever, this doesn't match with your PR description. This should be `IllegalStateException`.\r\n```\r\nAfter fixing this regression I saw:\r\n> java.lang.IllegalStateException: operating on canonicalized plan\r\n```",
        "createdAt" : "2021-03-10T05:59:54Z",
        "updatedAt" : "2021-03-10T05:59:54Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "a54ba8729a3516e1dc2373da1eab82431eedf78e",
    "line" : 25,
    "diffHunk" : "@@ -1,1 +884,888 @@      val doExecute = c.getClass.getMethod(\"doExecute\")\n      doExecute.setAccessible(true)\n      val ex = intercept[InvocationTargetException] {\n        doExecute.invoke(c)\n      }"
  },
  {
    "id" : "088e5332-b97a-4044-8efd-5e77df7d8ce4",
    "prId" : 31793,
    "prUrl" : "https://github.com/apache/spark/pull/31793#pullrequestreview-608358081",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a0769e65-0297-4f38-bde7-7d743baa41c5",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Please remove this since this is not the expected exception.",
        "createdAt" : "2021-03-10T06:00:11Z",
        "updatedAt" : "2021-03-10T06:00:11Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "78dd7e57-c172-4a19-bc7a-c3178c04ffe3",
        "parentId" : "a0769e65-0297-4f38-bde7-7d743baa41c5",
        "authorId" : "7e060620-bf02-418e-832f-c68dddfa2611",
        "body" : "Thanks for the review. The InvocationTargetException is here because the test is using reflection to invoke doExecute. This exception is wrapping the IllegalStateException. I will update the test tomorrow to make this clearer.",
        "createdAt" : "2021-03-10T06:03:42Z",
        "updatedAt" : "2021-03-10T06:03:43Z",
        "lastEditedBy" : "7e060620-bf02-418e-832f-c68dddfa2611",
        "tags" : [
        ]
      },
      {
        "id" : "8f8590b9-af11-4039-837c-a50c8fefddae",
        "parentId" : "a0769e65-0297-4f38-bde7-7d743baa41c5",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Oh, got it. I missed that.",
        "createdAt" : "2021-03-10T06:07:56Z",
        "updatedAt" : "2021-03-10T06:07:56Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "a54ba8729a3516e1dc2373da1eab82431eedf78e",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +19,23 @@\nimport java.io.File\nimport java.lang.reflect.InvocationTargetException\nimport java.net.URI\n"
  },
  {
    "id" : "a772da83-ce23-4b13-ba62-ad07464dbaf7",
    "prId" : 31793,
    "prUrl" : "https://github.com/apache/spark/pull/31793#pullrequestreview-608356357",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c29aeb97-6136-4fcc-9a6b-2cbb4c2ba30b",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "For the following question, ScalaTest's `PrivateMethodTester.invokePrivate` is recommended.\r\n> The problem is that doExecute is protected, and I can't test by calling execute because it has separate checks for canonicalized plans. What would be the recommended approach here?\r\n\r\nSo, you can revise this test case like the following.\r\n\r\n```scala\r\n-      // we can't just call execute() because that has separate checks for canonicalized plans\r\n-      val doExecute = c.getClass.getMethod(\"doExecute\")\r\n-      doExecute.setAccessible(true)\r\n-      val ex = intercept[InvocationTargetException] {\r\n-        doExecute.invoke(c)\r\n+      val doExecute = PrivateMethod[Unit](Symbol(\"doExecute\"))\r\n+      val ex = intercept[IllegalStateException] {\r\n+        c.invokePrivate(doExecute())\r\n       }\r\n-      assert(ex.getCause.getMessage === \"operating on canonicalized plan\")\r\n+      assert(ex.getMessage === \"operating on canonicalized plan\")\r\n```",
        "createdAt" : "2021-03-10T06:03:53Z",
        "updatedAt" : "2021-03-10T06:03:53Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "a54ba8729a3516e1dc2373da1eab82431eedf78e",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +887,891 @@        doExecute.invoke(c)\n      }\n      assert(ex.getCause.getMessage === \"operating on canonicalized plan\")\n    }\n  }"
  }
]