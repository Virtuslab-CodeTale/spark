[
  {
    "id" : "da06e98b-2eeb-49bc-9947-fdeab66b3ea9",
    "prId" : 32648,
    "prUrl" : "https://github.com/apache/spark/pull/32648#pullrequestreview-666447786",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "268e29cd-c47a-454b-b301-5fe86ea20dfc",
        "parentId" : null,
        "authorId" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "body" : "```\r\n/spark/sql/core/src/test/scala/org/apache/spark/sql/SparkSessionExtensionSuite.scala:512:26\r\nweaker access privileges in overriding\r\ndef getChild(x$1: Int): org.apache.spark.sql.vectorized.ColumnVector (defined in class ColumnVector)\r\n  override should be public\r\n  override protected def getChild(ordinal: Int): ColumnVector = wrapped.getChild(ordinal)\r\n\r\n```",
        "createdAt" : "2021-05-24T06:56:34Z",
        "updatedAt" : "2021-05-24T06:56:34Z",
        "lastEditedBy" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "tags" : [
        ]
      }
    ],
    "commit" : "2a867a6277c1c6c4f1a6497973588304f907bede",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +510,514 @@  override def getBinary(rowId: Int): Array[Byte] = wrapped.getBinary(rowId)\n\n  override def getChild(ordinal: Int): ColumnVector = wrapped.getChild(ordinal)\n}\n"
  },
  {
    "id" : "251217ef-7729-4756-b8f6-b3bb3f10cd53",
    "prId" : 29134,
    "prUrl" : "https://github.com/apache/spark/pull/29134#pullrequestreview-450071083",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0233f0aa-7e93-48ee-9b33-fe62c0daa5d9",
        "parentId" : null,
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "might be nice to comment what 11121 equals in terms of the execs - MyBroadcastExchangeExec, etc..",
        "createdAt" : "2020-07-16T17:39:09Z",
        "updatedAt" : "2020-07-16T19:48:54Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "2fd2c089c91df76d9ab2958e788853e954c9eb8d",
    "line" : 84,
    "diffHunk" : "@@ -1,1 +195,199 @@      // Verify that both pre and post processing of the plan worked.\n      val found = collectPlanSteps(df.queryExecution.executedPlan).sum\n      assert(found == 11121)\n      // Verify that we get back the expected, wrong, result\n      val result = df.collect()"
  },
  {
    "id" : "ac42cb2c-92c8-40b5-b3fe-962dff15fcca",
    "prId" : 27221,
    "prUrl" : "https://github.com/apache/spark/pull/27221#pullrequestreview-343257589",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "705d0efd-52e4-49e9-be95-73cffe01e6f1",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "The string format satisfies to the conditions: https://github.com/apache/spark/blob/21db2f86f7c196c535e8f0b5675ae48cb2c372f7/sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/ExpressionInfo.java#L116",
        "createdAt" : "2020-01-15T14:29:49Z",
        "updatedAt" : "2020-01-15T19:53:07Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "d6cfa93f0c61016b148a0f833c3b8ed4cefbf82a",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +340,344 @@      \"extended usage\",\n      \"    Examples:\",\n      \"\"\"\n       note\n      \"\"\","
  }
]