[
  {
    "id" : "7fe4d57a-afa3-460d-9c28-9f90c45f1cb9",
    "prId" : 31258,
    "prUrl" : "https://github.com/apache/spark/pull/31258#pullrequestreview-582567244",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a8fa0796-8405-4f93-a541-27fd7b10020d",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "is it because the DPP filter adds more projects?",
        "createdAt" : "2021-02-03T16:40:53Z",
        "updatedAt" : "2021-02-08T13:36:57Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "1e1b097c38ba468b751260b00262c21781892047",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +41,45 @@    val df = sql(query)\n    // When enabling AQE, the DPP subquery filters is replaced in runtime.\n    df.collect()\n    assertProjectExecCount(df, enabled)\n    val result = df.collect()"
  },
  {
    "id" : "ff4ad633-701d-4e44-a9e2-4f7c35dd18ec",
    "prId" : 29734,
    "prUrl" : "https://github.com/apache/spark/pull/29734#pullrequestreview-488227741",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "381cf455-5673-450f-97e6-1d5276e580ea",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Is this PR related to AQE? (the PR description does say nothing about it though)",
        "createdAt" : "2020-09-14T07:59:21Z",
        "updatedAt" : "2020-09-15T21:43:16Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "0d689e96-bfab-4959-a8c7-8209c3f4bdc6",
        "parentId" : "381cf455-5673-450f-97e6-1d5276e580ea",
        "authorId" : "b3ba992a-312c-46eb-b3c3-8d861d15ac40",
        "body" : "This rule was also added in AQE's [query stage prep rules](https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala#L88) so the test should also make sure the rule works when AQE is enabled.",
        "createdAt" : "2020-09-14T23:44:49Z",
        "updatedAt" : "2020-09-15T21:43:16Z",
        "lastEditedBy" : "b3ba992a-312c-46eb-b3c3-8d861d15ac40",
        "tags" : [
        ]
      }
    ],
    "commit" : "16ebb830d83c12165148545b6427d51df30deed6",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +28,32 @@  extends QueryTest\n    with SharedSparkSession\n    with AdaptiveSparkPlanHelper {\n\n  private def assertProjectExecCount(df: DataFrame, expected: Int): Unit = {"
  },
  {
    "id" : "31fd4351-6c75-4031-b3ab-3ac20c5b7043",
    "prId" : 29031,
    "prUrl" : "https://github.com/apache/spark/pull/29031#pullrequestreview-445374811",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f95c5312-0200-40a5-b63e-11c918eea8ac",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "can we add one more test to apply this optimization inside subqueries?",
        "createdAt" : "2020-07-09T07:56:30Z",
        "updatedAt" : "2020-08-10T18:02:22Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "394126da654c15c59a58bd906f0ca4564ce1c4e5",
    "line" : 123,
    "diffHunk" : "@@ -1,1 +121,125 @@  }\n\n  test(\"subquery\") {\n    withTempView(\"testData\") {\n      val data = spark.sparkContext.parallelize((1 to 100).map(i => Row(i, i.toString)))"
  }
]