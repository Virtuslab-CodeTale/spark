[
  {
    "id" : "9f06051f-9cd6-44ce-b647-51f63f7509b1",
    "prId" : 29975,
    "prUrl" : "https://github.com/apache/spark/pull/29975#pullrequestreview-505367306",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a40658c3-66d3-4626-bfb6-950777c6b15d",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "`failingUdf` is evaluated twice for each row previously. Now it is only once. So we need to increase range to make it throw exception as before.",
        "createdAt" : "2020-10-09T05:12:32Z",
        "updatedAt" : "2020-10-12T01:42:10Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "2414bb041f47c00de28428014372ab6c55435e56",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +269,273 @@        }\n        // this input data will fail to read middle way.\n        val input = spark.range(15).select(failingUdf('id).as('i)).select('i, -'i as 'j)\n        val e3 = intercept[SparkException] {\n          input.write.format(cls.getName).option(\"path\", path).mode(\"overwrite\").save()"
  }
]