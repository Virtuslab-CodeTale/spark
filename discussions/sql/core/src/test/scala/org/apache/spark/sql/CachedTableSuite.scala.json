[
  {
    "id" : "4e26d88b-0140-48a4-a813-f6a641fabbbc",
    "prId" : 32482,
    "prUrl" : "https://github.com/apache/spark/pull/32482#pullrequestreview-655218164",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "758b0e2e-726e-4ead-83f8-0601fec205fe",
        "parentId" : null,
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "not related this pr, but affected the new added test with `t1`.",
        "createdAt" : "2021-05-10T04:20:43Z",
        "updatedAt" : "2021-05-11T08:42:22Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      }
    ],
    "commit" : "2e8492baf2c9b6dba3bd635f2b3a25d288e7f3ab",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1176,1180 @@\n  test(\"cache supports for intervals\") {\n    withTable(\"interval_cache\", \"t1\") {\n      Seq((1, \"1 second\"), (2, \"2 seconds\"), (2, null))\n        .toDF(\"k\", \"v\").write.saveAsTable(\"interval_cache\")"
  },
  {
    "id" : "c213cd1a-1582-43ad-8886-23abeddefea2",
    "prId" : 31462,
    "prUrl" : "https://github.com/apache/spark/pull/31462#pullrequestreview-582814341",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2352df02-7127-4f6c-bc3f-5552960b9ac1",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "How about `GLOBAL TEMPORARY VIEW` as well.",
        "createdAt" : "2021-02-03T20:58:43Z",
        "updatedAt" : "2021-02-04T06:10:08Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "15cb5af4-d3a6-46f4-a2e5-371e00a01d33",
        "parentId" : "2352df02-7127-4f6c-bc3f-5552960b9ac1",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Good point. Will add test for that.",
        "createdAt" : "2021-02-03T21:21:59Z",
        "updatedAt" : "2021-02-04T06:10:08Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      }
    ],
    "commit" : "27e86350b7d3802db7e8fcd22e3ec732693570c6",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +1432,1436 @@      withSQLConf(SQLConf.STORE_ANALYZED_PLAN_FOR_VIEW.key -> storeAnalyzed.toString) {\n        withTempView(\"view1\", \"view2\") {\n          sql(\"CREATE TEMPORARY VIEW view1 AS SELECT * FROM testData WHERE key > 1\")\n          sql(\"CACHE TABLE view2 AS SELECT * FROM view1 WHERE value > 1\")\n          assert(spark.catalog.isCached(\"view2\"))"
  },
  {
    "id" : "8f4a9297-91a2-4500-90ba-6152a1661afd",
    "prId" : 31300,
    "prUrl" : "https://github.com/apache/spark/pull/31300#pullrequestreview-574873865",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "52ee1f68-d207-4d10-85e3-f097c3f24cee",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Hm? Shouldn't we invalid t2 in any of cases if its source table t1 is dropped?",
        "createdAt" : "2021-01-23T08:26:56Z",
        "updatedAt" : "2021-01-23T08:26:56Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "392b4a24-1a0c-4533-8023-d7e92f520d7a",
        "parentId" : "52ee1f68-d207-4d10-85e3-f097c3f24cee",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "If the config `STORE_ANALYZED_PLAN_FOR_VIEW` is enabled, it means we store analyzed logical plan for temp view. Therefore, even in this case `t1` is dropped, the plan won't be updated and `t2` is still in cache, and `isCached(\"t2\")` will succeed.",
        "createdAt" : "2021-01-23T16:12:58Z",
        "updatedAt" : "2021-01-23T16:12:58Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "f7d7257e-91ca-4764-a65a-dd487c41d5e3",
        "parentId" : "52ee1f68-d207-4d10-85e3-f097c3f24cee",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Oh oh I see.",
        "createdAt" : "2021-01-23T20:17:43Z",
        "updatedAt" : "2021-01-23T20:17:43Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "57a07ac4007a881957b44053bad38835895bece6",
    "line" : 77,
    "diffHunk" : "@@ -1,1 +981,985 @@                val e = intercept[AnalysisException](spark.catalog.isCached(\"t2\"))\n                assert(e.message.contains(s\"Table or view not found\"))\n              }\n            }\n          }"
  },
  {
    "id" : "6493f692-c5fc-4968-8f7c-48293b001792",
    "prId" : 30699,
    "prUrl" : "https://github.com/apache/spark/pull/30699#pullrequestreview-550508954",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f639fb14-5b95-4280-b496-40b94c73a99d",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Thanks for adding this.",
        "createdAt" : "2020-12-11T19:15:09Z",
        "updatedAt" : "2020-12-11T19:15:09Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "bcf2df7d34559bbb078e00e0ba3bca1547a0585f",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +1252,1256 @@      }\n      // cleanup the rest of the files\n      src.listFiles().foreach(_.delete())\n      src.delete()\n    }"
  },
  {
    "id" : "fb1e79a5-fb4b-4a9a-a554-4245524813b3",
    "prId" : 30187,
    "prUrl" : "https://github.com/apache/spark/pull/30187#pullrequestreview-520941035",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cb769b6c-bff0-4c9d-8230-c061b929e45a",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "does it still work if we only refresh table t?",
        "createdAt" : "2020-10-30T04:23:12Z",
        "updatedAt" : "2020-10-31T01:40:49Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "1f51a527-c724-4d5a-af6f-684cbf79880c",
        "parentId" : "cb769b6c-bff0-4c9d-8230-c061b929e45a",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "+1 for @cloud-fan 's comment. Shall we add a test case for that?",
        "createdAt" : "2020-10-30T16:17:30Z",
        "updatedAt" : "2020-10-31T01:40:49Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "3dd45837-ea56-4ef2-b5c8-4e258220ed47",
        "parentId" : "cb769b6c-bff0-4c9d-8230-c061b929e45a",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "no ... if we refresh the table selecting from `tempView2` will fail with:\r\n``` \r\n\"java.io.FileNotFoundException: File ... does not exist\r\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n```\r\n\r\n(BTW this happens without this PR if we have `CACHE TABLE t`)\r\n\r\nI think the reason is that, in the case of refresh table `t`, we'd invalidate the table relation cache in the session catalog. Since `tempView2` is a temporary view, it will just use cached plan, which still contains a reference to the relation that was just removed from the cache, and therefore its metadata is not refreshed. On the other hand, since `tempView1` is a persistent view, it will recreate the relation and the file index, and produce correct result.\r\n\r\nThe reason why `REFRESH TABLE tempView1` works is because, as opposed to invalidate the table relation cache, we'd update the cached logical plan by refreshing its metadata (e.g., file index). This way, the reference to the relation in `tempView2` also gets updated.\r\n\r\nI'm not sure if this is a desired behavior. Perhaps in the case of refreshing table `t`, we should refresh its plan in the table relation cache instead of removing it from the cache?",
        "createdAt" : "2020-10-30T17:40:23Z",
        "updatedAt" : "2020-10-31T01:40:49Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "a66237f8-4ad1-42be-9d08-1085c5902786",
        "parentId" : "cb769b6c-bff0-4c9d-8230-c061b929e45a",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "It's okay. Please add a test case with `intercept[FileNotFoundException`. The test case will monitor the behavior change across Spark versions.",
        "createdAt" : "2020-10-30T18:40:31Z",
        "updatedAt" : "2020-10-31T01:40:49Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "b427b7d1c0e5b60f96d9a38c7d00dff43d15e423",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +1222,1226 @@\n          Utils.deleteRecursively(path)\n          sql(\"REFRESH TABLE tempView1\")\n          checkAnswer(sql(\"SELECT * FROM tempView1\"), Seq.empty)\n          checkAnswer(sql(\"SELECT * FROM tempView2\"), Seq.empty)"
  },
  {
    "id" : "f9d37df4-3840-4931-a0df-06db06527102",
    "prId" : 30187,
    "prUrl" : "https://github.com/apache/spark/pull/30187#pullrequestreview-521022288",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cf9c45c4-c510-4f7d-a8f8-963853cede2a",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Ur, in this case, does `SELECT * FROM t` works?",
        "createdAt" : "2020-10-30T20:14:48Z",
        "updatedAt" : "2020-10-31T01:40:49Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "03c1c04b-a949-4010-971c-b0de747dfb22",
        "parentId" : "cf9c45c4-c510-4f7d-a8f8-963853cede2a",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Yes it works because unlike `tempView1`, this will recreate a logical relation with the updated file index. I can add a\r\n```\r\ncheckAnswer(sql(\"SELECT * FROM t\"), Seq.empty)\r\n```\r\n\r\nif you want ...",
        "createdAt" : "2020-10-30T20:20:41Z",
        "updatedAt" : "2020-10-31T01:40:49Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "5f279b83-f800-4530-8cdb-6b66b8bd08a9",
        "parentId" : "cf9c45c4-c510-4f7d-a8f8-963853cede2a",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Thanks! Yes. Please!",
        "createdAt" : "2020-10-30T20:40:16Z",
        "updatedAt" : "2020-10-31T01:40:49Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "b427b7d1c0e5b60f96d9a38c7d00dff43d15e423",
    "line" : 35,
    "diffHunk" : "@@ -1,1 +1240,1244 @@\n          Utils.deleteRecursively(path)\n          sql(\"REFRESH TABLE t\")\n          checkAnswer(sql(\"SELECT * FROM t\"), Seq.empty)\n          val exception = intercept[Exception] {"
  },
  {
    "id" : "68be23d6-9680-4858-a14c-05e9856a815e",
    "prId" : 30187,
    "prUrl" : "https://github.com/apache/spark/pull/30187#pullrequestreview-521102278",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3e79a088-0e5d-4dd0-ad28-027ba443a817",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Since the test case name is `querying cached view`, do we need `CACHE TABLE` after this line?",
        "createdAt" : "2020-10-30T22:56:21Z",
        "updatedAt" : "2020-10-31T01:40:49Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "f98989e1-5e59-41cf-8625-aa82486d5611",
        "parentId" : "3e79a088-0e5d-4dd0-ad28-027ba443a817",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "You're right. I'll remove \"cached\" from the test name instead since cache is irrelevant in this issue.",
        "createdAt" : "2020-10-31T01:39:48Z",
        "updatedAt" : "2020-10-31T01:40:49Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      }
    ],
    "commit" : "b427b7d1c0e5b60f96d9a38c7d00dff43d15e423",
    "line" : 31,
    "diffHunk" : "@@ -1,1 +1236,1240 @@          Seq((1 -> \"a\")).toDF(\"i\", \"j\").write.parquet(path.getCanonicalPath)\n          sql(s\"CREATE TABLE t USING parquet LOCATION '${path.toURI}'\")\n          sql(\"CREATE TEMPORARY VIEW tempView1 AS SELECT * FROM t\")\n          checkAnswer(sql(\"SELECT * FROM tempView1\"), Seq(Row(1, \"a\")))\n"
  },
  {
    "id" : "75cbde5e-0196-4383-b4f7-41a66444d61c",
    "prId" : 27185,
    "prUrl" : "https://github.com/apache/spark/pull/27185#pullrequestreview-378977055",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dd5186d4-2bed-4a7b-a5a5-5a1e88c286a0",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "This looks like an invalid test scope because this is testing `uncacheQuery` at line 1164 instead of `create or replace view`. Shall we revise this whole test case?",
        "createdAt" : "2020-03-22T00:14:13Z",
        "updatedAt" : "2020-03-22T07:06:08Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "32ca0bef-4fcd-4998-8520-481c4d7db2e3",
        "parentId" : "dd5186d4-2bed-4a7b-a5a5-5a1e88c286a0",
        "authorId" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "body" : "1164 and 1165 is to make sure our patch is working, no cached data leak for persisted view. Without this patch, the assert 1165 will fails. So I don't think it's an invalid test.",
        "createdAt" : "2020-03-22T06:59:13Z",
        "updatedAt" : "2020-03-22T07:06:08Z",
        "lastEditedBy" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "tags" : [
        ]
      }
    ],
    "commit" : "e59fc98dfef892673a6ecd7063fc615550f92aeb",
    "line" : 43,
    "diffHunk" : "@@ -1,1 +1162,1166 @@      spark.sharedState.cacheManager.uncacheQuery(spark.table(\"view1\"), cascade = false)\n      // make sure there is no cached data leak\n      assert(spark.sharedState.cacheManager.isEmpty)\n    }\n  }"
  }
]