[
  {
    "id" : "5645321f-df75-4e18-a236-98a2136a65ef",
    "prId" : 31449,
    "prUrl" : "https://github.com/apache/spark/pull/31449#pullrequestreview-582000090",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6ad2a6da-4986-48a3-a170-d5519d4e5cea",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Let's just ensure we test on path truncation; other cases should be tested against UtilsSuite.",
        "createdAt" : "2021-02-03T04:46:33Z",
        "updatedAt" : "2021-02-03T08:43:19Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "c6cd28914e2b582244f5121b9f307aa7987b1e23",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +125,129 @@      val dir = path.getCanonicalPath\n\n      // create a sub-directory with long name so that each root path will always exceed the limit\n      // this is to ensure we always test the case for the path truncation\n      val dataDirName = Random.alphanumeric.take(100).toList.mkString"
  },
  {
    "id" : "993fd3ae-b571-4bc6-a53e-e97fa3519c89",
    "prId" : 31435,
    "prUrl" : "https://github.com/apache/spark/pull/31435#pullrequestreview-581079227",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "081d3f5d-f4ca-4b80-9207-8b0b1479ecdf",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "I intentionally excluded the thing on closing bracket (']'), but the closing bracket will be added in any way, regardless of the current location string length. This was also not accounted as well.",
        "createdAt" : "2021-02-02T08:14:58Z",
        "updatedAt" : "2021-02-02T08:16:31Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "36943c6563278a86131e0d41b3334a538366d597",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +147,151 @@        location.get.indexOf('[') + 1, location.get.indexOf(']')).split(\", \").toSeq\n\n      // If the temp path length is less than (stop appending threshold - 1), say, 100 - 1 = 99,\n      // location should include more than one paths. Otherwise location should include only one\n      // path."
  },
  {
    "id" : "6209c570-bb77-4878-b658-9636b8c9a12d",
    "prId" : 31435,
    "prUrl" : "https://github.com/apache/spark/pull/31435#pullrequestreview-581352444",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "68b83697-22e4-42ef-8a18-f0164621b343",
        "parentId" : null,
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "We still need to check that the paths are truncated here.",
        "createdAt" : "2021-02-02T12:08:20Z",
        "updatedAt" : "2021-02-02T12:08:20Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "86bdd1ed-a223-4ed3-963e-35bfb86857b6",
        "parentId" : "68b83697-22e4-42ef-8a18-f0164621b343",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "I'm not sure I understand. Could you please elaborate?\n\nWe don't truncate the path itself, right? I think it's also something to be fixed (I'd rather want to see path being truncated with ellipses (...) instead of not adding and leaving it as it is.) but it's more likely bigger fix which may worth another fix instead of test fix.\n\nIf you meant counting the number of paths or something for edge-case, dealing with another UT would be easier, like we could simply add edge-cases there in this PR. (And that UT already tests basic functionality.)",
        "createdAt" : "2021-02-02T12:33:02Z",
        "updatedAt" : "2021-02-02T12:36:29Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "79b5a393-1e05-4f34-b769-7e402a58fe19",
        "parentId" : "68b83697-22e4-42ef-8a18-f0164621b343",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "I mean simply checking \r\n```\r\nassert(pathsInLocation.size < paths.size)\r\n```\r\nBut this is a minor comment. You can ignore it since there is UT for it already.",
        "createdAt" : "2021-02-02T12:36:54Z",
        "updatedAt" : "2021-02-02T12:36:54Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "5c1ce4e0-2eb2-48db-ab11-73015335527e",
        "parentId" : "68b83697-22e4-42ef-8a18-f0164621b343",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Yeah makes sense, but let's leave it as it is, as I think I'd propose the another form which would make clear how many elements they are instead of only showing available paths and don't say there're more.",
        "createdAt" : "2021-02-02T13:42:37Z",
        "updatedAt" : "2021-02-02T13:42:37Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "36943c6563278a86131e0d41b3334a538366d597",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +152,156 @@      // (Note we apply subtraction with 1 to count start bracket '['.)\n      if (paths.head.length < 99) {\n        assert(pathsInLocation.size >= 2)\n      } else {\n        assert(pathsInLocation.size == 1)"
  },
  {
    "id" : "570344e6-8f69-4ab8-aac3-4821f6148b4e",
    "prId" : 27021,
    "prUrl" : "https://github.com/apache/spark/pull/27021#pullrequestreview-350811744",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f136a43b-07e1-463d-96d0-230561816aa2",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "to actually test DSV2, let's call `withSQLConf` in the test and set `SQLConf.USE_V1_SOURCE_LIST` to empty.",
        "createdAt" : "2020-01-30T13:15:05Z",
        "updatedAt" : "2020-01-30T13:59:07Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "34425ad1eae32fe1ca177ad359685c410a467760",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +172,176 @@  }\n\n  test(\"SPARK-30362: test input metrics for DSV2\") {\n    withSQLConf(SQLConf.USE_V1_SOURCE_LIST.key -> \"\") {\n      Seq(\"json\", \"orc\", \"parquet\").foreach { format =>"
  },
  {
    "id" : "7966152c-761e-47ca-a921-2ef67bdfe5ca",
    "prId" : 24745,
    "prUrl" : "https://github.com/apache/spark/pull/24745#pullrequestreview-243875540",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1b34c2d7-3ae0-4960-a4c7-3fac8cfeaeca",
        "parentId" : null,
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "Actually matching `[^\\\\]]+` should be enough. Matching spaces is just for double insurance. ",
        "createdAt" : "2019-05-30T15:52:03Z",
        "updatedAt" : "2019-05-30T15:56:33Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "352d5d4e60cac095700566e2e1283d40de3aab46",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +32,36 @@\n  override protected def sparkConf: SparkConf = super.sparkConf\n    .set(\"spark.redaction.string.regex\", \"file:/[^\\\\]\\\\s]+\")\n\n  final protected def isIncluded(queryExecution: QueryExecution, msg: String): Boolean = {"
  }
]