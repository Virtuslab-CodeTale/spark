[
  {
    "id" : "7b069404-a8ff-43cb-be11-133522b1fe73",
    "prId" : 33268,
    "prUrl" : "https://github.com/apache/spark/pull/33268#pullrequestreview-702179501",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4a1e4d12-d41f-496f-91b0-fb26d681cb2b",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Could you check type of the result, please. Is it `timestamp_ntz`?",
        "createdAt" : "2021-07-08T14:48:41Z",
        "updatedAt" : "2021-07-08T14:48:46Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "046c589d-a511-4344-a55a-232182c8a5ec",
        "parentId" : "4a1e4d12-d41f-496f-91b0-fb26d681cb2b",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "Done",
        "createdAt" : "2021-07-08T15:02:40Z",
        "updatedAt" : "2021-07-08T15:02:41Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "032eb76bf00d24180392f9c1a158acb2d0826ef0",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +1408,1412 @@      new StructType().add(StructField(\"ts\", TimestampNTZType)).add(\"count\", LongType, false)\n    assert (df.schema == expectedSchema)\n    checkAnswer(df, Seq(Row(LocalDateTime.parse(ts1), 2), Row(LocalDateTime.parse(ts2), 1)))\n  }\n}"
  },
  {
    "id" : "4c650676-d296-4739-b00f-8c65183136a2",
    "prId" : 32107,
    "prUrl" : "https://github.com/apache/spark/pull/32107#pullrequestreview-634128026",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2c977a4c-de3e-4af1-bd8b-3958ea6b07ea",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Could you test more cases:\r\n1. Negative tests such as overflow\r\n2. Aggregate nulls and non-nulls\r\n3. Looking at https://github.com/apache/spark/pull/26325, we will need to add more tests as soon as we support construction of the intervals in SQL via cast or `make_interval`",
        "createdAt" : "2021-04-11T07:46:49Z",
        "updatedAt" : "2021-04-18T09:31:21Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "5b4131fe-570f-4dfa-82d1-a857b4ba0a22",
        "parentId" : "2c977a4c-de3e-4af1-bd8b-3958ea6b07ea",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "For the third suggestion, If we support intervals in SQL, we will add new test cases.",
        "createdAt" : "2021-04-13T02:52:42Z",
        "updatedAt" : "2021-04-18T09:31:21Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "d0224926ee599bb65fc4ba728a7048cfbdbf5622",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +1115,1119 @@  }\n\n  test(\"SPARK-34716: Support ANSI SQL intervals by the aggregate function `sum`\") {\n    val df = Seq((1, Period.ofMonths(10), Duration.ofDays(10)),\n      (2, Period.ofMonths(1), Duration.ofDays(1)),"
  }
]