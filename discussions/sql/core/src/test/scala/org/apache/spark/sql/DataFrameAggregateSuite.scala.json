[
  {
    "id" : "7b069404-a8ff-43cb-be11-133522b1fe73",
    "prId" : 33268,
    "prUrl" : "https://github.com/apache/spark/pull/33268#pullrequestreview-702179501",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4a1e4d12-d41f-496f-91b0-fb26d681cb2b",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Could you check type of the result, please. Is it `timestamp_ntz`?",
        "createdAt" : "2021-07-08T14:48:41Z",
        "updatedAt" : "2021-07-08T14:48:46Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "046c589d-a511-4344-a55a-232182c8a5ec",
        "parentId" : "4a1e4d12-d41f-496f-91b0-fb26d681cb2b",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "Done",
        "createdAt" : "2021-07-08T15:02:40Z",
        "updatedAt" : "2021-07-08T15:02:41Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "032eb76bf00d24180392f9c1a158acb2d0826ef0",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +1408,1412 @@      new StructType().add(StructField(\"ts\", TimestampNTZType)).add(\"count\", LongType, false)\n    assert (df.schema == expectedSchema)\n    checkAnswer(df, Seq(Row(LocalDateTime.parse(ts1), 2), Row(LocalDateTime.parse(ts2), 1)))\n  }\n}"
  },
  {
    "id" : "4c650676-d296-4739-b00f-8c65183136a2",
    "prId" : 32107,
    "prUrl" : "https://github.com/apache/spark/pull/32107#pullrequestreview-634128026",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2c977a4c-de3e-4af1-bd8b-3958ea6b07ea",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Could you test more cases:\r\n1. Negative tests such as overflow\r\n2. Aggregate nulls and non-nulls\r\n3. Looking at https://github.com/apache/spark/pull/26325, we will need to add more tests as soon as we support construction of the intervals in SQL via cast or `make_interval`",
        "createdAt" : "2021-04-11T07:46:49Z",
        "updatedAt" : "2021-04-18T09:31:21Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "5b4131fe-570f-4dfa-82d1-a857b4ba0a22",
        "parentId" : "2c977a4c-de3e-4af1-bd8b-3958ea6b07ea",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "For the third suggestion, If we support intervals in SQL, we will add new test cases.",
        "createdAt" : "2021-04-13T02:52:42Z",
        "updatedAt" : "2021-04-18T09:31:21Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "d0224926ee599bb65fc4ba728a7048cfbdbf5622",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +1115,1119 @@  }\n\n  test(\"SPARK-34716: Support ANSI SQL intervals by the aggregate function `sum`\") {\n    val df = Seq((1, Period.ofMonths(10), Duration.ofDays(10)),\n      (2, Period.ofMonths(1), Duration.ofDays(1)),"
  },
  {
    "id" : "85e294cf-bbb3-4383-85f9-b7b1161f5421",
    "prId" : 31808,
    "prUrl" : "https://github.com/apache/spark/pull/31808#pullrequestreview-609503368",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8a6da203-1bc5-4c26-89ce-8afc108c2d48",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Thank you for adding this test case. BTW, do you think we can have a narrow-downed test case in `catalyst` module instead of this test in `sql` module?",
        "createdAt" : "2021-03-11T08:07:24Z",
        "updatedAt" : "2021-03-11T08:08:17Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "dddbd94b-30da-4653-9bd8-a516f1d77418",
        "parentId" : "8a6da203-1bc5-4c26-89ce-8afc108c2d48",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "We don't have a UT for `ResolveCreateNamedStruct` yet. The bug is trivial so I didn't spend time building a new UT.",
        "createdAt" : "2021-03-11T08:34:23Z",
        "updatedAt" : "2021-03-11T08:34:24Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "a35e700696f44698e8b945f99a14f04fa266e0ab",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1093,1097 @@  }\n\n  test(\"SPARK-34713: group by CreateStruct with ExtractValue\") {\n    val structDF = Seq(Tuple1(1 -> 1)).toDF(\"col\")\n    checkAnswer(structDF.groupBy(struct($\"col._1\")).count().select(\"count\"), Row(1))"
  },
  {
    "id" : "94918c63-e49d-4573-b542-0467ee41a839",
    "prId" : 31808,
    "prUrl" : "https://github.com/apache/spark/pull/31808#pullrequestreview-610015627",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9a72f1f1-7742-4654-bc9f-dbd560837806",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Oh, since line 1102 and 1105 works and the following works, we don't care the field name at all?\r\n```scala\r\nscala> Seq(Tuple1(Map(\"b\" -> \"b\"))).toDF(\"col\").groupBy(struct($\"col.a\")).count().select(\"count\").show\r\n+-----+\r\n|count|\r\n+-----+\r\n|    1|\r\n+-----+\r\n```",
        "createdAt" : "2021-03-11T08:16:13Z",
        "updatedAt" : "2021-03-11T08:16:13Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "35c83416-bede-43d6-8a13-3a25bf3fa9ed",
        "parentId" : "9a72f1f1-7742-4654-bc9f-dbd560837806",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "cc @maropu , too.",
        "createdAt" : "2021-03-11T08:16:24Z",
        "updatedAt" : "2021-03-11T08:16:24Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "d0fd3531-021f-4bab-88af-84af72c44107",
        "parentId" : "9a72f1f1-7742-4654-bc9f-dbd560837806",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "The `col` is a map-type column and the syntax `a.b` can get map values.",
        "createdAt" : "2021-03-11T08:35:46Z",
        "updatedAt" : "2021-03-11T08:35:46Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "1880a3f9-f6a9-4bad-85f8-8155c18b2d1b",
        "parentId" : "9a72f1f1-7742-4654-bc9f-dbd560837806",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Ah, I was confused here. Thanks.",
        "createdAt" : "2021-03-11T17:20:40Z",
        "updatedAt" : "2021-03-11T17:20:41Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "a35e700696f44698e8b945f99a14f04fa266e0ab",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +1104,1108 @@\n    val nonStringMapDF = Seq(Tuple1(Map(1 -> 1))).toDF(\"col\")\n    // Spark implicit casts string literal \"a\" to int to match the key type.\n    checkAnswer(nonStringMapDF.groupBy(struct($\"col.a\")).count().select(\"count\"), Row(1))\n"
  },
  {
    "id" : "4828f7f7-b222-4d70-b469-7b6eef25542f",
    "prId" : 29983,
    "prUrl" : "https://github.com/apache/spark/pull/29983#pullrequestreview-505611130",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7ce07822-ac95-42a6-9177-d86c1f25bc31",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "How about organizing tests like this? (I think it'd better not to update the existing tests as much as possible):\r\n```\r\n  test(\"zero moments\") {\r\n    withSQLConf(SQLConf.LEGACY_CENTRAL_MOMENT_AGG_BEHAVIOR.key -> \"true\") {\r\n      // Don't touch the existing tests\r\n      val input = Seq((1, 2)).toDF(\"a\", \"b\")\r\n      checkAnswer(\r\n        input.agg(stddev($\"a\"), stddev_samp($\"a\"), stddev_pop($\"a\"), variance($\"a\"),\r\n          var_samp($\"a\"), var_pop($\"a\"), skewness($\"a\"), kurtosis($\"a\")),\r\n        Row(Double.NaN, Double.NaN, 0.0, Double.NaN, Double.NaN, 0.0,\r\n          Double.NaN, Double.NaN))\r\n\r\n      checkAnswer(\r\n        input.agg(\r\n          expr(\"stddev(a)\"),\r\n          expr(\"stddev_samp(a)\"),\r\n          expr(\"stddev_pop(a)\"),\r\n          expr(\"variance(a)\"),\r\n          expr(\"var_samp(a)\"),\r\n          expr(\"var_pop(a)\"),\r\n          expr(\"skewness(a)\"),\r\n          expr(\"kurtosis(a)\")),\r\n        Row(Double.NaN, Double.NaN, 0.0, Double.NaN, Double.NaN, 0.0,\r\n          Double.NaN, Double.NaN))\r\n    }\r\n  }\r\n\r\n  test(\"SPARK-13860: xxxx\") {\r\n    // Writes tests for the new behaviour\r\n  }\r\n```",
        "createdAt" : "2020-10-09T12:06:47Z",
        "updatedAt" : "2020-10-13T08:29:12Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "ddc522cd2a455a08e3151c363645cc3f001465d5",
    "line" : 3,
    "diffHunk" : "@@ -1,1 +456,460 @@  }\n\n  test(\"zero moments\") {\n    withSQLConf(SQLConf.LEGACY_STATISTICAL_AGGREGATE.key -> \"true\") {\n      val input = Seq((1, 2)).toDF(\"a\", \"b\")"
  },
  {
    "id" : "7f3b0ebc-3e9b-4477-9c0f-2c7cc2cb2504",
    "prId" : 28496,
    "prUrl" : "https://github.com/apache/spark/pull/28496#pullrequestreview-411799369",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "37623baf-7381-446a-8799-13774d118003",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "ditto",
        "createdAt" : "2020-05-14T13:24:27Z",
        "updatedAt" : "2020-05-14T13:41:59Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "493157a3b97616d221ec2b5ddf1a21cdf9a1a3f4",
    "line" : 34,
    "diffHunk" : "@@ -1,1 +1004,1008 @@          checkAnswer(df, Row(\"str1\") :: Nil)\n\n          // test ObjectHashAggregateExec\n          df = sql(\"select collect_list(d), sum(if(c > (select a from t1), d, 0)) as csum from t2\")\n          assert(df.queryExecution.executedPlan"
  },
  {
    "id" : "8e65c1fd-0c25-4bdf-be9c-2958fbc460c2",
    "prId" : 28351,
    "prUrl" : "https://github.com/apache/spark/pull/28351#pullrequestreview-403989420",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2068bff2-c1ad-4c62-9233-aeadc4133496",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "nit: `collect_set() of BinaryType should not return duplicate elements`?",
        "createdAt" : "2020-05-01T02:11:10Z",
        "updatedAt" : "2020-05-01T09:44:24Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "67f55a6e4ce9bb53934290e5140542947ce28f23",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +531,535 @@  }\n\n  test(\"SPARK-31500: collect_set() of BinaryType returns duplicate elements\") {\n    val bytesTest1 = \"test1\".getBytes\n    val bytesTest2 = \"test2\".getBytes"
  },
  {
    "id" : "42817c28-89fc-4735-b0f4-b2744dc94c47",
    "prId" : 27872,
    "prUrl" : "https://github.com/apache/spark/pull/27872#pullrequestreview-372554792",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "77c76747-2517-4a2a-bd80-54c0f8ca9a71",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Originally I was crafting the patch against Spark 2.3 - in Spark 2.3, setting this to 100 throws exception which is not from Janino bug, but from either hitting 64KB limit or parameter limitation on method signature. (That's why I added the details on exceptions when the value exceeds upper limit.)\r\n\r\nFor Spark 2.3, `70` is the thing making `switch statement` failing and `if ~ else if ~ else statement` passing.",
        "createdAt" : "2020-03-11T08:32:28Z",
        "updatedAt" : "2020-03-12T03:25:43Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "4bd12d87c0789ec8a3932078e1241bfe49e599a9",
    "line" : 42,
    "diffHunk" : "@@ -1,1 +987,991 @@      // Note that the value depends on the Spark logic as well - different Spark versions may\n      // require different value to ensure the test failing with switch statement.\n      val numNewFields = 100\n\n      df = df.withColumns("
  },
  {
    "id" : "4163a04a-a972-4986-acf5-51d33b9eca6b",
    "prId" : 27872,
    "prUrl" : "https://github.com/apache/spark/pull/27872#pullrequestreview-373991993",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "801979e4-b97f-49be-a218-085e8fdd1ab1",
        "parentId" : null,
        "authorId" : "99e31844-a5b3-48a2-af71-412edb607a13",
        "body" : "nit: How about `(1 to numNewFields)`?",
        "createdAt" : "2020-03-12T18:52:56Z",
        "updatedAt" : "2020-03-12T18:52:56Z",
        "lastEditedBy" : "99e31844-a5b3-48a2-af71-412edb607a13",
        "tags" : [
        ]
      },
      {
        "id" : "1646fd36-86e2-43c0-8e35-8573c2103eda",
        "parentId" : "801979e4-b97f-49be-a218-085e8fdd1ab1",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "I was using `steps` parameter and eventually removed it. Given we seem to be between neutral to negative on adopting this patch, I'll defer addressing the nit for now.",
        "createdAt" : "2020-03-13T01:13:37Z",
        "updatedAt" : "2020-03-13T01:13:37Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "4bd12d87c0789ec8a3932078e1241bfe49e599a9",
    "line" : 51,
    "diffHunk" : "@@ -1,1 +996,1000 @@      )\n\n      val aggExprs: Array[Column] = Range(1, numNewFields).map { idx =>\n        if (idx % 2 == 0) {\n          coalesce(countDistinct(s\"a$idx\"), lit(0))"
  },
  {
    "id" : "f694fbe9-8b01-414a-86df-e3dc4900079c",
    "prId" : 26813,
    "prUrl" : "https://github.com/apache/spark/pull/26813#pullrequestreview-350948652",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2bd1c655-eb85-46c2-9cdc-ea280ec1def3",
        "parentId" : null,
        "authorId" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "body" : "Isn't this a bug? Maybe the `conf` passed in `AdaptiveSparkPlanExec` is not right?",
        "createdAt" : "2020-01-30T16:12:20Z",
        "updatedAt" : "2020-01-30T16:12:21Z",
        "lastEditedBy" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "tags" : [
        ]
      }
    ],
    "commit" : "8b5e7442c63fe326db7c7f46f7a194fbae8f0d46",
    "line" : 26,
    "diffHunk" : "@@ -1,1 +618,622 @@        (SQLConf.USE_OBJECT_HASH_AGG.key, useObjectHashAgg.toString),\n        (SQLConf.ADAPTIVE_EXECUTION_ENABLED.key -> \"false\")) {\n        // When enable AQE, the WholeStageCodegenExec is added during QueryStageExec.\n\n        val df = Seq((\"1\", 1), (\"1\", 2), (\"2\", 3), (\"2\", 4)).toDF(\"x\", \"y\")"
  },
  {
    "id" : "1efc9430-f5a6-4f96-8cfe-6b032b0618a8",
    "prId" : 24557,
    "prUrl" : "https://github.com/apache/spark/pull/24557#pullrequestreview-235164286",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "34f71fea-f265-4fa2-92b2-ef172a3a5f98",
        "parentId" : null,
        "authorId" : "a11722a8-ecfa-4827-88e3-4d708f023846",
        "body" : "This returns `null` because all values of the ordering column are `null`? That seems to match Presto behavior:\r\n\r\n```\r\nSELECT max_by(x, y) FROM (\r\n  VALUES\r\n    ('a', null),\r\n    ('b', null)\r\n) AS tab (x, y)\r\n```\r\n\r\nalso returns `null` in Presto :thumbsup:",
        "createdAt" : "2019-05-08T16:16:50Z",
        "updatedAt" : "2019-05-13T08:40:13Z",
        "lastEditedBy" : "a11722a8-ecfa-4827-88e3-4d708f023846",
        "tags" : [
        ]
      },
      {
        "id" : "2e1a4e01-9b4c-480b-98ba-be2365c9f44f",
        "parentId" : "34f71fea-f265-4fa2-92b2-ef172a3a5f98",
        "authorId" : "a11722a8-ecfa-4827-88e3-4d708f023846",
        "body" : "This makes sense if you think of this function as being semantically equivalent to \r\n\r\n```\r\nSELECT first(x) FROM tab WHERE y = max(y)\r\n```",
        "createdAt" : "2019-05-08T16:37:37Z",
        "updatedAt" : "2019-05-13T08:40:13Z",
        "lastEditedBy" : "a11722a8-ecfa-4827-88e3-4d708f023846",
        "tags" : [
        ]
      }
    ],
    "commit" : "05f1767dc49e6caebd91abf207014bcd8029b4f1",
    "line" : 31,
    "diffHunk" : "@@ -1,1 +810,814 @@\n    checkAnswer(\n      sql(\"SELECT max_by(x, y) FROM VALUES (('a', null)), (('b', null)) AS tab(x, y)\"),\n      Row(null) :: Nil\n    )"
  },
  {
    "id" : "70d6e429-5948-47b8-b7e3-969be3021c45",
    "prId" : 24335,
    "prUrl" : "https://github.com/apache/spark/pull/24335#pullrequestreview-247269587",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ad81da3d-39ac-4a7e-a85c-13f46ecc5a33",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Since `COUNT_IF` can be used in `HAVING` clauses, could you add more tests like the followings?\r\n```scala\r\nscala> sql(\"SELECT x FROM tempView GROUP BY x HAVING COUNT_IF(y % 2 = 0) = 1\").show\r\n+---+\r\n|  x|\r\n+---+\r\n|  a|\r\n+---+\r\n\r\nscala> sql(\"SELECT x FROM tempView GROUP BY x HAVING COUNT_IF(y % 2 = 0) = 2\").show\r\n+---+\r\n|  x|\r\n+---+\r\n|  b|\r\n+---+\r\n```\r\n",
        "createdAt" : "2019-06-07T16:52:05Z",
        "updatedAt" : "2019-06-10T04:23:47Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "ff61d001-27f8-45f9-9c24-3049bb39b27d",
        "parentId" : "ad81da3d-39ac-4a7e-a85c-13f46ecc5a33",
        "authorId" : "53a170d8-4931-4d25-9c47-ee1d36a0f726",
        "body" : "Ok, I just added additional cases.",
        "createdAt" : "2019-06-07T19:29:11Z",
        "updatedAt" : "2019-06-10T04:23:47Z",
        "lastEditedBy" : "53a170d8-4931-4d25-9c47-ee1d36a0f726",
        "tags" : [
        ]
      }
    ],
    "commit" : "c0a32897b108c2a0c90e6f32e0dd5bee8509e3be",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +911,915 @@        sql(\"SELECT x, COUNT_IF(NULL), COUNT_IF(y % 2 = 0), COUNT_IF(y % 2 <> 0), \" +\n          \"COUNT_IF(y IS NULL) FROM tempView GROUP BY x\"),\n        Row(\"a\", 0L, 1L, 2L, 1L) :: Row(\"b\", 0L, 2L, 1L, 1L) :: Nil)\n\n      checkAnswer("
  }
]