[
  {
    "id" : "7b069404-a8ff-43cb-be11-133522b1fe73",
    "prId" : 33268,
    "prUrl" : "https://github.com/apache/spark/pull/33268#pullrequestreview-702179501",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4a1e4d12-d41f-496f-91b0-fb26d681cb2b",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Could you check type of the result, please. Is it `timestamp_ntz`?",
        "createdAt" : "2021-07-08T14:48:41Z",
        "updatedAt" : "2021-07-08T14:48:46Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "046c589d-a511-4344-a55a-232182c8a5ec",
        "parentId" : "4a1e4d12-d41f-496f-91b0-fb26d681cb2b",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "Done",
        "createdAt" : "2021-07-08T15:02:40Z",
        "updatedAt" : "2021-07-08T15:02:41Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "032eb76bf00d24180392f9c1a158acb2d0826ef0",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +1408,1412 @@      new StructType().add(StructField(\"ts\", TimestampNTZType)).add(\"count\", LongType, false)\n    assert (df.schema == expectedSchema)\n    checkAnswer(df, Seq(Row(LocalDateTime.parse(ts1), 2), Row(LocalDateTime.parse(ts2), 1)))\n  }\n}"
  },
  {
    "id" : "4c650676-d296-4739-b00f-8c65183136a2",
    "prId" : 32107,
    "prUrl" : "https://github.com/apache/spark/pull/32107#pullrequestreview-634128026",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2c977a4c-de3e-4af1-bd8b-3958ea6b07ea",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Could you test more cases:\r\n1. Negative tests such as overflow\r\n2. Aggregate nulls and non-nulls\r\n3. Looking at https://github.com/apache/spark/pull/26325, we will need to add more tests as soon as we support construction of the intervals in SQL via cast or `make_interval`",
        "createdAt" : "2021-04-11T07:46:49Z",
        "updatedAt" : "2021-04-18T09:31:21Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "5b4131fe-570f-4dfa-82d1-a857b4ba0a22",
        "parentId" : "2c977a4c-de3e-4af1-bd8b-3958ea6b07ea",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "For the third suggestion, If we support intervals in SQL, we will add new test cases.",
        "createdAt" : "2021-04-13T02:52:42Z",
        "updatedAt" : "2021-04-18T09:31:21Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "d0224926ee599bb65fc4ba728a7048cfbdbf5622",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +1115,1119 @@  }\n\n  test(\"SPARK-34716: Support ANSI SQL intervals by the aggregate function `sum`\") {\n    val df = Seq((1, Period.ofMonths(10), Duration.ofDays(10)),\n      (2, Period.ofMonths(1), Duration.ofDays(1)),"
  },
  {
    "id" : "85e294cf-bbb3-4383-85f9-b7b1161f5421",
    "prId" : 31808,
    "prUrl" : "https://github.com/apache/spark/pull/31808#pullrequestreview-609503368",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8a6da203-1bc5-4c26-89ce-8afc108c2d48",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Thank you for adding this test case. BTW, do you think we can have a narrow-downed test case in `catalyst` module instead of this test in `sql` module?",
        "createdAt" : "2021-03-11T08:07:24Z",
        "updatedAt" : "2021-03-11T08:08:17Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "dddbd94b-30da-4653-9bd8-a516f1d77418",
        "parentId" : "8a6da203-1bc5-4c26-89ce-8afc108c2d48",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "We don't have a UT for `ResolveCreateNamedStruct` yet. The bug is trivial so I didn't spend time building a new UT.",
        "createdAt" : "2021-03-11T08:34:23Z",
        "updatedAt" : "2021-03-11T08:34:24Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "a35e700696f44698e8b945f99a14f04fa266e0ab",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1093,1097 @@  }\n\n  test(\"SPARK-34713: group by CreateStruct with ExtractValue\") {\n    val structDF = Seq(Tuple1(1 -> 1)).toDF(\"col\")\n    checkAnswer(structDF.groupBy(struct($\"col._1\")).count().select(\"count\"), Row(1))"
  },
  {
    "id" : "94918c63-e49d-4573-b542-0467ee41a839",
    "prId" : 31808,
    "prUrl" : "https://github.com/apache/spark/pull/31808#pullrequestreview-610015627",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9a72f1f1-7742-4654-bc9f-dbd560837806",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Oh, since line 1102 and 1105 works and the following works, we don't care the field name at all?\r\n```scala\r\nscala> Seq(Tuple1(Map(\"b\" -> \"b\"))).toDF(\"col\").groupBy(struct($\"col.a\")).count().select(\"count\").show\r\n+-----+\r\n|count|\r\n+-----+\r\n|    1|\r\n+-----+\r\n```",
        "createdAt" : "2021-03-11T08:16:13Z",
        "updatedAt" : "2021-03-11T08:16:13Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "35c83416-bede-43d6-8a13-3a25bf3fa9ed",
        "parentId" : "9a72f1f1-7742-4654-bc9f-dbd560837806",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "cc @maropu , too.",
        "createdAt" : "2021-03-11T08:16:24Z",
        "updatedAt" : "2021-03-11T08:16:24Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "d0fd3531-021f-4bab-88af-84af72c44107",
        "parentId" : "9a72f1f1-7742-4654-bc9f-dbd560837806",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "The `col` is a map-type column and the syntax `a.b` can get map values.",
        "createdAt" : "2021-03-11T08:35:46Z",
        "updatedAt" : "2021-03-11T08:35:46Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "1880a3f9-f6a9-4bad-85f8-8155c18b2d1b",
        "parentId" : "9a72f1f1-7742-4654-bc9f-dbd560837806",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Ah, I was confused here. Thanks.",
        "createdAt" : "2021-03-11T17:20:40Z",
        "updatedAt" : "2021-03-11T17:20:41Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "a35e700696f44698e8b945f99a14f04fa266e0ab",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +1104,1108 @@\n    val nonStringMapDF = Seq(Tuple1(Map(1 -> 1))).toDF(\"col\")\n    // Spark implicit casts string literal \"a\" to int to match the key type.\n    checkAnswer(nonStringMapDF.groupBy(struct($\"col.a\")).count().select(\"count\"), Row(1))\n"
  }
]