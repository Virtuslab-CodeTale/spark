[
  {
    "id" : "e92c4472-83ec-4a6b-a752-db9dcbc265b3",
    "prId" : 31352,
    "prUrl" : "https://github.com/apache/spark/pull/31352#pullrequestreview-576729490",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bf0ab5c3-0680-4bcf-b47e-7933eb899ede",
        "parentId" : null,
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "Note that the existing test with the same query worked fine because `r` is a dataframe temp view, which doesn't have the `View` node.",
        "createdAt" : "2021-01-26T20:23:04Z",
        "updatedAt" : "2021-01-30T19:30:01Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "76772e720c693d26f6b6befdbabd8652d9180acb",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +519,523 @@        sql(\"create view vr as select * from tr\")\n        checkAnswer(\n          sql(\"select a, (select sum(d) from vr where a = c) sum_d from l l1 group by 1, 2\"),\n          Row(1, null) :: Row(2, 6.0) :: Row(3, 2.0) :: Row(null, null) :: Row(6, null) :: Nil)\n      }"
  },
  {
    "id" : "2c71d409-be41-418f-9575-4e2f35079906",
    "prId" : 26437,
    "prUrl" : "https://github.com/apache/spark/pull/26437#pullrequestreview-338439424",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2ce837d6-fa5f-4cc4-b944-a65e5acfbcd9",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Why need to change the existing test?",
        "createdAt" : "2020-01-05T18:06:14Z",
        "updatedAt" : "2020-01-06T03:57:21Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "b59522db-84bf-4d18-bdfe-27a651379e0f",
        "parentId" : "2ce837d6-fa5f-4cc4-b944-a65e5acfbcd9",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> Why need to change the existing test?\r\n\r\nhttps://github.com/apache/spark/pull/26437#discussion_r362742121",
        "createdAt" : "2020-01-06T01:43:07Z",
        "updatedAt" : "2020-01-06T03:57:21Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "26258b0bb285644ea9d1b43f7ac20a7e02c5d6f4",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +894,898 @@            |SELECT * FROM t1 a\n            |WHERE\n            |NOT EXISTS (SELECT * FROM t1 b WHERE a.i = b.i)\n          \"\"\".stripMargin\n        val optimizedPlan = sql(sqlText).queryExecution.optimizedPlan"
  },
  {
    "id" : "e4e785f6-dde5-48d1-a3ec-f7530c4705fd",
    "prId" : 25854,
    "prUrl" : "https://github.com/apache/spark/pull/25854#pullrequestreview-311504266",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "35d00fb1-df41-4798-991d-2223dbc13df8",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "Can we move it to SQLQueryTestSuite? \r\n\r\nIt sounds like it does not contain any test case that check the EXISTS subquery? Could you also add it?",
        "createdAt" : "2019-11-05T05:02:19Z",
        "updatedAt" : "2019-11-05T05:02:20Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "26c2cb1a-d348-44d1-a6ed-5492fbab2933",
        "parentId" : "35d00fb1-df41-4798-991d-2223dbc13df8",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> Can we move it to SQLQueryTestSuite?\r\n> \r\n> It sounds like it does not contain any test case that check the EXISTS subquery? Could you also add it?\r\n\r\nOk, will raise a pr follow your comment.",
        "createdAt" : "2019-11-05T05:10:27Z",
        "updatedAt" : "2019-11-05T05:10:27Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "307802a5e599fadfafc9fb4ffe2ccbd10d60f6ba",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +205,209 @@  }\n\n  test(\"SPARK-29145: JOIN Condition use QueryList\") {\n    withTempView(\"s1\", \"s2\", \"s3\") {\n      Seq(1, 3, 5, 7, 9).toDF(\"id\").createOrReplaceTempView(\"s1\")"
  },
  {
    "id" : "1d31e006-8dc7-4e1f-8f25-30be63c1b5b5",
    "prId" : 25204,
    "prUrl" : "https://github.com/apache/spark/pull/25204#pullrequestreview-267453837",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2739b718-c683-4cde-a223-8d7a75cee708",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "BTW, we should add `assume(shouldTestPythonUDFs)`. Maybe it's not a biggie in general but it can matter in other venders' testing base. For instance, if somebody launches a test in a minimal docker image, it might make the tests failed suddenly.\r\n\r\nThis skipping stuff isn't completely new in our test base. See `TestUtils.testCommandAvailable` for instance.",
        "createdAt" : "2019-07-27T03:32:46Z",
        "updatedAt" : "2019-07-27T03:32:47Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "fd296776b826f6ce041a52cdc4dce4e278d3d786",
    "line" : 146,
    "diffHunk" : "@@ -1,1 +1527,1531 @@\n    val pythonTestUDF = TestPythonUDF(name = \"udf\")\n    registerTestUDF(pythonTestUDF, spark)\n\n    checkAnswer("
  },
  {
    "id" : "4dc4872b-8df7-46f9-af8a-4f9449c2ee0a",
    "prId" : 25008,
    "prUrl" : "https://github.com/apache/spark/pull/25008#pullrequestreview-259785836",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1e1f6bda-852a-4376-8f2b-9711dc619278",
        "parentId" : null,
        "authorId" : "0f2e4443-2e01-4589-9e1c-17979892188b",
        "body" : "Ok, so `ColumnarToRowExec` is here because `InputAdapter` may `supportColumnar`, right? Why is `InputAdapter` getting added here?",
        "createdAt" : "2019-07-09T17:34:12Z",
        "updatedAt" : "2019-07-10T15:36:54Z",
        "lastEditedBy" : "0f2e4443-2e01-4589-9e1c-17979892188b",
        "tags" : [
        ]
      },
      {
        "id" : "a0128bb9-d141-478d-a118-512189f731cd",
        "parentId" : "1e1f6bda-852a-4376-8f2b-9711dc619278",
        "authorId" : "b19fe247-920f-40ae-83e7-6b8ec9979f6b",
        "body" : "WholeStageCodeGen marks the end of a code generation stage.  InputAdapter marks the beginning of a code generation stage.  So what we had before was a WholeStageCodeGen that had it's first entry a FileSourceScanExec because before this change FileSourceScanExec supported code generation to convert ColumnarBatchs into rows.  The InputAdapter would logically have been a child of FileSourceScanExec, but it has no children so it is not there.\r\n\r\nAfter this change ColumnarToRowExec is the only thing in the code generation stage, so it is flanked by the WholeStageCodegenExec and the InputAdaptor.  FileSourceScanExec is returning batches and is not doing code gen because it is not needed any longer.",
        "createdAt" : "2019-07-09T22:10:30Z",
        "updatedAt" : "2019-07-10T15:36:54Z",
        "lastEditedBy" : "b19fe247-920f-40ae-83e7-6b8ec9979f6b",
        "tags" : [
        ]
      }
    ],
    "commit" : "2cce2fa9e057cb379e628fe01ea2cef280a9b198",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +1294,1298 @@      // need to execute the query before we can examine fs.inputRDDs()\n      assert(df.queryExecution.executedPlan match {\n        case WholeStageCodegenExec(ColumnarToRowExec(InputAdapter(\n            fs @ FileSourceScanExec(_, _, _, partitionFilters, _, _, _), _))) =>\n          partitionFilters.exists(ExecSubqueryExpression.hasSubquery) &&"
  },
  {
    "id" : "88bc3561-c4fd-4e36-8b61-d5b1cec09c7c",
    "prId" : 24652,
    "prUrl" : "https://github.com/apache/spark/pull/24652#pullrequestreview-242052752",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f17d725e-ede9-44a4-a9c8-de06d9a1fba4",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Thank you so much for updating, @dilipbiswal !",
        "createdAt" : "2019-05-26T19:44:45Z",
        "updatedAt" : "2019-05-26T21:43:47Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "4d6ab9af12218a7a5d6550760c0610e659eec0c9",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +1381,1385 @@        s\n    }\n    assert(subqueryExecs.forall(_.name.startsWith(\"scalar-subquery#\")),\n          \"SubqueryExec name should start with scalar-subquery#\")\n  }"
  },
  {
    "id" : "8a7c3546-4e53-438e-9b37-bce756517168",
    "prId" : 24344,
    "prUrl" : "https://github.com/apache/spark/pull/24344#pullrequestreview-225451134",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c960f665-6598-42c6-b7f6-34a033dc731d",
        "parentId" : null,
        "authorId" : "4db0fc8e-ae25-4ad9-989f-2fb7198f797a",
        "body" : "We may need more tests here, but I'm not sure what case to test.",
        "createdAt" : "2019-04-11T10:25:33Z",
        "updatedAt" : "2019-05-22T07:43:45Z",
        "lastEditedBy" : "4db0fc8e-ae25-4ad9-989f-2fb7198f797a",
        "tags" : [
        ]
      }
    ],
    "commit" : "86f6c9deb99fcf4eb28b29a1dfe6beea68bedb38",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +1373,1377 @@  }\n\n  test(\"SPARK-27440: Rewrite Uncorrelated Subquery\") {\n    withTempView(\"t1\", \"t2\") {\n      sql(\"create temporary view t1 as select * from values ('b',null), (null, 1) as t1(t1a, t1b)\")"
  },
  {
    "id" : "72e09f2e-5fab-4fae-b83c-5a4d4bfd944c",
    "prId" : 24344,
    "prUrl" : "https://github.com/apache/spark/pull/24344#pullrequestreview-228126274",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "04d8c4e6-de05-4762-a098-19c24c658ac0",
        "parentId" : null,
        "authorId" : "4db0fc8e-ae25-4ad9-989f-2fb7198f797a",
        "body" : "I changed this test because it focuses on correlated subquery which will rewrite as join.",
        "createdAt" : "2019-04-18T07:00:19Z",
        "updatedAt" : "2019-05-22T07:43:45Z",
        "lastEditedBy" : "4db0fc8e-ae25-4ad9-989f-2fb7198f797a",
        "tags" : [
        ]
      }
    ],
    "commit" : "86f6c9deb99fcf4eb28b29a1dfe6beea68bedb38",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +894,898 @@            |SELECT * FROM t1\n            |WHERE\n            |NOT EXISTS (SELECT * FROM t1 AS t2 WHERE t2.i = t1.i)\n          \"\"\".stripMargin\n        val optimizedPlan = sql(sqlText).queryExecution.optimizedPlan"
  }
]