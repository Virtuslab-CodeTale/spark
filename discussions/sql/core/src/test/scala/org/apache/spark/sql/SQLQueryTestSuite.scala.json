[
  {
    "id" : "2b33c892-e8f5-459e-90ef-e439a6c7d0fd",
    "prId" : 32648,
    "prUrl" : "https://github.com/apache/spark/pull/32648#pullrequestreview-667647992",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9a99f207-eb56-4ab2-a16d-b8d968d5371a",
        "parentId" : null,
        "authorId" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "body" : "```\r\n[ERROR] [Error] /spark/sql/core/src/test/scala/org/apache/spark/sql/SQLQueryTestSuite.scala:225: The outer reference in this type test cannot be checked at run time.\r\n[ERROR] [Error] /spark/sql/core/src/test/scala/org/apache/spark/sql/SQLQueryTestSuite.scala:357: The outer reference in this type test cannot be checked at run time.\r\n[ERROR] [Error] /spark/sql/core/src/test/scala/org/apache/spark/sql/SQLQueryTestSuite.scala:363: The outer reference in this type test cannot be checked at run time.\r\n[ERROR] [Error] /spark/sql/core/src/test/scala/org/apache/spark/sql/SQLQueryTestSuite.scala:371: The outer reference in this type test cannot be checked at run time.\r\n[ERROR] [Error] /spark/sql/core/src/test/scala/org/apache/spark/sql/SQLQueryTestSuite.scala:414: The outer reference in this type test cannot be checked at run time.\r\n```",
        "createdAt" : "2021-05-24T06:50:39Z",
        "updatedAt" : "2021-05-24T06:50:39Z",
        "lastEditedBy" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "tags" : [
        ]
      },
      {
        "id" : "1f3db27c-378b-4aa3-80c5-773e56254081",
        "parentId" : "9a99f207-eb56-4ab2-a16d-b8d968d5371a",
        "authorId" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "body" : "@som-snytt It seems that this compilation fix is related [https://github.com/scala/scala/pull/9504](https://github.com/scala/scala/pull/9504).\r\n\r\nSo I have another question to ask you for help, `UDFTest`  is an internal `trait` defined in `SQLQueryTestSuite`,  must full path be used for this case in future Scala versions?\r\n\r\n\r\n\r\n",
        "createdAt" : "2021-05-25T08:58:02Z",
        "updatedAt" : "2021-05-25T08:58:02Z",
        "lastEditedBy" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "tags" : [
        ]
      },
      {
        "id" : "52becfa5-7bfc-4cef-9d55-a1b8e91e226e",
        "parentId" : "9a99f207-eb56-4ab2-a16d-b8d968d5371a",
        "authorId" : "6d99b742-247b-435a-a34f-d9bcc594780f",
        "body" : "I don't have a precise answer, but I would expect ongoing improvements and also some noise. You can use `-Wconf` to suppress (or escalate) the message. I will try to take another look tomorrow; I haven't followed the pattern matching changes yet.",
        "createdAt" : "2021-05-25T10:05:15Z",
        "updatedAt" : "2021-05-25T10:05:15Z",
        "lastEditedBy" : "6d99b742-247b-435a-a34f-d9bcc594780f",
        "tags" : [
        ]
      },
      {
        "id" : "b2f10e17-41c2-454c-a91e-662226c0f893",
        "parentId" : "9a99f207-eb56-4ab2-a16d-b8d968d5371a",
        "authorId" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "body" : "thanks, I tried to suppress this behavior in the past, but it seems that this is a compilation error now, I'll try it again",
        "createdAt" : "2021-05-25T10:17:31Z",
        "updatedAt" : "2021-05-25T10:17:31Z",
        "lastEditedBy" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "tags" : [
        ]
      }
    ],
    "commit" : "2a867a6277c1c6c4f1a6497973588304f907bede",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +223,227 @@      ignore(testCase.name) { /* Do nothing */ }\n    } else testCase match {\n      case udfTestCase: SQLQueryTestSuite#UDFTest\n          if udfTestCase.udf.isInstanceOf[TestPythonUDF] && !shouldTestPythonUDFs =>\n        ignore(s\"${testCase.name} is skipped because \" +"
  },
  {
    "id" : "76d451bf-031d-44a6-9729-7c045f9e7768",
    "prId" : 31996,
    "prUrl" : "https://github.com/apache/spark/pull/31996#pullrequestreview-624196962",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a28d6c46-051a-484e-8251-93d5d3f279e2",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "we need to do the same in thriftserver query test.",
        "createdAt" : "2021-03-30T12:31:06Z",
        "updatedAt" : "2021-03-30T16:24:17Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "cbe3248fa40a1af90dca73e071def2d54d30cf0d",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +384,388 @@    }\n    // TODO(SPARK-34905): Enable ANSI intervals in SQLQueryTestSuite\n    localSparkSession.conf.set(SQLConf.LEGACY_INTERVAL_ENABLED.key, true)\n\n    if (configSet.nonEmpty) {"
  },
  {
    "id" : "fa84dd8b-afac-4f8b-ae30-1d02a6f2c957",
    "prId" : 31946,
    "prUrl" : "https://github.com/apache/spark/pull/31946#pullrequestreview-619286834",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "961188ca-099a-40e0-a4a8-3c99d8f99676",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "I just removed the unnecessary part.",
        "createdAt" : "2021-03-24T03:48:05Z",
        "updatedAt" : "2021-03-24T03:48:05Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "a317cee9aa6c85743f98424d5a34a00e3a5c1b80",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +405,409 @@        s\"-- Automatically generated by ${getClass.getSimpleName}\\n\" +\n        s\"-- Number of queries: ${outputs.size}\\n\\n\\n\" +\n        outputs.mkString(\"\\n\\n\\n\") + \"\\n\"\n      }\n      val resultFile = new File(testCase.resultFile)"
  },
  {
    "id" : "e5129e15-ecd6-48cc-8923-ec1af682d16b",
    "prId" : 31886,
    "prUrl" : "https://github.com/apache/spark/pull/31886#pullrequestreview-615937859",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "812adc18-4969-4e86-ab5b-fcc127e6bbe9",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "I just removed the unnecessary code here.",
        "createdAt" : "2021-03-18T22:54:52Z",
        "updatedAt" : "2021-03-30T12:30:15Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "841005c36cda15352cc026c67401571e43e7610a",
    "line" : 52,
    "diffHunk" : "@@ -1,1 +396,400 @@        s\"-- Automatically generated by ${getClass.getSimpleName}\\n\" +\n        s\"-- Number of queries: ${outputs.size}\\n\\n\\n\" +\n        outputs.mkString(\"\\n\\n\\n\") + \"\\n\"\n      }\n      val resultFile = new File(testCase.resultFile)"
  },
  {
    "id" : "1eabf35e-c03c-4950-9681-cf89fa8f2650",
    "prId" : 31466,
    "prUrl" : "https://github.com/apache/spark/pull/31466#pullrequestreview-586088538",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "be194adf-7e61-472c-aeb0-17d3cc640c84",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "`SQL test 'transform.sql' will use` -> `SQL test 'transform.sql' which uses`",
        "createdAt" : "2021-02-08T13:17:23Z",
        "updatedAt" : "2021-02-08T13:17:24Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "7452c471-76b3-4b04-906d-4f90ba6600f8",
        "parentId" : "be194adf-7e61-472c-aeb0-17d3cc640c84",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "When there are other PRs in the future, change them by the way.",
        "createdAt" : "2021-02-09T02:04:39Z",
        "updatedAt" : "2021-02-09T02:04:39Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "01caf85014a7733402a0abcb091fa739eeafa32d",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +155,159 @@    .set(SQLConf.SHUFFLE_PARTITIONS, 4)\n\n  // SPARK-32106 Since we add SQL test 'transform.sql' will use `cat` command,\n  // here we need to ignore it.\n  private val otherIgnoreList ="
  },
  {
    "id" : "dedf669c-29b8-4c8c-a85a-85da3654ddf5",
    "prId" : 29085,
    "prUrl" : "https://github.com/apache/spark/pull/29085#pullrequestreview-452980893",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "df6bac75-1bd9-4d4c-bfe0-e15b9849be66",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Please add some comments about why this needed here.",
        "createdAt" : "2020-07-22T01:44:33Z",
        "updatedAt" : "2020-07-23T03:08:36Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "8b57648f-63c3-432e-ba19-c529a3bddc5a",
        "parentId" : "df6bac75-1bd9-4d4c-bfe0-e15b9849be66",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "Done",
        "createdAt" : "2020-07-22T04:10:19Z",
        "updatedAt" : "2020-07-23T03:08:36Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "03d3409f6ca641a4f10fdc2ac71479445220f676",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +261,265 @@    // SPARK-32106 Since we add SQL test 'transform.sql' will use `cat` command,\n    // here we need to check command available\n    assume(TestUtils.testCommandAvailable(\"/bin/bash\"))\n    val input = fileToString(new File(testCase.inputFile))\n"
  },
  {
    "id" : "c608d062-7200-496c-823e-3b916a2af51f",
    "prId" : 28060,
    "prUrl" : "https://github.com/apache/spark/pull/28060#pullrequestreview-391732326",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "458571b5-ebf0-4654-b93e-32b8a02e371b",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "This seems to be the root cause of failure.\r\n```\r\n java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: jar:file:/home/jenkins/workspace/spark-branch-3.0-test-maven-hadoop-2.7-hive-2.3/sql/core/target/spark-sql_2.12-3.0.1-SNAPSHOT-tests.jar!/test-data/postgresql/agg.data\r\n```",
        "createdAt" : "2020-04-11T03:38:12Z",
        "updatedAt" : "2020-04-11T03:38:12Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "af42f50c53aa9be5fa7540591fe2a6277357377c",
    "line" : 46,
    "diffHunk" : "@@ -1,1 +602,606 @@      .options(Map(\"delimiter\" -> \"\\t\", \"header\" -> \"false\"))\n      .schema(\"a int, b float\")\n      .load(testFile(\"test-data/postgresql/agg.data\"))\n      .write\n      .format(\"parquet\")"
  },
  {
    "id" : "9686a695-4a9c-49c8-b4ad-57b918edf41f",
    "prId" : 28018,
    "prUrl" : "https://github.com/apache/spark/pull/28018#pullrequestreview-381640894",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f03ba395-6331-459a-a21c-fb813cc1cb25",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Is this fix related to this PR?",
        "createdAt" : "2020-03-25T22:53:25Z",
        "updatedAt" : "2020-03-26T12:14:02Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "a6013826-47bf-498f-af0e-615e85c78934",
        "parentId" : "f03ba395-6331-459a-a21c-fb813cc1cb25",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "Of course.",
        "createdAt" : "2020-03-26T01:10:17Z",
        "updatedAt" : "2020-03-26T12:14:02Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "1e60d836b5e5f0cef7eeecf6683bf7857426031c",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +258,262 @@    }\n\n    def splitCommentsAndCodes(input: String) = input.split(\"\\n\").partition { line =>\n      val newLine = line.trim\n      newLine.startsWith(\"--\") && !newLine.startsWith(\"--QUERY-DELIMITER\")"
  },
  {
    "id" : "c59910ea-9b2f-4885-9cb0-cc87a673e1db",
    "prId" : 27552,
    "prUrl" : "https://github.com/apache/spark/pull/27552#pullrequestreview-365665935",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "67285b9d-85ee-4ae2-9628-f74102d362ab",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "in `ThriftServerQueryTestSuite`, we get the result by JDBC, so there is no DataFrame created.\r\n\r\nWe should follow pgsql and return java 8 datetime when the config is enabled. https://jdbc.postgresql.org/documentation/head/8-date-time.html\r\n\r\ncc @wangyum @yaooqinn ",
        "createdAt" : "2020-02-27T13:10:12Z",
        "updatedAt" : "2020-02-27T13:10:13Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "880b1dec95595b58688a3d5f7a2e0637a090cb47",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +512,516 @@    // Get answer, but also get rid of the #1234 expression ids that show up in explain plans\n    val answer = SQLExecution.withNewExecutionId(df.queryExecution, Some(sql)) {\n      hiveResultString(df).map(replaceNotIncludedMsg)\n    }\n"
  },
  {
    "id" : "f9d9d557-a8f9-4f00-82c9-da2c0790f0b7",
    "prId" : 27494,
    "prUrl" : "https://github.com/apache/spark/pull/27494#pullrequestreview-355887099",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1f5c3f00-792e-4541-8916-baf75af60a2a",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I think this is a bug in `HiveResult`, not just this test suite.\r\n\r\nWhen we produce string result, we must stick with the Proleptic Gregorian calendar and call `LocalDate/Instant.toString`. I think we should update `HiveResult.hiveResultString` and enable java 8 datetime API before calling `collect`. ",
        "createdAt" : "2020-02-10T10:28:42Z",
        "updatedAt" : "2020-02-12T12:44:38Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "bafdd0d2-80b1-4ccf-990c-0c9c978e3af3",
        "parentId" : "1f5c3f00-792e-4541-8916-baf75af60a2a",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "HiveResult is used in `hive-thriftserver` too. If I set `spark.sql.datetime.java8API.enabled` to `true` in `HiveResult.hiveResultString`, this might effect on the Thrift server. Though I can correct expected results in the tests, but Thrift server may require broader changes.",
        "createdAt" : "2020-02-10T10:42:46Z",
        "updatedAt" : "2020-02-12T12:44:38Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "ccd81115-6222-4fc3-9f9c-9d87c27161c1",
        "parentId" : "1f5c3f00-792e-4541-8916-baf75af60a2a",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Can we open a new PR to do it? I believe we need to do this after switching to Proleptic Gregorian calendar. We should avoid using different calendars inconsistently.",
        "createdAt" : "2020-02-10T10:48:17Z",
        "updatedAt" : "2020-02-12T12:44:38Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "4fae7b6d-fea1-41f8-bed2-877fa8fd8c26",
        "parentId" : "1f5c3f00-792e-4541-8916-baf75af60a2a",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Sure, I will do that after this PR will be merged because I will need the changes, most likely.",
        "createdAt" : "2020-02-10T12:08:27Z",
        "updatedAt" : "2020-02-12T12:44:38Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "e5d463f67d68efcff046851479b4789d1e4f50a0",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +338,342 @@      case _ =>\n    }\n    localSparkSession.conf.set(SQLConf.DATETIME_JAVA8API_ENABLED.key, true)\n\n    if (configSet.nonEmpty) {"
  },
  {
    "id" : "c895b179-fd83-43d0-8798-468821befae4",
    "prId" : 27481,
    "prUrl" : "https://github.com/apache/spark/pull/27481#pullrequestreview-357413869",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d9f0dba3-77c1-444e-bf9a-ec929bcc92fb",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "This is better than my original idea. We only need to use this special delimiter for queries that need it. Good job!",
        "createdAt" : "2020-02-12T11:16:44Z",
        "updatedAt" : "2020-02-13T06:57:46Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "1529394c-45ce-451d-a6ed-216302fe2c83",
        "parentId" : "d9f0dba3-77c1-444e-bf9a-ec929bcc92fb",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "Thanks!",
        "createdAt" : "2020-02-12T12:30:56Z",
        "updatedAt" : "2020-02-13T06:57:46Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "900cc733397cad9eeeba4a65c016fc5f5008ac33",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +70,74 @@ *     --QUERY-DELIMITER-START and --QUERY-DELIMITER-END represent the beginning and end of a query,\n *     respectively. Code that is not surrounded by lines that begin with --QUERY-DELIMITER-START\n *     and --QUERY-DELIMITER-END is still separated by semicolons.\n *  2. Lines starting with -- are treated as comments and ignored.\n *  3. Lines starting with --SET are used to specify the configs when running this testing file. You"
  },
  {
    "id" : "1a519fc4-8084-470b-93a2-49afc54f48c9",
    "prId" : 27481,
    "prUrl" : "https://github.com/apache/spark/pull/27481#pullrequestreview-357414322",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bd08abfc-e0d9-453b-a43f-6c4b74018bb3",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "After the lookp ends, it's possible that `otherCodes` is not empty. We should \"flush\" it.",
        "createdAt" : "2020-02-12T11:26:59Z",
        "updatedAt" : "2020-02-13T06:57:46Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "58176970-019d-40cb-bb93-57a8e05bd78c",
        "parentId" : "bd08abfc-e0d9-453b-a43f-6c4b74018bb3",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "OK. I forgot it.",
        "createdAt" : "2020-02-12T12:31:42Z",
        "updatedAt" : "2020-02-13T06:57:46Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "900cc733397cad9eeeba4a65c016fc5f5008ac33",
    "line" : 70,
    "diffHunk" : "@@ -1,1 +300,304 @@        querys ++= splitWithSemicolon(otherCodes.toSeq)\n      }\n      querys.toSeq\n    } else {\n      splitWithSemicolon(allCode).toSeq"
  },
  {
    "id" : "c263559f-a8b2-4696-8e6e-b7847ea4ad56",
    "prId" : 26661,
    "prUrl" : "https://github.com/apache/spark/pull/26661#pullrequestreview-322109546",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "61edadaa-c7b5-4c91-b019-7b4f3eafa5ff",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "not very related but to simplify things.",
        "createdAt" : "2019-11-25T08:31:51Z",
        "updatedAt" : "2019-11-25T08:31:51Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "7ec9ac4e-413b-4b9a-8663-7e3a0191e7d3",
        "parentId" : "61edadaa-c7b5-4c91-b019-7b4f3eafa5ff",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "+1 for this simplication.",
        "createdAt" : "2019-11-25T08:32:16Z",
        "updatedAt" : "2019-11-25T08:32:16Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "5b28908c-29e8-4ff0-8496-0da9eb53c077",
        "parentId" : "61edadaa-c7b5-4c91-b019-7b4f3eafa5ff",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "+1",
        "createdAt" : "2019-11-25T08:41:25Z",
        "updatedAt" : "2019-11-25T08:41:26Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "a1125b5cc90278596324bac922a2a82363522e44",
    "line" : 42,
    "diffHunk" : "@@ -1,1 +314,318 @@      queries: Seq[String],\n      testCase: TestCase,\n      configSet: Seq[(String, String)]): Unit = {\n    // Create a local SparkSession to have stronger isolation between different test cases.\n    // This does not isolate catalog changes."
  },
  {
    "id" : "c3895a10-94ff-4ec8-9cf5-7d94e7973c04",
    "prId" : 26657,
    "prUrl" : "https://github.com/apache/spark/pull/26657#pullrequestreview-322062640",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a8b6128c-0221-4a49-83a3-f834048ff230",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "First of all, we had better use `-Phive-2.3`.\r\nSecond, please wait for a while. This PR will become obsolete soon.\r\nAs you see, it's still under **slow** changing to avoid any breakage.\r\nYesterday, we added `hive-1.2/2.3`. Today, we regenerates the manifest via `test-dependencies.sh`.\r\nAnd, I'm working on the dependency changes. The following is the history of the migration.\r\n\r\na1706e2fa7 [SPARK-30005][INFRA] Update `test-dependencies.sh` to check `hive-1.2/2.3` profile\r\na60da23d64 [SPARK-30007][INFRA] Publish snapshot/release artifacts with `-Phive-2.3` only\r\n13338eaa95 [SPARK-29554][SQL][FOLLOWUP] Rename Version to SparkVersion\r\n6625b69027 [SPARK-29981][BUILD][FOLLOWUP] Change hive.version.short\r\nc98e5eb339 [SPARK-29981][BUILD] Add hive-1.2/2.3 profiles",
        "createdAt" : "2019-11-25T05:32:29Z",
        "updatedAt" : "2019-11-25T05:32:30Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "756614b1-46a5-4f5f-8a39-a794dee84264",
        "parentId" : "a8b6128c-0221-4a49-83a3-f834048ff230",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "Thanks for  the instructions",
        "createdAt" : "2019-11-25T05:37:44Z",
        "updatedAt" : "2019-11-25T05:37:44Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "1302e6a6-6409-416b-9236-4b8d0aae98e6",
        "parentId" : "a8b6128c-0221-4a49-83a3-f834048ff230",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Thank you for understanding and sorry for the inconvenience. It's difficult to change the tires of the running cars.",
        "createdAt" : "2019-11-25T05:41:43Z",
        "updatedAt" : "2019-11-25T05:41:44Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "d1274940-cf8d-43b1-90f0-1edb05a40bb7",
        "parentId" : "a8b6128c-0221-4a49-83a3-f834048ff230",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "Never mind. I will leave this pull request open for a while in case someone encounters the same issue until it is completely fixed.",
        "createdAt" : "2019-11-25T05:48:01Z",
        "updatedAt" : "2019-11-25T05:48:02Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "50edec81-2db6-41f9-a3fd-fbec4ea4b241",
        "parentId" : "a8b6128c-0221-4a49-83a3-f834048ff230",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Although I'm still working on, I create a WIP PR to share with you.\r\n- https://github.com/apache/spark/pull/26658",
        "createdAt" : "2019-11-25T06:33:49Z",
        "updatedAt" : "2019-11-25T06:33:50Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "2e7183fdbeee52f6eaebab47886e294f16377045",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +44,48 @@ * To run the entire test suite:\n * {{{\n *   build/sbt -Phive-1.2 \"sql/test-only *SQLQueryTestSuite\"\n * }}}\n *"
  },
  {
    "id" : "8f8f7990-c481-40d0-b4f8-f79eb6f78d3f",
    "prId" : 26657,
    "prUrl" : "https://github.com/apache/spark/pull/26657#pullrequestreview-322151061",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d04a8ae4-0367-4961-9cba-052bd334d492",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Plus about the default profile (and it will become obsolete), such documentation should better go to somewhere related to Spark build or testing.",
        "createdAt" : "2019-11-25T09:51:33Z",
        "updatedAt" : "2019-11-25T09:51:33Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "2e7183fdbeee52f6eaebab47886e294f16377045",
    "line" : 27,
    "diffHunk" : "@@ -1,1 +63,67 @@ * }}}\n *\n * Change profiles:\n * We use `hadoop-2.7` as default now, so the above scripts is equivalent to\n * `-Phadoop-2.7 -Phive-1.2`. You can change profiles to `-Phadoop-3.2 -Phive-2.3` to verify higher"
  },
  {
    "id" : "0c6847ab-1f4c-4573-adbd-73b00bc8a145",
    "prId" : 26623,
    "prUrl" : "https://github.com/apache/spark/pull/26623#pullrequestreview-320841942",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3af86e24-999e-4936-8ab7-e3f05cc05383",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "This is not related to this pr though, I found the misleading name here...",
        "createdAt" : "2019-11-21T12:20:15Z",
        "updatedAt" : "2019-11-21T22:26:08Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "7e63c0fd0389e023ad0307f748700601a1098bdc",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +256,260 @@    // If `--IMPORT` found, load code from another test case file, then insert them\n    // into the head in this test.\n    val importedTestCaseNames = comments.filter(_.startsWith(\"--IMPORT \")).map(_.substring(9))\n    val importedCode = importedTestCaseNames.flatMap { testCaseName =>\n      listTestCases.find(_.name == testCaseName).map { testCase =>"
  },
  {
    "id" : "86b976f6-de86-4594-9fa7-ff1ce4235d17",
    "prId" : 26612,
    "prUrl" : "https://github.com/apache/spark/pull/26612#pullrequestreview-320581609",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "52bdbbf0-0949-4146-a366-8e8f035bc042",
        "parentId" : null,
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "We can remove this line:\r\nhttps://github.com/apache/spark/blob/3be4734f317b3b26da278ad088b31ec4b7750213/sql/core/src/test/scala/org/apache/spark/sql/SQLQueryTestSuite.scala#L114",
        "createdAt" : "2019-11-21T00:52:38Z",
        "updatedAt" : "2019-11-21T04:13:06Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      }
    ],
    "commit" : "9562caefaefed3fae9546c32f99ec09d1ffa0dd7",
    "line" : 44,
    "diffHunk" : "@@ -1,1 +266,270 @@    })\n\n    if (regenerateGoldenFiles) {\n      runQueries(queries, testCase, Some(settings))\n    } else {"
  },
  {
    "id" : "aa88ae0b-918f-413e-8c85-d24b81f5d570",
    "prId" : 26612,
    "prUrl" : "https://github.com/apache/spark/pull/26612#pullrequestreview-321890218",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f395e68e-638a-40b1-9b99-4da76bca4f66",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Better to update how-to use `CONFIG_DIM` in https://github.com/apache/spark/blob/6146dc4562739c1c947eb944897c2fe85d1016e0/sql/core/src/test/scala/org/apache/spark/sql/SQLQueryTestSuite.scala#L69",
        "createdAt" : "2019-11-23T00:35:19Z",
        "updatedAt" : "2019-11-23T00:35:19Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "3693f1f7-a89f-4e33-8203-5e4fd21898ed",
        "parentId" : "f395e68e-638a-40b1-9b99-4da76bca4f66",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Yeah we should document this there ...",
        "createdAt" : "2019-11-23T03:02:20Z",
        "updatedAt" : "2019-11-23T03:04:48Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "9562caefaefed3fae9546c32f99ec09d1ffa0dd7",
    "line" : 57,
    "diffHunk" : "@@ -1,1 +275,279 @@      // We need to do cartesian product for all the config dimensions, to get a list of\n      // config sets, and run the query once for each config set.\n      val configDimLines = comments.filter(_.startsWith(\"--CONFIG_DIM\")).map(_.substring(12))\n      val configDims = configDimLines.groupBy(_.takeWhile(_ != ' ')).mapValues { lines =>\n        lines.map(_.dropWhile(_ != ' ').substring(1)).map(_.split(\",\").map { kv =>"
  },
  {
    "id" : "21b5a52c-2cb9-4760-bb0e-cb3f8e9fdf34",
    "prId" : 26557,
    "prUrl" : "https://github.com/apache/spark/pull/26557#pullrequestreview-319475316",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a2ba3fbd-6cdd-4f5e-91e2-5af5707239fd",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "ah seems we have 2 different kinds of settings now:\r\n1. settings that shouldn't change result. We want to generate the golden files without the settings and run tests with settings.\r\n2. setting that can change result.\r\n\r\nThe second one is pretty useful with `--import`. We want to run the same queries with different settings and save the answers.\r\n\r\nShall we have different directives for them? @maropu ",
        "createdAt" : "2019-11-18T07:21:21Z",
        "updatedAt" : "2019-11-18T08:51:21Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "263f2670-b407-49b2-830d-b6669bb751a2",
        "parentId" : "a2ba3fbd-6cdd-4f5e-91e2-5af5707239fd",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "After a second thought, we only expect the `--SET` to change behavior if we import queries. So the change here LGTM.\r\n\r\n@yaooqinn can you update the comment to explain it?",
        "createdAt" : "2019-11-18T07:48:19Z",
        "updatedAt" : "2019-11-18T08:51:21Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "7d3ac90d-a753-487b-be33-b749884688d3",
        "parentId" : "a2ba3fbd-6cdd-4f5e-91e2-5af5707239fd",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "BTW now `--SET` is upper case and `--import` is lower case. Seems better to always use upper case.",
        "createdAt" : "2019-11-18T07:49:03Z",
        "updatedAt" : "2019-11-18T08:51:21Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "5458d109-1d11-49b4-8923-8982ea151d1c",
        "parentId" : "a2ba3fbd-6cdd-4f5e-91e2-5af5707239fd",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "also cc @wangyum , why do we need `isTestWithConfigSets` for thriftserver tests?",
        "createdAt" : "2019-11-18T07:49:56Z",
        "updatedAt" : "2019-11-18T08:51:21Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "92ad7f95-d613-4648-a492-3ce6728395ae",
        "parentId" : "a2ba3fbd-6cdd-4f5e-91e2-5af5707239fd",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "OK, I will update the comments and change case for `--import`",
        "createdAt" : "2019-11-18T07:58:36Z",
        "updatedAt" : "2019-11-18T08:51:21Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "a7a227bf-4ec4-4c3c-b6de-70d1225f8089",
        "parentId" : "a2ba3fbd-6cdd-4f5e-91e2-5af5707239fd",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "Now we are not able to use `--SET` and `--IMPORT` together for `ThriftServerQueryTestSuite`, for the `--SET` will always be ignored. Is there a chance for `--SET` + `--IMPORT` working for `ThriftServerQueryTestSuite`ï¼Ÿ",
        "createdAt" : "2019-11-18T08:47:03Z",
        "updatedAt" : "2019-11-18T08:51:21Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "03d23625-5dee-4998-b793-aecfd4b95efd",
        "parentId" : "a2ba3fbd-6cdd-4f5e-91e2-5af5707239fd",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "@cloud-fan To reduce the test time. The `mvn` test often timed out at that time: https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-maven-hadoop-2.7/\r\n",
        "createdAt" : "2019-11-18T14:09:48Z",
        "updatedAt" : "2019-11-18T14:09:48Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      },
      {
        "id" : "96ccbd91-69ff-4e55-8040-03d48094876d",
        "parentId" : "a2ba3fbd-6cdd-4f5e-91e2-5af5707239fd",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "do you mean setting config is expensive?",
        "createdAt" : "2019-11-18T14:58:19Z",
        "updatedAt" : "2019-11-18T14:58:20Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "c7d7a8a9-02b5-4cb5-ab00-df8b09c85955",
        "parentId" : "a2ba3fbd-6cdd-4f5e-91e2-5af5707239fd",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "It will run `configSets` times:\r\nhttps://github.com/apache/spark/blob/ec5d698d99634e5bb8fc7b0fa1c270dd67c129c8/sql/core/src/test/scala/org/apache/spark/sql/SQLQueryTestSuite.scala#L287-L298",
        "createdAt" : "2019-11-18T16:53:00Z",
        "updatedAt" : "2019-11-18T16:53:01Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      },
      {
        "id" : "08e78259-d736-4a15-86da-2d76f81f6b9b",
        "parentId" : "a2ba3fbd-6cdd-4f5e-91e2-5af5707239fd",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "ah that's a problem. We should think about how to reduce test time.",
        "createdAt" : "2019-11-18T17:28:09Z",
        "updatedAt" : "2019-11-18T17:28:10Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "20455980-ceed-470d-99c1-17e11bfa4019",
        "parentId" : "a2ba3fbd-6cdd-4f5e-91e2-5af5707239fd",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Oh, I'm late.  The change looks reasonable...",
        "createdAt" : "2019-11-20T02:23:37Z",
        "updatedAt" : "2019-11-20T02:23:37Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "3192e152b34c6a4c9bfe4e7c6f8ffc2f6ca6b153",
    "line" : 34,
    "diffHunk" : "@@ -1,1 +290,294 @@    // same queries from the original file but with different settings and save the answers. So the\n    // `--SET` will be respected in this case.\n    if ((regenerateGoldenFiles && importedTestCaseName.isEmpty) || !isTestWithConfigSets) {\n      runQueries(queries, testCase, None)\n    } else {"
  },
  {
    "id" : "621f66e4-6d1d-4fa9-b24d-2f6d3132d2b4",
    "prId" : 26538,
    "prUrl" : "https://github.com/apache/spark/pull/26538#pullrequestreview-317959859",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "94897fe0-6cb0-48e6-9540-3fc77379ded6",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Do you need this vs just a newline? no big deal anyway",
        "createdAt" : "2019-11-15T08:21:22Z",
        "updatedAt" : "2019-11-15T08:21:25Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "dd85156a-190b-4a09-83f3-9a1ca630496a",
        "parentId" : "94897fe0-6cb0-48e6-9540-3fc77379ded6",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I think just a newline is fine too. Just wanted to avoid any potential problems specific to OSes.",
        "createdAt" : "2019-11-16T08:30:09Z",
        "updatedAt" : "2019-11-16T08:30:10Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "57294c346d59b7b01061b962a76e888b19d07b93",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +393,397 @@      case udfTestCase: UDFTest\n          if udfTestCase.udf.isInstanceOf[TestScalarPandasUDF] && shouldTestScalarPandasUDFs =>\n        s\"${testCase.name}${System.lineSeparator()}\" +\n          s\"Python: $pythonVer Pandas: $pandasVer PyArrow: $pyarrowVer${System.lineSeparator()}\"\n      case _ =>"
  },
  {
    "id" : "82393208-3915-45ef-81d7-7516aaf7221f",
    "prId" : 26497,
    "prUrl" : "https://github.com/apache/spark/pull/26497#pullrequestreview-316177084",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e188eb3b-88d7-4530-b663-04fc6a6f66d7",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "We should remove it in the followup, and see if it fails tests. Ideally pgsql dialect should not be affected by ansi mode config. cc @gengliangwang ",
        "createdAt" : "2019-11-13T11:31:33Z",
        "updatedAt" : "2019-11-14T01:20:02Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "ab0730be8b017be319c6cbc7828dfdea39625617",
    "line" : 69,
    "diffHunk" : "@@ -1,1 +344,348 @@        // vol used by boolean.sql and case.sql.\n        localSparkSession.udf.register(\"vol\", (s: String) => s)\n        localSparkSession.conf.set(SQLConf.ANSI_ENABLED.key, true)\n        localSparkSession.conf.set(SQLConf.DIALECT.key, SQLConf.Dialect.POSTGRESQL.toString)\n      case _: AnsiTest =>"
  },
  {
    "id" : "7c29b401-6918-4c4d-82ae-3277f075f45b",
    "prId" : 26107,
    "prUrl" : "https://github.com/apache/spark/pull/26107#pullrequestreview-310800502",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9b5d245a-6d5e-4b9f-9784-497d81bf5881",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "@gengliangwang Sorry, I just realized recently that my changed are not tested by `date.sql` and `timestamp.sql` any more when I run: `build/sbt \"sql/test-only *SQLQueryTestSuite -- -z date.sql\"`. Did you disable them forever or are they executed in some way by jenkins?",
        "createdAt" : "2019-11-02T10:02:20Z",
        "updatedAt" : "2019-11-02T10:06:25Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "8cf54310-32f8-465f-91b2-39844553917d",
        "parentId" : "9b5d245a-6d5e-4b9f-9784-497d81bf5881",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "How about just setting `storeAssignmentPolicy=LEGACY` for the PgSQL tests? They have a lots of INSERT queries having implicit casts from string literals to numeric values.",
        "createdAt" : "2019-11-03T04:37:09Z",
        "updatedAt" : "2019-11-03T04:37:10Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "7caa9552-7c7d-4d4c-911c-dd9bdb0d4e27",
        "parentId" : "9b5d245a-6d5e-4b9f-9784-497d81bf5881",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "@maropu I am not sure about that. \r\nFor PgSQL, it disable inserting string values to numeric columns except for literals. Setting `storeAssignmentPolicy=LEGACY` for all the PgSQL tests seems inaccurate. \r\n",
        "createdAt" : "2019-11-03T04:47:37Z",
        "updatedAt" : "2019-11-03T04:47:38Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "b888e676-0683-4afb-a0f8-e9fd1d45bcec",
        "parentId" : "9b5d245a-6d5e-4b9f-9784-497d81bf5881",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "But, before this pr's been merged, we tested the PgSQL tests in the LEGACY mode? Is my understanding wrong? Personally, I think we need to explicitly file issues in jira (Or, comment out them?) if we have inaccurate tests in `pgSQL/`.",
        "createdAt" : "2019-11-03T05:05:44Z",
        "updatedAt" : "2019-11-03T05:08:36Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "e969987c-9451-4ee9-9d6f-62805f3bb876",
        "parentId" : "9b5d245a-6d5e-4b9f-9784-497d81bf5881",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "How about changing the timestamp/date values from string literal to timestamp/date literal in those sql files, just as https://github.com/apache/spark/pull/26107/files#diff-431a4d1f056a06e853da8a60c46e9ca0R68\r\nI am not sure about whether there is a guideline in porting the PgSQL test files. Such modifications are allowed, right?",
        "createdAt" : "2019-11-03T05:12:18Z",
        "updatedAt" : "2019-11-03T05:12:19Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "3a1f097b-1716-4d39-9c41-67aebdb4f86f",
        "parentId" : "9b5d245a-6d5e-4b9f-9784-497d81bf5881",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "> I am not sure about whether there is a guideline in porting the PgSQL test files. Such modifications are allowed, right?\r\n\r\nYea, I think that's ok.  cc: @wangyum @dongjoon-hyun @HyukjinKwon \r\nIf you have no time to do, I'll check next week.",
        "createdAt" : "2019-11-03T05:16:10Z",
        "updatedAt" : "2019-11-03T05:23:20Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "5a151ed3-1c14-47fd-979a-a9bdefad35d7",
        "parentId" : "9b5d245a-6d5e-4b9f-9784-497d81bf5881",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "I'm +1 for @gengliangwang 's suggestion `enabling those tests with that modification`.",
        "createdAt" : "2019-11-03T05:50:24Z",
        "updatedAt" : "2019-11-03T05:50:25Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "b9abb67cd9761e59ee62ecb77b269deb725b1185",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +155,159 @@    // SPARK-28885 String value is not allowed to be stored as date/timestamp type with\n    // ANSI store assignment policy.\n    \"postgreSQL/date.sql\",\n    \"postgreSQL/timestamp.sql\"\n  )"
  },
  {
    "id" : "ee40ba4e-de59-4c06-a9a2-99a3a7b26569",
    "prId" : 26028,
    "prUrl" : "https://github.com/apache/spark/pull/26028#pullrequestreview-298344362",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f1f24441-de20-42dd-aacf-beffa8df27f4",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Could you add a function description because we override this differently?\r\n- SQLQueryTestSuite seems to return `(struct<>, ...)`\r\n- ThriftServerQueryTestSuite seems to return `(\"\", answer.sorted)`",
        "createdAt" : "2019-10-07T16:22:36Z",
        "updatedAt" : "2019-10-07T19:01:08Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "e82c3f53-7a2a-4bb0-a023-62aa7323943a",
        "parentId" : "f1f24441-de20-42dd-aacf-beffa8df27f4",
        "authorId" : "3d4870da-39a8-4406-b00d-131930d14cd8",
        "body" : "No, both returns a `(String, Seq[String])` tuple where the first is the schema and the second is the result. Since it's impossible to get the exact spark schema back from a `java.sql.ResultSet` we use empty string in `ThriftServerQueryTestSuite`.",
        "createdAt" : "2019-10-07T18:36:13Z",
        "updatedAt" : "2019-10-07T19:01:08Z",
        "lastEditedBy" : "3d4870da-39a8-4406-b00d-131930d14cd8",
        "tags" : [
        ]
      },
      {
        "id" : "50ba42a3-eaef-4d75-aca2-75304e751231",
        "parentId" : "f1f24441-de20-42dd-aacf-beffa8df27f4",
        "authorId" : "3d4870da-39a8-4406-b00d-131930d14cd8",
        "body" : "I've added a description to it and to its override.",
        "createdAt" : "2019-10-07T19:02:05Z",
        "updatedAt" : "2019-10-07T19:02:05Z",
        "lastEditedBy" : "3d4870da-39a8-4406-b00d-131930d14cd8",
        "tags" : [
        ]
      }
    ],
    "commit" : "019e37ff70cf05051c08f38112bfe8849ade0465",
    "line" : 46,
    "diffHunk" : "@@ -1,1 +397,401 @@   * @param result a function that returns a pair of schema and output\n   */\n  protected def handleExceptions(result: => (String, Seq[String])): (String, Seq[String]) = {\n    try {\n      result"
  },
  {
    "id" : "e717f6c5-b4ad-45e9-82bd-b822a03f619c",
    "prId" : 26028,
    "prUrl" : "https://github.com/apache/spark/pull/26028#pullrequestreview-298326918",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6c4d2472-0a26-4011-ad1d-9a62011fe2a1",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Could you add a test case which this is required?",
        "createdAt" : "2019-10-07T16:23:36Z",
        "updatedAt" : "2019-10-07T19:01:08Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "b9a19c6c-2c8a-4203-a5fa-dc5bea1cd429",
        "parentId" : "6c4d2472-0a26-4011-ad1d-9a62011fe2a1",
        "authorId" : "3d4870da-39a8-4406-b00d-131930d14cd8",
        "body" : "No particular test case. Since I touched this method and `StructType(Seq.empty)` was used 3 times so I just moved it to a `val`.",
        "createdAt" : "2019-10-07T18:31:49Z",
        "updatedAt" : "2019-10-07T19:01:08Z",
        "lastEditedBy" : "3d4870da-39a8-4406-b00d-131930d14cd8",
        "tags" : [
        ]
      }
    ],
    "commit" : "019e37ff70cf05051c08f38112bfe8849ade0465",
    "line" : 66,
    "diffHunk" : "@@ -1,1 +406,410 @@        // with a generic pattern \"###\".\n        val msg = if (a.plan.nonEmpty) a.getSimpleMessage else a.getMessage\n        (emptySchema, Seq(a.getClass.getName, msg.replaceAll(\"#\\\\d+\", \"#x\")))\n      case s: SparkException if s.getCause != null =>\n        // For a runtime exception, it is hard to match because its message contains"
  }
]