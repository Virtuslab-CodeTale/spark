[
  {
    "id" : "8044ccea-1d4a-4dae-a326-878402053a30",
    "prId" : 32520,
    "prUrl" : "https://github.com/apache/spark/pull/32520#pullrequestreview-658494475",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "baec0c79-2576-45ec-967e-20d211fe1e6e",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "another idea:\r\n```\r\nval tpcdsQueries: Seq[String] = 1.to(99).map(\"q\" + _).flatMap { q => \r\n  if (Seq(\"q14\", \"q23\", \"q24\", \"q39\").contains(q)) Seq(q + \"a\", q + \"b\") else Seq(q)\r\n}.filterNot { q =>\r\n  // ...\r\n  Seq(\"q6\", \"q34\", \"q64\", \"q74\", \"q75\", \"q78\").contains(q)\r\n}\r\n```",
        "createdAt" : "2021-05-12T13:53:18Z",
        "updatedAt" : "2021-05-12T13:53:18Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "b359c668-f1b4-4d88-a6d7-b0550f629310",
        "parentId" : "baec0c79-2576-45ec-967e-20d211fe1e6e",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "I feel the `if` part looks a bit tricky, so I prefer to keep it as it is.",
        "createdAt" : "2021-05-13T00:41:00Z",
        "updatedAt" : "2021-05-13T00:41:01Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "299abb537bf715506d77079b65a4704a04a2829f",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +43,47 @@  private val excludedTpcdsQueries: Set[String] = Set(\"q6\", \"q34\", \"q64\", \"q74\", \"q75\", \"q78\")\n\n  val tpcdsQueries: Seq[String] = tpcdsAllQueries.filterNot(excludedTpcdsQueries.contains)\n\n  // This list only includes TPCDS v2.7 queries that are different from v1.4 ones"
  },
  {
    "id" : "bdb03a5b-0355-4cf1-b815-84d323dd4894",
    "prId" : 32520,
    "prUrl" : "https://github.com/apache/spark/pull/32520#pullrequestreview-658494143",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1f32c7ca-645a-4569-8123-ea12ffdea2aa",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "This hides the previous reasoning. I believe it's worth to keep SPARK-35327 comment explicitly.\r\n```\r\nSPARK-35327: Filters out the TPC-DS queries that can cause flaky test results\r\n```",
        "createdAt" : "2021-05-12T15:41:45Z",
        "updatedAt" : "2021-05-12T15:41:52Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "8844824e-d015-4655-a473-f3e8f97ce2f7",
        "parentId" : "1f32c7ca-645a-4569-8123-ea12ffdea2aa",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Sure, updated.",
        "createdAt" : "2021-05-13T00:39:55Z",
        "updatedAt" : "2021-05-13T00:40:02Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "299abb537bf715506d77079b65a4704a04a2829f",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +38,42 @@\n  // Since `tpcdsQueriesV2_7_0` has almost the same queries with these ones below,\n  // we skip them in the TPCDS-related tests.\n  // NOTE: q6\" and \"q75\" can cause flaky test results, so we must exclude them.\n  // For more details, see SPARK-35327."
  },
  {
    "id" : "87888d65-5508-4baa-bdce-497b442aa0e9",
    "prId" : 32454,
    "prUrl" : "https://github.com/apache/spark/pull/32454#pullrequestreview-657858773",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "58756001-6ff9-4df5-a09a-356130a744f4",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "shall we remove q6 from here for all the tests, if the only difference is an extra order by column?",
        "createdAt" : "2021-05-10T06:43:52Z",
        "updatedAt" : "2021-05-10T06:43:52Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "2857eece-319c-4c87-999a-92047174082f",
        "parentId" : "58756001-6ff9-4df5-a09a-356130a744f4",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "okay, I'll check it and make a PR to fix it.",
        "createdAt" : "2021-05-11T00:43:45Z",
        "updatedAt" : "2021-05-11T00:43:46Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "1c8c80e4-da09-417d-9053-036126f69cc0",
        "parentId" : "58756001-6ff9-4df5-a09a-356130a744f4",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "See https://github.com/apache/spark/pull/32520",
        "createdAt" : "2021-05-12T13:05:14Z",
        "updatedAt" : "2021-05-12T13:05:14Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "386d6669c3ab01acb79218e1d4be82e9a509a54d",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +26,30 @@  // The TPCDS queries below are based on v1.4\n  def tpcdsQueries: Seq[String] = Seq(\n    \"q1\", \"q2\", \"q3\", \"q4\", \"q5\", \"q6\", \"q7\", \"q8\", \"q9\", \"q10\", \"q11\",\n    \"q12\", \"q13\", \"q14a\", \"q14b\", \"q15\", \"q16\", \"q17\", \"q18\", \"q19\", \"q20\",\n    \"q21\", \"q22\", \"q23a\", \"q23b\", \"q24a\", \"q24b\", \"q25\", \"q26\", \"q27\", \"q28\", \"q29\", \"q30\","
  },
  {
    "id" : "0818fcd9-bf2d-49fb-8b59-d9d0aed9b2fd",
    "prId" : 32037,
    "prUrl" : "https://github.com/apache/spark/pull/32037#pullrequestreview-630924003",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "68b1e145-21cb-43c8-9565-ed3b6272c80b",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "> One thing might be clear that we should replace bigint type which is now used in web_returns and store_returns with int type.\r\nAnother thing that might need to be further discussed is - shall we use bigint for all the integer columns in the TPCDS Data Definition to meet 2.2.2.1 b)?\r\n\r\nThe statement in the spec below implicitly suggests `Integer` should be bigint?\r\n```\r\nb) Integer means that the column shall be able to exactly represent integer values (i.e., values in increments of\r\n1) in the range of at least ( − 2n − 1) to (2n − 1 − 1), where n is 64.\r\n```",
        "createdAt" : "2021-04-08T00:24:54Z",
        "updatedAt" : "2021-04-13T01:02:05Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "d1354bd9-f59f-407c-a97e-9f91b84e8f82",
        "parentId" : "68b1e145-21cb-43c8-9565-ed3b6272c80b",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "> After this PR, the field schemas are now consistent with those DDLs in the `tpcds.sql` from tpc-ds tool kit, see https://gist.github.com/yaooqinn/b9978a77bbf4f871a95d6a9103019907\r\n\r\nremoved, according to this line",
        "createdAt" : "2021-04-08T02:16:41Z",
        "updatedAt" : "2021-04-13T01:02:05Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "3ddd4505fe8cd262e5fe93b16cd01399c9b8b329",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +36,40 @@ *    |  Identifier   |      INT      |\n *    |---------------|---------------|\n *    |    Integer    |      INT      |\n *    |---------------|---------------|\n *    | Decimal(d, f) | Decimal(d, f) |"
  },
  {
    "id" : "078104f7-e2d7-424e-8045-63f93f0f57c2",
    "prId" : 31243,
    "prUrl" : "https://github.com/apache/spark/pull/31243#pullrequestreview-571791992",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b0965b54-91b3-4e52-9437-1f39ca1866c3",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "nit: Could you leave some comments about how we decided these partition keys?",
        "createdAt" : "2021-01-20T00:50:02Z",
        "updatedAt" : "2021-01-20T01:08:19Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "d5ca93bb-cf2d-451f-bced-1548756b59d0",
        "parentId" : "b0965b54-91b3-4e52-9437-1f39ca1866c3",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "ok",
        "createdAt" : "2021-01-20T01:09:04Z",
        "updatedAt" : "2021-01-20T01:09:05Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      }
    ],
    "commit" : "e30f20b6884b8de65e71fb80134d6958bb64d051",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +273,277 @@\n  // The partition column is consistent with the databricks/spark-sql-perf project.\n  private val tablePartitionColumns = Map(\n    \"catalog_sales\" -> Seq(\"`cs_sold_date_sk`\"),\n    \"catalog_returns\" -> Seq(\"`cr_returned_date_sk`\"),"
  },
  {
    "id" : "7652ff53-1b30-4b64-b13f-6b3278258cc7",
    "prId" : 31012,
    "prUrl" : "https://github.com/apache/spark/pull/31012#pullrequestreview-618541374",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "256de692-c907-4a25-bc18-d230a45e5671",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "This should be `char(6)` (see p28 in http://www.tpc.org/tpc_documents_current_versions/pdf/tpc-ds_v2.9.0.pdf)",
        "createdAt" : "2021-03-23T10:31:07Z",
        "updatedAt" : "2021-03-23T10:31:08Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "0f8ebceb-99b5-43f1-b0e7-4c36ce503a6b",
        "parentId" : "256de692-c907-4a25-bc18-d230a45e5671",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "https://github.com/apache/spark/pull/31886/files#r599317278",
        "createdAt" : "2021-03-23T10:32:58Z",
        "updatedAt" : "2021-03-23T10:32:58Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "3ca5fd6a-05ab-4028-af69-5482aae6f4d1",
        "parentId" : "256de692-c907-4a25-bc18-d230a45e5671",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "I'll fix this in myPR #31886.",
        "createdAt" : "2021-03-23T10:33:14Z",
        "updatedAt" : "2021-03-23T10:33:14Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "cc594674-5e9f-4c88-a56f-3d8e79795d59",
        "parentId" : "256de692-c907-4a25-bc18-d230a45e5671",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "nice catch!",
        "createdAt" : "2021-03-23T10:34:46Z",
        "updatedAt" : "2021-03-23T10:34:46Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "9ea25490-eb69-42b5-acf1-930297ebeb1e",
        "parentId" : "256de692-c907-4a25-bc18-d230a45e5671",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Can we do that separately becuase #31886 is about adding a new GitHub Action job?",
        "createdAt" : "2021-03-23T10:43:54Z",
        "updatedAt" : "2021-03-23T10:43:54Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "b24297af-527b-4ef7-8d01-9a18e4f24727",
        "parentId" : "256de692-c907-4a25-bc18-d230a45e5671",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Yea, sure, @dongjoon-hyun ~",
        "createdAt" : "2021-03-23T12:01:59Z",
        "updatedAt" : "2021-03-23T12:02:00Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "34ddb7bd-9c34-4281-8252-f8089b6feb23",
        "parentId" : "256de692-c907-4a25-bc18-d230a45e5671",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "https://github.com/apache/spark/pull/31943",
        "createdAt" : "2021-03-23T12:44:28Z",
        "updatedAt" : "2021-03-23T12:44:29Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "1aa22f055db370e77e506403666000e8e1e19f9c",
    "line" : 516,
    "diffHunk" : "@@ -1,1 +443,447 @@        |`d_fy_week_seq` INT,\n        |`d_day_name` CHAR(9),\n        |`d_quarter_name` CHAR(1),\n        |`d_holiday` CHAR(1),\n        |`d_weekend` CHAR(1),"
  },
  {
    "id" : "3bafbbb6-ca3f-43b4-8b77-476f0341e003",
    "prId" : 29270,
    "prUrl" : "https://github.com/apache/spark/pull/29270#pullrequestreview-468262076",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ff31cf4f-f806-411c-87ce-8f7076fd7b47",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "It looks better to check the `preferSortMergeJoin=false` case, too? I think some users might actively use hash joins instead.",
        "createdAt" : "2020-08-15T07:39:27Z",
        "updatedAt" : "2020-08-17T09:39:09Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "9d19cbd4-52d7-4d8c-a13b-379c51cd22fe",
        "parentId" : "ff31cf4f-f806-411c-87ce-8f7076fd7b47",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "hmm..I'm not sure about this. but it seems `preferSortMergeJoin` is enabled by default? Or you mean we should test both `preferSortMergeJoin` enabled and disabled?",
        "createdAt" : "2020-08-17T02:40:10Z",
        "updatedAt" : "2020-08-17T09:39:09Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "88169b0a-d43f-4f0a-b298-ddce97bccc1c",
        "parentId" : "ff31cf4f-f806-411c-87ce-8f7076fd7b47",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Let's do it later. The `TPCDSQuerySuite` doesn't test shuffle hash join either and we should fix it as well.",
        "createdAt" : "2020-08-17T05:18:20Z",
        "updatedAt" : "2020-08-17T09:39:09Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "53134e94-6f1b-4789-9f4d-df31be45fec7",
        "parentId" : "ff31cf4f-f806-411c-87ce-8f7076fd7b47",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "The configurations have been extracted to the base class(`TPCDSBase`). If we change it here, I think it will also take effect for the  `TPCDSQuerySuite`.",
        "createdAt" : "2020-08-17T05:21:36Z",
        "updatedAt" : "2020-08-17T09:39:09Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "5a6c032c-70cc-4eb5-ba2a-b681576ac87d",
        "parentId" : "ff31cf4f-f806-411c-87ce-8f7076fd7b47",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I believe it needs more config tunning, to make sure we can plan shuffle hash join, not sort merge join or broadcast hash join.",
        "createdAt" : "2020-08-17T08:22:44Z",
        "updatedAt" : "2020-08-17T09:39:09Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "ebdf7fe325ef10228eb129dd10c6db566ef584d8",
    "line" : 54,
    "diffHunk" : "@@ -1,1 +298,302 @@      conf.setConf(SQLConf.CBO_ENABLED, true)\n      conf.setConf(SQLConf.PLAN_STATS_ENABLED, true)\n      conf.setConf(SQLConf.JOIN_REORDER_ENABLED, true)\n    }\n    tableNames.foreach { tableName =>"
  }
]