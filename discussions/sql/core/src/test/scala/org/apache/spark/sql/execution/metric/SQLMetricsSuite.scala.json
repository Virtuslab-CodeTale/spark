[
  {
    "id" : "dd47ee63-6375-4704-82ff-e872107c6a06",
    "prId" : 33494,
    "prUrl" : "https://github.com/apache/spark/pull/33494#pullrequestreview-717752300",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "14b8080b-8943-4965-bcec-9ae2decc9095",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "From the comment, looks like that it originally only wants to verify two build data sizes are different.",
        "createdAt" : "2021-07-29T03:08:17Z",
        "updatedAt" : "2021-07-29T03:08:17Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "3e3b963f-79f8-4349-8b8f-2a208d59d3c6",
        "parentId" : "14b8080b-8943-4965-bcec-9ae2decc9095",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "well it's actually better to verify build size of full outer is larger than build size of right outer. I can update comment if you think it's needed.",
        "createdAt" : "2021-07-29T07:07:17Z",
        "updatedAt" : "2021-07-29T07:07:17Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      },
      {
        "id" : "11539e6e-852c-4c8a-b8de-bd5ccbfa903a",
        "parentId" : "14b8080b-8943-4965-bcec-9ae2decc9095",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "That's fine. The current change looks good.",
        "createdAt" : "2021-07-29T07:21:02Z",
        "updatedAt" : "2021-07-29T07:21:02Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "cb9230f7d54b173aa5403d3559dc74c9721b50bd",
    "line" : 33,
    "diffHunk" : "@@ -1,1 +418,422 @@\n      // Test right outer join as well to verify build data size to be different\n      // from full outer join. This makes sure we take extra BitSet/OpenHashSet\n      // for full outer join into account.\n      val rojDf = leftDf.hint(\"shuffle_hash\").join("
  },
  {
    "id" : "88a1bd29-32c5-4475-ad2a-7ca602e26596",
    "prId" : 33447,
    "prUrl" : "https://github.com/apache/spark/pull/33447#pullrequestreview-713881678",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d1e9bd01-989b-4958-be61-afdfb706555a",
        "parentId" : null,
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "Thanks @viirya, I can also help look into this.",
        "createdAt" : "2021-07-23T10:20:18Z",
        "updatedAt" : "2021-07-23T10:20:22Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      },
      {
        "id" : "6f493a2c-2062-49f6-8fbd-c27835a2d71f",
        "parentId" : "d1e9bd01-989b-4958-be61-afdfb706555a",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "ðŸ™ ",
        "createdAt" : "2021-07-23T10:25:19Z",
        "updatedAt" : "2021-07-23T10:25:19Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "a9e5391c-ef14-4113-9788-c6ffe0f65f4e",
        "parentId" : "d1e9bd01-989b-4958-be61-afdfb706555a",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "Filed https://github.com/apache/spark/pull/33494 now to log several data size metrics separately and keep debugging there.",
        "createdAt" : "2021-07-23T11:33:28Z",
        "updatedAt" : "2021-07-23T11:33:28Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      },
      {
        "id" : "61cd2ef6-b17d-43ac-8447-cbd58dd5e86f",
        "parentId" : "d1e9bd01-989b-4958-be61-afdfb706555a",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Thank you.",
        "createdAt" : "2021-07-23T16:11:38Z",
        "updatedAt" : "2021-07-23T16:11:38Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "b3009d5ed8d85372fa7affbfdbe47c264bfa1d82",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +399,403 @@  // TODO (SPARK-36272): Reenable this after we figure out why the expected size doesn't\n  // match after we adjust building's memory settings.\n  ignore(\"SPARK-32629: ShuffledHashJoin(full outer) metrics\") {\n    val uniqueLeftDf = Seq((\"1\", \"1\"), (\"11\", \"11\")).toDF(\"key\", \"value\")\n    val nonUniqueLeftDf = Seq((\"1\", \"1\"), (\"1\", \"2\"), (\"11\", \"11\")).toDF(\"key\", \"value\")"
  },
  {
    "id" : "39d114aa-7bb0-4a18-a226-7a9aed68b779",
    "prId" : 32012,
    "prUrl" : "https://github.com/apache/spark/pull/32012#pullrequestreview-625837115",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "39a9f70d-aa11-4cc7-80f6-5290a4d42253",
        "parentId" : null,
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "`repartition(1)` + `groupBy ` will eliminate the agg required exchange since `SinglePartition` can satisfies with all `Distribution` (exclude broadcast). So there change to `repartition(2)`.",
        "createdAt" : "2021-04-01T01:31:07Z",
        "updatedAt" : "2021-04-01T01:31:08Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      }
    ],
    "commit" : "1366ed31c2acc4ef5b6a77a5a4a5b4c4118800f4",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +169,173 @@    //             LocalTableScan(nodeId = 6)\n    Seq(true, false).foreach { enableWholeStage =>\n      val df = generateRandomBytesDF().repartition(2).groupBy('a).count()\n      val nodeIds = if (enableWholeStage) {\n        Set(4L, 1L)"
  }
]