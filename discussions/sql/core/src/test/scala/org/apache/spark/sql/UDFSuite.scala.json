[
  {
    "id" : "6176127f-002c-41d5-8a92-ac411bea23a3",
    "prId" : 29106,
    "prUrl" : "https://github.com/apache/spark/pull/29106#pullrequestreview-608610553",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "945777c7-3334-42d3-b55f-dce10db096e7",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Aggression ? Maybe Aggregation?",
        "createdAt" : "2021-03-08T11:12:47Z",
        "updatedAt" : "2021-03-08T11:12:59Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "9013bdba-4958-4e6c-b1a6-058739721cbd",
        "parentId" : "945777c7-3334-42d3-b55f-dce10db096e7",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "yea should be Aggregation",
        "createdAt" : "2021-03-08T11:45:33Z",
        "updatedAt" : "2021-03-08T11:45:33Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "8b7a37a3-b595-48b3-8f49-1af2e65baeeb",
        "parentId" : "945777c7-3334-42d3-b55f-dce10db096e7",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "I am writing tests for intervals in UDF now. I will fix this in the PR for intervals, if you don't mind.",
        "createdAt" : "2021-03-08T11:54:11Z",
        "updatedAt" : "2021-03-08T11:54:46Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "e1cd634c-59e3-4fe6-a2a6-27b027844320",
        "parentId" : "945777c7-3334-42d3-b55f-dce10db096e7",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "thanks!",
        "createdAt" : "2021-03-08T12:09:16Z",
        "updatedAt" : "2021-03-08T12:09:16Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "0435bc76-1eda-4f36-930c-2558ccea97e0",
        "parentId" : "945777c7-3334-42d3-b55f-dce10db096e7",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Here is the PR with the fix: https://github.com/apache/spark/pull/31779",
        "createdAt" : "2021-03-08T12:33:56Z",
        "updatedAt" : "2021-03-08T12:33:56Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "1e0d320c-48a3-4511-bf50-5f524e3f403a",
        "parentId" : "945777c7-3334-42d3-b55f-dce10db096e7",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "lol",
        "createdAt" : "2021-03-10T11:33:00Z",
        "updatedAt" : "2021-03-10T11:33:00Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "427d112f3fcff00076f2895dc3a47a1ba9e035a7",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +777,781 @@  }\n\n  test(\"SPARK-32307: Aggression that use map type input UDF as group expression\") {\n    spark.udf.register(\"key\", udf((m: Map[String, String]) => m.keys.head.toInt))\n    Seq(Map(\"1\" -> \"one\", \"2\" -> \"two\")).toDF(\"a\").createOrReplaceTempView(\"t\")"
  },
  {
    "id" : "e5d16b34-70fc-4c57-ad56-a515393b6479",
    "prId" : 29050,
    "prUrl" : "https://github.com/apache/spark/pull/29050#pullrequestreview-446096832",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "382f8aca-bb51-4a48-af84-51305e331b52",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Just a qestion; we need the two cases here for checking the issue? I mean; is either one not enough for tests? ",
        "createdAt" : "2020-07-09T11:26:54Z",
        "updatedAt" : "2020-07-09T11:36:31Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "0f08bf4c-021f-4053-a09f-721fca386e99",
        "parentId" : "382f8aca-bb51-4a48-af84-51305e331b52",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "It should be enough. But just want to improve test coverage on both non-primitive and primitive types for Scala UDF.",
        "createdAt" : "2020-07-10T03:17:55Z",
        "updatedAt" : "2020-07-10T03:17:56Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "08749bd0c1c2e6524866c7eb8e36e3c0ffb57835",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +686,690 @@    OuterScopes.addOuterScope(MalformedClassObject)\n    val f1 = new MalformedClassObject.MalformedNonPrimitiveFunction()\n    val f2 = new MalformedClassObject.MalformedPrimitiveFunction()\n\n    val e1 = intercept[SparkException] {"
  },
  {
    "id" : "34c2ac31-85d9-4055-9260-465850f1170d",
    "prId" : 28979,
    "prUrl" : "https://github.com/apache/spark/pull/28979#pullrequestreview-441658508",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f1b21483-c163-4a0f-bae5-512bf0b6d509",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "can we add one more test to combine `Instant` and `Timestamp`? This is not possible before this PR.",
        "createdAt" : "2020-07-02T13:06:00Z",
        "updatedAt" : "2020-07-09T13:18:13Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "871c35ffd3d71cef6040849561ae07a8d4e6b370",
    "line" : 65,
    "diffHunk" : "@@ -1,1 +531,535 @@  }\n\n  test(\"Using combined types of Instant/LocalDate in UDF\") {\n    val dtf = DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss\")\n    val date = LocalDate.parse(\"2019-02-26\")"
  },
  {
    "id" : "4b36beb2-6643-4ad4-b489-d0c7a5596c23",
    "prId" : 28979,
    "prUrl" : "https://github.com/apache/spark/pull/28979#pullrequestreview-444548726",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b79a1350-d821-4992-80a2-65451e35cdf6",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Thanks for adding this test. To check the unsupported case, could you add tests for the toplevel-null case (catch encoder exceptions)? https://github.com/apache/spark/pull/28979#discussion_r450548418\r\n",
        "createdAt" : "2020-07-08T02:56:43Z",
        "updatedAt" : "2020-07-09T13:18:13Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "e65d81c8-93f7-44ef-aea6-9c06e8eeb94d",
        "parentId" : "b79a1350-d821-4992-80a2-65451e35cdf6",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "added in 8314a04",
        "createdAt" : "2020-07-08T09:00:52Z",
        "updatedAt" : "2020-07-09T13:18:13Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "871c35ffd3d71cef6040849561ae07a8d4e6b370",
    "line" : 122,
    "diffHunk" : "@@ -1,1 +588,592 @@      udf((t: Timestamp, i: Instant) => null.asInstanceOf[TimestampInstantType]))\n    checkAnswer(df.selectExpr(\"buildTimestampInstantType(t, i) as ti\"),\n      Row(null))\n  }\n"
  },
  {
    "id" : "ce2356de-ce39-4ac5-83d9-81c37c1a958d",
    "prId" : 28645,
    "prUrl" : "https://github.com/apache/spark/pull/28645#pullrequestreview-418443673",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "51f1c2dd-d463-413b-bed7-91acfb7c5f9e",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "can we also test some special cases:\r\n1. the catalyst schema has more fields than the case class. e.g. `struct<key: int, value: string, col: int>` and `case class TestData(key: Int, value: String)`\r\n2. the fields order doesn't match, e.g. `struct<value: string, key: int>` and `case class TestData(key: Int, value: String)`\r\n3. the catalyst schema has missing fields, e.g. `struct<key: int>` and `case class TestData(key: Int, value: String)`",
        "createdAt" : "2020-05-26T15:56:23Z",
        "updatedAt" : "2020-06-19T06:05:17Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "f29a62ac985c6714f0cd6ff5bdd919b2fce89723",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +583,587 @@  }\n\n  test(\"case class as element type of Seq/Array\") {\n    val f1 = (s: Seq[TestData]) => s.map(d => d.key * d.value.toInt).sum\n    val myUdf1 = udf(f1)"
  },
  {
    "id" : "259768da-b4f3-4965-906c-56fde37a3f84",
    "prId" : 28645,
    "prUrl" : "https://github.com/apache/spark/pull/28645#pullrequestreview-433047949",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3ec37fd4-2800-4bec-a523-98d16bf55fe0",
        "parentId" : null,
        "authorId" : "eb4e5e35-a753-4069-bd95-fda614c83ab3",
        "body" : "some option tests would be good too. \r\n\r\noption in a simple seq:\r\n`Seq(Seq(Some(1), None)).toDF.withColumn(\"value\", udf{ s: Seq[Option[Int]] => s.map(_.map(_ + 1)) }.apply(col(\"value\")) )`\r\n\r\noption for function argument:\r\n`Seq(None, Some(1), None).toDF.withColumn(\"value\", udf{ o: Option[Int] => o.map(_ + 1) }.apply(col(\"value\")))`\r\n\r\nnote that the top level option to express nullability is a very common use case in particular and supported by encoders. the equivalent in Dataset is:\r\n`Seq(None, Some(1), None).toDS.map{ o: Option[Int] => o.map(_ + 1) }`\r\n",
        "createdAt" : "2020-05-26T17:05:50Z",
        "updatedAt" : "2020-06-19T06:05:17Z",
        "lastEditedBy" : "eb4e5e35-a753-4069-bd95-fda614c83ab3",
        "tags" : [
        ]
      },
      {
        "id" : "8c81f256-3ec5-4250-9cc6-354fad8fdd28",
        "parentId" : "3ec37fd4-2800-4bec-a523-98d16bf55fe0",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "There's implementation difference between `udf` and `Dataset.map`. So for the second case you're mentioned, it only work in  `Dataset.map` but fail in `udf`.",
        "createdAt" : "2020-05-27T13:50:59Z",
        "updatedAt" : "2020-06-19T06:05:17Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "b57ac8de-515b-4cda-afff-eaadd25342dc",
        "parentId" : "3ec37fd4-2800-4bec-a523-98d16bf55fe0",
        "authorId" : "eb4e5e35-a753-4069-bd95-fda614c83ab3",
        "body" : "> There's implementation difference between `udf` and `Dataset.map`. So for the second case you're mentioned, it only work in `Dataset.map` but fail in `udf`.\r\n\r\nif the goal is to stop people from writing their own implementations of udf then the second case is also needed...",
        "createdAt" : "2020-06-05T13:00:37Z",
        "updatedAt" : "2020-06-19T06:05:17Z",
        "lastEditedBy" : "eb4e5e35-a753-4069-bd95-fda614c83ab3",
        "tags" : [
        ]
      },
      {
        "id" : "5cb84ece-0614-4d79-8182-7fbd46d95324",
        "parentId" : "3ec37fd4-2800-4bec-a523-98d16bf55fe0",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "@koertkuipers I've updated to support the second case.",
        "createdAt" : "2020-06-18T08:19:48Z",
        "updatedAt" : "2020-06-19T06:05:17Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "f29a62ac985c6714f0cd6ff5bdd919b2fce89723",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +583,587 @@  }\n\n  test(\"case class as element type of Seq/Array\") {\n    val f1 = (s: Seq[TestData]) => s.map(d => d.key * d.value.toInt).sum\n    val myUdf1 = udf(f1)"
  }
]