[
  {
    "id" : "34c245c3-95bb-4153-989e-34f2a06d954a",
    "prId" : 31131,
    "prUrl" : "https://github.com/apache/spark/pull/31131#pullrequestreview-565650533",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2480a4ee-b0c3-4f88-a06e-99f39cc1e520",
        "parentId" : null,
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "nit: not sure why need to have `QueryTest` here - I thought the trait already extends it?",
        "createdAt" : "2021-01-11T18:14:38Z",
        "updatedAt" : "2021-01-12T17:21:06Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "342e1a4b-23c2-4a17-a824-2cac01e2542c",
        "parentId" : "2480a4ee-b0c3-4f88-a06e-99f39cc1e520",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "This is a workaround for the issue which I don't know how to resolve something else. If remove `QueryTest`, I am getting the error:\r\n```\r\nAn exception or error caused a run to abort: Method org/apache/spark/sql/hive/execution/command/AlterTableRenamePartitionSuite.org$apache$spark$sql$execution$command$v1$AlterTableRenamePartitionSuiteBase$$super$checkAnswer(Lscala/Function0;Lscala/collection/Seq;)V is abstract \r\njava.lang.AbstractMethodError: Method org/apache/spark/sql/hive/execution/command/AlterTableRenamePartitionSuite.org$apache$spark$sql$execution$command$v1$AlterTableRenamePartitionSuiteBase$$super$checkAnswer(Lscala/Function0;Lscala/collection/Seq;)V is abstract\r\n\tat org.apache.spark.sql.hive.execution.command.AlterTableRenamePartitionSuite.org$apache$spark$sql$execution$command$v1$AlterTableRenamePartitionSuiteBase$$super$checkAnswer(AlterTableRenamePartitionSuite.scala)\r\n\tat org.apache.spark.sql.execution.command.v1.AlterTableRenamePartitionSuiteBase.$anonfun$$init$$6(AlterTableRenamePartitionSuite.scala:60)\r\n\tat org.apache.spark.sql.execution.command.v1.AlterTableRenamePartitionSuiteBase.$anonfun$$init$$6$adapted(AlterTableRenamePartitionSuite.scala:53)\r\n```\r\n\r\nIf you have any ideas how to solve the issue, you are welcome.",
        "createdAt" : "2021-01-11T18:59:57Z",
        "updatedAt" : "2021-01-12T17:21:06Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "4153ba2b254756fb7d865e8746d48cc2defd1e91",
    "line" : 32,
    "diffHunk" : "@@ -1,1 +65,69 @@        assert(spark.catalog.isCached(t))\n        assert(tableSize == getTableSize(t))\n        QueryTest.checkAnswer(sql(s\"SELECT * FROM $t\"), Seq(Row(0, 2), Row(1, 1)))\n      }\n    }"
  },
  {
    "id" : "f06c6080-c9a4-4265-9d7d-7eb79d9c6eef",
    "prId" : 30863,
    "prUrl" : "https://github.com/apache/spark/pull/30863#pullrequestreview-557066328",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8f97d59b-d09c-42ad-b311-4b0abc10d3b3",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "I put the 4 tests here because they fail with V1 Hive External Catalog:\r\n```\r\n  spark_catalog.ns.tbl is not a valid TableIdentifier as it has more than 2 name parts.\r\n  org.apache.spark.sql.AnalysisException: spark_catalog.ns.tbl is not a valid TableIdentifier as it has more than 2 name parts.\r\n      at org.apache.spark.sql.connector.catalog.CatalogV2Implicits$MultipartIdentifierHelper.asTableIdentifier(CatalogV2Implicits.scala:130)\r\n      at org.apache.spark.sql.hive.test.TestHiveQueryExecution$$anonfun$1.applyOrElse(TestHive.scala:606)\r\n      at org.apache.spark.sql.hive.test.TestHiveQueryExecution$$anonfun$1.applyOrElse(TestHive.scala:606)\r\n```",
        "createdAt" : "2020-12-20T15:39:14Z",
        "updatedAt" : "2020-12-22T15:53:16Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "f2581376-2faf-45f5-a846-a67c3998cec7",
        "parentId" : "8f97d59b-d09c-42ad-b311-4b0abc10d3b3",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "This is the second appearance. Can we fix this issue first?",
        "createdAt" : "2020-12-21T13:25:17Z",
        "updatedAt" : "2020-12-22T15:53:16Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "3f40b624-e500-4bcb-bd32-f27f66ea3c4a",
        "parentId" : "8f97d59b-d09c-42ad-b311-4b0abc10d3b3",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "@cloud-fan Here is the fix https://github.com/apache/spark/pull/30883",
        "createdAt" : "2020-12-22T07:33:59Z",
        "updatedAt" : "2020-12-22T15:53:16Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "04ca5581-bb00-4e52-aea5-a22afc1cccc6",
        "parentId" : "8f97d59b-d09c-42ad-b311-4b0abc10d3b3",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "I moved those 4 tests to the common v1 trait.",
        "createdAt" : "2020-12-22T13:08:19Z",
        "updatedAt" : "2020-12-22T15:53:16Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "0eb89aeba46213a280bee1984d04acaa2205f989",
    "line" : 76,
    "diffHunk" : "@@ -1,1 +74,78 @@  }\n\n  test(\"single part partition\") {\n    withNamespaceAndTable(\"ns\", \"tbl\") { t =>\n      createSinglePartTable(t)"
  },
  {
    "id" : "85b5d8ef-6690-4192-8cf2-8abe3b13d354",
    "prId" : 30863,
    "prUrl" : "https://github.com/apache/spark/pull/30863#pullrequestreview-558825915",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "71c686e0-8849-409d-ab90-169a9865bc0f",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "It would be helpful if you add a comment before this PR about the new hierarchy of the test suites, @MaxGekk .",
        "createdAt" : "2020-12-23T01:00:29Z",
        "updatedAt" : "2020-12-23T01:00:29Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "96858d8b-67d2-4a19-a8c7-b73adffd21e7",
        "parentId" : "71c686e0-8849-409d-ab90-169a9865bc0f",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "this is a good idea. Shall we do it for all the new test suites?",
        "createdAt" : "2020-12-23T09:05:57Z",
        "updatedAt" : "2020-12-23T09:05:57Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "3296924a-3768-4729-a74f-0059a4f98956",
        "parentId" : "71c686e0-8849-409d-ab90-169a9865bc0f",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Let me do that separately for all unified test suites, and  to don't block this PR. ",
        "createdAt" : "2020-12-23T10:01:33Z",
        "updatedAt" : "2020-12-23T10:02:38Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "5068f3c4-29d1-4742-a5dc-41e6516d55ee",
        "parentId" : "71c686e0-8849-409d-ab90-169a9865bc0f",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "> Let me do that separately for all unified test suites, and to don't block this PR.\r\n\r\nSince this is merged, please proceed the comment PR, @MaxGekk .",
        "createdAt" : "2020-12-23T13:41:39Z",
        "updatedAt" : "2020-12-23T13:41:39Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "02e200a2-c840-43cc-9b5a-eaf586914f1c",
        "parentId" : "71c686e0-8849-409d-ab90-169a9865bc0f",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Here is the PR https://github.com/apache/spark/pull/30929 , please, review it.",
        "createdAt" : "2020-12-25T15:56:27Z",
        "updatedAt" : "2020-12-25T15:56:27Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "0eb89aeba46213a280bee1984d04acaa2205f989",
    "line" : 25,
    "diffHunk" : "@@ -1,1 +23,27 @@import org.apache.spark.sql.internal.SQLConf\n\ntrait AlterTableRenamePartitionSuiteBase extends command.AlterTableRenamePartitionSuiteBase {\n  protected def createSinglePartTable(t: String): Unit = {\n    sql(s\"CREATE TABLE $t (id bigint, data string) $defaultUsing PARTITIONED BY (id)\")"
  }
]