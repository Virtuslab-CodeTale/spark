[
  {
    "id" : "5ea7cd77-e7bb-4f0a-be49-799bb7674b41",
    "prId" : 31491,
    "prUrl" : "https://github.com/apache/spark/pull/31491#pullrequestreview-587754351",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "290ab22a-757e-4aec-b821-a1a4e048dbf1",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "h2 database cannot generate rowid-typed data?",
        "createdAt" : "2021-02-10T12:51:27Z",
        "updatedAt" : "2021-02-17T01:59:57Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "a6ed9b89-22b4-490e-8289-7fb0a8b32a6c",
        "parentId" : "290ab22a-757e-4aec-b821-a1a4e048dbf1",
        "authorId" : "3a227965-84e0-47cf-9974-11293764f028",
        "body" : "It cannot.\r\nH2 has `_rowid_` as a hidden column but it's not compatible with JDBC ROWID.\r\n`_rowid_` is represented as `long` and H2 doesn't support `getRowId`.\r\nhttps://github.com/h2database/h2database/blob/6290b79a2418189c5faa0e0506bf6503fc7630e6/h2/src/main/org/h2/jdbc/JdbcResultSet.java#L3292",
        "createdAt" : "2021-02-10T15:46:20Z",
        "updatedAt" : "2021-02-17T01:59:57Z",
        "lastEditedBy" : "3a227965-84e0-47cf-9974-11293764f028",
        "tags" : [
        ]
      }
    ],
    "commit" : "73f53d91e609a7450819ee3bb14b81eee34d42f9",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +1786,1790 @@\n  test(\"SPARK-34379: Map JDBC RowID to StringType rather than LongType\") {\n    val mockRsmd = mock(classOf[java.sql.ResultSetMetaData])\n    when(mockRsmd.getColumnCount).thenReturn(1)\n    when(mockRsmd.getColumnLabel(anyInt())).thenReturn(\"rowid\")"
  },
  {
    "id" : "c42b4510-8216-4b5a-aeb3-2f3b0686aa18",
    "prId" : 29191,
    "prUrl" : "https://github.com/apache/spark/pull/29191#pullrequestreview-453856030",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c92c65a1-2f38-4091-9eaf-b9d15a01bb3b",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "This is the test case for https://github.com/apache/spark/pull/29191#discussion_r459223066 , @cloud-fan ~",
        "createdAt" : "2020-07-23T06:11:06Z",
        "updatedAt" : "2020-07-23T06:11:07Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "81ac6c614c16e8d3c43ff14888216a797df04275",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +1722,1726 @@  }\n\n  test(\"SPARK-32364: JDBCOption constructor\") {\n    val extraOptions = CaseInsensitiveMap[String](Map(\"UrL\" -> \"url1\", \"dBTable\" -> \"table1\"))\n    val connectionProperties = new Properties()"
  },
  {
    "id" : "e4e5caab-3f1b-430a-b5c1-ce8ed1163824",
    "prId" : 26334,
    "prUrl" : "https://github.com/apache/spark/pull/26334#pullrequestreview-309997475",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9a6eeaa3-5ecb-46b8-8ef9-b0e692a24bd6",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "You can probably just check for the first sentence rather than depend on this exact entire string. But it doesn't matter much.",
        "createdAt" : "2019-10-31T16:04:16Z",
        "updatedAt" : "2019-10-31T16:04:19Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      }
    ],
    "commit" : "0de85296406d0b431eadde1742400eb0d9954aba",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +1660,1664 @@    }.getMessage\n    assert(e.contains(\n      \"Invalid value `test` for parameter `isolationLevel`. This can be \" +\n      \"`NONE`, `READ_UNCOMMITTED`, `READ_COMMITTED`, `REPEATABLE_READ` or `SERIALIZABLE`.\"))\n  }"
  },
  {
    "id" : "b070d2ca-0ce1-4312-a174-9a0f41b8bf74",
    "prId" : 26242,
    "prUrl" : "https://github.com/apache/spark/pull/26242#pullrequestreview-307746246",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1cdf1ddf-5072-4391-99a3-64bf3d9a0148",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "nit: It seems the most part of this test is the same with https://github.com/apache/spark/blob/master/sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCSuite.scala#L1517-L1565\r\nSo, can we share it by defining a helper func?",
        "createdAt" : "2019-10-25T02:04:56Z",
        "updatedAt" : "2019-10-25T07:38:45Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "4d5c78c3-f529-4830-9aea-7c5fec0480cb",
        "parentId" : "1cdf1ddf-5072-4391-99a3-64bf3d9a0148",
        "authorId" : "9c99fec9-cc01-4c44-8bb8-60e23ac802b1",
        "body" : "I reused it to verify the correctness of the method.",
        "createdAt" : "2019-10-25T02:10:50Z",
        "updatedAt" : "2019-10-25T02:45:21Z",
        "lastEditedBy" : "9c99fec9-cc01-4c44-8bb8-60e23ac802b1",
        "tags" : [
        ]
      },
      {
        "id" : "a2a04a73-3af0-4f5f-ad93-c3655f8711f7",
        "parentId" : "1cdf1ddf-5072-4391-99a3-64bf3d9a0148",
        "authorId" : "9c99fec9-cc01-4c44-8bb8-60e23ac802b1",
        "body" : "@maropu \r\nI didn't think of a good way, maybe maybe I can combine them like this\r\n```\r\ntest(\"SPARK-22814 support date/timestamp types in partitionColumn\") {\r\n    val expectedResult = Seq(\r\n      (\"2018-07-06\", \"2018-07-06 05:50:00.0\"),\r\n      (\"2018-07-06\", \"2018-07-06 08:10:08.0\"),\r\n      (\"2018-07-08\", \"2018-07-08 13:32:01.0\"),\r\n      (\"2018-07-12\", \"2018-07-12 09:51:15.0\")\r\n    ).map { case (date, timestamp) =>\r\n      Row(Date.valueOf(date), Timestamp.valueOf(timestamp))\r\n    }\r\n\r\n    // DateType partition column\r\n    val df1 = spark.read.format(\"jdbc\")\r\n      .option(\"url\", urlWithUserAndPass)\r\n      .option(\"dbtable\", \"TEST.DATETIME\")\r\n      .option(\"partitionColumn\", \"d\")\r\n      .option(\"lowerBound\", \"2018-07-06\")\r\n      .option(\"upperBound\", \"2018-07-20\")\r\n      .option(\"numPartitions\", 3)\r\n      .load()\r\n\r\n    val df2 = spark.read.jdbc(\r\n      url = urlWithUserAndPass,\r\n      table = \"TEST.DATETIME\",\r\n      columnName = \"d\",\r\n      lowerBound = \"2018-07-06\",\r\n      upperBound = \"2018-07-20\",\r\n      numPartitions = 3,\r\n      connectionProperties = new Properties()\r\n    )\r\n\r\n    dateMatch(df1)\r\n    dateMatch(df2)\r\n    checkAnswer(df1, expectedResult)\r\n    checkAnswer(df2, expectedResult)\r\n\r\n    def dateMatch(df: DataFrame): Unit ={\r\n      df.logicalPlan match {\r\n        case LogicalRelation(JDBCRelation(_, parts, _), _, _, _) =>\r\n          val whereClauses = parts.map(_.asInstanceOf[JDBCPartition].whereClause).toSet\r\n          assert(whereClauses === Set(\r\n            \"\"\"\"D\" < '2018-07-10' or \"D\" is null\"\"\",\r\n            \"\"\"\"D\" >= '2018-07-10' AND \"D\" < '2018-07-14'\"\"\",\r\n            \"\"\"\"D\" >= '2018-07-14'\"\"\"))\r\n      }\r\n    }\r\n\r\n    // TimestampType partition column\r\n    val df3 = spark.read.format(\"jdbc\")\r\n      .option(\"url\", urlWithUserAndPass)\r\n      .option(\"dbtable\", \"TEST.DATETIME\")\r\n      .option(\"partitionColumn\", \"t\")\r\n      .option(\"lowerBound\", \"2018-07-04 03:30:00.0\")\r\n      .option(\"upperBound\", \"2018-07-27 14:11:05.0\")\r\n      .option(\"numPartitions\", 2)\r\n      .load()\r\n\r\n    val df4 = spark.read.jdbc(\r\n      url = urlWithUserAndPass,\r\n      table = \"TEST.DATETIME\",\r\n      columnName = \"t\",\r\n      lowerBound = \"2018-07-04 03:30:00.0\",\r\n      upperBound = \"2018-07-27 14:11:05.0\",\r\n      numPartitions = 2,\r\n      connectionProperties = new Properties()\r\n    )\r\n\r\n    timeStampMatch(df3)\r\n    timeStampMatch(df4)\r\n    checkAnswer(df3, expectedResult)\r\n    checkAnswer(df4, expectedResult)\r\n\r\n    def timeStampMatch(df: DataFrame): Unit ={\r\n      df.logicalPlan match {\r\n        case LogicalRelation(JDBCRelation(_, parts, _), _, _, _) =>\r\n          val whereClauses = parts.map(_.asInstanceOf[JDBCPartition].whereClause).toSet\r\n          assert(whereClauses === Set(\r\n            \"\"\"\"T\" < '2018-07-15 20:50:32.5' or \"T\" is null\"\"\",\r\n            \"\"\"\"T\" >= '2018-07-15 20:50:32.5'\"\"\"))\r\n      }\r\n    }\r\n  }\r\n```",
        "createdAt" : "2019-10-28T10:12:22Z",
        "updatedAt" : "2019-10-28T10:12:22Z",
        "lastEditedBy" : "9c99fec9-cc01-4c44-8bb8-60e23ac802b1",
        "tags" : [
        ]
      }
    ],
    "commit" : "23a2420a017b8b03f84fddd5cd559d709a45af36",
    "line" : 131,
    "diffHunk" : "@@ -1,1 +1712,1716 @@    }\n    checkAnswer(df2, expectedResult)\n  }\n}"
  },
  {
    "id" : "4d24d5ec-caa4-4436-8998-c214729d96a4",
    "prId" : 25375,
    "prUrl" : "https://github.com/apache/spark/pull/25375#pullrequestreview-272669695",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "89655469-3f83-49fc-8045-ce8b70396d55",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Can we explicitly set the conf in the test?",
        "createdAt" : "2019-08-08T02:08:23Z",
        "updatedAt" : "2019-08-08T22:58:33Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "96f9235b-7633-4371-bf25-14956962e758",
        "parentId" : "89655469-3f83-49fc-8045-ce8b70396d55",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "Do you mean remove `password` declaration?",
        "createdAt" : "2019-08-08T15:50:15Z",
        "updatedAt" : "2019-08-08T22:58:33Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      }
    ],
    "commit" : "e764f9eb66c8ecd199dd89687e9115b2a0f8bad5",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +1032,1036 @@\n  test(\"Hide credentials in show create table\") {\n    val password = \"testPass\"\n    val tableName = \"tab1\"\n    withTable(tableName) {"
  },
  {
    "id" : "c3545f8c-b7a3-456e-bf8b-853b620708eb",
    "prId" : 25375,
    "prUrl" : "https://github.com/apache/spark/pull/25375#pullrequestreview-279377575",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d52576af-229f-46f4-b3ab-85ae28198728",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "WithSQLConf should be used for testing this. For example, change the value of spark.sql.redaction.options.regex and see whether it works as expected. \r\n\r\nNormally, we do not rely on the default value of a SQLConf. ",
        "createdAt" : "2019-08-26T00:28:01Z",
        "updatedAt" : "2019-08-26T00:28:01Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "330d723e-1deb-4d3b-821e-d258d465d7f6",
        "parentId" : "d52576af-229f-46f4-b3ab-85ae28198728",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "https://github.com/apache/spark/pull/25579",
        "createdAt" : "2019-08-26T02:39:45Z",
        "updatedAt" : "2019-08-26T02:39:45Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      }
    ],
    "commit" : "e764f9eb66c8ecd199dd89687e9115b2a0f8bad5",
    "line" : 34,
    "diffHunk" : "@@ -1,1 +1048,1052 @@      val show = ShowCreateTableCommand(TableIdentifier(tableName))\n      spark.sessionState.executePlan(show).executedPlan.executeCollect().foreach { r =>\n        assert(!r.toString.contains(password))\n      }\n"
  }
]