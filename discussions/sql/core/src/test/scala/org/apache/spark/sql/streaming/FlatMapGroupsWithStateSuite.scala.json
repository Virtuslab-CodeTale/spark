[
  {
    "id" : "2d675d32-5e8d-4605-b124-0ed9800e283a",
    "prId" : 33093,
    "prUrl" : "https://github.com/apache/spark/pull/33093#pullrequestreview-697104176",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0fe6dd7b-547a-4936-974e-d5ce92da3a4d",
        "parentId" : null,
        "authorId" : "9cd657f0-37cd-41bd-908a-f2c996b95a7a",
        "body" : "docs explaining what this function does",
        "createdAt" : "2021-06-30T15:17:06Z",
        "updatedAt" : "2021-06-30T15:17:06Z",
        "lastEditedBy" : "9cd657f0-37cd-41bd-908a-f2c996b95a7a",
        "tags" : [
        ]
      },
      {
        "id" : "ab12eccf-3a5f-4206-8f63-65c882a24cd8",
        "parentId" : "0fe6dd7b-547a-4936-974e-d5ce92da3a4d",
        "authorId" : "0e7383af-ae8f-4891-9ab6-e80efa3990c0",
        "body" : "done",
        "createdAt" : "2021-07-01T10:56:36Z",
        "updatedAt" : "2021-07-01T10:56:36Z",
        "lastEditedBy" : "0e7383af-ae8f-4891-9ab6-e80efa3990c0",
        "tags" : [
        ]
      }
    ],
    "commit" : "eb83b684fdc7d62846b3860f90d26ec119c136c5",
    "line" : 27,
    "diffHunk" : "@@ -1,1 +1274,1278 @@   * along with the updated state. The state is incremented for every value.\n   */\n  val flatMapGroupsWithStateFunc =\n    (key: String, values: Iterator[String], state: GroupState[RunningCount]) => {\n      val valList = values.toSeq"
  },
  {
    "id" : "da8b3867-32aa-4b4f-91da-442d8c88aebb",
    "prId" : 33093,
    "prUrl" : "https://github.com/apache/spark/pull/33093#pullrequestreview-697708038",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fcd7cf69-7b43-43ef-81b9-58d7d3360ff9",
        "parentId" : null,
        "authorId" : "0e7383af-ae8f-4891-9ab6-e80efa3990c0",
        "body" : "nit: fix this",
        "createdAt" : "2021-07-01T22:52:16Z",
        "updatedAt" : "2021-07-01T22:52:16Z",
        "lastEditedBy" : "0e7383af-ae8f-4891-9ab6-e80efa3990c0",
        "tags" : [
        ]
      }
    ],
    "commit" : "eb83b684fdc7d62846b3860f90d26ec119c136c5",
    "line" : 296,
    "diffHunk" : "@@ -1,1 +1542,1546 @@      // Returns the data and the count (-1 if count reached beyond 2 and state was just removed)\n      val stateFunc =\n          (key: String, values: Iterator[(String, Long)], state: GroupState[RunningCount]) => {\n        if (state.hasTimedOut) {\n          state.remove()"
  },
  {
    "id" : "4805a7f7-b8ec-4615-a473-2f240a43c5bb",
    "prId" : 33093,
    "prUrl" : "https://github.com/apache/spark/pull/33093#pullrequestreview-697729759",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f2424434-ecc7-439d-84bd-e6d4dfd7b572",
        "parentId" : null,
        "authorId" : "9cd657f0-37cd-41bd-908a-f2c996b95a7a",
        "body" : "rather than doing this complicated logic of no  updating when some magical set of conditions are met .. isnt it simpler to have \r\n\r\n`if (!key.contains(\"NoUpdate\")) state.update(...)`\r\n\r\nand then pass a key name `keyOnlyInStateButNoUpate` or `keyInStateAndDataButNoUpate`??",
        "createdAt" : "2021-07-01T23:53:31Z",
        "updatedAt" : "2021-07-01T23:53:31Z",
        "lastEditedBy" : "9cd657f0-37cd-41bd-908a-f2c996b95a7a",
        "tags" : [
        ]
      }
    ],
    "commit" : "eb83b684fdc7d62846b3860f90d26ec119c136c5",
    "line" : 42,
    "diffHunk" : "@@ -1,1 +1289,1293 @@      if (!key.contains(\"NoUpdate\")) {\n        // this is not reached when valList is empty and the state count is 2\n        state.update(new RunningCount(count))\n      }\n      Iterator((key, valList, count.toString))"
  },
  {
    "id" : "4a332226-e614-4546-a5aa-05a70c2c38d2",
    "prId" : 33093,
    "prUrl" : "https://github.com/apache/spark/pull/33093#pullrequestreview-697738080",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f376ae7e-c0d6-4ba4-8695-dfdf7da9026b",
        "parentId" : null,
        "authorId" : "9cd657f0-37cd-41bd-908a-f2c996b95a7a",
        "body" : "`keyInStateAndData-1` is NOT in data. this is confusing!",
        "createdAt" : "2021-07-01T23:55:44Z",
        "updatedAt" : "2021-07-01T23:55:45Z",
        "lastEditedBy" : "9cd657f0-37cd-41bd-908a-f2c996b95a7a",
        "tags" : [
        ]
      },
      {
        "id" : "9491341f-ecae-4f04-bfc5-1ec374f17101",
        "parentId" : "f376ae7e-c0d6-4ba4-8695-dfdf7da9026b",
        "authorId" : "0e7383af-ae8f-4891-9ab6-e80efa3990c0",
        "body" : "hmm it is added in second batch",
        "createdAt" : "2021-07-02T00:14:01Z",
        "updatedAt" : "2021-07-02T00:14:01Z",
        "lastEditedBy" : "0e7383af-ae8f-4891-9ab6-e80efa3990c0",
        "tags" : [
        ]
      },
      {
        "id" : "f4ecc600-d920-44cf-916c-4e550180a162",
        "parentId" : "f376ae7e-c0d6-4ba4-8695-dfdf7da9026b",
        "authorId" : "9cd657f0-37cd-41bd-908a-f2c996b95a7a",
        "body" : "this is nit, the point naming it like this is its in both state and data of the first batch which is what we are mainly testing. hence it is confusing.",
        "createdAt" : "2021-07-02T00:18:58Z",
        "updatedAt" : "2021-07-02T00:18:58Z",
        "lastEditedBy" : "9cd657f0-37cd-41bd-908a-f2c996b95a7a",
        "tags" : [
        ]
      }
    ],
    "commit" : "eb83b684fdc7d62846b3860f90d26ec119c136c5",
    "line" : 75,
    "diffHunk" : "@@ -1,1 +1322,1326 @@            (\"keyNoUpdate\", Seq[String](), \"2\"), // update will not be called\n            (\"keyInStateAndData-2\", Seq[String](\"keyInStateAndData-2\"), \"3\"), // inc by 1\n            (\"keyInStateAndData-1\", Seq[String](), \"1\"),\n            (\"keyOnlyInData\", Seq[String](\"keyOnlyInData\"), \"1\") // inc by 1\n          ),"
  },
  {
    "id" : "5b3f8857-8dff-43cf-a727-ac28755f7135",
    "prId" : 32938,
    "prUrl" : "https://github.com/apache/spark/pull/32938#pullrequestreview-688961569",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "21930a52-1807-40e5-b676-73471eb10d8d",
        "parentId" : null,
        "authorId" : "9cd657f0-37cd-41bd-908a-f2c996b95a7a",
        "body" : "We should add a Java unit test!",
        "createdAt" : "2021-06-22T00:21:16Z",
        "updatedAt" : "2021-06-22T00:21:16Z",
        "lastEditedBy" : "9cd657f0-37cd-41bd-908a-f2c996b95a7a",
        "tags" : [
        ]
      }
    ],
    "commit" : "ba3a3abf60b92bcbc309306c639d00612766921b",
    "line" : 427,
    "diffHunk" : "@@ -1,1 +1415,1419 @@  }\n\n  def newStateStore(): StateStore = new MemoryStateStore()\n\n  val intProj = UnsafeProjection.create(Array[DataType](IntegerType))"
  },
  {
    "id" : "18a4c316-f6f5-4c3e-9b5c-5945b30d9602",
    "prId" : 27333,
    "prUrl" : "https://github.com/apache/spark/pull/27333#pullrequestreview-516291993",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3cce7a06-3a56-40fc-a308-bd6403a27b2a",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Why we need to add data here? Don't we rely on `MultiAddData` below to add more data to `newInput1` and `newInput2`?",
        "createdAt" : "2020-10-24T04:15:25Z",
        "updatedAt" : "2020-10-24T04:15:26Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "37a6fa04-0758-48bb-87c2-97cffa1cc37a",
        "parentId" : "3cce7a06-3a56-40fc-a308-bd6403a27b2a",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "We're initializing the different memory streams here, which are not aware of previous data and require us to fill the same data again on restore. (Each addition to the memory stream represents a batch, hence you'll need to add the same for the new memory stream.)\n\nProbably there could be the another kind of small improvement for tests if we have convenient way to push all the input rows we put before to the new memory stream. That may be dealt with minor PR.",
        "createdAt" : "2020-10-24T06:59:05Z",
        "updatedAt" : "2020-10-24T06:59:06Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "bf2c14e8-54c6-464a-97c6-1f02ab56ef9a",
        "parentId" : "3cce7a06-3a56-40fc-a308-bd6403a27b2a",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Oh I got it. We reuse the same checkpoint directory and so we need to fill same batches of previous stream query. Actually we even don't need to put the same data.\r\n\r\nThen the new query begins from the new added data, and read the same state store.",
        "createdAt" : "2020-10-24T17:03:19Z",
        "updatedAt" : "2020-10-24T18:01:35Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "466363edb22ea83a81e21a72f1b983dc7b5a733e",
    "line" : 53,
    "diffHunk" : "@@ -1,1 +1060,1064 @@\n      newInput1.addData(\"input1-a\")\n      newInput2.addData(\"input2-a\")\n\n      testStream(newUnionDf, Update)("
  }
]