[
  {
    "id" : "82820a6c-0293-44ca-bcd0-5e17d858034f",
    "prId" : 33172,
    "prUrl" : "https://github.com/apache/spark/pull/33172#pullrequestreview-697300023",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3f3bf983-a249-4fc5-8cd0-6807fec72eea",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "now partition coalescing applies here and then we are sorting one single partition, which fails this test. Adding the partition number to skip partition coalescing",
        "createdAt" : "2021-07-01T14:15:07Z",
        "updatedAt" : "2021-07-01T14:15:25Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "51546e87c33064c80f902d4eecff1f640222dfb2",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1814,1818 @@\n    // Distribute and order by.\n    val df4 = data.repartition(5, $\"a\").sortWithinPartitions($\"b\".desc)\n    // Walk each partition and verify that it is sorted descending and does not contain all\n    // the values."
  },
  {
    "id" : "2d0b31fb-2737-4af1-83c8-3ed4cd69d2a5",
    "prId" : 33103,
    "prUrl" : "https://github.com/apache/spark/pull/33103#pullrequestreview-693366749",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ed61eccd-19be-4c08-9a36-0c208dee0c6c",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Can we have a test coverage in `catalyst` module additionally?",
        "createdAt" : "2021-06-26T23:59:50Z",
        "updatedAt" : "2021-06-27T00:00:00Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "b610e17e-6f72-469d-8097-b9a45914e56b",
        "parentId" : "ed61eccd-19be-4c08-9a36-0c208dee0c6c",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Ok, let me add one.",
        "createdAt" : "2021-06-27T00:01:06Z",
        "updatedAt" : "2021-06-27T00:01:06Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "9a082844a1bf9c3eac5f956fbfde1f36b7a65ed3",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +2916,2920 @@  }\n\n  test(\"SPARK-35886: PromotePrecision should be subexpr replaced\") {\n    withTable(\"tbl\") {\n      sql("
  },
  {
    "id" : "9a33c559-886a-4a9b-a1e5-697c8d968668",
    "prId" : 32699,
    "prUrl" : "https://github.com/apache/spark/pull/32699#pullrequestreview-673870495",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "30664bf4-35ac-40dd-95c4-6e132e26feda",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Q: What if a tree has more deeply-nested common exprs? The current logic can work well? e.g., I thought it like this;\r\n```\r\n        // subExpr1 = simpleUDF($\"id\");\r\n        // subExpr2 = functions.length(subExpr1);\r\n        // subExpr3 = functions.xxxx(subExpr2);\r\n        // subExpr4 = ...\r\n```",
        "createdAt" : "2021-06-01T14:26:33Z",
        "updatedAt" : "2021-06-01T14:26:33Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "370c0cf7-6628-4a9a-a1c0-3e27672d6562",
        "parentId" : "30664bf4-35ac-40dd-95c4-6e132e26feda",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Yes, this is actually the cases this logic to deal with. Previous common expression gen-ed codes will put into the map. The code generator looks up into the map when generating code for later common expressions to replace the semantic-equal expression with gen-ed code value.\r\n",
        "createdAt" : "2021-06-01T16:19:19Z",
        "updatedAt" : "2021-06-01T16:19:20Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "60f44f15-128a-4bfe-a6fa-964a76e7b98b",
        "parentId" : "30664bf4-35ac-40dd-95c4-6e132e26feda",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Nice",
        "createdAt" : "2021-06-02T07:27:03Z",
        "updatedAt" : "2021-06-02T07:27:03Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "f1f64f7ba0f6246a21515ffddc2d99efbe2409db",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +2895,2899 @@        // Common exprs:\n        //  1. simpleUDF($\"id\")\n        //  2. functions.length(simpleUDF($\"id\"))\n        // We should only evaluate `simpleUDF($\"id\")` once, i.e.\n        // subExpr1 = simpleUDF($\"id\");"
  },
  {
    "id" : "750ba251-6d08-4a39-90b3-b506d6a7680a",
    "prId" : 32559,
    "prUrl" : "https://github.com/apache/spark/pull/32559#pullrequestreview-663732731",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ba627d44-4834-4ed0-8b22-1360aff0465d",
        "parentId" : null,
        "authorId" : "92aa9573-b64a-4a86-aebf-54c827df3ed0",
        "body" : "I think the fix for https://issues.apache.org/jira/browse/SPARK-35449 will break this, since it's really a \"bug\" that the case value is included in subexpression resolution without an else value. Not a huge deal, I can try to fix in my follow up once this is merged",
        "createdAt" : "2021-05-19T23:16:10Z",
        "updatedAt" : "2021-05-19T23:16:10Z",
        "lastEditedBy" : "92aa9573-b64a-4a86-aebf-54c827df3ed0",
        "tags" : [
        ]
      }
    ],
    "commit" : "9973c1a4d6bbaa089b120e47a8418e99b132704d",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +2870,2874 @@      s\n    })\n    val df1 = spark.range(5).select(when(functions.length(simpleUDF($\"id\")) > 0,\n      functions.length(simpleUDF($\"id\"))))\n    df1.collect()"
  },
  {
    "id" : "637d68d8-c9d8-4565-b975-02ab2b43616c",
    "prId" : 31955,
    "prUrl" : "https://github.com/apache/spark/pull/31955#pullrequestreview-620841563",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6d0fd560-83e3-4bd2-a054-6a499acababe",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Can this issue happens only in codegen paths? Could you add tests for interpreted paths, too?",
        "createdAt" : "2021-03-25T00:29:50Z",
        "updatedAt" : "2021-03-26T10:50:32Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "1b04d5fc-03e7-4d24-aff9-868276cd3982",
        "parentId" : "6d0fd560-83e3-4bd2-a054-6a499acababe",
        "authorId" : "3d4870da-39a8-4406-b00d-131930d14cd8",
        "body" : "I don't think there is a codegen path for `ArrayTransform`. It can call `ScalaUDF.eval()` on its interpreted path only (https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/higherOrderFunctions.scala#L282).",
        "createdAt" : "2021-03-25T08:35:44Z",
        "updatedAt" : "2021-03-26T10:50:32Z",
        "lastEditedBy" : "3d4870da-39a8-4406-b00d-131930d14cd8",
        "tags" : [
        ]
      }
    ],
    "commit" : "d00e70596e1b679d2331375629249905e9c75395",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +2763,2767 @@    val df = Seq(Array(\"abc\", \"def\")).toDF(\"array\")\n    val test = df.select(transform(col(\"array\"), s => reverse(s)))\n    checkAnswer(test, Row(Array(\"cba\", \"fed\")) :: Nil)\n\n    val df2 = Seq(Array(Bar2(\"abc\"), Bar2(\"def\"))).toDF(\"array\")"
  },
  {
    "id" : "7112aba4-bf4a-4fd6-b2ab-5c641b893395",
    "prId" : 31854,
    "prUrl" : "https://github.com/apache/spark/pull/31854#pullrequestreview-619340335",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "50a24d04-40a4-4ec7-b4d4-cbacd2879bfc",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I don't think this is intentional. How many tests are broken if we switch to `parseMultipartIdentifier`? We probably should make this behavior change.",
        "createdAt" : "2021-03-22T08:28:02Z",
        "updatedAt" : "2021-03-22T08:28:02Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "1103f284-b42a-4113-a955-47088a804776",
        "parentId" : "50a24d04-40a4-4ec7-b4d4-cbacd2879bfc",
        "authorId" : "3a227965-84e0-47cf-9974-11293764f028",
        "body" : "> ow many tests are broken if we switch to parseMultipartIdentifier\r\n\r\n40+ tests fail.\r\nhttps://github.com/apache/spark/runs/2164624369?check_suite_focus=true\r\n\r\n> I don't think this is intentional\r\n\r\nHmm, according to the comment on `UnresolvedAttribute.quotedString`, it seems to rely on the behavior.\r\nhttps://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/unresolved.scala#L188-L193",
        "createdAt" : "2021-03-24T04:40:52Z",
        "updatedAt" : "2021-03-24T04:41:46Z",
        "lastEditedBy" : "3a227965-84e0-47cf-9974-11293764f028",
        "tags" : [
        ]
      },
      {
        "id" : "250fc2d2-fc05-489c-a183-ddbed75a035c",
        "parentId" : "50a24d04-40a4-4ec7-b4d4-cbacd2879bfc",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "If 40+ tests fail, we definitely shouldn't do it in this PR.\r\n\r\nIt's intentional when we write the code, but I don't think it's intentional to allow end-users to do something like `df3.select($\"*-#&% ?.`a``b.c`\")`.",
        "createdAt" : "2021-03-24T05:32:02Z",
        "updatedAt" : "2021-03-24T05:32:02Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "249f8ccf3c910b58fe3440ace768700b267e623f",
    "line" : 26,
    "diffHunk" : "@@ -1,1 +2761,2765 @@    checkAnswer(df3.select(df3(\"*-#&% ?.`a``b.c`\")), Row(\"col1\"))\n    checkAnswer(df3.select(col(\"*-#&% ?.`a``b.c`\")), Row(\"col1\"))\n    checkAnswer(df3.select($\"*-#&% ?.`a``b.c`\"), Row(\"col1\"))\n  }\n"
  }
]