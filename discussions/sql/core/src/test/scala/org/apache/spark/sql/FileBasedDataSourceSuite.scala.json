[
  {
    "id" : "7c6c7149-e7f2-468c-8468-b1230b037026",
    "prId" : 29761,
    "prUrl" : "https://github.com/apache/spark/pull/29761#pullrequestreview-490253573",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "54b3a450-9a00-4102-bd89-732d1696bf31",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Please change like this.\r\n```scala\r\n  Seq(\"json\", \"orc\").foreach { format =>\r\n    test(s\"SPARK-32889: column name supports special characters using $format\") {\r\n      Seq(\"$\", \" \", \",\", \";\", \"{\", \"}\", \"(\", \")\", \"\\n\", \"\\t\", \"=\").foreach { name =>\r\n        withTempDir { dir =>\r\n          val dataDir = new File(dir, \"file\").getCanonicalPath\r\n          Seq(1).toDF(name).write.format(format).save(dataDir)\r\n          val schema = spark.read.format(format).load(dataDir).schema\r\n          assert(schema.size == 1)\r\n          assertResult(name)(schema.head.name)\r\n        }\r\n      }\r\n    }\r\n  }\r\n```",
        "createdAt" : "2020-09-17T05:30:43Z",
        "updatedAt" : "2020-09-17T06:34:53Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "7d63fd96462fda9b820873a679464fe2662e6869",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +247,251 @@    }\n  }\n\n  // Text file format only supports string type\n  test(\"SPARK-24691 error handling for unsupported types - text\") {"
  },
  {
    "id" : "6832025d-fb60-419d-8d13-fa7ff39048b1",
    "prId" : 29565,
    "prUrl" : "https://github.com/apache/spark/pull/29565#pullrequestreview-478176122",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "30c28485-ac12-4fe5-ae7b-3cb61d85cc12",
        "parentId" : null,
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "@dongjoon-hyun I kept this since `FileSourceScanExec` in v1 doesn't expose API to get pushed filters. Let me know if you have better idea to test this.",
        "createdAt" : "2020-08-30T06:39:40Z",
        "updatedAt" : "2020-09-11T20:44:56Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      }
    ],
    "commit" : "5f32ee5fda08f4271959589c7f6312195898af08",
    "line" : 46,
    "diffHunk" : "@@ -1,1 +910,914 @@\n    Seq(\"orc\", \"parquet\").foreach { format =>\n      withSQLConf(SQLConf.USE_V1_SOURCE_LIST.key -> \"\") {\n        withTempPath { dir =>\n          spark.range(100).map(i => (i.toShort, i.toString)).toDF(\"id\", \"s\")"
  },
  {
    "id" : "8c879931-70f0-4b9e-a80c-327f73c6ac2d",
    "prId" : 28948,
    "prUrl" : "https://github.com/apache/spark/pull/28948#pullrequestreview-439257851",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6f218af2-82c7-4dfe-b690-df64ece9bbea",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Nice check!",
        "createdAt" : "2020-06-29T15:26:34Z",
        "updatedAt" : "2020-06-30T06:46:13Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "2d7975ec67d12025f6c09d688d6cdb033ef5072f",
    "line" : 52,
    "diffHunk" : "@@ -1,1 +935,939 @@  override def initialize(name: URI, conf: Configuration): Unit = {\n    super.initialize(name, conf)\n    require(conf.get(\"ds_option\", \"\") == \"value\")\n  }\n}"
  }
]