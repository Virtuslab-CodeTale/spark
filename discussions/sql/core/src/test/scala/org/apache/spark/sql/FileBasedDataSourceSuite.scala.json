[
  {
    "id" : "7c6c7149-e7f2-468c-8468-b1230b037026",
    "prId" : 29761,
    "prUrl" : "https://github.com/apache/spark/pull/29761#pullrequestreview-490253573",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "54b3a450-9a00-4102-bd89-732d1696bf31",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Please change like this.\r\n```scala\r\n  Seq(\"json\", \"orc\").foreach { format =>\r\n    test(s\"SPARK-32889: column name supports special characters using $format\") {\r\n      Seq(\"$\", \" \", \",\", \";\", \"{\", \"}\", \"(\", \")\", \"\\n\", \"\\t\", \"=\").foreach { name =>\r\n        withTempDir { dir =>\r\n          val dataDir = new File(dir, \"file\").getCanonicalPath\r\n          Seq(1).toDF(name).write.format(format).save(dataDir)\r\n          val schema = spark.read.format(format).load(dataDir).schema\r\n          assert(schema.size == 1)\r\n          assertResult(name)(schema.head.name)\r\n        }\r\n      }\r\n    }\r\n  }\r\n```",
        "createdAt" : "2020-09-17T05:30:43Z",
        "updatedAt" : "2020-09-17T06:34:53Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "7d63fd96462fda9b820873a679464fe2662e6869",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +247,251 @@    }\n  }\n\n  // Text file format only supports string type\n  test(\"SPARK-24691 error handling for unsupported types - text\") {"
  },
  {
    "id" : "6832025d-fb60-419d-8d13-fa7ff39048b1",
    "prId" : 29565,
    "prUrl" : "https://github.com/apache/spark/pull/29565#pullrequestreview-478176122",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "30c28485-ac12-4fe5-ae7b-3cb61d85cc12",
        "parentId" : null,
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "@dongjoon-hyun I kept this since `FileSourceScanExec` in v1 doesn't expose API to get pushed filters. Let me know if you have better idea to test this.",
        "createdAt" : "2020-08-30T06:39:40Z",
        "updatedAt" : "2020-09-11T20:44:56Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      }
    ],
    "commit" : "5f32ee5fda08f4271959589c7f6312195898af08",
    "line" : 46,
    "diffHunk" : "@@ -1,1 +910,914 @@\n    Seq(\"orc\", \"parquet\").foreach { format =>\n      withSQLConf(SQLConf.USE_V1_SOURCE_LIST.key -> \"\") {\n        withTempPath { dir =>\n          spark.range(100).map(i => (i.toShort, i.toString)).toDF(\"id\", \"s\")"
  },
  {
    "id" : "8c879931-70f0-4b9e-a80c-327f73c6ac2d",
    "prId" : 28948,
    "prUrl" : "https://github.com/apache/spark/pull/28948#pullrequestreview-439257851",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6f218af2-82c7-4dfe-b690-df64ece9bbea",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Nice check!",
        "createdAt" : "2020-06-29T15:26:34Z",
        "updatedAt" : "2020-06-30T06:46:13Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "2d7975ec67d12025f6c09d688d6cdb033ef5072f",
    "line" : 52,
    "diffHunk" : "@@ -1,1 +935,939 @@  override def initialize(name: URI, conf: Configuration): Unit = {\n    super.initialize(name, conf)\n    require(conf.get(\"ds_option\", \"\") == \"value\")\n  }\n}"
  },
  {
    "id" : "8082bb46-d843-4db4-9967-31c11380d7a2",
    "prId" : 27888,
    "prUrl" : "https://github.com/apache/spark/pull/27888#pullrequestreview-375001960",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4f306736-3ef2-416e-80aa-e1de9ab37b97",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "@kimtkyeom . I revised your test code. Could you use the following, please?\r\n- Generalize test case name\r\n- Generalize test code by using `format`.\r\n- Generalize test code by adding `orc` together. (This is a test coverage parity: the goal of this test suite.)\r\n```scala\r\n  test(\"SPARK-31116: Select nested schema with case insensitive mode\") {\r\n    // This test case failed at only Parquet. ORC is added for test coverage parity.\r\n    Seq(\"orc\", \"parquet\").foreach { format =>\r\n      Seq(\"true\", \"false\").foreach { nestedSchemaPruningEnabled =>\r\n        withSQLConf(\r\n          SQLConf.CASE_SENSITIVE.key -> \"false\",\r\n          SQLConf.NESTED_SCHEMA_PRUNING_ENABLED.key -> nestedSchemaPruningEnabled) {\r\n          withTempPath { dir =>\r\n            val path = dir.getCanonicalPath\r\n\r\n            // Prepare values for testing nested parquet data\r\n            spark\r\n              .range(1L)\r\n              .selectExpr(\"NAMED_STRUCT('lowercase', id, 'camelCase', id + 1) AS StructColumn\")\r\n              .write\r\n              .format(format)\r\n              .save(path)\r\n\r\n            val exactSchema = \"StructColumn struct<lowercase: LONG, camelCase: LONG>\"\r\n\r\n            checkAnswer(spark.read.schema(exactSchema).format(format).load(path), Row(Row(0, 1)))\r\n\r\n            // In case insensitive manner, parquet's column cases are ignored\r\n            val innerColumnCaseInsensitiveSchema =\r\n              \"StructColumn struct<Lowercase: LONG, camelcase: LONG>\"\r\n            checkAnswer(\r\n              spark.read.schema(innerColumnCaseInsensitiveSchema).format(format).load(path),\r\n              Row(Row(0, 1)))\r\n\r\n            val rootColumnCaseInsensitiveSchema =\r\n              \"structColumn struct<lowercase: LONG, camelCase: LONG>\"\r\n            checkAnswer(\r\n              spark.read.schema(rootColumnCaseInsensitiveSchema).format(format).load(path),\r\n              Row(Row(0, 1)))\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n```",
        "createdAt" : "2020-03-16T07:49:54Z",
        "updatedAt" : "2020-03-16T12:45:40Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "8f0a60ac-3836-418d-a4ec-32b59cb8a7d4",
        "parentId" : "4f306736-3ef2-416e-80aa-e1de9ab37b97",
        "authorId" : "4ad0d346-13a3-48ad-a114-c9a75205e416",
        "body" : "Cnaged it. Thank you for your revision.",
        "createdAt" : "2020-03-16T08:24:25Z",
        "updatedAt" : "2020-03-16T12:45:40Z",
        "lastEditedBy" : "4ad0d346-13a3-48ad-a114-c9a75205e416",
        "tags" : [
        ]
      },
      {
        "id" : "9dcc6476-3501-45ab-a004-4283954a2511",
        "parentId" : "4f306736-3ef2-416e-80aa-e1de9ab37b97",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Thanks!",
        "createdAt" : "2020-03-16T08:36:25Z",
        "updatedAt" : "2020-03-16T12:45:40Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "a0d9a19b2995e766d5b8a8d751d0426aea367c47",
    "line" : 43,
    "diffHunk" : "@@ -1,1 +882,886 @@      }\n    }\n  }\n}\n"
  },
  {
    "id" : "05b8967f-7588-4d87-b000-5c06be4df3b9",
    "prId" : 24830,
    "prUrl" : "https://github.com/apache/spark/pull/24830#pullrequestreview-250873970",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "568a1397-5075-477d-bc9b-12bf9768b63c",
        "parentId" : null,
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "It would be great if we also add an end-to-end test with both option `pathGlobFilter` and `recursiveFileLookup` enabled.",
        "createdAt" : "2019-06-18T06:02:50Z",
        "updatedAt" : "2019-06-19T04:15:36Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "6f2e75a56ac73240317e5cbabdc0dea34fbfe2fd",
    "line" : 80,
    "diffHunk" : "@@ -1,1 +642,646 @@    assert(fileList.toSet === expectedFileList.toSet)\n  }\n\n  test(\"Return correct results when data columns overlap with partition columns\") {\n    Seq(\"parquet\", \"orc\", \"json\").foreach { format =>"
  },
  {
    "id" : "f9657edd-5e45-4dde-b5f9-3289859ce497",
    "prId" : 24830,
    "prUrl" : "https://github.com/apache/spark/pull/24830#pullrequestreview-251466002",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1449fc1e-fb09-49be-97ed-d5b6b6ace0b2",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "nit: `withTempDir` then you don't need to call `path.mkDir`",
        "createdAt" : "2019-06-19T02:37:27Z",
        "updatedAt" : "2019-06-19T04:15:36Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "00a79d8b-9dbb-4ed5-a2d6-213430fa023b",
        "parentId" : "1449fc1e-fb09-49be-97ed-d5b6b6ace0b2",
        "authorId" : "7e665d8b-d739-4edf-88c8-7379ff8585c2",
        "body" : "There's a line `path.delete` inside `withTempDir`.\r\n```\r\n  protected def withTempPath(f: File => Unit): Unit = {\r\n    val path = Utils.createTempDir()\r\n    path.delete()\r\n    try f(path) finally Utils.deleteRecursively(path)\r\n  }\r\n```",
        "createdAt" : "2019-06-19T04:14:18Z",
        "updatedAt" : "2019-06-19T04:15:36Z",
        "lastEditedBy" : "7e665d8b-d739-4edf-88c8-7379ff8585c2",
        "tags" : [
        ]
      }
    ],
    "commit" : "6f2e75a56ac73240317e5cbabdc0dea34fbfe2fd",
    "line" : 40,
    "diffHunk" : "@@ -1,1 +602,606 @@    }\n\n    withTempPath { path =>\n      path.mkdir()\n      createDir(path, \"root\", 0)"
  },
  {
    "id" : "fd87cb85-223a-4d48-ae58-f48c74aa49d5",
    "prId" : 24327,
    "prUrl" : "https://github.com/apache/spark/pull/24327#pullrequestreview-232335626",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4e344892-cfbf-4e10-b604-88107f6a0413",
        "parentId" : null,
        "authorId" : "8c82da5a-f351-4a37-a8a9-13809311b07b",
        "body" : "nit: Use `true` directly (and cut the remaining lines)",
        "createdAt" : "2019-04-27T00:41:10Z",
        "updatedAt" : "2019-06-14T16:45:37Z",
        "lastEditedBy" : "8c82da5a-f351-4a37-a8a9-13809311b07b",
        "tags" : [
        ]
      },
      {
        "id" : "5cf9b9e3-283e-4fe4-b0a7-99c8fd51a303",
        "parentId" : "4e344892-cfbf-4e10-b604-88107f6a0413",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "We still need to specify the value of `useV1List`",
        "createdAt" : "2019-04-30T18:31:59Z",
        "updatedAt" : "2019-06-14T16:45:37Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "f658e9265ba741922fc96eec76038addcb6491a1",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +379,383 @@    Seq(true).foreach { useV1 =>\n      val useV1List = if (useV1) {\n        \"csv,orc,parquet\"\n      } else {\n        \"\""
  },
  {
    "id" : "0fe1d515-5ac7-4f45-afed-fb9ed9a068bc",
    "prId" : 24318,
    "prUrl" : "https://github.com/apache/spark/pull/24318#pullrequestreview-224507673",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "335b8ed4-bd0f-4819-9df1-4926d88cd9b5",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "@gengliangwang . In this suite, we need to test all available data sources like the following instead of using `.orc`.\r\n```\r\nSeq(\"csv\", \"orc\", \"text\").foreach { format =>\r\n```",
        "createdAt" : "2019-04-09T00:38:18Z",
        "updatedAt" : "2019-04-09T00:40:07Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "41fa25cf-b215-4ade-9d83-49293be6845d",
        "parentId" : "335b8ed4-bd0f-4819-9df1-4926d88cd9b5",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Could you generalize this test case?\r\nAlso, please add JIRA issue for `Parquet DSv2 Migration` as a TODO comment.",
        "createdAt" : "2019-04-09T00:38:47Z",
        "updatedAt" : "2019-04-09T01:26:18Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "be4d1a39-24af-4b6c-b632-ae0b7b71f343",
        "parentId" : "335b8ed4-bd0f-4819-9df1-4926d88cd9b5",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "@dongjoon-hyun The cache invalidation is for all file sources. Testing ORC here is quite sufficient, just like only Parquet is tested in  #13566.",
        "createdAt" : "2019-04-09T03:00:38Z",
        "updatedAt" : "2019-04-09T03:00:38Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "e5162a40-376c-4f60-a88f-dbc318c0eda7",
        "parentId" : "335b8ed4-bd0f-4819-9df1-4926d88cd9b5",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "~If that is the logic, let's hold on this until Parquet is ready to migrate. We don't need to move around the test logic from here to there.~\r\n\r\n~We are going to migrate Parquet anyway, aren't we?~",
        "createdAt" : "2019-04-09T16:10:10Z",
        "updatedAt" : "2019-04-09T16:21:44Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "8b01978f-c8d2-4438-a5aa-bbc9c843eeaf",
        "parentId" : "335b8ed4-bd0f-4819-9df1-4926d88cd9b5",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "~In general, this test suite is designed to verify for all data sources from the beginning.~",
        "createdAt" : "2019-04-09T16:11:43Z",
        "updatedAt" : "2019-04-09T16:21:50Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "039324d3be26337a0fe7d0b1d20d00b68a8f3bc9",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +500,504 @@        withTempDir { dir =>\n          val path = dir.toString\n          spark.range(1000).write.mode(\"overwrite\").orc(path)\n          val df = spark.read.orc(path).cache()\n          assert(df.count() == 1000)"
  },
  {
    "id" : "5a2913d3-b0c6-4a1d-bbc9-89fa8cf889d4",
    "prId" : 24318,
    "prUrl" : "https://github.com/apache/spark/pull/24318#pullrequestreview-224156355",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ff05ad56-1cf0-4085-8cb0-aeb01e9537cf",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "ditto.",
        "createdAt" : "2019-04-09T01:25:02Z",
        "updatedAt" : "2019-04-09T01:25:02Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "039324d3be26337a0fe7d0b1d20d00b68a8f3bc9",
    "line" : 25,
    "diffHunk" : "@@ -1,1 +516,520 @@        withTempDir { dir =>\n          val path = dir.toString\n          spark.range(1000).write.mode(\"append\").orc(path)\n          val df = spark.read.orc(path).cache()\n          assert(df.count() == 1000)"
  }
]