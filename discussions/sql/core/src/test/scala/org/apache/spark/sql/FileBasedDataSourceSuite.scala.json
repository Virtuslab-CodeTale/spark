[
  {
    "id" : "7c6c7149-e7f2-468c-8468-b1230b037026",
    "prId" : 29761,
    "prUrl" : "https://github.com/apache/spark/pull/29761#pullrequestreview-490253573",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "54b3a450-9a00-4102-bd89-732d1696bf31",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Please change like this.\r\n```scala\r\n  Seq(\"json\", \"orc\").foreach { format =>\r\n    test(s\"SPARK-32889: column name supports special characters using $format\") {\r\n      Seq(\"$\", \" \", \",\", \";\", \"{\", \"}\", \"(\", \")\", \"\\n\", \"\\t\", \"=\").foreach { name =>\r\n        withTempDir { dir =>\r\n          val dataDir = new File(dir, \"file\").getCanonicalPath\r\n          Seq(1).toDF(name).write.format(format).save(dataDir)\r\n          val schema = spark.read.format(format).load(dataDir).schema\r\n          assert(schema.size == 1)\r\n          assertResult(name)(schema.head.name)\r\n        }\r\n      }\r\n    }\r\n  }\r\n```",
        "createdAt" : "2020-09-17T05:30:43Z",
        "updatedAt" : "2020-09-17T06:34:53Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "7d63fd96462fda9b820873a679464fe2662e6869",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +247,251 @@    }\n  }\n\n  // Text file format only supports string type\n  test(\"SPARK-24691 error handling for unsupported types - text\") {"
  },
  {
    "id" : "6832025d-fb60-419d-8d13-fa7ff39048b1",
    "prId" : 29565,
    "prUrl" : "https://github.com/apache/spark/pull/29565#pullrequestreview-478176122",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "30c28485-ac12-4fe5-ae7b-3cb61d85cc12",
        "parentId" : null,
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "@dongjoon-hyun I kept this since `FileSourceScanExec` in v1 doesn't expose API to get pushed filters. Let me know if you have better idea to test this.",
        "createdAt" : "2020-08-30T06:39:40Z",
        "updatedAt" : "2020-09-11T20:44:56Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      }
    ],
    "commit" : "5f32ee5fda08f4271959589c7f6312195898af08",
    "line" : 46,
    "diffHunk" : "@@ -1,1 +910,914 @@\n    Seq(\"orc\", \"parquet\").foreach { format =>\n      withSQLConf(SQLConf.USE_V1_SOURCE_LIST.key -> \"\") {\n        withTempPath { dir =>\n          spark.range(100).map(i => (i.toShort, i.toString)).toDF(\"id\", \"s\")"
  },
  {
    "id" : "8c879931-70f0-4b9e-a80c-327f73c6ac2d",
    "prId" : 28948,
    "prUrl" : "https://github.com/apache/spark/pull/28948#pullrequestreview-439257851",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6f218af2-82c7-4dfe-b690-df64ece9bbea",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Nice check!",
        "createdAt" : "2020-06-29T15:26:34Z",
        "updatedAt" : "2020-06-30T06:46:13Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "2d7975ec67d12025f6c09d688d6cdb033ef5072f",
    "line" : 52,
    "diffHunk" : "@@ -1,1 +935,939 @@  override def initialize(name: URI, conf: Configuration): Unit = {\n    super.initialize(name, conf)\n    require(conf.get(\"ds_option\", \"\") == \"value\")\n  }\n}"
  },
  {
    "id" : "8082bb46-d843-4db4-9967-31c11380d7a2",
    "prId" : 27888,
    "prUrl" : "https://github.com/apache/spark/pull/27888#pullrequestreview-375001960",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4f306736-3ef2-416e-80aa-e1de9ab37b97",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "@kimtkyeom . I revised your test code. Could you use the following, please?\r\n- Generalize test case name\r\n- Generalize test code by using `format`.\r\n- Generalize test code by adding `orc` together. (This is a test coverage parity: the goal of this test suite.)\r\n```scala\r\n  test(\"SPARK-31116: Select nested schema with case insensitive mode\") {\r\n    // This test case failed at only Parquet. ORC is added for test coverage parity.\r\n    Seq(\"orc\", \"parquet\").foreach { format =>\r\n      Seq(\"true\", \"false\").foreach { nestedSchemaPruningEnabled =>\r\n        withSQLConf(\r\n          SQLConf.CASE_SENSITIVE.key -> \"false\",\r\n          SQLConf.NESTED_SCHEMA_PRUNING_ENABLED.key -> nestedSchemaPruningEnabled) {\r\n          withTempPath { dir =>\r\n            val path = dir.getCanonicalPath\r\n\r\n            // Prepare values for testing nested parquet data\r\n            spark\r\n              .range(1L)\r\n              .selectExpr(\"NAMED_STRUCT('lowercase', id, 'camelCase', id + 1) AS StructColumn\")\r\n              .write\r\n              .format(format)\r\n              .save(path)\r\n\r\n            val exactSchema = \"StructColumn struct<lowercase: LONG, camelCase: LONG>\"\r\n\r\n            checkAnswer(spark.read.schema(exactSchema).format(format).load(path), Row(Row(0, 1)))\r\n\r\n            // In case insensitive manner, parquet's column cases are ignored\r\n            val innerColumnCaseInsensitiveSchema =\r\n              \"StructColumn struct<Lowercase: LONG, camelcase: LONG>\"\r\n            checkAnswer(\r\n              spark.read.schema(innerColumnCaseInsensitiveSchema).format(format).load(path),\r\n              Row(Row(0, 1)))\r\n\r\n            val rootColumnCaseInsensitiveSchema =\r\n              \"structColumn struct<lowercase: LONG, camelCase: LONG>\"\r\n            checkAnswer(\r\n              spark.read.schema(rootColumnCaseInsensitiveSchema).format(format).load(path),\r\n              Row(Row(0, 1)))\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n```",
        "createdAt" : "2020-03-16T07:49:54Z",
        "updatedAt" : "2020-03-16T12:45:40Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "8f0a60ac-3836-418d-a4ec-32b59cb8a7d4",
        "parentId" : "4f306736-3ef2-416e-80aa-e1de9ab37b97",
        "authorId" : "4ad0d346-13a3-48ad-a114-c9a75205e416",
        "body" : "Cnaged it. Thank you for your revision.",
        "createdAt" : "2020-03-16T08:24:25Z",
        "updatedAt" : "2020-03-16T12:45:40Z",
        "lastEditedBy" : "4ad0d346-13a3-48ad-a114-c9a75205e416",
        "tags" : [
        ]
      },
      {
        "id" : "9dcc6476-3501-45ab-a004-4283954a2511",
        "parentId" : "4f306736-3ef2-416e-80aa-e1de9ab37b97",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Thanks!",
        "createdAt" : "2020-03-16T08:36:25Z",
        "updatedAt" : "2020-03-16T12:45:40Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "a0d9a19b2995e766d5b8a8d751d0426aea367c47",
    "line" : 43,
    "diffHunk" : "@@ -1,1 +882,886 @@      }\n    }\n  }\n}\n"
  },
  {
    "id" : "05b8967f-7588-4d87-b000-5c06be4df3b9",
    "prId" : 24830,
    "prUrl" : "https://github.com/apache/spark/pull/24830#pullrequestreview-250873970",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "568a1397-5075-477d-bc9b-12bf9768b63c",
        "parentId" : null,
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "It would be great if we also add an end-to-end test with both option `pathGlobFilter` and `recursiveFileLookup` enabled.",
        "createdAt" : "2019-06-18T06:02:50Z",
        "updatedAt" : "2019-06-19T04:15:36Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "6f2e75a56ac73240317e5cbabdc0dea34fbfe2fd",
    "line" : 80,
    "diffHunk" : "@@ -1,1 +642,646 @@    assert(fileList.toSet === expectedFileList.toSet)\n  }\n\n  test(\"Return correct results when data columns overlap with partition columns\") {\n    Seq(\"parquet\", \"orc\", \"json\").foreach { format =>"
  },
  {
    "id" : "f9657edd-5e45-4dde-b5f9-3289859ce497",
    "prId" : 24830,
    "prUrl" : "https://github.com/apache/spark/pull/24830#pullrequestreview-251466002",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1449fc1e-fb09-49be-97ed-d5b6b6ace0b2",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "nit: `withTempDir` then you don't need to call `path.mkDir`",
        "createdAt" : "2019-06-19T02:37:27Z",
        "updatedAt" : "2019-06-19T04:15:36Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "00a79d8b-9dbb-4ed5-a2d6-213430fa023b",
        "parentId" : "1449fc1e-fb09-49be-97ed-d5b6b6ace0b2",
        "authorId" : "7e665d8b-d739-4edf-88c8-7379ff8585c2",
        "body" : "There's a line `path.delete` inside `withTempDir`.\r\n```\r\n  protected def withTempPath(f: File => Unit): Unit = {\r\n    val path = Utils.createTempDir()\r\n    path.delete()\r\n    try f(path) finally Utils.deleteRecursively(path)\r\n  }\r\n```",
        "createdAt" : "2019-06-19T04:14:18Z",
        "updatedAt" : "2019-06-19T04:15:36Z",
        "lastEditedBy" : "7e665d8b-d739-4edf-88c8-7379ff8585c2",
        "tags" : [
        ]
      }
    ],
    "commit" : "6f2e75a56ac73240317e5cbabdc0dea34fbfe2fd",
    "line" : 40,
    "diffHunk" : "@@ -1,1 +602,606 @@    }\n\n    withTempPath { path =>\n      path.mkdir()\n      createDir(path, \"root\", 0)"
  }
]