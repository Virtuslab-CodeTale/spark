[
  {
    "id" : "387cb145-bd25-4ba8-bd78-762acbff8c6f",
    "prId" : 29000,
    "prUrl" : "https://github.com/apache/spark/pull/29000#pullrequestreview-471383286",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "32764e02-b600-4a95-809b-e182eb231f88",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "I tried the test and it can pass without this fix. Can you take a look? ",
        "createdAt" : "2020-08-04T09:38:26Z",
        "updatedAt" : "2020-11-24T16:38:32Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "cacc43fe-d8fe-4c2a-89e1-cbc309f9f1c0",
        "parentId" : "32764e02-b600-4a95-809b-e182eb231f88",
        "authorId" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "body" : "@WinkerDu update this case?",
        "createdAt" : "2020-08-17T12:01:12Z",
        "updatedAt" : "2020-11-24T16:38:32Z",
        "lastEditedBy" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "tags" : [
        ]
      },
      {
        "id" : "d08dc617-687b-4fec-9557-a042ac79032e",
        "parentId" : "32764e02-b600-4a95-809b-e182eb231f88",
        "authorId" : "19bcd642-216d-45ca-9e34-13e334fdc8fc",
        "body" : "@Ngone51 already fix it, please have an another try :)",
        "createdAt" : "2020-08-20T08:39:42Z",
        "updatedAt" : "2020-11-24T16:38:32Z",
        "lastEditedBy" : "19bcd642-216d-45ca-9e34-13e334fdc8fc",
        "tags" : [
        ]
      }
    ],
    "commit" : "85aa12a618ceadfe510a4f9fc3718a746a1bc357",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +167,171 @@  }\n\n  test(\"SPARK-27194 SPARK-29302: Fix commit collision in dynamic partition overwrite mode\") {\n    withSQLConf(SQLConf.PARTITION_OVERWRITE_MODE.key ->\n      SQLConf.PartitionOverwriteMode.DYNAMIC.toString,"
  },
  {
    "id" : "bfbdf80a-0ba4-4bbb-b1b9-e3890a0500c1",
    "prId" : 29000,
    "prUrl" : "https://github.com/apache/spark/pull/29000#pullrequestreview-534701873",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0f9fc714-c7e9-4c42-be40-7badac036e4d",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "So `FileOutputCommitter` will clean up the output path `.spark-staging-$jobId`at the beginning, which avoids the FileAlreadyExistException?",
        "createdAt" : "2020-11-19T13:47:32Z",
        "updatedAt" : "2020-11-24T16:38:32Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "4ea2bdc3-d032-4592-babe-d3bf15b68072",
        "parentId" : "0f9fc714-c7e9-4c42-be40-7badac036e4d",
        "authorId" : "19bcd642-216d-45ca-9e34-13e334fdc8fc",
        "body" : "Actually FileOutputCommitter will not clean the path `.spark-staging-$jobId`, you can try to run this ut without the fix, `FileAlreadyExistException` would raise.",
        "createdAt" : "2020-11-19T17:49:38Z",
        "updatedAt" : "2020-11-24T16:38:32Z",
        "lastEditedBy" : "19bcd642-216d-45ca-9e34-13e334fdc8fc",
        "tags" : [
        ]
      }
    ],
    "commit" : "85aa12a618ceadfe510a4f9fc3718a746a1bc357",
    "line" : 55,
    "diffHunk" : "@@ -1,1 +207,211 @@    val stagingPartDir = new File(stagingDir, \"p1=2\")\n    stagingPartDir.mkdirs()\n    val conflictTaskFile = new File(stagingPartDir, s\"part-00000-$jobId.c000.snappy.parquet\")\n    conflictTaskFile.createNewFile()\n  }"
  },
  {
    "id" : "db049133-9ba7-4857-b5e1-2f30da619a5f",
    "prId" : 25863,
    "prUrl" : "https://github.com/apache/spark/pull/25863#pullrequestreview-294662075",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "33f99c4d-7eaf-4481-abc7-9b2e457cc0ab",
        "parentId" : null,
        "authorId" : "b6393788-12cc-4066-9294-857f4baec092",
        "body" : "@advancedxy \r\nRemove this UT, because dataFrame can not specify partition key value.\r\nSo with our current approach, they can not write concurrently.\r\n\r\nhttps://github.com/apache/spark/blob/a1b90bfc0faa2b4b2b7388443a734a217792d585/sql/core/src/main/scala/org/apache/spark/sql/DataFrameWriter.scala#L412-L420\r\n\r\nMay be we can support specify partition key for data frame in the future.",
        "createdAt" : "2019-09-25T18:14:50Z",
        "updatedAt" : "2019-12-09T16:11:48Z",
        "lastEditedBy" : "b6393788-12cc-4066-9294-857f4baec092",
        "tags" : [
        ]
      },
      {
        "id" : "afa5de71-6f10-4e40-9cbf-d4e64cf26fd3",
        "parentId" : "33f99c4d-7eaf-4481-abc7-9b2e457cc0ab",
        "authorId" : "b6393788-12cc-4066-9294-857f4baec092",
        "body" : "Or we can parse the literal partition column to static partition key value.",
        "createdAt" : "2019-09-25T23:09:17Z",
        "updatedAt" : "2019-12-09T16:11:48Z",
        "lastEditedBy" : "b6393788-12cc-4066-9294-857f4baec092",
        "tags" : [
        ]
      },
      {
        "id" : "67b2fb29-7a7a-43c9-869d-5bb9fe6fb570",
        "parentId" : "33f99c4d-7eaf-4481-abc7-9b2e457cc0ab",
        "authorId" : "b6393788-12cc-4066-9294-857f4baec092",
        "body" : "P.S. unreachable code in this ut will be removed with other commits together.",
        "createdAt" : "2019-09-25T23:12:27Z",
        "updatedAt" : "2019-12-09T16:11:48Z",
        "lastEditedBy" : "b6393788-12cc-4066-9294-857f4baec092",
        "tags" : [
        ]
      },
      {
        "id" : "20821300-b0b7-4297-a372-52beb10aca0e",
        "parentId" : "33f99c4d-7eaf-4481-abc7-9b2e457cc0ab",
        "authorId" : "8e87861a-4202-49a2-baf7-6d51f6aaa5a2",
        "body" : "Ah, that's a limitation of `DataFrameWriter`, we may need to extend `DataFrameWriter` to support that. \r\n\r\nBut currently, i think we can simply use the SQL syntax since we can use `spark.sql` and get the same behaviour.",
        "createdAt" : "2019-09-29T03:29:15Z",
        "updatedAt" : "2019-12-09T16:11:48Z",
        "lastEditedBy" : "8e87861a-4202-49a2-baf7-6d51f6aaa5a2",
        "tags" : [
        ]
      }
    ],
    "commit" : "f45ca9bf6c51d3efb064a87de56b985e17b60788",
    "line" : 125,
    "diffHunk" : "@@ -1,1 +246,250 @@        checkAnswer(spark.sql(\"select a, b from ta where b = 2\"), df2)\n      }\n    }\n  }\n"
  }
]