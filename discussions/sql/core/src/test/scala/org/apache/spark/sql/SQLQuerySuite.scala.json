[
  {
    "id" : "9a90ee21-5828-4cba-a2f8-f4f1834b1f72",
    "prId" : 32465,
    "prUrl" : "https://github.com/apache/spark/pull/32465#pullrequestreview-654904676",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8d0194a6-c210-44e1-a520-96d83d74b083",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "If you don't mind, could you add a test case for HINT explicitly?",
        "createdAt" : "2021-05-07T22:21:30Z",
        "updatedAt" : "2021-05-08T09:40:56Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "51632625e9fb3f09800112fe121de1dcae1dacee",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +4102,4106 @@  }\n\n  test(\"SPARK-35331: Fix resolving original expression in RepartitionByExpression after aliased\") {\n    Seq(\"CLUSTER\", \"DISTRIBUTE\").foreach { keyword =>\n      Seq(\"a\", \"substr(a, 0, 3)\").foreach { expr =>"
  },
  {
    "id" : "7762e03f-4de3-43f6-bc9b-f69903ba9c7f",
    "prId" : 32239,
    "prUrl" : "https://github.com/apache/spark/pull/32239#pullrequestreview-640526140",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b7a31d04-4d59-498c-9646-a1c57ea23fea",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "nit: It's safer for branch-3.0 to have this test, too, I think.",
        "createdAt" : "2021-04-21T00:13:12Z",
        "updatedAt" : "2021-04-21T00:13:13Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "63cac4d0c13214cec2b9bed38ea848651dbe387b",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +3599,3603 @@\n      assert(spark.table(\"t\").groupBy($\"c.json_string\").count().schema.fieldNames ===\n        Seq(\"json_string\", \"count\"))\n    }\n  }"
  },
  {
    "id" : "4f3ff6b2-0d1a-4d66-a925-89595c4e9477",
    "prId" : 32167,
    "prUrl" : "https://github.com/apache/spark/pull/32167#pullrequestreview-641781071",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5e2f0674-51ee-4946-bce8-2aa562ec317c",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I think this case is covered at least at `sql/core/src/test/resources/sql-tests/inputs/outer-join.sql`",
        "createdAt" : "2021-04-15T09:59:38Z",
        "updatedAt" : "2021-04-15T09:59:38Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "2ffb95f9-6dd9-4984-b48d-6a612eb4e082",
        "parentId" : "5e2f0674-51ee-4946-bce8-2aa562ec317c",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "WDYT about the above @HyukjinKwon 's comment, @zengruios ? Is this PR different because this is `String`? If this is already covered, shall we close this PR?",
        "createdAt" : "2021-04-22T05:09:51Z",
        "updatedAt" : "2021-04-22T05:09:52Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "586b690c96fb2e3c36aa8d294be052b95ebdfd4e",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +406,410 @@      Row(null))\n    checkAnswer(\n      sql(\"SELECT COALESCE(s, n) FROM nullStrings\"),\n      Seq(Row(\"abc\"), Row(\"ABC\"), Row(\"3\")))\n  }"
  },
  {
    "id" : "9c7caeb6-4384-45c4-a229-b487097775a8",
    "prId" : 32158,
    "prUrl" : "https://github.com/apache/spark/pull/32158#pullrequestreview-635235473",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d64ffe52-f52a-4215-9a85-dff1912c4309",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "```suggestion\r\n      checkAnswer(queryCoalesce, Row(\"2\") :: Nil)\r\n```",
        "createdAt" : "2021-04-14T05:56:10Z",
        "updatedAt" : "2021-04-16T10:45:10Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "91ec3479-3f0b-4279-b03c-e3c9cfc7d340",
        "parentId" : "d64ffe52-f52a-4215-9a85-dff1912c4309",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "In the worst case (e.g., author is inactive but PR is ready .. ?), we could push an empty commit to their branch to retrigger .. of course this is an ugly way though",
        "createdAt" : "2021-04-14T05:57:06Z",
        "updatedAt" : "2021-04-16T10:45:10Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "fc61869ad7fda1fd62b48067539f3c5d84d05e00",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +65,69 @@\n      checkAnswer(queryCaseWhen, Row(\"1.0\") :: Nil)\n      checkAnswer(queryCoalesce, Row(\"2\") :: Nil)\n    }\n  }"
  },
  {
    "id" : "a22ec104-6119-43bb-a9f6-b91954ea3dcf",
    "prId" : 32144,
    "prUrl" : "https://github.com/apache/spark/pull/32144#pullrequestreview-635494049",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e7cda342-c598-4154-8e98-bc93d1e247c7",
        "parentId" : null,
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "w/o the patch\r\n```scala\r\nspark-sql> SET io.file.buffer.size;\r\nio.file.buffer.size\t<undefined>\r\n```",
        "createdAt" : "2021-04-14T11:10:02Z",
        "updatedAt" : "2021-04-14T14:43:40Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "b6d26b7d87917e7e230ca38f2817c6110b461370",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +1082,1086 @@\n    // io.file.buffer.size has a default value from `SparkHadoopUtil.newConfiguration`\n    checkAnswer(sql(\"SET io.file.buffer.size\"), Row(\"io.file.buffer.size\", \"65536\"))\n  }\n"
  },
  {
    "id" : "16693624-3b7c-4e34-a857-63ca418a2209",
    "prId" : 32144,
    "prUrl" : "https://github.com/apache/spark/pull/32144#pullrequestreview-640165194",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b1e3f1a5-c046-45ae-a7ad-856f8a8a2f5b",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "This breaks the Hadoop 2.7 tests, seems the behavior is different between hadoop 2.7 and hadoop 3: https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test/job/spark-master-test-sbt-hadoop-2.7/2364/testReport/org.apache.spark.sql/SQLQuerySuite/SPARK_35044__SET_command_shall_display_default_value_for_hadoop_conf_correctly/",
        "createdAt" : "2021-04-20T16:07:53Z",
        "updatedAt" : "2021-04-20T16:07:53Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "7a6c4df6-75c5-47ab-8fa0-e69c21faa425",
        "parentId" : "b1e3f1a5-c046-45ae-a7ad-856f8a8a2f5b",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I picked a different config which seems to work: `dfs.hosts`\r\n\r\n@yaooqinn can you create a followup PR to try it?",
        "createdAt" : "2021-04-20T16:13:48Z",
        "updatedAt" : "2021-04-20T16:13:48Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "10d7780b-99df-430a-ba38-432fac39d136",
        "parentId" : "b1e3f1a5-c046-45ae-a7ad-856f8a8a2f5b",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "OK",
        "createdAt" : "2021-04-20T16:16:12Z",
        "updatedAt" : "2021-04-20T16:16:12Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "b6d26b7d87917e7e230ca38f2817c6110b461370",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +1079,1083 @@    // these keys does not exist as default yet\n    checkAnswer(sql(s\"SET ${key}no\"), Row(key + \"no\", \"<undefined>\"))\n    checkAnswer(sql(\"SET dfs.replication\"), Row(\"dfs.replication\", \"<undefined>\"))\n\n    // io.file.buffer.size has a default value from `SparkHadoopUtil.newConfiguration`"
  },
  {
    "id" : "c08c1888-e2c2-4b98-a11b-f0eec7493e64",
    "prId" : 31892,
    "prUrl" : "https://github.com/apache/spark/pull/31892#pullrequestreview-616084768",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1129f893-0510-4967-827e-faf0f56c53a6",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "is INSERT necessary to reproduce the bug?",
        "createdAt" : "2021-03-19T05:50:43Z",
        "updatedAt" : "2021-03-19T05:50:43Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "f24e4de7-44bc-4476-b0c3-7d59872f1483",
        "parentId" : "1129f893-0510-4967-827e-faf0f56c53a6",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "@cloud-fan - yes I think so.\r\n\r\nFor query:\r\n\r\n```\r\nSELECT t1.k FROM left_table t1\r\nJOIN empty_right_table t2\r\nON t1.k = t2.k\r\nLIMIT 3\r\n```\r\n\r\nThe physical plan:\r\n\r\n\r\n```\r\nCollectLimit 3\r\n+- *(2) Project [k#228L]\r\n   +- *(2) BroadcastHashJoin [k#228L], [k#229L], Inner, BuildRight, false\r\n      :- *(2) Filter isnotnull(k#228L)\r\n      :  +- *(2) ColumnarToRow\r\n      :     +- FileScan parquet default.left_table[k#228L] Batched: true, DataFilters: [isnotnull(k#228L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/chengsu/spark/sql/core/spark-warehouse/org.apache.spark.sq..., PartitionFilters: [], PushedFilters: [IsNotNull(k)], ReadSchema: struct<k:bigint>\r\n      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [id=#148]\r\n         +- *(1) Filter isnotnull(k#229L)\r\n            +- *(1) ColumnarToRow\r\n               +- FileScan parquet default.empty_right_table[k#229L] Batched: true, DataFilters: [isnotnull(k#229L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/chengsu/spark/sql/core/spark-warehouse/org.apache.spark.sq..., PartitionFilters: [], PushedFilters: [IsNotNull(k)], ReadSchema: struct<k:bigint>\r\n```\r\n\r\nThis is not reproduced any more as there's no code-gen `BaseLimitExec` after `BroadcastHashJoin`.",
        "createdAt" : "2021-03-19T06:06:10Z",
        "updatedAt" : "2021-03-19T06:06:10Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      }
    ],
    "commit" : "d57f914b79f8c6372f1ecf3116a5956f0bbce54b",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +4108,4112 @@        spark.sql(\n          \"\"\"\n            |INSERT INTO TABLE output_table\n            |SELECT t1.k FROM left_table t1\n            |JOIN empty_right_table t2"
  },
  {
    "id" : "eb053ef6-7298-4719-a5d3-59939db11bcb",
    "prId" : 31892,
    "prUrl" : "https://github.com/apache/spark/pull/31892#pullrequestreview-616121777",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b72b2661-4a97-4a8b-bfcc-fa4c3cf40c79",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "nit: how about checking `checkAnswer(spark.table(\"output_table\"), Row(...) ...)` just in case?",
        "createdAt" : "2021-03-19T05:54:50Z",
        "updatedAt" : "2021-03-19T05:54:51Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "2992e0fe-ef99-45c5-9194-7d3841fdf13f",
        "parentId" : "b72b2661-4a97-4a8b-bfcc-fa4c3cf40c79",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "@maropu - here we are inner-joining an empty table `empty_right_table`, so the output table still has nothing after query. Shall we check answer as well? I am fine either way.",
        "createdAt" : "2021-03-19T06:26:51Z",
        "updatedAt" : "2021-03-19T06:26:51Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      },
      {
        "id" : "33d3fa86-96ac-486e-9644-717fad16f368",
        "parentId" : "b72b2661-4a97-4a8b-bfcc-fa4c3cf40c79",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "okay, looks fine as it is.",
        "createdAt" : "2021-03-19T07:29:29Z",
        "updatedAt" : "2021-03-19T07:29:30Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "d57f914b79f8c6372f1ecf3116a5956f0bbce54b",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +4113,4117 @@            |ON t1.k = t2.k\n            |LIMIT 3\n          \"\"\".stripMargin)\n      }\n    }"
  },
  {
    "id" : "0ec7193c-c25a-45dd-b693-7478c0f9d494",
    "prId" : 31820,
    "prUrl" : "https://github.com/apache/spark/pull/31820#pullrequestreview-611667646",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1d2899cc-db95-4a77-bd62-93ec8b92e52c",
        "parentId" : null,
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "do we really need `df.collect()` here? shouldn't `AdaptiveSparkPlanHelper.collect()` below take care of going through query plan properly with AQE being enabled?",
        "createdAt" : "2021-03-14T05:01:48Z",
        "updatedAt" : "2021-03-16T08:20:48Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      },
      {
        "id" : "2bdd6bcb-feb0-405d-b4f3-a6ce181185dc",
        "parentId" : "1d2899cc-db95-4a77-bd62-93ec8b92e52c",
        "authorId" : "3d4870da-39a8-4406-b00d-131930d14cd8",
        "body" : "Yes, we do need to run the query first and then check the plan as this is an AQE compatible query where `ReusedExchangeExec` nodes are inserted during execution.",
        "createdAt" : "2021-03-14T07:39:44Z",
        "updatedAt" : "2021-03-16T08:20:48Z",
        "lastEditedBy" : "3d4870da-39a8-4406-b00d-131930d14cd8",
        "tags" : [
        ]
      }
    ],
    "commit" : "483686d98e887c278974c0ef89bf32cbb22b580d",
    "line" : 26,
    "diffHunk" : "@@ -1,1 +4081,4085 @@              |JOIN t AS t3 ON t3.id = t2.id\n              |\"\"\".stripMargin)\n          df.collect()\n          val reusedExchanges = collect(df.queryExecution.executedPlan) {\n            case r: ReusedExchangeExec => r"
  },
  {
    "id" : "521feca8-ba4e-4316-95dd-958ec6397400",
    "prId" : 31550,
    "prUrl" : "https://github.com/apache/spark/pull/31550#pullrequestreview-593166493",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b7f9fcce-e74d-4493-96a6-a1f448c3a68b",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Can we move the test to `SQLViewTestSuite`, so that it's tested against temp view, global temp view, and permanent view automatically?",
        "createdAt" : "2021-02-18T12:22:59Z",
        "updatedAt" : "2021-02-18T12:23:00Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "83b6dc39-b9e1-4ce4-8c51-1dc9c7ad0bb8",
        "parentId" : "b7f9fcce-e74d-4493-96a6-a1f448c3a68b",
        "authorId" : "29c89154-22f7-40ee-b295-115a55734d6c",
        "body" : "because temp view and permanent view have different behaviors, it's not easy to add this to `SQLViewTestSuite`",
        "createdAt" : "2021-02-18T12:27:30Z",
        "updatedAt" : "2021-02-18T12:27:30Z",
        "lastEditedBy" : "29c89154-22f7-40ee-b295-115a55734d6c",
        "tags" : [
        ]
      },
      {
        "id" : "ba861048-a237-45b6-adce-c28ca31bde9d",
        "parentId" : "b7f9fcce-e74d-4493-96a6-a1f448c3a68b",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "ah I see, makes sense.",
        "createdAt" : "2021-02-18T12:36:23Z",
        "updatedAt" : "2021-02-18T12:36:23Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "ce34b5ee88ac066270a9c6ac6c50cd546ba38929",
    "line" : 43,
    "diffHunk" : "@@ -1,1 +3983,3987 @@  }\n\n  test(\"SPARK-34421: Resolve temporary objects in permanent views with CTEs\") {\n    val tempFuncName = \"temp_func\"\n    withUserDefinedFunction((tempFuncName, true)) {"
  },
  {
    "id" : "a0f15b1b-23e1-4365-b578-470c70be91ae",
    "prId" : 31357,
    "prUrl" : "https://github.com/apache/spark/pull/31357#pullrequestreview-577095215",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d20f8168-7156-4fc8-8782-e0d03c130488",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Before this fix, this will throw a weird error from the parquet library.",
        "createdAt" : "2021-01-27T08:39:36Z",
        "updatedAt" : "2021-01-27T08:39:36Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "0fee33bf7fef207bfef061747a0d89fd93522087",
    "line" : 60,
    "diffHunk" : "@@ -1,1 +3905,3909 @@      withSQLConf(SQLConf.PARQUET_VECTORIZED_READER_ENABLED.key -> \"false\") {\n        checkAnswer(readParquet(\"a DECIMAL(3, 2)\", path), sql(\"SELECT 1.00\"))\n        checkAnswer(readParquet(\"b DECIMAL(3, 2)\", path), Row(null))\n        checkAnswer(readParquet(\"b DECIMAL(11, 1)\", path), sql(\"SELECT 123456.0\"))\n        checkAnswer(readParquet(\"c DECIMAL(11, 1)\", path), Row(null))"
  },
  {
    "id" : "0390c740-b7a7-48dd-bddf-41d642b6ae3f",
    "prId" : 31319,
    "prUrl" : "https://github.com/apache/spark/pull/31319#pullrequestreview-577610183",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5d95d375-4c47-44d1-aff8-513367fd7b5e",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I'm not very sure about the last one. When the data is binary in parquet files, we don't know what the binary means and is hard to read it out as decimal. For example, what happens if the data is `CAST('abc' AS BINARY)`?",
        "createdAt" : "2021-01-27T04:20:34Z",
        "updatedAt" : "2021-01-27T08:36:55Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "c1acb12e-2d92-4131-8157-cc7f7d0cd3b1",
        "parentId" : "5d95d375-4c47-44d1-aff8-513367fd7b5e",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "It's probably better to fail if we try to read binary as decimal, as we don't know the precision/scale of it (while we do know that for int/long).",
        "createdAt" : "2021-01-27T04:23:33Z",
        "updatedAt" : "2021-01-27T04:23:33Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "56c7c80c-f654-4f5d-8411-7dfbc971f5c4",
        "parentId" : "5d95d375-4c47-44d1-aff8-513367fd7b5e",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I've opened a PR to refine it.",
        "createdAt" : "2021-01-27T08:36:29Z",
        "updatedAt" : "2021-01-27T08:36:29Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "cfcd0187-14da-49f9-9cf8-e79b839fbbda",
        "parentId" : "5d95d375-4c47-44d1-aff8-513367fd7b5e",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Thanks! The follow-up looks correct to me and is merged.",
        "createdAt" : "2021-01-27T17:56:41Z",
        "updatedAt" : "2021-01-27T17:56:42Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "7932069e5c70fe6ccb99aa7b8a8725a48b402c7e",
    "line" : 46,
    "diffHunk" : "@@ -1,1 +3904,3908 @@\n    withTempPath { path =>\n      val df2 = sql(s\"SELECT 1 a, ${Int.MaxValue + 1L} b, CAST(2 AS BINARY) c\")\n      df2.write.parquet(path.toString)\n"
  },
  {
    "id" : "8fe7bd67-d188-4e9d-97f5-f4029658c344",
    "prId" : 30824,
    "prUrl" : "https://github.com/apache/spark/pull/30824#pullrequestreview-555169851",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d8e9b69d-b5a7-42e1-a67e-0cb2904ec5d8",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Thanks!",
        "createdAt" : "2020-12-18T04:47:15Z",
        "updatedAt" : "2020-12-18T06:14:59Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "5d55b388eaba22551bc8611772f91542e2f2c8ff",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +3760,3764 @@      }\n\n      withSQLConf(SQLConf.ORC_VECTORIZED_READER_ENABLED.key -> value) {\n        withTable(\"t2\") {\n          sql("
  },
  {
    "id" : "6b53bc55-9d2e-422f-aa32-fea8d0dc7d39",
    "prId" : 30246,
    "prUrl" : "https://github.com/apache/spark/pull/30246#pullrequestreview-523532620",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fa67d5fe-6ffe-4251-b179-b60ed251cd46",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "how about `SELECT map('k1', 'v1', 'k2', 'v2')[k] FROM t GROUP BY map('k2', 'v2', 'k1', 'v1')[k]`?",
        "createdAt" : "2020-11-04T14:31:40Z",
        "updatedAt" : "2020-11-04T14:31:41Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "7107f43c-6163-411c-b3d3-28c0fb242fb8",
        "parentId" : "fa67d5fe-6ffe-4251-b179-b60ed251cd46",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Since we don't normalize the literal maps, they are not the same maps, @cloud-fan . We should not handle it here, @cloud-fan . ",
        "createdAt" : "2020-11-04T16:31:49Z",
        "updatedAt" : "2020-11-04T16:31:49Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "b7367d71d02d1938ea0bf74c42db50b10fda1fab",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +3713,3717 @@      Seq(\n        \"SELECT map('k1', 'v1')[k] FROM t GROUP BY 1\",\n        \"SELECT map('k1', 'v1')[k] FROM t GROUP BY map('k1', 'v1')[k]\",\n        \"SELECT map('k1', 'v1')[k] a FROM t GROUP BY a\").foreach { statement =>\n        checkAnswer(sql(statement), Row(\"v1\"))"
  },
  {
    "id" : "f896fdd6-a325-43ca-bf7b-9488d52f8d52",
    "prId" : 30199,
    "prUrl" : "https://github.com/apache/spark/pull/30199#pullrequestreview-520654379",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5de09cd5-f32d-4f26-8396-34320fd1fdc5",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "If you refactor the parsing logic for `GROUP BY`, I think the parser cannot accept this case. Right? If so, I think we don't need to update the Analyzer code.",
        "createdAt" : "2020-10-30T11:49:04Z",
        "updatedAt" : "2020-10-30T11:49:05Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "87e032b5-db5a-4059-9a1c-94c5d715046e",
        "parentId" : "5de09cd5-f32d-4f26-8396-34320fd1fdc5",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> If you refactor the parsing logic for `GROUP BY`, I think the parser cannot accept this case. Right? If so, I think we don't need to update the Analyzer code.\r\n\r\nIdeally, we can throw such an exception at the Parser layer. ",
        "createdAt" : "2020-10-30T13:15:23Z",
        "updatedAt" : "2020-10-30T13:15:23Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "c3f030dec1f4f51e5c9ec57ef2646445469ad221",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +3709,3713 @@      )\n      checkAnalysisException(\n        \"SELECT a, b, count(c) FROM t GROUP BY rollup(a, b) WITH CUBE\",\n        \"Use grouping analytics function like `cube|rollup` with \" +\n          \"`WITH CUBE` and `WITH ROLLUP` is not supported.\""
  },
  {
    "id" : "bfbc082f-154d-4655-8fc1-2fb8bf6979ca",
    "prId" : 29637,
    "prUrl" : "https://github.com/apache/spark/pull/29637#pullrequestreview-481771127",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "faaaee4c-7cc7-466b-a499-7e7e7504dbc9",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "a bit surprised that this query failed... nice catch.",
        "createdAt" : "2020-09-03T12:06:04Z",
        "updatedAt" : "2020-09-03T12:16:53Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "3e4a6a5d9a8f4151735b67c9dc585201e8543ab5",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +3689,3693 @@    withTable(\"t\") {\n      spark.range(1).write.saveAsTable(\"t\")\n      checkAnswer(sql(\"SELECT id FROM t WHERE (SELECT true)\"), Row(0L))\n    }\n  }"
  },
  {
    "id" : "f874194f-9ced-4eb5-ae43-e8200440c258",
    "prId" : 29208,
    "prUrl" : "https://github.com/apache/spark/pull/29208#pullrequestreview-454598999",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0f5fd64a-5cba-4c0f-96a6-3595443ea508",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Original PR doesn't need to enable this?",
        "createdAt" : "2020-07-23T20:59:53Z",
        "updatedAt" : "2020-07-23T20:59:53Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "80253637-3848-4691-b657-a75c5e8febf8",
        "parentId" : "0f5fd64a-5cba-4c0f-96a6-3595443ea508",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Yeah, the default value has been changed since Spark 3.0: https://github.com/apache/spark/pull/25520.",
        "createdAt" : "2020-07-24T01:57:48Z",
        "updatedAt" : "2020-07-24T01:57:48Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "ca9755a1c3682e1b4dcd3778fcdc10872bea3309",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +3094,3098 @@\n  test(\"SPARK-32280: Avoid duplicate rewrite attributes when there're multiple JOINs\") {\n    withSQLConf(SQLConf.CROSS_JOINS_ENABLED.key -> \"true\") {\n      sql(\"SELECT 1 AS id\").createOrReplaceTempView(\"A\")\n      sql(\"SELECT id, 'foo' AS kind FROM A\").createOrReplaceTempView(\"B\")"
  },
  {
    "id" : "6ed478cb-728b-431b-896d-d7c8af6daa90",
    "prId" : 29062,
    "prUrl" : "https://github.com/apache/spark/pull/29062#pullrequestreview-461392885",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b6d549db-f785-42a0-9478-71d2478fa628",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "as long as we have a hint in CTE, it will fail in 3.0? can you add more test cases?",
        "createdAt" : "2020-07-30T20:18:50Z",
        "updatedAt" : "2020-07-30T20:18:50Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "af680465-dbe5-48d8-8131-3530f67e9080",
        "parentId" : "b6d549db-f785-42a0-9478-71d2478fa628",
        "authorId" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "body" : "Yes. The same issue reported by multiple times in Jira with different use cases, like SPARK-32237, SPARK-32347, SPARK-32535, I will file a new PR to add more unit tests.",
        "createdAt" : "2020-08-05T07:04:41Z",
        "updatedAt" : "2020-08-05T07:07:06Z",
        "lastEditedBy" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "tags" : [
        ]
      }
    ],
    "commit" : "e361401596693557877d750d195c7555fa99cd7c",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +3567,3571 @@      checkAnswer(\n        sql(s\"\"\"\n          |WITH cte AS (SELECT /*+ REPARTITION(3) */ * FROM t)\n          |SELECT * FROM cte\n        \"\"\".stripMargin),"
  },
  {
    "id" : "9a4ad250-02e6-406e-884b-26349469fd39",
    "prId" : 29062,
    "prUrl" : "https://github.com/apache/spark/pull/29062#pullrequestreview-461393112",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ae35519f-a5ff-4636-ae65-e1de919eafbd",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "Like JoinHintSuite, we should have our own hint-specific suite. It can help us understand the test coverage of SQL HINT.",
        "createdAt" : "2020-07-30T20:20:12Z",
        "updatedAt" : "2020-07-30T20:20:12Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "657f4a8a-97f8-4129-94a6-33ebc0a44446",
        "parentId" : "ae35519f-a5ff-4636-ae65-e1de919eafbd",
        "authorId" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "body" : "Sure. I will add a new suite.",
        "createdAt" : "2020-08-05T07:05:04Z",
        "updatedAt" : "2020-08-05T07:05:04Z",
        "lastEditedBy" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "tags" : [
        ]
      }
    ],
    "commit" : "e361401596693557877d750d195c7555fa99cd7c",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +3562,3566 @@  }\n\n  test(\"SPARK-32237: Hint in CTE\") {\n    withTable(\"t\") {\n      sql(\"CREATE TABLE t USING PARQUET AS SELECT 1 AS id\")"
  },
  {
    "id" : "078ba31c-5a33-489b-8fae-840734f9c343",
    "prId" : 28860,
    "prUrl" : "https://github.com/apache/spark/pull/28860#pullrequestreview-438199923",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4a3fd1c6-2319-4e3a-a8e1-bfa9cbcd1bfc",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Can we add deeper cases? Also, you can simplify the test cases, for example, as below:\r\n\r\n```scala\r\nval df = spark.range(10).select(array(struct(array(struct(\"id\")).alias(\"col1\"))).alias(\"col0\"))\r\ndf.selectExpr(\"col0.col1.id\")\r\n```",
        "createdAt" : "2020-06-26T10:52:37Z",
        "updatedAt" : "2020-06-26T10:59:50Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "b6e92c025a4f3746165a3ba660fe80cca63bb91f",
    "line" : 25,
    "diffHunk" : "@@ -1,1 +3536,3540 @@      checkAnswer(sql(\n        \"\"\"\n          |SELECT a.b.c FROM nest\n        \"\"\".stripMargin),\n        Row(ArrayBuffer(ArrayBuffer(ArrayBuffer(1, 2)))) ::"
  },
  {
    "id" : "ee22b534-701b-47bf-aeb1-cc663d299afd",
    "prId" : 28600,
    "prUrl" : "https://github.com/apache/spark/pull/28600#pullrequestreview-416698215",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3f07b054-9c07-446c-aca8-6579c92e8f47",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "please keep the blank like above it.",
        "createdAt" : "2020-05-22T07:19:09Z",
        "updatedAt" : "2020-05-23T06:29:22Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "3a1a061bb955f1186675f5dda7525e6250614b17",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +3506,3510 @@}\n\ncase class Foo(bar: Option[String])"
  },
  {
    "id" : "c3c0cef7-7781-4461-a5e0-abe06a27506b",
    "prId" : 28490,
    "prUrl" : "https://github.com/apache/spark/pull/28490#pullrequestreview-478377961",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "05a0037f-a37a-459e-8744-1d7026e6f35f",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "does normal GROUP BY have this bug?",
        "createdAt" : "2020-08-31T06:55:33Z",
        "updatedAt" : "2020-09-02T04:42:25Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "7ea49368-4179-494a-bfa7-eb6fcad4b1eb",
        "parentId" : "05a0037f-a37a-459e-8744-1d7026e6f35f",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> does normal GROUP BY have this bug?\r\n\r\nNo, this happened when construct group Analytics, and changed code only fix this too.",
        "createdAt" : "2020-08-31T07:19:20Z",
        "updatedAt" : "2020-09-02T04:42:25Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "475fda87-6fcd-4e68-8072-c4107df62423",
        "parentId" : "05a0037f-a37a-459e-8744-1d7026e6f35f",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> does normal GROUP BY have this bug?\r\n\r\nAdd  a test case for normal GROUP BY",
        "createdAt" : "2020-08-31T07:24:28Z",
        "updatedAt" : "2020-09-02T04:42:25Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "e6fb91fc8124c60e27899846b675853c22537791",
    "line" : 33,
    "diffHunk" : "@@ -1,1 +3526,3530 @@            |FROM t\n            |GROUP BY a, c.json_string\n            |WITH CUBE\n            |\"\"\".stripMargin),\n        Row(\"A\", \"{\\\"i\\\": 1}\", 3) :: Row(\"A\", \"{\\\"i\\\": 2}\", 2) :: Row(\"A\", null, 5) ::"
  },
  {
    "id" : "d9f12278-6b6d-474e-9a03-9b7817cf4548",
    "prId" : 28392,
    "prUrl" : "https://github.com/apache/spark/pull/28392#pullrequestreview-402368222",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dcaa26e9-67fa-424b-b158-f15bb7e16296",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "If we add `assert` at line 3435, this test will fail.",
        "createdAt" : "2020-04-29T05:13:22Z",
        "updatedAt" : "2020-04-29T06:31:00Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "d90b2e726135e91eadf10c2972199c3f45887b5c",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +3438,3442 @@    val df1 = sql(\"SELECT rand()\")\n    assert(df1.schema.head.name === \"rand()\")\n    checkIfSeedExistsInExplain(df1)\n    val df2 = sql(\"SELECT rand(1L)\")\n    assert(df2.schema.head.name === \"rand(1)\")"
  },
  {
    "id" : "3bda59a1-c4ae-41ef-9fba-23f1d6bba0be",
    "prId" : 28178,
    "prUrl" : "https://github.com/apache/spark/pull/28178#pullrequestreview-391324873",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c4f3199f-06bb-4901-a1e7-4787aa346ef6",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Why don't we use `clonedSpark.conf.set(...)`?",
        "createdAt" : "2020-04-10T08:09:48Z",
        "updatedAt" : "2020-04-13T08:39:14Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "ab0696b8-8564-4a5c-8d90-7246545f113a",
        "parentId" : "c4f3199f-06bb-4901-a1e7-4787aa346ef6",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "that API can only use String, not `ConfigEntry`, so less type safe.",
        "createdAt" : "2020-04-10T08:11:59Z",
        "updatedAt" : "2020-04-13T08:39:14Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "cdf49973-31fb-4ed6-be5b-8d73e24715dc",
        "parentId" : "c4f3199f-06bb-4901-a1e7-4787aa346ef6",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "~~It takes boolean too~~, oh you mean the key. You can do `spark.conf.set(SQLConf.COALESCE_PARTITIONS_ENABLED.key, false)`. But okay, it's a nit",
        "createdAt" : "2020-04-10T08:15:12Z",
        "updatedAt" : "2020-04-13T08:39:14Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "cc6bb863e0390645e7a395cf2069efcfce583823",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +183,187 @@      val clonedSpark = spark.cloneSession()\n      // Coalescing partitions can change result order, so disable it.\n      clonedSpark.sessionState.conf.setConf(SQLConf.COALESCE_PARTITIONS_ENABLED, false)\n      val info = clonedSpark.sessionState.catalog.lookupFunctionInfo(funcId)\n      val className = info.getClassName"
  },
  {
    "id" : "d5c499dc-326b-420a-8fda-b4d2f41a72a3",
    "prId" : 27926,
    "prUrl" : "https://github.com/apache/spark/pull/27926#pullrequestreview-375675263",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "19296ac7-da79-4599-a01b-262b854f62f2",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Nice catch! It seems #27542 broke some other queries merging `map<null,null>` and `map<some type, some type>`, e.g., `map_concat`;\r\n```\r\n// the current master\r\nscala> sql(\"select map_concat(map(), map(1, 2))\").show()\r\norg.apache.spark.sql.AnalysisException: cannot resolve 'map_concat(map(), map(1, 2))' due to data type mismatch: input to function map_concat should all be the same type, but it's [map<null,null>, map<int,int>]; line 1 pos 7;\r\n'Project [unresolvedalias(map_concat(map(), map(1, 2)), None)]\r\n+- OneRowRelation\r\n\r\n  at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\r\n```\r\nThis PR can fix this query, too.",
        "createdAt" : "2020-03-17T01:00:47Z",
        "updatedAt" : "2020-03-17T01:00:48Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "125d09408edcbee3a5bae591e949c53c90878ca7",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +3493,3497 @@      sql(\"(SELECT map()) UNION ALL (SELECT map(1, 2))\"),\n      Seq(Row(Map[Int, Int]()), Row(Map(1 -> 2))))\n  }\n}\n"
  },
  {
    "id" : "1e9bfa36-2140-40c6-8702-0a349b985b4f",
    "prId" : 27702,
    "prUrl" : "https://github.com/apache/spark/pull/27702#pullrequestreview-365464802",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ea90fb50-5bbd-4799-90c2-bac714dd052c",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "The below test coverage looks better. Last time, it checked only the exception occurs or not. BTW, do we need to store as table to avoid some local table optimization?",
        "createdAt" : "2020-02-27T04:14:35Z",
        "updatedAt" : "2020-02-27T20:05:27Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "1bd8c960-75d3-47af-8953-28ab5d1557c6",
        "parentId" : "ea90fb50-5bbd-4799-90c2-bac714dd052c",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Yea, if not store as a table, the query becomes a simple LocalRelation. ",
        "createdAt" : "2020-02-27T06:21:36Z",
        "updatedAt" : "2020-02-27T20:05:27Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "8d96c7f9-a617-4732-9890-9f3b53177c93",
        "parentId" : "ea90fb50-5bbd-4799-90c2-bac714dd052c",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Sorry if I am being dumb here but I can't fully follow why LocalRelation matters here. How does the current fix relate to the test fix here?\r\n\r\nIf we concern about `ConvertToLocalRelation`, it seems not effective here:\r\n\r\n```\r\n\r\n== Parsed Logical Plan ==\r\n'Project ['explodedvalue.field]\r\n+- 'Generate 'explode('value), false, as, ['explodedvalue]\r\n   +- 'SubqueryAlias AS\r\n      +- 'UnresolvedInlineTable [value], [List('array('named_struct(field, 'named_struct(a, 1, b, 2))))]\r\n\r\n== Analyzed Logical Plan ==\r\nfield: struct<a:int,b:int>\r\nProject [explodedvalue#219.field AS field#220]\r\n+- Generate explode(value#218), false, as, [explodedvalue#219]\r\n   +- SubqueryAlias AS\r\n      +- LocalRelation [value#218]\r\n\r\n== Optimized Logical Plan ==\r\nProject [explodedvalue#219.field AS field#220]\r\n+- Generate explode(value#218), [0], false, as, [explodedvalue#219]\r\n   +- LocalRelation [value#218]\r\n\r\n== Physical Plan ==\r\n*(1) Project [explodedvalue#219.field AS field#220]\r\n+- Generate explode(value#218), false, [explodedvalue#219]\r\n   +- LocalTableScan [value#218]\r\n```",
        "createdAt" : "2020-02-27T07:10:43Z",
        "updatedAt" : "2020-02-27T20:05:27Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "ca890989-6385-41e9-95ab-af0b8b607cf3",
        "parentId" : "ea90fb50-5bbd-4799-90c2-bac714dd052c",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "I changed the test added by SPARK-30870, because the root cause of the failure is not due to selecting whole structure. Here I change this test to more fit the original purpose of SPARK-30870: when selecting whole structure, column pruning shouldn't alias the nested column.",
        "createdAt" : "2020-02-27T07:15:57Z",
        "updatedAt" : "2020-02-27T20:05:27Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "4ce52ab8-e08a-43dd-99fb-bf1a4fc5bf7f",
        "parentId" : "ea90fb50-5bbd-4799-90c2-bac714dd052c",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "The test added by SPARK-30870 is moved to `SchemaPruningSuite`.",
        "createdAt" : "2020-02-27T07:16:53Z",
        "updatedAt" : "2020-02-27T20:05:27Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "363ae173-25f1-47fc-8113-fa88f1edecaf",
        "parentId" : "ea90fb50-5bbd-4799-90c2-bac714dd052c",
        "authorId" : "3d4870da-39a8-4406-b00d-131930d14cd8",
        "body" : "Both PRs fix the concrete SQL in SPARK-30870 independently because \r\n- the whole structure was selected (my pr fixed it) and \r\n- a nested column was selected from the output of a generate (this PR fixes it).\r\n\r\nBut this PR is better for cases that contain generate as it can handle nested field of a nested field https://github.com/apache/spark/pull/27702/files#diff-957112380b0a2ef014abc8227d0b70acR313-R318",
        "createdAt" : "2020-02-27T07:38:48Z",
        "updatedAt" : "2020-02-27T20:05:27Z",
        "lastEditedBy" : "3d4870da-39a8-4406-b00d-131930d14cd8",
        "tags" : [
        ]
      }
    ],
    "commit" : "5862428c552c82d858e9edc29d59fd4d9419460f",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +3403,3407 @@          |FROM VALUES array(named_struct('field', named_struct('a', 1, 'b', 2))) AS (value)\n        \"\"\".stripMargin)\n      df.write.format(\"parquet\").saveAsTable(\"t\")\n\n      val df2 = spark.table(\"t\")"
  },
  {
    "id" : "d6190894-ff6c-4135-8dbc-ff3ec35cec98",
    "prId" : 27632,
    "prUrl" : "https://github.com/apache/spark/pull/27632#pullrequestreview-364761119",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b4733b00-8661-4691-97f2-ef56537bed70",
        "parentId" : null,
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "It will throw `TreeNodeException: Once strategy's idempotence is broken for batch Infer Filters` before this PR:\r\n```\r\n[info] - SPARK-30872: Constraints inferred from inferred attributes *** FAILED *** (146 milliseconds)\r\n[info]   org.apache.spark.sql.catalyst.errors.package$TreeNodeException: Once strategy's idempotence is broken for batch Infer Filters\r\n[info]  Aggregate [count(1) AS count(1)#19182L]                                                                                                                                                                                                                        Aggregate [count(1) AS count(1)#19182L]\r\n[info]  +- Project                                                                                                                                                                                                                                                     +- Project\r\n[info] !   +- Filter ((((((a#19179L = c#19181L) AND isnotnull(b#19180L)) AND isnotnull(c#19181L)) AND ((b#19180L = 3) OR (b#19180L = 13))) AND isnotnull(a#19179L)) AND (((a#19179L = b#19180L) AND (b#19180L = c#19181L)) AND ((c#19181L = 3) OR (c#19181L = 13))))      +- Filter (((a#19179L = 3) OR (a#19179L = 13)) AND ((((((a#19179L = c#19181L) AND isnotnull(b#19180L)) AND isnotnull(c#19181L)) AND ((b#19180L = 3) OR (b#19180L = 13))) AND isnotnull(a#19179L)) AND (((a#19179L = b#19180L) AND (b#19180L = c#19181L)) AND ((c#19181L = 3) OR (c#19181L = 13)))))\r\n[info]        +- Relation[a#19179L,b#19180L,c#19181L] parquet                                                                                                                                                                                                                +- Relation[a#19179L,b#19180L,c#19181L] parquet\r\n[info]           , tree:\r\n[info] Aggregate [count(1) AS count(1)#19182L]\r\n[info] +- Project\r\n[info]    +- Filter (((a#19179L = 3) OR (a#19179L = 13)) AND ((((((a#19179L = c#19181L) AND isnotnull(b#19180L)) AND isnotnull(c#19181L)) AND ((b#19180L = 3) OR (b#19180L = 13))) AND isnotnull(a#19179L)) AND (((a#19179L = b#19180L) AND (b#19180L = c#19181L)) AND ((c#19181L = 3) OR (c#19181L = 13)))))\r\n[info]       +- Relation[a#19179L,b#19180L,c#19181L] parquet\r\n[info]   at org.apache.spark.sql.catalyst.rules.RuleExecutor.checkBatchIdempotence(RuleExecutor.scala:100)\r\n[info]   at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:187)\r\n[info]   at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:132)\r\n[info]   at scala.collection.immutable.List.foreach(List.scala:392)\r\n[info]   at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:132)\r\n[info]   at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:111)\r\n[info]   at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)\r\n[info]   at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:111)\r\n[info]   at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:82)\r\n[info]   at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\r\n[info]   at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:119)\r\n[info]   at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:762)\r\n[info]   at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:119)\r\n[info]   at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:82)\r\n[info]   at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:79)\r\n[info]   at org.apache.spark.sql.QueryTest.assertEmptyMissingInput(QueryTest.scala:231)\r\n[info]   at org.apache.spark.sql.QueryTest.checkAnswer(QueryTest.scala:154)\r\n[info]   at org.apache.spark.sql.SQLQuerySuite.$anonfun$new$746(SQLQuerySuite.scala:3413)\r\n[info]   at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\r\n```",
        "createdAt" : "2020-02-26T09:56:31Z",
        "updatedAt" : "2020-02-26T09:57:07Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      }
    ],
    "commit" : "bf7b8e561029791daa17ffb90aac9e970f002cf9",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +3406,3410 @@  }\n\n  test(\"SPARK-30872: Constraints inferred from inferred attributes\") {\n    withTable(\"t1\") {\n      spark.range(20).selectExpr(\"id as a\", \"id as b\", \"id as c\").write.saveAsTable(\"t1\")"
  },
  {
    "id" : "401469fa-79f6-4d23-9c23-20d1a46925a4",
    "prId" : 27580,
    "prUrl" : "https://github.com/apache/spark/pull/27580#pullrequestreview-360854923",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3184a54b-bf32-431b-8a7a-d45e926466a1",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "`hashExpression` seems not used.",
        "createdAt" : "2020-02-17T02:12:40Z",
        "updatedAt" : "2020-02-26T05:24:32Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "0bab75e4-77ba-4822-a366-45e1011d42ee",
        "parentId" : "3184a54b-bf32-431b-8a7a-d45e926466a1",
        "authorId" : "832e1988-205f-40f9-89d5-c37ea16224c9",
        "body" : "I modified the code.",
        "createdAt" : "2020-02-19T05:45:44Z",
        "updatedAt" : "2020-02-26T05:24:32Z",
        "lastEditedBy" : "832e1988-205f-40f9-89d5-c37ea16224c9",
        "tags" : [
        ]
      }
    ],
    "commit" : "8dfee675f6bfaf04068b856fc15158cea307f188",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +2132,2136 @@  test(\"SPARK-27619: when spark.sql.legacy.useHashOnMapType is true, hash can be used on Maptype\") {\n    Seq(\"hash\", \"xxhash64\").foreach {\n      case hashExpression =>\n        withSQLConf(SQLConf.LEGACY_USE_HASH_ON_MAPTYPE.key -> \"true\") {\n          val df = spark.createDataset(Map() :: Nil)"
  },
  {
    "id" : "8507eeac-ac5e-48d7-a6e6-2c9954e9d97b",
    "prId" : 27324,
    "prUrl" : "https://github.com/apache/spark/pull/27324#pullrequestreview-347105787",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "87bf3919-200d-4f95-8c07-a99a968b7786",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Is there any other place like this?",
        "createdAt" : "2020-01-22T21:38:13Z",
        "updatedAt" : "2020-01-23T08:15:15Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "265419ec-5e3f-40be-a20b-5500bca6e315",
        "parentId" : "87bf3919-200d-4f95-8c07-a99a968b7786",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Yea, I think you can just `git grep -r \"assert\" . | grep \"\\.sorted\"` and see if they are all necessary, e.g., remove them and run the tests, see if they pass. Looks not too many:\r\n\r\n```bash\r\n$ git grep -r \"assert\" . | grep \"\\.sorted\" | wc -l\r\n      74\r\n```",
        "createdAt" : "2020-01-23T01:39:27Z",
        "updatedAt" : "2020-01-23T08:15:15Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "8d04a8ff-6fb9-4fcf-99c6-1c420f51de6e",
        "parentId" : "87bf3919-200d-4f95-8c07-a99a968b7786",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Even sorting is not necessary in other places, I don't think it makes sense to change those places here in the PR which is just a follow up https://github.com/apache/spark/pull/26813#discussion_r369743505\r\n\r\nHow about to open new JIRA for reviewing other places?",
        "createdAt" : "2020-01-23T05:34:56Z",
        "updatedAt" : "2020-01-23T08:15:15Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "9fd34760-3f52-4f65-b880-fd9ccbe12f98",
        "parentId" : "87bf3919-200d-4f95-8c07-a99a968b7786",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Ya. What I meant was the other places in the original AQE PR only.",
        "createdAt" : "2020-01-23T07:02:27Z",
        "updatedAt" : "2020-01-23T08:15:15Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "8fbeec76-09e0-4289-b053-7bb2f6522acc",
        "parentId" : "87bf3919-200d-4f95-8c07-a99a968b7786",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "I see, let me check.",
        "createdAt" : "2020-01-23T07:10:33Z",
        "updatedAt" : "2020-01-23T08:15:15Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "82e669e2-52de-4137-bb92-8ade2e747802",
        "parentId" : "87bf3919-200d-4f95-8c07-a99a968b7786",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "This one sorts a text too https://github.com/apache/spark/blob/883ae331c36aa2279b163b87571997c8ffebfb6c/sql/core/src/test/scala/org/apache/spark/sql/SQLQueryTestSuite.scala#L422-L423",
        "createdAt" : "2020-01-23T07:15:39Z",
        "updatedAt" : "2020-01-23T08:15:15Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "66405d50-8ff0-47ce-9a8b-8d678c08b6b3",
        "parentId" : "87bf3919-200d-4f95-8c07-a99a968b7786",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Let's fix it too. I thought it's a list of string that for the result rows...",
        "createdAt" : "2020-01-23T07:47:26Z",
        "updatedAt" : "2020-01-23T08:15:15Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "2a2a807881bc716ddd3adea97cf6db4a4206a11b",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +194,198 @@                  hiveResultString(df.queryExecution.executedPlan).mkString(\"\\n\"))\n                val expected = unindentAndTrim(output)\n                assert(actual === expected)\n              case _ =>\n            })"
  },
  {
    "id" : "85ce89b5-5685-4b92-a201-dcb234d7d35c",
    "prId" : 26813,
    "prUrl" : "https://github.com/apache/spark/pull/26813#pullrequestreview-347066627",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ac036016-f635-4534-866f-7b9b62c7a3fa",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Why do you sort them? After the changes, output of failed tests is unusable. Instead of `\"[true]\" did not equal \"[false]\"` I am getting `\"[ertu]\" did not equal \"[aefls]\"`",
        "createdAt" : "2020-01-22T18:58:45Z",
        "updatedAt" : "2020-01-22T18:58:56Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "65867fbd-4d2c-4f50-93ff-ae81bf01f0b5",
        "parentId" : "ac036016-f635-4534-866f-7b9b62c7a3fa",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "From my understanding, output of examples must be precisely the same as in the examples. Let me try to remove the sorting.",
        "createdAt" : "2020-01-22T19:05:13Z",
        "updatedAt" : "2020-01-22T19:05:14Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "f8039530-b128-4117-8b0c-83c4694ac04e",
        "parentId" : "ac036016-f635-4534-866f-7b9b62c7a3fa",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "oh I thought `actual` and `expected` is a list of results. Sorry didn't catch it during review. @MaxGekk can you help to fix it? thanks!",
        "createdAt" : "2020-01-23T02:04:39Z",
        "updatedAt" : "2020-01-23T02:04:40Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "739dc6c8-10c6-4f14-9131-75719c50fac7",
        "parentId" : "ac036016-f635-4534-866f-7b9b62c7a3fa",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "@cloud-fan Here is the PR https://github.com/apache/spark/pull/27324 , please, review it.",
        "createdAt" : "2020-01-23T05:28:14Z",
        "updatedAt" : "2020-01-23T05:28:15Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "8b5e7442c63fe326db7c7f46f7a194fbae8f0d46",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +193,197 @@                  hiveResultString(df.queryExecution.executedPlan).mkString(\"\\n\"))\n                val expected = unindentAndTrim(output)\n                assert(actual.sorted === expected.sorted)\n              case _ =>\n            })"
  }
]