[
  {
    "id" : "9a90ee21-5828-4cba-a2f8-f4f1834b1f72",
    "prId" : 32465,
    "prUrl" : "https://github.com/apache/spark/pull/32465#pullrequestreview-654904676",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8d0194a6-c210-44e1-a520-96d83d74b083",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "If you don't mind, could you add a test case for HINT explicitly?",
        "createdAt" : "2021-05-07T22:21:30Z",
        "updatedAt" : "2021-05-08T09:40:56Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "51632625e9fb3f09800112fe121de1dcae1dacee",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +4102,4106 @@  }\n\n  test(\"SPARK-35331: Fix resolving original expression in RepartitionByExpression after aliased\") {\n    Seq(\"CLUSTER\", \"DISTRIBUTE\").foreach { keyword =>\n      Seq(\"a\", \"substr(a, 0, 3)\").foreach { expr =>"
  },
  {
    "id" : "7762e03f-4de3-43f6-bc9b-f69903ba9c7f",
    "prId" : 32239,
    "prUrl" : "https://github.com/apache/spark/pull/32239#pullrequestreview-640526140",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b7a31d04-4d59-498c-9646-a1c57ea23fea",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "nit: It's safer for branch-3.0 to have this test, too, I think.",
        "createdAt" : "2021-04-21T00:13:12Z",
        "updatedAt" : "2021-04-21T00:13:13Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "63cac4d0c13214cec2b9bed38ea848651dbe387b",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +3599,3603 @@\n      assert(spark.table(\"t\").groupBy($\"c.json_string\").count().schema.fieldNames ===\n        Seq(\"json_string\", \"count\"))\n    }\n  }"
  },
  {
    "id" : "4f3ff6b2-0d1a-4d66-a925-89595c4e9477",
    "prId" : 32167,
    "prUrl" : "https://github.com/apache/spark/pull/32167#pullrequestreview-641781071",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5e2f0674-51ee-4946-bce8-2aa562ec317c",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I think this case is covered at least at `sql/core/src/test/resources/sql-tests/inputs/outer-join.sql`",
        "createdAt" : "2021-04-15T09:59:38Z",
        "updatedAt" : "2021-04-15T09:59:38Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "2ffb95f9-6dd9-4984-b48d-6a612eb4e082",
        "parentId" : "5e2f0674-51ee-4946-bce8-2aa562ec317c",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "WDYT about the above @HyukjinKwon 's comment, @zengruios ? Is this PR different because this is `String`? If this is already covered, shall we close this PR?",
        "createdAt" : "2021-04-22T05:09:51Z",
        "updatedAt" : "2021-04-22T05:09:52Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "586b690c96fb2e3c36aa8d294be052b95ebdfd4e",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +406,410 @@      Row(null))\n    checkAnswer(\n      sql(\"SELECT COALESCE(s, n) FROM nullStrings\"),\n      Seq(Row(\"abc\"), Row(\"ABC\"), Row(\"3\")))\n  }"
  },
  {
    "id" : "9c7caeb6-4384-45c4-a229-b487097775a8",
    "prId" : 32158,
    "prUrl" : "https://github.com/apache/spark/pull/32158#pullrequestreview-635235473",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d64ffe52-f52a-4215-9a85-dff1912c4309",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "```suggestion\r\n      checkAnswer(queryCoalesce, Row(\"2\") :: Nil)\r\n```",
        "createdAt" : "2021-04-14T05:56:10Z",
        "updatedAt" : "2021-04-16T10:45:10Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "91ec3479-3f0b-4279-b03c-e3c9cfc7d340",
        "parentId" : "d64ffe52-f52a-4215-9a85-dff1912c4309",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "In the worst case (e.g., author is inactive but PR is ready .. ?), we could push an empty commit to their branch to retrigger .. of course this is an ugly way though",
        "createdAt" : "2021-04-14T05:57:06Z",
        "updatedAt" : "2021-04-16T10:45:10Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "fc61869ad7fda1fd62b48067539f3c5d84d05e00",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +65,69 @@\n      checkAnswer(queryCaseWhen, Row(\"1.0\") :: Nil)\n      checkAnswer(queryCoalesce, Row(\"2\") :: Nil)\n    }\n  }"
  },
  {
    "id" : "a22ec104-6119-43bb-a9f6-b91954ea3dcf",
    "prId" : 32144,
    "prUrl" : "https://github.com/apache/spark/pull/32144#pullrequestreview-635494049",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e7cda342-c598-4154-8e98-bc93d1e247c7",
        "parentId" : null,
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "w/o the patch\r\n```scala\r\nspark-sql> SET io.file.buffer.size;\r\nio.file.buffer.size\t<undefined>\r\n```",
        "createdAt" : "2021-04-14T11:10:02Z",
        "updatedAt" : "2021-04-14T14:43:40Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "b6d26b7d87917e7e230ca38f2817c6110b461370",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +1082,1086 @@\n    // io.file.buffer.size has a default value from `SparkHadoopUtil.newConfiguration`\n    checkAnswer(sql(\"SET io.file.buffer.size\"), Row(\"io.file.buffer.size\", \"65536\"))\n  }\n"
  },
  {
    "id" : "16693624-3b7c-4e34-a857-63ca418a2209",
    "prId" : 32144,
    "prUrl" : "https://github.com/apache/spark/pull/32144#pullrequestreview-640165194",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b1e3f1a5-c046-45ae-a7ad-856f8a8a2f5b",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "This breaks the Hadoop 2.7 tests, seems the behavior is different between hadoop 2.7 and hadoop 3: https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test/job/spark-master-test-sbt-hadoop-2.7/2364/testReport/org.apache.spark.sql/SQLQuerySuite/SPARK_35044__SET_command_shall_display_default_value_for_hadoop_conf_correctly/",
        "createdAt" : "2021-04-20T16:07:53Z",
        "updatedAt" : "2021-04-20T16:07:53Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "7a6c4df6-75c5-47ab-8fa0-e69c21faa425",
        "parentId" : "b1e3f1a5-c046-45ae-a7ad-856f8a8a2f5b",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I picked a different config which seems to work: `dfs.hosts`\r\n\r\n@yaooqinn can you create a followup PR to try it?",
        "createdAt" : "2021-04-20T16:13:48Z",
        "updatedAt" : "2021-04-20T16:13:48Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "10d7780b-99df-430a-ba38-432fac39d136",
        "parentId" : "b1e3f1a5-c046-45ae-a7ad-856f8a8a2f5b",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "OK",
        "createdAt" : "2021-04-20T16:16:12Z",
        "updatedAt" : "2021-04-20T16:16:12Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "b6d26b7d87917e7e230ca38f2817c6110b461370",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +1079,1083 @@    // these keys does not exist as default yet\n    checkAnswer(sql(s\"SET ${key}no\"), Row(key + \"no\", \"<undefined>\"))\n    checkAnswer(sql(\"SET dfs.replication\"), Row(\"dfs.replication\", \"<undefined>\"))\n\n    // io.file.buffer.size has a default value from `SparkHadoopUtil.newConfiguration`"
  },
  {
    "id" : "c08c1888-e2c2-4b98-a11b-f0eec7493e64",
    "prId" : 31892,
    "prUrl" : "https://github.com/apache/spark/pull/31892#pullrequestreview-616084768",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1129f893-0510-4967-827e-faf0f56c53a6",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "is INSERT necessary to reproduce the bug?",
        "createdAt" : "2021-03-19T05:50:43Z",
        "updatedAt" : "2021-03-19T05:50:43Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "f24e4de7-44bc-4476-b0c3-7d59872f1483",
        "parentId" : "1129f893-0510-4967-827e-faf0f56c53a6",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "@cloud-fan - yes I think so.\r\n\r\nFor query:\r\n\r\n```\r\nSELECT t1.k FROM left_table t1\r\nJOIN empty_right_table t2\r\nON t1.k = t2.k\r\nLIMIT 3\r\n```\r\n\r\nThe physical plan:\r\n\r\n\r\n```\r\nCollectLimit 3\r\n+- *(2) Project [k#228L]\r\n   +- *(2) BroadcastHashJoin [k#228L], [k#229L], Inner, BuildRight, false\r\n      :- *(2) Filter isnotnull(k#228L)\r\n      :  +- *(2) ColumnarToRow\r\n      :     +- FileScan parquet default.left_table[k#228L] Batched: true, DataFilters: [isnotnull(k#228L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/chengsu/spark/sql/core/spark-warehouse/org.apache.spark.sq..., PartitionFilters: [], PushedFilters: [IsNotNull(k)], ReadSchema: struct<k:bigint>\r\n      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [id=#148]\r\n         +- *(1) Filter isnotnull(k#229L)\r\n            +- *(1) ColumnarToRow\r\n               +- FileScan parquet default.empty_right_table[k#229L] Batched: true, DataFilters: [isnotnull(k#229L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/chengsu/spark/sql/core/spark-warehouse/org.apache.spark.sq..., PartitionFilters: [], PushedFilters: [IsNotNull(k)], ReadSchema: struct<k:bigint>\r\n```\r\n\r\nThis is not reproduced any more as there's no code-gen `BaseLimitExec` after `BroadcastHashJoin`.",
        "createdAt" : "2021-03-19T06:06:10Z",
        "updatedAt" : "2021-03-19T06:06:10Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      }
    ],
    "commit" : "d57f914b79f8c6372f1ecf3116a5956f0bbce54b",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +4108,4112 @@        spark.sql(\n          \"\"\"\n            |INSERT INTO TABLE output_table\n            |SELECT t1.k FROM left_table t1\n            |JOIN empty_right_table t2"
  },
  {
    "id" : "eb053ef6-7298-4719-a5d3-59939db11bcb",
    "prId" : 31892,
    "prUrl" : "https://github.com/apache/spark/pull/31892#pullrequestreview-616121777",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b72b2661-4a97-4a8b-bfcc-fa4c3cf40c79",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "nit: how about checking `checkAnswer(spark.table(\"output_table\"), Row(...) ...)` just in case?",
        "createdAt" : "2021-03-19T05:54:50Z",
        "updatedAt" : "2021-03-19T05:54:51Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "2992e0fe-ef99-45c5-9194-7d3841fdf13f",
        "parentId" : "b72b2661-4a97-4a8b-bfcc-fa4c3cf40c79",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "@maropu - here we are inner-joining an empty table `empty_right_table`, so the output table still has nothing after query. Shall we check answer as well? I am fine either way.",
        "createdAt" : "2021-03-19T06:26:51Z",
        "updatedAt" : "2021-03-19T06:26:51Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      },
      {
        "id" : "33d3fa86-96ac-486e-9644-717fad16f368",
        "parentId" : "b72b2661-4a97-4a8b-bfcc-fa4c3cf40c79",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "okay, looks fine as it is.",
        "createdAt" : "2021-03-19T07:29:29Z",
        "updatedAt" : "2021-03-19T07:29:30Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "d57f914b79f8c6372f1ecf3116a5956f0bbce54b",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +4113,4117 @@            |ON t1.k = t2.k\n            |LIMIT 3\n          \"\"\".stripMargin)\n      }\n    }"
  },
  {
    "id" : "0ec7193c-c25a-45dd-b693-7478c0f9d494",
    "prId" : 31820,
    "prUrl" : "https://github.com/apache/spark/pull/31820#pullrequestreview-611667646",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1d2899cc-db95-4a77-bd62-93ec8b92e52c",
        "parentId" : null,
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "do we really need `df.collect()` here? shouldn't `AdaptiveSparkPlanHelper.collect()` below take care of going through query plan properly with AQE being enabled?",
        "createdAt" : "2021-03-14T05:01:48Z",
        "updatedAt" : "2021-03-16T08:20:48Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      },
      {
        "id" : "2bdd6bcb-feb0-405d-b4f3-a6ce181185dc",
        "parentId" : "1d2899cc-db95-4a77-bd62-93ec8b92e52c",
        "authorId" : "3d4870da-39a8-4406-b00d-131930d14cd8",
        "body" : "Yes, we do need to run the query first and then check the plan as this is an AQE compatible query where `ReusedExchangeExec` nodes are inserted during execution.",
        "createdAt" : "2021-03-14T07:39:44Z",
        "updatedAt" : "2021-03-16T08:20:48Z",
        "lastEditedBy" : "3d4870da-39a8-4406-b00d-131930d14cd8",
        "tags" : [
        ]
      }
    ],
    "commit" : "483686d98e887c278974c0ef89bf32cbb22b580d",
    "line" : 26,
    "diffHunk" : "@@ -1,1 +4081,4085 @@              |JOIN t AS t3 ON t3.id = t2.id\n              |\"\"\".stripMargin)\n          df.collect()\n          val reusedExchanges = collect(df.queryExecution.executedPlan) {\n            case r: ReusedExchangeExec => r"
  },
  {
    "id" : "521feca8-ba4e-4316-95dd-958ec6397400",
    "prId" : 31550,
    "prUrl" : "https://github.com/apache/spark/pull/31550#pullrequestreview-593166493",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b7f9fcce-e74d-4493-96a6-a1f448c3a68b",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Can we move the test to `SQLViewTestSuite`, so that it's tested against temp view, global temp view, and permanent view automatically?",
        "createdAt" : "2021-02-18T12:22:59Z",
        "updatedAt" : "2021-02-18T12:23:00Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "83b6dc39-b9e1-4ce4-8c51-1dc9c7ad0bb8",
        "parentId" : "b7f9fcce-e74d-4493-96a6-a1f448c3a68b",
        "authorId" : "29c89154-22f7-40ee-b295-115a55734d6c",
        "body" : "because temp view and permanent view have different behaviors, it's not easy to add this to `SQLViewTestSuite`",
        "createdAt" : "2021-02-18T12:27:30Z",
        "updatedAt" : "2021-02-18T12:27:30Z",
        "lastEditedBy" : "29c89154-22f7-40ee-b295-115a55734d6c",
        "tags" : [
        ]
      },
      {
        "id" : "ba861048-a237-45b6-adce-c28ca31bde9d",
        "parentId" : "b7f9fcce-e74d-4493-96a6-a1f448c3a68b",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "ah I see, makes sense.",
        "createdAt" : "2021-02-18T12:36:23Z",
        "updatedAt" : "2021-02-18T12:36:23Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "ce34b5ee88ac066270a9c6ac6c50cd546ba38929",
    "line" : 43,
    "diffHunk" : "@@ -1,1 +3983,3987 @@  }\n\n  test(\"SPARK-34421: Resolve temporary objects in permanent views with CTEs\") {\n    val tempFuncName = \"temp_func\"\n    withUserDefinedFunction((tempFuncName, true)) {"
  },
  {
    "id" : "a0f15b1b-23e1-4365-b578-470c70be91ae",
    "prId" : 31357,
    "prUrl" : "https://github.com/apache/spark/pull/31357#pullrequestreview-577095215",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d20f8168-7156-4fc8-8782-e0d03c130488",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Before this fix, this will throw a weird error from the parquet library.",
        "createdAt" : "2021-01-27T08:39:36Z",
        "updatedAt" : "2021-01-27T08:39:36Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "0fee33bf7fef207bfef061747a0d89fd93522087",
    "line" : 60,
    "diffHunk" : "@@ -1,1 +3905,3909 @@      withSQLConf(SQLConf.PARQUET_VECTORIZED_READER_ENABLED.key -> \"false\") {\n        checkAnswer(readParquet(\"a DECIMAL(3, 2)\", path), sql(\"SELECT 1.00\"))\n        checkAnswer(readParquet(\"b DECIMAL(3, 2)\", path), Row(null))\n        checkAnswer(readParquet(\"b DECIMAL(11, 1)\", path), sql(\"SELECT 123456.0\"))\n        checkAnswer(readParquet(\"c DECIMAL(11, 1)\", path), Row(null))"
  },
  {
    "id" : "0390c740-b7a7-48dd-bddf-41d642b6ae3f",
    "prId" : 31319,
    "prUrl" : "https://github.com/apache/spark/pull/31319#pullrequestreview-577610183",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5d95d375-4c47-44d1-aff8-513367fd7b5e",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I'm not very sure about the last one. When the data is binary in parquet files, we don't know what the binary means and is hard to read it out as decimal. For example, what happens if the data is `CAST('abc' AS BINARY)`?",
        "createdAt" : "2021-01-27T04:20:34Z",
        "updatedAt" : "2021-01-27T08:36:55Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "c1acb12e-2d92-4131-8157-cc7f7d0cd3b1",
        "parentId" : "5d95d375-4c47-44d1-aff8-513367fd7b5e",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "It's probably better to fail if we try to read binary as decimal, as we don't know the precision/scale of it (while we do know that for int/long).",
        "createdAt" : "2021-01-27T04:23:33Z",
        "updatedAt" : "2021-01-27T04:23:33Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "56c7c80c-f654-4f5d-8411-7dfbc971f5c4",
        "parentId" : "5d95d375-4c47-44d1-aff8-513367fd7b5e",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I've opened a PR to refine it.",
        "createdAt" : "2021-01-27T08:36:29Z",
        "updatedAt" : "2021-01-27T08:36:29Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "cfcd0187-14da-49f9-9cf8-e79b839fbbda",
        "parentId" : "5d95d375-4c47-44d1-aff8-513367fd7b5e",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Thanks! The follow-up looks correct to me and is merged.",
        "createdAt" : "2021-01-27T17:56:41Z",
        "updatedAt" : "2021-01-27T17:56:42Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "7932069e5c70fe6ccb99aa7b8a8725a48b402c7e",
    "line" : 46,
    "diffHunk" : "@@ -1,1 +3904,3908 @@\n    withTempPath { path =>\n      val df2 = sql(s\"SELECT 1 a, ${Int.MaxValue + 1L} b, CAST(2 AS BINARY) c\")\n      df2.write.parquet(path.toString)\n"
  },
  {
    "id" : "8fe7bd67-d188-4e9d-97f5-f4029658c344",
    "prId" : 30824,
    "prUrl" : "https://github.com/apache/spark/pull/30824#pullrequestreview-555169851",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d8e9b69d-b5a7-42e1-a67e-0cb2904ec5d8",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Thanks!",
        "createdAt" : "2020-12-18T04:47:15Z",
        "updatedAt" : "2020-12-18T06:14:59Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "5d55b388eaba22551bc8611772f91542e2f2c8ff",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +3760,3764 @@      }\n\n      withSQLConf(SQLConf.ORC_VECTORIZED_READER_ENABLED.key -> value) {\n        withTable(\"t2\") {\n          sql("
  },
  {
    "id" : "6b53bc55-9d2e-422f-aa32-fea8d0dc7d39",
    "prId" : 30246,
    "prUrl" : "https://github.com/apache/spark/pull/30246#pullrequestreview-523532620",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fa67d5fe-6ffe-4251-b179-b60ed251cd46",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "how about `SELECT map('k1', 'v1', 'k2', 'v2')[k] FROM t GROUP BY map('k2', 'v2', 'k1', 'v1')[k]`?",
        "createdAt" : "2020-11-04T14:31:40Z",
        "updatedAt" : "2020-11-04T14:31:41Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "7107f43c-6163-411c-b3d3-28c0fb242fb8",
        "parentId" : "fa67d5fe-6ffe-4251-b179-b60ed251cd46",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Since we don't normalize the literal maps, they are not the same maps, @cloud-fan . We should not handle it here, @cloud-fan . ",
        "createdAt" : "2020-11-04T16:31:49Z",
        "updatedAt" : "2020-11-04T16:31:49Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "b7367d71d02d1938ea0bf74c42db50b10fda1fab",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +3713,3717 @@      Seq(\n        \"SELECT map('k1', 'v1')[k] FROM t GROUP BY 1\",\n        \"SELECT map('k1', 'v1')[k] FROM t GROUP BY map('k1', 'v1')[k]\",\n        \"SELECT map('k1', 'v1')[k] a FROM t GROUP BY a\").foreach { statement =>\n        checkAnswer(sql(statement), Row(\"v1\"))"
  },
  {
    "id" : "f896fdd6-a325-43ca-bf7b-9488d52f8d52",
    "prId" : 30199,
    "prUrl" : "https://github.com/apache/spark/pull/30199#pullrequestreview-520654379",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5de09cd5-f32d-4f26-8396-34320fd1fdc5",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "If you refactor the parsing logic for `GROUP BY`, I think the parser cannot accept this case. Right? If so, I think we don't need to update the Analyzer code.",
        "createdAt" : "2020-10-30T11:49:04Z",
        "updatedAt" : "2020-10-30T11:49:05Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "87e032b5-db5a-4059-9a1c-94c5d715046e",
        "parentId" : "5de09cd5-f32d-4f26-8396-34320fd1fdc5",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> If you refactor the parsing logic for `GROUP BY`, I think the parser cannot accept this case. Right? If so, I think we don't need to update the Analyzer code.\r\n\r\nIdeally, we can throw such an exception at the Parser layer. ",
        "createdAt" : "2020-10-30T13:15:23Z",
        "updatedAt" : "2020-10-30T13:15:23Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "c3f030dec1f4f51e5c9ec57ef2646445469ad221",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +3709,3713 @@      )\n      checkAnalysisException(\n        \"SELECT a, b, count(c) FROM t GROUP BY rollup(a, b) WITH CUBE\",\n        \"Use grouping analytics function like `cube|rollup` with \" +\n          \"`WITH CUBE` and `WITH ROLLUP` is not supported.\""
  },
  {
    "id" : "bfbc082f-154d-4655-8fc1-2fb8bf6979ca",
    "prId" : 29637,
    "prUrl" : "https://github.com/apache/spark/pull/29637#pullrequestreview-481771127",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "faaaee4c-7cc7-466b-a499-7e7e7504dbc9",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "a bit surprised that this query failed... nice catch.",
        "createdAt" : "2020-09-03T12:06:04Z",
        "updatedAt" : "2020-09-03T12:16:53Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "3e4a6a5d9a8f4151735b67c9dc585201e8543ab5",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +3689,3693 @@    withTable(\"t\") {\n      spark.range(1).write.saveAsTable(\"t\")\n      checkAnswer(sql(\"SELECT id FROM t WHERE (SELECT true)\"), Row(0L))\n    }\n  }"
  },
  {
    "id" : "f874194f-9ced-4eb5-ae43-e8200440c258",
    "prId" : 29208,
    "prUrl" : "https://github.com/apache/spark/pull/29208#pullrequestreview-454598999",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0f5fd64a-5cba-4c0f-96a6-3595443ea508",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Original PR doesn't need to enable this?",
        "createdAt" : "2020-07-23T20:59:53Z",
        "updatedAt" : "2020-07-23T20:59:53Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "80253637-3848-4691-b657-a75c5e8febf8",
        "parentId" : "0f5fd64a-5cba-4c0f-96a6-3595443ea508",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Yeah, the default value has been changed since Spark 3.0: https://github.com/apache/spark/pull/25520.",
        "createdAt" : "2020-07-24T01:57:48Z",
        "updatedAt" : "2020-07-24T01:57:48Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "ca9755a1c3682e1b4dcd3778fcdc10872bea3309",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +3094,3098 @@\n  test(\"SPARK-32280: Avoid duplicate rewrite attributes when there're multiple JOINs\") {\n    withSQLConf(SQLConf.CROSS_JOINS_ENABLED.key -> \"true\") {\n      sql(\"SELECT 1 AS id\").createOrReplaceTempView(\"A\")\n      sql(\"SELECT id, 'foo' AS kind FROM A\").createOrReplaceTempView(\"B\")"
  },
  {
    "id" : "6ed478cb-728b-431b-896d-d7c8af6daa90",
    "prId" : 29062,
    "prUrl" : "https://github.com/apache/spark/pull/29062#pullrequestreview-461392885",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b6d549db-f785-42a0-9478-71d2478fa628",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "as long as we have a hint in CTE, it will fail in 3.0? can you add more test cases?",
        "createdAt" : "2020-07-30T20:18:50Z",
        "updatedAt" : "2020-07-30T20:18:50Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "af680465-dbe5-48d8-8131-3530f67e9080",
        "parentId" : "b6d549db-f785-42a0-9478-71d2478fa628",
        "authorId" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "body" : "Yes. The same issue reported by multiple times in Jira with different use cases, like SPARK-32237, SPARK-32347, SPARK-32535, I will file a new PR to add more unit tests.",
        "createdAt" : "2020-08-05T07:04:41Z",
        "updatedAt" : "2020-08-05T07:07:06Z",
        "lastEditedBy" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "tags" : [
        ]
      }
    ],
    "commit" : "e361401596693557877d750d195c7555fa99cd7c",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +3567,3571 @@      checkAnswer(\n        sql(s\"\"\"\n          |WITH cte AS (SELECT /*+ REPARTITION(3) */ * FROM t)\n          |SELECT * FROM cte\n        \"\"\".stripMargin),"
  },
  {
    "id" : "9a4ad250-02e6-406e-884b-26349469fd39",
    "prId" : 29062,
    "prUrl" : "https://github.com/apache/spark/pull/29062#pullrequestreview-461393112",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ae35519f-a5ff-4636-ae65-e1de919eafbd",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "Like JoinHintSuite, we should have our own hint-specific suite. It can help us understand the test coverage of SQL HINT.",
        "createdAt" : "2020-07-30T20:20:12Z",
        "updatedAt" : "2020-07-30T20:20:12Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "657f4a8a-97f8-4129-94a6-33ebc0a44446",
        "parentId" : "ae35519f-a5ff-4636-ae65-e1de919eafbd",
        "authorId" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "body" : "Sure. I will add a new suite.",
        "createdAt" : "2020-08-05T07:05:04Z",
        "updatedAt" : "2020-08-05T07:05:04Z",
        "lastEditedBy" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "tags" : [
        ]
      }
    ],
    "commit" : "e361401596693557877d750d195c7555fa99cd7c",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +3562,3566 @@  }\n\n  test(\"SPARK-32237: Hint in CTE\") {\n    withTable(\"t\") {\n      sql(\"CREATE TABLE t USING PARQUET AS SELECT 1 AS id\")"
  },
  {
    "id" : "078ba31c-5a33-489b-8fae-840734f9c343",
    "prId" : 28860,
    "prUrl" : "https://github.com/apache/spark/pull/28860#pullrequestreview-438199923",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4a3fd1c6-2319-4e3a-a8e1-bfa9cbcd1bfc",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Can we add deeper cases? Also, you can simplify the test cases, for example, as below:\r\n\r\n```scala\r\nval df = spark.range(10).select(array(struct(array(struct(\"id\")).alias(\"col1\"))).alias(\"col0\"))\r\ndf.selectExpr(\"col0.col1.id\")\r\n```",
        "createdAt" : "2020-06-26T10:52:37Z",
        "updatedAt" : "2020-06-26T10:59:50Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "b6e92c025a4f3746165a3ba660fe80cca63bb91f",
    "line" : 25,
    "diffHunk" : "@@ -1,1 +3536,3540 @@      checkAnswer(sql(\n        \"\"\"\n          |SELECT a.b.c FROM nest\n        \"\"\".stripMargin),\n        Row(ArrayBuffer(ArrayBuffer(ArrayBuffer(1, 2)))) ::"
  },
  {
    "id" : "ee22b534-701b-47bf-aeb1-cc663d299afd",
    "prId" : 28600,
    "prUrl" : "https://github.com/apache/spark/pull/28600#pullrequestreview-416698215",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3f07b054-9c07-446c-aca8-6579c92e8f47",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "please keep the blank like above it.",
        "createdAt" : "2020-05-22T07:19:09Z",
        "updatedAt" : "2020-05-23T06:29:22Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "3a1a061bb955f1186675f5dda7525e6250614b17",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +3506,3510 @@}\n\ncase class Foo(bar: Option[String])"
  }
]