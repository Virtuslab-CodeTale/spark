[
  {
    "id" : "9a90ee21-5828-4cba-a2f8-f4f1834b1f72",
    "prId" : 32465,
    "prUrl" : "https://github.com/apache/spark/pull/32465#pullrequestreview-654904676",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8d0194a6-c210-44e1-a520-96d83d74b083",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "If you don't mind, could you add a test case for HINT explicitly?",
        "createdAt" : "2021-05-07T22:21:30Z",
        "updatedAt" : "2021-05-08T09:40:56Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "51632625e9fb3f09800112fe121de1dcae1dacee",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +4102,4106 @@  }\n\n  test(\"SPARK-35331: Fix resolving original expression in RepartitionByExpression after aliased\") {\n    Seq(\"CLUSTER\", \"DISTRIBUTE\").foreach { keyword =>\n      Seq(\"a\", \"substr(a, 0, 3)\").foreach { expr =>"
  },
  {
    "id" : "7762e03f-4de3-43f6-bc9b-f69903ba9c7f",
    "prId" : 32239,
    "prUrl" : "https://github.com/apache/spark/pull/32239#pullrequestreview-640526140",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b7a31d04-4d59-498c-9646-a1c57ea23fea",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "nit: It's safer for branch-3.0 to have this test, too, I think.",
        "createdAt" : "2021-04-21T00:13:12Z",
        "updatedAt" : "2021-04-21T00:13:13Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "63cac4d0c13214cec2b9bed38ea848651dbe387b",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +3599,3603 @@\n      assert(spark.table(\"t\").groupBy($\"c.json_string\").count().schema.fieldNames ===\n        Seq(\"json_string\", \"count\"))\n    }\n  }"
  },
  {
    "id" : "4f3ff6b2-0d1a-4d66-a925-89595c4e9477",
    "prId" : 32167,
    "prUrl" : "https://github.com/apache/spark/pull/32167#pullrequestreview-641781071",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5e2f0674-51ee-4946-bce8-2aa562ec317c",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I think this case is covered at least at `sql/core/src/test/resources/sql-tests/inputs/outer-join.sql`",
        "createdAt" : "2021-04-15T09:59:38Z",
        "updatedAt" : "2021-04-15T09:59:38Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "2ffb95f9-6dd9-4984-b48d-6a612eb4e082",
        "parentId" : "5e2f0674-51ee-4946-bce8-2aa562ec317c",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "WDYT about the above @HyukjinKwon 's comment, @zengruios ? Is this PR different because this is `String`? If this is already covered, shall we close this PR?",
        "createdAt" : "2021-04-22T05:09:51Z",
        "updatedAt" : "2021-04-22T05:09:52Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "586b690c96fb2e3c36aa8d294be052b95ebdfd4e",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +406,410 @@      Row(null))\n    checkAnswer(\n      sql(\"SELECT COALESCE(s, n) FROM nullStrings\"),\n      Seq(Row(\"abc\"), Row(\"ABC\"), Row(\"3\")))\n  }"
  },
  {
    "id" : "9c7caeb6-4384-45c4-a229-b487097775a8",
    "prId" : 32158,
    "prUrl" : "https://github.com/apache/spark/pull/32158#pullrequestreview-635235473",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d64ffe52-f52a-4215-9a85-dff1912c4309",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "```suggestion\r\n      checkAnswer(queryCoalesce, Row(\"2\") :: Nil)\r\n```",
        "createdAt" : "2021-04-14T05:56:10Z",
        "updatedAt" : "2021-04-16T10:45:10Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "91ec3479-3f0b-4279-b03c-e3c9cfc7d340",
        "parentId" : "d64ffe52-f52a-4215-9a85-dff1912c4309",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "In the worst case (e.g., author is inactive but PR is ready .. ?), we could push an empty commit to their branch to retrigger .. of course this is an ugly way though",
        "createdAt" : "2021-04-14T05:57:06Z",
        "updatedAt" : "2021-04-16T10:45:10Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "fc61869ad7fda1fd62b48067539f3c5d84d05e00",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +65,69 @@\n      checkAnswer(queryCaseWhen, Row(\"1.0\") :: Nil)\n      checkAnswer(queryCoalesce, Row(\"2\") :: Nil)\n    }\n  }"
  },
  {
    "id" : "a22ec104-6119-43bb-a9f6-b91954ea3dcf",
    "prId" : 32144,
    "prUrl" : "https://github.com/apache/spark/pull/32144#pullrequestreview-635494049",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e7cda342-c598-4154-8e98-bc93d1e247c7",
        "parentId" : null,
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "w/o the patch\r\n```scala\r\nspark-sql> SET io.file.buffer.size;\r\nio.file.buffer.size\t<undefined>\r\n```",
        "createdAt" : "2021-04-14T11:10:02Z",
        "updatedAt" : "2021-04-14T14:43:40Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "b6d26b7d87917e7e230ca38f2817c6110b461370",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +1082,1086 @@\n    // io.file.buffer.size has a default value from `SparkHadoopUtil.newConfiguration`\n    checkAnswer(sql(\"SET io.file.buffer.size\"), Row(\"io.file.buffer.size\", \"65536\"))\n  }\n"
  },
  {
    "id" : "16693624-3b7c-4e34-a857-63ca418a2209",
    "prId" : 32144,
    "prUrl" : "https://github.com/apache/spark/pull/32144#pullrequestreview-640165194",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b1e3f1a5-c046-45ae-a7ad-856f8a8a2f5b",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "This breaks the Hadoop 2.7 tests, seems the behavior is different between hadoop 2.7 and hadoop 3: https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test/job/spark-master-test-sbt-hadoop-2.7/2364/testReport/org.apache.spark.sql/SQLQuerySuite/SPARK_35044__SET_command_shall_display_default_value_for_hadoop_conf_correctly/",
        "createdAt" : "2021-04-20T16:07:53Z",
        "updatedAt" : "2021-04-20T16:07:53Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "7a6c4df6-75c5-47ab-8fa0-e69c21faa425",
        "parentId" : "b1e3f1a5-c046-45ae-a7ad-856f8a8a2f5b",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I picked a different config which seems to work: `dfs.hosts`\r\n\r\n@yaooqinn can you create a followup PR to try it?",
        "createdAt" : "2021-04-20T16:13:48Z",
        "updatedAt" : "2021-04-20T16:13:48Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "10d7780b-99df-430a-ba38-432fac39d136",
        "parentId" : "b1e3f1a5-c046-45ae-a7ad-856f8a8a2f5b",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "OK",
        "createdAt" : "2021-04-20T16:16:12Z",
        "updatedAt" : "2021-04-20T16:16:12Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "b6d26b7d87917e7e230ca38f2817c6110b461370",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +1079,1083 @@    // these keys does not exist as default yet\n    checkAnswer(sql(s\"SET ${key}no\"), Row(key + \"no\", \"<undefined>\"))\n    checkAnswer(sql(\"SET dfs.replication\"), Row(\"dfs.replication\", \"<undefined>\"))\n\n    // io.file.buffer.size has a default value from `SparkHadoopUtil.newConfiguration`"
  },
  {
    "id" : "c08c1888-e2c2-4b98-a11b-f0eec7493e64",
    "prId" : 31892,
    "prUrl" : "https://github.com/apache/spark/pull/31892#pullrequestreview-616084768",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1129f893-0510-4967-827e-faf0f56c53a6",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "is INSERT necessary to reproduce the bug?",
        "createdAt" : "2021-03-19T05:50:43Z",
        "updatedAt" : "2021-03-19T05:50:43Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "f24e4de7-44bc-4476-b0c3-7d59872f1483",
        "parentId" : "1129f893-0510-4967-827e-faf0f56c53a6",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "@cloud-fan - yes I think so.\r\n\r\nFor query:\r\n\r\n```\r\nSELECT t1.k FROM left_table t1\r\nJOIN empty_right_table t2\r\nON t1.k = t2.k\r\nLIMIT 3\r\n```\r\n\r\nThe physical plan:\r\n\r\n\r\n```\r\nCollectLimit 3\r\n+- *(2) Project [k#228L]\r\n   +- *(2) BroadcastHashJoin [k#228L], [k#229L], Inner, BuildRight, false\r\n      :- *(2) Filter isnotnull(k#228L)\r\n      :  +- *(2) ColumnarToRow\r\n      :     +- FileScan parquet default.left_table[k#228L] Batched: true, DataFilters: [isnotnull(k#228L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/chengsu/spark/sql/core/spark-warehouse/org.apache.spark.sq..., PartitionFilters: [], PushedFilters: [IsNotNull(k)], ReadSchema: struct<k:bigint>\r\n      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [id=#148]\r\n         +- *(1) Filter isnotnull(k#229L)\r\n            +- *(1) ColumnarToRow\r\n               +- FileScan parquet default.empty_right_table[k#229L] Batched: true, DataFilters: [isnotnull(k#229L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/chengsu/spark/sql/core/spark-warehouse/org.apache.spark.sq..., PartitionFilters: [], PushedFilters: [IsNotNull(k)], ReadSchema: struct<k:bigint>\r\n```\r\n\r\nThis is not reproduced any more as there's no code-gen `BaseLimitExec` after `BroadcastHashJoin`.",
        "createdAt" : "2021-03-19T06:06:10Z",
        "updatedAt" : "2021-03-19T06:06:10Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      }
    ],
    "commit" : "d57f914b79f8c6372f1ecf3116a5956f0bbce54b",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +4108,4112 @@        spark.sql(\n          \"\"\"\n            |INSERT INTO TABLE output_table\n            |SELECT t1.k FROM left_table t1\n            |JOIN empty_right_table t2"
  },
  {
    "id" : "eb053ef6-7298-4719-a5d3-59939db11bcb",
    "prId" : 31892,
    "prUrl" : "https://github.com/apache/spark/pull/31892#pullrequestreview-616121777",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b72b2661-4a97-4a8b-bfcc-fa4c3cf40c79",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "nit: how about checking `checkAnswer(spark.table(\"output_table\"), Row(...) ...)` just in case?",
        "createdAt" : "2021-03-19T05:54:50Z",
        "updatedAt" : "2021-03-19T05:54:51Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "2992e0fe-ef99-45c5-9194-7d3841fdf13f",
        "parentId" : "b72b2661-4a97-4a8b-bfcc-fa4c3cf40c79",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "@maropu - here we are inner-joining an empty table `empty_right_table`, so the output table still has nothing after query. Shall we check answer as well? I am fine either way.",
        "createdAt" : "2021-03-19T06:26:51Z",
        "updatedAt" : "2021-03-19T06:26:51Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      },
      {
        "id" : "33d3fa86-96ac-486e-9644-717fad16f368",
        "parentId" : "b72b2661-4a97-4a8b-bfcc-fa4c3cf40c79",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "okay, looks fine as it is.",
        "createdAt" : "2021-03-19T07:29:29Z",
        "updatedAt" : "2021-03-19T07:29:30Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "d57f914b79f8c6372f1ecf3116a5956f0bbce54b",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +4113,4117 @@            |ON t1.k = t2.k\n            |LIMIT 3\n          \"\"\".stripMargin)\n      }\n    }"
  },
  {
    "id" : "0ec7193c-c25a-45dd-b693-7478c0f9d494",
    "prId" : 31820,
    "prUrl" : "https://github.com/apache/spark/pull/31820#pullrequestreview-611667646",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1d2899cc-db95-4a77-bd62-93ec8b92e52c",
        "parentId" : null,
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "do we really need `df.collect()` here? shouldn't `AdaptiveSparkPlanHelper.collect()` below take care of going through query plan properly with AQE being enabled?",
        "createdAt" : "2021-03-14T05:01:48Z",
        "updatedAt" : "2021-03-16T08:20:48Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      },
      {
        "id" : "2bdd6bcb-feb0-405d-b4f3-a6ce181185dc",
        "parentId" : "1d2899cc-db95-4a77-bd62-93ec8b92e52c",
        "authorId" : "3d4870da-39a8-4406-b00d-131930d14cd8",
        "body" : "Yes, we do need to run the query first and then check the plan as this is an AQE compatible query where `ReusedExchangeExec` nodes are inserted during execution.",
        "createdAt" : "2021-03-14T07:39:44Z",
        "updatedAt" : "2021-03-16T08:20:48Z",
        "lastEditedBy" : "3d4870da-39a8-4406-b00d-131930d14cd8",
        "tags" : [
        ]
      }
    ],
    "commit" : "483686d98e887c278974c0ef89bf32cbb22b580d",
    "line" : 26,
    "diffHunk" : "@@ -1,1 +4081,4085 @@              |JOIN t AS t3 ON t3.id = t2.id\n              |\"\"\".stripMargin)\n          df.collect()\n          val reusedExchanges = collect(df.queryExecution.executedPlan) {\n            case r: ReusedExchangeExec => r"
  }
]