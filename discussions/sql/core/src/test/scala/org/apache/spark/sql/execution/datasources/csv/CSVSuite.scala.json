[
  {
    "id" : "a621e563-e6e7-42cc-8a2a-08fda5478e42",
    "prId" : 29516,
    "prUrl" : "https://github.com/apache/spark/pull/29516#pullrequestreview-472973398",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7e85f666-4ad9-4c93-b0ad-960e4cff3cf4",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "I think this test was wrong in 2 ways. First it relied on, actually, ignoring lines starting with `\\u0000`, which is the very bug we're fixing. You can see below it's asserting there is no result at all, when there should be _some_ result.",
        "createdAt" : "2020-08-23T02:03:08Z",
        "updatedAt" : "2020-08-24T14:45:57Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      }
    ],
    "commit" : "b685f1c0228594965d9daca8e9796d88ecb5680f",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1903,1907 @@  test(\"SPARK-25387: bad input should not cause NPE\") {\n    val schema = StructType(StructField(\"a\", IntegerType) :: Nil)\n    val input = spark.createDataset(Seq(\"\\u0001\\u0000\\u0001234\"))\n\n    checkAnswer(spark.read.schema(schema).csv(input), Row(null))"
  },
  {
    "id" : "09c140f6-7f86-449d-abcc-09199c277a6a",
    "prId" : 29516,
    "prUrl" : "https://github.com/apache/spark/pull/29516#pullrequestreview-472973398",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "626ac1b6-295c-4f3f-9bac-ffa89f4c9007",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "The other problem I think is that this was asserting there is no corrupt record -- no result at all -- when I think clearly the test should result in a single row with a corrupt record.",
        "createdAt" : "2020-08-23T02:03:42Z",
        "updatedAt" : "2020-08-24T14:45:57Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      }
    ],
    "commit" : "b685f1c0228594965d9daca8e9796d88ecb5680f",
    "line" : 26,
    "diffHunk" : "@@ -1,1 +1920,1924 @@        .schema(schema)\n        .csv(input),\n      Row(null, \"\\u0001\\u0000\\u0001234\"))\n    assert(spark.read.schema(schema).csv(input).collect().toSet ==\n      Set(Row(null, \"\\u0001\\u0000\\u0001234\")))"
  }
]