[
  {
    "id" : "a621e563-e6e7-42cc-8a2a-08fda5478e42",
    "prId" : 29516,
    "prUrl" : "https://github.com/apache/spark/pull/29516#pullrequestreview-472973398",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7e85f666-4ad9-4c93-b0ad-960e4cff3cf4",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "I think this test was wrong in 2 ways. First it relied on, actually, ignoring lines starting with `\\u0000`, which is the very bug we're fixing. You can see below it's asserting there is no result at all, when there should be _some_ result.",
        "createdAt" : "2020-08-23T02:03:08Z",
        "updatedAt" : "2020-08-24T14:45:57Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      }
    ],
    "commit" : "b685f1c0228594965d9daca8e9796d88ecb5680f",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1903,1907 @@  test(\"SPARK-25387: bad input should not cause NPE\") {\n    val schema = StructType(StructField(\"a\", IntegerType) :: Nil)\n    val input = spark.createDataset(Seq(\"\\u0001\\u0000\\u0001234\"))\n\n    checkAnswer(spark.read.schema(schema).csv(input), Row(null))"
  },
  {
    "id" : "09c140f6-7f86-449d-abcc-09199c277a6a",
    "prId" : 29516,
    "prUrl" : "https://github.com/apache/spark/pull/29516#pullrequestreview-472973398",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "626ac1b6-295c-4f3f-9bac-ffa89f4c9007",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "The other problem I think is that this was asserting there is no corrupt record -- no result at all -- when I think clearly the test should result in a single row with a corrupt record.",
        "createdAt" : "2020-08-23T02:03:42Z",
        "updatedAt" : "2020-08-24T14:45:57Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      }
    ],
    "commit" : "b685f1c0228594965d9daca8e9796d88ecb5680f",
    "line" : 26,
    "diffHunk" : "@@ -1,1 +1920,1924 @@        .schema(schema)\n        .csv(input),\n      Row(null, \"\\u0001\\u0000\\u0001234\"))\n    assert(spark.read.schema(schema).csv(input).collect().toSet ==\n      Set(Row(null, \"\\u0001\\u0000\\u0001234\")))"
  },
  {
    "id" : "48af063d-3657-4a36-9c29-f3218c884905",
    "prId" : 29088,
    "prUrl" : "https://github.com/apache/spark/pull/29088#pullrequestreview-447870983",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b43f5f19-d6a6-47ba-9d31-ccff78556627",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "LOL",
        "createdAt" : "2020-07-14T06:25:29Z",
        "updatedAt" : "2020-07-14T12:42:36Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "f568965e-1578-4997-8a3e-fc0d0c2769d7",
        "parentId" : "b43f5f19-d6a6-47ba-9d31-ccff78556627",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "Is it correct? I'm not sure.",
        "createdAt" : "2020-07-14T07:46:29Z",
        "updatedAt" : "2020-07-14T12:42:36Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      },
      {
        "id" : "45cd5b0a-8aaa-4da2-9709-06aec485f069",
        "parentId" : "b43f5f19-d6a6-47ba-9d31-ccff78556627",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Yup!",
        "createdAt" : "2020-07-14T07:51:38Z",
        "updatedAt" : "2020-07-14T12:42:36Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "9dde8c277dfb7d4925cd4981f7c3183c51f4af8e",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +2359,2363 @@    // scalastyle:off nonascii\n    val chinese = \"我爱中文\"\n    val korean = \"나는 한국인을 좋아한다\"\n    val japanese = \"私は日本人が好き\"\n    // scalastyle:on nonascii"
  },
  {
    "id" : "df1430ac-fde7-43de-b4c6-a8453c06bbb6",
    "prId" : 29088,
    "prUrl" : "https://github.com/apache/spark/pull/29088#pullrequestreview-448652218",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "70ae57b1-0673-4949-9946-26e16a5d45df",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Oh, @wangyum BTW, do you mean \"I like Korean\" but Korean as a language? If that's the case, I think you should write like \"나는 한국어를 좋아한다\". The current one is more like I like Korean people.",
        "createdAt" : "2020-07-15T06:05:33Z",
        "updatedAt" : "2020-07-15T06:05:34Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "9dde8c277dfb7d4925cd4981f7c3183c51f4af8e",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +2359,2363 @@    // scalastyle:off nonascii\n    val chinese = \"我爱中文\"\n    val korean = \"나는 한국인을 좋아한다\"\n    val japanese = \"私は日本人が好き\"\n    // scalastyle:on nonascii"
  },
  {
    "id" : "02c7def1-1670-4eac-ae88-bb18fdafab00",
    "prId" : 29088,
    "prUrl" : "https://github.com/apache/spark/pull/29088#pullrequestreview-448680528",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ec1f1225-ec1b-4796-af80-4be08b1f3140",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I guess Japanese is the same case @ueshin or @maropu?",
        "createdAt" : "2020-07-15T06:05:50Z",
        "updatedAt" : "2020-07-15T06:05:51Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "969e9a3f-f33b-4751-95cd-39b59ec7482b",
        "parentId" : "ec1f1225-ec1b-4796-af80-4be08b1f3140",
        "authorId" : "43998e22-6c2f-401d-9914-8cecf6fad929",
        "body" : "If you mean Japanese as a language, you should use \"私は日本語が好き\". The current one means \"I like Japanese people\".",
        "createdAt" : "2020-07-15T07:02:19Z",
        "updatedAt" : "2020-07-15T07:02:20Z",
        "lastEditedBy" : "43998e22-6c2f-401d-9914-8cecf6fad929",
        "tags" : [
        ]
      }
    ],
    "commit" : "9dde8c277dfb7d4925cd4981f7c3183c51f4af8e",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +2360,2364 @@    val chinese = \"我爱中文\"\n    val korean = \"나는 한국인을 좋아한다\"\n    val japanese = \"私は日本人が好き\"\n    // scalastyle:on nonascii\n    val english = \"I love English\""
  },
  {
    "id" : "f1788fc4-3656-4a13-b212-d8f9fde5930c",
    "prId" : 26027,
    "prUrl" : "https://github.com/apache/spark/pull/26027#pullrequestreview-301519624",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b69f65f9-e7c3-4d80-951f-9a09ff1a53d8",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Hm, wait, one more possible issue. This specifies the delimiter as string `_/-\\_`, which should then be unescaped to `_/-_`, which isn't what your test case expects. The test passes because I don't think it checks anything but year? Hm, I actually trying to figure out how it works at all in this case, as the CSV file uses `_/-\\_` as the delimiter. @jeff303 am I missing something or is there a test issue?",
        "createdAt" : "2019-10-12T15:46:04Z",
        "updatedAt" : "2019-10-14T20:16:45Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "95d90c06-9017-49e3-8704-d64b57bad4ed",
        "parentId" : "b69f65f9-e7c3-4d80-951f-9a09ff1a53d8",
        "authorId" : "131e32f1-d23a-4a45-83d3-5ba1dac05455",
        "body" : "I added some additional assertions for the rest of the columns (besides year).  The delimiter is given as `_/-\\\\_`.  This is needed because the fourth character of this delimiter is a literal backslash.  So after unescaping (via the new utility method), we end up with `_/-\\_` as the String that is passed into the Univocity parser.",
        "createdAt" : "2019-10-14T20:15:11Z",
        "updatedAt" : "2019-10-14T20:16:45Z",
        "lastEditedBy" : "131e32f1-d23a-4a45-83d3-5ba1dac05455",
        "tags" : [
        ]
      },
      {
        "id" : "d407b16c-95fc-4629-aea9-8f3441226da3",
        "parentId" : "b69f65f9-e7c3-4d80-951f-9a09ff1a53d8",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Oh right, Scala doesn't unescape it because of `\"\"\"`. This makes sense then. Good to have more tests.",
        "createdAt" : "2019-10-14T20:19:42Z",
        "updatedAt" : "2019-10-14T20:19:42Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      }
    ],
    "commit" : "17ebe0cbcd639e2ef436ed105696bac3fd159b40",
    "line" : 31,
    "diffHunk" : "@@ -1,1 +209,213 @@  test(\"SPARK-24540: test with multiple (crazy) character delimiter\") {\n    val cars = spark.read\n        .options(Map(\"quote\" -> \"\\'\", \"delimiter\" -> \"\"\"_/-\\\\_\"\"\", \"header\" -> \"true\"))\n        .csv(testFile(carsMultiCharCrazyDelimitedFile))\n"
  },
  {
    "id" : "49dbe118-561a-40ef-92a5-440a16ab7c5e",
    "prId" : 25184,
    "prUrl" : "https://github.com/apache/spark/pull/25184#pullrequestreview-264592753",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e4d9fedb-db85-454f-8c79-72cf3cfc8b44",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Nit: seems we don't need to +1",
        "createdAt" : "2019-07-21T04:47:41Z",
        "updatedAt" : "2019-07-22T06:57:15Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "2b0b786c-ace1-4b27-9829-abf6b625911d",
        "parentId" : "e4d9fedb-db85-454f-8c79-72cf3cfc8b44",
        "authorId" : "7e665d8b-d739-4edf-88c8-7379ff8585c2",
        "body" : "It do need \"+1\". Without \"+1\" won't throw exception. Because it do not exceed \"max\" value configured.",
        "createdAt" : "2019-07-22T04:03:23Z",
        "updatedAt" : "2019-07-22T06:57:15Z",
        "lastEditedBy" : "7e665d8b-d739-4edf-88c8-7379ff8585c2",
        "tags" : [
        ]
      },
      {
        "id" : "6abfca2e-d29a-4a4d-bdfd-4679ab13ba5c",
        "parentId" : "e4d9fedb-db85-454f-8c79-72cf3cfc8b44",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Ah, yes. I thought the limit was not inclusive..",
        "createdAt" : "2019-07-22T04:08:00Z",
        "updatedAt" : "2019-07-22T06:57:15Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "9e8fccabf1effa76b58527f6f061a2c8e525ab07",
    "line" : 25,
    "diffHunk" : "@@ -1,1 +2091,2095 @@    withTempPath { path =>\n      val maxCharsPerCol = 10000\n      val str = \"a\" * (maxCharsPerCol + 1)\n\n      Files.write("
  }
]