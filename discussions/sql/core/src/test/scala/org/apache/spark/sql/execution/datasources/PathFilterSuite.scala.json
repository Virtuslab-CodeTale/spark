[
  {
    "id" : "1f580e25-a4bf-4e4c-a71b-0a2ed2e68c70",
    "prId" : 30411,
    "prUrl" : "https://github.com/apache/spark/pull/30411#pullrequestreview-533881531",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "78c55741-a5f3-42b9-9d7d-93c761732650",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Note to reviewers: two tests were moved from FileBasedDataSourceSuite as this PR adds the specific suite for filtering option.",
        "createdAt" : "2020-11-18T21:14:32Z",
        "updatedAt" : "2020-11-20T03:05:41Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "ce00b6dc54ed681e7172db2856fb444d51d3f75c",
    "line" : 193,
    "diffHunk" : "@@ -1,1 +191,195 @@  }\n\n  test(\"Option pathGlobFilter: filter files correctly\") {\n    withTempPath { path =>\n      val dataDir = path.getCanonicalPath"
  },
  {
    "id" : "f1144075-c75f-4bd5-97bd-d561f9f24138",
    "prId" : 28841,
    "prUrl" : "https://github.com/apache/spark/pull/28841#pullrequestreview-470011594",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "39cd3467-13ff-4b4c-aaab-5ea5252f6fd5",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "https://github.com/apache/spark/pull/28841#discussion_r468969864\r\nCould you make the test title much clearer?",
        "createdAt" : "2020-08-19T01:34:47Z",
        "updatedAt" : "2020-11-05T10:34:16Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "bf2a66535c634ec68d667f3435a22220f38d29b5",
    "line" : 190,
    "diffHunk" : "@@ -1,1 +188,192 @@          .load(dir.getCanonicalPath)\n      }.getMessage\n      assert(msg.contains(\"Unable to infer schema for CSV\"))\n    }\n  }"
  },
  {
    "id" : "6ea4aed4-c102-44bf-be1f-a6716520bba0",
    "prId" : 28841,
    "prUrl" : "https://github.com/apache/spark/pull/28841#pullrequestreview-470078514",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6983e0d7-5a86-4575-9ee3-e159ebecf839",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Could you add tests for the case where the timestamps are totally the same?",
        "createdAt" : "2020-08-19T01:40:48Z",
        "updatedAt" : "2020-11-05T10:34:16Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "abeb8302-1983-4327-a283-ba484fd3bc6c",
        "parentId" : "6983e0d7-5a86-4575-9ee3-e159ebecf839",
        "authorId" : "e0be6955-58e9-4e7f-8132-27fdb08564c9",
        "body" : "So, looking for addition of one test which should expect a false outcome when timestamps are completely the same since we have not passed that respective time at that particular moment?",
        "createdAt" : "2020-08-19T01:43:48Z",
        "updatedAt" : "2020-11-05T10:34:16Z",
        "lastEditedBy" : "e0be6955-58e9-4e7f-8132-27fdb08564c9",
        "tags" : [
        ]
      },
      {
        "id" : "8c631b2e-5b34-4c2d-b9fc-5fd4c0295493",
        "parentId" : "6983e0d7-5a86-4575-9ee3-e159ebecf839",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "These timestamps should be inclusive or exclusive? e.g., `(modifiedAfter, modifiedBefore)`,  `[modifiedAfter, modifiedBefore]`... Looks like the current code follows `(modifiedAfter, modifiedBefore]`? Anyway, I think we need to test all the boundary cases.",
        "createdAt" : "2020-08-19T02:16:28Z",
        "updatedAt" : "2020-11-05T10:34:16Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "bf2a66535c634ec68d667f3435a22220f38d29b5",
    "line" : 198,
    "diffHunk" : "@@ -1,1 +196,200 @@      val file1 = createSingleFile(dir)\n      val file2 = createSingleFile(dir)\n      file1.setLastModified(DateTimeUtils.currentTimestamp())\n      file2.setLastModified(0)\n"
  },
  {
    "id" : "348fda27-5093-44ce-adc5-53316f1a5b0e",
    "prId" : 28841,
    "prUrl" : "https://github.com/apache/spark/pull/28841#pullrequestreview-474732662",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "83be443d-44e1-45f3-81a1-9f6c3518b732",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Suggestion; I think the tests in this file have almost similar structure, so could you define a helper function to share code and make test code less?",
        "createdAt" : "2020-08-25T15:12:23Z",
        "updatedAt" : "2020-11-05T10:34:16Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "0c6dc605-4a0f-462a-8246-5a837e13dec1",
        "parentId" : "83be443d-44e1-45f3-81a1-9f6c3518b732",
        "authorId" : "e0be6955-58e9-4e7f-8132-27fdb08564c9",
        "body" : "I'd love to follow up with something like this perhaps in a secondary PR but I probably wouldn't have time to get around to refactoring these tests again for a couple weeks.  ",
        "createdAt" : "2020-08-25T18:10:14Z",
        "updatedAt" : "2020-11-05T10:34:16Z",
        "lastEditedBy" : "e0be6955-58e9-4e7f-8132-27fdb08564c9",
        "tags" : [
        ]
      }
    ],
    "commit" : "bf2a66535c634ec68d667f3435a22220f38d29b5",
    "line" : 30,
    "diffHunk" : "@@ -1,1 +28,32 @@import org.apache.spark.sql.test.SharedSparkSession\n\nclass PathFilterSuite extends QueryTest with SharedSparkSession {\n  import testImplicits._\n"
  },
  {
    "id" : "94e69f56-2421-4f71-90e8-22d87cb709ee",
    "prId" : 28841,
    "prUrl" : "https://github.com/apache/spark/pull/28841#pullrequestreview-495506549",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dbd7c505-e0b8-45f9-b032-da54c49476c4",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Just for other reviewers: I see this is simply moved, not a new one.",
        "createdAt" : "2020-09-24T12:34:58Z",
        "updatedAt" : "2020-11-05T10:34:16Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "bf2a66535c634ec68d667f3435a22220f38d29b5",
    "line" : 562,
    "diffHunk" : "@@ -1,1 +560,564 @@  }\n\n  test(\"Option pathGlobFilter: filter files correctly\") {\n    withTempPath { path =>\n      val dataDir = path.getCanonicalPath"
  },
  {
    "id" : "bb8b9b31-bde4-4b93-bb54-3145871eb475",
    "prId" : 28841,
    "prUrl" : "https://github.com/apache/spark/pull/28841#pullrequestreview-495506549",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3ead5d3e-78c9-40e8-81a9-cf1084dda319",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Just for other reviewers: I see this is simply moved, not a new one.",
        "createdAt" : "2020-09-24T12:35:02Z",
        "updatedAt" : "2020-11-05T10:34:16Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "bf2a66535c634ec68d667f3435a22220f38d29b5",
    "line" : 579,
    "diffHunk" : "@@ -1,1 +577,581 @@  }\n\n  test(\"Option pathGlobFilter: simple extension filtering should contains partition info\") {\n    withTempPath { path =>\n      val input = Seq((\"foo\", 1), (\"oof\", 2)).toDF(\"a\", \"b\")"
  },
  {
    "id" : "3128c1a9-92ff-480f-a7a8-7a089e88623b",
    "prId" : 28841,
    "prUrl" : "https://github.com/apache/spark/pull/28841#pullrequestreview-495506549",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "27ba8f78-1f9b-4b8c-ae55-d4ce256e7ebf",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Probably good to make helper methods private. My preference is moving private methods bottom, but that's only me so not a big deal.",
        "createdAt" : "2020-09-24T12:39:15Z",
        "updatedAt" : "2020-11-05T10:34:16Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "bf2a66535c634ec68d667f3435a22220f38d29b5",
    "line" : 33,
    "diffHunk" : "@@ -1,1 +31,35 @@  import testImplicits._\n\n  def createSingleFile(dir: File): File = {\n    val file = new File(dir, \"temp\" + Random.nextInt(1000) + \".csv\")\n    stringToFile(file, \"text\")"
  },
  {
    "id" : "f5e8c0ef-fdef-4e86-97e9-b268e64514f2",
    "prId" : 28841,
    "prUrl" : "https://github.com/apache/spark/pull/28841#pullrequestreview-495506549",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0ff4645a-3163-4a33-941d-8341aaf1dd49",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "nit: why not use 1000000 instead? :)",
        "createdAt" : "2020-09-24T12:39:48Z",
        "updatedAt" : "2020-11-05T10:34:16Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "bf2a66535c634ec68d667f3435a22220f38d29b5",
    "line" : 34,
    "diffHunk" : "@@ -1,1 +32,36 @@\n  def createSingleFile(dir: File): File = {\n    val file = new File(dir, \"temp\" + Random.nextInt(1000) + \".csv\")\n    stringToFile(file, \"text\")\n  }"
  },
  {
    "id" : "197ecb19-3322-4cd5-89c3-3f72ea0ec100",
    "prId" : 28841,
    "prUrl" : "https://github.com/apache/spark/pull/28841#pullrequestreview-495506549",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f6eeb6cb-08fd-40ed-914d-4a56229c796f",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Shall we do the similar with below test instead of using specific date/format? \r\n\r\nMoreover, it would be consistent if we can go with LocalDateTime in all cases whenever possible.",
        "createdAt" : "2020-09-24T12:47:49Z",
        "updatedAt" : "2020-11-05T10:34:16Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "bf2a66535c634ec68d667f3435a22220f38d29b5",
    "line" : 158,
    "diffHunk" : "@@ -1,1 +156,160 @@    withTempDir { dir =>\n      val file = createSingleFile(dir)\n      file.setLastModified(DateTimeUtils.currentTimestamp())\n      val df = spark.read\n        .option(\"modifiedAfter\", \"2019-05-10T01:11:00\")"
  },
  {
    "id" : "4932ebff-d22c-4ce9-839f-619e832f2b62",
    "prId" : 28841,
    "prUrl" : "https://github.com/apache/spark/pull/28841#pullrequestreview-495506549",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ee6a380e-e6c8-49f5-bb34-cbc7f44cca3b",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "The overall new tests have similar structure, which can be extracted out.\r\n\r\n```\r\n  private def executeTest(\r\n      dir: File,\r\n      fileDate: LocalDateTime,\r\n      modifiedBefore: Option[String] = None,\r\n      modifiedAfter: Option[String] = None,\r\n      pathGlobFilter: Option[String] = None): DataFrame = {\r\n    executeTest(dir, Seq(fileDate), modifiedBefore, modifiedAfter, pathGlobFilter)\r\n  }\r\n\r\n  private def executeTest(\r\n      dir: File,\r\n      fileDates: Seq[LocalDateTime],\r\n      modifiedBefore: Option[String] = None,\r\n      modifiedAfter: Option[String] = None,\r\n      pathGlobFilter: Option[String] = None): DataFrame = {\r\n    fileDates.foreach { fileDate =>\r\n      val file = createSingleFile(dir)\r\n      setFileTime(fileDate, file)\r\n    }\r\n\r\n    var dfReader = spark.read.format(\"csv\").option(\"timeZone\", \"UTC\")\r\n    modifiedBefore.foreach { opt => dfReader = dfReader.option(\"modifiedBefore\", opt) }\r\n    modifiedAfter.foreach { opt => dfReader = dfReader.option(\"modifiedAfter\", opt) }\r\n    pathGlobFilter.foreach { opt => dfReader = dfReader.option(\"pathGlobFilter\", opt) }\r\n\r\n    dfReader.load(dir.getCanonicalPath)\r\n  }\r\n```\r\n\r\nAfter adding these methods, this test could be changed to below:\r\n\r\n```\r\n  test(\r\n    \"SPARK-31962: when modifiedBefore specified\" +\r\n      \" and sharing same timestamp with file last modified time.\") {\r\n    withTempDir { dir =>\r\n      val time = LocalDateTime.now(ZoneOffset.UTC)\r\n      val exc = intercept[AnalysisException] {\r\n        executeTest(dir, time, modifiedBefore = Some(formatTime(time)))\r\n      }\r\n      assert(exc.getMessage.contains(\"Unable to infer schema for CSV\"))\r\n    }\r\n  }\r\n```",
        "createdAt" : "2020-09-24T13:12:11Z",
        "updatedAt" : "2020-11-05T10:34:16Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "bf2a66535c634ec68d667f3435a22220f38d29b5",
    "line" : 57,
    "diffHunk" : "@@ -1,1 +55,59 @@  }\n\n  test(\n    \"SPARK-31962: when modifiedBefore specified\" +\n      \" and sharing same timestamp with file last modified time.\") {"
  }
]