[
  {
    "id" : "b62f2caa-d964-4b56-95cf-542b5c6ccd63",
    "prId" : 33680,
    "prUrl" : "https://github.com/apache/spark/pull/33680#pullrequestreview-724987120",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4b918b7b-23b1-4e71-95c0-538edf6f8849",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Oh, I see. #30652 also only updated this.",
        "createdAt" : "2021-08-09T03:34:19Z",
        "updatedAt" : "2021-08-09T03:34:19Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "2878965d786424cb1e3d053d5b45a50847f4cc7f",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +461,465 @@            \"|PushedFilters: \\\\[IsNotNull\\\\(value\\\\), GreaterThan\\\\(value,2\\\\)\\\\]\",\n          \"orc\" ->\n            \"|PushedFilters: \\\\[IsNotNull\\\\(value\\\\), GreaterThan\\\\(value,2\\\\)\\\\]\",\n          \"csv\" ->\n            \"|PushedFilters: \\\\[IsNotNull\\\\(value\\\\), GreaterThan\\\\(value,2\\\\)\\\\]\","
  },
  {
    "id" : "005843a0-1a90-49b0-b2f4-4f3fe007b903",
    "prId" : 33067,
    "prUrl" : "https://github.com/apache/spark/pull/33067#pullrequestreview-692510287",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b155e04a-8335-41f8-8f5d-16adc3b99691",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "I don't get the test title? ",
        "createdAt" : "2021-06-25T04:42:17Z",
        "updatedAt" : "2021-06-25T04:42:17Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "04d5d8d4-f29a-44d6-b927-9574b09658d7",
        "parentId" : "b155e04a-8335-41f8-8f5d-16adc3b99691",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "It's for testing this code path: https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala#L342",
        "createdAt" : "2021-06-25T04:56:39Z",
        "updatedAt" : "2021-06-25T04:56:39Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "baaa94e7-9f03-4553-b3d8-977446b4084e",
        "parentId" : "b155e04a-8335-41f8-8f5d-16adc3b99691",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Oh, I see. Thanks.",
        "createdAt" : "2021-06-25T07:06:13Z",
        "updatedAt" : "2021-06-25T07:06:13Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "827602107474e5380a153e4e470e6337012bc24b",
    "line" : 85,
    "diffHunk" : "@@ -1,1 +593,597 @@  }\n\n  test(\"SPARK-35884: Explain should only display one plan before AQE takes effect\") {\n    val df = (0 to 10).toDF(\"id\").where('id > 5)\n    val modes = Seq(SimpleMode, ExtendedMode, CostMode, FormattedMode)"
  },
  {
    "id" : "7c3f8ccc-418b-4bc6-9e4d-b23c745f126e",
    "prId" : 32776,
    "prUrl" : "https://github.com/apache/spark/pull/32776#pullrequestreview-716211451",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d212c79b-8e65-4dab-9b1b-70af1c625cfd",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "just for curiosity: which code change leads to this?",
        "createdAt" : "2021-07-27T17:36:23Z",
        "updatedAt" : "2021-07-27T17:36:23Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "333cb02b-7b74-4fc4-a6b6-a37eb3b43889",
        "parentId" : "d212c79b-8e65-4dab-9b1b-70af1c625cfd",
        "authorId" : "9acf210d-4935-4e03-8384-d944eb558b45",
        "body" : "There isn't one in this PR.  This line was part of the actual output before. The test checks if the actual plan contains\r\n```\r\n      \"\"\"\r\n        |(11) AQEShuffleRead\r\n        |Input [5]: [k#x, count#xL, sum#xL, sum#x, count#xL]\r\n        |Arguments: coalesced\r\n        |\"\"\".stripMargin,\r\n```\r\nwhich is true in both versions.",
        "createdAt" : "2021-07-27T17:51:26Z",
        "updatedAt" : "2021-07-27T17:51:26Z",
        "lastEditedBy" : "9acf210d-4935-4e03-8384-d944eb558b45",
        "tags" : [
        ]
      }
    ],
    "commit" : "7f5140228c5f4f5876dd6653d90472fc92bdb5f4",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +573,577 @@        |(11) AQEShuffleRead\n        |Input [5]: [k#x, count#xL, sum#xL, sum#x, count#xL]\n        |Arguments: coalesced\n        |\"\"\".stripMargin,\n      \"\"\""
  },
  {
    "id" : "081bcf1c-2c8b-4e5a-817e-92e72f2f28ca",
    "prId" : 32430,
    "prUrl" : "https://github.com/apache/spark/pull/32430#pullrequestreview-653971039",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "596a2bbe-1804-4d8e-83a2-62fe4701890e",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Hi, @c21 . \r\nAlthough SPARK-33679 enabled AQE by default for Apache Spark 3.2, we don't know what happens during QA and Apache Spark 3.2.0 RC period. In addition, we had better an explicit configuration in the test case. Could you add `spark.sql.adaptive.enabled=true` additionally?",
        "createdAt" : "2021-05-06T23:21:45Z",
        "updatedAt" : "2021-05-06T23:22:21Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "a4290706-d54a-42be-b30e-604694cf4e06",
        "parentId" : "596a2bbe-1804-4d8e-83a2-62fe4701890e",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "@dongjoon-hyun - sorry if it's not clear, this test is under `ExplainSuiteAE` which extends `EnableAdaptiveExecutionSuite` to enforce AQE enabled already.",
        "createdAt" : "2021-05-06T23:24:21Z",
        "updatedAt" : "2021-05-06T23:24:21Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      },
      {
        "id" : "8caa24ac-038b-41cc-a2d8-a40b821e86bd",
        "parentId" : "596a2bbe-1804-4d8e-83a2-62fe4701890e",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Oh, got it. Then, it's perfect.",
        "createdAt" : "2021-05-06T23:28:13Z",
        "updatedAt" : "2021-05-06T23:28:13Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "fae6c5d60a87eec3be5f1d5583f3b97f92696e07",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +568,572 @@\n  test(\"SPARK-35133: explain codegen should work with AQE\") {\n    withSQLConf(SQLConf.WHOLESTAGE_CODEGEN_ENABLED.key -> \"true\") {\n      withTempView(\"df\") {\n        val df = spark.range(5).select(col(\"id\").as(\"key\"), col(\"id\").as(\"value\"))"
  },
  {
    "id" : "d25e3b4a-4c74-416f-9a84-9f09c070199c",
    "prId" : 30855,
    "prUrl" : "https://github.com/apache/spark/pull/30855#pullrequestreview-555988391",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6c151e50-7599-42dd-a0ae-1b0a653221da",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "nit: `df1` -> `df` (this is a nit comment, so I think we don't need a follow-up pr to fix it)",
        "createdAt" : "2020-12-20T06:01:16Z",
        "updatedAt" : "2020-12-20T06:01:16Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "13fb225b3742566676009e4fc0f8d5a9d19820c0",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +286,290 @@            .format(\"parquet\")\n            .mode(\"overwrite\")\n            .saveAsTable(\"df1\")\n\n          val sqlText = \"EXPLAIN FORMATTED SELECT (SELECT min(id) FROM df1) as v\""
  },
  {
    "id" : "5a96f043-0317-446d-bf4f-5227e2f91911",
    "prId" : 30855,
    "prUrl" : "https://github.com/apache/spark/pull/30855#pullrequestreview-555988404",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c3b116dd-0c9c-446f-a172-211f0aa4e814",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "nit: we don't the number in the name, I think. `expected_pattern1` -> `expected_pattern`",
        "createdAt" : "2020-12-20T06:01:39Z",
        "updatedAt" : "2020-12-20T06:01:40Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "13fb225b3742566676009e4fc0f8d5a9d19820c0",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +289,293 @@\n          val sqlText = \"EXPLAIN FORMATTED SELECT (SELECT min(id) FROM df1) as v\"\n          val expected_pattern1 =\n            \"Subquery:1 Hosting operator id = 2 Hosting Expression = Subquery subquery#x\"\n"
  },
  {
    "id" : "b05bd73d-d630-4b3a-afd5-604e1b73b76b",
    "prId" : 30855,
    "prUrl" : "https://github.com/apache/spark/pull/30855#pullrequestreview-555988464",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7a7502e5-b7cd-4177-830d-955c655b4f77",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "I think its better to use temporary views in tests where possible.",
        "createdAt" : "2020-12-20T06:03:37Z",
        "updatedAt" : "2020-12-20T06:03:37Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "13fb225b3742566676009e4fc0f8d5a9d19820c0",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +282,286 @@      withSQLConf(SQLConf.ADAPTIVE_EXECUTION_ENABLED.key -> \"true\") {\n        withTable(\"df1\") {\n          spark.range(1, 100)\n            .write\n            .format(\"parquet\")"
  },
  {
    "id" : "2c1b6241-df3d-4c42-9ced-98d9f0d9aa59",
    "prId" : 30647,
    "prUrl" : "https://github.com/apache/spark/pull/30647#pullrequestreview-546787712",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "06386a01-ea59-4d19-b483-57619653cb61",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "I updated this pattern for avoiding a unexpected longest string match in `expected_pattern3`; this PR makes the explain output be placed in a single line because line breaks are removed in `show`. So, the two consecutive `Location:....` strings are regarded as a single match, and then the test in L272 fails.",
        "createdAt" : "2020-12-08T06:48:28Z",
        "updatedAt" : "2020-12-11T05:40:39Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "feb02779c017cca24aa18240aa4370591c248ad6",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +263,267 @@          val expected_pattern3 =\n            \"Location: InMemoryFileIndex \\\\[\\\\S*org.apache.spark.sql.ExplainSuite\" +\n              \"/df2/\\\\S*, ... 99 entries\\\\]\"\n          val expected_pattern4 =\n            \"Location: InMemoryFileIndex \\\\[\\\\S*org.apache.spark.sql.ExplainSuite\" +"
  },
  {
    "id" : "8bd9ecf5-dd77-4830-a686-031add2db281",
    "prId" : 28425,
    "prUrl" : "https://github.com/apache/spark/pull/28425#pullrequestreview-406355966",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "353bd912-cfb1-4292-a2c5-0cf275d66f26",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "can we add a `table-scan-explain.sql` to test it? It's easier to see the result.",
        "createdAt" : "2020-05-05T07:48:53Z",
        "updatedAt" : "2020-07-12T06:12:07Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "9449510c-e590-468b-93e1-620e2e882cad",
        "parentId" : "353bd912-cfb1-4292-a2c5-0cf275d66f26",
        "authorId" : "a2fcc15e-a51a-42f0-87a6-137048a28e30",
        "body" : "@cloud-fan Agree. Actually i had tried but could not get the V2 scan set up through SQL. Could you please tell me how to do it ? ",
        "createdAt" : "2020-05-06T01:16:21Z",
        "updatedAt" : "2020-07-12T06:12:07Z",
        "lastEditedBy" : "a2fcc15e-a51a-42f0-87a6-137048a28e30",
        "tags" : [
        ]
      },
      {
        "id" : "53e05471-b221-4079-bb1b-66c28b04fd25",
        "parentId" : "353bd912-cfb1-4292-a2c5-0cf275d66f26",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Oh I see. Currently DS v2 scan is enabled only in `DataFrameReader`, so we can't get it through pure SQL. Then this is fine.",
        "createdAt" : "2020-05-06T04:38:31Z",
        "updatedAt" : "2020-07-12T06:12:07Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "65faf5d2-d0c0-4b39-b7ae-b320f8fed462",
        "parentId" : "353bd912-cfb1-4292-a2c5-0cf275d66f26",
        "authorId" : "a2fcc15e-a51a-42f0-87a6-137048a28e30",
        "body" : "+1. Thank you.",
        "createdAt" : "2020-05-06T05:21:53Z",
        "updatedAt" : "2020-07-12T06:12:07Z",
        "lastEditedBy" : "a2fcc15e-a51a-42f0-87a6-137048a28e30",
        "tags" : [
        ]
      },
      {
        "id" : "5a43932b-ab8a-4478-bb65-c0b1c54e800a",
        "parentId" : "353bd912-cfb1-4292-a2c5-0cf275d66f26",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Yea, I think so...",
        "createdAt" : "2020-05-06T07:19:08Z",
        "updatedAt" : "2020-07-12T06:12:07Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "e177c2a3a99ce2ed64c0562035f0552a4c25c919",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +362,366 @@  }\n\n  test(\"Explain formatted output for scan operator for datasource V2\") {\n    withTempDir { dir =>\n      Seq(\"parquet\", \"orc\", \"csv\", \"json\").foreach { fmt =>"
  },
  {
    "id" : "1526870e-c487-4881-96a0-fc7e948cea8c",
    "prId" : 28425,
    "prUrl" : "https://github.com/apache/spark/pull/28425#pullrequestreview-446848759",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3c3968f0-98ec-492c-bcbc-1c2df35b2662",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Can we simply put `\"\"`?",
        "createdAt" : "2020-07-11T15:11:30Z",
        "updatedAt" : "2020-07-12T06:12:07Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "91399110-f037-47c8-a4cd-c973d31f259a",
        "parentId" : "3c3968f0-98ec-492c-bcbc-1c2df35b2662",
        "authorId" : "a2fcc15e-a51a-42f0-87a6-137048a28e30",
        "body" : "@dongjoon-hyun I had tried and it didn't work for me. Perhaps there is a better way to do this. Basically, for JSON, i don't want a line printed for pushedFilters. Putting a `\"\"` results in the following as expected output. Here i wanted to get rid of the empty line between `PartitionFilters` and `ReadSchema`\r\n```\r\n\\(1\\) BatchScan\r\nOutput \\[2\\]: \\[value#x, id#x\\]\r\nDataFilters: \\[isnotnull\\(value#x\\), \\(value#x > 2\\)\\]\r\nFormat: json\r\nLocation: InMemoryFileIndex\\[.*\\]\r\nPartitionFilters: \\[isnotnull\\(id#x\\), \\(id#x > 1\\)\\]\r\n\r\nReadSchema: struct\\<value:int\\>\r\n```",
        "createdAt" : "2020-07-12T06:05:39Z",
        "updatedAt" : "2020-07-12T06:12:07Z",
        "lastEditedBy" : "a2fcc15e-a51a-42f0-87a6-137048a28e30",
        "tags" : [
        ]
      }
    ],
    "commit" : "e177c2a3a99ce2ed64c0562035f0552a4c25c919",
    "line" : 26,
    "diffHunk" : "@@ -1,1 +374,378 @@            \"|PushedFilers: \\\\[IsNotNull\\\\(value\\\\), GreaterThan\\\\(value,2\\\\)\\\\]\",\n          \"json\" ->\n            \"|remove_marker\"\n        )\n        val expected_plan_fragment1 ="
  },
  {
    "id" : "9ed67b2a-c3bb-46b7-8bc4-693b107c7cec",
    "prId" : 28425,
    "prUrl" : "https://github.com/apache/spark/pull/28425#pullrequestreview-446848780",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7e626d16-97df-44a0-83d2-9c9d071bd731",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "It seems that we can remove `.replaceAll(\"\\nremove_marker\", \"\")` if we fix line 376. WDYT?",
        "createdAt" : "2020-07-11T15:12:12Z",
        "updatedAt" : "2020-07-12T06:12:07Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "f4d5de7f-dcba-483b-baff-3d772865a0e4",
        "parentId" : "7e626d16-97df-44a0-83d2-9c9d071bd731",
        "authorId" : "a2fcc15e-a51a-42f0-87a6-137048a28e30",
        "body" : "@dongjoon-hyun Please see my response above.",
        "createdAt" : "2020-07-12T06:06:09Z",
        "updatedAt" : "2020-07-12T06:12:07Z",
        "lastEditedBy" : "a2fcc15e-a51a-42f0-87a6-137048a28e30",
        "tags" : [
        ]
      }
    ],
    "commit" : "e177c2a3a99ce2ed64c0562035f0552a4c25c919",
    "line" : 38,
    "diffHunk" : "@@ -1,1 +386,390 @@             ${pushFilterMaps.get(fmt).get}\n             |ReadSchema: struct\\\\<value:int\\\\>\n             |\"\"\".stripMargin.replaceAll(\"\\nremove_marker\", \"\").trim\n\n        spark.range(10)"
  },
  {
    "id" : "f9eaf52b-e447-4cd5-8eb8-5ab347e7e0a3",
    "prId" : 28271,
    "prUrl" : "https://github.com/apache/spark/pull/28271#pullrequestreview-396386262",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "448adea0-9b88-40de-b33e-c58e65ae0b2e",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@Ngone51, it would be better if we show the before/after results in the PR description. What was it before out of curiosity? Was it just `AdaptiveSparkPlan (14)`?",
        "createdAt" : "2020-04-20T09:51:26Z",
        "updatedAt" : "2020-04-22T12:11:09Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "68e7cdd1-571c-4689-8ef7-5d35a04ba3bb",
        "parentId" : "448adea0-9b88-40de-b33e-c58e65ae0b2e",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Thanks, I'm updating the description.",
        "createdAt" : "2020-04-20T11:58:42Z",
        "updatedAt" : "2020-04-22T12:11:09Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "5893e4004a0c14e3a00d0730e44779a1d71ebfe0",
    "line" : 78,
    "diffHunk" : "@@ -1,1 +358,362 @@    val ws = \" \"\n    //   == Physical Plan ==\n    //   AdaptiveSparkPlan (14)\n    //   +- * HashAggregate (13)\n    //      +- CustomShuffleReader (12)"
  },
  {
    "id" : "735971d0-921b-4d6f-9160-b8e8d9c91da0",
    "prId" : 26042,
    "prUrl" : "https://github.com/apache/spark/pull/26042#pullrequestreview-301749519",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a2b2f2e9-ff0c-4abe-bcda-6dac7819920d",
        "parentId" : null,
        "authorId" : "a2fcc15e-a51a-42f0-87a6-137048a28e30",
        "body" : "@cloud-fan Is this not a DPP filter ? ",
        "createdAt" : "2019-10-15T08:15:05Z",
        "updatedAt" : "2019-10-17T15:08:45Z",
        "lastEditedBy" : "a2fcc15e-a51a-42f0-87a6-137048a28e30",
        "tags" : [
        ]
      },
      {
        "id" : "500bf861-ca23-4dbb-b337-15997ea8c179",
        "parentId" : "a2b2f2e9-ff0c-4abe-bcda-6dac7819920d",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "ah i see, we already print all the partition filters. LGTM then",
        "createdAt" : "2019-10-15T08:55:53Z",
        "updatedAt" : "2019-10-17T15:08:45Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "490ee3ba033ad797da2a2d3ecc8e06de535aaaad",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +246,250 @@          val expected_pattern2 =\n            \"PartitionFilters: \\\\[isnotnull\\\\(k#xL\\\\), dynamicpruningexpression\\\\(k#xL \" +\n              \"IN subquery#x\\\\)\\\\]\"\n          val expected_pattern3 =\n            \"Location: PrunedInMemoryFileIndex \\\\[.*org.apache.spark.sql.ExplainSuite\" +"
  }
]