[
  {
    "id" : "c5be3414-ba01-4f22-8086-5b4d403fc192",
    "prId" : 30521,
    "prUrl" : "https://github.com/apache/spark/pull/30521#pullrequestreview-544704387",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bfb12c90-27da-43eb-a286-92b10a356426",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "nit: `load` instead of `loads`, but as it's a nit, OK to fix in follow-up PR.",
        "createdAt" : "2020-12-04T07:13:02Z",
        "updatedAt" : "2020-12-04T07:22:32Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "32f9940641d479470530dcbaaeed4be12e1eef61",
    "line" : 122,
    "diffHunk" : "@@ -1,1 +276,280 @@      withTable(tableName) {\n        // The file written by batch will not be seen after the table was written by a streaming\n        // query. This is because we loads files from the metadata log instead of listing them\n        // using HDFS API.\n        Seq(4, 5, 6).toDF(\"value\").write.format(\"parquet\")"
  },
  {
    "id" : "46543a3e-6039-494a-87cf-9ec72106091b",
    "prId" : 30521,
    "prUrl" : "https://github.com/apache/spark/pull/30521#pullrequestreview-544704387",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "39e4f625-3121-4b55-b35d-dd5e8231f38f",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "same here",
        "createdAt" : "2020-12-04T07:13:27Z",
        "updatedAt" : "2020-12-04T07:22:32Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "32f9940641d479470530dcbaaeed4be12e1eef61",
    "line" : 136,
    "diffHunk" : "@@ -1,1 +290,294 @@    withTable(tableName) {\n      // The file written by batch will not be seen after the table was written by a streaming\n      // query. This is because we loads files from the metadata log instead of listing them\n      // using HDFS API.\n      Seq(4, 5, 6).toDF(\"value\").write.format(\"parquet\").saveAsTable(tableName)"
  },
  {
    "id" : "aaec8e93-6a54-4978-adcf-5f4e618cd47c",
    "prId" : 30521,
    "prUrl" : "https://github.com/apache/spark/pull/30521#pullrequestreview-544704387",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6642d1ed-5ff0-49f0-96aa-234acb3aa84d",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "same here",
        "createdAt" : "2020-12-04T07:18:10Z",
        "updatedAt" : "2020-12-04T07:22:32Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "32f9940641d479470530dcbaaeed4be12e1eef61",
    "line" : 149,
    "diffHunk" : "@@ -1,1 +303,307 @@      withTable(tableName) {\n        // The file written by batch will not be seen after the table was written by a streaming\n        // query. This is because we loads files from the metadata log instead of listing them\n        // using HDFS API.\n        Seq(4, 5, 6).toDF(\"value\").write"
  }
]