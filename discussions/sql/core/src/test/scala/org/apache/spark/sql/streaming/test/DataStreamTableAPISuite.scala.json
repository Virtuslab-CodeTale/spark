[
  {
    "id" : "c5be3414-ba01-4f22-8086-5b4d403fc192",
    "prId" : 30521,
    "prUrl" : "https://github.com/apache/spark/pull/30521#pullrequestreview-544704387",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bfb12c90-27da-43eb-a286-92b10a356426",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "nit: `load` instead of `loads`, but as it's a nit, OK to fix in follow-up PR.",
        "createdAt" : "2020-12-04T07:13:02Z",
        "updatedAt" : "2020-12-04T07:22:32Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "32f9940641d479470530dcbaaeed4be12e1eef61",
    "line" : 122,
    "diffHunk" : "@@ -1,1 +276,280 @@      withTable(tableName) {\n        // The file written by batch will not be seen after the table was written by a streaming\n        // query. This is because we loads files from the metadata log instead of listing them\n        // using HDFS API.\n        Seq(4, 5, 6).toDF(\"value\").write.format(\"parquet\")"
  },
  {
    "id" : "46543a3e-6039-494a-87cf-9ec72106091b",
    "prId" : 30521,
    "prUrl" : "https://github.com/apache/spark/pull/30521#pullrequestreview-544704387",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "39e4f625-3121-4b55-b35d-dd5e8231f38f",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "same here",
        "createdAt" : "2020-12-04T07:13:27Z",
        "updatedAt" : "2020-12-04T07:22:32Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "32f9940641d479470530dcbaaeed4be12e1eef61",
    "line" : 136,
    "diffHunk" : "@@ -1,1 +290,294 @@    withTable(tableName) {\n      // The file written by batch will not be seen after the table was written by a streaming\n      // query. This is because we loads files from the metadata log instead of listing them\n      // using HDFS API.\n      Seq(4, 5, 6).toDF(\"value\").write.format(\"parquet\").saveAsTable(tableName)"
  },
  {
    "id" : "aaec8e93-6a54-4978-adcf-5f4e618cd47c",
    "prId" : 30521,
    "prUrl" : "https://github.com/apache/spark/pull/30521#pullrequestreview-544704387",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6642d1ed-5ff0-49f0-96aa-234acb3aa84d",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "same here",
        "createdAt" : "2020-12-04T07:18:10Z",
        "updatedAt" : "2020-12-04T07:22:32Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "32f9940641d479470530dcbaaeed4be12e1eef61",
    "line" : 149,
    "diffHunk" : "@@ -1,1 +303,307 @@      withTable(tableName) {\n        // The file written by batch will not be seen after the table was written by a streaming\n        // query. This is because we loads files from the metadata log instead of listing them\n        // using HDFS API.\n        Seq(4, 5, 6).toDF(\"value\").write"
  },
  {
    "id" : "94c5145d-a8de-49f4-885b-8e270a483e7b",
    "prId" : 29767,
    "prUrl" : "https://github.com/apache/spark/pull/29767#pullrequestreview-502837737",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "52407077-8cf1-40db-939c-7341bbd292fe",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "can we use a fake v2 provider that doesn't support streaming? We might support file source v2 streaming write in the future.",
        "createdAt" : "2020-10-06T08:08:52Z",
        "updatedAt" : "2020-10-08T06:37:15Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "3884e322-332a-4edc-9f7f-e2f1865b5f39",
        "parentId" : "52407077-8cf1-40db-939c-7341bbd292fe",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "The tests I added are more likely E2E tests so that we don't miss something. A table backed by file source is definitely something we want to ensure whether it's working or not at specific moment. When we support streaming write on file source v2, this test should be modified at that time to verify it.\r\n\r\nIf we want to also add a test for V2 table which doesn't support streaming, it warrants a new test. Please let me know whether we'd like to add it.",
        "createdAt" : "2020-10-06T11:25:38Z",
        "updatedAt" : "2020-10-08T06:37:15Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "3f80c4fc78ca4fcfc1f6a843a3175b403fb4bb3c",
    "line" : 145,
    "diffHunk" : "@@ -1,1 +219,223 @@  }\n\n  test(\"write: write to file provider based table isn't allowed yet\") {\n    val tableIdentifier = \"table_name\"\n"
  },
  {
    "id" : "57295cfc-6cf7-4757-8969-c99cf874954f",
    "prId" : 29767,
    "prUrl" : "https://github.com/apache/spark/pull/29767#pullrequestreview-504152695",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "106e7d4f-baf7-4ba3-96c8-8630b34b1385",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Thank you for adding this explicitly.",
        "createdAt" : "2020-10-07T18:22:11Z",
        "updatedAt" : "2020-10-08T06:37:15Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "3f80c4fc78ca4fcfc1f6a843a3175b403fb4bb3c",
    "line" : 159,
    "diffHunk" : "@@ -1,1 +233,237 @@  }\n\n  test(\"write: write to temporary view isn't allowed yet\") {\n    val tableIdentifier = \"testcat.table_name\"\n    val tempViewIdentifier = \"temp_view\""
  },
  {
    "id" : "ac13bcc5-9c5c-4f7d-a8e7-88a39e385db0",
    "prId" : 29756,
    "prUrl" : "https://github.com/apache/spark/pull/29756#pullrequestreview-494398189",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8bd78cb3-f09a-4ce2-b3f9-3a0dd77e1f1f",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "if we read a batch temp view using this API, shall we fail?",
        "createdAt" : "2020-09-21T15:51:52Z",
        "updatedAt" : "2020-09-24T07:05:24Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "5aabe7c7-cd4f-4ca8-9763-23b36d597fc5",
        "parentId" : "8bd78cb3-f09a-4ce2-b3f9-3a0dd77e1f1f",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "Yes, add a new case in fad1976.",
        "createdAt" : "2020-09-23T08:00:37Z",
        "updatedAt" : "2020-09-24T07:05:24Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "d2eb23fedc024b2e88b35baa777363348f9fffb0",
    "line" : 81,
    "diffHunk" : "@@ -1,1 +79,83 @@  }\n\n  test(\"stream table API with temp view\") {\n    val tblName = \"my_table\"\n    val stream = MemoryStream[Int]"
  },
  {
    "id" : "ceba9823-cc68-47b7-9ba3-b66bc395cdb3",
    "prId" : 29756,
    "prUrl" : "https://github.com/apache/spark/pull/29756#pullrequestreview-494398473",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "83c5fc9d-ed07-4089-b510-cfd1649f659e",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "is there a way to test the v1 fallback?",
        "createdAt" : "2020-09-21T15:52:58Z",
        "updatedAt" : "2020-09-24T07:05:24Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "0dec6141-b0b9-4b63-92e4-f4f800e9b1f8",
        "parentId" : "83c5fc9d-ed07-4089-b510-cfd1649f659e",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "Yes, add a new case and refactor the fallback code in fad1976",
        "createdAt" : "2020-09-23T08:00:58Z",
        "updatedAt" : "2020-09-24T07:05:24Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "d2eb23fedc024b2e88b35baa777363348f9fffb0",
    "line" : 152,
    "diffHunk" : "@@ -1,1 +150,154 @@      )\n    }\n  }\n\n  test(\"fallback to V1 relation\") {"
  }
]