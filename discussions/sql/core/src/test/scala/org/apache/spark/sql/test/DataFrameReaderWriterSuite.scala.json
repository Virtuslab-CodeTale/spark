[
  {
    "id" : "03690757-e3c7-4de1-9810-5628ed0cd0f9",
    "prId" : 25192,
    "prUrl" : "https://github.com/apache/spark/pull/25192#pullrequestreview-267532397",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4f2d90ea-f8a8-4175-a18d-33adcc8b0d75",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Ideally we should print out the problematic field then during comparison.\r\nSpark has been using `catalogString` for now for simplicity. Let's stick to `catalogString` for now to be consistent.",
        "createdAt" : "2019-07-29T00:42:16Z",
        "updatedAt" : "2019-07-29T06:25:19Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "a0aa5f54276290bb70bffcfff8539c4781af0956",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +432,436 @@    // when users specify the schema\n    val inputSchema = new StructType().add(\"s\", IntegerType, nullable = false)\n    val expectedSchema = StructType(StructField(\"i\", IntegerType, nullable = false) :: Nil)\n    val e = intercept[AnalysisException] { dfReader.schema(inputSchema).load() }\n    assert(e.getMessage.contains("
  }
]