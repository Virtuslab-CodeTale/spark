[
  {
    "id" : "03690757-e3c7-4de1-9810-5628ed0cd0f9",
    "prId" : 25192,
    "prUrl" : "https://github.com/apache/spark/pull/25192#pullrequestreview-267532397",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4f2d90ea-f8a8-4175-a18d-33adcc8b0d75",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Ideally we should print out the problematic field then during comparison.\r\nSpark has been using `catalogString` for now for simplicity. Let's stick to `catalogString` for now to be consistent.",
        "createdAt" : "2019-07-29T00:42:16Z",
        "updatedAt" : "2019-07-29T06:25:19Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "a0aa5f54276290bb70bffcfff8539c4781af0956",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +432,436 @@    // when users specify the schema\n    val inputSchema = new StructType().add(\"s\", IntegerType, nullable = false)\n    val expectedSchema = StructType(StructField(\"i\", IntegerType, nullable = false) :: Nil)\n    val e = intercept[AnalysisException] { dfReader.schema(inputSchema).load() }\n    assert(e.getMessage.contains("
  },
  {
    "id" : "a5474203-5b38-418e-a89a-87d6050f7f0c",
    "prId" : 24784,
    "prUrl" : "https://github.com/apache/spark/pull/24784#pullrequestreview-246894736",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "16065579-bfee-46de-9f3d-4e60bad02c2c",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "`decodePartitioningColumns` is under `execution` package that's not supposed to be exposed so users shouldn't use this util directly.\r\n\r\nDid we document this option to any public datasource v1 API? We should also say this is a JSON string.",
        "createdAt" : "2019-06-07T02:32:49Z",
        "updatedAt" : "2019-06-07T02:32:49Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "bbd8d14b939b1d21fe9237393ceacd297c2a0e87",
    "line" : 25,
    "diffHunk" : "@@ -1,1 +232,236 @@\n    val partColumns = LastOptions.parameters(DataSourceUtils.PARTITIONING_COLUMNS_KEY)\n    assert(DataSourceUtils.decodePartitioningColumns(partColumns) === Seq(\"col1\", \"col2\"))\n  }\n"
  },
  {
    "id" : "917a84cf-9880-4a0a-a56e-a8b1197702fd",
    "prId" : 24369,
    "prUrl" : "https://github.com/apache/spark/pull/24369#pullrequestreview-226407783",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "75eb25ff-eba6-4a4b-9c97-56888da7b76a",
        "parentId" : null,
        "authorId" : "99156419-7ce7-4671-b858-d0b5c4711f88",
        "body" : "why remove `toLowerCase(Locale.ROOT)`?",
        "createdAt" : "2019-04-14T18:34:37Z",
        "updatedAt" : "2019-04-14T18:34:49Z",
        "lastEditedBy" : "99156419-7ce7-4671-b858-d0b5c4711f88",
        "tags" : [
        ]
      },
      {
        "id" : "300d395a-16eb-40ec-849d-087957c87dc4",
        "parentId" : "75eb25ff-eba6-4a4b-9c97-56888da7b76a",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "Because both V1 and V2 show exactly the same message. \r\n`toLowerCase(Locale.ROOT)` was added in the migration of CSV V2 https://github.com/apache/spark/pull/24005/files#diff-b9ddfbc9be8d83ecf100b3b8ff9610b9R431 ",
        "createdAt" : "2019-04-14T18:43:05Z",
        "updatedAt" : "2019-04-14T18:43:06Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "7eaae340740d77d6c70ea9e492871a785ce836bc",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +429,433 @@      testRead(spark.read.csv(), Seq.empty, schema)\n    }.getMessage\n    assert(message.contains(\"Unable to infer schema for CSV. It must be specified manually.\"))\n\n    testRead(spark.read.csv(dir), data, schema)"
  }
]