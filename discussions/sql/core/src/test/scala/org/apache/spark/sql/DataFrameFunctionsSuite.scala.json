[
  {
    "id" : "cc6314ef-fa50-4770-9971-cc10e1ef2ff0",
    "prId" : 31887,
    "prUrl" : "https://github.com/apache/spark/pull/31887#pullrequestreview-628356228",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6469e194-b76b-4e9f-9ade-0d53cf0de1e8",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "nit: `nested transform (DSL)` -> `SPARK-34794: nested transform`",
        "createdAt" : "2021-04-06T01:05:59Z",
        "updatedAt" : "2021-04-06T01:10:48Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "8c2ad611bf6aa4a0acdbebc6f78248d0a2a6ac9e",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +2262,2266 @@  }\n\n  test(\"nested transform (DSL)\") {\n    val df = Seq(\n        (Seq(1, 2, 3), Seq(\"a\", \"b\", \"c\"))"
  },
  {
    "id" : "f9db6ef2-fd16-43c3-b159-fe4adee79159",
    "prId" : 31887,
    "prUrl" : "https://github.com/apache/spark/pull/31887#pullrequestreview-628356228",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cccdcd78-15c3-455d-a123-ea7164cdea71",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "I think we need exhaustive tests for this issue, e.g., two/three argument cases and their combination cases.",
        "createdAt" : "2021-04-06T01:10:25Z",
        "updatedAt" : "2021-04-06T01:10:48Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "8c2ad611bf6aa4a0acdbebc6f78248d0a2a6ac9e",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +2272,2276 @@            transform(\n              $\"numbers\",\n              (number: Column) => transform(\n                $\"letters\",\n                (letter: Column) => struct("
  },
  {
    "id" : "833773e6-3051-4511-a9f7-b32e4a0084ca",
    "prId" : 27991,
    "prUrl" : "https://github.com/apache/spark/pull/27991#pullrequestreview-399617281",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e720d3c3-fbbf-4335-966e-f3316a247e28",
        "parentId" : null,
        "authorId" : "a80e0991-f7b1-415a-8d2e-ab615cc6ef4a",
        "body" : "How about `concat(array(), array(NULL))`? That should have the same type as `array(NULL)`.",
        "createdAt" : "2020-04-23T12:01:21Z",
        "updatedAt" : "2020-04-23T12:01:22Z",
        "lastEditedBy" : "a80e0991-f7b1-415a-8d2e-ab615cc6ef4a",
        "tags" : [
        ]
      },
      {
        "id" : "4e4c8d2c-9fd7-4bb5-ae4c-13198ef6e97b",
        "parentId" : "e720d3c3-fbbf-4335-966e-f3316a247e28",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Yea, the output has the same type with array(null);\r\n```\r\nscala> sql(\"select concat(array(), array(NULL))\").printSchema\r\nroot\r\n |-- concat(array(), array(NULL)): array (nullable = false)\r\n |    |-- element: null (containsNull = true)\r\n\r\nscala> sql(\"select array()\").printSchema\r\nroot\r\n |-- array(): array (nullable = false)\r\n |    |-- element: null (containsNull = false)\r\n\r\nscala> sql(\"select array(null)\").printSchema\r\nroot\r\n |-- array(NULL): array (nullable = false)\r\n |    |-- element: null (containsNull = true)\r\n```\r\nAny concern?",
        "createdAt" : "2020-04-23T12:26:24Z",
        "updatedAt" : "2020-04-23T12:26:24Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "7b0450eb-2e77-409d-b6ef-122900035337",
        "parentId" : "e720d3c3-fbbf-4335-966e-f3316a247e28",
        "authorId" : "a80e0991-f7b1-415a-8d2e-ab615cc6ef4a",
        "body" : "Only that it should be tested, since it's an interesting corner case!",
        "createdAt" : "2020-04-23T13:56:39Z",
        "updatedAt" : "2020-04-23T13:56:39Z",
        "lastEditedBy" : "a80e0991-f7b1-415a-8d2e-ab615cc6ef4a",
        "tags" : [
        ]
      },
      {
        "id" : "b8cc1bac-f47e-440d-a3ab-d86f1f8912ed",
        "parentId" : "e720d3c3-fbbf-4335-966e-f3316a247e28",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I believe those cases are tested in `CastSuite.scala` and `TypeCoercionSuite.scala` including all types if I didn't miss anything. I just kept one e2e test here since it was the reported case in the JIRA SPARK-31227.",
        "createdAt" : "2020-04-24T03:33:50Z",
        "updatedAt" : "2020-04-24T03:33:50Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "1bf68f03acce3cb46380ecbadd66693cbff39195",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +1533,1537 @@  }\n\n  test(\"SPARK-31227: Non-nullable null type should not coerce to nullable type in concat\") {\n    val actual = spark.range(1).selectExpr(\"concat(array(), array(1)) as arr\")\n    val expected = spark.range(1).selectExpr(\"array(1) as arr\")"
  },
  {
    "id" : "77f6c4b1-ceb5-4b34-967a-23c53b873eb2",
    "prId" : 26811,
    "prUrl" : "https://github.com/apache/spark/pull/26811#pullrequestreview-331822139",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4fa6bea0-05e4-4215-9222-40c448eecd3a",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Since this is a bug, can you split these three tests into a separate test unit and add a test title with the jira ID(SPARK-29600)?",
        "createdAt" : "2019-12-13T07:10:29Z",
        "updatedAt" : "2019-12-13T12:33:30Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "3dbb09f3-f825-4bba-a4d1-5a568b5cd6dc",
        "parentId" : "4fa6bea0-05e4-4215-9222-40c448eecd3a",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Also, can you update the title, too?",
        "createdAt" : "2019-12-13T07:10:45Z",
        "updatedAt" : "2019-12-13T12:33:30Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "49db9257-de46-47cc-99e2-dfab88f27aa7",
        "parentId" : "4fa6bea0-05e4-4215-9222-40c448eecd3a",
        "authorId" : "75554730-8183-463c-bf1d-7cd5b4e9f22b",
        "body" : "Sure. I'll update",
        "createdAt" : "2019-12-13T11:47:15Z",
        "updatedAt" : "2019-12-13T12:33:30Z",
        "lastEditedBy" : "75554730-8183-463c-bf1d-7cd5b4e9f22b",
        "tags" : [
        ]
      }
    ],
    "commit" : "7b12d6cf78d2228d36192e78cca5a12648c1db80",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +867,871 @@\n  test(\"SPARK-29600: ArrayContains function may return incorrect result for DecimalType\") {\n    checkAnswer(\n      sql(\"select array_contains(array(1.10), 1.1)\"),\n      Seq(Row(true))"
  },
  {
    "id" : "e050d552-de5b-4e64-9510-6b1cdd83ef53",
    "prId" : 26811,
    "prUrl" : "https://github.com/apache/spark/pull/26811#pullrequestreview-333262750",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a47ab5bd-4197-4a84-97f7-12a80e593e4a",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "why precision becomes 38 in this case?",
        "createdAt" : "2019-12-13T12:55:11Z",
        "updatedAt" : "2019-12-13T12:55:12Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "f4b52777-5ed3-4d84-9d45-e75014920264",
        "parentId" : "a47ab5bd-4197-4a84-97f7-12a80e593e4a",
        "authorId" : "75554730-8183-463c-bf1d-7cd5b4e9f22b",
        "body" : "https://github.com/apache/spark/blob/1fc353d51a62cb554e6af23dbc9a613e214e3af1/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/TypeCoercion.scala#L864-L869\r\nFor query `array_contains(array(1), .01234567890123456790123456780)`\\\r\n`e.inputTypes` will return `Seq(Array(Decimal(38,29)), Decimal(38,29))` and above code will cast `.01234567890123456790123456780` as `Decimal(38,29)`. \r\nPreviously, when we were using `findWiderTypeForTwo`, decimal types were not getting upcasted but `findWiderTypeWithoutStringPromotionForTwo` will successfully upcast DecimalType ",
        "createdAt" : "2019-12-16T07:01:53Z",
        "updatedAt" : "2019-12-16T07:01:53Z",
        "lastEditedBy" : "75554730-8183-463c-bf1d-7cd5b4e9f22b",
        "tags" : [
        ]
      },
      {
        "id" : "fefa0ae5-12a6-4eba-949d-65a5d20e78ba",
        "parentId" : "a47ab5bd-4197-4a84-97f7-12a80e593e4a",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "> Previously, when we were using findWiderTypeForTwo\r\n\r\nBefore this PR, we were using `findTightestCommonType`. Why do we add cast but still can't resolve `ArrayContains`?",
        "createdAt" : "2019-12-16T11:27:51Z",
        "updatedAt" : "2019-12-16T11:27:51Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "c8849085-6ecb-4987-b282-25d21e47a273",
        "parentId" : "a47ab5bd-4197-4a84-97f7-12a80e593e4a",
        "authorId" : "75554730-8183-463c-bf1d-7cd5b4e9f22b",
        "body" : "Do you mean why in above test case query, `ArrayContains` is throwing `AnalysisException` instead of casting integer to Decimal?\r\n\r\nAn integer cannot be casted to decimal with scale > 28.\r\n\r\n```\r\ndecimalWith28Zeroes = 1.0000000000000000000000000000\r\nSELECT array_contains(array(1), decimalWith28Zeroes);\r\nResult =>> true\r\n```\r\n\r\n```\r\ndecimalWith29Zeroes = 1.00000000000000000000000000000\r\nSELECT array_contains(array(1), decimalWith29Zeroes);\r\nResult =>> AnalysisException\r\n```",
        "createdAt" : "2019-12-17T06:15:41Z",
        "updatedAt" : "2019-12-17T06:15:42Z",
        "lastEditedBy" : "75554730-8183-463c-bf1d-7cd5b4e9f22b",
        "tags" : [
        ]
      },
      {
        "id" : "f15a3c1e-4dbf-4ab2-80c9-35c48104a807",
        "parentId" : "a47ab5bd-4197-4a84-97f7-12a80e593e4a",
        "authorId" : "75554730-8183-463c-bf1d-7cd5b4e9f22b",
        "body" : "cc @maropu @cloud-fan ",
        "createdAt" : "2019-12-17T06:17:18Z",
        "updatedAt" : "2019-12-17T06:17:19Z",
        "lastEditedBy" : "75554730-8183-463c-bf1d-7cd5b4e9f22b",
        "tags" : [
        ]
      },
      {
        "id" : "d277b42b-9d42-476b-9d2f-a4819940ac9a",
        "parentId" : "a47ab5bd-4197-4a84-97f7-12a80e593e4a",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Yea I get that we can't do cast here. My question is: since we can't do cast, we should leave the expression un-touched. But now we add cast to one side and leave the expression unresolved. Where do we add that useless cast?",
        "createdAt" : "2019-12-17T07:22:43Z",
        "updatedAt" : "2019-12-17T07:22:50Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "15e47a65-6796-45ba-8fb5-f2e136555cf5",
        "parentId" : "a47ab5bd-4197-4a84-97f7-12a80e593e4a",
        "authorId" : "75554730-8183-463c-bf1d-7cd5b4e9f22b",
        "body" : "https://github.com/apache/spark/blob/1fc353d51a62cb554e6af23dbc9a613e214e3af1/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/TypeCoercion.scala#L864-L869\r\n\r\nThis code is to cast left and right expression one by one. Here,\r\n- `e.childern` is `Seq( array<int>, decimal(29,29))`, and\r\n- `e.inputTypes` will return `Seq(array<decimal(38,29)>, decimal(38,29))`\r\n\r\n`impicitCast(array<int>, array<decimal(38,29)>)` will return `None`, since `int` can't be casted to `decimal(38,29)`.",
        "createdAt" : "2019-12-17T08:38:34Z",
        "updatedAt" : "2019-12-17T08:38:34Z",
        "lastEditedBy" : "75554730-8183-463c-bf1d-7cd5b4e9f22b",
        "tags" : [
        ]
      },
      {
        "id" : "508cdaf1-efe6-4d3f-843c-1359f797f789",
        "parentId" : "a47ab5bd-4197-4a84-97f7-12a80e593e4a",
        "authorId" : "75554730-8183-463c-bf1d-7cd5b4e9f22b",
        "body" : "Above code is creating new expression by updating only right child.",
        "createdAt" : "2019-12-17T08:40:34Z",
        "updatedAt" : "2019-12-17T08:40:34Z",
        "lastEditedBy" : "75554730-8183-463c-bf1d-7cd5b4e9f22b",
        "tags" : [
        ]
      },
      {
        "id" : "6d59fb26-edd5-44c3-809f-06f0c973fe2e",
        "parentId" : "a47ab5bd-4197-4a84-97f7-12a80e593e4a",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "ah thanks for finding this out!",
        "createdAt" : "2019-12-17T13:14:10Z",
        "updatedAt" : "2019-12-17T13:14:10Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "7b12d6cf78d2228d36192e78cca5a12648c1db80",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +851,855 @@      s\"\"\"\n         |Input to function array_contains should have been array followed by a\n         |value with same element type, but it's [array<int>, decimal(38,29)].\n       \"\"\".stripMargin.replace(\"\\n\", \" \").trim()\n    assert(e1.message.contains(errorMsg1))"
  },
  {
    "id" : "d6bb7299-28b6-44d9-b197-d61013beb49d",
    "prId" : 25728,
    "prUrl" : "https://github.com/apache/spark/pull/25728#pullrequestreview-288430596",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2d1600e3-e4a1-4b42-be42-56926f1f1102",
        "parentId" : null,
        "authorId" : "99e31844-a5b3-48a2-af71-412edb607a13",
        "body" : "nit: add a blank line before this line.",
        "createdAt" : "2019-09-16T02:57:38Z",
        "updatedAt" : "2019-11-17T19:07:26Z",
        "lastEditedBy" : "99e31844-a5b3-48a2-af71-412edb607a13",
        "tags" : [
        ]
      },
      {
        "id" : "59b61d09-2e23-42be-8937-0afb388b25ae",
        "parentId" : "2d1600e3-e4a1-4b42-be42-56926f1f1102",
        "authorId" : "5b9e802a-4319-492f-a0ba-8cdf41bed39a",
        "body" : "done",
        "createdAt" : "2019-09-16T06:22:05Z",
        "updatedAt" : "2019-11-17T19:07:26Z",
        "lastEditedBy" : "5b9e802a-4319-492f-a0ba-8cdf41bed39a",
        "tags" : [
        ]
      }
    ],
    "commit" : "ef28d4fd63d6fa71081035b45293a90fc1575687",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +313,317 @@  }\n\n  test(\"array_sort with lambda functions\") {\n\n    spark.udf.register(\"fAsc\", (x: Int, y: Int) => {"
  },
  {
    "id" : "7cfb5617-1a55-4d3a-9cd1-096ab75b8c6f",
    "prId" : 25728,
    "prUrl" : "https://github.com/apache/spark/pull/25728#pullrequestreview-293866479",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bfd24f71-1392-4bb7-bc72-f4fcfc65291a",
        "parentId" : null,
        "authorId" : "99e31844-a5b3-48a2-af71-412edb607a13",
        "body" : "nit: delete empty line",
        "createdAt" : "2019-09-26T16:56:09Z",
        "updatedAt" : "2019-11-17T19:07:26Z",
        "lastEditedBy" : "99e31844-a5b3-48a2-af71-412edb607a13",
        "tags" : [
        ]
      }
    ],
    "commit" : "ef28d4fd63d6fa71081035b45293a90fc1575687",
    "line" : 77,
    "diffHunk" : "@@ -1,1 +386,390 @@        Row(Seq(\"dc\", \"bc\", \"ab\", null)))\n    )\n\n    spark.sql(\"drop temporary function fAsc\")\n    spark.sql(\"drop temporary function fDesc\")"
  },
  {
    "id" : "aeddb14c-f13f-48c0-bfa1-787580d743ca",
    "prId" : 25728,
    "prUrl" : "https://github.com/apache/spark/pull/25728#pullrequestreview-304506642",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "21e69c51-7229-4a5e-a784-3e5d716792d1",
        "parentId" : null,
        "authorId" : "43998e22-6c2f-401d-9914-8cecf6fad929",
        "body" : "We should unregister those after the test.",
        "createdAt" : "2019-10-21T12:40:04Z",
        "updatedAt" : "2019-11-17T19:07:26Z",
        "lastEditedBy" : "43998e22-6c2f-401d-9914-8cecf6fad929",
        "tags" : [
        ]
      }
    ],
    "commit" : "ef28d4fd63d6fa71081035b45293a90fc1575687",
    "line" : 34,
    "diffHunk" : "@@ -1,1 +343,347 @@      else if (x.length == y.length) 0\n      else 1\n    })\n\n    val df1 = Seq(Array[Int](3, 2, 5, 1, 2)).toDF(\"a\")"
  }
]