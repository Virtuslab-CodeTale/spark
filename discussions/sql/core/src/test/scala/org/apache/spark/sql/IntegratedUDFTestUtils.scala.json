[
  {
    "id" : "fd7c47ae-4adc-4578-93f9-6a462230011d",
    "prId" : 29217,
    "prUrl" : "https://github.com/apache/spark/pull/29217#pullrequestreview-454973944",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fc09c813-669a-4082-939f-889da751d1d9",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Since this is `GitHub Action` specific issue, shall we use `export PYSPARK_PYTHON` in GitHub Action side?",
        "createdAt" : "2020-07-24T15:18:30Z",
        "updatedAt" : "2020-07-24T15:18:30Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "d72c4c12bf61aab4dfd93824cef801d7d93e7a4c",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +198,202 @@  lazy val pythonExec: String = {\n    val pythonExec = sys.env.getOrElse(\n      \"PYSPARK_DRIVER_PYTHON\", sys.env.getOrElse(\"PYSPARK_PYTHON\", \"python3\"))\n    if (TestUtils.testCommandAvailable(pythonExec)) {\n      pythonExec"
  },
  {
    "id" : "df5744a3-b06c-4882-8458-4eec5fe9af83",
    "prId" : 25130,
    "prUrl" : "https://github.com/apache/spark/pull/25130#pullrequestreview-261565809",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "464af520-aefa-4e9f-aa94-5351176d6a41",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Looks like I can even try to (de)serialize to/from JSON string for complex types to work around.\r\n\r\nBut I think it's an overkill for now. So far, there look not many tests that use complex types for UDF test cases. I will add it later if we see some values on that.",
        "createdAt" : "2019-07-14T05:17:10Z",
        "updatedAt" : "2019-07-17T04:07:14Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "720eea7bbd95c74065add08f6195f78d355d7e3a",
    "line" : 183,
    "diffHunk" : "@@ -1,1 +326,330 @@        assert(expr.resolved, \"column should be resolved to use the same type \" +\n          \"as input. Try df(name) or df.col(name)\")\n        Column(Cast(createScalaUDF(Cast(expr, StringType) :: Nil), expr.dataType))\n      }\n    }"
  },
  {
    "id" : "a255ccc0-f747-4c3e-b959-72a321696e39",
    "prId" : 25130,
    "prUrl" : "https://github.com/apache/spark/pull/25130#pullrequestreview-261619560",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "58df21d3-0d17-4cd7-af41-b7ef66969547",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "BTW, I think case to case inheritance is forbidden but regular class to case is fine. I don't think this is a good practice but at least affected scope is only tests and it's minimised change. So I guess it's fine.",
        "createdAt" : "2019-07-15T03:04:26Z",
        "updatedAt" : "2019-07-17T04:07:14Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "720eea7bbd95c74065add08f6195f78d355d7e3a",
    "line" : 78,
    "diffHunk" : "@@ -1,1 +223,227 @@   */\n  case class TestPythonUDF(name: String) extends TestUDF {\n    private[IntegratedUDFTestUtils] lazy val udf = new UserDefinedPythonFunction(\n      name = name,\n      func = PythonFunction("
  },
  {
    "id" : "eeb76c2f-4188-4397-a710-fde06492042d",
    "prId" : 24945,
    "prUrl" : "https://github.com/apache/spark/pull/24945#pullrequestreview-253749141",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "db49b056-cbf6-4c9c-a218-bfe615944639",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "This is for IDE case. `spark.test.home` can be missing if we run the tests in IDE without any other settings. In that case, it falls back to `SPARK_HOME`.",
        "createdAt" : "2019-06-24T03:26:35Z",
        "updatedAt" : "2019-06-24T16:11:44Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "d7df3d30-2bed-4b34-abc5-67c5d9191d5f",
        "parentId" : "db49b056-cbf6-4c9c-a218-bfe615944639",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Should we add a comment for this reason?",
        "createdAt" : "2019-06-25T02:31:11Z",
        "updatedAt" : "2019-06-25T02:31:11Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "1ca8fedd-ebda-4959-a62e-97fa07d43b7e",
        "parentId" : "db49b056-cbf6-4c9c-a218-bfe615944639",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "oops, I missed this. Actually there are multiple places like this. Let me fix them together later separately.",
        "createdAt" : "2019-06-25T03:00:02Z",
        "updatedAt" : "2019-06-25T03:00:02Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "8fe247435ff278f62eec39cdcdc0c20d07af31f9",
    "line" : 40,
    "diffHunk" : "@@ -1,1 +71,75 @@    assert(sys.props.contains(\"spark.test.home\") ||\n      sys.env.contains(\"SPARK_HOME\"), \"spark.test.home or SPARK_HOME is not set.\")\n    sys.props.getOrElse(\"spark.test.home\", sys.env(\"SPARK_HOME\"))\n  } else {\n    assert(sys.env.contains(\"SPARK_HOME\"), \"SPARK_HOME is not set.\")"
  },
  {
    "id" : "eb902004-61f0-4082-a024-0213e03502fc",
    "prId" : 24945,
    "prUrl" : "https://github.com/apache/spark/pull/24945#pullrequestreview-253512412",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f8b27fdb-17fa-493e-b886-cec03294e18b",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Will we use it? In `SQLQueryTestSuite`,  I think udfs are all registered for `UDFTestCase`?",
        "createdAt" : "2019-06-24T15:35:24Z",
        "updatedAt" : "2019-06-24T16:11:44Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "0ef956fb-22b1-4e5a-ae24-08d873bccf55",
        "parentId" : "f8b27fdb-17fa-493e-b886-cec03294e18b",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Ah this one will be used at #24946",
        "createdAt" : "2019-06-24T16:08:41Z",
        "updatedAt" : "2019-06-24T16:11:44Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "8fe247435ff278f62eec39cdcdc0c20d07af31f9",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +61,65 @@ *   sql(\"SELECT udf_name(1)\")\n *   spark.range(10).select(expr(\"udf_name(id)\")\n *   spark.range(10).select(pandasTestUDF($\"id\"))\n * }}}\n */"
  },
  {
    "id" : "b8ffa6d6-32b4-4a5e-97c3-a543119c27a6",
    "prId" : 24752,
    "prUrl" : "https://github.com/apache/spark/pull/24752#pullrequestreview-248571171",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "12f76313-3688-4aef-8b7c-b9dc89eca8a4",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "do we need to update this when we upgrade py4j in Spark?",
        "createdAt" : "2019-06-12T06:28:37Z",
        "updatedAt" : "2019-06-12T06:28:37Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "7d015d01-36ae-46df-bcf8-e9f7fe50eed3",
        "parentId" : "12f76313-3688-4aef-8b7c-b9dc89eca8a4",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Yes. @cloud-fan . And, it's already replaced with `PY4J_ZIP_NAME` at 8b18ef5c7b6 .",
        "createdAt" : "2019-06-12T06:39:53Z",
        "updatedAt" : "2019-06-12T06:39:54Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "bf251160-19a5-4622-9ec3-7c9c1692b05e",
        "parentId" : "12f76313-3688-4aef-8b7c-b9dc89eca8a4",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "cool, thanks!",
        "createdAt" : "2019-06-12T06:45:37Z",
        "updatedAt" : "2019-06-12T06:45:37Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "d673703b58087e2aa2a1daa764fa46de25b94c8b",
    "line" : 77,
    "diffHunk" : "@@ -1,1 +75,79 @@  private lazy val sourcePath = Paths.get(sparkHome, \"python\").toAbsolutePath\n  private lazy val py4jPath = Paths.get(\n    sparkHome, \"python\", \"lib\", \"py4j-0.10.8.1-src.zip\").toAbsolutePath\n  private lazy val pysparkPythonPath = s\"$py4jPath:$sourcePath\"\n"
  }
]