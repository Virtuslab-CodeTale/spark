[
  {
    "id" : "8caa020f-c221-480a-a971-e0cd3aac356c",
    "prId" : 29660,
    "prUrl" : "https://github.com/apache/spark/pull/29660#pullrequestreview-483655276",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "13452378-85fb-462c-b67c-8f3de73b1e9f",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Is this change part of the PR on purpose?",
        "createdAt" : "2020-09-07T15:23:01Z",
        "updatedAt" : "2020-09-09T03:02:25Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "2be67189-fcb3-4319-8324-6cf8944a99c0",
        "parentId" : "13452378-85fb-462c-b67c-8f3de73b1e9f",
        "authorId" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "body" : "> Add a manual sort to classFunsMap in ExpressionsSchemaSuite because Iterable.groupBy in Scala 2.13 has different result with TraversableLike.groupBy in Scala 2.12\r\n\r\nYes,  Because the change of `ExpressionsSchemaSuite`, this file need regenerate.",
        "createdAt" : "2020-09-07T15:54:30Z",
        "updatedAt" : "2020-09-09T03:02:25Z",
        "lastEditedBy" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "tags" : [
        ]
      },
      {
        "id" : "f9c07ae0-7d51-49d6-b89a-a4184fe35316",
        "parentId" : "13452378-85fb-462c-b67c-8f3de73b1e9f",
        "authorId" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "body" : "Code changed from \r\n```\r\nval classFunsMap = funInfos.groupBy(_.getClassName).toSeq.sortBy(_._1)\r\n```\r\nto\r\n```\r\nval classFunsMap = funInfos.groupBy(_.getClassName).toSeq.sortBy(_._1).map {\r\n      case (className, infos) => (className, infos.sortBy(_.getName))\r\n}\r\n```\r\nin `ExpressionsSchemaSuite`",
        "createdAt" : "2020-09-07T16:20:16Z",
        "updatedAt" : "2020-09-09T03:02:25Z",
        "lastEditedBy" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "tags" : [
        ]
      }
    ],
    "commit" : "9185a95c29bc532e9f0aea5bdd4f2185c5093642",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +3,7 @@  - Number of queries: 339\n  - Number of expressions that missing example: 34\n  - Expressions missing examples: and,bigint,binary,boolean,date,decimal,double,float,int,smallint,string,timestamp,tinyint,struct,cume_dist,dense_rank,input_file_block_length,input_file_block_start,input_file_name,lag,lead,monotonically_increasing_id,ntile,!,not,or,percent_rank,rank,row_number,spark_partition_id,version,window,positive,count_min_sketch\n## Schema of Built-in Functions\n| Class name | Function name or alias | Query example | Output schema |"
  },
  {
    "id" : "4a170fdc-f13d-45ef-9b12-7b13a8c2e21f",
    "prId" : 28764,
    "prUrl" : "https://github.com/apache/spark/pull/28764#pullrequestreview-452124812",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c6df5320-202f-41f8-be10-6f1668dde742",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "The output schema is `widthbucket`? It should be `width_bucket ` if we define the prettyName to `width_bucket`?",
        "createdAt" : "2020-07-21T02:41:13Z",
        "updatedAt" : "2020-07-21T02:41:13Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "0163c862-5e59-41a8-b708-255f57eb25b3",
        "parentId" : "c6df5320-202f-41f8-be10-6f1668dde742",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "yea, it looks better. I'll fix it later.",
        "createdAt" : "2020-07-21T04:53:25Z",
        "updatedAt" : "2020-07-21T04:53:25Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "9e8eab25eeffba0260d1a98a199f832251809c1d",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +292,296 @@| org.apache.spark.sql.catalyst.expressions.WeekDay | weekday | SELECT weekday('2009-07-30') | struct<weekday(CAST(2009-07-30 AS DATE)):int> |\n| org.apache.spark.sql.catalyst.expressions.WeekOfYear | weekofyear | SELECT weekofyear('2008-02-20') | struct<weekofyear(CAST(2008-02-20 AS DATE)):int> |\n| org.apache.spark.sql.catalyst.expressions.WidthBucket | width_bucket | SELECT width_bucket(5.3, 0.2, 10.6, 5) | struct<widthbucket(CAST(5.3 AS DOUBLE), CAST(0.2 AS DOUBLE), CAST(10.6 AS DOUBLE), CAST(5 AS BIGINT)):bigint> |\n| org.apache.spark.sql.catalyst.expressions.XxHash64 | xxhash64 | SELECT xxhash64('Spark', array(123), 2) | struct<xxhash64(Spark, array(123), 2):bigint> |\n| org.apache.spark.sql.catalyst.expressions.Year | year | SELECT year('2016-07-30') | struct<year(CAST(2016-07-30 AS DATE)):int> |"
  }
]