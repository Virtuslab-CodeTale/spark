[
  {
    "id" : "cb7eae75-54e1-4b87-94fe-6290bd50083f",
    "prId" : 32995,
    "prUrl" : "https://github.com/apache/spark/pull/32995#pullrequestreview-688654745",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b54b872c-3f11-47d3-92f1-524108dc936b",
        "parentId" : null,
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "I will have a follow up to improve the error message.\r\nThis error message is used in multiple SQL output files and some of them are not related to this PR.",
        "createdAt" : "2021-06-21T07:47:19Z",
        "updatedAt" : "2021-06-21T07:47:25Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "2d4f1be1-e8ce-48fe-a66a-73ccfdd88b35",
        "parentId" : "b54b872c-3f11-47d3-92f1-524108dc936b",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "good point. The new function should bypass this upgrade check in the timestamp formatter.",
        "createdAt" : "2021-06-21T16:40:34Z",
        "updatedAt" : "2021-06-21T16:40:34Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "d5afd6abdb6eb56e91b2d95b0fecad6468d2a64b",
    "line" : 323,
    "diffHunk" : "@@ -1,1 +1221,1225 @@-- !query output\norg.apache.spark.SparkUpgradeException\nYou may get a different result due to the upgrading of Spark 3.0: Fail to recognize 'yyyy-MM-dd GGGGG' pattern in the DateTimeFormatter. 1) You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0. 2) You can form a valid datetime pattern with the guide from https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n\n"
  },
  {
    "id" : "413f7190-5fc6-403a-a654-38f1dd7d2c18",
    "prId" : 32099,
    "prUrl" : "https://github.com/apache/spark/pull/32099#pullrequestreview-633290737",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "19658e9d-a2a1-4749-87f7-092a71f50a5c",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Have we finalized the SQL name for the new interval types? How about other databases?",
        "createdAt" : "2021-04-12T08:44:44Z",
        "updatedAt" : "2021-04-12T08:44:44Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "7268cadf-0bf7-4b42-b11f-26c1a1009903",
        "parentId" : "19658e9d-a2a1-4749-87f7-092a71f50a5c",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Not yet. So far, I took the names for the sub-types from the SQL standard, see https://github.com/apache/spark/pull/31810\r\n\r\nProbably, we will need to re-define them when we will implement parsing of interval types from SQL.",
        "createdAt" : "2021-04-12T08:52:52Z",
        "updatedAt" : "2021-04-12T08:52:53Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "77248bee7f90e0061b15472f1a3e5b69bfd4ff10",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +360,364 @@select date'2020-01-01' - timestamp'2019-10-06 10:11:12.345678'\n-- !query schema\nstruct<subtracttimestamps(DATE '2020-01-01', TIMESTAMP '2019-10-06 10:11:12.345678'):day-time interval>\n-- !query output\n86 13:48:47.654322000"
  }
]