[
  {
    "id" : "2f7c1ab0-b874-40cf-9f5a-23b514d74b16",
    "prId" : 32209,
    "prUrl" : "https://github.com/apache/spark/pull/32209#pullrequestreview-638528466",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e7de3c08-f299-4ffa-bd6f-987fa6724ad3",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "why does the data type change?",
        "createdAt" : "2021-04-19T07:14:26Z",
        "updatedAt" : "2021-04-19T08:09:08Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "295c1165-714e-476d-b96b-5c6d0ee1447b",
        "parentId" : "e7de3c08-f299-4ffa-bd6f-987fa6724ad3",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Ah I see, we turn on the legacy config to match pgsql behavior.",
        "createdAt" : "2021-04-19T07:14:58Z",
        "updatedAt" : "2021-04-19T08:09:08Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "790815029a4072346ac38c57737b98a619080aa0",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +483,487 @@SELECT f1 - date '2000-01-01' AS `Days From 2K` FROM DATE_TBL\n-- !query schema\nstruct<Days From 2K:interval>\n-- !query output\n-2 years -10 months"
  },
  {
    "id" : "64b98a9d-f296-4217-acf5-38142c0a1834",
    "prId" : 28582,
    "prUrl" : "https://github.com/apache/spark/pull/28582#pullrequestreview-415377097",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "db65b740-5c36-4389-b6c8-31398d61e1f4",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "This is compatible with Spark 2.4 because it uses SimpleDateFormat which cannot format negative years. ",
        "createdAt" : "2020-05-20T14:10:53Z",
        "updatedAt" : "2020-05-20T14:10:53Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "31e8ffdcedfc417909f842f469c3e86889a8af42",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +585,589 @@struct<make_date(-44, 3, 15):date>\n-- !query output\n0045-03-15\n\n"
  },
  {
    "id" : "fa750f4d-f3ae-4049-a71f-d170eda1f9d0",
    "prId" : 27494,
    "prUrl" : "https://github.com/apache/spark/pull/27494#pullrequestreview-355898551",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "42702541-89cd-465a-8b61-72b5a9c74548",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "I was going to ask if there are any behavior changes. This is one, but it looks like it was wrong before?\r\n\r\nAre there any others that we know of? Given recent discussions I'm extra cautious about introducing behavior changes I suppose.",
        "createdAt" : "2020-02-09T16:03:58Z",
        "updatedAt" : "2020-02-12T12:44:38Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "e1d0afdf-338a-44e6-a792-cbe0a4462569",
        "parentId" : "42702541-89cd-465a-8b61-72b5a9c74548",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "This had been already described in the SQL migration guide. The numbers are different because internally we use Proleptic Gregorian calendar in `date_trunc` but when we collect data to the driver, we convert internal timestamps to `java.sql.Timestamp`. Actually, we switch to Julian calendar for old dates here.\r\n\r\nBy settings `localSparkSession.conf.set(SQLConf.DATETIME_JAVA8API_ENABLED.key, true)` in `SQLQueryTestSuite.scala`, I just keep the same calendar in all transformations.",
        "createdAt" : "2020-02-09T16:18:43Z",
        "updatedAt" : "2020-02-12T12:44:38Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "e724b7c6-e746-4ab7-81b0-216284816973",
        "parentId" : "42702541-89cd-465a-8b61-72b5a9c74548",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "This is caused by https://github.com/apache/spark/pull/27494/files#diff-432455394ca50800d5de508861984ca5R340 , not the fix itself, right?\r\n\r\nDo we have any known behavior changes caused by this fix?",
        "createdAt" : "2020-02-10T12:16:51Z",
        "updatedAt" : "2020-02-12T12:44:38Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "e4542d55-6103-46e1-b122-496890eb2a71",
        "parentId" : "42702541-89cd-465a-8b61-72b5a9c74548",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Yes, the change in this particular line is caused by `localSparkSession.conf.set(SQLConf.DATETIME_JAVA8API_ENABLED.key, true)`. But I had to set the settings to fix another test failures like in https://github.com/apache/spark/pull/27494#issuecomment-583675754",
        "createdAt" : "2020-02-10T12:22:59Z",
        "updatedAt" : "2020-02-12T12:44:38Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "1ca59aa7-34cf-4b5d-a4bc-c989818d175d",
        "parentId" : "42702541-89cd-465a-8b61-72b5a9c74548",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Do you think it's helpful to do https://github.com/apache/spark/pull/27494#discussion_r376988480 first and then we can know what gets changed exactly by this fix?",
        "createdAt" : "2020-02-10T12:29:28Z",
        "updatedAt" : "2020-02-12T12:44:38Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "e5d463f67d68efcff046851479b4789d1e4f50a0",
    "line" : 50,
    "diffHunk" : "@@ -1,1 +873,877 @@struct<date_trunc(DECADE, CAST(to_date('0002-12-31 BC', 'yyyy-MM-dd G') AS TIMESTAMP)):timestamp>\n-- !query output\n-0010-01-01 00:00:00\n\n"
  }
]