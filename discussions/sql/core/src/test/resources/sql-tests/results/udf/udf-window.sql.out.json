[
  {
    "id" : "e7b9b50b-c272-4ffc-b99a-5f56f33cf060",
    "prId" : 28821,
    "prUrl" : "https://github.com/apache/spark/pull/28821#pullrequestreview-442611129",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cb899f33-61e8-41cb-adca-34b818a6afdb",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Hm .. so we allow timestamp types whereas other DBMSes disallow?",
        "createdAt" : "2020-06-26T07:35:29Z",
        "updatedAt" : "2020-07-02T11:58:14Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "8daf32d7-1705-4e61-bb8f-46f10a81c202",
        "parentId" : "cb899f33-61e8-41cb-adca-34b818a6afdb",
        "authorId" : "e29a7794-ac2e-4e6f-a690-737e83e1bace",
        "body" : "Well, there isn't a real consensus around other DBMS'es. Keeping it a Timestamp seems like something that you would expect. MySQL's behavior is much more awkward in my opinion. Spark needs to pave the path on this one :)",
        "createdAt" : "2020-06-30T09:58:05Z",
        "updatedAt" : "2020-07-02T11:58:14Z",
        "lastEditedBy" : "e29a7794-ac2e-4e6f-a690-737e83e1bace",
        "tags" : [
        ]
      },
      {
        "id" : "a817281b-4862-484c-9c50-c61032df50df",
        "parentId" : "cb899f33-61e8-41cb-adca-34b818a6afdb",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "It seems Oracle cannot accept timestamp for average, too. Any other system supporting this behaivour?",
        "createdAt" : "2020-07-04T13:52:41Z",
        "updatedAt" : "2020-07-04T13:52:41Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "3772374ed915ff57045107aecff5e52e70719953",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +165,169 @@2017-07-31 17:00:00\tb\t2017-08-09 03:00:00\n2017-08-17 13:00:00\tb\t2017-08-17 13:00:00\n2020-12-30 16:00:00\tb\t2020-12-30 16:00:00\n\n"
  },
  {
    "id" : "5d3e0873-e5ea-4f36-b692-4f833341babe",
    "prId" : 25195,
    "prUrl" : "https://github.com/apache/spark/pull/25195#pullrequestreview-266232519",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "57be5845-e01d-4395-9864-7a7123b70558",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Hmm ...\r\n\r\n```diff\r\n -- !query 3\r\n-SELECT val_long, cate, sum(val_long) OVER(PARTITION BY cate ORDER BY val_long\r\n-ROWS BETWEEN CURRENT ROW AND 2147483648 FOLLOWING) FROM testData ORDER BY cate, val_long\r\n+SELECT val_long, udf(cate), sum(val_long) OVER(PARTITION BY cate ORDER BY val_long\r\n+ROWS BETWEEN CURRENT ROW AND CAST(2147483648 AS int) FOLLOWING) FROM testData ORDER BY cate, val_long\r\n -- !query 3 schema\r\n-struct<>\r\n+struct<val_long:bigint,CAST(udf(cast(cate as string)) AS STRING):string,sum(val_long) OVER (PARTITION BY cate ORDER BY val_long ASC NULLS FIRST ROWS BETWEEN CURRENT ROW AND CAST(2147483648 AS INT) FOLLOWING):bigint>\r\n -- !query 3 output\r\n-org.apache.spark.sql.AnalysisException\r\n-cannot resolve 'ROWS BETWEEN CURRENT ROW AND 2147483648L FOLLOWING' due to data type mismatch: The data type of the upper bound 'bigint' does not match the expected data type 'int'.; line 1 pos 41\r\n+NULL   NULL    1\r\n+1      NULL    1\r\n+1      a       2147483654\r\n+1      a       2147483653\r\n+2      a       2147483652\r\n+2147483650     a       2147483650\r\n+NULL   b       2147483653\r\n+3      b       2147483653\r\n+2147483650     b       2147483650\r\n```\r\n\r\nDo you know why this works when it's wrapped by udf?",
        "createdAt" : "2019-07-24T00:49:33Z",
        "updatedAt" : "2019-07-24T19:11:09Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "fe456fab-8c62-48d8-91f8-6b93669d849c",
        "parentId" : "57be5845-e01d-4395-9864-7a7123b70558",
        "authorId" : "4e84a572-ebe7-42c7-ba1f-55b902bd71ee",
        "body" : "as I put CAST function:\r\nCAST(2147483648 AS int)\r\n\r\nbut I put it back to 2147483648 ",
        "createdAt" : "2019-07-24T19:18:31Z",
        "updatedAt" : "2019-07-24T19:18:31Z",
        "lastEditedBy" : "4e84a572-ebe7-42c7-ba1f-55b902bd71ee",
        "tags" : [
        ]
      }
    ],
    "commit" : "e6ce51a7a8039b14326605adb4b3c57fad9557ef",
    "line" : 57,
    "diffHunk" : "@@ -1,1 +55,59 @@\n\n-- !query 3\nSELECT val_long, udf(cate), sum(val_long) OVER(PARTITION BY cate ORDER BY udf(val_long)\nROWS BETWEEN CURRENT ROW AND 2147483648 FOLLOWING) FROM testData ORDER BY udf(cate), val_long"
  }
]