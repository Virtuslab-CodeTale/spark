[
  {
    "id" : "72802070-1e55-4ed2-a27c-d300653dcd73",
    "prId" : 29259,
    "prUrl" : "https://github.com/apache/spark/pull/29259#pullrequestreview-456913239",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b7c162e9-19da-4956-bfde-ed3cae6defbf",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "> For example, one of the tests does a comparison between\r\nLong.MIN_VALUE and 1 in order to trigger an overflow condition that\r\nexisted in the past (i.e. Long.MIN_VALUE - 1). These constants\r\ncorrespond to the values 0x80..00 and 0x00..01. However on a\r\nlittle-endian machine the bytes in these values are now swapped\r\nbefore they are compared. This means that we will now be comparing\r\n0x00..80 with 0x01..00. 0x00..80 - 0x01..00 does not overflow\r\ntherefore missing the original purpose of the test.\r\n\r\nI'm a bit confused by the PR description; I checked the original PR that added this test case and it seems like the overflow in the test title comes from the old code: https://github.com/apache/spark/pull/22101/files#diff-4ec35a60ad6a3f3f60f4d5ce91f59933L61-L63 To keep the original intention, why do you think we need to update the existing test in little-endian cases?",
        "createdAt" : "2020-07-28T03:57:23Z",
        "updatedAt" : "2020-08-05T11:20:08Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "103b036c-e8f0-4f90-83ee-b89261984a3b",
        "parentId" : "b7c162e9-19da-4956-bfde-ed3cae6defbf",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "If you mean, this is kind of a different issue -- yes. Should be a new JIRA. I'd summarize this as: the bytes that this test sets up and asserts about are different on big-endian. It creates the wrong test.",
        "createdAt" : "2020-07-28T17:57:27Z",
        "updatedAt" : "2020-08-05T11:20:08Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "921a03a0-ad1b-43ed-ab21-3a55daeb8870",
        "parentId" : "b7c162e9-19da-4956-bfde-ed3cae6defbf",
        "authorId" : "99e31844-a5b3-48a2-af71-412edb607a13",
        "body" : "IIUC, the intention of this test is to perform the comparison at `compare()` regardless endianness.   \r\n \r\nFor people, would it be good to add byte pattern to be tested.\r\n```\r\n80 00 00 00 00 00 00 00\r\n00 00 00 00 00 00 00 01\r\n```",
        "createdAt" : "2020-07-28T18:49:08Z",
        "updatedAt" : "2020-08-05T11:20:08Z",
        "lastEditedBy" : "99e31844-a5b3-48a2-af71-412edb607a13",
        "tags" : [
        ]
      }
    ],
    "commit" : "5d2956015cbe2d0f73650fd2f1f369eaebbee114",
    "line" : 65,
    "diffHunk" : "@@ -1,1 +313,317 @@    long row1Data = Long.MIN_VALUE;\n    long row2Data = 1L;\n    if (ByteOrder.nativeOrder().equals(ByteOrder.LITTLE_ENDIAN)) {\n      row1Data = Long.reverseBytes(row1Data);\n      row2Data = Long.reverseBytes(row2Data);"
  },
  {
    "id" : "18ce9200-d9fd-42b0-bc2b-2313b494f053",
    "prId" : 29259,
    "prUrl" : "https://github.com/apache/spark/pull/29259#pullrequestreview-460758191",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9519290a-1a04-42fb-b80f-6bd7910b6e1f",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "My understanding is: the comparator compares bytes from left to right. It doesn't assume the byte ordering of the data. It's expected that different byte order leads to different comparison results.\r\n\r\nI think a simple way to fix the test is:\r\n```\r\nif (LITTLE_ENDIAN) {\r\n  Assert.assertTrue(compare(0, 1) < 0);\r\n} else {\r\n  Assert.assertTrue(compare(0, 1) > 0);\r\n}\r\n```",
        "createdAt" : "2020-08-04T05:57:42Z",
        "updatedAt" : "2020-08-05T11:20:08Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "7d60a88e-47df-4c28-be5d-2a133ac1f1a7",
        "parentId" : "9519290a-1a04-42fb-b80f-6bd7910b6e1f",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "That would also work here. I think it's a little less ideal, because then you are running a different test on little- vs big-endian (and those are the correct answers to the two different tests). I think only one of those tests was the intended one AFAICT, and I buy the argument that the proposed change _restores_ it as well as addresses endianness.",
        "createdAt" : "2020-08-04T11:54:43Z",
        "updatedAt" : "2020-08-05T11:20:08Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "d7778336-2e44-4c46-8963-05ced7f62ed0",
        "parentId" : "9519290a-1a04-42fb-b80f-6bd7910b6e1f",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "> then you are running a different test on little- vs big-endian\r\n\r\nYea it's true. But it's also true that this is what happens in the reality: the comparator compares bytes from left to right, no matter it's little- or big-endian.",
        "createdAt" : "2020-08-04T12:09:41Z",
        "updatedAt" : "2020-08-05T11:20:08Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "e9312408-863f-45cb-9d17-5f5ce271675f",
        "parentId" : "9519290a-1a04-42fb-b80f-6bd7910b6e1f",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "That's right. There is no endian-ness issue in the logic being tested. There is endian-ness in how this test constructs the bytes to compare, because it writes a long into memory. I doubt this test's purpose holds _both_ ways? It \"works\" but either little- or big-endian machines aren't running the test as intended. It feels like there's one intended test to run here, not either of two.",
        "createdAt" : "2020-08-04T12:19:00Z",
        "updatedAt" : "2020-08-05T11:20:08Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      }
    ],
    "commit" : "5d2956015cbe2d0f73650fd2f1f369eaebbee114",
    "line" : 46,
    "diffHunk" : "@@ -1,1 +294,298 @@    insertRow(row2);\n\n    Assert.assertTrue(compare(0, 1) < 0);\n  }\n"
  },
  {
    "id" : "c673c405-6ca2-42a8-8558-cf923e62efd7",
    "prId" : 29259,
    "prUrl" : "https://github.com/apache/spark/pull/29259#pullrequestreview-461814127",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f89009ca-5655-452a-b37f-c35f1fc88fbf",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "My worry is that: this changes what we were testing before. When we wrote the test, we assume the test is run in little-endian machines. Now we reverse the bytes for little-endian machines, and as a result the testing result also changes (`result > 0` -> `result < 0`).\r\n\r\nI'm not very sure about the intention of these 2 tests. Maybe they were wrong at the very beginning and we should reverse the bytes for testing. I'll leave it for others to decide.",
        "createdAt" : "2020-08-05T14:43:22Z",
        "updatedAt" : "2020-08-05T14:43:22Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "4381d342-c506-4774-91e8-2cbc577fe442",
        "parentId" : "f89009ca-5655-452a-b37f-c35f1fc88fbf",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "That is perhaps the only remaining issue. We should fix the test that this sets up of course, but, which was the intended test? I'm also not 100% sure, but @mundaym had a reasonable argument at https://github.com/apache/spark/pull/29259#issuecomment-664480885",
        "createdAt" : "2020-08-05T15:11:38Z",
        "updatedAt" : "2020-08-05T15:11:39Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "24e3e8af-9a04-4ac9-99fa-fc12bbc12c45",
        "parentId" : "f89009ca-5655-452a-b37f-c35f1fc88fbf",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "ah I see, LGTM",
        "createdAt" : "2020-08-05T16:10:27Z",
        "updatedAt" : "2020-08-05T16:10:27Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "5d2956015cbe2d0f73650fd2f1f369eaebbee114",
    "line" : 25,
    "diffHunk" : "@@ -1,1 +276,280 @@    long row1Data = 11L;\n    long row2Data = 11L + Integer.MAX_VALUE;\n    if (ByteOrder.nativeOrder().equals(ByteOrder.LITTLE_ENDIAN)) {\n      row1Data = Long.reverseBytes(row1Data);\n      row2Data = Long.reverseBytes(row2Data);"
  },
  {
    "id" : "96d24bc3-637c-4890-a648-53ed3d90a6e5",
    "prId" : 26548,
    "prUrl" : "https://github.com/apache/spark/pull/26548#pullrequestreview-317892388",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "804290a8-f50e-4a9f-99c2-631f60bbf5b8",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "So, do you mean this is wrong before this PR?",
        "createdAt" : "2019-11-15T18:28:41Z",
        "updatedAt" : "2019-11-19T03:02:40Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "0b342ae3-edb2-43f7-a563-0f484d627bda",
        "parentId" : "804290a8-f50e-4a9f-99c2-631f60bbf5b8",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "The change definitely changes the ordering, as bytes are compared in a different order.",
        "createdAt" : "2019-11-15T20:36:46Z",
        "updatedAt" : "2019-11-19T03:02:40Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "d7d73da2-41ed-47c9-91c9-a5f54e1cdc8a",
        "parentId" : "804290a8-f50e-4a9f-99c2-631f60bbf5b8",
        "authorId" : "31f9c2ba-7775-44e4-b6fa-5e9ab26c8dff",
        "body" : "> So, do you mean this is wrong before this PR?\r\n\r\n`RecordBinaryComparator` is used as a local sort comparator in `RoundRobinPartition` to make sure the order of each records are the same after rerun. But the *relative order* is not important. ",
        "createdAt" : "2019-11-15T21:59:47Z",
        "updatedAt" : "2019-11-19T03:02:40Z",
        "lastEditedBy" : "31f9c2ba-7775-44e4-b6fa-5e9ab26c8dff",
        "tags" : [
        ]
      }
    ],
    "commit" : "753883d7f84552c7392295aa42f5023c0909c615",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +275,279 @@    insertRow(row2);\n\n    assert(compare(0, 1) > 0);\n  }\n"
  },
  {
    "id" : "4c5e443b-8e29-4ede-b01a-a408d2ad9a29",
    "prId" : 26548,
    "prUrl" : "https://github.com/apache/spark/pull/26548#pullrequestreview-318498611",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2002e256-6d6b-4d6f-ad82-0648a1cff10f",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "I think several of these lines are too long (> 100 chars)",
        "createdAt" : "2019-11-18T17:20:41Z",
        "updatedAt" : "2019-11-19T03:02:40Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      }
    ],
    "commit" : "753883d7f84552c7392295aa42f5023c0909c615",
    "line" : 53,
    "diffHunk" : "@@ -1,1 +355,359 @@    Platform.putLong(arr2, arrayOffset + 4, 0x0000000000000000L);\n    // both leftBaseOffset and rightBaseOffset are aligned, so it will start by comparing long\n    int result1 = binaryComparator.compare(arr1, arrayOffset + 4, 8, arr2, arrayOffset + 4, 8);\n\n    long[] arr3 = new long[2];"
  },
  {
    "id" : "1abf08c8-17d4-4d21-9a26-58d6dee85279",
    "prId" : 26548,
    "prUrl" : "https://github.com/apache/spark/pull/26548#pullrequestreview-328558825",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e66918aa-503a-4ced-ac42-b0002ec6bc6a",
        "parentId" : null,
        "authorId" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "body" : "Do we need to consider the uaoSize here?",
        "createdAt" : "2019-12-07T00:49:37Z",
        "updatedAt" : "2019-12-07T00:49:37Z",
        "lastEditedBy" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "tags" : [
        ]
      },
      {
        "id" : "c5ad08e5-6f51-4cb6-b2fc-9f9ab905efc2",
        "parentId" : "e66918aa-503a-4ced-ac42-b0002ec6bc6a",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Hm, good question. I don't think it comes up here as it's just messing with an `long[]` that was allocated normally? rather than data we wrote into an arbitrary location. But I may not be fully aware of the issue you have in mind.",
        "createdAt" : "2019-12-07T14:21:00Z",
        "updatedAt" : "2019-12-07T14:21:00Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      }
    ],
    "commit" : "753883d7f84552c7392295aa42f5023c0909c615",
    "line" : 39,
    "diffHunk" : "@@ -1,1 +341,345 @@    Platform.putLong(arr4, arrayOffset, 0x0000000000000001L);\n    // both left and right offset is not aligned, it will start with byte-by-byte comparison\n    int result2 = binaryComparator.compare(arr3, arrayOffset, 8, arr4, arrayOffset, 8);\n\n    Assert.assertEquals(result1, result2);"
  },
  {
    "id" : "e27d16e5-a87a-4117-9897-34606d1769f2",
    "prId" : 26548,
    "prUrl" : "https://github.com/apache/spark/pull/26548#pullrequestreview-334345333",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6b6aac3d-42cd-4d1c-af21-2f9f8ae8c143",
        "parentId" : null,
        "authorId" : "3a12ce0f-9e73-4cfb-a4b4-b19368cddc2f",
        "body" : "The test cases in this PR look problematic: the `long arrayOffset = 12;` is hard coded with implicit assumption on the underlying JVM's object layout in memory.\r\nThe object layout isn't guaranteed to be the same among different JVM, and in fact can be different even with the same JVM on the same machine but just different VM options.\r\n\r\nIt's preferable to use `Platform.LONG_ARRAY_OFFSET` as your mental model of where \"0-offset\" is. You should never write anything into the offset range `[0, Platform.LONG_ARRAY_OFFSET[` because there's no guarantee what's there.\r\ne.g. on the HotSpot JVM, that's where the object header lives.\r\n\r\nFor example, on the HotSpot JVM, the object header size for `long[]` is:\r\n- 32-bit: 16 bytes = 4 mark work + 4 klassptr + 4 array length + 4 padding\r\n- 64-bit with Compressed Class pointer: 16 bytes = 8 mark word + 4 klassptr + 4 array length\r\n- 64-bit without Compressed Class pointer: 24 bytes = 8 mark work + 8 klassptr + 4 array length + 4 padding.\r\n12 is always a bad offset to use in this test case on HotSpot, regardless of which mode above it's in -- writing to this offset will corrupt the header of the `long[]` and if you happen to go into a GC right after the corruption, you'll find the VM doing weird stuff (including crashing with a SIGSEGV).",
        "createdAt" : "2019-12-19T00:22:39Z",
        "updatedAt" : "2019-12-19T00:23:28Z",
        "lastEditedBy" : "3a12ce0f-9e73-4cfb-a4b4-b19368cddc2f",
        "tags" : [
        ]
      }
    ],
    "commit" : "753883d7f84552c7392295aa42f5023c0909c615",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +326,330 @@  @Test\n  public void testCompareLongsAsLittleEndian() {\n    long arrayOffset = 12;\n\n    long[] arr1 = new long[2];"
  }
]