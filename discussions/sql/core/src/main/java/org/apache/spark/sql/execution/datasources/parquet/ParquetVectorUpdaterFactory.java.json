[
  {
    "id" : "70e481a0-fe88-4cb0-b7c8-53b6dbba5507",
    "prId" : 32777,
    "prUrl" : "https://github.com/apache/spark/pull/32777#pullrequestreview-677880169",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2c7e87d8-2599-43ba-822e-2e214b7851a7",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "This seems not performance-sensitive and can be written in Scala?",
        "createdAt" : "2021-06-07T03:52:40Z",
        "updatedAt" : "2021-06-07T03:52:40Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "e54a3cb4-1f4b-4d17-8dc0-3b548333108d",
        "parentId" : "2c7e87d8-2599-43ba-822e-2e214b7851a7",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Yes it can. I'm not totally sure it is worthwhile though since the classes that use this and the classes this depends on are all in Java, and we are able to keep this and the updater implementations in the same file. One advantage for Scala is that we may be able to switch Scala enum instead of string for `datetimeRebaseMode` and `int96RebaseMode`. ",
        "createdAt" : "2021-06-07T20:59:05Z",
        "updatedAt" : "2021-06-07T20:59:23Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      }
    ],
    "commit" : "ede6b02c5c0b7cfe908c754a991e36124e6b7138",
    "line" : 43,
    "diffHunk" : "@@ -1,1 +41,45 @@import java.util.Arrays;\n\npublic class ParquetVectorUpdaterFactory {\n  private static final ZoneId UTC = ZoneOffset.UTC;\n"
  },
  {
    "id" : "4d5b5a37-5739-4f41-8545-1ab2c2b1c533",
    "prId" : 32777,
    "prUrl" : "https://github.com/apache/spark/pull/32777#pullrequestreview-677863086",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9fee6470-fd5d-4e15-911d-bd221ed96e35",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "shall we check if the logical type is UINT32?",
        "createdAt" : "2021-06-07T04:14:09Z",
        "updatedAt" : "2021-06-07T04:14:09Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "b9d887fc-d70b-4d04-9842-ea5e253fd14c",
        "parentId" : "9fee6470-fd5d-4e15-911d-bd221ed96e35",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Yes I think it would be more clear that way. As refactoring I just directly translated the existing logic to here though. Also this has been discussed in the [original PR](https://github.com/apache/spark/pull/31921/files#diff-09a6513eee0907b1a6610471aa4ebd6af2bef01bf4b5c082dc8c9ad1d6b030d9R583).",
        "createdAt" : "2021-06-07T20:36:17Z",
        "updatedAt" : "2021-06-07T20:44:56Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      }
    ],
    "commit" : "ede6b02c5c0b7cfe908c754a991e36124e6b7138",
    "line" : 79,
    "diffHunk" : "@@ -1,1 +77,81 @@          // For unsigned int32, it stores as plain signed int32 in Parquet when dictionary\n          // fallbacks. We read them as long values.\n          return new UnsignedIntegerUpdater();\n        } else if (sparkType == DataTypes.ByteType) {\n          return new ByteUpdater();"
  }
]