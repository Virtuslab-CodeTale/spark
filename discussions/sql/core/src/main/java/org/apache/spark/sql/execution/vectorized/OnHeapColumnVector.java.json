[
  {
    "id" : "4e70478a-eee9-4198-a015-c08e121fa9af",
    "prId" : 24788,
    "prUrl" : "https://github.com/apache/spark/pull/24788#pullrequestreview-246686480",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c0b9efc7-f64b-43d7-9d15-c26fe7238478",
        "parentId" : null,
        "authorId" : "99e31844-a5b3-48a2-af71-412edb607a13",
        "body" : "Could you please let us know which test cases are failed due to this code on `s390x`?",
        "createdAt" : "2019-06-04T09:35:49Z",
        "updatedAt" : "2019-06-12T10:45:42Z",
        "lastEditedBy" : "99e31844-a5b3-48a2-af71-412edb607a13",
        "tags" : [
        ]
      },
      {
        "id" : "bc99dba0-4edb-424c-b63d-51a53881543b",
        "parentId" : "c0b9efc7-f64b-43d7-9d15-c26fe7238478",
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "This change looks risky. Any reason?",
        "createdAt" : "2019-06-04T21:36:58Z",
        "updatedAt" : "2019-06-12T10:45:42Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "7eddfb4b-4997-4f03-b207-dd3a9504b0d2",
        "parentId" : "c0b9efc7-f64b-43d7-9d15-c26fe7238478",
        "authorId" : "99e31844-a5b3-48a2-af71-412edb607a13",
        "body" : "I agree. Once we know the test cases that cause failures, I will double-check by running them on another big endian environment.",
        "createdAt" : "2019-06-05T02:29:26Z",
        "updatedAt" : "2019-06-12T10:45:42Z",
        "lastEditedBy" : "99e31844-a5b3-48a2-af71-412edb607a13",
        "tags" : [
        ]
      },
      {
        "id" : "41b65bb3-7e5d-45c4-9d69-09f44409a36e",
        "parentId" : "c0b9efc7-f64b-43d7-9d15-c26fe7238478",
        "authorId" : "fa9d4e35-692a-43fd-9079-74047a9b3445",
        "body" : "below are the test suites which had some test failing initially and then passed on doing the above source code changes\r\n\r\nspark/sql/core/src/test/scala/org/apache/spark/sql/execution/columnar/compression/PassThroughEncodingSuite.scala\r\n\r\nspark/sql/core/src/test/scala/org/apache/spark/sql/execution/vectorized/ColumnVectorSuite.scala\r\n\r\nspark/sql/core/src/test/scala/org/apache/spark/sql/execution/columnar/InMemoryColumnarQuerySuite.scala\r\n\r\nspark/sql/core/src/test/scala/org/apache/spark/sql/DataFrameTungstenSuite.scala",
        "createdAt" : "2019-06-06T14:54:15Z",
        "updatedAt" : "2019-06-12T10:45:42Z",
        "lastEditedBy" : "fa9d4e35-692a-43fd-9079-74047a9b3445",
        "tags" : [
        ]
      },
      {
        "id" : "4d5f0f81-2064-46fd-a086-383116c9de0c",
        "parentId" : "c0b9efc7-f64b-43d7-9d15-c26fe7238478",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Hm, I guess the changes make sense? we don't test the big-endian branch on Jenkins, so I'm not surprised it still passes Jenkins. For a big-endian platform, indeed, wouldn't you have to read floats from the ByteBuffer as big-endian?",
        "createdAt" : "2019-06-06T15:27:17Z",
        "updatedAt" : "2019-06-12T10:45:42Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "6c4f60ed-12c5-4bfe-b060-36bb6831ebf3",
        "parentId" : "c0b9efc7-f64b-43d7-9d15-c26fe7238478",
        "authorId" : "99e31844-a5b3-48a2-af71-412edb607a13",
        "body" : "Thanks, I will test them on ppc64 linux (BE).",
        "createdAt" : "2019-06-06T16:39:29Z",
        "updatedAt" : "2019-06-12T10:45:42Z",
        "lastEditedBy" : "99e31844-a5b3-48a2-af71-412edb607a13",
        "tags" : [
        ]
      }
    ],
    "commit" : "9aa423f7e1e82cfc9b3046637ae35140ece6c960",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +397,401 @@          Platform.DOUBLE_ARRAY_OFFSET + rowId * 4L, count * 4L);\n    } else {\n      ByteBuffer bb = ByteBuffer.wrap(src).order(ByteOrder.BIG_ENDIAN);\n      for (int i = 0; i < count; ++i) {\n        floatData[i + rowId] = bb.getFloat(srcIndex + (4 * i));"
  }
]