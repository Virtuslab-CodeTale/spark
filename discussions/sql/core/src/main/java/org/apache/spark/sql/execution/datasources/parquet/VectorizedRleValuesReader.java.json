[
  {
    "id" : "ec292dce-3b64-4983-9f2c-c8110d6d434f",
    "prId" : 33330,
    "prUrl" : "https://github.com/apache/spark/pull/33330#pullrequestreview-705876361",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dc658387-1e4a-484b-9963-ea3731436b74",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Is else case for null value?",
        "createdAt" : "2021-07-14T05:47:38Z",
        "updatedAt" : "2021-07-14T05:47:38Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "ae2334a7-7876-401f-b78b-b7a4334bed80",
        "parentId" : "dc658387-1e4a-484b-9963-ea3731436b74",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "yes the else case is when the value is null - let me add some comments. ",
        "createdAt" : "2021-07-14T05:49:59Z",
        "updatedAt" : "2021-07-14T05:49:59Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      }
    ],
    "commit" : "cfd73a73ec60eb8b44823baf60f422377e9bced9",
    "line" : 42,
    "diffHunk" : "@@ -1,1 +272,276 @@          if (currentValue == state.maxDefinitionLevel) {\n            updater.skipValues(num, valuesReader);\n          }\n          break;\n        case PACKED:"
  },
  {
    "id" : "5df7aa6b-1829-4793-8d4d-278b172502b3",
    "prId" : 33330,
    "prUrl" : "https://github.com/apache/spark/pull/33330#pullrequestreview-705875399",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ceda7180-e366-46f5-adcf-d00c358d3661",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "ditto.\r\n\r\nIf so, maybe it is good to add a comment.",
        "createdAt" : "2021-07-14T05:48:03Z",
        "updatedAt" : "2021-07-14T05:48:04Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "cfd73a73ec60eb8b44823baf60f422377e9bced9",
    "line" : 49,
    "diffHunk" : "@@ -1,1 +279,283 @@            if (currentBuffer[currentBufferIdx++] == state.maxDefinitionLevel) {\n              updater.skipValues(1, valuesReader);\n            }\n          }\n          break;"
  },
  {
    "id" : "fd06c72f-1f19-4902-b0b6-3b42314a3ec3",
    "prId" : 33330,
    "prUrl" : "https://github.com/apache/spark/pull/33330#pullrequestreview-705879196",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3e6f0bcb-f872-41a1-848e-90440c03b32e",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Hmm, this is a old bug?",
        "createdAt" : "2021-07-14T05:49:31Z",
        "updatedAt" : "2021-07-14T05:49:31Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "c4cfb4e8-15a6-4610-bf71-d2400497e90b",
        "parentId" : "3e6f0bcb-f872-41a1-848e-90440c03b32e",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "no - it used to call `advance` which does the same thing, but since we removed the method I just replaced it with the old method body",
        "createdAt" : "2021-07-14T05:51:16Z",
        "updatedAt" : "2021-07-14T05:51:16Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "11c5edcd-d4ae-4ca8-8652-9b37b2b719a4",
        "parentId" : "3e6f0bcb-f872-41a1-848e-90440c03b32e",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Do you remove `advance()` method? I don't see it is removed here.",
        "createdAt" : "2021-07-14T05:53:02Z",
        "updatedAt" : "2021-07-14T05:53:02Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "d32720b8-35d9-450b-9339-2e81e728f3b5",
        "parentId" : "3e6f0bcb-f872-41a1-848e-90440c03b32e",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "hmm somehow it survived - let me remove it for real.",
        "createdAt" : "2021-07-14T05:55:39Z",
        "updatedAt" : "2021-07-14T05:55:40Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      }
    ],
    "commit" : "cfd73a73ec60eb8b44823baf60f422377e9bced9",
    "line" : 72,
    "diffHunk" : "@@ -1,1 +397,401 @@          break;\n      }\n      currentCount -= n;\n      left -= n;\n    }"
  },
  {
    "id" : "cf97f4c9-e72f-4f4d-817d-5d8997a5297e",
    "prId" : 33006,
    "prUrl" : "https://github.com/apache/spark/pull/33006#pullrequestreview-690448086",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9a044121-c6fd-457e-94af-c099250c7a28",
        "parentId" : null,
        "authorId" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "body" : "not related to this PR: is there an `IOException` to throw?",
        "createdAt" : "2021-06-23T00:06:48Z",
        "updatedAt" : "2021-06-23T00:06:49Z",
        "lastEditedBy" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "tags" : [
        ]
      },
      {
        "id" : "7a2bc4c4-3086-4ed3-bf94-e82b1d02d4d0",
        "parentId" : "9a044121-c6fd-457e-94af-c099250c7a28",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "good catch - I think there isn't `IOException` to throw here and we can just remove it.",
        "createdAt" : "2021-06-23T06:05:22Z",
        "updatedAt" : "2021-06-23T06:05:22Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "4c96e6cd-9f3c-45a0-ae16-9ffbbca541cf",
        "parentId" : "9a044121-c6fd-457e-94af-c099250c7a28",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "For this one, let's proceed separately if needed. There might be more places like this.",
        "createdAt" : "2021-06-23T09:55:54Z",
        "updatedAt" : "2021-06-23T09:55:54Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "f5e97b77005456894cce6e01bc54160ee0cf1d01",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +174,178 @@      WritableColumnVector values,\n      VectorizedValuesReader valueReader,\n      ParquetVectorUpdater updater) throws IOException {\n    int offset = state.offset;\n    int left = Math.min(state.valuesToReadInBatch, state.valuesToReadInPage);"
  },
  {
    "id" : "145e347d-46ca-4e64-9113-afa165fb2850",
    "prId" : 32753,
    "prUrl" : "https://github.com/apache/spark/pull/32753#pullrequestreview-688727523",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8d0cc7bf-54a0-4692-93f0-0ec1a6d8725e",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I need more context to review this PR\r\n1. where do we read the column index in `VectorizedColumnReader`?\r\n2. where do we leverage the index to skip pages? In `VectorizedRleValuesReader`? ",
        "createdAt" : "2021-06-21T17:36:47Z",
        "updatedAt" : "2021-06-21T17:36:47Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "36f7dca2-4271-4326-b196-139d8dddc2bb",
        "parentId" : "8d0cc7bf-54a0-4692-93f0-0ec1a6d8725e",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Sorry @cloud-fan, I should've add more context in the PR description. Let me try to add here and copy later to there.\r\n\r\n1. The column index filtering is largely implemented in parquet-mr (via classes such as `ColumnIndex` and `ColumnIndexFilter`), and the filtered Parquet pages are returned to Spark through the `ParquetFileReader.readNextFilteredRowGroup` and `ParquetFileReader.getFilteredRecordCount` API. Please see #31393 for the related changes in the vectorized reader path.\r\n2. Spark needs more work to handle mis-aligned Parquet pages returned from parquet-mr side, when there are multiple columns and their type width are different (e.g., int and bigint). For this issue, @lxian already gave a pretty good description in [SPARK-34859](https://issues.apache.org/jira/browse/SPARK-34859). To support the case, Spark needs to leverage the API [`PageReadStore.getRowIndexes`](https://javadoc.io/doc/org.apache.parquet/parquet-column/latest/org/apache/parquet/column/page/PageReadStore.html), which returns the indexes of all rows (note the difference between rows and values: for flat schema there is no difference between the two, but for nested schema they're different) after filtering within a Parquet row group. In addition, because there are gaps between pages, we'll need to know what is the index for the first row in a page, so we can compare indexes of values (rows) from a page with the row indexes mentioned above. This is provided by the `DataPage.getFirstRowIndex` method.",
        "createdAt" : "2021-06-21T18:07:34Z",
        "updatedAt" : "2021-06-21T18:12:05Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      }
    ],
    "commit" : "6541d99bd6788ae7ee3da1ccc76d30e9b39b9cb5",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +164,168 @@   * non-null values. If the value is null, `values` will be populated with null value.\n   */\n  public void readBatch(\n      ParquetReadState state,\n      WritableColumnVector values,"
  },
  {
    "id" : "a2c7f493-5dba-423a-b1e2-00334b31029f",
    "prId" : 32753,
    "prUrl" : "https://github.com/apache/spark/pull/32753#pullrequestreview-694688821",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3f89008d-c7aa-4b5e-a9fd-65a69f470cdc",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "do we need to update `leftInBatch` here?",
        "createdAt" : "2021-06-28T17:01:37Z",
        "updatedAt" : "2021-06-28T17:01:37Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "7f56175b-1c01-41ba-ab90-16a42ffaa8de",
        "parentId" : "3f89008d-c7aa-4b5e-a9fd-65a69f470cdc",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "No, because we are skipping the batch here, and not adding the values into the batch.",
        "createdAt" : "2021-06-28T17:24:33Z",
        "updatedAt" : "2021-06-28T17:24:33Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "f69f14da-2890-4cd8-a749-845cc78456d0",
        "parentId" : "3f89008d-c7aa-4b5e-a9fd-65a69f470cdc",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "I think it is because the name `skipBatch` somehow confuses. Actually it means to skip values from the reader.",
        "createdAt" : "2021-06-28T23:35:46Z",
        "updatedAt" : "2021-06-28T23:35:46Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "a459d266-d24c-44eb-8faa-247ddc149abe",
        "parentId" : "3f89008d-c7aa-4b5e-a9fd-65a69f470cdc",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Maybe rename it? E.g. `skipFromReader`?",
        "createdAt" : "2021-06-29T00:02:49Z",
        "updatedAt" : "2021-06-29T00:02:49Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "ef804aa2-7ad5-4297-97dc-998ed433acdf",
        "parentId" : "3f89008d-c7aa-4b5e-a9fd-65a69f470cdc",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I see, so there are 2 actions here:\r\n1. get values from the parquet reader\r\n2. put values into the spark columnar batch.\r\n\r\n`skipBatch` does look confusing, we should use a better name, `skipValues`?",
        "createdAt" : "2021-06-29T02:48:34Z",
        "updatedAt" : "2021-06-29T02:48:35Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "3e7f05cb-5c6b-4d45-910c-4b4270e1d7c7",
        "parentId" : "3f89008d-c7aa-4b5e-a9fd-65a69f470cdc",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "> I see, so there are 2 actions here: ...\r\n\r\nIf you mean `readBatch`, then yes it does the two bullets above. For `skipBatch` it just skip the next `n` values from `valueReader`.\r\n\r\n@viirya @cloud-fan Yeah I can change the method name to `skipValues` or `skipFromReader` if that's more clearer :) my preference, though, is to keep this close to the `readBatch` above. So how about I update the 3 methods in `ParquetVectorUpdater` to be: `readValues`, `readValue` and `skipValues`?",
        "createdAt" : "2021-06-29T05:15:31Z",
        "updatedAt" : "2021-06-29T05:15:31Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "df438b16-ad7f-4101-bc63-a6108cadba44",
        "parentId" : "3f89008d-c7aa-4b5e-a9fd-65a69f470cdc",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "SGTM, I think `readValues` implies that it puts the values to the spark columnar batch",
        "createdAt" : "2021-06-29T07:37:17Z",
        "updatedAt" : "2021-06-29T07:37:17Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "6541d99bd6788ae7ee3da1ccc76d30e9b39b9cb5",
    "line" : 60,
    "diffHunk" : "@@ -1,1 +185,189 @@        advance(n);\n        rowId += n;\n        leftInPage -= n;\n      } else if (rowId > rangeEnd) {\n        state.nextRange();"
  },
  {
    "id" : "d08c9659-b517-47c2-a9e8-2c8d167463d5",
    "prId" : 32753,
    "prUrl" : "https://github.com/apache/spark/pull/32753#pullrequestreview-694473588",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "53912e02-a292-4d9f-9dd5-703a3eb89888",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "This is different to what I commented (https://github.com/apache/spark/pull/32753#discussion_r656582039) before. This looks more straightforward. ",
        "createdAt" : "2021-06-28T23:31:54Z",
        "updatedAt" : "2021-06-28T23:31:54Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "6541d99bd6788ae7ee3da1ccc76d30e9b39b9cb5",
    "line" : 51,
    "diffHunk" : "@@ -1,1 +176,180 @@    while (leftInBatch > 0 && leftInPage > 0) {\n      if (this.currentCount == 0) this.readNextGroup();\n      int n = Math.min(leftInBatch, Math.min(leftInPage, this.currentCount));\n\n      long rangeStart = state.currentRangeStart();"
  },
  {
    "id" : "92c8f0c7-7e0e-45e9-b0ca-5236bee7e3e7",
    "prId" : 32753,
    "prUrl" : "https://github.com/apache/spark/pull/32753#pullrequestreview-696502673",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f7d01a1c-8fd5-42ce-8f36-d9024e94234f",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Just a question. We may have two negative value cases.\r\n1. start < rowId\r\n2. (start - rowId) > Int.MaxValue\r\n\r\nAre we considering both? Or, there is no change for case (2)?",
        "createdAt" : "2021-06-30T07:25:08Z",
        "updatedAt" : "2021-06-30T07:25:20Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "eb16a646-c8fd-493d-b586-bf6eef37539f",
        "parentId" : "f7d01a1c-8fd5-42ce-8f36-d9024e94234f",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "To be safe, I'd like to recommend to move the type casting `(int)` into inside this `if` statement. For `if (toSkip > 0) {` check, we had better use `long`. If the ranges are protected by line 191 ~ 192, then ignore this comment.",
        "createdAt" : "2021-06-30T07:27:01Z",
        "updatedAt" : "2021-06-30T07:29:57Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "fccd4836-34fb-4427-8796-2a3bf2761f39",
        "parentId" : "f7d01a1c-8fd5-42ce-8f36-d9024e94234f",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "`start` must >= `rowId` because it is defined as `long start = Math.max(rangeStart, rowId)`. Therefore, the case 1 `start < rowId` will never happen.\r\n\r\nThe second case, `(start - rowId) > Int.MaxValue`, can only occur if `start` is equal to `rangeStart`. In this case we also know that `rangeStart <= rowId + n` (from line 183) and `n` is `Math.min(leftInBatch, Math.min(leftInPage, this.currentCount))` which is guaranteed to be within integer range. Therefore, the cast is safe.",
        "createdAt" : "2021-06-30T18:40:13Z",
        "updatedAt" : "2021-06-30T18:40:14Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      }
    ],
    "commit" : "6541d99bd6788ae7ee3da1ccc76d30e9b39b9cb5",
    "line" : 70,
    "diffHunk" : "@@ -1,1 +195,199 @@        // skip the part [rowId, start)\n        int toSkip = (int) (start - rowId);\n        if (toSkip > 0) {\n          updater.skipValues(toSkip, valueReader);\n          advance(toSkip);"
  },
  {
    "id" : "803d9f92-fb9c-4f1e-b140-725060c25b27",
    "prId" : 31921,
    "prUrl" : "https://github.com/apache/spark/pull/31921#pullrequestreview-617241781",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f544e334-7af8-438e-ba88-1bfe4db3ff75",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "@yaooqinn do you mean `data.readInteger()` also works for uint8 and unit16?",
        "createdAt" : "2021-03-22T08:05:10Z",
        "updatedAt" : "2021-03-25T04:16:05Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "bd1f6ef0-8d30-4e5c-ae07-b4a56a3d370f",
        "parentId" : "f544e334-7af8-438e-ba88-1bfe4db3ff75",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "Yes, in Parquet, `8, 16, and 32 bit values are stored using the INT32 physical type`, we read them by `readInteger` and cast them based on Catalyst type later. The [0, Int.MaxValue] is enough for uint 8, and 16, while [Int.MinValue, 0) is needed for uint32",
        "createdAt" : "2021-03-22T08:16:31Z",
        "updatedAt" : "2021-03-25T04:16:05Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "71496bdc4d5c8081139e5a26fa9bddfa1ddc38ed",
    "line" : 26,
    "diffHunk" : "@@ -1,1 +226,230 @@          for (int i = 0; i < n; ++i) {\n            if (currentBuffer[currentBufferIdx++] == level) {\n              c.putLong(rowId + i, Integer.toUnsignedLong(data.readInteger()));\n            } else {\n              c.putNull(rowId + i);"
  }
]