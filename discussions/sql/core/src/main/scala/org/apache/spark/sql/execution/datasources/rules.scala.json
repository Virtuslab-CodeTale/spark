[
  {
    "id" : "cac7bcf6-4d02-425f-920d-0bb21de7bdd5",
    "prId" : 30097,
    "prUrl" : "https://github.com/apache/spark/pull/30097#pullrequestreview-528862463",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8eb835ed-0c41-470d-9bcf-8a5efa82398c",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "ditto",
        "createdAt" : "2020-11-12T08:44:40Z",
        "updatedAt" : "2020-11-12T08:44:40Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "7612695c78456155a95ad4f7d54ef70e53f88921",
    "line" : 25,
    "diffHunk" : "@@ -1,1 +74,78 @@ * Preprocess [[CreateTable]], to do some normalization and checking.\n */\nobject PreprocessTableCreation extends Rule[LogicalPlan] {\n  // catalog is a def and not a val/lazy val as the latter would introduce a circular reference\n  private def catalog = SparkSession.active.sessionState.catalog"
  },
  {
    "id" : "1818aa26-9e34-4735-bd6b-7b3669f993cf",
    "prId" : 28833,
    "prUrl" : "https://github.com/apache/spark/pull/28833#pullrequestreview-440721670",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "107f2be0-cf6e-4a61-a254-e855122a49a7",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Is this needed? I think the changes in `ResolveCatalogs` and `ResolveSessionCatalog` should cover all the commands.",
        "createdAt" : "2020-07-01T04:40:56Z",
        "updatedAt" : "2020-07-08T00:27:04Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "58d28d99-4846-4adc-a253-8854cd43ecea",
        "parentId" : "107f2be0-cf6e-4a61-a254-e855122a49a7",
        "authorId" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "body" : "Without this, \"CREATE TABLE t1 USING PARQUET AS SELECT null as null_col\" in Spark will throw `Parquet data source does not support null data type.` instead of `Cannot create tables with VOID type`\r\n\r\nComparing the error message from Hive `SemanticException [Error 10305]: CREATE-TABLE-AS-SELECT creates a VOID type, please use CAST to specify the type, near field: col`, it's confused. So better to keep it.",
        "createdAt" : "2020-07-01T06:46:25Z",
        "updatedAt" : "2020-07-08T00:27:04Z",
        "lastEditedBy" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "tags" : [
        ]
      },
      {
        "id" : "b40e40bc-5a03-4542-a14a-1fe747eac63f",
        "parentId" : "107f2be0-cf6e-4a61-a254-e855122a49a7",
        "authorId" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "body" : "@cloud-fan \r\n> Without this, \"CREATE TABLE t1 USING PARQUET AS SELECT null as null_col\" in Spark will throw `Parquet data source does not support null data type.` instead of `Cannot create tables with VOID type`\r\n\r\nSorry, above description is incorrect. Without this, CTAS for Hive table `CREATE TABLE t2 AS SELECT null as null_col` will pass. No exception throws.\r\n\r\nSeems Hive table (non-parquet/orc format) doesn't go through `ResolveSessionCatalog`",
        "createdAt" : "2020-07-01T09:54:56Z",
        "updatedAt" : "2020-07-08T00:27:04Z",
        "lastEditedBy" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "tags" : [
        ]
      }
    ],
    "commit" : "9ad57d17bac47ea0f801004ec0aba9197e631bc7",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +294,298 @@      sparkSession.sessionState.conf.caseSensitiveAnalysis)\n\n    assertNoNullTypeInSchema(schema)\n\n    val normalizedPartCols = normalizePartitionColumns(schema, table)"
  },
  {
    "id" : "3f2d73e6-eddf-4304-aaaf-3b493e929e66",
    "prId" : 25305,
    "prUrl" : "https://github.com/apache/spark/pull/25305#pullrequestreview-272684133",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2b2a5310-f1eb-4478-9795-7374f7a355f5",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "shall we add an assert that the plan must be `CreateTableAsSelect` if schema is empty?",
        "createdAt" : "2019-08-07T08:01:29Z",
        "updatedAt" : "2019-08-08T16:17:23Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "3c1b4599-9cee-4827-bf2f-6d952eb88727",
        "parentId" : "2b2a5310-f1eb-4478-9795-7374f7a355f5",
        "authorId" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "body" : "no. Existing data may exist for a table, and all that you may want is to create a pointer to that table (Think of creating a pointer to a JDBC table in the MetaStore)",
        "createdAt" : "2019-08-08T16:14:04Z",
        "updatedAt" : "2019-08-08T16:17:23Z",
        "lastEditedBy" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "tags" : [
        ]
      }
    ],
    "commit" : "b7a5ac549c26153a3aa8bf3e73bd415929a78a1f",
    "line" : 38,
    "diffHunk" : "@@ -1,1 +255,259 @@        partitioning, \"in the partitioning\", isCaseSensitive)\n\n      if (schema.isEmpty) {\n        if (partitioning.nonEmpty) {\n          throw new AnalysisException(\"It is not allowed to specify partitioning when the \" +"
  },
  {
    "id" : "1f80b83c-d0ee-49c2-8d31-6a4b192bda68",
    "prId" : 25198,
    "prUrl" : "https://github.com/apache/spark/pull/25198#pullrequestreview-272460229",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "479bafee-c91f-42ff-8216-b89c220020f0",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I think the rationale here is that, we don't want to create tables with null type field. It should be same for both `CREATE TABLE` and `CTAS`.",
        "createdAt" : "2019-08-08T07:16:51Z",
        "updatedAt" : "2019-09-30T09:56:42Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "1554695a-c2b1-4072-a50a-9740460c3c5d",
        "parentId" : "479bafee-c91f-42ff-8216-b89c220020f0",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "OK, and also add `AlterTable`.",
        "createdAt" : "2019-08-08T09:48:42Z",
        "updatedAt" : "2019-09-30T09:56:42Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      }
    ],
    "commit" : "358e2a225d6744274599774ca9ea9878c712a1e5",
    "line" : 54,
    "diffHunk" : "@@ -1,1 +479,483 @@        checkSchema(tableDesc.schema)\n\n      case CreateV2Table(_, _, tableSchema, _, _, _) =>\n        checkSchema(tableSchema)\n"
  },
  {
    "id" : "d6336a30-33c6-4b16-848d-42e77c74d354",
    "prId" : 24806,
    "prUrl" : "https://github.com/apache/spark/pull/24806#pullrequestreview-249073392",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "58d4e396-eaa9-48f6-8fc9-a6cb970e3c81",
        "parentId" : null,
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "Why not run `ResolveOutputRelation`?",
        "createdAt" : "2019-06-05T17:51:28Z",
        "updatedAt" : "2019-06-12T07:04:18Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      },
      {
        "id" : "dd772755-8a81-454f-8587-ddcddca08f7d",
        "parentId" : "58d4e396-eaa9-48f6-8fc9-a6cb970e3c81",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "Here is for the V1 path. `InsertIntoTable` won't be matched in `ResolveOutputRelation`.",
        "createdAt" : "2019-06-06T02:59:24Z",
        "updatedAt" : "2019-06-12T07:04:18Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "818fd7ad-4d55-46d7-b3ab-0f4229f803a4",
        "parentId" : "58d4e396-eaa9-48f6-8fc9-a6cb970e3c81",
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "If you aren't going to use the `ResolveOutputRelation` rule, can you explain exactly what the behavior this code path should have and how it differs from `ResolveOutputRelation`?",
        "createdAt" : "2019-06-06T18:11:10Z",
        "updatedAt" : "2019-06-12T07:04:18Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      },
      {
        "id" : "9ed20620-b2dc-43b5-a0b0-2f4a5d8c58b6",
        "parentId" : "58d4e396-eaa9-48f6-8fc9-a6cb970e3c81",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "I check and I think it is workable. I will try moving it into the `ResolveOutputRelation` rule.",
        "createdAt" : "2019-06-10T16:06:17Z",
        "updatedAt" : "2019-06-12T07:04:18Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "4e1c1574-cd87-4a34-b8c6-63db2c738bd1",
        "parentId" : "58d4e396-eaa9-48f6-8fc9-a6cb970e3c81",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Have we moved it?",
        "createdAt" : "2019-06-12T06:55:45Z",
        "updatedAt" : "2019-06-12T07:04:18Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "73aa061a-516c-48d1-bb04-13c2f6780a6b",
        "parentId" : "58d4e396-eaa9-48f6-8fc9-a6cb970e3c81",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "We need to figure out if we can up-cast decimal to double/float. \r\nIf we can't, then maybe we can't continue this PR.\r\nI have created a new PR for the upcasting: https://github.com/apache/spark/pull/24849",
        "createdAt" : "2019-06-12T08:16:45Z",
        "updatedAt" : "2019-06-12T08:17:04Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "672ac290-ea91-4519-96dc-b0debded3f0c",
        "parentId" : "58d4e396-eaa9-48f6-8fc9-a6cb970e3c81",
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "@gengliangwang, why is decimal to double/float required? That should not be allowed in either v1 or v2, so I see no reason why we can't use the same rule for both.",
        "createdAt" : "2019-06-12T19:03:24Z",
        "updatedAt" : "2019-06-12T19:03:24Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      },
      {
        "id" : "a3d62162-9ec8-4b67-8ab1-e9c913a7222d",
        "parentId" : "58d4e396-eaa9-48f6-8fc9-a6cb970e3c81",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "For sql\r\n```\r\ncreate table t (d double);\r\ninsert into t values (10.0);\r\n```\r\nthe 10.0 is decimal in Spark SQL parser.",
        "createdAt" : "2019-06-12T23:59:35Z",
        "updatedAt" : "2019-06-12T23:59:35Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "3e2949131a8a7579149a7dd4153650461f5b5da2",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +362,366 @@      DDLPreprocessingUtils.castAndRenameQueryOutput(insert.query, expectedColumns, conf)\n    } else {\n      val errors = new mutable.ArrayBuffer[String]()\n      val resolved = insert.query.output.zip(expectedColumns).flatMap {\n        case (queryExpr, tableAttr) =>"
  },
  {
    "id" : "fa9a51a1-9b3d-4eeb-8b34-7dc294864789",
    "prId" : 24741,
    "prUrl" : "https://github.com/apache/spark/pull/24741#pullrequestreview-246318398",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "671efac7-f651-46b7-84bf-1dd5cbd46e23",
        "parentId" : null,
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "This is a separate problem, but could you also file an issue to fix this rule? This relies on rule ordering for correctness because it will convert any 2-part identifier to a SQL-on-file plan if the namespace is also a DataSource name. This should additionally check that the path either looks like a path (contains `/`) or exists in some file system.",
        "createdAt" : "2019-06-05T17:20:40Z",
        "updatedAt" : "2019-06-13T01:41:28Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      },
      {
        "id" : "c707e25b-fc79-4264-8add-a79846c3a18a",
        "parentId" : "671efac7-f651-46b7-84bf-1dd5cbd46e23",
        "authorId" : "f5b3f57a-75a6-496c-af94-c32580ada13a",
        "body" : "Sounds good.\r\n\r\nWhat should be the expected behavior for this test in sql.SQLQuerySuite.\"run sql directly on files\"?\r\n```\r\n    e = intercept[AnalysisException] {\r\n      sql(\"select * from json.invalid_file\")\r\n    }\r\n    assert(e.message.contains(\"Path does not exist\"))\r\n```",
        "createdAt" : "2019-06-06T00:20:44Z",
        "updatedAt" : "2019-06-13T01:41:28Z",
        "lastEditedBy" : "f5b3f57a-75a6-496c-af94-c32580ada13a",
        "tags" : [
        ]
      },
      {
        "id" : "faee8630-028d-46f5-a894-5834a864a7eb",
        "parentId" : "671efac7-f651-46b7-84bf-1dd5cbd46e23",
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "I think it should be that the table could not be found. If there were some way to determine that it is a path, like checking for `/`, then that message would be correct. So it could be updated to this:\r\n\r\n```scala\r\n    e = intercept[AnalysisException] {\r\n      sql(\"select * from json.`/invalid_file`\")\r\n    }\r\n    assert(e.message.contains(\"Path does not exist\"))\r\n```",
        "createdAt" : "2019-06-06T00:25:14Z",
        "updatedAt" : "2019-06-13T01:41:28Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      }
    ],
    "commit" : "b8cdf6c22172585b3b3a9452d5e4d2d591ece88e",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +38,42 @@class ResolveSQLOnFile(sparkSession: SparkSession) extends Rule[LogicalPlan] {\n  private def maybeSQLFile(u: UnresolvedRelation): Boolean = {\n    sparkSession.sessionState.conf.runSQLonFile && u.multipartIdentifier.size == 2\n  }\n"
  },
  {
    "id" : "5cbb95a6-10ac-4ac4-98ad-2e12ced56ed6",
    "prId" : 24372,
    "prUrl" : "https://github.com/apache/spark/pull/24372#pullrequestreview-237779056",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d0c824a9-876b-47a3-8344-32c8b7dd2087",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "what if the column is not nullable in the target table?",
        "createdAt" : "2019-05-08T08:30:07Z",
        "updatedAt" : "2019-06-13T08:04:29Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "5c825330-e44b-4173-889b-1e255cffb9f5",
        "parentId" : "d0c824a9-876b-47a3-8344-32c8b7dd2087",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "> what if the column is not nullable in the target table?\r\n\r\nAs far as I can tell, Hive may not have field constraints `NOT NULL`.",
        "createdAt" : "2019-05-08T09:29:45Z",
        "updatedAt" : "2019-06-13T08:04:29Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      },
      {
        "id" : "f73928a4-f2f7-4789-9f06-be0882ced3c2",
        "parentId" : "d0c824a9-876b-47a3-8344-32c8b7dd2087",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "is there any other considerations? @cloud-fan ",
        "createdAt" : "2019-05-15T12:11:13Z",
        "updatedAt" : "2019-06-13T08:04:29Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "146d1ad1956ebebecad3d838f45172079a84cf1c",
    "line" : 73,
    "diffHunk" : "@@ -1,1 +387,391 @@          i += 1\n        } else {\n          filledProjectList += Alias(Literal(null, NullType), \"NULL\")()\n        }\n      }"
  }
]