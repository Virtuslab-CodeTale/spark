[
  {
    "id" : "d67891e1-cd0f-45cc-aa68-4dab895db5d0",
    "prId" : 25416,
    "prUrl" : "https://github.com/apache/spark/pull/25416#pullrequestreview-288403211",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bd820f19-888d-4ba5-bc8a-823fd92c320d",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Can we avoid collect everything into driver side?",
        "createdAt" : "2019-09-10T00:37:17Z",
        "updatedAt" : "2019-09-16T02:56:10Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "624c145b-021e-4616-b500-e4294880e056",
        "parentId" : "bd820f19-888d-4ba5-bc8a-823fd92c320d",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "Do you have a good suggestion? Thanks.",
        "createdAt" : "2019-09-10T04:36:22Z",
        "updatedAt" : "2019-09-16T02:56:10Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      },
      {
        "id" : "d9d59459-b1f3-4e95-83ca-d48b2f7ee7b2",
        "parentId" : "bd820f19-888d-4ba5-bc8a-823fd92c320d",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "@HyukjinKwon I have resolved this issue collect results to driver.",
        "createdAt" : "2019-09-16T02:57:44Z",
        "updatedAt" : "2019-09-16T02:57:44Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "e50aff7cbf96f443eff6124962864296a1eecb5d",
    "line" : 40,
    "diffHunk" : "@@ -1,1 +38,42 @@  override def outputOrdering: Seq[SortOrder] = child.outputOrdering\n\n  override def executeCollect(): Array[InternalRow] = child.executeCollect.drop(offset)\n\n  private val serializer: Serializer = new UnsafeRowSerializer(child.output.size)"
  },
  {
    "id" : "237b55ac-6efc-447e-af6e-2677abd3abc5",
    "prId" : 25416,
    "prUrl" : "https://github.com/apache/spark/pull/25416#pullrequestreview-299913868",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ec2bd47a-b65b-4356-90ce-b7d804689b70",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "We need some design about how to implement OFFSET on a distributed data set. The current approach works, but it's not robust as it needs to broadcast `arr` (which can be large if OFFSET is large) and relies on object equality.\r\n\r\nWe may need to follow how LIMIT is implemented in Spark.",
        "createdAt" : "2019-10-09T08:25:32Z",
        "updatedAt" : "2019-10-09T08:25:33Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "bffa3979-7c4c-415e-a906-2c41acab488b",
        "parentId" : "ec2bd47a-b65b-4356-90ce-b7d804689b70",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "Yes, I think so.\r\nI have referenced the implement of `LIMIT`, but `OFFSET` looks can't follow the same way as `LIMIT`.\r\nSecond, `OFFSET` easier to generate large amounts of data than `LIMIT`.\r\nI have an immature suggestion give a limitation on `OFFSET`.",
        "createdAt" : "2019-10-09T08:58:32Z",
        "updatedAt" : "2019-10-09T09:14:57Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      },
      {
        "id" : "dbd76429-2e09-4221-96b5-7c43774fb1c7",
        "parentId" : "ec2bd47a-b65b-4356-90ce-b7d804689b70",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "A rough idea:\r\n1. get the numRecords of the first partition\r\n2. If the numRecords is bigger than OFFSET, go to step 4\r\n3. get numRecords of more partitions (quadruple and retry like LIMIT), until total numRecords is bigger than OFFSET.\r\n4. Now we have the numRecords of some head partitions that totoal numRecords exceeds the OFFSET, we can easily skip the head records.\r\n\r\nIf we have accurate per-partition numRecords statistics, we can go step 4 directly.",
        "createdAt" : "2019-10-09T09:42:32Z",
        "updatedAt" : "2019-10-09T09:42:32Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "b5ed9779-0061-47f4-825b-9b62ff53619d",
        "parentId" : "ec2bd47a-b65b-4356-90ce-b7d804689b70",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "@cloud-fan It is a better idea for me. I will reactor the current implement.",
        "createdAt" : "2019-10-09T10:04:06Z",
        "updatedAt" : "2019-10-09T10:04:06Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      },
      {
        "id" : "09f727f4-afa5-4402-9930-f32be8691c5e",
        "parentId" : "ec2bd47a-b65b-4356-90ce-b7d804689b70",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "@cloud-fan There are exists a problem the index of partition and the order of data are inconsistent.\r\nI have a new implement but not works file as I can't assurance the order of output produced by child plans.\r\n```\r\n  protected override def doExecute(): RDD[InternalRow] = {\r\n    val rdd = child.execute()\r\n    val partIdxToCountItr = rdd.mapPartitionsWithIndex{(partIdx, iter) => {\r\n      val partIdxToRowCount = scala.collection.mutable.Map[Int,Int]()\r\n      var rowCount = 0\r\n      while(iter.hasNext){\r\n        rowCount += 1\r\n        iter.next()\r\n      }\r\n      partIdxToRowCount.put(partIdx, rowCount)\r\n      partIdxToRowCount.iterator\r\n    }}.collect().iterator\r\n    var remainder = offset\r\n    val partIdxToSkipCount = scala.collection.mutable.Map[Int,Int]()\r\n    while (partIdxToCountItr.hasNext && remainder > 0) {\r\n      val kv = partIdxToCountItr.next()\r\n    \tval partIdx = kv._1\r\n      val count = kv._2\r\n      if (count > remainder) {\r\n        partIdxToSkipCount(partIdx) = remainder\r\n        remainder = 0\r\n      } else {\r\n        partIdxToSkipCount(partIdx) = count\r\n        remainder -= count\r\n      }\r\n    }\r\n    val broadcastPartIdxToSkipCount = sparkContext.broadcast(partIdxToSkipCount)\r\n    rdd.mapPartitionsWithIndex{(partIdx, iter) => {\r\n      val skipCount = broadcastPartIdxToSkipCount.value.getOrElse(partIdx, 0)\r\n      iter.drop(skipCount)\r\n    }}\r\n  }\r\n```",
        "createdAt" : "2019-10-10T08:54:38Z",
        "updatedAt" : "2019-10-10T08:55:11Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      },
      {
        "id" : "35ac6718-6aaf-4c56-bebd-219a0b9f6196",
        "parentId" : "ec2bd47a-b65b-4356-90ce-b7d804689b70",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "So, I want know how to assurance the order?",
        "createdAt" : "2019-10-10T08:57:03Z",
        "updatedAt" : "2019-10-10T08:57:03Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "e50aff7cbf96f443eff6124962864296a1eecb5d",
    "line" : 62,
    "diffHunk" : "@@ -1,1 +60,64 @@    val rdd = child.execute()\n    val arr = rdd.take(offset)\n    rdd.filter(!arr.contains(_))\n  }\n"
  }
]