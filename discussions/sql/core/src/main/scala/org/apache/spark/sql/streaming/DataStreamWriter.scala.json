[
  {
    "id" : "7eaaea11-ff7d-4032-89cf-21a7c6629e03",
    "prId" : 30521,
    "prUrl" : "https://github.com/apache/spark/pull/30521#pullrequestreview-542786843",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "14ef0870-4194-4a57-81b6-244a3f87b156",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "I think leveraging the old (probably DSv1) options is not sufficient - this doesn't have full coverage on DSv2 table - no `Transform` on partitioning, no `properties`, no `options`. \r\n\r\nUsing `source` (via `format(...)`) as `USE <provider>` is also not intuitive - it is only effective when table creation is taking place, and it occurs implicitly.\r\n\r\nPlease compare the usage with creating table on DataFrameWriterV2. I still think this worths having V2 writer for streaming.",
        "createdAt" : "2020-12-02T02:16:37Z",
        "updatedAt" : "2020-12-03T04:18:53Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "81b292b7-00fd-4905-9571-ebbbc25ff294",
        "parentId" : "14ef0870-4194-4a57-81b6-244a3f87b156",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "`Using source (via format(...)) as USE <provider> is also not intuitive - it is only effective when table creation is taking place, and it occurs implicitly.`\r\nYes, this is indeed a reasonable concern. We should check the source and provider. Especially when they are different. Done in b6393ba and UT added.",
        "createdAt" : "2020-12-02T12:29:11Z",
        "updatedAt" : "2020-12-03T04:18:53Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "32f9940641d479470530dcbaaeed4be12e1eef61",
    "line" : 74,
    "diffHunk" : "@@ -1,1 +332,336 @@       * TODO (SPARK-33638): Full support of v2 table creation\n       */\n      val cmd = CreateTableStatement(\n        originalMultipartIdentifier,\n        df.schema.asNullable,"
  },
  {
    "id" : "df5b1e4b-b038-40f9-9b56-19748467f2fd",
    "prId" : 30521,
    "prUrl" : "https://github.com/apache/spark/pull/30521#pullrequestreview-544711617",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "415d8742-8feb-4116-ae97-927f7b7dfe4a",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Writing to V1 table shouldn't rely on `normalizedParCols`, but then you'll be stuck how to provide `Array[Transform]` (provided by CatalogTable) to `Seq[String]`.",
        "createdAt" : "2020-12-02T03:18:02Z",
        "updatedAt" : "2020-12-03T04:18:53Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "ff5560fa-7144-486a-89fe-074121de6dea",
        "parentId" : "415d8742-8feb-4116-ae97-927f7b7dfe4a",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "I see `ResolveSessionCatalog.buildCatalogTable()` leverages `partitioning.asPartitionColumns` - that could be used here to set `partitionBy`. Not beauty to deal with putting provider and partitioning manually here instead of letting analyzer does it, but as there's no logical node for Streaming write, I guess there's no option.",
        "createdAt" : "2020-12-02T03:39:49Z",
        "updatedAt" : "2020-12-03T04:18:53Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "5bcf4246-992a-4993-8c86-247da44ecb4b",
        "parentId" : "415d8742-8feb-4116-ae97-927f7b7dfe4a",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "I think we shouldn't set `partitionBy` here since that will ignore the original setting of partitioningColumns.",
        "createdAt" : "2020-12-02T12:47:47Z",
        "updatedAt" : "2020-12-03T04:18:53Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      },
      {
        "id" : "df8642a4-5ba3-41e2-be3c-74de508cda61",
        "parentId" : "415d8742-8feb-4116-ae97-927f7b7dfe4a",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "So are we completely ignoring the table's partition information? I don't think this is same as DSv2 path.",
        "createdAt" : "2020-12-02T20:10:52Z",
        "updatedAt" : "2020-12-03T04:18:53Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "fe8c220c-797d-4113-83df-5207a268a55b",
        "parentId" : "415d8742-8feb-4116-ae97-927f7b7dfe4a",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "We didn't ignore. IMO, the partition information should be handled at the DataSource level, no matter V1/V2. Different DS should have their own ability and strategy to handle the partition overwrite/append issue(part discussion of schema evolution). So for the API level, we need to pass both information down to the runtime.",
        "createdAt" : "2020-12-03T03:52:18Z",
        "updatedAt" : "2020-12-03T04:18:53Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      },
      {
        "id" : "52b98332-957e-43f8-94d4-e9daedcabaa6",
        "parentId" : "415d8742-8feb-4116-ae97-927f7b7dfe4a",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Please find the usage of `normalizedParCols` in DataStreamWriter. This config is only effective in DSv1.",
        "createdAt" : "2020-12-03T04:13:30Z",
        "updatedAt" : "2020-12-03T04:18:53Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "34e2e857-ec71-4729-9c98-25b841d46f45",
        "parentId" : "415d8742-8feb-4116-ae97-927f7b7dfe4a",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "Got the point. So your point focus on how DSv2 addressing the partition column conflict. My point is want to explain we shouldn't overwrite the user's input, ~we need to pass both(user-input and table catalog partition info) into the data source.~ Data source need to know both(user-input and table catalog partition info). I already added this and further discussion to SPARK-33638's [comment](https://issues.apache.org/jira/browse/SPARK-33638?focusedCommentId=17243070&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17243070) since it's part of full support for the V2 table.",
        "createdAt" : "2020-12-03T10:01:16Z",
        "updatedAt" : "2020-12-04T07:29:48Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      },
      {
        "id" : "14815f33-8291-4d06-9c3a-5b95a7562ab8",
        "parentId" : "415d8742-8feb-4116-ae97-927f7b7dfe4a",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Sorry I missed this. I'm not sure we provide the both into data source, but data source is probably able to know about the partitioning (as table partitioning is given by data source), so consider this as minor and make a follow-up if necessary. In anyway you'll want to check this to achieve my review comments on documentation.",
        "createdAt" : "2020-12-04T07:11:03Z",
        "updatedAt" : "2020-12-04T07:11:04Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "955e1738-dbfe-4b72-a50d-f1a1e1ba773a",
        "parentId" : "415d8742-8feb-4116-ae97-927f7b7dfe4a",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "`I'm not sure we provide the both into data source`\r\nMy bad, didn't make it clear. We don't provide both, only pass the user-provided one(in V1). The partitioning in the catalog is able to know in a data source as you said. I need to change `we need to pass both` to `data source need to know both(we need to pass the user-provided partitioning)`. Let me rephrase the last comment to make it clear.\r\n\r\nYes, of cause, a follow-up is needed.",
        "createdAt" : "2020-12-04T07:28:46Z",
        "updatedAt" : "2020-12-04T07:28:46Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "32f9940641d479470530dcbaaeed4be12e1eef61",
    "line" : 107,
    "diffHunk" : "@@ -1,1 +359,363 @@          s\"$tableName's data source provider(${table.provider.get}).\")\n      }\n      format(table.provider.get)\n        .option(\"path\", new Path(table.location).toString).start()\n    }"
  },
  {
    "id" : "0dc8cd5c-999a-43b7-af42-7473809a310f",
    "prId" : 29767,
    "prUrl" : "https://github.com/apache/spark/pull/29767#pullrequestreview-490536315",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e96fe12f-1539-4d8b-b291-aa32e27fce19",
        "parentId" : null,
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "Skip for the table non-existence is good for the first version, but it seems a good-to-have one in the future.",
        "createdAt" : "2020-09-17T10:54:29Z",
        "updatedAt" : "2020-10-08T06:37:15Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      },
      {
        "id" : "be54eb2c-3fa9-477e-bcf3-44b35e0b9f9d",
        "parentId" : "e96fe12f-1539-4d8b-b291-aa32e27fce19",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Could you please elaborate? Which one do we want to handle here except DSv2 table?\r\n(Personally I would say we should accelerate migration to DSv2 instead of spending efforts to extend supporting DSv1.) ",
        "createdAt" : "2020-09-17T11:41:42Z",
        "updatedAt" : "2020-10-08T06:37:15Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "33fcc585-10ba-4032-8323-0b5ee764a4cd",
        "parentId" : "e96fe12f-1539-4d8b-b291-aa32e27fce19",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "Ah sorry for the unclear, I mean the behavior when the table does not exist. Should we support create a new table and append data into? Seems a good-to-have feature. IMO it's the significant difference between the reader and the writer side.",
        "createdAt" : "2020-09-17T12:05:49Z",
        "updatedAt" : "2020-10-08T06:37:15Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      },
      {
        "id" : "906e2816-a688-420e-ba36-7e636113a6b2",
        "parentId" : "e96fe12f-1539-4d8b-b291-aa32e27fce19",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Ah OK. It brings additional options as we see in DataFrameWriterV2 to support create table, so if we want to do that, I think it warrants DataStreamWriterV2.",
        "createdAt" : "2020-09-17T12:24:14Z",
        "updatedAt" : "2020-10-08T06:37:15Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "3f80c4fc78ca4fcfc1f6a843a3175b403fb4bb3c",
    "line" : 60,
    "diffHunk" : "@@ -1,1 +333,337 @@      }\n\n      val tableInstance = catalog.asTableCatalog.loadTable(identifier)\n\n      import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Implicits._"
  },
  {
    "id" : "bb9a7ab2-6c89-4d2a-a024-22c4a6e6bd93",
    "prId" : 29767,
    "prUrl" : "https://github.com/apache/spark/pull/29767#pullrequestreview-504461816",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "795ff91a-e82a-4670-bd87-e0ac132c452c",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "@HeartSaVioR . For `CaseInsensitiveMap`, `def toMap: Map[String, T] = originalMap`. It seems that we need `toMap` explicitly here as we did line 385. (cc @cloud-fan )\r\n```\r\nstartQuery(sink, optionsWithPath.originalMap)\r\n```",
        "createdAt" : "2020-10-07T18:45:41Z",
        "updatedAt" : "2020-10-08T06:37:15Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "5b96ca16-14ee-4b56-905d-a7ee9dd6e2c8",
        "parentId" : "795ff91a-e82a-4670-bd87-e0ac132c452c",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Previously, I and @cloud-fan hit case-sensitivity issues in another JIRAs due to this. Please make it sure that this PR doesn't re-introduce it because AS-IS PR switches `extraOptions.toMap` -> `extraOptions` silently.",
        "createdAt" : "2020-10-07T18:48:28Z",
        "updatedAt" : "2020-10-08T06:37:15Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "addfce4c-9903-440d-a0c4-e948c28b9f75",
        "parentId" : "795ff91a-e82a-4670-bd87-e0ac132c452c",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "If you already checked that, please add a test case for that. Or, we just use the old way `extraOptions.toMap` to avoid any side effect.",
        "createdAt" : "2020-10-07T18:50:16Z",
        "updatedAt" : "2020-10-08T06:37:15Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "fa803c98-a014-4041-a250-1b5e47057f68",
        "parentId" : "795ff91a-e82a-4670-bd87-e0ac132c452c",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Ah OK thanks for pointing out. Nice finding. I'll just explicitly call `.toMap` as it was.",
        "createdAt" : "2020-10-08T05:07:26Z",
        "updatedAt" : "2020-10-08T06:37:15Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "947c6d9d-89be-4545-93be-c9980b4875e1",
        "parentId" : "795ff91a-e82a-4670-bd87-e0ac132c452c",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "nice catch @dongjoon-hyun !",
        "createdAt" : "2020-10-08T06:27:46Z",
        "updatedAt" : "2020-10-08T06:37:15Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "3f80c4fc78ca4fcfc1f6a843a3175b403fb4bb3c",
    "line" : 125,
    "diffHunk" : "@@ -1,1 +364,368 @@      }\n      val sink = new ForeachBatchSink[T](foreachBatchWriter, ds.exprEnc)\n      startQuery(sink, extraOptions)\n    } else {\n      val cls = DataSource.lookupDataSource(source, df.sparkSession.sessionState.conf)"
  },
  {
    "id" : "b1a61763-9226-4ec2-a220-99fd6534aa34",
    "prId" : 29767,
    "prUrl" : "https://github.com/apache/spark/pull/29767#pullrequestreview-504465490",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7d707587-620b-406b-a2c5-109eeaccd34b",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "We can follow the previous code style\r\n```\r\n...startQuery(\r\n  newOptions.get(\"queryName\"),\r\n  newOptions.get(\"checkpointLocation\"),\r\n  df,\r\n  newOptions. originalMap,\r\n  ...\r\n```",
        "createdAt" : "2020-10-08T06:32:24Z",
        "updatedAt" : "2020-10-08T06:37:15Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "4bbc6311-8681-45a0-a9d6-77126bf76ebe",
        "parentId" : "7d707587-620b-406b-a2c5-109eeaccd34b",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "OK let me keep it as it is.",
        "createdAt" : "2020-10-08T06:35:13Z",
        "updatedAt" : "2020-10-08T06:37:15Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "3f80c4fc78ca4fcfc1f6a843a3175b403fb4bb3c",
    "line" : 153,
    "diffHunk" : "@@ -1,1 +407,411 @@    val useTempCheckpointLocation = SOURCES_ALLOW_ONE_TIME_QUERY.contains(source)\n\n    df.sparkSession.sessionState.streamingQueryManager.startQuery(\n      newOptions.get(\"queryName\"),\n      newOptions.get(\"checkpointLocation\"),"
  },
  {
    "id" : "95ad042a-70e8-45d1-9acc-ff68bfb76ce3",
    "prId" : 28051,
    "prUrl" : "https://github.com/apache/spark/pull/28051#pullrequestreview-383307004",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d31b44f7-b892-4d57-9647-880034bacb01",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "is the `timezone` option effective for writing? AFAIK it's for parsing only.",
        "createdAt" : "2020-03-27T11:48:07Z",
        "updatedAt" : "2020-03-28T11:58:30Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "55507393-2ce4-412c-a2fd-71d597545d98",
        "parentId" : "d31b44f7-b892-4d57-9647-880034bacb01",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "If a pattern contains sub-pattern for time zone like `V` or `z`, which one formatter should output?",
        "createdAt" : "2020-03-27T12:18:03Z",
        "updatedAt" : "2020-03-28T11:58:30Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "3458bfa4-e1a5-422f-ba21-d478173fae18",
        "parentId" : "d31b44f7-b892-4d57-9647-880034bacb01",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "ah got it",
        "createdAt" : "2020-03-27T12:29:47Z",
        "updatedAt" : "2020-03-28T11:58:30Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "ac803363-02a9-4a87-8338-fda127f21734",
        "parentId" : "d31b44f7-b892-4d57-9647-880034bacb01",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "shall we update the comment? It's not for parsing.",
        "createdAt" : "2020-03-27T12:30:11Z",
        "updatedAt" : "2020-03-28T11:58:30Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "fc2a6d95-3ff4-4206-83ce-3d7647677016",
        "parentId" : "d31b44f7-b892-4d57-9647-880034bacb01",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Updated",
        "createdAt" : "2020-03-28T11:58:51Z",
        "updatedAt" : "2020-03-28T11:58:52Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "99ccc589ef9650f817ac2d06ab13aa1ffabff9a8",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +210,214 @@   * You can set the following option(s):\n   * <ul>\n   * <li>`timeZone` (default session local timezone): sets the string that indicates a time zone ID\n   * to be used to format timestamps in the JSON/CSV datasources or partition values. The following\n   * formats of `timeZone` are supported:"
  }
]