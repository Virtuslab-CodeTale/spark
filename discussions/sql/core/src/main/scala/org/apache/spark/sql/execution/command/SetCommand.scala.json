[
  {
    "id" : "8fbba9cd-f71b-4dcb-9df3-182c06e29ed6",
    "prId" : 32144,
    "prUrl" : "https://github.com/apache/spark/pull/32144#pullrequestreview-634518290",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3cb04dd2-0a69-4d7d-960d-38c1d02ed8e3",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Can we make the PR title clearer? It seems the SET command can already set/get hadoop configs in the SQLConf. The only problem is we don't display the default value correctly, which is from `sparkSession.sharedState.hadoopConf`.\r\n\r\nBTW do we have a valid use case? e.g. a hadoop conf is not in `SQLConf` but in `sparkSession.sharedState.hadoopConf`.",
        "createdAt" : "2021-04-13T11:50:03Z",
        "updatedAt" : "2021-04-14T14:43:40Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "365907a0-7a14-4243-9691-f2f1eebcfdda",
        "parentId" : "3cb04dd2-0a69-4d7d-960d-38c1d02ed8e3",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "> Can we make the PR title clearer?\r\n\r\nOK\r\n\r\n> BTW do we have a valid use case? e.g. a hadoop conf is not in `SQLConf` but in `sparkSession.sharedState.hadoopConf`.\r\n\r\nThe pre-loaded ones from `core-site.xml, hive-site.xml` etc., will only stay in `sparkSession.sharedState.hadoopConf` or `sc. _hadoopConfiguation` not `SQLConf`. Some of them that related the Hive Metastore connection(never change it spark runtime), e.g. `hive.metastore.uris`, are clearly global static and unchangeable but displayable I guess. Some of the ones that might be related to, for example, the output codec/compression, preset in Hadoop/hive config files like `core-site.xml` shall bestill changeable from case to case, table to table, file to file, etc. So, it's meaningful to show the defaults for users to change based on that.\r\n",
        "createdAt" : "2021-04-13T12:19:58Z",
        "updatedAt" : "2021-04-14T14:43:40Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "b6d26b7d87917e7e230ca38f2817c6110b461370",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +161,165 @@          // take affect from table to table, file to file, so they are not static and users are\n          // very likely to change them based the default value they see.\n          sparkSession.sharedState.hadoopConf.get(key, \"<undefined>\")\n        }\n        Seq(Row(key, value))"
  },
  {
    "id" : "296a409d-71e2-43de-b5d8-fc4afe1ded46",
    "prId" : 32144,
    "prUrl" : "https://github.com/apache/spark/pull/32144#pullrequestreview-634614203",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bea2b254-dc77-404a-b2dc-3cd697ed2b2e",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "let's add a few comments here.",
        "createdAt" : "2021-04-13T13:49:41Z",
        "updatedAt" : "2021-04-14T14:43:40Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "b6d26b7d87917e7e230ca38f2817c6110b461370",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +161,165 @@          // take affect from table to table, file to file, so they are not static and users are\n          // very likely to change them based the default value they see.\n          sparkSession.sharedState.hadoopConf.get(key, \"<undefined>\")\n        }\n        Seq(Row(key, value))"
  },
  {
    "id" : "4e093722-b261-4333-93ae-16c225f2bf2e",
    "prId" : 30045,
    "prUrl" : "https://github.com/apache/spark/pull/30045#pullrequestreview-510403412",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d648cbc1-28c7-4dbf-b91d-fb057f81c4f9",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "ah so this only adds the warehouse dir config compared to the `sparkContext.conf`?",
        "createdAt" : "2020-10-16T11:04:53Z",
        "updatedAt" : "2020-10-22T15:38:55Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "ed09aa46-bbc4-49ee-bd9a-15a82dc5e43d",
        "parentId" : "d648cbc1-28c7-4dbf-b91d-fb057f81c4f9",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "No. The `sharedState.conf` is a cloned `sparkContext.conf` with other initial options including the warehouse dir and other static/dynamic configs set by the 1st SparkSession instance which creates the one and only `sharedState`. \r\nIf there is existing sc when we create the 1st SparkSession (e.g. see the PR description), the initial configs will go to the `sharedState.conf`, and we use this version of conf as defaults. Also, later created SparkSession with other options will not affect the `sharedState.conf`",
        "createdAt" : "2020-10-16T12:00:45Z",
        "updatedAt" : "2020-10-22T15:38:55Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "d1947961a366cd76086b9996c4d9687b7c63f3f7",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +173,177 @@\n  override def run(sparkSession: SparkSession): Seq[Row] = {\n    val defaults = sparkSession.sharedState.conf\n    config match {\n      case Some(key) =>"
  },
  {
    "id" : "4a1f4c63-ec73-4b98-94c7-3499f6c49e4c",
    "prId" : 29202,
    "prUrl" : "https://github.com/apache/spark/pull/29202#pullrequestreview-457551184",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "49a74c70-61e0-463d-920e-2339c173b240",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "Could you try some SQL conf whose default is NONE? For example, spark.sql.optimizer.excludedRules?\r\n\r\n",
        "createdAt" : "2020-07-29T05:51:45Z",
        "updatedAt" : "2020-07-29T05:51:46Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "df501f90-ea3c-497d-8ec4-6653794c68a2",
        "parentId" : "49a74c70-61e0-463d-920e-2339c173b240",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "Looks like the tests for former version reset command didn't cover these cases either, let me add some.",
        "createdAt" : "2020-07-29T14:00:48Z",
        "updatedAt" : "2020-07-29T14:00:49Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "2bb6d7895a826f60d884fec7ffd5d93843b838a4",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +178,182 @@      case Some(key) =>\n        conf.unsetConf(key)\n        defaults.getOption(key).foreach(conf.setConfString(key, _))\n      case None =>\n        conf.clear()"
  },
  {
    "id" : "2374f9b6-7513-4eb1-aa6a-7e59dd259995",
    "prId" : 29202,
    "prUrl" : "https://github.com/apache/spark/pull/29202#pullrequestreview-457552175",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8a41c1ca-9087-42bb-9308-20a1d5c1d9d6",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "reset some static sql conf? for example, spark.sql.warehouse.dir? do we block it with a reasonable exception message?",
        "createdAt" : "2020-07-29T05:54:34Z",
        "updatedAt" : "2020-07-29T05:54:34Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "ecb8a94e-2e98-4c71-8491-3bde126343c9",
        "parentId" : "8a41c1ca-9087-42bb-9308-20a1d5c1d9d6",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "ah, SET also fails with static conf, this makes sense to me.",
        "createdAt" : "2020-07-29T06:34:53Z",
        "updatedAt" : "2020-07-29T06:34:53Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "5733ad89-9613-4746-ba47-273ec557b0a9",
        "parentId" : "8a41c1ca-9087-42bb-9308-20a1d5c1d9d6",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "I'm ok to keep it consistent with SET command",
        "createdAt" : "2020-07-29T14:01:54Z",
        "updatedAt" : "2020-07-29T14:01:55Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "2bb6d7895a826f60d884fec7ffd5d93843b838a4",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +174,178 @@  override def run(sparkSession: SparkSession): Seq[Row] = {\n    val conf = sparkSession.sessionState.conf\n    val defaults = sparkSession.sparkContext.conf\n    config match {\n      case Some(key) =>"
  }
]