[
  {
    "id" : "e7786f5c-d6cc-44f8-a94d-ad5daace5c7f",
    "prId" : 24327,
    "prUrl" : "https://github.com/apache/spark/pull/24327#pullrequestreview-232148716",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "07bcecf1-c76a-422a-b9dc-f5b8168a838a",
        "parentId" : null,
        "authorId" : "8c82da5a-f351-4a37-a8a9-13809311b07b",
        "body" : "nit: Use object with a val (for code reuse)",
        "createdAt" : "2019-04-27T00:34:29Z",
        "updatedAt" : "2019-06-14T16:45:37Z",
        "lastEditedBy" : "8c82da5a-f351-4a37-a8a9-13809311b07b",
        "tags" : [
        ]
      },
      {
        "id" : "6e9260f2-6101-4a64-8303-7add182b7f37",
        "parentId" : "07bcecf1-c76a-422a-b9dc-f5b8168a838a",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "AFAIK a method just returns a string literal is pretty efficient.",
        "createdAt" : "2019-04-30T12:49:33Z",
        "updatedAt" : "2019-06-14T16:45:37Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "f658e9265ba741922fc96eec76038addcb6491a1",
    "line" : 30,
    "diffHunk" : "@@ -1,1 +28,32 @@  override def fallbackFileFormat: Class[_ <: FileFormat] = classOf[ParquetFileFormat]\n\n  override def shortName(): String = \"parquet\"\n\n  override def getTable(options: CaseInsensitiveStringMap): Table = {"
  },
  {
    "id" : "bd9ddb29-e6be-443d-996e-fd89de9006dc",
    "prId" : 24327,
    "prUrl" : "https://github.com/apache/spark/pull/24327#pullrequestreview-246739699",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bb61bf85-0ea8-4add-8c9a-89fe2d8e547b",
        "parentId" : null,
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "What does this path do?\r\n\r\nParquet files always have their own schema and if I remember correctly, the Parquet read path doesn't support type coercion or files that are missing fields (maybe this was updated?). If that's the case, then shouldn't this path throw an exception?",
        "createdAt" : "2019-05-30T16:06:39Z",
        "updatedAt" : "2019-06-14T16:45:37Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      },
      {
        "id" : "15754d1e-ea48-4e01-9b0f-e4b29cb95ee4",
        "parentId" : "bb61bf85-0ea8-4add-8c9a-89fe2d8e547b",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : ">  the Parquet read path doesn't support type coercion or files that are missing fields \r\n\r\nYes, this is the current status.\r\n\r\nBut as a feature, I think we still need to handle the user-specified schema. The provided schema could be a subset of the inferred schema\r\n",
        "createdAt" : "2019-05-31T03:18:51Z",
        "updatedAt" : "2019-06-14T16:45:37Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "07e436d3-1df5-48f1-b053-954947129568",
        "parentId" : "bb61bf85-0ea8-4add-8c9a-89fe2d8e547b",
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "@gengliangwang, that is not how the DSv2 API works. The user-specified schema is not used to supply a projection. It is used to fill in missing schema details where Spark cannot infer them. This should be removed.",
        "createdAt" : "2019-05-31T16:13:20Z",
        "updatedAt" : "2019-06-14T16:45:37Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      },
      {
        "id" : "3a08b977-0ecd-431a-8b97-bc290f0be829",
        "parentId" : "bb61bf85-0ea8-4add-8c9a-89fe2d8e547b",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "> @gengliangwang, that is not how the DSv2 API works. The user-specified schema is not used to supply a projection. It is used to fill in missing schema details where Spark cannot infer them. This should be removed.\r\n\r\nIf I understand correctly, that is to say, we won't support the following for Parquet V2?\r\n```\r\nspark.read.schema(schema).parquet(...)\r\n```\r\n",
        "createdAt" : "2019-05-31T16:31:33Z",
        "updatedAt" : "2019-06-14T16:45:37Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "97be3544-8e70-49dc-8642-1d8ed432bd93",
        "parentId" : "bb61bf85-0ea8-4add-8c9a-89fe2d8e547b",
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "That pattern is only supported in v1 under very strict requirements for the schema. It makes no sense to continue pretending that it is available in v2.",
        "createdAt" : "2019-05-31T16:46:45Z",
        "updatedAt" : "2019-06-14T16:45:37Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      },
      {
        "id" : "d0745379-c322-47b7-a6bb-afcc433e8e74",
        "parentId" : "bb61bf85-0ea8-4add-8c9a-89fe2d8e547b",
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "Actually, I guess we could continue to support it for backward-compatibility if there is a check that the schema matches the inferred schema exactly and if there is a noticeable warning that this should be removed.\r\n\r\nI would prefer to remove this support in v2. We already know that v2 will require a migration, so we may as well fix this while we can.",
        "createdAt" : "2019-05-31T16:48:20Z",
        "updatedAt" : "2019-06-14T16:45:37Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      },
      {
        "id" : "f73a5f0f-7c0c-40b5-8425-431c7d002e38",
        "parentId" : "bb61bf85-0ea8-4add-8c9a-89fe2d8e547b",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "How about this, let's support it in this migration PR. And we can discuss it in another followup PR, where test cases would be updated/added as well.",
        "createdAt" : "2019-05-31T17:56:04Z",
        "updatedAt" : "2019-06-14T16:45:37Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "dd38acb5-3637-459e-b606-cf68e30ff192",
        "parentId" : "bb61bf85-0ea8-4add-8c9a-89fe2d8e547b",
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "Does this fail tests if it is removed?",
        "createdAt" : "2019-05-31T19:27:13Z",
        "updatedAt" : "2019-06-14T16:45:37Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      },
      {
        "id" : "1b904b09-1572-4ccf-a9d9-0c4f9c1182fb",
        "parentId" : "bb61bf85-0ea8-4add-8c9a-89fe2d8e547b",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "Yes, tests will fail.",
        "createdAt" : "2019-06-01T01:17:10Z",
        "updatedAt" : "2019-06-14T16:45:37Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "9b41a085-93ef-4201-9b15-743aa8ac95da",
        "parentId" : "bb61bf85-0ea8-4add-8c9a-89fe2d8e547b",
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "Okay, let's leave it in to get the majority of this work finished. I just filed SPARK-27960 for the ORC implementation as well, so I think that schema handling in the v2 file sources is a larger issue.",
        "createdAt" : "2019-06-06T18:25:12Z",
        "updatedAt" : "2019-06-14T16:45:37Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      }
    ],
    "commit" : "f658e9265ba741922fc96eec76038addcb6491a1",
    "line" : 41,
    "diffHunk" : "@@ -1,1 +39,43 @@    val paths = getPaths(options)\n    val tableName = getTableName(paths)\n    ParquetTable(tableName, sparkSession, options, paths, Some(schema), fallbackFileFormat)\n  }\n}"
  }
]