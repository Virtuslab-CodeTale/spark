[
  {
    "id" : "ab53db90-e85d-4486-923d-a92ca2823f3f",
    "prId" : 27801,
    "prUrl" : "https://github.com/apache/spark/pull/27801#pullrequestreview-374064417",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "26441aa6-9f54-4479-b16a-5442a4986b9c",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Hi, @manuzhang .\r\nWhich test did you referring in the PR description? Could you add a UT explicitly?\r\n```\r\n## How was this patch tested?\r\n\r\nExisting tests.\r\n```",
        "createdAt" : "2020-03-12T03:49:46Z",
        "updatedAt" : "2020-03-13T05:54:59Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "2bd2fb93-ca01-4798-88e2-93a8aba74614",
        "parentId" : "26441aa6-9f54-4479-b16a-5442a4986b9c",
        "authorId" : "334a081c-cba1-403e-ac80-30e6124077d1",
        "body" : "Sorry, I didn't find a UT for `DistributedFileSystem`. Could you suggest on how to test it ?",
        "createdAt" : "2020-03-12T06:27:03Z",
        "updatedAt" : "2020-03-13T05:54:59Z",
        "lastEditedBy" : "334a081c-cba1-403e-ac80-30e6124077d1",
        "tags" : [
        ]
      },
      {
        "id" : "eecf5c3a-182e-4288-9f88-bb0bd89b2ca5",
        "parentId" : "26441aa6-9f54-4479-b16a-5442a4986b9c",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "`Mock`?\r\n```\r\n$ git grep 'DistributedFileSystem' | grep Suite\r\ncore/src/test/scala/org/apache/spark/deploy/history/FsHistoryProviderSuite.scala:import org.apache.hadoop.hdfs.{DFSInputStream, DistributedFileSystem}\r\ncore/src/test/scala/org/apache/spark/deploy/history/FsHistoryProviderSuite.scala:    val dfs = mock(classOf[DistributedFileSystem])\r\nsql/core/src/test/scala/org/apache/spark/sql/execution/datasources/FileSourceStrategySuite.scala:      \"fs.file.impl\" -> classOf[MockDistributedFileSystem].getName,\r\nsql/core/src/test/scala/org/apache/spark/sql/execution/datasources/FileSourceStrategySuite.scala:// This file system is for SPARK-14959 (DistributedFileSystem will throw an exception\r\nsql/core/src/test/scala/org/apache/spark/sql/execution/datasources/FileSourceStrategySuite.scala:class MockDistributedFileSystem extends RawLocalFileSystem {\r\n```",
        "createdAt" : "2020-03-12T08:47:24Z",
        "updatedAt" : "2020-03-13T05:54:59Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "c89a08e3-a90d-4506-b0da-d063017d9f5e",
        "parentId" : "26441aa6-9f54-4479-b16a-5442a4986b9c",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "BTW, without a test case, your contribution is not protected from accidental removal. That's the reason why we ask a UT always.",
        "createdAt" : "2020-03-12T08:48:30Z",
        "updatedAt" : "2020-03-13T05:54:59Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "c6584f4c-6a28-4dad-a084-5d47fa5a2ea3",
        "parentId" : "26441aa6-9f54-4479-b16a-5442a4986b9c",
        "authorId" : "334a081c-cba1-403e-ac80-30e6124077d1",
        "body" : "thanks, UT added",
        "createdAt" : "2020-03-13T05:56:02Z",
        "updatedAt" : "2020-03-13T05:56:03Z",
        "lastEditedBy" : "334a081c-cba1-403e-ac80-30e6124077d1",
        "tags" : [
        ]
      }
    ],
    "commit" : "93e7e0da42931801cbd4d06493d7753f3a8364e0",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +315,319 @@        // to listStatus is because the default implementation would potentially throw a\n        // FileNotFoundException which is better handled by doing the lookups manually below.\n        case (_: DistributedFileSystem | _: ViewFileSystem) if !ignoreLocality =>\n          val remoteIter = fs.listLocatedStatus(path)\n          new Iterator[LocatedFileStatus]() {"
  },
  {
    "id" : "fbbb761f-4b32-4939-ad1e-2170913428ff",
    "prId" : 27213,
    "prUrl" : "https://github.com/apache/spark/pull/27213#pullrequestreview-352691079",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fd8c1513-7d56-479c-b681-a38885db33e1",
        "parentId" : null,
        "authorId" : "f06b291a-bc5c-4fd8-9bac-e5a6d035891b",
        "body" : "Just add an assertion incidentally here, this is not necessary for this PR.\r\nif not ok, i can remove it.",
        "createdAt" : "2020-02-04T02:30:25Z",
        "updatedAt" : "2020-02-10T08:58:10Z",
        "lastEditedBy" : "f06b291a-bc5c-4fd8-9bac-e5a6d035891b",
        "tags" : [
        ]
      }
    ],
    "commit" : "2e210a092442bbc65ff92f97a2eb2c06aaf83376",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +61,65 @@    s\"The rootPathsSpecified ($rootPathsSpecified) is inconsistent with the file paths \" +\n      s\"of userSpecifiedPartitionSpec (${userSpecifiedPartitionSpec.get.partitions.map(_.path)}).\")\n\n  // Filter out streaming metadata dirs or files such as \"/.../_spark_metadata\" (the metadata dir)\n  // or \"/.../_spark_metadata/0\" (a file in the metadata dir). `rootPathsSpecified` might contain"
  },
  {
    "id" : "32fc0619-aae4-413f-858b-2e91fc5a444a",
    "prId" : 27079,
    "prUrl" : "https://github.com/apache/spark/pull/27079#pullrequestreview-339609440",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0e1dacc7-e07a-4866-a63d-e2890306dbf2",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Nit: there may not be 100 paths here. Just say \"The first several paths are ...\" and maybe just show like 10?",
        "createdAt" : "2020-01-07T14:33:44Z",
        "updatedAt" : "2020-01-08T01:45:47Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "eefc50c5-ba35-4235-a653-9bc3164dcec7",
        "parentId" : "0e1dacc7-e07a-4866-a63d-e2890306dbf2",
        "authorId" : "78cb7df6-a1c6-4bcb-99c1-16fd18fb32f7",
        "body" : "yes, that seems better",
        "createdAt" : "2020-01-08T01:48:57Z",
        "updatedAt" : "2020-01-08T01:48:57Z",
        "lastEditedBy" : "78cb7df6-a1c6-4bcb-99c1-16fd18fb32f7",
        "tags" : [
        ]
      }
    ],
    "commit" : "b7ac910a8431f7bad97efe5d0cbeb2b64d04e775",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +192,196 @@    }\n\n    logInfo(s\"Listing leaf files and directories in parallel under ${paths.length} paths.\" +\n      s\" The first several paths are: ${paths.take(10).mkString(\", \")}.\")\n    HiveCatalogMetrics.incrementParallelListingJobCount(1)"
  },
  {
    "id" : "2d783e74-228d-4bb6-93b3-6a6743e6ce0a",
    "prId" : 26850,
    "prUrl" : "https://github.com/apache/spark/pull/26850#pullrequestreview-339082382",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4a932239-0ed9-4a7a-b464-d3032ac8a42b",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "why not put the `if (userSpecifiedPartitionSpec.isDefined)` inside this if?",
        "createdAt" : "2020-01-07T07:29:07Z",
        "updatedAt" : "2020-01-07T13:59:08Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "7cdfaffb-b5b0-4570-ac87-8a0358434c8e",
        "parentId" : "4a932239-0ed9-4a7a-b464-d3032ac8a42b",
        "authorId" : "f06b291a-bc5c-4fd8-9bac-e5a6d035891b",
        "body" : "The test failed when I put it in this if branch : \r\nhttps://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/116149/testReport\r\nBut it should be an irrelevant error. Will retry it.",
        "createdAt" : "2020-01-07T08:10:22Z",
        "updatedAt" : "2020-01-07T13:59:08Z",
        "lastEditedBy" : "f06b291a-bc5c-4fd8-9bac-e5a6d035891b",
        "tags" : [
        ]
      }
    ],
    "commit" : "5ff1905b511512c29e7225a87ae4dd75c685baa1",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +75,79 @@        cachedPartitionSpec = userSpecifiedPartitionSpec.get\n      } else {\n        cachedPartitionSpec = inferPartitioning()\n      }\n    }"
  },
  {
    "id" : "a15ea823-3f61-44c7-b8a9-c8612f861de9",
    "prId" : 24672,
    "prUrl" : "https://github.com/apache/spark/pull/24672#pullrequestreview-240439909",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2c8cf5b2-a446-4859-93e2-345b8fce97c2",
        "parentId" : null,
        "authorId" : "a11722a8-ecfa-4827-88e3-4d708f023846",
        "body" : "FYI, over at https://github.com/apache/spark/pull/24668 I'm proposing a change to the `FileNotFoundException` handling logic here: after my patch's changes, we'll only ignore `FileNotFoundException` if  we are not listing the root directory of the table (i.e. we are listing a subfolder) _and_ the `ignoreMissingFiles` setting has not been set.\r\n\r\nDoes that change have any implications for this PR?",
        "createdAt" : "2019-05-22T05:53:32Z",
        "updatedAt" : "2019-05-22T05:53:59Z",
        "lastEditedBy" : "a11722a8-ecfa-4827-88e3-4d708f023846",
        "tags" : [
        ]
      },
      {
        "id" : "097a6779-44f6-4e7b-a073-15d145485e84",
        "parentId" : "2c8cf5b2-a446-4859-93e2-345b8fce97c2",
        "authorId" : "b636c5db-fb7c-43dd-afa5-7b84717a9c28",
        "body" : "I believe that should not matter here as I tried to preserve the original logic (which would be your new logic now) when using listStatus which has that issue. The only real change here is using the DistributedFileSystem.listLocatedStatus and there should be no case where FileNotFoundException happens when using that as they will all be LocatedFileStatus that are returned in the array.",
        "createdAt" : "2019-05-22T05:57:34Z",
        "updatedAt" : "2019-05-22T05:57:34Z",
        "lastEditedBy" : "b636c5db-fb7c-43dd-afa5-7b84717a9c28",
        "tags" : [
        ]
      },
      {
        "id" : "e0e58ef6-cf9f-4185-bedd-d62abec31616",
        "parentId" : "2c8cf5b2-a446-4859-93e2-345b8fce97c2",
        "authorId" : "a11722a8-ecfa-4827-88e3-4d708f023846",
        "body" : "Ah, I think I get it now: I was originally confused by the comment, but on re-read it sounds like you're saying that we _could_, in principle, just call `listLocatedStatus()` in all cases, except that there's no performance benefit for non `DistributedFileSystem` subclasses and we lose more granular error handling.\r\n\r\nIn fact, the granular error handling could be really important in order to preserve the behavior before my changes in #24668 (a.k.a. the new behavior when `ignoreMissingFiles = true`): previously, the deletion of a leaf file after a directory listing but before listing the file status would cause _only that file_ to be ignored, whereas using `listLocatedStatus` for non-`DistributedFileSystem` subclasses would allow that `FileNotFoundException` to trigger the dropping of _all_ files from the listing (even ones whose statuses could be retrieved).\r\n\r\nI feel like `DistributedFileSystem` must be having some sort of internal \"atomicity\"-type guarantee for this to be safe, though (that it if it sees a file then it's able to also see its status).",
        "createdAt" : "2019-05-22T06:25:22Z",
        "updatedAt" : "2019-05-22T06:25:23Z",
        "lastEditedBy" : "a11722a8-ecfa-4827-88e3-4d708f023846",
        "tags" : [
        ]
      },
      {
        "id" : "99abe156-4f0d-4877-831f-cd50857a4d70",
        "parentId" : "2c8cf5b2-a446-4859-93e2-345b8fce97c2",
        "authorId" : "b636c5db-fb7c-43dd-afa5-7b84717a9c28",
        "body" : "Yeah these were my thoughts exactly. My first version of this removed error handling logic and just used listLocatedStatus instead but when i inspected the source I realized we'd definitely run into issues with discarding the entire directory with 1 file not found when its not a DistributedFileSystem. And I assume the some for the atomicity of getting it from the namenode.",
        "createdAt" : "2019-05-22T06:32:59Z",
        "updatedAt" : "2019-05-22T06:35:40Z",
        "lastEditedBy" : "b636c5db-fb7c-43dd-afa5-7b84717a9c28",
        "tags" : [
        ]
      },
      {
        "id" : "c34161ad-c11d-45d0-9423-c0a66a3d9d27",
        "parentId" : "2c8cf5b2-a446-4859-93e2-345b8fce97c2",
        "authorId" : "a11722a8-ecfa-4827-88e3-4d708f023846",
        "body" : "ðŸ‘ \r\n\r\nBased on this discussion I've strengthened my test assertions to guard against this problem: https://github.com/apache/spark/pull/24668/commits/42e8b9881d2654150a92bbc7fc341904f4862900. The tests will now fail in case `listLocatedStatus()` is always unconditionally used.",
        "createdAt" : "2019-05-22T06:59:38Z",
        "updatedAt" : "2019-05-22T06:59:38Z",
        "lastEditedBy" : "a11722a8-ecfa-4827-88e3-4d708f023846",
        "tags" : [
        ]
      }
    ],
    "commit" : "40830175b983e71354dfcea79ea02ab7dca1e43e",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +281,285 @@        // to retrieve the file status with the file block location. The reason to still fallback\n        // to listStatus is because the default implementation would potentially throw a\n        // FileNotFoundException which is better handled by doing the lookups manually below.\n        case _: DistributedFileSystem =>\n          val remoteIter = fs.listLocatedStatus(path)"
  },
  {
    "id" : "fa26df21-55bb-4e0b-8d62-840ff2119672",
    "prId" : 24672,
    "prUrl" : "https://github.com/apache/spark/pull/24672#pullrequestreview-241426832",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "daab1a19-9f47-42ac-863a-f266e2b50378",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Why do you need to wrap the iterator? Should be able to use JavaConversions at worst to make it .asScala.toArray",
        "createdAt" : "2019-05-23T17:55:04Z",
        "updatedAt" : "2019-05-23T17:55:15Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "fc05dedb-d183-48e5-a2e2-0d1681c0640d",
        "parentId" : "daab1a19-9f47-42ac-863a-f266e2b50378",
        "authorId" : "b636c5db-fb7c-43dd-afa5-7b84717a9c28",
        "body" : "It's not a real iterator unfortunately its a RemoteIterator which which doesn't actually extend the Iterator interface due to throwing IOExceptions.",
        "createdAt" : "2019-05-23T18:12:40Z",
        "updatedAt" : "2019-05-23T18:18:58Z",
        "lastEditedBy" : "b636c5db-fb7c-43dd-afa5-7b84717a9c28",
        "tags" : [
        ]
      },
      {
        "id" : "360aeef2-205a-47f0-923e-fdc06f12057c",
        "parentId" : "daab1a19-9f47-42ac-863a-f266e2b50378",
        "authorId" : "b636c5db-fb7c-43dd-afa5-7b84717a9c28",
        "body" : "This seemed cleaner to me than a while loop and an array buffer which is another way to do it. I could do it that way if its preferred or I'm also open to other suggestions",
        "createdAt" : "2019-05-23T18:25:00Z",
        "updatedAt" : "2019-05-23T18:25:00Z",
        "lastEditedBy" : "b636c5db-fb7c-43dd-afa5-7b84717a9c28",
        "tags" : [
        ]
      },
      {
        "id" : "0c639e0a-b311-45fc-8abd-42bfc0869c69",
        "parentId" : "daab1a19-9f47-42ac-863a-f266e2b50378",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "I see, makes sense then. ",
        "createdAt" : "2019-05-23T20:13:40Z",
        "updatedAt" : "2019-05-23T20:13:40Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      }
    ],
    "commit" : "40830175b983e71354dfcea79ea02ab7dca1e43e",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +284,288 @@        case _: DistributedFileSystem =>\n          val remoteIter = fs.listLocatedStatus(path)\n          new Iterator[LocatedFileStatus]() {\n            def next(): LocatedFileStatus = remoteIter.next\n            def hasNext(): Boolean = remoteIter.hasNext"
  },
  {
    "id" : "83dffd54-1029-4315-9c5a-704238a81c1f",
    "prId" : 24672,
    "prUrl" : "https://github.com/apache/spark/pull/24672#pullrequestreview-242827696",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c2de0f9e-6841-41a7-9024-e341bcb51fad",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "@melin Are you hitting a perf regression? Do we need an internal SQLConf? \r\n\r\ncc @JoshRosen ",
        "createdAt" : "2019-05-27T05:18:34Z",
        "updatedAt" : "2019-05-27T05:18:55Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "6fc22648-331d-4a5f-bcf4-7816f072d4ba",
        "parentId" : "c2de0f9e-6841-41a7-9024-e341bcb51fad",
        "authorId" : "a11722a8-ecfa-4827-88e3-4d708f023846",
        "body" : "Unless I'm misunderstanding something, I don't think that @melin's posted benchmark is an apples-to-apples comparison: `listLocatedStatus` is going to be slower than `listStatus` because it's doing more work in order to get location information. In order for this to be a fair performance comparison you'd want to compare `listStatus + getBlockLocations` to `listLocatedStatus` (because we'd end up having to call `getBlockLocations` further down if we didn't compute the locations as part of `listLocatedStatus`).\r\n\r\nThe direct comparison is only useful in cases where we're never going to make use of the location information and to my knowledge we currently do not have a flag to bypass locality fetching in `InMemoryFileIndex` (so the location-free performance isn't relevant).",
        "createdAt" : "2019-05-27T05:45:27Z",
        "updatedAt" : "2019-05-27T05:46:56Z",
        "lastEditedBy" : "a11722a8-ecfa-4827-88e3-4d708f023846",
        "tags" : [
        ]
      },
      {
        "id" : "8baad113-9b57-4c65-a283-ace45eaa7405",
        "parentId" : "c2de0f9e-6841-41a7-9024-e341bcb51fad",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "I'm +1 for @JoshRosen 's comment.\r\nAnd, Yep. Of course, if there is a corner case, we had better have conf for this. For now, it looks like we need more description about what he is hitting. SPARK JIRA would be good for that.",
        "createdAt" : "2019-05-27T05:48:01Z",
        "updatedAt" : "2019-05-27T05:52:27Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "e98c9cbd-2f4c-4ecf-b891-cc4bfcc6e681",
        "parentId" : "c2de0f9e-6841-41a7-9024-e341bcb51fad",
        "authorId" : "b636c5db-fb7c-43dd-afa5-7b84717a9c28",
        "body" : "Before coming up with this change I was actually wondering why there was no conf options to ignore locality and why it was required to do those extra lookups. In situations where the spark job is running on a small set of servers out of a big HDFS cluster, locality is a bit useless as the hit rate is super low, the time to get block locations was getting out of control for us and locality wait per task was slowing down the jobs. \r\n\r\nI think this change to get the blocks from the namenode is good enough for us now but I can definitely see the benefit of having a conf option to disable locality entirely which would turn off fetching of getFileBlockLocations and also use listStatus always instead of my optimization to use listLocatedStatus. ",
        "createdAt" : "2019-05-28T17:43:56Z",
        "updatedAt" : "2019-05-28T17:43:56Z",
        "lastEditedBy" : "b636c5db-fb7c-43dd-afa5-7b84717a9c28",
        "tags" : [
        ]
      },
      {
        "id" : "f078d748-61aa-46e7-9da7-6a3843cc5b48",
        "parentId" : "c2de0f9e-6841-41a7-9024-e341bcb51fad",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Please file a JIRA for the further discussions on new `conf` if it's not filed yet. (I didn't search it, but might be exist in some other context.)\r\n> having a conf option to disable locality entirely which would turn off fetching of getFileBlockLocations",
        "createdAt" : "2019-05-28T17:52:13Z",
        "updatedAt" : "2019-05-28T17:52:13Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "40830175b983e71354dfcea79ea02ab7dca1e43e",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +282,286 @@        // to listStatus is because the default implementation would potentially throw a\n        // FileNotFoundException which is better handled by doing the lookups manually below.\n        case _: DistributedFileSystem =>\n          val remoteIter = fs.listLocatedStatus(path)\n          new Iterator[LocatedFileStatus]() {"
  }
]