[
  {
    "id" : "e5d661b9-87de-49f3-8c07-fe3e3d6717d4",
    "prId" : 31423,
    "prUrl" : "https://github.com/apache/spark/pull/31423#pullrequestreview-581033105",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d8fc5630-d535-4ca6-92e6-8420829a3f8e",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "New index re-uses the file statuses cache `fileStatusCache`, so, this should allow to avoid additional accesses to the file  system.",
        "createdAt" : "2021-02-02T06:58:13Z",
        "updatedAt" : "2021-02-03T08:19:07Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "eeda34a4dca66c9ae747c0f5fa1b963f877f6c87",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +420,424 @@            getOrInferFileFormatSchema(format, () => indexForSchemaInference)\n          val index = new InMemoryFileIndex(\n            sparkSession, globbedPaths, options, Some(resultPartitionSchema), fileStatusCache)\n          (index, resultDataSchema, resultPartitionSchema)\n        }"
  },
  {
    "id" : "9516b962-6a31-42d6-9168-828831fb6c89",
    "prId" : 31423,
    "prUrl" : "https://github.com/apache/spark/pull/31423#pullrequestreview-582099348",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e035c1e9-44bb-456e-851a-bc82d91970fc",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "What's diff from calling `createInMemoryFileIndex` here?",
        "createdAt" : "2021-02-03T08:02:06Z",
        "updatedAt" : "2021-02-03T08:19:07Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "075b03e6-bb57-4d59-bdba-d69547f99059",
        "parentId" : "e035c1e9-44bb-456e-851a-bc82d91970fc",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "`fileStatusCache`. I want to re-use it.",
        "createdAt" : "2021-02-03T08:16:31Z",
        "updatedAt" : "2021-02-03T08:19:07Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "eeda34a4dca66c9ae747c0f5fa1b963f877f6c87",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +416,420 @@          val fileStatusCache = FileStatusCache.getOrCreate(sparkSession)\n          val indexForSchemaInference = new InMemoryFileIndex(\n            sparkSession, globbedPaths, options, userSpecifiedSchema, fileStatusCache)\n          val (resultDataSchema, resultPartitionSchema) =\n            getOrInferFileFormatSchema(format, () => indexForSchemaInference)"
  },
  {
    "id" : "a38e6c68-fe3f-48ec-aea5-4ebc9e3b6156",
    "prId" : 29437,
    "prUrl" : "https://github.com/apache/spark/pull/29437#pullrequestreview-469009097",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7d4eaede-f4aa-416b-be8b-71f9bfe9f696",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Fixing at the caller seems good. But `inferSchema` is actually implemented by various format, we don't know if they will use `path` in the implementation. That's one concern.",
        "createdAt" : "2020-08-18T04:16:36Z",
        "updatedAt" : "2020-08-19T05:34:32Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "c9e36c4c-679a-4761-a4d1-0fe776d3e1ef",
        "parentId" : "7d4eaede-f4aa-416b-be8b-71f9bfe9f696",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "`FileFormat` is an internal API, I think it's natural to assume `paths` parameter will be used, and all builtin implementations do.",
        "createdAt" : "2020-08-18T05:51:41Z",
        "updatedAt" : "2020-08-19T05:34:32Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "6f147ee0ac20af8142a3dc715a9dbc7952f99265",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +196,200 @@      format.inferSchema(\n        sparkSession,\n        caseInsensitiveOptions - \"path\",\n        tempFileIndex.allFiles())\n    }.getOrElse {"
  },
  {
    "id" : "b3a0b518-98c6-4915-a67a-265e0374cce6",
    "prId" : 25899,
    "prUrl" : "https://github.com/apache/spark/pull/25899#pullrequestreview-315163818",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ed3fd19d-0922-44a5-92b2-b3a3d6886bf0",
        "parentId" : null,
        "authorId" : "224991ce-ad69-410b-8143-bf394b6b5c59",
        "body" : "is there ever the case that the cause is null?",
        "createdAt" : "2019-11-11T13:56:35Z",
        "updatedAt" : "2019-12-11T08:38:18Z",
        "lastEditedBy" : "224991ce-ad69-410b-8143-bf394b6b5c59",
        "tags" : [
        ]
      },
      {
        "id" : "e32a224e-11c9-4bd5-8274-cb1791a27afc",
        "parentId" : "ed3fd19d-0922-44a5-92b2-b3a3d6886bf0",
        "authorId" : "810d9033-ebe0-4a7b-a1e4-9dd98fbaaf44",
        "body" : "SparkException comes from [ThreadUtils#parmap](https://github.com/apache/spark/blob/a6a27485851a160b9fc4bb86481c60f10573aad0/core/src/main/scala/org/apache/spark/util/ThreadUtils.scala#L285) and [ThreadUtils#awaitResult](https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/util/ThreadUtils.scala#L227)\r\n\r\nWhich always seems to wrap another exception i.e. never null",
        "createdAt" : "2019-11-11T21:21:42Z",
        "updatedAt" : "2019-12-11T08:38:18Z",
        "lastEditedBy" : "810d9033-ebe0-4a7b-a1e4-9dd98fbaaf44",
        "tags" : [
        ]
      }
    ],
    "commit" : "105235ce1cdbdfffb912baba02d31abd009f44c2",
    "line" : 59,
    "diffHunk" : "@@ -1,1 +764,768 @@        }.flatten\n      } catch {\n        case e: SparkException => throw e.getCause\n      }\n"
  },
  {
    "id" : "b5d42501-be73-43f4-b2e3-7567e0b83968",
    "prId" : 25899,
    "prUrl" : "https://github.com/apache/spark/pull/25899#pullrequestreview-330369636",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3980865d-6faf-428f-a680-e6b6bb25a335",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Can you pull this out of what's parallelized? here and below? I don't think it has to be retrieved individually for each call",
        "createdAt" : "2019-12-10T13:24:43Z",
        "updatedAt" : "2019-12-11T08:38:18Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "8e6a8990-943d-4349-a8ad-2208d78144f6",
        "parentId" : "3980865d-6faf-428f-a680-e6b6bb25a335",
        "authorId" : "810d9033-ebe0-4a7b-a1e4-9dd98fbaaf44",
        "body" : "https://github.com/apache/spark/pull/25899/commits/8cbc28aad9aa34ad0ab0d16609863cbfbea176d7\r\n\r\nThis probably never happens, but what if Spark is given paths with different filesystems?\r\ni.e. `spark.read().csv(\"s3://file1\", \"hdfs://file2\")`",
        "createdAt" : "2019-12-11T00:05:26Z",
        "updatedAt" : "2019-12-11T08:38:18Z",
        "lastEditedBy" : "810d9033-ebe0-4a7b-a1e4-9dd98fbaaf44",
        "tags" : [
        ]
      },
      {
        "id" : "bd17e002-ae1f-4f26-8335-58dcb65bfb49",
        "parentId" : "3980865d-6faf-428f-a680-e6b6bb25a335",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Makes sense, I'd leave it.",
        "createdAt" : "2019-12-11T02:12:01Z",
        "updatedAt" : "2019-12-11T08:38:18Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "f659605c-0965-4662-bcea-245d5468a351",
        "parentId" : "3980865d-6faf-428f-a680-e6b6bb25a335",
        "authorId" : "810d9033-ebe0-4a7b-a1e4-9dd98fbaaf44",
        "body" : "@srowen Sorry, just to clarify, should I revert https://github.com/apache/spark/commit/8cbc28aad9aa34ad0ab0d16609863cbfbea176d7?",
        "createdAt" : "2019-12-11T08:16:59Z",
        "updatedAt" : "2019-12-11T08:38:18Z",
        "lastEditedBy" : "810d9033-ebe0-4a7b-a1e4-9dd98fbaaf44",
        "tags" : [
        ]
      },
      {
        "id" : "3f822dce-49c6-4873-b00f-73d9f417a227",
        "parentId" : "3980865d-6faf-428f-a680-e6b6bb25a335",
        "authorId" : "810d9033-ebe0-4a7b-a1e4-9dd98fbaaf44",
        "body" : "I reverted it here: https://github.com/apache/spark/pull/25899/commits/105235ce1cdbdfffb912baba02d31abd009f44c2",
        "createdAt" : "2019-12-11T08:38:42Z",
        "updatedAt" : "2019-12-11T08:38:42Z",
        "lastEditedBy" : "810d9033-ebe0-4a7b-a1e4-9dd98fbaaf44",
        "tags" : [
        ]
      }
    ],
    "commit" : "105235ce1cdbdfffb912baba02d31abd009f44c2",
    "line" : 49,
    "diffHunk" : "@@ -1,1 +754,758 @@      try {\n        ThreadUtils.parmap(globPaths, \"globPath\", numThreads) { globPath =>\n          val fs = globPath.getFileSystem(hadoopConf)\n          val globResult = SparkHadoopUtil.get.globPath(fs, globPath)\n"
  },
  {
    "id" : "b0fdaf3f-14d8-4ecb-b5ae-e2b4db323dec",
    "prId" : 25465,
    "prUrl" : "https://github.com/apache/spark/pull/25465#pullrequestreview-277237805",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9dd1a86a-04d8-4a9d-a8ed-5aefde6541e4",
        "parentId" : null,
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "I know this is following the previous logic.\r\nBut what if the configuration `USE_V1_SOURCE_LIST` contains the canonical name of the `DataSourceRegister` class.  Maybe we can check this as well.",
        "createdAt" : "2019-08-20T13:24:32Z",
        "updatedAt" : "2019-08-27T05:48:29Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "8af82e6d-92d6-4c49-b849-0435c40ab621",
        "parentId" : "9dd1a86a-04d8-4a9d-a8ed-5aefde6541e4",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "It's checked in the next line.",
        "createdAt" : "2019-08-20T15:07:56Z",
        "updatedAt" : "2019-08-27T05:48:29Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "d9f664438417ee8f4bb952d41f53c8c0b0e8d500",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +723,727 @@    val cls = lookupDataSource(provider, conf)\n    cls.newInstance() match {\n      case d: DataSourceRegister if useV1Sources.contains(d.shortName()) => None\n      case t: TableProvider\n          if !useV1Sources.contains(cls.getCanonicalName.toLowerCase(Locale.ROOT)) =>"
  }
]