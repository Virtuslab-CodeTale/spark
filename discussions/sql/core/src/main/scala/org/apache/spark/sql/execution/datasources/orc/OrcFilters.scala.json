[
  {
    "id" : "2a67eed2-f23a-43d6-a996-333ea05c2e74",
    "prId" : 33030,
    "prUrl" : "https://github.com/apache/spark/pull/33030#pullrequestreview-689865533",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1422905d-32fe-40c1-92a4-cf79a6bc0fcb",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Thank you so much for your swift fix, @cloud-fan . Do you think we can have a test coverage to prevent us from making this mistake again?",
        "createdAt" : "2021-06-22T18:16:56Z",
        "updatedAt" : "2021-06-22T18:17:04Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "2fe46030-93b2-4bcb-8a40-edc34343025e",
        "parentId" : "1422905d-32fe-40c1-92a4-cf79a6bc0fcb",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "The test added by @yaooqinn should be good enough. AFAIK ORC is the only popular file format that supports CHAR/VARCHAR natively, and the only file format in Spark that we use a parser to convert its schema to catalyst schema.\r\n\r\nFor long-term maintainability, I think we should follow `ParquetToSparkSchemaConverter` and write a true schema converter for ORC, instead of using the parser.",
        "createdAt" : "2021-06-22T18:25:54Z",
        "updatedAt" : "2021-06-22T18:25:55Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "87124f2c7e7827a776a71c091d88ab1baa9d8d22",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +143,147 @@    case ByteType | ShortType | IntegerType | LongType => PredicateLeaf.Type.LONG\n    case FloatType | DoubleType => PredicateLeaf.Type.FLOAT\n    case StringType => PredicateLeaf.Type.STRING\n    case DateType => PredicateLeaf.Type.DATE\n    case TimestampType => PredicateLeaf.Type.TIMESTAMP"
  }
]