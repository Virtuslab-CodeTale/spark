[
  {
    "id" : "8ba58951-3df1-4664-8ea0-041bed7b3669",
    "prId" : 29975,
    "prUrl" : "https://github.com/apache/spark/pull/29975#pullrequestreview-504464324",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0859c815-3f57-494e-8e33-0ea913776b54",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "can you give an example of this part of generated code for the query in https://issues.apache.org/jira/browse/SPARK-32989 ?",
        "createdAt" : "2020-10-08T06:17:20Z",
        "updatedAt" : "2020-10-12T01:42:10Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "d91688fe-a5f9-49c2-86ac-de38519742e0",
        "parentId" : "0859c815-3f57-494e-8e33-0ea913776b54",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "OK. I will update to the PR description.",
        "createdAt" : "2020-10-08T06:27:14Z",
        "updatedAt" : "2020-10-12T01:42:10Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "fcaafdde-c70b-4ab0-9e49-6d9a77c7a684",
        "parentId" : "0859c815-3f57-494e-8e33-0ea913776b54",
        "authorId" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "body" : "manual testing examples like SPARK-32989 , the performance is much better than before :)",
        "createdAt" : "2020-10-08T06:30:14Z",
        "updatedAt" : "2020-10-12T01:42:10Z",
        "lastEditedBy" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "tags" : [
        ]
      },
      {
        "id" : "b1297dbe-cb61-403a-9d52-09d5136b850c",
        "parentId" : "0859c815-3f57-494e-8e33-0ea913776b54",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Thanks for testing! It is late in my timezone, I will update the generated code tomorrow.",
        "createdAt" : "2020-10-08T06:32:59Z",
        "updatedAt" : "2020-10-12T01:42:10Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "2414bb041f47c00de28428014372ab6c55435e56",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +80,84 @@    // Evaluation of non-deterministic expressions can't be deferred.\n    val nonDeterministicAttrs = projectList.filterNot(_.deterministic).map(_.toAttribute)\n    s\"\"\"\n       |// common sub-expressions\n       |${evaluateVariables(localValInputs)}"
  },
  {
    "id" : "85d6dbce-64af-44b6-8afe-38c4fa1a1570",
    "prId" : 29681,
    "prUrl" : "https://github.com/apache/spark/pull/29681#pullrequestreview-484039597",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c1b389c9-ad3f-4eaa-a42f-d31012f85b77",
        "parentId" : null,
        "authorId" : "3a227965-84e0-47cf-9974-11293764f028",
        "body" : "For reviewers: There are no changes within this `else` block.\r\nThe actual change is `if` block above.",
        "createdAt" : "2020-09-08T11:26:41Z",
        "updatedAt" : "2020-09-10T10:37:55Z",
        "lastEditedBy" : "3a227965-84e0-47cf-9974-11293764f028",
        "tags" : [
        ]
      }
    ],
    "commit" : "c3f2f6740c12bdd6e9d25e3cc8c707b359cc53d7",
    "line" : 65,
    "diffHunk" : "@@ -1,1 +566,570 @@    if (isEmptyRange) {\n      new EmptyRDD[InternalRow](sqlContext.sparkContext)\n    } else {\n      sqlContext\n        .sparkContext"
  },
  {
    "id" : "39840099-0158-4e93-96cf-6e16d406062b",
    "prId" : 27509,
    "prUrl" : "https://github.com/apache/spark/pull/27509#pullrequestreview-395928328",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4a1b6a54-1fbb-4915-83e8-3d36fea46758",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "Can we remove the space before \":\"?\r\n\r\n```\r\n(3) Filter [codegen id : 1]\r\nInput [1]: [col.dots#22]\r\nCondition : (isnotnull(col.dots#22) AND (col.dots#22 = 500))\r\n```",
        "createdAt" : "2020-04-18T16:24:23Z",
        "updatedAt" : "2020-04-18T16:24:23Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      }
    ],
    "commit" : "1290cd523d6fdead5399923ea4acac450a5c2175",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +245,249 @@       |(${ExplainUtils.getOpId(this)}) $nodeName ${ExplainUtils.getCodegenId(this)}\n       |${ExplainUtils.generateFieldString(\"Input\", child.output)}\n       |Condition : ${condition}\n     \"\"\".stripMargin\n  }"
  },
  {
    "id" : "c40d6b56-8a24-4a8d-874c-e55aba940f9b",
    "prId" : 26387,
    "prUrl" : "https://github.com/apache/spark/pull/26387#pullrequestreview-311242897",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "35e3c63b-60d3-4436-b76a-041845b94d7b",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Good catch!",
        "createdAt" : "2019-11-04T16:17:35Z",
        "updatedAt" : "2019-11-04T16:17:35Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "1c2af809-3859-46d3-b091-69ba60127d06",
        "parentId" : "35e3c63b-60d3-4436-b76a-041845b94d7b",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Oh, yes. Indeed.",
        "createdAt" : "2019-11-04T17:43:23Z",
        "updatedAt" : "2019-11-04T17:43:23Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "09b3abe2688054f08108e256e097e4a4fe5c9269",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +301,305 @@\n  override def needCopyResult: Boolean = {\n    child.asInstanceOf[CodegenSupport].needCopyResult || withReplacement\n  }\n"
  },
  {
    "id" : "0e2dce3a-c62e-4b0b-a2e2-e09258067ae0",
    "prId" : 25319,
    "prUrl" : "https://github.com/apache/spark/pull/25319#pullrequestreview-270251825",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "603feecd-3932-4682-b0d4-36f38f06bd99",
        "parentId" : null,
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "What's your plan to add this info for all Exec nodes? Add the function into SparkPlan?",
        "createdAt" : "2019-08-02T10:27:12Z",
        "updatedAt" : "2019-08-02T17:52:21Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      },
      {
        "id" : "5d8b2712-3158-48e0-995a-dc0c3adb85ca",
        "parentId" : "603feecd-3932-4682-b0d4-36f38f06bd99",
        "authorId" : "51be1be0-7c72-4f32-a21f-91b1707ec363",
        "body" : "Different nodes must implement diferent versions of addExtra info because the structure of the nodes and the interesting informatiÃ³n to show is different.\r\nOf course we could move this function to SparkPlan, with a predefined empty behavior and implement it in the different exec nodes that we want to add extra info. Do you think it would be better?",
        "createdAt" : "2019-08-02T15:37:10Z",
        "updatedAt" : "2019-08-02T17:52:21Z",
        "lastEditedBy" : "51be1be0-7c72-4f32-a21f-91b1707ec363",
        "tags" : [
        ]
      }
    ],
    "commit" : "29d135c3c0f9873541b4a8a574c5467de8c285c5",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +617,621 @@  }\n\n  def addExtraInfo: RDD[InternalRow] => RDD[InternalRow] = {\n    _.setExtraInfo(s\"Union(${output.mkString(\", \")})\")\n  }"
  },
  {
    "id" : "125f7936-3138-4c63-8741-7c90e3cbd8ec",
    "prId" : 24765,
    "prUrl" : "https://github.com/apache/spark/pull/24765#pullrequestreview-244602516",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "03f049aa-9147-48d2-bcaa-88a70227dda9",
        "parentId" : null,
        "authorId" : "a11722a8-ecfa-4827-88e3-4d708f023846",
        "body" : "I found the old code here to be slightly confusing because it seemed to be using `notNullPreds` for two different purposes:\r\n\r\n1. If we see `IsNotNull` conjuncts in the filter then evaluate them first / earlier because (a) these expressions are cheap to evaluate and may allow for short-circuiting and skipping more expensive expressions, and (b) evaluating these earlier allows other expressions to omit null checks (for example, if we have `IsNotNull(x)` and `x * 100 < 10` then we _already_ implicitly need to null-check `x` as part of the second expression so we might as well do the explicit null check expression first).\r\n2. Given that tuples have successfully passed through the filter, we can rely on the presence of `IsNotNull` checks to default subsequent expressions' null checks to `false`. For example, let's say we had a `.filter().select()` which gets compiled into a single whole stage codegen: after tuples have passed through the filter we know that certain fields cannot possibly be null, so we can elide null checks _at codegen time_ by just setting `nullable = false` in subsequent code.\r\n\r\nThere might be some subtleties related in (1) related to non-deterministic expressions, but I think that's accounted for further down at the place where we're actually generating the checks.\r\n\r\nIn the old code, the `(notNullPreds, otherPreds)` on this line was being used for both purposes: for (1) I think we could simply collect _all_ `IsNotNull` expressions, but the existing implementation of (2) relied on the additional `nullIntolerant` / `a.references` checks in order to be correct.\r\n\r\nIn this PR, I've separated these two usages: the \"update nullability for downstream operators\" now uses the more precise condition implemented in `getImpliedNotNullExprIds`, while the \"optimize short-circuiting\" simply checks for `IsNotNull` and ignores child attributes.",
        "createdAt" : "2019-06-01T21:53:28Z",
        "updatedAt" : "2019-06-01T22:15:44Z",
        "lastEditedBy" : "a11722a8-ecfa-4827-88e3-4d708f023846",
        "tags" : [
        ]
      }
    ],
    "commit" : "a10632fea46e102226b771793a3df8ff733768be",
    "line" : 3,
    "diffHunk" : "@@ -1,1 +89,93 @@\n  // Split out all the IsNotNulls from condition.\n  private val (notNullPreds, otherPreds) = splitConjunctivePredicates(condition).partition {\n    case IsNotNull(_) => true\n    case _ => false"
  },
  {
    "id" : "5079c833-18ed-43f4-8a1a-cda6e7cd3ac7",
    "prId" : 24759,
    "prUrl" : "https://github.com/apache/spark/pull/24759#pullrequestreview-278392056",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dd2eb453-2465-40a9-bae6-820ee7e85b07",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "why do we need this if?",
        "createdAt" : "2019-08-21T13:25:18Z",
        "updatedAt" : "2019-08-26T07:56:18Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "3523c907-a551-42ad-aff7-4d7ab4b4e304",
        "parentId" : "dd2eb453-2465-40a9-bae6-820ee7e85b07",
        "authorId" : "a2fcc15e-a51a-42f0-87a6-137048a28e30",
        "body" : "@cloud-fan Oh.. because we don't show these SubqueryExec ops in the new format. Here is an example:\r\n** SQL **\r\n```\r\nexplain select * from explain_temp1 where key = (select max(key) from explain_temp2);\r\n```\r\n** Old output **\r\n```\r\n== Physical Plan ==\r\n*(1) Project [key#33776, val#33777]\r\n+- *(1) Filter (isnotnull(key#33776) AND (key#33776 = Subquery scalar-subquery#33790))\r\n   :  +- Subquery scalar-subquery#33790\r\n   :     +- *(2) HashAggregate(keys=[], functions=[max(key#33778)])\r\n   :        +- Exchange SinglePartition, true\r\n   :           +- *(1) HashAggregate(keys=[], functions=[partial_max(key#33778)])\r\n   :              +- *(1) ColumnarToRow\r\n   :                 +- FileScan parquet default.explain_temp2[key#33778] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex[file:/user/hive/warehouse/explain_temp2], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<key:int>\r\n   +- *(1) ColumnarToRow\r\n      +- FileScan parquet default.explain_temp1[key#33776,val#33777] Batched: true, DataFilters: [isnotnull(key#33776)], Format: Parquet, Location: InMemoryFileIndex[file:/user/hive/warehouse/explain_temp1], PartitionFilters: [], PushedFilters: [IsNotNull(key)], ReadSchema: struct<key:int,val:int>\r\n\r\n```\r\nThe line  `Subquery scalar-subquery#33790` does not show up in the new plan as we only show the actual plan and not the containing operator.",
        "createdAt" : "2019-08-22T00:26:57Z",
        "updatedAt" : "2019-08-26T07:56:18Z",
        "lastEditedBy" : "a2fcc15e-a51a-42f0-87a6-137048a28e30",
        "tags" : [
        ]
      },
      {
        "id" : "2a2bd113-4d6f-46ca-9c39-9403c5fa38d7",
        "parentId" : "dd2eb453-2465-40a9-bae6-820ee7e85b07",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "can you add some comments to briefly explain it?",
        "createdAt" : "2019-08-22T12:46:20Z",
        "updatedAt" : "2019-08-26T07:56:18Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "f401175f7bd0e94db1722653734b92c9db57a779",
    "line" : 58,
    "diffHunk" : "@@ -1,1 +716,720 @@     * excluded from the main plan.\n     */\n    if (!printNodeId) {\n      super.generateTreeString(\n        depth,"
  }
]