[
  {
    "id" : "8f708fce-327a-47fc-9aa1-b9fc9e479293",
    "prId" : 31357,
    "prUrl" : "https://github.com/apache/spark/pull/31357#pullrequestreview-577587706",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "609a446e-8bc4-4604-befc-1d695fa79f4c",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Got it.",
        "createdAt" : "2021-01-27T17:31:34Z",
        "updatedAt" : "2021-01-27T17:31:35Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "0fee33bf7fef207bfef061747a0d89fd93522087",
    "line" : 31,
    "diffHunk" : "@@ -1,1 +275,279 @@          // INT32 value.\n          new ParquetIntDictionaryAwareDecimalConverter(\n            DecimalType.IntDecimal.precision, 0, updater)\n        } else {\n          new ParquetIntDictionaryAwareDecimalConverter("
  },
  {
    "id" : "f33f7b49-b38b-430d-9402-362e1696e77e",
    "prId" : 31319,
    "prUrl" : "https://github.com/apache/spark/pull/31319#pullrequestreview-576052194",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "31caa554-cbdb-4bdf-b206-43b19bb7c869",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "same question as the avro PR: how do we handle the precision/scale inconsistency between the decimal value and the catalyst decimal type?",
        "createdAt" : "2021-01-26T05:38:08Z",
        "updatedAt" : "2021-01-26T20:11:19Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "138b8259-599b-442d-b261-bd0aec9cd181",
        "parentId" : "31caa554-cbdb-4bdf-b206-43b19bb7c869",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "1. For object handling, after creating decimal object, the upper layer uses the object's precision and scale to handle the object's value. Do you have some usual-suspect code?\r\n2. For the user-facing side, this doesn't change the user-provided schema. So, there is no change from user's perspective.",
        "createdAt" : "2021-01-26T06:19:08Z",
        "updatedAt" : "2021-01-26T20:11:19Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "7932069e5c70fe6ccb99aa7b8a8725a48b402c7e",
    "line" : 26,
    "diffHunk" : "@@ -1,1 +284,288 @@      case t: DecimalType if parquetType.asPrimitiveType().getPrimitiveTypeName == INT32 =>\n        val (precision, scale) = getPrecisionAndScale(parquetType, t)\n        new ParquetIntDictionaryAwareDecimalConverter(precision, scale, updater)\n\n      // For INT64 backed decimals"
  },
  {
    "id" : "d176b274-f91f-4e1b-9e97-4ae0af20bf8f",
    "prId" : 31319,
    "prUrl" : "https://github.com/apache/spark/pull/31319#pullrequestreview-576722424",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "05dd5dfd-9a8d-45b7-81e0-0a50931d6890",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "This is the new logic, @cloud-fan .",
        "createdAt" : "2021-01-26T20:12:28Z",
        "updatedAt" : "2021-01-26T20:12:28Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "7932069e5c70fe6ccb99aa7b8a8725a48b402c7e",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +247,251 @@   * 1. If there is a decimal metadata, we read decimal values with the given precision and scale.\n   * 2. If there is no metadata, we read decimal values with scale `0` because it's plain integers\n   *    when it is written into INT32/INT64/BINARY/FIXED_LEN_BYTE_ARRAY types.\n   */\n  private def getPrecisionAndScale(parquetType: Type, t: DecimalType): (Int, Int) = {"
  }
]