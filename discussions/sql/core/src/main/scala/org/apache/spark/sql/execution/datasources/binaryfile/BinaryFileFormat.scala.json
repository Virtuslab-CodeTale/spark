[
  {
    "id" : "e4c3f730-e046-4bb6-a150-e8ff9f8b2fc9",
    "prId" : 24483,
    "prUrl" : "https://github.com/apache/spark/pull/24483#pullrequestreview-231880418",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "03ce0469-2cf4-4ca5-9bb5-257b60a78966",
        "parentId" : null,
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "Nit: we can define a method in `SQLConf`, like `SQLConf.maxRecordsPerFile`.",
        "createdAt" : "2019-04-29T19:45:44Z",
        "updatedAt" : "2019-04-29T20:16:28Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "d60b7665-4e94-4b4e-9f06-0d08712e867c",
        "parentId" : "03ce0469-2cf4-4ca5-9bb5-257b60a78966",
        "authorId" : "0c812942-02cb-4975-9748-394d1387affa",
        "body" : "? The logic is not general enough to be applied outside binary data source.",
        "createdAt" : "2019-04-29T20:17:15Z",
        "updatedAt" : "2019-04-29T20:17:16Z",
        "lastEditedBy" : "0c812942-02cb-4975-9748-394d1387affa",
        "tags" : [
        ]
      }
    ],
    "commit" : "0d6f92c4160c2b92d547e68f48280f440a65748f",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +102,106 @@    val pathGlobPattern = binaryFileSourceOptions.pathGlobFilter\n    val filterFuncs = filters.map(filter => createFilterFunction(filter))\n    val maxLength = sparkSession.conf.get(SOURCES_BINARY_FILE_MAX_LENGTH)\n\n    file: PartitionedFile => {"
  },
  {
    "id" : "c1701f0c-ccfb-41f7-8819-ef8fd51b1468",
    "prId" : 24483,
    "prUrl" : "https://github.com/apache/spark/pull/24483#pullrequestreview-231884720",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2811dfb2-dd8d-4393-857a-26c45d7a980f",
        "parentId" : null,
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "I think we can move this to line 113.",
        "createdAt" : "2019-04-29T19:52:52Z",
        "updatedAt" : "2019-04-29T20:16:28Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "e3a02419-086b-4123-b7ec-618519d0c7bb",
        "parentId" : "2811dfb2-dd8d-4393-857a-26c45d7a980f",
        "authorId" : "0c812942-02cb-4975-9748-394d1387affa",
        "body" : "I don't get it. The conf is to prevent reading very large files that we are sure about failures. User can still use the data source if they don't need `content`.",
        "createdAt" : "2019-04-29T20:17:54Z",
        "updatedAt" : "2019-04-29T20:17:54Z",
        "lastEditedBy" : "0c812942-02cb-4975-9748-394d1387affa",
        "tags" : [
        ]
      },
      {
        "id" : "9cf7af36-faef-4e79-9024-51200bda730f",
        "parentId" : "2811dfb2-dd8d-4393-857a-26c45d7a980f",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "I see. I am actually OK with either way.",
        "createdAt" : "2019-04-29T20:26:34Z",
        "updatedAt" : "2019-04-29T20:26:34Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "0d6f92c4160c2b92d547e68f48280f440a65748f",
    "line" : 26,
    "diffHunk" : "@@ -1,1 +119,123 @@              writer.write(i, DateTimeUtils.fromMillis(status.getModificationTime))\n            case (CONTENT, i) =>\n              if (status.getLen > maxLength) {\n                throw new SparkException(\n                  s\"The length of ${status.getPath} is ${status.getLen}, \" +"
  },
  {
    "id" : "0a79d7b9-38c4-47da-bd2c-73484ce12336",
    "prId" : 24473,
    "prUrl" : "https://github.com/apache/spark/pull/24473#pullrequestreview-231449465",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "473bcb44-f777-406a-8d57-dc9e53cdece3",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Can we make it one if-else while we're here? For instance,\r\n\r\n```scala\r\nval isPatternMatched = pathGlobPattern.forall(new GlobFilter(_).accept(fsPath))\r\n\r\n// These vals are intentionally lazy to avoid unnecessary file access via short-circuiting. \r\nlazy val fs = fsPath.getFileSystem(broadcastedHadoopConf.value.value)\r\nlazy val fileStatus = fs.getFileStatus(fsPath)\r\nlazy val shouldNotFilterOut = filterFuncs.forall(_.apply(fileStatus))\r\n      \r\nif (isPatternMatched && shouldNotFilterOut) {\r\n  ...\r\n  Iterator(requiredColumns(internalRow))\r\n} else {\r\n  Iterator.empty[InternalRow]\r\n}\r\n```",
        "createdAt" : "2019-04-28T01:50:46Z",
        "updatedAt" : "2019-04-28T05:40:24Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "20bf3a9e-2a72-4a68-af9e-7976c107982e",
        "parentId" : "473bcb44-f777-406a-8d57-dc9e53cdece3",
        "authorId" : "0c812942-02cb-4975-9748-394d1387affa",
        "body" : "In the current impl, `getFileStatus` and `filterFuncs` are not touched if path doesn't match.",
        "createdAt" : "2019-04-28T05:29:17Z",
        "updatedAt" : "2019-04-28T05:40:24Z",
        "lastEditedBy" : "0c812942-02cb-4975-9748-394d1387affa",
        "tags" : [
        ]
      },
      {
        "id" : "a76dc6b9-01ec-47ab-bdb9-aeb06aa0084a",
        "parentId" : "473bcb44-f777-406a-8d57-dc9e53cdece3",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Yes, this suggestion does not touch both because they are lazy",
        "createdAt" : "2019-04-28T05:43:22Z",
        "updatedAt" : "2019-04-28T05:43:22Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "f4a64698805c768e37f081eefe2a5c8f06b8e4b0",
    "line" : 68,
    "diffHunk" : "@@ -1,1 +104,108 @@      val path = new Path(file.filePath)\n      // TODO: Improve performance here: each file will recompile the glob pattern here.\n      if (pathGlobPattern.forall(new GlobFilter(_).accept(path))) {\n        val fs = path.getFileSystem(broadcastedHadoopConf.value.value)\n        val status = fs.getFileStatus(path)"
  },
  {
    "id" : "055cd0cf-98d3-47f8-9574-6426f0cbbb83",
    "prId" : 24387,
    "prUrl" : "https://github.com/apache/spark/pull/24387#pullrequestreview-231474463",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "762d20f3-5002-4c12-8be5-61cdc6bbb096",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Looks `EqualNullSafe` and `In` are missing.",
        "createdAt" : "2019-04-28T15:47:54Z",
        "updatedAt" : "2019-04-28T15:47:54Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "71db855468691b18f9365f70ca376225fc046989",
    "line" : 189,
    "diffHunk" : "@@ -1,1 +202,206 @@        _.getModificationTime >= value.getTime\n      case EqualTo(MODIFICATION_TIME, value: Timestamp) =>\n        _.getModificationTime == value.getTime\n\n      case _ => (_ => true)"
  },
  {
    "id" : "f3a099bc-6aad-48fb-9c60-3c77a9c529cf",
    "prId" : 24354,
    "prUrl" : "https://github.com/apache/spark/pull/24354#pullrequestreview-226196765",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "00a32890-a52f-403d-80fd-f67de502b731",
        "parentId" : null,
        "authorId" : "0c812942-02cb-4975-9748-394d1387affa",
        "body" : "If would be very helpful if as a user I can do `spark.read.format(\"binaryFile\").load(dir).filter($\"status.len\" < 1000000)` without loading files larger than 1000000. I'm not sure whether data source supports such optimization.\r\n\r\ncc: @cloud-fan @gatorsmile ",
        "createdAt" : "2019-04-12T01:06:32Z",
        "updatedAt" : "2019-04-16T20:04:28Z",
        "lastEditedBy" : "0c812942-02cb-4975-9748-394d1387affa",
        "tags" : [
        ]
      },
      {
        "id" : "e48eee31-e5aa-4a3a-aa4e-e28dde595c7e",
        "parentId" : "00a32890-a52f-403d-80fd-f67de502b731",
        "authorId" : "99156419-7ce7-4671-b858-d0b5c4711f88",
        "body" : "We actually find it more useful to have the \"binary\" be a column of parquet files. that way the filter pushdown actually works...",
        "createdAt" : "2019-04-12T06:43:48Z",
        "updatedAt" : "2019-04-16T20:04:28Z",
        "lastEditedBy" : "99156419-7ce7-4671-b858-d0b5c4711f88",
        "tags" : [
        ]
      },
      {
        "id" : "2303ef5a-4ac5-4e10-84b0-bf858d5b887e",
        "parentId" : "00a32890-a52f-403d-80fd-f67de502b731",
        "authorId" : "0c812942-02cb-4975-9748-394d1387affa",
        "body" : "@felixcheung I didn't get what you mean. `content` column already contains the binary content of the file. My suggestion is to avoid unnecessary I/O if user put a filter on `status.len`.",
        "createdAt" : "2019-04-12T17:18:42Z",
        "updatedAt" : "2019-04-16T20:04:28Z",
        "lastEditedBy" : "0c812942-02cb-4975-9748-394d1387affa",
        "tags" : [
        ]
      }
    ],
    "commit" : "dd8e8c65b96cc3165274ca8cb742e8bdc557c2ad",
    "line" : 107,
    "diffHunk" : "@@ -1,1 +105,109 @@        val length = fileStatus.getLen()\n        val modificationTime = fileStatus.getModificationTime()\n        val stream = fs.open(fsPath)\n\n        val content = try {"
  },
  {
    "id" : "903ba294-051b-4bc4-84e8-c81c460a5a39",
    "prId" : 24354,
    "prUrl" : "https://github.com/apache/spark/pull/24354#pullrequestreview-226231967",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "717d2452-ddf9-4c1c-9c46-b4d6fe7cb7a8",
        "parentId" : null,
        "authorId" : "99156419-7ce7-4671-b858-d0b5c4711f88",
        "body" : "is this case sensitive when use in `format(\"binaryFile\")`?",
        "createdAt" : "2019-04-12T06:38:41Z",
        "updatedAt" : "2019-04-16T20:04:28Z",
        "lastEditedBy" : "99156419-7ce7-4671-b858-d0b5c4711f88",
        "tags" : [
        ]
      },
      {
        "id" : "10b265bf-46bd-4951-a2c3-3f5f97b53e99",
        "parentId" : "717d2452-ddf9-4c1c-9c46-b4d6fe7cb7a8",
        "authorId" : "7e665d8b-d739-4edf-88c8-7379ff8585c2",
        "body" : "No. It is case-insensitive.",
        "createdAt" : "2019-04-12T18:39:45Z",
        "updatedAt" : "2019-04-16T20:04:28Z",
        "lastEditedBy" : "7e665d8b-d739-4edf-88c8-7379ff8585c2",
        "tags" : [
        ]
      }
    ],
    "commit" : "dd8e8c65b96cc3165274ca8cb742e8bdc557c2ad",
    "line" : 78,
    "diffHunk" : "@@ -1,1 +76,80 @@  }\n\n  override def shortName(): String = \"binaryFile\"\n\n  override protected def buildReader("
  },
  {
    "id" : "efce5963-e8e5-4f8a-a1b1-e5c8bde77fbb",
    "prId" : 24354,
    "prUrl" : "https://github.com/apache/spark/pull/24354#pullrequestreview-227105351",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1548c950-1eb3-436e-876a-744d845749eb",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "are we sure about this? Always return false means one file one RDD partition.",
        "createdAt" : "2019-04-14T07:57:50Z",
        "updatedAt" : "2019-04-16T20:04:28Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "7d682a04-06f5-4096-bb9f-c38b7744b778",
        "parentId" : "1548c950-1eb3-436e-876a-744d845749eb",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I don't think binary partitions should be splittable like `binaryFiles` because usually one binary is a minimal logical unit for arbitrary binary files.",
        "createdAt" : "2019-04-14T08:21:46Z",
        "updatedAt" : "2019-04-16T20:04:28Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "5b65939c-a383-4b98-9387-c56e08ba3cc3",
        "parentId" : "1548c950-1eb3-436e-876a-744d845749eb",
        "authorId" : "0c812942-02cb-4975-9748-394d1387affa",
        "body" : "@cloud-fan Does it mean that the file itself cannot be split into multiple parts? It shouldn't lead to one file per partition.",
        "createdAt" : "2019-04-15T17:28:23Z",
        "updatedAt" : "2019-04-16T20:04:28Z",
        "lastEditedBy" : "0c812942-02cb-4975-9748-394d1387affa",
        "tags" : [
        ]
      },
      {
        "id" : "1c881887-0782-4056-9207-6572da9c3dfa",
        "parentId" : "1548c950-1eb3-436e-876a-744d845749eb",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "if `isSplitable` returns false, then Spark can only read the entire file with a single thread, so it's one file per partition.\r\n\r\nThe file splitting is actually very complicated. For example, the text format splits the file w.r.t. the line boundary. A line of text will not be split into multiple partitions. I'm not sure how to define the file splitting logic for binary files.",
        "createdAt" : "2019-04-16T03:09:48Z",
        "updatedAt" : "2019-04-16T20:04:28Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "2e8f5867-7229-4c50-a67f-65af72c61f84",
        "parentId" : "1548c950-1eb3-436e-876a-744d845749eb",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "> It shouldn't lead to one file per partition.\r\n\r\n@mengxr, do you mean one binary file should be split into multiple parts? In that case, the splitting rule should be defined and fixed so that end users can process it and use it. If users are not aware of the rule to split up, there wouldn't be a way for users to use it (for instance image).\r\n\r\nI thought this will be implemented by `maxBytesPerPartition`, and by default one file per one partition.",
        "createdAt" : "2019-04-16T03:11:46Z",
        "updatedAt" : "2019-04-16T20:04:28Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "3fbf6eaf-e042-45e2-81b0-36cc3978a8e2",
        "parentId" : "1548c950-1eb3-436e-876a-744d845749eb",
        "authorId" : "0c812942-02cb-4975-9748-394d1387affa",
        "body" : "@cloud-fan Why single thread leads to one file per partition? One partition can still have multiple files, but one file cannot be split into multiple records.\r\n\r\n@HyukjinKwon We don't want to split a file into parts.",
        "createdAt" : "2019-04-16T04:21:35Z",
        "updatedAt" : "2019-04-16T20:04:28Z",
        "lastEditedBy" : "0c812942-02cb-4975-9748-394d1387affa",
        "tags" : [
        ]
      },
      {
        "id" : "3210b09a-6c04-44cf-96a7-ca4fe1077315",
        "parentId" : "1548c950-1eb3-436e-876a-744d845749eb",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "to be clear: one file per file partition. It's still possible that one RDD partition contains many files.",
        "createdAt" : "2019-04-16T05:05:07Z",
        "updatedAt" : "2019-04-16T20:04:28Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "478ab4a8-1ce6-4856-8e3b-a5476c454518",
        "parentId" : "1548c950-1eb3-436e-876a-744d845749eb",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "(Oops, I was confused that we were talking about one partition that has multiple parts)",
        "createdAt" : "2019-04-16T10:17:13Z",
        "updatedAt" : "2019-04-16T20:04:28Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "dd8e8c65b96cc3165274ca8cb742e8bdc557c2ad",
    "line" : 75,
    "diffHunk" : "@@ -1,1 +73,77 @@      options: Map[String, String],\n      path: Path): Boolean = {\n    false\n  }\n"
  },
  {
    "id" : "a03c72ee-3857-447e-8b2c-a29dbb7d08f7",
    "prId" : 24354,
    "prUrl" : "https://github.com/apache/spark/pull/24354#pullrequestreview-226457751",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a5147918-51c2-47ce-b076-0a203b82925d",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "are we going to leverage the `filters` here?",
        "createdAt" : "2019-04-14T08:17:16Z",
        "updatedAt" : "2019-04-16T20:04:28Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "e59e8c26-e055-4a53-8c16-7740cf1887d0",
        "parentId" : "a5147918-51c2-47ce-b076-0a203b82925d",
        "authorId" : "7e665d8b-d739-4edf-88c8-7379ff8585c2",
        "body" : "I can put it in later PR.",
        "createdAt" : "2019-04-15T05:31:19Z",
        "updatedAt" : "2019-04-16T20:04:28Z",
        "lastEditedBy" : "7e665d8b-d739-4edf-88c8-7379ff8585c2",
        "tags" : [
        ]
      }
    ],
    "commit" : "dd8e8c65b96cc3165274ca8cb742e8bdc557c2ad",
    "line" : 85,
    "diffHunk" : "@@ -1,1 +83,87 @@      partitionSchema: StructType,\n      requiredSchema: StructType,\n      filters: Seq[Filter],\n      options: Map[String, String],\n      hadoopConf: Configuration): (PartitionedFile) => Iterator[InternalRow] = {"
  },
  {
    "id" : "bc2c957a-2dc3-4d1b-986f-b596e643c55d",
    "prId" : 24354,
    "prUrl" : "https://github.com/apache/spark/pull/24354#pullrequestreview-226978858",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c18e7efd-be19-440f-add4-0196b04e6316",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "The name, we don't add `file` postfix so far. Parquet, ORC, Text, JSON, CSV. shall we just name it binary to be consistent?",
        "createdAt" : "2019-04-14T08:27:16Z",
        "updatedAt" : "2019-04-16T20:04:28Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "cff86231-466f-4572-a654-24cf39557e5f",
        "parentId" : "c18e7efd-be19-440f-add4-0196b04e6316",
        "authorId" : "0c812942-02cb-4975-9748-394d1387affa",
        "body" : "I don't feel `binary` itself is accurate since both parquet orc are binary format.",
        "createdAt" : "2019-04-15T17:30:07Z",
        "updatedAt" : "2019-04-16T20:04:28Z",
        "lastEditedBy" : "0c812942-02cb-4975-9748-394d1387affa",
        "tags" : [
        ]
      },
      {
        "id" : "bb88ced9-7c68-4230-9228-f9d644886de2",
        "parentId" : "c18e7efd-be19-440f-add4-0196b04e6316",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "JSON and CSV are text format too. I was thinking `SparkContext.binaryFiles` -> `binary`, `SparkContext.textFiles` -> `text`.\r\n\r\nOtherwise, we might have to make another name to be consistent. Parquet file, ORC file, all make sense ...",
        "createdAt" : "2019-04-16T03:14:29Z",
        "updatedAt" : "2019-04-16T20:04:28Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "c6c21392-3718-4924-80bc-0ae9b85c33c0",
        "parentId" : "c18e7efd-be19-440f-add4-0196b04e6316",
        "authorId" : "0c812942-02cb-4975-9748-394d1387affa",
        "body" : "`binary` is too broad. I don't think it is as clear as `text`. For text data source, we have a wholeTextFile option. This binary file data source is not equivalent to text data source alone but text data source + wholeTextFile option.",
        "createdAt" : "2019-04-16T04:23:24Z",
        "updatedAt" : "2019-04-16T20:04:28Z",
        "lastEditedBy" : "0c812942-02cb-4975-9748-394d1387affa",
        "tags" : [
        ]
      }
    ],
    "commit" : "dd8e8c65b96cc3165274ca8cb742e8bdc557c2ad",
    "line" : 78,
    "diffHunk" : "@@ -1,1 +76,80 @@  }\n\n  override def shortName(): String = \"binaryFile\"\n\n  override protected def buildReader("
  },
  {
    "id" : "2698fa1c-6209-480a-a4a3-0e06f3c7464c",
    "prId" : 24354,
    "prUrl" : "https://github.com/apache/spark/pull/24354#pullrequestreview-226850828",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "97a0db82-5e08-498a-8fb9-6e1582400754",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Can we `Utils.tryWithResource`?",
        "createdAt" : "2019-04-14T08:28:31Z",
        "updatedAt" : "2019-04-16T20:04:28Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "3a7db900-687f-4ace-823a-736d7e03483f",
        "parentId" : "97a0db82-5e08-498a-8fb9-6e1582400754",
        "authorId" : "7e665d8b-d739-4edf-88c8-7379ff8585c2",
        "body" : "Here use `Closeables.close(stream, swallowIOException=true)` which `Utils.tryWithResource` do not support.",
        "createdAt" : "2019-04-15T19:59:44Z",
        "updatedAt" : "2019-04-16T20:04:28Z",
        "lastEditedBy" : "7e665d8b-d739-4edf-88c8-7379ff8585c2",
        "tags" : [
        ]
      }
    ],
    "commit" : "dd8e8c65b96cc3165274ca8cb742e8bdc557c2ad",
    "line" : 109,
    "diffHunk" : "@@ -1,1 +107,111 @@        val stream = fs.open(fsPath)\n\n        val content = try {\n          ByteStreams.toByteArray(stream)\n        } finally {"
  },
  {
    "id" : "38f6e786-947f-4ddb-ac3e-41073dd0b690",
    "prId" : 24354,
    "prUrl" : "https://github.com/apache/spark/pull/24354#pullrequestreview-227104807",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8a3da037-828c-4bbc-b2e9-8343d61ad888",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "this does not help the performance. We still read the file content even if `content` column is not required.",
        "createdAt" : "2019-04-16T10:14:30Z",
        "updatedAt" : "2019-04-16T20:04:28Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "a542fbae-f98b-499e-af72-5e28e9f3e2a2",
        "parentId" : "8a3da037-828c-4bbc-b2e9-8343d61ad888",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "This is OK for now, maybe we can leave a TODO and implement the real column pruning in the future.",
        "createdAt" : "2019-04-16T10:15:52Z",
        "updatedAt" : "2019-04-16T20:04:28Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "dd8e8c65b96cc3165274ca8cb742e8bdc557c2ad",
    "line" : 124,
    "diffHunk" : "@@ -1,1 +122,126 @@        // TODO: Add column pruning\n        // currently it still read the file content even if content column is not required.\n        val requiredColumns = GenerateUnsafeProjection.generate(requiredOutput, fullOutput)\n\n        val internalRow = InternalRow("
  },
  {
    "id" : "e42e3d62-2479-4ae7-8477-526ccb8ca4a6",
    "prId" : 24354,
    "prUrl" : "https://github.com/apache/spark/pull/24354#pullrequestreview-227324815",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e1acf54a-f1aa-4fde-8be9-de69a01189b7",
        "parentId" : null,
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "I think we should make it a general option, which can be applied in all data sources. Also we should pass the option to `FileIndex`, so that Spark can split file partition more precisely.\r\nWe can have a follow-up PR for this.",
        "createdAt" : "2019-04-16T16:59:28Z",
        "updatedAt" : "2019-04-16T20:04:28Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "dd8e8c65b96cc3165274ca8cb742e8bdc557c2ad",
    "line" : 100,
    "diffHunk" : "@@ -1,1 +98,102 @@      val fsPath = new Path(path)\n\n      // TODO: Improve performance here: each file will recompile the glob pattern here.\n      val globFilter = pathGlobPattern.map(new GlobFilter(_))\n      if (!globFilter.isDefined || globFilter.get.accept(fsPath)) {"
  },
  {
    "id" : "1f0504e1-d62f-452a-8ff3-87547b005e58",
    "prId" : 24354,
    "prUrl" : "https://github.com/apache/spark/pull/24354#pullrequestreview-230173314",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1aa97e2f-3b25-4ab2-b66e-a896848cc4dd",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "If I remember correctly, the usual behavior in Spark is not to throw an exception but prefers null value. At this point, should we assign `content` null value instead of throwing exception?",
        "createdAt" : "2019-04-24T14:50:51Z",
        "updatedAt" : "2019-04-24T14:50:52Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "a0b936e1-fd2d-4342-80f9-51c4ee5eaaf4",
        "parentId" : "1aa97e2f-3b25-4ab2-b66e-a896848cc4dd",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Oh, we can control it with `ignoreCorruptFiles`.",
        "createdAt" : "2019-04-24T15:06:32Z",
        "updatedAt" : "2019-04-24T15:06:32Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "dd8e8c65b96cc3165274ca8cb742e8bdc557c2ad",
    "line" : 110,
    "diffHunk" : "@@ -1,1 +108,112 @@\n        val content = try {\n          ByteStreams.toByteArray(stream)\n        } finally {\n          Closeables.close(stream, true)"
  },
  {
    "id" : "5c7f4c68-f3c1-4cce-84af-ae4ddaf2a0a0",
    "prId" : 24354,
    "prUrl" : "https://github.com/apache/spark/pull/24354#pullrequestreview-230163883",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0811822e-3d26-42c9-9e5d-4bc3ffc5fbbf",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Related to above comment, should we not propagate IO exceptions?",
        "createdAt" : "2019-04-24T14:52:33Z",
        "updatedAt" : "2019-04-24T14:52:34Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "dd8e8c65b96cc3165274ca8cb742e8bdc557c2ad",
    "line" : 112,
    "diffHunk" : "@@ -1,1 +110,114 @@          ByteStreams.toByteArray(stream)\n        } finally {\n          Closeables.close(stream, true)\n        }\n"
  },
  {
    "id" : "74f0ee93-00b8-4d9c-a706-42941e5bc3cb",
    "prId" : 24354,
    "prUrl" : "https://github.com/apache/spark/pull/24354#pullrequestreview-231474778",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d97d986d-8f5d-4cdd-b133-d3a2051a5638",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Not a big deal at all but let me leave a note before I forget. `BinaryFileSourceOptions` -> `BinaryFileOptions` to be consistent with `[SourceName]Options` - `TextOptions`, `OrcOptions`,` ParquetOptions`, `CSVOptions`,`JDBCOptions`, `ImageOptions`, etc.",
        "createdAt" : "2019-04-28T15:56:03Z",
        "updatedAt" : "2019-04-28T15:56:12Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "dd8e8c65b96cc3165274ca8cb742e8bdc557c2ad",
    "line" : 167,
    "diffHunk" : "@@ -1,1 +165,169 @@}\n\nclass BinaryFileSourceOptions(\n    @transient private val parameters: CaseInsensitiveMap[String]) extends Serializable {\n"
  }
]