[
  {
    "id" : "eacbaedd-d0c7-47d2-bad4-c9e789b01d58",
    "prId" : 33370,
    "prUrl" : "https://github.com/apache/spark/pull/33370#pullrequestreview-708130933",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0fc3caa9-33bf-4db7-b2e0-35b9770e37b0",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "You made this class for testing? Instead of creating an unnecessary class hierarchy, how about just creating an inner method for testing like this?\r\n```\r\n\r\n  // For testing\r\n  private[jdbc] def doCreate(\r\n    driver: Driver,\r\n    providers: Seq[JdbcConnectionProvider],\r\n    options: Map[String, String]): Connection = {\r\n    \r\n    ...\r\n  }\r\n\r\n  def create(driver: Driver, options: Map[String, String]): Connection = {\r\n    doCreate(driver, providers, options)\r\n  }\r\n```",
        "createdAt" : "2021-07-16T01:33:56Z",
        "updatedAt" : "2021-07-16T01:44:02Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "e1ab2d52-9c62-4ddc-a04c-ff715b1a6787",
        "parentId" : "0fc3caa9-33bf-4db7-b2e0-35b9770e37b0",
        "authorId" : "a8db2e87-20cc-46d2-a9f0-b5142f4fe30a",
        "body" : "I thought about it. The reason I decided to go with a separate class is ability to add new methods that rely on providers list and testing of such methods fully. My concern was adding a custom logic in `create()` method which would not be tested by `doCreate()`. Also this should mitigate a potential confusion how providers list is used in the class.",
        "createdAt" : "2021-07-16T08:27:50Z",
        "updatedAt" : "2021-07-16T16:54:42Z",
        "lastEditedBy" : "a8db2e87-20cc-46d2-a9f0-b5142f4fe30a",
        "tags" : [
        ]
      }
    ],
    "commit" : "e94cf0cc507fe31f26651587a257a36db16763fc",
    "line" : 69,
    "diffHunk" : "@@ -1,1 +101,105 @@}\n\nprivate[jdbc] object ConnectionProvider extends ConnectionProviderBase"
  },
  {
    "id" : "5ffdfbe4-81f8-44b9-98aa-3da6f31fa097",
    "prId" : 29964,
    "prUrl" : "https://github.com/apache/spark/pull/29964#pullrequestreview-504872497",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3c886db9-2956-4fbd-93a9-87529b3cb098",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Could you move this comment in L58?\r\n```\r\n    }.toSeq // toSeq seems duplicate but it's needed for Scala 2.13\r\n```",
        "createdAt" : "2020-10-07T11:46:53Z",
        "updatedAt" : "2020-10-08T14:58:35Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "d56d9385-a0e7-4252-82e7-00ec62c3a46b",
        "parentId" : "3c886db9-2956-4fbd-93a9-87529b3cb098",
        "authorId" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "body" : "Fixed.",
        "createdAt" : "2020-10-08T14:55:16Z",
        "updatedAt" : "2020-10-08T14:58:35Z",
        "lastEditedBy" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "tags" : [
        ]
      }
    ],
    "commit" : "b6458cf988f482134925414309956223af4d6739",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +50,54 @@\n    val disabledProviders = Utils.stringToSeq(SQLConf.get.disabledJdbcConnectionProviders)\n    // toSeq seems duplicate but it's needed for Scala 2.13\n    providers.filterNot(p => disabledProviders.contains(p.name)).toSeq\n  }"
  },
  {
    "id" : "1ee1c926-368b-4925-9cc4-79a08bdf9aab",
    "prId" : 29024,
    "prUrl" : "https://github.com/apache/spark/pull/29024#pullrequestreview-503977512",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "368614a0-e9a9-4885-a4f2-fca1174153f0",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "I am getting the following exception on my console permanently while running JDBC tests. Should it be really logged as an error?\r\n```\r\n14:31:25.070 ERROR org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProvider: Failed to load built in provider.\r\njava.util.ServiceConfigurationError: org.apache.spark.sql.jdbc.JdbcConnectionProvider: Provider org.apache.spark.sql.execution.datasources.jdbc.connection.IntentionallyFaultyConnectionProvider could not be instantiated\r\n\tat java.util.ServiceLoader.fail(ServiceLoader.java:232)\r\n\tat java.util.ServiceLoader.access$100(ServiceLoader.java:185)\r\n\tat java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:384)\r\n\tat java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:404)\r\n\tat java.util.ServiceLoader$1.next(ServiceLoader.java:480)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProvider$.loadProviders(ConnectionProvider.scala:41)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProvider$.<init>(ConnectionProvider.scala:31)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProvider$.<clinit>(ConnectionProvider.scala)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$createConnectionFactory$1(JdbcUtils.scala:66)\r\n\tat org.apache.spark.sql.execution.datasources.v2.jdbc.JDBCTableCatalog.withConnection(JDBCTableCatalog.scala:156)\r\n\tat org.apache.spark.sql.execution.datasources.v2.jdbc.JDBCTableCatalog.listTables(JDBCTableCatalog.scala:58)\r\n\tat org.apache.spark.sql.execution.datasources.v2.ShowTablesExec.run(ShowTablesExec.scala:42)\r\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:39)\r\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:39)\r\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:45)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:229)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3675)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:769)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3673)\r\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:229)\r\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:769)\r\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\r\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:612)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:769)\r\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:607)\r\n\tat org.apache.spark.sql.test.SQLTestUtilsBase.$anonfun$sql$1(SQLTestUtils.scala:231)\r\n\tat org.apache.spark.sql.execution.datasources.v2.jdbc.JDBCTableCatalogSuite.$anonfun$new$2(JDBCTableCatalogSuite.scala:67)\r\n\tat org.apache.spark.sql.QueryTest.checkAnswer(QueryTest.scala:134)\r\n\tat org.apache.spark.sql.execution.datasources.v2.jdbc.JDBCTableCatalogSuite.$anonfun$new$1(JDBCTableCatalogSuite.scala:67)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)\r\n\tat org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)\r\n\tat org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)\r\n\tat org.scalatest.Transformer.apply(Transformer.scala:22)\r\n\tat org.scalatest.Transformer.apply(Transformer.scala:20)\r\n\tat org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:189)\r\n\tat org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:176)\r\n\tat org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:187)\r\n\tat org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:199)\r\n\tat org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)\r\n\tat org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:199)\r\n\tat org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:181)\r\n\tat org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(SparkFunSuite.scala:61)\r\n\tat org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234)\r\n\tat org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227)\r\n\tat org.apache.spark.SparkFunSuite.runTest(SparkFunSuite.scala:61)\r\n\tat org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:232)\r\n\tat org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)\r\n\tat scala.collection.immutable.List.foreach(List.scala:392)\r\n\tat org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)\r\n\tat org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)\r\n\tat org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)\r\n\tat org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:232)\r\n\tat org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:231)\r\n\tat org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1562)\r\n\tat org.scalatest.Suite.run(Suite.scala:1112)\r\n\tat org.scalatest.Suite.run$(Suite.scala:1094)\r\n\tat org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1562)\r\n\tat org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:236)\r\n\tat org.scalatest.SuperEngine.runImpl(Engine.scala:535)\r\n\tat org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:236)\r\n\tat org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:235)\r\n\tat org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:61)\r\n\tat org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)\r\n\tat org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)\r\n\tat org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)\r\n\tat org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:61)\r\n\tat org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:45)\r\n\tat org.scalatest.tools.Runner$.$anonfun$doRunRunRunDaDoRunRun$13(Runner.scala:1320)\r\n\tat org.scalatest.tools.Runner$.$anonfun$doRunRunRunDaDoRunRun$13$adapted(Runner.scala:1314)\r\n\tat scala.collection.immutable.List.foreach(List.scala:392)\r\n\tat org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:1314)\r\n\tat org.scalatest.tools.Runner$.$anonfun$runOptionallyWithPassFailReporter$24(Runner.scala:993)\r\n\tat org.scalatest.tools.Runner$.$anonfun$runOptionallyWithPassFailReporter$24$adapted(Runner.scala:971)\r\n\tat org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:1480)\r\n\tat org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:971)\r\n\tat org.scalatest.tools.Runner$.run(Runner.scala:798)\r\n\tat org.scalatest.tools.Runner.run(Runner.scala)\r\n\tat org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.runScalaTest2or3(ScalaTestRunner.java:40)\r\n\tat org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.main(ScalaTestRunner.java:27)\r\nCaused by: java.lang.IllegalArgumentException: Intentional Exception\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.connection.IntentionallyFaultyConnectionProvider.<init>(IntentionallyFaultyConnectionProvider.scala:26)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\r\n\tat java.lang.Class.newInstance(Class.java:442)\r\n\tat java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:380)\r\n\t... 82 more\r\n```",
        "createdAt" : "2020-10-07T11:42:35Z",
        "updatedAt" : "2020-10-07T11:42:42Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "58451bf8-9cb1-44e3-a33c-2f3fbdaa300e",
        "parentId" : "368614a0-e9a9-4885-a4f2-fca1174153f0",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Maybe log it as a warning?",
        "createdAt" : "2020-10-07T11:50:12Z",
        "updatedAt" : "2020-10-07T11:50:12Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "c709009a-095a-4ee3-8570-616ce40d52f2",
        "parentId" : "368614a0-e9a9-4885-a4f2-fca1174153f0",
        "authorId" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "body" : "We can decrease it to warning. The main message is to notify the user.",
        "createdAt" : "2020-10-07T12:03:31Z",
        "updatedAt" : "2020-10-07T12:03:31Z",
        "lastEditedBy" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "tags" : [
        ]
      },
      {
        "id" : "f57a24b1-0b93-4e17-b5fe-dc0ba51afe50",
        "parentId" : "368614a0-e9a9-4885-a4f2-fca1174153f0",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Is it okay to ignore the error case where it fails to load *builtin* providers? `DataSource` throws an exception if it fails to load builtin datasources:\r\nhttps://github.com/apache/spark/blob/94d648dff5f24b4dea3873fd8e6609b1a099d0a2/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala#L694-L702",
        "createdAt" : "2020-10-07T12:35:59Z",
        "updatedAt" : "2020-10-07T12:35:59Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "38d16594-eb2f-43ca-81f6-fe590cde269b",
        "parentId" : "368614a0-e9a9-4885-a4f2-fca1174153f0",
        "authorId" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "body" : "How do you mean ignore? Providers must be loaded independently so we need to catch and ignore the exception.",
        "createdAt" : "2020-10-07T12:47:54Z",
        "updatedAt" : "2020-10-07T12:47:54Z",
        "lastEditedBy" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "tags" : [
        ]
      },
      {
        "id" : "6247ec9e-870b-4e75-a7b3-03f8460aa8a0",
        "parentId" : "368614a0-e9a9-4885-a4f2-fca1174153f0",
        "authorId" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "body" : "If the suggestion is let the exception fire then I would say it's bad idea. If a provider is not able to be loaded then the rest must go. We've had similar issue and expectation in hadoop delegation token area.",
        "createdAt" : "2020-10-07T12:51:03Z",
        "updatedAt" : "2020-10-07T12:51:03Z",
        "lastEditedBy" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "tags" : [
        ]
      },
      {
        "id" : "9780c960-9166-46e6-aa9d-dc570b56aa44",
        "parentId" : "368614a0-e9a9-4885-a4f2-fca1174153f0",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Yea, the policy looks okay for secure connections, but how about the basic one, `BasicConnectionProvider`? At least, until Spark v3.0, creating basic JDBC connections does not fail because of the loading failure.",
        "createdAt" : "2020-10-07T13:14:51Z",
        "updatedAt" : "2020-10-07T13:41:33Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "d0c0ff47-17bc-435b-85cb-d69e72755361",
        "parentId" : "368614a0-e9a9-4885-a4f2-fca1174153f0",
        "authorId" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "body" : "> At least, until Spark v3.0, creating basic JDBC connections does not fail because of the loading failure.\r\n\r\nHere it's the same since `BasicConnectionProvider` is built-in and no pre-load inside. The main issue would come when one adds a new provider which unable to be loaded. That failure would make all the rest workload fail if we don't load them independently.\r\n",
        "createdAt" : "2020-10-07T14:17:36Z",
        "updatedAt" : "2020-10-07T14:17:36Z",
        "lastEditedBy" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "tags" : [
        ]
      },
      {
        "id" : "155f8f44-0ca5-4f76-870b-281f5715898d",
        "parentId" : "368614a0-e9a9-4885-a4f2-fca1174153f0",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "https://github.com/apache/spark/pull/29968",
        "createdAt" : "2020-10-07T15:01:39Z",
        "updatedAt" : "2020-10-07T15:01:39Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "04124905187fcab366c79adc587cfe2495bff5b3",
    "line" : 78,
    "diffHunk" : "@@ -1,1 +44,48 @@      } catch {\n        case t: Throwable =>\n          logError(s\"Failed to load built in provider.\", t)\n      }\n    }"
  }
]