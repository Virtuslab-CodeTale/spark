[
  {
    "id" : "22db4c87-75ea-4ab0-b512-a5c8d0c0dfab",
    "prId" : 30974,
    "prUrl" : "https://github.com/apache/spark/pull/30974#pullrequestreview-568825410",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "84a8f313-1e02-466f-9461-db7b9f499d56",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "can we just do `case c: Cast => UnresolvedAlias(c, Some(Column.generateAlias))`?",
        "createdAt" : "2021-01-15T03:36:38Z",
        "updatedAt" : "2021-01-19T03:57:19Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "64b14279-1585-405f-a6de-6e81af62c58c",
        "parentId" : "84a8f313-1e02-466f-9461-db7b9f499d56",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "Seems we can't. This code will strip the `attribute` which is below the `Cast` so I leave it.\r\n\r\nA example, with the code, `spark.range(1).selectExpr(\"CAST(CAST(id as int) as double)\").columns` will return the `c1` instead of `CAST(CAST(c1 as INT) as DOUBLE)`.",
        "createdAt" : "2021-01-15T05:26:49Z",
        "updatedAt" : "2021-01-19T03:57:19Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      }
    ],
    "commit" : "1b00b8537cd4c2be4ad7715f4106947c3dc9a53a",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +178,182 @@    // If we have a top level Cast, there is a chance to give it a better alias, if there is a\n    // NamedExpression under this Cast.\n    case c: Cast =>\n      c.transformUp {\n        case c @ Cast(_: NamedExpression, _, _) => UnresolvedAlias(c)"
  },
  {
    "id" : "fbcc83e6-8abd-484f-a36d-fde0552082f1",
    "prId" : 30488,
    "prUrl" : "https://github.com/apache/spark/pull/30488#pullrequestreview-541808504",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "06fb0534-01e3-4496-a11d-5997f958e789",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "add some comments to explain the reason.",
        "createdAt" : "2020-12-01T11:34:19Z",
        "updatedAt" : "2020-12-02T15:39:45Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "d13b88c25a10022be0ff0190fd6fa8d158b24560",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +1168,1172 @@    // These denied metadata keys are used to strip the column reference related metadata for\n    // the Alias. So it won't be caught as a column reference in DetectAmbiguousSelfJoin.\n    Alias(expr, alias)(deniedMetadataKeys = Seq(Dataset.DATASET_ID_KEY, Dataset.COL_POS_KEY))\n  }\n"
  },
  {
    "id" : "d5396164-321e-4d62-88e8-3f672d86e9f0",
    "prId" : 30412,
    "prUrl" : "https://github.com/apache/spark/pull/30412#pullrequestreview-539735488",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "979a580d-e88e-47d4-8a3e-8d3ac03f8a80",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "So we can do `cast(CharType)`? It actually casts to StringType? But don't we loss length info?",
        "createdAt" : "2020-11-26T08:23:44Z",
        "updatedAt" : "2020-11-27T10:45:20Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "8949b783-aed1-4440-a08d-b381dc6dafc2",
        "parentId" : "979a580d-e88e-47d4-8a3e-8d3ac03f8a80",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "If you do `col.cast(\"char(5)\")` before this PR, Spark already silently treats it as cast to string type. I don't want to change this behavior here. We can make it better later, by failing explicitly and saying that cast to char type is not supported.",
        "createdAt" : "2020-11-27T06:35:28Z",
        "updatedAt" : "2020-11-27T10:45:20Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "73b99dc7a98f5d0673d309adba457cd19144be92",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +1184,1188 @@  def cast(to: DataType): Column = withExpr {\n    Cast(expr, CharVarcharUtils.replaceCharVarcharWithString(to))\n  }\n\n  /**"
  },
  {
    "id" : "36cc3f46-8971-4f3d-9e11-e0beeca9d224",
    "prId" : 29795,
    "prUrl" : "https://github.com/apache/spark/pull/29795#pullrequestreview-492782599",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "daf91a11-a13f-405b-9bd6-32c619e78cd7",
        "parentId" : null,
        "authorId" : "6fc88871-fca6-485c-a47e-5e17709c7b68",
        "body" : "One of the issues in master branch with the current `Column.withField` implementation is the size of the parsed logical plan scales non-linearly with the number of directly-add-**nested**-column operations. This results in the driver spending a considerable amount of time analyzing and optimizing the logical plan (literally minutes, if it ever completes). \r\nUsers can avoid this issue entirely by writing their queries in a performant manner. \r\nFor example: \r\n\r\n```\r\n  lazy val nullableStructLevel2: DataFrame = spark.createDataFrame(\r\n    sparkContext.parallelize(Row(Row(Row(0))) :: Nil),\r\n    StructType(Seq(\r\n      StructField(\"a1\", StructType(Seq(\r\n        StructField(\"a2\", StructType(Seq(\r\n          StructField(\"col0\", IntegerType, nullable = false))),\r\n          nullable = true))),\r\n        nullable = true))))\r\n\r\n  val numColsToAdd = 100\r\n\r\n  val expectedRows = Row(Row(Row(0 to numColsToAdd: _*))) :: Nil\r\n  val expectedSchema =\r\n    StructType(Seq(\r\n      StructField(\"a1\", StructType(Seq(\r\n        StructField(\"a2\", StructType((0 to numColsToAdd).map(num =>\r\n          StructField(s\"col$num\", IntegerType, nullable = false))),\r\n          nullable = true))),\r\n        nullable = true)))\r\n\r\n  test(\"good way of writing query\") {\r\n    // Spark can easily analyze and optimize the parsed logical plan in seconds\r\n    checkAnswer(\r\n      nullableStructLevel2\r\n        .select(col(\"a1\").withField(\"a2\", (1 to numColsToAdd).foldLeft(col(\"a1.a2\")) {\r\n          (column, num) => column.withField(s\"col$num\", lit(num))\r\n        }).as(\"a1\")),\r\n      expectedRows,\r\n      expectedSchema)\r\n  }\r\n\r\n  test(\"bad way of writing the same query that will eventually fail with timeout exception with as little as numColsToAdd = 10\") {\r\n    checkAnswer(\r\n      nullableStructLevel2\r\n        .select((1 to numColsToAdd).foldLeft(col(\"a1\")) {\r\n          (column, num) => column.withField(s\"a2.col$num\", lit(num))\r\n        }.as(\"a1\")),\r\n      expectedRows,\r\n      expectedSchema)\r\n  }\r\n```\r\n\r\nThis issue and its solution is what I've attempted to capture here as part of the method doc. \r\n\r\nThere are other options here instead of method-doc-note: \r\n- We could potentially write some kind of optimization in `updateFieldsHelper` (I've bashed my head against this for a while but haven't been able to come up with anything satisfactory).\r\n- Remove the ability to change nested fields directly entirely. While this has the advantage that there will be absolutely no way to run into this \"performance\" issue, the user-experience definitely suffers for more advanced users who would know how to use these methods properly.  \r\n\r\nI've gone with what made most sense to me (method-doc-note) but am open to hearing other people's thoughts on the matter. \r\n",
        "createdAt" : "2020-09-21T00:58:42Z",
        "updatedAt" : "2020-09-29T20:56:48Z",
        "lastEditedBy" : "6fc88871-fca6-485c-a47e-5e17709c7b68",
        "tags" : [
        ]
      },
      {
        "id" : "c3a227ae-ddbf-4736-90f4-b43646cf6acf",
        "parentId" : "daf91a11-a13f-405b-9bd6-32c619e78cd7",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I think the same issue happens in `withColumn` as well. I'm fine with method doc.",
        "createdAt" : "2020-09-21T16:23:25Z",
        "updatedAt" : "2020-09-29T20:56:48Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "7e51f35580db72fda11153d76caf232b83e617cd",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +918,922 @@   *   // result: {\"a\":{\"a\":1,\"b\":2,\"c\":3,\"d\":4}}\n   * }}}\n   *\n   * @group expr_ops\n   * @since 3.1.0"
  },
  {
    "id" : "39a13d51-6f69-431a-9021-b2fbb76a9634",
    "prId" : 29795,
    "prUrl" : "https://github.com/apache/spark/pull/29795#pullrequestreview-496108643",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2b53db96-d6c1-46b8-a042-6a256a43a10e",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Let's explicitly mention that, if the name doesn't match any field, it's noop.",
        "createdAt" : "2020-09-23T04:43:29Z",
        "updatedAt" : "2020-09-29T20:56:48Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "2e51a2dd-54a3-4d07-9f8f-b61ce74a0668",
        "parentId" : "2b53db96-d6c1-46b8-a042-6a256a43a10e",
        "authorId" : "6fc88871-fca6-485c-a47e-5e17709c7b68",
        "body" : "I've made this change but now that I think about it, I don't think its actually classifies as a \"noop\". We still reconstruct the struct unfortunately e.g.\r\n```\r\nval structType = StructType(Seq(\r\n    StructField(\"a\", IntegerType, nullable = false),\r\n    StructField(\"b\", IntegerType, nullable = true),\r\n    StructField(\"c\", IntegerType, nullable = false)))\r\n\r\nval structLevel1: DataFrame = spark.createDataFrame(\r\n    sparkContext.parallelize(Row(Row(1, null, 3)) :: Nil),\r\n    StructType(Seq(StructField(\"a\", structType, nullable = false))))\r\n\r\nstructLevel1.withColumn(\"a\", 'a.dropFields(\"d\")).explain()\r\n\r\n== Physical Plan ==\r\n*(1) Project [named_struct(a, a#1.a, b, a#1.b, c, a#1.c) AS a#3]\r\n+- *(1) Scan ExistingRDD[a#1]\r\n```\r\nShould I revert this?",
        "createdAt" : "2020-09-25T01:44:45Z",
        "updatedAt" : "2020-09-29T20:56:48Z",
        "lastEditedBy" : "6fc88871-fca6-485c-a47e-5e17709c7b68",
        "tags" : [
        ]
      },
      {
        "id" : "27031667-c330-4839-84c1-0a01175f414a",
        "parentId" : "2b53db96-d6c1-46b8-a042-6a256a43a10e",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "It's semantically noop. We can optimize away the struct reconstructing later.",
        "createdAt" : "2020-09-25T04:51:13Z",
        "updatedAt" : "2020-09-29T20:56:48Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "7e51f35580db72fda11153d76caf232b83e617cd",
    "line" : 34,
    "diffHunk" : "@@ -1,1 +931,935 @@  // scalastyle:off line.size.limit\n  /**\n   * An expression that drops fields in `StructType` by name.\n   * This is a no-op if schema doesn't contain field name(s).\n   *"
  },
  {
    "id" : "981e9d3f-0158-4cbd-b3b1-94a60aee9645",
    "prId" : 29795,
    "prUrl" : "https://github.com/apache/spark/pull/29795#pullrequestreview-496057700",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c99d489f-e7f1-400d-b5f9-c53d62cce6aa",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "shall we fail here? I don't think the field name can be an empty string.\r\n\r\nIt's not related to this PR and we can consider it later.",
        "createdAt" : "2020-09-23T04:45:06Z",
        "updatedAt" : "2020-09-29T20:56:48Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "98dfe4fc-4d1b-454f-8623-7e3f5167de43",
        "parentId" : "c99d489f-e7f1-400d-b5f9-c53d62cce6aa",
        "authorId" : "6fc88871-fca6-485c-a47e-5e17709c7b68",
        "body" : "we've discussed this before [here](https://github.com/apache/spark/pull/27066#discussion_r448416127) :)\r\nIts needed for `withField` and I think we should support it in `dropFields` as well because `Dataset.drop` supports it: \r\n```\r\nscala> Seq((1, 2)).toDF(\"a\", \"\").drop(\"\").printSchema\r\nroot\r\n |-- a: integer (nullable = false)\r\n```\r\nI've added a test case to demonstrate this works on the `dropFields` side but otherwise left the code unchanged. \r\n",
        "createdAt" : "2020-09-25T01:45:48Z",
        "updatedAt" : "2020-09-29T20:56:48Z",
        "lastEditedBy" : "6fc88871-fca6-485c-a47e-5e17709c7b68",
        "tags" : [
        ]
      }
    ],
    "commit" : "7e51f35580db72fda11153d76caf232b83e617cd",
    "line" : 105,
    "diffHunk" : "@@ -1,1 +1002,1006 @@\n    if (fieldName.isEmpty) {\n      fieldName :: Nil\n    } else {\n      CatalystSqlParser.parseMultipartIdentifier(fieldName)"
  },
  {
    "id" : "01771044-9c9c-4719-a50c-8743498551e1",
    "prId" : 28328,
    "prUrl" : "https://github.com/apache/spark/pull/28328#pullrequestreview-400120837",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "84da13d4-150d-46d2-a099-60c62e438bd0",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "So, this is caused by SPARK-29048 (Improve performance on Column.isInCollection() with a large size collection, https://github.com/apache/spark/pull/25754 ) and only affects 3.0.0, right?",
        "createdAt" : "2020-04-24T15:45:03Z",
        "updatedAt" : "2020-04-27T10:37:01Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "c4f5f67d-33b5-446a-960a-d2b113b62c1e",
        "parentId" : "84da13d4-150d-46d2-a099-60c62e438bd0",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Correct",
        "createdAt" : "2020-04-24T16:13:49Z",
        "updatedAt" : "2020-04-27T10:37:01Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "55bfdbcd-4a4a-4642-bcc5-715cdfecce8a",
        "parentId" : "84da13d4-150d-46d2-a099-60c62e438bd0",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Thanks for confirming, @MaxGekk .\r\ncc @WeichenXu123 and @gatorsmile ",
        "createdAt" : "2020-04-24T17:02:59Z",
        "updatedAt" : "2020-04-27T10:37:01Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "4bc0e269df9320e9bb9244afb58b6d1fbbf0e95e",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +834,838 @@      InSet(expr, exprValues.map(_.eval()).toSet, elemType)\n    } else {\n      In(expr, exprValues)\n    }\n  }"
  },
  {
    "id" : "d562dd98-0de3-462b-9a50-eaa7fa9c7e30",
    "prId" : 28328,
    "prUrl" : "https://github.com/apache/spark/pull/28328#pullrequestreview-400829203",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "656c178c-e7d5-4af9-8bec-611c0fac2771",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "How can we make sure the `expr` has the same data type as `exprValues`? Do we have a type coercion rule for it?",
        "createdAt" : "2020-04-27T07:20:00Z",
        "updatedAt" : "2020-04-27T10:37:01Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "9a5c730f-0d33-4ff1-a296-1bf7d0513f40",
        "parentId" : "656c178c-e7d5-4af9-8bec-611c0fac2771",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "To make sure, we need something similar to `In.checkInputDataTypes()` in `InSet`:\r\nhttps://github.com/apache/spark/blob/7d8216a6642f40af0d1b623129b1d5f4c86bec68/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/predicates.scala#L313-L322\r\n\r\nI could add such check in the PR if you don't mind.",
        "createdAt" : "2020-04-27T09:58:11Z",
        "updatedAt" : "2020-04-27T10:37:01Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "64b81dd4-c0ec-4f51-854e-9e3e87f13418",
        "parentId" : "656c178c-e7d5-4af9-8bec-611c0fac2771",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "I added similar check to InSet",
        "createdAt" : "2020-04-27T10:37:11Z",
        "updatedAt" : "2020-04-27T10:37:11Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "4bc0e269df9320e9bb9244afb58b6d1fbbf0e95e",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +832,836 @@    if (exprValues.size > SQLConf.get.optimizerInSetConversionThreshold) {\n      val elemType = exprValues.headOption.map(_.dataType).getOrElse(NullType)\n      InSet(expr, exprValues.map(_.eval()).toSet, elemType)\n    } else {\n      In(expr, exprValues)"
  },
  {
    "id" : "bcee7095-198d-4401-9f23-cd787ccaf26d",
    "prId" : 27066,
    "prUrl" : "https://github.com/apache/spark/pull/27066#pullrequestreview-440954876",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "80a64780-e08f-4a7f-82f5-acdce8680f69",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "field name can't be an empty string. Shall we throw exception here?",
        "createdAt" : "2020-07-01T14:23:24Z",
        "updatedAt" : "2020-07-03T12:43:28Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "96d80071-8746-47ad-adae-3f074b78019b",
        "parentId" : "80a64780-e08f-4a7f-82f5-acdce8680f69",
        "authorId" : "6fc88871-fca6-485c-a47e-5e17709c7b68",
        "body" : "I consider this to be resolved given the results of the discussion [here](https://github.com/apache/spark/pull/27066#discussion_r448416127). ",
        "createdAt" : "2020-07-01T15:03:02Z",
        "updatedAt" : "2020-07-03T12:43:28Z",
        "lastEditedBy" : "6fc88871-fca6-485c-a47e-5e17709c7b68",
        "tags" : [
        ]
      }
    ],
    "commit" : "4315e9244829cc41ffcf224f29f8fdcbcd88b981",
    "line" : 43,
    "diffHunk" : "@@ -1,1 +911,915 @@\n    val nameParts = if (fieldName.isEmpty) {\n      fieldName :: Nil\n    } else {\n      CatalystSqlParser.parseMultipartIdentifier(fieldName)"
  },
  {
    "id" : "ced324f8-b0b9-4b58-836b-bc281d99d2cd",
    "prId" : 27066,
    "prUrl" : "https://github.com/apache/spark/pull/27066#pullrequestreview-460126058",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cedfdef5-9ee5-4547-9d74-a8c1d221e5ff",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "Have any of you try to run these examples? The optimizer ConstantFolding rule will break these examples. ",
        "createdAt" : "2020-08-03T01:50:33Z",
        "updatedAt" : "2020-08-03T01:50:33Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "7ed5fca5-d49c-45b7-849c-dadd235b5043",
        "parentId" : "cedfdef5-9ee5-4547-9d74-a8c1d221e5ff",
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "cc @fqaiser94 @dbtsai @cloud-fan ",
        "createdAt" : "2020-08-03T01:51:31Z",
        "updatedAt" : "2020-08-03T01:51:31Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "17cd21ab-63d9-4ef2-9e95-90886406ed90",
        "parentId" : "cedfdef5-9ee5-4547-9d74-a8c1d221e5ff",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "weird, we have tests to cover these examples. @fqaiser94 can you take a look?",
        "createdAt" : "2020-08-03T03:00:10Z",
        "updatedAt" : "2020-08-03T03:00:10Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "62b15c4d-3bd5-421d-aa97-bfa6562e920c",
        "parentId" : "cedfdef5-9ee5-4547-9d74-a8c1d221e5ff",
        "authorId" : "6fc88871-fca6-485c-a47e-5e17709c7b68",
        "body" : "I failed to write a test case to cover this scenario, my bad. \r\nAnd yea, I just tried this example again, and I can see that it fails. \r\nThe issue is that I `override foldable` for this `Unevaluable` Expression. And so, when `foldable` returns true, Spark tries to evaluate the expression and it fails at that point. \r\nI kind-of realized this as well recently and in my PR for `dropFields` [here](https://github.com/apache/spark/pull/29322/files#diff-c1758d627a06084e577be0d33d47f44eL566), I've fixed the issue (basically i just don't `override foldable` anymore, which by default returns `false`). \r\nI guess I should submit a follow-up PR to fix this immediately with associated unit tests? ",
        "createdAt" : "2020-08-03T03:32:45Z",
        "updatedAt" : "2020-08-03T03:34:21Z",
        "lastEditedBy" : "6fc88871-fca6-485c-a47e-5e17709c7b68",
        "tags" : [
        ]
      },
      {
        "id" : "36fb9fb6-1cb7-4847-85b7-d1e9ccd8b1ed",
        "parentId" : "cedfdef5-9ee5-4547-9d74-a8c1d221e5ff",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Yes, please",
        "createdAt" : "2020-08-03T03:52:25Z",
        "updatedAt" : "2020-08-03T03:52:25Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "b20fc39a-9eea-4b46-8897-5d8b8b8db3a7",
        "parentId" : "cedfdef5-9ee5-4547-9d74-a8c1d221e5ff",
        "authorId" : "6fc88871-fca6-485c-a47e-5e17709c7b68",
        "body" : "Thanks for raising the issue @gatorsmile \r\nI've created a [JIRA](https://issues.apache.org/jira/browse/SPARK-32521) and [PR](https://github.com/apache/spark/pull/29338) to address the issue. ",
        "createdAt" : "2020-08-03T15:31:46Z",
        "updatedAt" : "2020-08-03T15:31:46Z",
        "lastEditedBy" : "6fc88871-fca6-485c-a47e-5e17709c7b68",
        "tags" : [
        ]
      }
    ],
    "commit" : "4315e9244829cc41ffcf224f29f8fdcbcd88b981",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +878,882 @@   * {{{\n   *   val df = sql(\"SELECT named_struct('a', 1, 'b', 2) struct_col\")\n   *   df.select($\"struct_col\".withField(\"c\", lit(3)))\n   *   // result: {\"a\":1,\"b\":2,\"c\":3}\n   *"
  }
]