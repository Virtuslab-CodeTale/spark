[
  {
    "id" : "22db4c87-75ea-4ab0-b512-a5c8d0c0dfab",
    "prId" : 30974,
    "prUrl" : "https://github.com/apache/spark/pull/30974#pullrequestreview-568825410",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "84a8f313-1e02-466f-9461-db7b9f499d56",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "can we just do `case c: Cast => UnresolvedAlias(c, Some(Column.generateAlias))`?",
        "createdAt" : "2021-01-15T03:36:38Z",
        "updatedAt" : "2021-01-19T03:57:19Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "64b14279-1585-405f-a6de-6e81af62c58c",
        "parentId" : "84a8f313-1e02-466f-9461-db7b9f499d56",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "Seems we can't. This code will strip the `attribute` which is below the `Cast` so I leave it.\r\n\r\nA example, with the code, `spark.range(1).selectExpr(\"CAST(CAST(id as int) as double)\").columns` will return the `c1` instead of `CAST(CAST(c1 as INT) as DOUBLE)`.",
        "createdAt" : "2021-01-15T05:26:49Z",
        "updatedAt" : "2021-01-19T03:57:19Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      }
    ],
    "commit" : "1b00b8537cd4c2be4ad7715f4106947c3dc9a53a",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +178,182 @@    // If we have a top level Cast, there is a chance to give it a better alias, if there is a\n    // NamedExpression under this Cast.\n    case c: Cast =>\n      c.transformUp {\n        case c @ Cast(_: NamedExpression, _, _) => UnresolvedAlias(c)"
  },
  {
    "id" : "fbcc83e6-8abd-484f-a36d-fde0552082f1",
    "prId" : 30488,
    "prUrl" : "https://github.com/apache/spark/pull/30488#pullrequestreview-541808504",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "06fb0534-01e3-4496-a11d-5997f958e789",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "add some comments to explain the reason.",
        "createdAt" : "2020-12-01T11:34:19Z",
        "updatedAt" : "2020-12-02T15:39:45Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "d13b88c25a10022be0ff0190fd6fa8d158b24560",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +1168,1172 @@    // These denied metadata keys are used to strip the column reference related metadata for\n    // the Alias. So it won't be caught as a column reference in DetectAmbiguousSelfJoin.\n    Alias(expr, alias)(deniedMetadataKeys = Seq(Dataset.DATASET_ID_KEY, Dataset.COL_POS_KEY))\n  }\n"
  },
  {
    "id" : "d5396164-321e-4d62-88e8-3f672d86e9f0",
    "prId" : 30412,
    "prUrl" : "https://github.com/apache/spark/pull/30412#pullrequestreview-539735488",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "979a580d-e88e-47d4-8a3e-8d3ac03f8a80",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "So we can do `cast(CharType)`? It actually casts to StringType? But don't we loss length info?",
        "createdAt" : "2020-11-26T08:23:44Z",
        "updatedAt" : "2020-11-27T10:45:20Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "8949b783-aed1-4440-a08d-b381dc6dafc2",
        "parentId" : "979a580d-e88e-47d4-8a3e-8d3ac03f8a80",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "If you do `col.cast(\"char(5)\")` before this PR, Spark already silently treats it as cast to string type. I don't want to change this behavior here. We can make it better later, by failing explicitly and saying that cast to char type is not supported.",
        "createdAt" : "2020-11-27T06:35:28Z",
        "updatedAt" : "2020-11-27T10:45:20Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "73b99dc7a98f5d0673d309adba457cd19144be92",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +1184,1188 @@  def cast(to: DataType): Column = withExpr {\n    Cast(expr, CharVarcharUtils.replaceCharVarcharWithString(to))\n  }\n\n  /**"
  },
  {
    "id" : "36cc3f46-8971-4f3d-9e11-e0beeca9d224",
    "prId" : 29795,
    "prUrl" : "https://github.com/apache/spark/pull/29795#pullrequestreview-492782599",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "daf91a11-a13f-405b-9bd6-32c619e78cd7",
        "parentId" : null,
        "authorId" : "6fc88871-fca6-485c-a47e-5e17709c7b68",
        "body" : "One of the issues in master branch with the current `Column.withField` implementation is the size of the parsed logical plan scales non-linearly with the number of directly-add-**nested**-column operations. This results in the driver spending a considerable amount of time analyzing and optimizing the logical plan (literally minutes, if it ever completes). \r\nUsers can avoid this issue entirely by writing their queries in a performant manner. \r\nFor example: \r\n\r\n```\r\n  lazy val nullableStructLevel2: DataFrame = spark.createDataFrame(\r\n    sparkContext.parallelize(Row(Row(Row(0))) :: Nil),\r\n    StructType(Seq(\r\n      StructField(\"a1\", StructType(Seq(\r\n        StructField(\"a2\", StructType(Seq(\r\n          StructField(\"col0\", IntegerType, nullable = false))),\r\n          nullable = true))),\r\n        nullable = true))))\r\n\r\n  val numColsToAdd = 100\r\n\r\n  val expectedRows = Row(Row(Row(0 to numColsToAdd: _*))) :: Nil\r\n  val expectedSchema =\r\n    StructType(Seq(\r\n      StructField(\"a1\", StructType(Seq(\r\n        StructField(\"a2\", StructType((0 to numColsToAdd).map(num =>\r\n          StructField(s\"col$num\", IntegerType, nullable = false))),\r\n          nullable = true))),\r\n        nullable = true)))\r\n\r\n  test(\"good way of writing query\") {\r\n    // Spark can easily analyze and optimize the parsed logical plan in seconds\r\n    checkAnswer(\r\n      nullableStructLevel2\r\n        .select(col(\"a1\").withField(\"a2\", (1 to numColsToAdd).foldLeft(col(\"a1.a2\")) {\r\n          (column, num) => column.withField(s\"col$num\", lit(num))\r\n        }).as(\"a1\")),\r\n      expectedRows,\r\n      expectedSchema)\r\n  }\r\n\r\n  test(\"bad way of writing the same query that will eventually fail with timeout exception with as little as numColsToAdd = 10\") {\r\n    checkAnswer(\r\n      nullableStructLevel2\r\n        .select((1 to numColsToAdd).foldLeft(col(\"a1\")) {\r\n          (column, num) => column.withField(s\"a2.col$num\", lit(num))\r\n        }.as(\"a1\")),\r\n      expectedRows,\r\n      expectedSchema)\r\n  }\r\n```\r\n\r\nThis issue and its solution is what I've attempted to capture here as part of the method doc. \r\n\r\nThere are other options here instead of method-doc-note: \r\n- We could potentially write some kind of optimization in `updateFieldsHelper` (I've bashed my head against this for a while but haven't been able to come up with anything satisfactory).\r\n- Remove the ability to change nested fields directly entirely. While this has the advantage that there will be absolutely no way to run into this \"performance\" issue, the user-experience definitely suffers for more advanced users who would know how to use these methods properly.  \r\n\r\nI've gone with what made most sense to me (method-doc-note) but am open to hearing other people's thoughts on the matter. \r\n",
        "createdAt" : "2020-09-21T00:58:42Z",
        "updatedAt" : "2020-09-29T20:56:48Z",
        "lastEditedBy" : "6fc88871-fca6-485c-a47e-5e17709c7b68",
        "tags" : [
        ]
      },
      {
        "id" : "c3a227ae-ddbf-4736-90f4-b43646cf6acf",
        "parentId" : "daf91a11-a13f-405b-9bd6-32c619e78cd7",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I think the same issue happens in `withColumn` as well. I'm fine with method doc.",
        "createdAt" : "2020-09-21T16:23:25Z",
        "updatedAt" : "2020-09-29T20:56:48Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "7e51f35580db72fda11153d76caf232b83e617cd",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +918,922 @@   *   // result: {\"a\":{\"a\":1,\"b\":2,\"c\":3,\"d\":4}}\n   * }}}\n   *\n   * @group expr_ops\n   * @since 3.1.0"
  },
  {
    "id" : "39a13d51-6f69-431a-9021-b2fbb76a9634",
    "prId" : 29795,
    "prUrl" : "https://github.com/apache/spark/pull/29795#pullrequestreview-496108643",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2b53db96-d6c1-46b8-a042-6a256a43a10e",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Let's explicitly mention that, if the name doesn't match any field, it's noop.",
        "createdAt" : "2020-09-23T04:43:29Z",
        "updatedAt" : "2020-09-29T20:56:48Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "2e51a2dd-54a3-4d07-9f8f-b61ce74a0668",
        "parentId" : "2b53db96-d6c1-46b8-a042-6a256a43a10e",
        "authorId" : "6fc88871-fca6-485c-a47e-5e17709c7b68",
        "body" : "I've made this change but now that I think about it, I don't think its actually classifies as a \"noop\". We still reconstruct the struct unfortunately e.g.\r\n```\r\nval structType = StructType(Seq(\r\n    StructField(\"a\", IntegerType, nullable = false),\r\n    StructField(\"b\", IntegerType, nullable = true),\r\n    StructField(\"c\", IntegerType, nullable = false)))\r\n\r\nval structLevel1: DataFrame = spark.createDataFrame(\r\n    sparkContext.parallelize(Row(Row(1, null, 3)) :: Nil),\r\n    StructType(Seq(StructField(\"a\", structType, nullable = false))))\r\n\r\nstructLevel1.withColumn(\"a\", 'a.dropFields(\"d\")).explain()\r\n\r\n== Physical Plan ==\r\n*(1) Project [named_struct(a, a#1.a, b, a#1.b, c, a#1.c) AS a#3]\r\n+- *(1) Scan ExistingRDD[a#1]\r\n```\r\nShould I revert this?",
        "createdAt" : "2020-09-25T01:44:45Z",
        "updatedAt" : "2020-09-29T20:56:48Z",
        "lastEditedBy" : "6fc88871-fca6-485c-a47e-5e17709c7b68",
        "tags" : [
        ]
      },
      {
        "id" : "27031667-c330-4839-84c1-0a01175f414a",
        "parentId" : "2b53db96-d6c1-46b8-a042-6a256a43a10e",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "It's semantically noop. We can optimize away the struct reconstructing later.",
        "createdAt" : "2020-09-25T04:51:13Z",
        "updatedAt" : "2020-09-29T20:56:48Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "7e51f35580db72fda11153d76caf232b83e617cd",
    "line" : 34,
    "diffHunk" : "@@ -1,1 +931,935 @@  // scalastyle:off line.size.limit\n  /**\n   * An expression that drops fields in `StructType` by name.\n   * This is a no-op if schema doesn't contain field name(s).\n   *"
  },
  {
    "id" : "981e9d3f-0158-4cbd-b3b1-94a60aee9645",
    "prId" : 29795,
    "prUrl" : "https://github.com/apache/spark/pull/29795#pullrequestreview-496057700",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c99d489f-e7f1-400d-b5f9-c53d62cce6aa",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "shall we fail here? I don't think the field name can be an empty string.\r\n\r\nIt's not related to this PR and we can consider it later.",
        "createdAt" : "2020-09-23T04:45:06Z",
        "updatedAt" : "2020-09-29T20:56:48Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "98dfe4fc-4d1b-454f-8623-7e3f5167de43",
        "parentId" : "c99d489f-e7f1-400d-b5f9-c53d62cce6aa",
        "authorId" : "6fc88871-fca6-485c-a47e-5e17709c7b68",
        "body" : "we've discussed this before [here](https://github.com/apache/spark/pull/27066#discussion_r448416127) :)\r\nIts needed for `withField` and I think we should support it in `dropFields` as well because `Dataset.drop` supports it: \r\n```\r\nscala> Seq((1, 2)).toDF(\"a\", \"\").drop(\"\").printSchema\r\nroot\r\n |-- a: integer (nullable = false)\r\n```\r\nI've added a test case to demonstrate this works on the `dropFields` side but otherwise left the code unchanged. \r\n",
        "createdAt" : "2020-09-25T01:45:48Z",
        "updatedAt" : "2020-09-29T20:56:48Z",
        "lastEditedBy" : "6fc88871-fca6-485c-a47e-5e17709c7b68",
        "tags" : [
        ]
      }
    ],
    "commit" : "7e51f35580db72fda11153d76caf232b83e617cd",
    "line" : 105,
    "diffHunk" : "@@ -1,1 +1002,1006 @@\n    if (fieldName.isEmpty) {\n      fieldName :: Nil\n    } else {\n      CatalystSqlParser.parseMultipartIdentifier(fieldName)"
  }
]