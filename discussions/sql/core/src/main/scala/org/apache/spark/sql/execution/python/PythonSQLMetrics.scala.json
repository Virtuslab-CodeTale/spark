[
  {
    "id" : "bb96738f-4e76-42d5-b6e1-2bfb6cd1c318",
    "prId" : 31367,
    "prUrl" : "https://github.com/apache/spark/pull/31367#pullrequestreview-581528305",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "86025e56-d18e-4866-965d-51ea5aff4ea4",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "If you want to implement many metrics, how about using `Map[String, SQLMetric]` instead? (see `ShuffledRowRDD`)",
        "createdAt" : "2021-01-29T00:29:38Z",
        "updatedAt" : "2021-04-02T13:39:31Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "6aabf856-a6fc-457a-b883-7b120c347fae",
        "parentId" : "86025e56-d18e-4866-965d-51ea5aff4ea4",
        "authorId" : "acf5aefc-4c46-451e-a28d-492ceaffd160",
        "body" : "My original thought here was to use a trait so that we can reuse the metrics definitaion with the many physical plans that need it (BatchEvalPythonExec, ArrowEcalPythonExec, AggregateInPandasExec, etc). Other implementations of this can work too. I see that `ShuffleRowRDD` uses a custom object `SQLShuffleMetricsReporter` to define and handle the metrics. Can you please elaborate a bit more on your proposal?",
        "createdAt" : "2021-02-02T16:26:21Z",
        "updatedAt" : "2021-04-02T13:39:31Z",
        "lastEditedBy" : "acf5aefc-4c46-451e-a28d-492ceaffd160",
        "tags" : [
        ]
      }
    ],
    "commit" : "96904262c08cd668d22b8792c00e280974d5c9d7",
    "line" : 25,
    "diffHunk" : "@@ -1,1 +23,27 @@private[python] trait PythonSQLMetrics { self: SparkPlan =>\n\n  override val metrics = Map(\n    \"pythonExecTime\" ->\n      SQLMetrics.createNanoTimingMetric(sparkContext, \"time spent executing\"),"
  }
]