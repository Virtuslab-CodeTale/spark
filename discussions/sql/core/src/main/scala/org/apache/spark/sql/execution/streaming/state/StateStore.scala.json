[
  {
    "id" : "9bb036a6-04c9-4026-89b3-2a34ecfa18e8",
    "prId" : 33038,
    "prUrl" : "https://github.com/apache/spark/pull/33038#pullrequestreview-703649227",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7da0f282-7001-4008-a449-d84c5dcef2e1",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "\"A value not greater than 0 means that the StateStore doesn't support `prefixScan` API.\"",
        "createdAt" : "2021-07-11T07:42:38Z",
        "updatedAt" : "2021-07-11T08:08:07Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "ab7747bc-8870-43d3-a033-d6b079c154b8",
        "parentId" : "7da0f282-7001-4008-a449-d84c5dcef2e1",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Thanks for the input! I agree we'd need to add more details.\r\n\r\nOne thing we may want to consider is that caller defines the prefix key, and it's natural (or we can document the behavior to `prefixScan` method doc) that it won't work if there's no prefix key.\r\n\r\nI feel we can just emphasize the behavior when the number is less than 1. I'll update the doc, and let's see whether it's good for you as well or not.",
        "createdAt" : "2021-07-12T01:09:11Z",
        "updatedAt" : "2021-07-12T01:09:11Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "42c105c4755542e657d7e3069cc4dd522f1f5ab3",
    "line" : 59,
    "diffHunk" : "@@ -1,1 +256,260 @@   * @param keySchema Schema of keys to be stored\n   * @param valueSchema Schema of value to be stored\n   * @param numColsPrefixKey The number of leftmost columns to be used as prefix key.\n   *                         A value not greater than 0 means the operator doesn't activate prefix\n   *                         key, and the operator should not call prefixScan method in StateStore."
  },
  {
    "id" : "646becbf-b218-4c5c-8cf7-540b1c7242c6",
    "prId" : 33038,
    "prUrl" : "https://github.com/apache/spark/pull/33038#pullrequestreview-703699804",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d6685382-8ea0-48fc-9356-d8158a9e2f1a",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Is this necessary to support by third-party StateStore implementation? If an implementation doesn't support it, what would happens? Could you clarify it in the doc?",
        "createdAt" : "2021-07-11T07:46:29Z",
        "updatedAt" : "2021-07-11T08:08:07Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "e377aa26-5850-41e9-aeb7-e9350284d6ab",
        "parentId" : "d6685382-8ea0-48fc-9356-d8158a9e2f1a",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "E.g., `MemoryStateStore` doesn't support it.",
        "createdAt" : "2021-07-11T07:46:53Z",
        "updatedAt" : "2021-07-11T08:08:07Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "e4335f2d-2baa-4b4a-afa9-93ce86ca99f5",
        "parentId" : "d6685382-8ea0-48fc-9356-d8158a9e2f1a",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "MemoryStateStore is for test purpose. I just skipped implementing it as there's no usage of prefix scan against MemoryStateStore. We can add it whenever we need, like the case we start to feel it'd be easier to test the prefix scan against MemoryStateStore.\r\n\r\nFor 3rd party implementations, I'd say it is necessary to support prefix scan to continue supporting further Spark version. They can still use the trick that throwing exception and saying \"this state store provider doesn't support session window\", but I'd rather not say the feature is limited to session window in the future, hence it would be eventually no longer true and more and more things cannot be supported by such state store providers.\r\n\r\nIn the documentation we can guide the trick and what will happen with the trick, but we probably couldn't update the doc every time for which functionalities will break if prefix scan is not supported. If we still want to allow 3rd party implementations to support partial features, I'll add a guide in the doc.",
        "createdAt" : "2021-07-12T01:28:43Z",
        "updatedAt" : "2021-07-12T01:28:44Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "41672235-a035-4d73-9c55-6fc2b1062060",
        "parentId" : "d6685382-8ea0-48fc-9356-d8158a9e2f1a",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "I just commented on \"partial implementation\" of state store.",
        "createdAt" : "2021-07-12T04:09:24Z",
        "updatedAt" : "2021-07-12T04:09:24Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "42c105c4755542e657d7e3069cc4dd522f1f5ab3",
    "line" : 39,
    "diffHunk" : "@@ -1,1 +76,80 @@   * to the greater than 0.\n   */\n  def prefixScan(prefixKey: UnsafeRow): Iterator[UnsafeRowPair]\n\n  /** Return an iterator containing all the key-value pairs in the StateStore. */"
  },
  {
    "id" : "eea0ef4a-e4fd-48ca-bbcf-e64492ad4ae2",
    "prId" : 31369,
    "prUrl" : "https://github.com/apache/spark/pull/31369#pullrequestreview-579466033",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "30f5baf3-885e-44a3-a1ce-a04b52fbef02",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Just a question. Is it valid for all classes using `StateStoreCustomMetric` trait? At least for the following three in Spark.\r\n```\r\ncase class StateStoreCustomSumMetric(name: String, desc: String) extends StateStoreCustomMetric\r\ncase class StateStoreCustomSizeMetric(name: String, desc: String) extends StateStoreCustomMetric\r\ncase class StateStoreCustomTimingMetric(name: String, desc: String) extends StateStoreCustomMetric\r\n```",
        "createdAt" : "2021-01-29T17:24:23Z",
        "updatedAt" : "2021-01-29T17:40:50Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "fe2954f0-ad85-4274-b951-eb1464436d19",
        "parentId" : "30f5baf3-885e-44a3-a1ce-a04b52fbef02",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "yes",
        "createdAt" : "2021-01-29T17:25:55Z",
        "updatedAt" : "2021-01-29T17:40:50Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "da80d2f3ba816d308121387a8739dad5cab2fa36",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +170,174 @@    val combinedCustomMetrics = distinctCustomMetrics.map { customMetric =>\n      val sameMetrics = customMetrics.filter(_._1 == customMetric)\n      val sumOfMetrics = sameMetrics.map(_._2).sum\n      customMetric -> sumOfMetrics\n    }.toMap"
  },
  {
    "id" : "4d859b03-531f-469b-a90c-03c8e721f3ad",
    "prId" : 31369,
    "prUrl" : "https://github.com/apache/spark/pull/31369#pullrequestreview-580989364",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4fac776b-75da-4151-919e-d8e6d7c2189d",
        "parentId" : null,
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "nit but might be necessary: should we only do the sum logic for distinctCustomMetrics only when the overwriting happens? If you agree I'll submit a follow-up PR and let you review. @viirya @HeartSaVioR ",
        "createdAt" : "2021-02-02T02:57:52Z",
        "updatedAt" : "2021-02-02T02:58:45Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      },
      {
        "id" : "7e044f9b-5921-4194-933a-936d0af150e1",
        "parentId" : "4fac776b-75da-4151-919e-d8e6d7c2189d",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "My understanding is that combine is only called with multiple elements, where we expect these elements will have same custom metrics (as these are from state store provider). Please correct me if I'm missing here. ",
        "createdAt" : "2021-02-02T03:06:27Z",
        "updatedAt" : "2021-02-02T03:06:27Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "7f1050af-c201-45de-ae03-e52cf65ed9d5",
        "parentId" : "4fac776b-75da-4151-919e-d8e6d7c2189d",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Hm? You mean when there is only one custom metric without duplication? I think doing sum does not get incorrect result. Is there any benefit?",
        "createdAt" : "2021-02-02T03:07:53Z",
        "updatedAt" : "2021-02-02T03:07:53Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "b5f8485d-56cd-4518-aac1-9b02ba28c17e",
        "parentId" : "4fac776b-75da-4151-919e-d8e6d7c2189d",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "So if you assume the chance allMetrics is only having one element, then all these calculations are unnecessary. But this method already assumes the case we want multiple metrics to be combined, and I think we don't blindly call this method with one element. No benefit then.",
        "createdAt" : "2021-02-02T03:12:48Z",
        "updatedAt" : "2021-02-02T03:12:48Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "a710a17a-f998-4674-a31c-283112dd01b4",
        "parentId" : "4fac776b-75da-4151-919e-d8e6d7c2189d",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "```\r\nI think we don't blindly call this method with one element. No benefit then.\r\n```\r\nMake sense.",
        "createdAt" : "2021-02-02T05:09:16Z",
        "updatedAt" : "2021-02-02T05:09:16Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "da80d2f3ba816d308121387a8739dad5cab2fa36",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +168,172 @@    val distinctCustomMetrics = allMetrics.flatMap(_.customMetrics.keys).distinct\n    val customMetrics = allMetrics.flatMap(_.customMetrics)\n    val combinedCustomMetrics = distinctCustomMetrics.map { customMetric =>\n      val sameMetrics = customMetrics.filter(_._1 == customMetric)\n      val sumOfMetrics = sameMetrics.map(_._2).sum"
  },
  {
    "id" : "50a993ec-dbe0-4d90-be28-ee15ad3dba49",
    "prId" : 28707,
    "prUrl" : "https://github.com/apache/spark/pull/28707#pullrequestreview-432498276",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "51a6abcd-d062-4284-bb88-d56cca315f36",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "`For the first case`: I think it's for the cases?",
        "createdAt" : "2020-06-17T11:56:13Z",
        "updatedAt" : "2020-06-18T01:32:17Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "2ba92dc4-9ef7-448a-bfcc-b3a6314d0fc3",
        "parentId" : "51a6abcd-d062-4284-bb88-d56cca315f36",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "The resolution is for the first case. For the rest cases listing, they should be considered as user problems.",
        "createdAt" : "2020-06-17T15:05:22Z",
        "updatedAt" : "2020-06-18T01:32:17Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "557eb3099b3d0abe1fd2d7d91754fa747e05d200",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +152,156 @@    \"The following reasons may cause this: 1. An old Spark version wrote the checkpoint that is \" +\n    \"incompatible with the current one; 2. Broken checkpoint files; 3. The query is changed \" +\n    \"among restart. For the first case, you can try to restart the application without \" +\n    \"checkpoint or use the legacy Spark version to process the streaming state.\", null)\n"
  },
  {
    "id" : "595c0515-2c74-4eb7-aebf-2df92cbc6796",
    "prId" : 26935,
    "prUrl" : "https://github.com/apache/spark/pull/26935#pullrequestreview-517542935",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a4e92263-770d-40f7-9450-c750cc5167e8",
        "parentId" : null,
        "authorId" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "body" : "I think I understand what's the point of this comment but this is true for all API methods in `ReadStateStore`, right? Or have I missed something and this one is different somehow?",
        "createdAt" : "2020-10-26T09:20:21Z",
        "updatedAt" : "2020-10-29T23:31:56Z",
        "lastEditedBy" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "tags" : [
        ]
      },
      {
        "id" : "990f1c44-3546-4dd0-a8bf-5787a231d92b",
        "parentId" : "a4e92263-770d-40f7-9450-c750cc5167e8",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "`abort` is only the one which is not properly named in point of ReadStateStore. It should be `close`, but we found it tricky to respect backward compatibility with having `close`, hence we decided to leave it as it is. \r\n\r\nPlease refer https://github.com/apache/spark/pull/26935#discussion_r493236101",
        "createdAt" : "2020-10-26T22:42:09Z",
        "updatedAt" : "2020-10-29T23:31:56Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "53adce20-aa85-41b6-be09-a53af695f8bb",
        "parentId" : "a4e92263-770d-40f7-9450-c750cc5167e8",
        "authorId" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "body" : "After reading it I see now. It's not ideal but agree.",
        "createdAt" : "2020-10-27T10:19:55Z",
        "updatedAt" : "2020-10-29T23:31:56Z",
        "lastEditedBy" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "tags" : [
        ]
      }
    ],
    "commit" : "a58e9e1914d7a97a83e121d2ed10fa56e1d9445d",
    "line" : 46,
    "diffHunk" : "@@ -1,1 +81,85 @@   * Clean up the resource.\n   *\n   * The method name is to respect backward compatibility on [[StateStore]].\n   */\n  def abort(): Unit"
  },
  {
    "id" : "a5b3d72e-7f8c-4f1a-99a5-86eee0b30133",
    "prId" : 26935,
    "prUrl" : "https://github.com/apache/spark/pull/26935#pullrequestreview-517510014",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1fb8a254-560a-4d57-b1bc-303aaa2f99c3",
        "parentId" : null,
        "authorId" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "body" : "Having an API which may or may not need to be called looks super odd.\r\nNot yet sure so asking doesn't this break the `Liskow` substitution law? All baseclass instances must be replaceable w/ the subclass instance. If I understand correctly `ReadStateStore.abort` always called to free up some resources. What happens when `StateStore.abort` called? According to the doc it may or may not be called (which is good) but does subclass handles a call correctly? If a sublass fails or makes the behavior different only by calling `abort` then something is not 100%.\r\n",
        "createdAt" : "2020-10-26T13:52:04Z",
        "updatedAt" : "2020-10-29T23:31:56Z",
        "lastEditedBy" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "tags" : [
        ]
      },
      {
        "id" : "777550db-edd0-4d20-8ccd-ec6744e67281",
        "parentId" : "1fb8a254-560a-4d57-b1bc-303aaa2f99c3",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "The pattern `prepare -> commit or abort -> (abort if commit fails)` is widely used across Spark, which both `commit` and `abort` have the responsibility of cleaning up. (`abort` needs to be careful of not impacted by double cleanup)\r\n\r\nIn ReadStateStore, the pattern is simplified to `prepare -> abort`, which `abort` has the responsibility of cleaning up.\r\n\r\nI see one case the issue may come up:\r\n\r\n- `class A implements ReadStateStore`\r\n- `class B extends A implements StateStore`\r\n- Both A and B have its own resources and needs to be cleaned up.\r\n\r\nIn this case, correctly cleaning up A's resource from B becomes non-trivial. B can call `A.abort` in both `B.commit` and `B.abort` to make sure A's resource is cleaned up but it is B's responsibility to not call A.abort multiple times as A would think there's no double cleanup as the simplified pattern.\r\n\r\nThe answer is probably the same as before: we know the problem - we need a `close` method, but backward compatibility also matters as we should provide the default implementation of `close` which is very tricky for current implementations (`close` should rely on resource cleanup in `commit`/`abort`) and we decided not to.\r\n\r\nMy feeling is that once the provider has different implementations between ReadStateStore and StateStore they would have different implementations (no inheritance between them). If it isn't  quite sure and we don't want to open the possibility, then adding `close` could be probably reconsidered.",
        "createdAt" : "2020-10-26T23:25:27Z",
        "updatedAt" : "2020-10-29T23:31:56Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "b50075bd-081f-4db6-bdc4-07a219b3f3d1",
        "parentId" : "1fb8a254-560a-4d57-b1bc-303aaa2f99c3",
        "authorId" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "body" : "Thanks for the example! After had a second look + read your explanation I think it's fine.\r\n\r\nMy rationale: Since the following contract applies both cases the given example is fine: `either commit or abort called, which must free up resources with double free guard`. In the mentioned example an internal `protected def close...` can be added to A which can be called from `B.commit` and `B.abort`. The mentioned `close` can contain double guard so with the actual API proper solution can be implemented. I agree that adding close would be more clear but considering the complexity I agree with you guys.\r\n",
        "createdAt" : "2020-10-27T09:42:02Z",
        "updatedAt" : "2020-10-29T23:31:56Z",
        "lastEditedBy" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "tags" : [
        ]
      }
    ],
    "commit" : "a58e9e1914d7a97a83e121d2ed10fa56e1d9445d",
    "line" : 82,
    "diffHunk" : "@@ -1,1 +120,124 @@   * no more updates (puts, removes) can be after an abort in order to avoid incorrect usage.\n   */\n  override def abort(): Unit\n\n  /**"
  },
  {
    "id" : "b60d9d31-adbc-414b-9907-3e5984ab4b03",
    "prId" : 26935,
    "prUrl" : "https://github.com/apache/spark/pull/26935#pullrequestreview-517460304",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e584f1f5-c136-48bf-8ed3-af0204cad482",
        "parentId" : null,
        "authorId" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "body" : "Nit: Can we move the 2 instances into `getStateStoreProvider`?",
        "createdAt" : "2020-10-26T13:56:58Z",
        "updatedAt" : "2020-10-29T23:31:56Z",
        "lastEditedBy" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "tags" : [
        ]
      },
      {
        "id" : "97561fcd-7f2e-458f-a63c-e380213a4543",
        "parentId" : "e584f1f5-c136-48bf-8ed3-af0204cad482",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "`getStateStoreProvider` doesn't relate to version, right? I wouldn't correlate something only to reduce code duplication.",
        "createdAt" : "2020-10-26T23:30:05Z",
        "updatedAt" : "2020-10-29T23:31:56Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "76906d89-0791-476e-8a36-dbf7ec9792ae",
        "parentId" : "e584f1f5-c136-48bf-8ed3-af0204cad482",
        "authorId" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "body" : "OK, makes sense.",
        "createdAt" : "2020-10-27T08:43:48Z",
        "updatedAt" : "2020-10-29T23:31:56Z",
        "lastEditedBy" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "tags" : [
        ]
      }
    ],
    "commit" : "a58e9e1914d7a97a83e121d2ed10fa56e1d9445d",
    "line" : 142,
    "diffHunk" : "@@ -1,1 +438,442 @@      storeConf: StateStoreConf,\n      hadoopConf: Configuration): ReadStateStore = {\n    require(version >= 0)\n    val storeProvider = getStateStoreProvider(storeProviderId, keySchema, valueSchema,\n      indexOrdinal, storeConf, hadoopConf)"
  },
  {
    "id" : "8d96a9aa-b34e-4316-b799-e0b3a866f74f",
    "prId" : 26935,
    "prUrl" : "https://github.com/apache/spark/pull/26935#pullrequestreview-520122185",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2697af7b-0beb-48dc-9804-01273275b979",
        "parentId" : null,
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "How about adding a default param `readOnly: Boolean = false` here? The code changes will be smaller since the refactor and getReadOnly are no longer needed.",
        "createdAt" : "2020-10-29T09:24:04Z",
        "updatedAt" : "2020-10-29T23:31:56Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      },
      {
        "id" : "43370712-40f1-4357-b5c2-887c5f07db00",
        "parentId" : "2697af7b-0beb-48dc-9804-01273275b979",
        "authorId" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "body" : "Having only one API and implementation would keep the actual `super hard to follow the code` situation.\r\nFrom my perspective having a separate API is more clean when one want to read the code.",
        "createdAt" : "2020-10-29T14:48:48Z",
        "updatedAt" : "2020-10-29T23:31:56Z",
        "lastEditedBy" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "tags" : [
        ]
      },
      {
        "id" : "b2d01f68-2eba-4a3d-8f05-1dae7cde0308",
        "parentId" : "2697af7b-0beb-48dc-9804-01273275b979",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "You seem to miss the difference of return types between two. They were actually one method and I had to break to support different return types.",
        "createdAt" : "2020-10-29T22:05:13Z",
        "updatedAt" : "2020-10-29T23:31:57Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "a58e9e1914d7a97a83e121d2ed10fa56e1d9445d",
    "line" : 153,
    "diffHunk" : "@@ -1,1 +452,456 @@      version: Long,\n      storeConf: StateStoreConf,\n      hadoopConf: Configuration): StateStore = {\n    require(version >= 0)\n    val storeProvider = getStateStoreProvider(storeProviderId, keySchema, valueSchema,"
  },
  {
    "id" : "6ad5f3f0-a502-4e0f-ad78-3f86fd0a88b4",
    "prId" : 26935,
    "prUrl" : "https://github.com/apache/spark/pull/26935#pullrequestreview-520835040",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4cdf7979-14f3-44e2-914a-acc034d7caff",
        "parentId" : null,
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "How about renaming this to ReadOnlyStateStore. I think it should be a better name consistent with other new functions in this PR, e.g mapPartitionsWithReadOnlyStateStore, ReadOnlyStateStoreRDD, etc",
        "createdAt" : "2020-10-29T14:27:56Z",
        "updatedAt" : "2020-10-29T23:31:56Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      },
      {
        "id" : "2a738566-cbbf-4e9a-9006-1407f37f12d8",
        "parentId" : "4cdf7979-14f3-44e2-914a-acc034d7caff",
        "authorId" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "body" : "+1 on this, I've suggested the same but there was another agreement before.",
        "createdAt" : "2020-10-29T14:50:02Z",
        "updatedAt" : "2020-10-29T23:31:56Z",
        "lastEditedBy" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "tags" : [
        ]
      },
      {
        "id" : "b143e193-bf9d-4997-b865-7b9d468e3f79",
        "parentId" : "4cdf7979-14f3-44e2-914a-acc034d7caff",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Okay for me if you both think it is better. Not strong opinion here.",
        "createdAt" : "2020-10-29T17:13:49Z",
        "updatedAt" : "2020-10-29T23:31:56Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "2c83bc99-d2cf-4414-a9c8-5ed41fa18413",
        "parentId" : "4cdf7979-14f3-44e2-914a-acc034d7caff",
        "authorId" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "body" : "It's your call then @HeartSaVioR :)",
        "createdAt" : "2020-10-29T17:19:31Z",
        "updatedAt" : "2020-10-29T23:31:57Z",
        "lastEditedBy" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "tags" : [
        ]
      },
      {
        "id" : "4366d789-20b8-4a4d-801c-cb461c15e332",
        "parentId" : "4cdf7979-14f3-44e2-914a-acc034d7caff",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "OK I'll rename it. No big deal.",
        "createdAt" : "2020-10-29T22:43:47Z",
        "updatedAt" : "2020-10-29T23:31:57Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "c2bc561c-a4eb-4a7c-b2f4-c4f2715cd8ac",
        "parentId" : "4cdf7979-14f3-44e2-914a-acc034d7caff",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Ah sorry I reminded the comment from @viirya and still think his comment makes more sense. I'll rename others to make consistent for now, but I'm also open to rename this to ReadOnlyStateStore, if both of you think it's better after reading the comment from @viirya .\r\n\r\n@xuanyuanking @gaborgsomogyi Could you please go through the previous comment below and comment how you think?\r\nhttps://github.com/apache/spark/pull/26935#discussion_r491747706",
        "createdAt" : "2020-10-29T22:53:48Z",
        "updatedAt" : "2020-10-29T23:31:57Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "48ad5cf7-3231-45be-ace2-975a1ba16314",
        "parentId" : "4cdf7979-14f3-44e2-914a-acc034d7caff",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "Ah sorry +1... Didn't notice the previous discussion.\r\nI think to rename others for consistency is enough.",
        "createdAt" : "2020-10-30T03:23:21Z",
        "updatedAt" : "2020-10-30T03:23:22Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      },
      {
        "id" : "db9fcee2-82b3-494e-8298-30184c58dcb3",
        "parentId" : "4cdf7979-14f3-44e2-914a-acc034d7caff",
        "authorId" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "body" : "Just gone through and makes sense, +1.",
        "createdAt" : "2020-10-30T16:28:48Z",
        "updatedAt" : "2020-10-30T16:28:48Z",
        "lastEditedBy" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "tags" : [
        ]
      }
    ],
    "commit" : "a58e9e1914d7a97a83e121d2ed10fa56e1d9445d",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +44,48 @@ * the method.\n */\ntrait ReadStateStore {\n\n  /** Unique identifier of the store */"
  }
]