[
  {
    "id" : "9bb036a6-04c9-4026-89b3-2a34ecfa18e8",
    "prId" : 33038,
    "prUrl" : "https://github.com/apache/spark/pull/33038#pullrequestreview-703649227",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7da0f282-7001-4008-a449-d84c5dcef2e1",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "\"A value not greater than 0 means that the StateStore doesn't support `prefixScan` API.\"",
        "createdAt" : "2021-07-11T07:42:38Z",
        "updatedAt" : "2021-07-11T08:08:07Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "ab7747bc-8870-43d3-a033-d6b079c154b8",
        "parentId" : "7da0f282-7001-4008-a449-d84c5dcef2e1",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Thanks for the input! I agree we'd need to add more details.\r\n\r\nOne thing we may want to consider is that caller defines the prefix key, and it's natural (or we can document the behavior to `prefixScan` method doc) that it won't work if there's no prefix key.\r\n\r\nI feel we can just emphasize the behavior when the number is less than 1. I'll update the doc, and let's see whether it's good for you as well or not.",
        "createdAt" : "2021-07-12T01:09:11Z",
        "updatedAt" : "2021-07-12T01:09:11Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "42c105c4755542e657d7e3069cc4dd522f1f5ab3",
    "line" : 59,
    "diffHunk" : "@@ -1,1 +256,260 @@   * @param keySchema Schema of keys to be stored\n   * @param valueSchema Schema of value to be stored\n   * @param numColsPrefixKey The number of leftmost columns to be used as prefix key.\n   *                         A value not greater than 0 means the operator doesn't activate prefix\n   *                         key, and the operator should not call prefixScan method in StateStore."
  },
  {
    "id" : "646becbf-b218-4c5c-8cf7-540b1c7242c6",
    "prId" : 33038,
    "prUrl" : "https://github.com/apache/spark/pull/33038#pullrequestreview-703699804",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d6685382-8ea0-48fc-9356-d8158a9e2f1a",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Is this necessary to support by third-party StateStore implementation? If an implementation doesn't support it, what would happens? Could you clarify it in the doc?",
        "createdAt" : "2021-07-11T07:46:29Z",
        "updatedAt" : "2021-07-11T08:08:07Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "e377aa26-5850-41e9-aeb7-e9350284d6ab",
        "parentId" : "d6685382-8ea0-48fc-9356-d8158a9e2f1a",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "E.g., `MemoryStateStore` doesn't support it.",
        "createdAt" : "2021-07-11T07:46:53Z",
        "updatedAt" : "2021-07-11T08:08:07Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "e4335f2d-2baa-4b4a-afa9-93ce86ca99f5",
        "parentId" : "d6685382-8ea0-48fc-9356-d8158a9e2f1a",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "MemoryStateStore is for test purpose. I just skipped implementing it as there's no usage of prefix scan against MemoryStateStore. We can add it whenever we need, like the case we start to feel it'd be easier to test the prefix scan against MemoryStateStore.\r\n\r\nFor 3rd party implementations, I'd say it is necessary to support prefix scan to continue supporting further Spark version. They can still use the trick that throwing exception and saying \"this state store provider doesn't support session window\", but I'd rather not say the feature is limited to session window in the future, hence it would be eventually no longer true and more and more things cannot be supported by such state store providers.\r\n\r\nIn the documentation we can guide the trick and what will happen with the trick, but we probably couldn't update the doc every time for which functionalities will break if prefix scan is not supported. If we still want to allow 3rd party implementations to support partial features, I'll add a guide in the doc.",
        "createdAt" : "2021-07-12T01:28:43Z",
        "updatedAt" : "2021-07-12T01:28:44Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "41672235-a035-4d73-9c55-6fc2b1062060",
        "parentId" : "d6685382-8ea0-48fc-9356-d8158a9e2f1a",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "I just commented on \"partial implementation\" of state store.",
        "createdAt" : "2021-07-12T04:09:24Z",
        "updatedAt" : "2021-07-12T04:09:24Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "42c105c4755542e657d7e3069cc4dd522f1f5ab3",
    "line" : 39,
    "diffHunk" : "@@ -1,1 +76,80 @@   * to the greater than 0.\n   */\n  def prefixScan(prefixKey: UnsafeRow): Iterator[UnsafeRowPair]\n\n  /** Return an iterator containing all the key-value pairs in the StateStore. */"
  },
  {
    "id" : "eea0ef4a-e4fd-48ca-bbcf-e64492ad4ae2",
    "prId" : 31369,
    "prUrl" : "https://github.com/apache/spark/pull/31369#pullrequestreview-579466033",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "30f5baf3-885e-44a3-a1ce-a04b52fbef02",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Just a question. Is it valid for all classes using `StateStoreCustomMetric` trait? At least for the following three in Spark.\r\n```\r\ncase class StateStoreCustomSumMetric(name: String, desc: String) extends StateStoreCustomMetric\r\ncase class StateStoreCustomSizeMetric(name: String, desc: String) extends StateStoreCustomMetric\r\ncase class StateStoreCustomTimingMetric(name: String, desc: String) extends StateStoreCustomMetric\r\n```",
        "createdAt" : "2021-01-29T17:24:23Z",
        "updatedAt" : "2021-01-29T17:40:50Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "fe2954f0-ad85-4274-b951-eb1464436d19",
        "parentId" : "30f5baf3-885e-44a3-a1ce-a04b52fbef02",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "yes",
        "createdAt" : "2021-01-29T17:25:55Z",
        "updatedAt" : "2021-01-29T17:40:50Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "da80d2f3ba816d308121387a8739dad5cab2fa36",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +170,174 @@    val combinedCustomMetrics = distinctCustomMetrics.map { customMetric =>\n      val sameMetrics = customMetrics.filter(_._1 == customMetric)\n      val sumOfMetrics = sameMetrics.map(_._2).sum\n      customMetric -> sumOfMetrics\n    }.toMap"
  },
  {
    "id" : "4d859b03-531f-469b-a90c-03c8e721f3ad",
    "prId" : 31369,
    "prUrl" : "https://github.com/apache/spark/pull/31369#pullrequestreview-580989364",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4fac776b-75da-4151-919e-d8e6d7c2189d",
        "parentId" : null,
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "nit but might be necessary: should we only do the sum logic for distinctCustomMetrics only when the overwriting happens? If you agree I'll submit a follow-up PR and let you review. @viirya @HeartSaVioR ",
        "createdAt" : "2021-02-02T02:57:52Z",
        "updatedAt" : "2021-02-02T02:58:45Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      },
      {
        "id" : "7e044f9b-5921-4194-933a-936d0af150e1",
        "parentId" : "4fac776b-75da-4151-919e-d8e6d7c2189d",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "My understanding is that combine is only called with multiple elements, where we expect these elements will have same custom metrics (as these are from state store provider). Please correct me if I'm missing here. ",
        "createdAt" : "2021-02-02T03:06:27Z",
        "updatedAt" : "2021-02-02T03:06:27Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "7f1050af-c201-45de-ae03-e52cf65ed9d5",
        "parentId" : "4fac776b-75da-4151-919e-d8e6d7c2189d",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Hm? You mean when there is only one custom metric without duplication? I think doing sum does not get incorrect result. Is there any benefit?",
        "createdAt" : "2021-02-02T03:07:53Z",
        "updatedAt" : "2021-02-02T03:07:53Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "b5f8485d-56cd-4518-aac1-9b02ba28c17e",
        "parentId" : "4fac776b-75da-4151-919e-d8e6d7c2189d",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "So if you assume the chance allMetrics is only having one element, then all these calculations are unnecessary. But this method already assumes the case we want multiple metrics to be combined, and I think we don't blindly call this method with one element. No benefit then.",
        "createdAt" : "2021-02-02T03:12:48Z",
        "updatedAt" : "2021-02-02T03:12:48Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "a710a17a-f998-4674-a31c-283112dd01b4",
        "parentId" : "4fac776b-75da-4151-919e-d8e6d7c2189d",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "```\r\nI think we don't blindly call this method with one element. No benefit then.\r\n```\r\nMake sense.",
        "createdAt" : "2021-02-02T05:09:16Z",
        "updatedAt" : "2021-02-02T05:09:16Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "da80d2f3ba816d308121387a8739dad5cab2fa36",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +168,172 @@    val distinctCustomMetrics = allMetrics.flatMap(_.customMetrics.keys).distinct\n    val customMetrics = allMetrics.flatMap(_.customMetrics)\n    val combinedCustomMetrics = distinctCustomMetrics.map { customMetric =>\n      val sameMetrics = customMetrics.filter(_._1 == customMetric)\n      val sumOfMetrics = sameMetrics.map(_._2).sum"
  },
  {
    "id" : "50a993ec-dbe0-4d90-be28-ee15ad3dba49",
    "prId" : 28707,
    "prUrl" : "https://github.com/apache/spark/pull/28707#pullrequestreview-432498276",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "51a6abcd-d062-4284-bb88-d56cca315f36",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "`For the first case`: I think it's for the cases?",
        "createdAt" : "2020-06-17T11:56:13Z",
        "updatedAt" : "2020-06-18T01:32:17Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "2ba92dc4-9ef7-448a-bfcc-b3a6314d0fc3",
        "parentId" : "51a6abcd-d062-4284-bb88-d56cca315f36",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "The resolution is for the first case. For the rest cases listing, they should be considered as user problems.",
        "createdAt" : "2020-06-17T15:05:22Z",
        "updatedAt" : "2020-06-18T01:32:17Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "557eb3099b3d0abe1fd2d7d91754fa747e05d200",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +152,156 @@    \"The following reasons may cause this: 1. An old Spark version wrote the checkpoint that is \" +\n    \"incompatible with the current one; 2. Broken checkpoint files; 3. The query is changed \" +\n    \"among restart. For the first case, you can try to restart the application without \" +\n    \"checkpoint or use the legacy Spark version to process the streaming state.\", null)\n"
  }
]