[
  {
    "id" : "790523b2-ea13-4352-8792-8411f717b1bb",
    "prId" : 33279,
    "prUrl" : "https://github.com/apache/spark/pull/33279#pullrequestreview-705961435",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bbccf4ae-6135-47cb-9683-5a346e041ac1",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "After some more thought, I think it's better to use SQL metrics for it. It's very hard to know max/min/avg by reading the logs.\r\n\r\n@AngersZhuuuu I think you tried it before. Can you restore the work? ",
        "createdAt" : "2021-07-14T07:15:29Z",
        "updatedAt" : "2021-07-14T07:15:29Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "4dba9c78-afe5-4e75-ac2c-29c4a8f4e79c",
        "parentId" : "bbccf4ae-6135-47cb-9683-5a346e041ac1",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> After some more thought, I think it's better to use SQL metrics for it. It's very hard to know max/min/avg by reading the logs.\r\n> \r\n> @AngersZhuuuu I think you tried it before. Can you restore the work?\r\n\r\nYea, working on this",
        "createdAt" : "2021-07-14T07:51:47Z",
        "updatedAt" : "2021-07-14T07:51:47Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "fb3af2a23f6c4d9ca0bbc45451f73307341e78c6",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +308,312 @@          dataWriter.commit()\n        }\n        logInfo(s\"$taskAttemptID finished to write and commit. Elapsed time: $timeCost ms.\")\n        res\n      })(catchBlock = {"
  },
  {
    "id" : "622d39fe-4fd0-44d2-a7ea-e20404c4800c",
    "prId" : 32881,
    "prUrl" : "https://github.com/apache/spark/pull/32881#pullrequestreview-688077390",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4e780141-7141-43d0-ba61-6ab4b666d9aa",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "why not pass 2 parameter?",
        "createdAt" : "2021-06-21T05:23:18Z",
        "updatedAt" : "2021-06-21T05:23:18Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "87b1cb77-44ad-4a7d-a6a0-1107edfceccc",
        "parentId" : "4e780141-7141-43d0-ba61-6ab4b666d9aa",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "From my side, whenever I add the 11th parameter of a scala method, intellij will mark it as a lint error. Do we have # of parameter rule in Spark?",
        "createdAt" : "2021-06-21T06:45:42Z",
        "updatedAt" : "2021-06-21T06:45:47Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      }
    ],
    "commit" : "0f3df0f45dc12768c7b9843e84e28a50b279443e",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +97,101 @@      plan: SparkPlan,\n      fileFormat: FileFormat,\n      protocols: (FileCommitProtocol, FileNamingProtocol),\n      outputSpec: OutputSpec,\n      hadoopConf: Configuration,"
  }
]