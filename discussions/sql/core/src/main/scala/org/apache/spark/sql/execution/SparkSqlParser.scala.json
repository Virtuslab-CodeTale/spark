[
  {
    "id" : "b76da26d-cc7c-490b-902f-1dcd6e84621a",
    "prId" : 31721,
    "prUrl" : "https://github.com/apache/spark/pull/31721#pullrequestreview-603472820",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "274017a2-8569-46c8-928e-b0676b7a9456",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Can we update the docs above? -> ` Create a [[AddFileCommand]], [[AddJarCommand]], [[ListFilesCommand]] or  ... `",
        "createdAt" : "2021-03-03T23:45:41Z",
        "updatedAt" : "2021-03-09T06:39:03Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "ff708fa75c8a16943d0187d7854cd57480e22478",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +361,365 @@          case \"file\" => AddFileCommand(maybePaths)\n          case \"jar\" => AddJarCommand(maybePaths)\n          case \"archive\" => AddArchiveCommand(maybePaths)\n          case other => operationNotAllowed(s\"ADD with resource type '$other'\", ctx)\n        }"
  },
  {
    "id" : "b7e2984a-e4d2-4765-8c7b-a1249d5a14dd",
    "prId" : 30705,
    "prUrl" : "https://github.com/apache/spark/pull/30705#pullrequestreview-549751367",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "637b9d0a-0566-4789-9106-e94ffac252d4",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "can we update `getSerdeInfo` to remove the `skipCheck` parameter?",
        "createdAt" : "2020-12-10T13:21:32Z",
        "updatedAt" : "2020-12-14T02:09:26Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "86f4eba4-2693-4f6c-9baf-00c0c93070ab",
        "parentId" : "637b9d0a-0566-4789-9106-e94ffac252d4",
        "authorId" : "cce1c782-0596-44b6-8b99-6b77d2cca53c",
        "body" : "done",
        "createdAt" : "2020-12-11T02:08:04Z",
        "updatedAt" : "2020-12-14T02:09:26Z",
        "lastEditedBy" : "cce1c782-0596-44b6-8b99-6b77d2cca53c",
        "tags" : [
        ]
      }
    ],
    "commit" : "57825623ae1472a76a70c737760650aad55d8968",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +448,452 @@    val provider = ctx.tableProvider.asScala.headOption.map(_.multipartIdentifier.getText)\n    val location = visitLocationSpecList(ctx.locationSpec())\n    val serdeInfo = getSerdeInfo(\n      ctx.rowFormat.asScala.toSeq, ctx.createFileFormat.asScala.toSeq, ctx)\n    if (provider.isDefined && serdeInfo.isDefined) {"
  },
  {
    "id" : "942166d0-a7ec-4fc7-8720-938018da7898",
    "prId" : 29421,
    "prUrl" : "https://github.com/apache/spark/pull/29421#pullrequestreview-473157948",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "df427fbb-26f1-46eb-aee9-d55e220d6818",
        "parentId" : null,
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "@cloud-fan @maropu\r\nCheck more  in https://github.com/apache/spark/pull/29421#issuecomment-677495619\r\nOnly in hive  default mode \r\n```\r\n > select transform(*)\r\n    > using 'cat'\r\n    > from (\r\n    > select 1 as a, 2 as b, 3 as c\r\n    > ) tmp ;\r\n```\r\nwill take rest columns for last column as default. so changed here.\r\n",
        "createdAt" : "2020-08-21T15:17:18Z",
        "updatedAt" : "2020-10-26T02:51:45Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "e4a9c31e-4224-4b31-82e7-5c78140842d5",
        "parentId" : "df427fbb-26f1-46eb-aee9-d55e220d6818",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "We don't respect `WITH SERDEPROPERTIES`?",
        "createdAt" : "2020-08-24T06:07:06Z",
        "updatedAt" : "2020-10-26T02:51:45Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "fb16f4b8-d8e3-448b-ab2b-a53deff93e95",
        "parentId" : "df427fbb-26f1-46eb-aee9-d55e220d6818",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> We don't respect `WITH SERDEPROPERTIES`?\r\n\r\n```\r\nrowFormat\r\n    : ROW FORMAT SERDE name=STRING (WITH SERDEPROPERTIES props=tablePropertyList)?  #rowFormatSerde\r\n    | ROW FORMAT DELIMITED\r\n      (FIELDS TERMINATED BY fieldsTerminatedBy=STRING (ESCAPED BY escapedBy=STRING)?)?\r\n      (COLLECTION ITEMS TERMINATED BY collectionItemsTerminatedBy=STRING)?\r\n      (MAP KEYS TERMINATED BY keysTerminatedBy=STRING)?\r\n      (LINES TERMINATED BY linesSeparatedBy=STRING)?\r\n      (NULL DEFINED AS nullDefinedAs=STRING)?                                       #rowFormatDelimited\r\n    ;\r\n```\r\n\r\n`WITH SERDEPROPERTIES` must use with ROW FORMAT SERDE, and here is for case without `ROW FORMAT SERDE` or `ROW FORMAT DELIMITED`",
        "createdAt" : "2020-08-24T06:33:32Z",
        "updatedAt" : "2020-10-26T02:51:45Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "7f7c403f-3eca-470d-b64a-3c4d82e7cdd3",
        "parentId" : "df427fbb-26f1-46eb-aee9-d55e220d6818",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "And if user  specify ` ROW FORMAT SERDE`  will respect `WITH SERDEPROPERTIES`. \r\nALL UT IN `\"SPARK-32388: TRANSFORM should handle schema less correctly (hive serde)\"`",
        "createdAt" : "2020-08-24T06:37:06Z",
        "updatedAt" : "2020-10-26T02:51:45Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "ff8d3a6d-c71e-4e39-a5cc-cf7183b10b18",
        "parentId" : "df427fbb-26f1-46eb-aee9-d55e220d6818",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Ah, I see.",
        "createdAt" : "2020-08-24T06:41:47Z",
        "updatedAt" : "2020-10-26T02:51:45Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "32fef930-4fda-493e-a8bb-25ecdb122dd9",
        "parentId" : "df427fbb-26f1-46eb-aee9-d55e220d6818",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "https://issues.apache.org/jira/browse/SPARK-32684\r\nhttps://issues.apache.org/jira/browse/SPARK-32685\r\nThis two issue, WDYT?",
        "createdAt" : "2020-08-24T06:44:45Z",
        "updatedAt" : "2020-10-26T02:51:45Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "e83f6a55e20e7f4edfa11ad91e06115a5eb38c6d",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +788,792 @@        val props = Seq(\n          \"field.delim\" -> \"\\t\",\n          \"serialization.last.column.takes.rest\" -> \"true\")\n        val recordHandler = Option(conf.getConfString(configKey, defaultConfigValue))\n        (Nil, Option(name), props, recordHandler)"
  },
  {
    "id" : "91e45a27-ebf6-42c8-96e5-2f3bf842a175",
    "prId" : 29146,
    "prUrl" : "https://github.com/apache/spark/pull/29146#pullrequestreview-459723953",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "90287896-e6f3-4be4-91a8-7517b6d1f9a2",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Will it match something like `a ###`?  Shall we use `([a-zA-Z_\\d\\\\.:]+)$`?",
        "createdAt" : "2020-08-03T03:07:56Z",
        "updatedAt" : "2020-08-03T10:05:19Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "3065ed9aa5ce232ce4f16335cbf830bd93a73f3f",
    "line" : 26,
    "diffHunk" : "@@ -1,1 +73,77 @@      case configKeyValueDef(key, value) =>\n        SetCommand(Some(key -> Option(value.trim)))\n      case configKeyDef(key) =>\n        SetCommand(Some(key -> None))\n      case s if s == \"-v\" =>"
  },
  {
    "id" : "6e325957-6ccf-4770-a418-ae142906b85e",
    "prId" : 29146,
    "prUrl" : "https://github.com/apache/spark/pull/29146#pullrequestreview-459814582",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "da21b991-384e-4498-bd91-7216e5c5c1d2",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "`(EQ value=.*)` we have an alias, can we use it here?",
        "createdAt" : "2020-08-03T03:09:58Z",
        "updatedAt" : "2020-08-03T10:05:19Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "191a21f6-9cce-4e9e-8179-7ee088c6a412",
        "parentId" : "da21b991-384e-4498-bd91-7216e5c5c1d2",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "I did so first, but the added tests below failed;\r\n```\r\n     val keyStr = ctx.quotedConfigKey().getText\r\n     if (ctx.value != null) {\r\n-      SetCommand(Some(keyStr -> Option(remainder(ctx.EQ().getSymbol).trim)))\r\n+      SetCommand(Some(keyStr -> Option(ctx.value.getText)))\r\n     } else {\r\n       SetCommand(Some(keyStr -> None))\r\n     }\r\n\r\n// The failed tests\r\nassertEqual(\"SET `spark.sql.    key`=  -1\",\r\n  SetCommand(Some(\"spark.sql.    key\" -> Some(\"-1\"))))\r\n[info] - Report Error for invalid usage of SET command *** FAILED *** (104 milliseconds)\r\n[info]   == FAIL: Plans do not match ===\r\n[info]   !SetCommand (spark.sql.    key,Some(1))   SetCommand (spark.sql.    key,Some(-1)) (PlanTest.scala:157)\r\n...\r\n\r\nassertEqual(\"SET `spark.sql.    key`= v  a lu e \",\r\n  SetCommand(Some(\"spark.sql.    key\" -> Some(\"v  a lu e\"))))\r\n[info] - Report Error for invalid usage of SET command *** FAILED *** (121 milliseconds)\r\n[info]   == FAIL: Plans do not match ===\r\n[info]   !SetCommand (spark.sql.    key,Some(e))   SetCommand (spark.sql.    key,Some(v  a lu e)) (PlanTest.scala:157)\r\n[info]   org.scalatest.exceptions.TestFailedException:\r\n...\r\n```\r\nSo, I selected the current one to avoid the test failures. Do you know something about this behaivour?",
        "createdAt" : "2020-08-03T06:50:10Z",
        "updatedAt" : "2020-08-03T10:05:19Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "052c5ceb-20ba-488c-a8ea-dbc075edb323",
        "parentId" : "da21b991-384e-4498-bd91-7216e5c5c1d2",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "-      SetCommand(Some(keyStr -> Option(remainder(ctx.EQ().getSymbol).trim)))\r\n+      SetCommand(Some(keyStr -> Option(ctx.value.getText)))\r\n\r\none has trim but one doesn't. How about `ctx.value.getText.trim`?",
        "createdAt" : "2020-08-03T06:59:38Z",
        "updatedAt" : "2020-08-03T10:05:19Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "13540b25-7fca-4d5c-a019-0b986aff30ec",
        "parentId" : "da21b991-384e-4498-bd91-7216e5c5c1d2",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "I added `.trim` and checked that the same tests failed. To tell the truth, I'm not sure about why the rule `(EQ value=.*)` parses `v  a lu e` into `e` (Probably, this is the effect of the hidden channel?);\r\n```\r\n[info]   !SetCommand (spark.sql.    key,Some(e))   SetCommand (spark.sql.    key,Some(v  a lu e)) (PlanTest.scala:157)\r\n```",
        "createdAt" : "2020-08-03T07:39:10Z",
        "updatedAt" : "2020-08-03T10:05:19Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "2a4a3cec-8eb5-4dcb-9bf6-01769595105a",
        "parentId" : "da21b991-384e-4498-bd91-7216e5c5c1d2",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "ah, seems we should use `(value+=.)*`",
        "createdAt" : "2020-08-03T07:46:48Z",
        "updatedAt" : "2020-08-03T10:05:19Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "3065ed9aa5ce232ce4f16335cbf830bd93a73f3f",
    "line" : 42,
    "diffHunk" : "@@ -1,1 +89,93 @@    val keyStr = ctx.configKey().getText\n    if (ctx.EQ() != null) {\n      SetCommand(Some(keyStr -> Option(remainder(ctx.EQ().getSymbol).trim)))\n    } else {\n      SetCommand(Some(keyStr -> None))"
  },
  {
    "id" : "a1043ce4-4653-4eef-b6a5-e5cd86096bcc",
    "prId" : 29064,
    "prUrl" : "https://github.com/apache/spark/pull/29064#pullrequestreview-447120193",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "02f50b74-ae86-4c88-89d7-a3872af8be23",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "shall we use the `ZoneId.systemDefault()`?",
        "createdAt" : "2020-07-13T09:29:59Z",
        "updatedAt" : "2020-07-16T09:19:56Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "6064a76f-2e9a-4cb2-adec-bf185b72087d",
        "parentId" : "02f50b74-ae86-4c88-89d7-a3872af8be23",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "This is the same as the default value of `spark.sql.session.timeZone`, shall I change that part too if I change it here?",
        "createdAt" : "2020-07-13T09:45:39Z",
        "updatedAt" : "2020-07-16T09:19:56Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "d690d778-3a04-4ef1-906d-2b3cc3137dab",
        "parentId" : "02f50b74-ae86-4c88-89d7-a3872af8be23",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "OK let's keep it then. cc @MaxGekk ",
        "createdAt" : "2020-07-13T10:17:02Z",
        "updatedAt" : "2020-07-16T09:19:56Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "a4c60f3c84f2e1a90eea87fbc41d35741519f5af",
    "line" : 48,
    "diffHunk" : "@@ -1,1 +119,123 @@      ctx.timezone.getType match {\n        case SqlBaseParser.LOCAL =>\n          SetCommand(Some(key -> Some(TimeZone.getDefault.getID)))\n        case _ =>\n          SetCommand(Some(key -> Some(string(ctx.STRING))))"
  },
  {
    "id" : "c3dbef61-c74f-4e2a-93ea-85363034385b",
    "prId" : 29064,
    "prUrl" : "https://github.com/apache/spark/pull/29064#pullrequestreview-449603191",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c7dc5ead-57fb-4ca7-8ad0-1b5a710de4dd",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "So we don't respect the environment variable `TZ`. Shall we update the doc?",
        "createdAt" : "2020-07-15T16:55:56Z",
        "updatedAt" : "2020-07-16T09:19:56Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "6fdc0caf-aadb-431e-ada8-4f06afc6e01e",
        "parentId" : "c7dc5ead-57fb-4ca7-8ad0-1b5a710de4dd",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Do you mean the Java API already respects the `TZ` environment variable?",
        "createdAt" : "2020-07-16T07:43:48Z",
        "updatedAt" : "2020-07-16T09:19:56Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "615b608f-6d20-4bd7-ac79-42e23dd7bfe4",
        "parentId" : "c7dc5ead-57fb-4ca7-8ad0-1b5a710de4dd",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "```scala\r\n kentyao@hulk  ~/spark   SPARK-32272  TZ=\"America/Los_Angeles\" scala\r\nWelcome to Scala 2.13.1 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_251).\r\nType in expressions for evaluation. Or try :help.\r\n\r\nscala> java.util.TimeZone.getDefault.getID\r\nres0: String = America/Los_Angeles\r\n\r\n```\r\n\r\n``` scala\r\n  ✘ kentyao@hulk  ~/spark   SPARK-32272  TZ=\"Asia/Hong_kong\" scala\r\nWelcome to Scala 2.13.1 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_251).\r\nType in expressions for evaluation. Or try :help.\r\n\r\nscala> java.util.TimeZone.getDefault.getID\r\nres0: String = GMT+08:00\r\n```\r\ncheck on this",
        "createdAt" : "2020-07-16T07:46:43Z",
        "updatedAt" : "2020-07-16T09:19:56Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "543e55c8-e02f-49da-80c3-26eb03c75225",
        "parentId" : "c7dc5ead-57fb-4ca7-8ad0-1b5a710de4dd",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "Also,\r\n\r\n```\r\n kentyao@hulk  ~/spark   SPARK-32272  scala -Duser.timezone=\"America/Los_Angeles\"\r\nWelcome to Scala 2.13.1 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_251).\r\nType in expressions for evaluation. Or try :help.\r\n\r\nscala> java.util.TimeZone.getDefault.getID\r\nres0: String = America/Los_Angeles\r\n```\r\n```scala\r\n✘ kentyao@hulk  ~/spark   SPARK-32272  TZ=\"Asia/Hong_kong\" scala -Duser.timezone=\"America/Los_Angeles\"\r\nWelcome to Scala 2.13.1 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_251).\r\nType in expressions for evaluation. Or try :help.\r\n\r\nscala> java.util.TimeZone.getDefault.getID\r\nres0: String = America/Los_Angeles\r\n```\r\n",
        "createdAt" : "2020-07-16T07:51:55Z",
        "updatedAt" : "2020-07-16T09:19:56Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "a4c60f3c84f2e1a90eea87fbc41d35741519f5af",
    "line" : 48,
    "diffHunk" : "@@ -1,1 +119,123 @@      ctx.timezone.getType match {\n        case SqlBaseParser.LOCAL =>\n          SetCommand(Some(key -> Some(TimeZone.getDefault.getID)))\n        case _ =>\n          SetCommand(Some(key -> Some(string(ctx.STRING))))"
  },
  {
    "id" : "030baf77-2c5a-44e8-bbec-a84cccc874e4",
    "prId" : 28026,
    "prUrl" : "https://github.com/apache/spark/pull/28026#pullrequestreview-521329270",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "765c1325-7339-4dea-93fe-a534f1cec3b6",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "can we be consistent with https://github.com/apache/spark/pull/28026/files?file-filters%5B%5D=.scala#diff-26fe6b511f4b8aedf5bdffae0d7c03aba69463c09fa4f881d77520125796e0faR2999 ?",
        "createdAt" : "2020-11-02T04:46:36Z",
        "updatedAt" : "2020-11-24T17:03:15Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "25ec746753f29acd5e248d03db48211a3876a7c1",
    "line" : 231,
    "diffHunk" : "@@ -1,1 +442,446 @@    val serdeInfo = getSerdeInfo(\n      ctx.rowFormat.asScala, ctx.createFileFormat.asScala, ctx, skipCheck = true)\n    if (provider.isDefined && serdeInfo.isDefined) {\n      operationNotAllowed(s\"CREATE TABLE LIKE ... USING ... ${serdeInfo.get.describe}\", ctx)\n    }"
  },
  {
    "id" : "167d9b47-0a91-4466-8c6b-ebcb3902844c",
    "prId" : 27102,
    "prUrl" : "https://github.com/apache/spark/pull/27102#pullrequestreview-339016957",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "420260bb-bef2-49c2-8149-f607ca2c8202",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "If we use `visitCommentSpecList` here, I think we can also use `visitLocationSpecList` somewhere in this method as well.",
        "createdAt" : "2020-01-07T03:40:34Z",
        "updatedAt" : "2020-01-07T05:18:20Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "bd5544b67ae25241fcb61c3b2f1408ecc2592269",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +426,430 @@      partitionColumnNames = partitionCols.map(_.name),\n      properties = properties,\n      comment = visitCommentSpecList(ctx.commentSpec()))\n\n    val mode = if (ifNotExists) SaveMode.Ignore else SaveMode.ErrorIfExists"
  },
  {
    "id" : "e5e445b9-688c-4019-9048-512c3110db66",
    "prId" : 27039,
    "prUrl" : "https://github.com/apache/spark/pull/27039#pullrequestreview-359663303",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4ebeb4d2-db16-4dd2-92d2-ec7d70afe5d0",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Let's update this comment as well for `LOCAL`.",
        "createdAt" : "2020-02-17T08:24:11Z",
        "updatedAt" : "2020-02-17T13:14:30Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "f290a75b-b09a-4003-9c93-c10173b5d080",
        "parentId" : "4ebeb4d2-db16-4dd2-92d2-ec7d70afe5d0",
        "authorId" : "2c1bd8aa-c665-4159-a689-57743063347c",
        "body" : "Done",
        "createdAt" : "2020-02-17T11:20:25Z",
        "updatedAt" : "2020-02-17T13:14:30Z",
        "lastEditedBy" : "2c1bd8aa-c665-4159-a689-57743063347c",
        "tags" : [
        ]
      }
    ],
    "commit" : "6ddf84a68d9590f3ac7d2cd8d1b30ada6b699c64",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +760,764 @@   *   select_statement;\n   * }}}\n   */\n  override def visitInsertOverwriteDir(\n      ctx: InsertOverwriteDirContext): InsertDirParams = withOrigin(ctx) {"
  },
  {
    "id" : "6e296421-94ca-4a13-bf86-bfd79e797d33",
    "prId" : 26779,
    "prUrl" : "https://github.com/apache/spark/pull/26779#pullrequestreview-330308488",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0f086fa2-7918-4289-9842-b49f8f333a42",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Also, can you add some comments about admissible file path fixed by this pr in the description above?",
        "createdAt" : "2019-12-10T01:06:44Z",
        "updatedAt" : "2019-12-11T08:22:47Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "0c38d3cc-4633-42f6-9587-50bae303a353",
        "parentId" : "0f086fa2-7918-4289-9842-b49f8f333a42",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "+1",
        "createdAt" : "2019-12-10T03:26:10Z",
        "updatedAt" : "2019-12-11T08:22:47Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "dbf22dfa-0b92-4bb4-a222-691426d2c2d9",
        "parentId" : "0f086fa2-7918-4289-9842-b49f8f333a42",
        "authorId" : "832e1988-205f-40f9-89d5-c37ea16224c9",
        "body" : "I have added the comment about the expected file paths. Can you please review once.\r\n",
        "createdAt" : "2019-12-10T05:56:29Z",
        "updatedAt" : "2019-12-11T08:22:47Z",
        "lastEditedBy" : "832e1988-205f-40f9-89d5-c37ea16224c9",
        "tags" : [
        ]
      },
      {
        "id" : "181629fb-6cb3-4fd6-9a1c-f9c90f570fa4",
        "parentId" : "0f086fa2-7918-4289-9842-b49f8f333a42",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "nit: How about this? (I think we don't need the jira ID here)\r\n```\r\n   * Note that filepath/jarpath can be given as follows;\r\n   *  - /path/to/fileOrJar\r\n   *  - \"/path/to/fileOrJar\"\r\n   *  - '/path/to/fileOrJar'\r\n```",
        "createdAt" : "2019-12-11T05:24:40Z",
        "updatedAt" : "2019-12-11T08:22:47Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "e9d62771-4ab7-4fae-a165-e250e3ad9307",
        "parentId" : "0f086fa2-7918-4289-9842-b49f8f333a42",
        "authorId" : "832e1988-205f-40f9-89d5-c37ea16224c9",
        "body" : "Ok. I will make the changes. ",
        "createdAt" : "2019-12-11T05:42:05Z",
        "updatedAt" : "2019-12-11T08:22:47Z",
        "lastEditedBy" : "832e1988-205f-40f9-89d5-c37ea16224c9",
        "tags" : [
        ]
      }
    ],
    "commit" : "b808d51a05a4c382550b4648690ca68660ae0e3a",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +363,367 @@   */\n  override def visitManageResource(ctx: ManageResourceContext): LogicalPlan = withOrigin(ctx) {\n    val mayebePaths = if (ctx.STRING != null) string(ctx.STRING) else remainder(ctx.identifier).trim\n    ctx.op.getType match {\n      case SqlBaseParser.ADD =>"
  },
  {
    "id" : "16d1c214-1763-4cd1-b408-ca4670de0a0f",
    "prId" : 25398,
    "prUrl" : "https://github.com/apache/spark/pull/25398#pullrequestreview-273447957",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "29a962e8-6103-4d8e-94e8-e2525db92494",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Are you sure this is correct? It seems a valid value is `Some(DDLUtils.HIVE_PROVIDER)` or `None` for the third parameter.",
        "createdAt" : "2019-08-11T03:33:24Z",
        "updatedAt" : "2019-08-12T13:11:55Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "44e7fc94-3e5e-48f8-9a94-c9f3a4b3ff16",
        "parentId" : "29a962e8-6103-4d8e-94e8-e2525db92494",
        "authorId" : "a88f50e2-6440-4a6c-8883-3130888d05bc",
        "body" : "in case of `parquet` and `orc` we can use the respective file format instead of `hive`.\r\nIn case of ctas also we convert to use data source https://github.com/viirya/spark-1/blob/839a6ce1732fa37b5f8ec9afa2d51730fc6ca691/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveStrategies.scala#L188  \r\nThis will make it inline with that behavior.",
        "createdAt" : "2019-08-11T05:08:04Z",
        "updatedAt" : "2019-08-12T13:11:55Z",
        "lastEditedBy" : "a88f50e2-6440-4a6c-8883-3130888d05bc",
        "tags" : [
        ]
      }
    ],
    "commit" : "72d6dd4082cdc8d45204af8aabfecf78288c6eed",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +1399,1403 @@    val fileFormat = extractFileFormat(fileStorage.serde)\n    (ctx.LOCAL != null, storage, Some(fileFormat))\n  }\n\n  private def extractFileFormat(serde: Option[String]): String = {"
  },
  {
    "id" : "426f4c63-5dad-4f23-b269-f2d2361e3653",
    "prId" : 25398,
    "prUrl" : "https://github.com/apache/spark/pull/25398#pullrequestreview-300529380",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "03788658-787a-4f2c-ad3a-38cd3921f2de",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I think @maropu has kind of its point. Why don't you just use `STORED AS file_format` instead? BTW, I think it should be in `RelationConversions` with `spark.sql.hive.convertMetastoreXXX`.",
        "createdAt" : "2019-09-29T14:02:44Z",
        "updatedAt" : "2019-09-29T14:02:44Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "c4520835-7978-4637-b884-45c0b442a88e",
        "parentId" : "03788658-787a-4f2c-ad3a-38cd3921f2de",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Yea, I think so... `provider` seems to be mainly used for data source tables. In this hive path, that is just used for a flag, a hive table or not.",
        "createdAt" : "2019-10-01T00:52:05Z",
        "updatedAt" : "2019-10-01T00:59:35Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "1c39f4d1-00c4-452f-a8a2-b2ff4c54bfa9",
        "parentId" : "03788658-787a-4f2c-ad3a-38cd3921f2de",
        "authorId" : "a88f50e2-6440-4a6c-8883-3130888d05bc",
        "body" : "@HyukjinKwon I used `STORED AS file_format` only.. if we use `STORED AS` it will be hive managed, so in this PR i  convert it to data source if convertible, we follow same behavior  in case of `CTAS` which is fixed in [SPARK-25271](https://github.com/apache/spark/pull/22514)",
        "createdAt" : "2019-10-01T13:57:00Z",
        "updatedAt" : "2019-10-01T14:00:27Z",
        "lastEditedBy" : "a88f50e2-6440-4a6c-8883-3130888d05bc",
        "tags" : [
        ]
      },
      {
        "id" : "d560534b-1d15-4008-b481-350a378c638d",
        "parentId" : "03788658-787a-4f2c-ad3a-38cd3921f2de",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@Udbhav30, I more meant this seems not the right place to replace. You should add a configuration, and such replacement should be done in analysis/optimizing, not in the parser.\r\n\r\nIf that's the case, then why don't you use `USING file_format` explicitly? Can you please describe what this PR target to fix clearly? Spark lately added a new PR template (https://github.com/apache/spark/blob/master/.github/PULL_REQUEST_TEMPLATE). It might be better to follow this.",
        "createdAt" : "2019-10-11T02:11:41Z",
        "updatedAt" : "2019-10-11T02:12:29Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "da3120d6-4d70-4f8a-851f-6acacedcbeea",
        "parentId" : "03788658-787a-4f2c-ad3a-38cd3921f2de",
        "authorId" : "a88f50e2-6440-4a6c-8883-3130888d05bc",
        "body" : "@HyukjinKwon, i can use `USING file_format` explicitly that would serve the purpose but i thought it is better to fix this and make it inline with `CTAS` behavior  which is fixed in this [PR](https://github.com/apache/spark/pull/22514)\r\n\r\nIf you agree to go ahead i can try making changes in analysis/optimizing layer instead of parser as suggested by you. \r\nSure i will follow the template , I have updated the PR description.  thanks :)",
        "createdAt" : "2019-10-11T08:10:58Z",
        "updatedAt" : "2019-10-11T08:38:08Z",
        "lastEditedBy" : "a88f50e2-6440-4a6c-8883-3130888d05bc",
        "tags" : [
        ]
      }
    ],
    "commit" : "72d6dd4082cdc8d45204af8aabfecf78288c6eed",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +1401,1405 @@  }\n\n  private def extractFileFormat(serde: Option[String]): String = {\n    serde.map({ x =>\n      val lowerCaseSerde = x.toLowerCase(Locale.ROOT)"
  },
  {
    "id" : "f3bf1260-b508-4409-b774-fa64074e44f0",
    "prId" : 25390,
    "prUrl" : "https://github.com/apache/spark/pull/25390#pullrequestreview-274921732",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "62774175-44d7-4857-bf15-7001764e9c20",
        "parentId" : null,
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "What's the behavior of Hive for this scenario?",
        "createdAt" : "2019-08-09T03:06:23Z",
        "updatedAt" : "2019-08-14T18:42:31Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      },
      {
        "id" : "89cb5043-ba5b-4c66-ada8-e2e8647aabbd",
        "parentId" : "62774175-44d7-4857-bf15-7001764e9c20",
        "authorId" : "9d47f029-a518-4c37-8d35-e33a01bef60d",
        "body" : "In Spark 2.4, will throw exception like following and In Spark 3.0, will success\r\n```\r\nspark-sql> CREATE TABLE tbl(a int) PARTITIONED BY (b) STORED AS parquet;\r\nError in query:\r\nextraneous input ')' expecting {'SELECT', 'FROM', 'ADD', 'AS', 'ALL', 'ANY', 'DISTINCT', 'WHERE', 'GROUP', 'BY', 'GROUPING', 'SETS', 'CUBE', 'ROLLUP', 'ORDER', 'HAVING', 'LIMIT', 'AT', 'OR', 'AND', 'IN', NOT, 'NO', 'EXISTS', 'BETWEEN', 'LIKE', RLIKE, 'IS', 'NULL', 'TRUE', 'FALSE', 'NULLS', 'ASC', 'DESC', 'FOR', 'INTERVAL', 'CASE', 'WHEN', 'THEN', 'ELSE', 'END', 'JOIN', 'CROSS', 'OUTER', 'INNER', 'LEFT', 'SEMI', 'RIGHT', 'FULL', 'NATURAL', 'ON', 'PIVOT', 'LATERAL', 'WINDOW', 'OVER', 'PARTITION', 'RANGE', 'ROWS', 'UNBOUNDED', 'PRECEDING', 'FOLLOWING', 'CURRENT', 'FIRST', 'AFTER', 'LAST', 'ROW', 'WITH', 'VALUES', 'CREATE', 'TABLE', 'DIRECTORY', 'VIEW', 'REPLACE', 'INSERT', 'DELETE', 'INTO', 'DESCRIBE', 'EXPLAIN', 'FORMAT', 'LOGICAL', 'CODEGEN', 'COST', 'CAST', 'SHOW', 'TABLES', 'COLUMNS', 'COLUMN', 'USE', 'PARTITIONS', 'FUNCTIONS', 'DROP', 'UNION', 'EXCEPT', 'MINUS', 'INTERSECT', 'TO', 'TABLESAMPLE', 'STRATIFY', 'ALTER', 'RENAME', 'ARRAY', 'MAP', 'STRUCT', 'COMMENT', 'SET', 'RESET', 'DATA', 'START', 'TRANSACTION', 'COMMIT', 'ROLLBACK', 'MACRO', 'IGNORE', 'BOTH', 'LEADING', 'TRAILING', 'IF', 'POSITION', 'EXTRACT', 'DIV', 'PERCENT', 'BUCKET', 'OUT', 'OF', 'SORT', 'CLUSTER', 'DISTRIBUTE', 'OVERWRITE', 'TRANSFORM', 'REDUCE', 'SERDE', 'SERDEPROPERTIES', 'RECORDREADER', 'RECORDWRITER', 'DELIMITED', 'FIELDS', 'TERMINATED', 'COLLECTION', 'ITEMS', 'KEYS', 'ESCAPED', 'LINES', 'SEPARATED', 'FUNCTION', 'EXTENDED', 'REFRESH', 'CLEAR', 'CACHE', 'UNCACHE', 'LAZY', 'FORMATTED', 'GLOBAL', TEMPORARY, 'OPTIONS', 'UNSET', 'TBLPROPERTIES', 'DBPROPERTIES', 'BUCKETS', 'SKEWED', 'STORED', 'DIRECTORIES', 'LOCATION', 'EXCHANGE', 'ARCHIVE', 'UNARCHIVE', 'FILEFORMAT', 'TOUCH', 'COMPACT', 'CONCATENATE', 'CHANGE', 'CASCADE', 'RESTRICT', 'CLUSTERED', 'SORTED', 'PURGE', 'INPUTFORMAT', 'OUTPUTFORMAT', DATABASE, DATABASES, 'DFS', 'TRUNCATE', 'ANALYZE', 'COMPUTE', 'LIST', 'STATISTICS', 'PARTITIONED', 'EXTERNAL', 'DEFINED', 'REVOKE', 'GRANT', 'LOCK', 'UNLOCK', 'MSCK', 'REPAIR', 'RECOVER', 'EXPORT', 'IMPORT', 'LOAD', 'ROLE', 'ROLES', 'COMPACTIONS', 'PRINCIPALS', 'TRANSACTIONS', 'INDEX', 'INDEXES', 'LOCKS', 'OPTION', 'ANTI', 'LOCAL', 'INPATH', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 41)\r\n\r\n== SQL ==\r\nCREATE TABLE tbl(a int) PARTITIONED BY (b) STORED AS parquet\r\n-----------------------------------------^^^\r\n```\r\nI will test the behavior of Hive later",
        "createdAt" : "2019-08-09T03:28:40Z",
        "updatedAt" : "2019-08-14T18:42:31Z",
        "lastEditedBy" : "9d47f029-a518-4c37-8d35-e33a01bef60d",
        "tags" : [
        ]
      },
      {
        "id" : "195cfb50-e2df-40f0-a08b-275c8d717f77",
        "parentId" : "62774175-44d7-4857-bf15-7001764e9c20",
        "authorId" : "9d47f029-a518-4c37-8d35-e33a01bef60d",
        "body" : "In Hive 2.3.2, will throw exception like following:\r\n```\r\n> CREATE TABLE tbl(a int) PARTITIONED BY (b) STORED AS parquet;\r\nError: Error while compiling statement: FAILED: ParseException line 1:41 cannot recognize input near ')' 'STORED' 'AS' in column type (state=42000,code=40000)\r\n```",
        "createdAt" : "2019-08-09T13:35:11Z",
        "updatedAt" : "2019-08-14T18:42:31Z",
        "lastEditedBy" : "9d47f029-a518-4c37-8d35-e33a01bef60d",
        "tags" : [
        ]
      },
      {
        "id" : "23cba836-8d57-4118-b8f9-7668c8e85dc4",
        "parentId" : "62774175-44d7-4857-bf15-7001764e9c20",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "Thanks for your investigation, Hao!\r\nI think here to throw the exception for partition column type missing is the right behavior.\r\nThe current behavior should be the regression bug involves from #23376, it droped the partition column without type:\r\n```\r\nspark-sql> CREATE TABLE tbl(a int) PARTITIONED BY (b) STORED AS parquet;\r\nTime taken: 1.856 seconds\r\nspark-sql> desc tbl;\r\na\tint\tNULL\r\nTime taken: 0.46 seconds, Fetched 1 row(s)\r\n```\r\nCould you also test the behavior in Hive 3.0? ",
        "createdAt" : "2019-08-09T18:08:30Z",
        "updatedAt" : "2019-08-14T18:42:31Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      },
      {
        "id" : "7a0a4d18-dc3a-4b6c-b448-3be4437ab7e3",
        "parentId" : "62774175-44d7-4857-bf15-7001764e9c20",
        "authorId" : "9d47f029-a518-4c37-8d35-e33a01bef60d",
        "body" : "Hi, Yuanjian, thanks for the reasoning.\r\nAgree with you, Spark 2.4 and previous version,  will throw exception for partition column type missing,  [#23376](https://github.com/apache/spark/pull/23376) brought  the current behavior, this PR  intend to check this case and throw exception.\r\n\r\nDon't have an  hive 3 environment on hand , so I add a unit test case in Hive 3.1 branch and run it ,  exception will be thrown as hive 2\r\n```\r\njava.lang.RuntimeException: CREATE TABLE tbl(a int) PARTITIONED BY (b) STORED AS parquet; \r\nfailed: (responseCode = 40000, errorMessage = FAILED: ParseException line 1:41 cannot recognize input near ')' 'STORED' 'AS' in column type, SQLState = 42000, exception = line 1:41 cannot recognize input near ')' 'STORED' 'AS' in column type)\r\n```",
        "createdAt" : "2019-08-10T12:50:25Z",
        "updatedAt" : "2019-08-14T18:42:31Z",
        "lastEditedBy" : "9d47f029-a518-4c37-8d35-e33a01bef60d",
        "tags" : [
        ]
      },
      {
        "id" : "cceaadc0-4387-4af3-87d6-c5b2dc65462d",
        "parentId" : "62774175-44d7-4857-bf15-7001764e9c20",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "Thanks for checking, then I'm sure it's a regression bug. Is it possible to fix in SqlBase.g4?\r\nIf that's hard to do, then I'm ok to add this logic in SparkSqlParser.",
        "createdAt" : "2019-08-11T14:15:49Z",
        "updatedAt" : "2019-08-14T18:42:31Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      },
      {
        "id" : "9f9f7c63-aa95-4555-8d73-050ca197c12e",
        "parentId" : "62774175-44d7-4857-bf15-7001764e9c20",
        "authorId" : "9d47f029-a518-4c37-8d35-e33a01bef60d",
        "body" : "Hi, Yuanjian, I have tried some solutions which fix this case in SqlBase.g4 before and after this PR submitted, but the result isn't good, here is  the summary of all the solutions.\r\n\r\nIf we try to  fix this in SqlBase.g4 , i.e. during syntax analysis phase,  we have to  change createHiveTable syntax description to make Antlr split the analysis path into two branch,  one for CTAS  which accept syntax  that define partition column without data type and must have a AS query sub-clause at end of the DDL sentence.  Another  branch for  CT which accept syntax  that define partition column with data type and cannot  have a AS query sub-clause at end of the DDL sentence. \r\n\r\nWhen user use a illegal CT DDL with partition columns data type missed, the syntax analyzer will match this  with first branch, and give a misleading  error messages for user. And if user use CTAS DDL  but has partition columns data type defined,  the syntax analyzer will match this  with second branch, also give misleading error messages.\r\n\r\nIf we fix this in SparkSqlParser,   i.e. during semantic analysis phase,  we can not only hand the illegal CTAS&CT DDL case as Antlr,  but also give user a explicit and useful error message.",
        "createdAt" : "2019-08-12T16:30:09Z",
        "updatedAt" : "2019-08-14T18:42:31Z",
        "lastEditedBy" : "9d47f029-a518-4c37-8d35-e33a01bef60d",
        "tags" : [
        ]
      },
      {
        "id" : "15a281b8-218b-4f57-9451-582f27b2c324",
        "parentId" : "62774175-44d7-4857-bf15-7001764e9c20",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "Got it, thanks for trying! Then I'm ok to fix this behavior in SparkSqlParser.",
        "createdAt" : "2019-08-14T14:23:05Z",
        "updatedAt" : "2019-08-14T18:42:31Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "935af875d099451ff882662a91c9e137cd09b681",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +986,990 @@          CreateTable(tableDescWithPartitionColNames, mode, Some(q))\n        }\n      case None =>\n        // When creating partitioned table, we must specify data type for the partition columns.\n        if (Option(ctx.partitionColumnNames).isDefined) {"
  },
  {
    "id" : "0b9bb8b3-431d-4701-976c-db0404876079",
    "prId" : 24558,
    "prUrl" : "https://github.com/apache/spark/pull/24558#pullrequestreview-235553938",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "84982bcc-4aa8-4be3-bcd4-4414ca97ff9d",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Sorry, @sujith71955 . This will cause a huge behavior change. For me, the current Spark behavior is correct and more robust. If the location is given by users, we had better not remove it by `DROP TABLE`.\r\n\r\ncc @cloud-fan and @gatorsmile .",
        "createdAt" : "2019-05-08T18:32:45Z",
        "updatedAt" : "2019-05-08T18:33:12Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "bad2ed55-b2a3-4203-9a62-a53f7f581d38",
        "parentId" : "84982bcc-4aa8-4be3-bcd4-4414ca97ff9d",
        "authorId" : "af3c4746-a0c1-471e-870b-484d8f09968a",
        "body" : "k, then i will close this PR and JIRA.",
        "createdAt" : "2019-05-09T10:05:17Z",
        "updatedAt" : "2019-05-09T10:05:17Z",
        "lastEditedBy" : "af3c4746-a0c1-471e-870b-484d8f09968a",
        "tags" : [
        ]
      },
      {
        "id" : "106e0176-0107-4554-a0f7-2713705ec521",
        "parentId" : "84982bcc-4aa8-4be3-bcd4-4414ca97ff9d",
        "authorId" : "af3c4746-a0c1-471e-870b-484d8f09968a",
        "body" : "Thanks for your input.",
        "createdAt" : "2019-05-09T10:05:52Z",
        "updatedAt" : "2019-05-09T10:05:53Z",
        "lastEditedBy" : "af3c4746-a0c1-471e-870b-484d8f09968a",
        "tags" : [
        ]
      },
      {
        "id" : "de3690d7-eee6-4b66-ba0f-6f8a34028206",
        "parentId" : "84982bcc-4aa8-4be3-bcd4-4414ca97ff9d",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "CREATE EXTERNAL TABLE is a legacy syntax that we only keep it for hive compatibility. If you look at the CREATE TABLE syntax for native Spark tables, there is no EXTERNAL keyword.",
        "createdAt" : "2019-05-09T11:47:48Z",
        "updatedAt" : "2019-05-09T11:47:48Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "ae992b6c-092d-44b7-abf0-ff41d3c1af82",
        "parentId" : "84982bcc-4aa8-4be3-bcd4-4414ca97ff9d",
        "authorId" : "af3c4746-a0c1-471e-870b-484d8f09968a",
        "body" : "yeah i got the point, also this will make sure we wont delete user data, actually this point has come up because we got some usecases where customer facing issue in migrating the hive jobs to spark because of this behaviourial difference. they dont want to manage table :)",
        "createdAt" : "2019-05-09T12:12:05Z",
        "updatedAt" : "2019-05-09T12:12:05Z",
        "lastEditedBy" : "af3c4746-a0c1-471e-870b-484d8f09968a",
        "tags" : [
        ]
      },
      {
        "id" : "57c43017-5dfe-44fa-94e9-0f9cae3a6517",
        "parentId" : "84982bcc-4aa8-4be3-bcd4-4414ca97ff9d",
        "authorId" : "af3c4746-a0c1-471e-870b-484d8f09968a",
        "body" : "Even i am not able to figure out any work around to manage this scenario, except i can advice them to manually go and delete the data .",
        "createdAt" : "2019-05-09T12:16:31Z",
        "updatedAt" : "2019-05-09T12:16:31Z",
        "lastEditedBy" : "af3c4746-a0c1-471e-870b-484d8f09968a",
        "tags" : [
        ]
      }
    ],
    "commit" : "091a4c5da55d93ca38a70d77e1a6856b6f13e234",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +992,996 @@    // Otherwise, we may consider it as managed table where the data\n    // will be deleted on drop table command.\n    val tableType = if (external) {\n      CatalogTableType.EXTERNAL\n    } else {"
  }
]