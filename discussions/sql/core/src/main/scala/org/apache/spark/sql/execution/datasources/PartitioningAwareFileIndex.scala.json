[
  {
    "id" : "14290f23-0adc-4e4f-ba85-a5b217f8e0ab",
    "prId" : 28841,
    "prUrl" : "https://github.com/apache/spark/pull/28841#pullrequestreview-442116474",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4e4da2de-ffe0-4dc8-b09f-2d5c9267579f",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "`f` here is `FileStatus`. It looks inefficient to get the `FileStatus` again in https://github.com/apache/spark/pull/28841/files#diff-3c816e65d0bbdf16d46c7ea4e0b8cbaeR55",
        "createdAt" : "2020-07-02T13:29:15Z",
        "updatedAt" : "2020-11-05T10:34:15Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "5ed43420-cd30-41b4-827b-35af70f06656",
        "parentId" : "4e4da2de-ffe0-4dc8-b09f-2d5c9267579f",
        "authorId" : "e0be6955-58e9-4e7f-8132-27fdb08564c9",
        "body" : "> `f` here is `FileStatus`. It looks inefficient to get the `FileStatus` again in https://github.com/apache/spark/pull/28841/files#diff-3c816e65d0bbdf16d46c7ea4e0b8cbaeR55\r\n\r\nI agree.  I considered this in one of my earlier commits actually.  There are two places where we handle options in the *FileIndex classes, here in _PartitioningAwareFileIndex_ and _InMemoryFileIndex_.  In both places, we derive from the _PathFilter_ class which exposes an overridable method called _accept_ which takes _Path_ as its only parameter.  \r\n\r\nI could break with the abstraction and overload the method so that it would take a FileStatus directly.  I'll look into this unless there are any objections.",
        "createdAt" : "2020-07-02T20:07:19Z",
        "updatedAt" : "2020-11-05T10:34:15Z",
        "lastEditedBy" : "e0be6955-58e9-4e7f-8132-27fdb08564c9",
        "tags" : [
        ]
      },
      {
        "id" : "907f39a8-25cd-40ae-a4ef-82a931555742",
        "parentId" : "4e4da2de-ffe0-4dc8-b09f-2d5c9267579f",
        "authorId" : "e0be6955-58e9-4e7f-8132-27fdb08564c9",
        "body" : "I can put a wrapper around this to make the behavior more efficient.",
        "createdAt" : "2020-07-02T20:11:37Z",
        "updatedAt" : "2020-11-05T10:34:15Z",
        "lastEditedBy" : "e0be6955-58e9-4e7f-8132-27fdb08564c9",
        "tags" : [
        ]
      },
      {
        "id" : "09a4b7e6-40e8-4f76-b563-25479ce7af47",
        "parentId" : "4e4da2de-ffe0-4dc8-b09f-2d5c9267579f",
        "authorId" : "e0be6955-58e9-4e7f-8132-27fdb08564c9",
        "body" : "@cloud-fan This has been resolved.",
        "createdAt" : "2020-07-03T02:48:27Z",
        "updatedAt" : "2020-11-05T10:34:15Z",
        "lastEditedBy" : "e0be6955-58e9-4e7f-8132-27fdb08564c9",
        "tags" : [
        ]
      }
    ],
    "commit" : "bf2a66535c634ec68d667f3435a22220f38d29b5",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +84,88 @@            case Some(existingDir) =>\n              // Directory has children files in it, return them\n              existingDir.filter(f => matchPathPattern(f) && isNonEmptyFile(f))\n\n            case None =>"
  },
  {
    "id" : "f66a1b63-79f3-4357-94fa-ff64cd1f90e1",
    "prId" : 26195,
    "prUrl" : "https://github.com/apache/spark/pull/26195#pullrequestreview-325238677",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f4d9fb19-c0c1-4c28-acb0-212087f4154d",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Review note: I've inlined the `qualified()` function into `find()` clause.",
        "createdAt" : "2019-12-02T13:33:59Z",
        "updatedAt" : "2019-12-02T13:33:59Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "e889cda5b1ef4eb28dbdc276926a2318be3df531",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +225,229 @@        val qualifiedBasePathStr = qualifiedBasePath.toString\n        rootPaths\n          .find(!fs.makeQualified(_).toString.startsWith(qualifiedBasePathStr))\n          .foreach { rp =>\n            throw new IllegalArgumentException("
  },
  {
    "id" : "d6f04498-c728-475d-9381-9ea638295817",
    "prId" : 24830,
    "prUrl" : "https://github.com/apache/spark/pull/24830#pullrequestreview-250977315",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f23fe659-6da3-4c95-955f-ec7a4f4dc479",
        "parentId" : null,
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "This branch seems not reachable. Should we simply use `assert` here?",
        "createdAt" : "2019-06-18T06:00:49Z",
        "updatedAt" : "2019-06-19T04:15:36Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "ef103f8c-4e7a-4b36-9812-587342b8b84f",
        "parentId" : "f23fe659-6da3-4c95-955f-ec7a4f4dc479",
        "authorId" : "7e665d8b-d739-4edf-88c8-7379ff8585c2",
        "body" : "Oh, it is reachable I think.\r\nSee `class PrunedInMemoryFileIndex` which explicitly set `partitionSpec`.",
        "createdAt" : "2019-06-18T09:45:20Z",
        "updatedAt" : "2019-06-19T04:15:36Z",
        "lastEditedBy" : "7e665d8b-d739-4edf-88c8-7379ff8585c2",
        "tags" : [
        ]
      }
    ],
    "commit" : "6f2e75a56ac73240317e5cbabdc0dea34fbfe2fd",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +75,79 @@      PartitionDirectory(InternalRow.empty, allFiles().filter(isNonEmptyFile)) :: Nil\n    } else {\n      if (recursiveFileLookup) {\n        throw new IllegalArgumentException(\n          \"Datasource with partition do not allow recursive file loading.\")"
  },
  {
    "id" : "af0b7d0e-36b1-45f3-b6b2-1f2039a323b2",
    "prId" : 24830,
    "prUrl" : "https://github.com/apache/spark/pull/24830#pullrequestreview-325325169",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "42a12f16-9e74-4252-b2ab-06ac6833d2d1",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "shall we document the option in `DataFrameReader`?",
        "createdAt" : "2019-11-08T03:53:32Z",
        "updatedAt" : "2019-11-08T03:53:32Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "bbee1289-916d-4924-a973-c7802e5042e5",
        "parentId" : "42a12f16-9e74-4252-b2ab-06ac6833d2d1",
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "@Ngone51 Could you submit a follow-up PR to document this? This affects all the built-in file sources. We need to update the documentation of both PySpark and Scala APIs. ",
        "createdAt" : "2019-11-27T18:40:37Z",
        "updatedAt" : "2019-11-27T18:40:38Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "24e823f9-29b4-493c-bc43-3dbbe0d6408e",
        "parentId" : "42a12f16-9e74-4252-b2ab-06ac6833d2d1",
        "authorId" : "fd6ebc48-7da4-490b-8d41-2e8530d92720",
        "body" : "FYI, there is a Jira about adding this documentation which you will want to reference: [SPARK-29903](https://issues.apache.org/jira/browse/SPARK-29903)",
        "createdAt" : "2019-11-27T18:47:34Z",
        "updatedAt" : "2019-11-27T18:47:34Z",
        "lastEditedBy" : "fd6ebc48-7da4-490b-8d41-2e8530d92720",
        "tags" : [
        ]
      },
      {
        "id" : "8c835d61-545e-4f83-acc1-83264bb9ff9c",
        "parentId" : "42a12f16-9e74-4252-b2ab-06ac6833d2d1",
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "@nchammas Could you submit a PR to fix readwriter.py for supporting this new option? ",
        "createdAt" : "2019-11-27T19:04:36Z",
        "updatedAt" : "2019-11-27T19:04:36Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "56ac3b3c-996b-4b28-a41b-b1100d9a20ec",
        "parentId" : "42a12f16-9e74-4252-b2ab-06ac6833d2d1",
        "authorId" : "fd6ebc48-7da4-490b-8d41-2e8530d92720",
        "body" : "Sure, will do. I suppose we'll do that separately from adding the docs, which will get their own PR, correct?",
        "createdAt" : "2019-11-27T19:17:43Z",
        "updatedAt" : "2019-11-27T19:17:43Z",
        "lastEditedBy" : "fd6ebc48-7da4-490b-8d41-2e8530d92720",
        "tags" : [
        ]
      },
      {
        "id" : "8342778c-d1ad-423a-b129-3fe96a890e61",
        "parentId" : "42a12f16-9e74-4252-b2ab-06ac6833d2d1",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Guys, we should also update `DataStreamReadaer` and `streaming.py`.",
        "createdAt" : "2019-11-30T04:19:18Z",
        "updatedAt" : "2019-11-30T04:19:18Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "b97481e8-78ee-4aa6-ac4d-a3f05034575f",
        "parentId" : "42a12f16-9e74-4252-b2ab-06ac6833d2d1",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Ok, I'll submit a PR to document it. @gatorsmile ",
        "createdAt" : "2019-12-02T12:46:09Z",
        "updatedAt" : "2019-12-02T12:46:09Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "c6d57857-a10f-40dd-9c1e-579feebea706",
        "parentId" : "42a12f16-9e74-4252-b2ab-06ac6833d2d1",
        "authorId" : "fd6ebc48-7da4-490b-8d41-2e8530d92720",
        "body" : "FYI @Ngone51: #26718",
        "createdAt" : "2019-12-02T15:35:30Z",
        "updatedAt" : "2019-12-02T15:35:31Z",
        "lastEditedBy" : "fd6ebc48-7da4-490b-8d41-2e8530d92720",
        "tags" : [
        ]
      }
    ],
    "commit" : "6f2e75a56ac73240317e5cbabdc0dea34fbfe2fd",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +64,68 @@\n  protected lazy val recursiveFileLookup = {\n    parameters.getOrElse(\"recursiveFileLookup\", \"false\").toBoolean\n  }\n"
  },
  {
    "id" : "2735a21f-fa33-463e-898c-4a9633edb547",
    "prId" : 24518,
    "prUrl" : "https://github.com/apache/spark/pull/24518#pullrequestreview-233293967",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "acbab14a-1d39-47ac-9a2e-fb4c2a0b70f1",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I think this option should be documented in `DataFrameReader` and `DataStreamReader` like `timeZone`.",
        "createdAt" : "2019-05-03T00:35:07Z",
        "updatedAt" : "2019-05-07T18:03:42Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "d8f8420d9d3c97f96c1e09855e008ece3f275ad3",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +57,61 @@  protected def leafDirToChildrenFiles: Map[Path, Array[FileStatus]]\n\n  protected lazy val pathGlobFilter = parameters.get(\"pathGlobFilter\").map(new GlobFilter(_))\n\n  protected def matchGlobPattern(file: FileStatus): Boolean = {"
  }
]