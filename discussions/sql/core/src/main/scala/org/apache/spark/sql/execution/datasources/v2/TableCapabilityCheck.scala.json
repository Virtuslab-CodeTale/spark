[
  {
    "id" : "6ba8ea04-1cc6-4a8c-9f08-1d9993a37477",
    "prId" : 25679,
    "prUrl" : "https://github.com/apache/spark/pull/25679#pullrequestreview-283625676",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2be8bc7d-280f-48ce-99ad-a062ba12308c",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Here I add the batch scan check. It's possible that a table implements `SupportsRead` without reporting `BATCH_READ` capability. For example, a steaming table which doesn't support batch scan. We must check the `BATCH_READ` capability here, instead of relying on the `.isInstaceOf[SupportsRead]` check at the planner side.",
        "createdAt" : "2019-09-04T13:45:04Z",
        "updatedAt" : "2019-09-05T05:49:53Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "a25f9df12bdc8c05552ae22700bf8ff6cb9bdb50",
    "line" : 26,
    "diffHunk" : "@@ -1,1 +35,39 @@  override def apply(plan: LogicalPlan): Unit = {\n    plan foreach {\n      case r: DataSourceV2Relation if !r.table.supports(BATCH_READ) =>\n        failAnalysis(s\"Table ${r.table.name()} does not support batch scan.\")\n"
  }
]