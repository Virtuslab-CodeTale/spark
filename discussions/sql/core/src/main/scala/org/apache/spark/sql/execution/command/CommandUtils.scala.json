[
  {
    "id" : "09abbe9c-c94a-4eb2-a422-e54f0e44c823",
    "prId" : 27471,
    "prUrl" : "https://github.com/apache/spark/pull/27471#pullrequestreview-354237948",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a351c216-1327-4d90-8a6f-8b7dbce5605f",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "same size?",
        "createdAt" : "2020-02-06T06:51:12Z",
        "updatedAt" : "2020-02-06T08:04:59Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "caccbf30-ef1e-46a5-8706-9982a6dd30e9",
        "parentId" : "a351c216-1327-4d90-8a6f-8b7dbce5605f",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "the returned `Seq`.size is equal to `paths`.size.",
        "createdAt" : "2020-02-06T07:14:30Z",
        "updatedAt" : "2020-02-06T08:04:59Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "fa60c35f64ab390126e69bf2446340dd0ae8b1fb",
    "line" : 33,
    "diffHunk" : "@@ -1,1 +154,158 @@   * @param sparkSession the [[SparkSession]]\n   * @param paths the Seq of [[Option[Path]]]s\n   * @return a Seq of same size as `paths` where i-th element is total size of `paths(i)` or 0\n   *         if `paths(i)` is None\n   */"
  },
  {
    "id" : "72070576-8507-4430-89e5-d3185c3c8625",
    "prId" : 27293,
    "prUrl" : "https://github.com/apache/spark/pull/27293#pullrequestreview-351335021",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "07a524c2-b0b2-4357-8cde-469cb6e8940d",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "ditto. `Locations` and `Sizes`.",
        "createdAt" : "2020-01-22T04:32:07Z",
        "updatedAt" : "2020-01-22T09:10:33Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "31281358-29ef-4c71-b168-ddd3876a9f18",
        "parentId" : "07a524c2-b0b2-4357-8cde-469cb6e8940d",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "Renamed to `calculateLocationSizeParallel`, done in https://github.com/apache/spark/pull/27413/commits/ac8b771c1264fc8069a61a3cdb338f4ac6c406af, please check.",
        "createdAt" : "2020-01-31T07:44:47Z",
        "updatedAt" : "2020-01-31T07:44:47Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "06b3cb8109ea3e68aae54141d1d91ebb87f479c5",
    "line" : 78,
    "diffHunk" : "@@ -1,1 +156,160 @@   * @return total size of all partitions\n   */\n  def calculateLocationsSizesParallel(\n      sparkSession: SparkSession,\n      paths: Seq[Option[Path]]): Long = {"
  },
  {
    "id" : "f1df65c3-8712-4b8e-a2d8-a46d9bf00f92",
    "prId" : 27079,
    "prUrl" : "https://github.com/apache/spark/pull/27079#pullrequestreview-338380042",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0bc19584-8f79-484b-ac89-29de5a1f8647",
        "parentId" : null,
        "authorId" : "39fe625f-8c54-48bc-ac6f-7279921adf02",
        "body" : "$durationInMs is intended to round up to integer or long.\r\ndivided by 1e6 makes it double, may keep (1000 * 1000) please",
        "createdAt" : "2020-01-02T19:17:19Z",
        "updatedAt" : "2020-01-08T01:45:47Z",
        "lastEditedBy" : "39fe625f-8c54-48bc-ac6f-7279921adf02",
        "tags" : [
        ]
      },
      {
        "id" : "04d43df8-ffe2-4176-a43a-1e7beef449f1",
        "parentId" : "0bc19584-8f79-484b-ac89-29de5a1f8647",
        "authorId" : "78cb7df6-a1c6-4bcb-99c1-16fd18fb32f7",
        "body" : "Thanks for pointing it out!",
        "createdAt" : "2020-01-03T09:29:02Z",
        "updatedAt" : "2020-01-08T01:45:47Z",
        "lastEditedBy" : "78cb7df6-a1c6-4bcb-99c1-16fd18fb32f7",
        "tags" : [
        ]
      },
      {
        "id" : "0ecf4f3d-58aa-4b98-80ea-7088fec99e61",
        "parentId" : "0bc19584-8f79-484b-ac89-29de5a1f8647",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Do you mean to change to debug level here?",
        "createdAt" : "2020-01-04T16:27:02Z",
        "updatedAt" : "2020-01-08T01:45:47Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "b40e8400-f58e-48bc-bf87-50cfcdf5f202",
        "parentId" : "0bc19584-8f79-484b-ac89-29de5a1f8647",
        "authorId" : "78cb7df6-a1c6-4bcb-99c1-16fd18fb32f7",
        "body" : "@srowen yes, it could be called in the \"else\" branch above, and one partition per log would be too much if the number of partitions is very large.\r\n```\r\npartitions.map { p =>\r\n  calculateLocationSize(sessionState, catalogTable.identifier, p.storage.locationUri)\r\n}.sum\r\n```",
        "createdAt" : "2020-01-05T04:09:46Z",
        "updatedAt" : "2020-01-08T01:45:47Z",
        "lastEditedBy" : "78cb7df6-a1c6-4bcb-99c1-16fd18fb32f7",
        "tags" : [
        ]
      }
    ],
    "commit" : "b7ac910a8431f7bad97efe5d0cbeb2b64d04e775",
    "line" : 38,
    "diffHunk" : "@@ -1,1 +130,134 @@    }.getOrElse(0L)\n    val durationInMs = (System.nanoTime() - startTime) / (1000 * 1000)\n    logDebug(s\"It took $durationInMs ms to calculate the total file size under path $locationUri.\")\n\n    size"
  },
  {
    "id" : "1d5ccc49-99c5-4fc3-9e35-12155820b6fd",
    "prId" : 27079,
    "prUrl" : "https://github.com/apache/spark/pull/27079#pullrequestreview-338000772",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0a832a2e-1573-4152-bfc1-b6d0a02bea51",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "For easy-to-see, how about merging this log into one in line 80?",
        "createdAt" : "2020-01-02T23:22:06Z",
        "updatedAt" : "2020-01-08T01:45:47Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "c9b291cf-4833-4fc1-8e98-01d4a8e422e8",
        "parentId" : "0a832a2e-1573-4152-bfc1-b6d0a02bea51",
        "authorId" : "78cb7df6-a1c6-4bcb-99c1-16fd18fb32f7",
        "body" : "ok, merged to one line log",
        "createdAt" : "2020-01-03T09:29:38Z",
        "updatedAt" : "2020-01-08T01:45:47Z",
        "lastEditedBy" : "78cb7df6-a1c6-4bcb-99c1-16fd18fb32f7",
        "tags" : [
        ]
      }
    ],
    "commit" : "b7ac910a8431f7bad97efe5d0cbeb2b64d04e775",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +62,66 @@      // Calculate table size as a sum of the visible partitions. See SPARK-21079\n      val partitions = sessionState.catalog.listPartitions(catalogTable.identifier)\n      logInfo(s\"Starting to calculate sizes for ${partitions.length} partitions.\")\n      if (spark.sessionState.conf.parallelFileListingInStatsComputation) {\n        val paths = partitions.map(x => new Path(x.storage.locationUri.get))"
  },
  {
    "id" : "952c2d47-a469-4737-9e6e-58cff85f7be0",
    "prId" : 27079,
    "prUrl" : "https://github.com/apache/spark/pull/27079#pullrequestreview-338602410",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2c504d83-3931-4ce1-bf35-6b1fbce00597",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Because the two branches above differ in several ways, it might be cleaner to just put two different log statements in the branches above instead of this",
        "createdAt" : "2020-01-04T16:27:27Z",
        "updatedAt" : "2020-01-08T01:45:47Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "87e0ea54-b3be-4d55-9795-91e7e710656f",
        "parentId" : "2c504d83-3931-4ce1-bf35-6b1fbce00597",
        "authorId" : "78cb7df6-a1c6-4bcb-99c1-16fd18fb32f7",
        "body" : "If I put two different logs in the branches, I would need to have two `totalSize` values in two branches and return them after the logs. Besides, the majority of two logs would still be the same (except the partition info). So the code may look redundant that way... what do you think?",
        "createdAt" : "2020-01-05T04:25:21Z",
        "updatedAt" : "2020-01-08T01:45:47Z",
        "lastEditedBy" : "78cb7df6-a1c6-4bcb-99c1-16fd18fb32f7",
        "tags" : [
        ]
      },
      {
        "id" : "c985de23-84b6-4cf7-819d-7b486ac01b56",
        "parentId" : "2c504d83-3931-4ce1-bf35-6b1fbce00597",
        "authorId" : "78cb7df6-a1c6-4bcb-99c1-16fd18fb32f7",
        "body" : "@maropu @srowen Maybe I could change back to the initial version, which prints a log with partition info in the branch for partitioned table?\r\n```\r\nlogInfo(s\"Starting to calculate sizes for ${partitions.length} partitions.\")\r\n```\r\nIn this way we keep the \"partitioned table\" logic only in that branch. Then the final log applies to both non-partitioned and partitioned tables.\r\n```\r\nlogInfo(s\"It took ${(System.nanoTime() - startTime) / (1000 * 1000)} ms to calculate\" +\r\n        s\" the total size for table ${catalogTable.identifier}.\")\r\n```",
        "createdAt" : "2020-01-05T04:39:55Z",
        "updatedAt" : "2020-01-08T01:45:47Z",
        "lastEditedBy" : "78cb7df6-a1c6-4bcb-99c1-16fd18fb32f7",
        "tags" : [
        ]
      },
      {
        "id" : "078f9cb5-32c2-454d-ae4a-08df4565a02d",
        "parentId" : "2c504d83-3931-4ce1-bf35-6b1fbce00597",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "I don't feel strongly about it, just sounded simpler to me to keep the current structure",
        "createdAt" : "2020-01-05T14:41:11Z",
        "updatedAt" : "2020-01-08T01:45:47Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "65b775ce-5965-40ca-9f6e-6788d464f46e",
        "parentId" : "2c504d83-3931-4ce1-bf35-6b1fbce00597",
        "authorId" : "78cb7df6-a1c6-4bcb-99c1-16fd18fb32f7",
        "body" : "@srowen @maropu I did the changes as described above, please see if it's better",
        "createdAt" : "2020-01-06T11:49:09Z",
        "updatedAt" : "2020-01-08T01:45:47Z",
        "lastEditedBy" : "78cb7df6-a1c6-4bcb-99c1-16fd18fb32f7",
        "tags" : [
        ]
      }
    ],
    "commit" : "b7ac910a8431f7bad97efe5d0cbeb2b64d04e775",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +78,82 @@      }\n    }\n    logInfo(s\"It took ${(System.nanoTime() - startTime) / (1000 * 1000)} ms to calculate\" +\n      s\" the total size for table ${catalogTable.identifier}.\")\n    totalSize"
  },
  {
    "id" : "d92d2ddb-0355-46e2-a400-689764a35d5f",
    "prId" : 26905,
    "prUrl" : "https://github.com/apache/spark/pull/26905#pullrequestreview-333373788",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4104e901-cbfd-43de-a539-ae190b75c173",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "why change this?",
        "createdAt" : "2019-12-17T14:56:55Z",
        "updatedAt" : "2019-12-23T05:59:38Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "23a7363e-1208-48ee-a784-41c3c6eb67b4",
        "parentId" : "4104e901-cbfd-43de-a539-ae190b75c173",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "Literal(percentiles) creates ArrayType(_, containsNull = true)",
        "createdAt" : "2019-12-17T15:44:51Z",
        "updatedAt" : "2019-12-23T05:59:38Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "7a3014fb90958c82cf6bbb700f5012d70cba833f",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +216,220 @@        val aggFunc =\n          new ApproximatePercentile(attr,\n            Literal(new GenericArrayData(percentiles), ArrayType(DoubleType, false)),\n            Literal(conf.percentileAccuracy))\n        val expr = aggFunc.toAggregateExpression()"
  },
  {
    "id" : "9ce0bede-a798-4d89-88a2-503e751a83db",
    "prId" : 26016,
    "prUrl" : "https://github.com/apache/spark/pull/26016#pullrequestreview-324908926",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e300d6c1-d235-4d26-9202-79c85066ab34",
        "parentId" : null,
        "authorId" : "934976a5-cca7-44a7-8578-4c05ccf3cb53",
        "body" : "So if the user leaves spark.sql.statistics.deserFactor.calc.enabled set to true all the time, will Spark recalculate the deserFactor each time the table's data is updated (e.g., InsertIntoHiveTable)?",
        "createdAt" : "2019-10-07T23:40:35Z",
        "updatedAt" : "2020-03-17T14:17:29Z",
        "lastEditedBy" : "934976a5-cca7-44a7-8578-4c05ccf3cb53",
        "tags" : [
        ]
      },
      {
        "id" : "6933bb52-6c85-4906-9069-1839d88335ae",
        "parentId" : "e300d6c1-d235-4d26-9202-79c85066ab34",
        "authorId" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "body" : "When \"spark.sql.statistics.size.autoUpdate.enabled\" is enabled then yes it does. ",
        "createdAt" : "2019-11-30T21:17:58Z",
        "updatedAt" : "2020-03-17T14:17:29Z",
        "lastEditedBy" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "tags" : [
        ]
      }
    ],
    "commit" : "42933f2e3f6eb699745e399a1fc9f05288603d79",
    "line" : 43,
    "diffHunk" : "@@ -1,1 +64,68 @@      val newTable = catalog.getTableMetadata(table.identifier)\n      val oldDeserFactor = newTable.stats.flatMap(_.deserFactor)\n      val newSizeWithDeserFactor = CommandUtils.calculateTotalSize(sparkSession, newTable)\n      val newStats = CatalogStatistics(\n        sizeInBytes = newSizeWithDeserFactor.sizeInBytes,"
  },
  {
    "id" : "d6784124-317b-4b4a-9dcf-eb5c0ab4841c",
    "prId" : 26016,
    "prUrl" : "https://github.com/apache/spark/pull/26016#pullrequestreview-367987417",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "83d9ff46-3648-4591-8bb3-2dc34a44799c",
        "parentId" : null,
        "authorId" : "2e5e2a86-4faf-484a-9251-577e584e1564",
        "body" : "Are there any plan to support datasource table to calculate this stats on demand?  Maybe we can also add an option to enable this for on demand calculation?",
        "createdAt" : "2020-02-19T04:29:41Z",
        "updatedAt" : "2020-03-17T14:17:29Z",
        "lastEditedBy" : "2e5e2a86-4faf-484a-9251-577e584e1564",
        "tags" : [
        ]
      },
      {
        "id" : "228d9875-9fee-4b2c-8714-0ab029e3d31d",
        "parentId" : "83d9ff46-3648-4591-8bb3-2dc34a44799c",
        "authorId" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "body" : "I have not thought about it yet but I would like to keep the scope of this PR minimal. When this is in we can come back to it.",
        "createdAt" : "2020-03-03T13:44:27Z",
        "updatedAt" : "2020-03-17T14:17:29Z",
        "lastEditedBy" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "tags" : [
        ]
      }
    ],
    "commit" : "42933f2e3f6eb699745e399a1fc9f05288603d79",
    "line" : 101,
    "diffHunk" : "@@ -1,1 +120,124 @@    assert(fStatus.isFile)\n    val factor = if (calcDeserFact) {\n      val isOrc = serde.contains(orcSerDeCanonicalClass) || fStatus.getPath.getName.endsWith(\".orc\")\n      val rawSize = if (isOrc) OrcUtils.rawSize(hadoopConf, fStatus.getPath) else None\n      rawSize.map { rawSize =>"
  },
  {
    "id" : "5bbc3f95-c355-4135-a54b-8cb08c193516",
    "prId" : 24725,
    "prUrl" : "https://github.com/apache/spark/pull/24725#pullrequestreview-242779250",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "18f4d78d-0187-490f-8f17-9af2d7f9d129",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Do we need to touch this file? This looks unnecessary refactoring.",
        "createdAt" : "2019-05-28T16:08:40Z",
        "updatedAt" : "2019-05-28T16:08:40Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "8c88de9e-41fc-4515-86d7-6d0f06df4929",
        "parentId" : "18f4d78d-0187-490f-8f17-9af2d7f9d129",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Ur, is this used somewhere else?",
        "createdAt" : "2019-05-28T16:16:33Z",
        "updatedAt" : "2019-05-28T16:16:34Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "947b3816-345a-4f5f-941d-5db7f7f3279f",
        "parentId" : "18f4d78d-0187-490f-8f17-9af2d7f9d129",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "Yes. change `DataSourceUtils.isDataPath(path)` to `DataSourceUtils.isDataPath(status.getPath)`.",
        "createdAt" : "2019-05-28T16:16:43Z",
        "updatedAt" : "2019-05-28T16:16:43Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      },
      {
        "id" : "3f60367b-2adb-4ddc-b8f1-e0e79c0be965",
        "parentId" : "18f4d78d-0187-490f-8f17-9af2d7f9d129",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Got it. Thanks",
        "createdAt" : "2019-05-28T16:18:42Z",
        "updatedAt" : "2019-05-28T16:18:42Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "fb44905b6eff346919965065bc39da35fa247ef7",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +344,348 @@  private def isDataPath(path: Path, stagingDir: String): Boolean = {\n    !path.getName.startsWith(stagingDir) && DataSourceUtils.isDataPath(path)\n  }\n}"
  },
  {
    "id" : "79d7c05d-4b22-4df2-a80f-d5c85101893c",
    "prId" : 24725,
    "prUrl" : "https://github.com/apache/spark/pull/24725#pullrequestreview-246935841",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4292d649-d047-4d72-bc05-ca51c043ed14",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "I think we might need a change in the impl of DataSourceUtils.isDataPath(path). See what we did in InMemoryFileIndex:\r\n\r\nhttps://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InMemoryFileIndex.scala#L363-L369\r\n\r\n",
        "createdAt" : "2019-06-07T06:43:52Z",
        "updatedAt" : "2019-06-07T06:43:52Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      }
    ],
    "commit" : "fb44905b6eff346919965065bc39da35fa247ef7",
    "line" : 27,
    "diffHunk" : "@@ -1,1 +343,347 @@\n  private def isDataPath(path: Path, stagingDir: String): Boolean = {\n    !path.getName.startsWith(stagingDir) && DataSourceUtils.isDataPath(path)\n  }\n}"
  }
]