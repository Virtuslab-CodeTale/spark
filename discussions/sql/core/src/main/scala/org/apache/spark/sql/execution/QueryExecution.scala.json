[
  {
    "id" : "72c8407f-0924-4fea-a47b-615499470e3e",
    "prId" : 32704,
    "prUrl" : "https://github.com/apache/spark/pull/32704#pullrequestreview-671905420",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d9a25656-4ed0-4277-a442-f0fe30df97e4",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "cc @cloud-fan @maryannxue FYI",
        "createdAt" : "2021-05-31T02:18:16Z",
        "updatedAt" : "2021-05-31T02:18:17Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "b78a9c96d342f4f2c38929ce2dc4860a98a7f984",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +259,263 @@      // if the stats doesn't exist in the statsCache and update the statsCache corresponding\n      // to the node.\n      optimizedPlan.collectWithSubqueries {\n        case plan => plan.stats\n      }"
  },
  {
    "id" : "630edcd0-0674-4ce8-9ea3-02bbe26a2225",
    "prId" : 32704,
    "prUrl" : "https://github.com/apache/spark/pull/32704#pullrequestreview-672075370",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c2412e13-d66d-4a22-98a2-0f1d3935791a",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I see, so this basically triggers stats calculation for every node in the plan tree.",
        "createdAt" : "2021-05-31T07:36:29Z",
        "updatedAt" : "2021-05-31T07:36:29Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "aa86d53b-13f7-4691-8978-1f3720edf7f8",
        "parentId" : "c2412e13-d66d-4a22-98a2-0f1d3935791a",
        "authorId" : "d0d56018-475d-4da3-93a6-5e843402b7f2",
        "body" : "Yes, right.",
        "createdAt" : "2021-05-31T07:39:15Z",
        "updatedAt" : "2021-05-31T07:39:15Z",
        "lastEditedBy" : "d0d56018-475d-4da3-93a6-5e843402b7f2",
        "tags" : [
        ]
      },
      {
        "id" : "e07ca170-7368-4fd1-bc51-fa8f978397da",
        "parentId" : "c2412e13-d66d-4a22-98a2-0f1d3935791a",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Can we add a comment to explain why we need to do it? It's not that obvious to many reviewers.",
        "createdAt" : "2021-05-31T07:53:03Z",
        "updatedAt" : "2021-05-31T07:53:03Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "ec62ba82-b8cf-45e9-a96f-0e67ec22357a",
        "parentId" : "c2412e13-d66d-4a22-98a2-0f1d3935791a",
        "authorId" : "d0d56018-475d-4da3-93a6-5e843402b7f2",
        "body" : "Yes, I added a comment.",
        "createdAt" : "2021-05-31T08:23:43Z",
        "updatedAt" : "2021-05-31T08:23:43Z",
        "lastEditedBy" : "d0d56018-475d-4da3-93a6-5e843402b7f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "b78a9c96d342f4f2c38929ce2dc4860a98a7f984",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +260,264 @@      // to the node.\n      optimizedPlan.collectWithSubqueries {\n        case plan => plan.stats\n      }\n    } catch {"
  },
  {
    "id" : "b804e70c-5dcd-458b-8ce9-bd77647a39c4",
    "prId" : 31968,
    "prUrl" : "https://github.com/apache/spark/pull/31968#pullrequestreview-636445793",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "875332b8-e2b4-4717-aec0-04c575fbcce9",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Let's try our best to avoid mutable states. It will make the system harder to reason about.",
        "createdAt" : "2021-04-15T09:03:34Z",
        "updatedAt" : "2021-04-15T09:03:34Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "50b7764bef8c48466b426cae1e57534e4fef143c",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +59,63 @@  val id: Long = QueryExecution.nextExecutionId\n\n  private var _commandExecutionIdGenerated = false\n\n  def commandExecutionIdGenerated: Boolean = _commandExecutionIdGenerated"
  },
  {
    "id" : "1e92f2d8-31e5-40ff-8157-d8af95687fb3",
    "prId" : 30373,
    "prUrl" : "https://github.com/apache/spark/pull/30373#pullrequestreview-532581467",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "581aeb85-d0e5-4601-98af-205b24734157",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Could you leave some comments here about why we need to put this rule after `EnsureRequirements`?",
        "createdAt" : "2020-11-14T13:43:57Z",
        "updatedAt" : "2020-11-19T04:50:29Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "d1753647-9b78-4e57-81b0-9f8f73d1d1c3",
        "parentId" : "581aeb85-d0e5-4601-98af-205b24734157",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "+1 for @maropu 's comment.",
        "createdAt" : "2020-11-17T16:55:38Z",
        "updatedAt" : "2020-11-19T04:50:29Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "4e684df701c9421eba105d469b0338a5320ce42c",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +347,351 @@      // `RemoveRedundantSorts` needs to be added before `EnsureRequirements` to guarantee the same\n      // number of partitions when instantiating PartitioningCollection.\n      RemoveRedundantSorts,\n      DisableUnnecessaryBucketedScan,\n      ApplyColumnarRulesAndInsertTransitions(sparkSession.sessionState.columnarRules),"
  },
  {
    "id" : "25aea5d2-8438-44b0-a288-9b5c517c17c5",
    "prId" : 30373,
    "prUrl" : "https://github.com/apache/spark/pull/30373#pullrequestreview-537082050",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a33c1a71-7497-4715-9b56-4cec5081fc1d",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "before -> after ?",
        "createdAt" : "2020-11-21T17:30:17Z",
        "updatedAt" : "2020-11-21T17:30:17Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "998814e6-9e7f-4ee0-8841-c25bc6629544",
        "parentId" : "a33c1a71-7497-4715-9b56-4cec5081fc1d",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Ah, I missed it. @allisonwang-db could you fix it?",
        "createdAt" : "2020-11-22T00:17:32Z",
        "updatedAt" : "2020-11-22T00:17:32Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "3fc69546-17f2-438d-aa76-9717ebd4a3db",
        "parentId" : "a33c1a71-7497-4715-9b56-4cec5081fc1d",
        "authorId" : "b3ba992a-312c-46eb-b3c3-8d861d15ac40",
        "body" : "Thanks for catching it! Will create a fix.",
        "createdAt" : "2020-11-24T05:09:28Z",
        "updatedAt" : "2020-11-24T05:09:28Z",
        "lastEditedBy" : "b3ba992a-312c-46eb-b3c3-8d861d15ac40",
        "tags" : [
        ]
      }
    ],
    "commit" : "4e684df701c9421eba105d469b0338a5320ce42c",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +345,349 @@      RemoveRedundantProjects,\n      EnsureRequirements,\n      // `RemoveRedundantSorts` needs to be added before `EnsureRequirements` to guarantee the same\n      // number of partitions when instantiating PartitioningCollection.\n      RemoveRedundantSorts,"
  },
  {
    "id" : "85b16768-9e04-473c-9a71-e2466086c0e0",
    "prId" : 29544,
    "prUrl" : "https://github.com/apache/spark/pull/29544#pullrequestreview-489628359",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4f1c5ff4-0672-4911-92a7-3b6a336ec74b",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "We also run physical rules in `AdaptiveSparkPlanExec`, shall we log plan change there as well?",
        "createdAt" : "2020-09-16T09:02:27Z",
        "updatedAt" : "2020-09-16T09:02:27Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "dda7306f-2bff-4778-ab8f-ca0cd3f28b54",
        "parentId" : "4f1c5ff4-0672-4911-92a7-3b6a336ec74b",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Yea, sure. I'll make a PR later.",
        "createdAt" : "2020-09-16T10:05:55Z",
        "updatedAt" : "2020-09-16T10:05:55Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "e7ca5f82-f439-4570-99e9-e39e956d3413",
        "parentId" : "4f1c5ff4-0672-4911-92a7-3b6a336ec74b",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "https://github.com/apache/spark/pull/29774",
        "createdAt" : "2020-09-16T13:32:10Z",
        "updatedAt" : "2020-09-16T13:32:11Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "40c99a21ae19f4c1e50029e4b10c768339336cc6",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +360,364 @@      preparations: Seq[Rule[SparkPlan]],\n      plan: SparkPlan): SparkPlan = {\n    val planChangeLogger = new PlanChangeLogger[SparkPlan]()\n    val preparedPlan = preparations.foldLeft(plan) { case (sp, rule) =>\n      val result = rule.apply(sp)"
  },
  {
    "id" : "13302f03-c57d-40a6-a84f-585199ea0adc",
    "prId" : 29372,
    "prUrl" : "https://github.com/apache/spark/pull/29372#pullrequestreview-466398683",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "503dcb9c-be27-40ab-a15f-eb484f04d341",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "What is the use case to add this ID?",
        "createdAt" : "2020-08-12T02:30:07Z",
        "updatedAt" : "2020-08-12T02:30:07Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "90a83a94-1b8f-498e-9858-bb55d80847ac",
        "parentId" : "503dcb9c-be27-40ab-a15f-eb484f04d341",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "We have a listener which parses the logical & physical plan from the query execution and reflects the changes to Apache Atlas. While it works well in general, there's a pain point on the listener due to the fact Spark can callback the listener multiple times on the same query execution, whereas the instance of query execution doesn't have the unique ID to distinct. That's already I wrote in PR description as well as JIRA description.",
        "createdAt" : "2020-08-13T02:11:56Z",
        "updatedAt" : "2020-08-13T02:11:57Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "3f1bea27a26f33e20f332e25011da3284aec4986",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +56,60 @@    val tracker: QueryPlanningTracker = new QueryPlanningTracker) extends Logging {\n\n  val id: Long = QueryExecution.nextExecutionId\n\n  // TODO: Move the planner an optimizer into here from SessionState."
  },
  {
    "id" : "ddacf79f-8aad-4586-bb20-dedec2e5be7c",
    "prId" : 28885,
    "prUrl" : "https://github.com/apache/spark/pull/28885#pullrequestreview-438797082",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "92a5d66b-c2ec-40b4-b25c-1d7c0b7cb951",
        "parentId" : null,
        "authorId" : "def2dd9f-ce27-41ce-8156-f8225ddc68a9",
        "body" : "Why do we need this boolean parameter here? What will happen if we just always run the `WholePlanReuse` rule? ",
        "createdAt" : "2020-06-27T11:16:00Z",
        "updatedAt" : "2021-03-19T12:14:59Z",
        "lastEditedBy" : "def2dd9f-ce27-41ce-8156-f8225ddc68a9",
        "tags" : [
        ]
      },
      {
        "id" : "5205cf1e-0e2b-4625-9158-ec5c591de89e",
        "parentId" : "92a5d66b-c2ec-40b4-b25c-1d7c0b7cb951",
        "authorId" : "3d4870da-39a8-4406-b00d-131930d14cd8",
        "body" : "Sorry, I don't get this, why would we run this rule multiple times? This new rule traverses through the whole plan, does it make any sense to run in on subqueries and then run it on a main query which also incorporates traversing on subqueries?",
        "createdAt" : "2020-06-28T10:27:50Z",
        "updatedAt" : "2021-03-19T12:14:59Z",
        "lastEditedBy" : "3d4870da-39a8-4406-b00d-131930d14cd8",
        "tags" : [
        ]
      },
      {
        "id" : "b80f27b9-f0ec-4765-a1d6-310d45470985",
        "parentId" : "92a5d66b-c2ec-40b4-b25c-1d7c0b7cb951",
        "authorId" : "def2dd9f-ce27-41ce-8156-f8225ddc68a9",
        "body" : "Yes, from the performance perspective makes sense to exclude them. I sort of don't like having another parameter and select rules based on that, so was thinking if it's not a huge performance difference let's not do it, but it can be expensive with canonicalization, etc. I guess we don't have any other way of detecting if a physical plan is a subquery locally inside the new rule, so it's fine to do it like this, maybe we need a more explicit name for `QueryExecution.prepareExecutedPlan` in the future, like `PrepareSubqueryForExecution` to make it more clear that this method is only called for subqueries. ",
        "createdAt" : "2020-06-28T17:04:32Z",
        "updatedAt" : "2021-03-19T12:14:59Z",
        "lastEditedBy" : "def2dd9f-ce27-41ce-8156-f8225ddc68a9",
        "tags" : [
        ]
      }
    ],
    "commit" : "7187ebd2e053570d92017100dba4a0738fa2f014",
    "line" : 25,
    "diffHunk" : "@@ -1,1 +402,406 @@      sparkSession: SparkSession,\n      adaptiveExecutionRule: Option[InsertAdaptiveSparkPlan] = None,\n      subquery: Boolean): Seq[Rule[SparkPlan]] = {\n    // `AdaptiveSparkPlanExec` is a leaf node. If inserted, all the following rules will be no-op\n    // as the original plan is hidden behind `AdaptiveSparkPlanExec`."
  },
  {
    "id" : "df634b9b-8bcc-4aee-a68d-c7f0cc4ddae9",
    "prId" : 28493,
    "prUrl" : "https://github.com/apache/spark/pull/28493#pullrequestreview-417712197",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3ea49810-e84e-400e-adf9-63540d7c0e2e",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Could you describe `path` too.",
        "createdAt" : "2020-05-25T13:39:50Z",
        "updatedAt" : "2020-05-25T17:51:34Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "1fea0e2f1c7909083973027acd46b1e37eb41d21",
    "line" : 149,
    "diffHunk" : "@@ -1,1 +290,294 @@    /**\n     * Dumps debug information about query execution into the specified file.\n     *\n     * @param path path of the file the debug info is written to.\n     * @param maxFields maximum number of fields converted to string representation."
  },
  {
    "id" : "a4af5fb1-5274-4057-8279-410086e4f2bf",
    "prId" : 26778,
    "prUrl" : "https://github.com/apache/spark/pull/26778#pullrequestreview-328129357",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "60af1b9e-dadf-4849-8fa6-31b35bcffe00",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "can we just move them into the try-catch?",
        "createdAt" : "2019-12-06T10:53:12Z",
        "updatedAt" : "2019-12-06T12:02:07Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "44f09e0f37174fd3a48561794e5d8d006db3586d",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +146,150 @@      QueryPlan.append(analyzed, append, verbose, addSuffix, maxFields)\n      append(\"\\n== Optimized Logical Plan ==\\n\")\n      QueryPlan.append(optimizedPlan, append, verbose, addSuffix, maxFields)\n      append(\"\\n== Physical Plan ==\\n\")\n      QueryPlan.append(executedPlan, append, verbose, addSuffix, maxFields)"
  },
  {
    "id" : "767d991e-6ad2-4a71-87a4-f314e04c6b63",
    "prId" : 26778,
    "prUrl" : "https://github.com/apache/spark/pull/26778#pullrequestreview-328161284",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "34cd1412-8f15-4e71-938e-9e115a7dd80e",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Statements for if and else seems to be swapped.",
        "createdAt" : "2019-12-06T11:44:04Z",
        "updatedAt" : "2019-12-06T12:02:07Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "63fbd4ef-2ffe-44d9-83e6-bd6ffdda3f39",
        "parentId" : "34cd1412-8f15-4e71-938e-9e115a7dd80e",
        "authorId" : "75554730-8183-463c-bf1d-7cd5b4e9f22b",
        "body" : "If we move this to try-catch (as @cloud-fan suggested), we don't need if-else.",
        "createdAt" : "2019-12-06T11:58:37Z",
        "updatedAt" : "2019-12-06T12:02:07Z",
        "lastEditedBy" : "75554730-8183-463c-bf1d-7cd5b4e9f22b",
        "tags" : [
        ]
      },
      {
        "id" : "8589b521-5b5a-4f5d-bfee-80080be2a703",
        "parentId" : "34cd1412-8f15-4e71-938e-9e115a7dd80e",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Sure.",
        "createdAt" : "2019-12-06T12:01:19Z",
        "updatedAt" : "2019-12-06T12:02:07Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "44f09e0f37174fd3a48561794e5d8d006db3586d",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +144,148 @@      )\n      append(\"\\n\")\n      QueryPlan.append(analyzed, append, verbose, addSuffix, maxFields)\n      append(\"\\n== Optimized Logical Plan ==\\n\")\n      QueryPlan.append(optimizedPlan, append, verbose, addSuffix, maxFields)"
  },
  {
    "id" : "3915ea38-6ac5-45d8-92f0-32901c6aa3f9",
    "prId" : 26734,
    "prUrl" : "https://github.com/apache/spark/pull/26734#pullrequestreview-325886338",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d07b224f-19f8-4996-9592-ba89f8e6c5a5",
        "parentId" : null,
        "authorId" : "80d631a6-73e8-46a6-a01b-a80d1f1cc6cc",
        "body" : "Can you explain why it is printing the exception twice?",
        "createdAt" : "2019-12-02T21:16:55Z",
        "updatedAt" : "2019-12-02T21:17:42Z",
        "lastEditedBy" : "80d631a6-73e8-46a6-a01b-a80d1f1cc6cc",
        "tags" : [
        ]
      },
      {
        "id" : "5cd90fda-d85a-47f6-add7-44fa117510c5",
        "parentId" : "d07b224f-19f8-4996-9592-ba89f8e6c5a5",
        "authorId" : "75554730-8183-463c-bf1d-7cd5b4e9f22b",
        "body" : "First time it is printed when AnalysisException come while trying to find output schema\r\nhttps://github.com/apache/spark/blob/4021354b73dd86ee765f50ff90ab777edfc21bdb/sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala#L137-L142\r\nSecond time while generating a tree string for analyzed plan\r\nhttps://github.com/apache/spark/blob/4021354b73dd86ee765f50ff90ab777edfc21bdb/sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala#L145",
        "createdAt" : "2019-12-03T04:09:12Z",
        "updatedAt" : "2019-12-03T04:09:13Z",
        "lastEditedBy" : "75554730-8183-463c-bf1d-7cd5b4e9f22b",
        "tags" : [
        ]
      }
    ],
    "commit" : "f46e08defb724b3df1ea09c19d957dd9195645f0",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +156,160 @@    QueryPlan.append(logical, append, verbose, addSuffix, maxFields)\n    append(\"\\n== Analyzed Logical Plan ==\\n\")\n    try {\n      append(\n        truncatedString("
  },
  {
    "id" : "b77d9c8f-cfe7-4416-b29a-25f624b133ef",
    "prId" : 26734,
    "prUrl" : "https://github.com/apache/spark/pull/26734#pullrequestreview-326612490",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fc06b57a-6ab1-4da5-af3e-fc174f9e96f2",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "shall we put something to indicate that the output is unknown because plan is not resolved?",
        "createdAt" : "2019-12-03T18:17:04Z",
        "updatedAt" : "2019-12-03T18:17:05Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "cccf2619-b98a-444b-ae1e-0e09a11fb29b",
        "parentId" : "fc06b57a-6ab1-4da5-af3e-fc174f9e96f2",
        "authorId" : "75554730-8183-463c-bf1d-7cd5b4e9f22b",
        "body" : "Are you suggesting some comments here?",
        "createdAt" : "2019-12-03T18:33:51Z",
        "updatedAt" : "2019-12-03T18:34:08Z",
        "lastEditedBy" : "75554730-8183-463c-bf1d-7cd5b4e9f22b",
        "tags" : [
        ]
      },
      {
        "id" : "fa4f5637-f7fa-44d4-a344-240c670279e2",
        "parentId" : "fc06b57a-6ab1-4da5-af3e-fc174f9e96f2",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "OK seems fine to have no output for this case.",
        "createdAt" : "2019-12-04T05:41:38Z",
        "updatedAt" : "2019-12-04T05:41:39Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "f46e08defb724b3df1ea09c19d957dd9195645f0",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +163,167 @@      append(\"\\n\")\n    } catch {\n      case _: AnalysisException =>\n    }\n    QueryPlan.append(analyzed, append, verbose, addSuffix, maxFields)"
  },
  {
    "id" : "c5f6c973-9b7a-471d-87ae-ac426e5c8a59",
    "prId" : 26127,
    "prUrl" : "https://github.com/apache/spark/pull/26127#pullrequestreview-337068642",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c8712bb5-c363-47ca-8747-23c410f28390",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "Based on this PR description, `observedMetrics ` sounds like a public API for end users to use. \r\n\r\nHowever, it conflicts with what we commented above [in toRDD]. QueryExecution is not a public class and we discourage users from calling these internal functions. ",
        "createdAt" : "2019-12-30T00:19:09Z",
        "updatedAt" : "2019-12-30T00:21:46Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "116bfbe4-1f5e-4780-ac23-2096ddddda57",
        "parentId" : "c8712bb5-c363-47ca-8747-23c410f28390",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I think `queryExecution` is exposed in `Dataset` as an unstable API. I think the comments in the class implies that as well:\r\n\r\n>  \\* The primary workflow for executing relational queries using Spark.  Designed to allow easy\r\n>  \\* access to the intermediate phases of query execution for developers.\r\n>  \\*\r\n>  \\* While this is not a public class, we should avoid changing the function names for the sake of\r\n>  \\* changing them, because a lot of developers use the feature for debugging.\r\n\r\nI agree that using methods in this class here is discouraged though. Maybe we had to mark `Dataset.observe` as an unstable API or developer API too for now if it is difficult to avoid adding and using an API here.",
        "createdAt" : "2019-12-30T02:19:06Z",
        "updatedAt" : "2019-12-30T02:19:06Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "cf3807a3-48f8-4207-b490-f5c076b45045",
        "parentId" : "c8712bb5-c363-47ca-8747-23c410f28390",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Technically, QueryExecution should be public class, as two methods in QueryExecutionListener are marked as `@DeveloperApi` and the signature of methods contain QueryExecution. If we worry about restriction for modifying QueryException due to being public class, we may need to have other class only for reporting to QueryExecutionListener.",
        "createdAt" : "2019-12-30T02:19:42Z",
        "updatedAt" : "2019-12-30T02:19:42Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "30fb501f-859d-46f8-83d1-df92359cff17",
        "parentId" : "c8712bb5-c363-47ca-8747-23c410f28390",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "`@DeveloperApi` implies:\r\n\r\n> A lower-level, unstable API intended for developers.\r\n\r\nI agree that It might have to be considered a public class but unstable. I am sure most of methods there are discouraged even for developers to use without knowing exactly what it does.",
        "createdAt" : "2019-12-30T02:32:52Z",
        "updatedAt" : "2019-12-30T02:39:35Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "4cd069b1-b6f4-4a4b-bd80-823af6549ee1",
        "parentId" : "c8712bb5-c363-47ca-8747-23c410f28390",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Ah my bad. Thanks for pointing it out. \r\n\r\nThough I still feel listener stuff is more alike public API, as it can be used more than debug purpose. QueryExecutionListener and StreamingQueryListener take different approach, QueryExecutionListener directly exposes low-level internal instance and let end users use with caution, whereas StreamingQueryListener has individual classes which only contain information for report only, not exposing internal one.\r\n\r\nIMHO, I feel StreamingQueryListener is the right way to go, but QueryExecutionListener has been served long ago, so no strong opinion.",
        "createdAt" : "2019-12-30T02:41:55Z",
        "updatedAt" : "2019-12-30T02:41:55Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "8c2f817e-60f5-48b0-af8f-bb3f227f2d2c",
        "parentId" : "c8712bb5-c363-47ca-8747-23c410f28390",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Yeah, `StreamingQueryProgress.observedMetrics` with `StreamingQueryListener` seems fine. The problem here looks only `QueryExecution.observedMetrics` with `QueryExecutionListener`, which looks having a contradiction about its stability. It seems it has to be fixed by either avoid adding it to `QueryExecution` or explicitly marking `Dataset.observe` as an unstable API (or developer API).",
        "createdAt" : "2019-12-30T03:02:02Z",
        "updatedAt" : "2019-12-30T03:02:03Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "f6f74ecc-6fa3-4cf7-adb3-792d8507d263",
        "parentId" : "c8712bb5-c363-47ca-8747-23c410f28390",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "+1. We should make the batch listener not rely on `QueryExecution`.",
        "createdAt" : "2019-12-30T03:24:51Z",
        "updatedAt" : "2019-12-30T03:24:52Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "d1899d0c-eb2e-430a-997b-5d70352adfc2",
        "parentId" : "c8712bb5-c363-47ca-8747-23c410f28390",
        "authorId" : "80d631a6-73e8-46a6-a01b-a80d1f1cc6cc",
        "body" : "I am not sure what the issue is?\r\n\r\nBy definition anything in `QueryExecution` is internal, unstable, API. The reason that I added it here is that is a good narrow waist to add this, you want to collect the metrics for the entire query. It is public because most other methods in this class are public; this allows for some clever integrations (for the more adventurous developer) and makes debugging easier.\r\n\r\nThe batch listener is marked as experimental. A developer should be warned when (s)he uses this API for anything (including using it to collect observable metrics). I am not sure how realistic stabilizing the batch listener API is if you include `QueryExecution` (or a stabilized version of it). We could expose a stable callback, e,g. `onObservedMetrics(...)`, for observed metrics. On the other hand is kind of annoying that we can't expose this through the Dataframe itself, and that is because when we execute the Dataframe we often use a different one under the hood.\r\n",
        "createdAt" : "2019-12-30T10:23:38Z",
        "updatedAt" : "2019-12-30T10:23:38Z",
        "lastEditedBy" : "80d631a6-73e8-46a6-a01b-a80d1f1cc6cc",
        "tags" : [
        ]
      },
      {
        "id" : "9db6a9cd-babf-4719-9f50-67d0059c085e",
        "parentId" : "c8712bb5-c363-47ca-8747-23c410f28390",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "The issue is not about this PR itself, but about the bath query listener relying on `QueryExecution` which is not going to be public. I think the streaming query listener is better. It's still unstable now but we are going to make it stable in the future.",
        "createdAt" : "2019-12-30T10:45:07Z",
        "updatedAt" : "2019-12-30T10:45:08Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "5e3dbbe8-7212-4f9c-8e58-b4731f6b91fd",
        "parentId" : "c8712bb5-c363-47ca-8747-23c410f28390",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "The batch listener is no longer marked as experimental in 3.0, as there have been some of slight modifications (enough for `@Evolving`) but it doesn't change majorly during 4 years of life - see #25558. If we feel concerned it could be rolled back to Experimental/Unstable, though I know there're couple of projects in Spark ecosystem already leveraging it, and it represents that the possibility is not restricted to the debug purpose.\r\n\r\nThe major feature of batch listener is that (unlikely of streaming query listener which summarizes information and stores into a new data structure) it exposes various plans for both logical/physical, and I don't imagine these plans will be used for execution-purposes. Mostly read-only. It may not unrealistic if we could provide these plans for read-only (cloned, can't execute), but yeah, it may not be easy as it seems (not enough familiar with it).",
        "createdAt" : "2019-12-30T10:55:36Z",
        "updatedAt" : "2019-12-30T10:57:19Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "a07474c6c6131de1ae487ad96c91deb80cce2f1f",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +108,112 @@\n  /** Get the metrics observed during the execution of the query plan. */\n  def observedMetrics: Map[String, Row] = CollectMetricsExec.collect(executedPlan)\n\n  protected def preparations: Seq[Rule[SparkPlan]] = {"
  },
  {
    "id" : "829058e2-94f5-4482-82da-bdfd7c9caba9",
    "prId" : 25111,
    "prUrl" : "https://github.com/apache/spark/pull/25111#pullrequestreview-261618774",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fdfcae9c-086c-4d7e-a804-3f5af7ccd4d3",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Maybe not necessary, but should we clone `logical` too before sending to analyzer?",
        "createdAt" : "2019-07-14T14:36:07Z",
        "updatedAt" : "2019-07-23T07:31:57Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "5ace2d21-cffc-47f0-8303-8c054ea28dcb",
        "parentId" : "fdfcae9c-086c-4d7e-a804-3f5af7ccd4d3",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "yea I think we should",
        "createdAt" : "2019-07-15T02:55:37Z",
        "updatedAt" : "2019-07-23T07:31:57Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "4f75ba4db1455dcc35da88b86a46188ce568f37c",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +65,69 @@  }\n\n  lazy val withCachedData: LogicalPlan = {\n    assertAnalyzed()\n    assertSupported()"
  },
  {
    "id" : "7f17d27f-c1e4-434e-bf6f-c43517601f03",
    "prId" : 25111,
    "prUrl" : "https://github.com/apache/spark/pull/25111#pullrequestreview-264286234",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0e58925d-b504-4bcf-9e7c-50e8940b8126",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "> Since now query plan is mutable, I think it's better to limit the life cycle of a query plan instance. We can clone the query plan between analyzer, optimizer and planner, so that the life cycle is limited in one stage.\r\n\r\nIf we decide to clone the plan after each stage, will any test fail if we do not clone it?",
        "createdAt" : "2019-07-19T05:16:29Z",
        "updatedAt" : "2019-07-23T07:31:57Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "ba822254-a443-412a-9719-db24177ea100",
        "parentId" : "0e58925d-b504-4bcf-9e7c-50e8940b8126",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "test added",
        "createdAt" : "2019-07-19T15:36:27Z",
        "updatedAt" : "2019-07-23T07:31:57Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "4f75ba4db1455dcc35da88b86a46188ce568f37c",
    "line" : 30,
    "diffHunk" : "@@ -1,1 +76,80 @@    // clone the plan to avoid sharing the plan instance between different stages like analyzing,\n    // optimizing and planning.\n    sparkSession.sessionState.optimizer.executeAndTrack(withCachedData.clone(), tracker)\n  }\n"
  }
]