[
  {
    "id" : "72c8407f-0924-4fea-a47b-615499470e3e",
    "prId" : 32704,
    "prUrl" : "https://github.com/apache/spark/pull/32704#pullrequestreview-671905420",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d9a25656-4ed0-4277-a442-f0fe30df97e4",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "cc @cloud-fan @maryannxue FYI",
        "createdAt" : "2021-05-31T02:18:16Z",
        "updatedAt" : "2021-05-31T02:18:17Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "b78a9c96d342f4f2c38929ce2dc4860a98a7f984",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +259,263 @@      // if the stats doesn't exist in the statsCache and update the statsCache corresponding\n      // to the node.\n      optimizedPlan.collectWithSubqueries {\n        case plan => plan.stats\n      }"
  },
  {
    "id" : "630edcd0-0674-4ce8-9ea3-02bbe26a2225",
    "prId" : 32704,
    "prUrl" : "https://github.com/apache/spark/pull/32704#pullrequestreview-672075370",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c2412e13-d66d-4a22-98a2-0f1d3935791a",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I see, so this basically triggers stats calculation for every node in the plan tree.",
        "createdAt" : "2021-05-31T07:36:29Z",
        "updatedAt" : "2021-05-31T07:36:29Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "aa86d53b-13f7-4691-8978-1f3720edf7f8",
        "parentId" : "c2412e13-d66d-4a22-98a2-0f1d3935791a",
        "authorId" : "d0d56018-475d-4da3-93a6-5e843402b7f2",
        "body" : "Yes, right.",
        "createdAt" : "2021-05-31T07:39:15Z",
        "updatedAt" : "2021-05-31T07:39:15Z",
        "lastEditedBy" : "d0d56018-475d-4da3-93a6-5e843402b7f2",
        "tags" : [
        ]
      },
      {
        "id" : "e07ca170-7368-4fd1-bc51-fa8f978397da",
        "parentId" : "c2412e13-d66d-4a22-98a2-0f1d3935791a",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Can we add a comment to explain why we need to do it? It's not that obvious to many reviewers.",
        "createdAt" : "2021-05-31T07:53:03Z",
        "updatedAt" : "2021-05-31T07:53:03Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "ec62ba82-b8cf-45e9-a96f-0e67ec22357a",
        "parentId" : "c2412e13-d66d-4a22-98a2-0f1d3935791a",
        "authorId" : "d0d56018-475d-4da3-93a6-5e843402b7f2",
        "body" : "Yes, I added a comment.",
        "createdAt" : "2021-05-31T08:23:43Z",
        "updatedAt" : "2021-05-31T08:23:43Z",
        "lastEditedBy" : "d0d56018-475d-4da3-93a6-5e843402b7f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "b78a9c96d342f4f2c38929ce2dc4860a98a7f984",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +260,264 @@      // to the node.\n      optimizedPlan.collectWithSubqueries {\n        case plan => plan.stats\n      }\n    } catch {"
  },
  {
    "id" : "b804e70c-5dcd-458b-8ce9-bd77647a39c4",
    "prId" : 31968,
    "prUrl" : "https://github.com/apache/spark/pull/31968#pullrequestreview-636445793",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "875332b8-e2b4-4717-aec0-04c575fbcce9",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Let's try our best to avoid mutable states. It will make the system harder to reason about.",
        "createdAt" : "2021-04-15T09:03:34Z",
        "updatedAt" : "2021-04-15T09:03:34Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "50b7764bef8c48466b426cae1e57534e4fef143c",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +59,63 @@  val id: Long = QueryExecution.nextExecutionId\n\n  private var _commandExecutionIdGenerated = false\n\n  def commandExecutionIdGenerated: Boolean = _commandExecutionIdGenerated"
  },
  {
    "id" : "1e92f2d8-31e5-40ff-8157-d8af95687fb3",
    "prId" : 30373,
    "prUrl" : "https://github.com/apache/spark/pull/30373#pullrequestreview-532581467",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "581aeb85-d0e5-4601-98af-205b24734157",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Could you leave some comments here about why we need to put this rule after `EnsureRequirements`?",
        "createdAt" : "2020-11-14T13:43:57Z",
        "updatedAt" : "2020-11-19T04:50:29Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "d1753647-9b78-4e57-81b0-9f8f73d1d1c3",
        "parentId" : "581aeb85-d0e5-4601-98af-205b24734157",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "+1 for @maropu 's comment.",
        "createdAt" : "2020-11-17T16:55:38Z",
        "updatedAt" : "2020-11-19T04:50:29Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "4e684df701c9421eba105d469b0338a5320ce42c",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +347,351 @@      // `RemoveRedundantSorts` needs to be added before `EnsureRequirements` to guarantee the same\n      // number of partitions when instantiating PartitioningCollection.\n      RemoveRedundantSorts,\n      DisableUnnecessaryBucketedScan,\n      ApplyColumnarRulesAndInsertTransitions(sparkSession.sessionState.columnarRules),"
  },
  {
    "id" : "25aea5d2-8438-44b0-a288-9b5c517c17c5",
    "prId" : 30373,
    "prUrl" : "https://github.com/apache/spark/pull/30373#pullrequestreview-537082050",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a33c1a71-7497-4715-9b56-4cec5081fc1d",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "before -> after ?",
        "createdAt" : "2020-11-21T17:30:17Z",
        "updatedAt" : "2020-11-21T17:30:17Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "998814e6-9e7f-4ee0-8841-c25bc6629544",
        "parentId" : "a33c1a71-7497-4715-9b56-4cec5081fc1d",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Ah, I missed it. @allisonwang-db could you fix it?",
        "createdAt" : "2020-11-22T00:17:32Z",
        "updatedAt" : "2020-11-22T00:17:32Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "3fc69546-17f2-438d-aa76-9717ebd4a3db",
        "parentId" : "a33c1a71-7497-4715-9b56-4cec5081fc1d",
        "authorId" : "b3ba992a-312c-46eb-b3c3-8d861d15ac40",
        "body" : "Thanks for catching it! Will create a fix.",
        "createdAt" : "2020-11-24T05:09:28Z",
        "updatedAt" : "2020-11-24T05:09:28Z",
        "lastEditedBy" : "b3ba992a-312c-46eb-b3c3-8d861d15ac40",
        "tags" : [
        ]
      }
    ],
    "commit" : "4e684df701c9421eba105d469b0338a5320ce42c",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +345,349 @@      RemoveRedundantProjects,\n      EnsureRequirements,\n      // `RemoveRedundantSorts` needs to be added before `EnsureRequirements` to guarantee the same\n      // number of partitions when instantiating PartitioningCollection.\n      RemoveRedundantSorts,"
  },
  {
    "id" : "85b16768-9e04-473c-9a71-e2466086c0e0",
    "prId" : 29544,
    "prUrl" : "https://github.com/apache/spark/pull/29544#pullrequestreview-489628359",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4f1c5ff4-0672-4911-92a7-3b6a336ec74b",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "We also run physical rules in `AdaptiveSparkPlanExec`, shall we log plan change there as well?",
        "createdAt" : "2020-09-16T09:02:27Z",
        "updatedAt" : "2020-09-16T09:02:27Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "dda7306f-2bff-4778-ab8f-ca0cd3f28b54",
        "parentId" : "4f1c5ff4-0672-4911-92a7-3b6a336ec74b",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Yea, sure. I'll make a PR later.",
        "createdAt" : "2020-09-16T10:05:55Z",
        "updatedAt" : "2020-09-16T10:05:55Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "e7ca5f82-f439-4570-99e9-e39e956d3413",
        "parentId" : "4f1c5ff4-0672-4911-92a7-3b6a336ec74b",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "https://github.com/apache/spark/pull/29774",
        "createdAt" : "2020-09-16T13:32:10Z",
        "updatedAt" : "2020-09-16T13:32:11Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "40c99a21ae19f4c1e50029e4b10c768339336cc6",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +360,364 @@      preparations: Seq[Rule[SparkPlan]],\n      plan: SparkPlan): SparkPlan = {\n    val planChangeLogger = new PlanChangeLogger[SparkPlan]()\n    val preparedPlan = preparations.foldLeft(plan) { case (sp, rule) =>\n      val result = rule.apply(sp)"
  }
]