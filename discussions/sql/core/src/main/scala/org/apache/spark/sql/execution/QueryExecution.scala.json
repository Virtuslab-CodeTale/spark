[
  {
    "id" : "72c8407f-0924-4fea-a47b-615499470e3e",
    "prId" : 32704,
    "prUrl" : "https://github.com/apache/spark/pull/32704#pullrequestreview-671905420",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d9a25656-4ed0-4277-a442-f0fe30df97e4",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "cc @cloud-fan @maryannxue FYI",
        "createdAt" : "2021-05-31T02:18:16Z",
        "updatedAt" : "2021-05-31T02:18:17Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "b78a9c96d342f4f2c38929ce2dc4860a98a7f984",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +259,263 @@      // if the stats doesn't exist in the statsCache and update the statsCache corresponding\n      // to the node.\n      optimizedPlan.collectWithSubqueries {\n        case plan => plan.stats\n      }"
  },
  {
    "id" : "630edcd0-0674-4ce8-9ea3-02bbe26a2225",
    "prId" : 32704,
    "prUrl" : "https://github.com/apache/spark/pull/32704#pullrequestreview-672075370",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c2412e13-d66d-4a22-98a2-0f1d3935791a",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I see, so this basically triggers stats calculation for every node in the plan tree.",
        "createdAt" : "2021-05-31T07:36:29Z",
        "updatedAt" : "2021-05-31T07:36:29Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "aa86d53b-13f7-4691-8978-1f3720edf7f8",
        "parentId" : "c2412e13-d66d-4a22-98a2-0f1d3935791a",
        "authorId" : "d0d56018-475d-4da3-93a6-5e843402b7f2",
        "body" : "Yes, right.",
        "createdAt" : "2021-05-31T07:39:15Z",
        "updatedAt" : "2021-05-31T07:39:15Z",
        "lastEditedBy" : "d0d56018-475d-4da3-93a6-5e843402b7f2",
        "tags" : [
        ]
      },
      {
        "id" : "e07ca170-7368-4fd1-bc51-fa8f978397da",
        "parentId" : "c2412e13-d66d-4a22-98a2-0f1d3935791a",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Can we add a comment to explain why we need to do it? It's not that obvious to many reviewers.",
        "createdAt" : "2021-05-31T07:53:03Z",
        "updatedAt" : "2021-05-31T07:53:03Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "ec62ba82-b8cf-45e9-a96f-0e67ec22357a",
        "parentId" : "c2412e13-d66d-4a22-98a2-0f1d3935791a",
        "authorId" : "d0d56018-475d-4da3-93a6-5e843402b7f2",
        "body" : "Yes, I added a comment.",
        "createdAt" : "2021-05-31T08:23:43Z",
        "updatedAt" : "2021-05-31T08:23:43Z",
        "lastEditedBy" : "d0d56018-475d-4da3-93a6-5e843402b7f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "b78a9c96d342f4f2c38929ce2dc4860a98a7f984",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +260,264 @@      // to the node.\n      optimizedPlan.collectWithSubqueries {\n        case plan => plan.stats\n      }\n    } catch {"
  },
  {
    "id" : "b804e70c-5dcd-458b-8ce9-bd77647a39c4",
    "prId" : 31968,
    "prUrl" : "https://github.com/apache/spark/pull/31968#pullrequestreview-636445793",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "875332b8-e2b4-4717-aec0-04c575fbcce9",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Let's try our best to avoid mutable states. It will make the system harder to reason about.",
        "createdAt" : "2021-04-15T09:03:34Z",
        "updatedAt" : "2021-04-15T09:03:34Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "50b7764bef8c48466b426cae1e57534e4fef143c",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +59,63 @@  val id: Long = QueryExecution.nextExecutionId\n\n  private var _commandExecutionIdGenerated = false\n\n  def commandExecutionIdGenerated: Boolean = _commandExecutionIdGenerated"
  },
  {
    "id" : "1e92f2d8-31e5-40ff-8157-d8af95687fb3",
    "prId" : 30373,
    "prUrl" : "https://github.com/apache/spark/pull/30373#pullrequestreview-532581467",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "581aeb85-d0e5-4601-98af-205b24734157",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Could you leave some comments here about why we need to put this rule after `EnsureRequirements`?",
        "createdAt" : "2020-11-14T13:43:57Z",
        "updatedAt" : "2020-11-19T04:50:29Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "d1753647-9b78-4e57-81b0-9f8f73d1d1c3",
        "parentId" : "581aeb85-d0e5-4601-98af-205b24734157",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "+1 for @maropu 's comment.",
        "createdAt" : "2020-11-17T16:55:38Z",
        "updatedAt" : "2020-11-19T04:50:29Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "4e684df701c9421eba105d469b0338a5320ce42c",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +347,351 @@      // `RemoveRedundantSorts` needs to be added before `EnsureRequirements` to guarantee the same\n      // number of partitions when instantiating PartitioningCollection.\n      RemoveRedundantSorts,\n      DisableUnnecessaryBucketedScan,\n      ApplyColumnarRulesAndInsertTransitions(sparkSession.sessionState.columnarRules),"
  },
  {
    "id" : "25aea5d2-8438-44b0-a288-9b5c517c17c5",
    "prId" : 30373,
    "prUrl" : "https://github.com/apache/spark/pull/30373#pullrequestreview-537082050",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a33c1a71-7497-4715-9b56-4cec5081fc1d",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "before -> after ?",
        "createdAt" : "2020-11-21T17:30:17Z",
        "updatedAt" : "2020-11-21T17:30:17Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "998814e6-9e7f-4ee0-8841-c25bc6629544",
        "parentId" : "a33c1a71-7497-4715-9b56-4cec5081fc1d",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Ah, I missed it. @allisonwang-db could you fix it?",
        "createdAt" : "2020-11-22T00:17:32Z",
        "updatedAt" : "2020-11-22T00:17:32Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "3fc69546-17f2-438d-aa76-9717ebd4a3db",
        "parentId" : "a33c1a71-7497-4715-9b56-4cec5081fc1d",
        "authorId" : "b3ba992a-312c-46eb-b3c3-8d861d15ac40",
        "body" : "Thanks for catching it! Will create a fix.",
        "createdAt" : "2020-11-24T05:09:28Z",
        "updatedAt" : "2020-11-24T05:09:28Z",
        "lastEditedBy" : "b3ba992a-312c-46eb-b3c3-8d861d15ac40",
        "tags" : [
        ]
      }
    ],
    "commit" : "4e684df701c9421eba105d469b0338a5320ce42c",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +345,349 @@      RemoveRedundantProjects,\n      EnsureRequirements,\n      // `RemoveRedundantSorts` needs to be added before `EnsureRequirements` to guarantee the same\n      // number of partitions when instantiating PartitioningCollection.\n      RemoveRedundantSorts,"
  },
  {
    "id" : "85b16768-9e04-473c-9a71-e2466086c0e0",
    "prId" : 29544,
    "prUrl" : "https://github.com/apache/spark/pull/29544#pullrequestreview-489628359",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4f1c5ff4-0672-4911-92a7-3b6a336ec74b",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "We also run physical rules in `AdaptiveSparkPlanExec`, shall we log plan change there as well?",
        "createdAt" : "2020-09-16T09:02:27Z",
        "updatedAt" : "2020-09-16T09:02:27Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "dda7306f-2bff-4778-ab8f-ca0cd3f28b54",
        "parentId" : "4f1c5ff4-0672-4911-92a7-3b6a336ec74b",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Yea, sure. I'll make a PR later.",
        "createdAt" : "2020-09-16T10:05:55Z",
        "updatedAt" : "2020-09-16T10:05:55Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "e7ca5f82-f439-4570-99e9-e39e956d3413",
        "parentId" : "4f1c5ff4-0672-4911-92a7-3b6a336ec74b",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "https://github.com/apache/spark/pull/29774",
        "createdAt" : "2020-09-16T13:32:10Z",
        "updatedAt" : "2020-09-16T13:32:11Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "40c99a21ae19f4c1e50029e4b10c768339336cc6",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +360,364 @@      preparations: Seq[Rule[SparkPlan]],\n      plan: SparkPlan): SparkPlan = {\n    val planChangeLogger = new PlanChangeLogger[SparkPlan]()\n    val preparedPlan = preparations.foldLeft(plan) { case (sp, rule) =>\n      val result = rule.apply(sp)"
  },
  {
    "id" : "13302f03-c57d-40a6-a84f-585199ea0adc",
    "prId" : 29372,
    "prUrl" : "https://github.com/apache/spark/pull/29372#pullrequestreview-466398683",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "503dcb9c-be27-40ab-a15f-eb484f04d341",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "What is the use case to add this ID?",
        "createdAt" : "2020-08-12T02:30:07Z",
        "updatedAt" : "2020-08-12T02:30:07Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "90a83a94-1b8f-498e-9858-bb55d80847ac",
        "parentId" : "503dcb9c-be27-40ab-a15f-eb484f04d341",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "We have a listener which parses the logical & physical plan from the query execution and reflects the changes to Apache Atlas. While it works well in general, there's a pain point on the listener due to the fact Spark can callback the listener multiple times on the same query execution, whereas the instance of query execution doesn't have the unique ID to distinct. That's already I wrote in PR description as well as JIRA description.",
        "createdAt" : "2020-08-13T02:11:56Z",
        "updatedAt" : "2020-08-13T02:11:57Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "3f1bea27a26f33e20f332e25011da3284aec4986",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +56,60 @@    val tracker: QueryPlanningTracker = new QueryPlanningTracker) extends Logging {\n\n  val id: Long = QueryExecution.nextExecutionId\n\n  // TODO: Move the planner an optimizer into here from SessionState."
  },
  {
    "id" : "ddacf79f-8aad-4586-bb20-dedec2e5be7c",
    "prId" : 28885,
    "prUrl" : "https://github.com/apache/spark/pull/28885#pullrequestreview-438797082",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "92a5d66b-c2ec-40b4-b25c-1d7c0b7cb951",
        "parentId" : null,
        "authorId" : "def2dd9f-ce27-41ce-8156-f8225ddc68a9",
        "body" : "Why do we need this boolean parameter here? What will happen if we just always run the `WholePlanReuse` rule? ",
        "createdAt" : "2020-06-27T11:16:00Z",
        "updatedAt" : "2021-03-19T12:14:59Z",
        "lastEditedBy" : "def2dd9f-ce27-41ce-8156-f8225ddc68a9",
        "tags" : [
        ]
      },
      {
        "id" : "5205cf1e-0e2b-4625-9158-ec5c591de89e",
        "parentId" : "92a5d66b-c2ec-40b4-b25c-1d7c0b7cb951",
        "authorId" : "3d4870da-39a8-4406-b00d-131930d14cd8",
        "body" : "Sorry, I don't get this, why would we run this rule multiple times? This new rule traverses through the whole plan, does it make any sense to run in on subqueries and then run it on a main query which also incorporates traversing on subqueries?",
        "createdAt" : "2020-06-28T10:27:50Z",
        "updatedAt" : "2021-03-19T12:14:59Z",
        "lastEditedBy" : "3d4870da-39a8-4406-b00d-131930d14cd8",
        "tags" : [
        ]
      },
      {
        "id" : "b80f27b9-f0ec-4765-a1d6-310d45470985",
        "parentId" : "92a5d66b-c2ec-40b4-b25c-1d7c0b7cb951",
        "authorId" : "def2dd9f-ce27-41ce-8156-f8225ddc68a9",
        "body" : "Yes, from the performance perspective makes sense to exclude them. I sort of don't like having another parameter and select rules based on that, so was thinking if it's not a huge performance difference let's not do it, but it can be expensive with canonicalization, etc. I guess we don't have any other way of detecting if a physical plan is a subquery locally inside the new rule, so it's fine to do it like this, maybe we need a more explicit name for `QueryExecution.prepareExecutedPlan` in the future, like `PrepareSubqueryForExecution` to make it more clear that this method is only called for subqueries. ",
        "createdAt" : "2020-06-28T17:04:32Z",
        "updatedAt" : "2021-03-19T12:14:59Z",
        "lastEditedBy" : "def2dd9f-ce27-41ce-8156-f8225ddc68a9",
        "tags" : [
        ]
      }
    ],
    "commit" : "7187ebd2e053570d92017100dba4a0738fa2f014",
    "line" : 25,
    "diffHunk" : "@@ -1,1 +402,406 @@      sparkSession: SparkSession,\n      adaptiveExecutionRule: Option[InsertAdaptiveSparkPlan] = None,\n      subquery: Boolean): Seq[Rule[SparkPlan]] = {\n    // `AdaptiveSparkPlanExec` is a leaf node. If inserted, all the following rules will be no-op\n    // as the original plan is hidden behind `AdaptiveSparkPlanExec`."
  },
  {
    "id" : "df634b9b-8bcc-4aee-a68d-c7f0cc4ddae9",
    "prId" : 28493,
    "prUrl" : "https://github.com/apache/spark/pull/28493#pullrequestreview-417712197",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3ea49810-e84e-400e-adf9-63540d7c0e2e",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Could you describe `path` too.",
        "createdAt" : "2020-05-25T13:39:50Z",
        "updatedAt" : "2020-05-25T17:51:34Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "1fea0e2f1c7909083973027acd46b1e37eb41d21",
    "line" : 149,
    "diffHunk" : "@@ -1,1 +290,294 @@    /**\n     * Dumps debug information about query execution into the specified file.\n     *\n     * @param path path of the file the debug info is written to.\n     * @param maxFields maximum number of fields converted to string representation."
  },
  {
    "id" : "a4af5fb1-5274-4057-8279-410086e4f2bf",
    "prId" : 26778,
    "prUrl" : "https://github.com/apache/spark/pull/26778#pullrequestreview-328129357",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "60af1b9e-dadf-4849-8fa6-31b35bcffe00",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "can we just move them into the try-catch?",
        "createdAt" : "2019-12-06T10:53:12Z",
        "updatedAt" : "2019-12-06T12:02:07Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "44f09e0f37174fd3a48561794e5d8d006db3586d",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +146,150 @@      QueryPlan.append(analyzed, append, verbose, addSuffix, maxFields)\n      append(\"\\n== Optimized Logical Plan ==\\n\")\n      QueryPlan.append(optimizedPlan, append, verbose, addSuffix, maxFields)\n      append(\"\\n== Physical Plan ==\\n\")\n      QueryPlan.append(executedPlan, append, verbose, addSuffix, maxFields)"
  },
  {
    "id" : "767d991e-6ad2-4a71-87a4-f314e04c6b63",
    "prId" : 26778,
    "prUrl" : "https://github.com/apache/spark/pull/26778#pullrequestreview-328161284",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "34cd1412-8f15-4e71-938e-9e115a7dd80e",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Statements for if and else seems to be swapped.",
        "createdAt" : "2019-12-06T11:44:04Z",
        "updatedAt" : "2019-12-06T12:02:07Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "63fbd4ef-2ffe-44d9-83e6-bd6ffdda3f39",
        "parentId" : "34cd1412-8f15-4e71-938e-9e115a7dd80e",
        "authorId" : "75554730-8183-463c-bf1d-7cd5b4e9f22b",
        "body" : "If we move this to try-catch (as @cloud-fan suggested), we don't need if-else.",
        "createdAt" : "2019-12-06T11:58:37Z",
        "updatedAt" : "2019-12-06T12:02:07Z",
        "lastEditedBy" : "75554730-8183-463c-bf1d-7cd5b4e9f22b",
        "tags" : [
        ]
      },
      {
        "id" : "8589b521-5b5a-4f5d-bfee-80080be2a703",
        "parentId" : "34cd1412-8f15-4e71-938e-9e115a7dd80e",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Sure.",
        "createdAt" : "2019-12-06T12:01:19Z",
        "updatedAt" : "2019-12-06T12:02:07Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "44f09e0f37174fd3a48561794e5d8d006db3586d",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +144,148 @@      )\n      append(\"\\n\")\n      QueryPlan.append(analyzed, append, verbose, addSuffix, maxFields)\n      append(\"\\n== Optimized Logical Plan ==\\n\")\n      QueryPlan.append(optimizedPlan, append, verbose, addSuffix, maxFields)"
  }
]