[
  {
    "id" : "41f93f26-84a7-4434-81d0-2075672893ab",
    "prId" : 33624,
    "prUrl" : "https://github.com/apache/spark/pull/33624#pullrequestreview-723308539",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f5b89c24-9bb1-424c-a1e1-4a5ebddeb186",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "link to the previous discussion: https://github.com/apache/spark/pull/33624#discussion_r682088051",
        "createdAt" : "2021-08-04T08:37:00Z",
        "updatedAt" : "2021-08-04T08:37:00Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "46d659e0-e181-4364-9a48-054a67713387",
        "parentId" : "f5b89c24-9bb1-424c-a1e1-4a5ebddeb186",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "can we keep the docs of https://github.com/apache/spark/pull/33624/files#diff-ec42cd27662f3f528832c298a60fffa1d341feb04aa1d8c80044b70cbe0ebbfcL199-L203?",
        "createdAt" : "2021-08-05T00:57:28Z",
        "updatedAt" : "2021-08-05T00:57:29Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "db53c2c4-9333-45a9-a33f-a283cc6f6ae0",
        "parentId" : "f5b89c24-9bb1-424c-a1e1-4a5ebddeb186",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "that doc doesn't apply any more right now.",
        "createdAt" : "2021-08-05T12:46:02Z",
        "updatedAt" : "2021-08-05T12:46:03Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "f768a3f08c46edc8d4e85f1dc313ff9db539edea",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +67,71 @@    @transient preprocessingRules: Seq[Rule[SparkPlan]],\n    @transient isSubquery: Boolean,\n    @transient override val supportsColumnar: Boolean = false)\n  extends LeafExecNode {\n"
  },
  {
    "id" : "9eb7a868-7e3b-4f91-aed4-1dc179296d67",
    "prId" : 33624,
    "prUrl" : "https://github.com/apache/spark/pull/33624#pullrequestreview-722960096",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6eec92c2-8cab-4616-90a1-054e33f2b7b1",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "can we move this method back?",
        "createdAt" : "2021-08-05T00:51:29Z",
        "updatedAt" : "2021-08-05T00:51:29Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "6d05fc31-461b-4671-a25c-2dc6174c572c",
        "parentId" : "6eec92c2-8cab-4616-90a1-054e33f2b7b1",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "it's not just moving around, it follows the other methods to also call `withFinalPlanUpdate`.",
        "createdAt" : "2021-08-05T05:55:48Z",
        "updatedAt" : "2021-08-05T05:55:49Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "f768a3f08c46edc8d4e85f1dc313ff9db539edea",
    "line" : 80,
    "diffHunk" : "@@ -1,1 +349,353 @@  }\n\n  override def doExecuteBroadcast[T](): broadcast.Broadcast[T] = {\n    withFinalPlanUpdate { finalPlan =>\n      assert(finalPlan.isInstanceOf[BroadcastQueryStageExec])"
  },
  {
    "id" : "45dbfe4e-72c1-4e0c-aea3-bd844257cf7a",
    "prId" : 33541,
    "prUrl" : "https://github.com/apache/spark/pull/33541#pullrequestreview-716574738",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2c5fc7ac-9757-4221-8a16-c62b972d1601",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "If `requiredDistribution` is `None` (the case like `df.repartition(a, b).select(c)`), does the validation still work? If the user-specified distribution is removed, the validation seem not able to detect it?",
        "createdAt" : "2021-07-28T04:03:54Z",
        "updatedAt" : "2021-07-28T04:05:10Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "5d34c95d-4e11-4ac8-ae06-12e89926b7c7",
        "parentId" : "2c5fc7ac-9757-4221-8a16-c62b972d1601",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "If `requiredDistribution` is `None`, we don't need `ValidateRequirements` to check the distribution, as `EnsureRequirement` won't remove the effective user-specified repartition. I'll add more comments here.",
        "createdAt" : "2021-07-28T04:43:47Z",
        "updatedAt" : "2021-07-28T04:43:48Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "61cc6751e4baad96f0a8b251296c6e58ffbbf3a3",
    "line" : 72,
    "diffHunk" : "@@ -1,1 +141,145 @@            // out the user-specified repartition, thus we don't have a distribution requirement\n            // for the final plan.\n            requiredDistribution.getOrElse(UnspecifiedDistribution)\n          } else {\n            UnspecifiedDistribution"
  },
  {
    "id" : "666c578e-f4a8-4841-8b7e-3da5d3faffd1",
    "prId" : 33244,
    "prUrl" : "https://github.com/apache/spark/pull/33244#pullrequestreview-702612558",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "173307f4-eb14-4bd6-b21e-8a7222cbdd25",
        "parentId" : null,
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "I think it's a better place to fix this issue",
        "createdAt" : "2021-07-08T06:32:49Z",
        "updatedAt" : "2021-07-08T06:32:49Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      },
      {
        "id" : "7616278c-9943-4f0a-a1bf-0f21e282a420",
        "parentId" : "173307f4-eb14-4bd6-b21e-8a7222cbdd25",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Could you also add a simple comment mentioning why we use `inputPlan` instead of `currentPhysicalPlan` ?",
        "createdAt" : "2021-07-08T18:05:49Z",
        "updatedAt" : "2021-07-08T18:05:50Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "25f507c7-4121-4756-8a74-fd3b92751f08",
        "parentId" : "173307f4-eb14-4bd6-b21e-8a7222cbdd25",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "sure, added",
        "createdAt" : "2021-07-09T01:24:55Z",
        "updatedAt" : "2021-07-09T01:24:55Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      }
    ],
    "commit" : "ed90d588467b668a1ef314907c2f5ef777bdb95e",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +189,193 @@      // Use inputPlan logicalLink here in case some top level physical nodes may be removed\n      // during `initialPlan`\n      var currentLogicalPlan = inputPlan.logicalLink.get\n      var result = createQueryStages(currentPhysicalPlan)\n      val events = new LinkedBlockingQueue[StageMaterializationEvent]()"
  },
  {
    "id" : "d84bdc73-b2fd-4971-89c1-e6ec0b710f6d",
    "prId" : 32944,
    "prUrl" : "https://github.com/apache/spark/pull/32944#pullrequestreview-698640452",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "42b281a8-66ee-4760-abea-a703681638f4",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Hey, how do we use this? `CostEvaluator` isn't an API. whole execution package is for internal purpose. I don't think it makes much sense to make it pluggable.",
        "createdAt" : "2021-07-02T09:05:01Z",
        "updatedAt" : "2021-07-02T09:09:08Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "69777c10-4b67-4fdb-8e50-45361103a29b",
        "parentId" : "42b281a8-66ee-4760-abea-a703681638f4",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "@HyukjinKwon - good callout. If the whole execution package is for internal purpose (I think the only public one is around data source, right?), how about moving `Cost` and `CostEvaluator` to `org.apache.spark.sql.catalyst.planning` or `org.apache.spark.sql.catalyst.plans.physical`?",
        "createdAt" : "2021-07-02T09:27:21Z",
        "updatedAt" : "2021-07-02T09:27:22Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      },
      {
        "id" : "558d3f45-d7b3-4c0f-b9cc-e863469d181f",
        "parentId" : "42b281a8-66ee-4760-abea-a703681638f4",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "hey sorry I left some comments and removed it back. Yes, it makes sense to me.",
        "createdAt" : "2021-07-02T10:04:24Z",
        "updatedAt" : "2021-07-02T10:04:25Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "975dbd8c-ee32-4632-a959-a53195b0857d",
        "parentId" : "42b281a8-66ee-4760-abea-a703681638f4",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "`catalyst` is also an internal package. \r\n\r\n`SparkSessionExtensions` exposes `LogicalPlan` as well, and I think unstable developer API does not have a hard requirement on the package names. I'm fine to add `@Unstable`",
        "createdAt" : "2021-07-02T11:39:10Z",
        "updatedAt" : "2021-07-02T11:39:10Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "b04edb1d-de1d-4d47-bdc0-f773c5f75570",
        "parentId" : "42b281a8-66ee-4760-abea-a703681638f4",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Yeah, we can add `@Unstable` for now but I would also add a note that this class is supposed to be moved or changed in the near future. ",
        "createdAt" : "2021-07-04T01:51:45Z",
        "updatedAt" : "2021-07-04T01:51:45Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "550c35be-c5ee-4096-aa4a-3639960f686d",
        "parentId" : "42b281a8-66ee-4760-abea-a703681638f4",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "Sounds good, updated with `@Unstable` and added a note comment for class `Cost` and `CostEvaluator`.",
        "createdAt" : "2021-07-04T06:11:36Z",
        "updatedAt" : "2021-07-04T06:12:43Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      }
    ],
    "commit" : "ac5c12186035a47bbe7c0b1891564066db676354",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +132,136 @@\n  @transient private val costEvaluator =\n    conf.getConf(SQLConf.ADAPTIVE_CUSTOM_COST_EVALUATOR_CLASS) match {\n      case Some(className) => CostEvaluator.instantiate(className, session.sparkContext.getConf)\n      case _ => SimpleCostEvaluator"
  },
  {
    "id" : "f9d6e591-7d64-4370-ae54-168d31e72182",
    "prId" : 32705,
    "prUrl" : "https://github.com/apache/spark/pull/32705#pullrequestreview-673918946",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "29faf625-dbc0-437c-bf15-f0b4775bae40",
        "parentId" : null,
        "authorId" : "1b84a7ff-6bf9-4417-bf9f-e46e997e5974",
        "body" : "Why the getFinalPhysicalPlan() is `WholeStageCodegen  `and not `BroadcastQueryStage `+ `WholeStageCodegen  ` ?",
        "createdAt" : "2021-05-31T06:46:16Z",
        "updatedAt" : "2021-05-31T06:46:16Z",
        "lastEditedBy" : "1b84a7ff-6bf9-4417-bf9f-e46e997e5974",
        "tags" : [
        ]
      },
      {
        "id" : "e6d0b3de-a44f-4d77-8328-33ca9259e72b",
        "parentId" : "29faf625-dbc0-437c-bf15-f0b4775bae40",
        "authorId" : "1b84a7ff-6bf9-4417-bf9f-e46e997e5974",
        "body" : "The root cause of this bug may be the loss of `BroadcastExchange` node in DPP filter. I commit a initial idea in [PR#32741](https://github.com/apache/spark/pull/32741).",
        "createdAt" : "2021-06-02T08:18:25Z",
        "updatedAt" : "2021-06-02T08:18:25Z",
        "lastEditedBy" : "1b84a7ff-6bf9-4417-bf9f-e46e997e5974",
        "tags" : [
        ]
      }
    ],
    "commit" : "932edd7808ba8ae9220658eff37c9c3af77eb09f",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +313,317 @@\n  override def doExecuteBroadcast[T](): broadcast.Broadcast[T] = {\n    getFinalPhysicalPlan() match {\n      case b: BroadcastExchangeExec => b.doExecuteBroadcast()\n      case b: BroadcastQueryStageExec => b.doExecuteBroadcast()"
  },
  {
    "id" : "19fbca19-03ad-4abb-959b-2c7a662f2432",
    "prId" : 32195,
    "prUrl" : "https://github.com/apache/spark/pull/32195#pullrequestreview-658929655",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bd06b1f9-c591-40b0-a923-78668a989437",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "This breaks DPP. I'm not sure exactly why, but the key of stage cache was plan without applying `queryStageOptimizerRules`, now it's different. This probably breaks DPP.",
        "createdAt" : "2021-04-22T09:26:25Z",
        "updatedAt" : "2021-04-22T09:26:25Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "80ac25c2-cdaf-4ac4-abf4-25c4ee378667",
        "parentId" : "bd06b1f9-c591-40b0-a923-78668a989437",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "One idea is to introduce a new phase: `preStageCreationRules`. We can put `ApplyColumnarRulesAndInsertTransitions` and run it before reusing exchanges.",
        "createdAt" : "2021-04-22T09:28:48Z",
        "updatedAt" : "2021-04-22T09:28:49Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "14647275-b137-4396-ad08-f99ee6023d78",
        "parentId" : "bd06b1f9-c591-40b0-a923-78668a989437",
        "authorId" : "7e060620-bf02-418e-832f-c68dddfa2611",
        "body" : "Thanks for the suggestions. I am working on this now and will try this out.",
        "createdAt" : "2021-04-27T16:39:25Z",
        "updatedAt" : "2021-04-27T16:39:26Z",
        "lastEditedBy" : "7e060620-bf02-418e-832f-c68dddfa2611",
        "tags" : [
        ]
      },
      {
        "id" : "a3ffcca7-5633-4670-a951-8a17f5f74b1f",
        "parentId" : "bd06b1f9-c591-40b0-a923-78668a989437",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "thanks for pointing it out. That feels odd to me that it breaks the DPP. I did also notice seems more changes to the way that works in flight: https://github.com/apache/spark/pull/31756/files\r\n\r\n",
        "createdAt" : "2021-04-27T18:53:05Z",
        "updatedAt" : "2021-04-27T18:53:05Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "29fa4ec3-dcd1-4bb8-aea1-c20c6f4b5350",
        "parentId" : "bd06b1f9-c591-40b0-a923-78668a989437",
        "authorId" : "7e060620-bf02-418e-832f-c68dddfa2611",
        "body" : "If we merge https://github.com/apache/spark/pull/31756 and then rebase this PR the DPP tests pass so I think this is the best path forward.",
        "createdAt" : "2021-04-28T18:50:10Z",
        "updatedAt" : "2021-04-28T18:50:11Z",
        "lastEditedBy" : "7e060620-bf02-418e-832f-c68dddfa2611",
        "tags" : [
        ]
      },
      {
        "id" : "ac92dedc-0df5-401b-b667-3a56f32d3305",
        "parentId" : "bd06b1f9-c591-40b0-a923-78668a989437",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "#31756 was merged @andygrove  can you up merge and retest to make sure this works?",
        "createdAt" : "2021-05-13T13:57:19Z",
        "updatedAt" : "2021-05-13T13:57:19Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "dbe2f9cb1d4de25721d85153c0ca7164f19034f0",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +437,441 @@              // `stageCache` with the new stage.\n              val queryStage = context.stageCache.getOrElseUpdate(\n                newStage.plan.canonicalized, newStage)\n              if (queryStage.ne(newStage)) {\n                newStage = reuseQueryStage(queryStage, e)"
  },
  {
    "id" : "4417691a-ecbe-4cd8-9c1b-5658b673c137",
    "prId" : 31653,
    "prUrl" : "https://github.com/apache/spark/pull/31653#pullrequestreview-623428847",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "942557d1-2c06-427d-b695-c2be8ac5651a",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "This is not needed if we add `SkewJoinAwareCost`",
        "createdAt" : "2021-03-29T16:33:55Z",
        "updatedAt" : "2021-03-29T17:14:45Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "7cfda59a1e9a8e0f86e2e6424c21639c1b5b14e3",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +92,96 @@  ) ++ context.session.sessionState.queryStagePrepRules // can be set when creating SparkSession\n\n  private def queryStagePreparationRules2: Seq[Rule[SparkPlan]] = Seq(\n    new OptimizeSkewedJoin(inputPlan)\n  )"
  }
]