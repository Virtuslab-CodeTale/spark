[
  {
    "id" : "41f93f26-84a7-4434-81d0-2075672893ab",
    "prId" : 33624,
    "prUrl" : "https://github.com/apache/spark/pull/33624#pullrequestreview-723308539",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f5b89c24-9bb1-424c-a1e1-4a5ebddeb186",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "link to the previous discussion: https://github.com/apache/spark/pull/33624#discussion_r682088051",
        "createdAt" : "2021-08-04T08:37:00Z",
        "updatedAt" : "2021-08-04T08:37:00Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "46d659e0-e181-4364-9a48-054a67713387",
        "parentId" : "f5b89c24-9bb1-424c-a1e1-4a5ebddeb186",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "can we keep the docs of https://github.com/apache/spark/pull/33624/files#diff-ec42cd27662f3f528832c298a60fffa1d341feb04aa1d8c80044b70cbe0ebbfcL199-L203?",
        "createdAt" : "2021-08-05T00:57:28Z",
        "updatedAt" : "2021-08-05T00:57:29Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "db53c2c4-9333-45a9-a33f-a283cc6f6ae0",
        "parentId" : "f5b89c24-9bb1-424c-a1e1-4a5ebddeb186",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "that doc doesn't apply any more right now.",
        "createdAt" : "2021-08-05T12:46:02Z",
        "updatedAt" : "2021-08-05T12:46:03Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "f768a3f08c46edc8d4e85f1dc313ff9db539edea",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +67,71 @@    @transient preprocessingRules: Seq[Rule[SparkPlan]],\n    @transient isSubquery: Boolean,\n    @transient override val supportsColumnar: Boolean = false)\n  extends LeafExecNode {\n"
  },
  {
    "id" : "9eb7a868-7e3b-4f91-aed4-1dc179296d67",
    "prId" : 33624,
    "prUrl" : "https://github.com/apache/spark/pull/33624#pullrequestreview-722960096",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6eec92c2-8cab-4616-90a1-054e33f2b7b1",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "can we move this method back?",
        "createdAt" : "2021-08-05T00:51:29Z",
        "updatedAt" : "2021-08-05T00:51:29Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "6d05fc31-461b-4671-a25c-2dc6174c572c",
        "parentId" : "6eec92c2-8cab-4616-90a1-054e33f2b7b1",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "it's not just moving around, it follows the other methods to also call `withFinalPlanUpdate`.",
        "createdAt" : "2021-08-05T05:55:48Z",
        "updatedAt" : "2021-08-05T05:55:49Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "f768a3f08c46edc8d4e85f1dc313ff9db539edea",
    "line" : 80,
    "diffHunk" : "@@ -1,1 +349,353 @@  }\n\n  override def doExecuteBroadcast[T](): broadcast.Broadcast[T] = {\n    withFinalPlanUpdate { finalPlan =>\n      assert(finalPlan.isInstanceOf[BroadcastQueryStageExec])"
  },
  {
    "id" : "45dbfe4e-72c1-4e0c-aea3-bd844257cf7a",
    "prId" : 33541,
    "prUrl" : "https://github.com/apache/spark/pull/33541#pullrequestreview-716574738",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2c5fc7ac-9757-4221-8a16-c62b972d1601",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "If `requiredDistribution` is `None` (the case like `df.repartition(a, b).select(c)`), does the validation still work? If the user-specified distribution is removed, the validation seem not able to detect it?",
        "createdAt" : "2021-07-28T04:03:54Z",
        "updatedAt" : "2021-07-28T04:05:10Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "5d34c95d-4e11-4ac8-ae06-12e89926b7c7",
        "parentId" : "2c5fc7ac-9757-4221-8a16-c62b972d1601",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "If `requiredDistribution` is `None`, we don't need `ValidateRequirements` to check the distribution, as `EnsureRequirement` won't remove the effective user-specified repartition. I'll add more comments here.",
        "createdAt" : "2021-07-28T04:43:47Z",
        "updatedAt" : "2021-07-28T04:43:48Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "61cc6751e4baad96f0a8b251296c6e58ffbbf3a3",
    "line" : 72,
    "diffHunk" : "@@ -1,1 +141,145 @@            // out the user-specified repartition, thus we don't have a distribution requirement\n            // for the final plan.\n            requiredDistribution.getOrElse(UnspecifiedDistribution)\n          } else {\n            UnspecifiedDistribution"
  },
  {
    "id" : "666c578e-f4a8-4841-8b7e-3da5d3faffd1",
    "prId" : 33244,
    "prUrl" : "https://github.com/apache/spark/pull/33244#pullrequestreview-702612558",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "173307f4-eb14-4bd6-b21e-8a7222cbdd25",
        "parentId" : null,
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "I think it's a better place to fix this issue",
        "createdAt" : "2021-07-08T06:32:49Z",
        "updatedAt" : "2021-07-08T06:32:49Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      },
      {
        "id" : "7616278c-9943-4f0a-a1bf-0f21e282a420",
        "parentId" : "173307f4-eb14-4bd6-b21e-8a7222cbdd25",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Could you also add a simple comment mentioning why we use `inputPlan` instead of `currentPhysicalPlan` ?",
        "createdAt" : "2021-07-08T18:05:49Z",
        "updatedAt" : "2021-07-08T18:05:50Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "25f507c7-4121-4756-8a74-fd3b92751f08",
        "parentId" : "173307f4-eb14-4bd6-b21e-8a7222cbdd25",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "sure, added",
        "createdAt" : "2021-07-09T01:24:55Z",
        "updatedAt" : "2021-07-09T01:24:55Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      }
    ],
    "commit" : "ed90d588467b668a1ef314907c2f5ef777bdb95e",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +189,193 @@      // Use inputPlan logicalLink here in case some top level physical nodes may be removed\n      // during `initialPlan`\n      var currentLogicalPlan = inputPlan.logicalLink.get\n      var result = createQueryStages(currentPhysicalPlan)\n      val events = new LinkedBlockingQueue[StageMaterializationEvent]()"
  },
  {
    "id" : "d84bdc73-b2fd-4971-89c1-e6ec0b710f6d",
    "prId" : 32944,
    "prUrl" : "https://github.com/apache/spark/pull/32944#pullrequestreview-698640452",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "42b281a8-66ee-4760-abea-a703681638f4",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Hey, how do we use this? `CostEvaluator` isn't an API. whole execution package is for internal purpose. I don't think it makes much sense to make it pluggable.",
        "createdAt" : "2021-07-02T09:05:01Z",
        "updatedAt" : "2021-07-02T09:09:08Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "69777c10-4b67-4fdb-8e50-45361103a29b",
        "parentId" : "42b281a8-66ee-4760-abea-a703681638f4",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "@HyukjinKwon - good callout. If the whole execution package is for internal purpose (I think the only public one is around data source, right?), how about moving `Cost` and `CostEvaluator` to `org.apache.spark.sql.catalyst.planning` or `org.apache.spark.sql.catalyst.plans.physical`?",
        "createdAt" : "2021-07-02T09:27:21Z",
        "updatedAt" : "2021-07-02T09:27:22Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      },
      {
        "id" : "558d3f45-d7b3-4c0f-b9cc-e863469d181f",
        "parentId" : "42b281a8-66ee-4760-abea-a703681638f4",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "hey sorry I left some comments and removed it back. Yes, it makes sense to me.",
        "createdAt" : "2021-07-02T10:04:24Z",
        "updatedAt" : "2021-07-02T10:04:25Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "975dbd8c-ee32-4632-a959-a53195b0857d",
        "parentId" : "42b281a8-66ee-4760-abea-a703681638f4",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "`catalyst` is also an internal package. \r\n\r\n`SparkSessionExtensions` exposes `LogicalPlan` as well, and I think unstable developer API does not have a hard requirement on the package names. I'm fine to add `@Unstable`",
        "createdAt" : "2021-07-02T11:39:10Z",
        "updatedAt" : "2021-07-02T11:39:10Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "b04edb1d-de1d-4d47-bdc0-f773c5f75570",
        "parentId" : "42b281a8-66ee-4760-abea-a703681638f4",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Yeah, we can add `@Unstable` for now but I would also add a note that this class is supposed to be moved or changed in the near future. ",
        "createdAt" : "2021-07-04T01:51:45Z",
        "updatedAt" : "2021-07-04T01:51:45Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "550c35be-c5ee-4096-aa4a-3639960f686d",
        "parentId" : "42b281a8-66ee-4760-abea-a703681638f4",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "Sounds good, updated with `@Unstable` and added a note comment for class `Cost` and `CostEvaluator`.",
        "createdAt" : "2021-07-04T06:11:36Z",
        "updatedAt" : "2021-07-04T06:12:43Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      }
    ],
    "commit" : "ac5c12186035a47bbe7c0b1891564066db676354",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +132,136 @@\n  @transient private val costEvaluator =\n    conf.getConf(SQLConf.ADAPTIVE_CUSTOM_COST_EVALUATOR_CLASS) match {\n      case Some(className) => CostEvaluator.instantiate(className, session.sparkContext.getConf)\n      case _ => SimpleCostEvaluator"
  },
  {
    "id" : "f9d6e591-7d64-4370-ae54-168d31e72182",
    "prId" : 32705,
    "prUrl" : "https://github.com/apache/spark/pull/32705#pullrequestreview-673918946",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "29faf625-dbc0-437c-bf15-f0b4775bae40",
        "parentId" : null,
        "authorId" : "1b84a7ff-6bf9-4417-bf9f-e46e997e5974",
        "body" : "Why the getFinalPhysicalPlan() is `WholeStageCodegen  `and not `BroadcastQueryStage `+ `WholeStageCodegen  ` ?",
        "createdAt" : "2021-05-31T06:46:16Z",
        "updatedAt" : "2021-05-31T06:46:16Z",
        "lastEditedBy" : "1b84a7ff-6bf9-4417-bf9f-e46e997e5974",
        "tags" : [
        ]
      },
      {
        "id" : "e6d0b3de-a44f-4d77-8328-33ca9259e72b",
        "parentId" : "29faf625-dbc0-437c-bf15-f0b4775bae40",
        "authorId" : "1b84a7ff-6bf9-4417-bf9f-e46e997e5974",
        "body" : "The root cause of this bug may be the loss of `BroadcastExchange` node in DPP filter. I commit a initial idea in [PR#32741](https://github.com/apache/spark/pull/32741).",
        "createdAt" : "2021-06-02T08:18:25Z",
        "updatedAt" : "2021-06-02T08:18:25Z",
        "lastEditedBy" : "1b84a7ff-6bf9-4417-bf9f-e46e997e5974",
        "tags" : [
        ]
      }
    ],
    "commit" : "932edd7808ba8ae9220658eff37c9c3af77eb09f",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +313,317 @@\n  override def doExecuteBroadcast[T](): broadcast.Broadcast[T] = {\n    getFinalPhysicalPlan() match {\n      case b: BroadcastExchangeExec => b.doExecuteBroadcast()\n      case b: BroadcastQueryStageExec => b.doExecuteBroadcast()"
  },
  {
    "id" : "19fbca19-03ad-4abb-959b-2c7a662f2432",
    "prId" : 32195,
    "prUrl" : "https://github.com/apache/spark/pull/32195#pullrequestreview-658929655",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bd06b1f9-c591-40b0-a923-78668a989437",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "This breaks DPP. I'm not sure exactly why, but the key of stage cache was plan without applying `queryStageOptimizerRules`, now it's different. This probably breaks DPP.",
        "createdAt" : "2021-04-22T09:26:25Z",
        "updatedAt" : "2021-04-22T09:26:25Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "80ac25c2-cdaf-4ac4-abf4-25c4ee378667",
        "parentId" : "bd06b1f9-c591-40b0-a923-78668a989437",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "One idea is to introduce a new phase: `preStageCreationRules`. We can put `ApplyColumnarRulesAndInsertTransitions` and run it before reusing exchanges.",
        "createdAt" : "2021-04-22T09:28:48Z",
        "updatedAt" : "2021-04-22T09:28:49Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "14647275-b137-4396-ad08-f99ee6023d78",
        "parentId" : "bd06b1f9-c591-40b0-a923-78668a989437",
        "authorId" : "7e060620-bf02-418e-832f-c68dddfa2611",
        "body" : "Thanks for the suggestions. I am working on this now and will try this out.",
        "createdAt" : "2021-04-27T16:39:25Z",
        "updatedAt" : "2021-04-27T16:39:26Z",
        "lastEditedBy" : "7e060620-bf02-418e-832f-c68dddfa2611",
        "tags" : [
        ]
      },
      {
        "id" : "a3ffcca7-5633-4670-a951-8a17f5f74b1f",
        "parentId" : "bd06b1f9-c591-40b0-a923-78668a989437",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "thanks for pointing it out. That feels odd to me that it breaks the DPP. I did also notice seems more changes to the way that works in flight: https://github.com/apache/spark/pull/31756/files\r\n\r\n",
        "createdAt" : "2021-04-27T18:53:05Z",
        "updatedAt" : "2021-04-27T18:53:05Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "29fa4ec3-dcd1-4bb8-aea1-c20c6f4b5350",
        "parentId" : "bd06b1f9-c591-40b0-a923-78668a989437",
        "authorId" : "7e060620-bf02-418e-832f-c68dddfa2611",
        "body" : "If we merge https://github.com/apache/spark/pull/31756 and then rebase this PR the DPP tests pass so I think this is the best path forward.",
        "createdAt" : "2021-04-28T18:50:10Z",
        "updatedAt" : "2021-04-28T18:50:11Z",
        "lastEditedBy" : "7e060620-bf02-418e-832f-c68dddfa2611",
        "tags" : [
        ]
      },
      {
        "id" : "ac92dedc-0df5-401b-b667-3a56f32d3305",
        "parentId" : "bd06b1f9-c591-40b0-a923-78668a989437",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "#31756 was merged @andygrove  can you up merge and retest to make sure this works?",
        "createdAt" : "2021-05-13T13:57:19Z",
        "updatedAt" : "2021-05-13T13:57:19Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "dbe2f9cb1d4de25721d85153c0ca7164f19034f0",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +437,441 @@              // `stageCache` with the new stage.\n              val queryStage = context.stageCache.getOrElseUpdate(\n                newStage.plan.canonicalized, newStage)\n              if (queryStage.ne(newStage)) {\n                newStage = reuseQueryStage(queryStage, e)"
  },
  {
    "id" : "4417691a-ecbe-4cd8-9c1b-5658b673c137",
    "prId" : 31653,
    "prUrl" : "https://github.com/apache/spark/pull/31653#pullrequestreview-623428847",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "942557d1-2c06-427d-b695-c2be8ac5651a",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "This is not needed if we add `SkewJoinAwareCost`",
        "createdAt" : "2021-03-29T16:33:55Z",
        "updatedAt" : "2021-03-29T17:14:45Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "7cfda59a1e9a8e0f86e2e6424c21639c1b5b14e3",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +92,96 @@  ) ++ context.session.sessionState.queryStagePrepRules // can be set when creating SparkSession\n\n  private def queryStagePreparationRules2: Seq[Rule[SparkPlan]] = Seq(\n    new OptimizeSkewedJoin(inputPlan)\n  )"
  },
  {
    "id" : "b6f7874a-8d81-4ca5-8b6f-7671bfc2fc5b",
    "prId" : 31167,
    "prUrl" : "https://github.com/apache/spark/pull/31167#pullrequestreview-567809913",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3a12d9eb-b3a8-432d-877c-20119408c2e6",
        "parentId" : null,
        "authorId" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "body" : "indent: line 201 ~216",
        "createdAt" : "2021-01-13T14:43:08Z",
        "updatedAt" : "2021-01-13T15:22:16Z",
        "lastEditedBy" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "tags" : [
        ]
      },
      {
        "id" : "5062af7e-8a61-4bdf-b696-506650485554",
        "parentId" : "3a12d9eb-b3a8-432d-877c-20119408c2e6",
        "authorId" : "21ccf86e-7d54-44b8-aa50-2ad0a352ad3b",
        "body" : "I am not sure line 201 ~ 215 should have 2 more space indent. Just behavior same as line 225~ 236 (old code). ",
        "createdAt" : "2021-01-14T02:26:12Z",
        "updatedAt" : "2021-01-14T02:26:26Z",
        "lastEditedBy" : "21ccf86e-7d54-44b8-aa50-2ad0a352ad3b",
        "tags" : [
        ]
      },
      {
        "id" : "ef07eea5-0c74-4efb-b44c-7dc4d35b768e",
        "parentId" : "3a12d9eb-b3a8-432d-877c-20119408c2e6",
        "authorId" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "body" : "should be ：）",
        "createdAt" : "2021-01-14T02:34:11Z",
        "updatedAt" : "2021-01-14T02:34:12Z",
        "lastEditedBy" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "tags" : [
        ]
      }
    ],
    "commit" : "6bc38f07783c68e51de9eaceb86350a9eb4747a9",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +199,203 @@            .filter(_.isInstanceOf[BroadcastQueryStageExec])\n            .map { stage =>\n            var future: Future[Any] = null\n            try {\n              future = stage.materialize()"
  },
  {
    "id" : "8a09c13e-7af9-445d-8ed3-f7bba00d204d",
    "prId" : 31167,
    "prUrl" : "https://github.com/apache/spark/pull/31167#pullrequestreview-572963418",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0aa94501-9d21-46da-9f52-a4208e7b7681",
        "parentId" : null,
        "authorId" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "body" : "Is it necessary to wait until all `BroadcastQueryStageExec` are materialized, this may cause waste of resources as @cloud-fan said\r\n\r\n",
        "createdAt" : "2021-01-13T15:15:46Z",
        "updatedAt" : "2021-01-13T15:20:00Z",
        "lastEditedBy" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "tags" : [
        ]
      },
      {
        "id" : "fabe8846-5b7a-40d9-8f37-e0373938ba12",
        "parentId" : "0aa94501-9d21-46da-9f52-a4208e7b7681",
        "authorId" : "21ccf86e-7d54-44b8-aa50-2ad0a352ad3b",
        "body" : "In deed, there will be a little waste of resources. This is the same behavior as non-AQE. Given the lightweight of broadcast, it should not cause too much time, few seconds in normal. I think that's acceptable.\r\nIf not wait, there's still probability that situations in #30998 will occur and cause broadcast timeout.",
        "createdAt" : "2021-01-14T02:21:15Z",
        "updatedAt" : "2021-01-14T02:21:16Z",
        "lastEditedBy" : "21ccf86e-7d54-44b8-aa50-2ad0a352ad3b",
        "tags" : [
        ]
      },
      {
        "id" : "7434cc0d-682a-4318-aafd-316fc0a4c20c",
        "parentId" : "0aa94501-9d21-46da-9f52-a4208e7b7681",
        "authorId" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "body" : "@zhongyu09 It might be better to give a benchmark to compare the performance difference between before and after",
        "createdAt" : "2021-01-14T02:39:14Z",
        "updatedAt" : "2021-01-14T02:39:14Z",
        "lastEditedBy" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "tags" : [
        ]
      },
      {
        "id" : "8f4afe17-c61a-4862-8dfb-a9a93a32ff22",
        "parentId" : "0aa94501-9d21-46da-9f52-a4208e7b7681",
        "authorId" : "21ccf86e-7d54-44b8-aa50-2ad0a352ad3b",
        "body" : "Yes, do we have some benchmark testing framework?",
        "createdAt" : "2021-01-14T02:50:20Z",
        "updatedAt" : "2021-01-14T02:50:21Z",
        "lastEditedBy" : "21ccf86e-7d54-44b8-aa50-2ad0a352ad3b",
        "tags" : [
        ]
      },
      {
        "id" : "68129fcd-514b-4b8d-833f-16e0f65af353",
        "parentId" : "0aa94501-9d21-46da-9f52-a4208e7b7681",
        "authorId" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "body" : "Micro benchmark can base on `BenchmarkBase` or `SqlBasedBenchmark`, like `DataSourceReadBenchmark`. But for this scenario, I prefer to you can give a description of the test process and a comparison of the benchmark numbers, maybe need some screenshot",
        "createdAt" : "2021-01-14T06:16:25Z",
        "updatedAt" : "2021-01-14T06:16:26Z",
        "lastEditedBy" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "tags" : [
        ]
      },
      {
        "id" : "0c25ad9a-6d05-4e46-a2d9-bb1f945e6026",
        "parentId" : "0aa94501-9d21-46da-9f52-a4208e7b7681",
        "authorId" : "21ccf86e-7d54-44b8-aa50-2ad0a352ad3b",
        "body" : "I mean test tools and env like Kubernetes integration test or else, otherwise I need to run the test in my local computer? It seems at least we needs to run TPCDSQueryBenchmark.",
        "createdAt" : "2021-01-14T09:05:37Z",
        "updatedAt" : "2021-01-14T09:05:37Z",
        "lastEditedBy" : "21ccf86e-7d54-44b8-aa50-2ad0a352ad3b",
        "tags" : [
        ]
      },
      {
        "id" : "4a019e50-65c8-4e57-9cbb-4759e934cbe9",
        "parentId" : "0aa94501-9d21-46da-9f52-a4208e7b7681",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I agree the perf regression may be small, but the gain is also small. If we simply submit broadcast stages first, in most cases we are already fine. I'm not sure if it worths to introduce perf regression for such a small gain.",
        "createdAt" : "2021-01-14T15:17:46Z",
        "updatedAt" : "2021-01-14T15:17:46Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "538bfde8-ee50-4171-8276-7641b2e5c617",
        "parentId" : "0aa94501-9d21-46da-9f52-a4208e7b7681",
        "authorId" : "21ccf86e-7d54-44b8-aa50-2ad0a352ad3b",
        "body" : "Agree. But I am ok to do perf regression. I am just learning how to do it.",
        "createdAt" : "2021-01-15T09:29:00Z",
        "updatedAt" : "2021-01-15T09:29:01Z",
        "lastEditedBy" : "21ccf86e-7d54-44b8-aa50-2ad0a352ad3b",
        "tags" : [
        ]
      },
      {
        "id" : "24a80692-984d-44e6-a674-4a14a28aa682",
        "parentId" : "0aa94501-9d21-46da-9f52-a4208e7b7681",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I think people were OK with the original solution, it's just a problem of writing tests. We can add a way to record the stage submission order, then use it to test.",
        "createdAt" : "2021-01-15T13:54:37Z",
        "updatedAt" : "2021-01-15T13:54:37Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "64e143ad-026d-4516-ad00-20cc66c80f44",
        "parentId" : "0aa94501-9d21-46da-9f52-a4208e7b7681",
        "authorId" : "21ccf86e-7d54-44b8-aa50-2ad0a352ad3b",
        "body" : "But the original solution cannot guarantee the stage submission order. The UT may be flaky. I have tried to retry the test  when fail, and it's always fail after retry.",
        "createdAt" : "2021-01-15T14:41:09Z",
        "updatedAt" : "2021-01-15T14:41:09Z",
        "lastEditedBy" : "21ccf86e-7d54-44b8-aa50-2ad0a352ad3b",
        "tags" : [
        ]
      },
      {
        "id" : "459ee7f6-2d26-4876-b9cb-b22161aadc2a",
        "parentId" : "0aa94501-9d21-46da-9f52-a4208e7b7681",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "The UT is flaky because we were checking job submission order, while we only guarantee stage submission order. We can update the UT to check stage submission order.",
        "createdAt" : "2021-01-18T06:14:27Z",
        "updatedAt" : "2021-01-18T06:14:28Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "74150300-2342-4057-998c-359b3b2ec7f4",
        "parentId" : "0aa94501-9d21-46da-9f52-a4208e7b7681",
        "authorId" : "21ccf86e-7d54-44b8-aa50-2ad0a352ad3b",
        "body" : "You mean the order of calling materialize for QueryStage? It is not recorded yet. As I said, the original solution cannot guarantee the normal stage(not the QueryStage) submission order. And cannot avoid broadcast time out unless we catch the exception. But I don't think that's a good UT.",
        "createdAt" : "2021-01-19T02:30:57Z",
        "updatedAt" : "2021-01-19T02:30:57Z",
        "lastEditedBy" : "21ccf86e-7d54-44b8-aa50-2ad0a352ad3b",
        "tags" : [
        ]
      },
      {
        "id" : "7da25aa6-9cba-4e4c-99f5-3eaa8f3ddb37",
        "parentId" : "0aa94501-9d21-46da-9f52-a4208e7b7681",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "It's great to have a better fix to guarantee the job submission order, but the current one is not (can cause perf regression if the resource is sufficient). Guarantee query stage submission is not great but it's better than doing nothing. If we agree with the compromise, then we should write UT to verify the behavior, which is the query stage submission order.\r\n\r\nIf you don't like this compromise, let's slow down and think about how to fix the problem without perf regression.",
        "createdAt" : "2021-01-19T03:44:06Z",
        "updatedAt" : "2021-01-19T03:44:06Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "506ceaef-c0d3-4308-b614-c4e1bec0c22f",
        "parentId" : "0aa94501-9d21-46da-9f52-a4208e7b7681",
        "authorId" : "21ccf86e-7d54-44b8-aa50-2ad0a352ad3b",
        "body" : "I am fine with the partial fix like #30998. I wonder is it too heavy to add new event just for UT? \r\nI tend to fix the problem without perf regression. But we can also let the partial fix goes first.",
        "createdAt" : "2021-01-19T04:52:32Z",
        "updatedAt" : "2021-01-19T04:52:32Z",
        "lastEditedBy" : "21ccf86e-7d54-44b8-aa50-2ad0a352ad3b",
        "tags" : [
        ]
      },
      {
        "id" : "b2cd893e-3633-40da-b6bb-2bb7cf386f53",
        "parentId" : "0aa94501-9d21-46da-9f52-a4208e7b7681",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "We can also log the stage submission and then write test to verify the log.",
        "createdAt" : "2021-01-19T05:41:56Z",
        "updatedAt" : "2021-01-19T05:41:56Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "3fa9b7cc-592b-4816-99e1-f4820c07266c",
        "parentId" : "0aa94501-9d21-46da-9f52-a4208e7b7681",
        "authorId" : "21ccf86e-7d54-44b8-aa50-2ad0a352ad3b",
        "body" : "That's an idea. I will have a look for how to do this. Do we have any UT to verify the log? ",
        "createdAt" : "2021-01-19T10:11:55Z",
        "updatedAt" : "2021-01-19T10:11:56Z",
        "lastEditedBy" : "21ccf86e-7d54-44b8-aa50-2ad0a352ad3b",
        "tags" : [
        ]
      },
      {
        "id" : "fc31e222-ca80-4686-a9c1-9a7548b9e902",
        "parentId" : "0aa94501-9d21-46da-9f52-a4208e7b7681",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "yea a lot, e.g. `AdaptiveQueryExecSuite.test log level`",
        "createdAt" : "2021-01-19T10:27:30Z",
        "updatedAt" : "2021-01-19T10:27:30Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "a1f8d0ad-afd9-40ad-b437-e63191742675",
        "parentId" : "0aa94501-9d21-46da-9f52-a4208e7b7681",
        "authorId" : "21ccf86e-7d54-44b8-aa50-2ad0a352ad3b",
        "body" : "Put a partial fix as discussed in https://github.com/apache/spark/pull/31269  cc @viirya ",
        "createdAt" : "2021-01-21T03:50:11Z",
        "updatedAt" : "2021-01-21T03:50:11Z",
        "lastEditedBy" : "21ccf86e-7d54-44b8-aa50-2ad0a352ad3b",
        "tags" : [
        ]
      }
    ],
    "commit" : "6bc38f07783c68e51de9eaceb86350a9eb4747a9",
    "line" : 40,
    "diffHunk" : "@@ -1,1 +217,221 @@\n          // Wait for the materialization of all broadcast stages finish\n          broadcastMaterializationFutures.foreach(ThreadUtils.awaitReady(_, Duration.Inf))\n\n          // Start materialization of non-broadcast stages"
  },
  {
    "id" : "71c64cf3-ffab-420c-9f08-5d61058feacb",
    "prId" : 31167,
    "prUrl" : "https://github.com/apache/spark/pull/31167#pullrequestreview-571856017",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "42dacf4e-16e3-4429-82f8-b6e2dc37a7fd",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "> In non-AQE, we always wait the broadcast finish before submit shuffle map tasks.\r\n\r\nIs that true?\r\n\r\nI think we submit broadcast task when preparing a SparkPlan, but I don't think we wait for it finished before running on other tasks.",
        "createdAt" : "2021-01-14T20:06:34Z",
        "updatedAt" : "2021-01-14T20:06:35Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "4176c7b8-9d04-4ce7-93da-cc4d58a47520",
        "parentId" : "42dacf4e-16e3-4429-82f8-b6e2dc37a7fd",
        "authorId" : "21ccf86e-7d54-44b8-aa50-2ad0a352ad3b",
        "body" : "That's true, I also verified about it. You can see the implementation of BroadcastHashJoinExec. codegen will call prepareRelation() -> prepareBroadcast() and then call buildPlan.executeBroadcast[HashedRelation](), finally BroadcastExchangeExec.doExecuteBroadcast() will be called, which will get relationFuture and wait the broadcast finish.",
        "createdAt" : "2021-01-15T09:57:48Z",
        "updatedAt" : "2021-01-15T09:57:48Z",
        "lastEditedBy" : "21ccf86e-7d54-44b8-aa50-2ad0a352ad3b",
        "tags" : [
        ]
      },
      {
        "id" : "c6644539-2c0e-4f7c-ac2f-ceab3b722afc",
        "parentId" : "42dacf4e-16e3-4429-82f8-b6e2dc37a7fd",
        "authorId" : "21ccf86e-7d54-44b8-aa50-2ad0a352ad3b",
        "body" : "@cloud-fan please correct me if I am wrong. I am not sure when root node is not WholeStageCodegen.",
        "createdAt" : "2021-01-15T10:00:03Z",
        "updatedAt" : "2021-01-15T10:00:03Z",
        "lastEditedBy" : "21ccf86e-7d54-44b8-aa50-2ad0a352ad3b",
        "tags" : [
        ]
      },
      {
        "id" : "6aeffce7-099a-4672-bbc2-9c541010742f",
        "parentId" : "42dacf4e-16e3-4429-82f8-b6e2dc37a7fd",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Once we are going to execute a query, first thing is to prepare the SparkPlan and broadcast is triggered at preparing. We don't wait it and continue the execution until we reach the place we really need broadcasted result. At the place we call `executeBroadcast` which waits the result of broadcast.\r\n\r\nSo you can see we don't wait after the broadcast task is triggered.",
        "createdAt" : "2021-01-15T18:29:57Z",
        "updatedAt" : "2021-01-15T18:29:58Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "5bc8a7d3-2dd4-41e9-a814-88c3ae13193c",
        "parentId" : "42dacf4e-16e3-4429-82f8-b6e2dc37a7fd",
        "authorId" : "21ccf86e-7d54-44b8-aa50-2ad0a352ad3b",
        "body" : "I think we talk about the same thing. For non-AQE, usually it's one job for one query. So when we execute the query, transform it to RDD, we will need the broadcasted result. This is the place I mentioned. Before submit the job of the query, we will waits the result of broadcast.\r\n\r\nhere's part of the log I printed:\r\n```\r\n1610935584070 [Thread[ScalaTest-run-running-AdaptiveQueryExecSuite,5,main]] class org.apache.spark.sql.execution.WholeStageCodegenExec SparkPlan.prepare \r\n1610935584072 BroadcastExchangeExec.doPrepare()\r\n1610935584073 beforeCollect \r\n1610935584074 [Thread[broadcast-exchange-0,5,main]] class org.apache.spark.sql.execution.CoalesceExec SparkPlan.prepare \r\n1610935584163 sc.runJob \r\n1610935584223 DAGScheduler.runJob \r\n1610935584224 DAGScheduler.submitJob Id = 0 \r\n1610935584225 eventProcessLoop.post JobSubmitted \r\n1610935597943 beforeBuild \r\n1610935597982 promise.trySuccess(broadcasted) \r\n1610935597988 [Thread[ScalaTest-run-running-AdaptiveQueryExecSuite,5,main]] class org.apache.spark.sql.execution.InputAdapter SparkPlan.prepare \r\n1610935598184 sc.runJob \r\n1610935598194 DAGScheduler.runJob \r\n1610935598195 DAGScheduler.submitJob Id = 1 \r\n1610935598195 eventProcessLoop.post JobSubmitted \r\n```",
        "createdAt" : "2021-01-18T02:13:01Z",
        "updatedAt" : "2021-01-18T02:13:35Z",
        "lastEditedBy" : "21ccf86e-7d54-44b8-aa50-2ad0a352ad3b",
        "tags" : [
        ]
      },
      {
        "id" : "4988c7ec-d556-4957-9079-b4ab77397cc3",
        "parentId" : "42dacf4e-16e3-4429-82f8-b6e2dc37a7fd",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "No. For normal query without AQE, broadcast job is triggered in preparing the SparkPlan (please see `BroadcastExchangeExec.relationFuture`) and it is different to the job of the query itself. Then we _don't_ wait here. This is the main difference to your change here. Immediately after triggering the broadcast task, Spark continues other part of the query, until it _really_ needs the broadcasted result (please see `executeBroadcast`).\r\n\r\nBut here you wait for the materialization of broadcast task finishes. So even there are still resources available to run other shuffle stages, they won't be run. It is different to current AQE and non-AQE query execution.\r\n",
        "createdAt" : "2021-01-18T02:34:47Z",
        "updatedAt" : "2021-01-18T02:34:48Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "b2bac2e2-0df3-43eb-b879-205ef53cf090",
        "parentId" : "42dacf4e-16e3-4429-82f8-b6e2dc37a7fd",
        "authorId" : "21ccf86e-7d54-44b8-aa50-2ad0a352ad3b",
        "body" : "For normal query without AQE, I mean the query except the broadcast part is one job. So the job must submit to DAGScheduler after the broadcast. If the shuffle stage can be submitted before the broadcast finish, how to explain the log above for the UT? ",
        "createdAt" : "2021-01-19T02:24:44Z",
        "updatedAt" : "2021-01-19T02:24:44Z",
        "lastEditedBy" : "21ccf86e-7d54-44b8-aa50-2ad0a352ad3b",
        "tags" : [
        ]
      },
      {
        "id" : "19665eb0-3657-4e5d-a095-4a5dd0f06ce7",
        "parentId" : "42dacf4e-16e3-4429-82f8-b6e2dc37a7fd",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "It is submitted after the broadcast, I don't say it is submitted _before_ the broadcast. I said, the broadcast is triggered during preparing the SparkPlan, but the query won't stop and wait for it to finish...\r\n\r\nThe point is, whether the shuffle stage needs to _wait for_ the broadcast finishes.",
        "createdAt" : "2021-01-19T06:15:32Z",
        "updatedAt" : "2021-01-19T06:15:33Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "a524e6ad-98d2-4fa7-9086-e56bb9a844e8",
        "parentId" : "42dacf4e-16e3-4429-82f8-b6e2dc37a7fd",
        "authorId" : "21ccf86e-7d54-44b8-aa50-2ad0a352ad3b",
        "body" : "> the broadcast is triggered during preparing the SparkPlan\r\n\r\nSorry for didn't make it clear. I agree with this from the beginning.\r\n\r\n> The point is, whether the shuffle stage needs to wait for the broadcast finishes.\r\n\r\nI think _needs to wait for_.  You can see from the logs above, the job 1 is submitted after the broadcast finish ( I print the log in relationFuture => promise.trySuccess ). I add sleep 2 sec in broadcast to make it clear.",
        "createdAt" : "2021-01-19T08:22:14Z",
        "updatedAt" : "2021-01-19T08:22:30Z",
        "lastEditedBy" : "21ccf86e-7d54-44b8-aa50-2ad0a352ad3b",
        "tags" : [
        ]
      },
      {
        "id" : "e5666b25-c9cb-482a-be6b-35e92f5064e1",
        "parentId" : "42dacf4e-16e3-4429-82f8-b6e2dc37a7fd",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "No, it maybe true for your run. In your case, maybe all resources are occupied by the broadcast. So no other job can be really scheduled to executors. But logically we don't set this limitation. I repeat, if there are enough resources, broadcast job and shuffle stage can be run in parallel.\r\n\r\nLet me show it more clear...\r\n\r\n1. During preparing the SparkPlan, broadcast job is submitted to run, right? And we don't stop and wait here.\r\n2. Now Spark continues the execution of the query. If there is shuffle stage independent to the broadcast, it can be scheduled to run in parallel if there are enough resources in the cluster.\r\n3. Only if Spark calls `executeBroadcast` of the broadcast's query plan, we really stop and wait for the broadcast result.\r\n\r\nIs it clear to you?\r\n",
        "createdAt" : "2021-01-19T08:47:30Z",
        "updatedAt" : "2021-01-19T08:47:30Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "6a77d5d1-43ca-45a9-8c86-b8fc26491139",
        "parentId" : "42dacf4e-16e3-4429-82f8-b6e2dc37a7fd",
        "authorId" : "21ccf86e-7d54-44b8-aa50-2ad0a352ad3b",
        "body" : "> No, it maybe true for your run. In your case, maybe all resources are occupied by the broadcast. So no other job can be really scheduled to executors. But logically we don't set this limitation. I repeat, if there are enough resources, broadcast job and shuffle stage can be run in parallel.\r\n\r\nI can ensure there's enough resources to run broadcast and shuffle in parallel. Actually, there haven't come to TaskScheduler, \"sc.runJob\" is called after the broadcast finished.\r\n\r\n> 1. During preparing the SparkPlan, broadcast job is submitted to run, right? And we don't stop and wait here.\r\n>2. Now Spark continues the execution of the query. If there is shuffle stage independent to the broadcast, it can be scheduled to run in parallel if there are enough resources in the cluster.\r\n>3. Only if Spark calls executeBroadcast of the broadcast's query plan, we really stop and wait for the broadcast result.\r\n\r\nI agree with you for 1 and 3, but cannot agree with you for 2. In non-AQE, break a job into stages is happened in DAGScheduler, it is after physical plan has been transformed to RDD. I don't think DAGScheduler can call the executeBroadcast, which is part of spark-sql.\r\n\r\nBelow is the physical plan.\r\n\r\n== Physical Plan ==\r\n* Project (13)\r\n+- * BroadcastHashJoin Inner BuildRight (12)\r\n   :- * HashAggregate (6)\r\n   :  +- Exchange (5)\r\n   :     +- * HashAggregate (4)\r\n   :        +- * Project (3)\r\n   :           +- * SerializeFromObject (2)\r\n   :              +- Scan (1)\r\n   +- BroadcastExchange (11)\r\n      +- Coalesce (10)\r\n         +- * Project (9)\r\n            +- * SerializeFromObject (8)\r\n               +- Scan (7)\r\n\r\n![image](https://user-images.githubusercontent.com/3882710/105019167-c3124400-5a80-11eb-89b4-6145ec71525c.png)\r\n\r\n\r\nYou can see the submission time in the screenshot.\r\n\r\n![image](https://user-images.githubusercontent.com/3882710/105017437-ba207300-5a7e-11eb-9365-5fee24f27ea7.png)\r\n\r\nHere's the two stages for the result job. You can see stage 1 ( shuffle write map task ) is submitted after stage 0 (broadcast). We only need the broadcast result in stage 2.\r\n![image](https://user-images.githubusercontent.com/3882710/105017527-d58b7e00-5a7e-11eb-83f7-41babce76efc.png)\r\n\r\nThere are 4 cores and only 1 task for broadcast job:\r\n![image](https://user-images.githubusercontent.com/3882710/105017619-f653d380-5a7e-11eb-9e57-9fc92d458a31.png)\r\n\r\n\r\n",
        "createdAt" : "2021-01-19T09:58:13Z",
        "updatedAt" : "2021-01-19T10:04:28Z",
        "lastEditedBy" : "21ccf86e-7d54-44b8-aa50-2ad0a352ad3b",
        "tags" : [
        ]
      },
      {
        "id" : "2065d2a9-286c-47fc-b3e0-2c91b6c4dfdb",
        "parentId" : "42dacf4e-16e3-4429-82f8-b6e2dc37a7fd",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Ok, I see your point.\r\n\r\n> I can ensure there's enough resources to run broadcast and shuffle in parallel. Actually, there haven't come to TaskScheduler, \"sc.runJob\" is called after the broadcast finished.\r\n\r\nIn short, `submitMapState` can be used to submit a job too. Although seems it is only called now in AQE. I'm surprised that we don't do it similarly in normal query.\r\n\r\n> In non-AQE, we always wait the broadcast finish before submit shuffle map tasks.\r\n\r\nI think this isn't true because we don't forcibly wait broadcast to be finished like the change here. Technically, Spark doesn't restrict a shuffle to be run in parallel with a broadcast. An operator can call `submitMapState` to trigger a shuffle map stage if it wants, and the map stage can be run in parallel with broadcast.\r\n\r\nYou could say in non-AQE currently shuffle map tasks are submitted after the broadcast finishes. But not we always wait for the broadcast.\r\n",
        "createdAt" : "2021-01-19T19:01:12Z",
        "updatedAt" : "2021-01-19T19:01:12Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "a04c5ad1-a983-47c8-b6de-ffe2e4a5e437",
        "parentId" : "42dacf4e-16e3-4429-82f8-b6e2dc37a7fd",
        "authorId" : "21ccf86e-7d54-44b8-aa50-2ad0a352ad3b",
        "body" : "> In short, submitMapState can be used to submit a job too. Although seems it is only called now in AQE. I'm surprised that we don't do it similarly in normal query.\r\n\r\nsubmitMapState in DAGScheduler is introduced in SPARK-9851 just for AQE propose. I also think it can be used in normal query. I guess the reason we don't is there is less benefit as the complex it introduced. The architecture of sql execution should be changed just like AQE, one job will be break to many job, etc..\r\n\r\n> You could say in non-AQE currently shuffle map tasks are submitted after the broadcast finishes. But not we always wait for the broadcast.\r\n\r\nIt seems like the same thing. But we can make it better in AQE. If we don't like the solution, we can use the initial one  as partial fix and find a good solution, as discussed with @cloud-fan .",
        "createdAt" : "2021-01-20T02:45:17Z",
        "updatedAt" : "2021-01-20T02:45:17Z",
        "lastEditedBy" : "21ccf86e-7d54-44b8-aa50-2ad0a352ad3b",
        "tags" : [
        ]
      },
      {
        "id" : "99415d77-aba7-465d-9251-5a29cc64aa08",
        "parentId" : "42dacf4e-16e3-4429-82f8-b6e2dc37a7fd",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "I don't think we should block shuffle stages manually and wait for broadcast stages. The reason is like what @cloud-fan already mentioned. The first change looks better although it is not perfect too.",
        "createdAt" : "2021-01-20T04:15:13Z",
        "updatedAt" : "2021-01-20T04:15:13Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "6bc38f07783c68e51de9eaceb86350a9eb4747a9",
    "line" : 39,
    "diffHunk" : "@@ -1,1 +216,220 @@          }\n\n          // Wait for the materialization of all broadcast stages finish\n          broadcastMaterializationFutures.foreach(ThreadUtils.awaitReady(_, Duration.Inf))\n"
  },
  {
    "id" : "f7098345-850d-47f7-9bc8-2d1bfa21bdcb",
    "prId" : 30998,
    "prUrl" : "https://github.com/apache/spark/pull/30998#pullrequestreview-560850290",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6a3266de-f99d-4c55-87d1-6f6c597bc1a6",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Will this workaround be flaky? Does the order of calling `materialize` guarantee that the order of task to be scheduled to run?",
        "createdAt" : "2021-01-04T07:40:26Z",
        "updatedAt" : "2021-01-06T08:41:41Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "a248321a-d74f-402e-8bd8-fbed459e1dbb",
        "parentId" : "6a3266de-f99d-4c55-87d1-6f6c597bc1a6",
        "authorId" : "21ccf86e-7d54-44b8-aa50-2ad0a352ad3b",
        "body" : "As for flaky, I admit a little bit. \r\nI believe the order of calling `materialize` can guarantee that the order of task to be scheduled in normal circumstances, but, to be honest, this is guarantee is not strict since the submit of broadcast job and shuffle map stage(job) are in different thread. But, at least we reach the same level as non AQE.",
        "createdAt" : "2021-01-04T08:11:21Z",
        "updatedAt" : "2021-01-06T08:41:41Z",
        "lastEditedBy" : "21ccf86e-7d54-44b8-aa50-2ad0a352ad3b",
        "tags" : [
        ]
      },
      {
        "id" : "174c9291-ea2e-485f-9170-791b2886def6",
        "parentId" : "6a3266de-f99d-4c55-87d1-6f6c597bc1a6",
        "authorId" : "21ccf86e-7d54-44b8-aa50-2ad0a352ad3b",
        "body" : "I am also considering a hard guarantee, but it seems the changes are too large.",
        "createdAt" : "2021-01-04T08:17:17Z",
        "updatedAt" : "2021-01-06T08:41:41Z",
        "lastEditedBy" : "21ccf86e-7d54-44b8-aa50-2ad0a352ad3b",
        "tags" : [
        ]
      },
      {
        "id" : "c02d064f-dfce-458d-a811-adbdb7b9e508",
        "parentId" : "6a3266de-f99d-4c55-87d1-6f6c597bc1a6",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Yea, in non-AQE we always run broadcast jobs first.",
        "createdAt" : "2021-01-04T08:33:13Z",
        "updatedAt" : "2021-01-06T08:41:41Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "7cbeb14272833ac5d7e5aecc81f93c2c3a5cbadf",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +192,196 @@          // SPARK-33933: we should submit tasks of broadcast stages first, to avoid waiting\n          // for tasks to be scheduled and leading to broadcast timeout.\n          val reorderedNewStages = result.newStages\n            .sortWith {\n              case (_: BroadcastQueryStageExec, _: BroadcastQueryStageExec) => false"
  },
  {
    "id" : "96912b37-4eab-42e4-aec6-2335265ce1e1",
    "prId" : 29797,
    "prUrl" : "https://github.com/apache/spark/pull/29797#pullrequestreview-498500330",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7c92ce8e-7dc9-4aaf-8cf7-e1d3c00767f0",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "let's add comments to explain it.",
        "createdAt" : "2020-09-29T13:58:53Z",
        "updatedAt" : "2020-10-14T14:40:44Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "d3912696b1ae53efaf4b154236d6b3cc046768d6",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +110,114 @@        // SPARK-32932: Local shuffle reader could break partitioning that works best\n        // for the following writing command\n        queryStageOptimizerRules.filterNot(_.isInstanceOf[OptimizeLocalShuffleReader])\n      case _ =>\n        queryStageOptimizerRules"
  },
  {
    "id" : "890a0ee3-8f97-403b-8d9e-dae870139879",
    "prId" : 29774,
    "prUrl" : "https://github.com/apache/spark/pull/29774#pullrequestreview-490286074",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "94f80ef0-efeb-4b2d-a380-6e17154cd856",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "qq: should we remove `spark.sql.adaptive.logLevel`, @cloud-fan, @maryannxue, and @maropu? ",
        "createdAt" : "2020-09-17T04:13:12Z",
        "updatedAt" : "2020-09-17T06:33:14Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "42a8bee8-4c43-48b2-be05-0d7b96c2c099",
        "parentId" : "94f80ef0-efeb-4b2d-a380-6e17154cd856",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I think they are different. `spark.sql.adaptive.logLevel` controls the logging of plan changes for each AQE re-optimization round. While this PR is to log plan change before/after each rule.",
        "createdAt" : "2020-09-17T04:29:12Z",
        "updatedAt" : "2020-09-17T06:33:14Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "6f2ba5d1-e513-42c9-b18a-cffe77582d2e",
        "parentId" : "94f80ef0-efeb-4b2d-a380-6e17154cd856",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Looking around the code, logging granularity looks different as @cloud-fan said. A bit confusing though, I think its okay as it is.",
        "createdAt" : "2020-09-17T06:44:02Z",
        "updatedAt" : "2020-09-17T06:44:03Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "e29626fc0e96f151c30d090ed34e5ac46e61aa11",
    "line" : 29,
    "diffHunk" : "@@ -1,1 +232,236 @@        result = createQueryStages(currentPhysicalPlan)\n      }\n\n      // Run the final plan when there's no more unfinished stages.\n      currentPhysicalPlan = applyPhysicalRules("
  },
  {
    "id" : "18a9bd65-d026-4c40-b502-6f2002a91bfa",
    "prId" : 29262,
    "prUrl" : "https://github.com/apache/spark/pull/29262#pullrequestreview-456026577",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5556c411-f244-4431-a9b7-6efae3f34955",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "it's important to hide the exchange nodes from the stage optimizer rules, as that's the assumption of these rules.\r\n\r\nThen we don't need https://github.com/apache/spark/pull/29134/files#diff-a30c7a6fcdcdd13e57135fd04d05f3b7R115",
        "createdAt" : "2020-07-27T18:21:07Z",
        "updatedAt" : "2020-07-29T14:36:07Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "50175d75fe8bb0121d2f77f49a909212651c1035",
    "line" : 26,
    "diffHunk" : "@@ -1,1 +381,385 @@\n  private def newQueryStage(e: Exchange): QueryStageExec = {\n    val optimizedPlan = applyPhysicalRules(e.child, queryStageOptimizerRules)\n    val queryStage = e match {\n      case s: ShuffleExchangeLike =>"
  },
  {
    "id" : "b71c1874-dc19-46bf-b484-82cd682e3ff2",
    "prId" : 29262,
    "prUrl" : "https://github.com/apache/spark/pull/29262#pullrequestreview-456804136",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5fa3fbab-4304-4685-b359-dc7cbbac1e5f",
        "parentId" : null,
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "this should be able to stay ShuffleExchangeExec or be consistent and change braodcast below to be BroadcastExchangeLike",
        "createdAt" : "2020-07-28T16:25:24Z",
        "updatedAt" : "2020-07-29T14:36:07Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "50175d75fe8bb0121d2f77f49a909212651c1035",
    "line" : 32,
    "diffHunk" : "@@ -1,1 +383,387 @@    val optimizedPlan = applyPhysicalRules(e.child, queryStageOptimizerRules)\n    val queryStage = e match {\n      case s: ShuffleExchangeLike =>\n        val newShuffle = applyPhysicalRules(\n          s.withNewChildren(Seq(optimizedPlan)), postStageCreationRules)"
  },
  {
    "id" : "1dbc8a14-dd45-4565-bfda-d529730a2a6f",
    "prId" : 29262,
    "prUrl" : "https://github.com/apache/spark/pull/29262#pullrequestreview-457585413",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2ed91aca-55db-4469-bf2e-e015b6b516b3",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Do we need to make sure `newShuffle` is still `ShuffleExchangeLike`? Seems custom rules possibly break it?",
        "createdAt" : "2020-07-29T00:41:35Z",
        "updatedAt" : "2020-07-29T14:36:07Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "6286b4aa-57d6-4818-890d-1a89a2cc7cd4",
        "parentId" : "2ed91aca-55db-4469-bf2e-e015b6b516b3",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "added a check",
        "createdAt" : "2020-07-29T14:34:43Z",
        "updatedAt" : "2020-07-29T14:36:07Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "50175d75fe8bb0121d2f77f49a909212651c1035",
    "line" : 39,
    "diffHunk" : "@@ -1,1 +390,394 @@            \"Custom columnar rules cannot transform shuffle node to something else.\")\n        }\n        ShuffleQueryStageExec(currentStageId, newShuffle)\n      case b: BroadcastExchangeLike =>\n        val newBroadcast = applyPhysicalRules("
  },
  {
    "id" : "45a9977d-8860-433d-a100-25f075997be0",
    "prId" : 29224,
    "prUrl" : "https://github.com/apache/spark/pull/29224#pullrequestreview-468138593",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6a3e3fbd-6d2c-40d5-891f-adbbcceb9e88",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "It sounds like this function should be moved out of the physical node `AdaptiveSparkPlanExec`? cc @maryannxue @cloud-fan ",
        "createdAt" : "2020-08-17T02:58:15Z",
        "updatedAt" : "2020-08-17T02:58:36Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      }
    ],
    "commit" : "182d4f93e20a359ea42620eaf054b3d74c1ab746",
    "line" : 2,
    "diffHunk" : "@@ -1,1 +89,93 @@  // plan should reach a final status of query stages (i.e., no more addition or removal of\n  // Exchange nodes) after running these rules.\n  private def queryStagePreparationRules: Seq[Rule[SparkPlan]] = Seq(\n    ensureRequirements\n  ) ++ context.session.sessionState.queryStagePrepRules"
  },
  {
    "id" : "b02d23e0-5586-49f1-8497-28dba0bb8567",
    "prId" : 29224,
    "prUrl" : "https://github.com/apache/spark/pull/29224#pullrequestreview-468282783",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c562e26d-eec1-4ae8-ab0e-702e0377d99b",
        "parentId" : null,
        "authorId" : "80d631a6-73e8-46a6-a01b-a80d1f1cc6cc",
        "body" : "Should we do this after ensure requirements? The `queryStagePrepRules` might break requirements.",
        "createdAt" : "2020-08-17T08:03:41Z",
        "updatedAt" : "2020-08-17T08:03:41Z",
        "lastEditedBy" : "80d631a6-73e8-46a6-a01b-a80d1f1cc6cc",
        "tags" : [
        ]
      },
      {
        "id" : "2552ae74-1138-457c-b044-edccbf9892cb",
        "parentId" : "c562e26d-eec1-4ae8-ab0e-702e0377d99b",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "The custom rules may need to see exchange nodes as well. We can do validation at the end to make sure exchange requirements are still required.",
        "createdAt" : "2020-08-17T08:52:51Z",
        "updatedAt" : "2020-08-17T08:52:51Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "182d4f93e20a359ea42620eaf054b3d74c1ab746",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +91,95 @@  private def queryStagePreparationRules: Seq[Rule[SparkPlan]] = Seq(\n    ensureRequirements\n  ) ++ context.session.sessionState.queryStagePrepRules\n\n  // A list of physical optimizer rules to be applied to a new stage before its execution. These"
  },
  {
    "id" : "7eb5ab60-e365-434d-8afe-bb2365d2481f",
    "prId" : 29137,
    "prUrl" : "https://github.com/apache/spark/pull/29137#pullrequestreview-462072415",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ef24039f-7890-4a16-9f3f-9a54ad3ef0d6",
        "parentId" : null,
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "> Why it's Current Plan not Final Plan in EXPLAIN FORMATTED\r\n\r\n@cloud-fan I think it is from here.\r\n@allisonwang-db Could you update the PR description to explain the difference?",
        "createdAt" : "2020-08-05T06:35:18Z",
        "updatedAt" : "2020-08-06T17:12:18Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "43c432e5-01d9-42d3-a637-3bfdf934d8b8",
        "parentId" : "ef24039f-7890-4a16-9f3f-9a54ad3ef0d6",
        "authorId" : "b3ba992a-312c-46eb-b3c3-8d861d15ac40",
        "body" : "Since `EXPLAIN FORMATTED` will not actually execute the query, we use `Current Plan` to indicate the *current* query plan, and when the current adaptive query plan is final, we will use `Final Plan`.",
        "createdAt" : "2020-08-05T22:41:32Z",
        "updatedAt" : "2020-08-06T17:12:18Z",
        "lastEditedBy" : "b3ba992a-312c-46eb-b3c3-8d861d15ac40",
        "tags" : [
        ]
      }
    ],
    "commit" : "57de5e501a0aa666544fd42bbacd0eb9c80f1c91",
    "line" : 52,
    "diffHunk" : "@@ -1,1 +294,298 @@      indent)\n    generateTreeStringWithHeader(\n      if (isFinalPlan) \"Final Plan\" else \"Current Plan\",\n      currentPhysicalPlan,\n      depth,"
  },
  {
    "id" : "d51fe3fd-63f6-4afb-9ae3-fff8bd51f6eb",
    "prId" : 29134,
    "prUrl" : "https://github.com/apache/spark/pull/29134#pullrequestreview-450352664",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "225824b3-7c82-4977-8702-c09ace293e6c",
        "parentId" : null,
        "authorId" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "body" : "How can we guarantee the top node is still an Exchange after applying physical rules?",
        "createdAt" : "2020-07-17T03:34:36Z",
        "updatedAt" : "2020-07-17T03:34:36Z",
        "lastEditedBy" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "tags" : [
        ]
      },
      {
        "id" : "1101bd22-2ddb-4acc-a55c-b9c8ad55fc5d",
        "parentId" : "225824b3-7c82-4977-8702-c09ace293e6c",
        "authorId" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "body" : "I think it's safer to use `s.withNewChildren(optimizedChildPlan)` than adding special handling in those physical rules: https://github.com/apache/spark/pull/29134/files#diff-a30c7a6fcdcdd13e57135fd04d05f3b7R115-R117\r\n\r\nThat saves you the trouble of worrying about certain assumptions being broken in an arbitrary rule.",
        "createdAt" : "2020-07-17T03:39:20Z",
        "updatedAt" : "2020-07-17T03:39:20Z",
        "lastEditedBy" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "tags" : [
        ]
      }
    ],
    "commit" : "2fd2c089c91df76d9ab2958e788853e954c9eb8d",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +377,381 @@    // apply optimizer rules to the Exchange node and its children, allowing plugins to be\n    // able to replace the Exchange node itself\n    val optimizedPlan = applyPhysicalRules(e, queryStageOptimizerRules)\n    val queryStage = e match {\n      case _: ShuffleExchange =>"
  },
  {
    "id" : "77df58f4-bb2c-4a46-b3aa-7f37c27c3a44",
    "prId" : 28247,
    "prUrl" : "https://github.com/apache/spark/pull/28247#pullrequestreview-395722692",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c3becfd0-6bf9-4e09-90bd-1c0f7c91ee93",
        "parentId" : null,
        "authorId" : "80d631a6-73e8-46a6-a01b-a80d1f1cc6cc",
        "body" : "Maybe add a comment why this is needed. Something in the spirit of `Adaptive execution can replan a query outside of withActive scoped functions (e.g. plan.queryExecution.rdd and that is why we need to set it here.`",
        "createdAt" : "2020-04-17T19:36:34Z",
        "updatedAt" : "2020-04-17T22:57:35Z",
        "lastEditedBy" : "80d631a6-73e8-46a6-a01b-a80d1f1cc6cc",
        "tags" : [
        ]
      }
    ],
    "commit" : "877f6e13cb463f5e0e84c67964f0b89961b075b5",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +145,149 @@    // `plan.queryExecution.rdd`, we need to set active session here as new plan nodes can be\n    // created in the middle of the execution.\n    context.session.withActive {\n      // Subqueries do not have their own execution IDs and therefore rely on the main query to\n      // update UI."
  },
  {
    "id" : "6c234a02-0a15-40b4-a0c6-c496bd9369e8",
    "prId" : 27752,
    "prUrl" : "https://github.com/apache/spark/pull/27752#pullrequestreview-368504545",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "38f25ad7-7d58-4168-9b06-e762ec6e627e",
        "parentId" : null,
        "authorId" : "1b84a7ff-6bf9-4417-bf9f-e46e997e5974",
        "body" : "If we remove `and the cause is ${ex.getMessage}`, can we get the failed reason ?",
        "createdAt" : "2020-03-03T01:23:27Z",
        "updatedAt" : "2020-03-03T15:29:27Z",
        "lastEditedBy" : "1b84a7ff-6bf9-4417-bf9f-e46e997e5974",
        "tags" : [
        ]
      },
      {
        "id" : "cf1287cf-efb0-42b1-bcca-cda6a87d2ee7",
        "parentId" : "38f25ad7-7d58-4168-9b06-e762ec6e627e",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Yes, we can. `Throwable` will print the full stack of cause if any.",
        "createdAt" : "2020-03-03T01:39:20Z",
        "updatedAt" : "2020-03-03T15:29:27Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "74ad5bc5-667d-404a-be39-712f7df7eb42",
        "parentId" : "38f25ad7-7d58-4168-9b06-e762ec6e627e",
        "authorId" : "1b84a7ff-6bf9-4417-bf9f-e46e997e5974",
        "body" : "Yes, but the exception message does not contain the reason, right? If so, when we enable AQE, some exception check ut may fail. The aim to add `and the cause is ${ex.getMessage}` is also to resolve the failed exception check ut when enbale AQE. ",
        "createdAt" : "2020-03-04T03:10:20Z",
        "updatedAt" : "2020-03-04T03:10:20Z",
        "lastEditedBy" : "1b84a7ff-6bf9-4417-bf9f-e46e997e5974",
        "tags" : [
        ]
      },
      {
        "id" : "f4f05641-e9a8-4436-94c1-22c43f2ce57d",
        "parentId" : "38f25ad7-7d58-4168-9b06-e762ec6e627e",
        "authorId" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "body" : "This is when we need to change the tests.",
        "createdAt" : "2020-03-04T03:21:33Z",
        "updatedAt" : "2020-03-04T03:21:33Z",
        "lastEditedBy" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "tags" : [
        ]
      },
      {
        "id" : "fcafa30a-e49d-460b-8e39-05c606e572a0",
        "parentId" : "38f25ad7-7d58-4168-9b06-e762ec6e627e",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Note: this is the java standard to set the \"cause\" exception as the \"cause\", instead of embedding its error message in the current exception.",
        "createdAt" : "2020-03-04T04:57:05Z",
        "updatedAt" : "2020-03-04T04:57:06Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "654d9d33bbffdaee7d818f938dd6a1f271208c0d",
    "line" : 27,
    "diffHunk" : "@@ -1,1 +196,200 @@          case StageFailure(stage, ex) =>\n            errors.append(\n              new SparkException(s\"Failed to materialize query stage: ${stage.treeString}.\", ex))\n        }\n"
  },
  {
    "id" : "e1697b4c-f9d8-4e10-ab23-4dcfb006f6ab",
    "prId" : 27260,
    "prUrl" : "https://github.com/apache/spark/pull/27260#pullrequestreview-346650216",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "747ea516-cff8-4529-837d-ae114c4ed64a",
        "parentId" : null,
        "authorId" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "body" : "Can we either use map/flatMap or pass this `metrics` array in `collectSQLMetrics` ?",
        "createdAt" : "2020-01-22T14:56:15Z",
        "updatedAt" : "2020-01-22T14:56:16Z",
        "lastEditedBy" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "tags" : [
        ]
      }
    ],
    "commit" : "18df09a4280a5506e021ed96c024aa3075b49080",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +138,142 @@    plan.foreach {\n      case p: ShuffleQueryStageExec if (p.resultOption.isEmpty) =>\n        collectSQLMetrics(p.plan).foreach(metrics += _)\n      case p: BroadcastQueryStageExec if (p.resultOption.isEmpty) =>\n        collectSQLMetrics(p.plan).foreach(metrics += _)"
  },
  {
    "id" : "263ac748-b7e1-484a-ac2a-6fa3742a55ae",
    "prId" : 27260,
    "prUrl" : "https://github.com/apache/spark/pull/27260#pullrequestreview-346653080",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9f95182a-e0b4-485d-aae6-9ad65399f9b5",
        "parentId" : null,
        "authorId" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "body" : "nit: Why do we need this parameter `sqlMetrics: Seq[SQLMetric]` ? Can't we just pass `executionId`. And IMO, I don't even think this extra method is necessary at this point.",
        "createdAt" : "2020-01-22T14:59:45Z",
        "updatedAt" : "2020-01-22T14:59:46Z",
        "lastEditedBy" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "tags" : [
        ]
      }
    ],
    "commit" : "18df09a4280a5506e021ed96c024aa3075b49080",
    "line" : 67,
    "diffHunk" : "@@ -1,1 +512,516 @@  }\n\n  private def onUpdateSQLMetrics(sqlMetrics: Seq[SQLMetric], executionId: Long): Unit = {\n    val sqlPlanMetrics = sqlMetrics.map { case sqlMetric =>\n      SQLPlanMetric(sqlMetric.name.get, sqlMetric.id, sqlMetric.metricType)"
  },
  {
    "id" : "db062648-e12a-4806-8717-5d9a9fdd9551",
    "prId" : 26968,
    "prUrl" : "https://github.com/apache/spark/pull/26968#pullrequestreview-335781166",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3593f733-131d-4e41-a849-ca73c00a4eeb",
        "parentId" : null,
        "authorId" : "334a081c-cba1-403e-ac80-30e6124077d1",
        "body" : "Why is the plan sent out before `isFinalPlan = true` ? Isn't it the final plan ?",
        "createdAt" : "2019-12-23T07:25:47Z",
        "updatedAt" : "2019-12-23T07:25:47Z",
        "lastEditedBy" : "334a081c-cba1-403e-ac80-30e6124077d1",
        "tags" : [
        ]
      },
      {
        "id" : "1e2b0824-c59d-41ad-8588-19e2b8e26285",
        "parentId" : "3593f733-131d-4e41-a849-ca73c00a4eeb",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "It doesn't matter. `onUpdatePlan` only look at `currentPhysicalPlan`, so we must call `onUpdatePlan` after setting `currentPhysicalPlan`.",
        "createdAt" : "2019-12-23T07:28:01Z",
        "updatedAt" : "2019-12-23T07:28:01Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "4d527754-dbba-4c93-894b-2cf236282b07",
        "parentId" : "3593f733-131d-4e41-a849-ca73c00a4eeb",
        "authorId" : "334a081c-cba1-403e-ac80-30e6124077d1",
        "body" : "Does it matter for what's displayed on UI or anyone interested in sql update event ?",
        "createdAt" : "2019-12-23T07:55:59Z",
        "updatedAt" : "2019-12-23T07:56:00Z",
        "lastEditedBy" : "334a081c-cba1-403e-ac80-30e6124077d1",
        "tags" : [
        ]
      },
      {
        "id" : "999a168f-7d35-4216-90f2-38c07fb36a6a",
        "parentId" : "3593f733-131d-4e41-a849-ca73c00a4eeb",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "what do you mean? Even if you move `isFinalPlan = true` before this line, nothing gets changed.",
        "createdAt" : "2019-12-23T08:00:18Z",
        "updatedAt" : "2019-12-23T08:00:18Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "94977865-1627-4eed-b6df-834fdcccbcdb",
        "parentId" : "3593f733-131d-4e41-a849-ca73c00a4eeb",
        "authorId" : "334a081c-cba1-403e-ac80-30e6124077d1",
        "body" : "The root nodes of `Physical Plan` in `physicalPlanDescription`s  are different: `AdaptiveSparkPlan(isFinalPlan=true)` vs `AdaptiveSparkPlan(isFinalPlan=false)`. So are root nodes of `sparkPlan`",
        "createdAt" : "2019-12-23T08:11:11Z",
        "updatedAt" : "2019-12-23T08:11:11Z",
        "lastEditedBy" : "334a081c-cba1-403e-ac80-30e6124077d1",
        "tags" : [
        ]
      },
      {
        "id" : "df6a625f-e598-4218-9a91-c83379d5bf3d",
        "parentId" : "3593f733-131d-4e41-a849-ca73c00a4eeb",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "ah good catch! somehow I misread the code and thought it doesn't matter.\r\n\r\nFor UI it doesn't matter as we will strip the `AdaptiveSparkPlanExec`, but if there is a spark listener catching this event, it matters.\r\n\r\nCan you send a PR to fix it? Thanks!",
        "createdAt" : "2019-12-23T08:16:11Z",
        "updatedAt" : "2019-12-23T08:16:11Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "778ec994-c2b8-431f-8dea-4fa10d893cfe",
        "parentId" : "3593f733-131d-4e41-a849-ca73c00a4eeb",
        "authorId" : "334a081c-cba1-403e-ac80-30e6124077d1",
        "body" : "Sure. Thanks for your reply.",
        "createdAt" : "2019-12-23T08:52:09Z",
        "updatedAt" : "2019-12-23T08:52:09Z",
        "lastEditedBy" : "334a081c-cba1-403e-ac80-30e6124077d1",
        "tags" : [
        ]
      },
      {
        "id" : "b8e83be2-f7ec-4678-ba4d-64dfc9a4171e",
        "parentId" : "3593f733-131d-4e41-a849-ca73c00a4eeb",
        "authorId" : "334a081c-cba1-403e-ac80-30e6124077d1",
        "body" : "I've sent https://github.com/apache/spark/pull/26983. Please review.",
        "createdAt" : "2019-12-23T09:33:25Z",
        "updatedAt" : "2019-12-23T09:33:26Z",
        "lastEditedBy" : "334a081c-cba1-403e-ac80-30e6124077d1",
        "tags" : [
        ]
      }
    ],
    "commit" : "566918cec3e8c1051d602376bc30fbfcea460b3b",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +206,210 @@      // Run the final plan when there's no more unfinished stages.\n      currentPhysicalPlan = applyPhysicalRules(result.newPlan, queryStageOptimizerRules)\n      executionId.foreach(onUpdatePlan)\n      isFinalPlan = true\n      logDebug(s\"Final plan: $currentPhysicalPlan\")"
  },
  {
    "id" : "017187e1-7156-4118-a2e5-e9638b5eaeb2",
    "prId" : 26952,
    "prUrl" : "https://github.com/apache/spark/pull/26952#pullrequestreview-335060692",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1f8496c6-5240-47b0-8a90-544351302c67",
        "parentId" : null,
        "authorId" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "body" : "Since materialization status is updated single-threaded, I think we might be safe to copy the status, i.e., `resultOption`, here as well. Let's do it in another PR?",
        "createdAt" : "2019-12-19T15:28:05Z",
        "updatedAt" : "2019-12-19T15:28:05Z",
        "lastEditedBy" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "tags" : [
        ]
      },
      {
        "id" : "91212bc4-e76e-4b03-be0f-9c821b166db1",
        "parentId" : "1f8496c6-5240-47b0-8a90-544351302c67",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "sounds reasonable. Previously the `ReusedQueryStage` doesn't copy `resultOption` either, so I just followed it. We can improve it in a followup",
        "createdAt" : "2019-12-20T04:11:52Z",
        "updatedAt" : "2019-12-20T04:11:53Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "e95c1824574a17930a1e2c448ffba169ad4fb2ac",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +283,287 @@        case Some(existingStage) if conf.exchangeReuseEnabled =>\n          val stage = reuseQueryStage(existingStage, e)\n          // This is a leaf stage and is not materialized yet even if the reused exchange may has\n          // been completed. It will trigger re-optimization later and stage materialization will\n          // finish in instant if the underlying exchange is already completed."
  },
  {
    "id" : "dfa8a27e-3650-46d8-8ddc-308c778045d7",
    "prId" : 25456,
    "prUrl" : "https://github.com/apache/spark/pull/25456#pullrequestreview-276060678",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9673b1ed-2a2e-4aa9-975d-bb25ec24f39f",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "When we reach here, will `stagesToReplace` be non-empty?",
        "createdAt" : "2019-08-16T15:06:38Z",
        "updatedAt" : "2019-08-21T15:41:45Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "5ff8a860-2cb0-42d8-979d-0784eeffd493",
        "parentId" : "9673b1ed-2a2e-4aa9-975d-bb25ec24f39f",
        "authorId" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "body" : "Yes, one important idea i should have put in code comment here is:\r\n\r\nThe current logical plan is always updated together with the current physical plan, which means if a new physical plan is not adopted after re-optimization, the new logical plan (with stages replaced) is not taken either (https://github.com/apache/spark/pull/25456/files#diff-6954dd8020a9ca298f1fb9602c0e831cR181). That also means that the current logical plan is kind of behind the current status of the physical plan because the logical plan does not reflect the new stages created since last update (https://github.com/apache/spark/pull/25456/files#diff-6954dd8020a9ca298f1fb9602c0e831cR188). Yet we cannot update the logical plan alone, as all logical links of the current physical plan point to the original logical plan it is planned from. So as a fix for this \"out-of-date\" problem, we keep the logical plan together with this `stagesToReplace` list, and each time we re-optimize, we update the logical plan with those stages (that haven't been applied to it yet) first (https://github.com/apache/spark/pull/25456/files#diff-6954dd8020a9ca298f1fb9602c0e831cR177), and then start re-optimizing and re-planning on the updated logical plan. If the new physical plan is adopted, we take the new physical plan together with the new logical plan and clear the `stagesToReplace` list, otherwise, we keep the current logical plan and the list as they are.",
        "createdAt" : "2019-08-16T16:57:53Z",
        "updatedAt" : "2019-08-21T15:41:45Z",
        "lastEditedBy" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "tags" : [
        ]
      }
    ],
    "commit" : "a40c8b92c829817fd31a43a967f28f81d14c49ee",
    "line" : 47,
    "diffHunk" : "@@ -1,1 +144,148 @@        currentPhysicalPlan = result.newPlan\n        if (result.newStages.nonEmpty) {\n          stagesToReplace = result.newStages ++ stagesToReplace\n          executionId.foreach(onUpdatePlan)\n"
  },
  {
    "id" : "dcc12a78-a491-4c9a-834f-ba4971a48a38",
    "prId" : 25456,
    "prUrl" : "https://github.com/apache/spark/pull/25456#pullrequestreview-276063192",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "00a09a49-14ea-427f-9a8f-54e16b8b7940",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "will `currentPhysicalPlan.find(_.eq(stage))` be empty?",
        "createdAt" : "2019-08-16T15:08:56Z",
        "updatedAt" : "2019-08-21T15:41:45Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "d0b69344-4686-4bdd-8ee3-ed3dfbcd0c32",
        "parentId" : "00a09a49-14ea-427f-9a8f-54e16b8b7940",
        "authorId" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "body" : "Following this comment https://github.com/apache/spark/pull/25456/files#r314805998:\r\n\r\nNow that we might have `newStages` from different rounds of stage creation, some stages might have been included in newer stages already, so those are not \"reachable\" now and we don't need to worry about them any more. But meanwhile we need to make sure we always apply the latest stages first: https://github.com/apache/spark/pull/25456/files#diff-6954dd8020a9ca298f1fb9602c0e831cR142. ",
        "createdAt" : "2019-08-16T17:03:22Z",
        "updatedAt" : "2019-08-21T15:41:45Z",
        "lastEditedBy" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "tags" : [
        ]
      }
    ],
    "commit" : "a40c8b92c829817fd31a43a967f28f81d14c49ee",
    "line" : 217,
    "diffHunk" : "@@ -1,1 +377,381 @@    var logicalPlan = plan\n    stagesToReplace.foreach {\n      case stage if currentPhysicalPlan.find(_.eq(stage)).isDefined =>\n        val logicalNodeOpt = stage.getTagValue(TEMP_LOGICAL_PLAN_TAG).orElse(stage.logicalLink)\n        assert(logicalNodeOpt.isDefined)"
  },
  {
    "id" : "8632567f-e92f-482e-a500-440e12ea9530",
    "prId" : 25316,
    "prUrl" : "https://github.com/apache/spark/pull/25316#pullrequestreview-269837146",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4c3403b0-8046-457a-a3a7-f3e1f4bf1522",
        "parentId" : null,
        "authorId" : "80d631a6-73e8-46a6-a01b-a80d1f1cc6cc",
        "body" : "Can you add some doc on why this is needed. It is kinda annoying when you have to check the git blame to figure out why code is there.",
        "createdAt" : "2019-08-01T19:40:24Z",
        "updatedAt" : "2019-08-07T17:40:35Z",
        "lastEditedBy" : "80d631a6-73e8-46a6-a01b-a80d1f1cc6cc",
        "tags" : [
        ]
      }
    ],
    "commit" : "f5702810ac5e925e998331029f6927cfc94448e3",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +127,131 @@        val id = idStr.toLong\n        val qe = SQLExecution.getQueryExecution(id)\n        if (qe.eq(queryExecution)) Some(id) else None\n      }\n      var currentLogicalPlan = currentPhysicalPlan.logicalLink.get"
  }
]