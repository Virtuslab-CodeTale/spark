[
  {
    "id" : "06bc5896-e07d-41cc-bb09-740dcd41e20a",
    "prId" : 25600,
    "prUrl" : "https://github.com/apache/spark/pull/25600#pullrequestreview-280847972",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "06c75299-0fb9-4782-a0e3-c5e6c398c553",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Does this really help? once filter pushdown is applied, it's hard to make this rule idempotent. ",
        "createdAt" : "2019-08-28T06:43:47Z",
        "updatedAt" : "2019-09-04T14:47:15Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "04f5580a-6a79-49ab-822c-69512b300196",
        "parentId" : "06c75299-0fb9-4782-a0e3-c5e6c398c553",
        "authorId" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "body" : "This batch itself does not push down... but you have a point. Let's just keep it though.",
        "createdAt" : "2019-08-28T13:56:00Z",
        "updatedAt" : "2019-09-04T14:47:15Z",
        "lastEditedBy" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "tags" : [
        ]
      }
    ],
    "commit" : "b00225078c6471b5b01f8a920ab28a2361b8e48b",
    "line" : 201,
    "diffHunk" : "@@ -1,1 +199,203 @@  private def prune(plan: LogicalPlan): LogicalPlan = {\n    plan transformUp {\n      // skip this rule if there's already a DPP subquery on the LHS of a join\n      case j @ Join(Filter(_: DynamicPruningSubquery, _), _, _, _, _) => j\n      case j @ Join(_, Filter(_: DynamicPruningSubquery, _), _, _, _) => j"
  },
  {
    "id" : "7887c953-5ad6-43ed-b5ed-9015ef173628",
    "prId" : 25600,
    "prUrl" : "https://github.com/apache/spark/pull/25600#pullrequestreview-295998267",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d47ab68a-f5db-46c8-827b-e6498cb04c3e",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "what about `EqualNullSafe`?",
        "createdAt" : "2019-08-28T06:46:01Z",
        "updatedAt" : "2019-09-04T14:47:15Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "74ef4663-b4fa-4b55-aa98-da8342e8d8f8",
        "parentId" : "d47ab68a-f5db-46c8-827b-e6498cb04c3e",
        "authorId" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "body" : "Created a follow-up JIRA as https://issues.apache.org/jira/browse/SPARK-28959.",
        "createdAt" : "2019-09-03T14:40:18Z",
        "updatedAt" : "2019-09-04T14:47:15Z",
        "lastEditedBy" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "tags" : [
        ]
      },
      {
        "id" : "f8d74dd5-c6ea-404e-99d1-a219bee8035f",
        "parentId" : "d47ab68a-f5db-46c8-827b-e6498cb04c3e",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Just think about it. Is EqualNullSafe different to EqualTo here? Can partition column be a null value? If it can not be, when building key is null, EqualTo's value is null, EqualNullSafe's value is false. For a Filter predicate, null is considered a false, isn't?",
        "createdAt" : "2019-10-01T00:34:41Z",
        "updatedAt" : "2019-10-01T00:34:41Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "fa9b7261-b042-487e-85b2-03a097fad985",
        "parentId" : "d47ab68a-f5db-46c8-827b-e6498cb04c3e",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "partition column can be null, we will use a special string to represent null in the file path. ",
        "createdAt" : "2019-10-02T03:10:48Z",
        "updatedAt" : "2019-10-02T03:10:48Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "c3a096c8-598f-4b7e-89cf-092e9a6cac30",
        "parentId" : "d47ab68a-f5db-46c8-827b-e6498cb04c3e",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "oh, I see. `__HIVE_DEFAULT_PARTITION__`",
        "createdAt" : "2019-10-02T03:29:32Z",
        "updatedAt" : "2019-10-02T03:29:33Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "b00225078c6471b5b01f8a920ab28a2361b8e48b",
    "line" : 223,
    "diffHunk" : "@@ -1,1 +221,225 @@\n        splitConjunctivePredicates(condition).foreach {\n          case EqualTo(a: Expression, b: Expression)\n              if fromDifferentSides(a, b) =>\n            val (l, r) = if (a.references.subsetOf(left.outputSet) &&"
  },
  {
    "id" : "517ad6b7-c898-4316-a135-30f8ec77b1a2",
    "prId" : 25600,
    "prUrl" : "https://github.com/apache/spark/pull/25600#pullrequestreview-283654977",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8b12ea5e-3a0e-4fbb-902c-a9871b0e0a1e",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "> ... and a filter on the dimension table\r\n\r\nwhat if the dimension table itself is very small?",
        "createdAt" : "2019-08-28T06:53:13Z",
        "updatedAt" : "2019-09-04T14:47:15Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "ac4f3c0e-aaa0-40ef-8238-0d1c28e5e02c",
        "parentId" : "8b12ea5e-3a0e-4fbb-902c-a9871b0e0a1e",
        "authorId" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "body" : "If the dimension table is smaller than the table(s) on build side, the `pruningHasBenefit` will return false, which means:\r\n1) the filter can still be planned as a broadcast pruning filter if it's eventually a BHJ with the expected build plan here as the build side, and that broadcast will be a reuse so hopefully not much overhead anyway;\r\n2) otherwise, since `pruningHasBenefit` is false, this filter will be turned into a bypass filter (constant `true`), so nothing will happen.",
        "createdAt" : "2019-08-29T20:32:47Z",
        "updatedAt" : "2019-09-04T14:47:15Z",
        "lastEditedBy" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "tags" : [
        ]
      },
      {
        "id" : "cdbdcdae-b2ab-4fbb-92b2-d09067a0a247",
        "parentId" : "8b12ea5e-3a0e-4fbb-902c-a9871b0e0a1e",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "> the filter can still be planned as a broadcast pruning filter if it's eventually a BHJ\r\n\r\nBut from the code, we only add the filter if `hasPartitionPruningFilter(r, right)` is true, which requires a seletive predicate on the build side.",
        "createdAt" : "2019-09-04T07:45:00Z",
        "updatedAt" : "2019-09-04T14:47:15Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "c7ec79d4-b884-428d-8c5e-44fe99511c7b",
        "parentId" : "8b12ea5e-3a0e-4fbb-902c-a9871b0e0a1e",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "We can address it in a followup. The current check is more conservative.",
        "createdAt" : "2019-09-04T14:24:23Z",
        "updatedAt" : "2019-09-04T14:47:15Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "b00225078c6471b5b01f8a920ab28a2361b8e48b",
    "line" : 232,
    "diffHunk" : "@@ -1,1 +230,234 @@            }\n\n            // there should be a partitioned table and a filter on the dimension table,\n            // otherwise the pruning will not trigger\n            var partScan = getPartitionTableScan(l, left)"
  },
  {
    "id" : "c1ba9524-fabd-47c1-bfcf-8f08d6fa7303",
    "prId" : 25600,
    "prUrl" : "https://github.com/apache/spark/pull/25600#pullrequestreview-281692932",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "874ed546-2901-45d5-a267-5ba18e6da265",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "nit: might be better to write `class PartitionPruning(conf: SQLConf)` instead of writing a lot of `SQLConf.get` in this rule.",
        "createdAt" : "2019-08-28T07:04:10Z",
        "updatedAt" : "2019-09-04T14:47:15Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "a63c7791-44af-4ca4-8fb0-0363d353d557",
        "parentId" : "874ed546-2901-45d5-a267-5ba18e6da265",
        "authorId" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "body" : "I'd agree with you on that... but it gets confusing that a lot of optimization rules use SQLConf.get while others use conf as a param.",
        "createdAt" : "2019-08-29T20:06:15Z",
        "updatedAt" : "2019-09-04T14:47:15Z",
        "lastEditedBy" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "tags" : [
        ]
      }
    ],
    "commit" : "b00225078c6471b5b01f8a920ab28a2361b8e48b",
    "line" : 49,
    "diffHunk" : "@@ -1,1 +47,51 @@ *    (3) otherwise, we drop the subquery.\n */\nobject PartitionPruning extends Rule[LogicalPlan] with PredicateHelper {\n\n  /**"
  },
  {
    "id" : "590f6507-8965-4dd3-82f9-d73f0cf0b5d0",
    "prId" : 25600,
    "prUrl" : "https://github.com/apache/spark/pull/25600#pullrequestreview-281693651",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e15134fa-101b-4bee-8ee6-e3594cc844f2",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "hmm, when would users disable `dynamicPruningReuseBroadcast`?",
        "createdAt" : "2019-08-28T07:08:30Z",
        "updatedAt" : "2019-09-04T14:47:15Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "9f09946d-38d1-4386-bdb9-68236e3a46c2",
        "parentId" : "e15134fa-101b-4bee-8ee6-e3594cc844f2",
        "authorId" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "body" : "It's just a fallback option and it's an internal conf.",
        "createdAt" : "2019-08-29T20:07:49Z",
        "updatedAt" : "2019-09-04T14:47:15Z",
        "lastEditedBy" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "tags" : [
        ]
      }
    ],
    "commit" : "b00225078c6471b5b01f8a920ab28a2361b8e48b",
    "line" : 91,
    "diffHunk" : "@@ -1,1 +89,93 @@    val reuseEnabled = SQLConf.get.dynamicPartitionPruningReuseBroadcast\n    val index = joinKeys.indexOf(filteringKey)\n    if (hasBenefit || reuseEnabled) {\n      // insert a DynamicPruning wrapper to identify the subquery during query planning\n      Filter("
  },
  {
    "id" : "60ea0595-c6d9-4fec-b720-0740e81018c7",
    "prId" : 25600,
    "prUrl" : "https://github.com/apache/spark/pull/25600#pullrequestreview-283809556",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fd7d5203-9757-4160-beef-c1b5d31b5804",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "style: ident.",
        "createdAt" : "2019-09-04T18:42:26Z",
        "updatedAt" : "2019-09-04T19:10:15Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "b00225078c6471b5b01f8a920ab28a2361b8e48b",
    "line" : 79,
    "diffHunk" : "@@ -1,1 +77,81 @@   *    DynamicPruning expression that wraps a regular In expression\n   *  - we also insert a flag that indicates if the subquery duplication is worthwhile and it\n   *  should run regardless of the join strategy, or is too expensive and it should be run only if\n   *  we can reuse the results of a broadcast\n   */"
  },
  {
    "id" : "ae60d41c-e1b6-460d-a269-380be3dd5e24",
    "prId" : 25600,
    "prUrl" : "https://github.com/apache/spark/pull/25600#pullrequestreview-283842816",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2e802af5-7a0e-469b-819d-a9f7eb690b14",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Are we always be certain that the references from partExpr and otherExpr can match one by one?",
        "createdAt" : "2019-09-04T18:45:05Z",
        "updatedAt" : "2019-09-04T19:10:16Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "6daacded-1c23-4875-8db4-ced4fcf4ba40",
        "parentId" : "2e802af5-7a0e-469b-819d-a9f7eb690b14",
        "authorId" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "body" : "No, we can't. So we fall back to the configured ratio if they don't match or have multiple attributes.",
        "createdAt" : "2019-09-04T19:27:17Z",
        "updatedAt" : "2019-09-04T19:27:18Z",
        "lastEditedBy" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "tags" : [
        ]
      }
    ],
    "commit" : "b00225078c6471b5b01f8a920ab28a2361b8e48b",
    "line" : 129,
    "diffHunk" : "@@ -1,1 +127,131 @@    val fallbackRatio = SQLConf.get.dynamicPartitionPruningFallbackFilterRatio\n    // the filtering ratio based on the type of the join condition and on the column statistics\n    val filterRatio = (partExpr.references.toList, otherExpr.references.toList) match {\n      // filter out expressions with more than one attribute on any side of the operator\n      case (leftAttr :: Nil, rightAttr :: Nil)"
  },
  {
    "id" : "307e2105-6828-48d4-bc7f-9bf2f7f0a3ff",
    "prId" : 25600,
    "prUrl" : "https://github.com/apache/spark/pull/25600#pullrequestreview-283840713",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "58797dbb-b3fa-4908-bfb1-a7738f7fc2e7",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Don't we need to make sure the corresponding joining key r  is on the selective filter? seems hasPartitionPruningFilter only tells that right plan has a selective Filter, can't the Filter is selective on other join keys other than r?",
        "createdAt" : "2019-09-04T18:59:32Z",
        "updatedAt" : "2019-09-04T19:10:16Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "4cb8b95a-d0b5-4239-a2c5-c711d27ccb69",
        "parentId" : "58797dbb-b3fa-4908-bfb1-a7738f7fc2e7",
        "authorId" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "body" : "I don't think it matters. The reason why we need to check if we have a selective filter on the build plan is to make sure we have a rather small dataset to filter the partition columns with.",
        "createdAt" : "2019-09-04T19:23:07Z",
        "updatedAt" : "2019-09-04T19:23:07Z",
        "lastEditedBy" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "tags" : [
        ]
      }
    ],
    "commit" : "b00225078c6471b5b01f8a920ab28a2361b8e48b",
    "line" : 236,
    "diffHunk" : "@@ -1,1 +234,238 @@            var partScan = getPartitionTableScan(l, left)\n            if (partScan.isDefined && canPruneLeft(joinType) &&\n                hasPartitionPruningFilter(right)) {\n              val hasBenefit = pruningHasBenefit(l, partScan.get, r, right)\n              newLeft = insertPredicate(l, newLeft, r, right, rightKeys, hasBenefit)"
  },
  {
    "id" : "7fef0047-579a-4b4d-990c-d6da18aa6634",
    "prId" : 25600,
    "prUrl" : "https://github.com/apache/spark/pull/25600#pullrequestreview-296521777",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1a7ce739-d604-467c-9e09-4b3b4e0f5310",
        "parentId" : null,
        "authorId" : "a269db99-bf49-44c7-9b33-056e49f21bc5",
        "body" : "it's best if you can give an example here. otherwise it's pretty dry and difficult to understand",
        "createdAt" : "2019-10-02T20:57:29Z",
        "updatedAt" : "2019-10-02T20:57:29Z",
        "lastEditedBy" : "a269db99-bf49-44c7-9b33-056e49f21bc5",
        "tags" : [
        ]
      }
    ],
    "commit" : "b00225078c6471b5b01f8a920ab28a2361b8e48b",
    "line" : 29,
    "diffHunk" : "@@ -1,1 +27,31 @@\n/**\n * Dynamic partition pruning optimization is performed based on the type and\n * selectivity of the join operation. During query optimization, we insert a\n * predicate on the partitioned table using the filter from the other side of"
  }
]