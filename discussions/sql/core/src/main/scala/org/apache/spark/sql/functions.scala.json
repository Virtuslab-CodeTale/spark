[
  {
    "id" : "cc6997de-147e-4da6-adfb-e69084131c5c",
    "prId" : 31887,
    "prUrl" : "https://github.com/apache/spark/pull/31887#pullrequestreview-628356228",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1be7e7bb-d505-4242-9b59-6caac5ee8192",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "How about defining a companion object for UnresolvedNamedLambdaVariable like this?\r\n```\r\n\r\nobject UnresolvedNamedLambdaVariable {\r\n\r\n  // counter to ensure lambda variable names are unique\r\n  private val lambdaVarNameCounter = new AtomicInteger(0)\r\n\r\n  def apply(args: Seq[String]): UnresolvedNamedLambdaVariable = {\r\n    // add a counter number in the suffix\r\n  }\r\n}\r\n```",
        "createdAt" : "2021-04-06T01:08:16Z",
        "updatedAt" : "2021-04-06T01:10:48Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "8c2ad611bf6aa4a0acdbebc6f78248d0a2a6ac9e",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +3803,3807 @@\n  // counter to ensure lambda variable names are unique\n  private val lambdaVarNameCounter = new AtomicInteger(0)\n\n  private def createLambda(f: Column => Column) = {"
  },
  {
    "id" : "b98188e8-18f6-4991-8ceb-d7d84c625e8d",
    "prId" : 31408,
    "prUrl" : "https://github.com/apache/spark/pull/31408#pullrequestreview-580037781",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8694486d-3af3-4527-bd40-d9d89e12dedb",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "not `typed_lit` but `typedlit`?",
        "createdAt" : "2021-02-01T06:05:38Z",
        "updatedAt" : "2021-02-01T08:03:05Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "285bc6c2-5d89-4774-8be8-97ea714cb7c6",
        "parentId" : "8694486d-3af3-4527-bd40-d9d89e12dedb",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "For some reasons I like `typedlit` (like `datediff`, `shiftleft`, etc.). But I don't mind changing it back if other people prefer `typed_lit` more. It's not for SQL compliance and not in the function registry in any event so I think we can pick one.",
        "createdAt" : "2021-02-01T06:08:38Z",
        "updatedAt" : "2021-02-01T08:03:05Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "872891e3-c447-439c-95c6-0bd692f7dc54",
        "parentId" : "8694486d-3af3-4527-bd40-d9d89e12dedb",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Ah, I see. I just wanted to know the reason. The current one looks fine.",
        "createdAt" : "2021-02-01T06:11:36Z",
        "updatedAt" : "2021-02-01T08:03:05Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "414d833d06c33f2b0ec006327bbf96df69da7c83",
    "line" : 25,
    "diffHunk" : "@@ -1,1 +136,140 @@   * @since 3.2.0\n   */\n  def typedlit[T : TypeTag](literal: T): Column = literal match {\n    case c: Column => c\n    case s: Symbol => new ColumnName(s.name)"
  },
  {
    "id" : "07650833-9e53-42f2-86b3-0dd7158071c5",
    "prId" : 31408,
    "prUrl" : "https://github.com/apache/spark/pull/31408#pullrequestreview-580039605",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d3bcba7d-bb10-4054-9a8a-6fc1030a068c",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Could we use `@deprecated` for these aliases?",
        "createdAt" : "2021-02-01T06:07:49Z",
        "updatedAt" : "2021-02-01T08:03:05Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "1d2836bd-5432-4d17-b6ea-d1a67eb8bff0",
        "parentId" : "d3bcba7d-bb10-4054-9a8a-6fc1030a068c",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "oh I didn't intend to deprecate this (and `count_dictinct`). I will fix PR description.",
        "createdAt" : "2021-02-01T06:10:16Z",
        "updatedAt" : "2021-02-01T08:03:05Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "695b1209-e016-4a84-8409-82923fe1be6c",
        "parentId" : "d3bcba7d-bb10-4054-9a8a-6fc1030a068c",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Oh, I see. Yea, I misunderstood it from the word \"deprecate\" there.",
        "createdAt" : "2021-02-01T06:16:38Z",
        "updatedAt" : "2021-02-01T08:03:05Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "414d833d06c33f2b0ec006327bbf96df69da7c83",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +119,123 @@   * An alias of `typedlit`, and it is encouraged to use `typedlit` directly.\n   *\n   * @group normal_funcs\n   * @since 2.2.0\n   */"
  },
  {
    "id" : "72069cc3-de5c-48ce-9b72-9c02c33cd52b",
    "prId" : 31198,
    "prUrl" : "https://github.com/apache/spark/pull/31198#pullrequestreview-573387872",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "96fd3713-284b-4207-b39e-8527cca54fb6",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "let's copy the doc from the SQL expression.",
        "createdAt" : "2021-01-21T13:00:14Z",
        "updatedAt" : "2021-01-21T14:33:43Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "18301ac6-e8fe-40af-92c1-60706c831609",
        "parentId" : "96fd3713-284b-4207-b39e-8527cca54fb6",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "OK",
        "createdAt" : "2021-01-21T14:33:50Z",
        "updatedAt" : "2021-01-21T14:33:50Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "807783b4fc3543c98037ebe8bb4afb75be6af403",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1415,1419 @@\n  /**\n   * Get the value of the bit (0 or 1) at the specified position.\n   * The positions are numbered from right to left, starting at zero.\n   * The position argument cannot be negative."
  },
  {
    "id" : "80e19b04-d767-4557-b10c-507ca99d42f7",
    "prId" : 31073,
    "prUrl" : "https://github.com/apache/spark/pull/31073#pullrequestreview-564829762",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "24f7125d-7e42-4129-aaaf-2bd8c8d3dc48",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Can we expose in SparkR and PySpark too? If you're not used to it, we can run it separately too. cc @zero323 FYI",
        "createdAt" : "2021-01-07T05:42:10Z",
        "updatedAt" : "2021-01-26T03:25:46Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "73053f16-dba4-42b9-a4de-0bb5419d1aaf",
        "parentId" : "24f7125d-7e42-4129-aaaf-2bd8c8d3dc48",
        "authorId" : "8c82da5a-f351-4a37-a8a9-13809311b07b",
        "body" : "I recommend creating a JIRA issue for the other langs and keep focusing on one language only. That will make code review easier (for one-language-only eyes like mine).",
        "createdAt" : "2021-01-08T14:20:14Z",
        "updatedAt" : "2021-01-26T03:25:46Z",
        "lastEditedBy" : "8c82da5a-f351-4a37-a8a9-13809311b07b",
        "tags" : [
        ]
      },
      {
        "id" : "45716804-7039-445d-a0b1-d490d565485b",
        "parentId" : "24f7125d-7e42-4129-aaaf-2bd8c8d3dc48",
        "authorId" : "e1bdd418-d9b9-4129-91a0-0865740d24d2",
        "body" : "@zero323 - are you OK if I create a separate JIRA and do a separate pull request to add this function to PySpark & R?  Seems like @HyukjinKwon is open to doing everything in one PR or having separate PRs.  @jaceklaskowski prefers one PR for language.  Just let me know how you'd like me to proceed to keep this moving forward, thanks!",
        "createdAt" : "2021-01-09T18:28:45Z",
        "updatedAt" : "2021-01-26T03:25:46Z",
        "lastEditedBy" : "e1bdd418-d9b9-4129-91a0-0865740d24d2",
        "tags" : [
        ]
      },
      {
        "id" : "2195c0f4-4282-43be-bc2e-2554765ea33e",
        "parentId" : "24f7125d-7e42-4129-aaaf-2bd8c8d3dc48",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Yeah, we can run it separately.",
        "createdAt" : "2021-01-10T05:19:27Z",
        "updatedAt" : "2021-01-26T03:25:46Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "6c0f6bbd7036851f27b0540fc687508342477c0c",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +2856,2860 @@   * @since 3.2.0\n   */\n  def make_interval(\n      years: Column = lit(0),\n      months: Column = lit(0),"
  },
  {
    "id" : "b0990800-aa11-4e29-bca0-0c173a99b9ab",
    "prId" : 31073,
    "prUrl" : "https://github.com/apache/spark/pull/31073#pullrequestreview-569893531",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a98fae85-a8b4-409c-9e45-b3591ebad62a",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Can we avoid default argument? This doesn't work in Java.",
        "createdAt" : "2021-01-07T05:42:28Z",
        "updatedAt" : "2021-01-26T03:25:46Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "9da058b4-7ba9-4988-8aa0-9f8aeeba5290",
        "parentId" : "a98fae85-a8b4-409c-9e45-b3591ebad62a",
        "authorId" : "e1bdd418-d9b9-4129-91a0-0865740d24d2",
        "body" : "@HyukjinKwon - thank you for the code review.  The default arguments allow for this nice syntax:\r\n\r\n```scala\r\ndf.withColumn(\"plus_2_hours\", col(\"first_datetime\") + make_interval(hours = lit(2)))\r\n```\r\n\r\nWithout the default arguments, we'll need to invoke the method with a lot of arguments:\r\n\r\n```scala\r\nval twoHourInterval = make_interval(lit(0), lit(0), lit(0), lit(0), lit(2), lit(0), lit(0))\r\ndf.withColumn(\"plus_2_hours\", col(\"first_datetime\") + twoHourInterval)\r\n```\r\n\r\nWe could overload the method and allow for both syntaxes (using a little trick with the last argument that's optional).\r\n\r\n```scala\r\ndef make_interval(\r\n    years: Column,\r\n    months: Column,\r\n    weeks: Column,\r\n    days: Column,\r\n    hours: Column,\r\n    mins: Column,\r\n    secs: Column,\r\n    failOnError: Boolean): Column = withExpr {\r\n  MakeInterval(\r\n    years.expr, months.expr, weeks.expr, days.expr, hours.expr, mins.expr, secs.expr, failOnError)\r\n}\r\n```\r\n\r\nThis would allow Scala users to either manually list 8 arguments (e.g. `make_interval(lit(0), lit(0), lit(0), lit(0), lit(2), lit(0), lit(0), true)` or use the clean default syntax (e.g. `make_interval(hours = lit(2))`).  Would this give the Java users a usable interface?\r\n\r\nI'm also fine removing the default values if that's what's necessary to make this method compatible with Java.  @MaxGekk - open to your thoughts too cause I know you were originally in favor [of the arguments with default values approach](https://issues.apache.org/jira/browse/SPARK-33995?focusedCommentId=17258718&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17258718).",
        "createdAt" : "2021-01-07T16:19:14Z",
        "updatedAt" : "2021-01-26T03:25:46Z",
        "lastEditedBy" : "e1bdd418-d9b9-4129-91a0-0865740d24d2",
        "tags" : [
        ]
      },
      {
        "id" : "36e858c2-028d-4323-8a6e-8e3ac2a23b93",
        "parentId" : "a98fae85-a8b4-409c-9e45-b3591ebad62a",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "What kind of issue will we face to? Do we have to specify all arguments when we call such method from Java, or impossible to call it all?",
        "createdAt" : "2021-01-07T17:21:55Z",
        "updatedAt" : "2021-01-26T03:25:46Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "a33bae5d-c122-4e8a-aef0-b12ea409bbdf",
        "parentId" : "a98fae85-a8b4-409c-9e45-b3591ebad62a",
        "authorId" : "e1bdd418-d9b9-4129-91a0-0865740d24d2",
        "body" : "@MaxGekk - I [added a test](https://github.com/apache/spark/pull/31073/commits/c9492a99ffa8a3ab1380ebde21c1ea200d02c187) to demonstrate that `make_interval` can be called from the Java API (with the Scala method that has default values) when all 7 arguments are passed to the method.  e.g. this Java code works:\r\n\r\n```java\r\nColumn twoYears = make_interval(lit(2), lit(0), lit(0), lit(0), lit(0), lit(0), lit(0));\r\nDataset<Row> df = spark.createDataFrame(rows, schema).withColumn(\"plus_two_years\", col(\"some_date\").plus(twoYears));\r\n```\r\n\r\nSomeone with Java experience should check my result.  If the current implementation with default parameters works with the Java API, then I think it's ok to keep it, especially because it allows for much more elegant Scala code.  ",
        "createdAt" : "2021-01-07T22:10:04Z",
        "updatedAt" : "2021-01-26T03:25:46Z",
        "lastEditedBy" : "e1bdd418-d9b9-4129-91a0-0865740d24d2",
        "tags" : [
        ]
      },
      {
        "id" : "961a681a-e98f-4b7a-a0e9-776123501d8b",
        "parentId" : "a98fae85-a8b4-409c-9e45-b3591ebad62a",
        "authorId" : "8c82da5a-f351-4a37-a8a9-13809311b07b",
        "body" : "> Can we avoid default argument? This doesn't work in Java.\r\n\r\nThat's why I recommend focusing on one language only and think of others later. For Java we might need more functions to look Scala-ish. No idea what R requires.",
        "createdAt" : "2021-01-08T14:22:12Z",
        "updatedAt" : "2021-01-26T03:25:46Z",
        "lastEditedBy" : "8c82da5a-f351-4a37-a8a9-13809311b07b",
        "tags" : [
        ]
      },
      {
        "id" : "0c848ef7-fb0b-4e34-be19-d12c06c306f4",
        "parentId" : "a98fae85-a8b4-409c-9e45-b3591ebad62a",
        "authorId" : "e1bdd418-d9b9-4129-91a0-0865740d24d2",
        "body" : "@MaxGekk @HyukjinKwon - Here's where we currently stand on the default arguments issue:\r\n\r\n* Default arguments are not used by other functions in `org.apache.spark.sql.functions`, so exposing a function with default param values isn't consistent with what's been done in the past.\r\n* The default arguments to `make_interval` make the function better for Scala users because it allows for this nice syntax `make_interval(hours = lit(2))`\r\n* Java users can use the `make_interval` function that's defined with the default arguments, [as illustrated in this test](https://github.com/apache/spark/pull/31073/commits/c9492a99ffa8a3ab1380ebde21c1ea200d02c187) as long as they supply all seven arguments\r\n\r\nThere are good arguments to use and to avoid default arguments with `make_interval`.  Can you please let me know your updated thoughts on if you think we should use the default values or not?  Thank you.",
        "createdAt" : "2021-01-09T19:14:11Z",
        "updatedAt" : "2021-01-26T03:25:46Z",
        "lastEditedBy" : "e1bdd418-d9b9-4129-91a0-0865740d24d2",
        "tags" : [
        ]
      },
      {
        "id" : "5ea4c10d-ec5a-4aa5-9ecb-3da26fccf725",
        "parentId" : "a98fae85-a8b4-409c-9e45-b3591ebad62a",
        "authorId" : "981b170c-729a-429c-b115-0350ea50b32b",
        "body" : "> That's why I recommend focusing on one language only and think of others later. For Java we might need more functions to look Scala-ish. No idea what R requires.\r\n\r\nWhile I don't see a huge issue with adding Python and R API separately for this specific function, Scala API often sets limits for what is possible on the other side.  Moreover, we seldom extend JVM API just for the sake of guest languages these days. So unless API is trivial (like here) or uses well established interface (like ML model), it's better to design things at the same time.",
        "createdAt" : "2021-01-15T22:02:17Z",
        "updatedAt" : "2021-01-26T03:25:46Z",
        "lastEditedBy" : "981b170c-729a-429c-b115-0350ea50b32b",
        "tags" : [
        ]
      },
      {
        "id" : "bc03779e-ef1f-49a0-9e5d-97a4c965dd60",
        "parentId" : "a98fae85-a8b4-409c-9e45-b3591ebad62a",
        "authorId" : "e1bdd418-d9b9-4129-91a0-0865740d24d2",
        "body" : "@zero323 - that's a good point.  I plan on creating additional PRs for some of the other functions that are in the SQL API, but aren't in the Scala/Python APIs yet and I'll just add functions for all the APIs in a single PR going forward.\r\n\r\nDo you have any additional feedback for this PR or think it's good to get merged?",
        "createdAt" : "2021-01-16T02:37:45Z",
        "updatedAt" : "2021-01-26T03:25:46Z",
        "lastEditedBy" : "e1bdd418-d9b9-4129-91a0-0865740d24d2",
        "tags" : [
        ]
      },
      {
        "id" : "e01ffe65-69ae-4c25-b3de-4190d6820bdf",
        "parentId" : "a98fae85-a8b4-409c-9e45-b3591ebad62a",
        "authorId" : "981b170c-729a-429c-b115-0350ea50b32b",
        "body" : "LGTM @MrPowers ",
        "createdAt" : "2021-01-16T10:46:10Z",
        "updatedAt" : "2021-01-26T03:25:46Z",
        "lastEditedBy" : "981b170c-729a-429c-b115-0350ea50b32b",
        "tags" : [
        ]
      }
    ],
    "commit" : "6c0f6bbd7036851f27b0540fc687508342477c0c",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +2857,2861 @@   */\n  def make_interval(\n      years: Column = lit(0),\n      months: Column = lit(0),\n      weeks: Column = lit(0),"
  },
  {
    "id" : "a13fda87-03bc-48cf-93d5-53b26dea6fa1",
    "prId" : 31073,
    "prUrl" : "https://github.com/apache/spark/pull/31073#pullrequestreview-564830531",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a5bd467f-8a6c-4195-a009-b42b3251bbfb",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Can we just match how it's invoked in SQL side? We can have 7 overridden versions or leverage `varargs` like https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L493-L494",
        "createdAt" : "2021-01-10T05:35:42Z",
        "updatedAt" : "2021-01-26T03:25:46Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "6c0f6bbd7036851f27b0540fc687508342477c0c",
    "line" : 27,
    "diffHunk" : "@@ -1,1 +2865,2869 @@      secs: Column = lit(0)): Column = withExpr {\n    MakeInterval(years.expr, months.expr, weeks.expr, days.expr, hours.expr, mins.expr, secs.expr)\n  }\n\n  /**"
  },
  {
    "id" : "94ba854c-dfce-47d7-9464-3730feb389c4",
    "prId" : 30761,
    "prUrl" : "https://github.com/apache/spark/pull/30761#pullrequestreview-552011753",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "92e238c0-1de1-41fb-ab47-2761f264f2f0",
        "parentId" : null,
        "authorId" : "3cb8df92-1760-4e0e-94a5-f3ccadcc2882",
        "body" : "I copied the java doc from the previous function but it seems that it is a little bit not clear.\r\nThis parameter accepts more values:\r\n\r\n\r\n```     \r\n      case \"SU\" | \"SUN\" | \"SUNDAY\" => SUNDAY\r\n      case \"MO\" | \"MON\" | \"MONDAY\" => MONDAY\r\n      case \"TU\" | \"TUE\" | \"TUESDAY\" => TUESDAY\r\n      case \"WE\" | \"WED\" | \"WEDNESDAY\" => WEDNESDAY\r\n      case \"TH\" | \"THU\" | \"THURSDAY\" => THURSDAY\r\n      case \"FR\" | \"FRI\" | \"FRIDAY\" => FRIDAY\r\n      case \"SA\" | \"SAT\" | \"SATURDAY\" => SATURDAY\r\n```\r\n\r\nShould we improve the javadoc for these functions?",
        "createdAt" : "2020-12-14T12:31:18Z",
        "updatedAt" : "2020-12-15T07:34:05Z",
        "lastEditedBy" : "3cb8df92-1760-4e0e-94a5-f3ccadcc2882",
        "tags" : [
        ]
      },
      {
        "id" : "5c1881b1-7aef-4f4a-a9bc-fb8573757f01",
        "parentId" : "92e238c0-1de1-41fb-ab47-2761f264f2f0",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I think it's fine.",
        "createdAt" : "2020-12-15T00:23:43Z",
        "updatedAt" : "2020-12-15T07:34:05Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "7c32905643ec1e072f82936d429a8e445e365356",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +3086,3090 @@   * @param date      A date, timestamp or string. If a string, the data must be in a format that\n   *                  can be cast to a date, such as `yyyy-MM-dd` or `yyyy-MM-dd HH:mm:ss.SSSS`\n   * @param dayOfWeek A column of the day of week. Case insensitive, and accepts: \"Mon\", \"Tue\",\n   *                  \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"\n   * @return A date, or null if `date` was a string that could not be cast to a date or if"
  },
  {
    "id" : "d29bf53a-e29e-48e3-9053-332dcda4bf9e",
    "prId" : 30761,
    "prUrl" : "https://github.com/apache/spark/pull/30761#pullrequestreview-552172457",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e16ee9e0-66b0-4a80-83af-b95b2d76e15f",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Can you add `@since 3.2.0`?",
        "createdAt" : "2020-12-15T00:23:19Z",
        "updatedAt" : "2020-12-15T07:34:05Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "f108906a-002b-421b-bf7b-9d7abf3f110c",
        "parentId" : "e16ee9e0-66b0-4a80-83af-b95b2d76e15f",
        "authorId" : "3cb8df92-1760-4e0e-94a5-f3ccadcc2882",
        "body" : "done",
        "createdAt" : "2020-12-15T07:35:32Z",
        "updatedAt" : "2020-12-15T07:35:32Z",
        "lastEditedBy" : "3cb8df92-1760-4e0e-94a5-f3ccadcc2882",
        "tags" : [
        ]
      }
    ],
    "commit" : "7c32905643ec1e072f82936d429a8e445e365356",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +3090,3094 @@   * @return A date, or null if `date` was a string that could not be cast to a date or if\n   *         `dayOfWeek` was an invalid value\n   * @group datetime_funcs\n   * @since 3.2.0\n   */"
  },
  {
    "id" : "b6fb7ad8-a29f-40bc-a33f-c7b665ee7633",
    "prId" : 30745,
    "prUrl" : "https://github.com/apache/spark/pull/30745#pullrequestreview-599167098",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "54fd577b-8040-4f94-8a75-d4bdd9338561",
        "parentId" : null,
        "authorId" : "43998e22-6c2f-401d-9914-8cecf6fad929",
        "body" : "Don't we want to expose `def product(columnName: String): Column`?\r\nOther agg funcs seem to have the overloaded functions.",
        "createdAt" : "2021-02-26T00:35:30Z",
        "updatedAt" : "2021-03-01T17:18:18Z",
        "lastEditedBy" : "43998e22-6c2f-401d-9914-8cecf6fad929",
        "tags" : [
        ]
      },
      {
        "id" : "381433d6-1d1c-4773-a34a-ff6e31fe0d97",
        "parentId" : "54fd577b-8040-4f94-8a75-d4bdd9338561",
        "authorId" : "43998e22-6c2f-401d-9914-8cecf6fad929",
        "body" : "ah, new functions won't expose the signature. ",
        "createdAt" : "2021-02-26T00:49:23Z",
        "updatedAt" : "2021-03-01T17:18:18Z",
        "lastEditedBy" : "43998e22-6c2f-401d-9914-8cecf6fad929",
        "tags" : [
        ]
      }
    ],
    "commit" : "1cdabcbdab2d54c67f09139f3b6cea1b2c09e04e",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +737,741 @@   */\n  def product(e: Column): Column =\n    withAggregateFunction { new Product(e.expr) }\n\n  /**"
  },
  {
    "id" : "e7a63f18-ed7a-470e-b073-9ce284029a60",
    "prId" : 30201,
    "prUrl" : "https://github.com/apache/spark/pull/30201#pullrequestreview-520588790",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d2b45e5a-5214-4740-b892-a32f8e4ceff0",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Could you update `ExpressionDescription` (usage and examples) of `CsvToStructs` and `JsonToStructs`?",
        "createdAt" : "2020-10-30T11:39:12Z",
        "updatedAt" : "2020-10-30T18:03:08Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "88cd2a3354e587861f3214b94d0cadc8bc163747",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +4101,4105 @@   */\n  def from_json(e: Column, schema: String, options: Map[String, String]): Column = withExpr {\n    new JsonToStructs(e.expr, lit(schema).expr, options)\n  }\n"
  },
  {
    "id" : "a4c92b5e-0319-4322-a6b9-e504c87a66e7",
    "prId" : 30201,
    "prUrl" : "https://github.com/apache/spark/pull/30201#pullrequestreview-521210919",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dcc8b371-001b-4b1e-8d60-3e5781b15f59",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Wait .. I remember actually I had a discussion about this (with @maropu ..?) before. JSON format schema is actually pretty internal ser/de purpose, and just encouraging DDL format is better, and JSON format here was kept just for the compatibility reason.",
        "createdAt" : "2020-11-01T10:24:43Z",
        "updatedAt" : "2020-11-01T10:24:44Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "88cd2a3354e587861f3214b94d0cadc8bc163747",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +4101,4105 @@   */\n  def from_json(e: Column, schema: String, options: Map[String, String]): Column = withExpr {\n    new JsonToStructs(e.expr, lit(schema).expr, options)\n  }\n"
  },
  {
    "id" : "09d1a632-e444-4b0b-bb2e-7fb3d7111e60",
    "prId" : 29938,
    "prUrl" : "https://github.com/apache/spark/pull/29938#pullrequestreview-501562784",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fce11ef9-c1e8-4356-863c-b67898cf9c5b",
        "parentId" : null,
        "authorId" : "981b170c-729a-429c-b115-0350ea50b32b",
        "body" : "We should probably have `@since` annotation here and for the remaining ones.",
        "createdAt" : "2020-10-03T17:26:21Z",
        "updatedAt" : "2020-10-04T04:33:30Z",
        "lastEditedBy" : "981b170c-729a-429c-b115-0350ea50b32b",
        "tags" : [
        ]
      },
      {
        "id" : "6eaeb73b-9d4f-403b-bd7f-ecf6077e1a14",
        "parentId" : "fce11ef9-c1e8-4356-863c-b67898cf9c5b",
        "authorId" : "dc7a33cf-bba6-4512-a5a1-248c3cac6eae",
        "body" : "I wasn't sure whether this is something I should put in during the pull-request, or whether it gets added at a later stage.\r\n\r\nShall I presume that this is destined for Spark-3.1, or maybe 3.0.2?",
        "createdAt" : "2020-10-03T17:48:14Z",
        "updatedAt" : "2020-10-04T04:33:30Z",
        "lastEditedBy" : "dc7a33cf-bba6-4512-a5a1-248c3cac6eae",
        "tags" : [
        ]
      },
      {
        "id" : "b69f5929-9ed3-4ab2-b492-e18cdefb8e1e",
        "parentId" : "fce11ef9-c1e8-4356-863c-b67898cf9c5b",
        "authorId" : "981b170c-729a-429c-b115-0350ea50b32b",
        "body" : "It would be 3.1, as new functions are not added in maintenance releases.",
        "createdAt" : "2020-10-03T17:49:54Z",
        "updatedAt" : "2020-10-04T04:33:30Z",
        "lastEditedBy" : "981b170c-729a-429c-b115-0350ea50b32b",
        "tags" : [
        ]
      }
    ],
    "commit" : "7aed453983b6453940afea7b2c7f6535d69b1ffc",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +1433,1437 @@   * @group math_funcs\n   * @since 3.1.0\n   */\n  def acosh(e: Column): Column = withExpr { Acosh(e.expr) }\n"
  },
  {
    "id" : "0a7135e7-d84f-4c0a-83da-3058d1e716ec",
    "prId" : 28979,
    "prUrl" : "https://github.com/apache/spark/pull/28979#pullrequestreview-446031874",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b42f12be-67c0-4ddc-bccc-878f8b1dd0d5",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Hi all, in the latest update, acquring schema can fallback to previous `ScalaReflection.schemaFor[RT]` when `ExpressionEncoder[RT]()` fails. In this way, we can support both`udf((i: String) => null)`(return type is `NullType`) and `udf((i: String) => null.asInstanceOf[String])` (return type is `StringType`) at the same time and avoid behaviour change. cc @maropu ",
        "createdAt" : "2020-07-09T10:29:06Z",
        "updatedAt" : "2020-07-09T13:18:13Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "5e8ae010-63fe-4ec4-b3d2-660cbfbb41f1",
        "parentId" : "b42f12be-67c0-4ddc-bccc-878f8b1dd0d5",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Ah, I see. Keeping the current behaviour looks good. WDYT? @cloud-fan \r\nBut, this fallback is only for the null case?\r\n",
        "createdAt" : "2020-07-09T10:49:29Z",
        "updatedAt" : "2020-07-09T13:18:13Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "9f263fda-ca3f-4fa4-bc94-f69705f76005",
        "parentId" : "b42f12be-67c0-4ddc-bccc-878f8b1dd0d5",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "+1 to avoid breaking changes.",
        "createdAt" : "2020-07-09T12:12:51Z",
        "updatedAt" : "2020-07-09T13:18:13Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "d7005605-09d0-4e61-a042-9852f8f775ac",
        "parentId" : "b42f12be-67c0-4ddc-bccc-878f8b1dd0d5",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "I compared the support types between `schemaFor` between `serializerFor`(which is called by `ExpressionEncoder` indeed). It turns out that we can fallback three types: `NullType`, ~`CalendarInterval`~, ~`Decimal`~.\r\n\r\n(Actually, I'm thinking why `serializerFor` doesn't support them. )",
        "createdAt" : "2020-07-09T12:42:58Z",
        "updatedAt" : "2020-07-09T13:18:13Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "070604be-2486-4e0f-96f7-a77bd877913c",
        "parentId" : "b42f12be-67c0-4ddc-bccc-878f8b1dd0d5",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "> I compared the support types between schemaFor between serializerFor(which is called by ExpressionEncoder indeed). It turns out that we can fallback three types: NullType, CalendarInterval, Decimal.\r\n\r\nOh, I see. So, this update looks okay to me.",
        "createdAt" : "2020-07-09T12:50:30Z",
        "updatedAt" : "2020-07-09T13:18:13Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "567f9c7a-a47e-4ba2-9ec9-afcd4b70007d",
        "parentId" : "b42f12be-67c0-4ddc-bccc-878f8b1dd0d5",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Oh..wait!!\r\n\r\nI just realized that we should actually compare `schemaFor` and `serializerForType`(rather than `serializerFor`).  `serializerForType` will call `dataTypeFor` first.  `dataTypeFor` actually supports `NullType`, `CalendarInterval`, `Decimal` as well.\r\n\r\nThe reason `ExpressionEncoder` doesn't support `NullType` is, before `ExpressionEncoder` calls `serializerForType`, it calls `mirror.runtimeClass(tpe)` first:\r\n\r\nhttps://github.com/apache/spark/blob/fab4ca5156d5e1cc0e976c7c27b28a12fa61eb6d/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/encoders/ExpressionEncoder.scala#L56-L58\r\n\r\nand `null` (without explicit data type) can fail at `mirror.runtimeClass(tpe)`:\r\n```scala\r\njava.lang.ClassNotFoundException: scala.Null\r\n  at scala.reflect.internal.util.AbstractFileClassLoader.findClass(AbstractFileClassLoader.scala:72)\r\n  at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:588)\r\n  at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:521)\r\n  at java.base/java.lang.Class.forName0(Native Method)\r\n  at java.base/java.lang.Class.forName(Class.java:398)\r\n  at scala.reflect.runtime.JavaMirrors$JavaMirror.javaClass(JavaMirrors.scala:589)\r\n  at scala.reflect.runtime.JavaMirrors$JavaMirror.$anonfun$classToJava$1(JavaMirrors.scala:1265)\r\n  at scala.reflect.runtime.TwoWayCaches$TwoWayCache.$anonfun$toJava$1(TwoWayCaches.scala:61)\r\n  at scala.reflect.runtime.TwoWayCaches$TwoWayCache.toJava(TwoWayCaches.scala:57)\r\n  at scala.reflect.runtime.JavaMirrors$JavaMirror.classToJava(JavaMirrors.scala:1257)\r\n  at scala.reflect.runtime.JavaMirrors$JavaMirror.typeToJavaClass(JavaMirrors.scala:1351)\r\n  at scala.reflect.runtime.JavaMirrors$JavaMirror.runtimeClass(JavaMirrors.scala:227)\r\n  at scala.reflect.runtime.JavaMirrors$JavaMirror.runtimeClass(JavaMirrors.scala:68)\r\n  at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder$.apply(ExpressionEncoder.scala:56)\r\n  at org.apache.spark.sql.functions$.udf(functions.scala:4542)\r\n  ... 47 elided\r\n```",
        "createdAt" : "2020-07-09T13:16:41Z",
        "updatedAt" : "2020-07-09T13:18:13Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "1dae40f0-ce92-4a46-b970-be504bd154f1",
        "parentId" : "b42f12be-67c0-4ddc-bccc-878f8b1dd0d5",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Thanks for the explanation! LGTM",
        "createdAt" : "2020-07-09T23:26:08Z",
        "updatedAt" : "2020-07-09T23:26:08Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "871c35ffd3d71cef6040849561ae07a8d4e6b370",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +4524,4528 @@  def udf[RT: TypeTag](f: Function0[RT]): UserDefinedFunction = {\n    val outputEncoder = Try(ExpressionEncoder[RT]()).toOption\n    val ScalaReflection.Schema(dataType, nullable) = outputEncoder.map(_.dataTypeAndNullable).getOrElse(ScalaReflection.schemaFor[RT])\n    val inputEncoders = Nil\n    val udf = SparkUserDefinedFunction(f, dataType, inputEncoders, outputEncoder)"
  },
  {
    "id" : "ee0a55d4-fef9-4780-bc08-d8d5caae2993",
    "prId" : 28685,
    "prUrl" : "https://github.com/apache/spark/pull/28685#pullrequestreview-427892239",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cfbd606d-5a13-48e3-9116-78daaa821b12",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@beliefer, how is it different from `lag`?",
        "createdAt" : "2020-06-10T08:12:27Z",
        "updatedAt" : "2020-08-25T10:38:37Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "ad0f83ca-4090-4ebf-996e-7edb71d690c8",
        "parentId" : "cfbd606d-5a13-48e3-9116-78daaa821b12",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "They are existing two differences.\r\nThe first is:\r\nThe offset of `lag` based on the current row in the window. \r\nThe offset of `nth_value` based on the first row in the window. \r\nThe second is.\r\nThe offset of `lag` increases from back to front.\r\nThe offset of `nth_value` increases from front to back.",
        "createdAt" : "2020-06-10T09:47:15Z",
        "updatedAt" : "2020-08-25T10:38:37Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "301a847982445b01fdd75794d9780d23c4e58276",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +1003,1007 @@   * @since 3.1.0\n   */\n  def nth_value(columnName: String, offset: Int): Column = {\n    nth_value(Column(columnName), offset)\n  }"
  },
  {
    "id" : "bd64423b-b5fe-49e4-bfe7-98fa7431e363",
    "prId" : 28593,
    "prUrl" : "https://github.com/apache/spark/pull/28593#pullrequestreview-417777351",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "36e5f643-dfc4-4e0b-b723-1141156aa58b",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "can you double-check the indentation?\r\n```\r\n  /**\r\n   * docs..\r\n   */\r\n  def func\r\n```",
        "createdAt" : "2020-06-10T09:14:03Z",
        "updatedAt" : "2020-06-15T17:23:35Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "5528c8c5-50d4-4553-836d-beb36df862fd",
        "parentId" : "36e5f643-dfc4-4e0b-b723-1141156aa58b",
        "authorId" : "a67e5600-3f69-4b96-9c53-73795f48a0f6",
        "body" : "oops, let me  fix it.",
        "createdAt" : "2020-06-10T09:39:27Z",
        "updatedAt" : "2020-06-15T17:23:36Z",
        "lastEditedBy" : "a67e5600-3f69-4b96-9c53-73795f48a0f6",
        "tags" : [
        ]
      }
    ],
    "commit" : "12b42396f058569354040d466962904794fa5c5e",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +3364,3368 @@   * @since = 3.1.0\n   */\n  def timestamp_seconds(e: Column): Column = withExpr {\n    SecondsToTimestamp(e.expr)\n  }"
  },
  {
    "id" : "ff0177bc-eecd-4f5d-94d8-ad342406412e",
    "prId" : 28593,
    "prUrl" : "https://github.com/apache/spark/pull/28593#pullrequestreview-438084929",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b0889834-e8c6-42fc-9ef6-81329a745f52",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "nit: \r\n\r\n```\r\n@group datetime_funcs\r\n@since 3.1.0\r\n```",
        "createdAt" : "2020-06-26T07:51:20Z",
        "updatedAt" : "2020-06-26T07:51:20Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "12b42396f058569354040d466962904794fa5c5e",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +3362,3366 @@   * Creates timestamp from the number of seconds since UTC epoch.\n   * @group = datetime_funcs\n   * @since = 3.1.0\n   */\n  def timestamp_seconds(e: Column): Column = withExpr {"
  },
  {
    "id" : "5fa9d51f-b000-4c36-98b1-7f26549ca321",
    "prId" : 27488,
    "prUrl" : "https://github.com/apache/spark/pull/27488#pullrequestreview-371632885",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9dd95f77-8511-460c-a327-8e481ce15f8e",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "In the error message, we should give an example to show how to use the typed Scala UDF for implementing \"udf((x: Int) => x, IntegerType)\"",
        "createdAt" : "2020-03-10T00:26:09Z",
        "updatedAt" : "2020-03-10T00:26:09Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "0a2c1f7c-b9b4-421a-aeb9-29e8c57cb44e",
        "parentId" : "9dd95f77-8511-460c-a327-8e481ce15f8e",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "I see.",
        "createdAt" : "2020-03-10T01:46:37Z",
        "updatedAt" : "2020-03-10T01:46:37Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "0d1601bde2fc4ab39ed6b3c82e486e9c1174b915",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +4738,4742 @@        \"argument, and the closure will see the default value of the Java type for the null \" +\n        \"argument, e.g. `udf((x: Int) => x, IntegerType)`, the result is 0 for null input. \" +\n        \"You could use other typed Scala UDF APIs to avoid this problem, or set \" +\n        s\"${SQLConf.LEGACY_ALLOW_UNTYPED_SCALA_UDF.key} to true and use this API with caution.\"\n      throw new AnalysisException(errorMsg)"
  },
  {
    "id" : "beaa560c-e0cf-43ad-96c0-97c1cd96a527",
    "prId" : 27449,
    "prUrl" : "https://github.com/apache/spark/pull/27449#pullrequestreview-352865616",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fe8927d2-ed17-4a38-a6c8-55bdd2d2d6ba",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "have you run the example and verify that the indices start at 0?",
        "createdAt" : "2020-02-04T10:18:40Z",
        "updatedAt" : "2020-02-06T07:11:34Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "120c8610-f1da-49af-a820-c25b6cbe1669",
        "parentId" : "fe8927d2-ed17-4a38-a6c8-55bdd2d2d6ba",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Examples are copied from tests and it starts from 0 if you look at the test result:\r\n\r\nhttps://github.com/apache/spark/blob/8aebc80e0e67bcb1aa300b8c8b1a209159237632/sql/core/src/test/scala/org/apache/spark/sql/DataFrameFunctionsSuite.scala#L2026",
        "createdAt" : "2020-02-04T10:23:49Z",
        "updatedAt" : "2020-02-06T07:11:34Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "c86900fc55180649ad0f1b195307fdd623cf5923",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +3434,3438 @@   * @param column the input array column\n   * @param f (col, index) => transformed_col, the lambda function to filter the input column\n   *           given the index. Indices start at 0.\n   *\n   * @group collection_funcs"
  },
  {
    "id" : "8d94081e-5d4a-45a7-8a79-f41be68a4961",
    "prId" : 27449,
    "prUrl" : "https://github.com/apache/spark/pull/27449#pullrequestreview-353501473",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "354c2eff-7ccd-4f7f-bd62-f3b500e67dbe",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "nit: I would just call it `the boolean predicate` instead of `the Boolean predicate` in this file but no big deal.",
        "createdAt" : "2020-02-05T06:19:57Z",
        "updatedAt" : "2020-02-06T07:11:34Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "b9a26f62-3c4a-4e80-8275-a6aedda02a22",
        "parentId" : "354c2eff-7ccd-4f7f-bd62-f3b500e67dbe",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "It's intentionally fixed by #27382.",
        "createdAt" : "2020-02-05T06:49:25Z",
        "updatedAt" : "2020-02-06T07:11:34Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "2a526f9e-5b9e-4ab4-ba8e-38eca9777218",
        "parentId" : "354c2eff-7ccd-4f7f-bd62-f3b500e67dbe",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "okie. that's fine.",
        "createdAt" : "2020-02-05T07:06:37Z",
        "updatedAt" : "2020-02-06T07:11:34Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "c86900fc55180649ad0f1b195307fdd623cf5923",
    "line" : 36,
    "diffHunk" : "@@ -1,1 +3450,3454 @@   *\n   * @param column the input array column\n   * @param f col => predicate, the Boolean predicate to check the input column\n   *\n   * @group collection_funcs"
  },
  {
    "id" : "d21c4a11-01ef-4733-8af4-ae6b12761e32",
    "prId" : 27438,
    "prUrl" : "https://github.com/apache/spark/pull/27438#pullrequestreview-352474570",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "474a1f21-1cbf-4523-85d4-f4ecf8167f07",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "If you don't mind, I would like to say here that currently supported pattern for seconds fractions is `SSS` only. The second s fractions can be parsed but will be ignored while casting to TimestampType.",
        "createdAt" : "2020-02-03T18:41:32Z",
        "updatedAt" : "2020-02-03T18:41:32Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "c7f75a95bc18886161f2986d10ddcfa729755b86",
    "line" : 1,
    "diffHunk" : "@@ -1,1 +2965,2969 @@   * Converts time string with the given pattern to timestamp.\n   *\n   * See [[java.text.SimpleDateFormat]] for valid date and time format patterns\n   *\n   * @param s   A date, timestamp or string. If a string, the data must be in a format that can be"
  },
  {
    "id" : "9d595363-6c7d-4756-8add-edea568a1751",
    "prId" : 27382,
    "prUrl" : "https://github.com/apache/spark/pull/27382#pullrequestreview-350193059",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6df49f35-37e1-4876-b90f-486fc7d9d68f",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "This is fine, though 'boolean' is very commonly used as the noun, despite it being from a name",
        "createdAt" : "2020-01-29T15:25:28Z",
        "updatedAt" : "2020-01-29T15:25:53Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      }
    ],
    "commit" : "eba2959bb374a4d69f2f07f4e4a7f15e99d2a761",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +3458,3462 @@   *\n   * @param column: the input array column\n   * @param f: col => predicate, the Boolean predicate to filter the input column\n   *\n   * @group collection_funcs"
  }
]