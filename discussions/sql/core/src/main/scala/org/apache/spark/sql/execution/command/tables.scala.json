[
  {
    "id" : "53126310-310c-43bc-b994-b365e36bb79f",
    "prId" : 31804,
    "prUrl" : "https://github.com/apache/spark/pull/31804#pullrequestreview-609398956",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2e68d73b-fcdb-4ab0-aa5d-1278732f50e2",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Do we still have a test coverage for this line?",
        "createdAt" : "2021-03-11T04:55:18Z",
        "updatedAt" : "2021-03-30T07:59:14Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "f028eb37-c14a-4574-9425-007adbd14fea",
        "parentId" : "2e68d73b-fcdb-4ab0-aa5d-1278732f50e2",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "yes, the new `show-tables-legacy.sql` will import the corresponding tests to cover.  I can add some cases in `v1.ShowTablesSuite` if `show-tables-legacy.sql` is unintuitive",
        "createdAt" : "2021-03-11T05:09:47Z",
        "updatedAt" : "2021-03-30T07:59:14Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "3ef5b5f414b68ea149877eaa26618e9787252004",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +857,861 @@            Row(database, tableName, isTemp, tableType)\n          } else {\n            Row(database, tableName, isTemp)\n          }\n"
  },
  {
    "id" : "29bd4783-6950-478c-b3df-0dd042aba42c",
    "prId" : 31378,
    "prUrl" : "https://github.com/apache/spark/pull/31378#pullrequestreview-580376725",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c4636c85-f30e-44f2-894b-bc0da10023f0",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "do we need the `override val`?",
        "createdAt" : "2021-02-01T13:53:23Z",
        "updatedAt" : "2021-02-08T15:01:25Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "1b3376ae-0d9b-4266-a6d4-cd5c4608d35b",
        "parentId" : "c4636c85-f30e-44f2-894b-bc0da10023f0",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "Same change as last PR https://github.com/apache/spark/pull/31341/files\r\n![image](https://user-images.githubusercontent.com/46485123/106468154-87439980-64d8-11eb-89cb-c04c8316d323.png)\r\n",
        "createdAt" : "2021-02-01T13:57:57Z",
        "updatedAt" : "2021-02-08T15:01:25Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "cc9d8ac1105a336997384eab475fc82458d92b01",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +900,904 @@    table: TableIdentifier,\n    propertyKey: Option[String],\n    override val output: Seq[Attribute]) extends RunnableCommand {\n\n  override def run(sparkSession: SparkSession): Seq[Row] = {"
  },
  {
    "id" : "bce2af5f-c25c-4e72-b281-aeffb02141aa",
    "prId" : 31377,
    "prUrl" : "https://github.com/apache/spark/pull/31377#pullrequestreview-580380244",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c4fd725f-4b49-4ee2-a708-980ba676520d",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "do we need `override val`?",
        "createdAt" : "2021-02-01T13:56:15Z",
        "updatedAt" : "2021-02-01T13:56:15Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "b09ee167-febe-482f-861c-a4dab0d9fe2a",
        "parentId" : "c4fd725f-4b49-4ee2-a708-980ba676520d",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> do we need `override val`?\r\n\r\nSame change as last PR https://github.com/apache/spark/pull/31341/files\r\n![image](https://user-images.githubusercontent.com/46485123/106468154-87439980-64d8-11eb-89cb-c04c8316d323.png)\r\n",
        "createdAt" : "2021-02-01T14:01:47Z",
        "updatedAt" : "2021-02-01T14:01:47Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "81d5a38ac0209edce01e60e7d6e24aaa66ffa81e",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +937,941 @@    databaseName: Option[String],\n    tableName: TableIdentifier,\n    override val output: Seq[Attribute]) extends RunnableCommand {\n\n  override def run(sparkSession: SparkSession): Seq[Row] = {"
  },
  {
    "id" : "130ff021-c230-41f1-8a6d-1375cca79270",
    "prId" : 31368,
    "prUrl" : "https://github.com/apache/spark/pull/31368#pullrequestreview-577669100",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e9956f79-e6fb-457d-839a-11e278c8e191",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "the view plan can be unresolved (with cast and alias added), we should use the recorded view schema.",
        "createdAt" : "2021-01-27T19:07:26Z",
        "updatedAt" : "2021-01-29T04:46:02Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "2aebf0516f146456e4567612db8823fc3407641b",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +626,630 @@          s\"DESC PARTITION is not allowed on a temporary view: ${table.identifier}\")\n      }\n      val schema = catalog.getTempViewOrPermanentTableMetadata(table).schema\n      describeSchema(schema, result, header = false)\n    } else {"
  },
  {
    "id" : "0227b4f0-6965-4476-8f5b-373d2757e33c",
    "prId" : 31308,
    "prUrl" : "https://github.com/apache/spark/pull/31308#pullrequestreview-576208545",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "20392d5c-51ad-4ba7-8779-8ccc925d1415",
        "parentId" : null,
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Hmm... Not sure if we should catch any non-fatal exception like the previous one, since otherwise we'd skip updating stats?",
        "createdAt" : "2021-01-24T20:37:34Z",
        "updatedAt" : "2021-01-24T20:37:37Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "3ad37773-d940-4ddf-b925-57d063941707",
        "parentId" : "20392d5c-51ad-4ba7-8779-8ccc925d1415",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "1. We don't catch any exceptions in other commands\r\n2. What kind of exceptions should we hide (catch) from users here?",
        "createdAt" : "2021-01-24T20:51:24Z",
        "updatedAt" : "2021-01-24T21:01:50Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "064a1ebd-bc69-40b0-a702-daebc34ee82e",
        "parentId" : "20392d5c-51ad-4ba7-8779-8ccc925d1415",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Not clear which exceptions are caught here. `uncacheQuery()` doesn't throw anything, `spark.table(table.identifier).logicalPlan` could fail but in that case it is not clear how we reached this point.",
        "createdAt" : "2021-01-24T21:10:01Z",
        "updatedAt" : "2021-01-24T21:10:01Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "08c311cb-ce3c-465b-ba37-0358b416738c",
        "parentId" : "20392d5c-51ad-4ba7-8779-8ccc925d1415",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "I think someone could drop a table or permanent view in a different session, or drop a Hive table through beeline or HMS API. This may cause some cache which depend on them AND this truncated table to become invalid, and potentially analysis exception when recaching them. I haven't got a chance to verify this though.\r\n\r\nI feel overall it will be a good practice to recover from unknown errors here and continue. `DropTableCommand` does this as well.",
        "createdAt" : "2021-01-25T03:47:49Z",
        "updatedAt" : "2021-01-25T03:48:03Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "a0738496-f356-40d4-8091-fdf990892861",
        "parentId" : "20392d5c-51ad-4ba7-8779-8ccc925d1415",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "The case which you described here is applicable to any other commands like add/drop/rename/recover partitions. I do believe we either should \"fix\" all commands with tests for the case, or apply the approach w/o catching exceptions here as we do in other commands so far (otherwise the implementation looks inconsistent).",
        "createdAt" : "2021-01-25T06:20:29Z",
        "updatedAt" : "2021-01-25T06:20:29Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "4c522392-5da2-4a78-9777-9eb00f67e5f4",
        "parentId" : "20392d5c-51ad-4ba7-8779-8ccc925d1415",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Personally I'd keep just the try-catch logic here because I think the above do happens and we shouldn't skip updating stats in the case. But I don't really have strong opinion on this.",
        "createdAt" : "2021-01-25T08:25:02Z",
        "updatedAt" : "2021-01-25T08:25:03Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "14f7a989-4fbc-4fd6-936f-d9ebe08986f2",
        "parentId" : "20392d5c-51ad-4ba7-8779-8ccc925d1415",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "can we fix the inconsistency first? i.e. reach an agreement about whether we should add `try-cache` or not for all other commands.",
        "createdAt" : "2021-01-25T08:53:09Z",
        "updatedAt" : "2021-01-25T08:53:09Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "89790a28-0f87-43af-b8dd-0a3b3fe1bfd9",
        "parentId" : "20392d5c-51ad-4ba7-8779-8ccc925d1415",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "> ... someone could drop a table or permanent view in a different session, or drop a Hive table through beeline or HMS API.\r\n\r\nIf somebody dropped the table in parallel, updating statistics wouldn't matter any more. We should show the error to user as soon as possible.\r\n\r\n> can we fix the inconsistency first?\r\n\r\n@sunchao Can you write a test which reproduces the issue?",
        "createdAt" : "2021-01-25T09:16:16Z",
        "updatedAt" : "2021-01-25T09:16:17Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "1cbb21c2-0504-4559-ae29-92a7233d8d08",
        "parentId" : "20392d5c-51ad-4ba7-8779-8ccc925d1415",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "We might utilize `HiveThriftServer2Suites` for this - I can check when I got time.",
        "createdAt" : "2021-01-25T23:53:55Z",
        "updatedAt" : "2021-01-25T23:53:55Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "446ac16c-ee77-404e-bb6c-24a882cd3f16",
        "parentId" : "20392d5c-51ad-4ba7-8779-8ccc925d1415",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "So I wasn't able to reproduce it with the above example, sorry for the false alarm. Turned out the analysis exception will be thrown later when the cache is actually queried (rather than in `recacheByPlan` itself). Therefore, I think it should be fine in this case.\r\n\r\nI do agree we should keep it consistent (whether try-catch or not). IMO it can be done separately tho.",
        "createdAt" : "2021-01-26T10:20:42Z",
        "updatedAt" : "2021-01-26T10:20:42Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      }
    ],
    "commit" : "25b8583d65246372f8b6ac35071ead118b1c97f2",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +564,568 @@    // After deleting the data, refresh the table to make sure we don't keep around a stale\n    // file relation in the metastore cache and cached table data in the cache manager.\n    spark.catalog.refreshTable(tableIdentWithDB)\n\n    if (table.stats.nonEmpty) {"
  },
  {
    "id" : "70b596dd-1de9-4143-8b2b-a8d4c2b17290",
    "prId" : 31245,
    "prUrl" : "https://github.com/apache/spark/pull/31245#pullrequestreview-576937862",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d3418237-c1b8-4221-a5ef-a555389b607a",
        "parentId" : null,
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "We can remove this line since https://github.com/apache/spark/pull/31342 merged",
        "createdAt" : "2021-01-27T02:06:21Z",
        "updatedAt" : "2021-02-08T08:25:31Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "920c23f1-e327-43fa-bbd8-cbd1b68914a0",
        "parentId" : "d3418237-c1b8-4221-a5ef-a555389b607a",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "Yeah!",
        "createdAt" : "2021-01-27T02:47:41Z",
        "updatedAt" : "2021-02-08T08:25:31Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "815d36b35c294d110cf0770d2f0850d9ae1da151",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +829,833 @@    isExtended: Boolean = false,\n    partitionSpec: Option[TablePartitionSpec] = None) extends RunnableCommand {\n\n  override def run(sparkSession: SparkSession): Seq[Row] = {\n    // Since we need to return a Seq of rows, we will call getTables directly"
  },
  {
    "id" : "786c5bf1-dc6f-4783-a707-b0547d1e783e",
    "prId" : 30774,
    "prUrl" : "https://github.com/apache/spark/pull/30774#pullrequestreview-553152156",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "133b23ad-9e44-48b9-9f6f-ea76fc64d68b",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Does this miss the `tableName` if there is in the original cache?",
        "createdAt" : "2020-12-15T06:57:13Z",
        "updatedAt" : "2020-12-15T07:49:21Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "f17f2fdc-4681-482d-8abe-4bc2702bcb8c",
        "parentId" : "133b23ad-9e44-48b9-9f6f-ea76fc64d68b",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "Sorry, I didn't get this question. This is creating a new cache with a new table name.",
        "createdAt" : "2020-12-15T07:33:11Z",
        "updatedAt" : "2020-12-15T07:49:21Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      },
      {
        "id" : "be46b523-dd5c-4147-a53d-7457e8f219ef",
        "parentId" : "133b23ad-9e44-48b9-9f6f-ea76fc64d68b",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Hm, you can check the change like #30769. Especially how it recaches the table. There is a `cacheName` parameter. If the table was cached with a cache name, when recaching it, I think we should keep it.\r\n\r\n```scala\r\n    val cache = session.sharedState.cacheManager.lookupCachedData(v2Relation)\r\n    session.sharedState.cacheManager.uncacheQuery(session, v2Relation, cascade = true)\r\n    session.sharedState.cacheManager.uncacheQuery(session, v2Relation, cascade = true)\r\n    if (recacheTable && cache.isDefined) {\r\n      // save the cache name and cache level for recreation\r\n      val cacheName = cache.get.cachedRepresentation.cacheBuilder.tableName\r\n      val cacheLevel = cache.get.cachedRepresentation.cacheBuilder.storageLevel\r\n\r\n      // recache with the same name and cache level.\r\n      val ds = Dataset.ofRows(session, v2Relation)\r\n      session.sharedState.cacheManager.cacheQuery(ds, cacheName, cacheLevel)\r\n    }\r\n```",
        "createdAt" : "2020-12-15T08:53:57Z",
        "updatedAt" : "2020-12-15T08:53:58Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "fedffb95-8557-431e-920b-2b778f89df15",
        "parentId" : "133b23ad-9e44-48b9-9f6f-ea76fc64d68b",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "The previous code seems also recache with the new name?",
        "createdAt" : "2020-12-15T09:04:35Z",
        "updatedAt" : "2020-12-15T09:04:35Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "288ad92e-52b7-406c-acd5-74e2a3d2bce3",
        "parentId" : "133b23ad-9e44-48b9-9f6f-ea76fc64d68b",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "No, the refresh table command for v2 doesn't recache the table before #30769.",
        "createdAt" : "2020-12-15T09:09:06Z",
        "updatedAt" : "2020-12-15T09:09:06Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "c75e7042-05bb-465b-9140-c920ca9ed6c6",
        "parentId" : "133b23ad-9e44-48b9-9f6f-ea76fc64d68b",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I mean the previous code in `AlterTableRenameCommand`. We shouldn't change its behavior regarding cache name in this bug fix PR.",
        "createdAt" : "2020-12-15T14:06:07Z",
        "updatedAt" : "2020-12-15T14:06:07Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "bd71cbd6-78b8-493e-87bd-7f496e013158",
        "parentId" : "133b23ad-9e44-48b9-9f6f-ea76fc64d68b",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Hmm okay, actually it also sounds like a bug if alter table command changes the cache name. I'm fine to leave it unchanged here.",
        "createdAt" : "2020-12-15T17:40:02Z",
        "updatedAt" : "2020-12-15T17:40:02Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "0bbe6409-8dcf-41f9-b802-aea8dd9b86cd",
        "parentId" : "133b23ad-9e44-48b9-9f6f-ea76fc64d68b",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "I believe cache name is used for debug purpose only (for `RDD` name and `InMemoryTableScanExec`). So if the cache name - which is tied to the table name - doesn't change when the table is changed, wouldn't it cause a confusion since it will still refer to the old table name? I can do a follow up PR if this seems like a bug.",
        "createdAt" : "2020-12-16T00:13:48Z",
        "updatedAt" : "2020-12-16T00:13:48Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "11603d54d4a6b222edc2c5dbd4755e3a1e9072a2",
    "line" : 30,
    "diffHunk" : "@@ -1,1 +205,209 @@      catalog.renameTable(oldName, newName)\n      optStorageLevel.foreach { storageLevel =>\n        sparkSession.catalog.cacheTable(newName.unquotedString, storageLevel)\n      }\n    }"
  },
  {
    "id" : "d54482ec-54e0-47ac-b85e-e2dc09c655d2",
    "prId" : 29866,
    "prUrl" : "https://github.com/apache/spark/pull/29866#pullrequestreview-495977526",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "04add2b5-29ed-4635-a706-643452047303",
        "parentId" : null,
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "This is moved from `ddl.scala` and `RefreshTable` is renamed to `RefreshTableCommand`. Note that there is a TODO in this file that all the commands in `ddl.scala` need to move here, so I will handle it separately.",
        "createdAt" : "2020-09-24T21:54:16Z",
        "updatedAt" : "2020-09-25T03:53:55Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "d1051c6fad37993a299ed79617f574d6ce51c890",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +1400,1404 @@    Seq.empty[Row]\n  }\n}"
  },
  {
    "id" : "f8d3a780-02d4-4afc-97d2-92c6d8a8bc60",
    "prId" : 29387,
    "prUrl" : "https://github.com/apache/spark/pull/29387#pullrequestreview-472920172",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "711a4e06-2a2e-4437-bde5-1664e7b5672e",
        "parentId" : null,
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "do we need to have test cases to cover when the flag is on or off?",
        "createdAt" : "2020-08-21T21:03:49Z",
        "updatedAt" : "2020-08-24T22:01:57Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "6c530de2-84ff-4922-a756-8e90d70d623c",
        "parentId" : "711a4e06-2a2e-4437-bde5-1664e7b5672e",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "+1 for adding a test coverage for the new feature (if possible).",
        "createdAt" : "2020-08-22T00:22:40Z",
        "updatedAt" : "2020-08-24T22:01:57Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "5740af91-85e5-4d7a-a730-085aae13be17",
        "parentId" : "711a4e06-2a2e-4437-bde5-1664e7b5672e",
        "authorId" : "a88f50e2-6440-4a6c-8883-3130888d05bc",
        "body" : "added tests please review",
        "createdAt" : "2020-08-22T11:29:59Z",
        "updatedAt" : "2020-08-24T22:01:57Z",
        "lastEditedBy" : "a88f50e2-6440-4a6c-8883-3130888d05bc",
        "tags" : [
        ]
      }
    ],
    "commit" : "a2df53b48db372ed8f9a303cd8c0c499e33f3adf",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +516,520 @@          }\n\n          Utils.moveToTrashIfEnabled(fs, path, isTrashEnabled, hadoopConf)\n\n          // We should keep original permission/acl of the path."
  },
  {
    "id" : "61e7f5ce-9c76-4996-b6dc-aeb1274d4017",
    "prId" : 29127,
    "prUrl" : "https://github.com/apache/spark/pull/29127#pullrequestreview-449577938",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ef998968-e607-4a91-ba1c-cfaa33d5d9e1",
        "parentId" : null,
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "This will not be hit if you go thru `SHOW TBLPROPERTIES` command.",
        "createdAt" : "2020-07-16T07:29:20Z",
        "updatedAt" : "2020-07-24T00:35:29Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "933dc28ed5a54e06d8125e51ae3c86e95760f21e",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +921,925 @@    val catalog = sparkSession.sessionState.catalog\n    if (catalog.isTemporaryTable(table)) {\n      throw new AnalysisException(s\"SHOW TBLPROPERTIES is not allowed on a temporary view: $table\")\n    } else {\n      val catalogTable = catalog.getTableMetadata(table)"
  },
  {
    "id" : "4e698520-eebd-453d-9b6c-7d8133c660cf",
    "prId" : 29034,
    "prUrl" : "https://github.com/apache/spark/pull/29034#pullrequestreview-528728078",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c8accfba-f518-49dd-a8d2-9d1b1c59656d",
        "parentId" : null,
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "I'm not sure if `isCached` can work nicely with streaming tables. Can we test that?",
        "createdAt" : "2020-11-11T17:56:59Z",
        "updatedAt" : "2020-12-23T13:10:25Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "2d859efc-ba31-4047-b0dc-65651413a6bc",
        "parentId" : "c8accfba-f518-49dd-a8d2-9d1b1c59656d",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "IMO we can only cache batch table, with streaming we are allowed cache tables in `foreachBatch`. Isn't it?",
        "createdAt" : "2020-11-12T00:45:59Z",
        "updatedAt" : "2020-12-23T13:10:25Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      },
      {
        "id" : "5633a6f1-29bc-40f6-9a92-dc688eefd258",
        "parentId" : "c8accfba-f518-49dd-a8d2-9d1b1c59656d",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "yeah I think we cannot cache streaming table, but here the `isCached` is called irrespective of whether the table is cached or not, is that correct?",
        "createdAt" : "2020-11-12T00:47:47Z",
        "updatedAt" : "2020-12-23T13:10:25Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "b818fceb-1ebb-4458-ba0d-6b5152cdd314",
        "parentId" : "c8accfba-f518-49dd-a8d2-9d1b1c59656d",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "I think `SparkSession.table` inside the `isCached` method will create a `UnresolvedRelation` with `isStreaming = false`, and later on in the analysis this flag may cause failure when a streaming table is encountered with that flag.",
        "createdAt" : "2020-11-12T00:50:33Z",
        "updatedAt" : "2020-12-23T13:10:25Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "c325df9b-fa99-4901-a3f7-4fd76a06e389",
        "parentId" : "c8accfba-f518-49dd-a8d2-9d1b1c59656d",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "Based on current implementation of cache, we only cache the `LogicalPlan` without any metadata like `table name`. So here we list and filter the table one by one.",
        "createdAt" : "2020-11-12T01:02:38Z",
        "updatedAt" : "2020-12-23T13:10:25Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      },
      {
        "id" : "9aff78e7-ecb6-4ca0-b402-81bd87ab4744",
        "parentId" : "c8accfba-f518-49dd-a8d2-9d1b1c59656d",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "> I think SparkSession.table inside the isCached method will create a UnresolvedRelation with isStreaming = false, and later on in the analysis this flag may cause failure when a streaming table is encountered with that flag.\r\n\r\nI guess you consider about something like this ?\r\n```\r\n\r\ndef lookupTempView(\r\n    identifier: Seq[String], isStreaming: Boolean = false): Option[LogicalPlan] = {\r\n  // Permanent View can't refer to temp views, no need to lookup at all.\r\n  if (isResolvingView) return None\r\n\r\n  val tmpView = identifier match {\r\n    case Seq(part1) => v1SessionCatalog.lookupTempView(part1)\r\n    case Seq(part1, part2) => v1SessionCatalog.lookupGlobalTempView(part1, part2)\r\n    case _ => None\r\n  }\r\n\r\n  if (isStreaming && tmpView.nonEmpty && !tmpView.get.isStreaming) {\r\n    throw new AnalysisException(s\"${identifier.quoted} is not a temp view of streaming \" +\r\n      s\"logical plan, please use batch API such as `DataFrameReader.table` to read it.\")\r\n  }\r\n  tmpView\r\n}\r\n```",
        "createdAt" : "2020-11-12T01:19:30Z",
        "updatedAt" : "2020-12-23T13:10:25Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      },
      {
        "id" : "3aa9abcd-70b2-489a-b0aa-4d4af88ee0df",
        "parentId" : "c8accfba-f518-49dd-a8d2-9d1b1c59656d",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Yes this looks like it. I encountered something similar in #30211 and not sure the same will cause issue for this PR as well.",
        "createdAt" : "2020-11-12T02:15:53Z",
        "updatedAt" : "2020-12-23T13:10:25Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "e7cf1524-b9c3-42e3-90bc-0c9932dc88dc",
        "parentId" : "c8accfba-f518-49dd-a8d2-9d1b1c59656d",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "Thanks for your sharing!",
        "createdAt" : "2020-11-12T03:38:31Z",
        "updatedAt" : "2020-12-23T13:10:25Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      }
    ],
    "commit" : "635d3ab5be8635e4789345480ca3bcf2f0c72cd8",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +858,862 @@        tableIdentifierPattern.map(catalog.listTables(db, _)).getOrElse(catalog.listTables(db))\n      val filterCachedTables = if (showCached) {\n        tables.filter(table => sparkSession.catalog.isCached(table.quotedString))\n      } else {\n        tables"
  },
  {
    "id" : "d87ab9ee-988c-4e74-b20c-742ea936b561",
    "prId" : 28647,
    "prUrl" : "https://github.com/apache/spark/pull/28647#pullrequestreview-421362837",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a719c864-f276-422a-9731-692d3c764685",
        "parentId" : null,
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "Keep view behavior as before. Hive also does not copy view properties.",
        "createdAt" : "2020-05-29T07:38:42Z",
        "updatedAt" : "2020-11-24T09:28:18Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      },
      {
        "id" : "af71b11f-0e61-4d67-b6c7-da04ab6cb0bb",
        "parentId" : "a719c864-f276-422a-9731-692d3c764685",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "nit format;\r\n```\r\n\r\n    val newProperties = sourceTableDesc.tableType match {\r\n      case MANAGED | EXTERNAL =>\r\n        // Hive only retain the useful properties through serde class annotation.\r\n        // For better compatible with Hive, we remove the metastore properties.\r\n        sourceTableDesc.properties -- DDLUtils.METASTORE_GENERATED_PROPERTIES ++ properties\r\n\r\n      case VIEW =>\r\n        // For view, we just use new properties\r\n        properties\r\n    }\r\n```",
        "createdAt" : "2020-05-30T00:58:25Z",
        "updatedAt" : "2020-11-24T09:28:18Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "c45489ad5b8ddd53d5e81fbba4cd08c0b4fd9850",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +132,136 @@      case VIEW =>\n        // For view, we just use the new properties\n        properties\n      case other =>\n        throw new IllegalArgumentException("
  },
  {
    "id" : "f5c219f8-17a3-47cb-9c09-4eab8af80ea4",
    "prId" : 28647,
    "prUrl" : "https://github.com/apache/spark/pull/28647#pullrequestreview-440692694",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "88047843-e89a-4abd-b570-efcd6d46c2c5",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Why not just compare `newProvider` with `sourceTableDesc.provider`?\r\n\r\nWhen `provider.isEmpty` is true, it covers view case, Hive provider, do we need to copy properties for all these cases?",
        "createdAt" : "2020-06-30T21:56:10Z",
        "updatedAt" : "2020-11-24T09:28:18Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "162ae71c-f6f1-4afb-b987-8ca4ee0bb0c9",
        "parentId" : "88047843-e89a-4abd-b570-efcd6d46c2c5",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "`newProvider` checked `VIEW` and always used `parquet` as the provider.\r\n\r\nWe only copy properties for tables,  and already checked at below code\r\n```\r\nval newProperties = sourceTableDesc.tableType match {\r\n      case MANAGED | EXTERNAL if needCopyProperties =>\r\n        // Hive only retain the useful properties through serde class annotation.\r\n        // For better compatible with Hive, we remove the metastore properties.\r\n        sourceTableDesc.properties -- DDLUtils.METASTORE_GENERATED_PROPERTIES ++ properties\r\n      case MANAGED | EXTERNAL =>\r\n        properties\r\n      case VIEW =>\r\n        // For view, we just use new properties\r\n        properties\r\n      case other =>\r\n        throw new IllegalArgumentException(\r\n          s\"Unknown table type is found at createTableLikeCommand: $other\")\r\n    }\r\n```",
        "createdAt" : "2020-07-01T00:44:39Z",
        "updatedAt" : "2020-11-24T09:28:18Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      },
      {
        "id" : "6fd29778-1e35-459e-8c6d-0c6e728f26e5",
        "parentId" : "88047843-e89a-4abd-b570-efcd6d46c2c5",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Another case is `provider.isDefined` is false and `fileFormat.inputFormat.isDefined` is true. We might copy properties for it if `fileFormat.inputFormat == sourceTableDesc.storage.inputFormat`. Is it valid to copy properties for this case? When the source provider is different to Hive provider?",
        "createdAt" : "2020-07-01T00:55:40Z",
        "updatedAt" : "2020-11-24T09:28:18Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "f06a0981-2941-4357-ada0-1a73d1454e05",
        "parentId" : "88047843-e89a-4abd-b570-efcd6d46c2c5",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "`inputFormat` only used in Hive provider right ? it will not happen.",
        "createdAt" : "2020-07-01T01:22:28Z",
        "updatedAt" : "2020-11-24T09:28:18Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      },
      {
        "id" : "e78aa7a8-55d9-4735-ae43-b1379447bd3f",
        "parentId" : "88047843-e89a-4abd-b570-efcd6d46c2c5",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "We already have tests for all the case described above?",
        "createdAt" : "2020-07-01T01:28:20Z",
        "updatedAt" : "2020-11-24T09:28:18Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "defc6fe4-be9c-4086-8f96-19d18b8831fa",
        "parentId" : "88047843-e89a-4abd-b570-efcd6d46c2c5",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "ok, sounds good.",
        "createdAt" : "2020-07-01T01:29:00Z",
        "updatedAt" : "2020-11-24T09:28:18Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "d144f61c-0519-472d-8ac7-82a4329565ea",
        "parentId" : "88047843-e89a-4abd-b570-efcd6d46c2c5",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "+1 for @maropu's comment. If we have tests for them, it should be good.",
        "createdAt" : "2020-07-01T01:33:38Z",
        "updatedAt" : "2020-11-24T09:28:18Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "b33f3eaa-8962-4eba-b96f-1ce2ca46c5b9",
        "parentId" : "88047843-e89a-4abd-b570-efcd6d46c2c5",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "added more test.",
        "createdAt" : "2020-07-01T09:17:17Z",
        "updatedAt" : "2020-11-24T09:28:18Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      }
    ],
    "commit" : "c45489ad5b8ddd53d5e81fbba4cd08c0b4fd9850",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +121,125 @@    // We only copy source tbl properties if the format is the same with each other\n    val needCopyProperties =\n      (provider.isEmpty || provider == sourceTableDesc.provider) &&\n      (fileFormat.inputFormat.isEmpty ||\n        fileFormat.inputFormat == sourceTableDesc.storage.inputFormat)"
  },
  {
    "id" : "68e66a8e-57c7-41dd-98b5-c5ee99d86cae",
    "prId" : 28647,
    "prUrl" : "https://github.com/apache/spark/pull/28647#pullrequestreview-440488562",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "05bafc48-ba1b-498f-b183-c9f46658852d",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Plz update the comment, too? `if the formats is the same..`\r\n",
        "createdAt" : "2020-07-01T01:27:01Z",
        "updatedAt" : "2020-11-24T09:28:18Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "c45489ad5b8ddd53d5e81fbba4cd08c0b4fd9850",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +60,64 @@ * by default. Note that, tblproperties is copied into a new table if the table formats including\n * data source providers and storage input formats are the same.\n *\n * Use \"CREATE TABLE t1 LIKE t2 USING file_format\" to specify new provider for t1.\n * For Hive compatibility, use \"CREATE TABLE t1 LIKE t2 STORED AS hiveFormat\""
  },
  {
    "id" : "ddaabbe6-0895-4b60-af28-68a957b59f7b",
    "prId" : 28231,
    "prUrl" : "https://github.com/apache/spark/pull/28231#pullrequestreview-395201499",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0529af0e-35d1-4a3c-9c8b-c963335b3476",
        "parentId" : null,
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "Or a seq-config with spark built-in datasources as default value?",
        "createdAt" : "2020-04-17T06:52:00Z",
        "updatedAt" : "2020-04-17T06:52:01Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "a7bfde986e5d6b46e1f8aab2c168c7aea3c5b57e",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +316,320 @@\n    if (targetTable.provider.isDefined && !DDLUtils.isHiveTable(targetTable) &&\n      HiveSerDe.sourceToSerDe(targetTable.provider.get).isEmpty) {\n      throw new AnalysisException(s\"LOAD DATA is not supported for '${targetTable.provider.get}'\" +\n        s\" datasource table: $tableIdentwithDB\")"
  },
  {
    "id" : "249c7edf-81d4-46aa-a6dd-ffad69749d03",
    "prId" : 27984,
    "prUrl" : "https://github.com/apache/spark/pull/27984#pullrequestreview-387000721",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "76ed7246-9097-4a08-9599-17897281eb7c",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "can we create a method to generate `CREATE VIEW`? So we can just call one method in the `if` branch.",
        "createdAt" : "2020-03-25T09:03:20Z",
        "updatedAt" : "2020-04-05T06:02:44Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "3eab8dd9-453c-48c5-98fc-487e274883f5",
        "parentId" : "76ed7246-9097-4a08-9599-17897281eb7c",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "+1 for @cloud-fan 's advice. We had better have a function similar to `showCreateDataSourceTable`.",
        "createdAt" : "2020-03-29T22:05:53Z",
        "updatedAt" : "2020-04-05T06:02:44Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "5f83b8ff-7e9f-4d10-bc6d-52c6c08de16e",
        "parentId" : "76ed7246-9097-4a08-9599-17897281eb7c",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Created a method like suggestion.",
        "createdAt" : "2020-04-03T07:31:05Z",
        "updatedAt" : "2020-04-05T06:02:44Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "920be2c1e20ec5bbddee96b7618bc17166e23b84",
    "line" : 69,
    "diffHunk" : "@@ -1,1 +1162,1166 @@      val builder = StringBuilder.newBuilder\n\n      val stmt = if (tableMetadata.tableType == VIEW) {\n        builder ++= s\"CREATE VIEW ${table.quotedString} \"\n        showCreateView(metadata, builder)"
  },
  {
    "id" : "02026764-675b-4680-bed1-1bec6ea86005",
    "prId" : 27984,
    "prUrl" : "https://github.com/apache/spark/pull/27984#pullrequestreview-387943196",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f2950799-5062-42f9-8d89-09535a3e7a35",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I think we should put it in a higher level\r\n```\r\n      val metadata = if (DDLUtils.isDatasourceTable(tableMetadata) || tableMetadata.tableType == VIEW) {\r\n        tableMetadata\r\n      } else {\r\n        ...\r\n```",
        "createdAt" : "2020-04-03T08:20:49Z",
        "updatedAt" : "2020-04-05T06:02:44Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "2b32f746-b9d2-40a4-beb8-da6884ddf011",
        "parentId" : "f2950799-5062-42f9-8d89-09535a3e7a35",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "We need the check of unsupported feature like `partitioned view` in this branch.",
        "createdAt" : "2020-04-05T06:05:49Z",
        "updatedAt" : "2020-04-05T06:06:11Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "3680b133-f6d5-4fee-9a86-b79347f6a65f",
        "parentId" : "f2950799-5062-42f9-8d89-09535a3e7a35",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "ah makes sense",
        "createdAt" : "2020-04-06T05:33:35Z",
        "updatedAt" : "2020-04-06T05:33:35Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "920be2c1e20ec5bbddee96b7618bc17166e23b84",
    "line" : 59,
    "diffHunk" : "@@ -1,1 +1153,1157 @@        }\n\n        if (tableMetadata.tableType == VIEW) {\n          tableMetadata\n        } else {"
  },
  {
    "id" : "80be55cf-429b-411e-b9ab-e2d3ba90b9aa",
    "prId" : 27942,
    "prUrl" : "https://github.com/apache/spark/pull/27942#pullrequestreview-376567913",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "aa0524cb-b40e-486d-a5ac-9b5555edfb9e",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "BTW, are all views Hive view? It looks a bit weird to treat view as Hive serde table.",
        "createdAt" : "2020-03-18T02:53:58Z",
        "updatedAt" : "2020-03-18T02:53:58Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "51486688-4d6d-4de0-8758-bd58c9719fce",
        "parentId" : "aa0524cb-b40e-486d-a5ac-9b5555edfb9e",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "We have a `DDLUtils.isDatasourceTable` check above. Will other view come to here?",
        "createdAt" : "2020-03-18T03:09:15Z",
        "updatedAt" : "2020-03-18T03:09:16Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "3634efc9-a3d4-408f-aa9a-1b507c385d73",
        "parentId" : "aa0524cb-b40e-486d-a5ac-9b5555edfb9e",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Ideally, view is not table so SHOW CREATE TABLE should not work for views. If we don't want to add a SHOW CREATE VIEW and just want to support view in SHOW CREATE TABLE, then I think it's better to not require the AS SERDE.",
        "createdAt" : "2020-03-18T03:17:48Z",
        "updatedAt" : "2020-03-18T03:17:48Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "245e6eeb-1abc-4914-a3b7-b1e49934a0a0",
        "parentId" : "aa0524cb-b40e-486d-a5ac-9b5555edfb9e",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "What you mean \"not require the AS SERDE\"? You mean \"SHOW CREATE TABLE\" should work on Hive view? But \"SHOW CREATE TABLE\" produces Spark DDL, not Hive DDL.",
        "createdAt" : "2020-03-18T03:42:30Z",
        "updatedAt" : "2020-03-18T04:10:19Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "ef27a7b5-3d40-401a-97d1-5bf6ba2fffe3",
        "parentId" : "aa0524cb-b40e-486d-a5ac-9b5555edfb9e",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "IMO there is no such thing called \"Hive view\". Spark has permanent views which can be stored in Hive metatore, which is no different from views created by Hive.\r\n\r\nFurthermore, what's the Hive specify syntax for creating views? Maybe we should support views in both SHOW CREATE TABLE and SHOW CREATE TABLE AS SERDE",
        "createdAt" : "2020-03-18T04:24:24Z",
        "updatedAt" : "2020-03-18T04:24:24Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "d0d625fd-92b2-4360-b25c-85b5041255a6",
        "parentId" : "aa0524cb-b40e-486d-a5ac-9b5555edfb9e",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "OK. I get your point now.\r\n\r\nThis is Hive syntax for creating views:\r\n\r\n```\r\nCREATE VIEW [IF NOT EXISTS] [db_name.]view_name [(column_name [COMMENT column_comment], ...) ]\r\n  [COMMENT view_comment]\r\n  [TBLPROPERTIES (property_name = property_value, ...)]\r\n  AS SELECT ...;\r\n```\r\nhttps://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-CreateView\r\n\r\n\r\nThis is Spark syntax for creating views:\r\n```\r\nCREATE [OR REPLACE] [[GLOBAL] TEMPORARY] VIEW [IF NOT EXISTS [db_name.]view_name\r\n    create_view_clauses\r\n    AS query;\r\n```\r\nhttps://spark.apache.org/docs/3.0.0-preview/sql-ref-syntax-ddl-create-view.html\r\n\r\nLooks like it is basically the same. I think we could support views in both commands as you said.",
        "createdAt" : "2020-03-18T04:44:18Z",
        "updatedAt" : "2020-03-18T04:44:18Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "fe529698-c7ca-4347-bf93-93844a4e3ac0",
        "parentId" : "aa0524cb-b40e-486d-a5ac-9b5555edfb9e",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "If so, then I think I could close this follow-up and prepare a full PR to add the support. WDYT?",
        "createdAt" : "2020-03-18T04:46:20Z",
        "updatedAt" : "2020-03-18T04:46:20Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "ac7b1d56-33b6-4ac2-9a8e-0cd8b4e4f1cb",
        "parentId" : "aa0524cb-b40e-486d-a5ac-9b5555edfb9e",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Btw, for a view, what `DDLUtils.isDatasourceTable`  will tell about it?",
        "createdAt" : "2020-03-18T04:56:09Z",
        "updatedAt" : "2020-03-18T04:56:09Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "971f7925-ffb0-4e49-99c5-c19b4bf8548b",
        "parentId" : "aa0524cb-b40e-486d-a5ac-9b5555edfb9e",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "SGTM. To check a view, the provider should be empty and the table type should be VIEW.",
        "createdAt" : "2020-03-18T05:09:14Z",
        "updatedAt" : "2020-03-18T05:09:15Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "05028bd6-6407-4a9e-bd9e-7f01f1515090",
        "parentId" : "aa0524cb-b40e-486d-a5ac-9b5555edfb9e",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "`DDLUtils.isDatasourceTable` and `DDLUtils.isHiveTable` both return false for views.",
        "createdAt" : "2020-03-18T05:09:58Z",
        "updatedAt" : "2020-03-18T05:09:59Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "af99434aa34b47901c8fce9a92fad85ea527f15b",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1111,1115 @@\n        if (tableMetadata.tableType == VIEW) {\n          throw new AnalysisException(\"Hive view isn't supported by SHOW CREATE TABLE. \" +\n            s\"Please use `SHOW CREATE TABLE ${tableMetadata.identifier} AS SERDE` \" +\n            \"to show Hive DDL instead.\")"
  },
  {
    "id" : "b8b67102-82c8-4e9d-ba5d-dcd115363416",
    "prId" : 27548,
    "prUrl" : "https://github.com/apache/spark/pull/27548#pullrequestreview-357815703",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a72672d5-5483-4435-ad85-d8cb53ffcd5b",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Hi, @viirya . So, does your cluster has no default ACL configuration in production? I'm wondering if that is intentional or not.",
        "createdAt" : "2020-02-12T20:44:49Z",
        "updatedAt" : "2020-02-12T21:31:08Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "f41ea190-dee2-45ea-98be-d636890a06ff",
        "parentId" : "a72672d5-5483-4435-ad85-d8cb53ffcd5b",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Usually, `directory` has default ACL configuration and the new directory inherits from the parent and builds its own default ACL, doesn't it?",
        "createdAt" : "2020-02-12T20:53:36Z",
        "updatedAt" : "2020-02-12T21:31:08Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "f77adae7-5282-4699-b838-b0306553a791",
        "parentId" : "a72672d5-5483-4435-ad85-d8cb53ffcd5b",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "I'm not sure. For the failed case of our customer, there is no default ACLs so causing this issue. I think we should not assume it always has it.",
        "createdAt" : "2020-02-12T21:25:56Z",
        "updatedAt" : "2020-02-12T21:31:08Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "7348ffae-c0a3-4fc5-9a2c-1ee1b35c952e",
        "parentId" : "a72672d5-5483-4435-ad85-d8cb53ffcd5b",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Ya. The approach is correct for that case. I was wondering the default ACL is removed by the user or not in that cluster.",
        "createdAt" : "2020-02-12T21:54:02Z",
        "updatedAt" : "2020-02-12T21:54:03Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "7568db7d05b6f873b4ee63c5324ebe70ba99a2cf",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +540,544 @@            }\n            optAcls.foreach { acls =>\n              val aclEntries = acls.asScala.filter(_.getName != null).asJava\n\n              // If the path doesn't have default ACLs, `setAcl` API will throw an error"
  },
  {
    "id" : "2f76730a-96a6-4854-85f7-98148ab91e21",
    "prId" : 27548,
    "prUrl" : "https://github.com/apache/spark/pull/27548#pullrequestreview-357799018",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4b57d7cc-c06b-40c6-9003-fb9e9fbd8a60",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Could you revise a comment to mention that this happens only when the default ACL configuration doesn't exist?",
        "createdAt" : "2020-02-12T20:46:17Z",
        "updatedAt" : "2020-02-12T21:31:08Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "86508712-859e-4d23-b178-034dbb96783d",
        "parentId" : "4b57d7cc-c06b-40c6-9003-fb9e9fbd8a60",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Yea, sure. Let me revise it.",
        "createdAt" : "2020-02-12T21:26:25Z",
        "updatedAt" : "2020-02-12T21:31:08Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "7568db7d05b6f873b4ee63c5324ebe70ba99a2cf",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +545,549 @@              // as it expects user/group/other permissions must be in ACL entries.\n              // So we need to add tradition user/group/other permission\n              // in the form of ACL.\n              optPermission.map { permission =>\n                aclEntries.add(newAclEntry(AclEntryScope.ACCESS,"
  },
  {
    "id" : "0e5727ec-7186-46e9-ad50-057ed43d7af3",
    "prId" : 27505,
    "prUrl" : "https://github.com/apache/spark/pull/27505#pullrequestreview-355650362",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b5db915c-bfd8-4a3e-9639-76e44c79426a",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Do we have any test for this error case?",
        "createdAt" : "2020-02-09T23:18:38Z",
        "updatedAt" : "2020-02-09T23:18:39Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "f4a2307c-f22a-47a9-adce-ba3bfa18c1f6",
        "parentId" : "b5db915c-bfd8-4a3e-9639-76e44c79426a",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Yea, test case `hive partitioned view is not supported` in `HiveShowCreateTableSuite` is for this error.",
        "createdAt" : "2020-02-10T00:22:13Z",
        "updatedAt" : "2020-02-10T00:22:14Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "66f6d65c-85c8-4145-a9c7-3e93ad69540e",
        "parentId" : "b5db915c-bfd8-4a3e-9639-76e44c79426a",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Oh, I see. Thanks.",
        "createdAt" : "2020-02-10T00:44:04Z",
        "updatedAt" : "2020-02-10T00:44:04Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "e5f496e0303e07785e006f5231bbe40b2dfa8ae5",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +1079,1083 @@              tableMetadata.unsupportedFeatures.map(\" - \" + _).mkString(\"\\n\") + \". \" +\n              s\"Please use `SHOW CREATE TABLE ${tableMetadata.identifier} AS SERDE` \" +\n              \"to show Hive DDL instead.\"\n          )\n        }"
  },
  {
    "id" : "a21d7c2f-fad2-4d9a-aeae-877e841aef72",
    "prId" : 26956,
    "prUrl" : "https://github.com/apache/spark/pull/26956#pullrequestreview-337952317",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "53cbc3fa-0c8b-45d6-b0df-49e0bc749aae",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "not familiar with Hadoop FS APIs, but do we have something like `rm -rf tablePath/*`?. My point is, if we don't delete the parent folder, then we don't have this problem at all.",
        "createdAt" : "2020-01-02T07:59:46Z",
        "updatedAt" : "2020-01-10T01:19:12Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "f52f4e4f-af34-47a2-8970-74b7fe45cdd7",
        "parentId" : "53cbc3fa-0c8b-45d6-b0df-49e0bc749aae",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "In the [FileSystem API](https://hadoop.apache.org/docs/current/api/org/apache/hadoop/fs/FileSystem.html), I didn't find one like that. \r\n\r\nBut I just search again, there is [FileUtil API ](https://hadoop.apache.org/docs/current/api/org/apache/hadoop/fs/FileUtil.html#fullyDeleteContents-java.io.File-) that seems working like that.\r\n\r\nI will test tomorrow to see if it work well to keep permission/acl in a Spark cluster like current approach.",
        "createdAt" : "2020-01-02T08:14:37Z",
        "updatedAt" : "2020-01-10T01:19:12Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "b6e1f00e-b091-4742-878d-1be4882471ec",
        "parentId" : "53cbc3fa-0c8b-45d6-b0df-49e0bc749aae",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "oh, I think `FileUtil.fullyDeleteContents` only works on local file (java.io.File), not for files on distributed file system like HDFS.\r\n\r\n",
        "createdAt" : "2020-01-02T08:45:54Z",
        "updatedAt" : "2020-01-10T01:19:12Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "79effc63-ed4f-4eb6-9594-9464d1af9802",
        "parentId" : "53cbc3fa-0c8b-45d6-b0df-49e0bc749aae",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "So I think there is no single API to do `rm -rf tablePath/*` in Hadoop FS, if I haven't missed anything.\r\n\r\nWe can do `listStatus` and delete all contents in the given directory. But it is inefficient for DFS and is easier to have error.",
        "createdAt" : "2020-01-02T16:49:27Z",
        "updatedAt" : "2020-01-10T01:19:12Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "b504291a-5587-46d0-96b7-416f0041b48d",
        "parentId" : "53cbc3fa-0c8b-45d6-b0df-49e0bc749aae",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Do you know how Hive/Presto implement TRUNCATE TABLE? Are there other file attributes we need to retain?",
        "createdAt" : "2020-01-02T18:23:02Z",
        "updatedAt" : "2020-01-10T01:19:12Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "d07bae15-d47a-42b2-bb35-09d631cc5be0",
        "parentId" : "53cbc3fa-0c8b-45d6-b0df-49e0bc749aae",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "For Hive, I try to trace the code path for TRUNCATE TABLE:\r\n\r\n1. HiveMetaStore. truncateTableInternal:\r\n\r\nGet the locations of table/partitions and call Warehouse.deleteDir for each.\r\n\r\nhttps://github.com/apache/hive/blob/master/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java#L3109\r\n\r\n2. Warehouse.deleteDir delegates to MetaStoreFS.deleteDir.\r\n3. HiveMetaStoreFsImpl (implements MetaStoreFS) delegates to FileUtils.moveToTrash which calls `FileSystem.delete(f, true)`. \r\n\r\nIn `HiveMetaStore.truncateTableInternal`, after the location is deleted, new directory is created and previous file status including permissions, group, and ACLs are set to new directory: \r\n\r\nhttps://github.com/apache/hive/blob/35f86c749cefc2a9972a991deed78a1c3719093d/standalone-metastore/metastore-common/src/main/java/org/apache/hadoop/hive/metastore/utils/HdfsUtils.java#L288\r\n\r\n\r\n\r\n\r\n\r\n",
        "createdAt" : "2020-01-02T20:53:08Z",
        "updatedAt" : "2020-01-10T01:19:12Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "77b4e90d-3e64-4de7-a96a-e4001aa3c7fc",
        "parentId" : "53cbc3fa-0c8b-45d6-b0df-49e0bc749aae",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Now I retain permission and ACLs. I was doing `fs.setOwner` in first commit for retaining path owner and group. But `fs.setOwner` throws exception because it is only doable by super user.\r\n\r\nIn Hive code, since it is code running the metastore server, I think it is running with enough permission to set owner/group. For Spark, we may not running it with super user.",
        "createdAt" : "2020-01-02T22:26:40Z",
        "updatedAt" : "2020-01-10T01:19:12Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "07bbf7df-a02f-4563-abd4-1434baf95cba",
        "parentId" : "53cbc3fa-0c8b-45d6-b0df-49e0bc749aae",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "In Presto, looks like it takes the approach by listing all files/directories under the path to delete (i.e., `rm -rf tablePath/*`), and recursively deleting them:\r\n\r\nhttps://github.com/prestodb/presto/blob/9c21aaa659f9d4927fe12bec4b8015c8cbf4722e/presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/SemiTransactionalHiveMetastore.java#L485\r\n\r\nhttps://github.com/prestodb/presto/blob/9c21aaa659f9d4927fe12bec4b8015c8cbf4722e/presto-hive-metastore/src/main/java/com/facebook/presto/hive/metastore/SemiTransactionalHiveMetastore.java#L1796",
        "createdAt" : "2020-01-03T00:12:14Z",
        "updatedAt" : "2020-01-10T01:19:12Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "6572263a-d45b-4607-8963-d891334e0862",
        "parentId" : "53cbc3fa-0c8b-45d6-b0df-49e0bc749aae",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "the presto way seems safer to me. ",
        "createdAt" : "2020-01-03T03:54:31Z",
        "updatedAt" : "2020-01-10T01:19:12Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "2d975afd-5527-41c4-9ed3-cf6d4d9df844",
        "parentId" : "53cbc3fa-0c8b-45d6-b0df-49e0bc749aae",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "I was concerned with the presto way on performance regression. For a table which has many files/directories, deleting them one by one could be a bottleneck.\r\n\r\nIf we add a config for retaining permission/ACLs when truncating table? Users can choose to disable it and directly delete top directory (current behavior) without retaining permission/ACLs.",
        "createdAt" : "2020-01-03T05:29:20Z",
        "updatedAt" : "2020-01-10T01:19:12Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "8e429a364fc76573b7ee3637ccaaeef8269700d0",
    "line" : 37,
    "diffHunk" : "@@ -1,1 +521,525 @@          }\n\n          fs.delete(path, true)\n\n          // We should keep original permission/acl of the path."
  },
  {
    "id" : "e3c3e5f5-be09-4b1d-9bd6-bb7d01a2f178",
    "prId" : 26944,
    "prUrl" : "https://github.com/apache/spark/pull/26944#pullrequestreview-335033189",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7bf7f3e9-529e-408a-9f18-92d7e8ef4185",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Can you add tests for quoted text cases?",
        "createdAt" : "2019-12-20T00:50:40Z",
        "updatedAt" : "2020-01-03T09:38:20Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "498a608c-5e5f-4585-973d-200ea26351cf",
        "parentId" : "7bf7f3e9-529e-408a-9f18-92d7e8ef4185",
        "authorId" : "78cb7df6-a1c6-4bcb-99c1-16fd18fb32f7",
        "body" : "test added, thanks",
        "createdAt" : "2019-12-20T01:51:02Z",
        "updatedAt" : "2020-01-03T09:38:20Z",
        "lastEditedBy" : "78cb7df6-a1c6-4bcb-99c1-16fd18fb32f7",
        "tags" : [
        ]
      }
    ],
    "commit" : "c054672979e524330050d4389cc9da10371044de",
    "line" : 27,
    "diffHunk" : "@@ -1,1 +1035,1039 @@      val viewColumns = metadata.schema.map { f =>\n        val comment = f.getComment()\n          .map(escapeSingleQuotedString)\n          .map(\" COMMENT '\" + _ + \"'\")\n"
  },
  {
    "id" : "42ba5b75-f4e7-4510-8264-7d53cd640031",
    "prId" : 26927,
    "prUrl" : "https://github.com/apache/spark/pull/26927#pullrequestreview-335054427",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c025f23c-d151-45a2-ad6f-964f0c046f02",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "How about just simply writing it like this?\r\n```\r\n\r\n    val catalogStats = catalog.getTempViewOrPermanentTableMetadata(table).stats\r\n    val cs = if (conf.caseSensitiveAnalysis) {\r\n      catalogStats.flatMap { cs => cs.colStats.get(field.name) }\r\n    } else {\r\n      catalogStats.flatMap { cs =>\r\n        cs.colStats.map { case (k, v) => (k.toLowerCase(Locale.ROOT), v) }\r\n          .get(field.name.toLowerCase(Locale.ROOT))\r\n      }\r\n    }\r\n```",
        "createdAt" : "2019-12-20T02:16:11Z",
        "updatedAt" : "2019-12-24T12:28:22Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "3ea2584a-4125-4f8b-81cc-537fe1ee2170",
        "parentId" : "c025f23c-d151-45a2-ad6f-964f0c046f02",
        "authorId" : "8e72afd5-de89-4212-aae0-4b1455275a3f",
        "body" : "I have reworked on the change suggested. Could you review?",
        "createdAt" : "2019-12-20T03:35:05Z",
        "updatedAt" : "2019-12-24T12:28:22Z",
        "lastEditedBy" : "8e72afd5-de89-4212-aae0-4b1455275a3f",
        "tags" : [
        ]
      }
    ],
    "commit" : "5ff17aee94575b08c59dc149891b334560f718da",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +724,728 @@    val colStats = if (conf.caseSensitiveAnalysis) colStatsMap else CaseInsensitiveMap(colStatsMap)\n    val cs = colStats.get(field.name)\n\n    val comment = if (field.metadata.contains(\"comment\")) {\n      Option(field.metadata.getString(\"comment\"))"
  }
]