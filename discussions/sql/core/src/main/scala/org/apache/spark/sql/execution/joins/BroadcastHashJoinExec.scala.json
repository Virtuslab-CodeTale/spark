[
  {
    "id" : "152efbd1-2199-4074-ad2a-f03648448d62",
    "prId" : 31477,
    "prUrl" : "https://github.com/apache/spark/pull/31477#pullrequestreview-612321053",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4df9d0da-d85f-41d6-9931-c6bed6775736",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "This seems like a \"progress bar\" instead of a metrics. Most likely people don't care about it because it's usually `num output rows of left` * `num output rows of right`.\r\n\r\nHave we measured the perf overhead?",
        "createdAt" : "2021-03-02T15:41:02Z",
        "updatedAt" : "2021-03-15T15:03:56Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "81bbcf9e-5dd8-4926-b8d1-3f8f1a8d3b49",
        "parentId" : "4df9d0da-d85f-41d6-9931-c6bed6775736",
        "authorId" : "cdcefa9a-1880-4b7a-ae6a-ece33e944188",
        "body" : "I am running the TPC-DS benchmark right now. I will post the results shortly.",
        "createdAt" : "2021-03-02T18:30:32Z",
        "updatedAt" : "2021-03-15T15:03:56Z",
        "lastEditedBy" : "cdcefa9a-1880-4b7a-ae6a-ece33e944188",
        "tags" : [
        ]
      },
      {
        "id" : "6c44e33e-5412-4e5c-8139-35dd9619ef83",
        "parentId" : "4df9d0da-d85f-41d6-9931-c6bed6775736",
        "authorId" : "cdcefa9a-1880-4b7a-ae6a-ece33e944188",
        "body" : "I ran the benchmark and I see a clear performance degradation for query-16 and query-94. All the other queries seem to be fine. I will debug why there is a performance degradation for  query-16 & query-94. https://docs.google.com/document/d/16_cjk3g3_7mMH2C_T9bsFroxaH_CihJkXINYirt-mtg/edit?usp=sharing \r\ncc: @cloud-fan ",
        "createdAt" : "2021-03-02T23:05:40Z",
        "updatedAt" : "2021-03-15T15:03:56Z",
        "lastEditedBy" : "cdcefa9a-1880-4b7a-ae6a-ece33e944188",
        "tags" : [
        ]
      },
      {
        "id" : "b59f6838-3e2c-4136-a51c-94c5a6a44c8e",
        "parentId" : "4df9d0da-d85f-41d6-9931-c6bed6775736",
        "authorId" : "cdcefa9a-1880-4b7a-ae6a-ece33e944188",
        "body" : "Update: I figured out why the perf degradation is occurring for query-16 & query-94 and fixed it. I ran the benchmark for a couple of iterations and looks like my fix is working. I am running the benchmark with 25 iterations to identify if any other query has been effected.",
        "createdAt" : "2021-03-05T14:52:09Z",
        "updatedAt" : "2021-03-15T15:03:56Z",
        "lastEditedBy" : "cdcefa9a-1880-4b7a-ae6a-ece33e944188",
        "tags" : [
        ]
      },
      {
        "id" : "3110f925-5266-4902-abea-9c26cf6915bc",
        "parentId" : "4df9d0da-d85f-41d6-9931-c6bed6775736",
        "authorId" : "cdcefa9a-1880-4b7a-ae6a-ece33e944188",
        "body" : "Update: In my most recent commit I fixed a perf issue that is causing a significant degradation for query-16 & quer-94. However there still seems to be some perf degradation that I cannot fix. Maybe it is because adding the new metric itself or maybe there is something wrong with the way that I am running the tests. I will run the benchmark again on a bare metal machine and see if it makes any difference. I will also see if there is anything else that can be done to improve the performance. My most recent runs are shared in this doc. https://docs.google.com/spreadsheets/d/1A0jCx5BL0wcN28oDQDOs73QhVZ71uNbKDg9Q8S0IE6g/edit?usp=sharing",
        "createdAt" : "2021-03-15T15:21:56Z",
        "updatedAt" : "2021-03-15T15:21:57Z",
        "lastEditedBy" : "cdcefa9a-1880-4b7a-ae6a-ece33e944188",
        "tags" : [
        ]
      }
    ],
    "commit" : "4c35275fa7c670cd18c16548f36cd6184ec155ff",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +59,63 @@  override lazy val metrics = Map(\n    \"numOutputRows\" -> SQLMetrics.createMetric(sparkContext, \"number of output rows\"),\n    \"numMatchedRows\" -> SQLMetrics.createMetric(sparkContext, \"number of matched rows\"))\n\n  override def requiredChildDistribution: Seq[Distribution] = {"
  },
  {
    "id" : "29e3c5f2-c0a8-4a7f-94ba-13824a1ca362",
    "prId" : 29304,
    "prUrl" : "https://github.com/apache/spark/pull/29304#pullrequestreview-461398288",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "053b34cf-87ee-48bb-b237-25e681c93036",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "HI @leanken ... I am curious why we force unsafe for NAAJ ? Is that for efficiency or is the implementation assume unsafe row ? ",
        "createdAt" : "2020-08-05T06:04:21Z",
        "updatedAt" : "2020-08-05T07:11:49Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "67165763-ac58-47ca-b3ad-d78c6a06eef3",
        "parentId" : "053b34cf-87ee-48bb-b237-25e681c93036",
        "authorId" : "cce1c782-0596-44b6-8b99-6b77d2cca53c",
        "body" : "because if it's a single Long key, keyEv will be codegen as java long, but not UnsafeRow. In InvertedIndexHashedRelation, it only takes UnsafeRow as input to lookup in buildSide.",
        "createdAt" : "2020-08-05T07:14:09Z",
        "updatedAt" : "2020-08-05T07:14:09Z",
        "lastEditedBy" : "cce1c782-0596-44b6-8b99-6b77d2cca53c",
        "tags" : [
        ]
      }
    ],
    "commit" : "054581ae1d8f6148db7ec952ac23e5668ff9dc75",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +224,228 @@    if (isNullAwareAntiJoin) {\n      val (broadcastRelation, relationTerm) = prepareBroadcast(ctx)\n      val (keyEv, anyNull) = genStreamSideJoinKey(ctx, input, true)\n      val (matched, _, _) = getJoinCondition(ctx, input)\n      val numOutput = metricTerm(ctx, \"numOutputRows\")"
  },
  {
    "id" : "fc8e4ae1-c7ab-4745-879b-f4e204f4688b",
    "prId" : 29277,
    "prUrl" : "https://github.com/apache/spark/pull/29277#pullrequestreview-457193373",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ea31d9f4-0d36-4162-b96f-600dedff6c6f",
        "parentId" : null,
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "`codegenAnti` is changed to keep NULL-aware anti join separately here, and move other logic to `HashJoin.codegenAnti`.",
        "createdAt" : "2020-07-29T04:54:49Z",
        "updatedAt" : "2020-07-30T17:47:17Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      }
    ],
    "commit" : "ac64864363891c4c942d374f227c01bd60254fce",
    "line" : 303,
    "diffHunk" : "@@ -1,1 +223,227 @@   * Handles NULL-aware anti join (NAAJ) separately here.\n   */\n  protected override def codegenAnti(ctx: CodegenContext, input: Seq[ExprCode]): String = {\n    if (isNullAwareAntiJoin) {\n      val (broadcastRelation, relationTerm) = prepareBroadcast(ctx)"
  },
  {
    "id" : "f0f86fd6-e036-452c-8cc2-26755e55f240",
    "prId" : 29277,
    "prUrl" : "https://github.com/apache/spark/pull/29277#pullrequestreview-457193373",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "36a8fb8c-9be6-4a46-9073-929582a92662",
        "parentId" : null,
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "a new method `prepareRelation` is added to call `prepareBroadcast()` and get to know whether the key is known to be unique during codegen time.",
        "createdAt" : "2020-07-29T04:56:33Z",
        "updatedAt" : "2020-07-30T17:47:17Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      }
    ],
    "commit" : "ac64864363891c4c942d374f227c01bd60254fce",
    "line" : 247,
    "diffHunk" : "@@ -1,1 +214,218 @@  }\n\n  protected override def prepareRelation(ctx: CodegenContext): (String, Boolean) = {\n    val (broadcastRelation, relationTerm) = prepareBroadcast(ctx)\n    (relationTerm, broadcastRelation.value.keyIsUnique)"
  },
  {
    "id" : "835092be-bf9a-4063-8643-dcc4f2f5e860",
    "prId" : 29104,
    "prUrl" : "https://github.com/apache/spark/pull/29104#pullrequestreview-454000520",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4c4aa38c-e4ba-4057-a570-cf5ba9e42b52",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "shall we add these 2 lines to the NAAJ branch?",
        "createdAt" : "2020-07-22T18:50:15Z",
        "updatedAt" : "2020-07-27T22:06:38Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "5f36f9b8-3502-485c-9596-384eb1f1e29f",
        "parentId" : "4c4aa38c-e4ba-4057-a570-cf5ba9e42b52",
        "authorId" : "cce1c782-0596-44b6-8b99-6b77d2cca53c",
        "body" : "ok. will do the update",
        "createdAt" : "2020-07-22T18:54:10Z",
        "updatedAt" : "2020-07-27T22:06:38Z",
        "lastEditedBy" : "cce1c782-0596-44b6-8b99-6b77d2cca53c",
        "tags" : [
        ]
      },
      {
        "id" : "f57fb3e5-d5ae-4b77-8f3c-0522697f25e0",
        "parentId" : "4c4aa38c-e4ba-4057-a570-cf5ba9e42b52",
        "authorId" : "cce1c782-0596-44b6-8b99-6b77d2cca53c",
        "body" : "done",
        "createdAt" : "2020-07-23T10:16:18Z",
        "updatedAt" : "2020-07-27T22:06:38Z",
        "lastEditedBy" : "cce1c782-0596-44b6-8b99-6b77d2cca53c",
        "tags" : [
        ]
      }
    ],
    "commit" : "233eff6549377d6c7c850fe8d1990fcd58fe0ea0",
    "line" : 62,
    "diffHunk" : "@@ -1,1 +171,175 @@      streamedPlan.execute().mapPartitions { streamedIter =>\n        val hashed = broadcastRelation.value.asReadOnlyCopy()\n        TaskContext.get().taskMetrics().incPeakExecutionMemory(hashed.estimatedSize)\n        join(streamedIter, hashed, numOutputRows)\n      }"
  }
]