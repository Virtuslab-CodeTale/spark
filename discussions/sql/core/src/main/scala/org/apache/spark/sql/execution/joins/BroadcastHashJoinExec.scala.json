[
  {
    "id" : "152efbd1-2199-4074-ad2a-f03648448d62",
    "prId" : 31477,
    "prUrl" : "https://github.com/apache/spark/pull/31477#pullrequestreview-612321053",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4df9d0da-d85f-41d6-9931-c6bed6775736",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "This seems like a \"progress bar\" instead of a metrics. Most likely people don't care about it because it's usually `num output rows of left` * `num output rows of right`.\r\n\r\nHave we measured the perf overhead?",
        "createdAt" : "2021-03-02T15:41:02Z",
        "updatedAt" : "2021-03-15T15:03:56Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "81bbcf9e-5dd8-4926-b8d1-3f8f1a8d3b49",
        "parentId" : "4df9d0da-d85f-41d6-9931-c6bed6775736",
        "authorId" : "cdcefa9a-1880-4b7a-ae6a-ece33e944188",
        "body" : "I am running the TPC-DS benchmark right now. I will post the results shortly.",
        "createdAt" : "2021-03-02T18:30:32Z",
        "updatedAt" : "2021-03-15T15:03:56Z",
        "lastEditedBy" : "cdcefa9a-1880-4b7a-ae6a-ece33e944188",
        "tags" : [
        ]
      },
      {
        "id" : "6c44e33e-5412-4e5c-8139-35dd9619ef83",
        "parentId" : "4df9d0da-d85f-41d6-9931-c6bed6775736",
        "authorId" : "cdcefa9a-1880-4b7a-ae6a-ece33e944188",
        "body" : "I ran the benchmark and I see a clear performance degradation for query-16 and query-94. All the other queries seem to be fine. I will debug why there is a performance degradation for  query-16 & query-94. https://docs.google.com/document/d/16_cjk3g3_7mMH2C_T9bsFroxaH_CihJkXINYirt-mtg/edit?usp=sharing \r\ncc: @cloud-fan ",
        "createdAt" : "2021-03-02T23:05:40Z",
        "updatedAt" : "2021-03-15T15:03:56Z",
        "lastEditedBy" : "cdcefa9a-1880-4b7a-ae6a-ece33e944188",
        "tags" : [
        ]
      },
      {
        "id" : "b59f6838-3e2c-4136-a51c-94c5a6a44c8e",
        "parentId" : "4df9d0da-d85f-41d6-9931-c6bed6775736",
        "authorId" : "cdcefa9a-1880-4b7a-ae6a-ece33e944188",
        "body" : "Update: I figured out why the perf degradation is occurring for query-16 & query-94 and fixed it. I ran the benchmark for a couple of iterations and looks like my fix is working. I am running the benchmark with 25 iterations to identify if any other query has been effected.",
        "createdAt" : "2021-03-05T14:52:09Z",
        "updatedAt" : "2021-03-15T15:03:56Z",
        "lastEditedBy" : "cdcefa9a-1880-4b7a-ae6a-ece33e944188",
        "tags" : [
        ]
      },
      {
        "id" : "3110f925-5266-4902-abea-9c26cf6915bc",
        "parentId" : "4df9d0da-d85f-41d6-9931-c6bed6775736",
        "authorId" : "cdcefa9a-1880-4b7a-ae6a-ece33e944188",
        "body" : "Update: In my most recent commit I fixed a perf issue that is causing a significant degradation for query-16 & quer-94. However there still seems to be some perf degradation that I cannot fix. Maybe it is because adding the new metric itself or maybe there is something wrong with the way that I am running the tests. I will run the benchmark again on a bare metal machine and see if it makes any difference. I will also see if there is anything else that can be done to improve the performance. My most recent runs are shared in this doc. https://docs.google.com/spreadsheets/d/1A0jCx5BL0wcN28oDQDOs73QhVZ71uNbKDg9Q8S0IE6g/edit?usp=sharing",
        "createdAt" : "2021-03-15T15:21:56Z",
        "updatedAt" : "2021-03-15T15:21:57Z",
        "lastEditedBy" : "cdcefa9a-1880-4b7a-ae6a-ece33e944188",
        "tags" : [
        ]
      }
    ],
    "commit" : "4c35275fa7c670cd18c16548f36cd6184ec155ff",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +59,63 @@  override lazy val metrics = Map(\n    \"numOutputRows\" -> SQLMetrics.createMetric(sparkContext, \"number of output rows\"),\n    \"numMatchedRows\" -> SQLMetrics.createMetric(sparkContext, \"number of matched rows\"))\n\n  override def requiredChildDistribution: Seq[Distribution] = {"
  },
  {
    "id" : "29e3c5f2-c0a8-4a7f-94ba-13824a1ca362",
    "prId" : 29304,
    "prUrl" : "https://github.com/apache/spark/pull/29304#pullrequestreview-461398288",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "053b34cf-87ee-48bb-b237-25e681c93036",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "HI @leanken ... I am curious why we force unsafe for NAAJ ? Is that for efficiency or is the implementation assume unsafe row ? ",
        "createdAt" : "2020-08-05T06:04:21Z",
        "updatedAt" : "2020-08-05T07:11:49Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "67165763-ac58-47ca-b3ad-d78c6a06eef3",
        "parentId" : "053b34cf-87ee-48bb-b237-25e681c93036",
        "authorId" : "cce1c782-0596-44b6-8b99-6b77d2cca53c",
        "body" : "because if it's a single Long key, keyEv will be codegen as java long, but not UnsafeRow. In InvertedIndexHashedRelation, it only takes UnsafeRow as input to lookup in buildSide.",
        "createdAt" : "2020-08-05T07:14:09Z",
        "updatedAt" : "2020-08-05T07:14:09Z",
        "lastEditedBy" : "cce1c782-0596-44b6-8b99-6b77d2cca53c",
        "tags" : [
        ]
      }
    ],
    "commit" : "054581ae1d8f6148db7ec952ac23e5668ff9dc75",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +224,228 @@    if (isNullAwareAntiJoin) {\n      val (broadcastRelation, relationTerm) = prepareBroadcast(ctx)\n      val (keyEv, anyNull) = genStreamSideJoinKey(ctx, input, true)\n      val (matched, _, _) = getJoinCondition(ctx, input)\n      val numOutput = metricTerm(ctx, \"numOutputRows\")"
  },
  {
    "id" : "fc8e4ae1-c7ab-4745-879b-f4e204f4688b",
    "prId" : 29277,
    "prUrl" : "https://github.com/apache/spark/pull/29277#pullrequestreview-457193373",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ea31d9f4-0d36-4162-b96f-600dedff6c6f",
        "parentId" : null,
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "`codegenAnti` is changed to keep NULL-aware anti join separately here, and move other logic to `HashJoin.codegenAnti`.",
        "createdAt" : "2020-07-29T04:54:49Z",
        "updatedAt" : "2020-07-30T17:47:17Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      }
    ],
    "commit" : "ac64864363891c4c942d374f227c01bd60254fce",
    "line" : 303,
    "diffHunk" : "@@ -1,1 +223,227 @@   * Handles NULL-aware anti join (NAAJ) separately here.\n   */\n  protected override def codegenAnti(ctx: CodegenContext, input: Seq[ExprCode]): String = {\n    if (isNullAwareAntiJoin) {\n      val (broadcastRelation, relationTerm) = prepareBroadcast(ctx)"
  },
  {
    "id" : "f0f86fd6-e036-452c-8cc2-26755e55f240",
    "prId" : 29277,
    "prUrl" : "https://github.com/apache/spark/pull/29277#pullrequestreview-457193373",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "36a8fb8c-9be6-4a46-9073-929582a92662",
        "parentId" : null,
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "a new method `prepareRelation` is added to call `prepareBroadcast()` and get to know whether the key is known to be unique during codegen time.",
        "createdAt" : "2020-07-29T04:56:33Z",
        "updatedAt" : "2020-07-30T17:47:17Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      }
    ],
    "commit" : "ac64864363891c4c942d374f227c01bd60254fce",
    "line" : 247,
    "diffHunk" : "@@ -1,1 +214,218 @@  }\n\n  protected override def prepareRelation(ctx: CodegenContext): (String, Boolean) = {\n    val (broadcastRelation, relationTerm) = prepareBroadcast(ctx)\n    (relationTerm, broadcastRelation.value.keyIsUnique)"
  },
  {
    "id" : "835092be-bf9a-4063-8643-dcc4f2f5e860",
    "prId" : 29104,
    "prUrl" : "https://github.com/apache/spark/pull/29104#pullrequestreview-454000520",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4c4aa38c-e4ba-4057-a570-cf5ba9e42b52",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "shall we add these 2 lines to the NAAJ branch?",
        "createdAt" : "2020-07-22T18:50:15Z",
        "updatedAt" : "2020-07-27T22:06:38Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "5f36f9b8-3502-485c-9596-384eb1f1e29f",
        "parentId" : "4c4aa38c-e4ba-4057-a570-cf5ba9e42b52",
        "authorId" : "cce1c782-0596-44b6-8b99-6b77d2cca53c",
        "body" : "ok. will do the update",
        "createdAt" : "2020-07-22T18:54:10Z",
        "updatedAt" : "2020-07-27T22:06:38Z",
        "lastEditedBy" : "cce1c782-0596-44b6-8b99-6b77d2cca53c",
        "tags" : [
        ]
      },
      {
        "id" : "f57fb3e5-d5ae-4b77-8f3c-0522697f25e0",
        "parentId" : "4c4aa38c-e4ba-4057-a570-cf5ba9e42b52",
        "authorId" : "cce1c782-0596-44b6-8b99-6b77d2cca53c",
        "body" : "done",
        "createdAt" : "2020-07-23T10:16:18Z",
        "updatedAt" : "2020-07-27T22:06:38Z",
        "lastEditedBy" : "cce1c782-0596-44b6-8b99-6b77d2cca53c",
        "tags" : [
        ]
      }
    ],
    "commit" : "233eff6549377d6c7c850fe8d1990fcd58fe0ea0",
    "line" : 62,
    "diffHunk" : "@@ -1,1 +171,175 @@      streamedPlan.execute().mapPartitions { streamedIter =>\n        val hashed = broadcastRelation.value.asReadOnlyCopy()\n        TaskContext.get().taskMetrics().incPeakExecutionMemory(hashed.estimatedSize)\n        join(streamedIter, hashed, numOutputRows)\n      }"
  },
  {
    "id" : "9f72b214-d614-4f55-97cc-fcc27ec205cb",
    "prId" : 28676,
    "prUrl" : "https://github.com/apache/spark/pull/28676#pullrequestreview-446076707",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bb3c566d-0138-4f22-934b-26e686ce0cb2",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Why do the keys need to be bound?",
        "createdAt" : "2020-07-08T07:21:28Z",
        "updatedAt" : "2020-07-17T20:59:39Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "897ad749-0b14-4194-af58-ae13a1176fc2",
        "parentId" : "bb3c566d-0138-4f22-934b-26e686ce0cb2",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "Because this `mode` is  passed to `BroadcastExchangeExec` and eventually used to create `UnsafeProjection`. Btw, this is an existing code, but renamed per [this discussion](https://github.com/apache/spark/pull/28676#discussion_r448124006).",
        "createdAt" : "2020-07-10T02:02:22Z",
        "updatedAt" : "2020-07-17T20:59:39Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "9caeecddaa07ef825b73835a3666502df468f881",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +54,58 @@\n  override def requiredChildDistribution: Seq[Distribution] = {\n    val mode = HashedRelationBroadcastMode(buildBoundKeys)\n    buildSide match {\n      case BuildLeft =>"
  },
  {
    "id" : "118ed2fe-3e69-4554-b115-8551b5809722",
    "prId" : 28676,
    "prUrl" : "https://github.com/apache/spark/pull/28676#pullrequestreview-450210455",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b9e75b07-c50b-4b9a-b5a4-1051bbfc78d9",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "nit: `expandOutputPartitioning(c).partitionings`",
        "createdAt" : "2020-07-16T11:05:24Z",
        "updatedAt" : "2020-07-17T20:59:39Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "279a5a88-d156-4aa5-aca5-ad84580436da",
        "parentId" : "b9e75b07-c50b-4b9a-b5a4-1051bbfc78d9",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "Actually, that will change the original structure by removing the nested PartitioningCollection.",
        "createdAt" : "2020-07-16T20:57:01Z",
        "updatedAt" : "2020-07-17T20:59:39Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "9caeecddaa07ef825b73835a3666502df468f881",
    "line" : 62,
    "diffHunk" : "@@ -1,1 +94,98 @@    PartitioningCollection(partitioning.partitionings.flatMap {\n      case h: HashPartitioning => expandOutputPartitioning(h).partitionings\n      case c: PartitioningCollection => Seq(expandOutputPartitioning(c))\n      case other => Seq(other)\n    })"
  },
  {
    "id" : "61fbd74e-bca8-49ba-8450-6911ebed8cb6",
    "prId" : 24666,
    "prUrl" : "https://github.com/apache/spark/pull/24666#pullrequestreview-243584044",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "093eb3ec-41fb-4232-903b-6ea7633bb24d",
        "parentId" : null,
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "IIRC, for file sources, usually there is only sizeInBytes stats in logical plan level.  So the estimated numOutputRows for logical plan should be empty for file sources.\r\nWhat is the scenario of this PR?",
        "createdAt" : "2019-05-29T15:46:52Z",
        "updatedAt" : "2019-05-29T15:46:52Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "0143cf34-c60d-4893-bc75-ecd67697f7fb",
        "parentId" : "093eb3ec-41fb-4232-903b-6ea7633bb24d",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "for file source table, there will be row count stats if CBO is enabled.",
        "createdAt" : "2019-05-30T01:00:12Z",
        "updatedAt" : "2019-05-30T01:00:13Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "aae3653e4dab751ae5bff7e3ef26ac63d4f4cc4f",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +52,56 @@          sparkContext,\n          \"number of output rows\",\n          logicalPlan.map(_.stats.rowCount.map(_.toLong).getOrElse(-1L)).getOrElse(-1L)))\n  }\n"
  }
]