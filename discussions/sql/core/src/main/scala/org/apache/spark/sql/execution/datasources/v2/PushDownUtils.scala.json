[
  {
    "id" : "13a8ff61-1539-445f-9d77-3c68f67ec099",
    "prId" : 33584,
    "prUrl" : "https://github.com/apache/spark/pull/33584#pullrequestreview-719328967",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ba3f03e1-44d9-4c77-8be9-57947d190600",
        "parentId" : null,
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "can we move this into a separate util method? so that we can share it between here and `PruneFileSourcePartitions`?",
        "createdAt" : "2021-07-30T17:40:30Z",
        "updatedAt" : "2021-07-30T18:21:30Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      }
    ],
    "commit" : "bbc85db716de1db24bdb67f2ca9075dcaa648ded",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +86,90 @@    scanBuilder match {\n      case fileBuilder: FileScanBuilder =>\n        val partitionColumns = relation.resolve(\n          fileBuilder.readPartitionSchema(),\n          fileBuilder.getSparkSession.sessionState.analyzer.resolver)"
  },
  {
    "id" : "d09d6c1c-1451-4cbb-ac36-6c61341fa844",
    "prId" : 33584,
    "prUrl" : "https://github.com/apache/spark/pull/33584#pullrequestreview-719634360",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cc00cb0e-20ed-4673-b585-90d7bacdc1d2",
        "parentId" : null,
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "I think this name is a bit confusing: it doesn't **push** the partition filters but rather just set the filters in the scan builder?",
        "createdAt" : "2021-07-30T18:03:46Z",
        "updatedAt" : "2021-07-30T18:21:30Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "3385bd2e-49c8-4031-900d-4ac0c14405b9",
        "parentId" : "cc00cb0e-20ed-4673-b585-90d7bacdc1d2",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "So you want to move partition pruning for `DataSourceV2ScanRelation` from `PruneFileSourcePartitions` to `pushPartitionFilters`?",
        "createdAt" : "2021-08-01T07:59:09Z",
        "updatedAt" : "2021-08-01T08:07:03Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "bbc85db716de1db24bdb67f2ca9075dcaa648ded",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +80,84 @@   * @return pushed partition filters.\n   */\n  def pushPartitionFilters(\n      scanBuilder: ScanBuilder,\n      relation: DataSourceV2Relation,"
  },
  {
    "id" : "2df95682-19cb-46ce-b982-204c1e718688",
    "prId" : 33584,
    "prUrl" : "https://github.com/apache/spark/pull/33584#pullrequestreview-723076829",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6a945675-4f46-45ce-b6c9-e2f3be681ac7",
        "parentId" : null,
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "```\r\nIf we don't want to touch PruneFileSourcePartitions, I guess instead of checking if the postScanFilter is empty using if filters.isEmpty, we do something like this:\r\n\r\nif (JDBC)\r\n  if filters.isEmpty\r\n     push down aggregate\r\nelse // file based\r\n  if filters are only partition filters\r\n    push down aggregate\r\nThis looks hacky though. Please let me know if anybody has a better idea.\r\n```\r\n@huaxingao I don't have a better idea either. \r\nBut you have to match special case `FileScanBuilder` here anyway. I would prefer resolving the partition filters in `pushAggregates`, which is more straightforward. ",
        "createdAt" : "2021-08-05T05:18:34Z",
        "updatedAt" : "2021-08-05T05:18:34Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "015cce76-ef3e-4531-8225-34cd4418be75",
        "parentId" : "6a945675-4f46-45ce-b6c9-e2f3be681ac7",
        "authorId" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "body" : "@gengliangwang OK. I will not change `PruneFileSourcePartitions`. I will close this PR and find other way to do this. Thank you very much for your help!",
        "createdAt" : "2021-08-05T08:28:19Z",
        "updatedAt" : "2021-08-05T08:28:19Z",
        "lastEditedBy" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "tags" : [
        ]
      }
    ],
    "commit" : "bbc85db716de1db24bdb67f2ca9075dcaa648ded",
    "line" : 50,
    "diffHunk" : "@@ -1,1 +147,151 @@      filters: Seq[Expression]): (Scan, Seq[AttributeReference]) = {\n    val filePartitionFilter = scanBuilder match {\n      case fileScanBuilder: FileScanBuilder => fileScanBuilder.getPartitionFilters\n      case _ => Seq.empty[Expression]\n    }"
  },
  {
    "id" : "3798541c-016e-48fb-9c28-a7571bb130c7",
    "prId" : 33352,
    "prUrl" : "https://github.com/apache/spark/pull/33352#pullrequestreview-715301561",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "19fed2d4-d900-4110-9b0a-7f182f773cea",
        "parentId" : null,
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "nit: you can just use `Some(agg).filter(r.pushAggregation)`",
        "createdAt" : "2021-07-26T21:42:35Z",
        "updatedAt" : "2021-07-26T21:52:10Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      }
    ],
    "commit" : "fae570a68cde1cc25ae60e7ba74651e7972578e3",
    "line" : 45,
    "diffHunk" : "@@ -1,1 +102,106 @@\n        val agg = new Aggregation(translatedAggregates.toArray, translatedGroupBys.toArray)\n        if (r.pushAggregation(agg)) {\n          Some(agg)\n        } else {"
  },
  {
    "id" : "ad3c8cca-3e60-49da-bfad-4bbbdebd6d9d",
    "prId" : 33352,
    "prUrl" : "https://github.com/apache/spark/pull/33352#pullrequestreview-715301561",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4a0f33f5-70d7-4029-a260-593cf55976cf",
        "parentId" : null,
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "nit: can use `flatMap` instead of `map` + `flatten`.",
        "createdAt" : "2021-07-26T21:44:29Z",
        "updatedAt" : "2021-07-26T21:52:10Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      }
    ],
    "commit" : "fae570a68cde1cc25ae60e7ba74651e7972578e3",
    "line" : 36,
    "diffHunk" : "@@ -1,1 +93,97 @@    scanBuilder match {\n      case r: SupportsPushDownAggregates =>\n        val translatedAggregates = aggregates.map(DataSourceStrategy.translateAggregate).flatten\n        val translatedGroupBys = groupBy.map(columnAsString).flatten\n"
  },
  {
    "id" : "e94ee1f9-80bc-47ba-b619-3a729b4d8770",
    "prId" : 31993,
    "prUrl" : "https://github.com/apache/spark/pull/31993#pullrequestreview-634059678",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b6f33b48-c09c-4b96-903c-b2d95b6c681d",
        "parentId" : null,
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "Move [filter logical from `SchemaPruning`](https://github.com/apache/spark/blob/0f2c0b53e8fb18c86c67b5dd679c006db93f94a5/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/SchemaPruning.scala#L38-L39) to `PushDownUtils` to support datasource V2 column pruning.\r\n",
        "createdAt" : "2021-04-13T00:16:23Z",
        "updatedAt" : "2021-04-21T09:55:49Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      }
    ],
    "commit" : "4d0b510e10da4fe1fca07583ddecc5f5fe6d6392",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +95,99 @@        }\n        val neededFieldNames = neededOutput.map(_.name).toSet\n        r.pruneColumns(StructType(prunedSchema.filter(f => neededFieldNames.contains(f.name))))\n        val scan = r.build()\n        scan -> toOutputAttrs(scan.readSchema(), relation)"
  },
  {
    "id" : "62293e80-b491-4cea-b2b0-1b8802c2f2c1",
    "prId" : 29695,
    "prUrl" : "https://github.com/apache/spark/pull/29695#pullrequestreview-627388344",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9cf90376-8fdc-4ded-877b-4f6f6d747543",
        "parentId" : null,
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "what about \"else\" branch? perhaps we should revise this code to:\r\n```scala\r\n        case r: SupportsPushDownAggregates =>\r\n          val translatedAggregates = aggregates.map(DataSourceStrategy.translateAggregate)\r\n          val translatedGroupBys = groupBy.map(columnAsString)\r\n\r\n          if (translatedAggregates.exists(_.isEmpty) || translatedGroupBys.exists(_.isEmpty)) {\r\n            Aggregation.empty\r\n          } else {\r\n            r.pushAggregation(Aggregation(translatedAggregates.flatten, translatedGroupBys))\r\n            r.pushedAggregation\r\n          }\r\n```",
        "createdAt" : "2021-04-02T23:42:10Z",
        "updatedAt" : "2021-04-03T00:08:31Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      }
    ],
    "commit" : "782a0a827fbb1e60b8673d533cbda618df65dc96",
    "line" : 49,
    "diffHunk" : "@@ -1,1 +103,107 @@          if (!groupByCols.exists(_.isEmpty)) {\n            r.pushAggregation(Aggregation(translatedAggregates, groupByCols))\n          }\n          r.pushedAggregation\n"
  }
]