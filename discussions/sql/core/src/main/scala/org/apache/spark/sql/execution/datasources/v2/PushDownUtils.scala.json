[
  {
    "id" : "13a8ff61-1539-445f-9d77-3c68f67ec099",
    "prId" : 33584,
    "prUrl" : "https://github.com/apache/spark/pull/33584#pullrequestreview-719328967",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ba3f03e1-44d9-4c77-8be9-57947d190600",
        "parentId" : null,
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "can we move this into a separate util method? so that we can share it between here and `PruneFileSourcePartitions`?",
        "createdAt" : "2021-07-30T17:40:30Z",
        "updatedAt" : "2021-07-30T18:21:30Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      }
    ],
    "commit" : "bbc85db716de1db24bdb67f2ca9075dcaa648ded",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +86,90 @@    scanBuilder match {\n      case fileBuilder: FileScanBuilder =>\n        val partitionColumns = relation.resolve(\n          fileBuilder.readPartitionSchema(),\n          fileBuilder.getSparkSession.sessionState.analyzer.resolver)"
  },
  {
    "id" : "d09d6c1c-1451-4cbb-ac36-6c61341fa844",
    "prId" : 33584,
    "prUrl" : "https://github.com/apache/spark/pull/33584#pullrequestreview-719634360",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cc00cb0e-20ed-4673-b585-90d7bacdc1d2",
        "parentId" : null,
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "I think this name is a bit confusing: it doesn't **push** the partition filters but rather just set the filters in the scan builder?",
        "createdAt" : "2021-07-30T18:03:46Z",
        "updatedAt" : "2021-07-30T18:21:30Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "3385bd2e-49c8-4031-900d-4ac0c14405b9",
        "parentId" : "cc00cb0e-20ed-4673-b585-90d7bacdc1d2",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "So you want to move partition pruning for `DataSourceV2ScanRelation` from `PruneFileSourcePartitions` to `pushPartitionFilters`?",
        "createdAt" : "2021-08-01T07:59:09Z",
        "updatedAt" : "2021-08-01T08:07:03Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "bbc85db716de1db24bdb67f2ca9075dcaa648ded",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +80,84 @@   * @return pushed partition filters.\n   */\n  def pushPartitionFilters(\n      scanBuilder: ScanBuilder,\n      relation: DataSourceV2Relation,"
  },
  {
    "id" : "2df95682-19cb-46ce-b982-204c1e718688",
    "prId" : 33584,
    "prUrl" : "https://github.com/apache/spark/pull/33584#pullrequestreview-723076829",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6a945675-4f46-45ce-b6c9-e2f3be681ac7",
        "parentId" : null,
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "```\r\nIf we don't want to touch PruneFileSourcePartitions, I guess instead of checking if the postScanFilter is empty using if filters.isEmpty, we do something like this:\r\n\r\nif (JDBC)\r\n  if filters.isEmpty\r\n     push down aggregate\r\nelse // file based\r\n  if filters are only partition filters\r\n    push down aggregate\r\nThis looks hacky though. Please let me know if anybody has a better idea.\r\n```\r\n@huaxingao I don't have a better idea either. \r\nBut you have to match special case `FileScanBuilder` here anyway. I would prefer resolving the partition filters in `pushAggregates`, which is more straightforward. ",
        "createdAt" : "2021-08-05T05:18:34Z",
        "updatedAt" : "2021-08-05T05:18:34Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "015cce76-ef3e-4531-8225-34cd4418be75",
        "parentId" : "6a945675-4f46-45ce-b6c9-e2f3be681ac7",
        "authorId" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "body" : "@gengliangwang OK. I will not change `PruneFileSourcePartitions`. I will close this PR and find other way to do this. Thank you very much for your help!",
        "createdAt" : "2021-08-05T08:28:19Z",
        "updatedAt" : "2021-08-05T08:28:19Z",
        "lastEditedBy" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "tags" : [
        ]
      }
    ],
    "commit" : "bbc85db716de1db24bdb67f2ca9075dcaa648ded",
    "line" : 50,
    "diffHunk" : "@@ -1,1 +147,151 @@      filters: Seq[Expression]): (Scan, Seq[AttributeReference]) = {\n    val filePartitionFilter = scanBuilder match {\n      case fileScanBuilder: FileScanBuilder => fileScanBuilder.getPartitionFilters\n      case _ => Seq.empty[Expression]\n    }"
  },
  {
    "id" : "3798541c-016e-48fb-9c28-a7571bb130c7",
    "prId" : 33352,
    "prUrl" : "https://github.com/apache/spark/pull/33352#pullrequestreview-715301561",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "19fed2d4-d900-4110-9b0a-7f182f773cea",
        "parentId" : null,
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "nit: you can just use `Some(agg).filter(r.pushAggregation)`",
        "createdAt" : "2021-07-26T21:42:35Z",
        "updatedAt" : "2021-07-26T21:52:10Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      }
    ],
    "commit" : "fae570a68cde1cc25ae60e7ba74651e7972578e3",
    "line" : 45,
    "diffHunk" : "@@ -1,1 +102,106 @@\n        val agg = new Aggregation(translatedAggregates.toArray, translatedGroupBys.toArray)\n        if (r.pushAggregation(agg)) {\n          Some(agg)\n        } else {"
  },
  {
    "id" : "ad3c8cca-3e60-49da-bfad-4bbbdebd6d9d",
    "prId" : 33352,
    "prUrl" : "https://github.com/apache/spark/pull/33352#pullrequestreview-715301561",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4a0f33f5-70d7-4029-a260-593cf55976cf",
        "parentId" : null,
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "nit: can use `flatMap` instead of `map` + `flatten`.",
        "createdAt" : "2021-07-26T21:44:29Z",
        "updatedAt" : "2021-07-26T21:52:10Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      }
    ],
    "commit" : "fae570a68cde1cc25ae60e7ba74651e7972578e3",
    "line" : 36,
    "diffHunk" : "@@ -1,1 +93,97 @@    scanBuilder match {\n      case r: SupportsPushDownAggregates =>\n        val translatedAggregates = aggregates.map(DataSourceStrategy.translateAggregate).flatten\n        val translatedGroupBys = groupBy.map(columnAsString).flatten\n"
  },
  {
    "id" : "e94ee1f9-80bc-47ba-b619-3a729b4d8770",
    "prId" : 31993,
    "prUrl" : "https://github.com/apache/spark/pull/31993#pullrequestreview-634059678",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b6f33b48-c09c-4b96-903c-b2d95b6c681d",
        "parentId" : null,
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "Move [filter logical from `SchemaPruning`](https://github.com/apache/spark/blob/0f2c0b53e8fb18c86c67b5dd679c006db93f94a5/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/SchemaPruning.scala#L38-L39) to `PushDownUtils` to support datasource V2 column pruning.\r\n",
        "createdAt" : "2021-04-13T00:16:23Z",
        "updatedAt" : "2021-04-21T09:55:49Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      }
    ],
    "commit" : "4d0b510e10da4fe1fca07583ddecc5f5fe6d6392",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +95,99 @@        }\n        val neededFieldNames = neededOutput.map(_.name).toSet\n        r.pruneColumns(StructType(prunedSchema.filter(f => neededFieldNames.contains(f.name))))\n        val scan = r.build()\n        scan -> toOutputAttrs(scan.readSchema(), relation)"
  },
  {
    "id" : "62293e80-b491-4cea-b2b0-1b8802c2f2c1",
    "prId" : 29695,
    "prUrl" : "https://github.com/apache/spark/pull/29695#pullrequestreview-627388344",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9cf90376-8fdc-4ded-877b-4f6f6d747543",
        "parentId" : null,
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "what about \"else\" branch? perhaps we should revise this code to:\r\n```scala\r\n        case r: SupportsPushDownAggregates =>\r\n          val translatedAggregates = aggregates.map(DataSourceStrategy.translateAggregate)\r\n          val translatedGroupBys = groupBy.map(columnAsString)\r\n\r\n          if (translatedAggregates.exists(_.isEmpty) || translatedGroupBys.exists(_.isEmpty)) {\r\n            Aggregation.empty\r\n          } else {\r\n            r.pushAggregation(Aggregation(translatedAggregates.flatten, translatedGroupBys))\r\n            r.pushedAggregation\r\n          }\r\n```",
        "createdAt" : "2021-04-02T23:42:10Z",
        "updatedAt" : "2021-04-03T00:08:31Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      }
    ],
    "commit" : "782a0a827fbb1e60b8673d533cbda618df65dc96",
    "line" : 49,
    "diffHunk" : "@@ -1,1 +103,107 @@          if (!groupByCols.exists(_.isEmpty)) {\n            r.pushAggregation(Aggregation(translatedAggregates, groupByCols))\n          }\n          r.pushedAggregation\n"
  },
  {
    "id" : "e7b648e3-973d-45ce-a1c8-aa543faa2c6b",
    "prId" : 26751,
    "prUrl" : "https://github.com/apache/spark/pull/26751#pullrequestreview-328202606",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "63824761-4156-4578-8401-248d45e7df36",
        "parentId" : null,
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "Let's add a simple test case for this branch in `SchemaPruningSuite`",
        "createdAt" : "2019-12-04T19:21:02Z",
        "updatedAt" : "2019-12-11T13:38:26Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "6a7b3c7f-2d01-48d6-8e49-3222563fc92a",
        "parentId" : "63824761-4156-4578-8401-248d45e7df36",
        "authorId" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "body" : "Good idea",
        "createdAt" : "2019-12-04T20:36:53Z",
        "updatedAt" : "2019-12-11T13:38:27Z",
        "lastEditedBy" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "tags" : [
        ]
      },
      {
        "id" : "f0f6a072-ff57-4f88-87f3-43a1b581fd14",
        "parentId" : "63824761-4156-4578-8401-248d45e7df36",
        "authorId" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "body" : "Done",
        "createdAt" : "2019-12-06T13:30:18Z",
        "updatedAt" : "2019-12-11T13:38:27Z",
        "lastEditedBy" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "tags" : [
        ]
      }
    ],
    "commit" : "4d732b28eb0ca2a383b3d0ad4d9da7742f138d32",
    "line" : 31,
    "diffHunk" : "@@ -1,1 +90,94 @@          SchemaPruning.pruneDataSchema(relation.schema, rootFields)\n        } else {\n          new StructType()\n        }\n        r.pruneColumns(prunedSchema)"
  },
  {
    "id" : "761fd62e-14d6-4045-ae0b-07d77e83ff37",
    "prId" : 26751,
    "prUrl" : "https://github.com/apache/spark/pull/26751#pullrequestreview-327514798",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cb34d47f-1d4f-4272-9e3e-3d96b20da430",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "there was a check in `prunePhysicalColumns`:\r\n\r\n```\r\nif (requestedRootFields.exists { root: RootField => !root.derivedFromAtt }) {\r\n ...\r\n}\r\n```\r\n\r\nIs it removed from this move?",
        "createdAt" : "2019-12-05T05:03:53Z",
        "updatedAt" : "2019-12-11T13:38:27Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "df975e44-f3b2-486d-b3e8-270ec7e28b22",
        "parentId" : "cb34d47f-1d4f-4272-9e3e-3d96b20da430",
        "authorId" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "body" : "This check was needed to detect if any nested column was requested. The old rule would not apply otherwise. Here, the situation is different as we need to prune top-level columns even if no nested attributes are requested.",
        "createdAt" : "2019-12-05T12:30:23Z",
        "updatedAt" : "2019-12-11T13:38:27Z",
        "lastEditedBy" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "tags" : [
        ]
      }
    ],
    "commit" : "4d732b28eb0ca2a383b3d0ad4d9da7742f138d32",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +87,91 @@      case r: SupportsPushDownRequiredColumns if SQLConf.get.nestedSchemaPruningEnabled =>\n        val rootFields = SchemaPruning.identifyRootFields(projects, filters)\n        val prunedSchema = if (rootFields.nonEmpty) {\n          SchemaPruning.pruneDataSchema(relation.schema, rootFields)\n        } else {"
  },
  {
    "id" : "31e83fa4-42d6-4cbc-a83d-027f0e1c886f",
    "prId" : 26751,
    "prUrl" : "https://github.com/apache/spark/pull/26751#pullrequestreview-331174285",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ae7102cb-1e65-4def-acc6-25389f84c7e2",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "If we know scanBuilder does not support nested schema pruning, do we still need go this path?",
        "createdAt" : "2019-12-12T00:52:56Z",
        "updatedAt" : "2019-12-12T00:52:57Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "e065c796-bb04-4bb4-b263-548aef13bbda",
        "parentId" : "ae7102cb-1e65-4def-acc6-25389f84c7e2",
        "authorId" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "body" : "I don't think there is an API to find out whether a particular `ScanBuilder` supports nested schema pruning. Instead, we have `SupportsPushDownRequiredColumns` and data sources should use the passed schema as a reference and prune whatever they can. The flag that we added in this PR is specific to `FileScanBuilder` and I believe it will complicate the overall logic if we treat `FileScanBuilder` differently by introducing special branches or if conditions. Also, we might actually get rid of that flag soon, as mentioned in DB's [comment](https://github.com/apache/spark/pull/26751#discussion_r356771927).",
        "createdAt" : "2019-12-12T11:48:17Z",
        "updatedAt" : "2019-12-12T18:16:44Z",
        "lastEditedBy" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "tags" : [
        ]
      }
    ],
    "commit" : "4d732b28eb0ca2a383b3d0ad4d9da7742f138d32",
    "line" : 26,
    "diffHunk" : "@@ -1,1 +85,89 @@      filters: Seq[Expression]): (Scan, Seq[AttributeReference]) = {\n    scanBuilder match {\n      case r: SupportsPushDownRequiredColumns if SQLConf.get.nestedSchemaPruningEnabled =>\n        val rootFields = SchemaPruning.identifyRootFields(projects, filters)\n        val prunedSchema = if (rootFields.nonEmpty) {"
  }
]