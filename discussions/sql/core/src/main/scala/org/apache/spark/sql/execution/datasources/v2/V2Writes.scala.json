[
  {
    "id" : "0ef9a43c-3b7a-49ce-bfc4-2634785689e7",
    "prId" : 30806,
    "prUrl" : "https://github.com/apache/spark/pull/30806#pullrequestreview-556462738",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4c4fb7dc-2258-4f08-bd58-836b1901e854",
        "parentId" : null,
        "authorId" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "body" : "I kept the old logic but I am not sure whether we should also normalize filters. Thoughts, @cloud-fan @rdblue?",
        "createdAt" : "2020-12-17T20:12:37Z",
        "updatedAt" : "2020-12-21T15:35:33Z",
        "lastEditedBy" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "tags" : [
        ]
      },
      {
        "id" : "b34dc28a-0286-43e2-8787-cd72f05c0b29",
        "parentId" : "4c4fb7dc-2258-4f08-bd58-836b1901e854",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I think we should, to follow DS v1 and file source. ",
        "createdAt" : "2020-12-21T14:28:15Z",
        "updatedAt" : "2020-12-21T15:35:33Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "1955e8a3-f764-47ea-8a4c-848c3a88a206",
        "parentId" : "4c4fb7dc-2258-4f08-bd58-836b1901e854",
        "authorId" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "body" : "I've created SPARK-33868 as a follow-up item. I will keep the old behavior in this PR.",
        "createdAt" : "2020-12-21T15:29:14Z",
        "updatedAt" : "2020-12-21T15:35:33Z",
        "lastEditedBy" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "tags" : [
        ]
      }
    ],
    "commit" : "882e3210004c0e27aca914b58819008dafdde92a",
    "line" : 47,
    "diffHunk" : "@@ -1,1 +45,49 @@    case o @ OverwriteByExpression(r: DataSourceV2Relation, deleteExpr, query, options, _, None) =>\n      // fail if any filter cannot be converted. correctness depends on removing all matching data.\n      val filters = splitConjunctivePredicates(deleteExpr).flatMap { pred =>\n        val filter = DataSourceStrategy.translateFilter(pred, supportNestedPredicatePushdown = true)\n        if (filter.isEmpty) {"
  },
  {
    "id" : "3e520e34-ffdf-469b-a837-4b6a210bfa25",
    "prId" : 29066,
    "prUrl" : "https://github.com/apache/spark/pull/29066#pullrequestreview-546229178",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6cb7de49-6b4a-4868-b147-8c36d2dc7137",
        "parentId" : null,
        "authorId" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "body" : "This rule contains the same logic we had before except it is applied earlier now.",
        "createdAt" : "2020-07-14T20:13:13Z",
        "updatedAt" : "2020-12-07T14:50:31Z",
        "lastEditedBy" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "tags" : [
        ]
      },
      {
        "id" : "bd4be0e9-fa73-4c7a-9ead-da9aca64b6f6",
        "parentId" : "6cb7de49-6b4a-4868-b147-8c36d2dc7137",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Hmm, care to explain where are these rules before? I don't see anything moved to this new rule.",
        "createdAt" : "2020-12-03T19:34:32Z",
        "updatedAt" : "2020-12-07T14:50:31Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "c77259dc-a7e0-44dd-af57-786b2bd57046",
        "parentId" : "6cb7de49-6b4a-4868-b147-8c36d2dc7137",
        "authorId" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "body" : "I think `buildAndRun` methods in exec nodes still contain the old logic. Previously, it was called `run`.",
        "createdAt" : "2020-12-07T14:57:34Z",
        "updatedAt" : "2020-12-07T14:57:34Z",
        "lastEditedBy" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "tags" : [
        ]
      }
    ],
    "commit" : "32f5687fd628a4ffca38fa0840fd52447db7d680",
    "line" : 43,
    "diffHunk" : "@@ -1,1 +41,45 @@ * must undergo the expression optimization before we can construct a logical write.\n */\nobject V2Writes extends Rule[LogicalPlan] with PredicateHelper {\n\n  import DataSourceV2Implicits._"
  },
  {
    "id" : "306c254b-8c1f-45ef-97fc-e37be8a39a15",
    "prId" : 29066,
    "prUrl" : "https://github.com/apache/spark/pull/29066#pullrequestreview-546230614",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c7cecda7-0d6c-4efd-9f63-dce401de3b64",
        "parentId" : null,
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "Nit: `p` isn't a very good variable name. What about `pred`?",
        "createdAt" : "2020-11-25T22:09:44Z",
        "updatedAt" : "2020-12-07T14:50:31Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      },
      {
        "id" : "18309524-7c44-46a5-a0aa-18af1dda168e",
        "parentId" : "c7cecda7-0d6c-4efd-9f63-dce401de3b64",
        "authorId" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "body" : "I think I named it to stay on one line, I'll double check.",
        "createdAt" : "2020-12-07T14:58:58Z",
        "updatedAt" : "2020-12-07T14:58:58Z",
        "lastEditedBy" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "tags" : [
        ]
      }
    ],
    "commit" : "32f5687fd628a4ffca38fa0840fd52447db7d680",
    "line" : 55,
    "diffHunk" : "@@ -1,1 +53,57 @@    case o @ OverwriteByExpression(r: DataSourceV2Relation, deleteExpr, query, options, _, None) =>\n      // fail if any filter cannot be converted. correctness depends on removing all matching data.\n      val filters = splitConjunctivePredicates(deleteExpr).flatMap { p =>\n        val filter = DataSourceStrategy.translateFilter(p, supportNestedPredicatePushdown = true)\n        if (filter.isEmpty) {"
  },
  {
    "id" : "f8d0e89d-ee3c-44a7-a89f-288f128c43f8",
    "prId" : 29066,
    "prUrl" : "https://github.com/apache/spark/pull/29066#pullrequestreview-546230136",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c627b27f-1639-4412-969e-a26613b9756a",
        "parentId" : null,
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Seems to me that what this rule does it to add the distribution/ordering info. Do we plan to add other functionalities to this in future? is `V2Writes` too general as a name?",
        "createdAt" : "2020-12-02T20:24:25Z",
        "updatedAt" : "2020-12-07T14:50:31Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "9933aacf-f0f5-4ba7-8afa-cedf7784719c",
        "parentId" : "c627b27f-1639-4412-969e-a26613b9756a",
        "authorId" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "body" : "This rule not only inserts shuffle/sort but also build `Write`s. It is only applied if `Write` has not been constructed before.",
        "createdAt" : "2020-12-07T14:58:29Z",
        "updatedAt" : "2020-12-07T14:58:30Z",
        "lastEditedBy" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "tags" : [
        ]
      }
    ],
    "commit" : "32f5687fd628a4ffca38fa0840fd52447db7d680",
    "line" : 43,
    "diffHunk" : "@@ -1,1 +41,45 @@ * must undergo the expression optimization before we can construct a logical write.\n */\nobject V2Writes extends Rule[LogicalPlan] with PredicateHelper {\n\n  import DataSourceV2Implicits._"
  },
  {
    "id" : "f31164d7-2a7a-4169-8c6a-3f65056f0cc3",
    "prId" : 29066,
    "prUrl" : "https://github.com/apache/spark/pull/29066#pullrequestreview-546231102",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fbc76706-635e-49ee-8477-b6aa7d9b76f1",
        "parentId" : null,
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Should we put `SupportsOverwrite` before `SupportsTruncate`? if a builder class extends `SupportsOverwrite` (and `isTruncate` returns true) then it will be matched by the first clause and call the `truncate` method, but we want it to call `overwrite(filters)` right?",
        "createdAt" : "2020-12-03T01:36:09Z",
        "updatedAt" : "2020-12-07T14:50:31Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "743aa14b-e702-4b01-b4da-54c543602eb6",
        "parentId" : "fbc76706-635e-49ee-8477-b6aa7d9b76f1",
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "This is existing logic, just moved.\r\n\r\nThe reason for this is that `SupportsOverwrite` extends `SupportsTruncate` and calls `overwrite(true)`. Calling `truncate` ensures that the source can implement either one if it chooses. Sometimes truncate may be preferred, and it is easier for a source to receive that call directly rather than writing its own equivalent of `isTruncate`.",
        "createdAt" : "2020-12-03T17:34:12Z",
        "updatedAt" : "2020-12-07T14:50:31Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      },
      {
        "id" : "1218822e-e660-4af9-8b46-cf301bdf4b8b",
        "parentId" : "fbc76706-635e-49ee-8477-b6aa7d9b76f1",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Got it. I'm just not sure if some data source would extend `SupportsOverwrite` and decides to override/implement different behavior for `overwrite` and `truncate` (which no longer calls `overwrite(true)`, and here we'd pick `truncate` but what they really want is `overwrite`.",
        "createdAt" : "2020-12-03T18:42:00Z",
        "updatedAt" : "2020-12-07T14:50:31Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "037e0076-72ba-4f27-96b9-8df11616e04a",
        "parentId" : "fbc76706-635e-49ee-8477-b6aa7d9b76f1",
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "If the source doesn't implement `truncate`, then it gets `overwrite(true)`: https://github.com/apache/spark/blob/master/sql/catalyst/src/main/java/org/apache/spark/sql/connector/write/SupportsOverwrite.java#L46-L48",
        "createdAt" : "2020-12-04T17:06:08Z",
        "updatedAt" : "2020-12-07T14:50:31Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      },
      {
        "id" : "9199294e-6aae-4401-9236-0cb30635987a",
        "parentId" : "fbc76706-635e-49ee-8477-b6aa7d9b76f1",
        "authorId" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "body" : "This logic did not change and should match the previous behavior.",
        "createdAt" : "2020-12-07T14:59:24Z",
        "updatedAt" : "2020-12-07T14:59:24Z",
        "lastEditedBy" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "tags" : [
        ]
      }
    ],
    "commit" : "32f5687fd628a4ffca38fa0840fd52447db7d680",
    "line" : 68,
    "diffHunk" : "@@ -1,1 +66,70 @@        case builder: SupportsTruncate if isTruncate(filters) =>\n          builder.truncate().build()\n        case builder: SupportsOverwrite =>\n          builder.overwrite(filters).build()\n        case _ =>"
  },
  {
    "id" : "f1bdfb7e-1212-438f-8798-4fee578ce204",
    "prId" : 29066,
    "prUrl" : "https://github.com/apache/spark/pull/29066#pullrequestreview-548603345",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8ec161b6-96fb-4212-b561-e5e61b9e7734",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Is it possible the required distribution be changed later by other optimization? The distribution requirement from data source is a hard requirement? Once if the distribution is changed and not matched the requirement, how will data source react to it?",
        "createdAt" : "2020-12-09T07:16:17Z",
        "updatedAt" : "2020-12-09T07:29:47Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "0b156e7a-f01d-4971-8f3a-c71d2a800bf4",
        "parentId" : "8ec161b6-96fb-4212-b561-e5e61b9e7734",
        "authorId" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "body" : "We are inserting repartition/sort nodes directly before writing so my assumption that Spark will only remove them if the incoming plan already satisfies these requirements. `WriteDistributionAndOrderingSuite` is kind of meant for testing that. Do you have ideas when this assumption will break, @viirya?",
        "createdAt" : "2020-12-09T10:40:08Z",
        "updatedAt" : "2020-12-09T10:40:08Z",
        "lastEditedBy" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "tags" : [
        ]
      },
      {
        "id" : "83b71536-841d-4cf8-a14b-c6288e75ed42",
        "parentId" : "8ec161b6-96fb-4212-b561-e5e61b9e7734",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Oh, I see. Mis-reading this part. Looks fine. Thanks.",
        "createdAt" : "2020-12-09T20:58:53Z",
        "updatedAt" : "2020-12-09T20:58:53Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "32f5687fd628a4ffca38fa0840fd52447db7d680",
    "line" : 126,
    "diffHunk" : "@@ -1,1 +124,128 @@        // for OrderedDistribution and generic expressions for ClusteredDistribution\n        // this allows RepartitionByExpression to pick either range or hash partitioning\n        RepartitionByExpression(distribution, query, numShufflePartitions)\n      } else {\n        query"
  },
  {
    "id" : "6d89ad02-2e01-4758-a624-b33bf5672459",
    "prId" : 29066,
    "prUrl" : "https://github.com/apache/spark/pull/29066#pullrequestreview-548599568",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0a272d61-2b74-429f-a4a9-c2708e05fb6f",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "If we resolve it in the analyzer, cannot we optimize the resolved expressions later in the optimizer?",
        "createdAt" : "2020-12-09T07:25:49Z",
        "updatedAt" : "2020-12-09T07:29:47Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "db7e3e8b-6914-4604-87d1-efcdb0d33734",
        "parentId" : "0a272d61-2b74-429f-a4a9-c2708e05fb6f",
        "authorId" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "body" : "At this step, we construct a `Write` and pass the overwrite expressions to the data source. Expression optimization must have happened before.",
        "createdAt" : "2020-12-09T10:41:34Z",
        "updatedAt" : "2020-12-09T10:41:34Z",
        "lastEditedBy" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "tags" : [
        ]
      },
      {
        "id" : "16444c89-fd10-48b8-b611-9765139c2ccc",
        "parentId" : "0a272d61-2b74-429f-a4a9-c2708e05fb6f",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Got it. Thanks for clarifying.",
        "createdAt" : "2020-12-09T20:53:39Z",
        "updatedAt" : "2020-12-09T20:53:39Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "32f5687fd628a4ffca38fa0840fd52447db7d680",
    "line" : 153,
    "diffHunk" : "@@ -1,1 +151,155 @@      // this part is controversial as we perform resolution in the optimizer\n      // we cannot perform this step in the analyzer since we need to optimize expressions\n      // in nodes like OverwriteByExpression before constructing a logical write\n      query.resolve(ref.parts, resolver) match {\n        case Some(attr) => attr"
  }
]