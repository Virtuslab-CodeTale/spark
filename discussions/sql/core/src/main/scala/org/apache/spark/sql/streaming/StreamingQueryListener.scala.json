[
  {
    "id" : "27a02cf7-64c0-4a3f-b163-adc485cd5aca",
    "prId" : 27732,
    "prUrl" : "https://github.com/apache/spark/pull/27732#pullrequestreview-367769722",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a19742e8-030e-4cfc-b1d4-fec0b4c30f42",
        "parentId" : null,
        "authorId" : "2d6b46ba-4100-4c4d-9341-fff5e39647ec",
        "body" : "Change this to `string` type to be consistent with timestamps in `StreamingQueryProgress`.",
        "createdAt" : "2020-02-28T08:08:04Z",
        "updatedAt" : "2020-02-28T08:08:05Z",
        "lastEditedBy" : "2d6b46ba-4100-4c4d-9341-fff5e39647ec",
        "tags" : [
        ]
      },
      {
        "id" : "b251f912-a69f-4e7c-aac6-68bc8ee51327",
        "parentId" : "a19742e8-030e-4cfc-b1d4-fec0b4c30f42",
        "authorId" : "3a227965-84e0-47cf-9974-11293764f028",
        "body" : "`timestamp` is not used without parsing and the parsed result is the same as [startTimestamp](https://github.com/apache/spark/pull/27732/files#diff-6532dd3b63bdab0364fbcf2303e290e4R310). So `formatTimestamp(startTimestamp)` seems a little bit meaningless.\r\nI understand it's for consistency but how about rather changing the type of `StreamingQueryProgress#timestamp` to `Long` for consistency?\r\nAlmost all the use of `StreamingQueryProgress#timestamp` is with parsing so could we reduce the parsing code?\r\n",
        "createdAt" : "2020-03-03T05:19:14Z",
        "updatedAt" : "2020-03-03T05:19:53Z",
        "lastEditedBy" : "3a227965-84e0-47cf-9974-11293764f028",
        "tags" : [
        ]
      },
      {
        "id" : "e774f66a-eee5-4756-bdef-5578f6ac3e57",
        "parentId" : "a19742e8-030e-4cfc-b1d4-fec0b4c30f42",
        "authorId" : "2d6b46ba-4100-4c4d-9341-fff5e39647ec",
        "body" : "Yep, totally agreed that `Long` is better for coding. However, `StreamingQueryProgress#timestamp` was designed to be human readable. In addition, its type cannot be changed now because that's a public API. Since the user has already had codes to parse `StreamingQueryProgress.timestamp`, it should be fine to add a field with the same format.",
        "createdAt" : "2020-03-03T07:33:18Z",
        "updatedAt" : "2020-03-03T07:33:18Z",
        "lastEditedBy" : "2d6b46ba-4100-4c4d-9341-fff5e39647ec",
        "tags" : [
        ]
      },
      {
        "id" : "dd283982-92a7-49c2-82dc-84d2e50c5146",
        "parentId" : "a19742e8-030e-4cfc-b1d4-fec0b4c30f42",
        "authorId" : "3a227965-84e0-47cf-9974-11293764f028",
        "body" : "Ah, It's a public API. O.K, I understand that it's reasonable.",
        "createdAt" : "2020-03-03T08:10:16Z",
        "updatedAt" : "2020-03-03T08:10:17Z",
        "lastEditedBy" : "3a227965-84e0-47cf-9974-11293764f028",
        "tags" : [
        ]
      }
    ],
    "commit" : "14795023e1abd1d79cc4b5a2c3ea011304cb1b4c",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +91,95 @@      val runId: UUID,\n      val name: String,\n      val timestamp: String) extends Event\n\n  /**"
  },
  {
    "id" : "d7b1ab58-8f21-4964-8699-4c95f54bad89",
    "prId" : 27732,
    "prUrl" : "https://github.com/apache/spark/pull/27732#pullrequestreview-368013937",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bec6a5d5-d667-4f59-a888-bf4b800430ff",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "shall we mention that this is a timestamp string assuming the timezone is UTC? Or do we put the timezone at the end of the timestamp string?",
        "createdAt" : "2020-03-03T14:15:28Z",
        "updatedAt" : "2020-03-03T14:15:28Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "411cc7f2-7843-4a14-be5b-d816f6b4377a",
        "parentId" : "bec6a5d5-d667-4f59-a888-bf4b800430ff",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "nvm, we put `Z` at the end, to indicate it's UTC.",
        "createdAt" : "2020-03-03T14:17:40Z",
        "updatedAt" : "2020-03-03T14:17:41Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "14795023e1abd1d79cc4b5a2c3ea011304cb1b4c",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +83,87 @@   * @param runId A query id that is unique for every start/restart. See `StreamingQuery.runId()`.\n   * @param name User-specified name of the query, null if not specified.\n   * @param timestamp The timestamp to start a query.\n   * @since 2.1.0\n   */"
  },
  {
    "id" : "0bff5eb4-2178-4868-a9c1-738fe65d983a",
    "prId" : 27732,
    "prUrl" : "https://github.com/apache/spark/pull/27732#pullrequestreview-521280346",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ce269054-900f-4b70-9f4a-cf321501595d",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "This changes made harder processing of event logs machinery, for instance by Spark. If you have at least the following two records:\r\n```\r\n{\"Event\":\"SparkListenerExecutorAdded\",\"Timestamp\":1603819505374, ...}\r\n{\"Event\":\"org.apache.spark.sql.streaming.StreamingQueryListener$QueryStartedEvent\",\"timestamp\":\"2020-10-27T17:37:48.452Z\" ...}\r\n``` \r\nand want to load them by Spark. It will fails with:\r\n```\r\nFound duplicate column(s) in the data schema: `timestamp`;\r\norg.apache.spark.sql.AnalysisException: Found duplicate column(s) in the data schema: `timestamp`;\r\n\tat org.apache.spark.sql.util.SchemaUtils$.checkColumnNameDuplication(SchemaUtils.scala:90)\r\n\tat org.apache.spark.sql.util.SchemaUtils$.checkSchemaColumnNameDuplication(SchemaUtils.scala:45)\r\n```\r\nbecause we have two columns with the same name `timestamp` but with different types.\r\n\r\nHow about to unify schemas of all events (at least for top level columns), and use the same names and types across all `SparkListenerEvent`?",
        "createdAt" : "2020-10-31T08:59:39Z",
        "updatedAt" : "2020-10-31T08:59:47Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "3db200ba-aff2-4c9b-9085-fdba62ca56eb",
        "parentId" : "ce269054-900f-4b70-9f4a-cf321501595d",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "+1. Which type shall we use? String or Long?",
        "createdAt" : "2020-11-01T07:27:13Z",
        "updatedAt" : "2020-11-01T07:27:13Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "1e69bb23-5ec2-4ac2-a748-58647ec1adc4",
        "parentId" : "ce269054-900f-4b70-9f4a-cf321501595d",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "To be compatible to the events from `JsonProtocol`:\r\n- SparkListenerJobStart\r\n- SparkListenerJobEnd\r\n- SparkListenerBlockManagerAdded\r\n- SparkListenerBlockManagerRemoved\r\n- SparkListenerExecutorAdded\r\n- SparkListenerExecutorRemoved\r\n- SparkListenerExecutorBlacklisted\r\n- SparkListenerExecutorBlacklistedForStage\r\n- SparkListenerNodeBlacklistedForStage\r\n- SparkListenerExecutorUnblacklisted\r\n- SparkListenerNodeBlacklisted\r\n- SparkListenerNodeUnblacklisted\r\n- SparkListenerApplicationStart\r\n- SparkListenerApplicationEnd\r\n\r\nIt should be `Long`.",
        "createdAt" : "2020-11-01T09:14:45Z",
        "updatedAt" : "2020-11-01T09:14:46Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "5aecab3d-c9fd-4137-a90e-360c46d89a5d",
        "parentId" : "ce269054-900f-4b70-9f4a-cf321501595d",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "as an option if we want to have human-readable timestamps and avoid conflicts with other events, we could rename:\r\n`timestamp: String` -> `submissionTime: String`",
        "createdAt" : "2020-11-01T09:18:04Z",
        "updatedAt" : "2020-11-01T09:18:05Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "99c8811d-00a5-40d1-83b1-9f78a5b3d59f",
        "parentId" : "ce269054-900f-4b70-9f4a-cf321501595d",
        "authorId" : "2d6b46ba-4100-4c4d-9341-fff5e39647ec",
        "body" : "This is a public API. Renaming it would break compatibility.\r\n\r\nThe consumer of Spark event logs is Spark History Server. It's not designed to be friendly to Spark JSON data source since the beginning. If we would like to make Spark JSON source easy to read event logs, it would need a new design. Currently, all fields of each Spark event class is in the top level column and these fields are arbitrary, and we don't have enforcement on these event classes. Delta Lake has some similar classes that need to be serialized to JSON. We create a wrapper [SingleAction](https://github.com/delta-io/delta/blob/v0.7.0/src/main/scala/org/apache/spark/sql/delta/actions/actions.scala#L344) to store all possible types of `Action`s. This may be a possible approach for new Spark event logs. However, maintaining the compatibility is hard. This is probably not worth.\r\n",
        "createdAt" : "2020-11-02T00:34:06Z",
        "updatedAt" : "2020-11-02T00:34:06Z",
        "lastEditedBy" : "2d6b46ba-4100-4c4d-9341-fff5e39647ec",
        "tags" : [
        ]
      }
    ],
    "commit" : "14795023e1abd1d79cc4b5a2c3ea011304cb1b4c",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +91,95 @@      val runId: UUID,\n      val name: String,\n      val timestamp: String) extends Event\n\n  /**"
  }
]