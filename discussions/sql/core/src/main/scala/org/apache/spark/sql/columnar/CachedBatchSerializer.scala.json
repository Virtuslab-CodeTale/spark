[
  {
    "id" : "0a12364b-443a-4b3d-9846-03bde20c42cb",
    "prId" : 29067,
    "prUrl" : "https://github.com/apache/spark/pull/29067#pullrequestreview-452412301",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "aed83cfa-9299-4bea-8030-7eaafb254602",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I'd like to understand the API design concerns that lead to this design. IMO the two major concerns should be\r\n1. the input to the cache builder should be row-based or columnar?\r\n2. the output of the cache builder should be row-based or columnar?\r\n\r\nIt seems to me that `convertForCache` is to address 1. `SparkPlan` provides APIs to get row-based or columnar data. But I think exposing `SparkPlan` is overkill: it's a very big class that has many methods. Can we create a trait to abstract the main functionalities?\r\n\r\nFor 2, the current API looks reasonable.",
        "createdAt" : "2020-07-20T16:09:21Z",
        "updatedAt" : "2020-07-31T17:39:07Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "cf0a7da0-da50-4b2d-8a87-465c33465418",
        "parentId" : "aed83cfa-9299-4bea-8030-7eaafb254602",
        "authorId" : "b19fe247-920f-40ae-83e7-6b8ec9979f6b",
        "body" : "The major goal was to expose some way to provide custom code for compressing/decompressing the cached data.  Decompression was already really fast and had APIs for columnar output vs non-columnar output so I mostly just kept them in place. \r\n\r\n`convertForCache` does have a goal to be able to let someone process columnar input, but because of how the `SparkPlan` is created that is not a simple task. `QueryExecution.executedPlan` which is the source of the `SparkPlan` will always produce row based output. That was one of the design goals for the columnar execution work. It had to be truly transparent. Currently the only way to get around that is by walking the SparkPlan to check if the last stage is a code generation phase with only a child that is doing a columnar to row transition. If so then it is safe to remove that and process it directly as columnar. If you want me to create a trait for `SparkPlan` that exposes a simpler API just for caching, I can, but it will not be able to expose anything columnar unless we make larger changes. I am fine with making those larger changes, I just want to be sure that the community is willing to accept them conceptually before I start to touch a lot of the code.",
        "createdAt" : "2020-07-20T17:00:21Z",
        "updatedAt" : "2020-07-31T17:39:07Z",
        "lastEditedBy" : "b19fe247-920f-40ae-83e7-6b8ec9979f6b",
        "tags" : [
        ]
      },
      {
        "id" : "936e0420-988c-4cbd-be1c-42e8103e48ff",
        "parentId" : "aed83cfa-9299-4bea-8030-7eaafb254602",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "> Currently the only way to get around that is by walking the SparkPlan to check if the last stage is a code generation phase with only a child that is doing a columnar to row transition.\r\n\r\nI'd like to understand more about the physical path. So we only pass columnar data to the cache builder if the spark plan can produce columnar data natively (the last operator is a columnar to row transition)?\r\n\r\nIf that's the case, I'd suggest we add 3 methods: `convertForCache(inpur: RDD[InternalRow])`, `convertForCacheColumnar(inpur: RDD[ColumnarBatch])` and `supportsColumnarInput(schema): Boolean`. ",
        "createdAt" : "2020-07-20T17:54:12Z",
        "updatedAt" : "2020-07-31T17:39:07Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "9cbcf05a-9a75-407d-ae3b-19a2aa908526",
        "parentId" : "aed83cfa-9299-4bea-8030-7eaafb254602",
        "authorId" : "b19fe247-920f-40ae-83e7-6b8ec9979f6b",
        "body" : "Columnar to row transitions are inserted as a part of the `ApplyColumnarRulesAndInsertTransitions` rule in `QueryExecution.preparations`.  It looks for `SparkPlan` instances that output columnar data followed by ones that take row based data as input.  The rule inserts a `ColumnarToRowExec` transition between them.  This transition supports code generation so in normal operation it will then be wrapped in a `WholeStageCodeGenExec`. `ApplyColumnarRulesAndInsertTransitions` assumes that the output of any plan must be row based to conform with all of the other code that already makes that assumption.  It should be a minor change to be able to make that optional so `QueryExecution` could provide a new API that would return a `SparkPlan` that might be columnar and might not.\r\n\r\nI can put together an prototype of how that would look if you are OK with it.",
        "createdAt" : "2020-07-20T18:20:53Z",
        "updatedAt" : "2020-07-31T17:39:07Z",
        "lastEditedBy" : "b19fe247-920f-40ae-83e7-6b8ec9979f6b",
        "tags" : [
        ]
      },
      {
        "id" : "bd5a5321-0133-4989-8c15-a352f8cd5ad2",
        "parentId" : "aed83cfa-9299-4bea-8030-7eaafb254602",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "`SparkPlan` is so internal and I'd like to try harder to avoid leaking it to the developer APIs.\r\n\r\nAfter all, the cache builder only needs an RDD (either contains rows or columnar batches). We should keep the plan execution inside Spark and only exchange data (RDD) with the plugins.",
        "createdAt" : "2020-07-21T12:40:13Z",
        "updatedAt" : "2020-07-31T17:39:07Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "3f2f5278c7e8c05e32a5f54ddba1251250dfdca3",
    "line" : 48,
    "diffHunk" : "@@ -1,1 +46,50 @@@DeveloperApi\n@Since(\"3.1.0\")\ntrait CachedBatchSerializer extends Serializable {\n  /**\n   * Can `convertColumnarBatchToCachedBatch()` be called instead of"
  },
  {
    "id" : "9cf466ce-a302-4548-b069-46ab93da4a06",
    "prId" : 29067,
    "prUrl" : "https://github.com/apache/spark/pull/29067#pullrequestreview-457096601",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0a9f7f04-7769-4d11-a321-0fd576ab6c4e",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Just a question. `The exact java types of the columns` means simply one of `OnHeapColumnVector` and `OffHeapColumnVector` here?",
        "createdAt" : "2020-07-28T22:43:46Z",
        "updatedAt" : "2020-07-31T17:39:07Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "3702609d-02fb-4670-8ac7-77d24379dc7a",
        "parentId" : "0a9f7f04-7769-4d11-a321-0fd576ab6c4e",
        "authorId" : "b19fe247-920f-40ae-83e7-6b8ec9979f6b",
        "body" : "Typically yes, but it could be a custom implementation, like what Parquet and Orc do.",
        "createdAt" : "2020-07-28T23:27:44Z",
        "updatedAt" : "2020-07-31T17:39:07Z",
        "lastEditedBy" : "b19fe247-920f-40ae-83e7-6b8ec9979f6b",
        "tags" : [
        ]
      }
    ],
    "commit" : "3f2f5278c7e8c05e32a5f54ddba1251250dfdca3",
    "line" : 119,
    "diffHunk" : "@@ -1,1 +117,121 @@\n  /**\n   * The exact java types of the columns that are output in columnar processing mode. This\n   * is a performance optimization for code generation and is optional.\n   * @param attributes the attributes to be output."
  }
]