[
  {
    "id" : "b7ffcded-4ff9-4075-aa30-ea35cc6efdce",
    "prId" : 31403,
    "prUrl" : "https://github.com/apache/spark/pull/31403#pullrequestreview-580386951",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "05918c78-2a63-4571-abd9-a4361b085e53",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "It's probably time to figure out the public Scala API for multi-catalogs: Shall we have one single `CatalogImpl` instance, and all its methods support catalog? Or shall we have one `CatalogImpl` instance per catalog?\r\n\r\nThe change here makes `recoverPartitions` to support multi-catalogs.",
        "createdAt" : "2021-02-01T12:58:47Z",
        "updatedAt" : "2021-02-01T12:58:47Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "a8d5ddfc-e027-41cd-be42-18ae9773acf6",
        "parentId" : "05918c78-2a63-4571-abd9-a4361b085e53",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Currently, other methods in `CatalogImpl` can work with both v1 and v2 tables already. Look at `cacheTable()`, `isCached()`, `refreshTable()`. I don't see much difference between those methods and `recoverPartitions()`.\r\n\r\nAnd the focus of this PR is to re-use new resolution framework, and to have consistent error message. Not taking into account that recovering partition of v2 tables is not supported at the moment.\r\n\r\nIn general, `CatalogImpl` has a lot of pretty specific to v1 catalog implementation things. ",
        "createdAt" : "2021-02-01T13:43:05Z",
        "updatedAt" : "2021-02-01T13:43:05Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "6d1a9b69-b6ce-4245-8c40-b23278c5cd1e",
        "parentId" : "05918c78-2a63-4571-abd9-a4361b085e53",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "ah since it's already the case, let's move forward then.",
        "createdAt" : "2021-02-01T14:08:52Z",
        "updatedAt" : "2021-02-01T14:08:52Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "49a9455adf5e5c59e61d2ad6c413171802985b71",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +446,450 @@   * @since 2.1.1\n   */\n  override def recoverPartitions(tableName: String): Unit = {\n    val multiPartIdent = sparkSession.sessionState.sqlParser.parseMultipartIdentifier(tableName)\n    sparkSession.sessionState.executePlan("
  },
  {
    "id" : "e93acbd7-9f6a-4ff9-9f63-f4b4f037ab4d",
    "prId" : 31206,
    "prUrl" : "https://github.com/apache/spark/pull/31206#pullrequestreview-571981476",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7cc91fff-6f97-436a-8545-84245d46415d",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "This seems duplicated with the above statement `re-cache the table and its dependents lazily.`. Maybe we should change the above to `re-cache the table lazily.`",
        "createdAt" : "2021-01-20T02:21:36Z",
        "updatedAt" : "2021-01-21T09:07:03Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "6888eeb8-4630-414a-94d8-5a96a6914cf2",
        "parentId" : "7cc91fff-6f97-436a-8545-84245d46415d",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Don't think it is a duplicate. It describes the case when a table is not cached. See the PR https://github.com/apache/spark/pull/30187 in which the statement was added by @sunchao and committed by @dongjoon-hyun .",
        "createdAt" : "2021-01-20T08:41:27Z",
        "updatedAt" : "2021-01-21T09:07:03Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "357c69f888ed4f27ea8db3b78d79d123b4dac019",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +522,526 @@   * If this table is cached as an InMemoryRelation, re-cache the table and its dependents lazily.\n   *\n   * In addition, refreshing a table also clear all caches that have reference to the table\n   * in a cascading manner. This is to prevent incorrect result from the otherwise staled caches.\n   *"
  },
  {
    "id" : "8dfdad40-460d-4977-ab6d-a68bcffa786b",
    "prId" : 31136,
    "prUrl" : "https://github.com/apache/spark/pull/31136#pullrequestreview-566816000",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3a1fa58b-665c-4a59-897e-0e37b8572412",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "If there is table or view not found error, does it mean we skip uncaching the temp view?",
        "createdAt" : "2021-01-13T00:05:42Z",
        "updatedAt" : "2021-01-13T00:05:42Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "c5dbb9bd-c547-4d26-9d42-813fde2ec9e4",
        "parentId" : "3a1fa58b-665c-4a59-897e-0e37b8572412",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "This is a good question. I think currently this won't happen because:\r\n1. when dropping a table or permanent view, we'll drop all the caches with reference on it in a cascading fashion, so when we are dropping the caches themselves, they are already invalidated.\r\n2. On the other hand, we currently store temp view as analyzed logical plans so they won't be analyzed again upon retrieving, which means we won't run into the error you mentioned. Although, this also means the plans themselves could become stale and potentially generate incorrect result. #31107 proposes to change this, following similar changes done by #30567, so the behavior of temporary view as well as cache is more aligned to that of permanent view.",
        "createdAt" : "2021-01-13T00:51:50Z",
        "updatedAt" : "2021-01-13T00:52:21Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      }
    ],
    "commit" : "712b0799626e4c30760c90e1ea1b15216e2d28f8",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +402,406 @@          sparkSession, plan.analyzed, cascade = false)\n      } catch {\n        case NonFatal(_) => // ignore\n      }\n      sessionCatalog.dropTempView(viewName)"
  },
  {
    "id" : "9d9d9fca-8631-4a2f-88be-533c1f10d8da",
    "prId" : 31136,
    "prUrl" : "https://github.com/apache/spark/pull/31136#pullrequestreview-567209911",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "18d6b2b0-718c-4541-afdc-51b6f44c744e",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "is it safer to not add this try-catch? if there are failures, then the uncache fails and we should not swallow the exception?",
        "createdAt" : "2021-01-13T07:51:15Z",
        "updatedAt" : "2021-01-13T07:51:16Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "aad0cc40-e2cd-480b-aa05-f1ee71828734",
        "parentId" : "18d6b2b0-718c-4541-afdc-51b6f44c744e",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "This is also discussed above. Suppose we have:\r\n```sql\r\nCREATE TEMPORARY VIEW v1 AS SELECT * FROM v2\r\n```\r\na Spark user can drop v2 first followed by v1. In this case the `uncacheQuery` will fail because `v2` is already gone. Consequently, the view will not be dropped. This seems to be a quite common scenario. \r\n\r\nAlso this is [following the `DropTableCommand`](https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/command/ddl.scala#L239), which swallows any non-fatal exception and proceed to drop the view.",
        "createdAt" : "2021-01-13T08:22:03Z",
        "updatedAt" : "2021-01-13T08:22:24Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "ce065171-00b6-4d8d-b3f9-3bac253931bf",
        "parentId" : "18d6b2b0-718c-4541-afdc-51b6f44c744e",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "thanks for the explanation!",
        "createdAt" : "2021-01-13T13:17:13Z",
        "updatedAt" : "2021-01-13T13:17:13Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "712b0799626e4c30760c90e1ea1b15216e2d28f8",
    "line" : 27,
    "diffHunk" : "@@ -1,1 +423,427 @@          sparkSession, plan.analyzed, cascade = false)\n      } catch {\n        case NonFatal(_) => // ignore\n      }\n      sessionCatalog.dropGlobalTempView(viewName)"
  },
  {
    "id" : "66a4dca1-0615-4e0e-86d0-c00480c4d9dc",
    "prId" : 31107,
    "prUrl" : "https://github.com/apache/spark/pull/31107#pullrequestreview-571481286",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a1263dae-8785-4984-b1c0-f01f779dab3f",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Why cascade behavior should be depending on if there is view text?",
        "createdAt" : "2021-01-19T08:25:36Z",
        "updatedAt" : "2021-01-19T19:56:05Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "cc23931e-7016-4cc4-828f-46c453b47357",
        "parentId" : "a1263dae-8785-4984-b1c0-f01f779dab3f",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "<del>We should not un-cache cache entries dependent on the view, isn't? This is original behavior.</del>\r\n\r\nShouldn't we uncache dependent cache entries for all cases?",
        "createdAt" : "2021-01-19T08:27:30Z",
        "updatedAt" : "2021-01-19T19:56:05Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "6f6d108d-e3b7-4da2-9ef6-91fb996ff721",
        "parentId" : "a1263dae-8785-4984-b1c0-f01f779dab3f",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "The cascade behavior is from https://github.com/apache/spark/pull/21594\r\n\r\nAfter #30567 , SQL temp view should have the same semantic of permanent view, so should do cascade uncache.",
        "createdAt" : "2021-01-19T08:48:48Z",
        "updatedAt" : "2021-01-19T19:56:05Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "ccb7054b-9ae5-4cca-bb48-2a0a2cf77e97",
        "parentId" : "a1263dae-8785-4984-b1c0-f01f779dab3f",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Yeah, I see we should do cascade uncache. The modified question is, why cascade uncache depends on if it has view text? For a view without view text, we don't do cascade uncache?",
        "createdAt" : "2021-01-19T08:52:16Z",
        "updatedAt" : "2021-01-19T19:56:05Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "486cec6c-0f9c-40c1-8cf3-dca0a976f8d8",
        "parentId" : "a1263dae-8785-4984-b1c0-f01f779dab3f",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "A view without view text is dataframe temp views, or legacy SQL temp views. We shouldn't change their behavior and keep the non-cascade uncache.",
        "createdAt" : "2021-01-19T09:51:07Z",
        "updatedAt" : "2021-01-19T19:56:05Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "380eb8e5-284b-4c1f-8585-5a101e2ce657",
        "parentId" : "a1263dae-8785-4984-b1c0-f01f779dab3f",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Yeah those are the two scenarios there will be no view text and hence the old behavior, which means users could still get bad results if the view go stale. Since the legacy SQL temp view is turned off in default, I don't see a big concern there. For views created with dataset API, I'm not sure there is a way to generate view text.",
        "createdAt" : "2021-01-19T17:15:26Z",
        "updatedAt" : "2021-01-19T19:56:05Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "417321f3-39f5-41cf-af65-c35aede495d5",
        "parentId" : "a1263dae-8785-4984-b1c0-f01f779dab3f",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Hmm, so this is not for technical reason but to keep behavior unchanged, is it correct?",
        "createdAt" : "2021-01-19T17:18:00Z",
        "updatedAt" : "2021-01-19T19:56:05Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "233b8ae6-2596-4c3d-954e-9592221ea660",
        "parentId" : "a1263dae-8785-4984-b1c0-f01f779dab3f",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "If so, can we add a few comment for that reason? ",
        "createdAt" : "2021-01-19T17:19:12Z",
        "updatedAt" : "2021-01-19T19:56:05Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "9a274555-5069-48f4-83d6-693d493d6a73",
        "parentId" : "a1263dae-8785-4984-b1c0-f01f779dab3f",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Yes we still keep the existing behavior for the above scenarios which this PR doesn't touch. Technically we could switch them to all use cascade uncache but that is a broader behavior change and should be discussed separately.\r\n\r\nSure I can add some comments.",
        "createdAt" : "2021-01-19T17:29:52Z",
        "updatedAt" : "2021-01-19T19:56:05Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      }
    ],
    "commit" : "40ef4b12ae328493c4a5ae5fd8274a7f839050de",
    "line" : 53,
    "diffHunk" : "@@ -1,1 +430,434 @@      val plan = sparkSession.sessionState.executePlan(viewDef)\n      sparkSession.sharedState.cacheManager.uncacheQuery(\n        sparkSession, plan.analyzed, cascade = viewText.isDefined)\n    } catch {\n      case NonFatal(_) => // ignore"
  },
  {
    "id" : "a8ab8ab8-4502-40cb-9534-9994392ab827",
    "prId" : 30699,
    "prUrl" : "https://github.com/apache/spark/pull/30699#pullrequestreview-548904153",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1f372c7a-fab6-4441-88a0-8733bbfac08e",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Yea, the file index is cached in the logical plan so using original `table` logical plan will read incorrect files.",
        "createdAt" : "2020-12-10T07:07:14Z",
        "updatedAt" : "2020-12-11T19:02:59Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "bcf2df7d34559bbb078e00e0ba3bca1547a0585f",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +541,545 @@      // creates a new logical plan since the old table refers to old relation which\n      // should be refreshed\n      val newTable = sparkSession.table(tableIdent)\n\n      // recache with the same name and cache level."
  },
  {
    "id" : "a8a7d84b-af8d-4f96-846e-89e920ac36c7",
    "prId" : 30699,
    "prUrl" : "https://github.com/apache/spark/pull/30699#pullrequestreview-549724518",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "49a42668-792a-4bc0-9684-3d0e25759fb6",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "BTW, I think we can just call `cacheTable(tableName, storageLevel)` API. It does the same thing.",
        "createdAt" : "2020-12-10T07:08:15Z",
        "updatedAt" : "2020-12-11T19:02:59Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "caab8344-1495-4d8d-a514-053880ff8699",
        "parentId" : "49a42668-792a-4bc0-9684-3d0e25759fb6",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "I think `cacheTable` takes a string as table name? we'll need to modify or have a new overloaded version that takes `TableIdentifier`.",
        "createdAt" : "2020-12-10T17:27:33Z",
        "updatedAt" : "2020-12-11T19:02:59Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "17a1e4e7-c304-4c0c-83ae-2ac6a2bfdd2a",
        "parentId" : "49a42668-792a-4bc0-9684-3d0e25759fb6",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Passing the `tableName` to `cacheTable`?",
        "createdAt" : "2020-12-10T17:34:24Z",
        "updatedAt" : "2020-12-11T19:02:59Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "4967a7eb-0fd8-4293-a33c-3ff2af29ea90",
        "parentId" : "49a42668-792a-4bc0-9684-3d0e25759fb6",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "oops you're right, let me change that.",
        "createdAt" : "2020-12-10T17:37:19Z",
        "updatedAt" : "2020-12-11T19:02:59Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "0c16c70f-495d-432f-a7d4-b56756e6bd9c",
        "parentId" : "49a42668-792a-4bc0-9684-3d0e25759fb6",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "@viirya even though `cacheTable` looks nicer, we may need to follow the old logic here because of table name. Someone can first cache a table using `db.tbl` as table name and then call `refreshTable` on `tbl` within the database `db`. In this case we'll need to pick up the `db` part from `cache.get.cachedRepresentation.cacheBuilder.tableName`.",
        "createdAt" : "2020-12-11T00:27:10Z",
        "updatedAt" : "2020-12-11T19:02:59Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "906a239f-cb51-407c-a07f-9218f4ebe1c3",
        "parentId" : "49a42668-792a-4bc0-9684-3d0e25759fb6",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Hmm, I see. `cacheTable` doesn't take a custom cache table name as parameter so far.",
        "createdAt" : "2020-12-11T00:30:19Z",
        "updatedAt" : "2020-12-11T19:02:59Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "bbcaab18-59c9-4a06-934d-46a67b9c6544",
        "parentId" : "49a42668-792a-4bc0-9684-3d0e25759fb6",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Yea, it also doesn't take `None` as table name but ppl could pass `None` when caching, and perhaps we should maintain that when recaching the table.",
        "createdAt" : "2020-12-11T00:37:53Z",
        "updatedAt" : "2020-12-11T19:02:59Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "f5e145de-23af-430d-8214-e70dd2f32793",
        "parentId" : "49a42668-792a-4bc0-9684-3d0e25759fb6",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Okay good. Let's go back to old logic. Thanks for the try.",
        "createdAt" : "2020-12-11T00:48:26Z",
        "updatedAt" : "2020-12-11T19:02:59Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "bcf2df7d34559bbb078e00e0ba3bca1547a0585f",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +544,548 @@\n      // recache with the same name and cache level.\n      sparkSession.sharedState.cacheManager.cacheQuery(newTable, cacheName, cacheLevel)\n    }\n  }"
  },
  {
    "id" : "0477fd1e-a334-4761-a817-bd475d10c268",
    "prId" : 30215,
    "prUrl" : "https://github.com/apache/spark/pull/30215#pullrequestreview-521159155",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6ef9e3b8-ea3c-4084-9db4-adbc9d8f8269",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "We don't need `blocking = true`?",
        "createdAt" : "2020-10-31T18:01:27Z",
        "updatedAt" : "2020-10-31T18:01:27Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "d221bcf0-e3a5-4245-ab08-40261d5d288b",
        "parentId" : "6ef9e3b8-ea3c-4084-9db4-adbc9d8f8269",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "because the default value is already true",
        "createdAt" : "2020-10-31T18:21:21Z",
        "updatedAt" : "2020-10-31T18:21:21Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      }
    ],
    "commit" : "f5062f7b3706cbcbc6a9f9707361093a380329db",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +499,503 @@    // Uncache the logicalPlan. Note this is a no-op for the table itself if it's not cached, but\n    // will invalidate all caches referencing this table.\n    sparkSession.sharedState.cacheManager.uncacheQuery(table, cascade = true)\n    if (isCached(table)) {\n      // Cache it again."
  },
  {
    "id" : "694e4923-ad84-405f-9740-147577a3d990",
    "prId" : 30187,
    "prUrl" : "https://github.com/apache/spark/pull/30187#pullrequestreview-520900738",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6b734238-069f-44b1-b10a-47e711546d82",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Hmm, we will recache the table lazily. But for other caches referencing this table, we just uncache them and don't recache them. Does it sound inconsistent?",
        "createdAt" : "2020-10-30T05:49:04Z",
        "updatedAt" : "2020-10-31T01:40:49Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "f1fa022a-5f32-44bf-99b9-99167c0fdaa7",
        "parentId" : "6b734238-069f-44b1-b10a-47e711546d82",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Nvm, we only recache if the table is cached. So for this case, the table is not cached at all, we don't recache it.",
        "createdAt" : "2020-10-30T05:57:05Z",
        "updatedAt" : "2020-10-31T01:40:49Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "514dd3e8-a618-452f-aeb8-f975ae65b477",
        "parentId" : "6b734238-069f-44b1-b10a-47e711546d82",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Not related to this PR. Since we recache the table lazily, the other caches referencing this table seems to me need to be recached. It is more consistent and seems to me we should not implicitly change the cache status of the other cached query plans. I'd like to recache these other caches if possible. WDYT? @cloud-fan ",
        "createdAt" : "2020-10-30T16:52:04Z",
        "updatedAt" : "2020-10-31T01:40:49Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "286b6181-8e48-44fc-a0b9-ecc4073e72e9",
        "parentId" : "6b734238-069f-44b1-b10a-47e711546d82",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "you mean instead of invalidating all the other caches we'd recache them? yeah it sounds reasonable to me. It seems currently the behavior for _other_ caches is somewhat undefined.",
        "createdAt" : "2020-10-30T17:45:44Z",
        "updatedAt" : "2020-10-31T01:40:49Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      }
    ],
    "commit" : "b427b7d1c0e5b60f96d9a38c7d00dff43d15e423",
    "line" : 29,
    "diffHunk" : "@@ -1,1 +540,544 @@\n      // recache with the same name and cache level.\n      sparkSession.sharedState.cacheManager.cacheQuery(table, cacheName, cacheLevel)\n    }\n  }"
  },
  {
    "id" : "d1fa479c-4de4-4b07-8fcd-a194c637ae8e",
    "prId" : 30187,
    "prUrl" : "https://github.com/apache/spark/pull/30187#pullrequestreview-523899322",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9b115e98-c116-449f-9c94-bacf51dbf076",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "Add it to the migration guide?",
        "createdAt" : "2020-11-04T03:57:34Z",
        "updatedAt" : "2020-11-04T03:57:35Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "51bc9846-71b0-4b87-bbeb-6715a5ce9a8c",
        "parentId" : "9b115e98-c116-449f-9c94-bacf51dbf076",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "hmm which migration guide should I add this to? note Spark already invalidate cache when the target table is cached, and this just extend it to the case when the table itself is not cached.",
        "createdAt" : "2020-11-04T19:46:45Z",
        "updatedAt" : "2020-11-04T19:46:46Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "b153dd96-9e56-4ed2-9991-de64d28ffb68",
        "parentId" : "9b115e98-c116-449f-9c94-bacf51dbf076",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I think he meant https://github.com/apache/spark/blob/master/docs/sql-migration-guide.md to update. I tend to agree that it's safe to mention it unless it's obviously a bug fix.",
        "createdAt" : "2020-11-05T02:48:06Z",
        "updatedAt" : "2020-11-05T02:48:07Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "2e9c98a2-8ff9-442f-9844-e625e8d791e5",
        "parentId" : "9b115e98-c116-449f-9c94-bacf51dbf076",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Thanks for the pointer @HyukjinKwon . I'll update there.",
        "createdAt" : "2020-11-05T02:51:52Z",
        "updatedAt" : "2020-11-05T02:51:53Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "260c6176-26fd-44e2-a568-48ea943242b7",
        "parentId" : "9b115e98-c116-449f-9c94-bacf51dbf076",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Created #30256",
        "createdAt" : "2020-11-05T03:20:55Z",
        "updatedAt" : "2020-11-05T03:20:55Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "5d9ce13a-9e8c-454e-ba00-40b06af41c2c",
        "parentId" : "9b115e98-c116-449f-9c94-bacf51dbf076",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Thanks @sunchao ",
        "createdAt" : "2020-11-05T03:23:44Z",
        "updatedAt" : "2020-11-05T03:23:44Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "b427b7d1c0e5b60f96d9a38c7d00dff43d15e423",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +532,536 @@    // note this is a no-op for the table itself if it's not cached, but will invalidate all\n    // caches referencing this table.\n    sparkSession.sharedState.cacheManager.uncacheQuery(table, cascade = true)\n\n    if (cache.nonEmpty) {"
  },
  {
    "id" : "95fa6c8e-5a60-4c74-a81a-f1d14ad80b34",
    "prId" : 24440,
    "prUrl" : "https://github.com/apache/spark/pull/24440#pullrequestreview-229430236",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "738f28f5-2563-44d2-9ca8-adc252cec6ac",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "I'm not sure this is the correct fix. Could you add a UT, please?",
        "createdAt" : "2019-04-23T07:23:51Z",
        "updatedAt" : "2019-08-15T05:25:40Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "f64a83a2-abfd-420a-91fe-6f6f96c186d1",
        "parentId" : "738f28f5-2563-44d2-9ca8-adc252cec6ac",
        "authorId" : "9b63ed59-19ac-4b06-b0a6-e532f53caccf",
        "body" : "@dongjoon-hyun OK!",
        "createdAt" : "2019-04-23T08:13:08Z",
        "updatedAt" : "2019-08-15T05:25:40Z",
        "lastEditedBy" : "9b63ed59-19ac-4b06-b0a6-e532f53caccf",
        "tags" : [
        ]
      },
      {
        "id" : "d4718125-792d-4c9f-bdde-99c2d5b2328f",
        "parentId" : "738f28f5-2563-44d2-9ca8-adc252cec6ac",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Thanks!",
        "createdAt" : "2019-04-23T09:07:11Z",
        "updatedAt" : "2019-08-15T05:25:40Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "770ee4261335635fafe79afebb1ce7302db96d92",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +445,449 @@    if (isCached(tableName)) {\n      sparkSession.sharedState.cacheManager.uncacheQuery(sparkSession.table(tableIdent), cascade)\n      sparkSession.sessionState.catalog.dropTempView(tableIdent.table)\n    }\n  }"
  },
  {
    "id" : "be518119-d092-4945-9f81-c436a81331f4",
    "prId" : 24440,
    "prUrl" : "https://github.com/apache/spark/pull/24440#pullrequestreview-229935596",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6fa3ae91-c382-432d-88e6-94717a7d74ae",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "If the table is not a temporary view, no need to drop it.",
        "createdAt" : "2019-04-23T10:49:35Z",
        "updatedAt" : "2019-08-15T05:25:40Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "902f054d-52f4-4183-b082-3365c9ed0586",
        "parentId" : "6fa3ae91-c382-432d-88e6-94717a7d74ae",
        "authorId" : "9b63ed59-19ac-4b06-b0a6-e532f53caccf",
        "body" : "@viirya `dropTempView`  will only delete the temporary view. Are you suggesting that I increase a judgment on the tempory view here?",
        "createdAt" : "2019-04-24T07:21:59Z",
        "updatedAt" : "2019-08-15T05:25:40Z",
        "lastEditedBy" : "9b63ed59-19ac-4b06-b0a6-e532f53caccf",
        "tags" : [
        ]
      }
    ],
    "commit" : "770ee4261335635fafe79afebb1ce7302db96d92",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +445,449 @@    if (isCached(tableName)) {\n      sparkSession.sharedState.cacheManager.uncacheQuery(sparkSession.table(tableIdent), cascade)\n      sparkSession.sessionState.catalog.dropTempView(tableIdent.table)\n    }\n  }"
  },
  {
    "id" : "b6de3772-b7b1-461e-918b-2f522dfce3b7",
    "prId" : 24440,
    "prUrl" : "https://github.com/apache/spark/pull/24440#pullrequestreview-230428916",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3b044249-ab9b-4579-ac0c-ad8e173f0f19",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "I don't think we should put dropping view at `uncacheTable`. `cacheTable` in the same `CatalogImpl` doesn't create view.\r\n\r\n`CacheTableCommand` creates view for `cache table as select` query. So I think it is more correct to drop the view in `UncacheTableCommand`.",
        "createdAt" : "2019-04-23T15:52:01Z",
        "updatedAt" : "2019-08-15T05:25:40Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "a2c5531f-f741-43bc-9c5e-8b2dd3eb7c45",
        "parentId" : "3b044249-ab9b-4579-ac0c-ad8e173f0f19",
        "authorId" : "9b63ed59-19ac-4b06-b0a6-e532f53caccf",
        "body" : "I have also considered this issue. Finally, I decided to it in `uncacheTable` for two reasons.\r\n1. `UncacheTableCommand` only uses `sparkSession.catalog.uncacheTable` to do the operation of deleting the cache, and there is no other operation.\r\n\r\n2.  There are many other places that use `sparkSession.catalog.uncacheTable` to implement the operation of `uncache table`.  I think these places also need to consider drop the view.\r\n\r\n@viirya May I ask if these considerations are correct?",
        "createdAt" : "2019-04-24T07:41:19Z",
        "updatedAt" : "2019-08-15T05:25:40Z",
        "lastEditedBy" : "9b63ed59-19ac-4b06-b0a6-e532f53caccf",
        "tags" : [
        ]
      },
      {
        "id" : "20a82936-0880-48d4-8ba1-76b5fdaae04b",
        "parentId" : "3b044249-ab9b-4579-ac0c-ad8e173f0f19",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "If the view isn't created when caching, I don't think it is good to remove the view when uncaching. `CacheTableCommand` is the caching command creating the view. I think it makes more sense to remove the view at `UncacheTableCommand`. They are a pair of commands.\r\n\r\nWhen you do deleting view at `uncacheTable`. For a view which isn't created by `cache table as select`, uncaching it will delete it too? This sounds not correct to me.\r\n",
        "createdAt" : "2019-04-24T08:18:36Z",
        "updatedAt" : "2019-08-15T05:25:40Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "c91a4753-1ba5-4646-b8b7-995dd22e3667",
        "parentId" : "3b044249-ab9b-4579-ac0c-ad8e173f0f19",
        "authorId" : "9b63ed59-19ac-4b06-b0a6-e532f53caccf",
        "body" : "@viirya Yes. There is indeed this problem. However, there is still a problem of accidental deletion when removing the view at `UncacheTableCommand`. Because when executing `uncache table`, I can't judge whether the temporary view is created by the user or created when the `cache table` is executed.\r\n\r\nI want to ask, why create a view when executing `cache table as select` . What does this view do? Because there is no correspondence between `view `and `plan`. ",
        "createdAt" : "2019-04-25T02:21:15Z",
        "updatedAt" : "2019-08-15T05:25:40Z",
        "lastEditedBy" : "9b63ed59-19ac-4b06-b0a6-e532f53caccf",
        "tags" : [
        ]
      }
    ],
    "commit" : "770ee4261335635fafe79afebb1ce7302db96d92",
    "line" : 1,
    "diffHunk" : "@@ -1,1 +440,444 @@   * @since 2.0.0\n   */\n  override def uncacheTable(tableName: String): Unit = {\n    val tableIdent = sparkSession.sessionState.sqlParser.parseTableIdentifier(tableName)\n    val cascade = !sessionCatalog.isTemporaryTable(tableIdent)"
  }
]