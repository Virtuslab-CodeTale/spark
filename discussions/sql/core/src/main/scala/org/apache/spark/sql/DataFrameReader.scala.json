[
  {
    "id" : "deb0eafa-c525-4f90-8dc9-fa391631287c",
    "prId" : 33436,
    "prUrl" : "https://github.com/apache/spark/pull/33436#pullrequestreview-711231249",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "21c1f084-0c65-4a7a-b1e0-72beab989157",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Actually would you mind updating migration guide (https://github.com/apache/spark/blob/master/docs/sql-migration-guide.md)? You can describe, for example, non-nullable schema was not supported properly in previous Spark version so the output schema of `DataFrameReader.json(jsonDataset: Dataset[String])` and `DataFrameReader.csv(csvDataset: Dataset[String])` became nullable which also matches with `DataFrameReader.json(path: String)` and `DataFrameReader.csv(path: String)`.\r\n",
        "createdAt" : "2021-07-21T01:16:48Z",
        "updatedAt" : "2021-07-21T01:16:48Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "6131ffc2-df3c-4d1a-9229-8df71b35b583",
        "parentId" : "21c1f084-0c65-4a7a-b1e0-72beab989157",
        "authorId" : "c12eb4aa-c39d-4fa9-8470-ed071c0be24a",
        "body" : "Done. BTW, shall we backport this PR into the previous version?",
        "createdAt" : "2021-07-21T02:21:58Z",
        "updatedAt" : "2021-07-21T02:21:58Z",
        "lastEditedBy" : "c12eb4aa-c39d-4fa9-8470-ed071c0be24a",
        "tags" : [
        ]
      },
      {
        "id" : "4008e261-9869-46a3-a326-6b1b4eb6e7b9",
        "parentId" : "21c1f084-0c65-4a7a-b1e0-72beab989157",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Let's don't port it back. it will be merged into 3.3 only for now.",
        "createdAt" : "2021-07-21T03:40:15Z",
        "updatedAt" : "2021-07-21T03:40:15Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "56ceec462588aec85ec869fe15c66447ee93ccf1",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +522,526 @@      }\n\n    val schema = userSpecifiedSchema.map(_.asNullable).getOrElse {\n      TextInputCSVDataSource.inferFromDataset(\n        sparkSession,"
  }
]