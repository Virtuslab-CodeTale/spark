[
  {
    "id" : "deb0eafa-c525-4f90-8dc9-fa391631287c",
    "prId" : 33436,
    "prUrl" : "https://github.com/apache/spark/pull/33436#pullrequestreview-711231249",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "21c1f084-0c65-4a7a-b1e0-72beab989157",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Actually would you mind updating migration guide (https://github.com/apache/spark/blob/master/docs/sql-migration-guide.md)? You can describe, for example, non-nullable schema was not supported properly in previous Spark version so the output schema of `DataFrameReader.json(jsonDataset: Dataset[String])` and `DataFrameReader.csv(csvDataset: Dataset[String])` became nullable which also matches with `DataFrameReader.json(path: String)` and `DataFrameReader.csv(path: String)`.\r\n",
        "createdAt" : "2021-07-21T01:16:48Z",
        "updatedAt" : "2021-07-21T01:16:48Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "6131ffc2-df3c-4d1a-9229-8df71b35b583",
        "parentId" : "21c1f084-0c65-4a7a-b1e0-72beab989157",
        "authorId" : "c12eb4aa-c39d-4fa9-8470-ed071c0be24a",
        "body" : "Done. BTW, shall we backport this PR into the previous version?",
        "createdAt" : "2021-07-21T02:21:58Z",
        "updatedAt" : "2021-07-21T02:21:58Z",
        "lastEditedBy" : "c12eb4aa-c39d-4fa9-8470-ed071c0be24a",
        "tags" : [
        ]
      },
      {
        "id" : "4008e261-9869-46a3-a326-6b1b4eb6e7b9",
        "parentId" : "21c1f084-0c65-4a7a-b1e0-72beab989157",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Let's don't port it back. it will be merged into 3.3 only for now.",
        "createdAt" : "2021-07-21T03:40:15Z",
        "updatedAt" : "2021-07-21T03:40:15Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "56ceec462588aec85ec869fe15c66447ee93ccf1",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +522,526 @@      }\n\n    val schema = userSpecifiedSchema.map(_.asNullable).getOrElse {\n      TextInputCSVDataSource.inferFromDataset(\n        sparkSession,"
  },
  {
    "id" : "5754f193-af12-4a00-bde4-efec796310a9",
    "prId" : 32546,
    "prUrl" : "https://github.com/apache/spark/pull/32546#pullrequestreview-663979905",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a8fcee7b-62a1-4d4d-8db2-352891f3ed9a",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "ditto. it says ORC-specific options but also mentions about Generic Files Source Options later",
        "createdAt" : "2021-05-20T06:19:25Z",
        "updatedAt" : "2021-05-20T06:19:25Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "043d3087c838c2f6439d798bdc8e889d6a728ff7",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +875,879 @@   * Loads ORC files and returns the result as a `DataFrame`.\n   *\n   * ORC-specific option(s) for reading ORC files can be found in\n   * <a href=\n   *   \"https://spark.apache.org/docs/latest/sql-data-sources-orc.html#data-source-option\">"
  },
  {
    "id" : "589fd460-f3aa-4404-8693-604af5a26904",
    "prId" : 32546,
    "prUrl" : "https://github.com/apache/spark/pull/32546#pullrequestreview-665066612",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "658c2718-2774-4df0-998b-3eef4e8274c0",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Ditto.",
        "createdAt" : "2021-05-21T03:48:06Z",
        "updatedAt" : "2021-05-21T03:48:06Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "043d3087c838c2f6439d798bdc8e889d6a728ff7",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +877,881 @@   * ORC-specific option(s) for reading ORC files can be found in\n   * <a href=\n   *   \"https://spark.apache.org/docs/latest/sql-data-sources-orc.html#data-source-option\">\n   *   Data Source Option</a> in the version you use.\n   *"
  },
  {
    "id" : "5fb9397a-2738-4718-a8f2-4215bee2b97a",
    "prId" : 32204,
    "prUrl" : "https://github.com/apache/spark/pull/32204#pullrequestreview-663975403",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c88dd33c-2af6-4e5c-b0ad-4424e027102e",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "ditto. It says JSON specific options but it mentions \"Generic Files Source Options\"",
        "createdAt" : "2021-05-20T06:11:25Z",
        "updatedAt" : "2021-05-20T06:11:26Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "a10586c3d2887463de16984adb72d205f85f3796",
    "line" : 151,
    "diffHunk" : "@@ -1,1 +391,395 @@   * schema in advance, use the version that specifies the schema to avoid the extra scan.\n   *\n   * You can find the JSON-specific options for reading JSON files in\n   * <a href=\"https://spark.apache.org/docs/latest/sql-data-sources-json.html#data-source-option\">\n   *   Data Source Option</a> in the version you use."
  },
  {
    "id" : "a76de3c9-9fb8-44a9-b943-b61b4f51d49a",
    "prId" : 32161,
    "prUrl" : "https://github.com/apache/spark/pull/32161#pullrequestreview-658596588",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7c357be5-360d-422d-9180-d8bd24f299d3",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "can you add the general options here too",
        "createdAt" : "2021-05-13T05:25:25Z",
        "updatedAt" : "2021-05-13T05:25:25Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "d6417a8124eb61390089313d108eff18fd89e412",
    "line" : 47,
    "diffHunk" : "@@ -1,1 +816,820 @@   * <a href=\n   *   \"https://spark.apache.org/docs/latest/sql-data-sources-parquet.html#data-source-option\">\n   *   Data Source Option</a> in the version you use.\n   *\n   * @since 1.4.0"
  },
  {
    "id" : "17bc2cb1-a0de-4786-be3c-8653e3292717",
    "prId" : 32161,
    "prUrl" : "https://github.com/apache/spark/pull/32161#pullrequestreview-663970157",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "648f2a0f-ba47-4664-84c5-45f29fb4290e",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "The sentence doesn't make sense because you linked general options but it says Parquet specific options.",
        "createdAt" : "2021-05-20T06:01:42Z",
        "updatedAt" : "2021-05-20T06:01:42Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "d6417a8124eb61390089313d108eff18fd89e412",
    "line" : 44,
    "diffHunk" : "@@ -1,1 +813,817 @@   * Loads a Parquet file, returning the result as a `DataFrame`.\n   *\n   * Parquet-specific option(s) for reading Parquet files can be found in\n   * <a href=\n   *   \"https://spark.apache.org/docs/latest/sql-data-sources-parquet.html#data-source-option\">"
  },
  {
    "id" : "8a4272ef-211f-4569-9761-8e4681ff6378",
    "prId" : 31593,
    "prUrl" : "https://github.com/apache/spark/pull/31593#pullrequestreview-594704138",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "366a90e6-3813-4fdb-a1df-e3d4db480f2f",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Seems fine. It'd be OK IMHO to require it to be non-null too.",
        "createdAt" : "2021-02-19T15:10:11Z",
        "updatedAt" : "2021-02-22T09:18:49Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "4e08d99a-6194-4c01-8d5b-fd8445414ec8",
        "parentId" : "366a90e6-3813-4fdb-a1df-e3d4db480f2f",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "I'd also prefer not allowing null, but no strong voice. If we'd like to allow null, probably ideal to clearly define the behavior of null rather than leaving it as \"undefined behavior\".",
        "createdAt" : "2021-02-20T03:13:17Z",
        "updatedAt" : "2021-02-22T09:18:49Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "643f002379a712a7fe1da3245ac396a1c4c8ad42",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +74,78 @@   */\n  def schema(schema: StructType): DataFrameReader = {\n    if (schema != null) {\n      val replaced = CharVarcharUtils.failIfHasCharVarchar(schema).asInstanceOf[StructType]\n      this.userSpecifiedSchema = Option(replaced)"
  },
  {
    "id" : "f403cfeb-6b4d-4fde-92a9-6c824779b1b6",
    "prId" : 31593,
    "prUrl" : "https://github.com/apache/spark/pull/31593#pullrequestreview-594706606",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1e7d06db-9a3b-4fba-8910-b01f74f1a577",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "yeah, probably we should just disallow `null`s. Maybe we should use `None` and `Some` to explicitly note that it's optionally allowed?",
        "createdAt" : "2021-02-20T03:42:24Z",
        "updatedAt" : "2021-02-22T09:18:49Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "643f002379a712a7fe1da3245ac396a1c4c8ad42",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +74,78 @@   */\n  def schema(schema: StructType): DataFrameReader = {\n    if (schema != null) {\n      val replaced = CharVarcharUtils.failIfHasCharVarchar(schema).asInstanceOf[StructType]\n      this.userSpecifiedSchema = Option(replaced)"
  },
  {
    "id" : "4add9dd8-940d-436f-b191-91e0ac718531",
    "prId" : 30518,
    "prUrl" : "https://github.com/apache/spark/pull/30518#pullrequestreview-539708657",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d88cfa6f-55c4-44bd-9611-850b9e4909fa",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Seems fine. Can you also update `DataStreamReader.scala`, `readwriter.py`, and `streaming.py`?",
        "createdAt" : "2020-11-27T01:50:03Z",
        "updatedAt" : "2020-11-27T04:57:58Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "d66347b3-ac5b-4805-a020-91f63c155063",
        "parentId" : "d88cfa6f-55c4-44bd-9611-850b9e4909fa",
        "authorId" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "body" : "Address 1770c56 try to  fix this",
        "createdAt" : "2020-11-27T03:21:10Z",
        "updatedAt" : "2020-11-27T04:57:58Z",
        "lastEditedBy" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "tags" : [
        ]
      },
      {
        "id" : "5127dc95-208b-4f4d-81d0-f4c72b43a729",
        "parentId" : "d88cfa6f-55c4-44bd-9611-850b9e4909fa",
        "authorId" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "body" : "Address 84c1d59 try to fix python file,  I'm not familiar with Python :(",
        "createdAt" : "2020-11-27T03:43:02Z",
        "updatedAt" : "2020-11-27T04:57:58Z",
        "lastEditedBy" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "tags" : [
        ]
      },
      {
        "id" : "329eeb33-4b95-4f98-95f5-89210b850570",
        "parentId" : "d88cfa6f-55c4-44bd-9611-850b9e4909fa",
        "authorId" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "body" : "@HyukjinKwon also needs to be update `readwriter.pyi` , and `streaming.pyi`? Is that right?",
        "createdAt" : "2020-11-27T04:59:22Z",
        "updatedAt" : "2020-11-27T04:59:22Z",
        "lastEditedBy" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "tags" : [
        ]
      }
    ],
    "commit" : "ca2900de52497ce5bb3bad90f0f28678248c7595",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +728,732 @@   * <li>`maxCharsPerColumn` (default `-1`): defines the maximum number of characters allowed\n   * for any given value being read. By default, it is -1 meaning unlimited length</li>\n   * <li>`unescapedQuoteHandling` (default `STOP_AT_DELIMITER`): defines how the CsvParser\n   * will handle values with unescaped quotes.\n   *   <ul>"
  },
  {
    "id" : "f5cb76fa-2c46-4c64-958e-4089ec7a453e",
    "prId" : 29328,
    "prUrl" : "https://github.com/apache/spark/pull/29328#pullrequestreview-464744301",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "315169b2-bb96-4918-8562-bb870bbe3fb0",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "What I propose is\r\n```\r\ndef load(paths: String*): DataFrame = {\r\n  if (extraOptions.contains(\"path\") && paths.nonEmpty) fail ...\r\n  ...\r\n  if (DSV2) {\r\n    val pathOption = if (paths.isEmpty) {\r\n      None\r\n    } else if (paths.length == 1) {\r\n      Some(\"path\" -> paths.head)\r\n    } else {\r\n      Some(\"paths\" -> ...)\r\n    }\r\n  } else {\r\n    // for DS V1, do not generate extra path option\r\n  }\r\n}\r\n```",
        "createdAt" : "2020-08-10T04:38:49Z",
        "updatedAt" : "2020-08-22T23:06:49Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "78e80638-0e58-41de-ac8e-ea5bd76445dd",
        "parentId" : "315169b2-bb96-4918-8562-bb870bbe3fb0",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "Is the following required (or is this just for semantics)?\r\n\r\n>  else if (paths.length == 1) {\r\n>    Some(\"path\" -> paths.head)\r\n>  }\r\n\r\nI see `paths` should handle one or more paths: https://github.com/apache/spark/blob/8659ec554fe1993fcaf2ec4d1c4745205bddec7d/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/FileDataSourceV2.scala#L51-L57\r\n\r\n",
        "createdAt" : "2020-08-10T20:30:27Z",
        "updatedAt" : "2020-08-22T23:06:49Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      },
      {
        "id" : "081cc827-5363-46af-842a-c99d4eb1c798",
        "parentId" : "315169b2-bb96-4918-8562-bb870bbe3fb0",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "It's required, because\r\n```\r\ndef load(path: String): DataFrame = {\r\n  // force invocation of `load(...varargs...)`\r\n  option(\"path\", path).load(Seq.empty: _*)\r\n}\r\n```\r\n\r\nIf we remove this method and always call `def load(paths: String*)`, we should make sure the behavior doesn't change, and we still generate \"path\" option if only one path is given.",
        "createdAt" : "2020-08-11T03:29:00Z",
        "updatedAt" : "2020-08-22T23:06:49Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "26a8fcb2-4cad-4930-8ef7-d4a68da02f5a",
        "parentId" : "315169b2-bb96-4918-8562-bb870bbe3fb0",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "Hmm, `def parquet(path: String)` has been calling `def load(paths: String*)`, not `def load(path: String)`, and it seems to have worked fine so far (other than the inconsistency this PR addresses), no?\r\n\r\nBtw, we shouldn't remove `def load(path: String)` because it breaks the binary compatibility.",
        "createdAt" : "2020-08-11T04:34:13Z",
        "updatedAt" : "2020-08-22T23:06:49Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      },
      {
        "id" : "a2a2952b-e602-421c-903b-3685dcbb16d2",
        "parentId" : "315169b2-bb96-4918-8562-bb870bbe3fb0",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "parquet is a builtin source, so we can make it work anyway. `def load(path: String)` is general for other external sources, and we should guarantee backward compatibility here.",
        "createdAt" : "2020-08-11T04:42:26Z",
        "updatedAt" : "2020-08-22T23:06:49Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "33a39d2f-9aa5-4bf9-aad0-28de8259afd5",
        "parentId" : "315169b2-bb96-4918-8562-bb870bbe3fb0",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "Ah OK. Thanks.",
        "createdAt" : "2020-08-11T04:44:17Z",
        "updatedAt" : "2020-08-22T23:06:49Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "e1decd4c7921f58446a46081b339d308d36529cc",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +250,254 @@        \"read files of Hive data source directly.\")\n    }\n\n    val legacyPathOptionBehavior = sparkSession.sessionState.conf.legacyPathOptionBehavior\n    if (!legacyPathOptionBehavior &&"
  },
  {
    "id" : "c1523652-e623-4e33-8a69-80b2c8fc01eb",
    "prId" : 29328,
    "prUrl" : "https://github.com/apache/spark/pull/29328#pullrequestreview-472967776",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "76d2de82-98a3-4f67-a9fd-42688f347416",
        "parentId" : null,
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "This is needed to restore the legacy behavior. Note that the legacy behavior is different for the following:\r\n* `spark.read.option(\"path\", path).parquet(path)` => path is read twice\r\n* `spark.read.format(\"parquet\").option(\"path\", path).load(path)` => path is read once (option is overwritten)",
        "createdAt" : "2020-08-22T23:14:28Z",
        "updatedAt" : "2020-08-22T23:14:32Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "e1decd4c7921f58446a46081b339d308d36529cc",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +232,236 @@    // force invocation of `load(...varargs...)`\n    if (sparkSession.sessionState.conf.legacyPathOptionBehavior) {\n      option(\"path\", path).load(Seq.empty: _*)\n    } else {\n      load(Seq(path): _*)"
  }
]