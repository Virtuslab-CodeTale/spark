[
  {
    "id" : "c8e0d020-63c9-446b-b042-c7e9e400a0e8",
    "prId" : 33711,
    "prUrl" : "https://github.com/apache/spark/pull/33711#pullrequestreview-729196088",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9ad4375f-cf56-4596-bebd-ef159891a4b4",
        "parentId" : null,
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "`AllTuples` is only interesting when the underlying file scan operator has only 1 bucket. So I think we need have a way to check the scan operator to have only 1 bucket as well.",
        "createdAt" : "2021-08-11T19:21:16Z",
        "updatedAt" : "2021-08-11T19:25:45Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      },
      {
        "id" : "bdf532e6-cd80-4965-a607-2885e3f38991",
        "parentId" : "9ad4375f-cf56-4596-bebd-ef159891a4b4",
        "authorId" : "764fa38b-fbb0-4c57-955b-77974e2520bd",
        "body" : "`AllTuples` require a single partition. I don't think the additional check for the number of buckets is required.",
        "createdAt" : "2021-08-11T20:26:05Z",
        "updatedAt" : "2021-08-11T20:26:05Z",
        "lastEditedBy" : "764fa38b-fbb0-4c57-955b-77974e2520bd",
        "tags" : [
        ]
      },
      {
        "id" : "c054be57-81c0-4d64-a081-8dd71041e22e",
        "parentId" : "9ad4375f-cf56-4596-bebd-ef159891a4b4",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "For a query like below\r\n\r\n```\r\nSELECT SUM(id)\r\nFROM t1\r\n```\r\n\r\nIf table `t1` has more than 1 bucket (say 4 buckets), we can still disable bucket scan, because scan will have 4 RDD partitions/tasks with bucketed scan, and it does not satisfy single partition requirement from `AllTuples`.",
        "createdAt" : "2021-08-11T20:37:16Z",
        "updatedAt" : "2021-08-11T20:37:54Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      },
      {
        "id" : "ec73dcc6-6a40-45b9-88ea-3fc26ae9ba02",
        "parentId" : "9ad4375f-cf56-4596-bebd-ef159891a4b4",
        "authorId" : "764fa38b-fbb0-4c57-955b-77974e2520bd",
        "body" : "If you disable bucket scan, then it will not enter the `disableBucketWithInterestingPartition` right?",
        "createdAt" : "2021-08-11T20:56:36Z",
        "updatedAt" : "2021-08-11T20:56:36Z",
        "lastEditedBy" : "764fa38b-fbb0-4c57-955b-77974e2520bd",
        "tags" : [
        ]
      },
      {
        "id" : "9101157a-187e-4640-8ab6-07c185723c93",
        "parentId" : "9ad4375f-cf56-4596-bebd-ef159891a4b4",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "Actually after a second thought, I think the change here should work. For the query like https://github.com/apache/spark/pull/33711#discussion_r687173621, the query plan when entering this rule, would be:\r\n\r\n```\r\nHashAggregate(keys=[], functions=[sum(id#41)], output=[sum(id)#45L])\r\n+- Exchange SinglePartition, ENSURE_REQUIREMENTS, [id=#66]\r\n   +- HashAggregate(keys=[], functions=[partial_sum(id#41)], output=[sum#48L])\r\n       +- FileScan\r\n```\r\n\r\nSo the bucket scan will be automatically disabled by the rule, because there's a shuffle between `HashAggregate` and `FileScan`.",
        "createdAt" : "2021-08-11T21:12:22Z",
        "updatedAt" : "2021-08-11T21:12:23Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      },
      {
        "id" : "1c6f0071-0178-40bb-b185-313ed4f84367",
        "parentId" : "9ad4375f-cf56-4596-bebd-ef159891a4b4",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "It's flaky to fix this issue in an optimizer rule since this rule can be disabled sometime. Is it better that change the outpt partitioning in `FileSourceScanExec` to `UnknownPartitioning` if the bucket is 1 ?",
        "createdAt" : "2021-08-12T01:44:51Z",
        "updatedAt" : "2021-08-12T01:44:52Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      },
      {
        "id" : "222f86ce-147e-4aaf-8afa-2b817f516cd7",
        "parentId" : "9ad4375f-cf56-4596-bebd-ef159891a4b4",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "This is a bug of the `DisableUnnecessaryBucketedScan` rule, IIUC",
        "createdAt" : "2021-08-12T05:53:10Z",
        "updatedAt" : "2021-08-12T05:53:10Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "47005599-4b54-4a00-bba7-9fd6be7e33ae",
        "parentId" : "9ad4375f-cf56-4596-bebd-ef159891a4b4",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "@ulysses-you - this is a bug in `DisableUnnecessaryBucketedScan` itself. In addition:\r\n\r\n> Is it better that change the outpt partitioning in FileSourceScanExec to UnknownPartitioning if the bucket is 1 ?\r\n\r\nThis will cause regression for some queries, e.g. the aggregation query in https://github.com/apache/spark/pull/33711#discussion_r687173621. An extra shuffle is needed before aggregation, if the output partitioning is changed from `HashPartitioning(id, 1)` to `UnknownPartitioning`. ",
        "createdAt" : "2021-08-12T19:32:32Z",
        "updatedAt" : "2021-08-12T19:32:33Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      },
      {
        "id" : "c2a14a4c-2b9a-4a69-be30-8ea5fe2c1c22",
        "parentId" : "9ad4375f-cf56-4596-bebd-ef159891a4b4",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "ah I see it, thank you @cloud-fan @c21 !",
        "createdAt" : "2021-08-13T01:32:57Z",
        "updatedAt" : "2021-08-13T01:32:57Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      }
    ],
    "commit" : "266a5f1bedb640b0adfc2a07a0f0cb7d66188c5c",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +121,125 @@  private def hasInterestingPartition(plan: SparkPlan): Boolean = {\n    plan.requiredChildDistribution.exists {\n      case _: ClusteredDistribution | _: HashClusteredDistribution | AllTuples => true\n      case _ => false\n    }"
  },
  {
    "id" : "43cee069-4635-4924-9f9b-4b225c48bee9",
    "prId" : 29804,
    "prUrl" : "https://github.com/apache/spark/pull/29804#pullrequestreview-495274166",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "596c4a2d-049c-489c-9c56-b43ce5af0214",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "What if a scan operator reads most buckets? e.g., 999 of 1000 buckets. We select bucket scans even in this case?",
        "createdAt" : "2020-09-23T00:07:19Z",
        "updatedAt" : "2020-10-01T01:11:06Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "519da7ce-e718-46c3-a36c-be757ef29f57",
        "parentId" : "596c4a2d-049c-489c-9c56-b43ce5af0214",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "@maropu - this is a good question, and I think it is kind of out of scope for this PR and needs more thoughts later. We don't have a cost model to decide whether to do (bucketed filter + bucketed scan) vs (normal filter + non-bucketed scan). It can depend on number of buckets, size of filtered buckets, CPU cost for filter, etc.",
        "createdAt" : "2020-09-23T06:22:10Z",
        "updatedAt" : "2020-10-01T01:11:06Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      },
      {
        "id" : "067d4cc4-4902-4932-ac5e-df750cdd02f1",
        "parentId" : "596c4a2d-049c-489c-9c56-b43ce5af0214",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I'm fine with it for now. Technically I think filter by bucket ID and bucketed scan don't need to be coupled. We can always filter files by bucket id, and then do bucketed scan or not according to this rule.",
        "createdAt" : "2020-09-23T06:44:25Z",
        "updatedAt" : "2020-10-01T01:11:06Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "1113fe33-8b28-4b38-bccf-b7b8bd27384f",
        "parentId" : "596c4a2d-049c-489c-9c56-b43ce5af0214",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Yea, okay. Could you file jira later, @c21 ?",
        "createdAt" : "2020-09-24T01:21:23Z",
        "updatedAt" : "2020-10-01T01:11:06Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "2141e120-74ac-4ac5-9631-f21bc60d657b",
        "parentId" : "596c4a2d-049c-489c-9c56-b43ce5af0214",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "@maropu - sure, filed https://issues.apache.org/jira/browse/SPARK-32985 .",
        "createdAt" : "2020-09-24T07:04:27Z",
        "updatedAt" : "2020-10-01T01:11:06Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      }
    ],
    "commit" : "b29f688fc4a06bc35effa6d24632d2a64501c9fd",
    "line" : 146,
    "diffHunk" : "@@ -1,1 +144,148 @@    // because bucketed table scan is still useful here to save CPU/IO cost with\n    // only reading selected bucket files.\n    scan.bucketedScan && scan.optionalBucketSet.isEmpty\n  }\n"
  },
  {
    "id" : "2a812fd2-e0d0-4e87-a406-08d0bc224e4d",
    "prId" : 29804,
    "prUrl" : "https://github.com/apache/spark/pull/29804#pullrequestreview-497705097",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fca93e6d-5e81-4401-9706-1268330e74fe",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Can you add description why this is needed? HasInterestingPartition and at lease one Exchange sounds obvious condition, but this allowed unary exec node is not. Why we can disable bucketed scan only if those exec nodes?",
        "createdAt" : "2020-09-28T06:48:08Z",
        "updatedAt" : "2020-10-01T01:11:06Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "55fe1ab6-af7a-4f8c-a107-d631364707e8",
        "parentId" : "fca93e6d-5e81-4401-9706-1268330e74fe",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "@viirya - this is good question. I agree we can be more bold and we probably don't need a whitelist operators here, e.g. SMJ - shuffle - BHJ - scan, SMJ - Shuffle - union - Scan (and another scan) should also work, but my feeling is to start with more confidence change first and improve later. With a whitelist operators here, we have a high confidence that this feature should work without introducing regression, but much less confidence if we allow arbitrary operators in the middle (at least for me). For now, to be honest, I cannot find a case why arbitrary operators cannot work. But I want to play safer in the beginning and any future improvement for this is much welcomed. cc @cloud-fan and @maropu for thoughts.\r\n\r\nAdded a comment for now.",
        "createdAt" : "2020-09-28T08:15:48Z",
        "updatedAt" : "2020-10-01T01:11:06Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      },
      {
        "id" : "160063a4-bcc7-44fe-a17d-794668ded264",
        "parentId" : "fca93e6d-5e81-4401-9706-1268330e74fe",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "In my opnion, it is okay for this PR to focus on a basic (minimal) support for the auto bucket scan. In followup activities, I think we can optimize it step-by-step by adding test cases and checking performance improvements... (Anyway, it would be better to leave some comment there about it as @viirya suggested above)",
        "createdAt" : "2020-09-28T12:02:38Z",
        "updatedAt" : "2020-10-01T01:11:06Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "f5fc659f-0a51-4830-84f3-caccc709e314",
        "parentId" : "fca93e6d-5e81-4401-9706-1268330e74fe",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "We should make the code clear for developer and maintainer, so leaving some comment is nicer if we want to constrain the scope of this rule for now.",
        "createdAt" : "2020-09-28T16:15:43Z",
        "updatedAt" : "2020-10-01T01:11:06Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "16937fba-e386-4db2-a84a-d1c52040963f",
        "parentId" : "fca93e6d-5e81-4401-9706-1268330e74fe",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "I already added a comment in last iteration. Please suggest concretely for alternative comment if it's not looking good. Thanks.",
        "createdAt" : "2020-09-28T16:34:33Z",
        "updatedAt" : "2020-10-01T01:11:06Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      }
    ],
    "commit" : "b29f688fc4a06bc35effa6d24632d2a64501c9fd",
    "line" : 133,
    "diffHunk" : "@@ -1,1 +131,135 @@   * bucketed table scan and operator with interesting partition.\n   */\n  private def isAllowedUnaryExecNode(plan: SparkPlan): Boolean = {\n    plan match {\n      case _: SortExec | _: ProjectExec | _: FilterExec => true"
  }
]