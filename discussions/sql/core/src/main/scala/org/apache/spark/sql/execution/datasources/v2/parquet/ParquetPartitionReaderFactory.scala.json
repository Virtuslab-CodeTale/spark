[
  {
    "id" : "f69f7db8-b0de-4008-8000-a1b0a32b657d",
    "prId" : 32049,
    "prUrl" : "https://github.com/apache/spark/pull/32049#pullrequestreview-628374589",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a1d84906-cbee-453d-a541-855050d47d15",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "So if we use aggregate pushdown for Parquet, we cannot use vectorized Parquet reader, right? Can you describe it too in the config doc?",
        "createdAt" : "2021-04-04T17:43:41Z",
        "updatedAt" : "2021-04-28T06:21:16Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "33c97c5d-def4-476c-b127-e3c481493b0c",
        "parentId" : "a1d84906-cbee-453d-a541-855050d47d15",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "It seems that it's supported to read aggregation result into a `ColumnarBatch` below in `buildColumnarReader`. So we can still do aggregation push down with vectorized reader enabled right?",
        "createdAt" : "2021-04-05T07:43:07Z",
        "updatedAt" : "2021-04-28T06:21:16Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      },
      {
        "id" : "a022c08e-7efc-47ec-98f9-e3f005d3e31d",
        "parentId" : "a1d84906-cbee-453d-a541-855050d47d15",
        "authorId" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "body" : "I think it doesn't matter if the vectorized reader is enabled or not.  Since we are reading the statistics information from the parquet footer, we don't really create a VectorizedReader. But if columnar reader is enabled, we return a `ColumnarBatch` instead of a `InternalRow`.",
        "createdAt" : "2021-04-06T01:17:33Z",
        "updatedAt" : "2021-04-28T06:21:16Z",
        "lastEditedBy" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "tags" : [
        ]
      }
    ],
    "commit" : "5c2b630a61d8ea9263116c37e154884a5cabd2ca",
    "line" : 87,
    "diffHunk" : "@@ -1,1 +126,130 @@\n      val reader = if (enableVectorizedReader) {\n        createVectorizedReader(file)\n      } else {\n        createRowBaseReader(file)"
  },
  {
    "id" : "6d96f71b-bf1b-4a59-a48d-cf3cfca4bb42",
    "prId" : 31489,
    "prUrl" : "https://github.com/apache/spark/pull/31489#pullrequestreview-585492709",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5b342215-e3e8-4375-8aef-12dd7dc41714",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Does it really work? The 2 fields of `ParquetOptions`  are `transient`, and become null after (de)serialization.",
        "createdAt" : "2021-02-08T13:23:25Z",
        "updatedAt" : "2021-02-08T13:23:25Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "a07a5f3f-ac97-431d-b7fe-3187a23634ba",
        "parentId" : "5b342215-e3e8-4375-8aef-12dd7dc41714",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "ah nvm, we read the confs and put it in `val`.",
        "createdAt" : "2021-02-08T13:24:12Z",
        "updatedAt" : "2021-02-08T13:24:12Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "ebc7298bde1b6bb5c7eab05b731bea6d67127ff0",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +62,66 @@    partitionSchema: StructType,\n    filters: Array[Filter],\n    parquetOptions: ParquetOptions) extends FilePartitionReaderFactory with Logging {\n  private val isCaseSensitive = sqlConf.caseSensitiveAnalysis\n  private val resultSchema = StructType(partitionSchema.fields ++ readDataSchema.fields)"
  }
]