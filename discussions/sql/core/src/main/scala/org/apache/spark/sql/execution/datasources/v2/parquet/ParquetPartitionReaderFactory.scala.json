[
  {
    "id" : "f69f7db8-b0de-4008-8000-a1b0a32b657d",
    "prId" : 32049,
    "prUrl" : "https://github.com/apache/spark/pull/32049#pullrequestreview-628374589",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a1d84906-cbee-453d-a541-855050d47d15",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "So if we use aggregate pushdown for Parquet, we cannot use vectorized Parquet reader, right? Can you describe it too in the config doc?",
        "createdAt" : "2021-04-04T17:43:41Z",
        "updatedAt" : "2021-04-28T06:21:16Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "33c97c5d-def4-476c-b127-e3c481493b0c",
        "parentId" : "a1d84906-cbee-453d-a541-855050d47d15",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "It seems that it's supported to read aggregation result into a `ColumnarBatch` below in `buildColumnarReader`. So we can still do aggregation push down with vectorized reader enabled right?",
        "createdAt" : "2021-04-05T07:43:07Z",
        "updatedAt" : "2021-04-28T06:21:16Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      },
      {
        "id" : "a022c08e-7efc-47ec-98f9-e3f005d3e31d",
        "parentId" : "a1d84906-cbee-453d-a541-855050d47d15",
        "authorId" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "body" : "I think it doesn't matter if the vectorized reader is enabled or not.  Since we are reading the statistics information from the parquet footer, we don't really create a VectorizedReader. But if columnar reader is enabled, we return a `ColumnarBatch` instead of a `InternalRow`.",
        "createdAt" : "2021-04-06T01:17:33Z",
        "updatedAt" : "2021-04-28T06:21:16Z",
        "lastEditedBy" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "tags" : [
        ]
      }
    ],
    "commit" : "5c2b630a61d8ea9263116c37e154884a5cabd2ca",
    "line" : 87,
    "diffHunk" : "@@ -1,1 +126,130 @@\n      val reader = if (enableVectorizedReader) {\n        createVectorizedReader(file)\n      } else {\n        createRowBaseReader(file)"
  },
  {
    "id" : "6d96f71b-bf1b-4a59-a48d-cf3cfca4bb42",
    "prId" : 31489,
    "prUrl" : "https://github.com/apache/spark/pull/31489#pullrequestreview-585492709",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5b342215-e3e8-4375-8aef-12dd7dc41714",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Does it really work? The 2 fields of `ParquetOptions`  are `transient`, and become null after (de)serialization.",
        "createdAt" : "2021-02-08T13:23:25Z",
        "updatedAt" : "2021-02-08T13:23:25Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "a07a5f3f-ac97-431d-b7fe-3187a23634ba",
        "parentId" : "5b342215-e3e8-4375-8aef-12dd7dc41714",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "ah nvm, we read the confs and put it in `val`.",
        "createdAt" : "2021-02-08T13:24:12Z",
        "updatedAt" : "2021-02-08T13:24:12Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "ebc7298bde1b6bb5c7eab05b731bea6d67127ff0",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +62,66 @@    partitionSchema: StructType,\n    filters: Array[Filter],\n    parquetOptions: ParquetOptions) extends FilePartitionReaderFactory with Logging {\n  private val isCaseSensitive = sqlConf.caseSensitiveAnalysis\n  private val resultSchema = StructType(partitionSchema.fields ++ readDataSchema.fields)"
  },
  {
    "id" : "5230747b-3a49-4639-99d2-188ce8c31991",
    "prId" : 24327,
    "prUrl" : "https://github.com/apache/spark/pull/24327#pullrequestreview-244425501",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "31b4602a-81ec-42f1-968e-c7f1d57933be",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "`ParquetScanBuilder` already did most thing. Here, I guess we need `.reduceOption(FilterApi.and)` only. Please correct me if I'm wrong~\r\n\r\nAlso, cc @cloud-fan",
        "createdAt" : "2019-05-31T05:35:00Z",
        "updatedAt" : "2019-06-14T16:45:37Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "6650afb6-d2d5-4c6a-8bd1-8f917068506c",
        "parentId" : "31b4602a-81ec-42f1-968e-c7f1d57933be",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "We still need to do the actual push down here.\r\nSee my comments above: https://github.com/apache/spark/pull/24327#discussion_r289266681",
        "createdAt" : "2019-05-31T06:27:52Z",
        "updatedAt" : "2019-06-14T16:45:37Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "61f9c39c-7baf-4647-8cfb-5015e2fc190c",
        "parentId" : "31b4602a-81ec-42f1-968e-c7f1d57933be",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "`actual` pushdown implies the other pushdowns in DSv2 is `not-actual`. Also, this seems to make `pushedFilters` in the explained plan inconsistent with the one Parquet library received. I believe this will cause lots of confusion.",
        "createdAt" : "2019-05-31T16:41:04Z",
        "updatedAt" : "2019-06-14T16:45:37Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "d776c153-74d3-478a-a034-75730a4647e5",
        "parentId" : "31b4602a-81ec-42f1-968e-c7f1d57933be",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "We need the filter push down here as per the discussion in https://github.com/apache/spark/pull/21696#pullrequestreview-133842990 .",
        "createdAt" : "2019-05-31T17:21:34Z",
        "updatedAt" : "2019-06-14T16:45:37Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "f658e9265ba741922fc96eec76038addcb6491a1",
    "line" : 146,
    "diffHunk" : "@@ -1,1 +144,148 @@        // is used here.\n        .flatMap(parquetFilters.createFilter)\n        .reduceOption(FilterApi.and)\n    } else {\n      None"
  }
]