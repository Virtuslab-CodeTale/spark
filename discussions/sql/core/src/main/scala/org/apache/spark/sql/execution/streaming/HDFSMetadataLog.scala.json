[
  {
    "id" : "73812a0a-b081-4450-bcbb-2ae0db40e309",
    "prId" : 31495,
    "prUrl" : "https://github.com/apache/spark/pull/31495#pullrequestreview-584935673",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "35b251b2-f3aa-42c9-98e6-75e6a2901e0b",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Usually, `possibleTargetBatchIds.length` should be 1?",
        "createdAt" : "2021-02-06T13:19:16Z",
        "updatedAt" : "2021-02-09T00:33:26Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "e25e66aa-f508-4a67-a140-0cf19c6527b8",
        "parentId" : "35b251b2-f3aa-42c9-98e6-75e6a2901e0b",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Yes usually the value will be 1, once purge is executed once via listing. I just don't set it to 1 because I don't want to make the logic be tight to MicroBatchExecution/ContinuousExecution. (Someone outside of Spark may leverage this as well.)\r\n\r\nThat's a magic number based on heuristic so I don't mind too much about the value (just should be greater than 1), but considering the cost of list vs exist, the ideal threshold wouldn't be just 1.",
        "createdAt" : "2021-02-07T02:54:52Z",
        "updatedAt" : "2021-02-09T00:33:26Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "03801f726a1443736dd7e404ba6a6ac1f8740c10",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +247,251 @@  override def purge(thresholdBatchId: Long): Unit = {\n    val possibleTargetBatchIds = (lastPurgedBatchId + 1 until thresholdBatchId)\n    if (possibleTargetBatchIds.length <= 3) {\n      // avoid using list if we only need to purge at most 3 elements\n      possibleTargetBatchIds.foreach { batchId =>"
  }
]