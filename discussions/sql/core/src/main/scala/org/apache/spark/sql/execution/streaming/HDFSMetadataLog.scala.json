[
  {
    "id" : "73812a0a-b081-4450-bcbb-2ae0db40e309",
    "prId" : 31495,
    "prUrl" : "https://github.com/apache/spark/pull/31495#pullrequestreview-584935673",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "35b251b2-f3aa-42c9-98e6-75e6a2901e0b",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Usually, `possibleTargetBatchIds.length` should be 1?",
        "createdAt" : "2021-02-06T13:19:16Z",
        "updatedAt" : "2021-02-09T00:33:26Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "e25e66aa-f508-4a67-a140-0cf19c6527b8",
        "parentId" : "35b251b2-f3aa-42c9-98e6-75e6a2901e0b",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Yes usually the value will be 1, once purge is executed once via listing. I just don't set it to 1 because I don't want to make the logic be tight to MicroBatchExecution/ContinuousExecution. (Someone outside of Spark may leverage this as well.)\r\n\r\nThat's a magic number based on heuristic so I don't mind too much about the value (just should be greater than 1), but considering the cost of list vs exist, the ideal threshold wouldn't be just 1.",
        "createdAt" : "2021-02-07T02:54:52Z",
        "updatedAt" : "2021-02-09T00:33:26Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "03801f726a1443736dd7e404ba6a6ac1f8740c10",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +247,251 @@  override def purge(thresholdBatchId: Long): Unit = {\n    val possibleTargetBatchIds = (lastPurgedBatchId + 1 until thresholdBatchId)\n    if (possibleTargetBatchIds.length <= 3) {\n      // avoid using list if we only need to purge at most 3 elements\n      possibleTargetBatchIds.foreach { batchId =>"
  },
  {
    "id" : "94c55e95-b253-4aa1-9b7c-e8261c453ef1",
    "prId" : 28904,
    "prUrl" : "https://github.com/apache/spark/pull/28904#pullrequestreview-440007750",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f2da08c6-3764-4d34-81a0-d85b65de9417",
        "parentId" : null,
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "nit: duplicated code with `get`.",
        "createdAt" : "2020-06-30T15:00:24Z",
        "updatedAt" : "2020-08-18T04:54:59Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "e16ebe4e530d3c44bb0ba39981c4ec2287c3589e",
    "line" : 50,
    "diffHunk" : "@@ -1,1 +138,142 @@   * properly and make sure the logic is not affected by failing in the middle.\n   */\n  def applyFnToBatchByStream[RET](batchId: Long)(fn: InputStream => RET): RET = {\n    val batchMetadataFile = batchIdToPath(batchId)\n    if (fileManager.exists(batchMetadataFile)) {"
  },
  {
    "id" : "99d79a11-14a3-4f1b-99e6-fe4701e92a50",
    "prId" : 28904,
    "prUrl" : "https://github.com/apache/spark/pull/28904#pullrequestreview-440007750",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "aefef996-3bcf-4942-b500-2d613fa1b610",
        "parentId" : null,
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "ditto, duplicate code with writeBatchToFile",
        "createdAt" : "2020-06-30T15:01:40Z",
        "updatedAt" : "2020-08-18T04:54:59Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "e16ebe4e530d3c44bb0ba39981c4ec2287c3589e",
    "line" : 83,
    "diffHunk" : "@@ -1,1 +170,174 @@   * valid behavior, we still need to prevent it from destroying the files.\n   */\n  def addNewBatchByStream(batchId: Long)(fn: OutputStream => Unit): Boolean = {\n    get(batchId).map(_ => false).getOrElse {\n      // Only write metadata when the batch has not yet been written"
  }
]