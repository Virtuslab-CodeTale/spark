[
  {
    "id" : "1c17df39-57b4-4175-ae5d-f5497120ea73",
    "prId" : 31874,
    "prUrl" : "https://github.com/apache/spark/pull/31874#pullrequestreview-616074405",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "86c68e31-87e0-4fb5-ae35-8c62041c77f0",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "For this case, we don't need to add a mutable state for `buildRowArrayTerm `?",
        "createdAt" : "2021-03-19T01:28:47Z",
        "updatedAt" : "2021-03-21T06:09:04Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "14a0d375-063d-418c-8f35-e0fa017a8baa",
        "parentId" : "86c68e31-87e0-4fb5-ae35-8c62041c77f0",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "It's already added inside `prepareBroadcast`",
        "createdAt" : "2021-03-19T03:55:00Z",
        "updatedAt" : "2021-03-21T06:09:04Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "0e408d23-2c2b-4732-ba93-fa326d70851f",
        "parentId" : "86c68e31-87e0-4fb5-ae35-8c62041c77f0",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "~Ah, my qestion is wrong. I meant, is `buildRowArrayTerm` referenced in this case?~ Ah, nvm, it looks fine.",
        "createdAt" : "2021-03-19T05:36:50Z",
        "updatedAt" : "2021-03-21T06:09:04Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "f2e846d873f1e062429a58ceb56cb1bf2aaab14d",
    "line" : 81,
    "diffHunk" : "@@ -1,1 +482,486 @@        // 2. build side is non-empty for LeftAnti join.\n        \"\"\n      }\n    } else {\n      val (buildRow, checkCondition, _) = getJoinCondition(ctx, input, streamed, broadcast)"
  },
  {
    "id" : "6262fcb3-8770-42f4-87bd-ff8f55ea5393",
    "prId" : 31821,
    "prUrl" : "https://github.com/apache/spark/pull/31821#pullrequestreview-611353302",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "08be6f5b-1ec3-46a2-9fad-de4296f0dfd7",
        "parentId" : null,
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "`BuildRight` related logic here is not changed.",
        "createdAt" : "2021-03-13T01:41:42Z",
        "updatedAt" : "2021-03-13T06:21:03Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      }
    ],
    "commit" : "f0ba910eb308af926992afe08b5ac8f504b2624f",
    "line" : 43,
    "diffHunk" : "@@ -1,1 +200,204 @@      exists: Boolean): RDD[InternalRow] = {\n    buildSide match {\n      case BuildRight =>\n        streamed.execute().mapPartitionsInternal { streamedIter =>\n          val buildRows = relation.value"
  },
  {
    "id" : "d9a4b053-17a5-41d0-9982-ca0212b6e2b6",
    "prId" : 31821,
    "prUrl" : "https://github.com/apache/spark/pull/31821#pullrequestreview-611354356",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5d702a47-b9b9-469e-959b-672e7da922ac",
        "parentId" : null,
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "`BuildLeft` with `condition.isEmpty` is the added new change in this PR.",
        "createdAt" : "2021-03-13T01:42:21Z",
        "updatedAt" : "2021-03-13T06:21:03Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      }
    ],
    "commit" : "f0ba910eb308af926992afe08b5ac8f504b2624f",
    "line" : 62,
    "diffHunk" : "@@ -1,1 +215,219 @@          }\n        }\n      case BuildLeft if condition.isEmpty =>\n        // If condition is empty, do not need to read rows from streamed side at all.\n        // Only need to know whether streamed side is empty or not."
  },
  {
    "id" : "bf145092-2e73-42e6-baef-3e858d2cad7e",
    "prId" : 31821,
    "prUrl" : "https://github.com/apache/spark/pull/31821#pullrequestreview-611356813",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0ad47521-de3b-4d5d-ae93-58735c42c184",
        "parentId" : null,
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "`BuildRight` related logic here is not changed.",
        "createdAt" : "2021-03-13T01:43:59Z",
        "updatedAt" : "2021-03-13T06:21:03Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      }
    ],
    "commit" : "f0ba910eb308af926992afe08b5ac8f504b2624f",
    "line" : 131,
    "diffHunk" : "@@ -1,1 +244,248 @@  private def existenceJoin(relation: Broadcast[Array[InternalRow]]): RDD[InternalRow] = {\n    buildSide match {\n      case BuildRight =>\n        streamed.execute().mapPartitionsInternal { streamedIter =>\n          val buildRows = relation.value"
  },
  {
    "id" : "972f8a7e-9eb7-4cb6-9aa6-ed7d39135558",
    "prId" : 31821,
    "prUrl" : "https://github.com/apache/spark/pull/31821#pullrequestreview-611570996",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "49d58d1b-e4b8-43a3-b503-e4f66298189b",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "`defaultJoin` now has outer joins only, so we cannot merge it with `outerJoin`?",
        "createdAt" : "2021-03-13T05:08:56Z",
        "updatedAt" : "2021-03-13T06:21:03Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "627f24d2-a016-4254-a7ce-14d70dedb339",
        "parentId" : "49d58d1b-e4b8-43a3-b503-e4f66298189b",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "@maropu - we can, but I feel the code is too complicated as one method when combining these two.",
        "createdAt" : "2021-03-13T06:01:54Z",
        "updatedAt" : "2021-03-13T06:21:30Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      }
    ],
    "commit" : "f0ba910eb308af926992afe08b5ac8f504b2624f",
    "line" : 190,
    "diffHunk" : "@@ -1,1 +284,288 @@   *   FullOuter\n   */\n  private def defaultJoin(relation: Broadcast[Array[InternalRow]]): RDD[InternalRow] = {\n    val streamRdd = streamed.execute()\n    val matchedBroadcastRows = getMatchedBroadcastRowsBitSet(streamRdd, relation)"
  },
  {
    "id" : "437ab8f7-f504-4b59-b156-ee38cccf99fd",
    "prId" : 31821,
    "prUrl" : "https://github.com/apache/spark/pull/31821#pullrequestreview-611570996",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "21d3781a-3f45-4d0c-8b62-6e465f93cd4b",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "we need this variable `streamRdd ` here? How about referencing `streamed` directly?",
        "createdAt" : "2021-03-13T05:15:03Z",
        "updatedAt" : "2021-03-13T06:21:03Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "2959c2d4-bf6a-4aa7-9257-49b5eff882b8",
        "parentId" : "21d3781a-3f45-4d0c-8b62-6e465f93cd4b",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "@maropu - then we need to call `streamed.execute()`twice in `defaultJoin()`. I feel it's bit weird and not sure if it can lead to any bug in the future. Normally during execution, we only call `child.execute()` once.",
        "createdAt" : "2021-03-13T06:06:41Z",
        "updatedAt" : "2021-03-13T06:21:30Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      }
    ],
    "commit" : "f0ba910eb308af926992afe08b5ac8f504b2624f",
    "line" : 206,
    "diffHunk" : "@@ -1,1 +340,344 @@   */\n  private def getMatchedBroadcastRowsBitSet(\n      streamRdd: RDD[InternalRow],\n      relation: Broadcast[Array[InternalRow]]): BitSet = {\n    val matchedBuildRows = streamRdd.mapPartitionsInternal { streamedIter =>"
  },
  {
    "id" : "8d5cdfc9-1963-4944-a806-6f49152b821d",
    "prId" : 31821,
    "prUrl" : "https://github.com/apache/spark/pull/31821#pullrequestreview-611865192",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "362fd518-7f7b-4efb-abbc-e39c9d471774",
        "parentId" : null,
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "@dongjoon-hyun - for original left anti join, here's one of code paths to handle it (`condition` is empty and `exists` is false).",
        "createdAt" : "2021-03-15T06:44:42Z",
        "updatedAt" : "2021-03-15T06:44:42Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      }
    ],
    "commit" : "f0ba910eb308af926992afe08b5ac8f504b2624f",
    "line" : 66,
    "diffHunk" : "@@ -1,1 +219,223 @@        // Only need to know whether streamed side is empty or not.\n        val streamExists = !streamed.execute().isEmpty()\n        if (streamExists == exists) {\n          sparkContext.makeRDD(relation.value)\n        } else {"
  },
  {
    "id" : "7aa438e0-ebdd-4633-9253-658c4927e1bc",
    "prId" : 31821,
    "prUrl" : "https://github.com/apache/spark/pull/31821#pullrequestreview-611865561",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4e6bfbf8-f8b6-4612-8d5e-e093ba75c893",
        "parentId" : null,
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "@dongjoon-hyun - for original left anti join, here's the other code path to handle it (`condition` is non-empty and `exists` is false). This is same as original code in `defaultJoin()`. But this PR adds the optimization for empty `condition` case.",
        "createdAt" : "2021-03-15T06:45:36Z",
        "updatedAt" : "2021-03-15T06:45:36Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      }
    ],
    "commit" : "f0ba910eb308af926992afe08b5ac8f504b2624f",
    "line" : 102,
    "diffHunk" : "@@ -1,1 +230,234 @@        val buildRows = relation.value\n        while (i < buildRows.length) {\n          if (matchedBroadcastRows.get(i) == exists) {\n            buf += buildRows(i).copy()\n          }"
  },
  {
    "id" : "a06e0248-4f07-458a-aa10-181b13f46021",
    "prId" : 31821,
    "prUrl" : "https://github.com/apache/spark/pull/31821#pullrequestreview-612648580",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3b6f7a46-b5e7-4ea3-9bca-cfe399bf89e8",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "shall we follow `Dataset.isEmpty` and call `streamed.executeTake(1).isEmpty`?",
        "createdAt" : "2021-03-15T07:48:32Z",
        "updatedAt" : "2021-03-15T07:48:32Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "3c747de8-669a-44ec-be6f-0a62d3502413",
        "parentId" : "3b6f7a46-b5e7-4ea3-9bca-cfe399bf89e8",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "@cloud-fan - sounds reasonable to me. Created https://github.com/apache/spark/pull/31845 .",
        "createdAt" : "2021-03-15T21:13:21Z",
        "updatedAt" : "2021-03-15T21:13:21Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      }
    ],
    "commit" : "f0ba910eb308af926992afe08b5ac8f504b2624f",
    "line" : 65,
    "diffHunk" : "@@ -1,1 +218,222 @@        // If condition is empty, do not need to read rows from streamed side at all.\n        // Only need to know whether streamed side is empty or not.\n        val streamExists = !streamed.execute().isEmpty()\n        if (streamExists == exists) {\n          sparkContext.makeRDD(relation.value)"
  },
  {
    "id" : "361ade66-5d25-4b59-97fc-86ab46ee5ea2",
    "prId" : 31736,
    "prUrl" : "https://github.com/apache/spark/pull/31736#pullrequestreview-604919961",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "34a37e28-195e-4ee4-b823-4383c63ca230",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "always true? Is there a trade-off between the overheads of uniqueness checks and result copys?",
        "createdAt" : "2021-03-04T13:49:26Z",
        "updatedAt" : "2021-03-09T06:31:40Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "e204e240-60aa-4b13-9d6d-f1413309fc4a",
        "parentId" : "34a37e28-195e-4ee4-b823-4383c63ca230",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "@maropu - in case of inner/cross broadcast nested loop join, one input row can potentially have multiple output rows, so I am following the [comment here](https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/WholeStageCodegenExec.scala#L346-L347) to set it true. btw sort merge join has [same behavior](https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/SortMergeJoinExec.scala#L570). I think if we want to improve it, I can do in another followup PR. I am not very familiar with this part and I can check closer later.",
        "createdAt" : "2021-03-05T07:32:22Z",
        "updatedAt" : "2021-03-09T06:31:40Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      },
      {
        "id" : "752e7239-8324-499b-9ad5-bb22fdaae039",
        "parentId" : "34a37e28-195e-4ee4-b823-4383c63ca230",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "This is not a hash join and we don't know the uniqueness. I think it needs to be always true.",
        "createdAt" : "2021-03-05T08:49:02Z",
        "updatedAt" : "2021-03-09T06:31:40Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "6f6d511e571a2109cba4dcd6b31fa3def4e44e21",
    "line" : 35,
    "diffHunk" : "@@ -1,1 +404,408 @@  }\n\n  override def needCopyResult: Boolean = true\n\n  override def doProduce(ctx: CodegenContext): String = {"
  },
  {
    "id" : "b6cdece9-5946-4e52-beb6-5831ea0ab12a",
    "prId" : 31477,
    "prUrl" : "https://github.com/apache/spark/pull/31477#pullrequestreview-584023056",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "da36066e-630d-4c9d-a09c-43af450c5780",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "If `condition` is empty, `numOutputRows` == `numMatchedPairs `? If so, could we only show this new metric in the web UI when `condition` defined?",
        "createdAt" : "2021-02-04T23:54:33Z",
        "updatedAt" : "2021-03-15T15:03:56Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "896e83aa-e262-4d5f-87df-e258b353e739",
        "parentId" : "da36066e-630d-4c9d-a09c-43af450c5780",
        "authorId" : "cdcefa9a-1880-4b7a-ae6a-ece33e944188",
        "body" : "If the condition is empty, then `numOutputRows == numMatchedPairs` only in the case of InnerJoin. It is not necessarily the case for other types of join, because numMatchedPairs is only measuring the records when there is a common key on both the build side and the stream side.  For instance, in the case of leftOuterJoin `numOutputRows  = numMatchedPairs  + (number of non matched pairs on the left side of the join)`.",
        "createdAt" : "2021-02-05T02:08:19Z",
        "updatedAt" : "2021-03-15T15:03:56Z",
        "lastEditedBy" : "cdcefa9a-1880-4b7a-ae6a-ece33e944188",
        "tags" : [
        ]
      },
      {
        "id" : "2a96cd2a-1fe4-4c76-b72f-f8662e33c959",
        "parentId" : "da36066e-630d-4c9d-a09c-43af450c5780",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Ah, I see. What is the merit of being able to see this number for a user? I read the PR description and, at first, I thought this PR mainly targets at the case where a `condition` is given.",
        "createdAt" : "2021-02-05T04:15:46Z",
        "updatedAt" : "2021-03-15T15:03:56Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "cc0fe8a0-f358-4112-bde8-45c9a2aef6fd",
        "parentId" : "da36066e-630d-4c9d-a09c-43af450c5780",
        "authorId" : "cdcefa9a-1880-4b7a-ae6a-ece33e944188",
        "body" : "- Yes, the metric is very useful especially when there is a condition. It helps the users to recognize skew. But even if there is no condition, it could help the users to see the number of matched rows. For instance, in case of outer joins users will be able to see just how many rows matched between the stream side and the build side without explicitly doing another inner join.   \r\n\r\n- I also faced an implementation problem when I originally tried to show the metric only when there is a condition. If there is a metric defined, but if the metric is not used in some cases (For example if we don't increment the metric when there is no condition), then the tests in SQLMetricsSuite will fail because the accumulatorIds corresponding the metrics would get messed up. The size of metric values in `val metricValues = statusStore.executionMetrics(executionId)` and the metrics in `node.metrics` would become inconsistent. \r\n\r\n- It might also confuse the users to see the metric in some cases and not see the metric in other cases. It would probably be best to show the metric everytime there is a join node in the UI to avoid any confusion.",
        "createdAt" : "2021-02-05T04:57:33Z",
        "updatedAt" : "2021-03-15T15:03:56Z",
        "lastEditedBy" : "cdcefa9a-1880-4b7a-ae6a-ece33e944188",
        "tags" : [
        ]
      }
    ],
    "commit" : "4c35275fa7c670cd18c16548f36cd6184ec155ff",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +94,98 @@\n  @transient private lazy val boundCondition: InternalRow => Boolean =\n    if (condition.isDefined) {\n      Predicate.create(condition.get, streamed.output ++ broadcast.output).eval _\n    } else {"
  },
  {
    "id" : "3c34a1b4-abc9-4a8e-82d1-79ab205957f1",
    "prId" : 31477,
    "prUrl" : "https://github.com/apache/spark/pull/31477#pullrequestreview-592830653",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9b89f08e-cdd9-4c59-b25d-cfacd311f612",
        "parentId" : null,
        "authorId" : "3d4870da-39a8-4406-b00d-131930d14cd8",
        "body" : "shouldn't this be `lazy`? otherwise `metrics` becomes non lazy...",
        "createdAt" : "2021-02-17T10:06:35Z",
        "updatedAt" : "2021-03-15T15:03:56Z",
        "lastEditedBy" : "3d4870da-39a8-4406-b00d-131930d14cd8",
        "tags" : [
        ]
      },
      {
        "id" : "c95995dd-d890-47e7-b598-52e649ccf0f3",
        "parentId" : "9b89f08e-cdd9-4c59-b25d-cfacd311f612",
        "authorId" : "cdcefa9a-1880-4b7a-ae6a-ece33e944188",
        "body" : "Done. Fixed in latest commit.",
        "createdAt" : "2021-02-18T00:44:02Z",
        "updatedAt" : "2021-03-15T15:03:56Z",
        "lastEditedBy" : "cdcefa9a-1880-4b7a-ae6a-ece33e944188",
        "tags" : [
        ]
      },
      {
        "id" : "47d06506-ce20-48c1-9c44-fcbf3868830a",
        "parentId" : "9b89f08e-cdd9-4c59-b25d-cfacd311f612",
        "authorId" : "cdcefa9a-1880-4b7a-ae6a-ece33e944188",
        "body" : "Actually making it lazy is causing a NullPointerException in SqlQuerySuite, ExistenceJoinSuite, OuterJoinSuite.  Somehow the sqlContext object in line 67 is turning out to be null. I am still trying to understand why that is the case. For now I reverted the change.\r\n`java.lang.NullPointerException\r\n\tat org.apache.spark.sql.execution.SparkPlan.sparkContext(SparkPlan.scala:67)\r\n\tat org.apache.spark.sql.execution.joins.BroadcastNestedLoopJoinExec.metrics$lzycompute(BroadcastNestedLoopJoinExec.scala:42)\r\n\tat org.apache.spark.sql.execution.joins.BroadcastNestedLoopJoinExec.metrics(BroadcastNestedLoopJoinExec.scala:41)\r\n\tat org.apache.spark.sql.execution.SparkPlan.longMetric(SparkPlan.scala:135)\r\n\tat org.apache.spark.sql.execution.joins.BroadcastNestedLoopJoinExec.numMatchedRows$lzycompute(BroadcastNestedLoopJoinExec.scala:93)",
        "createdAt" : "2021-02-18T03:47:34Z",
        "updatedAt" : "2021-03-15T15:03:56Z",
        "lastEditedBy" : "cdcefa9a-1880-4b7a-ae6a-ece33e944188",
        "tags" : [
        ]
      }
    ],
    "commit" : "4c35275fa7c670cd18c16548f36cd6184ec155ff",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +91,95 @@  }\n\n  private val numMatchedRows = longMetric(\"numMatchedRows\")\n\n  @transient private lazy val boundCondition: InternalRow => Boolean ="
  }
]