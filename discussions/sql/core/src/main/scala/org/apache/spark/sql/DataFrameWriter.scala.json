[
  {
    "id" : "e03fe885-55e3-43a2-b072-4775e22ce1ef",
    "prId" : 32546,
    "prUrl" : "https://github.com/apache/spark/pull/32546#pullrequestreview-664206352",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3841e95c-ea94-4811-877c-be9e0ed7fae8",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Here you didn't link generic options but you did for Parquet. What's diff?",
        "createdAt" : "2021-05-20T06:19:42Z",
        "updatedAt" : "2021-05-20T06:19:43Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "005f4271-b79e-405b-a8ea-f73d5637040e",
        "parentId" : "3841e95c-ea94-4811-877c-be9e0ed7fae8",
        "authorId" : "2d8ced12-ada1-43e1-9240-bf1c11a01e4c",
        "body" : "@HyukjinKwon , this is because the options I removed doesn't include generic options. (For example it has only one option, `compression` in this case)\r\n\r\nWould it be better to just add the link to the Generic options no matter what we removed ??",
        "createdAt" : "2021-05-20T08:33:54Z",
        "updatedAt" : "2021-05-20T08:34:43Z",
        "lastEditedBy" : "2d8ced12-ada1-43e1-9240-bf1c11a01e4c",
        "tags" : [
        ]
      },
      {
        "id" : "70a426e8-95be-4d92-96ce-2b14435c1a2d",
        "parentId" : "3841e95c-ea94-4811-877c-be9e0ed7fae8",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Either way is fine. The point is that we should keep it consistent",
        "createdAt" : "2021-05-20T10:23:44Z",
        "updatedAt" : "2021-05-20T10:23:44Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "1737a103-d03e-4969-93c2-b3eeff2fde62",
        "parentId" : "3841e95c-ea94-4811-877c-be9e0ed7fae8",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I would just add it for simplicity, and in case we have write generic options in the future",
        "createdAt" : "2021-05-20T10:24:25Z",
        "updatedAt" : "2021-05-20T10:24:26Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "043d3087c838c2f6439d798bdc8e889d6a728ff7",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +885,889 @@   * <a href=\n   *   \"https://spark.apache.org/docs/latest/sql-data-sources-orc.html#data-source-option\">\n   *   Data Source Option</a> in the version you use.\n   *\n   * @since 1.5.0"
  },
  {
    "id" : "c0bff79f-cda2-401b-bb5d-669471ac5333",
    "prId" : 29885,
    "prUrl" : "https://github.com/apache/spark/pull/29885#pullrequestreview-504107320",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7caa7ab2-879c-426d-b16e-840b7d7b64df",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Returned catalog is always a `TableCatalog`?",
        "createdAt" : "2020-10-06T23:31:35Z",
        "updatedAt" : "2020-10-06T23:36:41Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "2ea1f583-888e-4570-8201-ab77ad36e1e7",
        "parentId" : "7caa7ab2-879c-426d-b16e-840b7d7b64df",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Or do we need to check if `catalog` is `JDBCTableCatalog`?",
        "createdAt" : "2020-10-06T23:36:15Z",
        "updatedAt" : "2020-10-06T23:36:41Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "298d2dc7-9d82-43c2-b89c-9f14d6bcd560",
        "parentId" : "7caa7ab2-879c-426d-b16e-840b7d7b64df",
        "authorId" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "body" : "Seems to me that the catalog in `NonSessionCatalogAndIdentifier` is always a `TableCatalog`.",
        "createdAt" : "2020-10-07T17:25:43Z",
        "updatedAt" : "2020-10-07T17:25:43Z",
        "lastEditedBy" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "tags" : [
        ]
      }
    ],
    "commit" : "9ff43135256f7c3ad3922f6772186634d69633a1",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +820,824 @@    session.sessionState.sqlParser.parseMultipartIdentifier(table) match {\n      case nameParts @ NonSessionCatalogAndIdentifier(catalog, tableIdentifier) =>\n        saveAsTable(catalog.asTableCatalog, tableIdentifier, nameParts)\n\n      case _ =>"
  },
  {
    "id" : "d75fd2ee-c4ff-43e8-a545-4715c1e88230",
    "prId" : 29543,
    "prUrl" : "https://github.com/apache/spark/pull/29543#pullrequestreview-485298760",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "abbe2e61-b70b-4773-a2ef-84382efaa391",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "The `path` here is a String, do we really need to check `path.nonEmpty`?",
        "createdAt" : "2020-09-09T07:51:43Z",
        "updatedAt" : "2020-09-09T07:51:44Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "13c4cf30-0892-4cfd-bde1-a02ae2cda3e6",
        "parentId" : "abbe2e61-b70b-4773-a2ef-84382efaa391",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "Good catch. I think we need to remove the check. The output is more confusing since `path` option is also set:\r\n```scala\r\nscala> Seq(1).toDF.write.option(\"path\", \"/tmp/path1\").parquet(\"\")\r\njava.lang.IllegalArgumentException: Can not create a Path from an empty string\r\n  at org.apache.hadoop.fs.Path.checkPathArg(Path.java:168)\r\n```\r\n\r\nI will create a PR for this.",
        "createdAt" : "2020-09-09T17:46:45Z",
        "updatedAt" : "2020-09-09T17:46:46Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      },
      {
        "id" : "16fcb7dc-55d5-453a-924b-074af2cbb5f5",
        "parentId" : "abbe2e61-b70b-4773-a2ef-84382efaa391",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "Created https://github.com/apache/spark/pull/29697",
        "createdAt" : "2020-09-09T18:43:40Z",
        "updatedAt" : "2020-09-09T18:43:40Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "261b6097907097a77b3c85606510e19c85a648d0",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +287,291 @@  def save(path: String): Unit = {\n    if (!df.sparkSession.sessionState.conf.legacyPathOptionBehavior &&\n        extraOptions.contains(\"path\") && path.nonEmpty) {\n      throw new AnalysisException(\"There is a 'path' option set and save() is called with a path \" +\n        \"parameter. Either remove the path option, or call save() without the parameter. \" +"
  },
  {
    "id" : "014edaa5-65ad-43c6-be67-fd948130f09c",
    "prId" : 29191,
    "prUrl" : "https://github.com/apache/spark/pull/29191#pullrequestreview-453502481",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a912c5eb-8676-4329-b8d7-ae71af27bb98",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "I missed that `dsOptions.asCaseSensitiveMap()` is used later at line 303, `extraOptions.originalMap` will preserve the key case-sensitivity.",
        "createdAt" : "2020-07-22T16:47:26Z",
        "updatedAt" : "2020-07-23T06:10:34Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "81ac6c614c16e8d3c43ff14888216a797df04275",
    "line" : 66,
    "diffHunk" : "@@ -1,1 +308,312 @@        provider, df.sparkSession.sessionState.conf)\n      val options = sessionOptions.filterKeys(!extraOptions.contains(_)) ++ extraOptions.toMap\n      val dsOptions = new CaseInsensitiveStringMap(options.asJava)\n\n      def getTable: Table = {"
  },
  {
    "id" : "ee83ba9c-0065-4239-9e5b-1c6c701e1f57",
    "prId" : 27992,
    "prUrl" : "https://github.com/apache/spark/pull/27992#pullrequestreview-384230569",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2b434fee-2704-42fc-b124-b2cbdcbaf609",
        "parentId" : null,
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "Not related to this PR, but do we plan to support buckets for datasource V2?",
        "createdAt" : "2020-03-30T20:34:20Z",
        "updatedAt" : "2020-03-31T06:29:04Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      },
      {
        "id" : "27d1b391-437e-4128-a91d-471c59e0ede4",
        "parentId" : "2b434fee-2704-42fc-b124-b2cbdcbaf609",
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "Yes. I have a design doc I can share with you.",
        "createdAt" : "2020-03-30T20:39:21Z",
        "updatedAt" : "2020-03-31T06:29:04Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      },
      {
        "id" : "bb0c37c5-f2d1-4e53-bc68-baddc431667e",
        "parentId" : "2b434fee-2704-42fc-b124-b2cbdcbaf609",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "Great, thanks!",
        "createdAt" : "2020-03-30T21:06:40Z",
        "updatedAt" : "2020-03-31T06:29:04Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "8d948739db7d247a80899b91ab13e5a38018cc21",
    "line" : 72,
    "diffHunk" : "@@ -1,1 +578,582 @@          df.queryExecution.analyzed,\n          partitioningAsV2,\n          None,\n          Map.empty,\n          Some(source),"
  },
  {
    "id" : "92f5db9c-9168-4171-8695-baecd68387b5",
    "prId" : 27992,
    "prUrl" : "https://github.com/apache/spark/pull/27992#pullrequestreview-384432788",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "25c83da1-1ce6-4f1f-aa0c-6e075a7fff0a",
        "parentId" : null,
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "Should this use the constants in `TableCatalog`?",
        "createdAt" : "2020-03-30T21:26:56Z",
        "updatedAt" : "2020-03-31T06:29:04Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      },
      {
        "id" : "6572dde5-8c05-4bfa-a18b-ad8238262872",
        "parentId" : "25c83da1-1ce6-4f1f-aa0c-6e075a7fff0a",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "the table location constant is \"location\", which is different from the \"path\" in `DataFrameReader/Writer`. I'll use the constant for the comment.",
        "createdAt" : "2020-03-31T06:26:22Z",
        "updatedAt" : "2020-03-31T06:29:04Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "8d948739db7d247a80899b91ab13e5a38018cc21",
    "line" : 76,
    "diffHunk" : "@@ -1,1 +582,586 @@          Some(source),\n          Map.empty,\n          extraOptions.get(\"path\"),\n          extraOptions.get(TableCatalog.PROP_COMMENT),\n          extraOptions.toMap,"
  },
  {
    "id" : "e0d965bc-1ee4-44eb-bf1a-c3c2e0fc945f",
    "prId" : 26957,
    "prUrl" : "https://github.com/apache/spark/pull/26957#pullrequestreview-347734720",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "24a0f749-ea8c-45bb-8207-07003297bb29",
        "parentId" : null,
        "authorId" : "8fec0990-98ca-4639-aa0d-4d79de603820",
        "body" : "A little curious why `Overwrite` doesn't need to create a `DataSourceV2Relation`?",
        "createdAt" : "2020-01-14T02:30:58Z",
        "updatedAt" : "2020-01-24T17:49:38Z",
        "lastEditedBy" : "8fec0990-98ca-4639-aa0d-4d79de603820",
        "tags" : [
        ]
      },
      {
        "id" : "c60babb8-0c19-47f3-9fd7-7d635bfcee91",
        "parentId" : "24a0f749-ea8c-45bb-8207-07003297bb29",
        "authorId" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "body" : "It drops and recreates a table. It's a DDL operation instead of DML",
        "createdAt" : "2020-01-14T21:53:15Z",
        "updatedAt" : "2020-01-24T17:49:38Z",
        "lastEditedBy" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "tags" : [
        ]
      },
      {
        "id" : "2e5f62de-a796-4264-9402-9a93eeca8fc1",
        "parentId" : "24a0f749-ea8c-45bb-8207-07003297bb29",
        "authorId" : "8fec0990-98ca-4639-aa0d-4d79de603820",
        "body" : "Still probably a dumb question. Why does DDL/DML affects how we generate the query plan? I'm asking this because in the `save()` function for the DataFrameWriter, we do generate a `DataSourceV2Relation` for `Overwrite ` mode. I'm curious about why there is such a difference here.",
        "createdAt" : "2020-01-15T02:40:39Z",
        "updatedAt" : "2020-01-24T17:49:38Z",
        "lastEditedBy" : "8fec0990-98ca-4639-aa0d-4d79de603820",
        "tags" : [
        ]
      },
      {
        "id" : "e212a938-73d3-4a58-a4e4-ab0d2448548c",
        "parentId" : "24a0f749-ea8c-45bb-8207-07003297bb29",
        "authorId" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "body" : "in save, we don't update table information in a catalog. saveAsTable updates the catalog, therefore is doing different work. So we need to do a separate operation",
        "createdAt" : "2020-01-24T03:18:15Z",
        "updatedAt" : "2020-01-24T17:49:38Z",
        "lastEditedBy" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "tags" : [
        ]
      }
    ],
    "commit" : "df29683d38260cb7f69adfccdc7fab2d315e663e",
    "line" : 69,
    "diffHunk" : "@@ -1,1 +562,566 @@        AppendData.byName(v2Relation, df.logicalPlan, extraOptions.toMap)\n\n      case (SaveMode.Overwrite, _) =>\n        ReplaceTableAsSelect(\n          catalog,"
  },
  {
    "id" : "121dd16b-f488-4309-9c9a-3b4fea529fcf",
    "prId" : 26913,
    "prUrl" : "https://github.com/apache/spark/pull/26913#pullrequestreview-334363446",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c8a48c22-9c81-43a8-98a3-6ffc067c73d2",
        "parentId" : null,
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "CTAS works for `SaveMode.ErrorIfExists`, but doesn't this also need to handle `SaveMode.Ignore`? It should check if the table exists and not run anything if it does.",
        "createdAt" : "2019-12-19T01:28:53Z",
        "updatedAt" : "2020-01-09T15:12:09Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      },
      {
        "id" : "e4c4f066-d933-4410-ab1d-3f3cafb842b7",
        "parentId" : "c8a48c22-9c81-43a8-98a3-6ffc067c73d2",
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "Nevermind, I see it sets `ignoreIfExists` correctly.",
        "createdAt" : "2019-12-19T01:29:37Z",
        "updatedAt" : "2020-01-09T15:12:09Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      }
    ],
    "commit" : "963133edc5004a465347fd7b1c3edfffd6ff5a8d",
    "line" : 76,
    "diffHunk" : "@@ -1,1 +307,311 @@\n              runCommand(df.sparkSession, \"save\") {\n                CreateTableAsSelect(\n                  catalog,\n                  ident,"
  },
  {
    "id" : "be8c7abc-0601-4b18-86b3-16dd89e87f57",
    "prId" : 26913,
    "prUrl" : "https://github.com/apache/spark/pull/26913#pullrequestreview-335981196",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d1fff517-dcc7-45c0-a78a-8f3ff27d9f37",
        "parentId" : null,
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "Could this be simplified to just call `saveAsTable` (or an equivalent internal method with the right args) when `SupportsCatalogOptions` is implemented? Or are the behaviors different?",
        "createdAt" : "2019-12-23T17:22:35Z",
        "updatedAt" : "2020-01-09T15:12:09Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      },
      {
        "id" : "08160ac8-f23d-4648-bc93-c499c663cee9",
        "parentId" : "d1fff517-dcc7-45c0-a78a-8f3ff27d9f37",
        "authorId" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "body" : "drop table gets a bit weird in overwrite mode. ",
        "createdAt" : "2019-12-23T19:02:56Z",
        "updatedAt" : "2020-01-09T15:12:09Z",
        "lastEditedBy" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "tags" : [
        ]
      },
      {
        "id" : "e0d11991-e74e-4d0f-9246-c61e3e041563",
        "parentId" : "d1fff517-dcc7-45c0-a78a-8f3ff27d9f37",
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "You're right, `saveAsTable` uses RTAS instead of truncate.",
        "createdAt" : "2019-12-23T19:15:16Z",
        "updatedAt" : "2020-01-09T15:12:09Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      }
    ],
    "commit" : "963133edc5004a465347fd7b1c3edfffd6ff5a8d",
    "line" : 27,
    "diffHunk" : "@@ -1,1 +262,266 @@        case SaveMode.Append | SaveMode.Overwrite =>\n          val table = provider match {\n            case supportsExtract: SupportsCatalogOptions =>\n              val ident = supportsExtract.extractIdentifier(dsOptions)\n              val sessionState = df.sparkSession.sessionState"
  },
  {
    "id" : "8256ba50-c342-49bd-951b-30ca80a31e34",
    "prId" : 26853,
    "prUrl" : "https://github.com/apache/spark/pull/26853#pullrequestreview-330871543",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e4a7d9d1-4813-4f00-9b71-2ef778bd4727",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "This looks much better (BTW, this is not a replacement from `CatalogObjectIdentifier` to `CatalogAndIdentifer`)",
        "createdAt" : "2019-12-11T18:37:47Z",
        "updatedAt" : "2019-12-11T21:38:09Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "cd3eaf28-b502-4efd-8a1e-ec13f5cf5007",
        "parentId" : "e4a7d9d1-4813-4f00-9b71-2ef778bd4727",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : " I like this change. Could you update the PR description more precisely?",
        "createdAt" : "2019-12-11T18:38:34Z",
        "updatedAt" : "2019-12-11T21:38:09Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "ec0484a4-c242-4c27-b966-003fb775ece4",
        "parentId" : "e4a7d9d1-4813-4f00-9b71-2ef778bd4727",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "Updated. Thanks!",
        "createdAt" : "2019-12-11T21:58:54Z",
        "updatedAt" : "2019-12-11T21:58:54Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "a1fd1c13fdf221b41450403af0e508784f43212e",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +358,362 @@\n    session.sessionState.sqlParser.parseMultipartIdentifier(tableName) match {\n      case NonSessionCatalogAndIdentifier(catalog, ident) =>\n        insertInto(catalog, ident)\n"
  },
  {
    "id" : "d230479e-f7c4-4bb8-b230-fc073c53a085",
    "prId" : 26297,
    "prUrl" : "https://github.com/apache/spark/pull/26297#pullrequestreview-309390204",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1b6e15b2-50ff-4a8d-ab23-2096ad746404",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "We can introduce a general API for it, but I don't think it's worth. It only applies to file source + DataFrameWriter, not file source tables or DataFrameWriterV2.",
        "createdAt" : "2019-10-30T17:30:11Z",
        "updatedAt" : "2019-11-27T16:28:24Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "95b62b6dd5a0c58b001f3faddd8527858bbc792c",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +264,268 @@        // the schema of input query and the user-specified partitioning to `getTable`. If the\n        // query schema is not compatible with the existing data, the write can still success but\n        // following reads would fail.\n        case p: FileDataSourceV2 =>\n          import org.apache.spark.sql.connector.catalog.CatalogV2Implicits._"
  },
  {
    "id" : "aa344fe7-79d5-4f9d-8e11-16b177b06275",
    "prId" : 26120,
    "prUrl" : "https://github.com/apache/spark/pull/26120#pullrequestreview-302986293",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a1cd1e35-0e0f-460e-a142-d6a206044021",
        "parentId" : null,
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "This may not be correct if the current catalog is v2 session catalog that doesn't delegate to the v1 session catalog? If you look at the previous behavior, it's always using v1 session catalog.",
        "createdAt" : "2019-10-16T20:01:48Z",
        "updatedAt" : "2019-10-18T09:21:58Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      },
      {
        "id" : "e2467248-d1be-40c6-80a3-6702c95200d7",
        "parentId" : "a1cd1e35-0e0f-460e-a142-d6a206044021",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "It's a known problem that if the v2 session catalog doesn't delegate to v1 session catalog, many things can be broken.\r\n\r\nI think the previous version was wrong. It always use the default v2 session catalog even if users set a custom v2 session catalog.",
        "createdAt" : "2019-10-17T03:31:19Z",
        "updatedAt" : "2019-10-18T09:21:58Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "b93c5e362e6492f86723f334cc891485b370ec4f",
    "line" : 46,
    "diffHunk" : "@@ -1,1 +493,497 @@      case CatalogObjectIdentifier(catalog, ident)\n        if isSessionCatalog(catalog) && canUseV2 && ident.namespace().length <= 1 =>\n        saveAsTable(catalog.asTableCatalog, ident)\n\n      case AsTableIdentifier(tableIdentifier) =>"
  },
  {
    "id" : "67fa7094-8197-4e0e-a582-5d90a490e440",
    "prId" : 26094,
    "prUrl" : "https://github.com/apache/spark/pull/26094#pullrequestreview-300834043",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4506803c-7cb9-4504-96bb-294bda4ba6ab",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "I think the `\"default\"` mode should be mentioned in the list: https://github.com/apache/spark/pull/26094/files#diff-94fbd986b04087223f53697d4b6cab24R77-R81",
        "createdAt" : "2019-10-11T17:44:56Z",
        "updatedAt" : "2019-10-14T16:27:53Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "b752271e848dc4562fdfe1df242d173a70a8dca0",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +88,92 @@      case \"append\" => mode(SaveMode.Append)\n      case \"ignore\" => mode(SaveMode.Ignore)\n      case \"error\" | \"errorifexists\" | \"default\" => mode(SaveMode.ErrorIfExists)\n      case _ => throw new IllegalArgumentException(s\"Unknown save mode: $saveMode. Accepted \" +\n        \"save modes are 'overwrite', 'append', 'ignore', 'error', 'errorifexists', 'default'.\")"
  },
  {
    "id" : "5093dda8-e9b4-4529-a9ec-ddf5debc4c38",
    "prId" : 25876,
    "prUrl" : "https://github.com/apache/spark/pull/25876#pullrequestreview-293278794",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c339b64f-5e70-453e-b675-dd5ba7881a57",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "A note to future readers: this is the old behavior, that non-overwrite mode means append. This is due to the bad design of `DataFrameWriter`: we only need to know overwrite or not when calling `insert`, but `DataFrameWriter` gives you a save mode. Since the default save mode is `ErrorIfExists`, treating non-overwrite mode as append is a reasonable compromise.\r\n\r\nNote that, we don't have this problem in the new `DataFrameWriterV2`.",
        "createdAt" : "2019-09-24T17:09:25Z",
        "updatedAt" : "2019-09-25T19:16:06Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "e65e1083-dc84-45a0-b4bb-9afd3041df04",
        "parentId" : "c339b64f-5e70-453e-b675-dd5ba7881a57",
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "Looks like the previous version used this:\r\n\r\n```\r\n      InsertIntoTable(\r\n        table = UnresolvedRelation(tableIdent),\r\n        partition = Map.empty[String, Option[String]],\r\n        query = df.logicalPlan,\r\n        overwrite = mode == SaveMode.Overwrite, // << Either overwrite or append\r\n        ifPartitionNotExists = false)\r\n```\r\n\r\nSo I agree that this is using the same behavior that v1 did.",
        "createdAt" : "2019-09-25T18:29:30Z",
        "updatedAt" : "2019-09-25T19:16:06Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      }
    ],
    "commit" : "3e054c75cd16ab9aa028e2914f525ce2f56862cc",
    "line" : 44,
    "diffHunk" : "@@ -1,1 +385,389 @@\n    val command = mode match {\n      case SaveMode.Append | SaveMode.ErrorIfExists | SaveMode.Ignore =>\n        AppendData.byPosition(table, df.logicalPlan, extraOptions.toMap)\n"
  },
  {
    "id" : "730e9bd5-1953-4abe-bfd2-fd3b0aa62ccc",
    "prId" : 25833,
    "prUrl" : "https://github.com/apache/spark/pull/25833#pullrequestreview-290241820",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8599359e-5786-4283-a166-a378c1c1fab7",
        "parentId" : null,
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "Why not use `AppendData` when mode is append?",
        "createdAt" : "2019-09-18T22:45:11Z",
        "updatedAt" : "2019-09-18T22:45:12Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      }
    ],
    "commit" : "e48a5c490334a33f3edade649d1ed8cefaf672d0",
    "line" : 60,
    "diffHunk" : "@@ -1,1 +281,285 @@                orCreate = true)      // Create the table if it doesn't exist\n\n            case (other, _) =>\n              CreateTableAsSelect(\n                canCreateTable,"
  },
  {
    "id" : "be249656-b17b-4aa8-9bf4-3a2599ce54c7",
    "prId" : 25767,
    "prUrl" : "https://github.com/apache/spark/pull/25767#pullrequestreview-290990171",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cbd45160-3bcc-4846-a6d8-953c1a0ab22c",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "good catch! Even if the format is a `TableProvider`, we may still fall back to v1. It's better to check the partition when we are really going to do a v2 write.",
        "createdAt" : "2019-09-20T07:09:27Z",
        "updatedAt" : "2019-09-20T07:09:27Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "f13a8ca4-2748-469f-97cc-556717522d2b",
        "parentId" : "cbd45160-3bcc-4846-a6d8-953c1a0ab22c",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "BTW, technically we only need to assert no partition columns for append and overwrite. Since the v2 write here only supports append and overwrite, we can revisit it later.",
        "createdAt" : "2019-09-20T07:14:52Z",
        "updatedAt" : "2019-09-20T07:14:52Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "cb396765335d92ed43584eb69d3fddb4dff02a9b",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +263,267 @@      provider.getTable(dsOptions) match {\n        case table: SupportsWrite if table.supports(BATCH_WRITE) =>\n          if (partitioningColumns.nonEmpty) {\n            throw new AnalysisException(\"Cannot write data to TableProvider implementation \" +\n              \"if partition columns are specified.\")"
  },
  {
    "id" : "b8acf7d3-af28-41aa-a25a-dfd5a0d5c68b",
    "prId" : 25699,
    "prUrl" : "https://github.com/apache/spark/pull/25699#pullrequestreview-285271287",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b3ae5a3e-05e8-410c-94cf-64f011e9d5b4",
        "parentId" : null,
        "authorId" : "9c99fec9-cc01-4c44-8bb8-60e23ac802b1",
        "body" : "@maropu i change variable-length params",
        "createdAt" : "2019-09-09T04:07:02Z",
        "updatedAt" : "2019-09-12T00:57:51Z",
        "lastEditedBy" : "9c99fec9-cc01-4c44-8bb8-60e23ac802b1",
        "tags" : [
        ]
      }
    ],
    "commit" : "d9dcf16c4fe3f573cbbd6fcc869b0a541d11955c",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +353,357 @@   * @since 3.0\n   */\n  def insertInto(tableName: String, partionInfo: String*): Unit = {\n    insertInto(tableName, Option(partionInfo))\n  }"
  },
  {
    "id" : "925d430d-afda-4482-ac3e-610669b58e31",
    "prId" : 25699,
    "prUrl" : "https://github.com/apache/spark/pull/25699#pullrequestreview-295326146",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "476e3f27-43a2-4af1-8156-6c2a97f43d72",
        "parentId" : null,
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Personally, I'd prefer us to expose the Map rather than doing string parsing. Unless is this matching an API in another system which is build like this?\r\nThis is more of my personal preference, but it's easy for us to make mistakes parsing strings so when I don't have to it's nice to avoid.",
        "createdAt" : "2019-10-01T01:08:32Z",
        "updatedAt" : "2019-10-01T01:10:07Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "d9dcf16c4fe3f573cbbd6fcc869b0a541d11955c",
    "line" : 31,
    "diffHunk" : "@@ -1,1 +378,382 @@    var parition = Map[String, Option[String]]()\n    if (partionInfo.isDefined) {\n      val res = partionInfo.get.foreach(partion => {\n        val partionKey = partion.replaceAll(\"'\", \"\").split(\"=\")(0)\n        val partionValue = partion.replaceAll(\"'\", \"\").split(\"=\")(1)"
  },
  {
    "id" : "c2c915e9-3e20-4487-9c15-aa0ebea21d58",
    "prId" : 25465,
    "prUrl" : "https://github.com/apache/spark/pull/25465#pullrequestreview-279120049",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "360d6568-1d3d-4374-ab39-12cc553b25f8",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "This is better than before, which use configs to disable file source v2 write: https://github.com/apache/spark/pull/25465/files#diff-9a6b543db706f1a90f790783d6930a13L1605\r\n\r\nconfigs can be changed, and we shouldn't allow users to use the broken file source v2 write.",
        "createdAt" : "2019-08-15T16:27:03Z",
        "updatedAt" : "2019-08-27T05:48:29Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "a98e09a8-2e6c-446f-bc3c-bb353cae8ce5",
        "parentId" : "360d6568-1d3d-4374-ab39-12cc553b25f8",
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "Why not add this to `lookupDataSourceV2` instead? Doesn't this logic apply to SQL as well?",
        "createdAt" : "2019-08-21T21:25:09Z",
        "updatedAt" : "2019-08-27T05:48:29Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      },
      {
        "id" : "956acc5b-184e-4125-af39-f1f2b12d3ed9",
        "parentId" : "360d6568-1d3d-4374-ab39-12cc553b25f8",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "This doesn't apply to `DataFrameReader`. We can still read file source v2 with `DataFrameReader` and I don't want to break it.",
        "createdAt" : "2019-08-22T02:14:13Z",
        "updatedAt" : "2019-08-27T05:48:29Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "6cf2cacb-3c40-4e9a-bd90-95021fdc4cf7",
        "parentId" : "360d6568-1d3d-4374-ab39-12cc553b25f8",
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "If the file sources don't support the schema from the metastore, then the read path should not be used. While it doesn't fail while inferring a schema, the source will read using the inferred schema instead of the metastore schema and that's a bug.\r\n\r\nFor example, if I create a table with 3 columns and drop one, the data and the inferred schema will have the original 3 columns. But the metastore table will have just 2 columns. Reading with v2 would bring back a column that is deleted.\r\n\r\nUntil the file sources are fixed, they should be disabled in all cases.",
        "createdAt" : "2019-08-22T15:55:13Z",
        "updatedAt" : "2019-08-27T05:48:29Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      },
      {
        "id" : "6ce3ad88-e653-4784-b413-598872cfd211",
        "parentId" : "360d6568-1d3d-4374-ab39-12cc553b25f8",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I'm talking about `spark.read.parquet(path)`. It's nothing about the metastore, and the behavior is well defined in Spark 2.x:\r\n1. if no schema is specified, e.g. `spark.read.parquet(path)`, then the schema will be inferred.\r\n2. if schema is specified by users, e.g. `spark.read.schema(...).parquet(path)`, then schema inference will be skipped and Spark fails at runtime if the specified schema doesn't match the actual file schema.\r\n\r\nI think what you were talking about is `CREATE TABLE ... USING parquet`. This is a different case and currently file source v2 is already disabled for it.",
        "createdAt" : "2019-08-23T02:29:21Z",
        "updatedAt" : "2019-08-27T05:48:29Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "54fb45ae-c458-408a-997b-1d67437db493",
        "parentId" : "360d6568-1d3d-4374-ab39-12cc553b25f8",
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "I think you're right that this is not affected by the metastore problem because this is reading a path-based table and not a metastore table.",
        "createdAt" : "2019-08-23T17:09:43Z",
        "updatedAt" : "2019-08-27T05:48:29Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      }
    ],
    "commit" : "d9f664438417ee8f4bb952d41f53c8c0b0e8d500",
    "line" : 89,
    "diffHunk" : "@@ -1,1 +835,839 @@    DataSource.lookupDataSourceV2(source, df.sparkSession.sessionState.conf) match {\n      // TODO(SPARK-28396): File source v2 write path is currently broken.\n      case Some(_: FileDataSourceV2) => None\n      case other => other\n    }"
  },
  {
    "id" : "58a2bbd8-2f65-48c6-9bc0-8ee0c9a82e6b",
    "prId" : 25402,
    "prUrl" : "https://github.com/apache/spark/pull/25402#pullrequestreview-275250309",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "996fc0c3-752c-4cc8-b32b-e62a0e207668",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "the `provider` here may not be the actual table provider, as `saveAsTable` can write to an existing table. Maybe we should always use v2 session catalog?\r\n ",
        "createdAt" : "2019-08-14T11:12:48Z",
        "updatedAt" : "2019-08-14T16:08:07Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "6180e1ef-b47b-4380-85c4-6e2d5bb565a7",
        "parentId" : "996fc0c3-752c-4cc8-b32b-e62a0e207668",
        "authorId" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "body" : "That works for me. Since the V2 code path will fallback to the V1 code path if it sees an UnresolvedTable",
        "createdAt" : "2019-08-14T16:12:39Z",
        "updatedAt" : "2019-08-14T16:13:08Z",
        "lastEditedBy" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "tags" : [
        ]
      },
      {
        "id" : "465a87f5-40a2-41e3-abdd-bb6edb7663be",
        "parentId" : "996fc0c3-752c-4cc8-b32b-e62a0e207668",
        "authorId" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "body" : "hmm. actually that causes issues if the table doesn't exist. Maybe we should use the statements instead of the logical plans?",
        "createdAt" : "2019-08-14T16:16:05Z",
        "updatedAt" : "2019-08-14T16:16:06Z",
        "lastEditedBy" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "tags" : [
        ]
      },
      {
        "id" : "3c851a04-b4bb-4866-a5eb-261f425f4231",
        "parentId" : "996fc0c3-752c-4cc8-b32b-e62a0e207668",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "+1 on using statements.",
        "createdAt" : "2019-08-15T03:31:27Z",
        "updatedAt" : "2019-08-15T03:31:27Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "afa1c514-6111-4919-90af-d73ad6a17e7e",
        "parentId" : "996fc0c3-752c-4cc8-b32b-e62a0e207668",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "let's do it in a followup.",
        "createdAt" : "2019-08-15T03:46:31Z",
        "updatedAt" : "2019-08-15T03:46:31Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "a70e72676da6442a45e3a358c734b6f161c615d0",
    "line" : 36,
    "diffHunk" : "@@ -1,1 +490,494 @@\n    val session = df.sparkSession\n    val provider = DataSource.lookupDataSource(source, session.sessionState.conf)\n    val canUseV2 = canUseV2Source(session, provider)\n    val sessionCatalogOpt = session.sessionState.analyzer.sessionCatalog"
  },
  {
    "id" : "208c1819-892d-48c9-b15e-baebab146563",
    "prId" : 25330,
    "prUrl" : "https://github.com/apache/spark/pull/25330#pullrequestreview-270505849",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "227f9015-ea69-413c-93d6-0ac5627694f5",
        "parentId" : null,
        "authorId" : "f5b3f57a-75a6-496c-af94-c32580ada13a",
        "body" : "Add unit test to verify byName not byPosition",
        "createdAt" : "2019-08-04T16:47:39Z",
        "updatedAt" : "2019-08-09T01:11:53Z",
        "lastEditedBy" : "f5b3f57a-75a6-496c-af94-c32580ada13a",
        "tags" : [
        ]
      }
    ],
    "commit" : "50f1eef93f84b7a51c0968420cac17a87fb22352",
    "line" : 90,
    "diffHunk" : "@@ -1,1 +527,531 @@    val command = (mode, tableOpt) match {\n      case (SaveMode.Append, Some(table)) =>\n        AppendData.byName(DataSourceV2Relation.create(table), df.logicalPlan)\n\n      case (SaveMode.Overwrite, _) =>"
  },
  {
    "id" : "75fe6910-96fb-4c06-ae7c-6fab970b8558",
    "prId" : 25330,
    "prUrl" : "https://github.com/apache/spark/pull/25330#pullrequestreview-271772728",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cd46ecad-a694-4383-ba65-b3b547d4400d",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "If we treat `saveAsTable` as one atomic operation, I think AppendMode means \"create or append table\". Since \"create or append table\" is not a standard SQL commannd, maybe it's fine to ignore this race condition.",
        "createdAt" : "2019-08-07T07:25:18Z",
        "updatedAt" : "2019-08-09T01:11:53Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "50f1eef93f84b7a51c0968420cac17a87fb22352",
    "line" : 105,
    "diffHunk" : "@@ -1,1 +542,546 @@        // We have a potential race condition here in AppendMode, if the table suddenly gets\n        // created between our existence check and physical execution, but this can't be helped\n        // in any case.\n        CreateTableAsSelect(\n          catalog,"
  },
  {
    "id" : "5eaca155-beb8-4dc1-a827-22503000f4f9",
    "prId" : 25330,
    "prUrl" : "https://github.com/apache/spark/pull/25330#pullrequestreview-272275571",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "56ce278c-1cb6-4e85-a630-368adb034706",
        "parentId" : null,
        "authorId" : "f5b3f57a-75a6-496c-af94-c32580ada13a",
        "body" : "Nit: the local variable is used only once.",
        "createdAt" : "2019-08-07T22:14:44Z",
        "updatedAt" : "2019-08-09T01:11:53Z",
        "lastEditedBy" : "f5b3f57a-75a6-496c-af94-c32580ada13a",
        "tags" : [
        ]
      },
      {
        "id" : "a20c824a-4ef5-43df-a78f-1b347a31bfd7",
        "parentId" : "56ce278c-1cb6-4e85-a630-368adb034706",
        "authorId" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "body" : "I'll use it in the follow up :)",
        "createdAt" : "2019-08-07T23:16:50Z",
        "updatedAt" : "2019-08-09T01:11:53Z",
        "lastEditedBy" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "tags" : [
        ]
      }
    ],
    "commit" : "50f1eef93f84b7a51c0968420cac17a87fb22352",
    "line" : 58,
    "diffHunk" : "@@ -1,1 +495,499 @@\n    import org.apache.spark.sql.catalog.v2.CatalogV2Implicits._\n    val session = df.sparkSession\n\n    session.sessionState.sqlParser.parseMultipartIdentifier(tableName) match {"
  },
  {
    "id" : "dc6f8dcf-6f9c-45e6-ad4b-f594c3f5f811",
    "prId" : 25330,
    "prUrl" : "https://github.com/apache/spark/pull/25330#pullrequestreview-272902883",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c5e139e3-3f5e-4385-9231-f482a72a4a6e",
        "parentId" : null,
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "This should check whether the provider is v2 and use v2 and the V2SessionCatalog if it is.\r\n\r\n```scala\r\ncase CatalogObjectIdentifier(None, ident) if isV1(provider) =>\r\n  saveAsTable(v2SessionCatalog, ident, modeForDSV2)\r\n```",
        "createdAt" : "2019-08-07T22:21:03Z",
        "updatedAt" : "2019-08-09T01:11:53Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      },
      {
        "id" : "ea2abe44-a271-49e8-a487-aab99b0f25a3",
        "parentId" : "c5e139e3-3f5e-4385-9231-f482a72a4a6e",
        "authorId" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "body" : "we separated the session catalog to a separate PR to get this in and add more tests to the session catalog support",
        "createdAt" : "2019-08-07T23:10:00Z",
        "updatedAt" : "2019-08-09T01:11:53Z",
        "lastEditedBy" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "tags" : [
        ]
      },
      {
        "id" : "95ffcf51-9564-4061-9ff7-db434400ba25",
        "parentId" : "c5e139e3-3f5e-4385-9231-f482a72a4a6e",
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "Sounds fine to me, but let's file a JIRA and mark it as a blocker for 3.0.",
        "createdAt" : "2019-08-07T23:24:52Z",
        "updatedAt" : "2019-08-09T01:11:53Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      },
      {
        "id" : "5bbf1578-9f04-439e-ae3c-e7121ddfa580",
        "parentId" : "c5e139e3-3f5e-4385-9231-f482a72a4a6e",
        "authorId" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "body" : "Created SPARK-28666",
        "createdAt" : "2019-08-08T17:31:20Z",
        "updatedAt" : "2019-08-09T01:11:53Z",
        "lastEditedBy" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "tags" : [
        ]
      },
      {
        "id" : "654fdf84-c46d-43af-b514-246f837230ee",
        "parentId" : "c5e139e3-3f5e-4385-9231-f482a72a4a6e",
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "Could you add a TODO comment to this as well?",
        "createdAt" : "2019-08-09T00:49:24Z",
        "updatedAt" : "2019-08-09T01:11:53Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      }
    ],
    "commit" : "50f1eef93f84b7a51c0968420cac17a87fb22352",
    "line" : 65,
    "diffHunk" : "@@ -1,1 +502,506 @@        // TODO(SPARK-28666): This should go through V2SessionCatalog\n\n      case AsTableIdentifier(tableIdentifier) =>\n        saveAsTable(tableIdentifier)\n"
  },
  {
    "id" : "0075b4bc-a40c-4a1e-9206-d0268c1b24b1",
    "prId" : 25330,
    "prUrl" : "https://github.com/apache/spark/pull/25330#pullrequestreview-272259949",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d4637d81-a308-46ca-839f-13f6417d9f93",
        "parentId" : null,
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "What about cases where `AsTableIdentifier` won't match? For example, `saveAsTable(\"ns1.ns2.table\")`. For those cases, we need a `case _ =>` that throws an exception because the table name is not supported by the session catalog.",
        "createdAt" : "2019-08-07T22:22:36Z",
        "updatedAt" : "2019-08-09T01:11:53Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      }
    ],
    "commit" : "50f1eef93f84b7a51c0968420cac17a87fb22352",
    "line" : 66,
    "diffHunk" : "@@ -1,1 +503,507 @@\n      case AsTableIdentifier(tableIdentifier) =>\n        saveAsTable(tableIdentifier)\n\n      case other =>"
  },
  {
    "id" : "57c222fa-9742-4348-b42d-dca8afb588d0",
    "prId" : 25330,
    "prUrl" : "https://github.com/apache/spark/pull/25330#pullrequestreview-272724153",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cc4058c6-4a4f-4a2b-9030-a09589f5db04",
        "parentId" : null,
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "I think that the behavior here is to truncate and write, not replace.",
        "createdAt" : "2019-08-07T22:23:25Z",
        "updatedAt" : "2019-08-09T01:11:53Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      },
      {
        "id" : "df5ef804-4325-4439-b0bc-471aef8f6fa1",
        "parentId" : "cc4058c6-4a4f-4a2b-9030-a09589f5db04",
        "authorId" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "body" : "we may want to change the table properties though, such as partitioning and schema, wouldn't we?",
        "createdAt" : "2019-08-07T23:11:30Z",
        "updatedAt" : "2019-08-09T01:11:53Z",
        "lastEditedBy" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "tags" : [
        ]
      },
      {
        "id" : "e5c084c7-ad3f-410b-872d-c2f47aa3099c",
        "parentId" : "cc4058c6-4a4f-4a2b-9030-a09589f5db04",
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "We need to match the behavior of v1 file sources. Do we know what that behavior is?",
        "createdAt" : "2019-08-07T23:24:31Z",
        "updatedAt" : "2019-08-09T01:11:53Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      },
      {
        "id" : "2e2c7854-867e-4bd4-850c-80cc94557b80",
        "parentId" : "cc4058c6-4a4f-4a2b-9030-a09589f5db04",
        "authorId" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "body" : "As discussed in DSV2 sync, the old behavior was to drop the old table and create a new one, therefore the Replace behavior here works.",
        "createdAt" : "2019-08-08T17:27:43Z",
        "updatedAt" : "2019-08-09T01:11:53Z",
        "lastEditedBy" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "tags" : [
        ]
      }
    ],
    "commit" : "50f1eef93f84b7a51c0968420cac17a87fb22352",
    "line" : 92,
    "diffHunk" : "@@ -1,1 +529,533 @@        AppendData.byName(DataSourceV2Relation.create(table), df.logicalPlan)\n\n      case (SaveMode.Overwrite, _) =>\n        ReplaceTableAsSelect(\n          catalog,"
  },
  {
    "id" : "15c480ee-73eb-4731-892f-258225643c8a",
    "prId" : 25330,
    "prUrl" : "https://github.com/apache/spark/pull/25330#pullrequestreview-272947133",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "656b7241-9cce-4f1d-857f-81c7b803a052",
        "parentId" : null,
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "I think this is an analysis error. The catalog was None, so it belongs to the session catalog. But the session catalog doesn't support namespaces with more than one part, so it is an invalid identifier for that catalog.",
        "createdAt" : "2019-08-09T00:48:53Z",
        "updatedAt" : "2019-08-09T01:11:53Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      },
      {
        "id" : "be9f271c-4595-4f16-a9b0-c1c50c87d3f9",
        "parentId" : "656b7241-9cce-4f1d-857f-81c7b803a052",
        "authorId" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "body" : "ah gotcha, that's the 4th case... I'd like to make that change along with the V2SessionCatalog support. Right now we just couldn't find a catalog for it, so... (Maybe they forgot to specify a default catalog). ",
        "createdAt" : "2019-08-09T00:54:23Z",
        "updatedAt" : "2019-08-09T01:11:53Z",
        "lastEditedBy" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "tags" : [
        ]
      },
      {
        "id" : "dcc65a80-72c0-40d5-819f-e76aefaaf274",
        "parentId" : "656b7241-9cce-4f1d-857f-81c7b803a052",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "+1. `saveAsTable(\"ns1.ns2.table\")` means we want to save the table to the builtin catalog, but `ns1.ns2` is not a valid namespace to the builtin catalog. (or we can report namespace not found?)",
        "createdAt" : "2019-08-09T03:09:39Z",
        "updatedAt" : "2019-08-09T03:09:39Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "5e9fdff4-5a24-4128-b962-9c56a18ca8a3",
        "parentId" : "656b7241-9cce-4f1d-857f-81c7b803a052",
        "authorId" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "body" : "That's true, but that could also mean that they forgot to configure their default catalog too, right?\r\nI can throw a better error when the SessionCatalog code path is implemented, since we need that before we can even say that `ns1.ns2.table` cannot be handled by the session catalog.",
        "createdAt" : "2019-08-09T05:15:48Z",
        "updatedAt" : "2019-08-09T05:15:48Z",
        "lastEditedBy" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "tags" : [
        ]
      }
    ],
    "commit" : "50f1eef93f84b7a51c0968420cac17a87fb22352",
    "line" : 70,
    "diffHunk" : "@@ -1,1 +507,511 @@      case other =>\n        throw new AnalysisException(\n          s\"Couldn't find a catalog to handle the identifier ${other.quoted}.\")\n    }\n  }"
  },
  {
    "id" : "1a43a46b-3d46-487e-8d39-5419b010814a",
    "prId" : 25330,
    "prUrl" : "https://github.com/apache/spark/pull/25330#pullrequestreview-273873488",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "35c58989-ef23-4153-8bc5-d64630b6d355",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Not related to this PR, but just a note. I've been working on V2SessionCatalog improvement for a while, and one issue I found is: it's not easy to implement \"if the table provider is v1, go to v1 session catalog. if the table provider is v2, go to v2 session catalog\".\r\n\r\nIt's easy to do it for CREATE TABLE, because the table provider is known at the beginning. But it's hard for SELECT and INSERT, as we need to look up the table from Hive catalog to get the table provider.\r\n\r\nI'd expect to have 2 analyzer rules to do it:\r\n1. one rule in sql/catalyst, which resolves `UnresolvedRelation` to `UnresolvedCatalogRelation` by looking up the table from hive catalog\r\n2. one rule in sql/core, which resolves `UnresolvedCatalogRelation` to `DataSourceV2Relation`, if the table provider is v2. This has to be in sql/core because `DataSource.lookupDataSource` is in sql/core. We have a rule `FindDataSourceTable` that resolves `UnresolvedCatalogRelation` to v1 relation if the table provider is v1.\r\n\r\nI think `DataFrameWriter` should produce `...Statement` plans, to reuse the analyzer rules for all the things.\r\n\r\n",
        "createdAt" : "2019-08-09T03:06:59Z",
        "updatedAt" : "2019-08-09T03:06:59Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "35c2822d-0f97-4863-90f6-e36546ebd3cd",
        "parentId" : "35c58989-ef23-4153-8bc5-d64630b6d355",
        "authorId" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "body" : "Well, the V2SessionCatalog already produces a `CatalogTableAsV2`, which gets unwrapped to the `UnresolvedCatalogRelation` already in `DataSourceResolution`, and then it can get resolved to the V1 code paths. I've also had a similar problem when trying to implement V2SessionCatalog support for `ALTER TABLE`. In that case I decided that it should reject and fallback to V1 plans if the loaded table is a `CatalogTableAsV2`.",
        "createdAt" : "2019-08-09T05:20:56Z",
        "updatedAt" : "2019-08-09T05:20:56Z",
        "lastEditedBy" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "tags" : [
        ]
      },
      {
        "id" : "810ec13f-b4b8-4b17-b897-c3cc4a8f2239",
        "parentId" : "35c58989-ef23-4153-8bc5-d64630b6d355",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "> V2SessionCatalog already produces a CatalogTableAsV2\r\n\r\nYes, for both v1 and v2 provider. So we still need a rule to check the table provider, and either unwrap it to a `UnresolvedCatalogRelation`, or create the actual v2 table.",
        "createdAt" : "2019-08-09T06:00:10Z",
        "updatedAt" : "2019-08-09T06:00:10Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "dc93a1b9-4afc-4d82-a912-01881c95fdee",
        "parentId" : "35c58989-ef23-4153-8bc5-d64630b6d355",
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "> Yes, for both v1 and v2 provider. So we still need a rule to check the table provider, and either unwrap it to a UnresolvedCatalogRelation, or create the actual v2 table.\r\n\r\nThis should be done in `loadTable`. The original implementation called `TableProvider` to create the table, but this path is currently broken. I think that `TableProvider` needs to be improved so that it works instead of adding more rules to convert from `CatalogTableAsV2`. \r\n\r\nThe catalog should return the correct `Table` instance for all v2 tables. Spark shouldn't convert between v2 table instances.\r\n",
        "createdAt" : "2019-08-12T18:32:53Z",
        "updatedAt" : "2019-08-12T18:32:53Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      }
    ],
    "commit" : "50f1eef93f84b7a51c0968420cac17a87fb22352",
    "line" : 40,
    "diffHunk" : "@@ -1,1 +376,380 @@      case CatalogObjectIdentifier(Some(catalog), ident) =>\n        insertInto(catalog, ident)\n      // TODO(SPARK-28667): Support the V2SessionCatalog\n      case AsTableIdentifier(tableIdentifier) =>\n        insertInto(tableIdentifier)"
  }
]