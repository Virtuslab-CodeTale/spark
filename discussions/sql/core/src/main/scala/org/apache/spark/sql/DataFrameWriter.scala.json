[
  {
    "id" : "e03fe885-55e3-43a2-b072-4775e22ce1ef",
    "prId" : 32546,
    "prUrl" : "https://github.com/apache/spark/pull/32546#pullrequestreview-664206352",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3841e95c-ea94-4811-877c-be9e0ed7fae8",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Here you didn't link generic options but you did for Parquet. What's diff?",
        "createdAt" : "2021-05-20T06:19:42Z",
        "updatedAt" : "2021-05-20T06:19:43Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "005f4271-b79e-405b-a8ea-f73d5637040e",
        "parentId" : "3841e95c-ea94-4811-877c-be9e0ed7fae8",
        "authorId" : "2d8ced12-ada1-43e1-9240-bf1c11a01e4c",
        "body" : "@HyukjinKwon , this is because the options I removed doesn't include generic options. (For example it has only one option, `compression` in this case)\r\n\r\nWould it be better to just add the link to the Generic options no matter what we removed ??",
        "createdAt" : "2021-05-20T08:33:54Z",
        "updatedAt" : "2021-05-20T08:34:43Z",
        "lastEditedBy" : "2d8ced12-ada1-43e1-9240-bf1c11a01e4c",
        "tags" : [
        ]
      },
      {
        "id" : "70a426e8-95be-4d92-96ce-2b14435c1a2d",
        "parentId" : "3841e95c-ea94-4811-877c-be9e0ed7fae8",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Either way is fine. The point is that we should keep it consistent",
        "createdAt" : "2021-05-20T10:23:44Z",
        "updatedAt" : "2021-05-20T10:23:44Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "1737a103-d03e-4969-93c2-b3eeff2fde62",
        "parentId" : "3841e95c-ea94-4811-877c-be9e0ed7fae8",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I would just add it for simplicity, and in case we have write generic options in the future",
        "createdAt" : "2021-05-20T10:24:25Z",
        "updatedAt" : "2021-05-20T10:24:26Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "043d3087c838c2f6439d798bdc8e889d6a728ff7",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +885,889 @@   * <a href=\n   *   \"https://spark.apache.org/docs/latest/sql-data-sources-orc.html#data-source-option\">\n   *   Data Source Option</a> in the version you use.\n   *\n   * @since 1.5.0"
  },
  {
    "id" : "c0bff79f-cda2-401b-bb5d-669471ac5333",
    "prId" : 29885,
    "prUrl" : "https://github.com/apache/spark/pull/29885#pullrequestreview-504107320",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7caa7ab2-879c-426d-b16e-840b7d7b64df",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Returned catalog is always a `TableCatalog`?",
        "createdAt" : "2020-10-06T23:31:35Z",
        "updatedAt" : "2020-10-06T23:36:41Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "2ea1f583-888e-4570-8201-ab77ad36e1e7",
        "parentId" : "7caa7ab2-879c-426d-b16e-840b7d7b64df",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Or do we need to check if `catalog` is `JDBCTableCatalog`?",
        "createdAt" : "2020-10-06T23:36:15Z",
        "updatedAt" : "2020-10-06T23:36:41Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "298d2dc7-9d82-43c2-b89c-9f14d6bcd560",
        "parentId" : "7caa7ab2-879c-426d-b16e-840b7d7b64df",
        "authorId" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "body" : "Seems to me that the catalog in `NonSessionCatalogAndIdentifier` is always a `TableCatalog`.",
        "createdAt" : "2020-10-07T17:25:43Z",
        "updatedAt" : "2020-10-07T17:25:43Z",
        "lastEditedBy" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "tags" : [
        ]
      }
    ],
    "commit" : "9ff43135256f7c3ad3922f6772186634d69633a1",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +820,824 @@    session.sessionState.sqlParser.parseMultipartIdentifier(table) match {\n      case nameParts @ NonSessionCatalogAndIdentifier(catalog, tableIdentifier) =>\n        saveAsTable(catalog.asTableCatalog, tableIdentifier, nameParts)\n\n      case _ =>"
  },
  {
    "id" : "d75fd2ee-c4ff-43e8-a545-4715c1e88230",
    "prId" : 29543,
    "prUrl" : "https://github.com/apache/spark/pull/29543#pullrequestreview-485298760",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "abbe2e61-b70b-4773-a2ef-84382efaa391",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "The `path` here is a String, do we really need to check `path.nonEmpty`?",
        "createdAt" : "2020-09-09T07:51:43Z",
        "updatedAt" : "2020-09-09T07:51:44Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "13c4cf30-0892-4cfd-bde1-a02ae2cda3e6",
        "parentId" : "abbe2e61-b70b-4773-a2ef-84382efaa391",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "Good catch. I think we need to remove the check. The output is more confusing since `path` option is also set:\r\n```scala\r\nscala> Seq(1).toDF.write.option(\"path\", \"/tmp/path1\").parquet(\"\")\r\njava.lang.IllegalArgumentException: Can not create a Path from an empty string\r\n  at org.apache.hadoop.fs.Path.checkPathArg(Path.java:168)\r\n```\r\n\r\nI will create a PR for this.",
        "createdAt" : "2020-09-09T17:46:45Z",
        "updatedAt" : "2020-09-09T17:46:46Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      },
      {
        "id" : "16fcb7dc-55d5-453a-924b-074af2cbb5f5",
        "parentId" : "abbe2e61-b70b-4773-a2ef-84382efaa391",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "Created https://github.com/apache/spark/pull/29697",
        "createdAt" : "2020-09-09T18:43:40Z",
        "updatedAt" : "2020-09-09T18:43:40Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "261b6097907097a77b3c85606510e19c85a648d0",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +287,291 @@  def save(path: String): Unit = {\n    if (!df.sparkSession.sessionState.conf.legacyPathOptionBehavior &&\n        extraOptions.contains(\"path\") && path.nonEmpty) {\n      throw new AnalysisException(\"There is a 'path' option set and save() is called with a path \" +\n        \"parameter. Either remove the path option, or call save() without the parameter. \" +"
  },
  {
    "id" : "014edaa5-65ad-43c6-be67-fd948130f09c",
    "prId" : 29191,
    "prUrl" : "https://github.com/apache/spark/pull/29191#pullrequestreview-453502481",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a912c5eb-8676-4329-b8d7-ae71af27bb98",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "I missed that `dsOptions.asCaseSensitiveMap()` is used later at line 303, `extraOptions.originalMap` will preserve the key case-sensitivity.",
        "createdAt" : "2020-07-22T16:47:26Z",
        "updatedAt" : "2020-07-23T06:10:34Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "81ac6c614c16e8d3c43ff14888216a797df04275",
    "line" : 66,
    "diffHunk" : "@@ -1,1 +308,312 @@        provider, df.sparkSession.sessionState.conf)\n      val options = sessionOptions.filterKeys(!extraOptions.contains(_)) ++ extraOptions.toMap\n      val dsOptions = new CaseInsensitiveStringMap(options.asJava)\n\n      def getTable: Table = {"
  },
  {
    "id" : "ee83ba9c-0065-4239-9e5b-1c6c701e1f57",
    "prId" : 27992,
    "prUrl" : "https://github.com/apache/spark/pull/27992#pullrequestreview-384230569",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2b434fee-2704-42fc-b124-b2cbdcbaf609",
        "parentId" : null,
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "Not related to this PR, but do we plan to support buckets for datasource V2?",
        "createdAt" : "2020-03-30T20:34:20Z",
        "updatedAt" : "2020-03-31T06:29:04Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      },
      {
        "id" : "27d1b391-437e-4128-a91d-471c59e0ede4",
        "parentId" : "2b434fee-2704-42fc-b124-b2cbdcbaf609",
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "Yes. I have a design doc I can share with you.",
        "createdAt" : "2020-03-30T20:39:21Z",
        "updatedAt" : "2020-03-31T06:29:04Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      },
      {
        "id" : "bb0c37c5-f2d1-4e53-bc68-baddc431667e",
        "parentId" : "2b434fee-2704-42fc-b124-b2cbdcbaf609",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "Great, thanks!",
        "createdAt" : "2020-03-30T21:06:40Z",
        "updatedAt" : "2020-03-31T06:29:04Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "8d948739db7d247a80899b91ab13e5a38018cc21",
    "line" : 72,
    "diffHunk" : "@@ -1,1 +578,582 @@          df.queryExecution.analyzed,\n          partitioningAsV2,\n          None,\n          Map.empty,\n          Some(source),"
  },
  {
    "id" : "92f5db9c-9168-4171-8695-baecd68387b5",
    "prId" : 27992,
    "prUrl" : "https://github.com/apache/spark/pull/27992#pullrequestreview-384432788",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "25c83da1-1ce6-4f1f-aa0c-6e075a7fff0a",
        "parentId" : null,
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "Should this use the constants in `TableCatalog`?",
        "createdAt" : "2020-03-30T21:26:56Z",
        "updatedAt" : "2020-03-31T06:29:04Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      },
      {
        "id" : "6572dde5-8c05-4bfa-a18b-ad8238262872",
        "parentId" : "25c83da1-1ce6-4f1f-aa0c-6e075a7fff0a",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "the table location constant is \"location\", which is different from the \"path\" in `DataFrameReader/Writer`. I'll use the constant for the comment.",
        "createdAt" : "2020-03-31T06:26:22Z",
        "updatedAt" : "2020-03-31T06:29:04Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "8d948739db7d247a80899b91ab13e5a38018cc21",
    "line" : 76,
    "diffHunk" : "@@ -1,1 +582,586 @@          Some(source),\n          Map.empty,\n          extraOptions.get(\"path\"),\n          extraOptions.get(TableCatalog.PROP_COMMENT),\n          extraOptions.toMap,"
  },
  {
    "id" : "e0d965bc-1ee4-44eb-bf1a-c3c2e0fc945f",
    "prId" : 26957,
    "prUrl" : "https://github.com/apache/spark/pull/26957#pullrequestreview-347734720",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "24a0f749-ea8c-45bb-8207-07003297bb29",
        "parentId" : null,
        "authorId" : "8fec0990-98ca-4639-aa0d-4d79de603820",
        "body" : "A little curious why `Overwrite` doesn't need to create a `DataSourceV2Relation`?",
        "createdAt" : "2020-01-14T02:30:58Z",
        "updatedAt" : "2020-01-24T17:49:38Z",
        "lastEditedBy" : "8fec0990-98ca-4639-aa0d-4d79de603820",
        "tags" : [
        ]
      },
      {
        "id" : "c60babb8-0c19-47f3-9fd7-7d635bfcee91",
        "parentId" : "24a0f749-ea8c-45bb-8207-07003297bb29",
        "authorId" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "body" : "It drops and recreates a table. It's a DDL operation instead of DML",
        "createdAt" : "2020-01-14T21:53:15Z",
        "updatedAt" : "2020-01-24T17:49:38Z",
        "lastEditedBy" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "tags" : [
        ]
      },
      {
        "id" : "2e5f62de-a796-4264-9402-9a93eeca8fc1",
        "parentId" : "24a0f749-ea8c-45bb-8207-07003297bb29",
        "authorId" : "8fec0990-98ca-4639-aa0d-4d79de603820",
        "body" : "Still probably a dumb question. Why does DDL/DML affects how we generate the query plan? I'm asking this because in the `save()` function for the DataFrameWriter, we do generate a `DataSourceV2Relation` for `Overwrite ` mode. I'm curious about why there is such a difference here.",
        "createdAt" : "2020-01-15T02:40:39Z",
        "updatedAt" : "2020-01-24T17:49:38Z",
        "lastEditedBy" : "8fec0990-98ca-4639-aa0d-4d79de603820",
        "tags" : [
        ]
      },
      {
        "id" : "e212a938-73d3-4a58-a4e4-ab0d2448548c",
        "parentId" : "24a0f749-ea8c-45bb-8207-07003297bb29",
        "authorId" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "body" : "in save, we don't update table information in a catalog. saveAsTable updates the catalog, therefore is doing different work. So we need to do a separate operation",
        "createdAt" : "2020-01-24T03:18:15Z",
        "updatedAt" : "2020-01-24T17:49:38Z",
        "lastEditedBy" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "tags" : [
        ]
      }
    ],
    "commit" : "df29683d38260cb7f69adfccdc7fab2d315e663e",
    "line" : 69,
    "diffHunk" : "@@ -1,1 +562,566 @@        AppendData.byName(v2Relation, df.logicalPlan, extraOptions.toMap)\n\n      case (SaveMode.Overwrite, _) =>\n        ReplaceTableAsSelect(\n          catalog,"
  },
  {
    "id" : "121dd16b-f488-4309-9c9a-3b4fea529fcf",
    "prId" : 26913,
    "prUrl" : "https://github.com/apache/spark/pull/26913#pullrequestreview-334363446",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c8a48c22-9c81-43a8-98a3-6ffc067c73d2",
        "parentId" : null,
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "CTAS works for `SaveMode.ErrorIfExists`, but doesn't this also need to handle `SaveMode.Ignore`? It should check if the table exists and not run anything if it does.",
        "createdAt" : "2019-12-19T01:28:53Z",
        "updatedAt" : "2020-01-09T15:12:09Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      },
      {
        "id" : "e4c4f066-d933-4410-ab1d-3f3cafb842b7",
        "parentId" : "c8a48c22-9c81-43a8-98a3-6ffc067c73d2",
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "Nevermind, I see it sets `ignoreIfExists` correctly.",
        "createdAt" : "2019-12-19T01:29:37Z",
        "updatedAt" : "2020-01-09T15:12:09Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      }
    ],
    "commit" : "963133edc5004a465347fd7b1c3edfffd6ff5a8d",
    "line" : 76,
    "diffHunk" : "@@ -1,1 +307,311 @@\n              runCommand(df.sparkSession, \"save\") {\n                CreateTableAsSelect(\n                  catalog,\n                  ident,"
  },
  {
    "id" : "be8c7abc-0601-4b18-86b3-16dd89e87f57",
    "prId" : 26913,
    "prUrl" : "https://github.com/apache/spark/pull/26913#pullrequestreview-335981196",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d1fff517-dcc7-45c0-a78a-8f3ff27d9f37",
        "parentId" : null,
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "Could this be simplified to just call `saveAsTable` (or an equivalent internal method with the right args) when `SupportsCatalogOptions` is implemented? Or are the behaviors different?",
        "createdAt" : "2019-12-23T17:22:35Z",
        "updatedAt" : "2020-01-09T15:12:09Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      },
      {
        "id" : "08160ac8-f23d-4648-bc93-c499c663cee9",
        "parentId" : "d1fff517-dcc7-45c0-a78a-8f3ff27d9f37",
        "authorId" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "body" : "drop table gets a bit weird in overwrite mode. ",
        "createdAt" : "2019-12-23T19:02:56Z",
        "updatedAt" : "2020-01-09T15:12:09Z",
        "lastEditedBy" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "tags" : [
        ]
      },
      {
        "id" : "e0d11991-e74e-4d0f-9246-c61e3e041563",
        "parentId" : "d1fff517-dcc7-45c0-a78a-8f3ff27d9f37",
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "You're right, `saveAsTable` uses RTAS instead of truncate.",
        "createdAt" : "2019-12-23T19:15:16Z",
        "updatedAt" : "2020-01-09T15:12:09Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      }
    ],
    "commit" : "963133edc5004a465347fd7b1c3edfffd6ff5a8d",
    "line" : 27,
    "diffHunk" : "@@ -1,1 +262,266 @@        case SaveMode.Append | SaveMode.Overwrite =>\n          val table = provider match {\n            case supportsExtract: SupportsCatalogOptions =>\n              val ident = supportsExtract.extractIdentifier(dsOptions)\n              val sessionState = df.sparkSession.sessionState"
  },
  {
    "id" : "8256ba50-c342-49bd-951b-30ca80a31e34",
    "prId" : 26853,
    "prUrl" : "https://github.com/apache/spark/pull/26853#pullrequestreview-330871543",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e4a7d9d1-4813-4f00-9b71-2ef778bd4727",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "This looks much better (BTW, this is not a replacement from `CatalogObjectIdentifier` to `CatalogAndIdentifer`)",
        "createdAt" : "2019-12-11T18:37:47Z",
        "updatedAt" : "2019-12-11T21:38:09Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "cd3eaf28-b502-4efd-8a1e-ec13f5cf5007",
        "parentId" : "e4a7d9d1-4813-4f00-9b71-2ef778bd4727",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : " I like this change. Could you update the PR description more precisely?",
        "createdAt" : "2019-12-11T18:38:34Z",
        "updatedAt" : "2019-12-11T21:38:09Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "ec0484a4-c242-4c27-b966-003fb775ece4",
        "parentId" : "e4a7d9d1-4813-4f00-9b71-2ef778bd4727",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "Updated. Thanks!",
        "createdAt" : "2019-12-11T21:58:54Z",
        "updatedAt" : "2019-12-11T21:58:54Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "a1fd1c13fdf221b41450403af0e508784f43212e",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +358,362 @@\n    session.sessionState.sqlParser.parseMultipartIdentifier(tableName) match {\n      case NonSessionCatalogAndIdentifier(catalog, ident) =>\n        insertInto(catalog, ident)\n"
  },
  {
    "id" : "d230479e-f7c4-4bb8-b230-fc073c53a085",
    "prId" : 26297,
    "prUrl" : "https://github.com/apache/spark/pull/26297#pullrequestreview-309390204",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1b6e15b2-50ff-4a8d-ab23-2096ad746404",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "We can introduce a general API for it, but I don't think it's worth. It only applies to file source + DataFrameWriter, not file source tables or DataFrameWriterV2.",
        "createdAt" : "2019-10-30T17:30:11Z",
        "updatedAt" : "2019-11-27T16:28:24Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "95b62b6dd5a0c58b001f3faddd8527858bbc792c",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +264,268 @@        // the schema of input query and the user-specified partitioning to `getTable`. If the\n        // query schema is not compatible with the existing data, the write can still success but\n        // following reads would fail.\n        case p: FileDataSourceV2 =>\n          import org.apache.spark.sql.connector.catalog.CatalogV2Implicits._"
  }
]