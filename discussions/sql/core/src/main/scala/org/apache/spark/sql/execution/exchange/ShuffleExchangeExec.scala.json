[
  {
    "id" : "a9023172-1f29-408a-a190-c03e9d4a9f3a",
    "prId" : 32932,
    "prUrl" : "https://github.com/apache/spark/pull/32932#pullrequestreview-690655939",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "52b1daea-7c7b-443f-afff-321b7abe241a",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "please update the doc for it",
        "createdAt" : "2021-06-23T13:28:27Z",
        "updatedAt" : "2021-06-23T13:28:27Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "0fe14d0001adedef66696699d478dd4d4e9c392c",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +96,100 @@// Spark will try to rebalance partitions that make per-partition size not too small and not\n// too big. Local shuffle reader will be used if possible to reduce network traffic.\ncase object REBALANCE_PARTITIONS_BY_NONE extends ShuffleOrigin\n\n// Indicates that the shuffle operator was added by the user-specified rebalance operator with"
  },
  {
    "id" : "d7688ed8-d4da-46fc-8798-99e53952108b",
    "prId" : 29262,
    "prUrl" : "https://github.com/apache/spark/pull/29262#pullrequestreview-456894717",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e0d3fbbb-d7b2-4ec6-a280-07b76d548382",
        "parentId" : null,
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "need to update the tests fto have this",
        "createdAt" : "2020-07-28T18:22:53Z",
        "updatedAt" : "2020-07-29T14:36:07Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "50175d75fe8bb0121d2f77f49a909212651c1035",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +50,54 @@   * Returns the number of mappers of this shuffle.\n   */\n  def numMappers: Int\n\n  /**"
  },
  {
    "id" : "1cab6480-949d-4e48-a02a-325f0555d04d",
    "prId" : 29134,
    "prUrl" : "https://github.com/apache/spark/pull/29134#pullrequestreview-450350677",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1642e054-8fb9-44c4-b779-a5b35250c0c7",
        "parentId" : null,
        "authorId" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "body" : "We can get this through MapOutputTracker.",
        "createdAt" : "2020-07-17T03:31:10Z",
        "updatedAt" : "2020-07-17T03:31:10Z",
        "lastEditedBy" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "tags" : [
        ]
      }
    ],
    "commit" : "2fd2c089c91df76d9ab2958e788853e954c9eb8d",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +49,53 @@abstract class ShuffleExchange extends Exchange {\n  def shuffleId: Int\n  def getNumMappers: Int\n  def getNumReducers: Int\n  def canChangeNumPartitions: Boolean"
  },
  {
    "id" : "1eae1b71-fb26-439f-ac3c-6c7a2f44562c",
    "prId" : 29134,
    "prUrl" : "https://github.com/apache/spark/pull/29134#pullrequestreview-450350787",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e78d6b22-7b0e-47fd-9632-0149537a1a33",
        "parentId" : null,
        "authorId" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "body" : "This is available in `mapOutputStats`",
        "createdAt" : "2020-07-17T03:31:41Z",
        "updatedAt" : "2020-07-17T03:31:42Z",
        "lastEditedBy" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "tags" : [
        ]
      }
    ],
    "commit" : "2fd2c089c91df76d9ab2958e788853e954c9eb8d",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +48,52 @@ */\nabstract class ShuffleExchange extends Exchange {\n  def shuffleId: Int\n  def getNumMappers: Int\n  def getNumReducers: Int"
  },
  {
    "id" : "31f7b820-cf7b-4d39-9fe2-b25fd6367924",
    "prId" : 26846,
    "prUrl" : "https://github.com/apache/spark/pull/26846#pullrequestreview-330357988",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2f222670-176b-45da-93ff-bbe0935a52dc",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "can we add a comment saying that this is only needed in AQE?",
        "createdAt" : "2019-12-11T08:14:43Z",
        "updatedAt" : "2019-12-12T01:58:49Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "ce80397e072a37170aaec0180076c777a9176b6a",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +68,72 @@\n  // 'mapOutputStatisticsFuture' is only needed when enable AQE.\n  @transient lazy val mapOutputStatisticsFuture: Future[MapOutputStatistics] = {\n    if (inputRDD.getNumPartitions == 0) {\n      Future.successful(null)"
  },
  {
    "id" : "64d9e223-acbe-42ef-a084-dbfabd35b636",
    "prId" : 26044,
    "prUrl" : "https://github.com/apache/spark/pull/26044#pullrequestreview-321014077",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "03918766-eba1-4dbc-b420-b9e5e83ccdf6",
        "parentId" : null,
        "authorId" : "3d4870da-39a8-4406-b00d-131930d14cd8",
        "body" : "Similar reasons for this change as above. This time the stacktrace is:\r\n```\r\n04:31:28.110 ERROR org.apache.spark.executor.Executor: Exception in task 0.0 in stage 8.0 (TID 10)\r\norg.apache.spark.sql.catalyst.errors.package$TreeNodeException: makeCopy, tree:\r\nExchange SinglePartition, true, [id=#180]\r\n+- *(1) HashAggregate(keys=[], functions=[partial_count(1)], output=[count#397L])\r\n   +- *(1) Project\r\n      +- *(1) LocalTableScan <empty>, [col1#385]\r\n\r\n        at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\r\n        at org.apache.spark.sql.catalyst.trees.TreeNode.makeCopy(TreeNode.scala:435)\r\n        at org.apache.spark.sql.catalyst.trees.TreeNode.makeCopy(TreeNode.scala:424)\r\n        at org.apache.spark.sql.execution.SparkPlan.makeCopy(SparkPlan.scala:102)\r\n        at org.apache.spark.sql.execution.SparkPlan.makeCopy(SparkPlan.scala:63)\r\n        at org.apache.spark.sql.catalyst.trees.TreeNode.withNewChildren(TreeNode.scala:263)\r\n        at org.apache.spark.sql.catalyst.plans.QueryPlan.doCanonicalize(QueryPlan.scala:277)\r\n        at org.apache.spark.sql.catalyst.plans.QueryPlan.canonicalized$lzycompute(QueryPlan.scala:245)\r\n        at org.apache.spark.sql.catalyst.plans.QueryPlan.canonicalized(QueryPlan.scala:244)\r\n        at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$doCanonicalize$1(QueryPlan.scala:259)\r\n        at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\r\n        at scala.collection.immutable.List.foreach(List.scala:392)\r\n        at scala.collection.TraversableLike.map(TraversableLike.scala:238)\r\n        at scala.collection.TraversableLike.map$(TraversableLike.scala:231)\r\n        at scala.collection.immutable.List.map(List.scala:298)\r\n        at org.apache.spark.sql.catalyst.plans.QueryPlan.doCanonicalize(QueryPlan.scala:259)\r\n        at org.apache.spark.sql.catalyst.plans.QueryPlan.canonicalized$lzycompute(QueryPlan.scala:245)\r\n        at org.apache.spark.sql.catalyst.plans.QueryPlan.canonicalized(QueryPlan.scala:244)\r\n        at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$doCanonicalize$1(QueryPlan.scala:259)\r\n        at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\r\n        at scala.collection.immutable.List.foreach(List.scala:392)\r\n        at scala.collection.TraversableLike.map(TraversableLike.scala:238)\r\n        at scala.collection.TraversableLike.map$(TraversableLike.scala:231)\r\n        at scala.collection.immutable.List.map(List.scala:298)\r\n        at org.apache.spark.sql.catalyst.plans.QueryPlan.doCanonicalize(QueryPlan.scala:259)\r\n        at org.apache.spark.sql.catalyst.plans.QueryPlan.canonicalized$lzycompute(QueryPlan.scala:245)\r\n        at org.apache.spark.sql.catalyst.plans.QueryPlan.canonicalized(QueryPlan.scala:244)\r\n        at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$doCanonicalize$1(QueryPlan.scala:259)\r\n        at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\r\n        at scala.collection.immutable.List.foreach(List.scala:392)\r\n        at scala.collection.TraversableLike.map(TraversableLike.scala:238)\r\n        at scala.collection.TraversableLike.map$(TraversableLike.scala:231)\r\n        at scala.collection.immutable.List.map(List.scala:298)\r\n        at org.apache.spark.sql.catalyst.plans.QueryPlan.doCanonicalize(QueryPlan.scala:259)\r\n        at org.apache.spark.sql.catalyst.plans.QueryPlan.canonicalized$lzycompute(QueryPlan.scala:245)\r\n        at org.apache.spark.sql.catalyst.plans.QueryPlan.canonicalized(QueryPlan.scala:244)\r\n        at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$doCanonicalize$1(QueryPlan.scala:259)\r\n        at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\r\n        at scala.collection.immutable.List.foreach(List.scala:392)\r\n        at scala.collection.TraversableLike.map(TraversableLike.scala:238)\r\n        at scala.collection.TraversableLike.map$(TraversableLike.scala:231)\r\n        at scala.collection.immutable.List.map(List.scala:298)\r\n        at org.apache.spark.sql.catalyst.plans.QueryPlan.doCanonicalize(QueryPlan.scala:259)\r\n        at org.apache.spark.sql.catalyst.plans.QueryPlan.canonicalized$lzycompute(QueryPlan.scala:245)\r\n        at org.apache.spark.sql.catalyst.plans.QueryPlan.canonicalized(QueryPlan.scala:244)\r\n        at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$doCanonicalize$1(QueryPlan.scala:259)\r\n        at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\r\n        at scala.collection.immutable.List.foreach(List.scala:392)\r\n        at scala.collection.TraversableLike.map(TraversableLike.scala:238)\r\n        at scala.collection.TraversableLike.map$(TraversableLike.scala:231)\r\n        at scala.collection.immutable.List.map(List.scala:298)\r\n        at org.apache.spark.sql.catalyst.plans.QueryPlan.doCanonicalize(QueryPlan.scala:259)\r\n        at org.apache.spark.sql.catalyst.plans.QueryPlan.canonicalized$lzycompute(QueryPlan.scala:245)\r\n        at org.apache.spark.sql.catalyst.plans.QueryPlan.canonicalized(QueryPlan.scala:244)\r\n        at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$doCanonicalize$1(QueryPlan.scala:259)\r\n        at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\r\n        at scala.collection.immutable.List.foreach(List.scala:392)\r\n        at scala.collection.TraversableLike.map(TraversableLike.scala:238)\r\n        at scala.collection.TraversableLike.map$(TraversableLike.scala:231)\r\n        at scala.collection.immutable.List.map(List.scala:298)\r\n        at org.apache.spark.sql.catalyst.plans.QueryPlan.doCanonicalize(QueryPlan.scala:259)\r\n        at org.apache.spark.sql.catalyst.plans.QueryPlan.canonicalized$lzycompute(QueryPlan.scala:245)\r\n        at org.apache.spark.sql.catalyst.plans.QueryPlan.canonicalized(QueryPlan.scala:244)\r\n        at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$doCanonicalize$1(QueryPlan.scala:259)\r\n        at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\r\n        at scala.collection.immutable.List.foreach(List.scala:392)\r\n        at scala.collection.TraversableLike.map(TraversableLike.scala:238)\r\n        at scala.collection.TraversableLike.map$(TraversableLike.scala:231)\r\n        at scala.collection.immutable.List.map(List.scala:298)\r\n        at org.apache.spark.sql.catalyst.plans.QueryPlan.doCanonicalize(QueryPlan.scala:259)\r\n        at org.apache.spark.sql.catalyst.plans.QueryPlan.canonicalized$lzycompute(QueryPlan.scala:245)\r\n        at org.apache.spark.sql.catalyst.plans.QueryPlan.canonicalized(QueryPlan.scala:244)\r\n        at org.apache.spark.sql.execution.SubqueryExec.doCanonicalize(basicPhysicalOperators.scala:772)\r\n        at org.apache.spark.sql.execution.SubqueryExec.doCanonicalize(basicPhysicalOperators.scala:742)\r\n        at org.apache.spark.sql.catalyst.plans.QueryPlan.canonicalized$lzycompute(QueryPlan.scala:245)\r\n        at org.apache.spark.sql.catalyst.plans.QueryPlan.canonicalized(QueryPlan.scala:244)\r\n        at org.apache.spark.sql.execution.ScalarSubquery.canonicalized$lzycompute(subquery.scala:109)\r\n        at org.apache.spark.sql.execution.ScalarSubquery.canonicalized(subquery.scala:108)\r\n        at org.apache.spark.sql.execution.ScalarSubquery.canonicalized(subquery.scala:62)\r\n        at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$canonicalized$1(Expression.scala:229)\r\n        at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\r\n        at scala.collection.immutable.List.foreach(List.scala:392)\r\n        at scala.collection.TraversableLike.map(TraversableLike.scala:238)\r\n        at scala.collection.TraversableLike.map$(TraversableLike.scala:231)\r\n        at scala.collection.immutable.List.map(List.scala:298)\r\n        at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:229)\r\n        at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:228)\r\n        at org.apache.spark.sql.catalyst.expressions.Expression.semanticHash(Expression.scala:248)\r\n        at org.apache.spark.sql.catalyst.expressions.EquivalentExpressions$Expr.hashCode(EquivalentExpressions.scala:41)\r\n        at scala.runtime.Statics.anyHash(Statics.java:122)\r\n        at scala.collection.mutable.HashTable$HashUtils.elemHashCode(HashTable.scala:416)\r\n        at scala.collection.mutable.HashTable$HashUtils.elemHashCode$(HashTable.scala:416)\r\n        at scala.collection.mutable.HashMap.elemHashCode(HashMap.scala:44)\r\n        at scala.collection.mutable.HashTable.findEntry(HashTable.scala:136)\r\n        at scala.collection.mutable.HashTable.findEntry$(HashTable.scala:135)\r\n        at scala.collection.mutable.HashMap.findEntry(HashMap.scala:44)\r\n        at scala.collection.mutable.HashMap.get(HashMap.scala:74)\r\n        at org.apache.spark.sql.catalyst.expressions.EquivalentExpressions.addExpr(EquivalentExpressions.scala:55)\r\n        at org.apache.spark.sql.catalyst.expressions.EquivalentExpressions.addExprTree(EquivalentExpressions.scala:99)\r\n        at org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.$anonfun$subexpressionElimination$1(CodeGenerator.scala:1118)\r\n        at org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.$anonfun$subexpressionElimination$1$adapted(CodeGenerator.scala:1118)\r\n        at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n        at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n        at org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.subexpressionElimination(CodeGenerator.scala:1118)\r\n        at org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.generateExpressions(CodeGenerator.scala:1170)\r\n        at org.apache.spark.sql.catalyst.expressions.codegen.GenerateMutableProjection$.create(GenerateMutableProjection.scala:64)\r\n        at org.apache.spark.sql.catalyst.expressions.codegen.GenerateMutableProjection$.generate(GenerateMutableProjection.scala:49)\r\n        at org.apache.spark.sql.catalyst.expressions.MutableProjection$.createCodeGeneratedObject(Projection.scala:84)\r\n        at org.apache.spark.sql.catalyst.expressions.MutableProjection$.createCodeGeneratedObject(Projection.scala:80)\r\n        at org.apache.spark.sql.catalyst.expressions.CodeGeneratorWithInterpretedFallback.createObject(CodeGeneratorWithInterpretedFallback.scala:47)\r\n        at org.apache.spark.sql.catalyst.expressions.MutableProjection$.create(Projection.scala:95)\r\n        at org.apache.spark.sql.catalyst.expressions.MutableProjection$.create(Projection.scala:103)\r\n        at org.apache.spark.sql.execution.SparkPlan.newMutableProjection(SparkPlan.scala:471)\r\n        at org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:116)\r\n        at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:837)\r\n        at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:837)\r\n        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n        at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n        at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n        at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n        at org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:425)\r\n        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:428)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n        at java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.lang.reflect.InvocationTargetException\r\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n        at java.lang.reflect.Constructor.newInstance(Constructor.java:423)\r\n        at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$makeCopy$7(TreeNode.scala:468)\r\n        at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:72)\r\n        at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$makeCopy$1(TreeNode.scala:467)\r\n        at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\r\n        ... 133 more\r\nCaused by: java.lang.NullPointerException\r\n        at org.apache.spark.sql.execution.SparkPlan.sparkContext(SparkPlan.scala:72)\r\n        at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.metrics$lzycompute(ShuffleExchangeExec.scala:57)\r\n        at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.metrics(ShuffleExchangeExec.scala:58)\r\n        at org.apache.spark.sql.execution.SparkPlan.longMetric(SparkPlan.scala:149)\r\n        at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.<init>(ShuffleExchangeExec.scala:63)\r\n        ... 141 more\r\n```\r\nAnd if `serializer` is not lazy then it makes no sense for `metrics` to be lazy.",
        "createdAt" : "2019-11-21T16:23:38Z",
        "updatedAt" : "2019-12-15T11:25:14Z",
        "lastEditedBy" : "3d4870da-39a8-4406-b00d-131930d14cd8",
        "tags" : [
        ]
      }
    ],
    "commit" : "ed26b339eb55a781d96dbf5943465f5e93ad0253",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +62,66 @@  override def nodeName: String = \"Exchange\"\n\n  private lazy val serializer: Serializer =\n    new UnsafeRowSerializer(child.output.size, longMetric(\"dataSize\"))\n"
  }
]