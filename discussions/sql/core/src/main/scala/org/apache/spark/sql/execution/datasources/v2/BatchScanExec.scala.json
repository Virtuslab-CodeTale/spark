[
  {
    "id" : "5362d6bb-9214-44a3-b6e7-a3dcc8c941e7",
    "prId" : 32921,
    "prUrl" : "https://github.com/apache/spark/pull/32921#pullrequestreview-693225668",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c68c6093-cab5-431a-ac77-d1b960de422d",
        "parentId" : null,
        "authorId" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "body" : "Calling `toBatch` again is a bit questionable. Any ideas are welcome.",
        "createdAt" : "2021-06-23T20:29:23Z",
        "updatedAt" : "2021-06-23T20:29:24Z",
        "lastEditedBy" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "tags" : [
        ]
      },
      {
        "id" : "a75c6ea5-b404-4af9-8be4-a259ff497dd3",
        "parentId" : "c68c6093-cab5-431a-ac77-d1b960de422d",
        "authorId" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "body" : "Even though I am calling toBatch one more time, I still use the original `readerFactory`.",
        "createdAt" : "2021-06-23T20:30:05Z",
        "updatedAt" : "2021-06-23T20:30:05Z",
        "lastEditedBy" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "tags" : [
        ]
      },
      {
        "id" : "d85004f4-2981-499c-9b03-b14e4b326cc6",
        "parentId" : "c68c6093-cab5-431a-ac77-d1b960de422d",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "We should update the comment in `Batch.java` since we are now calling `planInputPartitions` more than once and people might put some logic there that they assume is only run once.",
        "createdAt" : "2021-06-24T18:36:12Z",
        "updatedAt" : "2021-06-24T18:37:58Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "fb855ecb-2913-48df-8ed4-ac8c51ac1130",
        "parentId" : "c68c6093-cab5-431a-ac77-d1b960de422d",
        "authorId" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "body" : "Good point. I think the comment in `Batch` still holds as we call `planInputPartitions` on a given `Batch` only once. I guess we need to adapt the `Scan` docs to point that `toBatch` can now be called multiple times.",
        "createdAt" : "2021-06-25T20:28:19Z",
        "updatedAt" : "2021-06-25T20:28:19Z",
        "lastEditedBy" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "tags" : [
        ]
      },
      {
        "id" : "8c2af40d-9e3a-48dd-b28f-bf9b3eac78dd",
        "parentId" : "c68c6093-cab5-431a-ac77-d1b960de422d",
        "authorId" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "body" : "Added a comment to `Scan`.",
        "createdAt" : "2021-06-25T22:17:08Z",
        "updatedAt" : "2021-06-25T22:17:09Z",
        "lastEditedBy" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "tags" : [
        ]
      }
    ],
    "commit" : "881d2b2b7246d9453bfa7e074ab19334bf8d9876",
    "line" : 57,
    "diffHunk" : "@@ -1,1 +66,70 @@\n      // call toBatch again to get filtered partitions\n      val newPartitions = scan.toBatch.planInputPartitions()\n\n      originalPartitioning match {"
  },
  {
    "id" : "55aaf8d7-1ca2-482e-8fed-5973109a4da7",
    "prId" : 32921,
    "prUrl" : "https://github.com/apache/spark/pull/32921#pullrequestreview-693225810",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ede12727-e92f-44ad-bf5b-17f4998ca6ec",
        "parentId" : null,
        "authorId" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "body" : "Returning an empty RDD here.",
        "createdAt" : "2021-06-25T22:17:35Z",
        "updatedAt" : "2021-06-25T22:17:35Z",
        "lastEditedBy" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "tags" : [
        ]
      }
    ],
    "commit" : "881d2b2b7246d9453bfa7e074ab19334bf8d9876",
    "line" : 81,
    "diffHunk" : "@@ -1,1 +89,93 @@    if (filteredPartitions.isEmpty && outputPartitioning == SinglePartition) {\n      // return an empty RDD with 1 partition if dynamic filtering removed the only split\n      sparkContext.parallelize(Array.empty[InternalRow], 1)\n    } else {\n      new DataSourceRDD("
  },
  {
    "id" : "3b82befa-a85e-4e65-8523-b536e9642ae7",
    "prId" : 32921,
    "prUrl" : "https://github.com/apache/spark/pull/32921#pullrequestreview-697442675",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f6689aab-e706-4c07-95c9-86176c990f07",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Is this possible if we already check the number of partition in `originalPartitioning` must match new partition number?",
        "createdAt" : "2021-07-01T07:21:17Z",
        "updatedAt" : "2021-07-01T07:21:17Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "77beac90-d70d-4c64-bb95-17df58926ea5",
        "parentId" : "f6689aab-e706-4c07-95c9-86176c990f07",
        "authorId" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "body" : "We check the number of partitions before and after filtering match only if the source reported a specific partitioning through `SupportsReportPartitioning`. Only in that case we have `DataSourcePartitioning`. This situation, on the other hand, can happen if we inferred `SinglePartition` but the source did not report anything. ",
        "createdAt" : "2021-07-01T16:26:47Z",
        "updatedAt" : "2021-07-01T16:27:08Z",
        "lastEditedBy" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "tags" : [
        ]
      }
    ],
    "commit" : "881d2b2b7246d9453bfa7e074ab19334bf8d9876",
    "line" : 79,
    "diffHunk" : "@@ -1,1 +87,91 @@\n  override lazy val inputRDD: RDD[InternalRow] = {\n    if (filteredPartitions.isEmpty && outputPartitioning == SinglePartition) {\n      // return an empty RDD with 1 partition if dynamic filtering removed the only split\n      sparkContext.parallelize(Array.empty[InternalRow], 1)"
  }
]