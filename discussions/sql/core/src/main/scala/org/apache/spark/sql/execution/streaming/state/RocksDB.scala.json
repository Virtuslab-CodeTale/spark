[
  {
    "id" : "2738897a-1a61-48fe-bc7f-6a621e36b142",
    "prId" : 33455,
    "prUrl" : "https://github.com/apache/spark/pull/33455#pullrequestreview-713366480",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4f7a2b3b-4d46-4014-a9b0-9b5d681b2bf6",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Compared to `totalBytesReadByGet`, what `totalBytesReadThroughIterator` can tell differently? Is it important to add?",
        "createdAt" : "2021-07-21T08:22:26Z",
        "updatedAt" : "2021-07-21T08:22:26Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "e1b6de2d-9874-4cdb-8bce-5d9bdd290912",
        "parentId" : "4f7a2b3b-4d46-4014-a9b0-9b5d681b2bf6",
        "authorId" : "56016911-ce1a-4799-a941-6dac717edd44",
        "body" : "We use the iterator when processing the watermark or timeout for cleaning up the old records which reads the complete state currently. This is tracked separately from the `get` calls issued when processing the input.\r\n",
        "createdAt" : "2021-07-21T14:31:37Z",
        "updatedAt" : "2021-07-21T14:45:16Z",
        "lastEditedBy" : "56016911-ce1a-4799-a941-6dac717edd44",
        "tags" : [
        ]
      },
      {
        "id" : "bf069e5a-d287-43d5-8450-ea83bd50096b",
        "parentId" : "4f7a2b3b-4d46-4014-a9b0-9b5d681b2bf6",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Looks like two metrics track different things.",
        "createdAt" : "2021-07-23T02:53:35Z",
        "updatedAt" : "2021-07-23T02:53:36Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "3a6c42b5ca95afc8e573c2e06b03282a420d23c5",
    "line" : 52,
    "diffHunk" : "@@ -1,1 +354,358 @@      \"totalBytesWritten\" -> BYTES_WRITTEN,\n      /** The number of uncompressed bytes read from an iterator. */\n      \"totalBytesReadThroughIterator\" -> ITER_BYTES_READ,\n      /** Duration of writer requiring to wait for compaction or flush to finish. */\n      \"writerStallDuration\" -> STALL_MICROS,"
  },
  {
    "id" : "df5a5e5f-036b-48c1-b61e-0d476bd32b3e",
    "prId" : 32933,
    "prUrl" : "https://github.com/apache/spark/pull/32933#pullrequestreview-697844535",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d8c99139-6f79-44df-844a-f5534b4eb09f",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "When will we call this method?",
        "createdAt" : "2021-07-01T20:12:51Z",
        "updatedAt" : "2021-07-01T20:12:51Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "0a5a2206-73db-4c1c-9ca2-7c18b1155185",
        "parentId" : "d8c99139-6f79-44df-844a-f5534b4eb09f",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "It will be called in the `RocksDBStateStoreProvider.doMaintenace`. I'll submit the state store provider PR (the last one) today.",
        "createdAt" : "2021-07-02T05:52:22Z",
        "updatedAt" : "2021-07-02T05:52:22Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      },
      {
        "id" : "3f223f82-c951-47c2-b0cd-10dc15c7b23e",
        "parentId" : "d8c99139-6f79-44df-844a-f5534b4eb09f",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "okay.",
        "createdAt" : "2021-07-02T05:53:21Z",
        "updatedAt" : "2021-07-02T05:53:21Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "ea949836c26be0124e37df0eb641be806628d94d",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +254,258 @@  }\n\n  def cleanup(): Unit = {\n    val cleanupTime = timeTakenMs {\n      fileManager.deleteOldVersions(conf.minVersionsToRetain)"
  },
  {
    "id" : "9ae71979-63be-492f-a487-07705f5b6414",
    "prId" : 32928,
    "prUrl" : "https://github.com/apache/spark/pull/32928#pullrequestreview-693639672",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "87e0c094-ff9d-4a00-9757-e7bd952ec708",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Can these options be configured in the future?",
        "createdAt" : "2021-06-22T07:02:45Z",
        "updatedAt" : "2021-06-22T07:26:01Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "c1e9934d-43c4-4543-87e8-69aa6c49cace",
        "parentId" : "87e0c094-ff9d-4a00-9757-e7bd952ec708",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "The options from line 59-62 should not configurable since it's mainly for binding the sync mode of RocksDB. For the tableFormatConfig, I think they can be configured in the future if user have more performance requirements.",
        "createdAt" : "2021-06-28T07:34:20Z",
        "updatedAt" : "2021-06-28T07:34:20Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "21e1728cddab0fcd39886d914190a899aaac6d59",
    "line" : 62,
    "diffHunk" : "@@ -1,1 +60,64 @@  private val writeOptions = new WriteOptions().setSync(true)  // wait for batched write to complete\n  private val flushOptions = new FlushOptions().setWaitForFlush(true)  // wait for flush to complete\n  private val writeBatch = new WriteBatchWithIndex(true)  // overwrite multiple updates to a key\n\n  private val bloomFilter = new BloomFilter()"
  },
  {
    "id" : "327d9ab0-03f6-499d-980d-6525be61032f",
    "prId" : 32928,
    "prUrl" : "https://github.com/apache/spark/pull/32928#pullrequestreview-692427607",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "47299338-39cf-43ed-8659-ecb21bccf4ae",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "So it first looks at uncommitted value, if not exist, then looks for committed value?",
        "createdAt" : "2021-06-22T07:10:21Z",
        "updatedAt" : "2021-06-22T07:26:01Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "96828aff-4f11-4de4-9985-54aa690518c3",
        "parentId" : "47299338-39cf-43ed-8659-ecb21bccf4ae",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "That's right, these 2 things are done together in the `getFromBatchAndDB`. Here's part of the function description:\r\n```\r\nThis function will query both this batch and the DB and then merge\r\n the results using the DB's merge operator (if the batch contains any\r\n merge requests).\r\n```",
        "createdAt" : "2021-06-25T03:50:29Z",
        "updatedAt" : "2021-06-25T03:50:29Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "21e1728cddab0fcd39886d914190a899aaac6d59",
    "line" : 121,
    "diffHunk" : "@@ -1,1 +119,123 @@  /**\n   * Get the value for the given key if present, or null.\n   * @note This will return the last written value even if it was uncommitted.\n   */\n  def get(key: Array[Byte]): Array[Byte] = {"
  },
  {
    "id" : "3ad360a3-26b1-40fc-a70f-5a3973143b9c",
    "prId" : 32928,
    "prUrl" : "https://github.com/apache/spark/pull/32928#pullrequestreview-693709829",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ea193fe2-953a-4eae-b11e-1be2bd034f33",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "`openDB()` will close DB. This looks redundant.",
        "createdAt" : "2021-06-22T07:24:58Z",
        "updatedAt" : "2021-06-22T07:26:01Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "b6e0c255-d2ab-4f3f-9ece-38d25e7e44a7",
        "parentId" : "ea193fe2-953a-4eae-b11e-1be2bd034f33",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "We're modifying workingDir so looks safer to close DB first before modifying the dir.",
        "createdAt" : "2021-06-28T05:35:35Z",
        "updatedAt" : "2021-06-28T08:17:13Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "0e5e18a7-ae8c-4a3c-8048-5a9f286bff0d",
        "parentId" : "ea193fe2-953a-4eae-b11e-1be2bd034f33",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "Make sense, I delete the closeDB in `openDB` in f928820",
        "createdAt" : "2021-06-28T08:50:56Z",
        "updatedAt" : "2021-06-28T08:50:56Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "21e1728cddab0fcd39886d914190a899aaac6d59",
    "line" : 102,
    "diffHunk" : "@@ -1,1 +100,104 @@    try {\n      if (loadedVersion != version) {\n        closeDB()\n        val metadata = fileManager.loadCheckpointFromDfs(version, workingDir)\n        openDB()"
  },
  {
    "id" : "99a632e6-b560-4a5d-b495-43300ab9a2ba",
    "prId" : 32928,
    "prUrl" : "https://github.com/apache/spark/pull/32928#pullrequestreview-694537317",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "924a7b6e-e7ee-49ae-9c7b-31f4e0c08063",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "~After we re-throw this, `loadedVersion` is used for some purpose in the upper layer? I'm wondering if this is recoverable error and `loadedVersion` is a valuable information to be used later in that case.~ Never mind.",
        "createdAt" : "2021-06-29T02:29:06Z",
        "updatedAt" : "2021-06-29T02:33:13Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "21e1728cddab0fcd39886d914190a899aaac6d59",
    "line" : 114,
    "diffHunk" : "@@ -1,1 +112,116 @@      case t: Throwable =>\n        loadedVersion = -1  // invalidate loaded data\n        throw t\n    }\n    this"
  },
  {
    "id" : "1b2acc74-a9f0-4040-8ba3-8764e82cba68",
    "prId" : 32928,
    "prUrl" : "https://github.com/apache/spark/pull/32928#pullrequestreview-694636963",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2527750e-94e3-4cb4-94a0-c4caa55b6c66",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "If we want to do logging only at all `Exception`, `Utils.tryLogNonFatalError` might be better.",
        "createdAt" : "2021-06-29T02:47:06Z",
        "updatedAt" : "2021-06-29T02:47:06Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "ebdfdcc7-c9cd-4d47-8e80-390efceceb2f",
        "parentId" : "2527750e-94e3-4cb4-94a0-c4caa55b6c66",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "Thanks for the advice. But since NonFatal doesn't include `InterruptedException` which is also an Exception, maybe we'd better not change to use `Utils.tryLogNonFatalError` here.",
        "createdAt" : "2021-06-29T06:33:00Z",
        "updatedAt" : "2021-06-29T06:33:00Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "21e1728cddab0fcd39886d914190a899aaac6d59",
    "line" : 272,
    "diffHunk" : "@@ -1,1 +270,274 @@    } catch {\n      case e: Exception =>\n        logWarning(\"Error closing RocksDB\", e)\n    }\n  }"
  },
  {
    "id" : "84ff5110-d6d9-4c8c-9f90-cd8f9547b423",
    "prId" : 32928,
    "prUrl" : "https://github.com/apache/spark/pull/32928#pullrequestreview-694637743",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "885b8d9f-8bd1-41ff-89cc-802de36e5fcb",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "nit. Could you add `@GuardedBy(\"acquireLock\")`?",
        "createdAt" : "2021-06-29T02:56:31Z",
        "updatedAt" : "2021-06-29T02:56:31Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "00720f17-baf0-4221-8188-cdba352fc998",
        "parentId" : "885b8d9f-8bd1-41ff-89cc-802de36e5fcb",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "Thanks, done in the next commit.",
        "createdAt" : "2021-06-29T06:34:20Z",
        "updatedAt" : "2021-06-29T06:34:21Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "21e1728cddab0fcd39886d914190a899aaac6d59",
    "line" : 89,
    "diffHunk" : "@@ -1,1 +87,91 @@\n  @GuardedBy(\"acquireLock\")\n  @volatile private var acquiredThreadInfo: AcquiredThreadInfo = _\n\n  /**"
  },
  {
    "id" : "691b88e4-53e8-48ad-8025-5b1d7b9ce903",
    "prId" : 32928,
    "prUrl" : "https://github.com/apache/spark/pull/32928#pullrequestreview-694654638",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "485b82e3-ea46-4552-a54b-3057d8e39b48",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Note that we should document these configs at the end.",
        "createdAt" : "2021-06-29T05:55:10Z",
        "updatedAt" : "2021-06-29T05:55:10Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "88c34bec-7e52-418f-b7d0-df389249a502",
        "parentId" : "485b82e3-ea46-4552-a54b-3057d8e39b48",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "Yea, I planned to document all the config for RocksDB in the user guide. If we have the configurable requirement in the future, people can easily be referenced.",
        "createdAt" : "2021-06-29T06:57:56Z",
        "updatedAt" : "2021-06-29T06:57:57Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "21e1728cddab0fcd39886d914190a899aaac6d59",
    "line" : 408,
    "diffHunk" : "@@ -1,1 +406,410 @@  private val BLOCK_SIZE_KB_CONF = ConfEntry(\"blockSizeKB\", \"4\")\n  private val BLOCK_CACHE_SIZE_MB_CONF = ConfEntry(\"blockCacheSizeMB\", \"8\")\n  private val LOCK_ACQUIRE_TIMEOUT_MS_CONF = ConfEntry(\"lockAcquireTimeoutMs\", \"60000\")\n\n  def apply(storeConf: StateStoreConf): RocksDBConf = {"
  }
]