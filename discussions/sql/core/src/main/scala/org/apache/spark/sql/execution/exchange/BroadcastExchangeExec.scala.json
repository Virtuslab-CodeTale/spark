[
  {
    "id" : "45d6f838-c6b3-46b6-912c-0e9938ee4502",
    "prId" : 33058,
    "prUrl" : "https://github.com/apache/spark/pull/33058#pullrequestreview-692465660",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2de5be31-a187-40c5-8e3b-3de95c9f78bd",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "This is kind of the AQE version of \"execute\", as AQE won't call `execute` of shuffle/broadcast.",
        "createdAt" : "2021-06-24T12:39:30Z",
        "updatedAt" : "2021-06-24T12:39:30Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "d26dc1d7-2718-49f5-8006-6ae5c6e6c960",
        "parentId" : "2de5be31-a187-40c5-8e3b-3de95c9f78bd",
        "authorId" : "1b84a7ff-6bf9-4417-bf9f-e46e997e5974",
        "body" : "Can we add some comments for this method?",
        "createdAt" : "2021-06-25T05:40:50Z",
        "updatedAt" : "2021-06-25T05:40:50Z",
        "lastEditedBy" : "1b84a7ff-6bf9-4417-bf9f-e46e997e5974",
        "tags" : [
        ]
      }
    ],
    "commit" : "6a7e388cc221a1c0b4f8709c53c7cadcbb3c3450",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +60,64 @@   * It also does the preparations work, such as waiting for the subqueries.\n   */\n  final def submitBroadcastJob: scala.concurrent.Future[broadcast.Broadcast[Any]] = executeQuery {\n    completionFuture\n  }"
  },
  {
    "id" : "7ff1b576-28ff-44e1-b709-c8c27101982d",
    "prId" : 32911,
    "prUrl" : "https://github.com/apache/spark/pull/32911#pullrequestreview-684315252",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ca0b21d5-9eb8-499b-ab58-a3f7fb28c2b9",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Could you add some comments about this condition, @c21 ?",
        "createdAt" : "2021-06-15T15:56:53Z",
        "updatedAt" : "2021-06-15T15:56:54Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "14bfab3e-d3cb-4895-86be-16189c900002",
        "parentId" : "ca0b21d5-9eb8-499b-ab58-a3f7fb28c2b9",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "@dongjoon-hyun - sure, added.",
        "createdAt" : "2021-06-15T17:45:54Z",
        "updatedAt" : "2021-06-15T17:46:06Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      }
    ],
    "commit" : "01652b5e2abe7ed6532c25f5acb43580c8043619",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +112,116 @@      // NOTE: LongHashedRelation is used for single key with LongType. This should be kept\n      // consistent with HashedRelation.apply.\n      if !(key.length == 1 && key.head.dataType == LongType) =>\n      // Since the maximum number of keys that BytesToBytesMap supports is 1 << 29,\n      // and only 70% of the slots can be used before growing in UnsafeHashedRelation,"
  },
  {
    "id" : "a8f0b0bb-f3eb-4d76-a2b2-53a8ec412e61",
    "prId" : 32544,
    "prUrl" : "https://github.com/apache/spark/pull/32544#pullrequestreview-659993330",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0385ac5e-f1d7-4e2e-85ec-818297c5ffaa",
        "parentId" : null,
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "sorry why `8GB` is not accurate? `BroadcastExchangeExec.MAX_BROADCAST_TABLE_BYTES` seems to be set statically as `8L << 30`, right?",
        "createdAt" : "2021-05-14T05:33:44Z",
        "updatedAt" : "2021-05-14T05:33:49Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      },
      {
        "id" : "1097a2dc-2ff9-4511-8d1d-263ea6ed6e00",
        "parentId" : "0385ac5e-f1d7-4e2e-85ec-818297c5ffaa",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "It's not currently inaccurate, but if that constant changed values, it would be",
        "createdAt" : "2021-05-14T16:14:43Z",
        "updatedAt" : "2021-05-14T16:14:44Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      }
    ],
    "commit" : "a1ee451155074e2c3c0e534d908eaa24aa38093d",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +143,147 @@              throw new SparkException(\n                s\"Cannot broadcast the table that is larger than\" +\n                  s\" ${MAX_BROADCAST_TABLE_BYTES >> 30}GB: ${dataSize >> 30} GB\")\n            }\n"
  },
  {
    "id" : "15e6f691-956e-4b78-bd5b-c766a4f314a0",
    "prId" : 31119,
    "prUrl" : "https://github.com/apache/spark/pull/31119#pullrequestreview-570157158",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c0f646ad-a63c-4382-b3f5-02b4dea8b139",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "After a second thought, I think this is risky. It's possible that in a non-STS environment, users set job group id manually, and run some long-running jobs. If we capture the job group id here in broadcast exchange, when the broadcast timeout, it will cancel the whole job group which may kill the user's other long-running jobs unexpectedly.\r\n\r\nI think we need to revisit the STS's SQL statement canceling feature. We should use SQL execution ID to find out all the jobs of a SQL query, and assign a unique job group id to them.",
        "createdAt" : "2021-01-15T07:19:10Z",
        "updatedAt" : "2021-01-15T07:19:10Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "fb3aa87c-0604-4368-985d-7fd568a65e3b",
        "parentId" : "c0f646ad-a63c-4382-b3f5-02b4dea8b139",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "@LantaoJin what do you think?",
        "createdAt" : "2021-01-15T07:19:28Z",
        "updatedAt" : "2021-01-15T07:19:28Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "04350035-e3f7-4948-9936-8d09c2fac869",
        "parentId" : "c0f646ad-a63c-4382-b3f5-02b4dea8b139",
        "authorId" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "body" : "@cloud-fan yes, the case you said is a problem in current implementation. I will give a new PR. Revert this first?",
        "createdAt" : "2021-01-15T08:05:55Z",
        "updatedAt" : "2021-01-15T08:05:55Z",
        "lastEditedBy" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "tags" : [
        ]
      },
      {
        "id" : "f1155704-2cba-4db4-894b-7f8a8d7c5252",
        "parentId" : "c0f646ad-a63c-4382-b3f5-02b4dea8b139",
        "authorId" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "body" : "Job group id is still a basic API which used to cancel the a group of jobs (depends on custom business). In a non-STS environment, users can set job group id manually, and run some long-running jobs. In some cases, such as a custom exception, user want to cancel all jobs with the same job group. And broadcast timeout shouldn't use job group Id to can broadcast job.",
        "createdAt" : "2021-01-15T08:31:07Z",
        "updatedAt" : "2021-01-15T08:31:07Z",
        "lastEditedBy" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "tags" : [
        ]
      },
      {
        "id" : "66ee87ae-9b77-4b2c-9d51-4ded88bebe1e",
        "parentId" : "c0f646ad-a63c-4382-b3f5-02b4dea8b139",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Let me revert this first. Please let me know when you have a new fix, thanks!",
        "createdAt" : "2021-01-15T13:36:23Z",
        "updatedAt" : "2021-01-15T13:36:33Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "ded89999-82a8-465a-9961-d58a6578dcf4",
        "parentId" : "c0f646ad-a63c-4382-b3f5-02b4dea8b139",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "How about exposing a conf to users and let them determine whether to inherit the global jobGroupId for the internally generated jobs or not",
        "createdAt" : "2021-01-18T02:00:27Z",
        "updatedAt" : "2021-01-18T02:00:27Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "4a94d19d-c285-4b46-a851-bd1d03023286",
        "parentId" : "c0f646ad-a63c-4382-b3f5-02b4dea8b139",
        "authorId" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "body" : "> How about exposing a conf to users and let them determine whether to inherit the global jobGroupId for the internally generated jobs or not\r\n\r\nCould you explain more? Does it need users to modify their exist applications?",
        "createdAt" : "2021-01-18T02:09:53Z",
        "updatedAt" : "2021-01-18T02:09:53Z",
        "lastEditedBy" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "tags" : [
        ]
      },
      {
        "id" : "3f6e6658-69b4-4898-9fdd-3c6f49044d13",
        "parentId" : "c0f646ad-a63c-4382-b3f5-02b4dea8b139",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "W/ a conf(maybe named `spark.jobGroubID.inherited`) to decide whether the `runId` is re-generated or inherited from the former specified one. Users may develop applications like ThriftServer in C/S architecture as a server-like spark program.",
        "createdAt" : "2021-01-18T02:19:36Z",
        "updatedAt" : "2021-01-18T02:19:37Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "08bb7127-ffe3-437b-b37a-af3c9ce586c4",
        "parentId" : "c0f646ad-a63c-4382-b3f5-02b4dea8b139",
        "authorId" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "body" : "Sorry, I don't get the point. If a user set `spark.jobGroubID.inherited` to true and set a custom jobGroupId to a UUID value, when the broadcast timeout, what's behavior?",
        "createdAt" : "2021-01-18T02:35:15Z",
        "updatedAt" : "2021-01-18T02:35:15Z",
        "lastEditedBy" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "tags" : [
        ]
      },
      {
        "id" : "774e064d-6758-42c8-bfa3-c2dd62f4efd1",
        "parentId" : "c0f646ad-a63c-4382-b3f5-02b4dea8b139",
        "authorId" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "body" : "Do you mean this?\r\n```scala\r\n  override val runId: UUID =\r\n    if (SQLConf.get.getConf(spark.jobGroubID.inherited)) {\r\n       UUID.randomUUID\r\n    } else {\r\n      UUID.fromString(sparkContext.getLocalProperty(SparkContext.SPARK_JOB_GROUP_ID))\r\n    }\r\n```",
        "createdAt" : "2021-01-18T02:39:23Z",
        "updatedAt" : "2021-01-18T02:39:23Z",
        "lastEditedBy" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "tags" : [
        ]
      },
      {
        "id" : "f4aebd2e-e8f3-466d-b3ac-10271c19820a",
        "parentId" : "c0f646ad-a63c-4382-b3f5-02b4dea8b139",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "yea, something like this",
        "createdAt" : "2021-01-18T02:46:54Z",
        "updatedAt" : "2021-01-18T02:46:54Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "ef5814eb-956e-45a9-8187-ed57de4a418f",
        "parentId" : "c0f646ad-a63c-4382-b3f5-02b4dea8b139",
        "authorId" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "body" : "Ok, I know. To be transparent to users, how about add a new thread local property `SparkContext.SPARK_RESERVED_JOB_GROUP_ID` or `SPARK_THRIFTSERVER_JOB_GROUP_ID` to separate it.\r\n```\r\ndiff --git a/core/src/main/scala/org/apache/spark/SparkContext.scala b/core/src/main/scala/org/apache/spark/SparkContext.scala\r\nindex f6e8a5694d..cc3efed713 100644\r\n--- a/core/src/main/scala/org/apache/spark/SparkContext.scala\r\n+++ b/core/src/main/scala/org/apache/spark/SparkContext.scala\r\n@@ -760,9 +760,13 @@ class SparkContext(config: SparkConf) extends Logging {\r\n    * may respond to Thread.interrupt() by marking nodes as dead.\r\n    */\r\n   def setJobGroup(groupId: String,\r\n-      description: String, interruptOnCancel: Boolean = false): Unit = {\r\n+      description: String, interruptOnCancel: Boolean = false, reserved: Boolean = false): Unit = {\r\n     setLocalProperty(SparkContext.SPARK_JOB_DESCRIPTION, description)\r\n-    setLocalProperty(SparkContext.SPARK_JOB_GROUP_ID, groupId)\r\n+    if (reserved) {\r\n+      setLocalProperty(SparkContext.SPARK_RESERVED_JOB_GROUP_ID, groupId)\r\n+    } else {\r\n+      setLocalProperty(SparkContext.SPARK_JOB_GROUP_ID, groupId)\r\n+    }\r\n     // Note: Specifying interruptOnCancel in setJobGroup (rather than cancelJobGroup) avoids\r\n     // changing several public APIs and allows Spark cancellations outside of the cancelJobGroup\r\n     // APIs to also take advantage of this property (e.g., internal job failures or canceling from\r\n@@ -2760,6 +2764,7 @@ object SparkContext extends Logging {\r\n\r\n   private[spark] val SPARK_JOB_DESCRIPTION = \"spark.job.description\"\r\n   private[spark] val SPARK_JOB_GROUP_ID = \"spark.jobGroup.id\"\r\n+  private[spark] val SPARK_RESERVED_JOB_GROUP_ID = \"spark.reservedJobGroup.id\"\r\n   private[spark] val SPARK_JOB_INTERRUPT_ON_CANCEL = \"spark.job.interruptOnCancel\"\r\n   private[spark] val SPARK_SCHEDULER_POOL = \"spark.scheduler.pool\"\r\n   private[spark] val RDD_SCOPE_KEY = \"spark.rdd.scope\r\ndiff --git a/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/BroadcastExchangeExec.scala b/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/BroadcastExchangeExec.scala\r\nindex c322d5eef5..25abb4f2d3 100644\r\n--- a/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/BroadcastExchangeExec.scala\r\n+++ b/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/BroadcastExchangeExec.scala\r\n@@ -76,7 +76,8 @@ case class BroadcastExchangeExec(\r\n\r\n   // Cancelling a SQL statement from Spark ThriftServer needs to cancel\r\n   // its related broadcast sub-jobs. So set the run id to job group id if exists.\r\n-  override val runId: UUID = Option(sparkContext.getLocalProperty(SparkContext.SPARK_JOB_GROUP_ID))\r\n+  override val runId: UUID =\r\n+    Option(sparkContext.getLocalProperty(SparkContext.SPARK_RESERVED_JOB_GROUP_ID))\r\n       .map(UUID.fromString).getOrElse(UUID.randomUUID)\r\n\r\n   override lazy val metrics = Map(\r\ndiff --git a/sql/hive-thriftserver/src/main/scala/org/apache/spark/sql/hive/thriftserver/SparkExecuteStatementOperation.scala b/sql/hive-thriftserver/src/main/scala/org/apache/spark/sql/hive/thriftserver/SparkExecuteStatementOperation.scala\r\nindex 8ca0ab91a7..4db50e8d00 100644\r\n--- a/sql/hive-thriftserver/src/main/scala/org/apache/spark/sql/hive/thriftserver/SparkExecuteStatementOperation.scala\r\n+++ b/sql/hive-thriftserver/src/main/scala/org/apache/spark/sql/hive/thriftserver/SparkExecuteStatementOperation.scala\r\n@@ -286,7 +286,7 @@ private[hive] class SparkExecuteStatementOperation(\r\n         parentSession.getSessionState.getConf.setClassLoader(executionHiveClassLoader)\r\n       }\r\n\r\n-      sqlContext.sparkContext.setJobGroup(statementId, substitutorStatement, forceCancel)\r\n+      sqlContext.sparkContext.setJobGroup(statementId, substitutorStatement, forceCancel, true)\r\n       result = sqlContext.sql(statement)\r\n       logDebug(result.queryExecution.toString())\r\n       HiveThriftServer2.eventManager.onStatementParsed(statementId,\r\n```",
        "createdAt" : "2021-01-18T03:58:48Z",
        "updatedAt" : "2021-01-18T03:58:48Z",
        "lastEditedBy" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "tags" : [
        ]
      }
    ],
    "commit" : "b3f1453925dd8ba0bbb43b9f3d0b701e7c6f9507",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +77,81 @@  // Cancelling a SQL statement from Spark ThriftServer needs to cancel\n  // its related broadcast sub-jobs. So set the run id to job group id if exists.\n  override val runId: UUID = Option(sparkContext.getLocalProperty(SparkContext.SPARK_JOB_GROUP_ID))\n      .map(UUID.fromString).getOrElse(UUID.randomUUID)\n"
  },
  {
    "id" : "55a73fdf-68fd-4939-83e0-497317a58969",
    "prId" : 30039,
    "prUrl" : "https://github.com/apache/spark/pull/30039#pullrequestreview-508331588",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7f9e6176-b446-41d0-83bd-fa832c73832c",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "match `Statistics.rowCount`",
        "createdAt" : "2020-10-14T13:14:21Z",
        "updatedAt" : "2020-10-14T13:14:21Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "812c856ab1aabd677549967d5a0df407c3fca4c5",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +92,96 @@  override def runtimeStatistics: Statistics = {\n    val dataSize = metrics(\"dataSize\").value\n    val rowCount = metrics(\"numOutputRows\").value\n    Statistics(dataSize, Some(rowCount))\n  }"
  },
  {
    "id" : "38d7ad33-3f3c-4f86-9c78-7c052ca20d80",
    "prId" : 30039,
    "prUrl" : "https://github.com/apache/spark/pull/30039#pullrequestreview-508331972",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9a27f3a8-eb57-4386-a9b3-cfd9964e1992",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "always set the metrics even if failure happens.",
        "createdAt" : "2020-10-14T13:14:47Z",
        "updatedAt" : "2020-10-14T13:14:47Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "812c856ab1aabd677549967d5a0df407c3fca4c5",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +117,121 @@            // Use executeCollect/executeCollectIterator to avoid conversion to Scala types\n            val (numRows, input) = child.executeCollectIterator()\n            longMetric(\"numOutputRows\") += numRows\n            if (numRows >= MAX_BROADCAST_TABLE_ROWS) {\n              throw new SparkException("
  },
  {
    "id" : "24376b19-7520-455c-b96b-063d67bddd3b",
    "prId" : 29904,
    "prUrl" : "https://github.com/apache/spark/pull/29904#pullrequestreview-499974824",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "76f69d88-1e4c-40ae-9e99-5ebdcbed6006",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "It seems the PR description says nothing about this update, so could you describe it there?",
        "createdAt" : "2020-09-30T14:30:38Z",
        "updatedAt" : "2020-09-30T14:30:38Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "7f7aa442-eadc-4513-9c0e-80a65a5cca7b",
        "parentId" : "76f69d88-1e4c-40ae-9e99-5ebdcbed6006",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "Updated it to `This pr adds numRows to the metric and runtimeStatistics of BroadcastExchangeExec.`",
        "createdAt" : "2020-10-01T02:44:13Z",
        "updatedAt" : "2020-10-01T02:44:14Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      }
    ],
    "commit" : "7084bd1e80ce74cae6a7807af771b96be19b5deb",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +93,97 @@    val dataSize = metrics(\"dataSize\").value\n    val numRows = metrics(\"numRows\").value\n    Statistics(dataSize, Some(numRows))\n  }\n"
  },
  {
    "id" : "5390a363-7c7e-4036-a21c-1c277b855594",
    "prId" : 24706,
    "prUrl" : "https://github.com/apache/spark/pull/24706#pullrequestreview-243995120",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "27368e96-4b7e-4da6-a0da-7ac27096f808",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "can we just change this to scala future?",
        "createdAt" : "2019-05-30T18:18:49Z",
        "updatedAt" : "2019-06-15T01:12:56Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "59888482-c421-4965-bc40-fa115f531933",
        "parentId" : "27368e96-4b7e-4da6-a0da-7ac27096f808",
        "authorId" : "80d631a6-73e8-46a6-a01b-a80d1f1cc6cc",
        "body" : "Scala future is not cancellable :(",
        "createdAt" : "2019-05-30T18:26:09Z",
        "updatedAt" : "2019-06-15T01:12:56Z",
        "lastEditedBy" : "80d631a6-73e8-46a6-a01b-a80d1f1cc6cc",
        "tags" : [
        ]
      },
      {
        "id" : "f8e0633e-a3c7-4b53-9f3e-9aace46aa1aa",
        "parentId" : "27368e96-4b7e-4da6-a0da-7ac27096f808",
        "authorId" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "body" : "Yeah, just to include more info in our offline discussion here:\r\n\r\nWhat is needed by AQE is a \"cancellable\" Future with callbacks.\r\n\r\nJava Future is cancellable, but has no callbacks.\r\nScala Future has callbacks, but is not cancellable.\r\nThe Spark impl of a cancellable Scala Future (with callbacks), namely `FutureAction`, is more specific for Spark jobs than for general purpose, thus not so friendly to `BroadcastExchangeExec` here.\r\n\r\nLooks like an option can be to implement a general purpose `FutureAction` to replace the Java Future in `BroadcastExchangeExec`. Let's just mark this a follow-up though.",
        "createdAt" : "2019-05-30T19:52:01Z",
        "updatedAt" : "2019-06-15T01:12:56Z",
        "lastEditedBy" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "tags" : [
        ]
      }
    ],
    "commit" : "e2651049c408f211b225f40c93adf5b741b14eb4",
    "line" : 37,
    "diffHunk" : "@@ -1,1 +73,77 @@\n  @transient\n  private[sql] lazy val relationFuture: Future[broadcast.Broadcast[Any]] = {\n    // relationFuture is used in \"doExecute\". Therefore we can get the execution id correctly here.\n    val executionId = sparkContext.getLocalProperty(SQLExecution.EXECUTION_ID_KEY)"
  },
  {
    "id" : "e47e1bf1-2700-4e56-b472-0edc277bd1b6",
    "prId" : 24595,
    "prUrl" : "https://github.com/apache/spark/pull/24595#pullrequestreview-706941369",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7a65a2a3-a08a-48b0-b249-44e5634861a4",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "let's add a comment to explain why we set up a job group here. There is no other public API that can cancel a specific job AFAIK.",
        "createdAt" : "2019-05-15T03:05:12Z",
        "updatedAt" : "2019-05-15T18:20:20Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "d0ecf4f2-9f4c-4643-ac22-36b875145346",
        "parentId" : "7a65a2a3-a08a-48b0-b249-44e5634861a4",
        "authorId" : "88f0c774-9c59-4485-885d-f6aee36efcea",
        "body" : "Just curious, why can't we just inherit the job group id of the outside thread so that when the SQL statement was cancelled, these broadcast sub-jobs can be cancelled as a whole?",
        "createdAt" : "2020-10-23T08:15:48Z",
        "updatedAt" : "2020-10-23T08:18:16Z",
        "lastEditedBy" : "88f0c774-9c59-4485-885d-f6aee36efcea",
        "tags" : [
        ]
      },
      {
        "id" : "38c08d42-9634-4a2d-8122-d8496a7527db",
        "parentId" : "7a65a2a3-a08a-48b0-b249-44e5634861a4",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "That sounds like a good idea. We should only set the job group if there is no one outside.",
        "createdAt" : "2021-07-12T02:48:59Z",
        "updatedAt" : "2021-07-12T02:49:00Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "e4c6b392-a282-46e4-bf6b-a318d2c904a9",
        "parentId" : "7a65a2a3-a08a-48b0-b249-44e5634861a4",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Wouldn't that cancelling the broadcast job cause the outer main job to cancel?",
        "createdAt" : "2021-07-12T02:50:47Z",
        "updatedAt" : "2021-07-12T02:50:47Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "b0ae1b35-c726-482c-833b-f2ae1ab9dec2",
        "parentId" : "7a65a2a3-a08a-48b0-b249-44e5634861a4",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "> We should only set the job group if there is no one outside.\r\n\r\nand I guess it would be a partial fix?",
        "createdAt" : "2021-07-12T02:51:56Z",
        "updatedAt" : "2021-07-12T02:51:56Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "5bcffb7d-2d9c-410d-ae1d-2bec692339a1",
        "parentId" : "7a65a2a3-a08a-48b0-b249-44e5634861a4",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I rechecked the code, we will cancel the broadcast if a timeout is hit, and we shouldn't cancel the main job there. We need a way to only cancel the jobs of the broadcast.",
        "createdAt" : "2021-07-12T02:55:07Z",
        "updatedAt" : "2021-07-12T02:55:07Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "fc2a97b0-f65f-4c6f-9bd8-f93c8ffb7e4b",
        "parentId" : "7a65a2a3-a08a-48b0-b249-44e5634861a4",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "seems like the problem is, we didn't set the job group id back to its original value after broadcast is done?",
        "createdAt" : "2021-07-12T02:56:53Z",
        "updatedAt" : "2021-07-12T02:56:53Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "a8dda957-8186-40b7-b04a-c3fa457ef95c",
        "parentId" : "7a65a2a3-a08a-48b0-b249-44e5634861a4",
        "authorId" : "88f0c774-9c59-4485-885d-f6aee36efcea",
        "body" : "Can we somehow encode the fact that this RDD is a broadcast job in the jobid/jobgroupid?",
        "createdAt" : "2021-07-12T10:33:03Z",
        "updatedAt" : "2021-07-12T10:33:03Z",
        "lastEditedBy" : "88f0c774-9c59-4485-885d-f6aee36efcea",
        "tags" : [
        ]
      },
      {
        "id" : "d7145fd0-da78-42df-ba93-ff92320ca963",
        "parentId" : "7a65a2a3-a08a-48b0-b249-44e5634861a4",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I think that's hacky though. To fix it properly, we should either have multiple job group ids or job group hierarchy. ",
        "createdAt" : "2021-07-12T10:39:51Z",
        "updatedAt" : "2021-07-12T10:39:51Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "c39d9d9d-9813-4c5b-a97d-47061c22c70f",
        "parentId" : "7a65a2a3-a08a-48b0-b249-44e5634861a4",
        "authorId" : "88f0c774-9c59-4485-885d-f6aee36efcea",
        "body" : "I agree that we should have hierarchical job group.",
        "createdAt" : "2021-07-12T10:41:30Z",
        "updatedAt" : "2021-07-12T10:41:30Z",
        "lastEditedBy" : "88f0c774-9c59-4485-885d-f6aee36efcea",
        "tags" : [
        ]
      },
      {
        "id" : "1c75e714-4382-40e5-b401-36b489875282",
        "parentId" : "7a65a2a3-a08a-48b0-b249-44e5634861a4",
        "authorId" : "afcafcea-ead3-4649-a4d8-74eb9dfd1a22",
        "body" : "My idea is to leave the choice to the user. Since cancelling the broadcast job is an optimization method, and the UI is also user-oriented, it is better to add a configuration item, such as: \r\n`spark.broadcast.job.cancelling.enabled`\r\n When it is true, it supports cancelling the broadcast job. The default is false. By default,  if the user sets up job group and job description, he will not be surprised by the UI. If the user wants to optimize the broadcast job, he will not be surprised by the UI too. We can make it clear in the configuration doc. Mutiple job group can also achieve the goal, but I am worried that it will be more disruptive to program development. Some old users may not know how to set up mutiple job group without viewing the source code.In addition, multiple job group will bring more workload and higher error probability.",
        "createdAt" : "2021-07-14T02:34:36Z",
        "updatedAt" : "2021-07-14T02:49:29Z",
        "lastEditedBy" : "afcafcea-ead3-4649-a4d8-74eb9dfd1a22",
        "tags" : [
        ]
      },
      {
        "id" : "762f22b8-3e87-4138-bb97-6035d24f6d2e",
        "parentId" : "7a65a2a3-a08a-48b0-b249-44e5634861a4",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "It's always better to have fewer configs if possible. And I don't think we can override the job group id here if the config is true, as this is used to cancel broadcast after timeout.",
        "createdAt" : "2021-07-14T02:59:32Z",
        "updatedAt" : "2021-07-14T02:59:32Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "7706601a-38a9-49d6-98fb-c722368f8d89",
        "parentId" : "7a65a2a3-a08a-48b0-b249-44e5634861a4",
        "authorId" : "afcafcea-ead3-4649-a4d8-74eb9dfd1a22",
        "body" : "@HyukjinKwon  @jiangxb1987 @yeshengm Whatâ€˜s your opinion of my idea?",
        "createdAt" : "2021-07-15T02:52:58Z",
        "updatedAt" : "2021-07-15T02:53:03Z",
        "lastEditedBy" : "afcafcea-ead3-4649-a4d8-74eb9dfd1a22",
        "tags" : [
        ]
      },
      {
        "id" : "f6769548-1260-4d20-ac7f-18f58be8dd60",
        "parentId" : "7a65a2a3-a08a-48b0-b249-44e5634861a4",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I am discussing about multiple job group support which will fundamentally fix all these problems. This is actually a general problem that's not speicfic to SQL broadcast here only.",
        "createdAt" : "2021-07-15T03:12:06Z",
        "updatedAt" : "2021-07-15T03:12:06Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "e64730d2-6b21-44a3-873d-ee947e76ccc0",
        "parentId" : "7a65a2a3-a08a-48b0-b249-44e5634861a4",
        "authorId" : "afcafcea-ead3-4649-a4d8-74eb9dfd1a22",
        "body" : "@HyukjinKwon Could you please tell me where you are discussing? I also want to make a little contribution.",
        "createdAt" : "2021-07-15T05:21:16Z",
        "updatedAt" : "2021-07-15T05:21:25Z",
        "lastEditedBy" : "afcafcea-ead3-4649-a4d8-74eb9dfd1a22",
        "tags" : [
        ]
      },
      {
        "id" : "9a04e2f2-cdd9-4de9-8849-33759fcd64d6",
        "parentId" : "7a65a2a3-a08a-48b0-b249-44e5634861a4",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "sorry I am discussing offline first. I will send out an email or JIRA soon for more open discussion soon.",
        "createdAt" : "2021-07-15T05:38:08Z",
        "updatedAt" : "2021-07-15T05:38:17Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "5d82dd7aad830d36e24f9943f9d566fbc70b93aa",
    "line" : 83,
    "diffHunk" : "@@ -1,1 +73,77 @@          try {\n            // Setup a job group here so later it may get cancelled by groupId if necessary.\n            sparkContext.setJobGroup(runId.toString, s\"broadcast exchange (runId $runId)\",\n              interruptOnCancel = true)\n            val beforeCollect = System.nanoTime()"
  }
]