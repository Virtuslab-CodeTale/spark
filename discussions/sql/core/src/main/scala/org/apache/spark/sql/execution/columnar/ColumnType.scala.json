[
  {
    "id" : "34998558-d88c-4c93-ab99-9710732b1e4b",
    "prId" : 26699,
    "prUrl" : "https://github.com/apache/spark/pull/26699#pullrequestreview-329624097",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b20f37f7-f87c-4f74-8e04-7a4cc462a875",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "if the length is fixed (16), can we skip it?",
        "createdAt" : "2019-12-10T06:43:09Z",
        "updatedAt" : "2019-12-14T11:39:59Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "5317c6ac-4791-4e5d-a8d0-88704308879b",
        "parentId" : "b20f37f7-f87c-4f74-8e04-7a4cc462a875",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "this is needed for `MutableUnsafeRow`",
        "createdAt" : "2019-12-10T07:03:27Z",
        "updatedAt" : "2019-12-14T11:39:59Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "ac682ea1-3edc-4665-93a9-a667722ac0bc",
        "parentId" : "b20f37f7-f87c-4f74-8e04-7a4cc462a875",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "please check here https://github.com/apache/spark/blob/e1ea806b3075d279b5f08a29fe4c1ad6d3c4191a/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/ColumnType.scala#L411",
        "createdAt" : "2019-12-10T07:06:48Z",
        "updatedAt" : "2019-12-14T11:39:59Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "9e17f31a-de5b-48c8-bca0-1f15e6d7802f",
        "parentId" : "b20f37f7-f87c-4f74-8e04-7a4cc462a875",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "Or we can implement `extract` directly. ",
        "createdAt" : "2019-12-10T07:08:30Z",
        "updatedAt" : "2019-12-14T11:39:59Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "c5a7374671070ae2f9371895d3902d560aa76ee6",
    "line" : 37,
    "diffHunk" : "@@ -1,1 +730,734 @@\n  override def append(v: CalendarInterval, buffer: ByteBuffer): Unit = {\n    ByteBufferHelper.putInt(buffer, 16)\n    ByteBufferHelper.putInt(buffer, v.months)\n    ByteBufferHelper.putInt(buffer, v.days)"
  },
  {
    "id" : "f998de23-29a3-4069-ada2-4d00c4de1623",
    "prId" : 26699,
    "prUrl" : "https://github.com/apache/spark/pull/26699#pullrequestreview-332181233",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9b956498-47c6-4201-b169-8931dee1b7c0",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "We need to override `actualSize`.",
        "createdAt" : "2019-12-14T00:47:50Z",
        "updatedAt" : "2019-12-14T11:39:59Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "c5a7374671070ae2f9371895d3902d560aa76ee6",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +711,715 @@  override def dataType: DataType = CalendarIntervalType\n\n  override def defaultSize: Int = 16\n\n  override def actualSize(row: InternalRow, ordinal: Int): Int = 20"
  },
  {
    "id" : "c3bfcd7f-253a-49dd-a352-c0415b0dae98",
    "prId" : 26699,
    "prUrl" : "https://github.com/apache/spark/pull/26699#pullrequestreview-332361422",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9ba23fb1-24c1-442e-b2db-aa07e64d2b3d",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "why actual size is different from default size?",
        "createdAt" : "2019-12-16T06:35:13Z",
        "updatedAt" : "2019-12-16T06:35:14Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "c5a7374671070ae2f9371895d3902d560aa76ee6",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +713,717 @@  override def defaultSize: Int = 16\n\n  override def actualSize(row: InternalRow, ordinal: Int): Int = 20\n\n  override def getField(row: InternalRow, ordinal: Int): CalendarInterval = row.getInterval(ordinal)"
  },
  {
    "id" : "f92e5b90-7f52-4a57-bf58-c348760885d6",
    "prId" : 26699,
    "prUrl" : "https://github.com/apache/spark/pull/26699#pullrequestreview-332362619",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9604c8e5-5255-498f-b582-74999d86fc8b",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "the first int is the size?",
        "createdAt" : "2019-12-16T06:36:20Z",
        "updatedAt" : "2019-12-16T06:36:20Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "6911963a-d5a8-4f51-8bfc-679a2a61be4e",
        "parentId" : "9604c8e5-5255-498f-b582-74999d86fc8b",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Now I got why the actual size is 4 bytes larget than default size. We should document it.\r\n\r\nBTW can we skip storing the size? It's fixed anyway.",
        "createdAt" : "2019-12-16T06:37:11Z",
        "updatedAt" : "2019-12-16T06:37:12Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "b356bf81-8046-473b-b9c0-d4408b3fac3f",
        "parentId" : "9604c8e5-5255-498f-b582-74999d86fc8b",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "yes, we can skip it, the current implementation is just to reuse copy logic of `MutableUnsafeRow`",
        "createdAt" : "2019-12-16T06:39:57Z",
        "updatedAt" : "2019-12-16T06:39:57Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "c5a7374671070ae2f9371895d3902d560aa76ee6",
    "line" : 29,
    "diffHunk" : "@@ -1,1 +722,726 @@\n  override def extract(buffer: ByteBuffer): CalendarInterval = {\n    ByteBufferHelper.getInt(buffer)\n    val months = ByteBufferHelper.getInt(buffer)\n    val days = ByteBufferHelper.getInt(buffer)"
  }
]