[
  {
    "id" : "2d2804a1-7d4b-4057-bc8c-fd6004e4f697",
    "prId" : 29000,
    "prUrl" : "https://github.com/apache/spark/pull/29000#pullrequestreview-681319943",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "30834589-1e35-415f-b905-eb394029897e",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Is it the same if we pass the `committerOutputPath` in `InsertIntoHadoopFsRelationCommand` to `SQLHadoopMapReduceCommitProtocol` directly via `FileCommitProtocol.instantiate()`? ",
        "createdAt" : "2020-08-01T14:33:22Z",
        "updatedAt" : "2020-11-24T16:38:32Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "a56c2fb2-2825-4c9e-9cc1-63ee072d0251",
        "parentId" : "30834589-1e35-415f-b905-eb394029897e",
        "authorId" : "19bcd642-216d-45ca-9e34-13e334fdc8fc",
        "body" : "In my opinion, it is not the same. Since for  ` SQLHadoopMapReduceCommitProtocol` which derived from `HadoopMapReduceCommitProtocol`, final output path for `dynamicPartitionOverwrite` job is the same as other kind of job, there are 2 steps during job committing:\r\n1. move task committed directories to `/path/to/output/.spark-staging-{jobId}`, the code here inits a committer with output path `/path/to/output/.spark-staging-{jobId}`\r\n2. move partition directories under `/path/to/output/.spark-staging-{jobId}` to final output path",
        "createdAt" : "2020-08-02T17:57:21Z",
        "updatedAt" : "2020-11-24T16:38:32Z",
        "lastEditedBy" : "19bcd642-216d-45ca-9e34-13e334fdc8fc",
        "tags" : [
        ]
      },
      {
        "id" : "0e868c53-fd86-40aa-8927-5481254127a5",
        "parentId" : "30834589-1e35-415f-b905-eb394029897e",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "I mean, previously, we pass `outputPath` to `FileCommitProtocol.instantiate` and `qualifiedOutputPath` to `OutputSpec`. And now, you pass the qualified `committerOutputPath` (the one in `InsertIntoHadoopFsRelationCommand`) to `OutputSpec`, but you still pass `outputPath` to `FileCommitProtocol.instantiate`. So can we just pass the non-qualified `committerOutputPath` to `FileCommitProtocol.instantiate` or even the qualified one if it's the same?",
        "createdAt" : "2020-08-04T09:09:00Z",
        "updatedAt" : "2020-11-24T16:38:32Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "3f7941bd-184b-49e4-adcd-fbe3b57a0709",
        "parentId" : "30834589-1e35-415f-b905-eb394029897e",
        "authorId" : "19bcd642-216d-45ca-9e34-13e334fdc8fc",
        "body" : "It's not the same. It's a bit confused. I think there are 2 kinds of 'committer':\r\n1. `HadoopMapReduceCommitProtocol`, this is implemented by Spark, `outputPath` of this committer should be the `finalPath`, like `/path/to/output/`. In this patch, `FileCommitProtocol.instantiate` creates a instance of `HadoopMapReduceCommitProtocol`\r\n2. `FileOutputCommitter`, this is implemented by hadoop mapreduce client, `outputPath` of this committer should be `intermediatePath`, in this patch, it's like `/path/to/output/.spark-staging-{jobId}`. `HadoopMapReduceCommitProtocol` set up a committer of `FileOutputCommitter` to deal with actual commit task / job. In `InsertIntoHadoopFsRelationCommand`, we pass `OutputSpec` to `FileFormatWriter.write`, this will trigger `FileOutputFormat.setOutputPath(job, new Path(outputSpec.outputPath))`, which would set `outputPath` of `FileOutputCommitter`, this is a `intermediatePath` rather than a `finalPath`.\r\n\r\nBack to `SQLHadoopMapReduceCommitProtocol`, when `spark.sql.sources.outputCommitterClass` is set in conf and the specified committer class is assignable from `FileOutputCommitter`, the `outputPath` of this committer should be `intermediatePath`, but `outputPath` of `SQLHadoopMapReduceCommitProtocol` should be `finalPath`",
        "createdAt" : "2020-08-20T08:18:08Z",
        "updatedAt" : "2020-11-24T16:38:32Z",
        "lastEditedBy" : "19bcd642-216d-45ca-9e34-13e334fdc8fc",
        "tags" : [
        ]
      },
      {
        "id" : "0ca7bffd-383b-4661-b620-e1b475da77c4",
        "parentId" : "30834589-1e35-415f-b905-eb394029897e",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "I see. I got your point.",
        "createdAt" : "2020-08-27T16:09:07Z",
        "updatedAt" : "2020-11-24T16:38:32Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "2932dbfc-de4a-47ec-b18c-ad59b9bd07b5",
        "parentId" : "30834589-1e35-415f-b905-eb394029897e",
        "authorId" : "bd48244c-ec04-47b7-a7f0-ae8e070e5394",
        "body" : "Hey @WinkerDu - thank you for the PR. One question: when dynamicPartitionOverwrite is on, this code block will only execute when `clazz` is non-null, which means SQLConf.OUTPUT_COMMITTER_CLASS is set. It works for parquet files since that SQL property is set at https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala#L97. So what about other file formats, like Orc? There seems to be no such property set logic for other file formats, at lease in Spark repo. So is dynamicPartitionOverwrite supposed to be for Parquet only? Am I missing sth here? Thanks. \r\n\r\n@cloud-fan @Ngone51 @agrawaldevesh",
        "createdAt" : "2021-06-10T22:48:29Z",
        "updatedAt" : "2021-06-10T22:48:29Z",
        "lastEditedBy" : "bd48244c-ec04-47b7-a7f0-ae8e070e5394",
        "tags" : [
        ]
      }
    ],
    "commit" : "85aa12a618ceadfe510a4f9fc3718a746a1bc357",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +57,61 @@        val ctor = clazz.getDeclaredConstructor(classOf[Path], classOf[TaskAttemptContext])\n        val committerOutputPath = if (dynamicPartitionOverwrite) stagingDir else new Path(path)\n        committer = ctor.newInstance(committerOutputPath, context)\n      } else {\n        // The specified output committer is just an OutputCommitter."
  },
  {
    "id" : "9f270979-f469-43d7-bc7d-21e9e48f0d49",
    "prId" : 25863,
    "prUrl" : "https://github.com/apache/spark/pull/25863#pullrequestreview-293184422",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ae65c775-b473-481c-9c1b-11bb727469bb",
        "parentId" : null,
        "authorId" : "b6393788-12cc-4066-9294-857f4baec092",
        "body" : "make compatible with old version.",
        "createdAt" : "2019-09-25T15:57:05Z",
        "updatedAt" : "2019-12-09T16:11:48Z",
        "lastEditedBy" : "b6393788-12cc-4066-9294-857f4baec092",
        "tags" : [
        ]
      }
    ],
    "commit" : "f45ca9bf6c51d3efb064a87de56b985e17b60788",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +40,44 @@    this(jobId, path, Some(new FileSourceWriteDesc(dynamicPartitionOverwrite =\n      dynamicPartitionOverwrite)))\n\n  override protected def setupCommitter(context: TaskAttemptContext): OutputCommitter = {\n    var committer = super.setupCommitter(context)"
  },
  {
    "id" : "279101f2-fd8f-4bce-b7bb-7fbb0ba7846e",
    "prId" : 25840,
    "prUrl" : "https://github.com/apache/spark/pull/25840#pullrequestreview-301097498",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5bccd048-bd1c-4071-b483-fa7dd4ac23ac",
        "parentId" : null,
        "authorId" : "31c8cfa2-9d5f-412b-80d0-c7d204dd8cd1",
        "body" : "Can we make this parameter a collection/set/map. ?? making it so keeps the interface fixed and also extendable for future.\r\nSuppose we have more such behaviours to be added (which I am sure will happen), we cannot create new interfaces and break the contract each time(though we try not to).\r\nHaving a Key-Value pair of configuration will be much easier for such extensions.",
        "createdAt" : "2019-09-24T04:11:15Z",
        "updatedAt" : "2020-09-23T12:30:11Z",
        "lastEditedBy" : "31c8cfa2-9d5f-412b-80d0-c7d204dd8cd1",
        "tags" : [
        ]
      },
      {
        "id" : "aa0fd126-3169-4c60-8e25-dcc6b555ca78",
        "parentId" : "5bccd048-bd1c-4071-b483-fa7dd4ac23ac",
        "authorId" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "body" : "Done for this.",
        "createdAt" : "2019-09-24T07:04:15Z",
        "updatedAt" : "2020-09-23T12:30:11Z",
        "lastEditedBy" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "tags" : [
        ]
      },
      {
        "id" : "dc36ce16-25ff-421a-98da-b18f34e31717",
        "parentId" : "5bccd048-bd1c-4071-b483-fa7dd4ac23ac",
        "authorId" : "31c8cfa2-9d5f-412b-80d0-c7d204dd8cd1",
        "body" : "thanks. ",
        "createdAt" : "2019-10-14T03:57:47Z",
        "updatedAt" : "2020-09-23T12:30:11Z",
        "lastEditedBy" : "31c8cfa2-9d5f-412b-80d0-c7d204dd8cd1",
        "tags" : [
        ]
      }
    ],
    "commit" : "edd8aa513f8856a9b26e185b2974c6cbc4fb53e9",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +35,39 @@    jobId: String,\n    path: String,\n    dynamicPartitionOverwrite: Boolean = false,\n    restrictions: Map[String, Object] = Map.empty)\n  extends HadoopMapReduceCommitProtocol(jobId, path, dynamicPartitionOverwrite)"
  },
  {
    "id" : "7dce4403-72e4-487b-920d-eff7b25bdcb0",
    "prId" : 25840,
    "prUrl" : "https://github.com/apache/spark/pull/25840#pullrequestreview-301115840",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "35cbf141-231d-4c01-bd64-bfa392758ae2",
        "parentId" : null,
        "authorId" : "31c8cfa2-9d5f-412b-80d0-c7d204dd8cd1",
        "body" : "this implementation completely hides org.apache.spark.internal.io.HadoopMapReduceCommitProtocol#commitTask, which was the behaviour earlier.\r\n\r\nIs it intensional?",
        "createdAt" : "2019-10-14T04:07:54Z",
        "updatedAt" : "2020-09-23T12:30:11Z",
        "lastEditedBy" : "31c8cfa2-9d5f-412b-80d0-c7d204dd8cd1",
        "tags" : [
        ]
      },
      {
        "id" : "80d4441b-45c2-4b45-a8cc-86cd39d72c61",
        "parentId" : "35cbf141-231d-4c01-bd64-bfa392758ae2",
        "authorId" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "body" : "> this implementation completely hides org.apache.spark.internal.io.HadoopMapReduceCommitProtocol#commitTask\r\n\r\nNo. `onTaskCommit`  doesn't hide `commitTask`. Actually, `commitTask` is called on executor side but `onTaskCommit` is called on driver side.",
        "createdAt" : "2019-10-14T06:10:57Z",
        "updatedAt" : "2020-09-23T12:30:11Z",
        "lastEditedBy" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "tags" : [
        ]
      },
      {
        "id" : "5963b6ec-e8b6-4de8-965b-cfbe140081fe",
        "parentId" : "35cbf141-231d-4c01-bd64-bfa392758ae2",
        "authorId" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "body" : "`SQLHadoopMapReduceCommitProtocol.onTaskCommit` overrides `FileCommitProtocol.onTaskCommit` on purpose.",
        "createdAt" : "2019-10-14T06:14:08Z",
        "updatedAt" : "2020-09-23T12:30:11Z",
        "lastEditedBy" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "tags" : [
        ]
      }
    ],
    "commit" : "edd8aa513f8856a9b26e185b2974c6cbc4fb53e9",
    "line" : 56,
    "diffHunk" : "@@ -1,1 +99,103 @@   * Override it to check dynamic partition limitation on driver side.\n   */\n  override def onTaskCommit(taskCommit: TaskCommitMessage): Unit = {\n    logDebug(s\"onTaskCommit($taskCommit)\")\n    if (hasValidPath) {"
  }
]