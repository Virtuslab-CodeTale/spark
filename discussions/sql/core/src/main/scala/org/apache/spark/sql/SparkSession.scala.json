[
  {
    "id" : "60b1ed6a-8aef-41f1-9dac-42d38634a1bf",
    "prId" : 31598,
    "prUrl" : "https://github.com/apache/spark/pull/31598#pullrequestreview-598211127",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2f83fc34-1efa-4dcd-9880-44d97c993548",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "`spark.master` seems not here?",
        "createdAt" : "2021-02-24T05:47:08Z",
        "updatedAt" : "2021-03-16T03:58:12Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "f39765f0-f3fc-47c7-b38e-0b20ef44d3f5",
        "parentId" : "2f83fc34-1efa-4dcd-9880-44d97c993548",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> `spark.master` seems not here?\r\n\r\nRemove this.",
        "createdAt" : "2021-02-25T05:37:17Z",
        "updatedAt" : "2021-03-16T03:58:12Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "024929bda650d0125a3275355deb163426f61760",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +899,903 @@    }\n\n    // These configurations related to driver when deploy like `spark.master`,\n    // `spark.driver.memory`, this kind of properties may not be affected when\n    // setting programmatically through SparkConf in runtime, or the behavior is"
  },
  {
    "id" : "f5140617-968e-4385-95cd-e035e0534e2e",
    "prId" : 31598,
    "prUrl" : "https://github.com/apache/spark/pull/31598#pullrequestreview-598171423",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e4564ec4-5c50-4e91-8e1d-d178cd1e7699",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@AngersZhuuuu, I would first document this explicitly and what happen for each configuration before taking an action to show a warning. Also, shouldn't we do this in `SparkContext` too?",
        "createdAt" : "2021-02-24T05:48:22Z",
        "updatedAt" : "2021-03-16T03:58:12Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "be37567a-ddae-485a-ada9-8ac402d7d978",
        "parentId" : "e4564ec4-5c50-4e91-8e1d-d178cd1e7699",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "We can document first, and then link the documentation URL in the warning message.",
        "createdAt" : "2021-02-24T05:49:42Z",
        "updatedAt" : "2021-03-16T03:58:12Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "200bbd12-66ce-4e14-b45e-48a5a74d6f38",
        "parentId" : "e4564ec4-5c50-4e91-8e1d-d178cd1e7699",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> @AngersZhuuuu, I would first document this explicitly and what happen for each configuration before taking an action to show a warning. Also, shouldn't we do this in `SparkContext` too?\r\n\r\nHow about we add a new section in  https://spark.apache.org/docs/latest/configuration.html  to collect and show configuration's usage scope. \r\nAlso we can add a `scope` tag in ConfigBuilder for these special configuration. \r\n\r\nThen in this pr, we can just check the config type and warn message.",
        "createdAt" : "2021-02-24T07:30:24Z",
        "updatedAt" : "2021-03-16T03:58:12Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "0b07f8d7-5a3c-4c96-9491-5fc79ebce01c",
        "parentId" : "e4564ec4-5c50-4e91-8e1d-d178cd1e7699",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I think you can just add some docs for each configuration like from `blah blah` to `(Launcher scope) blah blah` or something like this.",
        "createdAt" : "2021-02-24T09:04:33Z",
        "updatedAt" : "2021-03-16T03:58:12Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "f93e18e9-e212-408b-8ce0-3126bb1b6e9c",
        "parentId" : "e4564ec4-5c50-4e91-8e1d-d178cd1e7699",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> I think you can just add some docs for each configuration like from `blah blah` to `(Launcher scope) blah blah` or something like this.\r\n\r\nUpdated, how about current? \r\n\r\nWarn message like \r\n```\r\n\r\n11:38:42.540 WARN org.apache.spark.sql.SparkSession$Builder: Since spark has been submitted, such configurations\r\n `spark.driver.memory -> 1g` may not take effect.\r\n For how to set these configuration correctly, you can refer to\r\n https://spark.apache.org/docs/latest/configuration.html#dynamically-loading-spark-properties.\r\n\r\n```\r\n\r\nAslo ping @dongjoon-hyun @maropu ",
        "createdAt" : "2021-02-25T03:40:28Z",
        "updatedAt" : "2021-03-16T03:58:12Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "024929bda650d0125a3275355deb163426f61760",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +903,907 @@    // setting programmatically through SparkConf in runtime, or the behavior is\n    // depending on which cluster manager and deploy mode you choose, so it would\n    // be suggested to set through configuration file or spark-submit command line options.\n    private val DRIVER_RELATED_LAUNCHER_CONFIG = Seq(DRIVER_MEMORY, DRIVER_CORES.key,\n      DRIVER_MEMORY_OVERHEAD.key, DRIVER_EXTRA_CLASSPATH,"
  },
  {
    "id" : "3cb9b8c5-888a-4138-aa01-db419786b7fb",
    "prId" : 31598,
    "prUrl" : "https://github.com/apache/spark/pull/31598#pullrequestreview-602577176",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "814032e3-3160-4940-a775-02a3b0ef7cd3",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@AngersZhuuuu, are they all configurations to include? is it just a subset of them?",
        "createdAt" : "2021-03-01T05:15:39Z",
        "updatedAt" : "2021-03-16T03:58:12Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "a48ca75e-3b74-4e6f-a3d8-f023c44ecbfd",
        "parentId" : "814032e3-3160-4940-a775-02a3b0ef7cd3",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> @AngersZhuuuu, are they all configurations to include? is it just a subset of them?\r\n\r\nThat's what I found through configuration page. I am not sure if there is configurations in code that I missed.",
        "createdAt" : "2021-03-01T05:17:08Z",
        "updatedAt" : "2021-03-16T03:58:12Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "500fcf56-3a40-40e6-a43e-f0fb6717025b",
        "parentId" : "814032e3-3160-4940-a775-02a3b0ef7cd3",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "It seems like something we should build into our config library. It's hard to maintain this list here.",
        "createdAt" : "2021-03-02T15:33:11Z",
        "updatedAt" : "2021-03-16T03:58:12Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "995eabe1-b959-4e12-abe2-f7c9abc6af62",
        "parentId" : "814032e3-3160-4940-a775-02a3b0ef7cd3",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> It seems like something we should build into our config library. It's hard to maintain this list here.\r\n\r\nYou mean like what  I have said in https://github.com/apache/spark/pull/31598#discussion_r581689593.\r\nWe should add a new tag in ConfigEntry? like `.version()` we add `.type()` or `.scope()` ?",
        "createdAt" : "2021-03-03T02:20:02Z",
        "updatedAt" : "2021-03-16T03:58:12Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "edcc7b26-1b10-4619-95b3-1ef32c25f7b5",
        "parentId" : "814032e3-3160-4940-a775-02a3b0ef7cd3",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "yea, something like `.scope`.",
        "createdAt" : "2021-03-03T04:55:42Z",
        "updatedAt" : "2021-03-16T03:58:12Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "a503bae4-4047-4953-9eee-f71af64b6d6f",
        "parentId" : "814032e3-3160-4940-a775-02a3b0ef7cd3",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> yea, something like `.scope`.\r\n\r\nyea. should we do it in this pr or a new one? since add tag and check is different things",
        "createdAt" : "2021-03-03T05:14:48Z",
        "updatedAt" : "2021-03-16T03:58:12Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "e6cf2845-1561-4c19-b407-3e0f701e854d",
        "parentId" : "814032e3-3160-4940-a775-02a3b0ef7cd3",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "We can do that in a new PR, and merge that PR first.",
        "createdAt" : "2021-03-03T06:42:06Z",
        "updatedAt" : "2021-03-16T03:58:12Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "17ba5fc6-a0d8-475d-9a47-8b018825b173",
        "parentId" : "814032e3-3160-4940-a775-02a3b0ef7cd3",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> We can do that in a new PR, and merge that PR first.\r\n\r\nOk, I think it's a very usage improvement, since many user don't understand spark's configuration very well.",
        "createdAt" : "2021-03-03T07:14:18Z",
        "updatedAt" : "2021-03-16T03:58:12Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "024929bda650d0125a3275355deb163426f61760",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +909,913 @@      \"spark.driver.resource\", PYSPARK_DRIVER_PYTHON, PYSPARK_PYTHON, SPARKR_R_SHELL,\n      CHILD_PROCESS_LOGGER_NAME, CHILD_CONNECTION_TIMEOUT, DRIVER_USER_CLASS_PATH_FIRST.key,\n      \"spark.yarn.*\")\n\n    /**"
  },
  {
    "id" : "1593356a-25a9-4985-92e2-05f505e0c253",
    "prId" : 30042,
    "prUrl" : "https://github.com/apache/spark/pull/30042#pullrequestreview-508988307",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9a74ff68-b1f3-4d4d-9d7d-37748321d24e",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "let's add deprecated annotation.",
        "createdAt" : "2020-10-15T04:37:15Z",
        "updatedAt" : "2020-10-15T11:43:45Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "b13b437f-fe41-4762-9220-38d407234976",
        "parentId" : "9a74ff68-b1f3-4d4d-9d7d-37748321d24e",
        "authorId" : "cce1c782-0596-44b6-8b99-6b77d2cca53c",
        "body" : "done",
        "createdAt" : "2020-10-15T06:01:30Z",
        "updatedAt" : "2020-10-15T11:43:45Z",
        "lastEditedBy" : "cce1c782-0596-44b6-8b99-6b77d2cca53c",
        "tags" : [
        ]
      }
    ],
    "commit" : "aa7bf5b50c4388e6caa24826d1bc83572af60e5a",
    "line" : 26,
    "diffHunk" : "@@ -1,1 +986,990 @@   */\n  @deprecated(\"This method is deprecated and will be removed in future versions.\", \"3.1.0\")\n  def setActiveSession(session: SparkSession): Unit = {\n    if (SQLConf.get.legacyAllowModifyActiveSession) {\n      setActiveSessionInternal(session)"
  },
  {
    "id" : "14846103-5465-4eed-9964-746bd92fe953",
    "prId" : 30042,
    "prUrl" : "https://github.com/apache/spark/pull/30042#pullrequestreview-509954447",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4f3b23b9-8325-41f4-819c-5808467e2ea2",
        "parentId" : null,
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "Shall we add some comments to tell users how to restore here or in the error msg?",
        "createdAt" : "2020-10-15T08:47:42Z",
        "updatedAt" : "2020-10-15T11:43:45Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "f406250c-80a7-49ac-a856-a04573a3581e",
        "parentId" : "4f3b23b9-8325-41f4-819c-5808467e2ea2",
        "authorId" : "cce1c782-0596-44b6-8b99-6b77d2cca53c",
        "body" : "unlike other deprecated API, this API does not have a better \"Option\"; we kind of hoping that user not touch these API at all. I will consider putting the legacy config in the error message as a hint in case user really really need these two API.",
        "createdAt" : "2020-10-15T08:58:41Z",
        "updatedAt" : "2020-10-15T11:43:45Z",
        "lastEditedBy" : "cce1c782-0596-44b6-8b99-6b77d2cca53c",
        "tags" : [
        ]
      },
      {
        "id" : "ad6c9bfd-6757-4b1d-8e79-c23da501eaa8",
        "parentId" : "4f3b23b9-8325-41f4-819c-5808467e2ea2",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "I agree that these APIs are not something users can call casually. \r\nBut I am not sure the deprecated annotation can cover the behavior change here. The config is internal and will not go to the public doc. Maybe we should add a migration guide if this is not only deprecated but also desupported",
        "createdAt" : "2020-10-15T09:18:21Z",
        "updatedAt" : "2020-10-15T11:43:45Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "af6922e1-4132-479a-b6c6-0a41619b63b0",
        "parentId" : "4f3b23b9-8325-41f4-819c-5808467e2ea2",
        "authorId" : "cce1c782-0596-44b6-8b99-6b77d2cca53c",
        "body" : "sure, i will update the migration guide as well.",
        "createdAt" : "2020-10-15T09:42:06Z",
        "updatedAt" : "2020-10-15T11:43:45Z",
        "lastEditedBy" : "cce1c782-0596-44b6-8b99-6b77d2cca53c",
        "tags" : [
        ]
      },
      {
        "id" : "4311b219-d8da-4544-b9d7-c751b22e1fc8",
        "parentId" : "4f3b23b9-8325-41f4-819c-5808467e2ea2",
        "authorId" : "cce1c782-0596-44b6-8b99-6b77d2cca53c",
        "body" : "done",
        "createdAt" : "2020-10-16T01:33:14Z",
        "updatedAt" : "2020-10-16T01:33:14Z",
        "lastEditedBy" : "cce1c782-0596-44b6-8b99-6b77d2cca53c",
        "tags" : [
        ]
      }
    ],
    "commit" : "aa7bf5b50c4388e6caa24826d1bc83572af60e5a",
    "line" : 25,
    "diffHunk" : "@@ -1,1 +985,989 @@   * @since 2.0.0\n   */\n  @deprecated(\"This method is deprecated and will be removed in future versions.\", \"3.1.0\")\n  def setActiveSession(session: SparkSession): Unit = {\n    if (SQLConf.get.legacyAllowModifyActiveSession) {"
  },
  {
    "id" : "eea54e70-9db6-4f34-abe7-0de361c8a203",
    "prId" : 30042,
    "prUrl" : "https://github.com/apache/spark/pull/30042#pullrequestreview-528758703",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c437093c-f77e-4994-ae62-77c94e9afd16",
        "parentId" : null,
        "authorId" : "2d6b46ba-4100-4c4d-9341-fff5e39647ec",
        "body" : "Can we add the flag in the error message? Then users will know how to unblock themselves without googling.",
        "createdAt" : "2020-11-12T03:48:47Z",
        "updatedAt" : "2020-11-12T03:48:47Z",
        "lastEditedBy" : "2d6b46ba-4100-4c4d-9341-fff5e39647ec",
        "tags" : [
        ]
      },
      {
        "id" : "a0edded6-cd32-4900-8fb8-f6d604ddf057",
        "parentId" : "c437093c-f77e-4994-ae62-77c94e9afd16",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "+1. @leanken can you create a follow-up? thanks!",
        "createdAt" : "2020-11-12T03:55:47Z",
        "updatedAt" : "2020-11-12T03:55:48Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "59466079-0a49-4094-93f6-bb42acb55f81",
        "parentId" : "c437093c-f77e-4994-ae62-77c94e9afd16",
        "authorId" : "cce1c782-0596-44b6-8b99-6b77d2cca53c",
        "body" : "OK.",
        "createdAt" : "2020-11-12T05:25:56Z",
        "updatedAt" : "2020-11-12T05:25:56Z",
        "lastEditedBy" : "cce1c782-0596-44b6-8b99-6b77d2cca53c",
        "tags" : [
        ]
      }
    ],
    "commit" : "aa7bf5b50c4388e6caa24826d1bc83572af60e5a",
    "line" : 30,
    "diffHunk" : "@@ -1,1 +990,994 @@      setActiveSessionInternal(session)\n    } else {\n      throw new UnsupportedOperationException(\"Not allowed to modify active Spark session.\")\n    }\n  }"
  },
  {
    "id" : "85e4ca61-139c-4bf3-bf0c-de5e29a13ac7",
    "prId" : 28868,
    "prUrl" : "https://github.com/apache/spark/pull/28868#pullrequestreview-440843122",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "06e859ad-74eb-49ac-996b-f5cbdf98fce6",
        "parentId" : null,
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "unify the entrance of active session.",
        "createdAt" : "2020-07-01T12:56:48Z",
        "updatedAt" : "2020-07-01T12:56:49Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      }
    ],
    "commit" : "a3b7d1e3191ac6ea6d396c0ffff49bc4aade76b8",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +759,763 @@    // set and not the default session. This to prevent that we promote the default session to the\n    // active session once we are done.\n    val old = SparkSession.getActiveSession.get\n    SparkSession.setActiveSession(this)\n    try block finally {"
  },
  {
    "id" : "6699f82e-86e4-45eb-83ed-2fc11e5771d2",
    "prId" : 28868,
    "prUrl" : "https://github.com/apache/spark/pull/28868#pullrequestreview-441684179",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1f33d23e-94a5-45e6-b87c-b6c6e5b7f94d",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I don't think it's necessary. A user may get the active session instance, and after that someone else stops the spark context. This check can't help this case.",
        "createdAt" : "2020-07-02T13:15:57Z",
        "updatedAt" : "2020-07-02T13:15:57Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "5515c047-a97a-40ba-b140-6b41a577c6ef",
        "parentId" : "1f33d23e-94a5-45e6-b87c-b6c6e5b7f94d",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "That's true. This place only checks if user get an active session, existing used session can not be covered. But in some case which used `session.withActive()` this can do the help.",
        "createdAt" : "2020-07-02T13:35:32Z",
        "updatedAt" : "2020-07-02T13:35:33Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      }
    ],
    "commit" : "a3b7d1e3191ac6ea6d396c0ffff49bc4aade76b8",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +1022,1026 @@    } else {\n      val session = Option(activeThreadSession.get)\n      session.foreach(_.sparkContext.assertNotStopped())\n      session\n    }"
  }
]