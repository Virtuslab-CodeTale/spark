[
  {
    "id" : "60b1ed6a-8aef-41f1-9dac-42d38634a1bf",
    "prId" : 31598,
    "prUrl" : "https://github.com/apache/spark/pull/31598#pullrequestreview-598211127",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2f83fc34-1efa-4dcd-9880-44d97c993548",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "`spark.master` seems not here?",
        "createdAt" : "2021-02-24T05:47:08Z",
        "updatedAt" : "2021-03-16T03:58:12Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "f39765f0-f3fc-47c7-b38e-0b20ef44d3f5",
        "parentId" : "2f83fc34-1efa-4dcd-9880-44d97c993548",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> `spark.master` seems not here?\r\n\r\nRemove this.",
        "createdAt" : "2021-02-25T05:37:17Z",
        "updatedAt" : "2021-03-16T03:58:12Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "024929bda650d0125a3275355deb163426f61760",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +899,903 @@    }\n\n    // These configurations related to driver when deploy like `spark.master`,\n    // `spark.driver.memory`, this kind of properties may not be affected when\n    // setting programmatically through SparkConf in runtime, or the behavior is"
  },
  {
    "id" : "f5140617-968e-4385-95cd-e035e0534e2e",
    "prId" : 31598,
    "prUrl" : "https://github.com/apache/spark/pull/31598#pullrequestreview-598171423",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e4564ec4-5c50-4e91-8e1d-d178cd1e7699",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@AngersZhuuuu, I would first document this explicitly and what happen for each configuration before taking an action to show a warning. Also, shouldn't we do this in `SparkContext` too?",
        "createdAt" : "2021-02-24T05:48:22Z",
        "updatedAt" : "2021-03-16T03:58:12Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "be37567a-ddae-485a-ada9-8ac402d7d978",
        "parentId" : "e4564ec4-5c50-4e91-8e1d-d178cd1e7699",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "We can document first, and then link the documentation URL in the warning message.",
        "createdAt" : "2021-02-24T05:49:42Z",
        "updatedAt" : "2021-03-16T03:58:12Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "200bbd12-66ce-4e14-b45e-48a5a74d6f38",
        "parentId" : "e4564ec4-5c50-4e91-8e1d-d178cd1e7699",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> @AngersZhuuuu, I would first document this explicitly and what happen for each configuration before taking an action to show a warning. Also, shouldn't we do this in `SparkContext` too?\r\n\r\nHow about we add a new section in  https://spark.apache.org/docs/latest/configuration.html  to collect and show configuration's usage scope. \r\nAlso we can add a `scope` tag in ConfigBuilder for these special configuration. \r\n\r\nThen in this pr, we can just check the config type and warn message.",
        "createdAt" : "2021-02-24T07:30:24Z",
        "updatedAt" : "2021-03-16T03:58:12Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "0b07f8d7-5a3c-4c96-9491-5fc79ebce01c",
        "parentId" : "e4564ec4-5c50-4e91-8e1d-d178cd1e7699",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I think you can just add some docs for each configuration like from `blah blah` to `(Launcher scope) blah blah` or something like this.",
        "createdAt" : "2021-02-24T09:04:33Z",
        "updatedAt" : "2021-03-16T03:58:12Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "f93e18e9-e212-408b-8ce0-3126bb1b6e9c",
        "parentId" : "e4564ec4-5c50-4e91-8e1d-d178cd1e7699",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> I think you can just add some docs for each configuration like from `blah blah` to `(Launcher scope) blah blah` or something like this.\r\n\r\nUpdated, how about current? \r\n\r\nWarn message like \r\n```\r\n\r\n11:38:42.540 WARN org.apache.spark.sql.SparkSession$Builder: Since spark has been submitted, such configurations\r\n `spark.driver.memory -> 1g` may not take effect.\r\n For how to set these configuration correctly, you can refer to\r\n https://spark.apache.org/docs/latest/configuration.html#dynamically-loading-spark-properties.\r\n\r\n```\r\n\r\nAslo ping @dongjoon-hyun @maropu ",
        "createdAt" : "2021-02-25T03:40:28Z",
        "updatedAt" : "2021-03-16T03:58:12Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "024929bda650d0125a3275355deb163426f61760",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +903,907 @@    // setting programmatically through SparkConf in runtime, or the behavior is\n    // depending on which cluster manager and deploy mode you choose, so it would\n    // be suggested to set through configuration file or spark-submit command line options.\n    private val DRIVER_RELATED_LAUNCHER_CONFIG = Seq(DRIVER_MEMORY, DRIVER_CORES.key,\n      DRIVER_MEMORY_OVERHEAD.key, DRIVER_EXTRA_CLASSPATH,"
  },
  {
    "id" : "3cb9b8c5-888a-4138-aa01-db419786b7fb",
    "prId" : 31598,
    "prUrl" : "https://github.com/apache/spark/pull/31598#pullrequestreview-602577176",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "814032e3-3160-4940-a775-02a3b0ef7cd3",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@AngersZhuuuu, are they all configurations to include? is it just a subset of them?",
        "createdAt" : "2021-03-01T05:15:39Z",
        "updatedAt" : "2021-03-16T03:58:12Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "a48ca75e-3b74-4e6f-a3d8-f023c44ecbfd",
        "parentId" : "814032e3-3160-4940-a775-02a3b0ef7cd3",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> @AngersZhuuuu, are they all configurations to include? is it just a subset of them?\r\n\r\nThat's what I found through configuration page. I am not sure if there is configurations in code that I missed.",
        "createdAt" : "2021-03-01T05:17:08Z",
        "updatedAt" : "2021-03-16T03:58:12Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "500fcf56-3a40-40e6-a43e-f0fb6717025b",
        "parentId" : "814032e3-3160-4940-a775-02a3b0ef7cd3",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "It seems like something we should build into our config library. It's hard to maintain this list here.",
        "createdAt" : "2021-03-02T15:33:11Z",
        "updatedAt" : "2021-03-16T03:58:12Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "995eabe1-b959-4e12-abe2-f7c9abc6af62",
        "parentId" : "814032e3-3160-4940-a775-02a3b0ef7cd3",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> It seems like something we should build into our config library. It's hard to maintain this list here.\r\n\r\nYou mean like what  I have said in https://github.com/apache/spark/pull/31598#discussion_r581689593.\r\nWe should add a new tag in ConfigEntry? like `.version()` we add `.type()` or `.scope()` ?",
        "createdAt" : "2021-03-03T02:20:02Z",
        "updatedAt" : "2021-03-16T03:58:12Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "edcc7b26-1b10-4619-95b3-1ef32c25f7b5",
        "parentId" : "814032e3-3160-4940-a775-02a3b0ef7cd3",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "yea, something like `.scope`.",
        "createdAt" : "2021-03-03T04:55:42Z",
        "updatedAt" : "2021-03-16T03:58:12Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "a503bae4-4047-4953-9eee-f71af64b6d6f",
        "parentId" : "814032e3-3160-4940-a775-02a3b0ef7cd3",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> yea, something like `.scope`.\r\n\r\nyea. should we do it in this pr or a new one? since add tag and check is different things",
        "createdAt" : "2021-03-03T05:14:48Z",
        "updatedAt" : "2021-03-16T03:58:12Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "e6cf2845-1561-4c19-b407-3e0f701e854d",
        "parentId" : "814032e3-3160-4940-a775-02a3b0ef7cd3",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "We can do that in a new PR, and merge that PR first.",
        "createdAt" : "2021-03-03T06:42:06Z",
        "updatedAt" : "2021-03-16T03:58:12Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "17ba5fc6-a0d8-475d-9a47-8b018825b173",
        "parentId" : "814032e3-3160-4940-a775-02a3b0ef7cd3",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> We can do that in a new PR, and merge that PR first.\r\n\r\nOk, I think it's a very usage improvement, since many user don't understand spark's configuration very well.",
        "createdAt" : "2021-03-03T07:14:18Z",
        "updatedAt" : "2021-03-16T03:58:12Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "024929bda650d0125a3275355deb163426f61760",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +909,913 @@      \"spark.driver.resource\", PYSPARK_DRIVER_PYTHON, PYSPARK_PYTHON, SPARKR_R_SHELL,\n      CHILD_PROCESS_LOGGER_NAME, CHILD_CONNECTION_TIMEOUT, DRIVER_USER_CLASS_PATH_FIRST.key,\n      \"spark.yarn.*\")\n\n    /**"
  }
]