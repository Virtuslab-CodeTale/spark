[
  {
    "id" : "60b1ed6a-8aef-41f1-9dac-42d38634a1bf",
    "prId" : 31598,
    "prUrl" : "https://github.com/apache/spark/pull/31598#pullrequestreview-598211127",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2f83fc34-1efa-4dcd-9880-44d97c993548",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "`spark.master` seems not here?",
        "createdAt" : "2021-02-24T05:47:08Z",
        "updatedAt" : "2021-03-16T03:58:12Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "f39765f0-f3fc-47c7-b38e-0b20ef44d3f5",
        "parentId" : "2f83fc34-1efa-4dcd-9880-44d97c993548",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> `spark.master` seems not here?\r\n\r\nRemove this.",
        "createdAt" : "2021-02-25T05:37:17Z",
        "updatedAt" : "2021-03-16T03:58:12Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "024929bda650d0125a3275355deb163426f61760",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +899,903 @@    }\n\n    // These configurations related to driver when deploy like `spark.master`,\n    // `spark.driver.memory`, this kind of properties may not be affected when\n    // setting programmatically through SparkConf in runtime, or the behavior is"
  },
  {
    "id" : "f5140617-968e-4385-95cd-e035e0534e2e",
    "prId" : 31598,
    "prUrl" : "https://github.com/apache/spark/pull/31598#pullrequestreview-598171423",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e4564ec4-5c50-4e91-8e1d-d178cd1e7699",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@AngersZhuuuu, I would first document this explicitly and what happen for each configuration before taking an action to show a warning. Also, shouldn't we do this in `SparkContext` too?",
        "createdAt" : "2021-02-24T05:48:22Z",
        "updatedAt" : "2021-03-16T03:58:12Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "be37567a-ddae-485a-ada9-8ac402d7d978",
        "parentId" : "e4564ec4-5c50-4e91-8e1d-d178cd1e7699",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "We can document first, and then link the documentation URL in the warning message.",
        "createdAt" : "2021-02-24T05:49:42Z",
        "updatedAt" : "2021-03-16T03:58:12Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "200bbd12-66ce-4e14-b45e-48a5a74d6f38",
        "parentId" : "e4564ec4-5c50-4e91-8e1d-d178cd1e7699",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> @AngersZhuuuu, I would first document this explicitly and what happen for each configuration before taking an action to show a warning. Also, shouldn't we do this in `SparkContext` too?\r\n\r\nHow about we add a new section in  https://spark.apache.org/docs/latest/configuration.html  to collect and show configuration's usage scope. \r\nAlso we can add a `scope` tag in ConfigBuilder for these special configuration. \r\n\r\nThen in this pr, we can just check the config type and warn message.",
        "createdAt" : "2021-02-24T07:30:24Z",
        "updatedAt" : "2021-03-16T03:58:12Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "0b07f8d7-5a3c-4c96-9491-5fc79ebce01c",
        "parentId" : "e4564ec4-5c50-4e91-8e1d-d178cd1e7699",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I think you can just add some docs for each configuration like from `blah blah` to `(Launcher scope) blah blah` or something like this.",
        "createdAt" : "2021-02-24T09:04:33Z",
        "updatedAt" : "2021-03-16T03:58:12Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "f93e18e9-e212-408b-8ce0-3126bb1b6e9c",
        "parentId" : "e4564ec4-5c50-4e91-8e1d-d178cd1e7699",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> I think you can just add some docs for each configuration like from `blah blah` to `(Launcher scope) blah blah` or something like this.\r\n\r\nUpdated, how about current? \r\n\r\nWarn message like \r\n```\r\n\r\n11:38:42.540 WARN org.apache.spark.sql.SparkSession$Builder: Since spark has been submitted, such configurations\r\n `spark.driver.memory -> 1g` may not take effect.\r\n For how to set these configuration correctly, you can refer to\r\n https://spark.apache.org/docs/latest/configuration.html#dynamically-loading-spark-properties.\r\n\r\n```\r\n\r\nAslo ping @dongjoon-hyun @maropu ",
        "createdAt" : "2021-02-25T03:40:28Z",
        "updatedAt" : "2021-03-16T03:58:12Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "024929bda650d0125a3275355deb163426f61760",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +903,907 @@    // setting programmatically through SparkConf in runtime, or the behavior is\n    // depending on which cluster manager and deploy mode you choose, so it would\n    // be suggested to set through configuration file or spark-submit command line options.\n    private val DRIVER_RELATED_LAUNCHER_CONFIG = Seq(DRIVER_MEMORY, DRIVER_CORES.key,\n      DRIVER_MEMORY_OVERHEAD.key, DRIVER_EXTRA_CLASSPATH,"
  },
  {
    "id" : "3cb9b8c5-888a-4138-aa01-db419786b7fb",
    "prId" : 31598,
    "prUrl" : "https://github.com/apache/spark/pull/31598#pullrequestreview-602577176",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "814032e3-3160-4940-a775-02a3b0ef7cd3",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@AngersZhuuuu, are they all configurations to include? is it just a subset of them?",
        "createdAt" : "2021-03-01T05:15:39Z",
        "updatedAt" : "2021-03-16T03:58:12Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "a48ca75e-3b74-4e6f-a3d8-f023c44ecbfd",
        "parentId" : "814032e3-3160-4940-a775-02a3b0ef7cd3",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> @AngersZhuuuu, are they all configurations to include? is it just a subset of them?\r\n\r\nThat's what I found through configuration page. I am not sure if there is configurations in code that I missed.",
        "createdAt" : "2021-03-01T05:17:08Z",
        "updatedAt" : "2021-03-16T03:58:12Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "500fcf56-3a40-40e6-a43e-f0fb6717025b",
        "parentId" : "814032e3-3160-4940-a775-02a3b0ef7cd3",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "It seems like something we should build into our config library. It's hard to maintain this list here.",
        "createdAt" : "2021-03-02T15:33:11Z",
        "updatedAt" : "2021-03-16T03:58:12Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "995eabe1-b959-4e12-abe2-f7c9abc6af62",
        "parentId" : "814032e3-3160-4940-a775-02a3b0ef7cd3",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> It seems like something we should build into our config library. It's hard to maintain this list here.\r\n\r\nYou mean like what  I have said in https://github.com/apache/spark/pull/31598#discussion_r581689593.\r\nWe should add a new tag in ConfigEntry? like `.version()` we add `.type()` or `.scope()` ?",
        "createdAt" : "2021-03-03T02:20:02Z",
        "updatedAt" : "2021-03-16T03:58:12Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "edcc7b26-1b10-4619-95b3-1ef32c25f7b5",
        "parentId" : "814032e3-3160-4940-a775-02a3b0ef7cd3",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "yea, something like `.scope`.",
        "createdAt" : "2021-03-03T04:55:42Z",
        "updatedAt" : "2021-03-16T03:58:12Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "a503bae4-4047-4953-9eee-f71af64b6d6f",
        "parentId" : "814032e3-3160-4940-a775-02a3b0ef7cd3",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> yea, something like `.scope`.\r\n\r\nyea. should we do it in this pr or a new one? since add tag and check is different things",
        "createdAt" : "2021-03-03T05:14:48Z",
        "updatedAt" : "2021-03-16T03:58:12Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "e6cf2845-1561-4c19-b407-3e0f701e854d",
        "parentId" : "814032e3-3160-4940-a775-02a3b0ef7cd3",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "We can do that in a new PR, and merge that PR first.",
        "createdAt" : "2021-03-03T06:42:06Z",
        "updatedAt" : "2021-03-16T03:58:12Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "17ba5fc6-a0d8-475d-9a47-8b018825b173",
        "parentId" : "814032e3-3160-4940-a775-02a3b0ef7cd3",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> We can do that in a new PR, and merge that PR first.\r\n\r\nOk, I think it's a very usage improvement, since many user don't understand spark's configuration very well.",
        "createdAt" : "2021-03-03T07:14:18Z",
        "updatedAt" : "2021-03-16T03:58:12Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "024929bda650d0125a3275355deb163426f61760",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +909,913 @@      \"spark.driver.resource\", PYSPARK_DRIVER_PYTHON, PYSPARK_PYTHON, SPARKR_R_SHELL,\n      CHILD_PROCESS_LOGGER_NAME, CHILD_CONNECTION_TIMEOUT, DRIVER_USER_CLASS_PATH_FIRST.key,\n      \"spark.yarn.*\")\n\n    /**"
  },
  {
    "id" : "1593356a-25a9-4985-92e2-05f505e0c253",
    "prId" : 30042,
    "prUrl" : "https://github.com/apache/spark/pull/30042#pullrequestreview-508988307",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9a74ff68-b1f3-4d4d-9d7d-37748321d24e",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "let's add deprecated annotation.",
        "createdAt" : "2020-10-15T04:37:15Z",
        "updatedAt" : "2020-10-15T11:43:45Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "b13b437f-fe41-4762-9220-38d407234976",
        "parentId" : "9a74ff68-b1f3-4d4d-9d7d-37748321d24e",
        "authorId" : "cce1c782-0596-44b6-8b99-6b77d2cca53c",
        "body" : "done",
        "createdAt" : "2020-10-15T06:01:30Z",
        "updatedAt" : "2020-10-15T11:43:45Z",
        "lastEditedBy" : "cce1c782-0596-44b6-8b99-6b77d2cca53c",
        "tags" : [
        ]
      }
    ],
    "commit" : "aa7bf5b50c4388e6caa24826d1bc83572af60e5a",
    "line" : 26,
    "diffHunk" : "@@ -1,1 +986,990 @@   */\n  @deprecated(\"This method is deprecated and will be removed in future versions.\", \"3.1.0\")\n  def setActiveSession(session: SparkSession): Unit = {\n    if (SQLConf.get.legacyAllowModifyActiveSession) {\n      setActiveSessionInternal(session)"
  },
  {
    "id" : "14846103-5465-4eed-9964-746bd92fe953",
    "prId" : 30042,
    "prUrl" : "https://github.com/apache/spark/pull/30042#pullrequestreview-509954447",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4f3b23b9-8325-41f4-819c-5808467e2ea2",
        "parentId" : null,
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "Shall we add some comments to tell users how to restore here or in the error msg?",
        "createdAt" : "2020-10-15T08:47:42Z",
        "updatedAt" : "2020-10-15T11:43:45Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "f406250c-80a7-49ac-a856-a04573a3581e",
        "parentId" : "4f3b23b9-8325-41f4-819c-5808467e2ea2",
        "authorId" : "cce1c782-0596-44b6-8b99-6b77d2cca53c",
        "body" : "unlike other deprecated API, this API does not have a better \"Option\"; we kind of hoping that user not touch these API at all. I will consider putting the legacy config in the error message as a hint in case user really really need these two API.",
        "createdAt" : "2020-10-15T08:58:41Z",
        "updatedAt" : "2020-10-15T11:43:45Z",
        "lastEditedBy" : "cce1c782-0596-44b6-8b99-6b77d2cca53c",
        "tags" : [
        ]
      },
      {
        "id" : "ad6c9bfd-6757-4b1d-8e79-c23da501eaa8",
        "parentId" : "4f3b23b9-8325-41f4-819c-5808467e2ea2",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "I agree that these APIs are not something users can call casually. \r\nBut I am not sure the deprecated annotation can cover the behavior change here. The config is internal and will not go to the public doc. Maybe we should add a migration guide if this is not only deprecated but also desupported",
        "createdAt" : "2020-10-15T09:18:21Z",
        "updatedAt" : "2020-10-15T11:43:45Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "af6922e1-4132-479a-b6c6-0a41619b63b0",
        "parentId" : "4f3b23b9-8325-41f4-819c-5808467e2ea2",
        "authorId" : "cce1c782-0596-44b6-8b99-6b77d2cca53c",
        "body" : "sure, i will update the migration guide as well.",
        "createdAt" : "2020-10-15T09:42:06Z",
        "updatedAt" : "2020-10-15T11:43:45Z",
        "lastEditedBy" : "cce1c782-0596-44b6-8b99-6b77d2cca53c",
        "tags" : [
        ]
      },
      {
        "id" : "4311b219-d8da-4544-b9d7-c751b22e1fc8",
        "parentId" : "4f3b23b9-8325-41f4-819c-5808467e2ea2",
        "authorId" : "cce1c782-0596-44b6-8b99-6b77d2cca53c",
        "body" : "done",
        "createdAt" : "2020-10-16T01:33:14Z",
        "updatedAt" : "2020-10-16T01:33:14Z",
        "lastEditedBy" : "cce1c782-0596-44b6-8b99-6b77d2cca53c",
        "tags" : [
        ]
      }
    ],
    "commit" : "aa7bf5b50c4388e6caa24826d1bc83572af60e5a",
    "line" : 25,
    "diffHunk" : "@@ -1,1 +985,989 @@   * @since 2.0.0\n   */\n  @deprecated(\"This method is deprecated and will be removed in future versions.\", \"3.1.0\")\n  def setActiveSession(session: SparkSession): Unit = {\n    if (SQLConf.get.legacyAllowModifyActiveSession) {"
  },
  {
    "id" : "eea54e70-9db6-4f34-abe7-0de361c8a203",
    "prId" : 30042,
    "prUrl" : "https://github.com/apache/spark/pull/30042#pullrequestreview-528758703",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c437093c-f77e-4994-ae62-77c94e9afd16",
        "parentId" : null,
        "authorId" : "2d6b46ba-4100-4c4d-9341-fff5e39647ec",
        "body" : "Can we add the flag in the error message? Then users will know how to unblock themselves without googling.",
        "createdAt" : "2020-11-12T03:48:47Z",
        "updatedAt" : "2020-11-12T03:48:47Z",
        "lastEditedBy" : "2d6b46ba-4100-4c4d-9341-fff5e39647ec",
        "tags" : [
        ]
      },
      {
        "id" : "a0edded6-cd32-4900-8fb8-f6d604ddf057",
        "parentId" : "c437093c-f77e-4994-ae62-77c94e9afd16",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "+1. @leanken can you create a follow-up? thanks!",
        "createdAt" : "2020-11-12T03:55:47Z",
        "updatedAt" : "2020-11-12T03:55:48Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "59466079-0a49-4094-93f6-bb42acb55f81",
        "parentId" : "c437093c-f77e-4994-ae62-77c94e9afd16",
        "authorId" : "cce1c782-0596-44b6-8b99-6b77d2cca53c",
        "body" : "OK.",
        "createdAt" : "2020-11-12T05:25:56Z",
        "updatedAt" : "2020-11-12T05:25:56Z",
        "lastEditedBy" : "cce1c782-0596-44b6-8b99-6b77d2cca53c",
        "tags" : [
        ]
      }
    ],
    "commit" : "aa7bf5b50c4388e6caa24826d1bc83572af60e5a",
    "line" : 30,
    "diffHunk" : "@@ -1,1 +990,994 @@      setActiveSessionInternal(session)\n    } else {\n      throw new UnsupportedOperationException(\"Not allowed to modify active Spark session.\")\n    }\n  }"
  },
  {
    "id" : "85e4ca61-139c-4bf3-bf0c-de5e29a13ac7",
    "prId" : 28868,
    "prUrl" : "https://github.com/apache/spark/pull/28868#pullrequestreview-440843122",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "06e859ad-74eb-49ac-996b-f5cbdf98fce6",
        "parentId" : null,
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "unify the entrance of active session.",
        "createdAt" : "2020-07-01T12:56:48Z",
        "updatedAt" : "2020-07-01T12:56:49Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      }
    ],
    "commit" : "a3b7d1e3191ac6ea6d396c0ffff49bc4aade76b8",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +759,763 @@    // set and not the default session. This to prevent that we promote the default session to the\n    // active session once we are done.\n    val old = SparkSession.getActiveSession.get\n    SparkSession.setActiveSession(this)\n    try block finally {"
  },
  {
    "id" : "6699f82e-86e4-45eb-83ed-2fc11e5771d2",
    "prId" : 28868,
    "prUrl" : "https://github.com/apache/spark/pull/28868#pullrequestreview-441684179",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1f33d23e-94a5-45e6-b87c-b6c6e5b7f94d",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I don't think it's necessary. A user may get the active session instance, and after that someone else stops the spark context. This check can't help this case.",
        "createdAt" : "2020-07-02T13:15:57Z",
        "updatedAt" : "2020-07-02T13:15:57Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "5515c047-a97a-40ba-b140-6b41a577c6ef",
        "parentId" : "1f33d23e-94a5-45e6-b87c-b6c6e5b7f94d",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "That's true. This place only checks if user get an active session, existing used session can not be covered. But in some case which used `session.withActive()` this can do the help.",
        "createdAt" : "2020-07-02T13:35:32Z",
        "updatedAt" : "2020-07-02T13:35:33Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      }
    ],
    "commit" : "a3b7d1e3191ac6ea6d396c0ffff49bc4aade76b8",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +1022,1026 @@    } else {\n      val session = Option(activeThreadSession.get)\n      session.foreach(_.sparkContext.assertNotStopped())\n      session\n    }"
  },
  {
    "id" : "2d9c83f2-75c7-4712-b3f7-4922c360fece",
    "prId" : 28778,
    "prUrl" : "https://github.com/apache/spark/pull/28778#pullrequestreview-427868937",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0c704769-1a1c-4c29-aefd-0655888db611",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "so we add a config, whose only usage is to let users get the config value?",
        "createdAt" : "2020-06-10T08:54:37Z",
        "updatedAt" : "2020-06-10T10:46:04Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "fd637423-638a-444a-a09f-cd235dd0ac7d",
        "parentId" : "0c704769-1a1c-4c29-aefd-0655888db611",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "As I said above. If add this config, I will move the exists `defaultParallelism` which in sql module follow up. e.g. [FilePartition.maxSplitBytes()](https://github.com/apache/spark/blob/82ff29be7afa2ff6350310ab9bdf6b474398fdc1/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FilePartition.scala#L91)",
        "createdAt" : "2020-06-10T09:03:23Z",
        "updatedAt" : "2020-06-10T10:46:04Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      },
      {
        "id" : "84ea75b0-24a0-41f0-bec3-b0b7dbf98bf9",
        "parentId" : "0c704769-1a1c-4c29-aefd-0655888db611",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "just do this in this pr ?",
        "createdAt" : "2020-06-10T09:07:36Z",
        "updatedAt" : "2020-06-10T10:46:04Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      },
      {
        "id" : "66c0a59d-d5bb-401a-8912-2bf5f695ab73",
        "parentId" : "0c704769-1a1c-4c29-aefd-0655888db611",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "please do, otherwise it's a useless config",
        "createdAt" : "2020-06-10T09:17:53Z",
        "updatedAt" : "2020-06-10T10:46:04Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "55d367f6a7a277c83eff1ac83dd7708cfca608e9",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +187,191 @@   */\n  def defaultParallelism: Int = {\n    sessionState.conf.defaultParallelism.getOrElse(sparkContext.defaultParallelism)\n  }\n"
  },
  {
    "id" : "82c60db1-e329-4e9a-bd10-aaab503ce1f8",
    "prId" : 28778,
    "prUrl" : "https://github.com/apache/spark/pull/28778#pullrequestreview-428823832",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5f02a25d-bfa8-4769-a0a3-2f6382d166db",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I'd like to not have this API, as `SparkSession` should provide high-level logical APIs, not physical ones.",
        "createdAt" : "2020-06-11T11:32:32Z",
        "updatedAt" : "2020-06-11T11:32:32Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "55d367f6a7a277c83eff1ac83dd7708cfca608e9",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +186,190 @@   * @since 3.1.0\n   */\n  def defaultParallelism: Int = {\n    sessionState.conf.defaultParallelism.getOrElse(sparkContext.defaultParallelism)\n  }"
  },
  {
    "id" : "1a7e996c-63b4-4869-84e3-dbe8f06985ed",
    "prId" : 27400,
    "prUrl" : "https://github.com/apache/spark/pull/27400#pullrequestreview-351307512",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ff366d3e-7cd0-4604-b176-cd7a97e066eb",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "```suggestion\r\n  lazy val emptyDataFrame: DataFrame = emptyDataset(RowEncoder(new StructType()))\r\n```\r\n?",
        "createdAt" : "2020-01-30T16:58:25Z",
        "updatedAt" : "2020-01-30T16:58:42Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "82f488cc-fb37-41f9-9dfd-00388180244e",
        "parentId" : "ff366d3e-7cd0-4604-b176-cd7a97e066eb",
        "authorId" : "80d631a6-73e8-46a6-a01b-a80d1f1cc6cc",
        "body" : "Why would that be better? This is basically re-implementing Dataset.ofRows.",
        "createdAt" : "2020-01-30T17:07:08Z",
        "updatedAt" : "2020-01-30T17:07:08Z",
        "lastEditedBy" : "80d631a6-73e8-46a6-a01b-a80d1f1cc6cc",
        "tags" : [
        ]
      },
      {
        "id" : "3df226b4-f5af-455e-9d2d-936db9f64425",
        "parentId" : "ff366d3e-7cd0-4604-b176-cd7a97e066eb",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "This is arguable for sure. DataFrame is Dataset[Row]. We already have a function which can construct empty Dataset[T], why not just re-use it. `emptyDataFrame` is some kind of specialization of `emptyDataset`.",
        "createdAt" : "2020-01-30T17:16:41Z",
        "updatedAt" : "2020-01-30T17:16:41Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "ac09f436-5371-4c08-a981-88e0c4186604",
        "parentId" : "ff366d3e-7cd0-4604-b176-cd7a97e066eb",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "If we take this suggestion, maybe: `emptyDataset(RowEncoder(StructType(Nil)))`. ",
        "createdAt" : "2020-01-31T01:37:31Z",
        "updatedAt" : "2020-01-31T01:37:48Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "67c45956-7e9e-4b2f-8b29-232ac4d88803",
        "parentId" : "ff366d3e-7cd0-4604-b176-cd7a97e066eb",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "`Dataset.ofRows(self, LocalRelation())` looks simpler as I don't need to jump into another method when reading the code.",
        "createdAt" : "2020-01-31T06:06:08Z",
        "updatedAt" : "2020-01-31T06:06:09Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "4bab4008f1471e8a687294f04082c3931a6e464f",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +275,279 @@   */\n  @transient\n  lazy val emptyDataFrame: DataFrame = Dataset.ofRows(self, LocalRelation())\n\n  /**"
  },
  {
    "id" : "8609fbca-466a-46e6-9ca6-0d8a71689856",
    "prId" : 27387,
    "prUrl" : "https://github.com/apache/spark/pull/27387#pullrequestreview-350780242",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "868ec374-ac08-4f8d-aaa5-a25bb49f5866",
        "parentId" : null,
        "authorId" : "80d631a6-73e8-46a6-a01b-a80d1f1cc6cc",
        "body" : "I am reasonably sure that there are more methods for which we need to add `withActive`.",
        "createdAt" : "2020-01-30T08:59:26Z",
        "updatedAt" : "2020-02-13T14:38:24Z",
        "lastEditedBy" : "80d631a6-73e8-46a6-a01b-a80d1f1cc6cc",
        "tags" : [
        ]
      },
      {
        "id" : "751b9d1d-40c0-420a-9195-9dae52e2c0ac",
        "parentId" : "868ec374-ac08-4f8d-aaa5-a25bb49f5866",
        "authorId" : "80d631a6-73e8-46a6-a01b-a80d1f1cc6cc",
        "body" : "For example `SparkSession.sql(..)`",
        "createdAt" : "2020-01-30T12:17:06Z",
        "updatedAt" : "2020-02-13T14:38:24Z",
        "lastEditedBy" : "80d631a6-73e8-46a6-a01b-a80d1f1cc6cc",
        "tags" : [
        ]
      }
    ],
    "commit" : "7f6274d1605aaef56e2cf978d9d0db927e6386b6",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +294,298 @@   * @since 2.0.0\n   */\n  def createDataFrame[A <: Product : TypeTag](rdd: RDD[A]): DataFrame = withActive {\n    val encoder = Encoders.product[A]\n    Dataset.ofRows(self, ExternalRDD(rdd, self)(encoder))"
  },
  {
    "id" : "8e519de0-372f-41bb-a08d-421d0735d8be",
    "prId" : 24807,
    "prUrl" : "https://github.com/apache/spark/pull/24807#pullrequestreview-246287575",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6457c91d-1f6b-4200-a561-0283acb48e96",
        "parentId" : null,
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "Shouldn't this remove itself from the active sessions before checking that there are 0 remaining?",
        "createdAt" : "2019-06-05T18:55:17Z",
        "updatedAt" : "2019-06-14T17:42:30Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      },
      {
        "id" : "eb81bfef-d0cc-4eb0-8320-3d7f16751ed4",
        "parentId" : "6457c91d-1f6b-4200-a561-0283acb48e96",
        "authorId" : "7461a219-d8ed-4555-89b3-b7d7770b2e9f",
        "body" : "ah, good catch",
        "createdAt" : "2019-06-05T22:20:32Z",
        "updatedAt" : "2019-06-14T17:42:30Z",
        "lastEditedBy" : "7461a219-d8ed-4555-89b3-b7d7770b2e9f",
        "tags" : [
        ]
      }
    ],
    "commit" : "92c7b22ab5a1db0f25b4b418277b793b0e851491",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +718,722 @@  def stop(): Unit = {\n    SparkSession.clearActiveSession()\n    if (SparkSession.numActiveSessions.get() == 0) {\n      sparkContext.stop()\n    }"
  },
  {
    "id" : "85e4c4c3-a3db-4cc4-8d1e-78239334e0da",
    "prId" : 24807,
    "prUrl" : "https://github.com/apache/spark/pull/24807#pullrequestreview-248156488",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8382c4e1-0caa-442e-9521-237e51cb00e0",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Hey, I think this was a design decision that stopping sessions stops spark context too. Why don't you just don't call `stop()` on the session since what it does it just stops the session? Seems like the behaviour is documented properly as well.",
        "createdAt" : "2019-06-07T02:53:05Z",
        "updatedAt" : "2019-06-14T17:42:30Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "6621b567-1313-4278-b352-ac631413f63e",
        "parentId" : "8382c4e1-0caa-442e-9521-237e51cb00e0",
        "authorId" : "806aa501-9ea7-4cec-b017-a84d9a1602d8",
        "body" : "The idea is that if one creates a multi-tenant Spark process, and you give each user a Spark session, you want to be able to close down the resources for one session (e.g. connections to JDBC, perhaps), but not stop the entire Spark Context, thus keeping the Spark Context alive for the other users.",
        "createdAt" : "2019-06-08T01:04:03Z",
        "updatedAt" : "2019-06-14T17:42:30Z",
        "lastEditedBy" : "806aa501-9ea7-4cec-b017-a84d9a1602d8",
        "tags" : [
        ]
      },
      {
        "id" : "2581140c-a94a-412d-9c64-e844af6d0c12",
        "parentId" : "8382c4e1-0caa-442e-9521-237e51cb00e0",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "@HyukjinKwon I agree but see http://mail-archives.apache.org/mod_mbox/spark-dev/201904.mbox/%3CCAO4re1=Nk1E1VwGzSZwQ5x0SY=_hEUpmEd8N5yYDccML_T56rA@mail.gmail.com%3E",
        "createdAt" : "2019-06-11T12:59:11Z",
        "updatedAt" : "2019-06-14T17:42:30Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      }
    ],
    "commit" : "92c7b22ab5a1db0f25b4b418277b793b0e851491",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +716,720 @@   * @since 2.0.0\n   */\n  def stop(): Unit = {\n    SparkSession.clearActiveSession()\n    if (SparkSession.numActiveSessions.get() == 0) {"
  },
  {
    "id" : "7d4b7cf7-6a2d-4558-bac0-86ed40986dd9",
    "prId" : 24807,
    "prUrl" : "https://github.com/apache/spark/pull/24807#pullrequestreview-247386197",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d54a1011-d671-4fa6-b878-41b78254cc6d",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "For me, the existing behavior of this function is a quiet no-op and looks better.",
        "createdAt" : "2019-06-09T00:37:16Z",
        "updatedAt" : "2019-06-14T17:42:30Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "92c7b22ab5a1db0f25b4b418277b793b0e851491",
    "line" : 90,
    "diffHunk" : "@@ -1,1 +1028,1032 @@    if (getDefaultSession.isDefined) {\n      defaultSession.set(null)\n    }\n  }\n"
  },
  {
    "id" : "191a3e29-fd5a-412b-8dec-4c1111df9f67",
    "prId" : 24807,
    "prUrl" : "https://github.com/apache/spark/pull/24807#pullrequestreview-252036403",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "774ac7d2-cd0a-4ae3-9153-7819aefc09e5",
        "parentId" : null,
        "authorId" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "body" : "Should we also clear the defaultSession() ? Otherwise, the `getOrCreate()` method still returns the previously created SparkSession",
        "createdAt" : "2019-06-17T23:57:35Z",
        "updatedAt" : "2019-06-18T00:14:04Z",
        "lastEditedBy" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "tags" : [
        ]
      },
      {
        "id" : "e9da6894-fb07-42b0-a35b-56b0aa2e0d36",
        "parentId" : "774ac7d2-cd0a-4ae3-9153-7819aefc09e5",
        "authorId" : "7461a219-d8ed-4555-89b3-b7d7770b2e9f",
        "body" : "I thought about this as well and here were my thoughts: \r\n\r\n1. The default session should simply be a \"default\" set of config that is applied when creating a new `SparkSession`, unless overrides to that config are specified. The only notion of `SparkSessions` that we care about is the set of active spark sessions. For that reason, I think clearing the default session may not be necessary here.\r\n2. We could completely get rid of the notion of default sessions (which I wouldn't be opposed to), and instead require that upon session creation time, each user is required to specify the full list of settings that they want in order to create their Session. How would people feel about this Alternatively, we could say that the default session is a session that is completely hidden from the user and that we grab configs from the first created active `SparkSession` (using `getOrCreate()`) to populate the config. This seems seems like confusing behavior to me though. ",
        "createdAt" : "2019-06-19T12:17:33Z",
        "updatedAt" : "2019-06-19T12:17:33Z",
        "lastEditedBy" : "7461a219-d8ed-4555-89b3-b7d7770b2e9f",
        "tags" : [
        ]
      },
      {
        "id" : "950f84b5-9897-4685-a75c-1f0658956760",
        "parentId" : "774ac7d2-cd0a-4ae3-9153-7819aefc09e5",
        "authorId" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "body" : "The problem is now we update both `activeSession` and `defaultSession` when we create a new SparkSession. If we don't clear the defaultSession here then we will still have defaultSession returned every time we call `getOrCreate()`. Whether to keep the notion of default session worth another mail thread or PR, and the quickest way to make this PR goes in would be to clear the defaultSession here, IIUC. ",
        "createdAt" : "2019-06-20T00:45:46Z",
        "updatedAt" : "2019-06-20T00:45:46Z",
        "lastEditedBy" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "92c7b22ab5a1db0f25b4b418277b793b0e851491",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +717,721 @@   */\n  def stop(): Unit = {\n    SparkSession.clearActiveSession()\n    if (SparkSession.numActiveSessions.get() == 0) {\n      sparkContext.stop()"
  },
  {
    "id" : "eac023d6-2c2c-4147-bd0d-584f78e4a61d",
    "prId" : 24807,
    "prUrl" : "https://github.com/apache/spark/pull/24807#pullrequestreview-252930994",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7b11f7d6-7202-47dd-ae2d-7ba82d8304a2",
        "parentId" : null,
        "authorId" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "body" : "This is error prone and also not easy to debug, we may need to keep track of each active SparkSession in SparkContext. We don't need to pass in the SparkSession instance, maybe only compute the `System.identityHashCode()` of current SparkSession object and pass that to the SparkContext. On SparkSession.stop() we only need to drop the corresponding identity from SparkContext, and when it's empty it means we can safely stop the SparkContext, too.",
        "createdAt" : "2019-06-18T00:07:16Z",
        "updatedAt" : "2019-06-18T00:14:04Z",
        "lastEditedBy" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "tags" : [
        ]
      },
      {
        "id" : "8d9ade9a-97ad-4b02-976f-d9e705e1ca75",
        "parentId" : "7b11f7d6-7202-47dd-ae2d-7ba82d8304a2",
        "authorId" : "7461a219-d8ed-4555-89b3-b7d7770b2e9f",
        "body" : "Yeah, I think the identity hash code could be an interesting solution here. I'll experiment with that",
        "createdAt" : "2019-06-21T16:58:54Z",
        "updatedAt" : "2019-06-21T16:58:54Z",
        "lastEditedBy" : "7461a219-d8ed-4555-89b3-b7d7770b2e9f",
        "tags" : [
        ]
      }
    ],
    "commit" : "92c7b22ab5a1db0f25b4b418277b793b0e851491",
    "line" : 31,
    "diffHunk" : "@@ -1,1 +780,784 @@object SparkSession extends Logging {\n\n  private[spark] val numActiveSessions: AtomicInteger = new AtomicInteger(0)\n\n  /**"
  },
  {
    "id" : "958730bc-94d5-4901-807e-c5856e50458d",
    "prId" : 24807,
    "prUrl" : "https://github.com/apache/spark/pull/24807#pullrequestreview-252930845",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "61d66c3e-6070-47b6-8def-e15c228f9f07",
        "parentId" : null,
        "authorId" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "body" : "I think this should be called in the `SparkSession.stop()` method?",
        "createdAt" : "2019-06-18T00:10:02Z",
        "updatedAt" : "2019-06-18T00:14:04Z",
        "lastEditedBy" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "tags" : [
        ]
      },
      {
        "id" : "889d7b07-4e18-481c-9257-a42f8af671ed",
        "parentId" : "61d66c3e-6070-47b6-8def-e15c228f9f07",
        "authorId" : "7461a219-d8ed-4555-89b3-b7d7770b2e9f",
        "body" : "Yeah, that would be ideal - the problem is that we lost the handle to this listener outside of this method. I could create a global `var` in the `SparkSession` to hold a reference to the listener, but that also seems kind of strange. Thoughts? ",
        "createdAt" : "2019-06-19T12:18:29Z",
        "updatedAt" : "2019-06-19T12:18:29Z",
        "lastEditedBy" : "7461a219-d8ed-4555-89b3-b7d7770b2e9f",
        "tags" : [
        ]
      },
      {
        "id" : "02973f8f-1a80-44d6-85c6-025ed793cc00",
        "parentId" : "61d66c3e-6070-47b6-8def-e15c228f9f07",
        "authorId" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "body" : "It should be fine to have a global var of SparkListener inside SparkSession.",
        "createdAt" : "2019-06-20T00:47:00Z",
        "updatedAt" : "2019-06-20T00:47:00Z",
        "lastEditedBy" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "tags" : [
        ]
      },
      {
        "id" : "2dbd1fb0-f8da-4813-9882-958926c6505f",
        "parentId" : "61d66c3e-6070-47b6-8def-e15c228f9f07",
        "authorId" : "7461a219-d8ed-4555-89b3-b7d7770b2e9f",
        "body" : "Okay, will add this in!",
        "createdAt" : "2019-06-21T16:58:38Z",
        "updatedAt" : "2019-06-21T16:58:38Z",
        "lastEditedBy" : "7461a219-d8ed-4555-89b3-b7d7770b2e9f",
        "tags" : [
        ]
      }
    ],
    "commit" : "92c7b22ab5a1db0f25b4b418277b793b0e851491",
    "line" : 41,
    "diffHunk" : "@@ -1,1 +965,969 @@            defaultSession.set(null)\n            // Should remove listener after this event fires\n            sparkContext.removeSparkListener(this)\n          }\n        })"
  },
  {
    "id" : "0fd67b64-7dd9-4bf7-b2d1-61d2568e9e04",
    "prId" : 24807,
    "prUrl" : "https://github.com/apache/spark/pull/24807#pullrequestreview-370044008",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bea210c5-fd4e-450f-ad2a-7fa9eb697606",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "The problem I see here is: if I have 2 sessions, and I set one as active then set another. If I keep doing this then the count here will be wrong.\r\n\r\nI don't have a good idea to track the last alive session. Even if we can, users may want to create more sessions later and not stop the SparkContext.\r\n\r\nThe `SparkSession` should be light-weighted, what's the memory leak you observed before?",
        "createdAt" : "2020-03-03T11:34:04Z",
        "updatedAt" : "2020-03-03T11:34:04Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "50885a7e-780a-444c-9e9b-0c86db027d2a",
        "parentId" : "bea210c5-fd4e-450f-ad2a-7fa9eb697606",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Another problem is that the current behaviour is clearly documented. It doesn't look particularly wrong either:\r\n\r\n> Stop the underlying `SparkContext`\r\n\r\nWe're trying to make a behaviour change just based on the new design choice.\r\nShell we at least keep the compatibility via a switch?",
        "createdAt" : "2020-03-03T13:55:13Z",
        "updatedAt" : "2020-03-03T13:55:13Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "a779c90e-f6fe-4829-85ea-8b2bf1297e4c",
        "parentId" : "bea210c5-fd4e-450f-ad2a-7fa9eb697606",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "yea, the right way to \"free\" a session is to leave it and wait for it to be GCed. If there is something that can't be GCed, it's a memory leak and we should fix it.",
        "createdAt" : "2020-03-03T14:05:09Z",
        "updatedAt" : "2020-03-03T14:05:09Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "457f18d2-1c80-44ff-8f15-aeac5fc6d818",
        "parentId" : "bea210c5-fd4e-450f-ad2a-7fa9eb697606",
        "authorId" : "7461a219-d8ed-4555-89b3-b7d7770b2e9f",
        "body" : "@cloud-fan @HyukjinKwon - thanks for the thoughts. \r\n\r\n@cloud-fan - The memory leak is detailed here https://github.com/apache/spark/pull/24807#issuecomment-500871806.\r\n\r\n@HyukjinKwon - I actually think that despite the fact that the current behavior is clearly documented, it actually doesn't make sense and is error prone. I detailed an example here https://github.com/apache/spark/pull/24807#issuecomment-500871806 of how this could bit us. \r\n\r\nIs there a world where a user should be able to call `.stop()` on a session and forcibly invalidate every other potential session (by killing the context)?",
        "createdAt" : "2020-03-04T12:43:21Z",
        "updatedAt" : "2020-03-04T12:43:22Z",
        "lastEditedBy" : "7461a219-d8ed-4555-89b3-b7d7770b2e9f",
        "tags" : [
        ]
      },
      {
        "id" : "b5cf5ec2-0a8c-4926-9315-5ce0a1c91be7",
        "parentId" : "bea210c5-fd4e-450f-ad2a-7fa9eb697606",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "If the leak is problem, we should fix it rather than changing the behaviour. It is documented and users are relying on this behaviour\r\nWe can understand `.stop()` is like `.stopContext()`, no?\r\n\r\nI don't think we should just change without guarding. All other projects related to Spark such as Zeppelin would need to revisit their behaviour about how to stop, and it would make it difficult them to support multiple Spark versions for instance.",
        "createdAt" : "2020-03-06T02:16:48Z",
        "updatedAt" : "2020-03-06T02:16:48Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "92c7b22ab5a1db0f25b4b418277b793b0e851491",
    "line" : 51,
    "diffHunk" : "@@ -1,1 +990,994 @@  def setActiveSession(session: SparkSession): Unit = {\n    if (session != getActiveSession.get && getActiveSession.isDefined) {\n      numActiveSessions.getAndIncrement\n      activeThreadSession.set(session)\n    } else if (session == null) {"
  }
]