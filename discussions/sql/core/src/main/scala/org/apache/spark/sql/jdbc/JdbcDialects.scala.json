[
  {
    "id" : "12ddfc19-550a-4844-b621-b05a28875d3c",
    "prId" : 30154,
    "prUrl" : "https://github.com/apache/spark/pull/30154#pullrequestreview-518879651",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6ffba41b-71bc-4aee-a1ae-6a0231ef5c53",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "it's a separate statement. Is it possible to specify table comment during table creation?",
        "createdAt" : "2020-10-28T08:00:22Z",
        "updatedAt" : "2020-11-06T06:50:03Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "b881029d-4fb6-48db-9986-1b7ab4b9e9a8",
        "parentId" : "6ffba41b-71bc-4aee-a1ae-6a0231ef5c53",
        "authorId" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "body" : "Seems only MySQL has a way to let user to specify table options such as `COMMENT`, `ENGINE`, etc. in `CREATE TABLE`. I use a separate statement for MySQL anyway to make it consistent with others.",
        "createdAt" : "2020-10-28T16:50:43Z",
        "updatedAt" : "2020-11-06T06:50:03Z",
        "lastEditedBy" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "tags" : [
        ]
      }
    ],
    "commit" : "ecc9a4e69af4d22a5df51abec0eefd6887ff5beb",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +268,272 @@\n  def getTableCommentQuery(table: String, comment: String): String = {\n    s\"COMMENT ON TABLE $table IS '$comment'\"\n  }\n"
  },
  {
    "id" : "ca7a5171-0e8b-454f-8b7e-bacca949fded",
    "prId" : 29952,
    "prUrl" : "https://github.com/apache/spark/pull/29952#pullrequestreview-504176601",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8a9f24be-f559-4a89-8fbf-da429454faa0",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "since we already have a `testH2Dialect`, how about we update `testH2Dialect` to implement `classifyException`, and use it in `JDBCTableCatalogSuite`? Then we don't need to have an official `H2Dialect`.",
        "createdAt" : "2020-10-07T17:36:18Z",
        "updatedAt" : "2020-10-07T17:36:18Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "f1c553ba-bb13-46d5-993a-7b6e2e1653f0",
        "parentId" : "8a9f24be-f559-4a89-8fbf-da429454faa0",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "If you look at `testH2Dialect`, it has test specific settings. Why should we have the settings in `JDBCTableCatalogSuite`?\r\n\r\n> Then we don't need to have an official `H2Dialect`.\r\n\r\nWhat is the problem to have built-in H2Dialect?",
        "createdAt" : "2020-10-07T18:21:03Z",
        "updatedAt" : "2020-10-07T18:21:04Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "7cd02227-306a-4472-9f97-a72f2c2554dd",
        "parentId" : "8a9f24be-f559-4a89-8fbf-da429454faa0",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Probably, not related to this. If we don't want to support H2 officially as a dialect, why do we test it so broadly in Spark.?Maybe it makes sense to switch all internal Spark tests to Derby?",
        "createdAt" : "2020-10-07T18:55:02Z",
        "updatedAt" : "2020-10-07T18:55:02Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "ac55879e80bbb964c1563466a8171f7a8fb2720d",
    "line" : 29,
    "diffHunk" : "@@ -1,1 +309,313 @@  registerDialect(OracleDialect)\n  registerDialect(TeradataDialect)\n  registerDialect(H2Dialect)\n\n  /**"
  },
  {
    "id" : "493b7671-df92-4431-a659-17425db616b4",
    "prId" : 29324,
    "prUrl" : "https://github.com/apache/spark/pull/29324#pullrequestreview-461373839",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "673bf69f-2530-451e-9d4b-f83f4bc0a2e1",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Because you will override this method in other places, not here. Remember to remove this later. :)",
        "createdAt" : "2020-08-05T06:28:58Z",
        "updatedAt" : "2020-08-05T07:11:02Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "ef55b5ee728a16c67c993eabecd37aec2587d0fe",
    "line" : 34,
    "diffHunk" : "@@ -1,1 +200,204 @@  /**\n   * Alter an existing table.\n   * TODO (SPARK-32523): Override this method in the dialects that have different syntax.\n   *\n   * @param tableName The name of the table to be altered."
  },
  {
    "id" : "3626ce17-a9cd-4358-a4c9-a0b56807e16a",
    "prId" : 26230,
    "prUrl" : "https://github.com/apache/spark/pull/26230#pullrequestreview-306576830",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dea5efc2-8ae6-43ec-9cb7-c003b58c0819",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Hmmm.. wait .. `properties: Map[String, String]` this will requires Scala map instead of Java map, if we should implement the dialect from Java side. We could just send Properties as are I suspect.",
        "createdAt" : "2019-10-24T08:25:59Z",
        "updatedAt" : "2019-10-24T08:25:59Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "041f4472-85cd-4c6a-9d9b-9227ab2f110a",
        "parentId" : "dea5efc2-8ae6-43ec-9cb7-c003b58c0819",
        "authorId" : "f06b291a-bc5c-4fd8-9bac-e5a6d035891b",
        "body" : "I chose scala Map because i saw the JdbcDialect.beforeFetch accepts scala Map as parameter.\r\nI agree with you, we may consider changing the parameter type for both beforeFetch and validateProperties, but i think we can leave this to another PR. \r\nWDYT?",
        "createdAt" : "2019-10-24T08:41:09Z",
        "updatedAt" : "2019-10-24T08:49:35Z",
        "lastEditedBy" : "f06b291a-bc5c-4fd8-9bac-e5a6d035891b",
        "tags" : [
        ]
      },
      {
        "id" : "fe894e0d-98ef-4415-bf96-d3c61914bdb9",
        "parentId" : "dea5efc2-8ae6-43ec-9cb7-c003b58c0819",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "I think it's OK to leave as-is here.",
        "createdAt" : "2019-10-24T13:33:53Z",
        "updatedAt" : "2019-10-24T13:33:53Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      }
    ],
    "commit" : "74f0325b039cb40d26566ae7c9cf82e916d23798",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +157,161 @@   * @param properties The connection properties.  This is passed through from the relation.\n   */\n  def validateProperties(properties: Map[String, String]): Unit = {\n    val fetchSize = properties.getOrElse(JDBCOptions.JDBC_BATCH_FETCH_SIZE, \"0\").toInt\n    require(fetchSize >= 0,"
  }
]