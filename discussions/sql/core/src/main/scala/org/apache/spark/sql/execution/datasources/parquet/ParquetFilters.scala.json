[
  {
    "id" : "87117b70-c3cd-4a7a-a3c6-f728c7ba478c",
    "prId" : 32568,
    "prUrl" : "https://github.com/apache/spark/pull/32568#pullrequestreview-660553560",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9a8de469-961d-486a-9987-a348823c404e",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Although this is HOTFIX, okay.",
        "createdAt" : "2021-05-17T05:04:16Z",
        "updatedAt" : "2021-05-17T05:04:16Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "40b1755551933fe6e64ccf12920c479e5bca88bf",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +301,305 @@        FilterApi.lt(longColumn(n), decimalToInt64(v.asInstanceOf[JBigDecimal]))\n    case ParquetSchemaType(_: DecimalLogicalTypeAnnotation, FIXED_LEN_BYTE_ARRAY, length)\n        if pushDownDecimal =>\n      (n: Array[String], v: Any) =>\n        FilterApi.lt(binaryColumn(n), decimalToByteArray(v.asInstanceOf[JBigDecimal], length))"
  },
  {
    "id" : "22c62cbe-c732-4502-8dbd-ffd879467dca",
    "prId" : 31776,
    "prUrl" : "https://github.com/apache/spark/pull/31776#pullrequestreview-652927095",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "56f17ebc-bd56-4313-9e34-0be12f54c7fc",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "cc @wangyum FYI",
        "createdAt" : "2021-05-06T03:20:37Z",
        "updatedAt" : "2021-05-10T06:28:22Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "2bc0391a879c009f5a1b43bb5ddd1d6c78af6c22",
    "line" : 44,
    "diffHunk" : "@@ -1,1 +108,112 @@      logicalTypeAnnotation: LogicalTypeAnnotation,\n      primitiveTypeName: PrimitiveTypeName,\n      length: Int)\n\n  private val ParquetBooleanType = ParquetSchemaType(null, BOOLEAN, 0)"
  },
  {
    "id" : "83291a50-e854-4e54-9119-943a63123f21",
    "prId" : 28699,
    "prUrl" : "https://github.com/apache/spark/pull/28699#pullrequestreview-422076676",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9e30562b-26f7-4983-9a0b-8a49d2d00cbf",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "This line causes the problem.",
        "createdAt" : "2020-06-01T18:23:04Z",
        "updatedAt" : "2020-06-01T18:23:04Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "4ba7e05073f47e9595fed34e035e219ca0ba19e1",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +152,156 @@    val timestamp = v.asInstanceOf[Timestamp]\n    val micros = DateTimeUtils.fromJavaTimestamp(timestamp)\n    val millis = DateTimeUtils.toMillis(micros)\n    millis.asInstanceOf[JLong]\n  }"
  },
  {
    "id" : "74694f25-e188-4655-a7ca-79cf589fc612",
    "prId" : 27817,
    "prUrl" : "https://github.com/apache/spark/pull/27817#pullrequestreview-370726830",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0c0d6d11-9f31-446e-8d2a-cbc86509bb2b",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Since Parquet is case-sensitive, could you check if we have a test case for this?",
        "createdAt" : "2020-03-06T22:43:44Z",
        "updatedAt" : "2020-03-07T06:28:38Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "d1f23df6-a210-4db2-9bd7-3f13404138a6",
        "parentId" : "0c0d6d11-9f31-446e-8d2a-cbc86509bb2b",
        "authorId" : "677aa336-324b-4b93-8300-21f8bd378f26",
        "body" : "There is code handling this, and I will check if there is a test.",
        "createdAt" : "2020-03-07T06:30:13Z",
        "updatedAt" : "2020-03-07T06:30:13Z",
        "lastEditedBy" : "677aa336-324b-4b93-8300-21f8bd378f26",
        "tags" : [
        ]
      }
    ],
    "commit" : "c5fc25f6ee70eb8a9e291020b51fecae580b8e8c",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +57,61 @@    val primitiveFields =\n    schema.getFields.asScala.filter(_.isPrimitive).map(_.asPrimitiveType()).map { f =>\n      quoteIfNeeded(f.getName) -> ParquetField(f.getName,\n        ParquetSchemaType(f.getOriginalType,\n          f.getPrimitiveTypeName, f.getTypeLength, f.getDecimalMetadata))"
  },
  {
    "id" : "b3a359fe-d158-4130-be48-f6c1d972d373",
    "prId" : 27728,
    "prUrl" : "https://github.com/apache/spark/pull/27728#pullrequestreview-366760707",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "aaf91ad3-3d99-4275-a951-49b4dd75f78a",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Can we add short comment here explaining what this method returns, especially for nested case?",
        "createdAt" : "2020-02-29T00:05:12Z",
        "updatedAt" : "2020-03-26T08:00:02Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "e854b589-d5fa-4b94-afed-11f0a32b01f3",
        "parentId" : "aaf91ad3-3d99-4275-a951-49b4dd75f78a",
        "authorId" : "677aa336-324b-4b93-8300-21f8bd378f26",
        "body" : "Done.",
        "createdAt" : "2020-02-29T00:54:50Z",
        "updatedAt" : "2020-03-26T08:00:03Z",
        "lastEditedBy" : "677aa336-324b-4b93-8300-21f8bd378f26",
        "tags" : [
        ]
      }
    ],
    "commit" : "5fd97c0a90eb1885a93fffb9d04a262b35f62bc3",
    "line" : 26,
    "diffHunk" : "@@ -1,1 +54,58 @@  // nested columns. If any part of the names contains `dots`, it is quoted to avoid confusion.\n  // See `org.apache.spark.sql.connector.catalog.quote` for implementation details.\n  private val nameToParquetField : Map[String, ParquetPrimitiveField] = {\n    // Recursively traverse the parquet schema to get primitive fields that can be pushed-down.\n    // `parentFieldNames` is used to keep track of the current nested level when traversing."
  },
  {
    "id" : "384791e9-3938-4965-b499-55cb5473f923",
    "prId" : 27728,
    "prUrl" : "https://github.com/apache/spark/pull/27728#pullrequestreview-366758986",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "02d64a49-fcc9-43bc-9938-7825cddd5e5a",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Based on the previous comment, we still need to prevent ``` `a.b`.c.d ```?",
        "createdAt" : "2020-02-29T00:13:25Z",
        "updatedAt" : "2020-03-26T08:00:02Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "60a8d3af-aab4-49ce-ba34-a09a31e76a27",
        "parentId" : "02d64a49-fcc9-43bc-9938-7825cddd5e5a",
        "authorId" : "677aa336-324b-4b93-8300-21f8bd378f26",
        "body" : "No. we don't have to. Should work.",
        "createdAt" : "2020-02-29T00:44:06Z",
        "updatedAt" : "2020-03-26T08:00:02Z",
        "lastEditedBy" : "677aa336-324b-4b93-8300-21f8bd378f26",
        "tags" : [
        ]
      }
    ],
    "commit" : "5fd97c0a90eb1885a93fffb9d04a262b35f62bc3",
    "line" : 83,
    "diffHunk" : "@@ -1,1 +494,498 @@\n  private def canMakeFilterOn(name: String, value: Any): Boolean = {\n    nameToParquetField.contains(name) && valueCanMakeFilterOn(name, value)\n  }\n"
  },
  {
    "id" : "520a9b55-dd58-4e2f-ba92-6adce20979ec",
    "prId" : 27728,
    "prUrl" : "https://github.com/apache/spark/pull/27728#pullrequestreview-368361469",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f2526f1f-fb76-49bd-8b8e-ff69138db77e",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "nit: move `import` outside `map?",
        "createdAt" : "2020-03-03T20:07:57Z",
        "updatedAt" : "2020-03-26T08:00:03Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "c1f58699-5985-47db-93b1-d4159891f06c",
        "parentId" : "f2526f1f-fb76-49bd-8b8e-ff69138db77e",
        "authorId" : "677aa336-324b-4b93-8300-21f8bd378f26",
        "body" : "this import contains implicit conversion, so I want to limit the scope.",
        "createdAt" : "2020-03-03T22:07:19Z",
        "updatedAt" : "2020-03-26T08:00:03Z",
        "lastEditedBy" : "677aa336-324b-4b93-8300-21f8bd378f26",
        "tags" : [
        ]
      }
    ],
    "commit" : "5fd97c0a90eb1885a93fffb9d04a262b35f62bc3",
    "line" : 49,
    "diffHunk" : "@@ -1,1 +77,81 @@\n    val primitiveFields = getPrimitiveFields(schema.getFields.asScala).map { field =>\n      import org.apache.spark.sql.connector.catalog.CatalogV2Implicits.MultipartIdentifierHelper\n      (field.fieldNames.toSeq.quoted, field)\n    }"
  },
  {
    "id" : "13f567dd-a055-471c-af5d-5cebc262bb8b",
    "prId" : 27574,
    "prUrl" : "https://github.com/apache/spark/pull/27574#pullrequestreview-358872249",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c5f6a4e8-4b03-49ef-a201-1752064650c3",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "good catch!",
        "createdAt" : "2020-02-14T11:15:09Z",
        "updatedAt" : "2020-02-14T11:15:09Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "5e632dfda94a7b657900f9d5ef9e8a0879663670",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +592,596 @@          if pushDownStartWith && canMakeFilterOn(name, prefix) =>\n        Option(prefix).map { v =>\n          FilterApi.userDefined(binaryColumn(nameToParquetField(name).fieldName),\n            new UserDefinedPredicate[Binary] with Serializable {\n              private val strToBinary = Binary.fromReusedByteArray(v.getBytes)"
  },
  {
    "id" : "2f4b4510-0baf-4650-b690-157d7bdab94f",
    "prId" : 24598,
    "prUrl" : "https://github.com/apache/spark/pull/24598#pullrequestreview-238139907",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a35e9822-994d-47be-80c4-414d2e44cc72",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "The rule is correct, but it looks a little dangerous to set `true` blindly without considering the given parameter `canPartialPushDownConjuncts`. Could you add a test case for that complex example like `OR` under `NOT` in the deep predicate tree?\r\n\r\nAlso, it would be great to add more higher level test case in `SQLQuerySuite.scala` to show the benefit of this additional predicate pushdown `a1 OR b1`. Could you add that, too?",
        "createdAt" : "2019-05-15T07:11:23Z",
        "updatedAt" : "2019-05-17T00:35:02Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "7b353011-e840-4e76-8b6a-157f6074564d",
        "parentId" : "a35e9822-994d-47be-80c4-414d2e44cc72",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "> The rule is correct, but it looks a little dangerous to set true blindly without considering the given parameter canPartialPushDownConjuncts. Could you add a test case for that complex example like OR under NOT in the deep predicate tree?\r\n\r\n@dongjoon-hyun nice catch. However, here the `Not` predicate won't have a child as `Or` or `And` predicate because of `BooleanSimplification` optimization rule.\r\nI have updated the code, but a new test case seems unnecessary.  `Not(a Or b)` can be converted as `Not(a) And Not(b)`\r\nAlso, I have created a PR for pushing down `Not` operator before(for double insurance), but seems the PR made thing too complex: https://github.com/apache/spark/pull/22687\r\n\r\n> Also, it would be great to add more higher level test case in SQLQuerySuite.scala to show the benefit of this additional predicate pushdown a1 OR b1. Could you add that, too?\r\n\r\nHow can we verify the predicate is pushed down? Match the `OrcScan` and check the `pushedFilters`? Only Orc V2 can be checked in this way currently.",
        "createdAt" : "2019-05-15T07:30:29Z",
        "updatedAt" : "2019-05-17T00:35:02Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "f190510e-f4ee-456d-85a2-6f5e4136fd27",
        "parentId" : "a35e9822-994d-47be-80c4-414d2e44cc72",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "The conversion from `(a1 AND a2) OR (b1 AND b2)` to`(a1 OR b1) AND (a1 OR b2) AND (a2 OR b1) AND (a2 OR b2)` always works no matter it's a top level boolean expression or not.",
        "createdAt" : "2019-05-15T12:02:39Z",
        "updatedAt" : "2019-05-17T00:35:02Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "8a4fd915-64d7-42d9-ad0f-e3889b421f1d",
        "parentId" : "a35e9822-994d-47be-80c4-414d2e44cc72",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "@gengliangwang . You can use  `explain` with `Console.withOut`. What we need is to check `PushedFilters: ` in the plan.\r\n\r\n> How can we verify the predicate is pushed down? Match the OrcScan and check the pushedFilters? Only Orc V2 can be checked in this way currently.\r\n\r\n@cloud-fan and @gengliangwang . Yes. Of course, I already agreed that the rule is correct. I want to have a test case in a complete end-to-end query form which shows the new benefit clearly. Could you please add a real use case you met?",
        "createdAt" : "2019-05-15T17:33:22Z",
        "updatedAt" : "2019-05-17T00:35:02Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "cadd0301-af37-48d7-9252-bee99606746d",
        "parentId" : "a35e9822-994d-47be-80c4-414d2e44cc72",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "+1 for an end-to-end test",
        "createdAt" : "2019-05-16T01:14:35Z",
        "updatedAt" : "2019-05-17T00:35:02Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "90b0b697246251b1e0b8acfe07f53f1153aefe45",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +537,541 @@        // a1 and b1 is convertible, while a2 and b2 is not.\n        // The predicate can be converted as\n        // (a1 OR b1) AND (a1 OR b2) AND (a2 OR b1) AND (a2 OR b2)\n        // As per the logical in And predicate, we can push down (a1 OR b1).\n        for {"
  }
]