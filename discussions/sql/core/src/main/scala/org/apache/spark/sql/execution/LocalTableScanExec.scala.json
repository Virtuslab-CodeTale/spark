[
  {
    "id" : "941cdcac-4852-44b2-8946-1a88ec5793e8",
    "prId" : 27530,
    "prUrl" : "https://github.com/apache/spark/pull/27530#pullrequestreview-356936659",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "67ccb79a-7d02-4140-8d0c-421d26c21f19",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Maybe, `unsafeRows.isEmpty`? Otherwise I have to look at the difference between `unsafeRows` and `rows`.",
        "createdAt" : "2020-02-11T10:18:28Z",
        "updatedAt" : "2020-02-11T19:35:55Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "36d55728-54d2-4ef0-8b7f-e6ddae4347b1",
        "parentId" : "67ccb79a-7d02-4140-8d0c-421d26c21f19",
        "authorId" : "80d631a6-73e8-46a6-a01b-a80d1f1cc6cc",
        "body" : "This way we avoid materializing the `unsafeRows` lazy val.",
        "createdAt" : "2020-02-11T19:34:53Z",
        "updatedAt" : "2020-02-11T19:35:55Z",
        "lastEditedBy" : "80d631a6-73e8-46a6-a01b-a80d1f1cc6cc",
        "tags" : [
        ]
      }
    ],
    "commit" : "6d46dec2061054da6b8c041b5649d0778d2e9471",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +47,51 @@\n  @transient private lazy val rdd: RDD[InternalRow] = {\n    if (rows.isEmpty) {\n      sqlContext.sparkContext.emptyRDD\n    } else {"
  },
  {
    "id" : "3f0a67f6-b4fd-4c94-af2f-d22a82611563",
    "prId" : 27530,
    "prUrl" : "https://github.com/apache/spark/pull/27530#pullrequestreview-356572713",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b791fe68-00be-4a33-b9a4-65e8b19f226a",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Just in case, does it make sense to put this code (handling empty rows) inside of `parallelize`?",
        "createdAt" : "2020-02-11T10:19:58Z",
        "updatedAt" : "2020-02-11T19:35:55Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "d6d9b2f5-c5c7-44c6-873b-c0855f844098",
        "parentId" : "b791fe68-00be-4a33-b9a4-65e8b19f226a",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "`parallelize` need to respect the `numSlices` parameter, even if the data is empty.",
        "createdAt" : "2020-02-11T11:09:32Z",
        "updatedAt" : "2020-02-11T19:35:55Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "6d46dec2061054da6b8c041b5649d0778d2e9471",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +51,55 @@    } else {\n      val numSlices = math.min(unsafeRows.length, sqlContext.sparkContext.defaultParallelism)\n      sqlContext.sparkContext.parallelize(unsafeRows, numSlices)\n    }\n  }"
  }
]