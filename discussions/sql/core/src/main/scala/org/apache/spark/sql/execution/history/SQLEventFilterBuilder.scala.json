[
  {
    "id" : "d2677196-2c7b-4eb2-85b6-7fb7d5a89a1e",
    "prId" : 27164,
    "prUrl" : "https://github.com/apache/spark/pull/27164#pullrequestreview-351323818",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d94762bb-2454-4ca8-a216-bfc632d45152",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "quick question: is this non-private only because of the testing purpose?",
        "createdAt" : "2020-01-31T03:19:56Z",
        "updatedAt" : "2020-01-31T03:19:56Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "044db0e9-e0ae-4246-85b1-760ab45c6c79",
        "parentId" : "d94762bb-2454-4ca8-a216-bfc632d45152",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Yes. Maybe we could reduce down the scope to package private.",
        "createdAt" : "2020-01-31T07:08:55Z",
        "updatedAt" : "2020-01-31T07:08:56Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "01ece298b6be6cd6cb40c89b62a391c7fb47a3e4",
    "line" : 44,
    "diffHunk" : "@@ -1,1 +42,46 @@  private val stages = new mutable.HashSet[Int]\n\n  def liveSQLExecutions: Set[Long] = _liveExecutionToJobs.keySet.toSet\n  def liveJobs: Set[Int] = _liveExecutionToJobs.values.flatten.toSet\n  def liveStages: Set[Int] = _stageToRDDs.keySet.toSet"
  },
  {
    "id" : "f9b25bee-5208-46a4-92e2-87cc03ca43d6",
    "prId" : 27164,
    "prUrl" : "https://github.com/apache/spark/pull/27164#pullrequestreview-351324620",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "099ec99d-3cdc-4682-aa62-e93b06cb93a2",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "another nit or quick question. Why did you use leading underscore? Seems the name doesn't conflict.",
        "createdAt" : "2020-01-31T03:22:06Z",
        "updatedAt" : "2020-01-31T03:22:07Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "37b8d93d-9534-4479-a409-6dffea2dbc58",
        "parentId" : "099ec99d-3cdc-4682-aa62-e93b06cb93a2",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "That was conflicted with below methods, and I renamed the methods as the purpose of methods changed slightly. Revisiting this now, looks like these fields don't need to have underscore.",
        "createdAt" : "2020-01-31T07:11:22Z",
        "updatedAt" : "2020-01-31T07:11:23Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "01ece298b6be6cd6cb40c89b62a391c7fb47a3e4",
    "line" : 38,
    "diffHunk" : "@@ -1,1 +36,40 @@ */\nprivate[spark] class SQLEventFilterBuilder extends SparkListener with EventFilterBuilder {\n  private val _liveExecutionToJobs = new mutable.HashMap[Long, mutable.Set[Int]]\n  private val _jobToStages = new mutable.HashMap[Int, Set[Int]]\n  private val _stageToTasks = new mutable.HashMap[Int, mutable.Set[Long]]"
  },
  {
    "id" : "7cc06a34-d6a2-4d59-95c0-0b5086e44bfe",
    "prId" : 27164,
    "prUrl" : "https://github.com/apache/spark/pull/27164#pullrequestreview-351325812",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c1f9b593-4c45-445b-bb76-9e53ea4dd23f",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Here too. The naming here seems confusing ...",
        "createdAt" : "2020-01-31T03:22:56Z",
        "updatedAt" : "2020-01-31T03:22:57Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "f52b5b07-2203-4e50-9f63-64945f06a644",
        "parentId" : "c1f9b593-4c45-445b-bb76-9e53ea4dd23f",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "They're just temporal variables and the only purpose is to pass to the constructor of JobEventFilter. As they don't conflict with parameters on  the constructor of JobEventFilter, I wouldn't mind removing underscore.",
        "createdAt" : "2020-01-31T07:15:17Z",
        "updatedAt" : "2020-01-31T07:15:17Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "01ece298b6be6cd6cb40c89b62a391c7fb47a3e4",
    "line" : 121,
    "diffHunk" : "@@ -1,1 +119,123 @@    _liveStages: Set[Int],\n    _liveTasks: Set[Long],\n    _liveRDDs: Set[Int])\n  extends JobEventFilter(None, _liveJobs, _liveStages, _liveTasks, _liveRDDs) with Logging {\n"
  },
  {
    "id" : "90b4c100-771c-429d-bbf5-2d4cc4362d0b",
    "prId" : 26416,
    "prUrl" : "https://github.com/apache/spark/pull/26416#pullrequestreview-317393708",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "015b24ab-8351-41ee-b467-97ad63489361",
        "parentId" : null,
        "authorId" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "body" : "Nit: `jobStart.stageIds.foreach(stageId => _stageToTasks += stageId -> mutable.HashSet[Long]())`",
        "createdAt" : "2019-11-14T17:15:18Z",
        "updatedAt" : "2019-12-26T02:35:39Z",
        "lastEditedBy" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "tags" : [
        ]
      },
      {
        "id" : "ecb64ab8-81ad-471e-921c-4574b5cfb27b",
        "parentId" : "015b24ab-8351-41ee-b467-97ad63489361",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "same here.",
        "createdAt" : "2019-11-15T03:38:23Z",
        "updatedAt" : "2019-12-26T02:35:39Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "e1a6e42b73f8d58dbc0a04882f2288d2fae0dad8",
    "line" : 63,
    "diffHunk" : "@@ -1,1 +61,65 @@    _jobToStages += jobStart.jobId -> jobStart.stageIds\n    stages ++= jobStart.stageIds\n    jobStart.stageIds.foreach { stageId => _stageToTasks += stageId -> mutable.HashSet[Long]() }\n  }\n"
  },
  {
    "id" : "089364cd-21f3-4553-adbe-9dc263f5ae09",
    "prId" : 26416,
    "prUrl" : "https://github.com/apache/spark/pull/26416#pullrequestreview-318051814",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "41f1ffc9-7548-4df2-8b86-2557eaac0deb",
        "parentId" : null,
        "authorId" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "body" : "```\r\n    if (executionIdString != null) {\r\n      val executionId = executionIdString.toLong\r\n```\r\n?",
        "createdAt" : "2019-11-14T17:16:53Z",
        "updatedAt" : "2019-12-26T02:35:39Z",
        "lastEditedBy" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "tags" : [
        ]
      },
      {
        "id" : "0571b4b1-649a-40de-8324-8b26ad3bc3d8",
        "parentId" : "41f1ffc9-7548-4df2-8b86-2557eaac0deb",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "This is an intended early-return, as if the job is not related to the SQL execution we don't mind. In fact it's the same code as `SQLAppStatusListener.onJobStart`.",
        "createdAt" : "2019-11-15T03:38:13Z",
        "updatedAt" : "2019-12-26T02:35:39Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "33a959dd-924b-4fd3-b1ee-77b642980730",
        "parentId" : "41f1ffc9-7548-4df2-8b86-2557eaac0deb",
        "authorId" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "body" : "In this case early-return can can be converted to a negated whole function wrapping condition. Since there is an example like this + minor enough we can leave it as is.",
        "createdAt" : "2019-11-15T11:57:33Z",
        "updatedAt" : "2019-12-26T02:35:39Z",
        "lastEditedBy" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "tags" : [
        ]
      },
      {
        "id" : "933491d4-8028-49ad-9a1b-1812a0e4d21f",
        "parentId" : "41f1ffc9-7548-4df2-8b86-2557eaac0deb",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "https://github.com/databricks/scala-style-guide#return-statements\r\n\r\n> Use return as a guard to simplify control flow without adding a level of indentation\r\n\r\nIf I understand correctly on the style guide, early return, especially placing return in mostly top of the method, is not a bad pattern which should be considered to be changed. This is a kind of preferences of individual, which should be checked with the style guide to not force one's preference.",
        "createdAt" : "2019-11-17T23:01:48Z",
        "updatedAt" : "2019-12-26T02:35:39Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "e1a6e42b73f8d58dbc0a04882f2288d2fae0dad8",
    "line" : 49,
    "diffHunk" : "@@ -1,1 +47,51 @@  override def onJobStart(jobStart: SparkListenerJobStart): Unit = {\n    val executionIdString = jobStart.properties.getProperty(SQLExecution.EXECUTION_ID_KEY)\n    if (executionIdString == null) {\n      // This is not a job created by SQL\n      return"
  }
]