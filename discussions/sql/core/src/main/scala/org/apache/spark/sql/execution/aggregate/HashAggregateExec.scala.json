[
  {
    "id" : "6290dee0-3461-499d-bfad-3e1e5ba878a3",
    "prId" : 32885,
    "prUrl" : "https://github.com/apache/spark/pull/32885#pullrequestreview-686216472",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d890c7f8-1bde-4f90-8beb-9cda4c105637",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "This is a hidden bug. `SubqueryExpression` will be sent to the executor side and build `Projection`, and be put in `EquivalentExpressions`, which needs to call `canonicalized`.\r\n\r\nThis means, Spark may serialize and send `HashAggregateExec` to the executor side, where `sqlContext` should be null.\r\n\r\nIt's hidden for a long time because `ScalarSubquery` didn't implement `canonicalized`, so the bug is not triggered. However, it also means `semanticHash` is wrong.\r\n\r\nI think it only affects common subquery elimination, and shouldn't be a serious bug.\r\n",
        "createdAt" : "2021-06-14T14:51:12Z",
        "updatedAt" : "2021-06-14T14:51:12Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "923c86ab-30e2-415b-921d-c5fc289f467e",
        "parentId" : "d890c7f8-1bde-4f90-8beb-9cda4c105637",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "In the long term, I think we should only send an \"expression evaluator\" to the executor side. The semantic check should only be done in the driver side.",
        "createdAt" : "2021-06-14T14:52:19Z",
        "updatedAt" : "2021-06-14T14:52:19Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "d8748046-96c3-4c04-bc76-49b12e543c65",
        "parentId" : "d890c7f8-1bde-4f90-8beb-9cda4c105637",
        "authorId" : "3d4870da-39a8-4406-b00d-131930d14cd8",
        "body" : "Sorry for the late comment @cloud-fan, but I think I've run into this issue before:\r\nhttps://github.com/apache/spark/pull/28885/files#diff-9b62cef6bfdeb6c802bb120c7a724a974d5067a69585285bebb64c48603f8d6fR105-R108. The point is that there might be other nodes where canonicalization on executor side can cause issues. `SortExec.enableRadixSort` is the other one I found.",
        "createdAt" : "2021-06-15T09:58:54Z",
        "updatedAt" : "2021-06-15T10:01:07Z",
        "lastEditedBy" : "3d4870da-39a8-4406-b00d-131930d14cd8",
        "tags" : [
        ]
      },
      {
        "id" : "40c29219-3939-4651-a83e-c61700045753",
        "parentId" : "d890c7f8-1bde-4f90-8beb-9cda4c105637",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "`InSubqueryExec` already implements `canonicalized` before this PR, so we need to fix these bugs anyway.\r\n\r\nI have an idea to fix this problem in all physical plans:\r\n1. remove `SparkPlan.sqlContext`, so that we can catch all the callers of it\r\n2. add `@transient final val session = SparkSession.getActiveSession.orNull`, to replace the previous `sqlContext`\r\n3. override `conf` in `SparkPlan`: `if (session != null) session.sessionState.conf else SQLConf.get`\r\n\r\n@peter-toth what do you think? AFAIK the only reason to access `SparkPlan.sqlContext` at executor side is to read a conf, and we can do that with `SQLConf.get` at executor side.",
        "createdAt" : "2021-06-15T10:43:06Z",
        "updatedAt" : "2021-06-15T10:43:06Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "053068ab-70ba-47a9-933a-a3a10bb3b48c",
        "parentId" : "d890c7f8-1bde-4f90-8beb-9cda4c105637",
        "authorId" : "3d4870da-39a8-4406-b00d-131930d14cd8",
        "body" : "Sounds good to me.",
        "createdAt" : "2021-06-15T10:59:03Z",
        "updatedAt" : "2021-06-15T10:59:04Z",
        "lastEditedBy" : "3d4870da-39a8-4406-b00d-131930d14cd8",
        "tags" : [
        ]
      },
      {
        "id" : "9500ee3a-a6e6-4d4a-b506-0d0db9089d39",
        "parentId" : "d890c7f8-1bde-4f90-8beb-9cda4c105637",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I'm quite busy this week and may not have time to implement this idea recently. @peter-toth feel free to pick up this idea and open a PR if you have time, or I'll do it next or next next week. Thanks in advance!",
        "createdAt" : "2021-06-15T17:52:52Z",
        "updatedAt" : "2021-06-15T17:52:52Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "c88dfb88-ca24-4162-a735-7cd15ecd61c1",
        "parentId" : "d890c7f8-1bde-4f90-8beb-9cda4c105637",
        "authorId" : "3d4870da-39a8-4406-b00d-131930d14cd8",
        "body" : "Ok, thanks. I will try to open a PR this week.",
        "createdAt" : "2021-06-15T18:00:50Z",
        "updatedAt" : "2021-06-15T18:00:50Z",
        "lastEditedBy" : "3d4870da-39a8-4406-b00d-131930d14cd8",
        "tags" : [
        ]
      },
      {
        "id" : "a1b19925-e302-4e6b-8ad3-d58d314b5860",
        "parentId" : "d890c7f8-1bde-4f90-8beb-9cda4c105637",
        "authorId" : "3d4870da-39a8-4406-b00d-131930d14cd8",
        "body" : "Opened https://github.com/apache/spark/pull/32947.",
        "createdAt" : "2021-06-17T11:59:49Z",
        "updatedAt" : "2021-06-17T11:59:49Z",
        "lastEditedBy" : "3d4870da-39a8-4406-b00d-131930d14cd8",
        "tags" : [
        ]
      }
    ],
    "commit" : "5095fffcd0cd76548eadef802cdb81261029ff12",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +74,78 @@  // map and/or the sort-based aggregation once it has processed a given number of input rows.\n  private val testFallbackStartsAt: Option[(Int, Int)] = {\n    Option(sqlContext).map { sc =>\n      sc.getConf(\"spark.sql.TungstenAggregate.testFallbackStartsAt\", null)\n    }.orNull match {"
  },
  {
    "id" : "52bad0a4-d644-4808-b89a-ff0d6518bb84",
    "prId" : 32242,
    "prUrl" : "https://github.com/apache/spark/pull/32242#pullrequestreview-639585400",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "026feae0-22bd-4e10-8f54-bff97b1645ab",
        "parentId" : null,
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "Adding the limit early termination for first level map as well. This is needed to fix test failure `SQLMetricsSuite.SPARK-25497: LIMIT within whole stage codegen should not consume all the inputs` in https://github.com/c21/spark/runs/2386397792?check_suite_focus=true. And this is good to have anyway.",
        "createdAt" : "2021-04-20T07:13:10Z",
        "updatedAt" : "2021-04-22T07:31:42Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      }
    ],
    "commit" : "965a35c7329bacad6f46010811aee4693bd04d95",
    "line" : 92,
    "diffHunk" : "@@ -1,1 +790,794 @@    val outputFunc = generateResultFunction(ctx)\n\n    val limitNotReachedCondition = limitNotReachedCond\n\n    def outputFromFastHashMap: String = {"
  },
  {
    "id" : "450ae8d7-6d9e-4863-b473-c937bd646df1",
    "prId" : 30766,
    "prUrl" : "https://github.com/apache/spark/pull/30766#pullrequestreview-551812747",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fe47258f-9379-4140-a409-db6a56a5427d",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I thought about this too,  but my concern is that we never promise the codegen to be thread-safe. We may have more places to fix, and we may break it again in the future.",
        "createdAt" : "2020-12-14T19:14:31Z",
        "updatedAt" : "2020-12-14T19:14:31Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "7d5d142e323362b147bc1c999565a2f261401901",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +160,164 @@  // The variables are used as aggregation buffers and each aggregate function has one or more\n  // ExprCode to initialize its buffer slots. Only used for aggregation without keys.\n  private val bufVar = new ThreadLocal[Seq[Seq[ExprCode]]] {\n    override def initialValue(): Seq[Seq[ExprCode]] = {\n      Seq.empty"
  },
  {
    "id" : "1b758787-34c4-4112-9f92-010056a36970",
    "prId" : 29583,
    "prUrl" : "https://github.com/apache/spark/pull/29583#pullrequestreview-478224105",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0f20005b-8960-481a-bee3-49a3bbd5e58f",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "nit: could you remove the unused import `import org.apache.spark.sql.catalyst.plans.physical._`?",
        "createdAt" : "2020-08-30T11:21:06Z",
        "updatedAt" : "2020-08-31T00:47:47Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "96afc1f7-40f1-4195-bda9-89ee3ccbb5fc",
        "parentId" : "0f20005b-8960-481a-bee3-49a3bbd5e58f",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "@maropu - sure, updated.",
        "createdAt" : "2020-08-30T18:19:31Z",
        "updatedAt" : "2020-08-31T00:47:47Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      }
    ],
    "commit" : "34e27c348e760529bc0582aa4dcfd12aba8db48e",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +52,56 @@    initialInputBufferOffset: Int,\n    resultExpressions: Seq[NamedExpression],\n    child: SparkPlan)\n  extends BaseAggregateExec\n  with BlockingOperatorWithCodegen {"
  }
]