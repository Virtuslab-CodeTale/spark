[
  {
    "id" : "158904ac-88fa-404e-8bcd-8c4c4be7efa2",
    "prId" : 33352,
    "prUrl" : "https://github.com/apache/spark/pull/33352#pullrequestreview-709643243",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dfd65c20-1279-4e58-89ff-d5339765e2fc",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Sorry I don't get it. Why not just `aggregates.map`? Why do we need an ArrayBuilder?",
        "createdAt" : "2021-07-19T14:49:58Z",
        "updatedAt" : "2021-07-19T14:49:58Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "fae570a68cde1cc25ae60e7ba74651e7972578e3",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +140,144 @@    def quote(colName: String): String = dialect.quoteIdentifier(colName)\n\n    aggregates.map {\n      case min: Min =>\n        assert(min.column.fieldNames.length == 1)"
  },
  {
    "id" : "2e2e631e-667e-4015-a235-a53fd9da74d0",
    "prId" : 33352,
    "prUrl" : "https://github.com/apache/spark/pull/33352#pullrequestreview-712266129",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "73dd6a77-48d1-4db7-bdf9-b88215baf679",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "shall we pass `Array[FieldReference]` and use empty array to represent no grouping?",
        "createdAt" : "2021-07-21T13:35:18Z",
        "updatedAt" : "2021-07-21T13:35:18Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "00672445-121a-4762-a3e9-766670473b35",
        "parentId" : "73dd6a77-48d1-4db7-bdf9-b88215baf679",
        "authorId" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "body" : "I guess no. I am using this `groupByColumns` to tell if aggregate is pushed down. Having this as an Option, I can do this:\r\n```\r\nif (groupByColumns.isEmpty) {  // no pushed down aggregate\r\n  quote columns\r\n} else {\r\n  // columns already quoated\r\n  if (groupByColumns.get.isEmpty) { // no group by columns\r\n    getGroupByClause = \"\"\r\n  }  else {\r\n    getGroupByClause = ...\r\n  }\r\n}\r\n\r\n```",
        "createdAt" : "2021-07-21T23:23:54Z",
        "updatedAt" : "2021-07-21T23:23:54Z",
        "lastEditedBy" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "tags" : [
        ]
      }
    ],
    "commit" : "fae570a68cde1cc25ae60e7ba74651e7972578e3",
    "line" : 59,
    "diffHunk" : "@@ -1,1 +186,190 @@      options: JDBCOptions,\n      outputSchema: Option[StructType] = None,\n      groupByColumns: Option[Array[FieldReference]] = None): RDD[InternalRow] = {\n    val url = options.url\n    val dialect = JdbcDialects.get(url)"
  },
  {
    "id" : "5994cfac-181a-495e-89f1-ba78e7fee274",
    "prId" : 33352,
    "prUrl" : "https://github.com/apache/spark/pull/33352#pullrequestreview-711661951",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e4640e6a-013b-4048-82c7-03ab96a48914",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "ditto",
        "createdAt" : "2021-07-21T13:35:25Z",
        "updatedAt" : "2021-07-21T13:35:25Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "fae570a68cde1cc25ae60e7ba74651e7972578e3",
    "line" : 90,
    "diffHunk" : "@@ -1,1 +222,226 @@    url: String,\n    options: JDBCOptions,\n    groupByColumns: Option[Array[FieldReference]])\n  extends RDD[InternalRow](sc, Nil) {\n"
  },
  {
    "id" : "ef9290e7-0689-4e61-8b34-c9c0f6d1ebfe",
    "prId" : 33352,
    "prUrl" : "https://github.com/apache/spark/pull/33352#pullrequestreview-711662579",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e0d085f4-52bf-407a-b439-d367f4e94fe3",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "nit: `assert(groupByColumns.forall(_.fieldNames.length == 1))`",
        "createdAt" : "2021-07-21T13:35:58Z",
        "updatedAt" : "2021-07-21T13:35:58Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "fae570a68cde1cc25ae60e7ba74651e7972578e3",
    "line" : 105,
    "diffHunk" : "@@ -1,1 +269,273 @@      assert(groupByColumns.get.forall(_.fieldNames.length == 1))\n      val dialect = JdbcDialects.get(url)\n      val quotedColumns = groupByColumns.get.map(c => dialect.quoteIdentifier(c.fieldNames.head))\n      s\"GROUP BY ${quotedColumns.mkString(\", \")}\"\n    } else {"
  },
  {
    "id" : "d331272f-3b43-401f-9a71-86c7e40e9765",
    "prId" : 33352,
    "prUrl" : "https://github.com/apache/spark/pull/33352#pullrequestreview-714572564",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1a6c89c4-b3a6-42ad-8621-cd6355876397",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Is the param doc correct? I don't see `aggregation` parameter but `outputSchema` and `groupByColumns`.",
        "createdAt" : "2021-07-26T08:24:05Z",
        "updatedAt" : "2021-07-26T08:24:06Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "fae570a68cde1cc25ae60e7ba74651e7972578e3",
    "line" : 48,
    "diffHunk" : "@@ -1,1 +174,178 @@   * @param options - JDBC options that contains url, table and other information.\n   * @param requiredSchema - The schema of the columns to SELECT.\n   * @param aggregation - The pushed down aggregation\n   *\n   * @return An RDD representing \"SELECT requiredColumns FROM fqTable\"."
  },
  {
    "id" : "2a69a92f-fda4-449f-b08b-02ff2dac3acb",
    "prId" : 32222,
    "prUrl" : "https://github.com/apache/spark/pull/32222#pullrequestreview-638310557",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8c50919e-80ea-46ba-97a7-0e2c1d3971b7",
        "parentId" : null,
        "authorId" : "6a8d0007-eb0f-4236-a65a-2174a14ac689",
        "body" : "why is the period set to 100 millis?",
        "createdAt" : "2021-04-18T09:02:23Z",
        "updatedAt" : "2021-04-26T05:45:48Z",
        "lastEditedBy" : "6a8d0007-eb0f-4236-a65a-2174a14ac689",
        "tags" : [
        ]
      },
      {
        "id" : "236a74cf-cd3e-4881-8fe7-b4aa5980614d",
        "parentId" : "8c50919e-80ea-46ba-97a7-0e2c1d3971b7",
        "authorId" : "354de106-0600-4904-94ce-bb8bd8e3f1da",
        "body" : "I have no way of knowing how long the sql execution needs to be processed. It may be a few seconds or a few hours, so I use the default 100 milliseconds to see if the job is interrupted/the query is completed. Do you mean that you need to parameterize this variable for configuration?",
        "createdAt" : "2021-04-18T09:25:00Z",
        "updatedAt" : "2021-04-26T05:45:48Z",
        "lastEditedBy" : "354de106-0600-4904-94ce-bb8bd8e3f1da",
        "tags" : [
        ]
      }
    ],
    "commit" : "72170d438a135d091137f696537753836b20f6ad",
    "line" : 61,
    "diffHunk" : "@@ -1,1 +329,333 @@      isFinished: AtomicBoolean): Unit = {\n    val thread = new Thread(() => {\n      val waitInterval = 100\n      // Always wait for job interruption or stmt.executeQuery() execution to complete\n      while (!context.isInterrupted() && !isFinished.get()) {"
  },
  {
    "id" : "ee99a949-67db-453a-a5b4-ce5648ead0d5",
    "prId" : 32222,
    "prUrl" : "https://github.com/apache/spark/pull/32222#pullrequestreview-638315677",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e736aa61-7d4f-4af3-b287-996044d64b28",
        "parentId" : null,
        "authorId" : "6a8d0007-eb0f-4236-a65a-2174a14ac689",
        "body" : "Could you add test suite for this code?",
        "createdAt" : "2021-04-18T09:03:17Z",
        "updatedAt" : "2021-04-26T05:45:48Z",
        "lastEditedBy" : "6a8d0007-eb0f-4236-a65a-2174a14ac689",
        "tags" : [
        ]
      },
      {
        "id" : "7c1ebdcc-ad5e-4e7a-a54d-b63568591825",
        "parentId" : "e736aa61-7d4f-4af3-b287-996044d64b28",
        "authorId" : "354de106-0600-4904-94ce-bb8bd8e3f1da",
        "body" : "Okay, I will add the test suite later",
        "createdAt" : "2021-04-18T09:26:10Z",
        "updatedAt" : "2021-04-26T05:45:48Z",
        "lastEditedBy" : "354de106-0600-4904-94ce-bb8bd8e3f1da",
        "tags" : [
        ]
      },
      {
        "id" : "81647127-3b09-4904-b157-010542eb603b",
        "parentId" : "e736aa61-7d4f-4af3-b287-996044d64b28",
        "authorId" : "354de106-0600-4904-94ce-bb8bd8e3f1da",
        "body" : "Test suite JdbcUtilsSuite has been added",
        "createdAt" : "2021-04-18T10:28:41Z",
        "updatedAt" : "2021-04-26T05:45:48Z",
        "lastEditedBy" : "354de106-0600-4904-94ce-bb8bd8e3f1da",
        "tags" : [
        ]
      }
    ],
    "commit" : "72170d438a135d091137f696537753836b20f6ad",
    "line" : 39,
    "diffHunk" : "@@ -1,1 +307,311 @@    stmt.setQueryTimeout(options.queryTimeout)\n\n    val isFinished = new AtomicBoolean(false)\n    startTheJobInterruptListenerThread(context, stmt, isFinished)\n    try{"
  }
]