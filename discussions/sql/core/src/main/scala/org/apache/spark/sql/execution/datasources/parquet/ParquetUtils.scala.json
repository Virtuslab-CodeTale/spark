[
  {
    "id" : "41b49d97-d31c-4b64-9bed-c2f44ad2f617",
    "prId" : 32049,
    "prUrl" : "https://github.com/apache/spark/pull/32049#pullrequestreview-667415293",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "aacfdcf0-4d8d-4ba8-a834-7aac338f8133",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "The part above is related to this PR (to support Min/Max/Count)? If not, how about removing the unnecessary part to simplify the PR?",
        "createdAt" : "2021-05-24T06:59:14Z",
        "updatedAt" : "2021-05-24T07:13:25Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "8b76ce5d-4761-4071-8deb-7b2daff1bb31",
        "parentId" : "aacfdcf0-4d8d-4ba8-a834-7aac338f8133",
        "authorId" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "body" : "I need this part because I am building an InternalRow for the Aggregate results",
        "createdAt" : "2021-05-25T05:57:12Z",
        "updatedAt" : "2021-05-25T05:57:12Z",
        "lastEditedBy" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "tags" : [
        ]
      }
    ],
    "commit" : "5c2b630a61d8ea9263116c37e154884a5cabd2ca",
    "line" : 86,
    "diffHunk" : "@@ -1,1 +197,201 @@        mutableRow.setFloat(i, values(i).asInstanceOf[Float])\n      case (PrimitiveType.PrimitiveTypeName.DOUBLE, i) =>\n        mutableRow.setDouble(i, values(i).asInstanceOf[Double])\n      case (PrimitiveType.PrimitiveTypeName.BOOLEAN, i) =>\n        mutableRow.setBoolean(i, values(i).asInstanceOf[Boolean])"
  },
  {
    "id" : "c32250e0-5968-4ecf-b65f-b5e0dab0aa93",
    "prId" : 32049,
    "prUrl" : "https://github.com/apache/spark/pull/32049#pullrequestreview-667416059",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "27d04cde-fbb6-4126-8cfd-9eef8be8adb4",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "We cannot avoid this exception on runtime? It looks inconvenient. For example, in `SupportsPushDownAggregates .pushAggregation`, we cannot give up pushing down aggregates in case of this unsupported one?",
        "createdAt" : "2021-05-24T07:09:02Z",
        "updatedAt" : "2021-05-24T07:13:25Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "b5bc23f3-4642-42e0-8c8e-6c5939447963",
        "parentId" : "27d04cde-fbb6-4126-8cfd-9eef8be8adb4",
        "authorId" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "body" : "This part bothers me too. I don't know why in some cases parquet doesn't return statistics info. We won't be able to find out until we read footer in `FilePartitionReaderFactory.createReader`. It's already passed the `SupportsPushDownAggregates .pushAggregation`. I can't figure out a good way to do this. ",
        "createdAt" : "2021-05-25T05:58:40Z",
        "updatedAt" : "2021-05-25T05:58:40Z",
        "lastEditedBy" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "tags" : [
        ]
      }
    ],
    "commit" : "5c2b630a61d8ea9263116c37e154884a5cabd2ca",
    "line" : 268,
    "diffHunk" : "@@ -1,1 +379,383 @@    if (!statistics.hasNonNullValue) {\n      throw new UnsupportedOperationException(\"No min/max found for parquet file, Set SQLConf\" +\n        \" PARQUET_AGGREGATE_PUSHDOWN_ENABLED to false and execute again\")\n    } else {\n      if (isMax) statistics.genericGetMax() else statistics.genericGetMin()"
  }
]