[
  {
    "id" : "3ced8dc8-eabf-445d-ae39-e21f0999eb87",
    "prId" : 31451,
    "prUrl" : "https://github.com/apache/spark/pull/31451#pullrequestreview-583684822",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0ee42223-917a-446f-9d31-e73fd90b8919",
        "parentId" : null,
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "Any plans to similarly support metrics in writes?",
        "createdAt" : "2021-02-03T21:51:12Z",
        "updatedAt" : "2021-04-20T06:45:33Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      },
      {
        "id" : "912351e4-f58f-4aac-8ca3-744b0695f3f5",
        "parentId" : "0ee42223-917a-446f-9d31-e73fd90b8919",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "I looked at a few V2 write nodes, but seems we don't have any SQL metrics there (even number of output rows). I guess we don't provide metrics for writes generally now?\r\n\r\nIf there is interest to see metrics in writes, I think it is okay to work on it later.",
        "createdAt" : "2021-02-04T01:17:56Z",
        "updatedAt" : "2021-04-20T06:45:33Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "25512e7c-852f-4ad0-b3c7-7cfa36305e83",
        "parentId" : "0ee42223-917a-446f-9d31-e73fd90b8919",
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "Looks like updating `context.taskMetrics().outputMetrics` is just in our branch. That just uses the Hadoop FS metrics collection that we use elsewhere, so it isn't metrics from the source as we want to support in this PR.\r\n\r\nI think it would be good to follow up and support metrics on the output side. It doesn't need to be done here, but metrics are really useful.",
        "createdAt" : "2021-02-04T18:07:50Z",
        "updatedAt" : "2021-04-20T06:45:33Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      },
      {
        "id" : "af53e1b6-893a-49e5-b05a-bc3186bba6ed",
        "parentId" : "0ee42223-917a-446f-9d31-e73fd90b8919",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Sounds good to me. I can work on it  in follow up PRs.",
        "createdAt" : "2021-02-04T18:19:02Z",
        "updatedAt" : "2021-04-20T06:45:33Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "06eb9c79a3fdd807ec08540deb2234939396325a",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +37,41 @@  }.toMap\n\n  override lazy val metrics = {\n    Map(\"numOutputRows\" -> SQLMetrics.createMetric(sparkContext, \"number of output rows\")) ++\n      customMetrics"
  }
]