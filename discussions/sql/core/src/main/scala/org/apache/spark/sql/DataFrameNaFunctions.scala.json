[
  {
    "id" : "85fbecbd-47bb-4b2c-872a-9610e271a4cd",
    "prId" : 31769,
    "prUrl" : "https://github.com/apache/spark/pull/31769#pullrequestreview-606279683",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "af6e9cc7-31a3-4c9a-8f97-4aaf31582f6f",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "The current behaviour just ignores input if given columns do not exist?\r\n```\r\nscala> df.na.replace(\"xxx\", Map(\"n/a\" -> \"Unknown\")).show()\r\n+---+---+\r\n| c0| c1|\r\n+---+---+\r\n|abc| 23|\r\n|def| 44|\r\n|n/a|  0|\r\n+---+---+\r\n```",
        "createdAt" : "2021-03-08T06:56:49Z",
        "updatedAt" : "2021-03-09T05:46:30Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "e7ce59cb-a645-4f93-8f4c-54a02ff0822a",
        "parentId" : "af6e9cc7-31a3-4c9a-8f97-4aaf31582f6f",
        "authorId" : "2aac12c0-5b21-45d8-99f0-6e7832629f6d",
        "body" : "`df.resolve` throws `org.apache.spark.sql.AnalysisException` with error message `Cannot resolve column name \"xxx\" among (c0, c1)`, if any of the given columns do not exist.",
        "createdAt" : "2021-03-08T07:33:42Z",
        "updatedAt" : "2021-03-09T05:46:30Z",
        "lastEditedBy" : "2aac12c0-5b21-45d8-99f0-6e7832629f6d",
        "tags" : [
        ]
      },
      {
        "id" : "3d45cfb0-927a-46e1-acef-79ea2ee62231",
        "parentId" : "af6e9cc7-31a3-4c9a-8f97-4aaf31582f6f",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Could you add tests for these exceptions? I think they are new behaviours for `replace`.",
        "createdAt" : "2021-03-08T11:38:02Z",
        "updatedAt" : "2021-03-09T05:46:30Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "85b9142a-fd65-4def-830a-e10acdc0b0cd",
        "parentId" : "af6e9cc7-31a3-4c9a-8f97-4aaf31582f6f",
        "authorId" : "2aac12c0-5b21-45d8-99f0-6e7832629f6d",
        "body" : "Added.",
        "createdAt" : "2021-03-08T13:22:45Z",
        "updatedAt" : "2021-03-09T05:46:30Z",
        "lastEditedBy" : "2aac12c0-5b21-45d8-99f0-6e7832629f6d",
        "tags" : [
        ]
      }
    ],
    "commit" : "58679bd9e27bb376fd9e6ea2baa780fe3454f23a",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +359,363 @@        case a: Attribute => a\n        case _ => throw new UnsupportedOperationException(\n          s\"Nested field ${colName} is not supported.\")\n      }\n      attr"
  },
  {
    "id" : "4d441f7b-537c-4697-b970-c31012cb2462",
    "prId" : 31769,
    "prUrl" : "https://github.com/apache/spark/pull/31769#pullrequestreview-606872033",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "40e8aac7-865d-4a2f-ae9d-1faae869b0f0",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "`AnalysisException` instead? How about following the behaivour of the other functions in this class? e.g.,\r\n```\r\nscala> df.show()\r\n+----+---------+\r\n|   a|        b|\r\n+----+---------+\r\n|null|{1, null}|\r\n+----+---------+\r\n\r\n\r\nscala> df.printSchema()\r\nroot\r\n |-- a: integer (nullable = true)\r\n |-- b: struct (nullable = false)\r\n |    |-- c0: integer (nullable = false)\r\n |    |-- c1: integer (nullable = true)\r\n\r\n\r\nscala> df.na.fill(3, Seq(\"a\")).show()\r\n+---+---------+\r\n|  a|        b|\r\n+---+---------+\r\n|  3|{1, null}|\r\n+---+---------+\r\n\r\n\r\nscala> df.na.fill(3, Seq(\"b\")).show()\r\n+----+---------+\r\n|   a|        b|\r\n+----+---------+\r\n|null|{1, null}|\r\n+----+---------+\r\n\r\n\r\nscala> df.na.fill(3, Seq(\"b.c2\")).show()\r\norg.apache.spark.sql.AnalysisException: No such struct field c2 in c0, c1\r\n  at org.apache.spark.sql.catalyst.expressions.ExtractValue$.findField(complexTypeExtractors.scala:82)\r\n  at org.apache.spark.sql.catalyst.expressions.ExtractValue$.apply(complexTypeExtractors.scala:55)\r\n  at org.apache.spark.sql.catalyst.expressions.package$AttributeSeq.$anonfun$resolve$1(package.scala:348)\r\n  at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)\r\n  at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)\r\n  at scala.collection.mutable.ArrayBuffer.foldLeft(ArrayBuffer.scala:49)\r\n  at org.apache.spark.sql.catalyst.expressions.package$AttributeSeq.resolve(package.scala:347)\r\n  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolve(LogicalPlan.scala:119)\r\n  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveQuoted(LogicalPlan.scala:130)\r\n  at org.apache.spark.sql.Dataset.resolve(Dataset.scala:262)\r\n  at org.apache.spark.sql.Dataset.col(Dataset.scala:1361)\r\n  at org.apache.spark.sql.DataFrameNaFunctions.$anonfun$toAttributes$1(DataFrameNaFunctions.scala:475)\r\n  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\r\n  at scala.collection.immutable.List.foreach(List.scala:392)\r\n  at scala.collection.TraversableLike.map(TraversableLike.scala:238)\r\n  at scala.collection.TraversableLike.map$(TraversableLike.scala:231)\r\n  at scala.collection.immutable.List.map(List.scala:298)\r\n  at org.apache.spark.sql.DataFrameNaFunctions.toAttributes(DataFrameNaFunctions.scala:475)\r\n  at org.apache.spark.sql.DataFrameNaFunctions.fill(DataFrameNaFunctions.scala:163)\r\n  ... 47 elided\r\n\r\nscala> df.na.fill(3, Seq(\"c\")).show()\r\norg.apache.spark.sql.AnalysisException: Cannot resolve column name \"c\" among (a, b)\r\n  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$resolveException(Dataset.scala:272)\r\n  at org.apache.spark.sql.Dataset.$anonfun$resolve$1(Dataset.scala:263)\r\n  at scala.Option.getOrElse(Option.scala:189)\r\n  at org.apache.spark.sql.Dataset.resolve(Dataset.scala:263)\r\n  at org.apache.spark.sql.Dataset.col(Dataset.scala:1361)\r\n  at org.apache.spark.sql.DataFrameNaFunctions.$anonfun$toAttributes$1(DataFrameNaFunctions.scala:475)\r\n  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\r\n  at scala.collection.immutable.List.foreach(List.scala:392)\r\n  at scala.collection.TraversableLike.map(TraversableLike.scala:238)\r\n  at scala.collection.TraversableLike.map$(TraversableLike.scala:231)\r\n  at scala.collection.immutable.List.map(List.scala:298)\r\n  at org.apache.spark.sql.DataFrameNaFunctions.toAttributes(DataFrameNaFunctions.scala:475)\r\n  at org.apache.spark.sql.DataFrameNaFunctions.fill(DataFrameNaFunctions.scala:163)\r\n  ... 47 elided\r\n```",
        "createdAt" : "2021-03-08T23:48:20Z",
        "updatedAt" : "2021-03-09T05:46:30Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "7e887e9e-6ffa-4182-81ed-1aae8c027a77",
        "parentId" : "40e8aac7-865d-4a2f-ae9d-1faae869b0f0",
        "authorId" : "2aac12c0-5b21-45d8-99f0-6e7832629f6d",
        "body" : "`Dataset.resolve` throws `AnalysisException` for the listed behavior of the other functions in this class. Same is used for resolving input column name here as well.\r\n`UnsupportedOperationException` is thrown when column is resolved but it is nested since replacing in nested column is not supported.",
        "createdAt" : "2021-03-09T02:08:21Z",
        "updatedAt" : "2021-03-09T05:46:30Z",
        "lastEditedBy" : "2aac12c0-5b21-45d8-99f0-6e7832629f6d",
        "tags" : [
        ]
      }
    ],
    "commit" : "58679bd9e27bb376fd9e6ea2baa780fe3454f23a",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +358,362 @@      val attr = df.resolve(colName) match {\n        case a: Attribute => a\n        case _ => throw new UnsupportedOperationException(\n          s\"Nested field ${colName} is not supported.\")\n      }"
  },
  {
    "id" : "0ce10adb-9218-4218-990c-ff12acd390ef",
    "prId" : 26738,
    "prUrl" : "https://github.com/apache/spark/pull/26738#pullrequestreview-337939826",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8252e56d-2fda-4c63-910c-651e8d7c7890",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "This fix relies on the type coercion rule to do the casting in the another side. It could cause the difference of query results. For example, \r\n\r\n ```\r\n  def createNaNDF(): DataFrame = {\r\n    Seq[(java.lang.Integer, java.lang.Long, java.lang.Short,\r\n      java.lang.Byte, java.lang.Float, java.lang.Double)](\r\n      (2, 2L, 2.toShort, 2.toByte, 2.0f, 2.0)\r\n    ).toDF(\"int\", \"long\", \"short\", \"byte\", \"float\", \"double\")\r\n  }\r\n\r\n  test(\"replace float with double\") {\r\n    createNaNDF().na.replace(\"*\", Map(\r\n      2.3 -> 9.0\r\n    )).show()\r\n\r\n    createNaNDF().na.replace(\"*\", Map(\r\n      2.3 -> 9.0\r\n    )).explain(true)\r\n  }\r\n```",
        "createdAt" : "2019-12-30T02:54:07Z",
        "updatedAt" : "2019-12-30T02:56:13Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "b6dc198c-7886-4278-b1a4-d6350831d42e",
        "parentId" : "8252e56d-2fda-4c63-910c-651e8d7c7890",
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "Before this PR, \r\n\r\n```\r\n+---+----+-----+----+-----+------+\r\n|int|long|short|byte|float|double|\r\n+---+----+-----+----+-----+------+\r\n|  9|   9|    9|   9|  2.0|   2.0|\r\n+---+----+-----+----+-----+------+\r\n\r\n== Parsed Logical Plan ==\r\nProject [CASE WHEN (int#99 = cast(2.3 as int)) THEN cast(9.0 as int) ELSE int#99 END AS int#117, CASE WHEN (long#100L = cast(2.3 as bigint)) THEN cast(9.0 as bigint) ELSE long#100L END AS long#118L, CASE WHEN (short#101 = cast(2.3 as smallint)) THEN cast(9.0 as smallint) ELSE short#101 END AS short#119, CASE WHEN (byte#102 = cast(2.3 as tinyint)) THEN cast(9.0 as tinyint) ELSE byte#102 END AS byte#120, CASE WHEN (float#103 = cast(2.3 as float)) THEN cast(9.0 as float) ELSE float#103 END AS float#121, CASE WHEN (double#104 = cast(2.3 as double)) THEN cast(9.0 as double) ELSE double#104 END AS double#122]\r\n+- Project [_1#86 AS int#99, _2#87L AS long#100L, _3#88 AS short#101, _4#89 AS byte#102, _5#90 AS float#103, _6#91 AS double#104]\r\n   +- LocalRelation [_1#86, _2#87L, _3#88, _4#89, _5#90, _6#91]\r\n\r\n== Analyzed Logical Plan ==\r\nint: int, long: bigint, short: smallint, byte: tinyint, float: float, double: double\r\nProject [CASE WHEN (int#99 = cast(2.3 as int)) THEN cast(9.0 as int) ELSE int#99 END AS int#117, CASE WHEN (long#100L = cast(2.3 as bigint)) THEN cast(9.0 as bigint) ELSE long#100L END AS long#118L, CASE WHEN (short#101 = cast(2.3 as smallint)) THEN cast(9.0 as smallint) ELSE short#101 END AS short#119, CASE WHEN (byte#102 = cast(2.3 as tinyint)) THEN cast(9.0 as tinyint) ELSE byte#102 END AS byte#120, CASE WHEN (float#103 = cast(2.3 as float)) THEN cast(9.0 as float) ELSE float#103 END AS float#121, CASE WHEN (double#104 = cast(2.3 as double)) THEN cast(9.0 as double) ELSE double#104 END AS double#122]\r\n+- Project [_1#86 AS int#99, _2#87L AS long#100L, _3#88 AS short#101, _4#89 AS byte#102, _5#90 AS float#103, _6#91 AS double#104]\r\n   +- LocalRelation [_1#86, _2#87L, _3#88, _4#89, _5#90, _6#91]\r\n\r\n== Optimized Logical Plan ==\r\nProject [CASE WHEN (_1#86 = 2) THEN 9 ELSE _1#86 END AS int#117, CASE WHEN (_2#87L = 2) THEN 9 ELSE _2#87L END AS long#118L, CASE WHEN (_3#88 = 2) THEN 9 ELSE _3#88 END AS short#119, CASE WHEN (_4#89 = 2) THEN 9 ELSE _4#89 END AS byte#120, CASE WHEN (_5#90 = 2.3) THEN 9.0 ELSE _5#90 END AS float#121, CASE WHEN (_6#91 = 2.3) THEN 9.0 ELSE _6#91 END AS double#122]\r\n+- LocalRelation [_1#86, _2#87L, _3#88, _4#89, _5#90, _6#91]\r\n\r\n== Physical Plan ==\r\n*(1) Project [CASE WHEN (_1#86 = 2) THEN 9 ELSE _1#86 END AS int#117, CASE WHEN (_2#87L = 2) THEN 9 ELSE _2#87L END AS long#118L, CASE WHEN (_3#88 = 2) THEN 9 ELSE _3#88 END AS short#119, CASE WHEN (_4#89 = 2) THEN 9 ELSE _4#89 END AS byte#120, CASE WHEN (_5#90 = 2.3) THEN 9.0 ELSE _5#90 END AS float#121, CASE WHEN (_6#91 = 2.3) THEN 9.0 ELSE _6#91 END AS double#122]\r\n+- *(1) LocalTableScan [_1#86, _2#87L, _3#88, _4#89, _5#90, _6#91]\r\n```\r\n\r\n\r\nAfter this PR, \r\n```\r\n+---+----+-----+----+-----+------+\r\n|int|long|short|byte|float|double|\r\n+---+----+-----+----+-----+------+\r\n|  2|   2|    2|   2|  2.0|   2.0|\r\n+---+----+-----+----+-----+------+\r\n\r\n== Parsed Logical Plan ==\r\n'Project [CASE WHEN (int#99 = 2.3) THEN cast(9.0 as int) ELSE int#99 END AS int#117, CASE WHEN (long#100L = 2.3) THEN cast(9.0 as bigint) ELSE long#100L END AS long#118, CASE WHEN (short#101 = 2.3) THEN cast(9.0 as smallint) ELSE short#101 END AS short#119, CASE WHEN (byte#102 = 2.3) THEN cast(9.0 as tinyint) ELSE byte#102 END AS byte#120, CASE WHEN (float#103 = 2.3) THEN cast(9.0 as float) ELSE float#103 END AS float#121, CASE WHEN (double#104 = 2.3) THEN cast(9.0 as double) ELSE double#104 END AS double#122]\r\n+- Project [_1#86 AS int#99, _2#87L AS long#100L, _3#88 AS short#101, _4#89 AS byte#102, _5#90 AS float#103, _6#91 AS double#104]\r\n   +- LocalRelation [_1#86, _2#87L, _3#88, _4#89, _5#90, _6#91]\r\n\r\n== Analyzed Logical Plan ==\r\nint: int, long: bigint, short: smallint, byte: tinyint, float: float, double: double\r\nProject [CASE WHEN (cast(int#99 as double) = 2.3) THEN cast(9.0 as int) ELSE int#99 END AS int#117, CASE WHEN (cast(long#100L as double) = 2.3) THEN cast(9.0 as bigint) ELSE long#100L END AS long#118L, CASE WHEN (cast(short#101 as double) = 2.3) THEN cast(9.0 as smallint) ELSE short#101 END AS short#119, CASE WHEN (cast(byte#102 as double) = 2.3) THEN cast(9.0 as tinyint) ELSE byte#102 END AS byte#120, CASE WHEN (cast(float#103 as double) = 2.3) THEN cast(9.0 as float) ELSE float#103 END AS float#121, CASE WHEN (double#104 = 2.3) THEN cast(9.0 as double) ELSE double#104 END AS double#122]\r\n+- Project [_1#86 AS int#99, _2#87L AS long#100L, _3#88 AS short#101, _4#89 AS byte#102, _5#90 AS float#103, _6#91 AS double#104]\r\n   +- LocalRelation [_1#86, _2#87L, _3#88, _4#89, _5#90, _6#91]\r\n\r\n== Optimized Logical Plan ==\r\nProject [CASE WHEN (cast(_1#86 as double) = 2.3) THEN 9 ELSE _1#86 END AS int#117, CASE WHEN (cast(_2#87L as double) = 2.3) THEN 9 ELSE _2#87L END AS long#118L, CASE WHEN (cast(_3#88 as double) = 2.3) THEN 9 ELSE _3#88 END AS short#119, CASE WHEN (cast(_4#89 as double) = 2.3) THEN 9 ELSE _4#89 END AS byte#120, CASE WHEN (cast(_5#90 as double) = 2.3) THEN 9.0 ELSE _5#90 END AS float#121, CASE WHEN (_6#91 = 2.3) THEN 9.0 ELSE _6#91 END AS double#122]\r\n+- LocalRelation [_1#86, _2#87L, _3#88, _4#89, _5#90, _6#91]\r\n\r\n== Physical Plan ==\r\n*(1) Project [CASE WHEN (cast(_1#86 as double) = 2.3) THEN 9 ELSE _1#86 END AS int#117, CASE WHEN (cast(_2#87L as double) = 2.3) THEN 9 ELSE _2#87L END AS long#118L, CASE WHEN (cast(_3#88 as double) = 2.3) THEN 9 ELSE _3#88 END AS short#119, CASE WHEN (cast(_4#89 as double) = 2.3) THEN 9 ELSE _4#89 END AS byte#120, CASE WHEN (cast(_5#90 as double) = 2.3) THEN 9.0 ELSE _5#90 END AS float#121, CASE WHEN (_6#91 = 2.3) THEN 9.0 ELSE _6#91 END AS double#122]\r\n+- *(1) LocalTableScan [_1#86, _2#87L, _3#88, _4#89, _5#90, _6#91]\r\n```",
        "createdAt" : "2019-12-30T02:55:38Z",
        "updatedAt" : "2019-12-30T02:56:13Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "430cfa34-16e7-428d-a129-d3b37f1b6a97",
        "parentId" : "8252e56d-2fda-4c63-910c-651e8d7c7890",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "The new behavior makes more sense, but I agree that the PR description needs update to reflect all the changes. cc @johnhany97 ",
        "createdAt" : "2019-12-30T03:23:12Z",
        "updatedAt" : "2019-12-30T03:23:12Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "7140da27-247b-45c1-972d-16adf3a505e1",
        "parentId" : "8252e56d-2fda-4c63-910c-651e8d7c7890",
        "authorId" : "6aa4fe6e-d8b4-470d-9ebd-61d8303b117f",
        "body" : "Nice catch there @gatorsmile. I've updated the PR description. Should I also update the PR title? Let me know if you'd like me to add in more details into the PR description.",
        "createdAt" : "2020-01-01T17:52:05Z",
        "updatedAt" : "2020-01-01T17:52:05Z",
        "lastEditedBy" : "6aa4fe6e-d8b4-470d-9ebd-61d8303b117f",
        "tags" : [
        ]
      },
      {
        "id" : "e68b870c-4c84-4237-8784-77e0c320c609",
        "parentId" : "8252e56d-2fda-4c63-910c-651e8d7c7890",
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "Yes, we also need to update the PR title. ",
        "createdAt" : "2020-01-03T03:28:43Z",
        "updatedAt" : "2020-01-03T03:28:44Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      }
    ],
    "commit" : "b3709a16e89284584922995f4877c31b5095aafe",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +457,461 @@    def buildExpr(v: Any) = Cast(Literal(v), keyExpr.dataType)\n    val branches = replacementMap.flatMap { case (source, target) =>\n      Seq(Literal(source), buildExpr(target))\n    }.toSeq\n    new Column(CaseKeyWhen(keyExpr, branches :+ keyExpr)).as(col.name)"
  },
  {
    "id" : "93ec2a31-2b0f-4e43-be33-161f9aa8bbaa",
    "prId" : 26593,
    "prUrl" : "https://github.com/apache/spark/pull/26593#pullrequestreview-321310989",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "496124ee-1c3a-4b83-bcce-5b91f205f6eb",
        "parentId" : null,
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "@cloud-fan I noticed that `drop()` is resolving column names by `cols.map(name => df.resolve(name)` instead of `df.col(name)`.  The difference (other than the return type) is that `df.col()` will try to resolve using regex and adding metadata. Do you think we need to make this consistent?",
        "createdAt" : "2019-11-20T19:29:43Z",
        "updatedAt" : "2019-11-25T06:19:50Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      },
      {
        "id" : "4547c3c0-d0bb-44bb-baa3-ec19a66e16c2",
        "parentId" : "496124ee-1c3a-4b83-bcce-5b91f205f6eb",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "We should. To avoid breaking change, I think we should change `drop` to follow `fill` to make it more powerful.",
        "createdAt" : "2019-11-22T02:47:05Z",
        "updatedAt" : "2019-11-25T06:19:50Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "96388fa8-01cc-479f-9bbe-cd56f626e322",
        "parentId" : "496124ee-1c3a-4b83-bcce-5b91f205f6eb",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "OK. I will do this in a separate PR which will address the issue with handling duplicate columns in `drop` when no columns are specified.",
        "createdAt" : "2019-11-22T02:53:51Z",
        "updatedAt" : "2019-11-25T06:19:50Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "033beb57c3c855c417b596d0bc450417eadb94da",
    "line" : 105,
    "diffHunk" : "@@ -1,1 +482,486 @@      case a: Attribute => a\n    }\n  }\n\n  private def outputAttributes: Seq[Attribute] = {"
  },
  {
    "id" : "a66f1b48-8e0a-4872-a776-4cb44000e101",
    "prId" : 26593,
    "prUrl" : "https://github.com/apache/spark/pull/26593#pullrequestreview-320662809",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "611b731e-b9ef-4618-87d3-cb320ff0b848",
        "parentId" : null,
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "@cloud-fan what's the behavior of \"*\" for `fill()`?\r\n\r\n```\r\nscala> right.show\r\n+----+----+\r\n|col1|col2|\r\n+----+----+\r\n|   1|   2|\r\n|   3|null|\r\n+----+----+\r\n\r\n\r\nscala> right.na.fill(\"hello\", Seq(\"*\")).show\r\n+----+----+\r\n|col1|col2|\r\n+----+----+\r\n|   1|   2|\r\n|   3|null|\r\n+----+----+\r\n```\r\nShould `*` work for `fill` such that all columns are affected? ",
        "createdAt" : "2019-11-20T19:40:10Z",
        "updatedAt" : "2019-11-25T06:19:50Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      },
      {
        "id" : "20b7fad8-6ac8-43a9-9f72-074ef7a4d4e4",
        "parentId" : "611b731e-b9ef-4618-87d3-cb320ff0b848",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "Actually, `drop` shows different behavior for `*` since it's using `df.resolve` instead of `df.col`:\r\n```Scala\r\nscala> right.na.drop(Seq(\"*\")).show\r\norg.apache.spark.sql.AnalysisException: Cannot resolve column name \"*\" among (col1, col2);\r\n  at org.apache.spark.sql.Dataset$$anonfun$resolve$1.apply(Dataset.scala:223)\r\n\r\n```",
        "createdAt" : "2019-11-20T20:17:58Z",
        "updatedAt" : "2019-11-25T06:19:50Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      },
      {
        "id" : "e934394b-f50b-47e6-92ca-12819817ae95",
        "parentId" : "611b731e-b9ef-4618-87d3-cb320ff0b848",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "good question. Personally I think we don't need to resolve `*` here, but to be safe let's keep the previous behavior unchanged.",
        "createdAt" : "2019-11-21T06:29:01Z",
        "updatedAt" : "2019-11-25T06:19:50Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "033beb57c3c855c417b596d0bc450417eadb94da",
    "line" : 1,
    "diffHunk" : "@@ -1,1 +128,132 @@  /**\n   * Returns a new `DataFrame` that replaces null or NaN values in numeric columns with `value`.\n   *\n   * @since 2.2.0\n   */"
  },
  {
    "id" : "9a8f0cb2-d622-428d-9c8a-555857c677ed",
    "prId" : 25768,
    "prUrl" : "https://github.com/apache/spark/pull/25768#pullrequestreview-291295798",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a51198ad-170f-4b10-9b12-4d6010e716b5",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "We can also traverse df.logicalPlan.output to avoid calling withColumns, but it might not be a big deal here.  ",
        "createdAt" : "2019-09-20T17:22:49Z",
        "updatedAt" : "2019-09-20T17:22:49Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      }
    ],
    "commit" : "59106dce05b97125c064950c67763ece4e3e19b6",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +489,493 @@\n    val columnEquals = df.sparkSession.sessionState.analyzer.resolver\n    val filledColumns = df.schema.fields.filter { f =>\n      val typeMatches = (targetType, f.dataType) match {\n        case (NumericType, dt) => dt.isInstanceOf[NumericType]"
  }
]