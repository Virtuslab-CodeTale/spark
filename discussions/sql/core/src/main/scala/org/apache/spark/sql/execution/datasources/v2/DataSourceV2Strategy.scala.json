[
  {
    "id" : "2bd96a57-137c-48e1-a46d-023887c56111",
    "prId" : 32931,
    "prUrl" : "https://github.com/apache/spark/pull/32931#pullrequestreview-685816490",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f4f3f5e1-ca50-4825-a3a9-a9afc54e2aeb",
        "parentId" : null,
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "case ShowCreateTable(rt: ResolvedTable, false, output) =>\r\n      ShowCreateTableExec(output, rt.table) :: Nil",
        "createdAt" : "2021-06-17T02:38:22Z",
        "updatedAt" : "2021-06-17T02:38:22Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "b8e6db8840b5de34213179ea54e02db287d6427f",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +378,382 @@      throw QueryCompilationErrors.loadDataNotSupportedForV2TablesError()\n\n    case ShowCreateTable(rt: ResolvedTable, asSerde, output) =>\n      if (asSerde) {\n        throw QueryCompilationErrors.showCreateTableAsSerdeNotSupportedForV2TablesError()"
  },
  {
    "id" : "16baa647-d6f2-4208-af3b-6f9d9314993b",
    "prId" : 32570,
    "prUrl" : "https://github.com/apache/spark/pull/32570#pullrequestreview-660919669",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "14913dfe-9d90-48be-a682-03ef4636a60a",
        "parentId" : null,
        "authorId" : "0e490efa-5aa1-407e-ab3b-a1b07a4ed0fa",
        "body" : "guess this is still better put on the line above",
        "createdAt" : "2021-05-17T12:29:57Z",
        "updatedAt" : "2021-05-17T12:29:57Z",
        "lastEditedBy" : "0e490efa-5aa1-407e-ab3b-a1b07a4ed0fa",
        "tags" : [
        ]
      },
      {
        "id" : "856523af-67a3-4072-a57b-0e9d6da59f26",
        "parentId" : "14913dfe-9d90-48be-a682-03ef4636a60a",
        "authorId" : "3a227965-84e0-47cf-9974-11293764f028",
        "body" : "If we do so, the length of line exceeds 100.",
        "createdAt" : "2021-05-17T12:43:16Z",
        "updatedAt" : "2021-05-17T12:43:16Z",
        "lastEditedBy" : "3a227965-84e0-47cf-9974-11293764f028",
        "tags" : [
        ]
      }
    ],
    "commit" : "4574440d134cf3ce0595b2afd12b47729749be97",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +113,117 @@\n    case PhysicalOperation(p, f, r: StreamingDataSourceV2Relation)\n      if r.startOffset.isDefined && r.endOffset.isDefined =>\n\n      val microBatchStream = r.stream.asInstanceOf[MicroBatchStream]"
  },
  {
    "id" : "b51dcdc8-4476-4ffc-9154-d08030dcc7fd",
    "prId" : 31422,
    "prUrl" : "https://github.com/apache/spark/pull/31422#pullrequestreview-580681146",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5f511371-14a0-413d-9b87-995d48d8d15f",
        "parentId" : null,
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "I will handle this TODO after this PR is merged; much easier to handle with new framework.",
        "createdAt" : "2021-02-01T19:14:05Z",
        "updatedAt" : "2021-02-02T18:21:37Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "8610b8e5924eba64394607ae80bcb0db89da5741",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +438,442 @@      AlterTableExec(table.catalog, table.identifier, changes) :: Nil\n\n    // TODO: v2 `UNSET TBLPROPERTIES` should respect the ifExists flag.\n    case AlterTableUnsetProperties(table: ResolvedTable, keys, _) =>\n      val changes = keys.map(key => TableChange.removeProperty(key))"
  },
  {
    "id" : "7269f02a-e62f-4327-9232-d708499877dc",
    "prId" : 31172,
    "prUrl" : "https://github.com/apache/spark/pull/31172#pullrequestreview-567685350",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "10225651-fcee-429d-a598-1ce9346921c7",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Is this correct? This looks wrong to me because we don't recache if the original table is not cached from the beginning. The new code looks like caching always.\r\n\r\ncc @sunchao ",
        "createdAt" : "2021-01-13T21:13:40Z",
        "updatedAt" : "2021-01-13T21:13:40Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "c4bc3859-d250-4100-a1e6-c7f9abac82eb",
        "parentId" : "10225651-fcee-429d-a598-1ce9346921c7",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Please see `if (cache.isDefined) {` condition, @MaxGekk .",
        "createdAt" : "2021-01-13T21:14:20Z",
        "updatedAt" : "2021-01-13T21:14:20Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "b530345a-c189-4045-a09e-7cf263a66742",
        "parentId" : "10225651-fcee-429d-a598-1ce9346921c7",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "> This looks wrong to me because we don't recache if the original table is not cached \r\n\r\nThis does the same. If something wasn't in the cache, it will not appear there.",
        "createdAt" : "2021-01-13T21:31:21Z",
        "updatedAt" : "2021-01-13T21:31:21Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "848392d6-7fc2-46d3-a930-22663856a095",
        "parentId" : "10225651-fcee-429d-a598-1ce9346921c7",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Let's imagine, we have cached Table1, and cached View1 which uses Table1.\r\n1. Current implementation: remove Table1 and View1, and add Table1 back. View1 will be uncached forever.\r\n2. Proposed: clear cached data of Table1 and View1, and keep them cached, so, next action will fill the cache for both Table1 and View1. ",
        "createdAt" : "2021-01-13T21:36:20Z",
        "updatedAt" : "2021-01-13T21:36:20Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "bb021e7a-d95c-44df-bd01-197da592f546",
        "parentId" : "10225651-fcee-429d-a598-1ce9346921c7",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Yes I'm +1 on the change. We should change the PR description & JIRA title to properly reflect this though (currently it looks like a refactoring). We should also document this in SQL migration guide.\r\n\r\nAlso cc @cloud-fan @gatorsmile for opinions on this.",
        "createdAt" : "2021-01-13T21:40:09Z",
        "updatedAt" : "2021-01-13T21:42:05Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "806a400a-e414-468a-a024-1fbbc3348b11",
        "parentId" : "10225651-fcee-429d-a598-1ce9346921c7",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "It was a refactoring till your test failed ;-)",
        "createdAt" : "2021-01-13T21:42:13Z",
        "updatedAt" : "2021-01-13T21:42:14Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "438e99e4-469d-4491-88fd-0dec008cb658",
        "parentId" : "10225651-fcee-429d-a598-1ce9346921c7",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Ah, got it. I was confused at `recacheTable`. Thanks.",
        "createdAt" : "2021-01-13T21:52:05Z",
        "updatedAt" : "2021-01-13T21:52:05Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "f4415ebe42443ce4127aebf4f83cf3f93ec531b2",
    "line" : 33,
    "diffHunk" : "@@ -1,1 +161,165 @@\n    case RefreshTable(r: ResolvedTable) =>\n      RefreshTableExec(r.catalog, r.identifier, recacheTable(r)) :: Nil\n\n    case ReplaceTable(catalog, ident, schema, parts, props, orCreate) =>"
  },
  {
    "id" : "9acda95b-aaa0-48f4-a446-375ff8218e8c",
    "prId" : 31172,
    "prUrl" : "https://github.com/apache/spark/pull/31172#pullrequestreview-568242677",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bbc5e9bf-8cca-48e1-a281-331195695d92",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "This seems the main point of this change. And as other comments, this looks like a behavior change or an inconsistent behavior to v1.\r\n\r\n> Reduce the number of calls to the Cache Manager when need to recache a table. Before the changes, invalidateCache() invokes the Cache Manager 3 times: lookupCachedData(), uncacheQuery() and cacheQuery().\r\n\r\nHmm, does this stand for the reason of this change? If we don't consider the behavior change part, `recacheByPlan` is actually more complicated than other three calls `lookupCachedData`, `uncacheQuery` and `cacheQuery`.\r\n\r\n",
        "createdAt" : "2021-01-14T06:56:37Z",
        "updatedAt" : "2021-01-14T06:56:38Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "ec47dab2-9cf8-45a0-ae46-2a688720f431",
        "parentId" : "bbc5e9bf-8cca-48e1-a281-331195695d92",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "I also think should be considered as a behavior change. Even though query result remains the same, it now lazily caches data which changes query execution time and memory/storage consumption. Because this, we should make a similar change for v1 as well.\r\n\r\nAnd please update the PR description to make it more clear (e.g., user-facing change).",
        "createdAt" : "2021-01-14T07:21:08Z",
        "updatedAt" : "2021-01-14T07:21:08Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "835c5e1e-a703-43b8-a82c-39ded7b5a380",
        "parentId" : "bbc5e9bf-8cca-48e1-a281-331195695d92",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "> this looks like a behavior change ...\r\n\r\nFrom correctness point of view, there are no behavior change.\r\n\r\n> an inconsistent behavior to v1\r\n\r\nLet's look at the command affected in this PR:\r\n- v1 `ALTER TABLE .. DROP PARTITION` does not do re-caching in v3.0, and has a bug, see https://github.com/apache/spark/pull/31006\r\n- v1 `ALTER TABLE .. ADD PARTITION` does not do re-caching too, and has a correctness bug in 3.0. See the fix https://github.com/apache/spark/pull/31116\r\n- v1 `ALTER TABLE .. RENAME PARTITION` also has correctness bug in 3.0: https://github.com/apache/spark/pull/31060\r\n\r\nCould you, please, explain what do you mean by \"inconsistent\" behavior to v1? Inconsistent to recent fixes?\r\n\r\n> does this stand for the reason of this change?\r\n\r\nNot main reason but one of the reasons.\r\n\r\n> recacheByPlan is actually more complicated than other three calls\r\n\r\nThis is arguable. I do believe it is simpler more efficient.\r\n",
        "createdAt" : "2021-01-14T07:24:35Z",
        "updatedAt" : "2021-01-14T07:24:36Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "5cebc525-73b3-47ff-af8f-77aef49927f3",
        "parentId" : "bbc5e9bf-8cca-48e1-a281-331195695d92",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "> ... we should make a similar change for v1 as well.\r\n\r\nI agree. We should change cache refreshing in v1 command either by introducing new function for that or modifying `CatalogImpl.refreshTable()`: https://github.com/apache/spark/blob/62d82b5b270c2eea27ba026e66b7d6598bbaa0a6/sql/core/src/main/scala/org/apache/spark/sql/internal/CatalogImpl.scala#L538-L558\r\nwhich can be used for v1 as well as for v2 tables/views (potentially but I am not sure that it works for v2 tables for now).",
        "createdAt" : "2021-01-14T07:39:25Z",
        "updatedAt" : "2021-01-14T07:39:25Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "891f5ebc-84dc-447f-b9d1-1b8980dba8de",
        "parentId" : "bbc5e9bf-8cca-48e1-a281-331195695d92",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Doesn't this only change for v2 behavior: https://github.com/apache/spark/pull/31172#discussion_r557031365? Based on https://github.com/apache/spark/pull/31172#discussion_r557030832, if we just uncache dependent caches for v1, isn't inconsistent to v1?\r\n\r\n> This is arguable. I do believe it is simpler more efficient.\r\n\r\nIf someone creates a method in cache manager to include three calls? :) I don't think the number of calls here is the point .\r\n\r\nBTW, I think recaching dependent caches is the most important point of this change, but the PR description doesn't explicitly mention it. This should be clear in the description.\r\n\r\nWhen I'm writing the comment, I saw you update the title and it looks more precise. That's good, but please update the description too.\r\n",
        "createdAt" : "2021-01-14T07:57:38Z",
        "updatedAt" : "2021-01-14T07:57:38Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "4e6f8fef-5089-4578-b94a-13ab939721cb",
        "parentId" : "bbc5e9bf-8cca-48e1-a281-331195695d92",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "> If someone creates a method in cache manager to include three calls? :) I don't think the number of calls here is the point .\r\n\r\nThose 2 back-to-back calls:\r\n```scala\r\n    val cache = session.sharedState.cacheManager.lookupCachedData(v2Relation)\r\n    session.sharedState.cacheManager.uncacheQuery(session, v2Relation, cascade = true)\r\n```\r\nlead to unnecessary traversals over cache. In total 3 calls performs more work in the cache, I do believe. \r\n\r\n> That's good, but please update the description too.\r\n\r\nI have updated it. Please, review it.",
        "createdAt" : "2021-01-14T08:29:48Z",
        "updatedAt" : "2021-01-14T08:29:48Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "e6ee5380-4983-49e2-87f4-1dfc42de1b4a",
        "parentId" : "bbc5e9bf-8cca-48e1-a281-331195695d92",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "yea let's update v1 tables as well (with another PR). It's weird if my query gets uncached because someone else refreshes a table.",
        "createdAt" : "2021-01-14T13:43:05Z",
        "updatedAt" : "2021-01-14T13:43:06Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "f4415ebe42443ce4127aebf4f83cf3f93ec531b2",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +64,68 @@    val v2Relation = DataSourceV2Relation.create(r.table, Some(r.catalog), Some(r.identifier))\n    session.sharedState.cacheManager.recacheByPlan(session, v2Relation)\n  }\n\n  // Invalidates the cache associated with the given table. If the invalidated cache matches the"
  },
  {
    "id" : "7ac0b453-cfb7-42bb-9ab7-589ba0379eaa",
    "prId" : 30881,
    "prUrl" : "https://github.com/apache/spark/pull/30881#pullrequestreview-558827844",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c40f57bf-6a15-435a-9081-30b97becc71c",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Is it possible to have `GetArrayStructFields` here? ",
        "createdAt" : "2020-12-25T08:29:19Z",
        "updatedAt" : "2020-12-30T16:14:02Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "6be12fe4-8f11-4c7a-bdb7-c19a7cb76a49",
        "parentId" : "c40f57bf-6a15-435a-9081-30b97becc71c",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "Thanks for the catch. In this case, I will match against non-`Attribute` for nested types similar to v1:\r\nhttps://github.com/apache/spark/blob/10b6466e91d2e954386c74bf6ab7d94f23dd6810/sql/core/src/main/scala/org/apache/spark/sql/execution/command/tables.scala#L768-L772",
        "createdAt" : "2020-12-25T16:36:45Z",
        "updatedAt" : "2020-12-30T16:14:02Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "a19f5f0b965d6c9dc59f1963ce6296e1639df751",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +276,280 @@\n    case desc @ DescribeColumn(_: ResolvedTable, column, isExtended) =>\n      column match {\n        case c: Attribute =>\n          DescribeColumnExec(desc.output, c, isExtended) :: Nil"
  },
  {
    "id" : "6c3ba1e4-dc8a-44ab-8565-5f92adf29f19",
    "prId" : 30856,
    "prUrl" : "https://github.com/apache/spark/pull/30856#pullrequestreview-555963354",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "74d4afbc-9de6-49b7-8410-daacedda433d",
        "parentId" : null,
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "This change is to work around #30610.",
        "createdAt" : "2020-12-19T19:29:24Z",
        "updatedAt" : "2020-12-19T22:09:19Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "378d73bcc7e828f15e7cf60b8ff0dce38a8c2329",
    "line" : 45,
    "diffHunk" : "@@ -1,1 +276,280 @@\n    case RenameTable(catalog, oldIdent, newIdent) =>\n      val tbl = ResolvedTable(catalog, oldIdent, catalog.loadTable(oldIdent))\n      RenameTableExec(\n        catalog,"
  },
  {
    "id" : "805e6e59-e3eb-4d01-bffb-6d59b77bfcf2",
    "prId" : 30742,
    "prUrl" : "https://github.com/apache/spark/pull/30742#pullrequestreview-551807115",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3624ae5a-9e83-4753-b441-4cb87aaff737",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "I think `cache` is only necessary in `if` block?",
        "createdAt" : "2020-12-12T05:29:37Z",
        "updatedAt" : "2020-12-14T21:45:08Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "3635662b-09cb-4bd6-8caf-cd8ea6a56405",
        "parentId" : "3624ae5a-9e83-4753-b441-4cb87aaff737",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "my bad - it should be checked in the if condition",
        "createdAt" : "2020-12-14T19:07:00Z",
        "updatedAt" : "2020-12-14T21:45:08Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      }
    ],
    "commit" : "3d8d3533f90fe1f777105337434c5128e5db9dde",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +59,63 @@  private def invalidateCache(r: ResolvedTable, recacheTable: Boolean = false)(): Unit = {\n    val v2Relation = DataSourceV2Relation.create(r.table, Some(r.catalog), Some(r.identifier))\n    val cache = session.sharedState.cacheManager.lookupCachedData(v2Relation)\n    session.sharedState.cacheManager.uncacheQuery(session, v2Relation, cascade = true)\n    if (recacheTable && cache.isDefined) {"
  },
  {
    "id" : "fdbf4476-0ab0-413c-945a-e12d70ba722f",
    "prId" : 30562,
    "prUrl" : "https://github.com/apache/spark/pull/30562#pullrequestreview-543204055",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dc3dc63b-78fa-4775-830a-7ce4ff840070",
        "parentId" : null,
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "is this exception handled later? the rewrite part for row deletion is a TBD?",
        "createdAt" : "2020-12-01T21:33:18Z",
        "updatedAt" : "2020-12-02T20:15:34Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "6bbf11bd-5c2c-44ff-8da8-3e0bd6a3b0c7",
        "parentId" : "dc3dc63b-78fa-4775-830a-7ce4ff840070",
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "The rewrite would happen earlier. This just throws a good error message if `deleteWhere` will fail.",
        "createdAt" : "2020-12-01T22:25:35Z",
        "updatedAt" : "2020-12-02T20:15:34Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      },
      {
        "id" : "3c7da5d3-7803-42e2-b1b6-ed9e9d37240e",
        "parentId" : "dc3dc63b-78fa-4775-830a-7ce4ff840070",
        "authorId" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "body" : "The rewrite part is yet to be done. This PR just adds a way to have more info at planning time. Specifically, we will know if the rewrite is needed.",
        "createdAt" : "2020-12-02T08:51:37Z",
        "updatedAt" : "2020-12-02T20:15:34Z",
        "lastEditedBy" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "tags" : [
        ]
      },
      {
        "id" : "e32a8df6-7ea9-4853-a0c9-b4f409d3eec7",
        "parentId" : "dc3dc63b-78fa-4775-830a-7ce4ff840070",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "I see, thanks. So this method will be called in an earlier place and before rewrite once the rewrite part is ready, is that right?",
        "createdAt" : "2020-12-02T19:23:22Z",
        "updatedAt" : "2020-12-02T20:15:34Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "4bdc62c9-f172-4c0e-8667-00d260aa4016",
        "parentId" : "dc3dc63b-78fa-4775-830a-7ce4ff840070",
        "authorId" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "body" : "It is going to be called at planning time to check if we should apply the rewrite or just pass filters down.",
        "createdAt" : "2020-12-02T20:18:42Z",
        "updatedAt" : "2020-12-02T20:18:43Z",
        "lastEditedBy" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "tags" : [
        ]
      }
    ],
    "commit" : "8b909f3abeb6ef84e9496b7f67bdfbf96f86940f",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +224,228 @@\n          if (!table.asDeletable.canDeleteWhere(filters)) {\n            throw new AnalysisException(\n              s\"Cannot delete from table ${table.name} where ${filters.mkString(\"[\", \", \", \"]\")}\")\n          }"
  },
  {
    "id" : "417d88a8-9de7-4efd-8b09-bf892ecbcfdf",
    "prId" : 30491,
    "prUrl" : "https://github.com/apache/spark/pull/30491#pullrequestreview-540409824",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f7f8ffa9-49ce-4c62-8d63-e4b71df7a57d",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "For `DeleteFromTable`, do we need to invalidate cache too? Doesn't this command also update table data?",
        "createdAt" : "2020-11-26T06:51:09Z",
        "updatedAt" : "2020-11-28T18:36:47Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "b5ba2cf2-7690-4553-8381-110e4e01a794",
        "parentId" : "f7f8ffa9-49ce-4c62-8d63-e4b71df7a57d",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Yes good catch! I think we should. I'll work on this in a separate PR.",
        "createdAt" : "2020-11-28T18:39:02Z",
        "updatedAt" : "2020-11-28T18:39:02Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      }
    ],
    "commit" : "98e9edeabc7df0f5b01c8f8cd9c886134bd0d85a",
    "line" : 56,
    "diffHunk" : "@@ -1,1 +207,211 @@        r.table.asWritable, writeOptions.asOptions, planLater(query), refreshCache(r)) :: Nil\n\n    case DeleteFromTable(relation, condition) =>\n      relation match {\n        case DataSourceV2ScanRelation(table, _, output) =>"
  },
  {
    "id" : "85cad231-a960-469f-bac0-566bc9e4890e",
    "prId" : 30229,
    "prUrl" : "https://github.com/apache/spark/pull/30229#pullrequestreview-523040994",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7e225f9f-77d9-4658-ae66-dfa0dc3a98e8",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "shall we fail it earlier in analyzer rules?",
        "createdAt" : "2020-11-03T14:54:53Z",
        "updatedAt" : "2020-11-03T18:46:05Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "d0bee98c-b320-4a11-9a8e-047a2c824473",
        "parentId" : "7e225f9f-77d9-4658-ae66-dfa0dc3a98e8",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "Do you want a separate rule to handle these? There will be more commands not supported in v2.\r\n\r\nBtw, we have similar checks in this file:\r\n```scala\r\n    case DescribeColumn(_: ResolvedTable, _, _) =>\r\n      throw new AnalysisException(\"Describing columns is not supported for v2 tables.\")\r\n```",
        "createdAt" : "2020-11-03T17:53:48Z",
        "updatedAt" : "2020-11-03T18:46:05Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      },
      {
        "id" : "e15e11be-c501-4be4-b99d-258fbdbc6708",
        "parentId" : "7e225f9f-77d9-4658-ae66-dfa0dc3a98e8",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "ah ok, then it's fine.",
        "createdAt" : "2020-11-04T04:47:41Z",
        "updatedAt" : "2020-11-04T04:47:41Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "c80e8eb5828ff68a1ebab4f9ee56b55b852ba7d0",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +282,286 @@\n    case AnalyzeTable(_: ResolvedTable, _, _) | AnalyzeColumn(_: ResolvedTable, _, _) =>\n      throw new AnalysisException(\"ANALYZE TABLE is not supported for v2 tables.\")\n\n    case _ => Nil"
  },
  {
    "id" : "a1991a1e-7001-451a-bc23-fead2a5687f8",
    "prId" : 30079,
    "prUrl" : "https://github.com/apache/spark/pull/30079#pullrequestreview-524620693",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "07e3f924-f9ca-480e-b19d-f888647bf617",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "not related to this PR, but we should think about how to handle the `purge` flag in v2 API.",
        "createdAt" : "2020-10-23T05:02:19Z",
        "updatedAt" : "2020-10-27T21:04:52Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "164abe73-41e2-4ccc-9082-b667f4de4a54",
        "parentId" : "07e3f924-f9ca-480e-b19d-f888647bf617",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "The `purge` option seems that it should be handled by the catalog implementation, not by Spark (e.g., `ifExists`). We can update `TableCatalog.dropTable` to pass an additional flag, or we could introduce `TableCatalog.purgeTable` for the catalog implementor can optionally implement.",
        "createdAt" : "2020-11-03T19:20:27Z",
        "updatedAt" : "2020-11-03T19:20:28Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      },
      {
        "id" : "1d019a98-31e9-4532-9b86-0d70bdef0a29",
        "parentId" : "07e3f924-f9ca-480e-b19d-f888647bf617",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "How useful is the `purge` option? If it's rarely used, we can just fail v2 DROP TABLE if `purge` is specified.\r\n\r\ncc @rdblue @brkyvz ",
        "createdAt" : "2020-11-04T03:02:43Z",
        "updatedAt" : "2020-11-04T03:02:44Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "76ad7be9-9126-48ea-8bdd-103ca2af0a9f",
        "parentId" : "07e3f924-f9ca-480e-b19d-f888647bf617",
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "I think it would make sense to add a variant of `dropTable` that accepts a purge option. Then have a default implementation that ignores it and calls `dropTable` without it. This seems like something that catalogs should be passed.",
        "createdAt" : "2020-11-04T17:14:24Z",
        "updatedAt" : "2020-11-04T17:14:25Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      },
      {
        "id" : "17d89936-14a3-4497-bc42-50dc03b4ae41",
        "parentId" : "07e3f924-f9ca-480e-b19d-f888647bf617",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Silently ignore the `purge` option as the default behavior looks risky. How about making the new variant of `dropTable` fail by default if `purge==true`?",
        "createdAt" : "2020-11-05T04:31:41Z",
        "updatedAt" : "2020-11-05T04:31:41Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "288d47c4-8b7f-4f89-9596-0bcd8dffe166",
        "parentId" : "07e3f924-f9ca-480e-b19d-f888647bf617",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "I created #30267 based on the feedback.",
        "createdAt" : "2020-11-05T19:59:23Z",
        "updatedAt" : "2020-11-05T19:59:23Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "dfc44923d10fa7cbb5cd6cde4dfcffa19a9e994d",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +229,233 @@      throw new AnalysisException(\"Describing columns is not supported for v2 tables.\")\n\n    case DropTable(r: ResolvedTable, ifExists, _) =>\n      DropTableExec(r.catalog, r.identifier, ifExists) :: Nil\n"
  },
  {
    "id" : "f8f9c48f-cbd2-4640-8a6b-8a71ba659455",
    "prId" : 26847,
    "prUrl" : "https://github.com/apache/spark/pull/26847#pullrequestreview-335311069",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e8e3081d-0a18-416c-9060-2fec0f111698",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "where do we remove the comment if it's an empty string?",
        "createdAt" : "2019-12-20T12:39:20Z",
        "updatedAt" : "2020-01-03T03:50:51Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "4bac4cc7-557c-4714-b3d4-43f7d889b5fc",
        "parentId" : "e8e3081d-0a18-416c-9060-2fec0f111698",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "we don't remove，we update the map with props_comment->empty string ",
        "createdAt" : "2019-12-20T12:50:05Z",
        "updatedAt" : "2020-01-03T03:50:51Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "7e3a260a-2669-4310-9481-b97fc7046c94",
        "parentId" : "e8e3081d-0a18-416c-9060-2fec0f111698",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Then our behavior is different from pgsql and presto?",
        "createdAt" : "2019-12-20T13:36:52Z",
        "updatedAt" : "2020-01-03T03:50:51Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "575540f5-8182-4a91-8def-8c97d1afc1a7",
        "parentId" : "e8e3081d-0a18-416c-9060-2fec0f111698",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "Ok, let’s get this straight.\r\nPostgres treats `null`/`empty string` the same as `null` to delete the comment key-value, but when we run `\\d+ xxx`, the Comment(Description) is a must-show field whether is null or not, see as below,\r\n```sql\r\npostgres=# \\d+\r\n                         List of relations\r\n Schema |    Name     | Type  |  Owner   |    Size    | Description\r\n--------+-------------+-------+----------+------------+-------------\r\n public | b           | table | postgres | 8192 bytes | abc\r\n public | bo          | table | postgres | 8192 bytes |\r\n public | i           | table | postgres | 0 bytes    |\r\n```\r\nIn Spark, w/ or w/o this PR, the Comment(Description) of a database is a must-show field for `describe namespace` as Postgres. When it's null(as if we delete it as Postgres), we have to fill it with an empty string for output.\r\nIn Spark w/o this PR, the Comment(Description) of a table is an optional field for `describe table`, it is not shown if it is null. when it an empty string, the field exists.\r\nIn Spark w/ this PR, the Comment(Description) of a table is a must-show field for `describe table` as Postgres.\r\n\r\nAdditionally, if we delete comment here when it is empty string, then we have to do the same in create-table-comment-clause.`CREATE TABLE(v int) COMMENT ''` should not leave a comment with empty string as it does now. \r\n\r\nAll in all, let's say the same result in different processes between spark and Postgres after this.",
        "createdAt" : "2019-12-20T15:10:23Z",
        "updatedAt" : "2020-01-03T03:50:51Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "d20b1b2a2beb7cd395b92b4330724bc2c1a2d6bd",
    "line" : 30,
    "diffHunk" : "@@ -1,1 +217,221 @@        catalog,\n        namespace,\n        Map(SupportsNamespaces.PROP_COMMENT -> comment)) :: Nil\n\n    case CommentOnTable(ResolvedTable(catalog, identifier, _), comment) =>"
  },
  {
    "id" : "a9430703-3bc7-4ccf-b6ba-fd15fd707f86",
    "prId" : 26847,
    "prUrl" : "https://github.com/apache/spark/pull/26847#pullrequestreview-336725929",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3c31fbd5-7da7-4a81-82ae-52ff2ff5559e",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "After some more thoughts, I think it's true that empty string comment is the same as no comment w.r.t. DESCRIBE command. But it's different to catalog developers who can access the properties directly. I think it's better to remove the comment property if the comment string is empty.",
        "createdAt" : "2019-12-27T08:47:04Z",
        "updatedAt" : "2020-01-03T03:50:51Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "7fffa735-0dd1-481a-9654-52e1b5ededc0",
        "parentId" : "3c31fbd5-7da7-4a81-82ae-52ff2ff5559e",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "This makes things a bit complicated and inconsistent. With our default implementation now in session catalog, the reserved props are unremovable https://github.com/apache/spark/blob/a0057131690b65b73aa6310500039d3a92466324/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2SessionCatalog.scala#L230-L235.\r\nIf we take null as empty string and update, then the logic between catalogs can be consistent  which also protected by the underlying  command `AlterNamespaceSetPropertiesExec`\r\nhttps://github.com/apache/spark/blob/c32e2286898fe8660c7deed9303f1c5c15b60757/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/AlterNamespaceSetPropertiesExec.scala#L32-L36",
        "createdAt" : "2019-12-27T09:38:55Z",
        "updatedAt" : "2020-01-03T03:50:51Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "0e6187b4-eebd-42b7-96f5-5fdbc193e694",
        "parentId" : "3c31fbd5-7da7-4a81-82ae-52ff2ff5559e",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "OK let's keep it as it is and revisit it later. I think we should only disallow end-users to explicitly remove reserved properties via `ALTER TABLE UNSET TBLPROPERTIES`.",
        "createdAt" : "2019-12-27T09:57:38Z",
        "updatedAt" : "2020-01-03T03:50:51Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "2c2b7700-3d1d-4ffd-8ac7-93d68d1e9889",
        "parentId" : "3c31fbd5-7da7-4a81-82ae-52ff2ff5559e",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "yes, I agree with that, we also may need to introduce `Alter Namespace Unset Properties` command too",
        "createdAt" : "2019-12-27T10:00:18Z",
        "updatedAt" : "2020-01-03T03:50:51Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "d20b1b2a2beb7cd395b92b4330724bc2c1a2d6bd",
    "line" : 26,
    "diffHunk" : "@@ -1,1 +213,217 @@      AlterNamespaceSetPropertiesExec(catalog, namespace, properties) :: Nil\n\n    case CommentOnNamespace(ResolvedNamespace(catalog, namespace), comment) =>\n      AlterNamespaceSetPropertiesExec(\n        catalog,"
  },
  {
    "id" : "03ed0da5-02aa-4a39-9b1a-5ee911e7dd52",
    "prId" : 26740,
    "prUrl" : "https://github.com/apache/spark/pull/26740#pullrequestreview-328646481",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3b275a0d-96e9-445c-b154-5af159de27e4",
        "parentId" : null,
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "This isn't the right place to throw an `AnalysisException` because this is after analysis has finished.\r\n\r\nThis rule should match `VacuumTable(DataSourceV2ScanRelation(table, _, _))` and produce the exec node. Then `CheckAnalysis` should have a rule that matches `VacuumTable` with any other relation that throws the analysis exception. ",
        "createdAt" : "2019-12-09T00:37:51Z",
        "updatedAt" : "2019-12-09T00:37:52Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      }
    ],
    "commit" : "cab9c673099d9043cb0d39565440f9263a386231",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +183,187 @@          VacuumTableExec(table.asVacuumable) :: Nil\n        case _ =>\n          throw new AnalysisException(\"VACUUM is only supported with v2 tables.\")\n      }\n"
  },
  {
    "id" : "968c9cd4-56ad-4189-b157-7399baf639b0",
    "prId" : 26231,
    "prUrl" : "https://github.com/apache/spark/pull/26231#pullrequestreview-341637608",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9a719c83-0b03-43bb-9089-72e1a8e43e00",
        "parentId" : null,
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "It seems like there is quite a bit of work done here in the strategy that should be in the exec node instead, like converting the RDD to a catalyst RDD. What about building a subclass of RowDataSourceScanExec that handles these concerns?",
        "createdAt" : "2019-12-20T19:35:01Z",
        "updatedAt" : "2020-01-16T08:35:54Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      },
      {
        "id" : "f602f05f-9f7c-47ec-a796-3673c4166290",
        "parentId" : "9a719c83-0b03-43bb-9089-72e1a8e43e00",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "This may be a good idea, but this is what we do for ds v1 (see the v1 rule `DataSourceStrategy`). I'd like to avoid changing the existing design choice we made before.",
        "createdAt" : "2019-12-23T04:35:54Z",
        "updatedAt" : "2020-01-16T08:35:54Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "fbffc940-91a0-40bf-b466-b0a8d4a7bc6c",
        "parentId" : "9a719c83-0b03-43bb-9089-72e1a8e43e00",
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "I'd like to clean this up before committing this change. I'm not so sure that this was a purposeful design choice in v1. If so, what was the rationale for putting this logic in the strategy?",
        "createdAt" : "2019-12-23T18:32:46Z",
        "updatedAt" : "2020-01-16T08:35:54Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      },
      {
        "id" : "a5b3cdf3-2152-446d-9ea3-aad36d5311ad",
        "parentId" : "9a719c83-0b03-43bb-9089-72e1a8e43e00",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I tried and it's not easy to refactor. The query plans in Spark are all case class and you can't create a sub-class of case class. And it's only a few lines of code that can be encapsulated, the part of getting the v1 scan should belong to the strategy.\r\n\r\nIf you have some ideas, please help and send a PR to my branch. I'm happy to see approaches that can make the code cleaner.",
        "createdAt" : "2019-12-24T08:34:14Z",
        "updatedAt" : "2020-01-16T08:35:54Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "ff4aa2fe-62ab-4446-a81c-57578085503e",
        "parentId" : "9a719c83-0b03-43bb-9089-72e1a8e43e00",
        "authorId" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "body" : "how about an alternate constructor?",
        "createdAt" : "2020-01-10T02:11:42Z",
        "updatedAt" : "2020-01-16T08:35:54Z",
        "lastEditedBy" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "tags" : [
        ]
      },
      {
        "id" : "feeefec5-e2ef-4ecf-966b-bdc78d527851",
        "parentId" : "9a719c83-0b03-43bb-9089-72e1a8e43e00",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "The Scala auxiliary constructors must call other constructors as the FIRST action, e.g. \r\n```\r\ndef this(...) {\r\n  do something\r\n  this(...)\r\n}\r\n```\r\nis not allowed.\r\n\r\nWe can create an object and add an `apply` method to encapsulate the 4 lines of code, but does it really matter? The 4 lines of code only appear once here.",
        "createdAt" : "2020-01-13T04:41:33Z",
        "updatedAt" : "2020-01-16T08:35:54Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "a48e7bb31bdab8abcd2b6ba9aa37faa464351f61",
    "line" : 55,
    "diffHunk" : "@@ -1,1 +67,71 @@      val originalOutputNames = relation.table.schema().map(_.name)\n      val requiredColumnsIndex = output.map(_.name).map(originalOutputNames.indexOf)\n      val dsScan = RowDataSourceScanExec(\n        output,\n        requiredColumnsIndex,"
  },
  {
    "id" : "d702b1e7-6ec6-4e0f-9384-96a26f588687",
    "prId" : 25679,
    "prUrl" : "https://github.com/apache/spark/pull/25679#pullrequestreview-283623223",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0e881134-2776-4314-bfc9-052f64562eed",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "also see https://github.com/apache/spark/pull/25626/files#diff-fc4a83061d40a242e7f7b729cb8ee870R251\r\n\r\nWe think that the subquery check is for the current limitation, which should not be treated as a capability check. It's better to put it near the implementation, and should be updated together when we improve the implementation.",
        "createdAt" : "2019-09-04T13:41:43Z",
        "updatedAt" : "2019-09-05T05:49:53Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "a25f9df12bdc8c05552ae22700bf8ff6cb9bdb50",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +238,242 @@\n    case DeleteFromTable(r: DataSourceV2Relation, condition) =>\n      if (SubqueryExpression.hasSubquery(condition)) {\n        throw new AnalysisException(\n          s\"Delete by condition with subquery is not supported: $condition\")"
  },
  {
    "id" : "021df729-401e-4fbd-810c-a361b65a94de",
    "prId" : 25586,
    "prUrl" : "https://github.com/apache/spark/pull/25586#pullrequestreview-284595922",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "97d4911e-c6a4-4a80-a34d-3e415a9035e8",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "BTW, shall we do the same for streaming scan?",
        "createdAt" : "2019-09-05T07:47:01Z",
        "updatedAt" : "2019-09-05T07:47:02Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "89b794a4-fcad-4e69-978d-b4ed02d196a3",
        "parentId" : "97d4911e-c6a4-4a80-a34d-3e415a9035e8",
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "I'm not as familiar with the streaming side. If we support columnar reads there, we probably can.",
        "createdAt" : "2019-09-05T23:41:52Z",
        "updatedAt" : "2019-09-05T23:41:52Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      }
    ],
    "commit" : "3fbb5dd90be4da916e33133ff748d31144181214",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +146,150 @@      val withFilter = filterCondition.map(FilterExec(_, batchExec)).getOrElse(batchExec)\n\n      val withProjection = if (withFilter.output != project || !batchExec.supportsColumnar) {\n        ProjectExec(project, withFilter)\n      } else {"
  }
]