[
  {
    "id" : "3f7a54f8-9423-4318-aa6e-9ce251cdde6b",
    "prId" : 32362,
    "prUrl" : "https://github.com/apache/spark/pull/32362#pullrequestreview-645573129",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4cfa2019-dac7-49ac-b483-99a8a678c52e",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "If all input RDDs have 0 partition, the query is very fast and we don't need to optimize?",
        "createdAt" : "2021-04-27T08:06:05Z",
        "updatedAt" : "2021-04-27T08:06:05Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "bd39955d-418b-4b82-bb6d-8c1d3eae6414",
        "parentId" : "4cfa2019-dac7-49ac-b483-99a8a678c52e",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "Logically it is. But for Spark server like `SparkThriftServer`, we always use large shuffle partitions (e.g. 8192) and depend on aqe to coalesce it. If some users query on a empty table it will waste too many tasks. And an another issue is that driver can be busy (unnecessary task event) with the single point problem.",
        "createdAt" : "2021-04-27T08:21:43Z",
        "updatedAt" : "2021-04-27T08:23:46Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      }
    ],
    "commit" : "9e395ac0b271470ee0f362064ed65f7cbea193bb",
    "line" : 27,
    "diffHunk" : "@@ -1,1 +69,73 @@      // `ShuffleQueryStageExec#mapStats` returns None when the input RDD has 0 partitions,\n      // we should skip it when calculating the `partitionStartIndices`.\n      // If all input RDDs have 0 partition, we create empty partition for every shuffle reader.\n      val validMetrics = shuffleStages.flatMap(_.mapStats)\n"
  }
]