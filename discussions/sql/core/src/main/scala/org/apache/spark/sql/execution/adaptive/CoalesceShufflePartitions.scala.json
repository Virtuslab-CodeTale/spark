[
  {
    "id" : "3f7a54f8-9423-4318-aa6e-9ce251cdde6b",
    "prId" : 32362,
    "prUrl" : "https://github.com/apache/spark/pull/32362#pullrequestreview-645573129",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4cfa2019-dac7-49ac-b483-99a8a678c52e",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "If all input RDDs have 0 partition, the query is very fast and we don't need to optimize?",
        "createdAt" : "2021-04-27T08:06:05Z",
        "updatedAt" : "2021-04-27T08:06:05Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "bd39955d-418b-4b82-bb6d-8c1d3eae6414",
        "parentId" : "4cfa2019-dac7-49ac-b483-99a8a678c52e",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "Logically it is. But for Spark server like `SparkThriftServer`, we always use large shuffle partitions (e.g. 8192) and depend on aqe to coalesce it. If some users query on a empty table it will waste too many tasks. And an another issue is that driver can be busy (unnecessary task event) with the single point problem.",
        "createdAt" : "2021-04-27T08:21:43Z",
        "updatedAt" : "2021-04-27T08:23:46Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      }
    ],
    "commit" : "9e395ac0b271470ee0f362064ed65f7cbea193bb",
    "line" : 27,
    "diffHunk" : "@@ -1,1 +69,73 @@      // `ShuffleQueryStageExec#mapStats` returns None when the input RDD has 0 partitions,\n      // we should skip it when calculating the `partitionStartIndices`.\n      // If all input RDDs have 0 partition, we create empty partition for every shuffle reader.\n      val validMetrics = shuffleStages.flatMap(_.mapStats)\n"
  },
  {
    "id" : "884fc4df-83da-46a0-b3e7-7ee854231018",
    "prId" : 31653,
    "prUrl" : "https://github.com/apache/spark/pull/31653#pullrequestreview-624549346",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9576a574-8fa4-4ec7-8e16-6c9e96faf595",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I agree with this comment, but then where shall we coalesce the partitions?",
        "createdAt" : "2021-03-29T16:39:03Z",
        "updatedAt" : "2021-03-29T17:14:45Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "9c8a5cc6-8441-470f-bc3b-dbd1865d78fa",
        "parentId" : "9576a574-8fa4-4ec7-8e16-6c9e96faf595",
        "authorId" : "9acf210d-4935-4e03-8384-d944eb558b45",
        "body" : "I don't understand this comment.  `CoalesceShufflePartitons` runs the same place as before.  The difference is that before it bailed out if `SparkPlan` it was processing already had `CustomShuffleReaderExec` nodes in it.  It cannot do so any more since `OptimizeSkewedJoin` now runs before coalesce and may insert `CustomShuffleReaderExec`.   \r\nSo I had to add `CoalesceShufflePartitions.newPartitionSpec` to make coalesce run after skew mitigation.",
        "createdAt" : "2021-03-30T17:53:51Z",
        "updatedAt" : "2021-03-30T17:53:51Z",
        "lastEditedBy" : "9acf210d-4935-4e03-8384-d944eb558b45",
        "tags" : [
        ]
      }
    ],
    "commit" : "7cfda59a1e9a8e0f86e2e6424c21639c1b5b14e3",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +40,44 @@    /* This is running before new QueryStageExec creation so either all leaves are\n     QueryStageExec nodes or all leaves are CustomShuffleReaderExec if OptimizeSkewJoin\n     mitigated something in the new stage. */\n    if (!plan.collectLeaves().forall(_.isInstanceOf[QueryStageExec])) {\n      // If not all leaf nodes are query stages, it's not safe to reduce the number of"
  },
  {
    "id" : "e5b27663-311d-4c06-903e-90c6df313617",
    "prId" : 28954,
    "prUrl" : "https://github.com/apache/spark/pull/28954#pullrequestreview-440569999",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "36c7374f-6ced-464a-837d-a62a402e17b2",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "if a query stage has multiple leaf shuffles, and only one of them has 0-partition input RDD. What shall we do?",
        "createdAt" : "2020-06-30T14:52:54Z",
        "updatedAt" : "2020-06-30T14:52:54Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "a0ef1071-7c66-4eac-b45e-1f51225abdeb",
        "parentId" : "36c7374f-6ced-464a-837d-a62a402e17b2",
        "authorId" : "334a081c-cba1-403e-ac80-30e6124077d1",
        "body" : "I think it's like coalescing one less shuffles and handled by the `nonEmpty` codes.",
        "createdAt" : "2020-07-01T06:09:52Z",
        "updatedAt" : "2020-07-01T06:09:52Z",
        "lastEditedBy" : "334a081c-cba1-403e-ac80-30e6124077d1",
        "tags" : [
        ]
      }
    ],
    "commit" : "5bf6de5ed486c6af98d0abce8ca2bb390f5091a8",
    "line" : 46,
    "diffHunk" : "@@ -1,1 +69,73 @@      }\n\n      if (validMetrics.isEmpty) {\n        updatePlan(Nil)\n      } else {"
  },
  {
    "id" : "8b5e030c-fea9-47b4-9426-cc6f86a83c50",
    "prId" : 28954,
    "prUrl" : "https://github.com/apache/spark/pull/28954#pullrequestreview-440162124",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cb13a8ca-1413-4206-a419-e2696515990f",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Can you add a comment for the case of 0-partition?",
        "createdAt" : "2020-06-30T15:57:49Z",
        "updatedAt" : "2020-06-30T15:58:14Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "5bf6de5ed486c6af98d0abce8ca2bb390f5091a8",
    "line" : 47,
    "diffHunk" : "@@ -1,1 +70,74 @@\n      if (validMetrics.isEmpty) {\n        updatePlan(Nil)\n      } else {\n        // We may have different pre-shuffle partition numbers, don't reduce shuffle partition"
  }
]