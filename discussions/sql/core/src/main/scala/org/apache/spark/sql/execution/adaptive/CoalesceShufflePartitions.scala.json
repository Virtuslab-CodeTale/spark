[
  {
    "id" : "3f7a54f8-9423-4318-aa6e-9ce251cdde6b",
    "prId" : 32362,
    "prUrl" : "https://github.com/apache/spark/pull/32362#pullrequestreview-645573129",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4cfa2019-dac7-49ac-b483-99a8a678c52e",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "If all input RDDs have 0 partition, the query is very fast and we don't need to optimize?",
        "createdAt" : "2021-04-27T08:06:05Z",
        "updatedAt" : "2021-04-27T08:06:05Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "bd39955d-418b-4b82-bb6d-8c1d3eae6414",
        "parentId" : "4cfa2019-dac7-49ac-b483-99a8a678c52e",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "Logically it is. But for Spark server like `SparkThriftServer`, we always use large shuffle partitions (e.g. 8192) and depend on aqe to coalesce it. If some users query on a empty table it will waste too many tasks. And an another issue is that driver can be busy (unnecessary task event) with the single point problem.",
        "createdAt" : "2021-04-27T08:21:43Z",
        "updatedAt" : "2021-04-27T08:23:46Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      }
    ],
    "commit" : "9e395ac0b271470ee0f362064ed65f7cbea193bb",
    "line" : 27,
    "diffHunk" : "@@ -1,1 +69,73 @@      // `ShuffleQueryStageExec#mapStats` returns None when the input RDD has 0 partitions,\n      // we should skip it when calculating the `partitionStartIndices`.\n      // If all input RDDs have 0 partition, we create empty partition for every shuffle reader.\n      val validMetrics = shuffleStages.flatMap(_.mapStats)\n"
  },
  {
    "id" : "884fc4df-83da-46a0-b3e7-7ee854231018",
    "prId" : 31653,
    "prUrl" : "https://github.com/apache/spark/pull/31653#pullrequestreview-624549346",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9576a574-8fa4-4ec7-8e16-6c9e96faf595",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I agree with this comment, but then where shall we coalesce the partitions?",
        "createdAt" : "2021-03-29T16:39:03Z",
        "updatedAt" : "2021-03-29T17:14:45Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "9c8a5cc6-8441-470f-bc3b-dbd1865d78fa",
        "parentId" : "9576a574-8fa4-4ec7-8e16-6c9e96faf595",
        "authorId" : "9acf210d-4935-4e03-8384-d944eb558b45",
        "body" : "I don't understand this comment.  `CoalesceShufflePartitons` runs the same place as before.  The difference is that before it bailed out if `SparkPlan` it was processing already had `CustomShuffleReaderExec` nodes in it.  It cannot do so any more since `OptimizeSkewedJoin` now runs before coalesce and may insert `CustomShuffleReaderExec`.   \r\nSo I had to add `CoalesceShufflePartitions.newPartitionSpec` to make coalesce run after skew mitigation.",
        "createdAt" : "2021-03-30T17:53:51Z",
        "updatedAt" : "2021-03-30T17:53:51Z",
        "lastEditedBy" : "9acf210d-4935-4e03-8384-d944eb558b45",
        "tags" : [
        ]
      }
    ],
    "commit" : "7cfda59a1e9a8e0f86e2e6424c21639c1b5b14e3",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +40,44 @@    /* This is running before new QueryStageExec creation so either all leaves are\n     QueryStageExec nodes or all leaves are CustomShuffleReaderExec if OptimizeSkewJoin\n     mitigated something in the new stage. */\n    if (!plan.collectLeaves().forall(_.isInstanceOf[QueryStageExec])) {\n      // If not all leaf nodes are query stages, it's not safe to reduce the number of"
  },
  {
    "id" : "e5b27663-311d-4c06-903e-90c6df313617",
    "prId" : 28954,
    "prUrl" : "https://github.com/apache/spark/pull/28954#pullrequestreview-440569999",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "36c7374f-6ced-464a-837d-a62a402e17b2",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "if a query stage has multiple leaf shuffles, and only one of them has 0-partition input RDD. What shall we do?",
        "createdAt" : "2020-06-30T14:52:54Z",
        "updatedAt" : "2020-06-30T14:52:54Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "a0ef1071-7c66-4eac-b45e-1f51225abdeb",
        "parentId" : "36c7374f-6ced-464a-837d-a62a402e17b2",
        "authorId" : "334a081c-cba1-403e-ac80-30e6124077d1",
        "body" : "I think it's like coalescing one less shuffles and handled by the `nonEmpty` codes.",
        "createdAt" : "2020-07-01T06:09:52Z",
        "updatedAt" : "2020-07-01T06:09:52Z",
        "lastEditedBy" : "334a081c-cba1-403e-ac80-30e6124077d1",
        "tags" : [
        ]
      }
    ],
    "commit" : "5bf6de5ed486c6af98d0abce8ca2bb390f5091a8",
    "line" : 46,
    "diffHunk" : "@@ -1,1 +69,73 @@      }\n\n      if (validMetrics.isEmpty) {\n        updatePlan(Nil)\n      } else {"
  },
  {
    "id" : "8b5e030c-fea9-47b4-9426-cc6f86a83c50",
    "prId" : 28954,
    "prUrl" : "https://github.com/apache/spark/pull/28954#pullrequestreview-440162124",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cb13a8ca-1413-4206-a419-e2696515990f",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Can you add a comment for the case of 0-partition?",
        "createdAt" : "2020-06-30T15:57:49Z",
        "updatedAt" : "2020-06-30T15:58:14Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "5bf6de5ed486c6af98d0abce8ca2bb390f5091a8",
    "line" : 47,
    "diffHunk" : "@@ -1,1 +70,74 @@\n      if (validMetrics.isEmpty) {\n        updatePlan(Nil)\n      } else {\n        // We may have different pre-shuffle partition numbers, don't reduce shuffle partition"
  },
  {
    "id" : "6fc6db8d-532c-4458-b456-76966d6b283c",
    "prId" : 27879,
    "prUrl" : "https://github.com/apache/spark/pull/27879#pullrequestreview-373228530",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "71610ac4-d611-491f-95d9-4b8be3722964",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "So, according to the PR description, do we still need to coalesce if the number of partitions has already been less than `minPartitionNum`?",
        "createdAt" : "2020-03-11T16:09:01Z",
        "updatedAt" : "2020-03-12T05:26:31Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "9397b0d9-0a15-47cc-a226-554eb256a9d1",
        "parentId" : "71610ac4-d611-491f-95d9-4b8be3722964",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "We don't. We should make it clear in `ShufflePartitionsUtil`",
        "createdAt" : "2020-03-11T16:52:27Z",
        "updatedAt" : "2020-03-12T05:26:31Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "5f72dbb6-6520-478f-86b8-5dac3f95ba97",
        "parentId" : "71610ac4-d611-491f-95d9-4b8be3722964",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "After looking into `ShufflePartitionsUtil`, we may still do coalesce if the partition sizes are really really small. The target size is at least 16 bytes, so if all partitions are 0 size, we still coalesce.\r\n\r\nI don't know why we pick 16, but this makes sense to me as it's obvious an overhead if we launch a task to read just 16 bytes.",
        "createdAt" : "2020-03-11T17:31:24Z",
        "updatedAt" : "2020-03-12T05:26:31Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "23054381-d2e7-45a9-b25c-c5084c606729",
        "parentId" : "71610ac4-d611-491f-95d9-4b8be3722964",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "I see.",
        "createdAt" : "2020-03-12T01:36:08Z",
        "updatedAt" : "2020-03-12T05:26:31Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "4f1b33ba03cb0a9b32f2deb4fb8a4df0d869a72b",
    "line" : 33,
    "diffHunk" : "@@ -1,1 +78,82 @@          lastPartitionIndex = distinctNumPreShufflePartitions.head,\n          advisoryTargetSize = conf.getConf(SQLConf.ADVISORY_PARTITION_SIZE_IN_BYTES),\n          minNumPartitions = minPartitionNum)\n        // This transformation adds new nodes, so we must use `transformUp` here.\n        val stageIds = shuffleStages.map(_.id).toSet"
  },
  {
    "id" : "db3795ed-7fc3-43df-9264-1c85a6f417f8",
    "prId" : 27879,
    "prUrl" : "https://github.com/apache/spark/pull/27879#pullrequestreview-372973130",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6c1b3ef0-d7d3-4c0a-939d-52f887600c13",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "This is the reason why we need `session` instead of `conf`?",
        "createdAt" : "2020-03-11T17:12:05Z",
        "updatedAt" : "2020-03-12T05:26:31Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "ced1d9d0-e36c-4c3a-9d5c-eb5050b73443",
        "parentId" : "6c1b3ef0-d7d3-4c0a-939d-52f887600c13",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Yup",
        "createdAt" : "2020-03-11T17:26:39Z",
        "updatedAt" : "2020-03-12T05:26:31Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "4f1b33ba03cb0a9b32f2deb4fb8a4df0d869a72b",
    "line" : 26,
    "diffHunk" : "@@ -1,1 +72,76 @@        // is not set, so to avoid perf regressions compared to no coalescing.\n        val minPartitionNum = conf.getConf(SQLConf.COALESCE_PARTITIONS_MIN_PARTITION_NUM)\n          .getOrElse(session.sparkContext.defaultParallelism)\n        val partitionSpecs = ShufflePartitionsUtil.coalescePartitions(\n          validMetrics.toArray,"
  }
]