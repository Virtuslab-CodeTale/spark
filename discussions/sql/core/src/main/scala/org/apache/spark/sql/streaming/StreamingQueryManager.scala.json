[
  {
    "id" : "fa9d5cf8-bc1a-4285-b7b1-2f076692378b",
    "prId" : 31842,
    "prUrl" : "https://github.com/apache/spark/pull/31842#pullrequestreview-616161542",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a155bdf7-24c3-48b2-9dc0-4b3820c381a5",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "This seems to introduce the analysis overhead here. Do we have a plan to add more rules? Or, maybe, something like: \r\n```scala\r\nval analyzedStreamWritePlan  = rules.foldLeft(dataStreamWritePlan) { case (p, r) => r.apply(p) }\r\n```\r\nshould be enough?\r\n\r\n",
        "createdAt" : "2021-03-18T13:04:56Z",
        "updatedAt" : "2021-03-19T14:29:22Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "8bbd6d99-27a0-4233-9282-5ff49133f237",
        "parentId" : "a155bdf7-24c3-48b2-9dc0-4b3820c381a5",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "We plan to let more rules to resolve the different parts of WriteToStreamStatement in the future. e.g. resolve table.",
        "createdAt" : "2021-03-19T08:31:44Z",
        "updatedAt" : "2021-03-19T14:29:22Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "7ec1afb4bfe269b472bf3110fc7bd580ae05e084",
    "line" : 75,
    "diffHunk" : "@@ -1,1 +253,257 @@\n    val analyzedStreamWritePlan =\n      sparkSession.sessionState.executePlan(dataStreamWritePlan).analyzed\n        .asInstanceOf[WriteToStream]\n"
  }
]