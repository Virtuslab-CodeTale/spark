[
  {
    "id" : "fa9d5cf8-bc1a-4285-b7b1-2f076692378b",
    "prId" : 31842,
    "prUrl" : "https://github.com/apache/spark/pull/31842#pullrequestreview-616161542",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a155bdf7-24c3-48b2-9dc0-4b3820c381a5",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "This seems to introduce the analysis overhead here. Do we have a plan to add more rules? Or, maybe, something like: \r\n```scala\r\nval analyzedStreamWritePlan  = rules.foldLeft(dataStreamWritePlan) { case (p, r) => r.apply(p) }\r\n```\r\nshould be enough?\r\n\r\n",
        "createdAt" : "2021-03-18T13:04:56Z",
        "updatedAt" : "2021-03-19T14:29:22Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "8bbd6d99-27a0-4233-9282-5ff49133f237",
        "parentId" : "a155bdf7-24c3-48b2-9dc0-4b3820c381a5",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "We plan to let more rules to resolve the different parts of WriteToStreamStatement in the future. e.g. resolve table.",
        "createdAt" : "2021-03-19T08:31:44Z",
        "updatedAt" : "2021-03-19T14:29:22Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "7ec1afb4bfe269b472bf3110fc7bd580ae05e084",
    "line" : 75,
    "diffHunk" : "@@ -1,1 +253,257 @@\n    val analyzedStreamWritePlan =\n      sparkSession.sessionState.executePlan(dataStreamWritePlan).analyzed\n        .asInstanceOf[WriteToStream]\n"
  },
  {
    "id" : "9afd52cf-b8fa-4445-a2af-df393db6d618",
    "prId" : 27678,
    "prUrl" : "https://github.com/apache/spark/pull/27678#pullrequestreview-363152157",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e4149191-b8a8-4f67-893f-c1a50d8d98c4",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "minor: While we're here, how about simply calling `awaitAnyTermination(Long.MAX_VALUE)` which is effectively same?",
        "createdAt" : "2020-02-24T02:51:42Z",
        "updatedAt" : "2020-02-24T02:52:07Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "e8e77f80082ff5853ee014bbc3aa07cd8917ce67",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +134,138 @@  @throws[StreamingQueryException]\n  def awaitAnyTermination(): Unit = {\n    awaitTerminationLock.synchronized {\n      while (lastTerminatedQueryException == null) {\n        awaitTerminationLock.wait(10)"
  },
  {
    "id" : "f5456b4d-7930-455a-bec9-b9d08de50369",
    "prId" : 26316,
    "prUrl" : "https://github.com/apache/spark/pull/26316#pullrequestreview-309055683",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4653fcc6-2910-4983-b1a7-d0aae399b73e",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Got it. This API is added at `3.0.0` only.",
        "createdAt" : "2019-10-30T09:22:19Z",
        "updatedAt" : "2019-10-30T09:22:19Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "d0560b1ba23ed18c1ff6ece6493ae092e8bd6c4a",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +201,205 @@  private[sql] def listListeners(): Array[StreamingQueryListener] = {\n    listenerBus.listeners.asScala.toArray\n  }\n\n  /** Post a listener event */"
  },
  {
    "id" : "2e40a088-80ea-43c7-9df1-dc7024095e33",
    "prId" : 26225,
    "prUrl" : "https://github.com/apache/spark/pull/26225#pullrequestreview-314500769",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "db575187-e284-4a8e-a01c-db37c5fccfc2",
        "parentId" : null,
        "authorId" : "9cd657f0-37cd-41bd-908a-f2c996b95a7a",
        "body" : "super nit: probably update this to  \"is already active in this SparkSession.\" to be clearer",
        "createdAt" : "2019-11-08T22:50:13Z",
        "updatedAt" : "2019-11-13T03:34:07Z",
        "lastEditedBy" : "9cd657f0-37cd-41bd-908a-f2c996b95a7a",
        "tags" : [
        ]
      }
    ],
    "commit" : "bff9162d134269eebb95e6597bba284d4a3b5944",
    "line" : 66,
    "diffHunk" : "@@ -1,1 +355,359 @@            s\"with that name is already active in this SparkSession\")\n        }\n      }\n\n      // Make sure no other query with same id is active across all sessions"
  },
  {
    "id" : "a6222cd6-341e-4a54-a3e8-3472a2920f2c",
    "prId" : 26225,
    "prUrl" : "https://github.com/apache/spark/pull/26225#pullrequestreview-314500769",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "84e47b3c-90ea-435c-8128-94507b6d3ca2",
        "parentId" : null,
        "authorId" : "9cd657f0-37cd-41bd-908a-f2c996b95a7a",
        "body" : "Isnt it more intuitive to make these conditions.\r\n\r\n```\r\nif (activeOption.isDefined) {\r\n   if (turnOffOldStream) { ...  } else { ... } \r\n}\r\n```",
        "createdAt" : "2019-11-09T00:36:35Z",
        "updatedAt" : "2019-11-13T03:34:07Z",
        "lastEditedBy" : "9cd657f0-37cd-41bd-908a-f2c996b95a7a",
        "tags" : [
        ]
      }
    ],
    "commit" : "bff9162d134269eebb95e6597bba284d4a3b5944",
    "line" : 96,
    "diffHunk" : "@@ -1,1 +378,382 @@              \"and retry.\")\n        }\n      } else {\n        // nothing to stop so, no-op\n        None"
  },
  {
    "id" : "a0c347e6-1f48-479b-a860-afbfbdc6f8de",
    "prId" : 26225,
    "prUrl" : "https://github.com/apache/spark/pull/26225#pullrequestreview-315960724",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "03849789-6c48-4664-8910-7fb281b27959",
        "parentId" : null,
        "authorId" : "9cd657f0-37cd-41bd-908a-f2c996b95a7a",
        "body" : "This is not the correct error message when `stopActiveRunOnRestart` is false. \r\nIf the active run was stopped, then this error message is correct.\r\nIf the active run was not stopped, then this error will be thrown and therefore should simply say that there is an active run (run id ...).\r\n\r\nIn other words, this can stay as the same message as it was in Spark 2.4,.... may be improved by adding the run id.",
        "createdAt" : "2019-11-13T02:02:53Z",
        "updatedAt" : "2019-11-13T03:34:07Z",
        "lastEditedBy" : "9cd657f0-37cd-41bd-908a-f2c996b95a7a",
        "tags" : [
        ]
      }
    ],
    "commit" : "bff9162d134269eebb95e6597bba284d4a3b5944",
    "line" : 113,
    "diffHunk" : "@@ -1,1 +395,399 @@      if (oldActiveQuery != null) {\n        throw new ConcurrentModificationException(\n          \"Another instance of this query was just started by a concurrent session.\")\n      }\n      activeQueries.put(query.id, query)"
  },
  {
    "id" : "046dd0e2-840c-451a-96d3-3a61ebcce8ad",
    "prId" : 26225,
    "prUrl" : "https://github.com/apache/spark/pull/26225#pullrequestreview-315961063",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "93bf5ee7-9a02-40ce-9106-2ef36cccc2c2",
        "parentId" : null,
        "authorId" : "9cd657f0-37cd-41bd-908a-f2c996b95a7a",
        "body" : "nit: This comment is true only if the active run was stopped. So qualify the comment accordingly.",
        "createdAt" : "2019-11-13T02:04:10Z",
        "updatedAt" : "2019-11-13T03:34:07Z",
        "lastEditedBy" : "9cd657f0-37cd-41bd-908a-f2c996b95a7a",
        "tags" : [
        ]
      }
    ],
    "commit" : "bff9162d134269eebb95e6597bba284d4a3b5944",
    "line" : 106,
    "diffHunk" : "@@ -1,1 +388,392 @@\n    activeQueriesSharedLock.synchronized {\n      // We still can have a race condition when two concurrent instances try to start the same\n      // stream, while a third one was already active and stopped above. In this case, we throw a\n      // ConcurrentModificationException."
  },
  {
    "id" : "54075d5b-904f-44e3-a892-d539b2dc596f",
    "prId" : 26225,
    "prUrl" : "https://github.com/apache/spark/pull/26225#pullrequestreview-315961363",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b6548801-bd61-4bcd-8a13-f4543baa90b8",
        "parentId" : null,
        "authorId" : "9cd657f0-37cd-41bd-908a-f2c996b95a7a",
        "body" : "nit: Please document here that stop() will automatically clear the `activeStreamingQueries`. Without this implicit easy-to-miss information, it is hard to reason about this code. ",
        "createdAt" : "2019-11-13T02:05:19Z",
        "updatedAt" : "2019-11-13T03:34:07Z",
        "lastEditedBy" : "9cd657f0-37cd-41bd-908a-f2c996b95a7a",
        "tags" : [
        ]
      }
    ],
    "commit" : "bff9162d134269eebb95e6597bba284d4a3b5944",
    "line" : 103,
    "diffHunk" : "@@ -1,1 +385,389 @@\n    // stop() will clear the queryId from activeStreamingQueries as well as activeQueries\n    activeRunOpt.foreach(_.stop())\n\n    activeQueriesSharedLock.synchronized {"
  },
  {
    "id" : "a40dde09-78b7-4bd5-a787-5d21e71e31d1",
    "prId" : 24529,
    "prUrl" : "https://github.com/apache/spark/pull/24529#pullrequestreview-234283949",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "705c322b-ba5a-4aef-9bc2-daddf82dda1a",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "I checked the code, I think the origin design do not want to delete the checkpoint directory unless you have not specify one.\r\nIf you want control delete behavior, not specify the checkpoint directory. Temp directory will be delete after app stop.",
        "createdAt" : "2019-05-07T02:25:37Z",
        "updatedAt" : "2019-05-07T07:59:13Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "0e4ced547d461cca4ed284a6be469d40f5a8a5a5",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +216,220 @@      trigger: Trigger,\n      triggerClock: Clock): StreamingQueryWrapper = {\n    var deleteCheckpointOnStop = sparkSession.sessionState.conf.isDeleteCheckpoint.getOrElse(false)\n    val checkpointLocation = userSpecifiedCheckpointLocation.map { userSpecified =>\n      new Path(userSpecified).toString"
  }
]