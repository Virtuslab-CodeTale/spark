[
  {
    "id" : "fa9d5cf8-bc1a-4285-b7b1-2f076692378b",
    "prId" : 31842,
    "prUrl" : "https://github.com/apache/spark/pull/31842#pullrequestreview-616161542",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a155bdf7-24c3-48b2-9dc0-4b3820c381a5",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "This seems to introduce the analysis overhead here. Do we have a plan to add more rules? Or, maybe, something like: \r\n```scala\r\nval analyzedStreamWritePlan  = rules.foldLeft(dataStreamWritePlan) { case (p, r) => r.apply(p) }\r\n```\r\nshould be enough?\r\n\r\n",
        "createdAt" : "2021-03-18T13:04:56Z",
        "updatedAt" : "2021-03-19T14:29:22Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "8bbd6d99-27a0-4233-9282-5ff49133f237",
        "parentId" : "a155bdf7-24c3-48b2-9dc0-4b3820c381a5",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "We plan to let more rules to resolve the different parts of WriteToStreamStatement in the future. e.g. resolve table.",
        "createdAt" : "2021-03-19T08:31:44Z",
        "updatedAt" : "2021-03-19T14:29:22Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "7ec1afb4bfe269b472bf3110fc7bd580ae05e084",
    "line" : 75,
    "diffHunk" : "@@ -1,1 +253,257 @@\n    val analyzedStreamWritePlan =\n      sparkSession.sessionState.executePlan(dataStreamWritePlan).analyzed\n        .asInstanceOf[WriteToStream]\n"
  },
  {
    "id" : "9afd52cf-b8fa-4445-a2af-df393db6d618",
    "prId" : 27678,
    "prUrl" : "https://github.com/apache/spark/pull/27678#pullrequestreview-363152157",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e4149191-b8a8-4f67-893f-c1a50d8d98c4",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "minor: While we're here, how about simply calling `awaitAnyTermination(Long.MAX_VALUE)` which is effectively same?",
        "createdAt" : "2020-02-24T02:51:42Z",
        "updatedAt" : "2020-02-24T02:52:07Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "e8e77f80082ff5853ee014bbc3aa07cd8917ce67",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +134,138 @@  @throws[StreamingQueryException]\n  def awaitAnyTermination(): Unit = {\n    awaitTerminationLock.synchronized {\n      while (lastTerminatedQueryException == null) {\n        awaitTerminationLock.wait(10)"
  },
  {
    "id" : "f5456b4d-7930-455a-bec9-b9d08de50369",
    "prId" : 26316,
    "prUrl" : "https://github.com/apache/spark/pull/26316#pullrequestreview-309055683",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4653fcc6-2910-4983-b1a7-d0aae399b73e",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Got it. This API is added at `3.0.0` only.",
        "createdAt" : "2019-10-30T09:22:19Z",
        "updatedAt" : "2019-10-30T09:22:19Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "d0560b1ba23ed18c1ff6ece6493ae092e8bd6c4a",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +201,205 @@  private[sql] def listListeners(): Array[StreamingQueryListener] = {\n    listenerBus.listeners.asScala.toArray\n  }\n\n  /** Post a listener event */"
  }
]