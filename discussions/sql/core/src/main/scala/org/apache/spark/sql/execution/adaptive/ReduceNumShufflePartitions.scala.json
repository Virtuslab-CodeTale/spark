[
  {
    "id" : "94096a68-5794-45e0-bc39-0086c273825c",
    "prId" : 27742,
    "prUrl" : "https://github.com/apache/spark/pull/27742#pullrequestreview-366963216",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "203fa23a-d288-4289-be51-5714bdb80571",
        "parentId" : null,
        "authorId" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "body" : "@cloud-fan Is that we should skip this optimization at all if there's a skew join instead of skip counting the stages as in https://github.com/apache/spark/pull/27742/files#diff-e23b4656e59b73d313271d62329eefc2L47-L48 ?",
        "createdAt" : "2020-02-29T03:38:04Z",
        "updatedAt" : "2020-03-02T15:02:56Z",
        "lastEditedBy" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "tags" : [
        ]
      },
      {
        "id" : "5c8533ca-c9bb-47b4-8a39-cb2fd6399a5e",
        "parentId" : "203fa23a-d288-4289-be51-5714bdb80571",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "It was written this way as it needs to coalesce the shuffles partitions for non-skew partitions. But it's not a problem now.",
        "createdAt" : "2020-03-02T05:54:15Z",
        "updatedAt" : "2020-03-02T15:02:56Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "25327df2a1474385336aae243fc237372d137051",
    "line" : 26,
    "diffHunk" : "@@ -1,1 +35,39 @@    }\n    if (!plan.collectLeaves().forall(_.isInstanceOf[QueryStageExec])\n        || plan.find(_.isInstanceOf[CustomShuffleReaderExec]).isDefined) {\n      // If not all leaf nodes are query stages, it's not safe to reduce the number of\n      // shuffle partitions, because we may break the assumption that all children of a spark plan"
  },
  {
    "id" : "fed8eea6-e2cc-4b95-aa60-2aa875b74ec6",
    "prId" : 26434,
    "prUrl" : "https://github.com/apache/spark/pull/26434#pullrequestreview-341614559",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "33f9721e-39d3-4e6c-8538-412cce82ace6",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "why do we need to define an `i`? looking at the code seems `i` is always equal to `nextPartitionIndices`",
        "createdAt" : "2019-12-18T14:34:20Z",
        "updatedAt" : "2020-01-14T08:24:47Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "4bc13cf5-2118-445d-b2bc-da49472e7667",
        "parentId" : "33f9721e-39d3-4e6c-8538-412cce82ace6",
        "authorId" : "1b84a7ff-6bf9-4417-bf9f-e46e997e5974",
        "body" : "the parameter `i `is used to omit the excluded partition. When the partition is excluded, `i `is not equal to `nextPartitionIndices`",
        "createdAt" : "2019-12-20T08:12:13Z",
        "updatedAt" : "2020-01-14T08:24:47Z",
        "lastEditedBy" : "1b84a7ff-6bf9-4417-bf9f-e46e997e5974",
        "tags" : [
        ]
      },
      {
        "id" : "195dd9d2-9662-4d36-9021-f1c51f249af5",
        "parentId" : "33f9721e-39d3-4e6c-8538-412cce82ace6",
        "authorId" : "334a081c-cba1-403e-ac80-30e6124077d1",
        "body" : "`i` is actually the (candidate) \"partitionEndIndex\" and would be better inited to `firstStartIndex + 1` (also reset to `nextPartitionIndex + 1`). That helps me to understand the logic here.",
        "createdAt" : "2020-01-11T04:50:30Z",
        "updatedAt" : "2020-01-14T08:24:47Z",
        "lastEditedBy" : "334a081c-cba1-403e-ac80-30e6124077d1",
        "tags" : [
        ]
      },
      {
        "id" : "d9dfe812-f4bd-4fe4-9668-ad8059c058c2",
        "parentId" : "33f9721e-39d3-4e6c-8538-412cce82ace6",
        "authorId" : "1b84a7ff-6bf9-4417-bf9f-e46e997e5974",
        "body" : "Here` i` stands for the` partitionStartIndex` and not the `partitionEndIndex`.",
        "createdAt" : "2020-01-13T01:08:44Z",
        "updatedAt" : "2020-01-14T08:24:47Z",
        "lastEditedBy" : "1b84a7ff-6bf9-4417-bf9f-e46e997e5974",
        "tags" : [
        ]
      }
    ],
    "commit" : "ac17a7cb869a093745ff0d60fe9d3837ede72c85",
    "line" : 144,
    "diffHunk" : "@@ -1,1 +177,181 @@    partitionStartIndices += firstStartIndex\n    var postShuffleInputSize = mapOutputStatistics.map(_.bytesByPartitionId(firstStartIndex)).sum\n    var i = firstStartIndex\n    includedPartitions.drop(1).foreach { nextPartitionIndex =>\n        val nextShuffleInputSize ="
  },
  {
    "id" : "b3469a7d-cbc3-47c3-a160-1f813b02d8cf",
    "prId" : 25479,
    "prUrl" : "https://github.com/apache/spark/pull/25479#pullrequestreview-276080526",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fd198c92-df22-4d08-a2c1-6c9d48a613e9",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "let's also give an example about when we will have different pre-shuffle partition numbers.",
        "createdAt" : "2019-08-16T15:40:23Z",
        "updatedAt" : "2019-08-16T17:39:38Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "08787d11-e6df-448d-9366-c349200a4d14",
        "parentId" : "fd198c92-df22-4d08-a2c1-6c9d48a613e9",
        "authorId" : "3d4870da-39a8-4406-b00d-131930d14cd8",
        "body" : "Ok, added. Please let me know if it should be more detailed.",
        "createdAt" : "2019-08-16T17:40:48Z",
        "updatedAt" : "2019-08-16T17:40:49Z",
        "lastEditedBy" : "3d4870da-39a8-4406-b00d-131930d14cd8",
        "tags" : [
        ]
      }
    ],
    "commit" : "31436a81f9761b26605eee99ca634e4231ac6191",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +83,87 @@      // we should skip it when calculating the `partitionStartIndices`.\n      val validMetrics = shuffleMetrics.filter(_ != null)\n      // We may have different pre-shuffle partition numbers, don't reduce shuffle partition number\n      // in that case. For example when we union fully aggregated data (data is arranged to a single\n      // partition) and a result of a SortMergeJoin (multiple partitions)."
  },
  {
    "id" : "df6f7fdb-ce8f-46f1-87c2-a78b6cc7ea68",
    "prId" : 25479,
    "prUrl" : "https://github.com/apache/spark/pull/25479#pullrequestreview-276389434",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6a044238-5dfe-4982-aad8-ec14f6cb8498",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "After we have this condition `distinctNumPreShufflePartitions.length == 1`, do we still need the assert at L136? Shall we remove the assert?",
        "createdAt" : "2019-08-16T20:41:55Z",
        "updatedAt" : "2019-08-16T21:56:27Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "73d7f8b3-fe56-4b6b-aef1-2e89a115e4b3",
        "parentId" : "6a044238-5dfe-4982-aad8-ec14f6cb8498",
        "authorId" : "3d4870da-39a8-4406-b00d-131930d14cd8",
        "body" : "Yes, we could remove it, but the assert has been there since the original version of `ReduceNumShufflePartitions` where the `distinctNumPreShufflePartitions.length == 1` check was also included. I'm not sure what is the plan with `ReduceNumShufflePartitions`. @carsonwang, @maryannxue do you want to improve `Union`/`SinglePartition` handling in this rule? Shall we remove the assert?",
        "createdAt" : "2019-08-17T09:03:12Z",
        "updatedAt" : "2019-08-17T09:04:27Z",
        "lastEditedBy" : "3d4870da-39a8-4406-b00d-131930d14cd8",
        "tags" : [
        ]
      },
      {
        "id" : "e9d3219b-b596-44b3-847e-05239a06a6f0",
        "parentId" : "6a044238-5dfe-4982-aad8-ec14f6cb8498",
        "authorId" : "863271f8-065a-4cc6-a85e-e2ed034131f6",
        "body" : "I think it is fine to remove it. We can improve the handling of `Union/SinglePartition` in future and it probably needs more changes and a new function to estimate the partition start indices. ",
        "createdAt" : "2019-08-19T07:48:41Z",
        "updatedAt" : "2019-08-19T07:48:41Z",
        "lastEditedBy" : "863271f8-065a-4cc6-a85e-e2ed034131f6",
        "tags" : [
        ]
      }
    ],
    "commit" : "31436a81f9761b26605eee99ca634e4231ac6191",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +88,92 @@      val distinctNumPreShufflePartitions =\n        validMetrics.map(stats => stats.bytesByPartitionId.length).distinct\n      if (validMetrics.nonEmpty && distinctNumPreShufflePartitions.length == 1) {\n        val partitionStartIndices = estimatePartitionStartIndices(validMetrics.toArray)\n        // This transformation adds new nodes, so we must use `transformUp` here."
  },
  {
    "id" : "d2f9c50f-039b-41e5-9163-718b8104c570",
    "prId" : 25121,
    "prUrl" : "https://github.com/apache/spark/pull/25121#pullrequestreview-275998965",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "595ba988-359d-4b75-8d1a-3dae10861e06",
        "parentId" : null,
        "authorId" : "3d4870da-39a8-4406-b00d-131930d14cd8",
        "body" : "@carsonwang is it safe to remove `distinctNumPreShufflePartitions.length == 1` from here? I think @hvanhovell's comment (https://github.com/apache/spark/pull/24978/files#r299396944) still applies here about `Union`. I run into an issue with my plan:\r\n```\r\nUnion\r\n:- Project [id_key#236, true AS row_type#249, link#232]\r\n:  +- Filter (isnotnull(min(id_key) OVER (ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)#246) AND (id_key#236 = min(id_key) OVER (ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)#246))\r\n:     +- Window [min(id_key#236) windowspecdefinition(specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS min(id_key) OVER (ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)#246]\r\n:        +- ShuffleQueryStage 5\r\n:           +- Exchange SinglePartition, true\r\n:              +- *(7) Project [id_key#236, link#232]\r\n:                 +- *(7) SortMergeJoin [link#237], [link#232], Inner, (id_key#236 > id_key#230)\r\n:                    :- *(5) Sort [link#237 ASC NULLS FIRST], false, 0\r\n:                    :  +- CoalescedShuffleReader [0]\r\n:                    :     +- ShuffleQueryStage 0\r\n:                    :        +- Exchange hashpartitioning(link#237, 5), true\r\n:                    :           +- *(1) Project [col1#224 AS id_key#236, col2#225 AS link#237]\r\n:                    :              +- *(1) LocalTableScan [col1#224, col2#225]\r\n:                    +- *(6) Sort [link#232 ASC NULLS FIRST], false, 0\r\n:                       +- CoalescedShuffleReader [0]\r\n:                          +- ShuffleQueryStage 1\r\n:                             +- Exchange hashpartitioning(link#232, 5), true\r\n:                                +- *(2) Project [id_key#230, link#232]\r\n:                                   +- *(2) Filter (isnotnull(link#232) AND isnotnull(id_key#230))\r\n:                                      +- *(2) Scan RecursiveReference iter[id_key#230,row_type#231,link#232]\r\n+- Project [id_key#240, new AS new#256, link#241]\r\n   +- SortMergeJoin [id_key#238], [id_key#240], Inner\r\n      :- Sort [id_key#238 ASC NULLS FIRST], false, 0\r\n      :  +- ShuffleQueryStage 4\r\n      :     +- Exchange hashpartitioning(id_key#238, 5), true\r\n      :        +- *(4) Project [id_key#238]\r\n      :           +- *(4) Filter (isnotnull(min(id_key) OVER (ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)#247) AND (id_key#238 = min(id_key) OVER (ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)#247))\r\n      :              +- Window [min(id_key#238) windowspecdefinition(specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS min(id_key) OVER (ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)#247]\r\n      :                 +- CoalescedShuffleReader [0]\r\n      :                    +- ShuffleQueryStage 2\r\n      :                       +- Exchange SinglePartition, true\r\n      :                          +- LocalTableScan <empty>, [id_key#238]\r\n      +- Sort [id_key#240 ASC NULLS FIRST], false, 0\r\n         +- ShuffleQueryStage 3\r\n            +- Exchange hashpartitioning(id_key#240, 5), true\r\n               +- *(3) Project [col1#228 AS id_key#240, col2#229 AS link#241]\r\n                  +- *(3) LocalTableScan [col1#228, col2#229]\r\n```\r\nwhere `ShuffleQueryStage 5` conflicts with `ShuffleQueryStage 4` and `ShuffleQueryStage 3`.",
        "createdAt" : "2019-08-13T14:58:27Z",
        "updatedAt" : "2019-08-13T14:58:37Z",
        "lastEditedBy" : "3d4870da-39a8-4406-b00d-131930d14cd8",
        "tags" : [
        ]
      },
      {
        "id" : "fb6623e0-dbae-4807-a013-1e582473f4eb",
        "parentId" : "595ba988-359d-4b75-8d1a-3dae10861e06",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "ah `SinglePartition` is an exception. So it's still possible to hit `distinctNumPreShufflePartitions.length > 1` here. Let's add back this check @carsonwang @maryannxue \r\n\r\nthanks for reporting it!",
        "createdAt" : "2019-08-14T03:22:19Z",
        "updatedAt" : "2019-08-14T03:22:19Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "ee67bb51-0bee-44c5-aabd-4391fc065932",
        "parentId" : "595ba988-359d-4b75-8d1a-3dae10861e06",
        "authorId" : "3d4870da-39a8-4406-b00d-131930d14cd8",
        "body" : "I've opened a small PR here: https://github.com/apache/spark/pull/25479 as a follow-up, please let me know if this requires a new ticket.",
        "createdAt" : "2019-08-16T14:57:35Z",
        "updatedAt" : "2019-08-16T14:57:35Z",
        "lastEditedBy" : "3d4870da-39a8-4406-b00d-131930d14cd8",
        "tags" : [
        ]
      }
    ],
    "commit" : "ba1bda815fe4219961222ed9c872f2810cfab1d5",
    "line" : 33,
    "diffHunk" : "@@ -1,1 +83,87 @@      // we should skip it when calculating the `partitionStartIndices`.\n      val validMetrics = shuffleMetrics.filter(_ != null)\n      if (validMetrics.nonEmpty) {\n        val partitionStartIndices = estimatePartitionStartIndices(validMetrics.toArray)\n        // This transformation adds new nodes, so we must use `transformUp` here."
  }
]