[
  {
    "id" : "94096a68-5794-45e0-bc39-0086c273825c",
    "prId" : 27742,
    "prUrl" : "https://github.com/apache/spark/pull/27742#pullrequestreview-366963216",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "203fa23a-d288-4289-be51-5714bdb80571",
        "parentId" : null,
        "authorId" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "body" : "@cloud-fan Is that we should skip this optimization at all if there's a skew join instead of skip counting the stages as in https://github.com/apache/spark/pull/27742/files#diff-e23b4656e59b73d313271d62329eefc2L47-L48 ?",
        "createdAt" : "2020-02-29T03:38:04Z",
        "updatedAt" : "2020-03-02T15:02:56Z",
        "lastEditedBy" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "tags" : [
        ]
      },
      {
        "id" : "5c8533ca-c9bb-47b4-8a39-cb2fd6399a5e",
        "parentId" : "203fa23a-d288-4289-be51-5714bdb80571",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "It was written this way as it needs to coalesce the shuffles partitions for non-skew partitions. But it's not a problem now.",
        "createdAt" : "2020-03-02T05:54:15Z",
        "updatedAt" : "2020-03-02T15:02:56Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "25327df2a1474385336aae243fc237372d137051",
    "line" : 26,
    "diffHunk" : "@@ -1,1 +35,39 @@    }\n    if (!plan.collectLeaves().forall(_.isInstanceOf[QueryStageExec])\n        || plan.find(_.isInstanceOf[CustomShuffleReaderExec]).isDefined) {\n      // If not all leaf nodes are query stages, it's not safe to reduce the number of\n      // shuffle partitions, because we may break the assumption that all children of a spark plan"
  },
  {
    "id" : "fed8eea6-e2cc-4b95-aa60-2aa875b74ec6",
    "prId" : 26434,
    "prUrl" : "https://github.com/apache/spark/pull/26434#pullrequestreview-341614559",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "33f9721e-39d3-4e6c-8538-412cce82ace6",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "why do we need to define an `i`? looking at the code seems `i` is always equal to `nextPartitionIndices`",
        "createdAt" : "2019-12-18T14:34:20Z",
        "updatedAt" : "2020-01-14T08:24:47Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "4bc13cf5-2118-445d-b2bc-da49472e7667",
        "parentId" : "33f9721e-39d3-4e6c-8538-412cce82ace6",
        "authorId" : "1b84a7ff-6bf9-4417-bf9f-e46e997e5974",
        "body" : "the parameter `i `is used to omit the excluded partition. When the partition is excluded, `i `is not equal to `nextPartitionIndices`",
        "createdAt" : "2019-12-20T08:12:13Z",
        "updatedAt" : "2020-01-14T08:24:47Z",
        "lastEditedBy" : "1b84a7ff-6bf9-4417-bf9f-e46e997e5974",
        "tags" : [
        ]
      },
      {
        "id" : "195dd9d2-9662-4d36-9021-f1c51f249af5",
        "parentId" : "33f9721e-39d3-4e6c-8538-412cce82ace6",
        "authorId" : "334a081c-cba1-403e-ac80-30e6124077d1",
        "body" : "`i` is actually the (candidate) \"partitionEndIndex\" and would be better inited to `firstStartIndex + 1` (also reset to `nextPartitionIndex + 1`). That helps me to understand the logic here.",
        "createdAt" : "2020-01-11T04:50:30Z",
        "updatedAt" : "2020-01-14T08:24:47Z",
        "lastEditedBy" : "334a081c-cba1-403e-ac80-30e6124077d1",
        "tags" : [
        ]
      },
      {
        "id" : "d9dfe812-f4bd-4fe4-9668-ad8059c058c2",
        "parentId" : "33f9721e-39d3-4e6c-8538-412cce82ace6",
        "authorId" : "1b84a7ff-6bf9-4417-bf9f-e46e997e5974",
        "body" : "Here` i` stands for the` partitionStartIndex` and not the `partitionEndIndex`.",
        "createdAt" : "2020-01-13T01:08:44Z",
        "updatedAt" : "2020-01-14T08:24:47Z",
        "lastEditedBy" : "1b84a7ff-6bf9-4417-bf9f-e46e997e5974",
        "tags" : [
        ]
      }
    ],
    "commit" : "ac17a7cb869a093745ff0d60fe9d3837ede72c85",
    "line" : 144,
    "diffHunk" : "@@ -1,1 +177,181 @@    partitionStartIndices += firstStartIndex\n    var postShuffleInputSize = mapOutputStatistics.map(_.bytesByPartitionId(firstStartIndex)).sum\n    var i = firstStartIndex\n    includedPartitions.drop(1).foreach { nextPartitionIndex =>\n        val nextShuffleInputSize ="
  }
]