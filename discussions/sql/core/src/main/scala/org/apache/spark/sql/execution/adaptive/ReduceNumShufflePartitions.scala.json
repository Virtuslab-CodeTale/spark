[
  {
    "id" : "94096a68-5794-45e0-bc39-0086c273825c",
    "prId" : 27742,
    "prUrl" : "https://github.com/apache/spark/pull/27742#pullrequestreview-366963216",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "203fa23a-d288-4289-be51-5714bdb80571",
        "parentId" : null,
        "authorId" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "body" : "@cloud-fan Is that we should skip this optimization at all if there's a skew join instead of skip counting the stages as in https://github.com/apache/spark/pull/27742/files#diff-e23b4656e59b73d313271d62329eefc2L47-L48 ?",
        "createdAt" : "2020-02-29T03:38:04Z",
        "updatedAt" : "2020-03-02T15:02:56Z",
        "lastEditedBy" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "tags" : [
        ]
      },
      {
        "id" : "5c8533ca-c9bb-47b4-8a39-cb2fd6399a5e",
        "parentId" : "203fa23a-d288-4289-be51-5714bdb80571",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "It was written this way as it needs to coalesce the shuffles partitions for non-skew partitions. But it's not a problem now.",
        "createdAt" : "2020-03-02T05:54:15Z",
        "updatedAt" : "2020-03-02T15:02:56Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "25327df2a1474385336aae243fc237372d137051",
    "line" : 26,
    "diffHunk" : "@@ -1,1 +35,39 @@    }\n    if (!plan.collectLeaves().forall(_.isInstanceOf[QueryStageExec])\n        || plan.find(_.isInstanceOf[CustomShuffleReaderExec]).isDefined) {\n      // If not all leaf nodes are query stages, it's not safe to reduce the number of\n      // shuffle partitions, because we may break the assumption that all children of a spark plan"
  },
  {
    "id" : "fed8eea6-e2cc-4b95-aa60-2aa875b74ec6",
    "prId" : 26434,
    "prUrl" : "https://github.com/apache/spark/pull/26434#pullrequestreview-341614559",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "33f9721e-39d3-4e6c-8538-412cce82ace6",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "why do we need to define an `i`? looking at the code seems `i` is always equal to `nextPartitionIndices`",
        "createdAt" : "2019-12-18T14:34:20Z",
        "updatedAt" : "2020-01-14T08:24:47Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "4bc13cf5-2118-445d-b2bc-da49472e7667",
        "parentId" : "33f9721e-39d3-4e6c-8538-412cce82ace6",
        "authorId" : "1b84a7ff-6bf9-4417-bf9f-e46e997e5974",
        "body" : "the parameter `i `is used to omit the excluded partition. When the partition is excluded, `i `is not equal to `nextPartitionIndices`",
        "createdAt" : "2019-12-20T08:12:13Z",
        "updatedAt" : "2020-01-14T08:24:47Z",
        "lastEditedBy" : "1b84a7ff-6bf9-4417-bf9f-e46e997e5974",
        "tags" : [
        ]
      },
      {
        "id" : "195dd9d2-9662-4d36-9021-f1c51f249af5",
        "parentId" : "33f9721e-39d3-4e6c-8538-412cce82ace6",
        "authorId" : "334a081c-cba1-403e-ac80-30e6124077d1",
        "body" : "`i` is actually the (candidate) \"partitionEndIndex\" and would be better inited to `firstStartIndex + 1` (also reset to `nextPartitionIndex + 1`). That helps me to understand the logic here.",
        "createdAt" : "2020-01-11T04:50:30Z",
        "updatedAt" : "2020-01-14T08:24:47Z",
        "lastEditedBy" : "334a081c-cba1-403e-ac80-30e6124077d1",
        "tags" : [
        ]
      },
      {
        "id" : "d9dfe812-f4bd-4fe4-9668-ad8059c058c2",
        "parentId" : "33f9721e-39d3-4e6c-8538-412cce82ace6",
        "authorId" : "1b84a7ff-6bf9-4417-bf9f-e46e997e5974",
        "body" : "Here` i` stands for the` partitionStartIndex` and not the `partitionEndIndex`.",
        "createdAt" : "2020-01-13T01:08:44Z",
        "updatedAt" : "2020-01-14T08:24:47Z",
        "lastEditedBy" : "1b84a7ff-6bf9-4417-bf9f-e46e997e5974",
        "tags" : [
        ]
      }
    ],
    "commit" : "ac17a7cb869a093745ff0d60fe9d3837ede72c85",
    "line" : 144,
    "diffHunk" : "@@ -1,1 +177,181 @@    partitionStartIndices += firstStartIndex\n    var postShuffleInputSize = mapOutputStatistics.map(_.bytesByPartitionId(firstStartIndex)).sum\n    var i = firstStartIndex\n    includedPartitions.drop(1).foreach { nextPartitionIndex =>\n        val nextShuffleInputSize ="
  },
  {
    "id" : "b3469a7d-cbc3-47c3-a160-1f813b02d8cf",
    "prId" : 25479,
    "prUrl" : "https://github.com/apache/spark/pull/25479#pullrequestreview-276080526",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fd198c92-df22-4d08-a2c1-6c9d48a613e9",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "let's also give an example about when we will have different pre-shuffle partition numbers.",
        "createdAt" : "2019-08-16T15:40:23Z",
        "updatedAt" : "2019-08-16T17:39:38Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "08787d11-e6df-448d-9366-c349200a4d14",
        "parentId" : "fd198c92-df22-4d08-a2c1-6c9d48a613e9",
        "authorId" : "3d4870da-39a8-4406-b00d-131930d14cd8",
        "body" : "Ok, added. Please let me know if it should be more detailed.",
        "createdAt" : "2019-08-16T17:40:48Z",
        "updatedAt" : "2019-08-16T17:40:49Z",
        "lastEditedBy" : "3d4870da-39a8-4406-b00d-131930d14cd8",
        "tags" : [
        ]
      }
    ],
    "commit" : "31436a81f9761b26605eee99ca634e4231ac6191",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +83,87 @@      // we should skip it when calculating the `partitionStartIndices`.\n      val validMetrics = shuffleMetrics.filter(_ != null)\n      // We may have different pre-shuffle partition numbers, don't reduce shuffle partition number\n      // in that case. For example when we union fully aggregated data (data is arranged to a single\n      // partition) and a result of a SortMergeJoin (multiple partitions)."
  },
  {
    "id" : "df6f7fdb-ce8f-46f1-87c2-a78b6cc7ea68",
    "prId" : 25479,
    "prUrl" : "https://github.com/apache/spark/pull/25479#pullrequestreview-276389434",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6a044238-5dfe-4982-aad8-ec14f6cb8498",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "After we have this condition `distinctNumPreShufflePartitions.length == 1`, do we still need the assert at L136? Shall we remove the assert?",
        "createdAt" : "2019-08-16T20:41:55Z",
        "updatedAt" : "2019-08-16T21:56:27Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "73d7f8b3-fe56-4b6b-aef1-2e89a115e4b3",
        "parentId" : "6a044238-5dfe-4982-aad8-ec14f6cb8498",
        "authorId" : "3d4870da-39a8-4406-b00d-131930d14cd8",
        "body" : "Yes, we could remove it, but the assert has been there since the original version of `ReduceNumShufflePartitions` where the `distinctNumPreShufflePartitions.length == 1` check was also included. I'm not sure what is the plan with `ReduceNumShufflePartitions`. @carsonwang, @maryannxue do you want to improve `Union`/`SinglePartition` handling in this rule? Shall we remove the assert?",
        "createdAt" : "2019-08-17T09:03:12Z",
        "updatedAt" : "2019-08-17T09:04:27Z",
        "lastEditedBy" : "3d4870da-39a8-4406-b00d-131930d14cd8",
        "tags" : [
        ]
      },
      {
        "id" : "e9d3219b-b596-44b3-847e-05239a06a6f0",
        "parentId" : "6a044238-5dfe-4982-aad8-ec14f6cb8498",
        "authorId" : "863271f8-065a-4cc6-a85e-e2ed034131f6",
        "body" : "I think it is fine to remove it. We can improve the handling of `Union/SinglePartition` in future and it probably needs more changes and a new function to estimate the partition start indices. ",
        "createdAt" : "2019-08-19T07:48:41Z",
        "updatedAt" : "2019-08-19T07:48:41Z",
        "lastEditedBy" : "863271f8-065a-4cc6-a85e-e2ed034131f6",
        "tags" : [
        ]
      }
    ],
    "commit" : "31436a81f9761b26605eee99ca634e4231ac6191",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +88,92 @@      val distinctNumPreShufflePartitions =\n        validMetrics.map(stats => stats.bytesByPartitionId.length).distinct\n      if (validMetrics.nonEmpty && distinctNumPreShufflePartitions.length == 1) {\n        val partitionStartIndices = estimatePartitionStartIndices(validMetrics.toArray)\n        // This transformation adds new nodes, so we must use `transformUp` here."
  },
  {
    "id" : "d2f9c50f-039b-41e5-9163-718b8104c570",
    "prId" : 25121,
    "prUrl" : "https://github.com/apache/spark/pull/25121#pullrequestreview-275998965",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "595ba988-359d-4b75-8d1a-3dae10861e06",
        "parentId" : null,
        "authorId" : "3d4870da-39a8-4406-b00d-131930d14cd8",
        "body" : "@carsonwang is it safe to remove `distinctNumPreShufflePartitions.length == 1` from here? I think @hvanhovell's comment (https://github.com/apache/spark/pull/24978/files#r299396944) still applies here about `Union`. I run into an issue with my plan:\r\n```\r\nUnion\r\n:- Project [id_key#236, true AS row_type#249, link#232]\r\n:  +- Filter (isnotnull(min(id_key) OVER (ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)#246) AND (id_key#236 = min(id_key) OVER (ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)#246))\r\n:     +- Window [min(id_key#236) windowspecdefinition(specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS min(id_key) OVER (ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)#246]\r\n:        +- ShuffleQueryStage 5\r\n:           +- Exchange SinglePartition, true\r\n:              +- *(7) Project [id_key#236, link#232]\r\n:                 +- *(7) SortMergeJoin [link#237], [link#232], Inner, (id_key#236 > id_key#230)\r\n:                    :- *(5) Sort [link#237 ASC NULLS FIRST], false, 0\r\n:                    :  +- CoalescedShuffleReader [0]\r\n:                    :     +- ShuffleQueryStage 0\r\n:                    :        +- Exchange hashpartitioning(link#237, 5), true\r\n:                    :           +- *(1) Project [col1#224 AS id_key#236, col2#225 AS link#237]\r\n:                    :              +- *(1) LocalTableScan [col1#224, col2#225]\r\n:                    +- *(6) Sort [link#232 ASC NULLS FIRST], false, 0\r\n:                       +- CoalescedShuffleReader [0]\r\n:                          +- ShuffleQueryStage 1\r\n:                             +- Exchange hashpartitioning(link#232, 5), true\r\n:                                +- *(2) Project [id_key#230, link#232]\r\n:                                   +- *(2) Filter (isnotnull(link#232) AND isnotnull(id_key#230))\r\n:                                      +- *(2) Scan RecursiveReference iter[id_key#230,row_type#231,link#232]\r\n+- Project [id_key#240, new AS new#256, link#241]\r\n   +- SortMergeJoin [id_key#238], [id_key#240], Inner\r\n      :- Sort [id_key#238 ASC NULLS FIRST], false, 0\r\n      :  +- ShuffleQueryStage 4\r\n      :     +- Exchange hashpartitioning(id_key#238, 5), true\r\n      :        +- *(4) Project [id_key#238]\r\n      :           +- *(4) Filter (isnotnull(min(id_key) OVER (ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)#247) AND (id_key#238 = min(id_key) OVER (ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)#247))\r\n      :              +- Window [min(id_key#238) windowspecdefinition(specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS min(id_key) OVER (ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)#247]\r\n      :                 +- CoalescedShuffleReader [0]\r\n      :                    +- ShuffleQueryStage 2\r\n      :                       +- Exchange SinglePartition, true\r\n      :                          +- LocalTableScan <empty>, [id_key#238]\r\n      +- Sort [id_key#240 ASC NULLS FIRST], false, 0\r\n         +- ShuffleQueryStage 3\r\n            +- Exchange hashpartitioning(id_key#240, 5), true\r\n               +- *(3) Project [col1#228 AS id_key#240, col2#229 AS link#241]\r\n                  +- *(3) LocalTableScan [col1#228, col2#229]\r\n```\r\nwhere `ShuffleQueryStage 5` conflicts with `ShuffleQueryStage 4` and `ShuffleQueryStage 3`.",
        "createdAt" : "2019-08-13T14:58:27Z",
        "updatedAt" : "2019-08-13T14:58:37Z",
        "lastEditedBy" : "3d4870da-39a8-4406-b00d-131930d14cd8",
        "tags" : [
        ]
      },
      {
        "id" : "fb6623e0-dbae-4807-a013-1e582473f4eb",
        "parentId" : "595ba988-359d-4b75-8d1a-3dae10861e06",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "ah `SinglePartition` is an exception. So it's still possible to hit `distinctNumPreShufflePartitions.length > 1` here. Let's add back this check @carsonwang @maryannxue \r\n\r\nthanks for reporting it!",
        "createdAt" : "2019-08-14T03:22:19Z",
        "updatedAt" : "2019-08-14T03:22:19Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "ee67bb51-0bee-44c5-aabd-4391fc065932",
        "parentId" : "595ba988-359d-4b75-8d1a-3dae10861e06",
        "authorId" : "3d4870da-39a8-4406-b00d-131930d14cd8",
        "body" : "I've opened a small PR here: https://github.com/apache/spark/pull/25479 as a follow-up, please let me know if this requires a new ticket.",
        "createdAt" : "2019-08-16T14:57:35Z",
        "updatedAt" : "2019-08-16T14:57:35Z",
        "lastEditedBy" : "3d4870da-39a8-4406-b00d-131930d14cd8",
        "tags" : [
        ]
      }
    ],
    "commit" : "ba1bda815fe4219961222ed9c872f2810cfab1d5",
    "line" : 33,
    "diffHunk" : "@@ -1,1 +83,87 @@      // we should skip it when calculating the `partitionStartIndices`.\n      val validMetrics = shuffleMetrics.filter(_ != null)\n      if (validMetrics.nonEmpty) {\n        val partitionStartIndices = estimatePartitionStartIndices(validMetrics.toArray)\n        // This transformation adds new nodes, so we must use `transformUp` here."
  },
  {
    "id" : "5b3aca29-6460-42f9-8051-036b02e6f9b1",
    "prId" : 24978,
    "prUrl" : "https://github.com/apache/spark/pull/24978#pullrequestreview-256009663",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e14e6137-e2bd-4d81-88d8-d95581db2c8e",
        "parentId" : null,
        "authorId" : "80d631a6-73e8-46a6-a01b-a80d1f1cc6cc",
        "body" : "How about first collecting all the leaves, then checking if they are all `QueryStageExec` nodes, and if they are collect the map stats outputs? That seems more efficient.",
        "createdAt" : "2019-06-27T08:00:23Z",
        "updatedAt" : "2019-07-04T00:36:36Z",
        "lastEditedBy" : "80d631a6-73e8-46a6-a01b-a80d1f1cc6cc",
        "tags" : [
        ]
      },
      {
        "id" : "037c3774-d2c5-492e-ba87-6603a49c1e61",
        "parentId" : "e14e6137-e2bd-4d81-88d8-d95581db2c8e",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "+1",
        "createdAt" : "2019-06-27T08:16:02Z",
        "updatedAt" : "2019-07-04T00:36:36Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "91b92379-5fce-4ff4-b104-cecc1320e8a9",
        "parentId" : "e14e6137-e2bd-4d81-88d8-d95581db2c8e",
        "authorId" : "863271f8-065a-4cc6-a85e-e2ed034131f6",
        "body" : "Good point!",
        "createdAt" : "2019-06-28T23:43:36Z",
        "updatedAt" : "2019-07-04T00:36:36Z",
        "lastEditedBy" : "863271f8-065a-4cc6-a85e-e2ed034131f6",
        "tags" : [
        ]
      }
    ],
    "commit" : "4b001ca824ea53716510b563105a092cce1d46a6",
    "line" : 60,
    "diffHunk" : "@@ -1,1 +58,62 @@      return plan\n    }\n    if (!plan.collectLeaves().forall(_.isInstanceOf[QueryStageExec])) {\n      // If not all leaf nodes are query stages, it's not safe to reduce the number of\n      // shuffle partitions, because we may break the assumption that all children of a spark plan"
  },
  {
    "id" : "98590856-566c-43ff-b19f-b1c5e10b8a45",
    "prId" : 24978,
    "prUrl" : "https://github.com/apache/spark/pull/24978#pullrequestreview-256010572",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7fba14c3-8287-43c4-ad81-8866beab40e6",
        "parentId" : null,
        "authorId" : "1b84a7ff-6bf9-4417-bf9f-e46e997e5974",
        "body" : "When the leaf node is `BroadcastQueryStageExec`, Do we need to adjust the reduce number ?",
        "createdAt" : "2019-06-27T08:07:48Z",
        "updatedAt" : "2019-07-04T00:36:36Z",
        "lastEditedBy" : "1b84a7ff-6bf9-4417-bf9f-e46e997e5974",
        "tags" : [
        ]
      },
      {
        "id" : "88c97c85-f798-4440-bb57-992780418a64",
        "parentId" : "7fba14c3-8287-43c4-ad81-8866beab40e6",
        "authorId" : "80d631a6-73e8-46a6-a01b-a80d1f1cc6cc",
        "body" : "We are ignoring `BroadcastQueryStageExec` right?",
        "createdAt" : "2019-06-27T13:16:33Z",
        "updatedAt" : "2019-07-04T00:36:36Z",
        "lastEditedBy" : "80d631a6-73e8-46a6-a01b-a80d1f1cc6cc",
        "tags" : [
        ]
      },
      {
        "id" : "b7080b3d-5b40-4acb-a732-cb0207b81c54",
        "parentId" : "7fba14c3-8287-43c4-ad81-8866beab40e6",
        "authorId" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "body" : "We can have situations with BHJ where:\r\n1. The BHJ is from compile-time and one side is a broadcast stage;\r\n2. The BHJ is from a former AQE optimization, then one side is shuffle stage and the other is a broadcast stage.\r\n\r\nI believe scenario 2 may still be applicable here, so maybe we can change the condition to \" isAllQueryStage && shuffleStageCount > 0\"?",
        "createdAt" : "2019-06-27T21:57:02Z",
        "updatedAt" : "2019-07-04T00:36:36Z",
        "lastEditedBy" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "tags" : [
        ]
      },
      {
        "id" : "b13780db-4c3f-4c42-9c2f-0e9cc53146ef",
        "parentId" : "7fba14c3-8287-43c4-ad81-8866beab40e6",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Obviously we don't want to adjust num shuffle partitions if there is no shuffle in this stage, so +1 to change the condition to `isAllQueryStage && shuffleStageCount > 0`.",
        "createdAt" : "2019-06-28T02:18:49Z",
        "updatedAt" : "2019-07-04T00:36:36Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "b1378282-5f55-4252-a07d-9eb248eb5ea5",
        "parentId" : "7fba14c3-8287-43c4-ad81-8866beab40e6",
        "authorId" : "863271f8-065a-4cc6-a85e-e2ed034131f6",
        "body" : "I believe we are safe here. After checking `isAllQueryStage`, we will get shuffle metrics from shuffle stages and we will only adjust num shuffle partitions if num shuffle metrics > 0.",
        "createdAt" : "2019-06-28T23:50:43Z",
        "updatedAt" : "2019-07-04T00:36:36Z",
        "lastEditedBy" : "863271f8-065a-4cc6-a85e-e2ed034131f6",
        "tags" : [
        ]
      }
    ],
    "commit" : "4b001ca824ea53716510b563105a092cce1d46a6",
    "line" : 60,
    "diffHunk" : "@@ -1,1 +58,62 @@      return plan\n    }\n    if (!plan.collectLeaves().forall(_.isInstanceOf[QueryStageExec])) {\n      // If not all leaf nodes are query stages, it's not safe to reduce the number of\n      // shuffle partitions, because we may break the assumption that all children of a spark plan"
  },
  {
    "id" : "2789b15b-cfb9-459e-96af-3e51b25efd7f",
    "prId" : 24978,
    "prUrl" : "https://github.com/apache/spark/pull/24978#pullrequestreview-256046512",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "17ed4e62-d005-4e85-8a11-a11a65f1a220",
        "parentId" : null,
        "authorId" : "1b84a7ff-6bf9-4417-bf9f-e46e997e5974",
        "body" : "We may need add a check of whether all the pre-shuffle partitions is same before calling the `estimatePartitionStartIndices` method to avoid the assert exception in Line 130.",
        "createdAt" : "2019-06-27T08:12:59Z",
        "updatedAt" : "2019-07-04T00:36:36Z",
        "lastEditedBy" : "1b84a7ff-6bf9-4417-bf9f-e46e997e5974",
        "tags" : [
        ]
      },
      {
        "id" : "aa85423e-90b0-4283-b214-89000d8fd8cb",
        "parentId" : "17ed4e62-d005-4e85-8a11-a11a65f1a220",
        "authorId" : "863271f8-065a-4cc6-a85e-e2ed034131f6",
        "body" : "Good catch. I'll add the check and the unit test. ",
        "createdAt" : "2019-06-29T18:29:33Z",
        "updatedAt" : "2019-07-04T00:36:36Z",
        "lastEditedBy" : "863271f8-065a-4cc6-a85e-e2ed034131f6",
        "tags" : [
        ]
      }
    ],
    "commit" : "4b001ca824ea53716510b563105a092cce1d46a6",
    "line" : 85,
    "diffHunk" : "@@ -1,1 +83,87 @@\n      if (validMetrics.nonEmpty && distinctNumPreShufflePartitions.length == 1) {\n        val partitionStartIndices = estimatePartitionStartIndices(validMetrics.toArray)\n        // This transformation adds new nodes, so we must use `transformUp` here.\n        plan.transformUp {"
  },
  {
    "id" : "19b6343f-9135-4d5c-a870-4f43610ea6c8",
    "prId" : 24978,
    "prUrl" : "https://github.com/apache/spark/pull/24978#pullrequestreview-257149861",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1fc66c62-98f6-499f-9dc7-0bed341c81a7",
        "parentId" : null,
        "authorId" : "34f85564-3e6b-4cb5-99f3-d5e383ee52fa",
        "body" : "Does this mean the case like https://github.com/Intel-bigdata/spark-adaptive/pull/54 ? \r\n:nit Maybe we can say `If not all leaf nodes are query stages, for example when some depending stage is already bucketed, there isn't a shuffle or query stage for that branch` to be clear because normally all leaves are QS.",
        "createdAt" : "2019-07-02T03:33:15Z",
        "updatedAt" : "2019-07-04T00:36:36Z",
        "lastEditedBy" : "34f85564-3e6b-4cb5-99f3-d5e383ee52fa",
        "tags" : [
        ]
      },
      {
        "id" : "d190afe5-50c0-4026-86d0-41458f8328bc",
        "parentId" : "1fc66c62-98f6-499f-9dc7-0bed341c81a7",
        "authorId" : "863271f8-065a-4cc6-a85e-e2ed034131f6",
        "body" : "Yes, that case is covered. There are many cases the leaves are not query stage, for example in the stage that does a table scan, the leaf is a table scan operator. The comment seems to cover these.",
        "createdAt" : "2019-07-02T21:06:32Z",
        "updatedAt" : "2019-07-04T00:36:36Z",
        "lastEditedBy" : "863271f8-065a-4cc6-a85e-e2ed034131f6",
        "tags" : [
        ]
      }
    ],
    "commit" : "4b001ca824ea53716510b563105a092cce1d46a6",
    "line" : 61,
    "diffHunk" : "@@ -1,1 +59,63 @@    }\n    if (!plan.collectLeaves().forall(_.isInstanceOf[QueryStageExec])) {\n      // If not all leaf nodes are query stages, it's not safe to reduce the number of\n      // shuffle partitions, because we may break the assumption that all children of a spark plan\n      // have same number of output partitions."
  },
  {
    "id" : "e8d2a4a1-5189-47ef-a3cb-6fde49f9a2a8",
    "prId" : 24978,
    "prUrl" : "https://github.com/apache/spark/pull/24978#pullrequestreview-257393831",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "921b5062-acac-4e0a-a2a6-29255594abd0",
        "parentId" : null,
        "authorId" : "34f85564-3e6b-4cb5-99f3-d5e383ee52fa",
        "body" : "If a user calls `val datasetAfter = datasetBefore.repartition(300)`, then he wants the post-shuffle partitionNum to be 300 exactly. But in this case we will go inside this branch, and then the partition merging on reduce side will break the user's expectation? cc @cloud-fan \r\nShould we check the original operation, if it's `repartition` we don't do Adaptive Execution then?",
        "createdAt" : "2019-07-02T03:48:19Z",
        "updatedAt" : "2019-07-04T00:36:36Z",
        "lastEditedBy" : "34f85564-3e6b-4cb5-99f3-d5e383ee52fa",
        "tags" : [
        ]
      },
      {
        "id" : "2de727d5-53ff-48bd-a746-045f3fd7604b",
        "parentId" : "921b5062-acac-4e0a-a2a6-29255594abd0",
        "authorId" : "863271f8-065a-4cc6-a85e-e2ed034131f6",
        "body" : "This was also discussed in the original PR. For df.repartition(500), AE does use the specified number for repartition. That is, each stage will writes the map output with 500 partitions. However, in the following up stage, AE may launch tasks less than 500 as one task can process multiple continues blocks. I think we can still do adaptive execution when it is possible. We can add a option to disable AE for repartition later if that is necessary. ",
        "createdAt" : "2019-07-02T21:21:35Z",
        "updatedAt" : "2019-07-04T00:36:36Z",
        "lastEditedBy" : "863271f8-065a-4cc6-a85e-e2ed034131f6",
        "tags" : [
        ]
      },
      {
        "id" : "fca820c0-2792-4eff-be10-4e4d6674e0d9",
        "parentId" : "921b5062-acac-4e0a-a2a6-29255594abd0",
        "authorId" : "34f85564-3e6b-4cb5-99f3-d5e383ee52fa",
        "body" : "It's pretty good to add a config entry, I think it's the best to make it internal(hidden). At the same time, we can add a function called `repartitonWithAdvice`, the different between  it and `repartition` is whether the newly added config entry is set to true. \r\n\r\n`RepartitionWithAdvice` means enabling AdaptiveExecution, to be different from the accurate `repartition`.",
        "createdAt" : "2019-07-03T08:41:39Z",
        "updatedAt" : "2019-07-04T00:36:36Z",
        "lastEditedBy" : "34f85564-3e6b-4cb5-99f3-d5e383ee52fa",
        "tags" : [
        ]
      },
      {
        "id" : "17fd1ec6-5dcd-474c-a197-843233aa839c",
        "parentId" : "921b5062-acac-4e0a-a2a6-29255594abd0",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Do we have this problem in `ExchangeCoordinator`? At physical phase we have no idea if the shuffle comes from a `df.repartition` or is added by `EnsureRequirements`. We may need to add a boolean flag in `ShuffleExchangeExec` to indicate that this shuffle can't change its num partitions. Maybe we can fix it in a followup.",
        "createdAt" : "2019-07-03T10:19:30Z",
        "updatedAt" : "2019-07-04T00:36:36Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "4b001ca824ea53716510b563105a092cce1d46a6",
    "line" : 84,
    "diffHunk" : "@@ -1,1 +82,86 @@        validMetrics.map(stats => stats.bytesByPartitionId.length).distinct\n\n      if (validMetrics.nonEmpty && distinctNumPreShufflePartitions.length == 1) {\n        val partitionStartIndices = estimatePartitionStartIndices(validMetrics.toArray)\n        // This transformation adds new nodes, so we must use `transformUp` here."
  },
  {
    "id" : "66a64bb9-51eb-4c54-a145-cfd5869284b3",
    "prId" : 24978,
    "prUrl" : "https://github.com/apache/spark/pull/24978#pullrequestreview-257332440",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8176bfb7-ce8f-4cd0-b425-32a6cc7daa93",
        "parentId" : null,
        "authorId" : "34f85564-3e6b-4cb5-99f3-d5e383ee52fa",
        "body" : "There are 2 conditions we don't do Adaptive Execution, so `plan` is directly returned.\r\nMaybe better to aggregate these conditions?",
        "createdAt" : "2019-07-02T03:51:10Z",
        "updatedAt" : "2019-07-04T00:36:36Z",
        "lastEditedBy" : "34f85564-3e6b-4cb5-99f3-d5e383ee52fa",
        "tags" : [
        ]
      },
      {
        "id" : "e427b181-c384-4711-b082-75cafe2f3ffa",
        "parentId" : "8176bfb7-ce8f-4cd0-b425-32a6cc7daa93",
        "authorId" : "863271f8-065a-4cc6-a85e-e2ed034131f6",
        "body" : "What do you mean by aggregate ?",
        "createdAt" : "2019-07-02T21:22:24Z",
        "updatedAt" : "2019-07-04T00:36:36Z",
        "lastEditedBy" : "863271f8-065a-4cc6-a85e-e2ed034131f6",
        "tags" : [
        ]
      },
      {
        "id" : "bad7aa2a-a446-4f0c-a229-70752cd31a6a",
        "parentId" : "8176bfb7-ce8f-4cd0-b425-32a6cc7daa93",
        "authorId" : "34f85564-3e6b-4cb5-99f3-d5e383ee52fa",
        "body" : "Never mind~",
        "createdAt" : "2019-07-03T08:27:58Z",
        "updatedAt" : "2019-07-04T00:36:36Z",
        "lastEditedBy" : "34f85564-3e6b-4cb5-99f3-d5e383ee52fa",
        "tags" : [
        ]
      }
    ],
    "commit" : "4b001ca824ea53716510b563105a092cce1d46a6",
    "line" : 82,
    "diffHunk" : "@@ -1,1 +80,84 @@      // We don't reduce shuffle partition number in that case.\n      val distinctNumPreShufflePartitions =\n        validMetrics.map(stats => stats.bytesByPartitionId.length).distinct\n\n      if (validMetrics.nonEmpty && distinctNumPreShufflePartitions.length == 1) {"
  },
  {
    "id" : "5d5e52f6-9825-4ae4-90d5-0614e827104d",
    "prId" : 24978,
    "prUrl" : "https://github.com/apache/spark/pull/24978#pullrequestreview-257270641",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a7cc3d6e-6830-4f02-81c8-740ce7d36868",
        "parentId" : null,
        "authorId" : "80d631a6-73e8-46a6-a01b-a80d1f1cc6cc",
        "body" : "This does not work well with Union right? Is there a way we can identify subtrees in the stage where all the input partitions are the same?",
        "createdAt" : "2019-07-02T09:46:13Z",
        "updatedAt" : "2019-07-04T00:36:36Z",
        "lastEditedBy" : "80d631a6-73e8-46a6-a01b-a80d1f1cc6cc",
        "tags" : [
        ]
      },
      {
        "id" : "c457423d-4bec-4ac9-980c-1d08446c66ad",
        "parentId" : "a7cc3d6e-6830-4f02-81c8-740ce7d36868",
        "authorId" : "863271f8-065a-4cc6-a85e-e2ed034131f6",
        "body" : "Right, Union may have child stages with different partition number, see the unit test added. Currently we check the shuffle metrics to ensure the pre-shuffle partition number of the stages are the same. ",
        "createdAt" : "2019-07-02T21:30:25Z",
        "updatedAt" : "2019-07-04T00:36:36Z",
        "lastEditedBy" : "863271f8-065a-4cc6-a85e-e2ed034131f6",
        "tags" : [
        ]
      },
      {
        "id" : "22ef79ea-8a75-4e46-8b2b-c1abdd146dcf",
        "parentId" : "a7cc3d6e-6830-4f02-81c8-740ce7d36868",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "This can get pretty complicated if we have multiple Union and Join in one stage. For now I'm OK with this simple approach that we always require all shuffles in one stage have the same number of partitions. BTW this is an existing issue.",
        "createdAt" : "2019-07-03T05:49:02Z",
        "updatedAt" : "2019-07-04T00:36:36Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "4b001ca824ea53716510b563105a092cce1d46a6",
    "line" : 66,
    "diffHunk" : "@@ -1,1 +64,68 @@      plan\n    } else {\n      val shuffleStages = plan.collect {\n        case stage: ShuffleQueryStageExec => stage\n        case ReusedQueryStageExec(_, stage: ShuffleQueryStageExec, _) => stage"
  },
  {
    "id" : "4cbbbf5d-f818-4fe0-98dc-32ca1a0263e6",
    "prId" : 24978,
    "prUrl" : "https://github.com/apache/spark/pull/24978#pullrequestreview-257161263",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "792944ee-fe53-435f-a0fa-33dc07feee53",
        "parentId" : null,
        "authorId" : "31afe32d-3af0-4fcf-93e2-115f5d7bab18",
        "body" : "`return`  is  not needed.",
        "createdAt" : "2019-07-02T11:24:13Z",
        "updatedAt" : "2019-07-04T00:36:36Z",
        "lastEditedBy" : "31afe32d-3af0-4fcf-93e2-115f5d7bab18",
        "tags" : [
        ]
      },
      {
        "id" : "8b3c98a0-b022-4ec2-be3b-be0606b7d728",
        "parentId" : "792944ee-fe53-435f-a0fa-33dc07feee53",
        "authorId" : "863271f8-065a-4cc6-a85e-e2ed034131f6",
        "body" : "Without return, I'll need a else with more indentation. I saw a few rules write like this, so I was just copying that. ",
        "createdAt" : "2019-07-02T21:32:23Z",
        "updatedAt" : "2019-07-04T00:36:36Z",
        "lastEditedBy" : "863271f8-065a-4cc6-a85e-e2ed034131f6",
        "tags" : [
        ]
      }
    ],
    "commit" : "4b001ca824ea53716510b563105a092cce1d46a6",
    "line" : 58,
    "diffHunk" : "@@ -1,1 +56,60 @@  override def apply(plan: SparkPlan): SparkPlan = {\n    if (!conf.reducePostShufflePartitionsEnabled) {\n      return plan\n    }\n    if (!plan.collectLeaves().forall(_.isInstanceOf[QueryStageExec])) {"
  },
  {
    "id" : "fb74a1f6-915b-4dec-9115-3a525e160810",
    "prId" : 24978,
    "prUrl" : "https://github.com/apache/spark/pull/24978#pullrequestreview-257166912",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cede814c-76f7-4fee-8b88-7f423d558f3a",
        "parentId" : null,
        "authorId" : "31afe32d-3af0-4fcf-93e2-115f5d7bab18",
        "body" : "\r\n```\r\n    var nextShuffleInputSize = 0L\r\n      var j = 0\r\n      while (j < mapOutputStatistics.length) {\r\n        nextShuffleInputSize += mapOutputStatistics(j).bytesByPartitionId(i)\r\n        j += 1\r\n      }\r\n```\r\n=>\r\n`val nextShuffleInputSize = mapOutputStatistics.map(_.bytesByPartitionId(i)).sum`",
        "createdAt" : "2019-07-02T11:35:24Z",
        "updatedAt" : "2019-07-04T00:36:36Z",
        "lastEditedBy" : "31afe32d-3af0-4fcf-93e2-115f5d7bab18",
        "tags" : [
        ]
      },
      {
        "id" : "3af0d8d2-1bee-4400-88ce-c497f92690f1",
        "parentId" : "cede814c-76f7-4fee-8b88-7f423d558f3a",
        "authorId" : "863271f8-065a-4cc6-a85e-e2ed034131f6",
        "body" : "This code was original there in `ExchangeCoordinator`. Not sure if it is in purpose to avoid creating many additional arrays when we write as you proposed. So let's leave it as is.",
        "createdAt" : "2019-07-02T21:46:16Z",
        "updatedAt" : "2019-07-04T00:36:36Z",
        "lastEditedBy" : "863271f8-065a-4cc6-a85e-e2ed034131f6",
        "tags" : [
        ]
      }
    ],
    "commit" : "4b001ca824ea53716510b563105a092cce1d46a6",
    "line" : 157,
    "diffHunk" : "@@ -1,1 +155,159 @@        j += 1\n      }\n\n      // If including the nextShuffleInputSize would exceed the target partition size, then start a\n      // new partition."
  }
]