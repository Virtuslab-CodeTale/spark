[
  {
    "id" : "94096a68-5794-45e0-bc39-0086c273825c",
    "prId" : 27742,
    "prUrl" : "https://github.com/apache/spark/pull/27742#pullrequestreview-366963216",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "203fa23a-d288-4289-be51-5714bdb80571",
        "parentId" : null,
        "authorId" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "body" : "@cloud-fan Is that we should skip this optimization at all if there's a skew join instead of skip counting the stages as in https://github.com/apache/spark/pull/27742/files#diff-e23b4656e59b73d313271d62329eefc2L47-L48 ?",
        "createdAt" : "2020-02-29T03:38:04Z",
        "updatedAt" : "2020-03-02T15:02:56Z",
        "lastEditedBy" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "tags" : [
        ]
      },
      {
        "id" : "5c8533ca-c9bb-47b4-8a39-cb2fd6399a5e",
        "parentId" : "203fa23a-d288-4289-be51-5714bdb80571",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "It was written this way as it needs to coalesce the shuffles partitions for non-skew partitions. But it's not a problem now.",
        "createdAt" : "2020-03-02T05:54:15Z",
        "updatedAt" : "2020-03-02T15:02:56Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "25327df2a1474385336aae243fc237372d137051",
    "line" : 26,
    "diffHunk" : "@@ -1,1 +35,39 @@    }\n    if (!plan.collectLeaves().forall(_.isInstanceOf[QueryStageExec])\n        || plan.find(_.isInstanceOf[CustomShuffleReaderExec]).isDefined) {\n      // If not all leaf nodes are query stages, it's not safe to reduce the number of\n      // shuffle partitions, because we may break the assumption that all children of a spark plan"
  }
]