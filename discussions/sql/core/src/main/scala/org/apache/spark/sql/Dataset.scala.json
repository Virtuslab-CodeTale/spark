[
  {
    "id" : "52ab4da5-f523-4bad-af51-58a62be89dd8",
    "prId" : 33422,
    "prUrl" : "https://github.com/apache/spark/pull/33422#pullrequestreview-711175439",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f05dcd96-fecd-41e9-8bcb-c265662bfb28",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "if it's simply adding an annotation `@varargs`, shall we just do it in this PR?",
        "createdAt" : "2021-07-20T13:03:39Z",
        "updatedAt" : "2021-07-20T13:03:39Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "eddad481-aaca-48a0-9ba6-c7472955447c",
        "parentId" : "f05dcd96-fecd-41e9-8bcb-c265662bfb28",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "oh yeah!",
        "createdAt" : "2021-07-20T13:16:47Z",
        "updatedAt" : "2021-07-20T13:16:47Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "7f3ddf1d-74bc-49a9-93dc-f9dc40cc671f",
        "parentId" : "f05dcd96-fecd-41e9-8bcb-c265662bfb28",
        "authorId" : "c4ecf4e4-1fda-4fed-9980-aed6c3b3e8e4",
        "body" : "what about `observe(String, Column, Column*)`?",
        "createdAt" : "2021-07-20T18:38:39Z",
        "updatedAt" : "2021-07-20T18:38:39Z",
        "lastEditedBy" : "c4ecf4e4-1fda-4fed-9980-aed6c3b3e8e4",
        "tags" : [
        ]
      },
      {
        "id" : "d1f55634-356f-4ede-8bd5-21d3ebad3455",
        "parentId" : "f05dcd96-fecd-41e9-8bcb-c265662bfb28",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "That's fine. Let's don't add it for now",
        "createdAt" : "2021-07-21T00:59:04Z",
        "updatedAt" : "2021-07-21T00:59:05Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "3b2d9ae8800b639e91cc53a44d662c7f72e0157b",
    "line" : 34,
    "diffHunk" : "@@ -1,1 +1971,1975 @@   */\n  @varargs\n  def observe(observation: Observation, expr: Column, exprs: Column*): Dataset[T] = {\n    observation.on(this, expr, exprs: _*)\n  }"
  },
  {
    "id" : "b6bd7010-4131-4e0e-91f8-8a2c7a45f66a",
    "prId" : 32963,
    "prUrl" : "https://github.com/apache/spark/pull/32963#pullrequestreview-689002669",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "349bf8c0-d53a-414a-ad73-f714f7ae8002",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Could you add a test coverage for this, @beliefer  ?",
        "createdAt" : "2021-06-19T17:41:58Z",
        "updatedAt" : "2021-06-19T17:41:58Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "417668fa-0d41-4131-9029-a50804f04c47",
        "parentId" : "349bf8c0-d53a-414a-ad73-f714f7ae8002",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "+1, this API has no test at all, we can add one in `DataFrameSuite`",
        "createdAt" : "2021-06-21T03:37:25Z",
        "updatedAt" : "2021-06-21T03:37:32Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "b8c1d7af-74e9-4840-bc3b-689783e5eb7d",
        "parentId" : "349bf8c0-d53a-414a-ad73-f714f7ae8002",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "Thank you for reminder.",
        "createdAt" : "2021-06-22T02:19:13Z",
        "updatedAt" : "2021-06-22T02:19:13Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "4b2a05533230110f2cc2940ecf0d91f6842ac4aa",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +599,603 @@   */\n  def isLocal: Boolean = logicalPlan.isInstanceOf[LocalRelation] ||\n    logicalPlan.isInstanceOf[CommandResult]\n\n  /**"
  },
  {
    "id" : "462ff7e8-4381-498d-9d5a-b8e7533bab2d",
    "prId" : 32863,
    "prUrl" : "https://github.com/apache/spark/pull/32863#pullrequestreview-681515717",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "53f375d0-2e01-4532-91c0-98ed44fc61ad",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "is this code repeated somewhere? and can we move it to a function to share code?",
        "createdAt" : "2021-06-11T05:50:02Z",
        "updatedAt" : "2021-06-11T05:50:02Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "f7d083c4-2353-4f04-85d7-55d059498384",
        "parentId" : "53f375d0-2e01-4532-91c0-98ed44fc61ad",
        "authorId" : "87a71d79-d4d2-4f2c-8e9a-be56cf39b035",
        "body" : "yes, it's repeated from join function, I have created a new function and replaced duplicated code with it",
        "createdAt" : "2021-06-11T07:09:18Z",
        "updatedAt" : "2021-06-11T07:09:18Z",
        "lastEditedBy" : "87a71d79-d4d2-4f2c-8e9a-be56cf39b035",
        "tags" : [
        ]
      }
    ],
    "commit" : "afd327952c795d6ab639b4fb4fc50eaaa5ddeb80",
    "line" : 73,
    "diffHunk" : "@@ -1,1 +1170,1174 @@\n    // If auto self join alias is enable\n    if (sqlContext.conf.dataFrameSelfJoinAutoResolveAmbiguity) {\n      joined = resolveSelfJoinCondition(joined)\n    }"
  },
  {
    "id" : "f87ad936-8cd4-4f08-8b6e-5cbb17b84776",
    "prId" : 32616,
    "prUrl" : "https://github.com/apache/spark/pull/32616#pullrequestreview-665118671",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "537b3642-933c-4b1e-8414-610b04280c29",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Q: Is there the possibility that the set will continue to increase in a long-running application?",
        "createdAt" : "2021-05-21T02:46:37Z",
        "updatedAt" : "2021-05-21T02:46:38Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "4cec5ddc-d2cc-46cf-a213-b89f82d821b2",
        "parentId" : "537b3642-933c-4b1e-8414-610b04280c29",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Looks no. Even in streaming query, it just reused the existing plan to run the query rather than re-transform the whole query.",
        "createdAt" : "2021-05-21T06:12:25Z",
        "updatedAt" : "2021-05-21T06:12:26Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "423f2b5ee2cd63bc4d0853305f7219d7891a1848",
    "line" : 27,
    "diffHunk" : "@@ -1,1 +234,238 @@    if (sparkSession.sessionState.conf.getConf(SQLConf.FAIL_AMBIGUOUS_SELF_JOIN_ENABLED)) {\n      val dsIds = plan.getTagValue(Dataset.DATASET_ID_TAG).getOrElse(new HashSet[Long])\n      dsIds.add(id)\n      plan.setTagValue(Dataset.DATASET_ID_TAG, dsIds)\n    }"
  },
  {
    "id" : "0a165980-e2da-4b79-a28c-65133fddb4ea",
    "prId" : 32032,
    "prUrl" : "https://github.com/apache/spark/pull/32032#pullrequestreview-632557373",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "55378630-8649-46e2-909d-6527113611a8",
        "parentId" : null,
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "Since `logicalPlan` is already analyzed, I am setting this to `true` here.",
        "createdAt" : "2021-04-09T15:49:00Z",
        "updatedAt" : "2021-04-14T03:19:24Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "111ef8be8bc03a62389aed4f0871b958714eb789",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +3375,3379 @@      replace = replace,\n      viewType = viewType,\n      isAnalyzed = true)\n  }\n"
  },
  {
    "id" : "4166e96b-b38f-4808-87d5-490e8c74d008",
    "prId" : 31905,
    "prUrl" : "https://github.com/apache/spark/pull/31905#pullrequestreview-709669124",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9f2d0ce8-6c1b-47b6-b0c1-b53a4a49c4b9",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "The test failure is likely from Java doc generation from these references. You'd have to wrap it with backticks for the references Java doc build cannot identify in Unidoc.",
        "createdAt" : "2021-07-19T10:36:23Z",
        "updatedAt" : "2021-07-19T10:36:23Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "c8182ae1-e5c0-4e9d-866b-3d588bf19149",
        "parentId" : "9f2d0ce8-6c1b-47b6-b0c1-b53a4a49c4b9",
        "authorId" : "c4ecf4e4-1fda-4fed-9980-aed6c3b3e8e4",
        "body" : "Can you please point me to the error and the command so I can reproduce this locally? Otherwise I wouldn't know if I am fixing the right thing.",
        "createdAt" : "2021-07-19T11:45:38Z",
        "updatedAt" : "2021-07-19T11:45:38Z",
        "lastEditedBy" : "c4ecf4e4-1fda-4fed-9980-aed6c3b3e8e4",
        "tags" : [
        ]
      },
      {
        "id" : "8e55f92a-4f01-4bda-9f48-2a0d4406f92b",
        "parentId" : "9f2d0ce8-6c1b-47b6-b0c1-b53a4a49c4b9",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I think you can reproduce it via `./build/sbt unidoc`. ",
        "createdAt" : "2021-07-19T12:30:08Z",
        "updatedAt" : "2021-07-19T12:30:08Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "f288019e-ad97-4eec-87c9-877e3bf8709c",
        "parentId" : "9f2d0ce8-6c1b-47b6-b0c1-b53a4a49c4b9",
        "authorId" : "c4ecf4e4-1fda-4fed-9980-aed6c3b3e8e4",
        "body" : "I tried that one, but it succeeds. I am now trying `./dev/run-tests --parallelism 2 --modules \"sql\" --excluded-tags \"org.apache.spark.tags.ExtendedSQLTest\"` as in https://github.com/apache/spark/runs/3102419062#step:9:5.",
        "createdAt" : "2021-07-19T12:49:04Z",
        "updatedAt" : "2021-07-19T12:49:04Z",
        "lastEditedBy" : "c4ecf4e4-1fda-4fed-9980-aed6c3b3e8e4",
        "tags" : [
        ]
      },
      {
        "id" : "29385bed-2941-45b7-9028-81924a2b0798",
        "parentId" : "9f2d0ce8-6c1b-47b6-b0c1-b53a4a49c4b9",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "hm, feel free to open a PR and leverage GitHub Actions build",
        "createdAt" : "2021-07-19T13:39:07Z",
        "updatedAt" : "2021-07-19T13:39:07Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "aeacbb4f-3477-4f98-8440-b711d77951e0",
        "parentId" : "9f2d0ce8-6c1b-47b6-b0c1-b53a4a49c4b9",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "oh btw some errors from unidoc are fake. I think the real errors are:\r\n\r\n```\r\n[error] /home/runner/work/spark/spark/sql/core/target/java/org/apache/spark/sql/Dataset.java:1800:1:  error: unexpected text\r\n[error]    * This is equivalent to calling {@link Dataset.observe(String, Column, Column*)} but does\r\n[error]                                    ^\r\n[error] /home/runner/work/spark/spark/sql/core/target/java/org/apache/spark/sql/Dataset.java:1804:1:  error: reference not found\r\n[error]    * A user can retrieve the metrics by accessing {@link org.apache.spark.sql.Observation.get}.\r\n[error]                                                          ^\r\n[error] /home/runner/work/spark/spark/sql/core/target/java/org/apache/spark/sql/Observation.java:51:1:  error: exception not thrown: java.lang.InterruptedException\r\n[error]    * @throws java.lang.InterruptedException interrupted while waiting\r\n[error]              ^\r\n```",
        "createdAt" : "2021-07-19T13:42:16Z",
        "updatedAt" : "2021-07-19T13:42:16Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "ff3702d6-8bbf-456f-a2a4-3949e7f5ddf3",
        "parentId" : "9f2d0ce8-6c1b-47b6-b0c1-b53a4a49c4b9",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "you might have to just wrap these three instances with backtics instead of `[[..]]` as a workaround.",
        "createdAt" : "2021-07-19T13:42:47Z",
        "updatedAt" : "2021-07-19T13:42:47Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "138a59b1-8e8f-48fc-980a-16ddb207585f",
        "parentId" : "9f2d0ce8-6c1b-47b6-b0c1-b53a4a49c4b9",
        "authorId" : "c4ecf4e4-1fda-4fed-9980-aed6c3b3e8e4",
        "body" : "Done. How is this merged into master, from another PR?",
        "createdAt" : "2021-07-19T14:56:47Z",
        "updatedAt" : "2021-07-19T14:56:47Z",
        "lastEditedBy" : "c4ecf4e4-1fda-4fed-9980-aed6c3b3e8e4",
        "tags" : [
        ]
      },
      {
        "id" : "f5b93381-8252-45bb-bf92-be6b7f3c6e3f",
        "parentId" : "9f2d0ce8-6c1b-47b6-b0c1-b53a4a49c4b9",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Yes, another PR would be cleaner because this PR seems to have a long history since March.. You can put the link of new PR here, @EnricoMi .",
        "createdAt" : "2021-07-19T15:00:01Z",
        "updatedAt" : "2021-07-19T15:00:41Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "fc5c90c7-2683-4292-9ae0-cb30d5df060a",
        "parentId" : "9f2d0ce8-6c1b-47b6-b0c1-b53a4a49c4b9",
        "authorId" : "c4ecf4e4-1fda-4fed-9980-aed6c3b3e8e4",
        "body" : "Done, thanks!",
        "createdAt" : "2021-07-19T15:11:03Z",
        "updatedAt" : "2021-07-19T15:11:03Z",
        "lastEditedBy" : "c4ecf4e4-1fda-4fed-9980-aed6c3b3e8e4",
        "tags" : [
        ]
      }
    ],
    "commit" : "ec7f7275ca7902db83aef86d174890bd1e240fe7",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1949,1953 @@\n  /**\n   * Observe (named) metrics through an [[org.apache.spark.sql.Observation]] instance.\n   * This is equivalent to calling [[Dataset.observe(String, Column, Column*)]] but does\n   * not require adding [[org.apache.spark.sql.util.QueryExecutionListener]] to the spark session."
  },
  {
    "id" : "b258705b-2c73-448b-bc64-6dec394b8edb",
    "prId" : 31700,
    "prUrl" : "https://github.com/apache/spark/pull/31700#pullrequestreview-603177943",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "900a4bbc-85d8-4b82-96a5-10445d76f337",
        "parentId" : null,
        "authorId" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "body" : "This bit is needed to prevent executing each batch early. We used `WriteToDataSourceV2` before, which did not extend `Command`.",
        "createdAt" : "2021-03-03T17:48:37Z",
        "updatedAt" : "2021-03-03T17:48:37Z",
        "lastEditedBy" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "tags" : [
        ]
      }
    ],
    "commit" : "e7f167fb0103d7e62127421cf5ad2a594d5f4337",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +225,229 @@    // to happen right away to let these side effects take place eagerly.\n    val plan = queryExecution.analyzed match {\n      case _: V2MicroBatchWriteCommand =>\n        queryExecution.analyzed\n      case c: Command =>"
  },
  {
    "id" : "8ffc264d-f77e-4f0b-a5ea-3c177b4b63bd",
    "prId" : 31296,
    "prUrl" : "https://github.com/apache/spark/pull/31296#pullrequestreview-574581181",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "be5f28fe-ab33-4a9f-a70e-bc43da980261",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "This sounds a little risky to me, `A process is invoked even for empty partitions`.\r\nThis may cause a hang situation if the command is expecting input.\r\nFor example, this PR's test case is using `cat`. And, `cat | wc -l` hangs.\r\nIf we are okay, could you add a test case of empty partition to make it sure that we handle those cases?",
        "createdAt" : "2021-01-22T19:46:20Z",
        "updatedAt" : "2021-01-26T08:19:19Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "43fdbdd5-2d91-41f5-bb54-dc97dddba7db",
        "parentId" : "be5f28fe-ab33-4a9f-a70e-bc43da980261",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Otherwise, you may want to add some warnings here.",
        "createdAt" : "2021-01-22T19:47:00Z",
        "updatedAt" : "2021-01-26T08:19:19Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "4720812f-396b-4e8d-82cb-b10b41160b4e",
        "parentId" : "be5f28fe-ab33-4a9f-a70e-bc43da980261",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "This is a good point. As you see this uses RDD.pipe, let me check if it addresses this or not. And yeah I will add a test case too.",
        "createdAt" : "2021-01-22T20:06:58Z",
        "updatedAt" : "2021-01-26T08:19:19Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "ac7460b0431e7012f17c3ac293226bb4f428bd15",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +2896,2900 @@   * separated by a newline. The resulting partition consists of the process's stdout output, with\n   * each line of stdout resulting in one element of the output partition. A process is invoked\n   * even for empty partitions.\n   *\n   * Note that for micro-batch streaming Dataset, the effect of pipe is only per micro-batch, not"
  },
  {
    "id" : "4f7bfd83-ebef-42ce-8996-0848bcd8e7d0",
    "prId" : 31296,
    "prUrl" : "https://github.com/apache/spark/pull/31296#pullrequestreview-576113504",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a0097328-eb51-4da1-838b-84837840acae",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "I see all examples are simply calling print function with converted string. Could we simply get serializer func like `serializeFn: (T => String)` instead, or have two overloaded methods allowing both cases if we are unsure printElement might be necessary in some cases? This should simplify the test codes and actual user codes (as `_.toString` would simply work).",
        "createdAt" : "2021-01-25T09:19:47Z",
        "updatedAt" : "2021-01-26T08:19:19Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "7142ce92-c25a-46b6-8b22-9fc0ee120ff6",
        "parentId" : "a0097328-eb51-4da1-838b-84837840acae",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "I think it is okay. Most cases there should be no difference. Only difference might be when we want to print out multi-lines per obj:\r\n\r\n```scala\r\ndef printElement(obj: T, printFunc: String => Unit) = {\r\n  printFunc(obj.a)\r\n  printFunc(obj.b)\r\n  ...\r\n}\r\n```\r\n\r\n```scala\r\ndef serializeFn(obj: T): String = {\r\n  s\"${obj.a}\\n${obj.b}\\n...\"\r\n}\r\n```\r\n\r\nI'm fine with either one as they are working the same effect although taking different form.\r\n\r\n",
        "createdAt" : "2021-01-26T08:24:52Z",
        "updatedAt" : "2021-01-26T08:24:52Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "ac7460b0431e7012f17c3ac293226bb4f428bd15",
    "line" : 25,
    "diffHunk" : "@@ -1,1 +2911,2915 @@   * @since 3.2.0\n   */\n  def pipe(command: String, printElement: (T, String => Unit) => Unit): Dataset[String] = {\n    implicit val stringEncoder = Encoders.STRING\n    withTypedPlan[String](PipeElements[T]("
  },
  {
    "id" : "2f3f1c1f-2639-44b4-8fbf-d09a8bbeace4",
    "prId" : 31296,
    "prUrl" : "https://github.com/apache/spark/pull/31296#pullrequestreview-575197994",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "187211e1-3614-4ec0-b231-a2b4ab2016b6",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "I'd kindly explain the case they need to be careful, like `e.g. If your external process does aggregation on inputs, the aggregation is applied per a partition in micro-batch. You may want to aggregate these outputs after calling pipe to get global aggregation across partitions and also across micro-batches.`",
        "createdAt" : "2021-01-25T09:27:53Z",
        "updatedAt" : "2021-01-26T08:19:19Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "ac7460b0431e7012f17c3ac293226bb4f428bd15",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +2898,2902 @@   * even for empty partitions.\n   *\n   * Note that for micro-batch streaming Dataset, the effect of pipe is only per micro-batch, not\n   * cross entire stream. If your external process does aggregation-like on inputs, e.g. `wc -l`,\n   * the aggregation is applied per a partition in micro-batch. You may want to aggregate these"
  },
  {
    "id" : "b67e1fb4-1551-4884-9e12-e381d63e1660",
    "prId" : 31254,
    "prUrl" : "https://github.com/apache/spark/pull/31254#pullrequestreview-577395242",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "de53d169-aa76-4ffe-8dd7-ea7728827998",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I am not sure how common it is to show distinct count. I believe this method was inspired by [`pandas.DataFrame.describe()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.describe.html) which however does not include distinct count.\r\n\r\nBTW, I think you should update the examples below too. ",
        "createdAt" : "2021-01-20T04:56:05Z",
        "updatedAt" : "2021-01-20T17:57:30Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "cb1917d2-bef4-4e79-baf2-a4abc1ba2fba",
        "parentId" : "de53d169-aa76-4ffe-8dd7-ea7728827998",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I wouldn't add this if there isn't any strong reason for it because you're already able to do this via `agg`.",
        "createdAt" : "2021-01-20T04:56:52Z",
        "updatedAt" : "2021-01-20T17:57:30Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "df47984b-ef86-4191-b970-046c7290fa6f",
        "parentId" : "de53d169-aa76-4ffe-8dd7-ea7728827998",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Meh, I think this could be useful as an _option_; it doesn't really expand the API surface or complexity. I'm OK with adding it, FWIW.",
        "createdAt" : "2021-01-20T11:24:50Z",
        "updatedAt" : "2021-01-20T17:57:30Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "141f1773-6613-41ac-b01a-b0e55c6c9db7",
        "parentId" : "de53d169-aa76-4ffe-8dd7-ea7728827998",
        "authorId" : "981b170c-729a-429c-b115-0350ea50b32b",
        "body" : "Wouldn't be more useful to expose approx variants here? ",
        "createdAt" : "2021-01-20T15:08:39Z",
        "updatedAt" : "2021-01-20T17:57:30Z",
        "lastEditedBy" : "981b170c-729a-429c-b115-0350ea50b32b",
        "tags" : [
        ]
      },
      {
        "id" : "8b6de835-30ec-42a5-bd44-4d14d4b047ed",
        "parentId" : "de53d169-aa76-4ffe-8dd7-ea7728827998",
        "authorId" : "e1bdd418-d9b9-4129-91a0-0865740d24d2",
        "body" : "@zero323 - I updated the PR to expose `approx_count_distinct` as well, good point.\r\n\r\n[One SO question on this](https://stackoverflow.com/questions/37949494/how-to-count-occurrences-of-each-distinct-value-for-every-column-in-a-dataframe) has 81,000 views and [the other](https://stackoverflow.com/questions/40888946/spark-dataframe-count-distinct-values-of-every-column) has 66,000 views.  There is a high demand for count distinct / approx_count_distinct exploratory data analysis queries by columns among the Spark userbase.\r\n\r\nThese options don't cost us much and should help the less technical Spark users that aren't comfortable using `agg`.  I help lots of folks that can't even program and they aren't code geniuses like you all ;)",
        "createdAt" : "2021-01-20T19:14:30Z",
        "updatedAt" : "2021-01-20T19:14:30Z",
        "lastEditedBy" : "e1bdd418-d9b9-4129-91a0-0865740d24d2",
        "tags" : [
        ]
      },
      {
        "id" : "4f24e799-be55-4d44-b66b-d4e9d0051afe",
        "parentId" : "de53d169-aa76-4ffe-8dd7-ea7728827998",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I am not sure. You can all have what you want via `agg`. This is just a shortcut to show basic stats, which was inspired by pandas' one, see https://github.com/apache/spark/commit/e1a172c201d68406faa53b113518b10c879f1ff6. We should of course avoid showing every stats here.\r\n\r\nYou can easily do `df.agg(\"col\" -> \"approx_count_distinct\", ...)` easily for other stats. Adding `count_distinct` here makes it ambiguous what to add in `DataFrame.describe` and `DataFrame.summary`.\r\n\r\n",
        "createdAt" : "2021-01-21T01:47:25Z",
        "updatedAt" : "2021-01-21T01:47:25Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "fc0352b6-15c6-411e-a166-4b185eabcbe0",
        "parentId" : "de53d169-aa76-4ffe-8dd7-ea7728827998",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "It's true, though, users looking for summary stats are probably looking at summary(), and sometimes looking for distinct values, and this puts them into one output with the others. I think it's slightly worth adding one more somewhat-common summary stat. ",
        "createdAt" : "2021-01-21T01:58:44Z",
        "updatedAt" : "2021-01-21T01:58:44Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "2802b13b-db2d-4439-ba7d-d1ae9c2777bb",
        "parentId" : "de53d169-aa76-4ffe-8dd7-ea7728827998",
        "authorId" : "981b170c-729a-429c-b115-0350ea50b32b",
        "body" : "> I am not sure. You can all have what you want via `agg`\r\n\r\nWhich is also more suitable for anything but quick visual inspection.\r\n\r\nMy biggest concern is somewhat different though â€’ existing summaries have good performance and predictable behavior. In contrast count distinct can be unstable and approx count distinct slower and more resource hungry. \r\n",
        "createdAt" : "2021-01-21T13:46:45Z",
        "updatedAt" : "2021-01-21T13:46:46Z",
        "lastEditedBy" : "981b170c-729a-429c-b115-0350ea50b32b",
        "tags" : [
        ]
      },
      {
        "id" : "a40041b1-2475-4ef1-87d9-6027f098a794",
        "parentId" : "de53d169-aa76-4ffe-8dd7-ea7728827998",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Sure, but this is only computed if the caller requests it.",
        "createdAt" : "2021-01-21T14:13:50Z",
        "updatedAt" : "2021-01-21T14:13:50Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "348bba8e-e2fd-45ed-81b3-c98d38338420",
        "parentId" : "de53d169-aa76-4ffe-8dd7-ea7728827998",
        "authorId" : "e1bdd418-d9b9-4129-91a0-0865740d24d2",
        "body" : "@HyukjinKwon - you raise a good point about API ambiguity between `Dataset.describe` and `Dataset.summary`, but those ambiguities already exist.  `df.describe(\"some_col\")`, `df.select(\"some_col\").summary()`, and `df.select(\"some_col\").summary(\"37%\", \"87%\")` all return different results.\r\n\r\n`describe` is a \"partial alias\" of `summary`, but the method signatures are different.\r\n\r\nThe Spark & Pandas `describe` methods aren't consistent either.  The Spark `describe` method takes column name arguments whereas the Pandas `describe` method takes options, see the Pandas method signature: `DataFrame.describe(percentiles=None, include=None, exclude=None, datetime_is_numeric=False)`.\r\n\r\nA Pandas user is already going to be confused by the API.  They're going to run `df.describe(percentiles=Array(.25, .5, .75))` and have to Google around to find that they need to run `df.summary(\"0.25%\", \"0.5%\", \"0.75%\")`.  \r\n\r\nAdding additional options to `Dataset.summary` seems consistent with the existing design.  The `summary` method is already where we put the extra metrics that aren't enabled by default, like the option to add arbitrary percentiles.\r\n\r\nBTW - I don't mean to complain here ;). It's easy for me to make comments on API decisions made years ago from folks in the trenches.  You are all doing a wonderful job maintaining this project and continuously moving it in a good direction.  I'm always impressed by the code you write and will hopefully get to your level some day.  My PR comments sound critical cause I'm just trying to get to the point.  I'm very grateful for all the work you've done on this project.",
        "createdAt" : "2021-01-26T04:42:07Z",
        "updatedAt" : "2021-01-26T04:42:08Z",
        "lastEditedBy" : "e1bdd418-d9b9-4129-91a0-0865740d24d2",
        "tags" : [
        ]
      },
      {
        "id" : "7b0f371c-71e5-4cc7-b680-138a57a159c8",
        "parentId" : "de53d169-aa76-4ffe-8dd7-ea7728827998",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "I would favor this change. If there aren't significant objections, I'd like to merge it.",
        "createdAt" : "2021-01-27T00:35:50Z",
        "updatedAt" : "2021-01-27T00:35:51Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "3126bfa1-6142-4c83-ae16-3c48ff59c5d4",
        "parentId" : "de53d169-aa76-4ffe-8dd7-ea7728827998",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I'm okay given that we don't change the default.",
        "createdAt" : "2021-01-27T04:14:14Z",
        "updatedAt" : "2021-01-27T04:14:14Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "42517638-ff3f-43bb-9223-0cab19b089af",
        "parentId" : "de53d169-aa76-4ffe-8dd7-ea7728827998",
        "authorId" : "981b170c-729a-429c-b115-0350ea50b32b",
        "body" : "Sounds good.",
        "createdAt" : "2021-01-27T14:31:10Z",
        "updatedAt" : "2021-01-27T14:31:10Z",
        "lastEditedBy" : "981b170c-729a-429c-b115-0350ea50b32b",
        "tags" : [
        ]
      }
    ],
    "commit" : "bc547a2c949dba8322378a334d8f7221e46e3784",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +2671,2675 @@   *   <li>max</li>\n   *   <li>arbitrary approximate percentiles specified as a percentage (e.g. 75%)</li>\n   *   <li>count_distinct</li>\n   *   <li>approx_count_distinct</li>\n   * </ul>"
  },
  {
    "id" : "a43637e8-b651-497f-9972-baedbb3a28f4",
    "prId" : 30775,
    "prUrl" : "https://github.com/apache/spark/pull/30775#pullrequestreview-565099293",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5e45ecd4-2a15-4d12-a919-3326bae286c4",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "can users work around it by calling untyped join and use `as[T]` to  get the typed dataset?",
        "createdAt" : "2021-01-11T06:35:38Z",
        "updatedAt" : "2021-01-11T07:01:11Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "3c1e946c-b71a-497f-a840-cf367683c189",
        "parentId" : "5e45ecd4-2a15-4d12-a919-3326bae286c4",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> can users work around it by calling untyped join and use `as[T]` to get the typed dataset?\r\n\r\nLogically, yes.  But seems not not very convenient or direct.",
        "createdAt" : "2021-01-11T06:56:10Z",
        "updatedAt" : "2021-01-11T07:01:11Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "3d13c3120bd7063942b04b1db660cf463fa136f9",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +1259,1263 @@   * @since 3.2.0\n   */\n  def joinPartial[U](other: Dataset[U], condition: Column, joinType: String): Dataset[T] = {\n    val joinedType = JoinType(joinType)\n"
  },
  {
    "id" : "0527a8e0-5b68-4adb-9d9f-96911fa86713",
    "prId" : 30647,
    "prUrl" : "https://github.com/apache/spark/pull/30647#pullrequestreview-547870986",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4befda5c-1fd4-4f64-b4f2-ba4d1eec3f42",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "The previous approach had an unacceptable behaviour ([it escaped unicode strings...](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/132367/testReport/org.apache.spark.sql/DatasetSuite/SPARK_25108_Fix_the_show_method_to_display_the_full_width_character_alignment_problem/)), so I've updated the code to replace `\\t` and `\\n` only.\r\n\r\n",
        "createdAt" : "2020-12-08T06:35:13Z",
        "updatedAt" : "2020-12-11T05:40:39Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "2708c3d4-b5c9-4926-88e4-ce6c1dce5e9a",
        "parentId" : "4befda5c-1fd4-4f64-b4f2-ba4d1eec3f42",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Just for my curiosity, why is `\\\\\\\\t` necessary? the extra escaping? this seems to replace with the string `\\\\t` rather than `\\t`",
        "createdAt" : "2020-12-08T14:54:27Z",
        "updatedAt" : "2020-12-11T05:40:39Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "f3e12d98-da86-47d7-880a-4daedf5573a9",
        "parentId" : "4befda5c-1fd4-4f64-b4f2-ba4d1eec3f42",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Yea, that's a reasonable question; It seems `$` and `\\` has a special meaning in the second param of `replaceAll` and see: https://docs.oracle.com/javase/8/docs/api/java/lang/String.html#replaceAll-java.lang.String-java.lang.String-\r\n```\r\nNote that backslashes (\\) and dollar signs ($) in the replacement string may cause the results\r\nto be different than if it were being treated as a literal replacement string; see Matcher.replaceAll. \r\nUse Matcher.quoteReplacement(java.lang.String) to suppress the special meaning\r\nof these characters, if desired.\r\n```",
        "createdAt" : "2020-12-09T06:19:58Z",
        "updatedAt" : "2020-12-11T05:40:39Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "feb02779c017cca24aa18240aa4370591c248ad6",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +311,315 @@          case _ =>\n            // Escapes meta-characters not to break the `showString` format\n            cell.toString.replaceAll(\"\\n\", \"\\\\\\\\n\").replaceAll(\"\\t\", \"\\\\\\\\t\")\n        }\n        if (truncate > 0 && str.length > truncate) {"
  },
  {
    "id" : "18646dfd-a7ae-4e18-8c46-5c58ca32aade",
    "prId" : 30488,
    "prUrl" : "https://github.com/apache/spark/pull/30488#pullrequestreview-540948039",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8d75fdb6-bde1-480f-b8ef-f1cf40411ca4",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "I find that `df.show()` can also change the dataset_id of LogicalPlan. So I made this change.",
        "createdAt" : "2020-11-25T13:35:18Z",
        "updatedAt" : "2020-12-02T15:39:45Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "8c05ded7-3e05-4c30-8810-810959ae4f60",
        "parentId" : "8d75fdb6-bde1-480f-b8ef-f1cf40411ca4",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Any existing test fails? If not, could you add tests for this change?",
        "createdAt" : "2020-11-26T06:33:47Z",
        "updatedAt" : "2020-12-02T15:39:45Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "7ddbdd64-157c-44d2-892f-b683082a5d67",
        "parentId" : "8d75fdb6-bde1-480f-b8ef-f1cf40411ca4",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Added, thanks!",
        "createdAt" : "2020-11-30T14:17:07Z",
        "updatedAt" : "2020-12-02T15:39:45Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "d13b88c25a10022be0ff0190fd6fa8d158b24560",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +233,237 @@    }\n    if (sparkSession.sessionState.conf.getConf(SQLConf.FAIL_AMBIGUOUS_SELF_JOIN_ENABLED) &&\n        plan.getTagValue(Dataset.DATASET_ID_TAG).isEmpty) {\n      plan.setTagValue(Dataset.DATASET_ID_TAG, id)\n    }"
  },
  {
    "id" : "5e463b64-711a-469d-835b-fb6e5f98c931",
    "prId" : 30257,
    "prUrl" : "https://github.com/apache/spark/pull/30257#pullrequestreview-525652408",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1ffec48b-cfaa-46d6-9e79-5bec8c822947",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Not sure. I would just simply select.",
        "createdAt" : "2020-11-05T04:46:18Z",
        "updatedAt" : "2020-11-05T04:46:19Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "233c87b8-3aaa-4dde-bc98-46992f9f662f",
        "parentId" : "1ffec48b-cfaa-46d6-9e79-5bec8c822947",
        "authorId" : "152034df-2d74-457f-aa5c-5d1f5614022c",
        "body" : "If use select, for users, which means they should  adjust the order of the cols.",
        "createdAt" : "2020-11-05T05:35:02Z",
        "updatedAt" : "2020-11-05T05:35:03Z",
        "lastEditedBy" : "152034df-2d74-457f-aa5c-5d1f5614022c",
        "tags" : [
        ]
      },
      {
        "id" : "d1331c9c-e7a6-49dd-a5a7-cc9a246a98f8",
        "parentId" : "1ffec48b-cfaa-46d6-9e79-5bec8c822947",
        "authorId" : "a3190564-c88c-4977-9012-ce9c80d993bb",
        "body" : "Instead of numeric position it would be way more convenient to reference a position by existing column names; perhaps add `before_column` and `after_column`? ",
        "createdAt" : "2020-11-05T18:20:35Z",
        "updatedAt" : "2020-11-05T18:23:38Z",
        "lastEditedBy" : "a3190564-c88c-4977-9012-ce9c80d993bb",
        "tags" : [
        ]
      },
      {
        "id" : "0002bf5d-c76c-4126-92d7-50764b05cf9e",
        "parentId" : "1ffec48b-cfaa-46d6-9e79-5bec8c822947",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Agree with @HyukjinKwon, seems `select` can do this.",
        "createdAt" : "2020-11-07T05:17:43Z",
        "updatedAt" : "2020-11-07T05:17:43Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "e27b6ee0-b297-4357-b0e8-1241e044ccdc",
        "parentId" : "1ffec48b-cfaa-46d6-9e79-5bec8c822947",
        "authorId" : "c3b617cc-e920-44cb-bdd6-d1ad5c252303",
        "body" : "yeah, as @HyukjinKwon said, i just realize with `select`.\r\n![image](https://user-images.githubusercontent.com/19250651/98440846-1fc6b980-2136-11eb-899d-e2c16e3ea965.png)",
        "createdAt" : "2020-11-07T12:22:03Z",
        "updatedAt" : "2020-11-07T12:22:03Z",
        "lastEditedBy" : "c3b617cc-e920-44cb-bdd6-d1ad5c252303",
        "tags" : [
        ]
      },
      {
        "id" : "8ea7c73c-4e8b-427c-960c-f15a36feee5a",
        "parentId" : "1ffec48b-cfaa-46d6-9e79-5bec8c822947",
        "authorId" : "152034df-2d74-457f-aa5c-5d1f5614022c",
        "body" : "what if you have 100 cols, and add new col in 50th position?\r\n you have to get `val cols:Array[String] =df.columns` and adjust the order of the cols and then use selectExpr(cols: _* ).\r\nthat's my point.",
        "createdAt" : "2020-11-07T13:17:22Z",
        "updatedAt" : "2020-11-07T13:17:22Z",
        "lastEditedBy" : "152034df-2d74-457f-aa5c-5d1f5614022c",
        "tags" : [
        ]
      }
    ],
    "commit" : "2b9b03d1bae7aebfc46ae2674af36d8a0dd67755",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +2392,2396 @@   * existing column that has the same name.\n   */\n  def withColumn(colName: String, col: Column, pos: Int): DataFrame =\n    withColumns(Seq(colName), Seq(col), pos)\n"
  },
  {
    "id" : "76d7146e-1dec-4b3d-93b7-d2f2018abfc3",
    "prId" : 28996,
    "prUrl" : "https://github.com/apache/spark/pull/28996#pullrequestreview-446816845",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bb296fcc-bc25-4bf7-914c-531dde2d87ce",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Could you add an illustrate example like 2016 ~ 2029, @viirya ?",
        "createdAt" : "2020-07-11T16:22:36Z",
        "updatedAt" : "2020-07-11T17:43:25Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "22f0d0ba-29f8-483a-9d99-ceff69d98ccf",
        "parentId" : "bb296fcc-bc25-4bf7-914c-531dde2d87ce",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "okay.",
        "createdAt" : "2020-07-11T17:25:52Z",
        "updatedAt" : "2020-07-11T17:43:25Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "873498394eaf80b2b302b6fc7aeec410e7113415",
    "line" : 41,
    "diffHunk" : "@@ -1,1 +2067,2071 @@   *   // +----+----+----+----+\n   * }}}\n   *\n   * @group typedrel\n   * @since 3.1.0"
  },
  {
    "id" : "24d8a7e3-1839-4ac7-8ed3-a705871d790e",
    "prId" : 28996,
    "prUrl" : "https://github.com/apache/spark/pull/28996#pullrequestreview-446816840",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "12f171c6-7c37-499d-99a3-ed188250655f",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "It's worth to document a little more about the order sensitive. Previously, it was simple because it follows the schema of original set(=left). With new options, the number of missing columns which will be added at the end are determined by `other` (=right).",
        "createdAt" : "2020-07-11T16:28:59Z",
        "updatedAt" : "2020-07-11T17:43:25Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "4a2b0f0c-f7e0-431c-aff0-c92250f7e49d",
        "parentId" : "12f171c6-7c37-499d-99a3-ed188250655f",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Good advice.",
        "createdAt" : "2020-07-11T17:25:47Z",
        "updatedAt" : "2020-07-11T17:43:25Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "873498394eaf80b2b302b6fc7aeec410e7113415",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +2040,2044 @@   *\n   * When the parameter `allowMissingColumns` is true, this function allows different set\n   * of column names between two Datasets. Missing columns at each side, will be filled with\n   * null values. The missing columns at left Dataset will be added at the end in the schema\n   * of the union result:"
  },
  {
    "id" : "a71e1f02-cc16-44d1-9300-91dc4d0c62bc",
    "prId" : 28996,
    "prUrl" : "https://github.com/apache/spark/pull/28996#pullrequestreview-448369095",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e176462a-dc4c-49df-aace-712a12f341f6",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Does it work with nested columns?",
        "createdAt" : "2020-07-13T04:57:01Z",
        "updatedAt" : "2020-07-13T04:57:01Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "06d5c8ed-da56-45bc-afed-54fb40b9bb20",
        "parentId" : "e176462a-dc4c-49df-aace-712a12f341f6",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "No, currently it doesn't.",
        "createdAt" : "2020-07-13T05:49:59Z",
        "updatedAt" : "2020-07-13T05:49:59Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "29bb60de-9a1d-4839-a121-007fc0ae8ba5",
        "parentId" : "e176462a-dc4c-49df-aace-712a12f341f6",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I think the major problem here is we put the by-name logic in the API method, not in the `Analyzer`. Shall we add 2 boolean parameters(byName and allowMissingCol) to `Union`, and move the by-name logic to the type coercion rules?",
        "createdAt" : "2020-07-13T06:31:51Z",
        "updatedAt" : "2020-07-13T06:35:20Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "83b4a809-a80a-43d2-bd29-b950bee16032",
        "parentId" : "e176462a-dc4c-49df-aace-712a12f341f6",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Ok. I will do it in another PR.",
        "createdAt" : "2020-07-13T15:40:37Z",
        "updatedAt" : "2020-07-13T15:40:37Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "efe2a946-dfa0-44e3-add6-65a9636f73ec",
        "parentId" : "e176462a-dc4c-49df-aace-712a12f341f6",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "@cloud-fan . `unionByName` (and `by-name logic`) has been here since Apache Spark 2.3.0. \r\nShall we proceed that refactoring suggestion as a separate JIRA?",
        "createdAt" : "2020-07-13T15:40:48Z",
        "updatedAt" : "2020-07-13T15:44:26Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "0f06c899-0453-4aac-860e-9f21ea503c8b",
        "parentId" : "e176462a-dc4c-49df-aace-712a12f341f6",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Yea it's better to have a new JIRA.",
        "createdAt" : "2020-07-13T16:31:10Z",
        "updatedAt" : "2020-07-13T16:31:10Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "e450d425-81c1-4fc2-a507-1a018a1f73bd",
        "parentId" : "e176462a-dc4c-49df-aace-712a12f341f6",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Thanks, @cloud-fan .",
        "createdAt" : "2020-07-14T18:41:46Z",
        "updatedAt" : "2020-07-14T18:41:46Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "873498394eaf80b2b302b6fc7aeec410e7113415",
    "line" : 56,
    "diffHunk" : "@@ -1,1 +2089,2093 @@    val rightProjectList = leftOutputAttrs.map { lattr =>\n      rightOutputAttrs.find { rattr => resolver(lattr.name, rattr.name) }.getOrElse {\n        if (allowMissingColumns) {\n          Alias(Literal(null, lattr.dataType), lattr.name)()\n        } else {"
  },
  {
    "id" : "341bd6cd-55cc-4241-b6f8-784b19dbe044",
    "prId" : 28996,
    "prUrl" : "https://github.com/apache/spark/pull/28996#pullrequestreview-482514644",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3905d670-c9bb-4622-be52-352f6f2517ee",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "Do we have a JIRA to add the corresponding API for Python? ",
        "createdAt" : "2020-09-04T06:41:07Z",
        "updatedAt" : "2020-09-04T06:41:07Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "c44968c8-6fdd-4b3b-a838-0160f6b0061c",
        "parentId" : "3905d670-c9bb-4622-be52-352f6f2517ee",
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "This is a good beginner task for new contributors.",
        "createdAt" : "2020-09-04T06:42:16Z",
        "updatedAt" : "2020-09-04T06:42:16Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "c4ff3c71-402c-445d-9946-fb02053cd82c",
        "parentId" : "3905d670-c9bb-4622-be52-352f6f2517ee",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "I should create a followup PR for Python and R. But it is okay for a beginner task too.",
        "createdAt" : "2020-09-04T06:44:19Z",
        "updatedAt" : "2020-09-04T06:44:20Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "42f4270b-a233-4cfe-b3fb-8297a6550cdb",
        "parentId" : "3905d670-c9bb-4622-be52-352f6f2517ee",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I filed at SPARK-32798 and SPARK-32799",
        "createdAt" : "2020-09-04T09:46:21Z",
        "updatedAt" : "2020-09-04T09:46:21Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "873498394eaf80b2b302b6fc7aeec410e7113415",
    "line" : 45,
    "diffHunk" : "@@ -1,1 +2071,2075 @@   * @since 3.1.0\n   */\n  def unionByName(other: Dataset[T], allowMissingColumns: Boolean): Dataset[T] = withSetOperator {\n    // Check column name duplication\n    val resolver = sparkSession.sessionState.analyzer.resolver"
  },
  {
    "id" : "d05eb071-febc-4dc7-aaa3-945f0abeb63a",
    "prId" : 28984,
    "prUrl" : "https://github.com/apache/spark/pull/28984#pullrequestreview-442812800",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c429a850-fa9e-4059-b9f1-245105ae3f5b",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "The built-in `to_json` is not enough for your usecase? cc: @HyukjinKwon ",
        "createdAt" : "2020-07-05T12:29:21Z",
        "updatedAt" : "2020-07-06T06:18:46Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "57eeec8c-56f6-4c5e-943b-de98cc01ea8e",
        "parentId" : "c429a850-fa9e-4059-b9f1-245105ae3f5b",
        "authorId" : "5b9e802a-4319-492f-a0ba-8cdf41bed39a",
        "body" : "I guess so, you'd have to do something like this `df.select(to_json(struct(\"*\"), Map(\"ignoreNullFields\" -> \"false\")).as(\"value\"))` to produce the same result as `df.toJSON(options)`",
        "createdAt" : "2020-07-05T13:12:56Z",
        "updatedAt" : "2020-07-06T06:18:46Z",
        "lastEditedBy" : "5b9e802a-4319-492f-a0ba-8cdf41bed39a",
        "tags" : [
        ]
      },
      {
        "id" : "867caa8c-1bbe-4934-958d-11b5cebdb9ca",
        "parentId" : "c429a850-fa9e-4059-b9f1-245105ae3f5b",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Yea, I think so.",
        "createdAt" : "2020-07-05T13:46:02Z",
        "updatedAt" : "2020-07-06T06:18:46Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "29b4d02f-8fa0-400b-a3ab-d6a5b5c213d6",
        "parentId" : "c429a850-fa9e-4059-b9f1-245105ae3f5b",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I don't feel strongly here. I would rather don't extend `toJSON` anymore given that we added `to_json`, `from_json`, etc. to cover such general cases. I am okay if any committer supports it.",
        "createdAt" : "2020-07-06T04:02:27Z",
        "updatedAt" : "2020-07-06T06:18:46Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "969e9c4c-d3fa-4996-b97c-921cc3ee2982",
        "parentId" : "c429a850-fa9e-4059-b9f1-245105ae3f5b",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Thanks for the check, @HyukjinKwon. I'm neutral on this, too. ",
        "createdAt" : "2020-07-06T06:50:35Z",
        "updatedAt" : "2020-07-06T06:50:36Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "73697c90476308a8b8b39ad63815437f41a60df9",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +3403,3407 @@   * @since 3.1.0\n   */\n  def toJSON(options: Map[String, String]): Dataset[String] = {\n    val rowSchema = this.schema\n    val sessionLocalTimeZone = sparkSession.sessionState.conf.sessionLocalTimeZone"
  },
  {
    "id" : "c4e9c256-c230-4874-9b2e-bc465dbb7c3d",
    "prId" : 28984,
    "prUrl" : "https://github.com/apache/spark/pull/28984#pullrequestreview-442797165",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "aa3a002b-21ef-460a-a4c1-afc3aa64176a",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I would deduplicate it by calling `toJSON(options: Map[String, String])` at `toJSON`.",
        "createdAt" : "2020-07-06T04:03:33Z",
        "updatedAt" : "2020-07-06T06:18:46Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "113226a0-ac62-4a73-bd24-f6cf730d7edd",
        "parentId" : "aa3a002b-21ef-460a-a4c1-afc3aa64176a",
        "authorId" : "5b9e802a-4319-492f-a0ba-8cdf41bed39a",
        "body" : "ok!",
        "createdAt" : "2020-07-06T06:14:56Z",
        "updatedAt" : "2020-07-06T06:18:46Z",
        "lastEditedBy" : "5b9e802a-4319-492f-a0ba-8cdf41bed39a",
        "tags" : [
        ]
      }
    ],
    "commit" : "73697c90476308a8b8b39ad63815437f41a60df9",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +3403,3407 @@   * @since 3.1.0\n   */\n  def toJSON(options: Map[String, String]): Dataset[String] = {\n    val rowSchema = this.schema\n    val sessionLocalTimeZone = sparkSession.sessionState.conf.sessionLocalTimeZone"
  },
  {
    "id" : "1c734818-71db-418c-ad1c-c4b43d175bbb",
    "prId" : 28830,
    "prUrl" : "https://github.com/apache/spark/pull/28830#pullrequestreview-430429141",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ef1cddf2-0388-4477-87f6-c005f081d445",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I think this is good for now.\r\n\r\nIn the future, this may still be broken by Scala version upgrade, and hopefully @xuanyuanking 's unsafe row validation can detect it. Then we can change it and use a deterministic order, as it will be broken anyway.",
        "createdAt" : "2020-06-15T06:33:28Z",
        "updatedAt" : "2020-06-15T06:33:49Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "6b779ed7-c6a1-4409-a39e-27fcb883e416",
        "parentId" : "ef1cddf2-0388-4477-87f6-c005f081d445",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "Yep, I also mentioned this at https://github.com/apache/spark/pull/28830#discussion_r439909489, we can relay on the validation checking and integrated tests.",
        "createdAt" : "2020-06-15T06:40:43Z",
        "updatedAt" : "2020-06-15T06:40:43Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      },
      {
        "id" : "50f972f2-c86b-4c16-82fa-24c7fd782884",
        "parentId" : "ef1cddf2-0388-4477-87f6-c005f081d445",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Worth noting that we need to have \"concrete\" solution eventually - if columns are all having same type neither #28830 nor #24173 catch the change and the result becomes silently incorrect. I roughly remember the similar issue on pyspark, which was trying to fix the issue on order vs name, don't remember how it ended up. cc. @HyukjinKwon ",
        "createdAt" : "2020-06-15T06:57:23Z",
        "updatedAt" : "2020-06-15T06:58:11Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "fe429ea8-16ee-4419-8be2-5def53becc20",
        "parentId" : "ef1cddf2-0388-4477-87f6-c005f081d445",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Ah, that was fixed in a way by adding an env variable. That case also was specific to Python 2 which is deprecated now so it's rather a corner case.",
        "createdAt" : "2020-06-15T08:31:43Z",
        "updatedAt" : "2020-06-15T08:31:44Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "7546ba4eebeee480d9a2ff8b948e900cd6023dfc",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +2544,2548 @@    // SPARK-31990: We must keep `toSet.toSeq` here because of the backward compatibility issue\n    // (the Streaming's state store depends on the `groupCols` order).\n    val groupCols = colNames.toSet.toSeq.flatMap { (colName: String) =>\n      // It is possibly there are more than one columns with the same name,\n      // so we call filter instead of find."
  },
  {
    "id" : "5fae005c-40e8-4fae-86a1-a8009c448da7",
    "prId" : 27565,
    "prUrl" : "https://github.com/apache/spark/pull/27565#pullrequestreview-358344805",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9132fc76-3a71-46df-b349-76f2468dab58",
        "parentId" : null,
        "authorId" : "7e665d8b-d739-4edf-88c8-7379ff8585c2",
        "body" : "Remove @DeveloperApi. Now it is user API.",
        "createdAt" : "2020-02-13T16:05:48Z",
        "updatedAt" : "2020-02-17T10:44:06Z",
        "lastEditedBy" : "7e665d8b-d739-4edf-88c8-7379ff8585c2",
        "tags" : [
        ]
      }
    ],
    "commit" : "0e749403b4bb127341849827cc95313752b6c715",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +3323,3327 @@   */\n  @DeveloperApi\n  def sameSemantics(other: Dataset[T]): Boolean = {\n    queryExecution.analyzed.sameResult(other.queryExecution.analyzed)\n  }"
  },
  {
    "id" : "250424dd-4f15-44a6-a02f-3c5464ea36d7",
    "prId" : 27565,
    "prUrl" : "https://github.com/apache/spark/pull/27565#pullrequestreview-358686968",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4e9d8198-91d3-4a34-b7fe-521f67058483",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "It should have `@since` too.",
        "createdAt" : "2020-02-14T02:58:19Z",
        "updatedAt" : "2020-02-17T10:44:06Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "0e749403b4bb127341849827cc95313752b6c715",
    "line" : 26,
    "diffHunk" : "@@ -1,1 +3333,3337 @@   *       simplified by tolerating the cosmetic differences such as attribute names.\n   * @since 3.1.0\n   */\n  @DeveloperApi\n  def semanticHash(): Int = {"
  },
  {
    "id" : "58fd207a-d433-44a1-8351-7dc529933b30",
    "prId" : 27499,
    "prUrl" : "https://github.com/apache/spark/pull/27499#pullrequestreview-356439384",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a6297f76-7636-49bf-96e2-9990b51b4ede",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "is it possible to support mixed typed and untyped columns?",
        "createdAt" : "2020-02-10T12:20:28Z",
        "updatedAt" : "2020-02-26T16:57:33Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "ecd0bcbd-99b8-4fdf-b977-8c89c201d0ba",
        "parentId" : "a6297f76-7636-49bf-96e2-9990b51b4ede",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "For typed column, there is encoder which specifies how we can encode its output. We have no such info for untyped columns.",
        "createdAt" : "2020-02-10T23:20:57Z",
        "updatedAt" : "2020-02-26T16:57:33Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "717d9a09-ac10-4119-86a5-7f44b9bb3443",
        "parentId" : "a6297f76-7636-49bf-96e2-9990b51b4ede",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Hm, but previously `count` case was possible to use it as untyped column.\r\n\r\n```scala\r\nscala> spark.range(1).select(count(\"id\"), count(\"id\"), count(\"id\"), count(\"id\"), count(\"id\"), count(\"id\")).show()\r\n+---------+---------+---------+---------+---------+---------+\r\n|count(id)|count(id)|count(id)|count(id)|count(id)|count(id)|\r\n+---------+---------+---------+---------+---------+---------+\r\n|        1|        1|        1|        1|        1|        1|\r\n+---------+---------+---------+---------+---------+---------+\r\n\r\n\r\nscala> count(\"id\")\r\nres0: org.apache.spark.sql.TypedColumn[Any,Long] = count(id)\r\n```\r\n",
        "createdAt" : "2020-02-11T04:32:30Z",
        "updatedAt" : "2020-02-26T16:57:33Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "a25e9e5d-4ae2-4a12-8057-8f99399d05c1",
        "parentId" : "a6297f76-7636-49bf-96e2-9990b51b4ede",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Ah, but problem is more specific to `Aggregator` because users can set the type explicitly and later `encoder` is required, specifically via `TypedColumn.withInputType` -> `TypedAggregateExpression.withInputInfo`",
        "createdAt" : "2020-02-11T04:41:38Z",
        "updatedAt" : "2020-02-26T16:57:33Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "b8df712c-9875-4012-a84f-05e677bc1bea",
        "parentId" : "a6297f76-7636-49bf-96e2-9990b51b4ede",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "What about a fix like this?\r\n\r\n```diff\r\ndiff --git a/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala b/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala\r\nindex 16f1cac3e0f..91b6e13b100 100644\r\n--- a/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala\r\n+++ b/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala\r\n@@ -1430,12 +1430,11 @@ class Dataset[T] private[sql](\r\n    */\r\n   @scala.annotation.varargs\r\n   def select(cols: Column*): DataFrame = withPlan {\r\n-    cols.find(_.isInstanceOf[TypedColumn[_, _]]).foreach { typedCol =>\r\n-      throw new AnalysisException(s\"$typedCol is a typed column that \" +\r\n-        \"cannot be passed in untyped `select` API. If you are going to select \" +\r\n-        \"multiple typed columns, you can use `Dataset.selectUntyped` API.\")\r\n+    val newCols = cols.map {\r\n+      case tc: TypedColumn[_, _] => tc.withInputType(exprEnc, logicalPlan.output)\r\n+      case c => c\r\n     }\r\n-    Project(cols.map(_.named), logicalPlan)\r\n+    Project(newCols.map(_.named), logicalPlan)\r\n```",
        "createdAt" : "2020-02-11T04:48:11Z",
        "updatedAt" : "2020-02-26T16:57:33Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "f89c9fb5-d02c-4533-9067-c8500fa82d56",
        "parentId" : "a6297f76-7636-49bf-96e2-9990b51b4ede",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "It cannot work because typed column's output is a domain object instead of a column. That is why untyped `select` returns DataFrame and typed `select` returns Dataset[U1], Dataset[(U1, U2)], Dataset[(U1, U2, U3)]...\r\n\r\n`count` is possible before because it is actually untyped column but wrongly being wrapped in a typed column.",
        "createdAt" : "2020-02-11T05:59:36Z",
        "updatedAt" : "2020-02-26T16:57:33Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "6fe279c9-b580-428f-a697-c179e789b250",
        "parentId" : "a6297f76-7636-49bf-96e2-9990b51b4ede",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Hm.. I see. So to support typed columns properly, we should either expose `selectUntyped` or make all cases for `def select[U1, ...](`  ..",
        "createdAt" : "2020-02-11T06:23:40Z",
        "updatedAt" : "2020-02-26T16:57:33Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "89c843fb-96fd-48c6-9a23-9d891bdd7e7c",
        "parentId" : "a6297f76-7636-49bf-96e2-9990b51b4ede",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Yea, I think so if I do not miss anything.",
        "createdAt" : "2020-02-11T06:43:32Z",
        "updatedAt" : "2020-02-26T16:57:33Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "7d045fbfb1b6bf35c5cc49e9948db2c5140bb15d",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +1431,1435 @@   */\n  @scala.annotation.varargs\n  def select(cols: Column*): DataFrame = withPlan {\n    val untypedCols = cols.map {\n      case typedCol: TypedColumn[_, _] =>"
  },
  {
    "id" : "145596d0-830a-4d2a-b17e-50d652cd49f3",
    "prId" : 27499,
    "prUrl" : "https://github.com/apache/spark/pull/27499#pullrequestreview-363018259",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a32567a6-5435-4bb3-bbc5-d94da395434e",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "hmm, we also have such usage in our test:\r\n\r\n```scala\r\ndf.select(count(\"*\"), countDistinct(\"*\"))\r\n```\r\n\r\nSo this is still a breaking change? @cloud-fan @HyukjinKwon \r\n",
        "createdAt" : "2020-02-21T21:32:21Z",
        "updatedAt" : "2020-02-26T16:57:33Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "ca1a41ef-9d53-400f-b349-4198c9b35a1e",
        "parentId" : "a32567a6-5435-4bb3-bbc5-d94da395434e",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "We already shipped `count` as `TypedColumn` for a while, now to change it to `Column` is a breaking change.\r\n\r\nIn order to not break current usage, I think the fix is to allow feasible typed columns (e.g. `count`) in the untyped `select`.",
        "createdAt" : "2020-02-22T08:41:06Z",
        "updatedAt" : "2020-02-26T16:57:33Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "7d045fbfb1b6bf35c5cc49e9948db2c5140bb15d",
    "line" : 30,
    "diffHunk" : "@@ -1,1 +1449,1453 @@\n      case other => other\n    }\n    Project(untypedCols.map(_.named), logicalPlan)\n  }"
  },
  {
    "id" : "5ec467a2-5bda-4ec2-a7ba-e4328f6185dc",
    "prId" : 27499,
    "prUrl" : "https://github.com/apache/spark/pull/27499#pullrequestreview-363018762",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "082b22c4-a50b-4cc1-923c-5230a25b71ee",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "In order to not make a breaking change, we need to accept simple typed columns like `count`. @cloud-fan @HyukjinKwon ",
        "createdAt" : "2020-02-22T08:55:32Z",
        "updatedAt" : "2020-02-26T16:57:33Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "7d045fbfb1b6bf35c5cc49e9948db2c5140bb15d",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +1433,1437 @@  def select(cols: Column*): DataFrame = withPlan {\n    val untypedCols = cols.map {\n      case typedCol: TypedColumn[_, _] =>\n        // Checks if a `TypedColumn` has been inserted with\n        // specific input type and schema by `withInputType`."
  },
  {
    "id" : "34f8b8ba-62d5-484f-8c49-84a4cac97aea",
    "prId" : 27387,
    "prUrl" : "https://github.com/apache/spark/pull/27387#pullrequestreview-358094289",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d0ef24a6-f1b5-443e-8b79-18093342daac",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "do we need to scope the active session here? The operations below, `sparkSession.sessionState.executePlan` and `new Dataset`, both seem to scope the active session themselves.",
        "createdAt" : "2020-02-12T17:06:20Z",
        "updatedAt" : "2020-02-13T14:38:24Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "d02548dc-4482-4e2f-85d5-4652363d02cf",
        "parentId" : "d0ef24a6-f1b5-443e-8b79-18093342daac",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I see, the creation of `RowEncoder` may access active session",
        "createdAt" : "2020-02-12T17:09:56Z",
        "updatedAt" : "2020-02-13T14:38:24Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "24883f3a-92fe-40d6-97ae-60efcab35525",
        "parentId" : "d0ef24a6-f1b5-443e-8b79-18093342daac",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "shall we only wrap it?",
        "createdAt" : "2020-02-12T17:10:15Z",
        "updatedAt" : "2020-02-13T14:38:24Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "a907c69c-af5a-403e-a414-5a6c5965ccd9",
        "parentId" : "d0ef24a6-f1b5-443e-8b79-18093342daac",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "nvm, it's too tricky to find out which part uses active session. It's safer to wrap the entire method.",
        "createdAt" : "2020-02-13T10:10:40Z",
        "updatedAt" : "2020-02-13T14:38:24Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "7f6274d1605aaef56e2cf978d9d0db927e6386b6",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +84,88 @@\n  def ofRows(sparkSession: SparkSession, logicalPlan: LogicalPlan): DataFrame =\n    sparkSession.withActive {\n      val qe = sparkSession.sessionState.executePlan(logicalPlan)\n      qe.assertAnalyzed()"
  },
  {
    "id" : "d92507c0-ed5d-4df1-a2c1-92727b9b00fd",
    "prId" : 27387,
    "prUrl" : "https://github.com/apache/spark/pull/27387#pullrequestreview-357629982",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3cef60b9-4e53-4462-b509-f73001f75c0d",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "ditto",
        "createdAt" : "2020-02-12T17:06:38Z",
        "updatedAt" : "2020-02-13T14:38:24Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "7f6274d1605aaef56e2cf978d9d0db927e6386b6",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +92,96 @@  /** A variant of ofRows that allows passing in a tracker so we can track query parsing time. */\n  def ofRows(sparkSession: SparkSession, logicalPlan: LogicalPlan, tracker: QueryPlanningTracker)\n    : DataFrame = sparkSession.withActive {\n    val qe = new QueryExecution(sparkSession, logicalPlan, tracker)\n    qe.assertAnalyzed()"
  },
  {
    "id" : "2fa21402-26c9-431a-92b6-6f7af3c7dff4",
    "prId" : 27387,
    "prUrl" : "https://github.com/apache/spark/pull/27387#pullrequestreview-357856120",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cc3aee76-1fbf-4c48-9e70-3e6c235ffce3",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "does this operation involve active session?",
        "createdAt" : "2020-02-12T17:08:29Z",
        "updatedAt" : "2020-02-13T14:38:24Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "00797cbe-f479-4123-bf71-a5c7cea56e30",
        "parentId" : "cc3aee76-1fbf-4c48-9e70-3e6c235ffce3",
        "authorId" : "80d631a6-73e8-46a6-a01b-a80d1f1cc6cc",
        "body" : "Unfortunately it does. We were seeing errors in one of the thrift suites, the behavior of some expressions depends on SQLConf.get. In this particular case `IntegralDivide` is the offender. This is actually super bad since it can lead to correctness problems, and we should fix this in more a complete way.",
        "createdAt" : "2020-02-12T23:12:06Z",
        "updatedAt" : "2020-02-13T14:38:24Z",
        "lastEditedBy" : "80d631a6-73e8-46a6-a01b-a80d1f1cc6cc",
        "tags" : [
        ]
      }
    ],
    "commit" : "7f6274d1605aaef56e2cf978d9d0db927e6386b6",
    "line" : 74,
    "diffHunk" : "@@ -1,1 +505,509 @@   */\n  def schema: StructType = sparkSession.withActive {\n    queryExecution.analyzed.schema\n  }\n"
  },
  {
    "id" : "5ddbe123-1dfc-4bc4-80b2-530ae4dc49fc",
    "prId" : 27128,
    "prUrl" : "https://github.com/apache/spark/pull/27128#pullrequestreview-339690491",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "87a83e6e-8fbc-4ade-82e5-b144ba457ddd",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "why do we need to copy? and also `drop` isn't an action. Also, looks we're going to lose existing plans.",
        "createdAt" : "2020-01-08T07:26:43Z",
        "updatedAt" : "2020-01-08T07:26:43Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "e1959a49-781f-430d-b947-57ea79227f35",
        "parentId" : "87a83e6e-8fbc-4ade-82e5-b144ba457ddd",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "e.g.:\r\n\r\n```scala\r\nSeq((1, 2), (3, 4)).toDF(\"a\", \"b\").write.parquet(\"/tmp/abc\")\r\nspark.read.parquet(\"/tmp/abc\").drop(\"a\").explain(true)\r\n```",
        "createdAt" : "2020-01-08T07:27:13Z",
        "updatedAt" : "2020-01-08T07:27:14Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "527450fe-7633-478d-94a8-0b78b3e48a47",
        "parentId" : "87a83e6e-8fbc-4ade-82e5-b144ba457ddd",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Probably, @amanomer just used my example code in the jira. That's just an example code and I didn't carefully check it (I deleted the jira comment cuz that's a misdirection I think now). I'm not currently sure that this is an issue to fix as I said in the jira: https://issues.apache.org/jira/browse/SPARK-30421",
        "createdAt" : "2020-01-08T07:48:24Z",
        "updatedAt" : "2020-01-08T08:43:16Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "c297e6a8b4a27b31b783daf5a1264eb8b4d37447",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +2378,2382 @@    } else {\n      val df = this.select(remainingCols: _*)\n      withAction(\"drop\", df.queryExecution) { physicalPlan =>\n        Dataset.ofRows(\n          sparkSession,"
  },
  {
    "id" : "d51df230-058a-451a-82cf-5fb3686e75f1",
    "prId" : 27046,
    "prUrl" : "https://github.com/apache/spark/pull/27046#pullrequestreview-337137495",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "92a01492-2b08-41ac-ba64-885b62d52e13",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "This was a minor bug on example: unlike `observedMetrics` available in QueryExecutionListener, `event.progress.observedMetrics` is a \"Java\" Map instead of \"Scala\" Map, hence to use foreach it needs `asScala` to convert to Scala Map.",
        "createdAt" : "2019-12-30T04:13:19Z",
        "updatedAt" : "2019-12-30T11:19:08Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "5e9e0228-fefd-4051-b472-208cf40c4278",
        "parentId" : "92a01492-2b08-41ac-ba64-885b62d52e13",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Sounds fine. The alternative is to wrap it in `Option(....get(...)).foreach` but that's messier",
        "createdAt" : "2019-12-30T15:13:45Z",
        "updatedAt" : "2019-12-30T15:13:46Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      }
    ],
    "commit" : "2878cdb5f546367e8a86429c847a8649849c53c7",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +1895,1899 @@  *   spark.streams.addListener(new StreamingQueryListener() {\n  *     override def onQueryProgress(event: QueryProgressEvent): Unit = {\n  *       event.progress.observedMetrics.asScala.get(\"my_event\").foreach { row =>\n  *         // Trigger if the number of errors exceeds 5 percent\n  *         val num_rows = row.getAs[Long](\"rc\")"
  },
  {
    "id" : "23283c97-71ae-42ac-82d3-ecaae62eb0f4",
    "prId" : 27046,
    "prUrl" : "https://github.com/apache/spark/pull/27046#pullrequestreview-336999837",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b888bba2-fb03-40cb-99a3-78da6015c42b",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Can we add `@Unstable` or `@DeveloperApi` while we're here? The usage with `QueryExecutionListener` should be considered as one of both. ",
        "createdAt" : "2019-12-30T04:51:06Z",
        "updatedAt" : "2019-12-30T11:19:08Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "2878cdb5f546367e8a86429c847a8649849c53c7",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +1913,1917 @@  * }}}\n  *\n  * @group typedrel\n  * @since 3.0.0\n  */"
  },
  {
    "id" : "03158c55-8168-41b9-8f2b-afc0434ef24e",
    "prId" : 26969,
    "prUrl" : "https://github.com/apache/spark/pull/26969#pullrequestreview-484199330",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "673b4d2d-c614-4832-8e79-a294b93220d6",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "After a second thought, I don't think it's possible to do it, as the encoder is a black box here. We can't just assume the encoder expression is simply selecting columns, it's not even true for case class, as case class encoder does `Upcast` to input columns.\r\n\r\nAs I said before, `as[U].map(identity)` should be good enough, and the performance is as good as this one if there are other typed operations before `as[U]` and the `identity` function will be merged into the previous typed operations and has nearly no overhead.",
        "createdAt" : "2020-08-18T09:29:25Z",
        "updatedAt" : "2020-12-18T07:50:20Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "60c358f1-cecc-4518-a867-1419a79eac97",
        "parentId" : "673b4d2d-c614-4832-8e79-a294b93220d6",
        "authorId" : "c4ecf4e4-1fda-4fed-9980-aed6c3b3e8e4",
        "body" : "I don't see why encoders that perform `UpCast` would not work here. The above line still does apply the encoder via `.as[U]`, and `UpCast` does not involve column names in any way, i.e. the projection. Can you given a code example showing why this should not work?\r\n\r\nWith `map(identity)` we perform an encoder round-trip encoding the given columns into the black-box representation, and decoding with the same encoder to some columns again. The decoded representation (columns) must work as input to encode again, hence column names on both sides are identical. I would argue if column names exist it can be inferred that they can be projected. I'd be happy to see some example code that show this assumption is wrong.\r\n\r\nIt is a common use-case to use `Dataset[T]` to create typed user code. Defining a function `def compute(data: DataFrame): DataFrame` would allow to call it with arbitrary `DataFrame`s and the function would have to assert the schema on every call. Defining the function typed instead like `def compute(data: Dataset[Raw]): Dataset[Compute]` removes those assertions from your code and makes your code not compile when your try to use the wrong kind of `DataFrame`. In that scenario, you are using `.as[T]` but might never call any method that actually uses the encoder (like `map` or `collect`). Thus, you never materialize that hidden schema.\r\n\r\nI think adding `.map(identity)` to `.as[T]` in user code looks redundant and meaningless from a user point of view and should be moved behind the Spark API.",
        "createdAt" : "2020-08-18T10:53:42Z",
        "updatedAt" : "2020-12-18T07:50:20Z",
        "lastEditedBy" : "c4ecf4e4-1fda-4fed-9980-aed6c3b3e8e4",
        "tags" : [
        ]
      },
      {
        "id" : "9f7c0e7e-b262-4ce1-804c-4fd9f533020e",
        "parentId" : "673b4d2d-c614-4832-8e79-a294b93220d6",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "For example, the df schema is `<a: int, b: int>` while `U` is `case class U(a: long, b: String)`. The final schema of `toDS[U]` should be `<a: long, b: string>`.\r\n\r\n`Upcast` is just one example, `toDS[U]` can't be implemented as simply selecting columns, because we must understand what the encoder expression is.",
        "createdAt" : "2020-08-18T13:45:18Z",
        "updatedAt" : "2020-12-18T07:50:20Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "69976686-7684-495c-92f2-4cf37a14e739",
        "parentId" : "673b4d2d-c614-4832-8e79-a294b93220d6",
        "authorId" : "c4ecf4e4-1fda-4fed-9980-aed6c3b3e8e4",
        "body" : "And if we define the `toDS[U]` operation as applying the schema of `U` only and not any other semantics of the encoder. The encoder has an output schema, and this is the output schema of `toDS[U]`. I think that is a sensible scope and a common use case.",
        "createdAt" : "2020-09-08T14:29:49Z",
        "updatedAt" : "2020-12-18T07:50:20Z",
        "lastEditedBy" : "c4ecf4e4-1fda-4fed-9980-aed6c3b3e8e4",
        "tags" : [
        ]
      }
    ],
    "commit" : "4b45f296a60992ae56925aa004bcc0d8bf0f0faf",
    "line" : 70,
    "diffHunk" : "@@ -1,1 +563,567 @@    }\n\n    this.select(projectedColumns.map(c => col(c.name)): _*).as[U]\n  }\n"
  },
  {
    "id" : "c5f97803-9257-4c8c-b6e0-b7a73543023c",
    "prId" : 26922,
    "prUrl" : "https://github.com/apache/spark/pull/26922#pullrequestreview-333631077",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "163a3c9e-8d8a-4682-b651-506b876f7de7",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Nit: close `<li>` tag",
        "createdAt" : "2019-12-17T16:31:44Z",
        "updatedAt" : "2019-12-18T01:53:10Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "1e9da92a-295d-4710-8f71-266acce36e51",
        "parentId" : "163a3c9e-8d8a-4682-b651-506b876f7de7",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "BTW... doesn't scaladoc interpret markdown tags? Is it just that these didn't have a blank line above or something? I don't know it well.",
        "createdAt" : "2019-12-17T16:32:50Z",
        "updatedAt" : "2019-12-18T01:53:10Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "e25cc581-e7d9-4ee0-9e6e-4b8bf4a26f4c",
        "parentId" : "163a3c9e-8d8a-4682-b651-506b876f7de7",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "nice catch!thanks! the scaladoc supports wiki-style syntax and HTML, not markdown.",
        "createdAt" : "2019-12-17T16:52:43Z",
        "updatedAt" : "2019-12-18T01:53:10Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "da42f648-86ea-465b-bbb6-ecb0e27e8a90",
        "parentId" : "163a3c9e-8d8a-4682-b651-506b876f7de7",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Oh OK. Maybe I'm crazy but I thought `- foo` style lists work, but remember that sometimes they don't unless there's a blank line above it. I'm just wondering if that's the issue here. Or if it's not -- there are many more instances like this in the code, no? let's fix more of them if it's necessary",
        "createdAt" : "2019-12-17T22:45:02Z",
        "updatedAt" : "2019-12-18T01:53:10Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      }
    ],
    "commit" : "2c1b3e273c7028ec1f0cdc041a5ab54c7f76d048",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +452,456 @@   * method used to map columns depend on the type of `U`:\n   * <ul>\n   *   <li>When `U` is a class, fields for the class will be mapped to columns of the same name\n   *   (case sensitivity is determined by `spark.sql.caseSensitive`).</li>\n   *   <li>When `U` is a tuple, the columns will be mapped by ordinal (i.e. the first column will"
  },
  {
    "id" : "2bac4969-f7fd-4e01-9a6c-f664d38ac1c1",
    "prId" : 26903,
    "prUrl" : "https://github.com/apache/spark/pull/26903#pullrequestreview-332361859",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "09db29b7-056b-4c52-afbc-022ae5dc8bea",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "This is matched to Python side doc added FYI.",
        "createdAt" : "2019-12-16T06:36:57Z",
        "updatedAt" : "2019-12-16T06:36:58Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "f47a2559424c8e6c724f2036af7e0c4615a7ec0d",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +534,538 @@   *               <li>`formatted`: Split explain output into two sections: a physical plan outline\n   *                 and node details.</li>\n   *             </ul>\n   * @group basic\n   * @since 3.0.0"
  },
  {
    "id" : "bf5d12fb-9f98-46e3-8219-1733710b1a4a",
    "prId" : 26898,
    "prUrl" : "https://github.com/apache/spark/pull/26898#pullrequestreview-332331274",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f99d3d0c-bb5e-4f9a-b9f6-d11491afde17",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "~I'm a little confused. This seems to be different from the original logic `ExplainCommandUtil.explainedQueryExecution(sparkSession, logicalPlan, queryExecution)` in case of  `logicalPlan.isStreaming == true`? Did I miss something?~\r\nOh, sorry. I found the new logic.",
        "createdAt" : "2019-12-16T04:01:26Z",
        "updatedAt" : "2019-12-16T04:02:05Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "48789561900c7124a7ac5485f946aeae7e224fc9",
    "line" : 65,
    "diffHunk" : "@@ -1,1 +535,539 @@\n    // scalastyle:off println\n    println(queryExecution.explainString(ExplainMode.fromString(mode)))\n    // scalastyle:on println\n  }"
  },
  {
    "id" : "9ea565ed-e181-4ad6-9062-738c0f8ea467",
    "prId" : 26898,
    "prUrl" : "https://github.com/apache/spark/pull/26898#pullrequestreview-332355123",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5bf8ebf3-d99b-4633-b5b3-c6649a91e411",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Shall we at least document the acceptable explain mode string here?",
        "createdAt" : "2019-12-16T05:59:23Z",
        "updatedAt" : "2019-12-16T05:59:23Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "78963b21-2e27-46d9-a9e9-3dee3f0cacec",
        "parentId" : "5bf8ebf3-d99b-4633-b5b3-c6649a91e411",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Sure.",
        "createdAt" : "2019-12-16T06:09:59Z",
        "updatedAt" : "2019-12-16T06:09:59Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "48789561900c7124a7ac5485f946aeae7e224fc9",
    "line" : 51,
    "diffHunk" : "@@ -1,1 +523,527 @@\n  /**\n   * Prints the plans (logical and physical) with a format specified by a given explain mode.\n   *\n   * @group basic"
  },
  {
    "id" : "e30612f0-e83a-4119-b8bd-57aac56832b7",
    "prId" : 26886,
    "prUrl" : "https://github.com/apache/spark/pull/26886#pullrequestreview-332176727",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6e684cb9-a189-48aa-bcbe-2b29bfd5bfb5",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "https://github.com/apache/spark/pull/26861#discussion_r357727679",
        "createdAt" : "2019-12-14T00:20:20Z",
        "updatedAt" : "2019-12-14T11:36:44Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "d98e261c36d5418c85b4b3618f740e0260c000ed",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +548,552 @@  }\n\n  // This method intends to be called from PySpark DataFrame\n  private[sql] def toExplainString(mode: String): String = {\n    mode.toLowerCase(Locale.ROOT) match {"
  },
  {
    "id" : "a692ffff-1b02-4bfa-a435-d22a02685f17",
    "prId" : 26861,
    "prUrl" : "https://github.com/apache/spark/pull/26861#pullrequestreview-331851881",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "29ea719a-6baa-48b8-a628-b3952f77b380",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Yes, I think there's a similar case at `mode` at `DataFrameWriter`. How about we have `explain(mode: String)` only instead of `explain(mode: ExplainMode)`? For enum one, I am not sure actually yet (e.g., `joinType`).",
        "createdAt" : "2019-12-12T09:14:41Z",
        "updatedAt" : "2019-12-12T22:05:35Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "63d06373-8d39-452b-9fa2-0c1c0bdaecf0",
        "parentId" : "29ea719a-6baa-48b8-a628-b3952f77b380",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Aha, I see. So, you mean `ExplainMode` is internally used only?",
        "createdAt" : "2019-12-12T10:10:14Z",
        "updatedAt" : "2019-12-12T22:05:35Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "8f62604b-e13d-41f9-b11c-6934ccd6a7bc",
        "parentId" : "29ea719a-6baa-48b8-a628-b3952f77b380",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Yes, that was my thinking. WDYT?",
        "createdAt" : "2019-12-12T10:38:52Z",
        "updatedAt" : "2019-12-12T22:05:35Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "00bce43a-85cc-49d1-864d-0a1a0a4cc166",
        "parentId" : "29ea719a-6baa-48b8-a628-b3952f77b380",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Yea, to me, the string argument looks more useful than enum (cuz we don't import anything for that and that interface is easy-to-use from python). But, we might need more comments about this. cc: @cloud-fan @dongjoon-hyun ",
        "createdAt" : "2019-12-12T11:43:37Z",
        "updatedAt" : "2019-12-13T00:51:02Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "d45b59ed-51e1-4b56-a988-25a4b71171da",
        "parentId" : "29ea719a-6baa-48b8-a628-b3952f77b380",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "But `SaveMode` is public. We can have both `explain(String)` and `explain(ExplainMode)`",
        "createdAt" : "2019-12-12T12:04:09Z",
        "updatedAt" : "2019-12-12T22:05:35Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "cd193218-1334-4e7f-8f0e-c1682662af87",
        "parentId" : "29ea719a-6baa-48b8-a628-b3952f77b380",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "But `joinType` doesn't expose enums as an example. `ExplainMode` was added in Spark 3.0 so we don't necessarily expose another API. Actually, isn't using string easier given that explain will be used in a debugging purpose more often? ",
        "createdAt" : "2019-12-12T12:07:51Z",
        "updatedAt" : "2019-12-12T22:05:35Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "34f39f8c-9fec-4e70-b503-f826cfed4699",
        "parentId" : "29ea719a-6baa-48b8-a628-b3952f77b380",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "If you guys don't feel strongly, can we just have `explain(String)` alone for now? I somewhat feel a bit strong that we should better start from fewer APIs.",
        "createdAt" : "2019-12-13T01:42:54Z",
        "updatedAt" : "2019-12-13T01:42:54Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "4aef8969-f9f9-4090-a63f-d889342c3be5",
        "parentId" : "29ea719a-6baa-48b8-a628-b3952f77b380",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "yea, I like fewer API designs and I think enum arguments doesn't matter for python/R users.",
        "createdAt" : "2019-12-13T12:55:17Z",
        "updatedAt" : "2019-12-13T12:55:18Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "56509fc1624e24304d9f985416dbaede3ccb502b",
    "line" : 55,
    "diffHunk" : "@@ -1,1 +566,570 @@   * @since 3.0.0\n   */\n  def explain(mode: ExplainMode): Unit = {\n    // scalastyle:off println\n    println(toExplainString(mode))"
  },
  {
    "id" : "1bfee4f5-5627-40ec-837b-e5b765298405",
    "prId" : 26861,
    "prUrl" : "https://github.com/apache/spark/pull/26861#pullrequestreview-332142352",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9e59e35e-2cc4-4807-a9d2-590070b6b14b",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Is this only for Python to call? I think it is better to add short comment.",
        "createdAt" : "2019-12-13T16:30:23Z",
        "updatedAt" : "2019-12-13T16:30:24Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "8920a4ef-2a19-4eb8-96d0-e71e0480d597",
        "parentId" : "9e59e35e-2cc4-4807-a9d2-590070b6b14b",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Ah, yes. ok, I'll follow up, thanks for the comment.",
        "createdAt" : "2019-12-13T22:07:51Z",
        "updatedAt" : "2019-12-13T22:07:52Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "56509fc1624e24304d9f985416dbaede3ccb502b",
    "line" : 37,
    "diffHunk" : "@@ -1,1 +548,552 @@  }\n\n  private[sql] def toExplainString(mode: String): String = {\n    mode.toLowerCase(Locale.ROOT) match {\n      case \"simple\" => toExplainString(ExplainMode.Simple)"
  },
  {
    "id" : "b4a54d50-6a6a-40c5-9f82-87ab59e48307",
    "prId" : 26829,
    "prUrl" : "https://github.com/apache/spark/pull/26829#pullrequestreview-330429676",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3bb78681-c141-4df6-88bb-f97d2b897030",
        "parentId" : null,
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "How about retain the old api and add a new api ?",
        "createdAt" : "2019-12-10T11:22:50Z",
        "updatedAt" : "2019-12-10T11:22:50Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      },
      {
        "id" : "ee5a2479-436a-4d0b-83f2-fed59cbebd06",
        "parentId" : "3bb78681-c141-4df6-88bb-f97d2b897030",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "@ulysses-you . @maropu already did. Please see [line 564](https://github.com/apache/spark/pull/26829/files#diff-7a46f10c3cedbf013cf255564d9483cdR564).\r\n",
        "createdAt" : "2019-12-10T17:42:11Z",
        "updatedAt" : "2019-12-10T17:42:11Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "df24844e-b535-4995-9b83-04862da64ffa",
        "parentId" : "3bb78681-c141-4df6-88bb-f97d2b897030",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Yea, we should keep this. Thanks for the comment, @dongjoon-hyun ",
        "createdAt" : "2019-12-11T05:33:59Z",
        "updatedAt" : "2019-12-11T05:33:59Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "9b91b314-82ed-44fc-849a-94feb8d8d344",
        "parentId" : "3bb78681-c141-4df6-88bb-f97d2b897030",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "Oh, I see it.",
        "createdAt" : "2019-12-11T10:15:12Z",
        "updatedAt" : "2019-12-11T10:15:13Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      }
    ],
    "commit" : "e8c4af1b9a0fbc9eb134bd0579c4ce7f9d0cb256",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +528,532 @@   * @since 3.0.0\n   */\n  def explain(mode: ExplainMode): Unit = {\n    // Because temporary views are resolved during analysis when we create a Dataset, and\n    // `ExplainCommand` analyzes input query plan and resolves temporary views again. Using"
  },
  {
    "id" : "7a3811c2-fff1-40b6-9200-e4186e5cd766",
    "prId" : 26809,
    "prUrl" : "https://github.com/apache/spark/pull/26809#pullrequestreview-329525045",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "87890a3e-6df8-4407-8da3-b6a46552996b",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Is this `last` only making sense for ordered dataset?\r\n\r\nThough `head` also refers to first rows, it can be seen as same as `limit`. For `last` it seems to be more related to an ordered dataset?",
        "createdAt" : "2019-12-09T23:23:06Z",
        "updatedAt" : "2019-12-30T04:46:12Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "4a283f1a-944c-4ca9-ac0e-ea75eac25991",
        "parentId" : "87890a3e-6df8-4407-8da3-b6a46552996b",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Yes, I rather targets the last rows (with natural order or explicit ordering) .. I think it's a similar argument. If the order is non-deterministic, there won't be not much difference comparing to `head()` , although both will return different values.",
        "createdAt" : "2019-12-10T00:36:29Z",
        "updatedAt" : "2019-12-30T04:46:12Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "738d3e1307144504ed8977a7758ce40a602b6cf3",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +2795,2799 @@\n  /**\n   * Returns the last `n` rows in the Dataset.\n   *\n   * Running tail requires moving data into the application's driver process, and doing so with"
  },
  {
    "id" : "6b6fcc17-f41f-45ad-a6ab-57f6edda2d8c",
    "prId" : 26127,
    "prUrl" : "https://github.com/apache/spark/pull/26127#pullrequestreview-324479797",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1999c44d-be74-4764-bcb6-8dcb2da3b082",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Does it follow the same rule of `Aggregate#aggregateExpressions`?",
        "createdAt" : "2019-10-18T16:29:27Z",
        "updatedAt" : "2019-12-02T22:49:24Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "f65f2df2-03cd-4201-8f6e-e13b1f954446",
        "parentId" : "1999c44d-be74-4764-bcb6-8dcb2da3b082",
        "authorId" : "80d631a6-73e8-46a6-a01b-a80d1f1cc6cc",
        "body" : "It follows the rules for global aggregate.",
        "createdAt" : "2019-11-29T00:08:17Z",
        "updatedAt" : "2019-12-02T22:49:24Z",
        "lastEditedBy" : "80d631a6-73e8-46a6-a01b-a80d1f1cc6cc",
        "tags" : [
        ]
      }
    ],
    "commit" : "a07474c6c6131de1ae487ad96c91deb80cce2f1f",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +1860,1864 @@  * Please note that continuous execution is currently not supported.\n  *\n  * The metrics columns must either contain a literal (e.g. lit(42)), or should contain one or\n  * more aggregate functions (e.g. sum(a) or sum(a + b) + avg(c) - lit(1)). Expressions that\n  * contain references to the input Dataset's columns must always be wrapped in an aggregate"
  },
  {
    "id" : "df69513e-1294-4b66-a2f5-15430b99e6a9",
    "prId" : 26127,
    "prUrl" : "https://github.com/apache/spark/pull/26127#pullrequestreview-337097845",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c435e6a3-2d6f-4d7e-84de-e33e9e7321df",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Can we have an example with `QueryExecutionListener` too?",
        "createdAt" : "2019-12-30T02:17:29Z",
        "updatedAt" : "2019-12-30T02:17:29Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "55824069-4539-44bc-9637-b29eabfd9a1e",
        "parentId" : "c435e6a3-2d6f-4d7e-84de-e33e9e7321df",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "I'll raise a PR for this, as I found a slight bug in example for streaming as well.",
        "createdAt" : "2019-12-30T04:05:29Z",
        "updatedAt" : "2019-12-30T04:05:29Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "b87912dd-7690-4d3e-95f5-99b7e2a86e46",
        "parentId" : "c435e6a3-2d6f-4d7e-84de-e33e9e7321df",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Raised #27046. Please note that the patch relies on the current status.",
        "createdAt" : "2019-12-30T04:14:38Z",
        "updatedAt" : "2019-12-30T04:14:38Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "c1b850aa-a424-4c1c-b8fd-c11136a035d0",
        "parentId" : "c435e6a3-2d6f-4d7e-84de-e33e9e7321df",
        "authorId" : "80d631a6-73e8-46a6-a01b-a80d1f1cc6cc",
        "body" : "So you want to encourage users to use an unstable API?",
        "createdAt" : "2019-12-30T09:42:52Z",
        "updatedAt" : "2019-12-30T09:42:52Z",
        "lastEditedBy" : "80d631a6-73e8-46a6-a01b-a80d1f1cc6cc",
        "tags" : [
        ]
      },
      {
        "id" : "0d5460af-ae68-45e9-95ea-5d578c094b8b",
        "parentId" : "c435e6a3-2d6f-4d7e-84de-e33e9e7321df",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I would like to show the documented usage.",
        "createdAt" : "2019-12-30T09:51:23Z",
        "updatedAt" : "2019-12-30T09:51:24Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "d09e388a-d0b7-4f84-85b6-b168d8ee108b",
        "parentId" : "c435e6a3-2d6f-4d7e-84de-e33e9e7321df",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "and mark this as unstable too .. given that it relies on an unstable APIs.",
        "createdAt" : "2019-12-30T09:57:03Z",
        "updatedAt" : "2019-12-30T09:57:03Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "168f07a5-7fb6-4ce3-aecc-39a68f9faa46",
        "parentId" : "c435e6a3-2d6f-4d7e-84de-e33e9e7321df",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Let's sort it out a bit.\r\n\r\nWas the batch use case excluded by intention due to the use of DeveloperApi? If that's the intention, then yes #27046 can just fix the slight bug on streaming example.\r\n\r\nIf the concern is focused on having QueryExecution as private API and exposing it to DeveloperApi, I think it's time to fix that, as no one would complain for the change on major release ",
        "createdAt" : "2019-12-30T10:08:16Z",
        "updatedAt" : "2019-12-30T10:08:16Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "f8c2619b-1e43-4eb8-9782-4569584904c3",
        "parentId" : "c435e6a3-2d6f-4d7e-84de-e33e9e7321df",
        "authorId" : "80d631a6-73e8-46a6-a01b-a80d1f1cc6cc",
        "body" : "We shouldn't document unstable features unless you commit to stabilizing them in the short term. Please don't add that example. A savy developer will know how work with this, and can weight the risk involved themselves.\r\n\r\nThe same goes for unstable. Please don't mark it as unstable, the function itself is stable. It is stable for streaming. The batch case relies on `QueryExecutionListener` and isn't, however that API is marked as such so there shouldn't be any confusion there.\r\n\r\n@HeartSaVioR it would great if we can fix the streaming example.",
        "createdAt" : "2019-12-30T10:29:37Z",
        "updatedAt" : "2019-12-30T10:29:38Z",
        "lastEditedBy" : "80d631a6-73e8-46a6-a01b-a80d1f1cc6cc",
        "tags" : [
        ]
      },
      {
        "id" : "18e0e82b-19d9-44f6-93d6-ec40fce373d3",
        "parentId" : "c435e6a3-2d6f-4d7e-84de-e33e9e7321df",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "OK thanks for clarification. Totally makes sense. Will update the PR there.",
        "createdAt" : "2019-12-30T10:58:54Z",
        "updatedAt" : "2019-12-30T10:58:54Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "ce46172c-c2af-4854-9122-6cdcb418340f",
        "parentId" : "c435e6a3-2d6f-4d7e-84de-e33e9e7321df",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Okay, I offline discussed with @hvanhovell. I am okie.",
        "createdAt" : "2019-12-30T13:00:34Z",
        "updatedAt" : "2019-12-30T13:00:34Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "a07474c6c6131de1ae487ad96c91deb80cce2f1f",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +1867,1871 @@  * A user can observe these metrics by either adding\n  * [[org.apache.spark.sql.streaming.StreamingQueryListener]] or a\n  * [[org.apache.spark.sql.util.QueryExecutionListener]] to the spark session.\n  *\n  * {{{"
  },
  {
    "id" : "8c7c163b-f705-47a4-ae98-af13f455693e",
    "prId" : 25354,
    "prUrl" : "https://github.com/apache/spark/pull/25354#pullrequestreview-278106588",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "adbeeaee-6655-4f4c-bc45-5367ebe31e94",
        "parentId" : null,
        "authorId" : "1aba4faa-ea5e-4487-baf9-d552ca566126",
        "body" : "I find there is `write` for v1\r\n\r\n```\r\n  /**\r\n   * Interface for saving the content of the non-streaming Dataset out into external storage.\r\n   *\r\n   * @group basic\r\n   * @since 1.6.0\r\n   */\r\n  def write: DataFrameWriter[T] = {\r\n    if (isStreaming) {\r\n      logicalPlan.failAnalysis(\r\n        \"'write' can not be called on streaming Dataset/DataFrame\")\r\n    }\r\n    new DataFrameWriter[T](this)\r\n  }\r\n```\r\nwhy not name it `writeV2` to be self-explaining? or overload `write` from different return type `DataFrameWriterV2[T] `?",
        "createdAt" : "2019-08-21T05:43:57Z",
        "updatedAt" : "2019-08-31T02:18:57Z",
        "lastEditedBy" : "1aba4faa-ea5e-4487-baf9-d552ca566126",
        "tags" : [
        ]
      },
      {
        "id" : "7e1db74f-db8a-409d-8775-7da63e66c726",
        "parentId" : "adbeeaee-6655-4f4c-bc45-5367ebe31e94",
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "We can't change the behavior of `write` because we don't want to break older jobs. And we need to pass the table name or path somewhere. I think this works, but if everyone prefers `writeV2`, we can rename it.",
        "createdAt" : "2019-08-21T16:13:13Z",
        "updatedAt" : "2019-08-31T02:18:57Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      },
      {
        "id" : "73c118ea-bf5f-4178-8b8c-9637c4cb2cfd",
        "parentId" : "adbeeaee-6655-4f4c-bc45-5367ebe31e94",
        "authorId" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "body" : "I like `writeTo`, since we're explaining exactly what we're writing to at that point. ",
        "createdAt" : "2019-08-21T22:25:37Z",
        "updatedAt" : "2019-08-31T02:18:57Z",
        "lastEditedBy" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "tags" : [
        ]
      }
    ],
    "commit" : "9864d42501feff1acd01e11aedd2a7dc84a88bd5",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +3210,3214 @@   * @since 3.0.0\n   */\n  def writeTo(table: String): DataFrameWriterV2[T] = {\n    // TODO: streaming could be adapted to use this interface\n    if (isStreaming) {"
  },
  {
    "id" : "43652950-e6e8-464d-9144-44e20e3a7519",
    "prId" : 25354,
    "prUrl" : "https://github.com/apache/spark/pull/25354#pullrequestreview-278119063",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d58dcf57-53a6-4076-a412-f3f0971d937e",
        "parentId" : null,
        "authorId" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "body" : "may be good to add: `use 'writeStream' instead`",
        "createdAt" : "2019-08-21T22:26:41Z",
        "updatedAt" : "2019-08-31T02:18:57Z",
        "lastEditedBy" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "tags" : [
        ]
      },
      {
        "id" : "4920577f-21f8-432d-baab-15879979f53d",
        "parentId" : "d58dcf57-53a6-4076-a412-f3f0971d937e",
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "I didn't include this because I'd rather not have the v2 API recommend using the v1 API. That seems confusing to me.",
        "createdAt" : "2019-08-21T23:07:17Z",
        "updatedAt" : "2019-08-31T02:18:57Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      }
    ],
    "commit" : "9864d42501feff1acd01e11aedd2a7dc84a88bd5",
    "line" : 27,
    "diffHunk" : "@@ -1,1 +3214,3218 @@    if (isStreaming) {\n      logicalPlan.failAnalysis(\n        \"'writeTo' can not be called on streaming Dataset/DataFrame\")\n    }\n    new DataFrameWriterV2[T](table, this)"
  },
  {
    "id" : "f1105336-3ad4-46bf-a935-621bda318829",
    "prId" : 25107,
    "prUrl" : "https://github.com/apache/spark/pull/25107#pullrequestreview-270485225",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "007ea2f2-7d7e-4ce3-b88a-fdc01c3ec908",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "Also, document these metadata will be removed in the rule DetectAmbbiguousSelfJoin",
        "createdAt" : "2019-08-04T07:43:33Z",
        "updatedAt" : "2019-08-05T06:04:46Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      }
    ],
    "commit" : "62228e8ffbb5d54ad2ec5a85dd2450cd1dec5ae2",
    "line" : 79,
    "diffHunk" : "@@ -1,1 +1332,1336 @@  // Attach the dataset id and column position to the column reference, so that we can detect\n  // ambiguous self-join correctly. See the rule `DetectAmbiguousSelfJoin`.\n  // This must be called before we return a `Column` that contains `AttributeReference`.\n  // Note that, the metadata added here are only avaiable in the analyzer, as the analyzer rule\n  // `DetectAmbiguousSelfJoin` will remove it."
  },
  {
    "id" : "09fd78e6-fb3c-4437-986f-aa81c8f0b78d",
    "prId" : 25055,
    "prUrl" : "https://github.com/apache/spark/pull/25055#pullrequestreview-258608389",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a6e93968-6399-42b6-a7ff-d7949e78b55d",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Is there no palce having the same issue?",
        "createdAt" : "2019-07-05T00:52:18Z",
        "updatedAt" : "2019-07-05T18:43:54Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "f106c36d-7fe6-42a2-a536-a3477395395f",
        "parentId" : "a6e93968-6399-42b6-a7ff-d7949e78b55d",
        "authorId" : "bd48244c-ec04-47b7-a7f0-ae8e070e5394",
        "body" : "Went through Dataset.scala - didn't find similar issue. However there might be the same problems in other places in our SQL code..",
        "createdAt" : "2019-07-05T05:27:09Z",
        "updatedAt" : "2019-07-05T18:43:54Z",
        "lastEditedBy" : "bd48244c-ec04-47b7-a7f0-ae8e070e5394",
        "tags" : [
        ]
      },
      {
        "id" : "77f4a277-d3cf-4c64-bd21-92cc10252d99",
        "parentId" : "a6e93968-6399-42b6-a7ff-d7949e78b55d",
        "authorId" : "24e1dd39-ae3f-4bbb-a391-f60afb62d075",
        "body" : "There are other places. Please see https://github.com/apache/spark/pull/21449.",
        "createdAt" : "2019-07-05T05:52:30Z",
        "updatedAt" : "2019-07-05T18:43:54Z",
        "lastEditedBy" : "24e1dd39-ae3f-4bbb-a391-f60afb62d075",
        "tags" : [
        ]
      },
      {
        "id" : "a5818519-b8e1-431d-95ff-d7040d07bfdc",
        "parentId" : "a6e93968-6399-42b6-a7ff-d7949e78b55d",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "ok, thanks. I think its ok to only target dataset.drop in this pr.",
        "createdAt" : "2019-07-07T03:41:44Z",
        "updatedAt" : "2019-07-07T03:41:44Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "1bc07422-928f-49fe-babf-5f5f466ced3a",
        "parentId" : "a6e93968-6399-42b6-a7ff-d7949e78b55d",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "+1 for @maropu 's opinion.",
        "createdAt" : "2019-07-07T03:46:40Z",
        "updatedAt" : "2019-07-07T03:46:40Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "1dc9aa73bf4077bf7de6ab46c7227f09d7d4ffdb",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +2323,2327 @@    val attrs = this.logicalPlan.output\n    val colsAfterDrop = attrs.filter { attr =>\n      !attr.semanticEquals(expression)\n    }.map(attr => Column(attr))\n    select(colsAfterDrop : _*)"
  },
  {
    "id" : "d2381082-01b4-46c9-a70b-080a02f1bd48",
    "prId" : 25055,
    "prUrl" : "https://github.com/apache/spark/pull/25055#pullrequestreview-258367875",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ce97283f-bb85-407a-b1a2-995c15314331",
        "parentId" : null,
        "authorId" : "599a53e9-f56e-44b2-84c4-c1306a5fd80a",
        "body" : "nice",
        "createdAt" : "2019-07-05T11:59:32Z",
        "updatedAt" : "2019-07-05T18:43:54Z",
        "lastEditedBy" : "599a53e9-f56e-44b2-84c4-c1306a5fd80a",
        "tags" : [
        ]
      }
    ],
    "commit" : "1dc9aa73bf4077bf7de6ab46c7227f09d7d4ffdb",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +2323,2327 @@    val attrs = this.logicalPlan.output\n    val colsAfterDrop = attrs.filter { attr =>\n      !attr.semanticEquals(expression)\n    }.map(attr => Column(attr))\n    select(colsAfterDrop : _*)"
  },
  {
    "id" : "4ea78d07-0ab7-478f-84d8-f1c981d241af",
    "prId" : 25055,
    "prUrl" : "https://github.com/apache/spark/pull/25055#pullrequestreview-264518181",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1a72d9c4-2af2-499b-bcea-98c2b1728515",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "```\r\n  def semanticEquals(other: Expression): Boolean =\r\n    deterministic && other.deterministic && canonicalized == other.canonicalized\r\n```\r\n\r\nWhat is the reason the comparison should be related to the deterministic when we want to drop it? ",
        "createdAt" : "2019-07-21T03:05:06Z",
        "updatedAt" : "2019-07-21T03:05:07Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "e188388e-374e-47a8-b34a-4b6ec223363b",
        "parentId" : "1a72d9c4-2af2-499b-bcea-98c2b1728515",
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "nvm. The output only contains Attribute",
        "createdAt" : "2019-07-21T03:06:27Z",
        "updatedAt" : "2019-07-21T03:06:28Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      }
    ],
    "commit" : "1dc9aa73bf4077bf7de6ab46c7227f09d7d4ffdb",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +2323,2327 @@    val attrs = this.logicalPlan.output\n    val colsAfterDrop = attrs.filter { attr =>\n      !attr.semanticEquals(expression)\n    }.map(attr => Column(attr))\n    select(colsAfterDrop : _*)"
  },
  {
    "id" : "bef1ec16-8819-4d7e-b041-4507a07d2104",
    "prId" : 24677,
    "prUrl" : "https://github.com/apache/spark/pull/24677#pullrequestreview-245581023",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "af03d4b6-23ac-45b8-9c30-c8c167e4d2e9",
        "parentId" : null,
        "authorId" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "body" : "I'm wondering if this and the code below should be in a finally block?",
        "createdAt" : "2019-05-30T00:28:37Z",
        "updatedAt" : "2019-05-30T00:30:40Z",
        "lastEditedBy" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "tags" : [
        ]
      },
      {
        "id" : "cd3a57a7-7750-4811-9cbc-947994fef61f",
        "parentId" : "af03d4b6-23ac-45b8-9c30-c8c167e4d2e9",
        "authorId" : "e5821ab5-ed30-4285-b919-301e171bb4f8",
        "body" : "If we put it into a finally block but only catch `SparkException` then that would be wrong: If a different exception gets thrown then we would go into `case None`, end the stream as if nothing happened and only get partial, incorrect data on the python side.\r\nIf we want to put this into a finally block then we should catch all exceptions but I figured I'd do the same as in https://github.com/apache/spark/pull/24070/files#r279589039\r\n\r\nIt should be fine as is, if any exception that isn't a `SparkException` gets thrown then we will never reach this code. Instead the `OutputStream` just gets closed and we get an `EofError` on the python side (like we do right now for all Exceptions).",
        "createdAt" : "2019-05-30T13:19:20Z",
        "updatedAt" : "2019-05-30T13:19:20Z",
        "lastEditedBy" : "e5821ab5-ed30-4285-b919-301e171bb4f8",
        "tags" : [
        ]
      },
      {
        "id" : "ecdd5b62-3910-47c9-846b-bca58263c081",
        "parentId" : "af03d4b6-23ac-45b8-9c30-c8c167e4d2e9",
        "authorId" : "e5821ab5-ed30-4285-b919-301e171bb4f8",
        "body" : "Any more thoughts on this @BryanCutler ?",
        "createdAt" : "2019-06-04T16:41:21Z",
        "updatedAt" : "2019-06-04T16:41:21Z",
        "lastEditedBy" : "e5821ab5-ed30-4285-b919-301e171bb4f8",
        "tags" : [
        ]
      },
      {
        "id" : "d37dc908-f60f-4b27-b4e3-f2180def7042",
        "parentId" : "af03d4b6-23ac-45b8-9c30-c8c167e4d2e9",
        "authorId" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "body" : "Yeah, I think this is fine",
        "createdAt" : "2019-06-04T17:05:37Z",
        "updatedAt" : "2019-06-04T17:05:37Z",
        "lastEditedBy" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "tags" : [
        ]
      }
    ],
    "commit" : "ccfeb9e408b3fd804c0f68308da7ca0adf3094b5",
    "line" : 32,
    "diffHunk" : "@@ -1,1 +3327,3331 @@\n        // After processing all partitions, end the batch stream\n        batchWriter.end()\n        sparkException match {\n          case Some(exception) =>"
  },
  {
    "id" : "c4481802-0705-4509-a141-1f7c8f9d12f6",
    "prId" : 24677,
    "prUrl" : "https://github.com/apache/spark/pull/24677#pullrequestreview-247424003",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b7f63a93-c2a1-4f99-bcca-3fb0b215703e",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Sorry for late response, @BryanCutler \r\n\r\nYea, I had the same question https://github.com/apache/spark/pull/24677#issuecomment-494879041. Thanks for details at https://github.com/apache/spark/pull/24677#issuecomment-494884105. It would have been better if those are commented since at least two committers raised the same questions :-).\r\n\r\n> @HyukjinKwon do you think would it make sense to patch branch-2.4 with a manual fix?\r\n\r\nYea, I don't mind backporting it (don't strongly feel we should do too).",
        "createdAt" : "2019-06-10T01:11:28Z",
        "updatedAt" : "2019-06-10T01:11:28Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "ccfeb9e408b3fd804c0f68308da7ca0adf3094b5",
    "line" : 43,
    "diffHunk" : "@@ -1,1 +3332,3336 @@            // Signal failure and write error message\n            out.writeInt(-1)\n            PythonRDD.writeUTF(exception.getMessage, out)\n          case None =>\n            // Write batch order indices"
  },
  {
    "id" : "0016ff81-23fc-4704-8022-339f1acd8fcc",
    "prId" : 24619,
    "prUrl" : "https://github.com/apache/spark/pull/24619#pullrequestreview-238632011",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c2f51bc0-34c3-41e3-8f61-5560244928eb",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "This is good because this prevents the exception type change. I'm wondering if we need to propagate `cause` here  because previously we didn't. And, for the other instance in this PR, the exception messages are not the same.",
        "createdAt" : "2019-05-16T16:25:51Z",
        "updatedAt" : "2019-05-16T17:51:26Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "763b6827-2eb5-4bbf-8026-deb24d1ed013",
        "parentId" : "c2f51bc0-34c3-41e3-8f61-5560244928eb",
        "authorId" : "2d6b46ba-4100-4c4d-9341-fff5e39647ec",
        "body" : "> I'm wondering if we need to propagate `cause` here because previously we didn't.\r\n\r\nPreviously it was just a `null` return value and there was no `cause`. Since we now get a `cause`, it's better to propagate it so that the error will be clearer.\r\n\r\n> And, for the other instance in this PR, the exception messages are not the same.\r\n\r\nThis is something we need to pay to unify all usages. At least, the exception type doesn't change.",
        "createdAt" : "2019-05-16T17:50:35Z",
        "updatedAt" : "2019-05-16T17:51:26Z",
        "lastEditedBy" : "2d6b46ba-4100-4c4d-9341-fff5e39647ec",
        "tags" : [
        ]
      },
      {
        "id" : "58518d57-fe15-463b-96c1-f1fe536f6e81",
        "parentId" : "c2f51bc0-34c3-41e3-8f61-5560244928eb",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Got it. Thanks!",
        "createdAt" : "2019-05-16T20:52:36Z",
        "updatedAt" : "2019-05-16T20:52:36Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "6e4b73332137fe81d285707f15d43bbcc9432763",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +702,706 @@          throw new AnalysisException(\n            s\"Unable to parse time delay '$delayThreshold'\",\n            cause = Some(e))\n      }\n    require(parsedDelay.milliseconds >= 0 && parsedDelay.months >= 0,"
  }
]