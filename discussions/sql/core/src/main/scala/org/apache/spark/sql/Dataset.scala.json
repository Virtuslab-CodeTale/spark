[
  {
    "id" : "52ab4da5-f523-4bad-af51-58a62be89dd8",
    "prId" : 33422,
    "prUrl" : "https://github.com/apache/spark/pull/33422#pullrequestreview-711175439",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f05dcd96-fecd-41e9-8bcb-c265662bfb28",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "if it's simply adding an annotation `@varargs`, shall we just do it in this PR?",
        "createdAt" : "2021-07-20T13:03:39Z",
        "updatedAt" : "2021-07-20T13:03:39Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "eddad481-aaca-48a0-9ba6-c7472955447c",
        "parentId" : "f05dcd96-fecd-41e9-8bcb-c265662bfb28",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "oh yeah!",
        "createdAt" : "2021-07-20T13:16:47Z",
        "updatedAt" : "2021-07-20T13:16:47Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "7f3ddf1d-74bc-49a9-93dc-f9dc40cc671f",
        "parentId" : "f05dcd96-fecd-41e9-8bcb-c265662bfb28",
        "authorId" : "c4ecf4e4-1fda-4fed-9980-aed6c3b3e8e4",
        "body" : "what about `observe(String, Column, Column*)`?",
        "createdAt" : "2021-07-20T18:38:39Z",
        "updatedAt" : "2021-07-20T18:38:39Z",
        "lastEditedBy" : "c4ecf4e4-1fda-4fed-9980-aed6c3b3e8e4",
        "tags" : [
        ]
      },
      {
        "id" : "d1f55634-356f-4ede-8bd5-21d3ebad3455",
        "parentId" : "f05dcd96-fecd-41e9-8bcb-c265662bfb28",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "That's fine. Let's don't add it for now",
        "createdAt" : "2021-07-21T00:59:04Z",
        "updatedAt" : "2021-07-21T00:59:05Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "3b2d9ae8800b639e91cc53a44d662c7f72e0157b",
    "line" : 34,
    "diffHunk" : "@@ -1,1 +1971,1975 @@   */\n  @varargs\n  def observe(observation: Observation, expr: Column, exprs: Column*): Dataset[T] = {\n    observation.on(this, expr, exprs: _*)\n  }"
  },
  {
    "id" : "b6bd7010-4131-4e0e-91f8-8a2c7a45f66a",
    "prId" : 32963,
    "prUrl" : "https://github.com/apache/spark/pull/32963#pullrequestreview-689002669",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "349bf8c0-d53a-414a-ad73-f714f7ae8002",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Could you add a test coverage for this, @beliefer  ?",
        "createdAt" : "2021-06-19T17:41:58Z",
        "updatedAt" : "2021-06-19T17:41:58Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "417668fa-0d41-4131-9029-a50804f04c47",
        "parentId" : "349bf8c0-d53a-414a-ad73-f714f7ae8002",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "+1, this API has no test at all, we can add one in `DataFrameSuite`",
        "createdAt" : "2021-06-21T03:37:25Z",
        "updatedAt" : "2021-06-21T03:37:32Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "b8c1d7af-74e9-4840-bc3b-689783e5eb7d",
        "parentId" : "349bf8c0-d53a-414a-ad73-f714f7ae8002",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "Thank you for reminder.",
        "createdAt" : "2021-06-22T02:19:13Z",
        "updatedAt" : "2021-06-22T02:19:13Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "4b2a05533230110f2cc2940ecf0d91f6842ac4aa",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +599,603 @@   */\n  def isLocal: Boolean = logicalPlan.isInstanceOf[LocalRelation] ||\n    logicalPlan.isInstanceOf[CommandResult]\n\n  /**"
  },
  {
    "id" : "462ff7e8-4381-498d-9d5a-b8e7533bab2d",
    "prId" : 32863,
    "prUrl" : "https://github.com/apache/spark/pull/32863#pullrequestreview-681515717",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "53f375d0-2e01-4532-91c0-98ed44fc61ad",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "is this code repeated somewhere? and can we move it to a function to share code?",
        "createdAt" : "2021-06-11T05:50:02Z",
        "updatedAt" : "2021-06-11T05:50:02Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "f7d083c4-2353-4f04-85d7-55d059498384",
        "parentId" : "53f375d0-2e01-4532-91c0-98ed44fc61ad",
        "authorId" : "87a71d79-d4d2-4f2c-8e9a-be56cf39b035",
        "body" : "yes, it's repeated from join function, I have created a new function and replaced duplicated code with it",
        "createdAt" : "2021-06-11T07:09:18Z",
        "updatedAt" : "2021-06-11T07:09:18Z",
        "lastEditedBy" : "87a71d79-d4d2-4f2c-8e9a-be56cf39b035",
        "tags" : [
        ]
      }
    ],
    "commit" : "afd327952c795d6ab639b4fb4fc50eaaa5ddeb80",
    "line" : 73,
    "diffHunk" : "@@ -1,1 +1170,1174 @@\n    // If auto self join alias is enable\n    if (sqlContext.conf.dataFrameSelfJoinAutoResolveAmbiguity) {\n      joined = resolveSelfJoinCondition(joined)\n    }"
  },
  {
    "id" : "f87ad936-8cd4-4f08-8b6e-5cbb17b84776",
    "prId" : 32616,
    "prUrl" : "https://github.com/apache/spark/pull/32616#pullrequestreview-665118671",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "537b3642-933c-4b1e-8414-610b04280c29",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Q: Is there the possibility that the set will continue to increase in a long-running application?",
        "createdAt" : "2021-05-21T02:46:37Z",
        "updatedAt" : "2021-05-21T02:46:38Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "4cec5ddc-d2cc-46cf-a213-b89f82d821b2",
        "parentId" : "537b3642-933c-4b1e-8414-610b04280c29",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Looks no. Even in streaming query, it just reused the existing plan to run the query rather than re-transform the whole query.",
        "createdAt" : "2021-05-21T06:12:25Z",
        "updatedAt" : "2021-05-21T06:12:26Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "423f2b5ee2cd63bc4d0853305f7219d7891a1848",
    "line" : 27,
    "diffHunk" : "@@ -1,1 +234,238 @@    if (sparkSession.sessionState.conf.getConf(SQLConf.FAIL_AMBIGUOUS_SELF_JOIN_ENABLED)) {\n      val dsIds = plan.getTagValue(Dataset.DATASET_ID_TAG).getOrElse(new HashSet[Long])\n      dsIds.add(id)\n      plan.setTagValue(Dataset.DATASET_ID_TAG, dsIds)\n    }"
  },
  {
    "id" : "0a165980-e2da-4b79-a28c-65133fddb4ea",
    "prId" : 32032,
    "prUrl" : "https://github.com/apache/spark/pull/32032#pullrequestreview-632557373",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "55378630-8649-46e2-909d-6527113611a8",
        "parentId" : null,
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "Since `logicalPlan` is already analyzed, I am setting this to `true` here.",
        "createdAt" : "2021-04-09T15:49:00Z",
        "updatedAt" : "2021-04-14T03:19:24Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "111ef8be8bc03a62389aed4f0871b958714eb789",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +3375,3379 @@      replace = replace,\n      viewType = viewType,\n      isAnalyzed = true)\n  }\n"
  },
  {
    "id" : "4166e96b-b38f-4808-87d5-490e8c74d008",
    "prId" : 31905,
    "prUrl" : "https://github.com/apache/spark/pull/31905#pullrequestreview-709669124",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9f2d0ce8-6c1b-47b6-b0c1-b53a4a49c4b9",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "The test failure is likely from Java doc generation from these references. You'd have to wrap it with backticks for the references Java doc build cannot identify in Unidoc.",
        "createdAt" : "2021-07-19T10:36:23Z",
        "updatedAt" : "2021-07-19T10:36:23Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "c8182ae1-e5c0-4e9d-866b-3d588bf19149",
        "parentId" : "9f2d0ce8-6c1b-47b6-b0c1-b53a4a49c4b9",
        "authorId" : "c4ecf4e4-1fda-4fed-9980-aed6c3b3e8e4",
        "body" : "Can you please point me to the error and the command so I can reproduce this locally? Otherwise I wouldn't know if I am fixing the right thing.",
        "createdAt" : "2021-07-19T11:45:38Z",
        "updatedAt" : "2021-07-19T11:45:38Z",
        "lastEditedBy" : "c4ecf4e4-1fda-4fed-9980-aed6c3b3e8e4",
        "tags" : [
        ]
      },
      {
        "id" : "8e55f92a-4f01-4bda-9f48-2a0d4406f92b",
        "parentId" : "9f2d0ce8-6c1b-47b6-b0c1-b53a4a49c4b9",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I think you can reproduce it via `./build/sbt unidoc`. ",
        "createdAt" : "2021-07-19T12:30:08Z",
        "updatedAt" : "2021-07-19T12:30:08Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "f288019e-ad97-4eec-87c9-877e3bf8709c",
        "parentId" : "9f2d0ce8-6c1b-47b6-b0c1-b53a4a49c4b9",
        "authorId" : "c4ecf4e4-1fda-4fed-9980-aed6c3b3e8e4",
        "body" : "I tried that one, but it succeeds. I am now trying `./dev/run-tests --parallelism 2 --modules \"sql\" --excluded-tags \"org.apache.spark.tags.ExtendedSQLTest\"` as in https://github.com/apache/spark/runs/3102419062#step:9:5.",
        "createdAt" : "2021-07-19T12:49:04Z",
        "updatedAt" : "2021-07-19T12:49:04Z",
        "lastEditedBy" : "c4ecf4e4-1fda-4fed-9980-aed6c3b3e8e4",
        "tags" : [
        ]
      },
      {
        "id" : "29385bed-2941-45b7-9028-81924a2b0798",
        "parentId" : "9f2d0ce8-6c1b-47b6-b0c1-b53a4a49c4b9",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "hm, feel free to open a PR and leverage GitHub Actions build",
        "createdAt" : "2021-07-19T13:39:07Z",
        "updatedAt" : "2021-07-19T13:39:07Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "aeacbb4f-3477-4f98-8440-b711d77951e0",
        "parentId" : "9f2d0ce8-6c1b-47b6-b0c1-b53a4a49c4b9",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "oh btw some errors from unidoc are fake. I think the real errors are:\r\n\r\n```\r\n[error] /home/runner/work/spark/spark/sql/core/target/java/org/apache/spark/sql/Dataset.java:1800:1:  error: unexpected text\r\n[error]    * This is equivalent to calling {@link Dataset.observe(String, Column, Column*)} but does\r\n[error]                                    ^\r\n[error] /home/runner/work/spark/spark/sql/core/target/java/org/apache/spark/sql/Dataset.java:1804:1:  error: reference not found\r\n[error]    * A user can retrieve the metrics by accessing {@link org.apache.spark.sql.Observation.get}.\r\n[error]                                                          ^\r\n[error] /home/runner/work/spark/spark/sql/core/target/java/org/apache/spark/sql/Observation.java:51:1:  error: exception not thrown: java.lang.InterruptedException\r\n[error]    * @throws java.lang.InterruptedException interrupted while waiting\r\n[error]              ^\r\n```",
        "createdAt" : "2021-07-19T13:42:16Z",
        "updatedAt" : "2021-07-19T13:42:16Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "ff3702d6-8bbf-456f-a2a4-3949e7f5ddf3",
        "parentId" : "9f2d0ce8-6c1b-47b6-b0c1-b53a4a49c4b9",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "you might have to just wrap these three instances with backtics instead of `[[..]]` as a workaround.",
        "createdAt" : "2021-07-19T13:42:47Z",
        "updatedAt" : "2021-07-19T13:42:47Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "138a59b1-8e8f-48fc-980a-16ddb207585f",
        "parentId" : "9f2d0ce8-6c1b-47b6-b0c1-b53a4a49c4b9",
        "authorId" : "c4ecf4e4-1fda-4fed-9980-aed6c3b3e8e4",
        "body" : "Done. How is this merged into master, from another PR?",
        "createdAt" : "2021-07-19T14:56:47Z",
        "updatedAt" : "2021-07-19T14:56:47Z",
        "lastEditedBy" : "c4ecf4e4-1fda-4fed-9980-aed6c3b3e8e4",
        "tags" : [
        ]
      },
      {
        "id" : "f5b93381-8252-45bb-bf92-be6b7f3c6e3f",
        "parentId" : "9f2d0ce8-6c1b-47b6-b0c1-b53a4a49c4b9",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Yes, another PR would be cleaner because this PR seems to have a long history since March.. You can put the link of new PR here, @EnricoMi .",
        "createdAt" : "2021-07-19T15:00:01Z",
        "updatedAt" : "2021-07-19T15:00:41Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "fc5c90c7-2683-4292-9ae0-cb30d5df060a",
        "parentId" : "9f2d0ce8-6c1b-47b6-b0c1-b53a4a49c4b9",
        "authorId" : "c4ecf4e4-1fda-4fed-9980-aed6c3b3e8e4",
        "body" : "Done, thanks!",
        "createdAt" : "2021-07-19T15:11:03Z",
        "updatedAt" : "2021-07-19T15:11:03Z",
        "lastEditedBy" : "c4ecf4e4-1fda-4fed-9980-aed6c3b3e8e4",
        "tags" : [
        ]
      }
    ],
    "commit" : "ec7f7275ca7902db83aef86d174890bd1e240fe7",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1949,1953 @@\n  /**\n   * Observe (named) metrics through an [[org.apache.spark.sql.Observation]] instance.\n   * This is equivalent to calling [[Dataset.observe(String, Column, Column*)]] but does\n   * not require adding [[org.apache.spark.sql.util.QueryExecutionListener]] to the spark session."
  },
  {
    "id" : "b258705b-2c73-448b-bc64-6dec394b8edb",
    "prId" : 31700,
    "prUrl" : "https://github.com/apache/spark/pull/31700#pullrequestreview-603177943",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "900a4bbc-85d8-4b82-96a5-10445d76f337",
        "parentId" : null,
        "authorId" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "body" : "This bit is needed to prevent executing each batch early. We used `WriteToDataSourceV2` before, which did not extend `Command`.",
        "createdAt" : "2021-03-03T17:48:37Z",
        "updatedAt" : "2021-03-03T17:48:37Z",
        "lastEditedBy" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "tags" : [
        ]
      }
    ],
    "commit" : "e7f167fb0103d7e62127421cf5ad2a594d5f4337",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +225,229 @@    // to happen right away to let these side effects take place eagerly.\n    val plan = queryExecution.analyzed match {\n      case _: V2MicroBatchWriteCommand =>\n        queryExecution.analyzed\n      case c: Command =>"
  },
  {
    "id" : "8ffc264d-f77e-4f0b-a5ea-3c177b4b63bd",
    "prId" : 31296,
    "prUrl" : "https://github.com/apache/spark/pull/31296#pullrequestreview-574581181",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "be5f28fe-ab33-4a9f-a70e-bc43da980261",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "This sounds a little risky to me, `A process is invoked even for empty partitions`.\r\nThis may cause a hang situation if the command is expecting input.\r\nFor example, this PR's test case is using `cat`. And, `cat | wc -l` hangs.\r\nIf we are okay, could you add a test case of empty partition to make it sure that we handle those cases?",
        "createdAt" : "2021-01-22T19:46:20Z",
        "updatedAt" : "2021-01-26T08:19:19Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "43fdbdd5-2d91-41f5-bb54-dc97dddba7db",
        "parentId" : "be5f28fe-ab33-4a9f-a70e-bc43da980261",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Otherwise, you may want to add some warnings here.",
        "createdAt" : "2021-01-22T19:47:00Z",
        "updatedAt" : "2021-01-26T08:19:19Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "4720812f-396b-4e8d-82cb-b10b41160b4e",
        "parentId" : "be5f28fe-ab33-4a9f-a70e-bc43da980261",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "This is a good point. As you see this uses RDD.pipe, let me check if it addresses this or not. And yeah I will add a test case too.",
        "createdAt" : "2021-01-22T20:06:58Z",
        "updatedAt" : "2021-01-26T08:19:19Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "ac7460b0431e7012f17c3ac293226bb4f428bd15",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +2896,2900 @@   * separated by a newline. The resulting partition consists of the process's stdout output, with\n   * each line of stdout resulting in one element of the output partition. A process is invoked\n   * even for empty partitions.\n   *\n   * Note that for micro-batch streaming Dataset, the effect of pipe is only per micro-batch, not"
  },
  {
    "id" : "4f7bfd83-ebef-42ce-8996-0848bcd8e7d0",
    "prId" : 31296,
    "prUrl" : "https://github.com/apache/spark/pull/31296#pullrequestreview-576113504",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a0097328-eb51-4da1-838b-84837840acae",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "I see all examples are simply calling print function with converted string. Could we simply get serializer func like `serializeFn: (T => String)` instead, or have two overloaded methods allowing both cases if we are unsure printElement might be necessary in some cases? This should simplify the test codes and actual user codes (as `_.toString` would simply work).",
        "createdAt" : "2021-01-25T09:19:47Z",
        "updatedAt" : "2021-01-26T08:19:19Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "7142ce92-c25a-46b6-8b22-9fc0ee120ff6",
        "parentId" : "a0097328-eb51-4da1-838b-84837840acae",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "I think it is okay. Most cases there should be no difference. Only difference might be when we want to print out multi-lines per obj:\r\n\r\n```scala\r\ndef printElement(obj: T, printFunc: String => Unit) = {\r\n  printFunc(obj.a)\r\n  printFunc(obj.b)\r\n  ...\r\n}\r\n```\r\n\r\n```scala\r\ndef serializeFn(obj: T): String = {\r\n  s\"${obj.a}\\n${obj.b}\\n...\"\r\n}\r\n```\r\n\r\nI'm fine with either one as they are working the same effect although taking different form.\r\n\r\n",
        "createdAt" : "2021-01-26T08:24:52Z",
        "updatedAt" : "2021-01-26T08:24:52Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "ac7460b0431e7012f17c3ac293226bb4f428bd15",
    "line" : 25,
    "diffHunk" : "@@ -1,1 +2911,2915 @@   * @since 3.2.0\n   */\n  def pipe(command: String, printElement: (T, String => Unit) => Unit): Dataset[String] = {\n    implicit val stringEncoder = Encoders.STRING\n    withTypedPlan[String](PipeElements[T]("
  },
  {
    "id" : "2f3f1c1f-2639-44b4-8fbf-d09a8bbeace4",
    "prId" : 31296,
    "prUrl" : "https://github.com/apache/spark/pull/31296#pullrequestreview-575197994",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "187211e1-3614-4ec0-b231-a2b4ab2016b6",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "I'd kindly explain the case they need to be careful, like `e.g. If your external process does aggregation on inputs, the aggregation is applied per a partition in micro-batch. You may want to aggregate these outputs after calling pipe to get global aggregation across partitions and also across micro-batches.`",
        "createdAt" : "2021-01-25T09:27:53Z",
        "updatedAt" : "2021-01-26T08:19:19Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "ac7460b0431e7012f17c3ac293226bb4f428bd15",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +2898,2902 @@   * even for empty partitions.\n   *\n   * Note that for micro-batch streaming Dataset, the effect of pipe is only per micro-batch, not\n   * cross entire stream. If your external process does aggregation-like on inputs, e.g. `wc -l`,\n   * the aggregation is applied per a partition in micro-batch. You may want to aggregate these"
  },
  {
    "id" : "b67e1fb4-1551-4884-9e12-e381d63e1660",
    "prId" : 31254,
    "prUrl" : "https://github.com/apache/spark/pull/31254#pullrequestreview-577395242",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "de53d169-aa76-4ffe-8dd7-ea7728827998",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I am not sure how common it is to show distinct count. I believe this method was inspired by [`pandas.DataFrame.describe()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.describe.html) which however does not include distinct count.\r\n\r\nBTW, I think you should update the examples below too. ",
        "createdAt" : "2021-01-20T04:56:05Z",
        "updatedAt" : "2021-01-20T17:57:30Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "cb1917d2-bef4-4e79-baf2-a4abc1ba2fba",
        "parentId" : "de53d169-aa76-4ffe-8dd7-ea7728827998",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I wouldn't add this if there isn't any strong reason for it because you're already able to do this via `agg`.",
        "createdAt" : "2021-01-20T04:56:52Z",
        "updatedAt" : "2021-01-20T17:57:30Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "df47984b-ef86-4191-b970-046c7290fa6f",
        "parentId" : "de53d169-aa76-4ffe-8dd7-ea7728827998",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Meh, I think this could be useful as an _option_; it doesn't really expand the API surface or complexity. I'm OK with adding it, FWIW.",
        "createdAt" : "2021-01-20T11:24:50Z",
        "updatedAt" : "2021-01-20T17:57:30Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "141f1773-6613-41ac-b01a-b0e55c6c9db7",
        "parentId" : "de53d169-aa76-4ffe-8dd7-ea7728827998",
        "authorId" : "981b170c-729a-429c-b115-0350ea50b32b",
        "body" : "Wouldn't be more useful to expose approx variants here? ",
        "createdAt" : "2021-01-20T15:08:39Z",
        "updatedAt" : "2021-01-20T17:57:30Z",
        "lastEditedBy" : "981b170c-729a-429c-b115-0350ea50b32b",
        "tags" : [
        ]
      },
      {
        "id" : "8b6de835-30ec-42a5-bd44-4d14d4b047ed",
        "parentId" : "de53d169-aa76-4ffe-8dd7-ea7728827998",
        "authorId" : "e1bdd418-d9b9-4129-91a0-0865740d24d2",
        "body" : "@zero323 - I updated the PR to expose `approx_count_distinct` as well, good point.\r\n\r\n[One SO question on this](https://stackoverflow.com/questions/37949494/how-to-count-occurrences-of-each-distinct-value-for-every-column-in-a-dataframe) has 81,000 views and [the other](https://stackoverflow.com/questions/40888946/spark-dataframe-count-distinct-values-of-every-column) has 66,000 views.  There is a high demand for count distinct / approx_count_distinct exploratory data analysis queries by columns among the Spark userbase.\r\n\r\nThese options don't cost us much and should help the less technical Spark users that aren't comfortable using `agg`.  I help lots of folks that can't even program and they aren't code geniuses like you all ;)",
        "createdAt" : "2021-01-20T19:14:30Z",
        "updatedAt" : "2021-01-20T19:14:30Z",
        "lastEditedBy" : "e1bdd418-d9b9-4129-91a0-0865740d24d2",
        "tags" : [
        ]
      },
      {
        "id" : "4f24e799-be55-4d44-b66b-d4e9d0051afe",
        "parentId" : "de53d169-aa76-4ffe-8dd7-ea7728827998",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I am not sure. You can all have what you want via `agg`. This is just a shortcut to show basic stats, which was inspired by pandas' one, see https://github.com/apache/spark/commit/e1a172c201d68406faa53b113518b10c879f1ff6. We should of course avoid showing every stats here.\r\n\r\nYou can easily do `df.agg(\"col\" -> \"approx_count_distinct\", ...)` easily for other stats. Adding `count_distinct` here makes it ambiguous what to add in `DataFrame.describe` and `DataFrame.summary`.\r\n\r\n",
        "createdAt" : "2021-01-21T01:47:25Z",
        "updatedAt" : "2021-01-21T01:47:25Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "fc0352b6-15c6-411e-a166-4b185eabcbe0",
        "parentId" : "de53d169-aa76-4ffe-8dd7-ea7728827998",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "It's true, though, users looking for summary stats are probably looking at summary(), and sometimes looking for distinct values, and this puts them into one output with the others. I think it's slightly worth adding one more somewhat-common summary stat. ",
        "createdAt" : "2021-01-21T01:58:44Z",
        "updatedAt" : "2021-01-21T01:58:44Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "2802b13b-db2d-4439-ba7d-d1ae9c2777bb",
        "parentId" : "de53d169-aa76-4ffe-8dd7-ea7728827998",
        "authorId" : "981b170c-729a-429c-b115-0350ea50b32b",
        "body" : "> I am not sure. You can all have what you want via `agg`\r\n\r\nWhich is also more suitable for anything but quick visual inspection.\r\n\r\nMy biggest concern is somewhat different though â€’ existing summaries have good performance and predictable behavior. In contrast count distinct can be unstable and approx count distinct slower and more resource hungry. \r\n",
        "createdAt" : "2021-01-21T13:46:45Z",
        "updatedAt" : "2021-01-21T13:46:46Z",
        "lastEditedBy" : "981b170c-729a-429c-b115-0350ea50b32b",
        "tags" : [
        ]
      },
      {
        "id" : "a40041b1-2475-4ef1-87d9-6027f098a794",
        "parentId" : "de53d169-aa76-4ffe-8dd7-ea7728827998",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Sure, but this is only computed if the caller requests it.",
        "createdAt" : "2021-01-21T14:13:50Z",
        "updatedAt" : "2021-01-21T14:13:50Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "348bba8e-e2fd-45ed-81b3-c98d38338420",
        "parentId" : "de53d169-aa76-4ffe-8dd7-ea7728827998",
        "authorId" : "e1bdd418-d9b9-4129-91a0-0865740d24d2",
        "body" : "@HyukjinKwon - you raise a good point about API ambiguity between `Dataset.describe` and `Dataset.summary`, but those ambiguities already exist.  `df.describe(\"some_col\")`, `df.select(\"some_col\").summary()`, and `df.select(\"some_col\").summary(\"37%\", \"87%\")` all return different results.\r\n\r\n`describe` is a \"partial alias\" of `summary`, but the method signatures are different.\r\n\r\nThe Spark & Pandas `describe` methods aren't consistent either.  The Spark `describe` method takes column name arguments whereas the Pandas `describe` method takes options, see the Pandas method signature: `DataFrame.describe(percentiles=None, include=None, exclude=None, datetime_is_numeric=False)`.\r\n\r\nA Pandas user is already going to be confused by the API.  They're going to run `df.describe(percentiles=Array(.25, .5, .75))` and have to Google around to find that they need to run `df.summary(\"0.25%\", \"0.5%\", \"0.75%\")`.  \r\n\r\nAdding additional options to `Dataset.summary` seems consistent with the existing design.  The `summary` method is already where we put the extra metrics that aren't enabled by default, like the option to add arbitrary percentiles.\r\n\r\nBTW - I don't mean to complain here ;). It's easy for me to make comments on API decisions made years ago from folks in the trenches.  You are all doing a wonderful job maintaining this project and continuously moving it in a good direction.  I'm always impressed by the code you write and will hopefully get to your level some day.  My PR comments sound critical cause I'm just trying to get to the point.  I'm very grateful for all the work you've done on this project.",
        "createdAt" : "2021-01-26T04:42:07Z",
        "updatedAt" : "2021-01-26T04:42:08Z",
        "lastEditedBy" : "e1bdd418-d9b9-4129-91a0-0865740d24d2",
        "tags" : [
        ]
      },
      {
        "id" : "7b0f371c-71e5-4cc7-b680-138a57a159c8",
        "parentId" : "de53d169-aa76-4ffe-8dd7-ea7728827998",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "I would favor this change. If there aren't significant objections, I'd like to merge it.",
        "createdAt" : "2021-01-27T00:35:50Z",
        "updatedAt" : "2021-01-27T00:35:51Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "3126bfa1-6142-4c83-ae16-3c48ff59c5d4",
        "parentId" : "de53d169-aa76-4ffe-8dd7-ea7728827998",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I'm okay given that we don't change the default.",
        "createdAt" : "2021-01-27T04:14:14Z",
        "updatedAt" : "2021-01-27T04:14:14Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "42517638-ff3f-43bb-9223-0cab19b089af",
        "parentId" : "de53d169-aa76-4ffe-8dd7-ea7728827998",
        "authorId" : "981b170c-729a-429c-b115-0350ea50b32b",
        "body" : "Sounds good.",
        "createdAt" : "2021-01-27T14:31:10Z",
        "updatedAt" : "2021-01-27T14:31:10Z",
        "lastEditedBy" : "981b170c-729a-429c-b115-0350ea50b32b",
        "tags" : [
        ]
      }
    ],
    "commit" : "bc547a2c949dba8322378a334d8f7221e46e3784",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +2671,2675 @@   *   <li>max</li>\n   *   <li>arbitrary approximate percentiles specified as a percentage (e.g. 75%)</li>\n   *   <li>count_distinct</li>\n   *   <li>approx_count_distinct</li>\n   * </ul>"
  }
]