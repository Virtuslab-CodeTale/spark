[
  {
    "id" : "52ab4da5-f523-4bad-af51-58a62be89dd8",
    "prId" : 33422,
    "prUrl" : "https://github.com/apache/spark/pull/33422#pullrequestreview-711175439",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f05dcd96-fecd-41e9-8bcb-c265662bfb28",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "if it's simply adding an annotation `@varargs`, shall we just do it in this PR?",
        "createdAt" : "2021-07-20T13:03:39Z",
        "updatedAt" : "2021-07-20T13:03:39Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "eddad481-aaca-48a0-9ba6-c7472955447c",
        "parentId" : "f05dcd96-fecd-41e9-8bcb-c265662bfb28",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "oh yeah!",
        "createdAt" : "2021-07-20T13:16:47Z",
        "updatedAt" : "2021-07-20T13:16:47Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "7f3ddf1d-74bc-49a9-93dc-f9dc40cc671f",
        "parentId" : "f05dcd96-fecd-41e9-8bcb-c265662bfb28",
        "authorId" : "c4ecf4e4-1fda-4fed-9980-aed6c3b3e8e4",
        "body" : "what about `observe(String, Column, Column*)`?",
        "createdAt" : "2021-07-20T18:38:39Z",
        "updatedAt" : "2021-07-20T18:38:39Z",
        "lastEditedBy" : "c4ecf4e4-1fda-4fed-9980-aed6c3b3e8e4",
        "tags" : [
        ]
      },
      {
        "id" : "d1f55634-356f-4ede-8bd5-21d3ebad3455",
        "parentId" : "f05dcd96-fecd-41e9-8bcb-c265662bfb28",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "That's fine. Let's don't add it for now",
        "createdAt" : "2021-07-21T00:59:04Z",
        "updatedAt" : "2021-07-21T00:59:05Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "3b2d9ae8800b639e91cc53a44d662c7f72e0157b",
    "line" : 34,
    "diffHunk" : "@@ -1,1 +1971,1975 @@   */\n  @varargs\n  def observe(observation: Observation, expr: Column, exprs: Column*): Dataset[T] = {\n    observation.on(this, expr, exprs: _*)\n  }"
  },
  {
    "id" : "b6bd7010-4131-4e0e-91f8-8a2c7a45f66a",
    "prId" : 32963,
    "prUrl" : "https://github.com/apache/spark/pull/32963#pullrequestreview-689002669",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "349bf8c0-d53a-414a-ad73-f714f7ae8002",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Could you add a test coverage for this, @beliefer  ?",
        "createdAt" : "2021-06-19T17:41:58Z",
        "updatedAt" : "2021-06-19T17:41:58Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "417668fa-0d41-4131-9029-a50804f04c47",
        "parentId" : "349bf8c0-d53a-414a-ad73-f714f7ae8002",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "+1, this API has no test at all, we can add one in `DataFrameSuite`",
        "createdAt" : "2021-06-21T03:37:25Z",
        "updatedAt" : "2021-06-21T03:37:32Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "b8c1d7af-74e9-4840-bc3b-689783e5eb7d",
        "parentId" : "349bf8c0-d53a-414a-ad73-f714f7ae8002",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "Thank you for reminder.",
        "createdAt" : "2021-06-22T02:19:13Z",
        "updatedAt" : "2021-06-22T02:19:13Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "4b2a05533230110f2cc2940ecf0d91f6842ac4aa",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +599,603 @@   */\n  def isLocal: Boolean = logicalPlan.isInstanceOf[LocalRelation] ||\n    logicalPlan.isInstanceOf[CommandResult]\n\n  /**"
  },
  {
    "id" : "462ff7e8-4381-498d-9d5a-b8e7533bab2d",
    "prId" : 32863,
    "prUrl" : "https://github.com/apache/spark/pull/32863#pullrequestreview-681515717",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "53f375d0-2e01-4532-91c0-98ed44fc61ad",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "is this code repeated somewhere? and can we move it to a function to share code?",
        "createdAt" : "2021-06-11T05:50:02Z",
        "updatedAt" : "2021-06-11T05:50:02Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "f7d083c4-2353-4f04-85d7-55d059498384",
        "parentId" : "53f375d0-2e01-4532-91c0-98ed44fc61ad",
        "authorId" : "87a71d79-d4d2-4f2c-8e9a-be56cf39b035",
        "body" : "yes, it's repeated from join function, I have created a new function and replaced duplicated code with it",
        "createdAt" : "2021-06-11T07:09:18Z",
        "updatedAt" : "2021-06-11T07:09:18Z",
        "lastEditedBy" : "87a71d79-d4d2-4f2c-8e9a-be56cf39b035",
        "tags" : [
        ]
      }
    ],
    "commit" : "afd327952c795d6ab639b4fb4fc50eaaa5ddeb80",
    "line" : 73,
    "diffHunk" : "@@ -1,1 +1170,1174 @@\n    // If auto self join alias is enable\n    if (sqlContext.conf.dataFrameSelfJoinAutoResolveAmbiguity) {\n      joined = resolveSelfJoinCondition(joined)\n    }"
  },
  {
    "id" : "f87ad936-8cd4-4f08-8b6e-5cbb17b84776",
    "prId" : 32616,
    "prUrl" : "https://github.com/apache/spark/pull/32616#pullrequestreview-665118671",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "537b3642-933c-4b1e-8414-610b04280c29",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Q: Is there the possibility that the set will continue to increase in a long-running application?",
        "createdAt" : "2021-05-21T02:46:37Z",
        "updatedAt" : "2021-05-21T02:46:38Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "4cec5ddc-d2cc-46cf-a213-b89f82d821b2",
        "parentId" : "537b3642-933c-4b1e-8414-610b04280c29",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Looks no. Even in streaming query, it just reused the existing plan to run the query rather than re-transform the whole query.",
        "createdAt" : "2021-05-21T06:12:25Z",
        "updatedAt" : "2021-05-21T06:12:26Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "423f2b5ee2cd63bc4d0853305f7219d7891a1848",
    "line" : 27,
    "diffHunk" : "@@ -1,1 +234,238 @@    if (sparkSession.sessionState.conf.getConf(SQLConf.FAIL_AMBIGUOUS_SELF_JOIN_ENABLED)) {\n      val dsIds = plan.getTagValue(Dataset.DATASET_ID_TAG).getOrElse(new HashSet[Long])\n      dsIds.add(id)\n      plan.setTagValue(Dataset.DATASET_ID_TAG, dsIds)\n    }"
  },
  {
    "id" : "0a165980-e2da-4b79-a28c-65133fddb4ea",
    "prId" : 32032,
    "prUrl" : "https://github.com/apache/spark/pull/32032#pullrequestreview-632557373",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "55378630-8649-46e2-909d-6527113611a8",
        "parentId" : null,
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "Since `logicalPlan` is already analyzed, I am setting this to `true` here.",
        "createdAt" : "2021-04-09T15:49:00Z",
        "updatedAt" : "2021-04-14T03:19:24Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "111ef8be8bc03a62389aed4f0871b958714eb789",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +3375,3379 @@      replace = replace,\n      viewType = viewType,\n      isAnalyzed = true)\n  }\n"
  },
  {
    "id" : "4166e96b-b38f-4808-87d5-490e8c74d008",
    "prId" : 31905,
    "prUrl" : "https://github.com/apache/spark/pull/31905#pullrequestreview-709669124",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9f2d0ce8-6c1b-47b6-b0c1-b53a4a49c4b9",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "The test failure is likely from Java doc generation from these references. You'd have to wrap it with backticks for the references Java doc build cannot identify in Unidoc.",
        "createdAt" : "2021-07-19T10:36:23Z",
        "updatedAt" : "2021-07-19T10:36:23Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "c8182ae1-e5c0-4e9d-866b-3d588bf19149",
        "parentId" : "9f2d0ce8-6c1b-47b6-b0c1-b53a4a49c4b9",
        "authorId" : "c4ecf4e4-1fda-4fed-9980-aed6c3b3e8e4",
        "body" : "Can you please point me to the error and the command so I can reproduce this locally? Otherwise I wouldn't know if I am fixing the right thing.",
        "createdAt" : "2021-07-19T11:45:38Z",
        "updatedAt" : "2021-07-19T11:45:38Z",
        "lastEditedBy" : "c4ecf4e4-1fda-4fed-9980-aed6c3b3e8e4",
        "tags" : [
        ]
      },
      {
        "id" : "8e55f92a-4f01-4bda-9f48-2a0d4406f92b",
        "parentId" : "9f2d0ce8-6c1b-47b6-b0c1-b53a4a49c4b9",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I think you can reproduce it via `./build/sbt unidoc`. ",
        "createdAt" : "2021-07-19T12:30:08Z",
        "updatedAt" : "2021-07-19T12:30:08Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "f288019e-ad97-4eec-87c9-877e3bf8709c",
        "parentId" : "9f2d0ce8-6c1b-47b6-b0c1-b53a4a49c4b9",
        "authorId" : "c4ecf4e4-1fda-4fed-9980-aed6c3b3e8e4",
        "body" : "I tried that one, but it succeeds. I am now trying `./dev/run-tests --parallelism 2 --modules \"sql\" --excluded-tags \"org.apache.spark.tags.ExtendedSQLTest\"` as in https://github.com/apache/spark/runs/3102419062#step:9:5.",
        "createdAt" : "2021-07-19T12:49:04Z",
        "updatedAt" : "2021-07-19T12:49:04Z",
        "lastEditedBy" : "c4ecf4e4-1fda-4fed-9980-aed6c3b3e8e4",
        "tags" : [
        ]
      },
      {
        "id" : "29385bed-2941-45b7-9028-81924a2b0798",
        "parentId" : "9f2d0ce8-6c1b-47b6-b0c1-b53a4a49c4b9",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "hm, feel free to open a PR and leverage GitHub Actions build",
        "createdAt" : "2021-07-19T13:39:07Z",
        "updatedAt" : "2021-07-19T13:39:07Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "aeacbb4f-3477-4f98-8440-b711d77951e0",
        "parentId" : "9f2d0ce8-6c1b-47b6-b0c1-b53a4a49c4b9",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "oh btw some errors from unidoc are fake. I think the real errors are:\r\n\r\n```\r\n[error] /home/runner/work/spark/spark/sql/core/target/java/org/apache/spark/sql/Dataset.java:1800:1:  error: unexpected text\r\n[error]    * This is equivalent to calling {@link Dataset.observe(String, Column, Column*)} but does\r\n[error]                                    ^\r\n[error] /home/runner/work/spark/spark/sql/core/target/java/org/apache/spark/sql/Dataset.java:1804:1:  error: reference not found\r\n[error]    * A user can retrieve the metrics by accessing {@link org.apache.spark.sql.Observation.get}.\r\n[error]                                                          ^\r\n[error] /home/runner/work/spark/spark/sql/core/target/java/org/apache/spark/sql/Observation.java:51:1:  error: exception not thrown: java.lang.InterruptedException\r\n[error]    * @throws java.lang.InterruptedException interrupted while waiting\r\n[error]              ^\r\n```",
        "createdAt" : "2021-07-19T13:42:16Z",
        "updatedAt" : "2021-07-19T13:42:16Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "ff3702d6-8bbf-456f-a2a4-3949e7f5ddf3",
        "parentId" : "9f2d0ce8-6c1b-47b6-b0c1-b53a4a49c4b9",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "you might have to just wrap these three instances with backtics instead of `[[..]]` as a workaround.",
        "createdAt" : "2021-07-19T13:42:47Z",
        "updatedAt" : "2021-07-19T13:42:47Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "138a59b1-8e8f-48fc-980a-16ddb207585f",
        "parentId" : "9f2d0ce8-6c1b-47b6-b0c1-b53a4a49c4b9",
        "authorId" : "c4ecf4e4-1fda-4fed-9980-aed6c3b3e8e4",
        "body" : "Done. How is this merged into master, from another PR?",
        "createdAt" : "2021-07-19T14:56:47Z",
        "updatedAt" : "2021-07-19T14:56:47Z",
        "lastEditedBy" : "c4ecf4e4-1fda-4fed-9980-aed6c3b3e8e4",
        "tags" : [
        ]
      },
      {
        "id" : "f5b93381-8252-45bb-bf92-be6b7f3c6e3f",
        "parentId" : "9f2d0ce8-6c1b-47b6-b0c1-b53a4a49c4b9",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Yes, another PR would be cleaner because this PR seems to have a long history since March.. You can put the link of new PR here, @EnricoMi .",
        "createdAt" : "2021-07-19T15:00:01Z",
        "updatedAt" : "2021-07-19T15:00:41Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "fc5c90c7-2683-4292-9ae0-cb30d5df060a",
        "parentId" : "9f2d0ce8-6c1b-47b6-b0c1-b53a4a49c4b9",
        "authorId" : "c4ecf4e4-1fda-4fed-9980-aed6c3b3e8e4",
        "body" : "Done, thanks!",
        "createdAt" : "2021-07-19T15:11:03Z",
        "updatedAt" : "2021-07-19T15:11:03Z",
        "lastEditedBy" : "c4ecf4e4-1fda-4fed-9980-aed6c3b3e8e4",
        "tags" : [
        ]
      }
    ],
    "commit" : "ec7f7275ca7902db83aef86d174890bd1e240fe7",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1949,1953 @@\n  /**\n   * Observe (named) metrics through an [[org.apache.spark.sql.Observation]] instance.\n   * This is equivalent to calling [[Dataset.observe(String, Column, Column*)]] but does\n   * not require adding [[org.apache.spark.sql.util.QueryExecutionListener]] to the spark session."
  },
  {
    "id" : "b258705b-2c73-448b-bc64-6dec394b8edb",
    "prId" : 31700,
    "prUrl" : "https://github.com/apache/spark/pull/31700#pullrequestreview-603177943",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "900a4bbc-85d8-4b82-96a5-10445d76f337",
        "parentId" : null,
        "authorId" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "body" : "This bit is needed to prevent executing each batch early. We used `WriteToDataSourceV2` before, which did not extend `Command`.",
        "createdAt" : "2021-03-03T17:48:37Z",
        "updatedAt" : "2021-03-03T17:48:37Z",
        "lastEditedBy" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "tags" : [
        ]
      }
    ],
    "commit" : "e7f167fb0103d7e62127421cf5ad2a594d5f4337",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +225,229 @@    // to happen right away to let these side effects take place eagerly.\n    val plan = queryExecution.analyzed match {\n      case _: V2MicroBatchWriteCommand =>\n        queryExecution.analyzed\n      case c: Command =>"
  },
  {
    "id" : "8ffc264d-f77e-4f0b-a5ea-3c177b4b63bd",
    "prId" : 31296,
    "prUrl" : "https://github.com/apache/spark/pull/31296#pullrequestreview-574581181",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "be5f28fe-ab33-4a9f-a70e-bc43da980261",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "This sounds a little risky to me, `A process is invoked even for empty partitions`.\r\nThis may cause a hang situation if the command is expecting input.\r\nFor example, this PR's test case is using `cat`. And, `cat | wc -l` hangs.\r\nIf we are okay, could you add a test case of empty partition to make it sure that we handle those cases?",
        "createdAt" : "2021-01-22T19:46:20Z",
        "updatedAt" : "2021-01-26T08:19:19Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "43fdbdd5-2d91-41f5-bb54-dc97dddba7db",
        "parentId" : "be5f28fe-ab33-4a9f-a70e-bc43da980261",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Otherwise, you may want to add some warnings here.",
        "createdAt" : "2021-01-22T19:47:00Z",
        "updatedAt" : "2021-01-26T08:19:19Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "4720812f-396b-4e8d-82cb-b10b41160b4e",
        "parentId" : "be5f28fe-ab33-4a9f-a70e-bc43da980261",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "This is a good point. As you see this uses RDD.pipe, let me check if it addresses this or not. And yeah I will add a test case too.",
        "createdAt" : "2021-01-22T20:06:58Z",
        "updatedAt" : "2021-01-26T08:19:19Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "ac7460b0431e7012f17c3ac293226bb4f428bd15",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +2896,2900 @@   * separated by a newline. The resulting partition consists of the process's stdout output, with\n   * each line of stdout resulting in one element of the output partition. A process is invoked\n   * even for empty partitions.\n   *\n   * Note that for micro-batch streaming Dataset, the effect of pipe is only per micro-batch, not"
  },
  {
    "id" : "4f7bfd83-ebef-42ce-8996-0848bcd8e7d0",
    "prId" : 31296,
    "prUrl" : "https://github.com/apache/spark/pull/31296#pullrequestreview-576113504",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a0097328-eb51-4da1-838b-84837840acae",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "I see all examples are simply calling print function with converted string. Could we simply get serializer func like `serializeFn: (T => String)` instead, or have two overloaded methods allowing both cases if we are unsure printElement might be necessary in some cases? This should simplify the test codes and actual user codes (as `_.toString` would simply work).",
        "createdAt" : "2021-01-25T09:19:47Z",
        "updatedAt" : "2021-01-26T08:19:19Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "7142ce92-c25a-46b6-8b22-9fc0ee120ff6",
        "parentId" : "a0097328-eb51-4da1-838b-84837840acae",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "I think it is okay. Most cases there should be no difference. Only difference might be when we want to print out multi-lines per obj:\r\n\r\n```scala\r\ndef printElement(obj: T, printFunc: String => Unit) = {\r\n  printFunc(obj.a)\r\n  printFunc(obj.b)\r\n  ...\r\n}\r\n```\r\n\r\n```scala\r\ndef serializeFn(obj: T): String = {\r\n  s\"${obj.a}\\n${obj.b}\\n...\"\r\n}\r\n```\r\n\r\nI'm fine with either one as they are working the same effect although taking different form.\r\n\r\n",
        "createdAt" : "2021-01-26T08:24:52Z",
        "updatedAt" : "2021-01-26T08:24:52Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "ac7460b0431e7012f17c3ac293226bb4f428bd15",
    "line" : 25,
    "diffHunk" : "@@ -1,1 +2911,2915 @@   * @since 3.2.0\n   */\n  def pipe(command: String, printElement: (T, String => Unit) => Unit): Dataset[String] = {\n    implicit val stringEncoder = Encoders.STRING\n    withTypedPlan[String](PipeElements[T]("
  },
  {
    "id" : "2f3f1c1f-2639-44b4-8fbf-d09a8bbeace4",
    "prId" : 31296,
    "prUrl" : "https://github.com/apache/spark/pull/31296#pullrequestreview-575197994",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "187211e1-3614-4ec0-b231-a2b4ab2016b6",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "I'd kindly explain the case they need to be careful, like `e.g. If your external process does aggregation on inputs, the aggregation is applied per a partition in micro-batch. You may want to aggregate these outputs after calling pipe to get global aggregation across partitions and also across micro-batches.`",
        "createdAt" : "2021-01-25T09:27:53Z",
        "updatedAt" : "2021-01-26T08:19:19Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "ac7460b0431e7012f17c3ac293226bb4f428bd15",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +2898,2902 @@   * even for empty partitions.\n   *\n   * Note that for micro-batch streaming Dataset, the effect of pipe is only per micro-batch, not\n   * cross entire stream. If your external process does aggregation-like on inputs, e.g. `wc -l`,\n   * the aggregation is applied per a partition in micro-batch. You may want to aggregate these"
  },
  {
    "id" : "b67e1fb4-1551-4884-9e12-e381d63e1660",
    "prId" : 31254,
    "prUrl" : "https://github.com/apache/spark/pull/31254#pullrequestreview-577395242",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "de53d169-aa76-4ffe-8dd7-ea7728827998",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I am not sure how common it is to show distinct count. I believe this method was inspired by [`pandas.DataFrame.describe()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.describe.html) which however does not include distinct count.\r\n\r\nBTW, I think you should update the examples below too. ",
        "createdAt" : "2021-01-20T04:56:05Z",
        "updatedAt" : "2021-01-20T17:57:30Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "cb1917d2-bef4-4e79-baf2-a4abc1ba2fba",
        "parentId" : "de53d169-aa76-4ffe-8dd7-ea7728827998",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I wouldn't add this if there isn't any strong reason for it because you're already able to do this via `agg`.",
        "createdAt" : "2021-01-20T04:56:52Z",
        "updatedAt" : "2021-01-20T17:57:30Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "df47984b-ef86-4191-b970-046c7290fa6f",
        "parentId" : "de53d169-aa76-4ffe-8dd7-ea7728827998",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Meh, I think this could be useful as an _option_; it doesn't really expand the API surface or complexity. I'm OK with adding it, FWIW.",
        "createdAt" : "2021-01-20T11:24:50Z",
        "updatedAt" : "2021-01-20T17:57:30Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "141f1773-6613-41ac-b01a-b0e55c6c9db7",
        "parentId" : "de53d169-aa76-4ffe-8dd7-ea7728827998",
        "authorId" : "981b170c-729a-429c-b115-0350ea50b32b",
        "body" : "Wouldn't be more useful to expose approx variants here? ",
        "createdAt" : "2021-01-20T15:08:39Z",
        "updatedAt" : "2021-01-20T17:57:30Z",
        "lastEditedBy" : "981b170c-729a-429c-b115-0350ea50b32b",
        "tags" : [
        ]
      },
      {
        "id" : "8b6de835-30ec-42a5-bd44-4d14d4b047ed",
        "parentId" : "de53d169-aa76-4ffe-8dd7-ea7728827998",
        "authorId" : "e1bdd418-d9b9-4129-91a0-0865740d24d2",
        "body" : "@zero323 - I updated the PR to expose `approx_count_distinct` as well, good point.\r\n\r\n[One SO question on this](https://stackoverflow.com/questions/37949494/how-to-count-occurrences-of-each-distinct-value-for-every-column-in-a-dataframe) has 81,000 views and [the other](https://stackoverflow.com/questions/40888946/spark-dataframe-count-distinct-values-of-every-column) has 66,000 views.  There is a high demand for count distinct / approx_count_distinct exploratory data analysis queries by columns among the Spark userbase.\r\n\r\nThese options don't cost us much and should help the less technical Spark users that aren't comfortable using `agg`.  I help lots of folks that can't even program and they aren't code geniuses like you all ;)",
        "createdAt" : "2021-01-20T19:14:30Z",
        "updatedAt" : "2021-01-20T19:14:30Z",
        "lastEditedBy" : "e1bdd418-d9b9-4129-91a0-0865740d24d2",
        "tags" : [
        ]
      },
      {
        "id" : "4f24e799-be55-4d44-b66b-d4e9d0051afe",
        "parentId" : "de53d169-aa76-4ffe-8dd7-ea7728827998",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I am not sure. You can all have what you want via `agg`. This is just a shortcut to show basic stats, which was inspired by pandas' one, see https://github.com/apache/spark/commit/e1a172c201d68406faa53b113518b10c879f1ff6. We should of course avoid showing every stats here.\r\n\r\nYou can easily do `df.agg(\"col\" -> \"approx_count_distinct\", ...)` easily for other stats. Adding `count_distinct` here makes it ambiguous what to add in `DataFrame.describe` and `DataFrame.summary`.\r\n\r\n",
        "createdAt" : "2021-01-21T01:47:25Z",
        "updatedAt" : "2021-01-21T01:47:25Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "fc0352b6-15c6-411e-a166-4b185eabcbe0",
        "parentId" : "de53d169-aa76-4ffe-8dd7-ea7728827998",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "It's true, though, users looking for summary stats are probably looking at summary(), and sometimes looking for distinct values, and this puts them into one output with the others. I think it's slightly worth adding one more somewhat-common summary stat. ",
        "createdAt" : "2021-01-21T01:58:44Z",
        "updatedAt" : "2021-01-21T01:58:44Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "2802b13b-db2d-4439-ba7d-d1ae9c2777bb",
        "parentId" : "de53d169-aa76-4ffe-8dd7-ea7728827998",
        "authorId" : "981b170c-729a-429c-b115-0350ea50b32b",
        "body" : "> I am not sure. You can all have what you want via `agg`\r\n\r\nWhich is also more suitable for anything but quick visual inspection.\r\n\r\nMy biggest concern is somewhat different though â€’ existing summaries have good performance and predictable behavior. In contrast count distinct can be unstable and approx count distinct slower and more resource hungry. \r\n",
        "createdAt" : "2021-01-21T13:46:45Z",
        "updatedAt" : "2021-01-21T13:46:46Z",
        "lastEditedBy" : "981b170c-729a-429c-b115-0350ea50b32b",
        "tags" : [
        ]
      },
      {
        "id" : "a40041b1-2475-4ef1-87d9-6027f098a794",
        "parentId" : "de53d169-aa76-4ffe-8dd7-ea7728827998",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Sure, but this is only computed if the caller requests it.",
        "createdAt" : "2021-01-21T14:13:50Z",
        "updatedAt" : "2021-01-21T14:13:50Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "348bba8e-e2fd-45ed-81b3-c98d38338420",
        "parentId" : "de53d169-aa76-4ffe-8dd7-ea7728827998",
        "authorId" : "e1bdd418-d9b9-4129-91a0-0865740d24d2",
        "body" : "@HyukjinKwon - you raise a good point about API ambiguity between `Dataset.describe` and `Dataset.summary`, but those ambiguities already exist.  `df.describe(\"some_col\")`, `df.select(\"some_col\").summary()`, and `df.select(\"some_col\").summary(\"37%\", \"87%\")` all return different results.\r\n\r\n`describe` is a \"partial alias\" of `summary`, but the method signatures are different.\r\n\r\nThe Spark & Pandas `describe` methods aren't consistent either.  The Spark `describe` method takes column name arguments whereas the Pandas `describe` method takes options, see the Pandas method signature: `DataFrame.describe(percentiles=None, include=None, exclude=None, datetime_is_numeric=False)`.\r\n\r\nA Pandas user is already going to be confused by the API.  They're going to run `df.describe(percentiles=Array(.25, .5, .75))` and have to Google around to find that they need to run `df.summary(\"0.25%\", \"0.5%\", \"0.75%\")`.  \r\n\r\nAdding additional options to `Dataset.summary` seems consistent with the existing design.  The `summary` method is already where we put the extra metrics that aren't enabled by default, like the option to add arbitrary percentiles.\r\n\r\nBTW - I don't mean to complain here ;). It's easy for me to make comments on API decisions made years ago from folks in the trenches.  You are all doing a wonderful job maintaining this project and continuously moving it in a good direction.  I'm always impressed by the code you write and will hopefully get to your level some day.  My PR comments sound critical cause I'm just trying to get to the point.  I'm very grateful for all the work you've done on this project.",
        "createdAt" : "2021-01-26T04:42:07Z",
        "updatedAt" : "2021-01-26T04:42:08Z",
        "lastEditedBy" : "e1bdd418-d9b9-4129-91a0-0865740d24d2",
        "tags" : [
        ]
      },
      {
        "id" : "7b0f371c-71e5-4cc7-b680-138a57a159c8",
        "parentId" : "de53d169-aa76-4ffe-8dd7-ea7728827998",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "I would favor this change. If there aren't significant objections, I'd like to merge it.",
        "createdAt" : "2021-01-27T00:35:50Z",
        "updatedAt" : "2021-01-27T00:35:51Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "3126bfa1-6142-4c83-ae16-3c48ff59c5d4",
        "parentId" : "de53d169-aa76-4ffe-8dd7-ea7728827998",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I'm okay given that we don't change the default.",
        "createdAt" : "2021-01-27T04:14:14Z",
        "updatedAt" : "2021-01-27T04:14:14Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "42517638-ff3f-43bb-9223-0cab19b089af",
        "parentId" : "de53d169-aa76-4ffe-8dd7-ea7728827998",
        "authorId" : "981b170c-729a-429c-b115-0350ea50b32b",
        "body" : "Sounds good.",
        "createdAt" : "2021-01-27T14:31:10Z",
        "updatedAt" : "2021-01-27T14:31:10Z",
        "lastEditedBy" : "981b170c-729a-429c-b115-0350ea50b32b",
        "tags" : [
        ]
      }
    ],
    "commit" : "bc547a2c949dba8322378a334d8f7221e46e3784",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +2671,2675 @@   *   <li>max</li>\n   *   <li>arbitrary approximate percentiles specified as a percentage (e.g. 75%)</li>\n   *   <li>count_distinct</li>\n   *   <li>approx_count_distinct</li>\n   * </ul>"
  },
  {
    "id" : "a43637e8-b651-497f-9972-baedbb3a28f4",
    "prId" : 30775,
    "prUrl" : "https://github.com/apache/spark/pull/30775#pullrequestreview-565099293",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5e45ecd4-2a15-4d12-a919-3326bae286c4",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "can users work around it by calling untyped join and use `as[T]` to  get the typed dataset?",
        "createdAt" : "2021-01-11T06:35:38Z",
        "updatedAt" : "2021-01-11T07:01:11Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "3c1e946c-b71a-497f-a840-cf367683c189",
        "parentId" : "5e45ecd4-2a15-4d12-a919-3326bae286c4",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> can users work around it by calling untyped join and use `as[T]` to get the typed dataset?\r\n\r\nLogically, yes.  But seems not not very convenient or direct.",
        "createdAt" : "2021-01-11T06:56:10Z",
        "updatedAt" : "2021-01-11T07:01:11Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "3d13c3120bd7063942b04b1db660cf463fa136f9",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +1259,1263 @@   * @since 3.2.0\n   */\n  def joinPartial[U](other: Dataset[U], condition: Column, joinType: String): Dataset[T] = {\n    val joinedType = JoinType(joinType)\n"
  },
  {
    "id" : "0527a8e0-5b68-4adb-9d9f-96911fa86713",
    "prId" : 30647,
    "prUrl" : "https://github.com/apache/spark/pull/30647#pullrequestreview-547870986",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4befda5c-1fd4-4f64-b4f2-ba4d1eec3f42",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "The previous approach had an unacceptable behaviour ([it escaped unicode strings...](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/132367/testReport/org.apache.spark.sql/DatasetSuite/SPARK_25108_Fix_the_show_method_to_display_the_full_width_character_alignment_problem/)), so I've updated the code to replace `\\t` and `\\n` only.\r\n\r\n",
        "createdAt" : "2020-12-08T06:35:13Z",
        "updatedAt" : "2020-12-11T05:40:39Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "2708c3d4-b5c9-4926-88e4-ce6c1dce5e9a",
        "parentId" : "4befda5c-1fd4-4f64-b4f2-ba4d1eec3f42",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Just for my curiosity, why is `\\\\\\\\t` necessary? the extra escaping? this seems to replace with the string `\\\\t` rather than `\\t`",
        "createdAt" : "2020-12-08T14:54:27Z",
        "updatedAt" : "2020-12-11T05:40:39Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "f3e12d98-da86-47d7-880a-4daedf5573a9",
        "parentId" : "4befda5c-1fd4-4f64-b4f2-ba4d1eec3f42",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Yea, that's a reasonable question; It seems `$` and `\\` has a special meaning in the second param of `replaceAll` and see: https://docs.oracle.com/javase/8/docs/api/java/lang/String.html#replaceAll-java.lang.String-java.lang.String-\r\n```\r\nNote that backslashes (\\) and dollar signs ($) in the replacement string may cause the results\r\nto be different than if it were being treated as a literal replacement string; see Matcher.replaceAll. \r\nUse Matcher.quoteReplacement(java.lang.String) to suppress the special meaning\r\nof these characters, if desired.\r\n```",
        "createdAt" : "2020-12-09T06:19:58Z",
        "updatedAt" : "2020-12-11T05:40:39Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "feb02779c017cca24aa18240aa4370591c248ad6",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +311,315 @@          case _ =>\n            // Escapes meta-characters not to break the `showString` format\n            cell.toString.replaceAll(\"\\n\", \"\\\\\\\\n\").replaceAll(\"\\t\", \"\\\\\\\\t\")\n        }\n        if (truncate > 0 && str.length > truncate) {"
  },
  {
    "id" : "18646dfd-a7ae-4e18-8c46-5c58ca32aade",
    "prId" : 30488,
    "prUrl" : "https://github.com/apache/spark/pull/30488#pullrequestreview-540948039",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8d75fdb6-bde1-480f-b8ef-f1cf40411ca4",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "I find that `df.show()` can also change the dataset_id of LogicalPlan. So I made this change.",
        "createdAt" : "2020-11-25T13:35:18Z",
        "updatedAt" : "2020-12-02T15:39:45Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "8c05ded7-3e05-4c30-8810-810959ae4f60",
        "parentId" : "8d75fdb6-bde1-480f-b8ef-f1cf40411ca4",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Any existing test fails? If not, could you add tests for this change?",
        "createdAt" : "2020-11-26T06:33:47Z",
        "updatedAt" : "2020-12-02T15:39:45Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "7ddbdd64-157c-44d2-892f-b683082a5d67",
        "parentId" : "8d75fdb6-bde1-480f-b8ef-f1cf40411ca4",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Added, thanks!",
        "createdAt" : "2020-11-30T14:17:07Z",
        "updatedAt" : "2020-12-02T15:39:45Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "d13b88c25a10022be0ff0190fd6fa8d158b24560",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +233,237 @@    }\n    if (sparkSession.sessionState.conf.getConf(SQLConf.FAIL_AMBIGUOUS_SELF_JOIN_ENABLED) &&\n        plan.getTagValue(Dataset.DATASET_ID_TAG).isEmpty) {\n      plan.setTagValue(Dataset.DATASET_ID_TAG, id)\n    }"
  },
  {
    "id" : "5e463b64-711a-469d-835b-fb6e5f98c931",
    "prId" : 30257,
    "prUrl" : "https://github.com/apache/spark/pull/30257#pullrequestreview-525652408",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1ffec48b-cfaa-46d6-9e79-5bec8c822947",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Not sure. I would just simply select.",
        "createdAt" : "2020-11-05T04:46:18Z",
        "updatedAt" : "2020-11-05T04:46:19Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "233c87b8-3aaa-4dde-bc98-46992f9f662f",
        "parentId" : "1ffec48b-cfaa-46d6-9e79-5bec8c822947",
        "authorId" : "152034df-2d74-457f-aa5c-5d1f5614022c",
        "body" : "If use select, for users, which means they should  adjust the order of the cols.",
        "createdAt" : "2020-11-05T05:35:02Z",
        "updatedAt" : "2020-11-05T05:35:03Z",
        "lastEditedBy" : "152034df-2d74-457f-aa5c-5d1f5614022c",
        "tags" : [
        ]
      },
      {
        "id" : "d1331c9c-e7a6-49dd-a5a7-cc9a246a98f8",
        "parentId" : "1ffec48b-cfaa-46d6-9e79-5bec8c822947",
        "authorId" : "a3190564-c88c-4977-9012-ce9c80d993bb",
        "body" : "Instead of numeric position it would be way more convenient to reference a position by existing column names; perhaps add `before_column` and `after_column`? ",
        "createdAt" : "2020-11-05T18:20:35Z",
        "updatedAt" : "2020-11-05T18:23:38Z",
        "lastEditedBy" : "a3190564-c88c-4977-9012-ce9c80d993bb",
        "tags" : [
        ]
      },
      {
        "id" : "0002bf5d-c76c-4126-92d7-50764b05cf9e",
        "parentId" : "1ffec48b-cfaa-46d6-9e79-5bec8c822947",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Agree with @HyukjinKwon, seems `select` can do this.",
        "createdAt" : "2020-11-07T05:17:43Z",
        "updatedAt" : "2020-11-07T05:17:43Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "e27b6ee0-b297-4357-b0e8-1241e044ccdc",
        "parentId" : "1ffec48b-cfaa-46d6-9e79-5bec8c822947",
        "authorId" : "c3b617cc-e920-44cb-bdd6-d1ad5c252303",
        "body" : "yeah, as @HyukjinKwon said, i just realize with `select`.\r\n![image](https://user-images.githubusercontent.com/19250651/98440846-1fc6b980-2136-11eb-899d-e2c16e3ea965.png)",
        "createdAt" : "2020-11-07T12:22:03Z",
        "updatedAt" : "2020-11-07T12:22:03Z",
        "lastEditedBy" : "c3b617cc-e920-44cb-bdd6-d1ad5c252303",
        "tags" : [
        ]
      },
      {
        "id" : "8ea7c73c-4e8b-427c-960c-f15a36feee5a",
        "parentId" : "1ffec48b-cfaa-46d6-9e79-5bec8c822947",
        "authorId" : "152034df-2d74-457f-aa5c-5d1f5614022c",
        "body" : "what if you have 100 cols, and add new col in 50th position?\r\n you have to get `val cols:Array[String] =df.columns` and adjust the order of the cols and then use selectExpr(cols: _* ).\r\nthat's my point.",
        "createdAt" : "2020-11-07T13:17:22Z",
        "updatedAt" : "2020-11-07T13:17:22Z",
        "lastEditedBy" : "152034df-2d74-457f-aa5c-5d1f5614022c",
        "tags" : [
        ]
      }
    ],
    "commit" : "2b9b03d1bae7aebfc46ae2674af36d8a0dd67755",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +2392,2396 @@   * existing column that has the same name.\n   */\n  def withColumn(colName: String, col: Column, pos: Int): DataFrame =\n    withColumns(Seq(colName), Seq(col), pos)\n"
  },
  {
    "id" : "76d7146e-1dec-4b3d-93b7-d2f2018abfc3",
    "prId" : 28996,
    "prUrl" : "https://github.com/apache/spark/pull/28996#pullrequestreview-446816845",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bb296fcc-bc25-4bf7-914c-531dde2d87ce",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Could you add an illustrate example like 2016 ~ 2029, @viirya ?",
        "createdAt" : "2020-07-11T16:22:36Z",
        "updatedAt" : "2020-07-11T17:43:25Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "22f0d0ba-29f8-483a-9d99-ceff69d98ccf",
        "parentId" : "bb296fcc-bc25-4bf7-914c-531dde2d87ce",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "okay.",
        "createdAt" : "2020-07-11T17:25:52Z",
        "updatedAt" : "2020-07-11T17:43:25Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "873498394eaf80b2b302b6fc7aeec410e7113415",
    "line" : 41,
    "diffHunk" : "@@ -1,1 +2067,2071 @@   *   // +----+----+----+----+\n   * }}}\n   *\n   * @group typedrel\n   * @since 3.1.0"
  },
  {
    "id" : "24d8a7e3-1839-4ac7-8ed3-a705871d790e",
    "prId" : 28996,
    "prUrl" : "https://github.com/apache/spark/pull/28996#pullrequestreview-446816840",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "12f171c6-7c37-499d-99a3-ed188250655f",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "It's worth to document a little more about the order sensitive. Previously, it was simple because it follows the schema of original set(=left). With new options, the number of missing columns which will be added at the end are determined by `other` (=right).",
        "createdAt" : "2020-07-11T16:28:59Z",
        "updatedAt" : "2020-07-11T17:43:25Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "4a2b0f0c-f7e0-431c-aff0-c92250f7e49d",
        "parentId" : "12f171c6-7c37-499d-99a3-ed188250655f",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Good advice.",
        "createdAt" : "2020-07-11T17:25:47Z",
        "updatedAt" : "2020-07-11T17:43:25Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "873498394eaf80b2b302b6fc7aeec410e7113415",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +2040,2044 @@   *\n   * When the parameter `allowMissingColumns` is true, this function allows different set\n   * of column names between two Datasets. Missing columns at each side, will be filled with\n   * null values. The missing columns at left Dataset will be added at the end in the schema\n   * of the union result:"
  },
  {
    "id" : "a71e1f02-cc16-44d1-9300-91dc4d0c62bc",
    "prId" : 28996,
    "prUrl" : "https://github.com/apache/spark/pull/28996#pullrequestreview-448369095",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e176462a-dc4c-49df-aace-712a12f341f6",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Does it work with nested columns?",
        "createdAt" : "2020-07-13T04:57:01Z",
        "updatedAt" : "2020-07-13T04:57:01Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "06d5c8ed-da56-45bc-afed-54fb40b9bb20",
        "parentId" : "e176462a-dc4c-49df-aace-712a12f341f6",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "No, currently it doesn't.",
        "createdAt" : "2020-07-13T05:49:59Z",
        "updatedAt" : "2020-07-13T05:49:59Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "29bb60de-9a1d-4839-a121-007fc0ae8ba5",
        "parentId" : "e176462a-dc4c-49df-aace-712a12f341f6",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I think the major problem here is we put the by-name logic in the API method, not in the `Analyzer`. Shall we add 2 boolean parameters(byName and allowMissingCol) to `Union`, and move the by-name logic to the type coercion rules?",
        "createdAt" : "2020-07-13T06:31:51Z",
        "updatedAt" : "2020-07-13T06:35:20Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "83b4a809-a80a-43d2-bd29-b950bee16032",
        "parentId" : "e176462a-dc4c-49df-aace-712a12f341f6",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Ok. I will do it in another PR.",
        "createdAt" : "2020-07-13T15:40:37Z",
        "updatedAt" : "2020-07-13T15:40:37Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "efe2a946-dfa0-44e3-add6-65a9636f73ec",
        "parentId" : "e176462a-dc4c-49df-aace-712a12f341f6",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "@cloud-fan . `unionByName` (and `by-name logic`) has been here since Apache Spark 2.3.0. \r\nShall we proceed that refactoring suggestion as a separate JIRA?",
        "createdAt" : "2020-07-13T15:40:48Z",
        "updatedAt" : "2020-07-13T15:44:26Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "0f06c899-0453-4aac-860e-9f21ea503c8b",
        "parentId" : "e176462a-dc4c-49df-aace-712a12f341f6",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Yea it's better to have a new JIRA.",
        "createdAt" : "2020-07-13T16:31:10Z",
        "updatedAt" : "2020-07-13T16:31:10Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "e450d425-81c1-4fc2-a507-1a018a1f73bd",
        "parentId" : "e176462a-dc4c-49df-aace-712a12f341f6",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Thanks, @cloud-fan .",
        "createdAt" : "2020-07-14T18:41:46Z",
        "updatedAt" : "2020-07-14T18:41:46Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "873498394eaf80b2b302b6fc7aeec410e7113415",
    "line" : 56,
    "diffHunk" : "@@ -1,1 +2089,2093 @@    val rightProjectList = leftOutputAttrs.map { lattr =>\n      rightOutputAttrs.find { rattr => resolver(lattr.name, rattr.name) }.getOrElse {\n        if (allowMissingColumns) {\n          Alias(Literal(null, lattr.dataType), lattr.name)()\n        } else {"
  },
  {
    "id" : "341bd6cd-55cc-4241-b6f8-784b19dbe044",
    "prId" : 28996,
    "prUrl" : "https://github.com/apache/spark/pull/28996#pullrequestreview-482514644",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3905d670-c9bb-4622-be52-352f6f2517ee",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "Do we have a JIRA to add the corresponding API for Python? ",
        "createdAt" : "2020-09-04T06:41:07Z",
        "updatedAt" : "2020-09-04T06:41:07Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "c44968c8-6fdd-4b3b-a838-0160f6b0061c",
        "parentId" : "3905d670-c9bb-4622-be52-352f6f2517ee",
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "This is a good beginner task for new contributors.",
        "createdAt" : "2020-09-04T06:42:16Z",
        "updatedAt" : "2020-09-04T06:42:16Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "c4ff3c71-402c-445d-9946-fb02053cd82c",
        "parentId" : "3905d670-c9bb-4622-be52-352f6f2517ee",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "I should create a followup PR for Python and R. But it is okay for a beginner task too.",
        "createdAt" : "2020-09-04T06:44:19Z",
        "updatedAt" : "2020-09-04T06:44:20Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "42f4270b-a233-4cfe-b3fb-8297a6550cdb",
        "parentId" : "3905d670-c9bb-4622-be52-352f6f2517ee",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I filed at SPARK-32798 and SPARK-32799",
        "createdAt" : "2020-09-04T09:46:21Z",
        "updatedAt" : "2020-09-04T09:46:21Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "873498394eaf80b2b302b6fc7aeec410e7113415",
    "line" : 45,
    "diffHunk" : "@@ -1,1 +2071,2075 @@   * @since 3.1.0\n   */\n  def unionByName(other: Dataset[T], allowMissingColumns: Boolean): Dataset[T] = withSetOperator {\n    // Check column name duplication\n    val resolver = sparkSession.sessionState.analyzer.resolver"
  },
  {
    "id" : "d05eb071-febc-4dc7-aaa3-945f0abeb63a",
    "prId" : 28984,
    "prUrl" : "https://github.com/apache/spark/pull/28984#pullrequestreview-442812800",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c429a850-fa9e-4059-b9f1-245105ae3f5b",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "The built-in `to_json` is not enough for your usecase? cc: @HyukjinKwon ",
        "createdAt" : "2020-07-05T12:29:21Z",
        "updatedAt" : "2020-07-06T06:18:46Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "57eeec8c-56f6-4c5e-943b-de98cc01ea8e",
        "parentId" : "c429a850-fa9e-4059-b9f1-245105ae3f5b",
        "authorId" : "5b9e802a-4319-492f-a0ba-8cdf41bed39a",
        "body" : "I guess so, you'd have to do something like this `df.select(to_json(struct(\"*\"), Map(\"ignoreNullFields\" -> \"false\")).as(\"value\"))` to produce the same result as `df.toJSON(options)`",
        "createdAt" : "2020-07-05T13:12:56Z",
        "updatedAt" : "2020-07-06T06:18:46Z",
        "lastEditedBy" : "5b9e802a-4319-492f-a0ba-8cdf41bed39a",
        "tags" : [
        ]
      },
      {
        "id" : "867caa8c-1bbe-4934-958d-11b5cebdb9ca",
        "parentId" : "c429a850-fa9e-4059-b9f1-245105ae3f5b",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Yea, I think so.",
        "createdAt" : "2020-07-05T13:46:02Z",
        "updatedAt" : "2020-07-06T06:18:46Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "29b4d02f-8fa0-400b-a3ab-d6a5b5c213d6",
        "parentId" : "c429a850-fa9e-4059-b9f1-245105ae3f5b",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I don't feel strongly here. I would rather don't extend `toJSON` anymore given that we added `to_json`, `from_json`, etc. to cover such general cases. I am okay if any committer supports it.",
        "createdAt" : "2020-07-06T04:02:27Z",
        "updatedAt" : "2020-07-06T06:18:46Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "969e9c4c-d3fa-4996-b97c-921cc3ee2982",
        "parentId" : "c429a850-fa9e-4059-b9f1-245105ae3f5b",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Thanks for the check, @HyukjinKwon. I'm neutral on this, too. ",
        "createdAt" : "2020-07-06T06:50:35Z",
        "updatedAt" : "2020-07-06T06:50:36Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "73697c90476308a8b8b39ad63815437f41a60df9",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +3403,3407 @@   * @since 3.1.0\n   */\n  def toJSON(options: Map[String, String]): Dataset[String] = {\n    val rowSchema = this.schema\n    val sessionLocalTimeZone = sparkSession.sessionState.conf.sessionLocalTimeZone"
  },
  {
    "id" : "c4e9c256-c230-4874-9b2e-bc465dbb7c3d",
    "prId" : 28984,
    "prUrl" : "https://github.com/apache/spark/pull/28984#pullrequestreview-442797165",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "aa3a002b-21ef-460a-a4c1-afc3aa64176a",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I would deduplicate it by calling `toJSON(options: Map[String, String])` at `toJSON`.",
        "createdAt" : "2020-07-06T04:03:33Z",
        "updatedAt" : "2020-07-06T06:18:46Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "113226a0-ac62-4a73-bd24-f6cf730d7edd",
        "parentId" : "aa3a002b-21ef-460a-a4c1-afc3aa64176a",
        "authorId" : "5b9e802a-4319-492f-a0ba-8cdf41bed39a",
        "body" : "ok!",
        "createdAt" : "2020-07-06T06:14:56Z",
        "updatedAt" : "2020-07-06T06:18:46Z",
        "lastEditedBy" : "5b9e802a-4319-492f-a0ba-8cdf41bed39a",
        "tags" : [
        ]
      }
    ],
    "commit" : "73697c90476308a8b8b39ad63815437f41a60df9",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +3403,3407 @@   * @since 3.1.0\n   */\n  def toJSON(options: Map[String, String]): Dataset[String] = {\n    val rowSchema = this.schema\n    val sessionLocalTimeZone = sparkSession.sessionState.conf.sessionLocalTimeZone"
  },
  {
    "id" : "1c734818-71db-418c-ad1c-c4b43d175bbb",
    "prId" : 28830,
    "prUrl" : "https://github.com/apache/spark/pull/28830#pullrequestreview-430429141",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ef1cddf2-0388-4477-87f6-c005f081d445",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I think this is good for now.\r\n\r\nIn the future, this may still be broken by Scala version upgrade, and hopefully @xuanyuanking 's unsafe row validation can detect it. Then we can change it and use a deterministic order, as it will be broken anyway.",
        "createdAt" : "2020-06-15T06:33:28Z",
        "updatedAt" : "2020-06-15T06:33:49Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "6b779ed7-c6a1-4409-a39e-27fcb883e416",
        "parentId" : "ef1cddf2-0388-4477-87f6-c005f081d445",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "Yep, I also mentioned this at https://github.com/apache/spark/pull/28830#discussion_r439909489, we can relay on the validation checking and integrated tests.",
        "createdAt" : "2020-06-15T06:40:43Z",
        "updatedAt" : "2020-06-15T06:40:43Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      },
      {
        "id" : "50f972f2-c86b-4c16-82fa-24c7fd782884",
        "parentId" : "ef1cddf2-0388-4477-87f6-c005f081d445",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Worth noting that we need to have \"concrete\" solution eventually - if columns are all having same type neither #28830 nor #24173 catch the change and the result becomes silently incorrect. I roughly remember the similar issue on pyspark, which was trying to fix the issue on order vs name, don't remember how it ended up. cc. @HyukjinKwon ",
        "createdAt" : "2020-06-15T06:57:23Z",
        "updatedAt" : "2020-06-15T06:58:11Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "fe429ea8-16ee-4419-8be2-5def53becc20",
        "parentId" : "ef1cddf2-0388-4477-87f6-c005f081d445",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Ah, that was fixed in a way by adding an env variable. That case also was specific to Python 2 which is deprecated now so it's rather a corner case.",
        "createdAt" : "2020-06-15T08:31:43Z",
        "updatedAt" : "2020-06-15T08:31:44Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "7546ba4eebeee480d9a2ff8b948e900cd6023dfc",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +2544,2548 @@    // SPARK-31990: We must keep `toSet.toSeq` here because of the backward compatibility issue\n    // (the Streaming's state store depends on the `groupCols` order).\n    val groupCols = colNames.toSet.toSeq.flatMap { (colName: String) =>\n      // It is possibly there are more than one columns with the same name,\n      // so we call filter instead of find."
  }
]