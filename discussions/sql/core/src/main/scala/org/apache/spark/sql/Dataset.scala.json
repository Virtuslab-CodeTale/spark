[
  {
    "id" : "52ab4da5-f523-4bad-af51-58a62be89dd8",
    "prId" : 33422,
    "prUrl" : "https://github.com/apache/spark/pull/33422#pullrequestreview-711175439",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f05dcd96-fecd-41e9-8bcb-c265662bfb28",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "if it's simply adding an annotation `@varargs`, shall we just do it in this PR?",
        "createdAt" : "2021-07-20T13:03:39Z",
        "updatedAt" : "2021-07-20T13:03:39Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "eddad481-aaca-48a0-9ba6-c7472955447c",
        "parentId" : "f05dcd96-fecd-41e9-8bcb-c265662bfb28",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "oh yeah!",
        "createdAt" : "2021-07-20T13:16:47Z",
        "updatedAt" : "2021-07-20T13:16:47Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "7f3ddf1d-74bc-49a9-93dc-f9dc40cc671f",
        "parentId" : "f05dcd96-fecd-41e9-8bcb-c265662bfb28",
        "authorId" : "c4ecf4e4-1fda-4fed-9980-aed6c3b3e8e4",
        "body" : "what about `observe(String, Column, Column*)`?",
        "createdAt" : "2021-07-20T18:38:39Z",
        "updatedAt" : "2021-07-20T18:38:39Z",
        "lastEditedBy" : "c4ecf4e4-1fda-4fed-9980-aed6c3b3e8e4",
        "tags" : [
        ]
      },
      {
        "id" : "d1f55634-356f-4ede-8bd5-21d3ebad3455",
        "parentId" : "f05dcd96-fecd-41e9-8bcb-c265662bfb28",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "That's fine. Let's don't add it for now",
        "createdAt" : "2021-07-21T00:59:04Z",
        "updatedAt" : "2021-07-21T00:59:05Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "3b2d9ae8800b639e91cc53a44d662c7f72e0157b",
    "line" : 34,
    "diffHunk" : "@@ -1,1 +1971,1975 @@   */\n  @varargs\n  def observe(observation: Observation, expr: Column, exprs: Column*): Dataset[T] = {\n    observation.on(this, expr, exprs: _*)\n  }"
  },
  {
    "id" : "b6bd7010-4131-4e0e-91f8-8a2c7a45f66a",
    "prId" : 32963,
    "prUrl" : "https://github.com/apache/spark/pull/32963#pullrequestreview-689002669",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "349bf8c0-d53a-414a-ad73-f714f7ae8002",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Could you add a test coverage for this, @beliefer  ?",
        "createdAt" : "2021-06-19T17:41:58Z",
        "updatedAt" : "2021-06-19T17:41:58Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "417668fa-0d41-4131-9029-a50804f04c47",
        "parentId" : "349bf8c0-d53a-414a-ad73-f714f7ae8002",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "+1, this API has no test at all, we can add one in `DataFrameSuite`",
        "createdAt" : "2021-06-21T03:37:25Z",
        "updatedAt" : "2021-06-21T03:37:32Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "b8c1d7af-74e9-4840-bc3b-689783e5eb7d",
        "parentId" : "349bf8c0-d53a-414a-ad73-f714f7ae8002",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "Thank you for reminder.",
        "createdAt" : "2021-06-22T02:19:13Z",
        "updatedAt" : "2021-06-22T02:19:13Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "4b2a05533230110f2cc2940ecf0d91f6842ac4aa",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +599,603 @@   */\n  def isLocal: Boolean = logicalPlan.isInstanceOf[LocalRelation] ||\n    logicalPlan.isInstanceOf[CommandResult]\n\n  /**"
  },
  {
    "id" : "462ff7e8-4381-498d-9d5a-b8e7533bab2d",
    "prId" : 32863,
    "prUrl" : "https://github.com/apache/spark/pull/32863#pullrequestreview-681515717",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "53f375d0-2e01-4532-91c0-98ed44fc61ad",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "is this code repeated somewhere? and can we move it to a function to share code?",
        "createdAt" : "2021-06-11T05:50:02Z",
        "updatedAt" : "2021-06-11T05:50:02Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "f7d083c4-2353-4f04-85d7-55d059498384",
        "parentId" : "53f375d0-2e01-4532-91c0-98ed44fc61ad",
        "authorId" : "87a71d79-d4d2-4f2c-8e9a-be56cf39b035",
        "body" : "yes, it's repeated from join function, I have created a new function and replaced duplicated code with it",
        "createdAt" : "2021-06-11T07:09:18Z",
        "updatedAt" : "2021-06-11T07:09:18Z",
        "lastEditedBy" : "87a71d79-d4d2-4f2c-8e9a-be56cf39b035",
        "tags" : [
        ]
      }
    ],
    "commit" : "afd327952c795d6ab639b4fb4fc50eaaa5ddeb80",
    "line" : 73,
    "diffHunk" : "@@ -1,1 +1170,1174 @@\n    // If auto self join alias is enable\n    if (sqlContext.conf.dataFrameSelfJoinAutoResolveAmbiguity) {\n      joined = resolveSelfJoinCondition(joined)\n    }"
  },
  {
    "id" : "f87ad936-8cd4-4f08-8b6e-5cbb17b84776",
    "prId" : 32616,
    "prUrl" : "https://github.com/apache/spark/pull/32616#pullrequestreview-665118671",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "537b3642-933c-4b1e-8414-610b04280c29",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Q: Is there the possibility that the set will continue to increase in a long-running application?",
        "createdAt" : "2021-05-21T02:46:37Z",
        "updatedAt" : "2021-05-21T02:46:38Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "4cec5ddc-d2cc-46cf-a213-b89f82d821b2",
        "parentId" : "537b3642-933c-4b1e-8414-610b04280c29",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Looks no. Even in streaming query, it just reused the existing plan to run the query rather than re-transform the whole query.",
        "createdAt" : "2021-05-21T06:12:25Z",
        "updatedAt" : "2021-05-21T06:12:26Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "423f2b5ee2cd63bc4d0853305f7219d7891a1848",
    "line" : 27,
    "diffHunk" : "@@ -1,1 +234,238 @@    if (sparkSession.sessionState.conf.getConf(SQLConf.FAIL_AMBIGUOUS_SELF_JOIN_ENABLED)) {\n      val dsIds = plan.getTagValue(Dataset.DATASET_ID_TAG).getOrElse(new HashSet[Long])\n      dsIds.add(id)\n      plan.setTagValue(Dataset.DATASET_ID_TAG, dsIds)\n    }"
  },
  {
    "id" : "0a165980-e2da-4b79-a28c-65133fddb4ea",
    "prId" : 32032,
    "prUrl" : "https://github.com/apache/spark/pull/32032#pullrequestreview-632557373",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "55378630-8649-46e2-909d-6527113611a8",
        "parentId" : null,
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "Since `logicalPlan` is already analyzed, I am setting this to `true` here.",
        "createdAt" : "2021-04-09T15:49:00Z",
        "updatedAt" : "2021-04-14T03:19:24Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "111ef8be8bc03a62389aed4f0871b958714eb789",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +3375,3379 @@      replace = replace,\n      viewType = viewType,\n      isAnalyzed = true)\n  }\n"
  },
  {
    "id" : "4166e96b-b38f-4808-87d5-490e8c74d008",
    "prId" : 31905,
    "prUrl" : "https://github.com/apache/spark/pull/31905#pullrequestreview-709669124",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9f2d0ce8-6c1b-47b6-b0c1-b53a4a49c4b9",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "The test failure is likely from Java doc generation from these references. You'd have to wrap it with backticks for the references Java doc build cannot identify in Unidoc.",
        "createdAt" : "2021-07-19T10:36:23Z",
        "updatedAt" : "2021-07-19T10:36:23Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "c8182ae1-e5c0-4e9d-866b-3d588bf19149",
        "parentId" : "9f2d0ce8-6c1b-47b6-b0c1-b53a4a49c4b9",
        "authorId" : "c4ecf4e4-1fda-4fed-9980-aed6c3b3e8e4",
        "body" : "Can you please point me to the error and the command so I can reproduce this locally? Otherwise I wouldn't know if I am fixing the right thing.",
        "createdAt" : "2021-07-19T11:45:38Z",
        "updatedAt" : "2021-07-19T11:45:38Z",
        "lastEditedBy" : "c4ecf4e4-1fda-4fed-9980-aed6c3b3e8e4",
        "tags" : [
        ]
      },
      {
        "id" : "8e55f92a-4f01-4bda-9f48-2a0d4406f92b",
        "parentId" : "9f2d0ce8-6c1b-47b6-b0c1-b53a4a49c4b9",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I think you can reproduce it via `./build/sbt unidoc`. ",
        "createdAt" : "2021-07-19T12:30:08Z",
        "updatedAt" : "2021-07-19T12:30:08Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "f288019e-ad97-4eec-87c9-877e3bf8709c",
        "parentId" : "9f2d0ce8-6c1b-47b6-b0c1-b53a4a49c4b9",
        "authorId" : "c4ecf4e4-1fda-4fed-9980-aed6c3b3e8e4",
        "body" : "I tried that one, but it succeeds. I am now trying `./dev/run-tests --parallelism 2 --modules \"sql\" --excluded-tags \"org.apache.spark.tags.ExtendedSQLTest\"` as in https://github.com/apache/spark/runs/3102419062#step:9:5.",
        "createdAt" : "2021-07-19T12:49:04Z",
        "updatedAt" : "2021-07-19T12:49:04Z",
        "lastEditedBy" : "c4ecf4e4-1fda-4fed-9980-aed6c3b3e8e4",
        "tags" : [
        ]
      },
      {
        "id" : "29385bed-2941-45b7-9028-81924a2b0798",
        "parentId" : "9f2d0ce8-6c1b-47b6-b0c1-b53a4a49c4b9",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "hm, feel free to open a PR and leverage GitHub Actions build",
        "createdAt" : "2021-07-19T13:39:07Z",
        "updatedAt" : "2021-07-19T13:39:07Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "aeacbb4f-3477-4f98-8440-b711d77951e0",
        "parentId" : "9f2d0ce8-6c1b-47b6-b0c1-b53a4a49c4b9",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "oh btw some errors from unidoc are fake. I think the real errors are:\r\n\r\n```\r\n[error] /home/runner/work/spark/spark/sql/core/target/java/org/apache/spark/sql/Dataset.java:1800:1:  error: unexpected text\r\n[error]    * This is equivalent to calling {@link Dataset.observe(String, Column, Column*)} but does\r\n[error]                                    ^\r\n[error] /home/runner/work/spark/spark/sql/core/target/java/org/apache/spark/sql/Dataset.java:1804:1:  error: reference not found\r\n[error]    * A user can retrieve the metrics by accessing {@link org.apache.spark.sql.Observation.get}.\r\n[error]                                                          ^\r\n[error] /home/runner/work/spark/spark/sql/core/target/java/org/apache/spark/sql/Observation.java:51:1:  error: exception not thrown: java.lang.InterruptedException\r\n[error]    * @throws java.lang.InterruptedException interrupted while waiting\r\n[error]              ^\r\n```",
        "createdAt" : "2021-07-19T13:42:16Z",
        "updatedAt" : "2021-07-19T13:42:16Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "ff3702d6-8bbf-456f-a2a4-3949e7f5ddf3",
        "parentId" : "9f2d0ce8-6c1b-47b6-b0c1-b53a4a49c4b9",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "you might have to just wrap these three instances with backtics instead of `[[..]]` as a workaround.",
        "createdAt" : "2021-07-19T13:42:47Z",
        "updatedAt" : "2021-07-19T13:42:47Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "138a59b1-8e8f-48fc-980a-16ddb207585f",
        "parentId" : "9f2d0ce8-6c1b-47b6-b0c1-b53a4a49c4b9",
        "authorId" : "c4ecf4e4-1fda-4fed-9980-aed6c3b3e8e4",
        "body" : "Done. How is this merged into master, from another PR?",
        "createdAt" : "2021-07-19T14:56:47Z",
        "updatedAt" : "2021-07-19T14:56:47Z",
        "lastEditedBy" : "c4ecf4e4-1fda-4fed-9980-aed6c3b3e8e4",
        "tags" : [
        ]
      },
      {
        "id" : "f5b93381-8252-45bb-bf92-be6b7f3c6e3f",
        "parentId" : "9f2d0ce8-6c1b-47b6-b0c1-b53a4a49c4b9",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Yes, another PR would be cleaner because this PR seems to have a long history since March.. You can put the link of new PR here, @EnricoMi .",
        "createdAt" : "2021-07-19T15:00:01Z",
        "updatedAt" : "2021-07-19T15:00:41Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "fc5c90c7-2683-4292-9ae0-cb30d5df060a",
        "parentId" : "9f2d0ce8-6c1b-47b6-b0c1-b53a4a49c4b9",
        "authorId" : "c4ecf4e4-1fda-4fed-9980-aed6c3b3e8e4",
        "body" : "Done, thanks!",
        "createdAt" : "2021-07-19T15:11:03Z",
        "updatedAt" : "2021-07-19T15:11:03Z",
        "lastEditedBy" : "c4ecf4e4-1fda-4fed-9980-aed6c3b3e8e4",
        "tags" : [
        ]
      }
    ],
    "commit" : "ec7f7275ca7902db83aef86d174890bd1e240fe7",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1949,1953 @@\n  /**\n   * Observe (named) metrics through an [[org.apache.spark.sql.Observation]] instance.\n   * This is equivalent to calling [[Dataset.observe(String, Column, Column*)]] but does\n   * not require adding [[org.apache.spark.sql.util.QueryExecutionListener]] to the spark session."
  },
  {
    "id" : "b258705b-2c73-448b-bc64-6dec394b8edb",
    "prId" : 31700,
    "prUrl" : "https://github.com/apache/spark/pull/31700#pullrequestreview-603177943",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "900a4bbc-85d8-4b82-96a5-10445d76f337",
        "parentId" : null,
        "authorId" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "body" : "This bit is needed to prevent executing each batch early. We used `WriteToDataSourceV2` before, which did not extend `Command`.",
        "createdAt" : "2021-03-03T17:48:37Z",
        "updatedAt" : "2021-03-03T17:48:37Z",
        "lastEditedBy" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "tags" : [
        ]
      }
    ],
    "commit" : "e7f167fb0103d7e62127421cf5ad2a594d5f4337",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +225,229 @@    // to happen right away to let these side effects take place eagerly.\n    val plan = queryExecution.analyzed match {\n      case _: V2MicroBatchWriteCommand =>\n        queryExecution.analyzed\n      case c: Command =>"
  }
]