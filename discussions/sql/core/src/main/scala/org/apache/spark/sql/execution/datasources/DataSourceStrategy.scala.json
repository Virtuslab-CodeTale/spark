[
  {
    "id" : "ad56ae04-29bb-4b7f-b593-c56ee416cf46",
    "prId" : 32921,
    "prUrl" : "https://github.com/apache/spark/pull/32921#pullrequestreview-693225391",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9bbb669e-adc3-45ff-8414-adb531e55672",
        "parentId" : null,
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Super minor suggestion, we could maybe add some scaladoc here on the expected behaviour, I note that in one case it rases an exception and in the other case it returns None to indicate it isn't able to handle the filter and I think clarifying that could be good.",
        "createdAt" : "2021-06-25T17:17:16Z",
        "updatedAt" : "2021-06-25T17:17:16Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "e987e7ff-b281-49c1-b5d7-cfcb28bd7d11",
        "parentId" : "9bbb669e-adc3-45ff-8414-adb531e55672",
        "authorId" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "body" : "Will add.",
        "createdAt" : "2021-06-25T20:37:17Z",
        "updatedAt" : "2021-06-25T20:37:17Z",
        "lastEditedBy" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "tags" : [
        ]
      },
      {
        "id" : "c23ca511-b2bd-406d-9ba9-e374cc91ae2f",
        "parentId" : "9bbb669e-adc3-45ff-8414-adb531e55672",
        "authorId" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "body" : "Added. Let me know if it is descriptive enough, @holdenk.",
        "createdAt" : "2021-06-25T22:16:20Z",
        "updatedAt" : "2021-06-25T22:16:21Z",
        "lastEditedBy" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "tags" : [
        ]
      }
    ],
    "commit" : "881d2b2b7246d9453bfa7e074ab19334bf8d9876",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +638,642 @@   * If the underlying subquery hasn't completed yet, this method will throw an exception.\n   */\n  protected[sql] def translateRuntimeFilter(expr: Expression): Option[Filter] = expr match {\n    case in @ InSubqueryExec(e @ PushableColumnAndNestedColumn(name), _, _, _) =>\n      val values = in.values().getOrElse {"
  },
  {
    "id" : "2968395d-4757-4928-84d5-0df68160ac44",
    "prId" : 32807,
    "prUrl" : "https://github.com/apache/spark/pull/32807#pullrequestreview-679358394",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e258a08e-dc37-4ad4-bf89-13a03f54766e",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "note that:\r\n1. nestedPredicatePushdownEnabled is always enabled for DS v2 (by default)\r\n2. nestedPredicatePushdownEnabled is never enabled for DS v1\r\n3. nestedPredicatePushdownEnabled is only enabled for file source parquet and orc (by default)\r\n\r\nAfter changing the quoting logic:\r\n1. DS v1 is not affected\r\n2. file source is builtin so we are fine\r\n3. DS v2 will be affected if the column name contains special chars.\r\n\r\nPersonally, I think the new quoting behavior is better (more ANSI SQL), and most v2 implementations won't be affected as they already need to deal with quoted names.",
        "createdAt" : "2021-06-08T05:01:29Z",
        "updatedAt" : "2021-06-08T05:01:29Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "a604104e-bd95-4f5b-8f0a-4713bcf7345c",
        "parentId" : "e258a08e-dc37-4ad4-bf89-13a03f54766e",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "cc @dbtsai @viirya ",
        "createdAt" : "2021-06-08T05:01:45Z",
        "updatedAt" : "2021-06-08T05:01:45Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "3eb26681-7b33-4a8a-ab58-3d57516e6da9",
        "parentId" : "e258a08e-dc37-4ad4-bf89-13a03f54766e",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "looks good.",
        "createdAt" : "2021-06-09T07:01:18Z",
        "updatedAt" : "2021-06-09T07:01:18Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "756d0ca4-7c83-4310-94a4-26708a4bfd92",
        "parentId" : "e258a08e-dc37-4ad4-bf89-13a03f54766e",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Only exceptional one might be, if a v2 implementation can only handle quoted \"dots\", but not other quoted chars, right? I think it should be rare cases.",
        "createdAt" : "2021-06-09T07:02:51Z",
        "updatedAt" : "2021-06-09T07:02:51Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "ac171554-ce24-4353-8d17-14fb87a42b90",
        "parentId" : "e258a08e-dc37-4ad4-bf89-13a03f54766e",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Yea I think that's rare. V2 implementations should already be using a decent parser that recognizes dots and backticks.",
        "createdAt" : "2021-06-09T08:26:10Z",
        "updatedAt" : "2021-06-09T08:26:11Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "d4d1489594fb1e6a3c54c4ffc1d92511ed874e36",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +700,704 @@  def unapply(e: Expression): Option[String] = {\n    import org.apache.spark.sql.connector.catalog.CatalogV2Implicits.MultipartIdentifierHelper\n    if (nestedPredicatePushdownEnabled) {\n      extractNestedCol(e).map(_.quoted)\n    } else {"
  },
  {
    "id" : "0b5f906f-53fd-49fa-bfe6-6abeb80f5c9c",
    "prId" : 30984,
    "prUrl" : "https://github.com/apache/spark/pull/30984#pullrequestreview-563971476",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ff8ae813-966e-4c34-842e-cb886b23110f",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "How do we distinguish the actual field name vs the pushed down `GetJsonObject` in the Data source implementation side?",
        "createdAt" : "2021-01-03T01:03:49Z",
        "updatedAt" : "2021-01-11T22:36:30Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "60a1b260-4840-4354-8c67-af47976df11e",
        "parentId" : "ff8ae813-966e-4c34-842e-cb886b23110f",
        "authorId" : "577bf34d-3430-4150-a920-e49f0db66262",
        "body" : "I gave an example for downstream implementation here:\r\n\r\nhttps://issues.apache.org/jira/browse/SPARK-33915?focusedCommentId=17257647&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17257647",
        "createdAt" : "2021-01-03T03:46:42Z",
        "updatedAt" : "2021-01-11T22:36:30Z",
        "lastEditedBy" : "577bf34d-3430-4150-a920-e49f0db66262",
        "tags" : [
        ]
      },
      {
        "id" : "9b984a3f-be24-4073-936a-40ac75257324",
        "parentId" : "ff8ae813-966e-4c34-842e-cb886b23110f",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I know it is rather rare case but how do we distinguish `GetJsonObject(col, field)` as a field name vs as the expression pushed down? I doubt if this is a general way that we can leverage to push expressions down.",
        "createdAt" : "2021-01-03T04:00:46Z",
        "updatedAt" : "2021-01-11T22:36:30Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "6c7a4744-45a8-4b5f-96a4-2805727e27ab",
        "parentId" : "ff8ae813-966e-4c34-842e-cb886b23110f",
        "authorId" : "577bf34d-3430-4150-a920-e49f0db66262",
        "body" : "Can you outline an example where GetJsonObject(col, field) is the field name ?\r\n\r\nthanks",
        "createdAt" : "2021-01-03T05:15:12Z",
        "updatedAt" : "2021-01-11T22:36:30Z",
        "lastEditedBy" : "577bf34d-3430-4150-a920-e49f0db66262",
        "tags" : [
        ]
      },
      {
        "id" : "6fb72144-f453-4d90-a2f5-86534ff5a3d7",
        "parentId" : "ff8ae813-966e-4c34-842e-cb886b23110f",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "For example,\r\n\r\n```\r\nscala> spark.range(1).toDF(\"GetJsonObject(col, field)\").show()\r\n+-------------------------+\r\n|GetJsonObject(col, field)|\r\n+-------------------------+\r\n|                        0|\r\n+-------------------------+\r\n\r\n\r\nscala> spark.range(1).selectExpr(\"id as `GetJsonObject(col, field)`\").show()\r\n+-------------------------+\r\n|GetJsonObject(col, field)|\r\n+-------------------------+\r\n|                        0|\r\n+-------------------------+\r\n```",
        "createdAt" : "2021-01-04T05:31:30Z",
        "updatedAt" : "2021-01-11T22:36:30Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "291c2ea9-26f7-4dde-a6da-b52c7f23c39e",
        "parentId" : "ff8ae813-966e-4c34-842e-cb886b23110f",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "How can we make sure all the data sources support `GetJsonObject` function?",
        "createdAt" : "2021-01-04T16:23:18Z",
        "updatedAt" : "2021-01-11T22:36:30Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "8e1bb8ca-d36d-4d93-b269-63dae40f316c",
        "parentId" : "ff8ae813-966e-4c34-842e-cb886b23110f",
        "authorId" : "577bf34d-3430-4150-a920-e49f0db66262",
        "body" : "The above question is orthogonal to @HyukjinKwon 's question, let me resolve that question and answer using another comment.",
        "createdAt" : "2021-01-04T16:52:53Z",
        "updatedAt" : "2021-01-11T22:36:30Z",
        "lastEditedBy" : "577bf34d-3430-4150-a920-e49f0db66262",
        "tags" : [
        ]
      },
      {
        "id" : "8fc01b84-11f8-415e-9d2c-126ba753e488",
        "parentId" : "ff8ae813-966e-4c34-842e-cb886b23110f",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@tedyu, I think there was something wrong when you build Spark. Just for doubly sure, downloading the latest Spark and running it works as below:\r\n\r\n```\r\nscala> spark.range(1).toDF(\"GetJsonObject(phone#37,$.phone)\").show()\r\n+-------------------------------+\r\n|GetJsonObject(phone#37,$.phone)|\r\n+-------------------------------+\r\n|                              0|\r\n+-------------------------------+\r\n```",
        "createdAt" : "2021-01-05T00:18:48Z",
        "updatedAt" : "2021-01-11T22:36:30Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "120aba43-60d7-400d-8e2e-fdb65e9c44f7",
        "parentId" : "ff8ae813-966e-4c34-842e-cb886b23110f",
        "authorId" : "577bf34d-3430-4150-a920-e49f0db66262",
        "body" : "I rebuilt Spark locally (with this change) and was able to get the same output for the query you gave above.\r\n\r\nPlease note that get_json_object should be used for SQL instead of GetJsonObject().\r\n```\r\nscala> spark.sql(\"select GetJsonObject(phone,'$.code') from t\")\r\norg.apache.spark.sql.AnalysisException: Undefined function: 'GetJsonObject'. This function is neither a registered temporary function nor a permanent function registered in the database 'default'.; line 1 pos 7\r\n  at org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$$anonfun$apply$16.$anonfun$applyOrElse$142(Analyzer.scala:2062)\r\n  at org.apache.spark.sql.catalyst.analysis.package$.withPosition(package.scala:53)\r\n```\r\n\r\nI tried the following:\r\n```\r\nscala> spark.sql(\"CREATE TABLE IF NOT EXISTS t(id int , phone string)\")\r\n...\r\nscala> spark.sql(\"select get_json_object(phone,'$.code') from t\")\r\nres10: org.apache.spark.sql.DataFrame = [get_json_object(phone, $.code): string]\r\n```\r\nIf an invalid column is specified:\r\n```\r\nscala> spark.sql(\"select get_json_object(phon,'$.code') from t\")\r\norg.apache.spark.sql.AnalysisException: cannot resolve '`phon`' given input columns: [spark_catalog.default.t.id, spark_catalog.default.t.phone]; line 1 pos 23;\r\n'Project [unresolvedalias('get_json_object('phon, $.code), None)]\r\n+- SubqueryAlias spark_catalog.default.t\r\n   +- HiveTableRelation [`default`.`t`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#17, phone#18], Partition Cols: []]\r\n```",
        "createdAt" : "2021-01-05T01:15:07Z",
        "updatedAt" : "2021-01-11T22:36:30Z",
        "lastEditedBy" : "577bf34d-3430-4150-a920-e49f0db66262",
        "tags" : [
        ]
      },
      {
        "id" : "72718d7a-e008-4ba8-a1fa-ac9a93963c3c",
        "parentId" : "ff8ae813-966e-4c34-842e-cb886b23110f",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@tedyu, it should be backquoted because it contains special characters:\r\n\r\n```scala\r\nspark.sql(\"select `GetJsonObject(phone,'$.code')` from t\")\r\n```\r\n\r\nI am more asking how the downstream DSv2 implementation should handle the string `GetJsonObject(phone,'$.code')`. Is it a field name or pushed JSON expression? This seems ambiguous.",
        "createdAt" : "2021-01-08T01:10:41Z",
        "updatedAt" : "2021-01-11T22:36:30Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "7a337b9c-9c90-41b1-99b3-39d2cc6a7026",
        "parentId" : "ff8ae813-966e-4c34-842e-cb886b23110f",
        "authorId" : "577bf34d-3430-4150-a920-e49f0db66262",
        "body" : "Running the above statement in a shell, I got the following:\r\n```\r\nscala> spark.sql(\"select * from t\")\r\nres2: org.apache.spark.sql.DataFrame = [id: int, phone: string]\r\n\r\norg.apache.spark.sql.AnalysisException: cannot resolve '`GetJsonObject(phone,'$.code')`' given input columns: [spark_catalog.default.t.id, spark_catalog.default.t.phone]; line 1 pos 7;\r\n'Project ['`GetJsonObject(phone,'$.code')`]\r\n+- SubqueryAlias spark_catalog.default.t\r\n   +- HiveTableRelation [`default`.`t`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#0, phone#1], Partition Cols: []]\r\n\r\n  at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\r\n  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(CheckAnalysis.scala:170)\r\n  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(CheckAnalysis.scala:167)\r\n  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$2(TreeNode.scala:341)\r\n  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:73)\r\n  at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:341)\r\n  at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsUp$1(QueryPlan.scala:104)\r\n  at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:116)\r\n  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:73)\r\n  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:116)\r\n  at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:127)\r\n  at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$3(QueryPlan.scala:132)\r\n  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\r\n  at scala.collection.immutable.List.foreach(List.scala:392)\r\n  at scala.collection.TraversableLike.map(TraversableLike.scala:238)\r\n  at scala.collection.TraversableLike.map$(TraversableLike.scala:231)\r\n  at scala.collection.immutable.List.map(List.scala:298)\r\n  at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:132)\r\n  at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:137)\r\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)\r\n  at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:137)\r\n  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:104)\r\n  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:167)\r\n  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:94)\r\n  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:183)\r\n  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:94)\r\n```\r\nI don't think there is ambiguity.",
        "createdAt" : "2021-01-08T02:51:24Z",
        "updatedAt" : "2021-01-11T22:36:30Z",
        "lastEditedBy" : "577bf34d-3430-4150-a920-e49f0db66262",
        "tags" : [
        ]
      },
      {
        "id" : "4511704a-ab80-415c-91b0-571fae8e7e10",
        "parentId" : "ff8ae813-966e-4c34-842e-cb886b23110f",
        "authorId" : "577bf34d-3430-4150-a920-e49f0db66262",
        "body" : "I tried creating a table with `GetJsonObject(phone,'$.code')` as column\r\n```\r\nscala> spark.sql(\"CREATE TABLE IF NOT EXISTS t1(`GetJsonObject(phone,'$.code')` int)\")\r\n21/01/08 02:56:34 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\r\n21/01/08 02:56:36 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\r\n21/01/08 02:56:36 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\r\n21/01/08 02:56:40 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\r\n21/01/08 02:56:40 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore tedyu@10.150.0.9\r\norg.apache.spark.sql.AnalysisException: Cannot create a table having a column whose name contains commas in Hive metastore. Table: `default`.`t1`; Column: GetJsonObject(phone,'$.code')\r\n  at org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$verifyDataSchema$4(HiveExternalCatalog.scala:175)\r\n  at org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$verifyDataSchema$4$adapted(HiveExternalCatalog.scala:171)\r\n  at scala.collection.Iterator.foreach(Iterator.scala:941)\r\n  at scala.collection.Iterator.foreach$(Iterator.scala:941)\r\n  at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\r\n  at scala.collection.IterableLike.foreach(IterableLike.scala:74)\r\n  at scala.collection.IterableLike.foreach$(IterableLike.scala:73)\r\n  at org.apache.spark.sql.types.StructType.foreach(StructType.scala:102)\r\n  at org.apache.spark.sql.hive.HiveExternalCatalog.verifyDataSchema(HiveExternalCatalog.scala:171)\r\n  at org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$createTable$1(HiveExternalCatalog.scala:252)\r\n  at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n  at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:102)\r\n  at org.apache.spark.sql.hive.HiveExternalCatalog.createTable(HiveExternalCatalog.scala:245)\r\n  at org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.createTable(ExternalCatalogWithListener.scala:94)\r\n  at org.apache.spark.sql.catalyst.catalog.SessionCatalog.createTable(SessionCatalog.scala:351)\r\n  at org.apache.spark.sql.execution.command.CreateTableCommand.run(tables.scala:167)\r\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\r\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\r\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)\r\n```",
        "createdAt" : "2021-01-08T02:54:53Z",
        "updatedAt" : "2021-01-11T22:36:30Z",
        "lastEditedBy" : "577bf34d-3430-4150-a920-e49f0db66262",
        "tags" : [
        ]
      }
    ],
    "commit" : "fe8034dd005e99fb48fdb36ede4c373c7721042c",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +739,743 @@        helper(s.child).map(_ :+ s.childSchema(s.ordinal).name)\n      case GetJsonObject(col, field) if nestedPredicatePushdownEnabled =>\n        Some(Seq(\"GetJsonObject(\" + col + \",\" + field + \")\"))\n      case _ => None\n    }"
  }
]