[
  {
    "id" : "ad56ae04-29bb-4b7f-b593-c56ee416cf46",
    "prId" : 32921,
    "prUrl" : "https://github.com/apache/spark/pull/32921#pullrequestreview-693225391",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9bbb669e-adc3-45ff-8414-adb531e55672",
        "parentId" : null,
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Super minor suggestion, we could maybe add some scaladoc here on the expected behaviour, I note that in one case it rases an exception and in the other case it returns None to indicate it isn't able to handle the filter and I think clarifying that could be good.",
        "createdAt" : "2021-06-25T17:17:16Z",
        "updatedAt" : "2021-06-25T17:17:16Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "e987e7ff-b281-49c1-b5d7-cfcb28bd7d11",
        "parentId" : "9bbb669e-adc3-45ff-8414-adb531e55672",
        "authorId" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "body" : "Will add.",
        "createdAt" : "2021-06-25T20:37:17Z",
        "updatedAt" : "2021-06-25T20:37:17Z",
        "lastEditedBy" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "tags" : [
        ]
      },
      {
        "id" : "c23ca511-b2bd-406d-9ba9-e374cc91ae2f",
        "parentId" : "9bbb669e-adc3-45ff-8414-adb531e55672",
        "authorId" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "body" : "Added. Let me know if it is descriptive enough, @holdenk.",
        "createdAt" : "2021-06-25T22:16:20Z",
        "updatedAt" : "2021-06-25T22:16:21Z",
        "lastEditedBy" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "tags" : [
        ]
      }
    ],
    "commit" : "881d2b2b7246d9453bfa7e074ab19334bf8d9876",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +638,642 @@   * If the underlying subquery hasn't completed yet, this method will throw an exception.\n   */\n  protected[sql] def translateRuntimeFilter(expr: Expression): Option[Filter] = expr match {\n    case in @ InSubqueryExec(e @ PushableColumnAndNestedColumn(name), _, _, _) =>\n      val values = in.values().getOrElse {"
  },
  {
    "id" : "2968395d-4757-4928-84d5-0df68160ac44",
    "prId" : 32807,
    "prUrl" : "https://github.com/apache/spark/pull/32807#pullrequestreview-679358394",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e258a08e-dc37-4ad4-bf89-13a03f54766e",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "note that:\r\n1. nestedPredicatePushdownEnabled is always enabled for DS v2 (by default)\r\n2. nestedPredicatePushdownEnabled is never enabled for DS v1\r\n3. nestedPredicatePushdownEnabled is only enabled for file source parquet and orc (by default)\r\n\r\nAfter changing the quoting logic:\r\n1. DS v1 is not affected\r\n2. file source is builtin so we are fine\r\n3. DS v2 will be affected if the column name contains special chars.\r\n\r\nPersonally, I think the new quoting behavior is better (more ANSI SQL), and most v2 implementations won't be affected as they already need to deal with quoted names.",
        "createdAt" : "2021-06-08T05:01:29Z",
        "updatedAt" : "2021-06-08T05:01:29Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "a604104e-bd95-4f5b-8f0a-4713bcf7345c",
        "parentId" : "e258a08e-dc37-4ad4-bf89-13a03f54766e",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "cc @dbtsai @viirya ",
        "createdAt" : "2021-06-08T05:01:45Z",
        "updatedAt" : "2021-06-08T05:01:45Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "3eb26681-7b33-4a8a-ab58-3d57516e6da9",
        "parentId" : "e258a08e-dc37-4ad4-bf89-13a03f54766e",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "looks good.",
        "createdAt" : "2021-06-09T07:01:18Z",
        "updatedAt" : "2021-06-09T07:01:18Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "756d0ca4-7c83-4310-94a4-26708a4bfd92",
        "parentId" : "e258a08e-dc37-4ad4-bf89-13a03f54766e",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Only exceptional one might be, if a v2 implementation can only handle quoted \"dots\", but not other quoted chars, right? I think it should be rare cases.",
        "createdAt" : "2021-06-09T07:02:51Z",
        "updatedAt" : "2021-06-09T07:02:51Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "ac171554-ce24-4353-8d17-14fb87a42b90",
        "parentId" : "e258a08e-dc37-4ad4-bf89-13a03f54766e",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Yea I think that's rare. V2 implementations should already be using a decent parser that recognizes dots and backticks.",
        "createdAt" : "2021-06-09T08:26:10Z",
        "updatedAt" : "2021-06-09T08:26:11Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "d4d1489594fb1e6a3c54c4ffc1d92511ed874e36",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +700,704 @@  def unapply(e: Expression): Option[String] = {\n    import org.apache.spark.sql.connector.catalog.CatalogV2Implicits.MultipartIdentifierHelper\n    if (nestedPredicatePushdownEnabled) {\n      extractNestedCol(e).map(_.quoted)\n    } else {"
  },
  {
    "id" : "0b5f906f-53fd-49fa-bfe6-6abeb80f5c9c",
    "prId" : 30984,
    "prUrl" : "https://github.com/apache/spark/pull/30984#pullrequestreview-563971476",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ff8ae813-966e-4c34-842e-cb886b23110f",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "How do we distinguish the actual field name vs the pushed down `GetJsonObject` in the Data source implementation side?",
        "createdAt" : "2021-01-03T01:03:49Z",
        "updatedAt" : "2021-01-11T22:36:30Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "60a1b260-4840-4354-8c67-af47976df11e",
        "parentId" : "ff8ae813-966e-4c34-842e-cb886b23110f",
        "authorId" : "577bf34d-3430-4150-a920-e49f0db66262",
        "body" : "I gave an example for downstream implementation here:\r\n\r\nhttps://issues.apache.org/jira/browse/SPARK-33915?focusedCommentId=17257647&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17257647",
        "createdAt" : "2021-01-03T03:46:42Z",
        "updatedAt" : "2021-01-11T22:36:30Z",
        "lastEditedBy" : "577bf34d-3430-4150-a920-e49f0db66262",
        "tags" : [
        ]
      },
      {
        "id" : "9b984a3f-be24-4073-936a-40ac75257324",
        "parentId" : "ff8ae813-966e-4c34-842e-cb886b23110f",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I know it is rather rare case but how do we distinguish `GetJsonObject(col, field)` as a field name vs as the expression pushed down? I doubt if this is a general way that we can leverage to push expressions down.",
        "createdAt" : "2021-01-03T04:00:46Z",
        "updatedAt" : "2021-01-11T22:36:30Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "6c7a4744-45a8-4b5f-96a4-2805727e27ab",
        "parentId" : "ff8ae813-966e-4c34-842e-cb886b23110f",
        "authorId" : "577bf34d-3430-4150-a920-e49f0db66262",
        "body" : "Can you outline an example where GetJsonObject(col, field) is the field name ?\r\n\r\nthanks",
        "createdAt" : "2021-01-03T05:15:12Z",
        "updatedAt" : "2021-01-11T22:36:30Z",
        "lastEditedBy" : "577bf34d-3430-4150-a920-e49f0db66262",
        "tags" : [
        ]
      },
      {
        "id" : "6fb72144-f453-4d90-a2f5-86534ff5a3d7",
        "parentId" : "ff8ae813-966e-4c34-842e-cb886b23110f",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "For example,\r\n\r\n```\r\nscala> spark.range(1).toDF(\"GetJsonObject(col, field)\").show()\r\n+-------------------------+\r\n|GetJsonObject(col, field)|\r\n+-------------------------+\r\n|                        0|\r\n+-------------------------+\r\n\r\n\r\nscala> spark.range(1).selectExpr(\"id as `GetJsonObject(col, field)`\").show()\r\n+-------------------------+\r\n|GetJsonObject(col, field)|\r\n+-------------------------+\r\n|                        0|\r\n+-------------------------+\r\n```",
        "createdAt" : "2021-01-04T05:31:30Z",
        "updatedAt" : "2021-01-11T22:36:30Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "291c2ea9-26f7-4dde-a6da-b52c7f23c39e",
        "parentId" : "ff8ae813-966e-4c34-842e-cb886b23110f",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "How can we make sure all the data sources support `GetJsonObject` function?",
        "createdAt" : "2021-01-04T16:23:18Z",
        "updatedAt" : "2021-01-11T22:36:30Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "8e1bb8ca-d36d-4d93-b269-63dae40f316c",
        "parentId" : "ff8ae813-966e-4c34-842e-cb886b23110f",
        "authorId" : "577bf34d-3430-4150-a920-e49f0db66262",
        "body" : "The above question is orthogonal to @HyukjinKwon 's question, let me resolve that question and answer using another comment.",
        "createdAt" : "2021-01-04T16:52:53Z",
        "updatedAt" : "2021-01-11T22:36:30Z",
        "lastEditedBy" : "577bf34d-3430-4150-a920-e49f0db66262",
        "tags" : [
        ]
      },
      {
        "id" : "8fc01b84-11f8-415e-9d2c-126ba753e488",
        "parentId" : "ff8ae813-966e-4c34-842e-cb886b23110f",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@tedyu, I think there was something wrong when you build Spark. Just for doubly sure, downloading the latest Spark and running it works as below:\r\n\r\n```\r\nscala> spark.range(1).toDF(\"GetJsonObject(phone#37,$.phone)\").show()\r\n+-------------------------------+\r\n|GetJsonObject(phone#37,$.phone)|\r\n+-------------------------------+\r\n|                              0|\r\n+-------------------------------+\r\n```",
        "createdAt" : "2021-01-05T00:18:48Z",
        "updatedAt" : "2021-01-11T22:36:30Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "120aba43-60d7-400d-8e2e-fdb65e9c44f7",
        "parentId" : "ff8ae813-966e-4c34-842e-cb886b23110f",
        "authorId" : "577bf34d-3430-4150-a920-e49f0db66262",
        "body" : "I rebuilt Spark locally (with this change) and was able to get the same output for the query you gave above.\r\n\r\nPlease note that get_json_object should be used for SQL instead of GetJsonObject().\r\n```\r\nscala> spark.sql(\"select GetJsonObject(phone,'$.code') from t\")\r\norg.apache.spark.sql.AnalysisException: Undefined function: 'GetJsonObject'. This function is neither a registered temporary function nor a permanent function registered in the database 'default'.; line 1 pos 7\r\n  at org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$$anonfun$apply$16.$anonfun$applyOrElse$142(Analyzer.scala:2062)\r\n  at org.apache.spark.sql.catalyst.analysis.package$.withPosition(package.scala:53)\r\n```\r\n\r\nI tried the following:\r\n```\r\nscala> spark.sql(\"CREATE TABLE IF NOT EXISTS t(id int , phone string)\")\r\n...\r\nscala> spark.sql(\"select get_json_object(phone,'$.code') from t\")\r\nres10: org.apache.spark.sql.DataFrame = [get_json_object(phone, $.code): string]\r\n```\r\nIf an invalid column is specified:\r\n```\r\nscala> spark.sql(\"select get_json_object(phon,'$.code') from t\")\r\norg.apache.spark.sql.AnalysisException: cannot resolve '`phon`' given input columns: [spark_catalog.default.t.id, spark_catalog.default.t.phone]; line 1 pos 23;\r\n'Project [unresolvedalias('get_json_object('phon, $.code), None)]\r\n+- SubqueryAlias spark_catalog.default.t\r\n   +- HiveTableRelation [`default`.`t`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#17, phone#18], Partition Cols: []]\r\n```",
        "createdAt" : "2021-01-05T01:15:07Z",
        "updatedAt" : "2021-01-11T22:36:30Z",
        "lastEditedBy" : "577bf34d-3430-4150-a920-e49f0db66262",
        "tags" : [
        ]
      },
      {
        "id" : "72718d7a-e008-4ba8-a1fa-ac9a93963c3c",
        "parentId" : "ff8ae813-966e-4c34-842e-cb886b23110f",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@tedyu, it should be backquoted because it contains special characters:\r\n\r\n```scala\r\nspark.sql(\"select `GetJsonObject(phone,'$.code')` from t\")\r\n```\r\n\r\nI am more asking how the downstream DSv2 implementation should handle the string `GetJsonObject(phone,'$.code')`. Is it a field name or pushed JSON expression? This seems ambiguous.",
        "createdAt" : "2021-01-08T01:10:41Z",
        "updatedAt" : "2021-01-11T22:36:30Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "7a337b9c-9c90-41b1-99b3-39d2cc6a7026",
        "parentId" : "ff8ae813-966e-4c34-842e-cb886b23110f",
        "authorId" : "577bf34d-3430-4150-a920-e49f0db66262",
        "body" : "Running the above statement in a shell, I got the following:\r\n```\r\nscala> spark.sql(\"select * from t\")\r\nres2: org.apache.spark.sql.DataFrame = [id: int, phone: string]\r\n\r\norg.apache.spark.sql.AnalysisException: cannot resolve '`GetJsonObject(phone,'$.code')`' given input columns: [spark_catalog.default.t.id, spark_catalog.default.t.phone]; line 1 pos 7;\r\n'Project ['`GetJsonObject(phone,'$.code')`]\r\n+- SubqueryAlias spark_catalog.default.t\r\n   +- HiveTableRelation [`default`.`t`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#0, phone#1], Partition Cols: []]\r\n\r\n  at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\r\n  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(CheckAnalysis.scala:170)\r\n  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(CheckAnalysis.scala:167)\r\n  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$2(TreeNode.scala:341)\r\n  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:73)\r\n  at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:341)\r\n  at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsUp$1(QueryPlan.scala:104)\r\n  at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:116)\r\n  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:73)\r\n  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:116)\r\n  at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:127)\r\n  at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$3(QueryPlan.scala:132)\r\n  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\r\n  at scala.collection.immutable.List.foreach(List.scala:392)\r\n  at scala.collection.TraversableLike.map(TraversableLike.scala:238)\r\n  at scala.collection.TraversableLike.map$(TraversableLike.scala:231)\r\n  at scala.collection.immutable.List.map(List.scala:298)\r\n  at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:132)\r\n  at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:137)\r\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)\r\n  at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:137)\r\n  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:104)\r\n  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:167)\r\n  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:94)\r\n  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:183)\r\n  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:94)\r\n```\r\nI don't think there is ambiguity.",
        "createdAt" : "2021-01-08T02:51:24Z",
        "updatedAt" : "2021-01-11T22:36:30Z",
        "lastEditedBy" : "577bf34d-3430-4150-a920-e49f0db66262",
        "tags" : [
        ]
      },
      {
        "id" : "4511704a-ab80-415c-91b0-571fae8e7e10",
        "parentId" : "ff8ae813-966e-4c34-842e-cb886b23110f",
        "authorId" : "577bf34d-3430-4150-a920-e49f0db66262",
        "body" : "I tried creating a table with `GetJsonObject(phone,'$.code')` as column\r\n```\r\nscala> spark.sql(\"CREATE TABLE IF NOT EXISTS t1(`GetJsonObject(phone,'$.code')` int)\")\r\n21/01/08 02:56:34 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\r\n21/01/08 02:56:36 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\r\n21/01/08 02:56:36 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\r\n21/01/08 02:56:40 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\r\n21/01/08 02:56:40 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore tedyu@10.150.0.9\r\norg.apache.spark.sql.AnalysisException: Cannot create a table having a column whose name contains commas in Hive metastore. Table: `default`.`t1`; Column: GetJsonObject(phone,'$.code')\r\n  at org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$verifyDataSchema$4(HiveExternalCatalog.scala:175)\r\n  at org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$verifyDataSchema$4$adapted(HiveExternalCatalog.scala:171)\r\n  at scala.collection.Iterator.foreach(Iterator.scala:941)\r\n  at scala.collection.Iterator.foreach$(Iterator.scala:941)\r\n  at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\r\n  at scala.collection.IterableLike.foreach(IterableLike.scala:74)\r\n  at scala.collection.IterableLike.foreach$(IterableLike.scala:73)\r\n  at org.apache.spark.sql.types.StructType.foreach(StructType.scala:102)\r\n  at org.apache.spark.sql.hive.HiveExternalCatalog.verifyDataSchema(HiveExternalCatalog.scala:171)\r\n  at org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$createTable$1(HiveExternalCatalog.scala:252)\r\n  at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n  at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:102)\r\n  at org.apache.spark.sql.hive.HiveExternalCatalog.createTable(HiveExternalCatalog.scala:245)\r\n  at org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.createTable(ExternalCatalogWithListener.scala:94)\r\n  at org.apache.spark.sql.catalyst.catalog.SessionCatalog.createTable(SessionCatalog.scala:351)\r\n  at org.apache.spark.sql.execution.command.CreateTableCommand.run(tables.scala:167)\r\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\r\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\r\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)\r\n```",
        "createdAt" : "2021-01-08T02:54:53Z",
        "updatedAt" : "2021-01-11T22:36:30Z",
        "lastEditedBy" : "577bf34d-3430-4150-a920-e49f0db66262",
        "tags" : [
        ]
      }
    ],
    "commit" : "fe8034dd005e99fb48fdb36ede4c373c7721042c",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +739,743 @@        helper(s.child).map(_ :+ s.childSchema(s.ordinal).name)\n      case GetJsonObject(col, field) if nestedPredicatePushdownEnabled =>\n        Some(Seq(\"GetJsonObject(\" + col + \",\" + field + \")\"))\n      case _ => None\n    }"
  },
  {
    "id" : "57c070c1-696c-49b8-b7bf-3a5c50a4e9a6",
    "prId" : 29756,
    "prUrl" : "https://github.com/apache/spark/pull/29756#pullrequestreview-488360751",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "268a3c37-5842-48ea-a747-af5dac3a8daa",
        "parentId" : null,
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "Keep the same behavior with DataFrameReader.table on respecting options(#29712)",
        "createdAt" : "2020-09-15T06:29:24Z",
        "updatedAt" : "2020-09-24T07:05:24Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "d2eb23fedc024b2e88b35baa777363348f9fffb0",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +268,272 @@      table: CatalogTable,\n      extraOptions: CaseInsensitiveStringMap): StreamingRelation = {\n    val dsOptions = DataSourceUtils.generateDatasourceOptions(extraOptions, table)\n    val dataSource = DataSource(\n      sparkSession,"
  },
  {
    "id" : "013738e2-c992-4e09-b58c-7743cc8ea8bd",
    "prId" : 29695,
    "prUrl" : "https://github.com/apache/spark/pull/29695#pullrequestreview-499945110",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "478bebf4-fb30-48d1-9e66-2743782efeae",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "What if `aggregates` has `isDistinct=true` or `filter`?",
        "createdAt" : "2020-09-29T13:29:26Z",
        "updatedAt" : "2021-03-27T18:58:02Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "34665b9e-f17b-4326-a36f-479ea85f8200",
        "parentId" : "478bebf4-fb30-48d1-9e66-2743782efeae",
        "authorId" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "body" : "I will need to change the following to add `isDistinct` and `filter`. Also change `translateAggregate` accordingly. When push down the aggregates, need to check the filter to make sure it can be pushed down too.\r\n```\r\ncase class Avg(column: String, isDistinct: Boolean, filter: Option[Filter]) extends AggregateFunc\r\n\r\ncase class Min(column: String, isDistinct: Boolean, filter: Option[Filter]) extends AggregateFunc\r\n\r\ncase class Max(column: String, isDistinct: Boolean, filter: Option[Filter]) extends AggregateFunc\r\n\r\ncase class Sum(column: String, isDistinct: Boolean, filter: Option[Filter]) extends AggregateFunc\r\n```",
        "createdAt" : "2020-10-01T00:48:46Z",
        "updatedAt" : "2021-03-27T18:58:02Z",
        "lastEditedBy" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "tags" : [
        ]
      }
    ],
    "commit" : "782a0a827fbb1e60b8673d533cbda618df65dc96",
    "line" : 53,
    "diffHunk" : "@@ -1,1 +722,726 @@\n  protected[sql] def translateAggregate(aggregates: AggregateExpression): Option[AggregateFunc] = {\n    aggregates.aggregateFunction match {\n      case min: aggregate.Min =>\n        val colName = columnAsString(min.child)"
  },
  {
    "id" : "6ab5b18c-165d-42aa-bd97-fe729cd0503c",
    "prId" : 29695,
    "prUrl" : "https://github.com/apache/spark/pull/29695#pullrequestreview-627388344",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e952b871-5489-4bfd-80e6-94bc26a7ce53",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "For predicate pushdown, seems we simplify the cases to handle by only looking at column name.\r\n\r\nThis covers a lot of cases but also makes it easy to break. We can begin with simplest case and add more supports later.",
        "createdAt" : "2021-03-17T17:47:02Z",
        "updatedAt" : "2021-03-27T18:58:02Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "0c7d7244-58c8-45e2-a53b-69faec72ed7c",
        "parentId" : "e952b871-5489-4bfd-80e6-94bc26a7ce53",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Let's wait for others. See if there is any other voices.",
        "createdAt" : "2021-03-17T19:20:32Z",
        "updatedAt" : "2021-03-27T18:58:02Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "7206a206-7535-4ce9-ac9b-1d264e14a172",
        "parentId" : "e952b871-5489-4bfd-80e6-94bc26a7ce53",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "> This covers a lot of cases but also makes it easy to break. We can begin with simplest case and add more supports later.\r\n\r\n+1\r\n\r\n",
        "createdAt" : "2021-03-26T07:20:58Z",
        "updatedAt" : "2021-03-27T18:58:02Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "1dd7fafb-695c-4ba7-85ab-cd58e89d35c2",
        "parentId" : "e952b871-5489-4bfd-80e6-94bc26a7ce53",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "+1. It also seems strange to convert binary expression into a \"magic\" string form that is (seems) special to JDBC datasources.  \r\n\r\nI also wonder if we should handle nested columns the same way as `PushableColumnBase`",
        "createdAt" : "2021-04-02T22:26:35Z",
        "updatedAt" : "2021-04-03T00:08:32Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      }
    ],
    "commit" : "782a0a827fbb1e60b8673d533cbda618df65dc96",
    "line" : 36,
    "diffHunk" : "@@ -1,1 +705,709 @@  }\n\n  private def columnAsString(e: Expression): String = e match {\n    case AttributeReference(name, _, _, _) => name\n    case Cast(child, _, _) => columnAsString (child)"
  },
  {
    "id" : "3400982a-d609-495c-87c3-48c6989bfb38",
    "prId" : 29695,
    "prUrl" : "https://github.com/apache/spark/pull/29695#pullrequestreview-627388344",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d34c6b17-3aac-4fa4-9fd6-6198acce990a",
        "parentId" : null,
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "nit: extra space after `columnAsString`.",
        "createdAt" : "2021-04-02T22:27:01Z",
        "updatedAt" : "2021-04-03T00:08:31Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      }
    ],
    "commit" : "782a0a827fbb1e60b8673d533cbda618df65dc96",
    "line" : 38,
    "diffHunk" : "@@ -1,1 +707,711 @@  private def columnAsString(e: Expression): String = e match {\n    case AttributeReference(name, _, _, _) => name\n    case Cast(child, _, _) => columnAsString (child)\n    case Add(left, right, _) =>\n      columnAsString(left) + \" + \" + columnAsString(right)"
  },
  {
    "id" : "d8a19c38-a004-4eb1-ac3e-f865ba1cdf45",
    "prId" : 29695,
    "prUrl" : "https://github.com/apache/spark/pull/29695#pullrequestreview-627388344",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8fa42cb0-6d31-49e5-98c4-60fb2e6f480c",
        "parentId" : null,
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "nit: extra space after `columnAsString`.",
        "createdAt" : "2021-04-02T22:27:06Z",
        "updatedAt" : "2021-04-03T00:08:31Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      }
    ],
    "commit" : "782a0a827fbb1e60b8673d533cbda618df65dc96",
    "line" : 47,
    "diffHunk" : "@@ -1,1 +716,720 @@    case Divide(left, right, _) =>\n      columnAsString(left) + \" / \" + columnAsString(right)\n    case CheckOverflow(child, _, _) => columnAsString (child)\n    case PromotePrecision(child) => columnAsString (child)\n    case _ => \"\""
  },
  {
    "id" : "ba613586-caa7-4ae4-87a4-14e9ea8415d8",
    "prId" : 29695,
    "prUrl" : "https://github.com/apache/spark/pull/29695#pullrequestreview-627388344",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "abe91714-3b39-4b4d-9e58-9f4a1f852f4f",
        "parentId" : null,
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "why this is \"1\"? also should we check if there is more than one elements in `children`?",
        "createdAt" : "2021-04-02T22:33:46Z",
        "updatedAt" : "2021-04-03T00:08:31Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      }
    ],
    "commit" : "782a0a827fbb1e60b8673d533cbda618df65dc96",
    "line" : 68,
    "diffHunk" : "@@ -1,1 +737,741 @@      case count: aggregate.Count =>\n        val columnName = count.children.head match {\n          case Literal(_, _) => \"1\"\n          case _ => columnAsString(count.children.head)\n        }"
  },
  {
    "id" : "dee86e51-bf83-48d5-a7c6-4ae13086a713",
    "prId" : 28272,
    "prUrl" : "https://github.com/apache/spark/pull/28272#pullrequestreview-397129599",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "73744a64-0ad5-4486-9ea7-92844a57287d",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I guess we're treating this as a temp fix for Spark 3.0?\r\nLooks like ideally we should support Java 8 datetime instances for this interface as well when `spark.sql.datetime.java8API.enabled` is enabled. It could cause more confusion. In addition, seems like `spark.sql.datetime.java8API.enabled` is disabled by default, too.",
        "createdAt" : "2020-04-21T08:23:56Z",
        "updatedAt" : "2020-04-21T19:46:14Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "346ad295-b2cf-4426-869b-8de30b046abd",
        "parentId" : "73744a64-0ad5-4486-9ea7-92844a57287d",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I think it's problematic to let the java8 config also control the value type inside `Filter`, as it can break existing DS v1 implementations. It's a bit unfortunate that we don't document clearly what the value type can be for `Filter`, but if we do, it's not user-friendly to say \"the value type depends on xxx config\". This just makes it harder to implement data source filter pushdown.",
        "createdAt" : "2020-04-21T08:54:59Z",
        "updatedAt" : "2020-04-21T19:46:14Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "16e441b6-09b4-4fea-9055-1a8f702bd216",
        "parentId" : "73744a64-0ad5-4486-9ea7-92844a57287d",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "@HyukjinKwon Taking into account https://github.com/apache/spark/pull/23811#issuecomment-580443958, the flag won't be enabled by default in the near future.",
        "createdAt" : "2020-04-21T09:00:09Z",
        "updatedAt" : "2020-04-21T19:46:14Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "2973dd795e739bb6845856388a9f882425d712e6",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +451,455 @@  private def translateLeafNodeFilter(predicate: Expression): Option[Filter] = predicate match {\n    case expressions.EqualTo(PushableColumn(name), Literal(v, t)) =>\n      Some(sources.EqualTo(name, convertToScala(v, t, false)))\n    case expressions.EqualTo(Literal(v, t), PushableColumn(name)) =>\n      Some(sources.EqualTo(name, convertToScala(v, t, false)))"
  }
]