[
  {
    "id" : "e8bd9030-fefd-4674-a16a-b72ed5296e49",
    "prId" : 31413,
    "prUrl" : "https://github.com/apache/spark/pull/31413#pullrequestreview-623210655",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dcd4d40a-b28d-4478-8234-4840bfd7d293",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "It's a bit odd that we call method name as `createNonBucketedReadRDD` but do something with buckets. I guess we could name `createNonBucketedReadRDD` like just `createReadRDD` or `createStandardReadRDD`",
        "createdAt" : "2021-03-29T13:11:32Z",
        "updatedAt" : "2021-03-29T13:11:32Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "03120af6b31af89dbc9fb9aad05045e98d52c699",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +591,595 @@      s\"open cost is considered as scanning $openCostInBytes bytes.\")\n\n    // Filter files with bucket pruning if possible\n    val bucketingEnabled = fsRelation.sparkSession.sessionState.conf.bucketingEnabled\n    val shouldProcess: Path => Boolean = optionalBucketSet match {"
  },
  {
    "id" : "de9777a3-fec5-4ba1-9725-1ffa6b61c36e",
    "prId" : 31413,
    "prUrl" : "https://github.com/apache/spark/pull/31413#pullrequestreview-623231689",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5f7b281e-4183-4bd2-b5a4-0d910d265067",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Hm, it could be one liner:\r\n\r\n```scala\r\n        filePath => BucketingUtils.getBucketId(filePath.getName).forall(bucketSet.get)\r\n```\r\n\r\nIf it looks less readable we could:\r\n\r\n```scala\r\n        filePath => BucketingUtils.getBucketId(filePath.getName).map(bucketSet.get).getOrElse\r\n```\r\n\r\nIf we worry about perf penalty from pattern matching, etc. we could do:\r\n\r\n```scala\r\n        filePath => {\r\n          val bucketId = BucketingUtils.getBucketId(filePath.getName)\r\n          if (bucketId.isEmpty) true else bucketSet.get(bucketId.get)\r\n        }\r\n``` \r\n",
        "createdAt" : "2021-03-29T13:31:51Z",
        "updatedAt" : "2021-03-29T13:31:51Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "03120af6b31af89dbc9fb9aad05045e98d52c699",
    "line" : 25,
    "diffHunk" : "@@ -1,1 +600,604 @@            case None =>\n              // Do not prune the file if bucket file name is invalid\n              true\n          }\n        }"
  },
  {
    "id" : "7650c8c8-93c7-42ba-9490-b234881558ca",
    "prId" : 29804,
    "prUrl" : "https://github.com/apache/spark/pull/29804#pullrequestreview-498644161",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b68d92d5-e145-48b9-98b0-8fc1e7a2bf4e",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "`map` -> `foreach`",
        "createdAt" : "2020-09-22T23:59:12Z",
        "updatedAt" : "2020-10-01T01:11:06Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "8e1e42bc-614a-4db5-b72f-edfe076ee538",
        "parentId" : "b68d92d5-e145-48b9-98b0-8fc1e7a2bf4e",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "@maropu - just for my own education, why does it matter? Updated anyway.",
        "createdAt" : "2020-09-23T06:14:15Z",
        "updatedAt" : "2020-10-01T01:11:06Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      },
      {
        "id" : "3b67c24e-9698-44a3-9090-d70048894e24",
        "parentId" : "b68d92d5-e145-48b9-98b0-8fc1e7a2bf4e",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Yea, I remember the previous discussion: https://issues.apache.org/jira/browse/SPARK-16694",
        "createdAt" : "2020-09-23T09:53:07Z",
        "updatedAt" : "2020-10-01T01:11:06Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "f174cba5-ad3f-43b4-a96e-2f0170dfcec4",
        "parentId" : "b68d92d5-e145-48b9-98b0-8fc1e7a2bf4e",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Yea, please only use `map` when you care about the return value. `foreach` is better if you just want to do some calculation if `Option` is `Some`",
        "createdAt" : "2020-09-29T11:13:46Z",
        "updatedAt" : "2020-10-01T01:11:06Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "c2607647-7599-412d-b0c8-471b3b3c4bd3",
        "parentId" : "b68d92d5-e145-48b9-98b0-8fc1e7a2bf4e",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "@cloud-fan , @maropu - I changed the code during iterations. The current change is just adding a `if (bucketedScan) { ... } else { ... }` on top of original code, where we still need to use `map` as it's returning value.",
        "createdAt" : "2020-09-29T15:57:47Z",
        "updatedAt" : "2020-10-01T01:11:06Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      }
    ],
    "commit" : "b29f688fc4a06bc35effa6d24632d2a64501c9fd",
    "line" : 40,
    "diffHunk" : "@@ -1,1 +355,359 @@    // TODO(SPARK-32986): Add bucketed scan info in explain output of FileSourceScanExec\n    if (bucketedScan) {\n      relation.bucketSpec.map { spec =>\n        val numSelectedBuckets = optionalBucketSet.map { b =>\n          b.cardinality()"
  },
  {
    "id" : "39455706-a2ed-4142-bbcc-96c643a6123e",
    "prId" : 29637,
    "prUrl" : "https://github.com/apache/spark/pull/29637#pullrequestreview-481710317",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "13fc6e89-4351-4652-9cf1-e0f84b20ad6e",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "This change is not needed, but I keep it here for: 1. avoid duplicated code. 2. make it super safe that we only update the `numPartitions` if table is partitioned.",
        "createdAt" : "2020-09-03T10:35:10Z",
        "updatedAt" : "2020-09-03T12:16:53Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "3e4a6a5d9a8f4151735b67c9dc585201e8543ab5",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +436,440 @@      driverMetrics(\"staticFilesSize\") = filesSize\n    }\n    if (relation.partitionSchemaOption.isDefined) {\n      driverMetrics(\"numPartitions\") = partitions.length\n    }"
  },
  {
    "id" : "85155632-6d7e-4b18-be35-2444273c535c",
    "prId" : 29473,
    "prUrl" : "https://github.com/apache/spark/pull/29473#pullrequestreview-476951709",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5c5a5166-f26a-4d1d-badd-7a0160a656b7",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "I'm not sure about how much this columnar execution makes performance gains though, the proposed idea is to give up the gains then use bucket repartitioning instead?",
        "createdAt" : "2020-08-24T13:08:58Z",
        "updatedAt" : "2020-08-30T04:42:06Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "363d1301-688d-4c62-967c-1eb4add56642",
        "parentId" : "5c5a5166-f26a-4d1d-badd-7a0160a656b7",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "Note that the datasource will still be read as batches in this case (if whole stage codegen is enabled).\r\n\r\nI see that physical plans operate on rows, so batches are converted to rows via `ColumnarToRow` anyway. So, I think perf impact would be minimal here; the difference could be the code-gen conversion from columnar to row vs. iterating `batch.rowIterator()` in `BucketRepartitioningRDD`.",
        "createdAt" : "2020-08-27T17:51:21Z",
        "updatedAt" : "2020-08-30T04:42:06Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "366c9c3b9a8872113d7ccf670b3e8bfbb53f892e",
    "line" : 34,
    "diffHunk" : "@@ -1,1 +177,181 @@    // row, thus columnar is not supported when `RepartitioningBucketRDD` is used to avoid\n    // conversions from batches to rows and back to batches.\n    relation.fileFormat.supportBatch(relation.sparkSession, schema) && !isRepartitioningBuckets\n  }\n"
  },
  {
    "id" : "9dff00ff-0025-4669-9221-b0b963871ade",
    "prId" : 29473,
    "prUrl" : "https://github.com/apache/spark/pull/29473#pullrequestreview-477107419",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "865fd48f-b441-4c04-80bf-69cccf8554d2",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "nit: we don't need `: Boolean `?",
        "createdAt" : "2020-08-24T13:09:26Z",
        "updatedAt" : "2020-08-30T04:42:06Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "11f8062b-f526-4cac-acde-74c4f4e376f4",
        "parentId" : "865fd48f-b441-4c04-80bf-69cccf8554d2",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "I followed the same style from `override lazy val supportsColumnar: Boolean`, etc. Is this still not needed?",
        "createdAt" : "2020-08-27T21:47:34Z",
        "updatedAt" : "2020-08-30T04:42:06Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "366c9c3b9a8872113d7ccf670b3e8bfbb53f892e",
    "line" : 37,
    "diffHunk" : "@@ -1,1 +180,184 @@  }\n\n  @transient private lazy val isRepartitioningBuckets: Boolean = {\n    bucketedScan && optionalNewNumBuckets.isDefined &&\n      optionalNewNumBuckets.get > relation.bucketSpec.get.numBuckets"
  },
  {
    "id" : "417a81d6-1253-4398-b346-d1516ed30a40",
    "prId" : 29473,
    "prUrl" : "https://github.com/apache/spark/pull/29473#pullrequestreview-478170995",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5db13e40-1dfb-4c98-a069-2b87c2d2be01",
        "parentId" : null,
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "we don't need `|| isRepartitioningBuckets` right? ",
        "createdAt" : "2020-08-29T06:04:41Z",
        "updatedAt" : "2020-08-30T04:42:06Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      },
      {
        "id" : "1f719506-9fee-415c-9e2c-b65016462136",
        "parentId" : "5db13e40-1dfb-4c98-a069-2b87c2d2be01",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "Repartition can still maintain the sort order whereas coalescing cannot, thus this check is needed.",
        "createdAt" : "2020-08-30T04:34:35Z",
        "updatedAt" : "2020-08-30T04:42:16Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "366c9c3b9a8872113d7ccf670b3e8bfbb53f892e",
    "line" : 57,
    "diffHunk" : "@@ -1,1 +324,328 @@\n        // TODO SPARK-24528 Sort order is currently ignored if buckets are coalesced.\n        if (singleFilePartitions && (optionalNewNumBuckets.isEmpty || isRepartitioningBuckets)) {\n          // TODO Currently Spark does not support writing columns sorting in descending order\n          // so using Ascending order. This can be fixed in future"
  },
  {
    "id" : "c1ac2703-571c-412a-b83f-6e6710da6966",
    "prId" : 29473,
    "prUrl" : "https://github.com/apache/spark/pull/29473#pullrequestreview-478170995",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c2f16647-ac8a-4e3c-8374-495467d9e3bd",
        "parentId" : null,
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "per `setFilesNumAndSizeMetric`, should we set `staticFilesNum` here or `numFiles ` ?",
        "createdAt" : "2020-08-29T06:20:31Z",
        "updatedAt" : "2020-08-30T04:42:06Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      },
      {
        "id" : "ac3d51fc-1234-4463-b14b-9295f57bb63d",
        "parentId" : "c2f16647-ac8a-4e3c-8374-495467d9e3bd",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "I think `staticFilesNum` is used only for dynamic partition pruning:\r\nhttps://github.com/apache/spark/blob/cfe012a4311a8fb1fc3a82390c3e68f6afcb1da6/sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala#L421-L424",
        "createdAt" : "2020-08-30T04:36:17Z",
        "updatedAt" : "2020-08-30T04:42:16Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "366c9c3b9a8872113d7ccf670b3e8bfbb53f892e",
    "line" : 117,
    "diffHunk" : "@@ -1,1 +590,594 @@        val filesNum = filePartitions.map(_.files.size.toLong).sum\n        val filesSize = filePartitions.map(_.files.map(_.length).sum).sum\n        driverMetrics(\"numFiles\") = filesNum\n        driverMetrics(\"filesSize\") = filesSize\n        new BucketRepartitioningRDD("
  },
  {
    "id" : "83676e40-1e10-4a5e-91ed-21ef811a9f03",
    "prId" : 28610,
    "prUrl" : "https://github.com/apache/spark/pull/28610#pullrequestreview-417129240",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c99a79de-66df-4650-af6e-0abef913eb88",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "private?",
        "createdAt" : "2020-05-22T13:35:32Z",
        "updatedAt" : "2020-05-22T22:16:39Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "ca66fb4e-7025-4ace-979e-de54ac0f56d9",
        "parentId" : "c99a79de-66df-4650-af6e-0abef913eb88",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "No, it is used both in DataSourceScanExec and FileSourceScanExec",
        "createdAt" : "2020-05-22T19:00:43Z",
        "updatedAt" : "2020-05-22T22:16:39Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "9d736efdc4f0ef456ae90eac7eb032c9e09ae8ed",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +56,60 @@  protected def metadata: Map[String, String]\n\n  protected val maxMetadataValueLength = 100\n\n  override def simpleString(maxFields: Int): String = {"
  },
  {
    "id" : "9d2b6ee1-d107-4bb7-818a-c615b37c72dc",
    "prId" : 28123,
    "prUrl" : "https://github.com/apache/spark/pull/28123#pullrequestreview-431737529",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "62ec600d-56c8-49fb-8b58-6aaec0a5d84e",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "I think we can have coalesced bucket info in explain. Looks like we still show original bucket numbers.",
        "createdAt" : "2020-06-12T04:08:37Z",
        "updatedAt" : "2020-06-19T04:52:18Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "01a43a09-4441-46f1-a472-a4aca98d0392",
        "parentId" : "62ec600d-56c8-49fb-8b58-6aaec0a5d84e",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "Yes, good idea. @cloud-fan/@maropu Do you want me to include this in this PR or do it as a follow up since this PR is already approved? I am fine with either one. Thanks @viirya!",
        "createdAt" : "2020-06-13T04:00:56Z",
        "updatedAt" : "2020-06-19T04:52:18Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      },
      {
        "id" : "43a722bc-13ad-4e3c-bc36-4b5fdceb7f6d",
        "parentId" : "62ec600d-56c8-49fb-8b58-6aaec0a5d84e",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "It's easy to fix: just update `FileSourceScanExec.metadata`. Let's do it in this PR",
        "createdAt" : "2020-06-16T11:29:25Z",
        "updatedAt" : "2020-06-19T04:52:18Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "708401d5-90d4-465b-9e5a-693a75821cbc",
        "parentId" : "62ec600d-56c8-49fb-8b58-6aaec0a5d84e",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "Will do.",
        "createdAt" : "2020-06-16T17:32:38Z",
        "updatedAt" : "2020-06-19T04:52:18Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "62a04a3e4d94e63b3787533f619e368a7e8d59f6",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +167,171 @@    partitionFilters: Seq[Expression],\n    optionalBucketSet: Option[BitSet],\n    optionalNumCoalescedBuckets: Option[Int],\n    dataFilters: Seq[Expression],\n    tableIdentifier: Option[TableIdentifier])"
  },
  {
    "id" : "bf17b150-2bb6-4520-9d35-975516ca1eb2",
    "prId" : 28123,
    "prUrl" : "https://github.com/apache/spark/pull/28123#pullrequestreview-433513848",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1564cd48-977f-4372-b915-0d2955517233",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Could you add test cases for this change?",
        "createdAt" : "2020-06-18T07:40:01Z",
        "updatedAt" : "2020-06-19T04:52:18Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "e642362a-2b45-47c4-ab50-54651afc0149",
        "parentId" : "1564cd48-977f-4372-b915-0d2955517233",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "Added.",
        "createdAt" : "2020-06-18T17:48:43Z",
        "updatedAt" : "2020-06-19T04:52:18Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "62a04a3e4d94e63b3787533f619e368a7e8d59f6",
    "line" : 44,
    "diffHunk" : "@@ -1,1 +362,366 @@      metadata + (\"SelectedBucketsCount\" ->\n        (s\"$numSelectedBuckets out of ${spec.numBuckets}\" +\n          optionalNumCoalescedBuckets.map { b => s\" (Coalesced to $b)\"}.getOrElse(\"\")))\n    } getOrElse {\n      metadata"
  },
  {
    "id" : "60a7c804-a4af-4f4c-837f-005bf8dc5906",
    "prId" : 27509,
    "prUrl" : "https://github.com/apache/spark/pull/27509#pullrequestreview-395939451",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d48f58e9-50ca-42a7-80d7-a2a968ea53f3",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "These changes are only for DSV1. Could we make the corresponding changes when using DSV2? Open the ticket https://issues.apache.org/jira/browse/SPARK-31480\r\n\r\nAlso, please check the output when the schema is very long. For example, containing 250+ columns. ",
        "createdAt" : "2020-04-18T18:33:48Z",
        "updatedAt" : "2020-04-18T18:33:49Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      }
    ],
    "commit" : "1290cd523d6fdead5399923ea4acac450a5c2175",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +78,82 @@       |(${ExplainUtils.getOpId(this)}) $nodeName ${ExplainUtils.getCodegenId(this)}\n       |${ExplainUtils.generateFieldString(\"Output\", producedAttributes)}\n       |${metadataStr.mkString(\"\\n\")}\n     \"\"\".stripMargin\n  }"
  },
  {
    "id" : "20839fbd-b3c2-4fff-acc6-38b5e4affbf2",
    "prId" : 26356,
    "prUrl" : "https://github.com/apache/spark/pull/26356#pullrequestreview-312305214",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ff70a9c3-0ef7-4594-a95e-93a829da3dd1",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Can you check if there are other places that needs to call `StringUtils.abbreviate` against `simpleString`?\r\n`simpleString` itself, IIRC, abbreviates when it's long unlike `catalogString`.",
        "createdAt" : "2019-11-06T02:53:45Z",
        "updatedAt" : "2019-11-06T03:33:59Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "baea25d4-4235-4e2d-986d-fd04b14df968",
        "parentId" : "ff70a9c3-0ef7-4594-a95e-93a829da3dd1",
        "authorId" : "f06b291a-bc5c-4fd8-9bac-e5a6d035891b",
        "body" : "@HyukjinKwon \r\nStringUtils.abbreviate is used in the two classes below as well : \r\n\r\n-     FileScan\r\n-     CachedRDDBuilder\r\n\r\nFor FileScan, it's also used to abbreviate metadata, so i think we can use maxMetadataValueStringLength there as well.\r\nI just update the code, please review again.",
        "createdAt" : "2019-11-06T03:41:11Z",
        "updatedAt" : "2019-11-06T03:45:49Z",
        "lastEditedBy" : "f06b291a-bc5c-4fd8-9bac-e5a6d035891b",
        "tags" : [
        ]
      },
      {
        "id" : "ff3a4d43-6a25-4353-976a-652abb9989c5",
        "parentId" : "ff70a9c3-0ef7-4594-a95e-93a829da3dd1",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Can we replace this place with, for instance, `truncatedString` (see StructType.scala) and reuse `spark.sql.debug.maxToStringFields` configuration?",
        "createdAt" : "2019-11-06T09:48:40Z",
        "updatedAt" : "2019-11-06T09:48:40Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "e7d51af493a57a3f21524e01fde2acd38509fd1d",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +59,63 @@    val metadataEntries = metadata.toSeq.sorted.map {\n      case (key, value) =>\n        key + \": \" + StringUtils.abbreviate(redact(value),\n          conf.maxMetadataValueStringLength)\n    }"
  },
  {
    "id" : "a1d7c9db-1a97-4426-afed-2915439a2855",
    "prId" : 25600,
    "prUrl" : "https://github.com/apache/spark/pull/25600#pullrequestreview-283809556",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "94de3b93-a665-4ffc-9b15-3e29f8b46165",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "shall we check `DynamicPruningExpression`?",
        "createdAt" : "2019-08-28T07:31:35Z",
        "updatedAt" : "2019-09-04T14:47:15Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "8a8b2105-da28-4744-97a7-0de13f9285cf",
        "parentId" : "94de3b93-a665-4ffc-9b15-3e29f8b46165",
        "authorId" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "body" : "A `DynamicPruningExpression` could also be a bypass condition, the literal `true`. Otherwise it will contain a subquery expression, so this check is better I believe.",
        "createdAt" : "2019-08-28T14:25:22Z",
        "updatedAt" : "2019-09-04T14:47:15Z",
        "lastEditedBy" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "tags" : [
        ]
      },
      {
        "id" : "48a8a9d7-c818-4169-bb28-1dc47bb8b192",
        "parentId" : "94de3b93-a665-4ffc-9b15-3e29f8b46165",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "I also have the same question at first glance. Maybe add a comment?",
        "createdAt" : "2019-09-04T19:06:41Z",
        "updatedAt" : "2019-09-04T19:10:16Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "b00225078c6471b5b01f8a920ab28a2361b8e48b",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +186,190 @@\n  private def isDynamicPruningFilter(e: Expression): Boolean =\n    e.find(_.isInstanceOf[PlanExpression[_]]).isDefined\n\n  @transient private lazy val selectedPartitions: Array[PartitionDirectory] = {"
  },
  {
    "id" : "9587850e-dafc-4662-bc91-fb33dbda3b65",
    "prId" : 25600,
    "prUrl" : "https://github.com/apache/spark/pull/25600#pullrequestreview-280864733",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "65c8e317-8a2f-4242-8dc1-d77a0d3ed275",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "We only access `selectedPartitions` before runtime when `LEGACY_BUCKETED_TABLE_SCAN_OUTPUT_ORDERING` is true. Can we add a todo that, once we remove `LEGACY_BUCKETED_TABLE_SCAN_OUTPUT_ORDERING`, we can merge `selectedPartitions` and `dynamicallySelectedPartitions`?",
        "createdAt" : "2019-08-28T07:37:28Z",
        "updatedAt" : "2019-09-04T14:47:15Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "42d807ed-2372-4447-8ace-bbb21142d3d1",
        "parentId" : "65c8e317-8a2f-4242-8dc1-d77a0d3ed275",
        "authorId" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "body" : "I'm +1 on that. But maybe we should still get the statically filtered partitions as a local variable and set the static metrics. ",
        "createdAt" : "2019-08-28T14:18:52Z",
        "updatedAt" : "2019-09-04T14:47:15Z",
        "lastEditedBy" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "tags" : [
        ]
      }
    ],
    "commit" : "b00225078c6471b5b01f8a920ab28a2361b8e48b",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +207,211 @@  // present. This is because such a filter relies on information that is only available at run\n  // time (for instance the keys used in the other side of a join).\n  @transient private lazy val dynamicallySelectedPartitions: Array[PartitionDirectory] = {\n    val dynamicPartitionFilters = partitionFilters.filter(isDynamicPruningFilter)\n"
  },
  {
    "id" : "bdde291c-c5ea-4733-845e-e404f688a579",
    "prId" : 25556,
    "prUrl" : "https://github.com/apache/spark/pull/25556#pullrequestreview-278773736",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3efaa29a-8b64-4989-8b3b-2e89d3c92ade",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "`conf.orderByFilePathWhenPartitioning`?",
        "createdAt" : "2019-08-23T02:07:14Z",
        "updatedAt" : "2019-08-23T02:07:15Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "497b0bb2-93f8-4a82-abf2-8c149cf39fcc",
        "parentId" : "3efaa29a-8b64-4989-8b3b-2e89d3c92ade",
        "authorId" : "59c055a4-ae32-4942-b7af-4170ce2e8d61",
        "body" : "@maropu Do you mean this name is not suitable?",
        "createdAt" : "2019-08-23T02:43:11Z",
        "updatedAt" : "2019-08-23T02:43:11Z",
        "lastEditedBy" : "59c055a4-ae32-4942-b7af-4170ce2e8d61",
        "tags" : [
        ]
      }
    ],
    "commit" : "9aca1719235985bb9068745941734c3158de68c2",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +483,487 @@    }\n\n    val orderByFilePath = fsRelation.sparkSession.sessionState.conf.orderByFilePathWhenPartitioning\n    val splitFiles = {\n      if (orderByFilePath) {"
  },
  {
    "id" : "b1b21f5c-150d-4f72-88d7-89a658da1109",
    "prId" : 25328,
    "prUrl" : "https://github.com/apache/spark/pull/25328#pullrequestreview-271149080",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f3d28bfb-7068-47b5-87bd-9e9b91d02c55",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Although previously it is `PartitionCount`, `numPartitions` looks more consistent.",
        "createdAt" : "2019-08-06T07:02:17Z",
        "updatedAt" : "2019-08-07T05:56:48Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "bf9b2617f81dd778d0d1355a66c1e664e7bc1f3b",
    "line" : 64,
    "diffHunk" : "@@ -1,1 +351,355 @@  } ++ {\n    if (relation.partitionSchemaOption.isDefined) {\n      Some(\"numPartitions\" -> SQLMetrics.createMetric(sparkContext, \"number of partitions read\"))\n    } else {\n      None"
  },
  {
    "id" : "46ff54d8-2972-4cac-8fac-ec0fe391d1e7",
    "prId" : 25008,
    "prUrl" : "https://github.com/apache/spark/pull/25008#pullrequestreview-260173799",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3ec60978-e278-4417-b41b-44990a01a462",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Is this change making all `DataSourceScanExec` nodes not codegen support, right?",
        "createdAt" : "2019-07-10T15:08:23Z",
        "updatedAt" : "2019-07-10T15:36:54Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "e2494e4e-65ed-4daa-a344-ec3d8f38b8f9",
        "parentId" : "3ec60978-e278-4417-b41b-44990a01a462",
        "authorId" : "b19fe247-920f-40ae-83e7-6b8ec9979f6b",
        "body" : "Correct, but there were only 2 things that the code gen was doing. Either convert ColumnarBatch into UnsafeRows or to convert whatever other rows were being returned by the DataSourceScanExec into UnsafeRows.  The ColumnarBatch conversion is now covered by ColumnarToRowExec.  The row to row conversion is covered by UnsafeProjections that are either inserted as a part of this patch or were already in the code, so we ended up doing a double conversion. ",
        "createdAt" : "2019-07-10T15:24:10Z",
        "updatedAt" : "2019-07-10T15:36:54Z",
        "lastEditedBy" : "b19fe247-920f-40ae-83e7-6b8ec9979f6b",
        "tags" : [
        ]
      }
    ],
    "commit" : "2cce2fa9e057cb379e628fe01ea2cef280a9b198",
    "line" : 31,
    "diffHunk" : "@@ -1,1 +149,153 @@    dataFilters: Seq[Expression],\n    override val tableIdentifier: Option[TableIdentifier])\n  extends DataSourceScanExec {\n\n  // Note that some vals referring the file-based relation are lazy intentionally"
  },
  {
    "id" : "d281fcd7-567f-49e1-bc91-60b25dc1a3c1",
    "prId" : 24527,
    "prUrl" : "https://github.com/apache/spark/pull/24527#pullrequestreview-233814885",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f9ca9139-ecf3-43cf-9a64-d6ee25fc08c7",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Hi, @LantaoJin . It would be very helpful if you provide a test case for your following claim.\r\n> Splitting RDD to too many small pieces doesn't make sense. Jobs will launch too many partitions and never complete.",
        "createdAt" : "2019-05-05T15:55:51Z",
        "updatedAt" : "2019-05-05T15:55:51Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "b64912ec-98cf-4f78-a9cb-0ec62a42d086",
        "parentId" : "f9ca9139-ecf3-43cf-9a64-d6ee25fc08c7",
        "authorId" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "body" : "It may be hard to provide a UT. This case only happens in one of our jobs which we enable multi-thread to read from one HDFS folder and write to different target HDFS folders with different filters. With DRA enabled and the job launched near 2000 executors with near 8000 active tasks. When the job runs for a while, the task number of filter/scan stages increases from 200 to over 5000. And we got many below logs:\r\n\r\n> 19/04/29 06:13:48 INFO FileSourceScanExec: Planning scan with bin packing, max size: 129026539 bytes, open cost is considered as scanning 4194304 bytes.\r\n19/04/29 06:13:48 INFO FileSourceScanExec: Planning scan with bin packing, max size: 129026539 bytes, open cost is considered as scanning 4194304 bytes.\r\n19/04/29 06:13:49 INFO FileSourceScanExec: Planning scan with bin packing, max size: 129026539 bytes, open cost is considered as scanning 4194304 bytes.\r\n\r\nChanged to\r\n> 19/04/29 06:15:49 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4474908 bytes, open cost is considered as scanning 4194304 bytes.\r\n19/04/29 06:16:15 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\r\n19/04/29 06:16:21 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\r\n19/04/29 06:16:23 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\r\n19/04/29 06:16:23 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\r\n\r\nThis issue would gone in four cases:\r\n\r\n1. set \"spark.default.parallelism\" to a fixed value.\r\n2. Disable DRA and set num-executors to a low value.\r\n3. The app can not get too many resources to launch executors\r\n4. Run jobs one by one instead multi-thread to run. \r\n\r\nAll of above will prevent app to require too many partitions since less cores:\r\n```scala\r\n  override def defaultParallelism(): Int = { //  if not set, more resources, more cores\r\n    conf.getInt(\"spark.default.parallelism\", math.max(totalCoreCount.get(), 2))\r\n  }\r\n\r\n  def maxSplitBytes(\r\n      sparkSession: SparkSession,\r\n      selectedPartitions: Seq[PartitionDirectory]): Long = {\r\n    val defaultMaxSplitBytes = sparkSession.sessionState.conf.filesMaxPartitionBytes\r\n    val openCostInBytes = sparkSession.sessionState.conf.filesOpenCostInBytes\r\n    val defaultParallelism = sparkSession.sparkContext.defaultParallelism\r\n    val totalBytes = selectedPartitions.flatMap(_.files.map(_.getLen + openCostInBytes)).sum\r\n    val bytesPerCore = totalBytes / defaultParallelism // more cores, less bytesPerCore\r\n    // less bytesPerCore, less maxSplitBytes\r\n    Math.min(defaultMaxSplitBytes, Math.max(openCostInBytes, bytesPerCore))\r\n  }\r\n\r\n  def splitFiles(\r\n      sparkSession: SparkSession,\r\n      file: FileStatus,\r\n      filePath: Path,\r\n      isSplitable: Boolean,\r\n      maxSplitBytes: Long,\r\n      partitionValues: InternalRow): Seq[PartitionedFile] = {\r\n    if (isSplitable) {\r\n      (0L until file.getLen by maxSplitBytes).map { offset => // less maxSplitBytes, more partitions\r\n        val remaining = file.getLen - offset\r\n        val size = if (remaining > maxSplitBytes) maxSplitBytes else remaining\r\n        val hosts = getBlockHosts(getBlockLocations(file), offset, size)\r\n        PartitionedFile(partitionValues, filePath.toUri.toString, offset, size, hosts)\r\n      }\r\n    } else {\r\n      Seq(getPartitionedFile(file, filePath, partitionValues))\r\n    }\r\n  }\r\n```",
        "createdAt" : "2019-05-06T02:49:39Z",
        "updatedAt" : "2019-05-06T02:51:19Z",
        "lastEditedBy" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "tags" : [
        ]
      }
    ],
    "commit" : "ba43e198bebbcde0a5a6dc400cd3aff653e8f503",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +426,430 @@      case _ =>\n        FilePartition.maxSplitBytes(fsRelation.sparkSession, selectedPartitions)\n    }\n    logInfo(s\"Planning scan with bin packing, max size: $maxSplitBytes bytes, \" +\n      s\"open cost is considered as scanning $openCostInBytes bytes.\")"
  },
  {
    "id" : "0d7423f7-d8c3-47dc-a427-a14a1d83fcf3",
    "prId" : 24527,
    "prUrl" : "https://github.com/apache/spark/pull/24527#pullrequestreview-233821950",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3371d640-b9c1-4ee2-8288-a5e10117b8c7",
        "parentId" : null,
        "authorId" : "99156419-7ce7-4671-b858-d0b5c4711f88",
        "body" : "shouldn't this be something provided by the ParquetSource?",
        "createdAt" : "2019-05-06T03:35:15Z",
        "updatedAt" : "2019-05-06T03:35:18Z",
        "lastEditedBy" : "99156419-7ce7-4671-b858-d0b5c4711f88",
        "tags" : [
        ]
      },
      {
        "id" : "4798cc12-6b4f-44ca-85d0-7b0609e770ed",
        "parentId" : "3371d640-b9c1-4ee2-8288-a5e10117b8c7",
        "authorId" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "body" : "Actually, `ParquetSource` here is alias of `ParquetFileFormat`\r\n```scala\r\nimport org.apache.spark.sql.execution.datasources.parquet.{ParquetFileFormat => ParquetSource}\r\n```",
        "createdAt" : "2019-05-06T04:01:01Z",
        "updatedAt" : "2019-05-06T04:01:02Z",
        "lastEditedBy" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "tags" : [
        ]
      },
      {
        "id" : "d8944453-fca5-45d8-9c5e-68ec1eef5c0d",
        "parentId" : "3371d640-b9c1-4ee2-8288-a5e10117b8c7",
        "authorId" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "body" : "Sorry, please ignore. Currently, `FileFormat` doesn't provide this. How to split is determined in `DataSourceScanExec`",
        "createdAt" : "2019-05-06T04:09:44Z",
        "updatedAt" : "2019-05-06T04:09:44Z",
        "lastEditedBy" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "tags" : [
        ]
      }
    ],
    "commit" : "ba43e198bebbcde0a5a6dc400cd3aff653e8f503",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +423,427 @@    val maxSplitBytes = relation.fileFormat match {\n      case _ : ParquetSource =>\n        fsRelation.sparkSession.sessionState.conf.filesMaxPartitionBytes // parquet.block.size\n      case _ =>\n        FilePartition.maxSplitBytes(fsRelation.sparkSession, selectedPartitions)"
  }
]