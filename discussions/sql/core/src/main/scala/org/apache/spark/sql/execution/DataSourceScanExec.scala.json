[
  {
    "id" : "e8bd9030-fefd-4674-a16a-b72ed5296e49",
    "prId" : 31413,
    "prUrl" : "https://github.com/apache/spark/pull/31413#pullrequestreview-623210655",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dcd4d40a-b28d-4478-8234-4840bfd7d293",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "It's a bit odd that we call method name as `createNonBucketedReadRDD` but do something with buckets. I guess we could name `createNonBucketedReadRDD` like just `createReadRDD` or `createStandardReadRDD`",
        "createdAt" : "2021-03-29T13:11:32Z",
        "updatedAt" : "2021-03-29T13:11:32Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "03120af6b31af89dbc9fb9aad05045e98d52c699",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +591,595 @@      s\"open cost is considered as scanning $openCostInBytes bytes.\")\n\n    // Filter files with bucket pruning if possible\n    val bucketingEnabled = fsRelation.sparkSession.sessionState.conf.bucketingEnabled\n    val shouldProcess: Path => Boolean = optionalBucketSet match {"
  },
  {
    "id" : "de9777a3-fec5-4ba1-9725-1ffa6b61c36e",
    "prId" : 31413,
    "prUrl" : "https://github.com/apache/spark/pull/31413#pullrequestreview-623231689",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5f7b281e-4183-4bd2-b5a4-0d910d265067",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Hm, it could be one liner:\r\n\r\n```scala\r\n        filePath => BucketingUtils.getBucketId(filePath.getName).forall(bucketSet.get)\r\n```\r\n\r\nIf it looks less readable we could:\r\n\r\n```scala\r\n        filePath => BucketingUtils.getBucketId(filePath.getName).map(bucketSet.get).getOrElse\r\n```\r\n\r\nIf we worry about perf penalty from pattern matching, etc. we could do:\r\n\r\n```scala\r\n        filePath => {\r\n          val bucketId = BucketingUtils.getBucketId(filePath.getName)\r\n          if (bucketId.isEmpty) true else bucketSet.get(bucketId.get)\r\n        }\r\n``` \r\n",
        "createdAt" : "2021-03-29T13:31:51Z",
        "updatedAt" : "2021-03-29T13:31:51Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "03120af6b31af89dbc9fb9aad05045e98d52c699",
    "line" : 25,
    "diffHunk" : "@@ -1,1 +600,604 @@            case None =>\n              // Do not prune the file if bucket file name is invalid\n              true\n          }\n        }"
  },
  {
    "id" : "7650c8c8-93c7-42ba-9490-b234881558ca",
    "prId" : 29804,
    "prUrl" : "https://github.com/apache/spark/pull/29804#pullrequestreview-498644161",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b68d92d5-e145-48b9-98b0-8fc1e7a2bf4e",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "`map` -> `foreach`",
        "createdAt" : "2020-09-22T23:59:12Z",
        "updatedAt" : "2020-10-01T01:11:06Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "8e1e42bc-614a-4db5-b72f-edfe076ee538",
        "parentId" : "b68d92d5-e145-48b9-98b0-8fc1e7a2bf4e",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "@maropu - just for my own education, why does it matter? Updated anyway.",
        "createdAt" : "2020-09-23T06:14:15Z",
        "updatedAt" : "2020-10-01T01:11:06Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      },
      {
        "id" : "3b67c24e-9698-44a3-9090-d70048894e24",
        "parentId" : "b68d92d5-e145-48b9-98b0-8fc1e7a2bf4e",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Yea, I remember the previous discussion: https://issues.apache.org/jira/browse/SPARK-16694",
        "createdAt" : "2020-09-23T09:53:07Z",
        "updatedAt" : "2020-10-01T01:11:06Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "f174cba5-ad3f-43b4-a96e-2f0170dfcec4",
        "parentId" : "b68d92d5-e145-48b9-98b0-8fc1e7a2bf4e",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Yea, please only use `map` when you care about the return value. `foreach` is better if you just want to do some calculation if `Option` is `Some`",
        "createdAt" : "2020-09-29T11:13:46Z",
        "updatedAt" : "2020-10-01T01:11:06Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "c2607647-7599-412d-b0c8-471b3b3c4bd3",
        "parentId" : "b68d92d5-e145-48b9-98b0-8fc1e7a2bf4e",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "@cloud-fan , @maropu - I changed the code during iterations. The current change is just adding a `if (bucketedScan) { ... } else { ... }` on top of original code, where we still need to use `map` as it's returning value.",
        "createdAt" : "2020-09-29T15:57:47Z",
        "updatedAt" : "2020-10-01T01:11:06Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      }
    ],
    "commit" : "b29f688fc4a06bc35effa6d24632d2a64501c9fd",
    "line" : 40,
    "diffHunk" : "@@ -1,1 +355,359 @@    // TODO(SPARK-32986): Add bucketed scan info in explain output of FileSourceScanExec\n    if (bucketedScan) {\n      relation.bucketSpec.map { spec =>\n        val numSelectedBuckets = optionalBucketSet.map { b =>\n          b.cardinality()"
  },
  {
    "id" : "39455706-a2ed-4142-bbcc-96c643a6123e",
    "prId" : 29637,
    "prUrl" : "https://github.com/apache/spark/pull/29637#pullrequestreview-481710317",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "13fc6e89-4351-4652-9cf1-e0f84b20ad6e",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "This change is not needed, but I keep it here for: 1. avoid duplicated code. 2. make it super safe that we only update the `numPartitions` if table is partitioned.",
        "createdAt" : "2020-09-03T10:35:10Z",
        "updatedAt" : "2020-09-03T12:16:53Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "3e4a6a5d9a8f4151735b67c9dc585201e8543ab5",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +436,440 @@      driverMetrics(\"staticFilesSize\") = filesSize\n    }\n    if (relation.partitionSchemaOption.isDefined) {\n      driverMetrics(\"numPartitions\") = partitions.length\n    }"
  },
  {
    "id" : "85155632-6d7e-4b18-be35-2444273c535c",
    "prId" : 29473,
    "prUrl" : "https://github.com/apache/spark/pull/29473#pullrequestreview-476951709",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5c5a5166-f26a-4d1d-badd-7a0160a656b7",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "I'm not sure about how much this columnar execution makes performance gains though, the proposed idea is to give up the gains then use bucket repartitioning instead?",
        "createdAt" : "2020-08-24T13:08:58Z",
        "updatedAt" : "2020-08-30T04:42:06Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "363d1301-688d-4c62-967c-1eb4add56642",
        "parentId" : "5c5a5166-f26a-4d1d-badd-7a0160a656b7",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "Note that the datasource will still be read as batches in this case (if whole stage codegen is enabled).\r\n\r\nI see that physical plans operate on rows, so batches are converted to rows via `ColumnarToRow` anyway. So, I think perf impact would be minimal here; the difference could be the code-gen conversion from columnar to row vs. iterating `batch.rowIterator()` in `BucketRepartitioningRDD`.",
        "createdAt" : "2020-08-27T17:51:21Z",
        "updatedAt" : "2020-08-30T04:42:06Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "366c9c3b9a8872113d7ccf670b3e8bfbb53f892e",
    "line" : 34,
    "diffHunk" : "@@ -1,1 +177,181 @@    // row, thus columnar is not supported when `RepartitioningBucketRDD` is used to avoid\n    // conversions from batches to rows and back to batches.\n    relation.fileFormat.supportBatch(relation.sparkSession, schema) && !isRepartitioningBuckets\n  }\n"
  },
  {
    "id" : "9dff00ff-0025-4669-9221-b0b963871ade",
    "prId" : 29473,
    "prUrl" : "https://github.com/apache/spark/pull/29473#pullrequestreview-477107419",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "865fd48f-b441-4c04-80bf-69cccf8554d2",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "nit: we don't need `: Boolean `?",
        "createdAt" : "2020-08-24T13:09:26Z",
        "updatedAt" : "2020-08-30T04:42:06Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "11f8062b-f526-4cac-acde-74c4f4e376f4",
        "parentId" : "865fd48f-b441-4c04-80bf-69cccf8554d2",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "I followed the same style from `override lazy val supportsColumnar: Boolean`, etc. Is this still not needed?",
        "createdAt" : "2020-08-27T21:47:34Z",
        "updatedAt" : "2020-08-30T04:42:06Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "366c9c3b9a8872113d7ccf670b3e8bfbb53f892e",
    "line" : 37,
    "diffHunk" : "@@ -1,1 +180,184 @@  }\n\n  @transient private lazy val isRepartitioningBuckets: Boolean = {\n    bucketedScan && optionalNewNumBuckets.isDefined &&\n      optionalNewNumBuckets.get > relation.bucketSpec.get.numBuckets"
  },
  {
    "id" : "417a81d6-1253-4398-b346-d1516ed30a40",
    "prId" : 29473,
    "prUrl" : "https://github.com/apache/spark/pull/29473#pullrequestreview-478170995",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5db13e40-1dfb-4c98-a069-2b87c2d2be01",
        "parentId" : null,
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "we don't need `|| isRepartitioningBuckets` right? ",
        "createdAt" : "2020-08-29T06:04:41Z",
        "updatedAt" : "2020-08-30T04:42:06Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      },
      {
        "id" : "1f719506-9fee-415c-9e2c-b65016462136",
        "parentId" : "5db13e40-1dfb-4c98-a069-2b87c2d2be01",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "Repartition can still maintain the sort order whereas coalescing cannot, thus this check is needed.",
        "createdAt" : "2020-08-30T04:34:35Z",
        "updatedAt" : "2020-08-30T04:42:16Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "366c9c3b9a8872113d7ccf670b3e8bfbb53f892e",
    "line" : 57,
    "diffHunk" : "@@ -1,1 +324,328 @@\n        // TODO SPARK-24528 Sort order is currently ignored if buckets are coalesced.\n        if (singleFilePartitions && (optionalNewNumBuckets.isEmpty || isRepartitioningBuckets)) {\n          // TODO Currently Spark does not support writing columns sorting in descending order\n          // so using Ascending order. This can be fixed in future"
  },
  {
    "id" : "c1ac2703-571c-412a-b83f-6e6710da6966",
    "prId" : 29473,
    "prUrl" : "https://github.com/apache/spark/pull/29473#pullrequestreview-478170995",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c2f16647-ac8a-4e3c-8374-495467d9e3bd",
        "parentId" : null,
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "per `setFilesNumAndSizeMetric`, should we set `staticFilesNum` here or `numFiles ` ?",
        "createdAt" : "2020-08-29T06:20:31Z",
        "updatedAt" : "2020-08-30T04:42:06Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      },
      {
        "id" : "ac3d51fc-1234-4463-b14b-9295f57bb63d",
        "parentId" : "c2f16647-ac8a-4e3c-8374-495467d9e3bd",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "I think `staticFilesNum` is used only for dynamic partition pruning:\r\nhttps://github.com/apache/spark/blob/cfe012a4311a8fb1fc3a82390c3e68f6afcb1da6/sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala#L421-L424",
        "createdAt" : "2020-08-30T04:36:17Z",
        "updatedAt" : "2020-08-30T04:42:16Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "366c9c3b9a8872113d7ccf670b3e8bfbb53f892e",
    "line" : 117,
    "diffHunk" : "@@ -1,1 +590,594 @@        val filesNum = filePartitions.map(_.files.size.toLong).sum\n        val filesSize = filePartitions.map(_.files.map(_.length).sum).sum\n        driverMetrics(\"numFiles\") = filesNum\n        driverMetrics(\"filesSize\") = filesSize\n        new BucketRepartitioningRDD("
  },
  {
    "id" : "83676e40-1e10-4a5e-91ed-21ef811a9f03",
    "prId" : 28610,
    "prUrl" : "https://github.com/apache/spark/pull/28610#pullrequestreview-417129240",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c99a79de-66df-4650-af6e-0abef913eb88",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "private?",
        "createdAt" : "2020-05-22T13:35:32Z",
        "updatedAt" : "2020-05-22T22:16:39Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "ca66fb4e-7025-4ace-979e-de54ac0f56d9",
        "parentId" : "c99a79de-66df-4650-af6e-0abef913eb88",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "No, it is used both in DataSourceScanExec and FileSourceScanExec",
        "createdAt" : "2020-05-22T19:00:43Z",
        "updatedAt" : "2020-05-22T22:16:39Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "9d736efdc4f0ef456ae90eac7eb032c9e09ae8ed",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +56,60 @@  protected def metadata: Map[String, String]\n\n  protected val maxMetadataValueLength = 100\n\n  override def simpleString(maxFields: Int): String = {"
  },
  {
    "id" : "9d2b6ee1-d107-4bb7-818a-c615b37c72dc",
    "prId" : 28123,
    "prUrl" : "https://github.com/apache/spark/pull/28123#pullrequestreview-431737529",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "62ec600d-56c8-49fb-8b58-6aaec0a5d84e",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "I think we can have coalesced bucket info in explain. Looks like we still show original bucket numbers.",
        "createdAt" : "2020-06-12T04:08:37Z",
        "updatedAt" : "2020-06-19T04:52:18Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "01a43a09-4441-46f1-a472-a4aca98d0392",
        "parentId" : "62ec600d-56c8-49fb-8b58-6aaec0a5d84e",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "Yes, good idea. @cloud-fan/@maropu Do you want me to include this in this PR or do it as a follow up since this PR is already approved? I am fine with either one. Thanks @viirya!",
        "createdAt" : "2020-06-13T04:00:56Z",
        "updatedAt" : "2020-06-19T04:52:18Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      },
      {
        "id" : "43a722bc-13ad-4e3c-bc36-4b5fdceb7f6d",
        "parentId" : "62ec600d-56c8-49fb-8b58-6aaec0a5d84e",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "It's easy to fix: just update `FileSourceScanExec.metadata`. Let's do it in this PR",
        "createdAt" : "2020-06-16T11:29:25Z",
        "updatedAt" : "2020-06-19T04:52:18Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "708401d5-90d4-465b-9e5a-693a75821cbc",
        "parentId" : "62ec600d-56c8-49fb-8b58-6aaec0a5d84e",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "Will do.",
        "createdAt" : "2020-06-16T17:32:38Z",
        "updatedAt" : "2020-06-19T04:52:18Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "62a04a3e4d94e63b3787533f619e368a7e8d59f6",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +167,171 @@    partitionFilters: Seq[Expression],\n    optionalBucketSet: Option[BitSet],\n    optionalNumCoalescedBuckets: Option[Int],\n    dataFilters: Seq[Expression],\n    tableIdentifier: Option[TableIdentifier])"
  },
  {
    "id" : "bf17b150-2bb6-4520-9d35-975516ca1eb2",
    "prId" : 28123,
    "prUrl" : "https://github.com/apache/spark/pull/28123#pullrequestreview-433513848",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1564cd48-977f-4372-b915-0d2955517233",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Could you add test cases for this change?",
        "createdAt" : "2020-06-18T07:40:01Z",
        "updatedAt" : "2020-06-19T04:52:18Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "e642362a-2b45-47c4-ab50-54651afc0149",
        "parentId" : "1564cd48-977f-4372-b915-0d2955517233",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "Added.",
        "createdAt" : "2020-06-18T17:48:43Z",
        "updatedAt" : "2020-06-19T04:52:18Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "62a04a3e4d94e63b3787533f619e368a7e8d59f6",
    "line" : 44,
    "diffHunk" : "@@ -1,1 +362,366 @@      metadata + (\"SelectedBucketsCount\" ->\n        (s\"$numSelectedBuckets out of ${spec.numBuckets}\" +\n          optionalNumCoalescedBuckets.map { b => s\" (Coalesced to $b)\"}.getOrElse(\"\")))\n    } getOrElse {\n      metadata"
  },
  {
    "id" : "60a7c804-a4af-4f4c-837f-005bf8dc5906",
    "prId" : 27509,
    "prUrl" : "https://github.com/apache/spark/pull/27509#pullrequestreview-395939451",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d48f58e9-50ca-42a7-80d7-a2a968ea53f3",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "These changes are only for DSV1. Could we make the corresponding changes when using DSV2? Open the ticket https://issues.apache.org/jira/browse/SPARK-31480\r\n\r\nAlso, please check the output when the schema is very long. For example, containing 250+ columns. ",
        "createdAt" : "2020-04-18T18:33:48Z",
        "updatedAt" : "2020-04-18T18:33:49Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      }
    ],
    "commit" : "1290cd523d6fdead5399923ea4acac450a5c2175",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +78,82 @@       |(${ExplainUtils.getOpId(this)}) $nodeName ${ExplainUtils.getCodegenId(this)}\n       |${ExplainUtils.generateFieldString(\"Output\", producedAttributes)}\n       |${metadataStr.mkString(\"\\n\")}\n     \"\"\".stripMargin\n  }"
  },
  {
    "id" : "20839fbd-b3c2-4fff-acc6-38b5e4affbf2",
    "prId" : 26356,
    "prUrl" : "https://github.com/apache/spark/pull/26356#pullrequestreview-312305214",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ff70a9c3-0ef7-4594-a95e-93a829da3dd1",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Can you check if there are other places that needs to call `StringUtils.abbreviate` against `simpleString`?\r\n`simpleString` itself, IIRC, abbreviates when it's long unlike `catalogString`.",
        "createdAt" : "2019-11-06T02:53:45Z",
        "updatedAt" : "2019-11-06T03:33:59Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "baea25d4-4235-4e2d-986d-fd04b14df968",
        "parentId" : "ff70a9c3-0ef7-4594-a95e-93a829da3dd1",
        "authorId" : "f06b291a-bc5c-4fd8-9bac-e5a6d035891b",
        "body" : "@HyukjinKwon \r\nStringUtils.abbreviate is used in the two classes below as well : \r\n\r\n-     FileScan\r\n-     CachedRDDBuilder\r\n\r\nFor FileScan, it's also used to abbreviate metadata, so i think we can use maxMetadataValueStringLength there as well.\r\nI just update the code, please review again.",
        "createdAt" : "2019-11-06T03:41:11Z",
        "updatedAt" : "2019-11-06T03:45:49Z",
        "lastEditedBy" : "f06b291a-bc5c-4fd8-9bac-e5a6d035891b",
        "tags" : [
        ]
      },
      {
        "id" : "ff3a4d43-6a25-4353-976a-652abb9989c5",
        "parentId" : "ff70a9c3-0ef7-4594-a95e-93a829da3dd1",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Can we replace this place with, for instance, `truncatedString` (see StructType.scala) and reuse `spark.sql.debug.maxToStringFields` configuration?",
        "createdAt" : "2019-11-06T09:48:40Z",
        "updatedAt" : "2019-11-06T09:48:40Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "e7d51af493a57a3f21524e01fde2acd38509fd1d",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +59,63 @@    val metadataEntries = metadata.toSeq.sorted.map {\n      case (key, value) =>\n        key + \": \" + StringUtils.abbreviate(redact(value),\n          conf.maxMetadataValueStringLength)\n    }"
  },
  {
    "id" : "a1d7c9db-1a97-4426-afed-2915439a2855",
    "prId" : 25600,
    "prUrl" : "https://github.com/apache/spark/pull/25600#pullrequestreview-283809556",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "94de3b93-a665-4ffc-9b15-3e29f8b46165",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "shall we check `DynamicPruningExpression`?",
        "createdAt" : "2019-08-28T07:31:35Z",
        "updatedAt" : "2019-09-04T14:47:15Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "8a8b2105-da28-4744-97a7-0de13f9285cf",
        "parentId" : "94de3b93-a665-4ffc-9b15-3e29f8b46165",
        "authorId" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "body" : "A `DynamicPruningExpression` could also be a bypass condition, the literal `true`. Otherwise it will contain a subquery expression, so this check is better I believe.",
        "createdAt" : "2019-08-28T14:25:22Z",
        "updatedAt" : "2019-09-04T14:47:15Z",
        "lastEditedBy" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "tags" : [
        ]
      },
      {
        "id" : "48a8a9d7-c818-4169-bb28-1dc47bb8b192",
        "parentId" : "94de3b93-a665-4ffc-9b15-3e29f8b46165",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "I also have the same question at first glance. Maybe add a comment?",
        "createdAt" : "2019-09-04T19:06:41Z",
        "updatedAt" : "2019-09-04T19:10:16Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "b00225078c6471b5b01f8a920ab28a2361b8e48b",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +186,190 @@\n  private def isDynamicPruningFilter(e: Expression): Boolean =\n    e.find(_.isInstanceOf[PlanExpression[_]]).isDefined\n\n  @transient private lazy val selectedPartitions: Array[PartitionDirectory] = {"
  },
  {
    "id" : "9587850e-dafc-4662-bc91-fb33dbda3b65",
    "prId" : 25600,
    "prUrl" : "https://github.com/apache/spark/pull/25600#pullrequestreview-280864733",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "65c8e317-8a2f-4242-8dc1-d77a0d3ed275",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "We only access `selectedPartitions` before runtime when `LEGACY_BUCKETED_TABLE_SCAN_OUTPUT_ORDERING` is true. Can we add a todo that, once we remove `LEGACY_BUCKETED_TABLE_SCAN_OUTPUT_ORDERING`, we can merge `selectedPartitions` and `dynamicallySelectedPartitions`?",
        "createdAt" : "2019-08-28T07:37:28Z",
        "updatedAt" : "2019-09-04T14:47:15Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "42d807ed-2372-4447-8ace-bbb21142d3d1",
        "parentId" : "65c8e317-8a2f-4242-8dc1-d77a0d3ed275",
        "authorId" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "body" : "I'm +1 on that. But maybe we should still get the statically filtered partitions as a local variable and set the static metrics. ",
        "createdAt" : "2019-08-28T14:18:52Z",
        "updatedAt" : "2019-09-04T14:47:15Z",
        "lastEditedBy" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "tags" : [
        ]
      }
    ],
    "commit" : "b00225078c6471b5b01f8a920ab28a2361b8e48b",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +207,211 @@  // present. This is because such a filter relies on information that is only available at run\n  // time (for instance the keys used in the other side of a join).\n  @transient private lazy val dynamicallySelectedPartitions: Array[PartitionDirectory] = {\n    val dynamicPartitionFilters = partitionFilters.filter(isDynamicPruningFilter)\n"
  },
  {
    "id" : "bdde291c-c5ea-4733-845e-e404f688a579",
    "prId" : 25556,
    "prUrl" : "https://github.com/apache/spark/pull/25556#pullrequestreview-278773736",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3efaa29a-8b64-4989-8b3b-2e89d3c92ade",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "`conf.orderByFilePathWhenPartitioning`?",
        "createdAt" : "2019-08-23T02:07:14Z",
        "updatedAt" : "2019-08-23T02:07:15Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "497b0bb2-93f8-4a82-abf2-8c149cf39fcc",
        "parentId" : "3efaa29a-8b64-4989-8b3b-2e89d3c92ade",
        "authorId" : "59c055a4-ae32-4942-b7af-4170ce2e8d61",
        "body" : "@maropu Do you mean this name is not suitable?",
        "createdAt" : "2019-08-23T02:43:11Z",
        "updatedAt" : "2019-08-23T02:43:11Z",
        "lastEditedBy" : "59c055a4-ae32-4942-b7af-4170ce2e8d61",
        "tags" : [
        ]
      }
    ],
    "commit" : "9aca1719235985bb9068745941734c3158de68c2",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +483,487 @@    }\n\n    val orderByFilePath = fsRelation.sparkSession.sessionState.conf.orderByFilePathWhenPartitioning\n    val splitFiles = {\n      if (orderByFilePath) {"
  },
  {
    "id" : "b1b21f5c-150d-4f72-88d7-89a658da1109",
    "prId" : 25328,
    "prUrl" : "https://github.com/apache/spark/pull/25328#pullrequestreview-271149080",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f3d28bfb-7068-47b5-87bd-9e9b91d02c55",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Although previously it is `PartitionCount`, `numPartitions` looks more consistent.",
        "createdAt" : "2019-08-06T07:02:17Z",
        "updatedAt" : "2019-08-07T05:56:48Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "bf9b2617f81dd778d0d1355a66c1e664e7bc1f3b",
    "line" : 64,
    "diffHunk" : "@@ -1,1 +351,355 @@  } ++ {\n    if (relation.partitionSchemaOption.isDefined) {\n      Some(\"numPartitions\" -> SQLMetrics.createMetric(sparkContext, \"number of partitions read\"))\n    } else {\n      None"
  }
]