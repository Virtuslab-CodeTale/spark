[
  {
    "id" : "e8bd9030-fefd-4674-a16a-b72ed5296e49",
    "prId" : 31413,
    "prUrl" : "https://github.com/apache/spark/pull/31413#pullrequestreview-623210655",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dcd4d40a-b28d-4478-8234-4840bfd7d293",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "It's a bit odd that we call method name as `createNonBucketedReadRDD` but do something with buckets. I guess we could name `createNonBucketedReadRDD` like just `createReadRDD` or `createStandardReadRDD`",
        "createdAt" : "2021-03-29T13:11:32Z",
        "updatedAt" : "2021-03-29T13:11:32Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "03120af6b31af89dbc9fb9aad05045e98d52c699",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +591,595 @@      s\"open cost is considered as scanning $openCostInBytes bytes.\")\n\n    // Filter files with bucket pruning if possible\n    val bucketingEnabled = fsRelation.sparkSession.sessionState.conf.bucketingEnabled\n    val shouldProcess: Path => Boolean = optionalBucketSet match {"
  },
  {
    "id" : "de9777a3-fec5-4ba1-9725-1ffa6b61c36e",
    "prId" : 31413,
    "prUrl" : "https://github.com/apache/spark/pull/31413#pullrequestreview-623231689",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5f7b281e-4183-4bd2-b5a4-0d910d265067",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Hm, it could be one liner:\r\n\r\n```scala\r\n        filePath => BucketingUtils.getBucketId(filePath.getName).forall(bucketSet.get)\r\n```\r\n\r\nIf it looks less readable we could:\r\n\r\n```scala\r\n        filePath => BucketingUtils.getBucketId(filePath.getName).map(bucketSet.get).getOrElse\r\n```\r\n\r\nIf we worry about perf penalty from pattern matching, etc. we could do:\r\n\r\n```scala\r\n        filePath => {\r\n          val bucketId = BucketingUtils.getBucketId(filePath.getName)\r\n          if (bucketId.isEmpty) true else bucketSet.get(bucketId.get)\r\n        }\r\n``` \r\n",
        "createdAt" : "2021-03-29T13:31:51Z",
        "updatedAt" : "2021-03-29T13:31:51Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "03120af6b31af89dbc9fb9aad05045e98d52c699",
    "line" : 25,
    "diffHunk" : "@@ -1,1 +600,604 @@            case None =>\n              // Do not prune the file if bucket file name is invalid\n              true\n          }\n        }"
  },
  {
    "id" : "7650c8c8-93c7-42ba-9490-b234881558ca",
    "prId" : 29804,
    "prUrl" : "https://github.com/apache/spark/pull/29804#pullrequestreview-498644161",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b68d92d5-e145-48b9-98b0-8fc1e7a2bf4e",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "`map` -> `foreach`",
        "createdAt" : "2020-09-22T23:59:12Z",
        "updatedAt" : "2020-10-01T01:11:06Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "8e1e42bc-614a-4db5-b72f-edfe076ee538",
        "parentId" : "b68d92d5-e145-48b9-98b0-8fc1e7a2bf4e",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "@maropu - just for my own education, why does it matter? Updated anyway.",
        "createdAt" : "2020-09-23T06:14:15Z",
        "updatedAt" : "2020-10-01T01:11:06Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      },
      {
        "id" : "3b67c24e-9698-44a3-9090-d70048894e24",
        "parentId" : "b68d92d5-e145-48b9-98b0-8fc1e7a2bf4e",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Yea, I remember the previous discussion: https://issues.apache.org/jira/browse/SPARK-16694",
        "createdAt" : "2020-09-23T09:53:07Z",
        "updatedAt" : "2020-10-01T01:11:06Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "f174cba5-ad3f-43b4-a96e-2f0170dfcec4",
        "parentId" : "b68d92d5-e145-48b9-98b0-8fc1e7a2bf4e",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Yea, please only use `map` when you care about the return value. `foreach` is better if you just want to do some calculation if `Option` is `Some`",
        "createdAt" : "2020-09-29T11:13:46Z",
        "updatedAt" : "2020-10-01T01:11:06Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "c2607647-7599-412d-b0c8-471b3b3c4bd3",
        "parentId" : "b68d92d5-e145-48b9-98b0-8fc1e7a2bf4e",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "@cloud-fan , @maropu - I changed the code during iterations. The current change is just adding a `if (bucketedScan) { ... } else { ... }` on top of original code, where we still need to use `map` as it's returning value.",
        "createdAt" : "2020-09-29T15:57:47Z",
        "updatedAt" : "2020-10-01T01:11:06Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      }
    ],
    "commit" : "b29f688fc4a06bc35effa6d24632d2a64501c9fd",
    "line" : 40,
    "diffHunk" : "@@ -1,1 +355,359 @@    // TODO(SPARK-32986): Add bucketed scan info in explain output of FileSourceScanExec\n    if (bucketedScan) {\n      relation.bucketSpec.map { spec =>\n        val numSelectedBuckets = optionalBucketSet.map { b =>\n          b.cardinality()"
  },
  {
    "id" : "39455706-a2ed-4142-bbcc-96c643a6123e",
    "prId" : 29637,
    "prUrl" : "https://github.com/apache/spark/pull/29637#pullrequestreview-481710317",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "13fc6e89-4351-4652-9cf1-e0f84b20ad6e",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "This change is not needed, but I keep it here for: 1. avoid duplicated code. 2. make it super safe that we only update the `numPartitions` if table is partitioned.",
        "createdAt" : "2020-09-03T10:35:10Z",
        "updatedAt" : "2020-09-03T12:16:53Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "3e4a6a5d9a8f4151735b67c9dc585201e8543ab5",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +436,440 @@      driverMetrics(\"staticFilesSize\") = filesSize\n    }\n    if (relation.partitionSchemaOption.isDefined) {\n      driverMetrics(\"numPartitions\") = partitions.length\n    }"
  },
  {
    "id" : "85155632-6d7e-4b18-be35-2444273c535c",
    "prId" : 29473,
    "prUrl" : "https://github.com/apache/spark/pull/29473#pullrequestreview-476951709",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5c5a5166-f26a-4d1d-badd-7a0160a656b7",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "I'm not sure about how much this columnar execution makes performance gains though, the proposed idea is to give up the gains then use bucket repartitioning instead?",
        "createdAt" : "2020-08-24T13:08:58Z",
        "updatedAt" : "2020-08-30T04:42:06Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "363d1301-688d-4c62-967c-1eb4add56642",
        "parentId" : "5c5a5166-f26a-4d1d-badd-7a0160a656b7",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "Note that the datasource will still be read as batches in this case (if whole stage codegen is enabled).\r\n\r\nI see that physical plans operate on rows, so batches are converted to rows via `ColumnarToRow` anyway. So, I think perf impact would be minimal here; the difference could be the code-gen conversion from columnar to row vs. iterating `batch.rowIterator()` in `BucketRepartitioningRDD`.",
        "createdAt" : "2020-08-27T17:51:21Z",
        "updatedAt" : "2020-08-30T04:42:06Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "366c9c3b9a8872113d7ccf670b3e8bfbb53f892e",
    "line" : 34,
    "diffHunk" : "@@ -1,1 +177,181 @@    // row, thus columnar is not supported when `RepartitioningBucketRDD` is used to avoid\n    // conversions from batches to rows and back to batches.\n    relation.fileFormat.supportBatch(relation.sparkSession, schema) && !isRepartitioningBuckets\n  }\n"
  },
  {
    "id" : "9dff00ff-0025-4669-9221-b0b963871ade",
    "prId" : 29473,
    "prUrl" : "https://github.com/apache/spark/pull/29473#pullrequestreview-477107419",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "865fd48f-b441-4c04-80bf-69cccf8554d2",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "nit: we don't need `: Boolean `?",
        "createdAt" : "2020-08-24T13:09:26Z",
        "updatedAt" : "2020-08-30T04:42:06Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "11f8062b-f526-4cac-acde-74c4f4e376f4",
        "parentId" : "865fd48f-b441-4c04-80bf-69cccf8554d2",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "I followed the same style from `override lazy val supportsColumnar: Boolean`, etc. Is this still not needed?",
        "createdAt" : "2020-08-27T21:47:34Z",
        "updatedAt" : "2020-08-30T04:42:06Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "366c9c3b9a8872113d7ccf670b3e8bfbb53f892e",
    "line" : 37,
    "diffHunk" : "@@ -1,1 +180,184 @@  }\n\n  @transient private lazy val isRepartitioningBuckets: Boolean = {\n    bucketedScan && optionalNewNumBuckets.isDefined &&\n      optionalNewNumBuckets.get > relation.bucketSpec.get.numBuckets"
  },
  {
    "id" : "417a81d6-1253-4398-b346-d1516ed30a40",
    "prId" : 29473,
    "prUrl" : "https://github.com/apache/spark/pull/29473#pullrequestreview-478170995",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5db13e40-1dfb-4c98-a069-2b87c2d2be01",
        "parentId" : null,
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "we don't need `|| isRepartitioningBuckets` right? ",
        "createdAt" : "2020-08-29T06:04:41Z",
        "updatedAt" : "2020-08-30T04:42:06Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      },
      {
        "id" : "1f719506-9fee-415c-9e2c-b65016462136",
        "parentId" : "5db13e40-1dfb-4c98-a069-2b87c2d2be01",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "Repartition can still maintain the sort order whereas coalescing cannot, thus this check is needed.",
        "createdAt" : "2020-08-30T04:34:35Z",
        "updatedAt" : "2020-08-30T04:42:16Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "366c9c3b9a8872113d7ccf670b3e8bfbb53f892e",
    "line" : 57,
    "diffHunk" : "@@ -1,1 +324,328 @@\n        // TODO SPARK-24528 Sort order is currently ignored if buckets are coalesced.\n        if (singleFilePartitions && (optionalNewNumBuckets.isEmpty || isRepartitioningBuckets)) {\n          // TODO Currently Spark does not support writing columns sorting in descending order\n          // so using Ascending order. This can be fixed in future"
  },
  {
    "id" : "c1ac2703-571c-412a-b83f-6e6710da6966",
    "prId" : 29473,
    "prUrl" : "https://github.com/apache/spark/pull/29473#pullrequestreview-478170995",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c2f16647-ac8a-4e3c-8374-495467d9e3bd",
        "parentId" : null,
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "per `setFilesNumAndSizeMetric`, should we set `staticFilesNum` here or `numFiles ` ?",
        "createdAt" : "2020-08-29T06:20:31Z",
        "updatedAt" : "2020-08-30T04:42:06Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      },
      {
        "id" : "ac3d51fc-1234-4463-b14b-9295f57bb63d",
        "parentId" : "c2f16647-ac8a-4e3c-8374-495467d9e3bd",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "I think `staticFilesNum` is used only for dynamic partition pruning:\r\nhttps://github.com/apache/spark/blob/cfe012a4311a8fb1fc3a82390c3e68f6afcb1da6/sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala#L421-L424",
        "createdAt" : "2020-08-30T04:36:17Z",
        "updatedAt" : "2020-08-30T04:42:16Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "366c9c3b9a8872113d7ccf670b3e8bfbb53f892e",
    "line" : 117,
    "diffHunk" : "@@ -1,1 +590,594 @@        val filesNum = filePartitions.map(_.files.size.toLong).sum\n        val filesSize = filePartitions.map(_.files.map(_.length).sum).sum\n        driverMetrics(\"numFiles\") = filesNum\n        driverMetrics(\"filesSize\") = filesSize\n        new BucketRepartitioningRDD("
  },
  {
    "id" : "83676e40-1e10-4a5e-91ed-21ef811a9f03",
    "prId" : 28610,
    "prUrl" : "https://github.com/apache/spark/pull/28610#pullrequestreview-417129240",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c99a79de-66df-4650-af6e-0abef913eb88",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "private?",
        "createdAt" : "2020-05-22T13:35:32Z",
        "updatedAt" : "2020-05-22T22:16:39Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "ca66fb4e-7025-4ace-979e-de54ac0f56d9",
        "parentId" : "c99a79de-66df-4650-af6e-0abef913eb88",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "No, it is used both in DataSourceScanExec and FileSourceScanExec",
        "createdAt" : "2020-05-22T19:00:43Z",
        "updatedAt" : "2020-05-22T22:16:39Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "9d736efdc4f0ef456ae90eac7eb032c9e09ae8ed",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +56,60 @@  protected def metadata: Map[String, String]\n\n  protected val maxMetadataValueLength = 100\n\n  override def simpleString(maxFields: Int): String = {"
  },
  {
    "id" : "9d2b6ee1-d107-4bb7-818a-c615b37c72dc",
    "prId" : 28123,
    "prUrl" : "https://github.com/apache/spark/pull/28123#pullrequestreview-431737529",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "62ec600d-56c8-49fb-8b58-6aaec0a5d84e",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "I think we can have coalesced bucket info in explain. Looks like we still show original bucket numbers.",
        "createdAt" : "2020-06-12T04:08:37Z",
        "updatedAt" : "2020-06-19T04:52:18Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "01a43a09-4441-46f1-a472-a4aca98d0392",
        "parentId" : "62ec600d-56c8-49fb-8b58-6aaec0a5d84e",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "Yes, good idea. @cloud-fan/@maropu Do you want me to include this in this PR or do it as a follow up since this PR is already approved? I am fine with either one. Thanks @viirya!",
        "createdAt" : "2020-06-13T04:00:56Z",
        "updatedAt" : "2020-06-19T04:52:18Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      },
      {
        "id" : "43a722bc-13ad-4e3c-bc36-4b5fdceb7f6d",
        "parentId" : "62ec600d-56c8-49fb-8b58-6aaec0a5d84e",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "It's easy to fix: just update `FileSourceScanExec.metadata`. Let's do it in this PR",
        "createdAt" : "2020-06-16T11:29:25Z",
        "updatedAt" : "2020-06-19T04:52:18Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "708401d5-90d4-465b-9e5a-693a75821cbc",
        "parentId" : "62ec600d-56c8-49fb-8b58-6aaec0a5d84e",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "Will do.",
        "createdAt" : "2020-06-16T17:32:38Z",
        "updatedAt" : "2020-06-19T04:52:18Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "62a04a3e4d94e63b3787533f619e368a7e8d59f6",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +167,171 @@    partitionFilters: Seq[Expression],\n    optionalBucketSet: Option[BitSet],\n    optionalNumCoalescedBuckets: Option[Int],\n    dataFilters: Seq[Expression],\n    tableIdentifier: Option[TableIdentifier])"
  },
  {
    "id" : "bf17b150-2bb6-4520-9d35-975516ca1eb2",
    "prId" : 28123,
    "prUrl" : "https://github.com/apache/spark/pull/28123#pullrequestreview-433513848",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1564cd48-977f-4372-b915-0d2955517233",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Could you add test cases for this change?",
        "createdAt" : "2020-06-18T07:40:01Z",
        "updatedAt" : "2020-06-19T04:52:18Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "e642362a-2b45-47c4-ab50-54651afc0149",
        "parentId" : "1564cd48-977f-4372-b915-0d2955517233",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "Added.",
        "createdAt" : "2020-06-18T17:48:43Z",
        "updatedAt" : "2020-06-19T04:52:18Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "62a04a3e4d94e63b3787533f619e368a7e8d59f6",
    "line" : 44,
    "diffHunk" : "@@ -1,1 +362,366 @@      metadata + (\"SelectedBucketsCount\" ->\n        (s\"$numSelectedBuckets out of ${spec.numBuckets}\" +\n          optionalNumCoalescedBuckets.map { b => s\" (Coalesced to $b)\"}.getOrElse(\"\")))\n    } getOrElse {\n      metadata"
  }
]