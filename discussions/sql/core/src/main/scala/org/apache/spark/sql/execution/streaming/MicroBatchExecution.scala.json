[
  {
    "id" : "a7c2f72c-a301-430b-8eba-5534e13c3b42",
    "prId" : 31842,
    "prUrl" : "https://github.com/apache/spark/pull/31842#pullrequestreview-614306978",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7762d199-dc6e-46c9-8715-1a799a08645d",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Can we put the logic of turning `StreamingRelation/StreamingRelationV2` to `StreamingDataSourceV2Relation` in the new rule as well?",
        "createdAt" : "2021-03-17T08:14:04Z",
        "updatedAt" : "2021-03-19T14:29:22Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "c249a1ae-dfaa-44f1-9ee8-2bae7e8b3513",
        "parentId" : "7762d199-dc6e-46c9-8715-1a799a08645d",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "Actually, I'm following this direction now based on this PR. I plan to move all the logic of generating `logicalPlan` in both MicroBatchExcecution and ContinuousExecution. The patch is still in development, and some issues (like the queryExecutionThread assertion) are investigating. Maybe we can split this task into another PR. WDYT?",
        "createdAt" : "2021-03-17T13:21:19Z",
        "updatedAt" : "2021-03-19T14:29:22Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      },
      {
        "id" : "535063a0-8b4e-4a8e-a797-aafbe81486c2",
        "parentId" : "7762d199-dc6e-46c9-8715-1a799a08645d",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "SGTM",
        "createdAt" : "2021-03-17T13:31:10Z",
        "updatedAt" : "2021-03-19T14:29:22Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "7ec1afb4bfe269b472bf3110fc7bd580ae05e084",
    "line" : 35,
    "diffHunk" : "@@ -1,1 +44,48 @@    sparkSession, plan.name, plan.checkpointLocation, plan.inputQuery, plan.sink, trigger,\n    triggerClock, plan.outputMode, plan.deleteCheckpointOnStop) {\n\n  @volatile protected var sources: Seq[SparkDataStream] = Seq.empty\n"
  },
  {
    "id" : "a21a96db-d18b-4e59-9b28-8ddc881f49d0",
    "prId" : 31700,
    "prUrl" : "https://github.com/apache/spark/pull/31700#pullrequestreview-601472446",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ac7ad2be-495d-4126-a34c-8618e8e8db6a",
        "parentId" : null,
        "authorId" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "body" : "Seems safe?",
        "createdAt" : "2021-03-02T06:45:24Z",
        "updatedAt" : "2021-03-03T17:45:38Z",
        "lastEditedBy" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "tags" : [
        ]
      }
    ],
    "commit" : "e7f167fb0103d7e62127421cf5ad2a594d5f4337",
    "line" : 55,
    "diffHunk" : "@@ -1,1 +582,586 @@      case _: SupportsWrite =>\n        newAttributePlan match {\n          case w: V2MicroBatchWriteCommand => w.withNewBatchId(currentBatchId)\n          case other => throw new IllegalArgumentException(s\"unknown write plan type: $other\")\n        }"
  },
  {
    "id" : "910b5cdc-b2f1-4791-b6ec-c3f8c5aceaf1",
    "prId" : 29998,
    "prUrl" : "https://github.com/apache/spark/pull/29998#pullrequestreview-506434586",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3b5eae74-a994-4431-92ab-3bac24f6a9bc",
        "parentId" : null,
        "authorId" : "7e665d8b-d739-4edf-88c8-7379ff8585c2",
        "body" : "why change this ?",
        "createdAt" : "2020-10-12T09:11:37Z",
        "updatedAt" : "2020-10-12T09:12:48Z",
        "lastEditedBy" : "7e665d8b-d739-4edf-88c8-7379ff8585c2",
        "tags" : [
        ]
      }
    ],
    "commit" : "67370185751f579b405d705cc31c3f1da88eab5c",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +77,81 @@\n    val disabledSources =\n      Utils.stringToSeq(sparkSession.sqlContext.conf.disabledV2StreamingMicroBatchReaders)\n\n    import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Implicits._"
  },
  {
    "id" : "a130a804-d592-4a57-b9e7-eba16015f5b1",
    "prId" : 29269,
    "prUrl" : "https://github.com/apache/spark/pull/29269#pullrequestreview-456449232",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d1643b72-f01e-4e57-adea-5d7a3d89f698",
        "parentId" : null,
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "Fix the nit of code indent.",
        "createdAt" : "2020-07-28T09:12:50Z",
        "updatedAt" : "2020-08-07T10:53:45Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "3f0e71f619ca9f80c906742508cb400259f89ee6",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +567,571 @@      new Dataset(lastExecution, RowEncoder(lastExecution.analyzed.schema))\n\n    val batchSinkProgress: Option[StreamWriterCommitProgress] = reportTimeTaken(\"addBatch\") {\n      SQLExecution.withNewExecutionId(lastExecution) {\n        sink match {"
  },
  {
    "id" : "27b0c3ee-c8d7-4f3c-a548-f088924616e7",
    "prId" : 27380,
    "prUrl" : "https://github.com/apache/spark/pull/27380#pullrequestreview-351240331",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "62033684-467e-44f3-8507-edd1d52c1f0a",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "I'm wondering if we can do this at Analyzer?",
        "createdAt" : "2020-01-31T00:30:09Z",
        "updatedAt" : "2020-01-31T01:28:24Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "4179c554-66c5-4054-9042-c645d4524670",
        "parentId" : "62033684-467e-44f3-8507-edd1d52c1f0a",
        "authorId" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "body" : "Triggers are a property of the system, not the query, so I don't think it fits into analysis",
        "createdAt" : "2020-01-31T00:56:36Z",
        "updatedAt" : "2020-01-31T01:28:24Z",
        "lastEditedBy" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "tags" : [
        ]
      }
    ],
    "commit" : "de5f4860d2bbb8ee861d54d08438ee07f258ff49",
    "line" : 31,
    "diffHunk" : "@@ -1,1 +127,131 @@        val limit = source.getDefaultReadLimit\n        if (trigger == OneTimeTrigger && limit != ReadLimit.allAvailable()) {\n          logWarning(s\"The read limit $limit for $source is ignored when Trigger.Once() is used.\")\n          source -> ReadLimit.allAvailable()\n        } else {"
  },
  {
    "id" : "100127f3-5d11-4ae7-8a9b-835d0c055553",
    "prId" : 24550,
    "prUrl" : "https://github.com/apache/spark/pull/24550#pullrequestreview-234859108",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dbfa0816-64b7-449b-a5ab-a5aab1222f81",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "It looks correct to me because this is `OffsetSeq`.\r\ncc @jose-torres ",
        "createdAt" : "2019-05-08T04:21:35Z",
        "updatedAt" : "2019-05-08T04:21:35Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "037c1782f4a1ad1152aa16b4dd3d8fb9350a8f5f",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +269,273 @@         * is the second latest batch id in the offset log. */\n        if (latestBatchId != 0) {\n          val secondLatestOffsets = offsetLog.get(latestBatchId - 1).getOrElse {\n            throw new IllegalStateException(s\"batch ${latestBatchId - 1} doesn't exist\")\n          }"
  }
]