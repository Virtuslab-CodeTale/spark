[
  {
    "id" : "a7c2f72c-a301-430b-8eba-5534e13c3b42",
    "prId" : 31842,
    "prUrl" : "https://github.com/apache/spark/pull/31842#pullrequestreview-614306978",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7762d199-dc6e-46c9-8715-1a799a08645d",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Can we put the logic of turning `StreamingRelation/StreamingRelationV2` to `StreamingDataSourceV2Relation` in the new rule as well?",
        "createdAt" : "2021-03-17T08:14:04Z",
        "updatedAt" : "2021-03-19T14:29:22Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "c249a1ae-dfaa-44f1-9ee8-2bae7e8b3513",
        "parentId" : "7762d199-dc6e-46c9-8715-1a799a08645d",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "Actually, I'm following this direction now based on this PR. I plan to move all the logic of generating `logicalPlan` in both MicroBatchExcecution and ContinuousExecution. The patch is still in development, and some issues (like the queryExecutionThread assertion) are investigating. Maybe we can split this task into another PR. WDYT?",
        "createdAt" : "2021-03-17T13:21:19Z",
        "updatedAt" : "2021-03-19T14:29:22Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      },
      {
        "id" : "535063a0-8b4e-4a8e-a797-aafbe81486c2",
        "parentId" : "7762d199-dc6e-46c9-8715-1a799a08645d",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "SGTM",
        "createdAt" : "2021-03-17T13:31:10Z",
        "updatedAt" : "2021-03-19T14:29:22Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "7ec1afb4bfe269b472bf3110fc7bd580ae05e084",
    "line" : 35,
    "diffHunk" : "@@ -1,1 +44,48 @@    sparkSession, plan.name, plan.checkpointLocation, plan.inputQuery, plan.sink, trigger,\n    triggerClock, plan.outputMode, plan.deleteCheckpointOnStop) {\n\n  @volatile protected var sources: Seq[SparkDataStream] = Seq.empty\n"
  },
  {
    "id" : "a21a96db-d18b-4e59-9b28-8ddc881f49d0",
    "prId" : 31700,
    "prUrl" : "https://github.com/apache/spark/pull/31700#pullrequestreview-601472446",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ac7ad2be-495d-4126-a34c-8618e8e8db6a",
        "parentId" : null,
        "authorId" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "body" : "Seems safe?",
        "createdAt" : "2021-03-02T06:45:24Z",
        "updatedAt" : "2021-03-03T17:45:38Z",
        "lastEditedBy" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "tags" : [
        ]
      }
    ],
    "commit" : "e7f167fb0103d7e62127421cf5ad2a594d5f4337",
    "line" : 55,
    "diffHunk" : "@@ -1,1 +582,586 @@      case _: SupportsWrite =>\n        newAttributePlan match {\n          case w: V2MicroBatchWriteCommand => w.withNewBatchId(currentBatchId)\n          case other => throw new IllegalArgumentException(s\"unknown write plan type: $other\")\n        }"
  },
  {
    "id" : "910b5cdc-b2f1-4791-b6ec-c3f8c5aceaf1",
    "prId" : 29998,
    "prUrl" : "https://github.com/apache/spark/pull/29998#pullrequestreview-506434586",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3b5eae74-a994-4431-92ab-3bac24f6a9bc",
        "parentId" : null,
        "authorId" : "7e665d8b-d739-4edf-88c8-7379ff8585c2",
        "body" : "why change this ?",
        "createdAt" : "2020-10-12T09:11:37Z",
        "updatedAt" : "2020-10-12T09:12:48Z",
        "lastEditedBy" : "7e665d8b-d739-4edf-88c8-7379ff8585c2",
        "tags" : [
        ]
      }
    ],
    "commit" : "67370185751f579b405d705cc31c3f1da88eab5c",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +77,81 @@\n    val disabledSources =\n      Utils.stringToSeq(sparkSession.sqlContext.conf.disabledV2StreamingMicroBatchReaders)\n\n    import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Implicits._"
  }
]