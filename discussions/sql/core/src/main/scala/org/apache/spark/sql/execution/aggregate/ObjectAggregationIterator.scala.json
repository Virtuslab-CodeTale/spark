[
  {
    "id" : "99566762-1205-40bc-b8f7-4831b5ea2ba0",
    "prId" : 31353,
    "prUrl" : "https://github.com/apache/spark/pull/31353#pullrequestreview-578299774",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4285476c-7935-4517-9792-4c4fb3d194d8",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Ah, ok. I've checked the code and the fix looks reasonable.",
        "createdAt" : "2021-01-28T12:51:12Z",
        "updatedAt" : "2021-01-28T12:51:12Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "feccf5a87b5a9b98f54c668b8dc24e2a27052cc4",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +159,163 @@\n        // The hash map gets too large, makes a sorted spill and clear the map.\n        if (hashMap.size >= fallbackCountThreshold && inputRows.hasNext) {\n          logInfo(\n            s\"Aggregation hash map size ${hashMap.size} reaches threshold \" +"
  },
  {
    "id" : "e5bfe951-f103-44c6-8281-c0e71be2fbbb",
    "prId" : 31340,
    "prUrl" : "https://github.com/apache/spark/pull/31340#pullrequestreview-576062482",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "427ca1e4-43d8-47b6-8e8c-bfa06ee9d717",
        "parentId" : null,
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "This is same as [hash aggregate](https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator.scala#L113).",
        "createdAt" : "2021-01-26T06:49:01Z",
        "updatedAt" : "2021-01-28T21:50:44Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      }
    ],
    "commit" : "cd7ca22459c7fa24e5c86cd32a81ba49094cf896",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +62,66 @@  // Remember spill data size of this task before execute this operator so that we can\n  // figure out how many bytes we spilled for this operator.\n  private val spillSizeBefore = TaskContext.get().taskMetrics().memoryBytesSpilled\n\n  // Hacking the aggregation mode to call AggregateFunction.merge to merge two aggregation buffers"
  },
  {
    "id" : "5f09aa0d-99b5-4d8d-92b3-74c11c0a8245",
    "prId" : 31340,
    "prUrl" : "https://github.com/apache/spark/pull/31340#pullrequestreview-576062681",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8441e6e1-99ba-44e2-87d0-2e8f55508592",
        "parentId" : null,
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "This is similar to [hash aggregate](https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator.scala#L377-L391).",
        "createdAt" : "2021-01-26T06:49:33Z",
        "updatedAt" : "2021-01-28T21:50:44Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      }
    ],
    "commit" : "cd7ca22459c7fa24e5c86cd32a81ba49094cf896",
    "line" : 29,
    "diffHunk" : "@@ -1,1 +86,90 @@    // At the end of the task, update the task's spill size.\n    spillSize.set(TaskContext.get().taskMetrics().memoryBytesSpilled - spillSizeBefore)\n  })\n\n  override final def hasNext: Boolean = {"
  },
  {
    "id" : "c5cc788a-e368-4138-b9e0-c07e7a3bb1e0",
    "prId" : 31340,
    "prUrl" : "https://github.com/apache/spark/pull/31340#pullrequestreview-578971803",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "905992d6-1896-44d4-ba63-32b7c29574b6",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Just a comment; I feel it would be nicer if we could inspect object `hashMap` memory size (non-spilled case), too, but it looks difficult to do so, right?",
        "createdAt" : "2021-01-28T13:35:48Z",
        "updatedAt" : "2021-01-28T21:50:44Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "5f47ed39-8fb0-4ec7-828b-eaec160f5669",
        "parentId" : "905992d6-1896-44d4-ba63-32b7c29574b6",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "@maropu - this is a good point. Given the [map value is a general `InternalRow`](https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/ObjectAggregationMap.scala#L36), I feel it's hard to get an accurate metrics for memory size of this map. Hive aggregation uses a combination of [current JVM heap memory usage](https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java#L872), and [estimation of map size](https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java#L919-L920) to decide whether to spill. I think Hive's approach might be better. I am also tagging queries in our production environment with object hash aggregate fallback, and see how to improve them. Is it \"memory-size-based object hash aggregation fallback\" a good topic to discuss? cc @maropu and @cloud-fan thanks.",
        "createdAt" : "2021-01-28T21:40:03Z",
        "updatedAt" : "2021-01-28T21:50:44Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      },
      {
        "id" : "6b51a03f-0e01-4de9-af45-fc4a9b5e5de5",
        "parentId" : "905992d6-1896-44d4-ba63-32b7c29574b6",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Yea, the approach sounds good to me if possible. Anyway, it looks like we need to investigate how to estimate the size in a separate ticket.",
        "createdAt" : "2021-01-29T00:24:55Z",
        "updatedAt" : "2021-01-29T00:24:56Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "6e4d963c-cd35-4010-90fb-5d9e505e572f",
        "parentId" : "905992d6-1896-44d4-ba63-32b7c29574b6",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "Thanks @maropu, created a followup JIRA - https://issues.apache.org/jira/browse/SPARK-34286 .",
        "createdAt" : "2021-01-29T06:27:42Z",
        "updatedAt" : "2021-01-29T06:27:43Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      }
    ],
    "commit" : "cd7ca22459c7fa24e5c86cd32a81ba49094cf896",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +85,89 @@  TaskContext.get().addTaskCompletionListener[Unit](_ => {\n    // At the end of the task, update the task's spill size.\n    spillSize.set(TaskContext.get().taskMetrics().memoryBytesSpilled - spillSizeBefore)\n  })\n"
  }
]