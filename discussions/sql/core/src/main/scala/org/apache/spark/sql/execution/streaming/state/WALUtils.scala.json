[
  {
    "id" : "51be04f0-6ff1-4a68-ab53-164d500c06bd",
    "prId" : 24922,
    "prUrl" : "https://github.com/apache/spark/pull/24922#pullrequestreview-253773163",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "84270c86-13bd-4a7d-ad02-1e99b92cd27c",
        "parentId" : null,
        "authorId" : "99156419-7ce7-4671-b858-d0b5c4711f88",
        "body" : "is there a JIRA on this?",
        "createdAt" : "2019-06-25T04:39:05Z",
        "updatedAt" : "2019-09-26T06:35:25Z",
        "lastEditedBy" : "99156419-7ce7-4671-b858-d0b5c4711f88",
        "tags" : [
        ]
      },
      {
        "id" : "9d511684-367d-4c84-9d96-6d60a716da6d",
        "parentId" : "84270c86-13bd-4a7d-ad02-1e99b92cd27c",
        "authorId" : "be7041e0-9315-4dbe-8cde-5135813054ec",
        "body" : "https://issues.apache.org/jira/browse/SPARK-21271\r\n\r\nI have abstracted out utils related to WAL from the class [HDFSBackedStateStoreProvider](https://github.com/apache/spark/blob/f0f2f8d3ce8b0a4aa62134090ba90bcf39d97dc8/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider.scala#L549) so that it can be used elsewhere.",
        "createdAt" : "2019-06-25T05:10:36Z",
        "updatedAt" : "2019-09-26T06:35:25Z",
        "lastEditedBy" : "be7041e0-9315-4dbe-8cde-5135813054ec",
        "tags" : [
        ]
      }
    ],
    "commit" : "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "line" : 173,
    "diffHunk" : "@@ -1,1 +171,175 @@            // This is a workaround for the following:\n            // Prior to Spark 2.3 mistakenly append 4 bytes to the value row in\n            // `RowBasedKeyValueBatch`, which gets persisted into the checkpoint data\n            valueRow.pointTo(valueRowBuffer, (valueSize / 8) * 8)\n            newRocksDb.put(keyRow, valueRow)"
  },
  {
    "id" : "31899cb4-74f5-424f-bb44-b15a405b2463",
    "prId" : 24922,
    "prUrl" : "https://github.com/apache/spark/pull/24922#pullrequestreview-256124515",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2b0f1454-449a-49cf-9dbb-eff1405742bd",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Please leave self-commenting to determine which code blocks are copied from where, and which code is new. And I think this class is to deduplicate between HDFS state store provider and RocksDB state store provider, but HDFS state store provider is not leveraging this, ends up with duplicated code.",
        "createdAt" : "2019-07-01T00:32:40Z",
        "updatedAt" : "2019-09-26T06:35:25Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "73528fdd-7b76-499b-b466-d317fbe782d0",
        "parentId" : "2b0f1454-449a-49cf-9dbb-eff1405742bd",
        "authorId" : "be7041e0-9315-4dbe-8cde-5135813054ec",
        "body" : "Good point on self-review. \r\n\r\nI have abstracted out a lot of code from HDFS state store to create WALUtils. I didn't make any change in HDFS state store provider to reduce the scope of this PR. I can either start a new PR for the refactoring or I can do it once the rest of the code is reviewed. ",
        "createdAt" : "2019-07-01T04:08:00Z",
        "updatedAt" : "2019-09-26T06:35:25Z",
        "lastEditedBy" : "be7041e0-9315-4dbe-8cde-5135813054ec",
        "tags" : [
        ]
      }
    ],
    "commit" : "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "line" : 1,
    "diffHunk" : "@@ -1,1 +-1,3 @@/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with"
  },
  {
    "id" : "df721391-22ff-4eb0-b54c-9ee75d6a86c7",
    "prId" : 24922,
    "prUrl" : "https://github.com/apache/spark/pull/24922#pullrequestreview-259261451",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5c858d59-85ee-41bc-ab26-7f334bae05d6",
        "parentId" : null,
        "authorId" : "be7041e0-9315-4dbe-8cde-5135813054ec",
        "body" : "This is a new util Function. Here we are fetching delta file from the checkpointed location and apply it in the rocksdb instance.",
        "createdAt" : "2019-07-31T09:54:53Z",
        "updatedAt" : "2019-09-26T06:35:25Z",
        "lastEditedBy" : "be7041e0-9315-4dbe-8cde-5135813054ec",
        "tags" : [
        ]
      }
    ],
    "commit" : "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "line" : 127,
    "diffHunk" : "@@ -1,1 +125,129 @@  }\n\n  def updateFromDeltaFile(\n      fm: CheckpointFileManager,\n      fileToRead: Path,"
  },
  {
    "id" : "e0f01b86-55e4-4fa4-b3f6-47af16064623",
    "prId" : 24922,
    "prUrl" : "https://github.com/apache/spark/pull/24922#pullrequestreview-259261451",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "35b4a67a-a6f4-47b1-b9e3-06bddd2577fd",
        "parentId" : null,
        "authorId" : "be7041e0-9315-4dbe-8cde-5135813054ec",
        "body" : "Same as https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider.scala#L654\r\n\r\nThe difference is that CheckpointFileManager is added in the signature of the function.",
        "createdAt" : "2019-07-31T09:58:23Z",
        "updatedAt" : "2019-09-26T06:35:25Z",
        "lastEditedBy" : "be7041e0-9315-4dbe-8cde-5135813054ec",
        "tags" : [
        ]
      }
    ],
    "commit" : "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "line" : 66,
    "diffHunk" : "@@ -1,1 +64,68 @@\n  /** Fetch all the files that back the store */\n  def fetchFiles(fm: CheckpointFileManager, baseDir: Path): Seq[StoreFile] = {\n    val files: Seq[FileStatus] = try {\n      fm.list(baseDir)"
  },
  {
    "id" : "688bd4f3-ce32-453e-9e96-e64e482da13b",
    "prId" : 24922,
    "prUrl" : "https://github.com/apache/spark/pull/24922#pullrequestreview-259261451",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6ab37976-a3a4-4b23-8f9e-630d10960ad5",
        "parentId" : null,
        "authorId" : "be7041e0-9315-4dbe-8cde-5135813054ec",
        "body" : "Same as https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider.scala#L627",
        "createdAt" : "2019-07-31T09:58:46Z",
        "updatedAt" : "2019-09-26T06:35:25Z",
        "lastEditedBy" : "be7041e0-9315-4dbe-8cde-5135813054ec",
        "tags" : [
        ]
      }
    ],
    "commit" : "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "line" : 41,
    "diffHunk" : "@@ -1,1 +39,43 @@\n  /** Files needed to recover the given version of the store */\n  def filesForVersion(allFiles: Seq[StoreFile], version: Long): Seq[StoreFile] = {\n    require(version >= 0)\n    require(allFiles.exists(_.version == version))"
  },
  {
    "id" : "07a0640f-3b55-41f6-9a65-2940566f297c",
    "prId" : 24922,
    "prUrl" : "https://github.com/apache/spark/pull/24922#pullrequestreview-259261451",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dda8e657-e794-4318-9906-ceeb9df528ce",
        "parentId" : null,
        "authorId" : "be7041e0-9315-4dbe-8cde-5135813054ec",
        "body" : "Same as https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider.scala#L685",
        "createdAt" : "2019-07-31T09:59:06Z",
        "updatedAt" : "2019-09-26T06:35:25Z",
        "lastEditedBy" : "be7041e0-9315-4dbe-8cde-5135813054ec",
        "tags" : [
        ]
      }
    ],
    "commit" : "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "line" : 96,
    "diffHunk" : "@@ -1,1 +94,98 @@  }\n\n  def compressStream(outputStream: DataOutputStream, sparkConf: SparkConf): DataOutputStream = {\n    val compressed = new LZ4CompressionCodec(sparkConf).compressedOutputStream(outputStream)\n    new DataOutputStream(compressed)"
  },
  {
    "id" : "2c9dbfe4-55ad-4016-9f87-3d17885ba4b7",
    "prId" : 24922,
    "prUrl" : "https://github.com/apache/spark/pull/24922#pullrequestreview-259261451",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "72569d95-cb9d-4b2b-8c83-cd35230d7b54",
        "parentId" : null,
        "authorId" : "be7041e0-9315-4dbe-8cde-5135813054ec",
        "body" : "Same as https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider.scala#L396",
        "createdAt" : "2019-07-31T09:59:45Z",
        "updatedAt" : "2019-09-26T06:35:25Z",
        "lastEditedBy" : "be7041e0-9315-4dbe-8cde-5135813054ec",
        "tags" : [
        ]
      }
    ],
    "commit" : "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "line" : 106,
    "diffHunk" : "@@ -1,1 +104,108 @@  }\n\n  def writeUpdateToDeltaFile(output: DataOutputStream, key: UnsafeRow, value: UnsafeRow): Unit = {\n    val keyBytes = key.getBytes()\n    val valueBytes = value.getBytes()"
  },
  {
    "id" : "07cef460-c0a6-408f-b504-1f4dadaee4ba",
    "prId" : 24922,
    "prUrl" : "https://github.com/apache/spark/pull/24922#pullrequestreview-259261451",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ef8a2494-83ed-471f-9d29-02a1319fecf6",
        "parentId" : null,
        "authorId" : "be7041e0-9315-4dbe-8cde-5135813054ec",
        "body" : "Same as https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider.scala#L503",
        "createdAt" : "2019-07-31T10:01:39Z",
        "updatedAt" : "2019-09-26T06:35:25Z",
        "lastEditedBy" : "be7041e0-9315-4dbe-8cde-5135813054ec",
        "tags" : [
        ]
      }
    ],
    "commit" : "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "line" : 190,
    "diffHunk" : "@@ -1,1 +188,192 @@   * @param rawStream the underlying stream which needs to be cancelled.\n   */\n  def cancelDeltaFile(\n      compressedStream: DataOutputStream,\n      rawStream: CancellableFSDataOutputStream): Unit = {"
  },
  {
    "id" : "c7e796b1-e340-4e29-984e-fe2769939044",
    "prId" : 24922,
    "prUrl" : "https://github.com/apache/spark/pull/24922#pullrequestreview-259261451",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a3359447-9a27-4fc7-a227-5ee9e339d007",
        "parentId" : null,
        "authorId" : "be7041e0-9315-4dbe-8cde-5135813054ec",
        "body" : "`uploadFile` and `downloadFile` are new util function used by RocksDBStateStore.",
        "createdAt" : "2019-07-31T10:02:54Z",
        "updatedAt" : "2019-09-26T06:35:25Z",
        "lastEditedBy" : "be7041e0-9315-4dbe-8cde-5135813054ec",
        "tags" : [
        ]
      }
    ],
    "commit" : "45e0d054a38958ac9e1b7c6a9429a3a3df9b8ff1",
    "line" : 205,
    "diffHunk" : "@@ -1,1 +203,207 @@  }\n\n  def uploadFile(\n      fm: CheckpointFileManager,\n      sourceFile: Path,"
  }
]