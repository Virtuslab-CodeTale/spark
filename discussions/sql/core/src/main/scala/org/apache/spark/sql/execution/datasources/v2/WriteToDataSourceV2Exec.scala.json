[
  {
    "id" : "ca2dd506-e887-431c-a3d8-85db4f945c37",
    "prId" : 32039,
    "prUrl" : "https://github.com/apache/spark/pull/32039#pullrequestreview-631693933",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "77368339-a879-4fd4-b291-c48dd6c5cd9d",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Instead of refreshing/invalidating the table per trigger, why we don't just invalidate the cache before we start the streaming query that writes the table?",
        "createdAt" : "2021-04-08T17:40:19Z",
        "updatedAt" : "2021-04-12T17:13:28Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "32962886-4477-45fc-810d-e4429558393a",
        "parentId" : "77368339-a879-4fd4-b291-c48dd6c5cd9d",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Yes that should work too and also will require fewer code changes. I went this way to be consistent with other V2 write commands. Also, in future we may introduce `DataStreamWriterV2` which could pass write node with `UnresolvedRelation` to analyzer and be converted to execution plan, and this approach may fit better in that case.\r\n\r\n\r\n",
        "createdAt" : "2021-04-08T18:04:37Z",
        "updatedAt" : "2021-04-12T17:13:28Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      }
    ],
    "commit" : "a884b472bc40f49404b75c853c0399c6549435cd",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +259,263 @@    val writtenRows = writeWithV2(batchWrite)\n    refreshCache()\n    writtenRows\n  }\n}"
  },
  {
    "id" : "9102638e-e852-4257-a798-73f5a1c8752d",
    "prId" : 31619,
    "prUrl" : "https://github.com/apache/spark/pull/31619#pullrequestreview-598398854",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e2a66e58-02e7-4b1c-9d3d-5a02de3e3b19",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "ditto",
        "createdAt" : "2021-02-25T10:15:04Z",
        "updatedAt" : "2021-03-18T17:50:37Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "d7a0f06a563af83c0166cef736541cdfd6105324",
    "line" : 27,
    "diffHunk" : "@@ -1,1 +75,79 @@      }\n\n      throw QueryCompilationErrors.tableAlreadyExistsError(ident)\n    }\n"
  },
  {
    "id" : "62f67cc9-1480-4496-ba8e-ba0242d6282b",
    "prId" : 31619,
    "prUrl" : "https://github.com/apache/spark/pull/31619#pullrequestreview-598398854",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ea3f3c34-bd2b-418f-b1a2-1f6ba7a79d26",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "ditto",
        "createdAt" : "2021-02-25T10:15:33Z",
        "updatedAt" : "2021-03-18T17:50:37Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "d7a0f06a563af83c0166cef736541cdfd6105324",
    "line" : 36,
    "diffHunk" : "@@ -1,1 +110,114 @@      }\n\n      throw QueryCompilationErrors.tableAlreadyExistsError(ident)\n    }\n    val schema = CharVarcharUtils.getRawSchema(query.schema).asNullable"
  },
  {
    "id" : "a256ca36-bc42-4b79-978a-107b7a7a8937",
    "prId" : 30806,
    "prUrl" : "https://github.com/apache/spark/pull/30806#pullrequestreview-554958904",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8543eb9b-09b1-4cb5-b52f-86f668e92c45",
        "parentId" : null,
        "authorId" : "f5b3f57a-75a6-496c-af94-c32580ada13a",
        "body" : "Nit: merge line 451-454 into:\r\n```\r\n          val writtenRows = table.newWriteBuilder(info).build() match {\r\n```",
        "createdAt" : "2020-12-17T05:59:50Z",
        "updatedAt" : "2020-12-21T15:35:33Z",
        "lastEditedBy" : "f5b3f57a-75a6-496c-af94-c32580ada13a",
        "tags" : [
        ]
      },
      {
        "id" : "98b23b6f-0cf9-4572-8095-93263f847483",
        "parentId" : "8543eb9b-09b1-4cb5-b52f-86f668e92c45",
        "authorId" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "body" : "I don't feel strongly about this place and can update it. However, I do prefer to split different logical parts into different variables. Here, I've separated building a logical write from actually writing the records. Let me know what are your thoughts, @jzhuge.",
        "createdAt" : "2020-12-17T20:08:30Z",
        "updatedAt" : "2020-12-21T15:35:33Z",
        "lastEditedBy" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "tags" : [
        ]
      },
      {
        "id" : "5421e8f5-15f3-46b8-b8d6-86d91496f57a",
        "parentId" : "8543eb9b-09b1-4cb5-b52f-86f668e92c45",
        "authorId" : "f5b3f57a-75a6-496c-af94-c32580ada13a",
        "body" : "Fine with me",
        "createdAt" : "2020-12-17T20:22:56Z",
        "updatedAt" : "2020-12-21T15:35:33Z",
        "lastEditedBy" : "f5b3f57a-75a6-496c-af94-c32580ada13a",
        "tags" : [
        ]
      }
    ],
    "commit" : "882e3210004c0e27aca914b58819008dafdde92a",
    "line" : 111,
    "diffHunk" : "@@ -1,1 +450,454 @@\n          val write = writeBuilder.build()\n          val writtenRows = write match {\n            case v1: V1Write => writeWithV1(v1.toInsertableRelation)\n            case v2 => writeWithV2(v2.toBatch)"
  },
  {
    "id" : "bd98bf1a-b538-429f-b8a3-750ae4624dd5",
    "prId" : 30193,
    "prUrl" : "https://github.com/apache/spark/pull/30193#pullrequestreview-521003042",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f8530216-5c78-43e1-a799-dcc2e3dbb0da",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Could you add a summarization of refactoring in the PR description?",
        "createdAt" : "2020-10-30T16:42:47Z",
        "updatedAt" : "2020-10-30T16:42:47Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "ff1472a4-f7f2-49a8-8c4e-04a366ca499f",
        "parentId" : "f8530216-5c78-43e1-a799-dcc2e3dbb0da",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Sure. Updated the description.",
        "createdAt" : "2020-10-30T20:12:39Z",
        "updatedAt" : "2020-10-30T20:12:39Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      }
    ],
    "commit" : "42722413419f83e16c32afbdcec8375f8ff53852",
    "line" : 141,
    "diffHunk" : "@@ -1,1 +433,437 @@}\n\nprivate[v2] trait TableWriteExec extends V2TableWriteExec with SupportsV1Write {\n  import org.apache.spark.sql.connector.catalog.CatalogV2Implicits.IdentifierHelper\n"
  },
  {
    "id" : "06738a5d-76bd-40e5-ada8-e1af09224c84",
    "prId" : 30193,
    "prUrl" : "https://github.com/apache/spark/pull/30193#pullrequestreview-522570054",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b2afd518-52ba-4a97-8bfe-9aaea0cab088",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Do we have classes that extend `V2TableWriteExec` but not `TableWriteExec`?",
        "createdAt" : "2020-11-01T05:46:16Z",
        "updatedAt" : "2020-11-01T05:46:17Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "0027fadc-e783-47b0-91da-7196cef0b733",
        "parentId" : "b2afd518-52ba-4a97-8bfe-9aaea0cab088",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "yes we do, e.g., `WriteToDataSourceV2Exec`.",
        "createdAt" : "2020-11-02T22:41:02Z",
        "updatedAt" : "2020-11-02T22:41:02Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "0056345c-c244-4ef9-86af-2e10940f98b7",
        "parentId" : "b2afd518-52ba-4a97-8bfe-9aaea0cab088",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Seems like the name should be `TableWriteExecWithV1Support`? It's hard to tell the difference between `TableWriteExec` and `V2TableWriteExec` by name.",
        "createdAt" : "2020-11-03T03:39:57Z",
        "updatedAt" : "2020-11-03T03:39:58Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "13f4f996-b515-49bd-bc67-f287b87c6004",
        "parentId" : "b2afd518-52ba-4a97-8bfe-9aaea0cab088",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Yes agree the name might be too general. Not sure about `TableWriteExecWithV1Support` though since it sounds like only support v1 write. What about `TableWriteExecHelper` (since it just expose one helper method for writing v1/v2 tables) or `TableWriteExecUtil`?",
        "createdAt" : "2020-11-03T07:59:57Z",
        "updatedAt" : "2020-11-03T08:00:04Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "167aff81-3e29-4b44-9a69-631312ff8f90",
        "parentId" : "b2afd518-52ba-4a97-8bfe-9aaea0cab088",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Both work for me.",
        "createdAt" : "2020-11-03T14:47:38Z",
        "updatedAt" : "2020-11-03T14:47:39Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "42722413419f83e16c32afbdcec8375f8ff53852",
    "line" : 141,
    "diffHunk" : "@@ -1,1 +433,437 @@}\n\nprivate[v2] trait TableWriteExec extends V2TableWriteExec with SupportsV1Write {\n  import org.apache.spark.sql.connector.catalog.CatalogV2Implicits.IdentifierHelper\n"
  }
]