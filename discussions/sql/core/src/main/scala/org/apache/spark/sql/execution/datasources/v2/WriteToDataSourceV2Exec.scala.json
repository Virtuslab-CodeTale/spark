[
  {
    "id" : "ca2dd506-e887-431c-a3d8-85db4f945c37",
    "prId" : 32039,
    "prUrl" : "https://github.com/apache/spark/pull/32039#pullrequestreview-631693933",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "77368339-a879-4fd4-b291-c48dd6c5cd9d",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Instead of refreshing/invalidating the table per trigger, why we don't just invalidate the cache before we start the streaming query that writes the table?",
        "createdAt" : "2021-04-08T17:40:19Z",
        "updatedAt" : "2021-04-12T17:13:28Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "32962886-4477-45fc-810d-e4429558393a",
        "parentId" : "77368339-a879-4fd4-b291-c48dd6c5cd9d",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Yes that should work too and also will require fewer code changes. I went this way to be consistent with other V2 write commands. Also, in future we may introduce `DataStreamWriterV2` which could pass write node with `UnresolvedRelation` to analyzer and be converted to execution plan, and this approach may fit better in that case.\r\n\r\n\r\n",
        "createdAt" : "2021-04-08T18:04:37Z",
        "updatedAt" : "2021-04-12T17:13:28Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      }
    ],
    "commit" : "a884b472bc40f49404b75c853c0399c6549435cd",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +259,263 @@    val writtenRows = writeWithV2(batchWrite)\n    refreshCache()\n    writtenRows\n  }\n}"
  },
  {
    "id" : "9102638e-e852-4257-a798-73f5a1c8752d",
    "prId" : 31619,
    "prUrl" : "https://github.com/apache/spark/pull/31619#pullrequestreview-598398854",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e2a66e58-02e7-4b1c-9d3d-5a02de3e3b19",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "ditto",
        "createdAt" : "2021-02-25T10:15:04Z",
        "updatedAt" : "2021-03-18T17:50:37Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "d7a0f06a563af83c0166cef736541cdfd6105324",
    "line" : 27,
    "diffHunk" : "@@ -1,1 +75,79 @@      }\n\n      throw QueryCompilationErrors.tableAlreadyExistsError(ident)\n    }\n"
  },
  {
    "id" : "62f67cc9-1480-4496-ba8e-ba0242d6282b",
    "prId" : 31619,
    "prUrl" : "https://github.com/apache/spark/pull/31619#pullrequestreview-598398854",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ea3f3c34-bd2b-418f-b1a2-1f6ba7a79d26",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "ditto",
        "createdAt" : "2021-02-25T10:15:33Z",
        "updatedAt" : "2021-03-18T17:50:37Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "d7a0f06a563af83c0166cef736541cdfd6105324",
    "line" : 36,
    "diffHunk" : "@@ -1,1 +110,114 @@      }\n\n      throw QueryCompilationErrors.tableAlreadyExistsError(ident)\n    }\n    val schema = CharVarcharUtils.getRawSchema(query.schema).asNullable"
  },
  {
    "id" : "a256ca36-bc42-4b79-978a-107b7a7a8937",
    "prId" : 30806,
    "prUrl" : "https://github.com/apache/spark/pull/30806#pullrequestreview-554958904",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8543eb9b-09b1-4cb5-b52f-86f668e92c45",
        "parentId" : null,
        "authorId" : "f5b3f57a-75a6-496c-af94-c32580ada13a",
        "body" : "Nit: merge line 451-454 into:\r\n```\r\n          val writtenRows = table.newWriteBuilder(info).build() match {\r\n```",
        "createdAt" : "2020-12-17T05:59:50Z",
        "updatedAt" : "2020-12-21T15:35:33Z",
        "lastEditedBy" : "f5b3f57a-75a6-496c-af94-c32580ada13a",
        "tags" : [
        ]
      },
      {
        "id" : "98b23b6f-0cf9-4572-8095-93263f847483",
        "parentId" : "8543eb9b-09b1-4cb5-b52f-86f668e92c45",
        "authorId" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "body" : "I don't feel strongly about this place and can update it. However, I do prefer to split different logical parts into different variables. Here, I've separated building a logical write from actually writing the records. Let me know what are your thoughts, @jzhuge.",
        "createdAt" : "2020-12-17T20:08:30Z",
        "updatedAt" : "2020-12-21T15:35:33Z",
        "lastEditedBy" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "tags" : [
        ]
      },
      {
        "id" : "5421e8f5-15f3-46b8-b8d6-86d91496f57a",
        "parentId" : "8543eb9b-09b1-4cb5-b52f-86f668e92c45",
        "authorId" : "f5b3f57a-75a6-496c-af94-c32580ada13a",
        "body" : "Fine with me",
        "createdAt" : "2020-12-17T20:22:56Z",
        "updatedAt" : "2020-12-21T15:35:33Z",
        "lastEditedBy" : "f5b3f57a-75a6-496c-af94-c32580ada13a",
        "tags" : [
        ]
      }
    ],
    "commit" : "882e3210004c0e27aca914b58819008dafdde92a",
    "line" : 111,
    "diffHunk" : "@@ -1,1 +450,454 @@\n          val write = writeBuilder.build()\n          val writtenRows = write match {\n            case v1: V1Write => writeWithV1(v1.toInsertableRelation)\n            case v2 => writeWithV2(v2.toBatch)"
  },
  {
    "id" : "bd98bf1a-b538-429f-b8a3-750ae4624dd5",
    "prId" : 30193,
    "prUrl" : "https://github.com/apache/spark/pull/30193#pullrequestreview-521003042",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f8530216-5c78-43e1-a799-dcc2e3dbb0da",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Could you add a summarization of refactoring in the PR description?",
        "createdAt" : "2020-10-30T16:42:47Z",
        "updatedAt" : "2020-10-30T16:42:47Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "ff1472a4-f7f2-49a8-8c4e-04a366ca499f",
        "parentId" : "f8530216-5c78-43e1-a799-dcc2e3dbb0da",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Sure. Updated the description.",
        "createdAt" : "2020-10-30T20:12:39Z",
        "updatedAt" : "2020-10-30T20:12:39Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      }
    ],
    "commit" : "42722413419f83e16c32afbdcec8375f8ff53852",
    "line" : 141,
    "diffHunk" : "@@ -1,1 +433,437 @@}\n\nprivate[v2] trait TableWriteExec extends V2TableWriteExec with SupportsV1Write {\n  import org.apache.spark.sql.connector.catalog.CatalogV2Implicits.IdentifierHelper\n"
  },
  {
    "id" : "06738a5d-76bd-40e5-ada8-e1af09224c84",
    "prId" : 30193,
    "prUrl" : "https://github.com/apache/spark/pull/30193#pullrequestreview-522570054",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b2afd518-52ba-4a97-8bfe-9aaea0cab088",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Do we have classes that extend `V2TableWriteExec` but not `TableWriteExec`?",
        "createdAt" : "2020-11-01T05:46:16Z",
        "updatedAt" : "2020-11-01T05:46:17Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "0027fadc-e783-47b0-91da-7196cef0b733",
        "parentId" : "b2afd518-52ba-4a97-8bfe-9aaea0cab088",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "yes we do, e.g., `WriteToDataSourceV2Exec`.",
        "createdAt" : "2020-11-02T22:41:02Z",
        "updatedAt" : "2020-11-02T22:41:02Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "0056345c-c244-4ef9-86af-2e10940f98b7",
        "parentId" : "b2afd518-52ba-4a97-8bfe-9aaea0cab088",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Seems like the name should be `TableWriteExecWithV1Support`? It's hard to tell the difference between `TableWriteExec` and `V2TableWriteExec` by name.",
        "createdAt" : "2020-11-03T03:39:57Z",
        "updatedAt" : "2020-11-03T03:39:58Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "13f4f996-b515-49bd-bc67-f287b87c6004",
        "parentId" : "b2afd518-52ba-4a97-8bfe-9aaea0cab088",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Yes agree the name might be too general. Not sure about `TableWriteExecWithV1Support` though since it sounds like only support v1 write. What about `TableWriteExecHelper` (since it just expose one helper method for writing v1/v2 tables) or `TableWriteExecUtil`?",
        "createdAt" : "2020-11-03T07:59:57Z",
        "updatedAt" : "2020-11-03T08:00:04Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "167aff81-3e29-4b44-9a69-631312ff8f90",
        "parentId" : "b2afd518-52ba-4a97-8bfe-9aaea0cab088",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Both work for me.",
        "createdAt" : "2020-11-03T14:47:38Z",
        "updatedAt" : "2020-11-03T14:47:39Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "42722413419f83e16c32afbdcec8375f8ff53852",
    "line" : 141,
    "diffHunk" : "@@ -1,1 +433,437 @@}\n\nprivate[v2] trait TableWriteExec extends V2TableWriteExec with SupportsV1Write {\n  import org.apache.spark.sql.connector.catalog.CatalogV2Implicits.IdentifierHelper\n"
  },
  {
    "id" : "2149ef7d-3327-4a89-b213-58ed58c3109c",
    "prId" : 29066,
    "prUrl" : "https://github.com/apache/spark/pull/29066#pullrequestreview-547757917",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "952d1bde-63a8-48cd-a11e-6ff457b13fac",
        "parentId" : null,
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Does this have to be an option? or it will always be non-empty? ",
        "createdAt" : "2020-12-03T19:32:11Z",
        "updatedAt" : "2020-12-07T14:50:31Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "aa5b0373-7257-49ee-8b30-bfd4a5a0e98b",
        "parentId" : "952d1bde-63a8-48cd-a11e-6ff457b13fac",
        "authorId" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "body" : "I kept it as `Option` just in case we don't want to apply the new logic all the time and will introduce a flag to fallback to the old approach. If we are not going to have that flag, we can make this required.",
        "createdAt" : "2020-12-07T15:02:17Z",
        "updatedAt" : "2020-12-07T15:02:17Z",
        "lastEditedBy" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "tags" : [
        ]
      },
      {
        "id" : "38a3e670-dffe-47db-a050-3d892949d28b",
        "parentId" : "952d1bde-63a8-48cd-a11e-6ff457b13fac",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Cool, it would simplify the logic if we know the new code will always be applied. Perhaps worth creating a JIRA to track this.",
        "createdAt" : "2020-12-09T01:02:42Z",
        "updatedAt" : "2020-12-09T01:02:42Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      }
    ],
    "commit" : "32f5687fd628a4ffca38fa0840fd52447db7d680",
    "line" : 86,
    "diffHunk" : "@@ -1,1 +322,326 @@trait V2ExistingTableWriteExec extends V2TableWriteExec {\n  def refreshCache: () => Unit\n  def write: Option[BatchWrite] = None\n\n  override protected def run(): Seq[InternalRow] = {"
  },
  {
    "id" : "7b9c4079-d55d-4cdc-b181-0cbe7daa96e0",
    "prId" : 27941,
    "prUrl" : "https://github.com/apache/spark/pull/27941#pullrequestreview-377052173",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b5ab7173-d6d6-4cce-9f28-f89c47e70aaa",
        "parentId" : null,
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "The physical plan still shows in the Spark UI because `UnaryExecNode` overrides `V2CommandExec.children` right?",
        "createdAt" : "2020-03-18T16:03:35Z",
        "updatedAt" : "2020-03-18T19:48:19Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      },
      {
        "id" : "cb5725b8-63b7-4d97-a696-40cc09727756",
        "parentId" : "b5ab7173-d6d6-4cce-9f28-f89c47e70aaa",
        "authorId" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "body" : "yes. The fact that the data is inserted means that the child does get overridden, and the UI should look fine as well",
        "createdAt" : "2020-03-18T16:49:34Z",
        "updatedAt" : "2020-03-18T19:48:19Z",
        "lastEditedBy" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "tags" : [
        ]
      }
    ],
    "commit" : "439c2dcbafa9345045739e9d94e034e2d9d7e6df",
    "line" : 85,
    "diffHunk" : "@@ -1,1 +351,355 @@ * The base physical plan for writing data into data source v2.\n */\ntrait V2TableWriteExec extends V2CommandExec with UnaryExecNode {\n  def query: SparkPlan\n"
  },
  {
    "id" : "bfe7e90e-3c34-41d1-add4-9baff831b5f3",
    "prId" : 25536,
    "prUrl" : "https://github.com/apache/spark/pull/25536#pullrequestreview-278148779",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "acb116fb-8ebd-4e2b-9e99-f0702a80eb77",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "We don't need to update the schema nullability of  the corresponding logical plans, `CreateTableAsSelect` and `ReplaceTable`, in the analyzer phase? Any reason to directly update the nullability of physical plans?",
        "createdAt" : "2019-08-21T23:55:43Z",
        "updatedAt" : "2019-08-21T23:55:43Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "c87c8ae7-9ef8-4551-909d-3d82b44efd59",
        "parentId" : "acb116fb-8ebd-4e2b-9e99-f0702a80eb77",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I think it's too much work if we need to transform the logical plan and add an extra Project to change the nullability.",
        "createdAt" : "2019-08-22T01:06:26Z",
        "updatedAt" : "2019-08-22T01:06:26Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "4d1c088a-926b-4dea-ab9e-a6f34dcb23f7",
        "parentId" : "acb116fb-8ebd-4e2b-9e99-f0702a80eb77",
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "I think it would be incorrect to change the logical plan. The behavior of CTAS should be that tables are created with nullable types. The query used by CTAS should not be changed.",
        "createdAt" : "2019-08-22T01:17:32Z",
        "updatedAt" : "2019-08-22T01:17:32Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      },
      {
        "id" : "08c58c9c-464e-4e2a-bb0f-498a8f54a44c",
        "parentId" : "acb116fb-8ebd-4e2b-9e99-f0702a80eb77",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Ah, I see. Thanks. Looks ok to me.",
        "createdAt" : "2019-08-22T01:19:24Z",
        "updatedAt" : "2019-08-22T01:19:24Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "655d07ef22bd3b775906fe69e04e4f8beb82a1a5",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +82,86 @@\n    Utils.tryWithSafeFinallyAndFailureCallbacks({\n      val schema = query.schema.asNullable\n      catalog.createTable(\n        ident, schema, partitioning.toArray, properties.asJava) match {"
  }
]