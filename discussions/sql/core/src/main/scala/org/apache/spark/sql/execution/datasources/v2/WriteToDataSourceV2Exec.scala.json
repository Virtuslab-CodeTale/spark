[
  {
    "id" : "ca2dd506-e887-431c-a3d8-85db4f945c37",
    "prId" : 32039,
    "prUrl" : "https://github.com/apache/spark/pull/32039#pullrequestreview-631693933",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "77368339-a879-4fd4-b291-c48dd6c5cd9d",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Instead of refreshing/invalidating the table per trigger, why we don't just invalidate the cache before we start the streaming query that writes the table?",
        "createdAt" : "2021-04-08T17:40:19Z",
        "updatedAt" : "2021-04-12T17:13:28Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "32962886-4477-45fc-810d-e4429558393a",
        "parentId" : "77368339-a879-4fd4-b291-c48dd6c5cd9d",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Yes that should work too and also will require fewer code changes. I went this way to be consistent with other V2 write commands. Also, in future we may introduce `DataStreamWriterV2` which could pass write node with `UnresolvedRelation` to analyzer and be converted to execution plan, and this approach may fit better in that case.\r\n\r\n\r\n",
        "createdAt" : "2021-04-08T18:04:37Z",
        "updatedAt" : "2021-04-12T17:13:28Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      }
    ],
    "commit" : "a884b472bc40f49404b75c853c0399c6549435cd",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +259,263 @@    val writtenRows = writeWithV2(batchWrite)\n    refreshCache()\n    writtenRows\n  }\n}"
  },
  {
    "id" : "9102638e-e852-4257-a798-73f5a1c8752d",
    "prId" : 31619,
    "prUrl" : "https://github.com/apache/spark/pull/31619#pullrequestreview-598398854",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e2a66e58-02e7-4b1c-9d3d-5a02de3e3b19",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "ditto",
        "createdAt" : "2021-02-25T10:15:04Z",
        "updatedAt" : "2021-03-18T17:50:37Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "d7a0f06a563af83c0166cef736541cdfd6105324",
    "line" : 27,
    "diffHunk" : "@@ -1,1 +75,79 @@      }\n\n      throw QueryCompilationErrors.tableAlreadyExistsError(ident)\n    }\n"
  },
  {
    "id" : "62f67cc9-1480-4496-ba8e-ba0242d6282b",
    "prId" : 31619,
    "prUrl" : "https://github.com/apache/spark/pull/31619#pullrequestreview-598398854",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ea3f3c34-bd2b-418f-b1a2-1f6ba7a79d26",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "ditto",
        "createdAt" : "2021-02-25T10:15:33Z",
        "updatedAt" : "2021-03-18T17:50:37Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "d7a0f06a563af83c0166cef736541cdfd6105324",
    "line" : 36,
    "diffHunk" : "@@ -1,1 +110,114 @@      }\n\n      throw QueryCompilationErrors.tableAlreadyExistsError(ident)\n    }\n    val schema = CharVarcharUtils.getRawSchema(query.schema).asNullable"
  }
]