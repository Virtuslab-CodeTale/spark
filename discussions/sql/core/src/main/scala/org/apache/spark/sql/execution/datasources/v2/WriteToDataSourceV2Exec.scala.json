[
  {
    "id" : "ca2dd506-e887-431c-a3d8-85db4f945c37",
    "prId" : 32039,
    "prUrl" : "https://github.com/apache/spark/pull/32039#pullrequestreview-631693933",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "77368339-a879-4fd4-b291-c48dd6c5cd9d",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Instead of refreshing/invalidating the table per trigger, why we don't just invalidate the cache before we start the streaming query that writes the table?",
        "createdAt" : "2021-04-08T17:40:19Z",
        "updatedAt" : "2021-04-12T17:13:28Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "32962886-4477-45fc-810d-e4429558393a",
        "parentId" : "77368339-a879-4fd4-b291-c48dd6c5cd9d",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Yes that should work too and also will require fewer code changes. I went this way to be consistent with other V2 write commands. Also, in future we may introduce `DataStreamWriterV2` which could pass write node with `UnresolvedRelation` to analyzer and be converted to execution plan, and this approach may fit better in that case.\r\n\r\n\r\n",
        "createdAt" : "2021-04-08T18:04:37Z",
        "updatedAt" : "2021-04-12T17:13:28Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      }
    ],
    "commit" : "a884b472bc40f49404b75c853c0399c6549435cd",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +259,263 @@    val writtenRows = writeWithV2(batchWrite)\n    refreshCache()\n    writtenRows\n  }\n}"
  }
]