[
  {
    "id" : "ca2dd506-e887-431c-a3d8-85db4f945c37",
    "prId" : 32039,
    "prUrl" : "https://github.com/apache/spark/pull/32039#pullrequestreview-631693933",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "77368339-a879-4fd4-b291-c48dd6c5cd9d",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Instead of refreshing/invalidating the table per trigger, why we don't just invalidate the cache before we start the streaming query that writes the table?",
        "createdAt" : "2021-04-08T17:40:19Z",
        "updatedAt" : "2021-04-12T17:13:28Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "32962886-4477-45fc-810d-e4429558393a",
        "parentId" : "77368339-a879-4fd4-b291-c48dd6c5cd9d",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Yes that should work too and also will require fewer code changes. I went this way to be consistent with other V2 write commands. Also, in future we may introduce `DataStreamWriterV2` which could pass write node with `UnresolvedRelation` to analyzer and be converted to execution plan, and this approach may fit better in that case.\r\n\r\n\r\n",
        "createdAt" : "2021-04-08T18:04:37Z",
        "updatedAt" : "2021-04-12T17:13:28Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      }
    ],
    "commit" : "a884b472bc40f49404b75c853c0399c6549435cd",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +259,263 @@    val writtenRows = writeWithV2(batchWrite)\n    refreshCache()\n    writtenRows\n  }\n}"
  },
  {
    "id" : "9102638e-e852-4257-a798-73f5a1c8752d",
    "prId" : 31619,
    "prUrl" : "https://github.com/apache/spark/pull/31619#pullrequestreview-598398854",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e2a66e58-02e7-4b1c-9d3d-5a02de3e3b19",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "ditto",
        "createdAt" : "2021-02-25T10:15:04Z",
        "updatedAt" : "2021-03-18T17:50:37Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "d7a0f06a563af83c0166cef736541cdfd6105324",
    "line" : 27,
    "diffHunk" : "@@ -1,1 +75,79 @@      }\n\n      throw QueryCompilationErrors.tableAlreadyExistsError(ident)\n    }\n"
  },
  {
    "id" : "62f67cc9-1480-4496-ba8e-ba0242d6282b",
    "prId" : 31619,
    "prUrl" : "https://github.com/apache/spark/pull/31619#pullrequestreview-598398854",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ea3f3c34-bd2b-418f-b1a2-1f6ba7a79d26",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "ditto",
        "createdAt" : "2021-02-25T10:15:33Z",
        "updatedAt" : "2021-03-18T17:50:37Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "d7a0f06a563af83c0166cef736541cdfd6105324",
    "line" : 36,
    "diffHunk" : "@@ -1,1 +110,114 @@      }\n\n      throw QueryCompilationErrors.tableAlreadyExistsError(ident)\n    }\n    val schema = CharVarcharUtils.getRawSchema(query.schema).asNullable"
  },
  {
    "id" : "a256ca36-bc42-4b79-978a-107b7a7a8937",
    "prId" : 30806,
    "prUrl" : "https://github.com/apache/spark/pull/30806#pullrequestreview-554958904",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8543eb9b-09b1-4cb5-b52f-86f668e92c45",
        "parentId" : null,
        "authorId" : "f5b3f57a-75a6-496c-af94-c32580ada13a",
        "body" : "Nit: merge line 451-454 into:\r\n```\r\n          val writtenRows = table.newWriteBuilder(info).build() match {\r\n```",
        "createdAt" : "2020-12-17T05:59:50Z",
        "updatedAt" : "2020-12-21T15:35:33Z",
        "lastEditedBy" : "f5b3f57a-75a6-496c-af94-c32580ada13a",
        "tags" : [
        ]
      },
      {
        "id" : "98b23b6f-0cf9-4572-8095-93263f847483",
        "parentId" : "8543eb9b-09b1-4cb5-b52f-86f668e92c45",
        "authorId" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "body" : "I don't feel strongly about this place and can update it. However, I do prefer to split different logical parts into different variables. Here, I've separated building a logical write from actually writing the records. Let me know what are your thoughts, @jzhuge.",
        "createdAt" : "2020-12-17T20:08:30Z",
        "updatedAt" : "2020-12-21T15:35:33Z",
        "lastEditedBy" : "4a94765a-5775-4d88-b283-6ee68d76ecc7",
        "tags" : [
        ]
      },
      {
        "id" : "5421e8f5-15f3-46b8-b8d6-86d91496f57a",
        "parentId" : "8543eb9b-09b1-4cb5-b52f-86f668e92c45",
        "authorId" : "f5b3f57a-75a6-496c-af94-c32580ada13a",
        "body" : "Fine with me",
        "createdAt" : "2020-12-17T20:22:56Z",
        "updatedAt" : "2020-12-21T15:35:33Z",
        "lastEditedBy" : "f5b3f57a-75a6-496c-af94-c32580ada13a",
        "tags" : [
        ]
      }
    ],
    "commit" : "882e3210004c0e27aca914b58819008dafdde92a",
    "line" : 111,
    "diffHunk" : "@@ -1,1 +450,454 @@\n          val write = writeBuilder.build()\n          val writtenRows = write match {\n            case v1: V1Write => writeWithV1(v1.toInsertableRelation)\n            case v2 => writeWithV2(v2.toBatch)"
  }
]