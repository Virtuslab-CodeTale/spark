[
  {
    "id" : "7ed9e239-c9dc-45b3-b3c9-c1636200b18e",
    "prId" : 28133,
    "prUrl" : "https://github.com/apache/spark/pull/28133#pullrequestreview-388797431",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1d75035e-2d56-4b68-8687-c0571d12270a",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "We either use a block comment or many End-Of-Line comments. We don't mix both like this.",
        "createdAt" : "2020-04-07T04:42:27Z",
        "updatedAt" : "2020-04-07T05:46:40Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "b8bf9720-3d07-49aa-851d-847712608c45",
        "parentId" : "1d75035e-2d56-4b68-8687-c0571d12270a",
        "authorId" : "2a18db55-97b4-449b-a4c8-af366a5a3225",
        "body" : "The hope was to resolve this TODO before merging (either by keeping the code here and cleaning todo or by moving to another layer and also cleaning todo)",
        "createdAt" : "2020-04-07T05:39:48Z",
        "updatedAt" : "2020-04-07T05:46:40Z",
        "lastEditedBy" : "2a18db55-97b4-449b-a4c8-af366a5a3225",
        "tags" : [
        ]
      }
    ],
    "commit" : "c580634e16b246af621dce1abf0ed26fa8449bb2",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +72,76 @@  // TODO: it might be helpful to have this helper in Dataset.scala,\n  // e.g. `drop` function uses exactly the same flow to deal with\n  // `Column` arguments\n  private def resolveColumn(df: DataFrame, col: Column): Column = {\n    col match {"
  },
  {
    "id" : "1a9ab3c7-af01-46b0-b5ca-3f96e7f7ca4b",
    "prId" : 28133,
    "prUrl" : "https://github.com/apache/spark/pull/28133#pullrequestreview-391938084",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "73b8bce2-2726-425a-9a89-9f45323d8ffe",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "The problem with Column is, it can contain an unresolved expression, for example `UnresolvedAttribute + UnresolvedAttribute ...`.\r\n\r\nWhen we are allowed to use column name only, we can rely on `df.resolve(colName)` to resolve it. Once you extend to Column, you cannot do the same check as before.",
        "createdAt" : "2020-04-07T05:21:40Z",
        "updatedAt" : "2020-04-07T05:46:40Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "4f76ff5a-866a-42c3-b9c5-b49672e02d22",
        "parentId" : "73b8bce2-2726-425a-9a89-9f45323d8ffe",
        "authorId" : "2a18db55-97b4-449b-a4c8-af366a5a3225",
        "body" : "The code here tries to resolve the column if it has `UnresolvedAttribute`. If it still does not provide clarity, I think it's fair to throw an exception. Similar to how `Dataset.drop` works if the argument given is a column with an unresolved attribute. ",
        "createdAt" : "2020-04-07T05:43:10Z",
        "updatedAt" : "2020-04-07T05:46:40Z",
        "lastEditedBy" : "2a18db55-97b4-449b-a4c8-af366a5a3225",
        "tags" : [
        ]
      },
      {
        "id" : "3f69debe-74e8-4e34-af0d-06223d668ca9",
        "parentId" : "73b8bce2-2726-425a-9a89-9f45323d8ffe",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "No, I mean for now you only handle the case that `Column(UnresolvedAttribute)`, but `Column` can contain any unresolved expression that can involve many `UnresolvedAttribute`. For the latter case, the added `resolveColumn` cannot resolve it correctly.  ",
        "createdAt" : "2020-04-07T05:54:30Z",
        "updatedAt" : "2020-04-07T06:01:09Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "4b40ec4c-62c2-4896-9cf6-ea5ca47f5d40",
        "parentId" : "73b8bce2-2726-425a-9a89-9f45323d8ffe",
        "authorId" : "2a18db55-97b4-449b-a4c8-af366a5a3225",
        "body" : "Exactly the same approach is used for `drop` implementation here: https://github.com/apache/spark/blob/22bb6b0fddb3ecd3ac0ad2b41a5024c86b8a6fc7/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L2500-L2505 -- is there a better approach?",
        "createdAt" : "2020-04-13T03:02:33Z",
        "updatedAt" : "2020-04-13T03:02:34Z",
        "lastEditedBy" : "2a18db55-97b4-449b-a4c8-af366a5a3225",
        "tags" : [
        ]
      },
      {
        "id" : "bd09ccd4-4d62-4583-aa90-4b00f7471f48",
        "parentId" : "73b8bce2-2726-425a-9a89-9f45323d8ffe",
        "authorId" : "2a18db55-97b4-449b-a4c8-af366a5a3225",
        "body" : "As far as we will do `df.select` to execution functions, we can do rely on \r\n\r\n```\r\ndf.select(cols: _*).queryExecution.analyzed.output\r\n```\r\n\r\nto check data types. `select` will throw exception in case of unresolvable column. @viirya WDYT?",
        "createdAt" : "2020-04-13T03:35:41Z",
        "updatedAt" : "2020-04-13T03:35:42Z",
        "lastEditedBy" : "2a18db55-97b4-449b-a4c8-af366a5a3225",
        "tags" : [
        ]
      }
    ],
    "commit" : "c580634e16b246af621dce1abf0ed26fa8449bb2",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +80,84 @@      case Column(_expr: Expression) => col\n    }\n  }\n\n  /**"
  }
]