[
  {
    "id" : "136d0bbb-31ee-4ff4-b97e-692e8fcbfc08",
    "prId" : 31491,
    "prUrl" : "https://github.com/apache/spark/pull/31491#pullrequestreview-585132612",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "daa4aae5-8492-4081-867b-63882cc41961",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@sarutak should we maybe update migration guide?",
        "createdAt" : "2021-02-07T02:41:03Z",
        "updatedAt" : "2021-02-17T01:59:57Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "79b3cc06-bd52-467a-8175-5e1d9c0e77c4",
        "parentId" : "daa4aae5-8492-4081-867b-63882cc41961",
        "authorId" : "3a227965-84e0-47cf-9974-11293764f028",
        "body" : "Thanks. I've updated.",
        "createdAt" : "2021-02-08T03:02:00Z",
        "updatedAt" : "2021-02-17T01:59:57Z",
        "lastEditedBy" : "3a227965-84e0-47cf-9974-11293764f028",
        "tags" : [
        ]
      }
    ],
    "commit" : "73f53d91e609a7450819ee3bb14b81eee34d42f9",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +227,231 @@      case java.sql.Types.REF           => StringType\n      case java.sql.Types.REF_CURSOR    => null\n      case java.sql.Types.ROWID         => StringType\n      case java.sql.Types.SMALLINT      => IntegerType\n      case java.sql.Types.SQLXML        => StringType"
  },
  {
    "id" : "8e9ffbc7-f277-43d6-96b8-6cf28b7e481f",
    "prId" : 31491,
    "prUrl" : "https://github.com/apache/spark/pull/31491#pullrequestreview-588764025",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3ec69a0b-9577-4293-adce-5e9fd4b8f9eb",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "So basically we can't assume the row ID is <= 8 bytes? if that's true then I agree.",
        "createdAt" : "2021-02-11T16:28:23Z",
        "updatedAt" : "2021-02-17T01:59:57Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "779553ad-6874-4eed-9ca2-4ad1410de486",
        "parentId" : "3ec69a0b-9577-4293-adce-5e9fd4b8f9eb",
        "authorId" : "3a227965-84e0-47cf-9974-11293764f028",
        "body" : "Not only we can't assume the length of the ROWID but also it's not required to be represented as integer.\r\nJDBC RowId declares `getBytes` and `toString` to represent ROWID so I think we can safely map ROWID to StringType.\r\nhttps://docs.oracle.com/javase/8/docs/api/java/sql/RowId.html",
        "createdAt" : "2021-02-11T16:44:04Z",
        "updatedAt" : "2021-02-17T01:59:57Z",
        "lastEditedBy" : "3a227965-84e0-47cf-9974-11293764f028",
        "tags" : [
        ]
      }
    ],
    "commit" : "73f53d91e609a7450819ee3bb14b81eee34d42f9",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +227,231 @@      case java.sql.Types.REF           => StringType\n      case java.sql.Types.REF_CURSOR    => null\n      case java.sql.Types.ROWID         => StringType\n      case java.sql.Types.SMALLINT      => IntegerType\n      case java.sql.Types.SQLXML        => StringType"
  },
  {
    "id" : "8cc7af0e-c190-4ab1-8a2e-8ad148b08a4c",
    "prId" : 31264,
    "prUrl" : "https://github.com/apache/spark/pull/31264#pullrequestreview-576461282",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f3b22654-9ccd-49fc-9625-79b2423ac105",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@MaxGekk, is this correct way `toJavaTimestamp(instantToMicros(row.getAs[Instant](pos)))`?",
        "createdAt" : "2021-01-26T14:17:27Z",
        "updatedAt" : "2021-01-29T06:35:41Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "019c5a4d-b721-41d5-a57e-eddfd78d80ee",
        "parentId" : "f3b22654-9ccd-49fc-9625-79b2423ac105",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "yes",
        "createdAt" : "2021-01-26T15:24:37Z",
        "updatedAt" : "2021-01-29T06:35:41Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "6a5a4b7de4bcd85a32d7544c7887f3d12378e42e",
    "line" : 27,
    "diffHunk" : "@@ -1,1 +589,593 @@      if (SQLConf.get.datetimeJava8ApiEnabled) {\n        (stmt: PreparedStatement, row: Row, pos: Int) =>\n          stmt.setTimestamp(pos + 1, toJavaTimestamp(instantToMicros(row.getAs[Instant](pos))))\n      } else {\n        (stmt: PreparedStatement, row: Row, pos: Int) =>"
  },
  {
    "id" : "22172ade-d07a-4d4e-92f7-3eba18f88cd3",
    "prId" : 31252,
    "prUrl" : "https://github.com/apache/spark/pull/31252#pullrequestreview-571933765",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5b0a1ed0-e1a2-47b7-97fd-d8273c9b306d",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "If this issue is only for postgresql, could you add this fix in `PostgresDialect`?",
        "createdAt" : "2021-01-20T00:20:52Z",
        "updatedAt" : "2021-01-20T00:20:52Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "4e299472-8501-41b1-8de6-9d5b8308a9e6",
        "parentId" : "5b0a1ed0-e1a2-47b7-97fd-d8273c9b306d",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "+1 for @maropu 's advice.",
        "createdAt" : "2021-01-20T03:36:44Z",
        "updatedAt" : "2021-01-20T03:36:45Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "0b9371ef-4ef6-4d46-a982-bb06073ee535",
        "parentId" : "5b0a1ed0-e1a2-47b7-97fd-d8273c9b306d",
        "authorId" : "137b17b4-9571-4fa2-a8d5-4b3d15930469",
        "body" : "The only way `fieldScale` can make it into the dialect is by the field metadata.\r\nIt was always added prior to the previous commit (which I agree with on a fundamental level)\r\nhttps://github.com/skestle/spark/commit/0b647fe69cf201b4dcbc0f4dfc0eb504a523571d#diff-c3859e97335ead4b131263565c987d877bea0af3adbd6c5bf2d3716768d2e083",
        "createdAt" : "2021-01-20T06:15:40Z",
        "updatedAt" : "2021-01-20T06:15:40Z",
        "lastEditedBy" : "137b17b4-9571-4fa2-a8d5-4b3d15930469",
        "tags" : [
        ]
      },
      {
        "id" : "8e0c3341-5455-4057-8226-8b20a99be9d5",
        "parentId" : "5b0a1ed0-e1a2-47b7-97fd-d8273c9b306d",
        "authorId" : "4a21ee68-297a-45ce-ad3f-eebc5801c797",
        "body" : "I've explained a few more rationale regarding this change proposed by @skestle in the comment below https://github.com/apache/spark/pull/31252#discussion_r560730953",
        "createdAt" : "2021-01-20T07:31:48Z",
        "updatedAt" : "2021-01-20T07:31:48Z",
        "lastEditedBy" : "4a21ee68-297a-45ce-ad3f-eebc5801c797",
        "tags" : [
        ]
      }
    ],
    "commit" : "9fe9769809b177a9967b195ecf78a3717e1cfd8f",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +314,318 @@        case java.sql.Types.NUMERIC => metadata.putLong(\"scale\", fieldScale)\n        case java.sql.Types.DECIMAL => metadata.putLong(\"scale\", fieldScale)\n        case java.sql.Types.ARRAY   => metadata.putLong(\"scale\", fieldScale) // PostgresDialect.scala wants this information\n        case java.sql.Types.TIME    => metadata.putBoolean(\"logical_time_type\", true)\n        case _                      =>"
  },
  {
    "id" : "37b00370-8502-4730-9dc0-4b7d7d03df91",
    "prId" : 31252,
    "prUrl" : "https://github.com/apache/spark/pull/31252#pullrequestreview-572009023",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "94e0b84a-aa72-421c-817f-3330e89d2e09",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "can we always include the scale metadata if it's present? cc @saikocat",
        "createdAt" : "2021-01-20T05:37:50Z",
        "updatedAt" : "2021-01-20T05:37:50Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "6967d7b4-b28d-4d4e-8e87-04049823044e",
        "parentId" : "94e0b84a-aa72-421c-817f-3330e89d2e09",
        "authorId" : "4a21ee68-297a-45ce-ad3f-eebc5801c797",
        "body" : "We can do that for simplification. But we need to fix the tests for the rest of the dialects. Cos it adds `{\"scale\": 0}` to all metadata, then existing tests failed (previously metadata didn't get build in getSchema(), so the JSON meta didn't generated)\r\n\r\nThat's why I choose to set it to only decimal and numeric. The problem also eluded me cos the test for array in Postgresql dialects call the toCalaystType directly instead of going through the code path of using metadata. Sorry on phone so it's hard for me to link the line.",
        "createdAt" : "2021-01-20T05:57:20Z",
        "updatedAt" : "2021-01-20T05:57:21Z",
        "lastEditedBy" : "4a21ee68-297a-45ce-ad3f-eebc5801c797",
        "tags" : [
        ]
      },
      {
        "id" : "76521966-5fd5-4822-abf1-327b19efcf5f",
        "parentId" : "94e0b84a-aa72-421c-817f-3330e89d2e09",
        "authorId" : "4a21ee68-297a-45ce-ad3f-eebc5801c797",
        "body" : "Alright, let me elaborate more so you two (cc: @skestle) can decide on which approach to go for. Though I'm kind of favor the current approach of adding data type matching for adding scale metadata cos fixing the failing tests will be more difficult and it makes the JDBCSuite test `\"jdbc data source shouldn't have unnecessary metadata in its schema\"` test slightly lose its meaning. \r\n\r\nSo in order to push \"logical_time_type\" to metadata, I have to force metadata to be built in the field type as of here:\r\nhttps://github.com/skestle/spark/commit/0b647fe69cf201b4dcbc0f4dfc0eb504a523571d#diff-c3859e97335ead4b131263565c987d877bea0af3adbd6c5bf2d3716768d2e083R323 whereas previously, metadata can be built  by the dialect or completely ignored (as default)\r\n\r\nThis will cause 3 tests in `JDBCSuite` to fail cause of schema mismatch (extra `{\"scale\": 0}` in the metadata always present) [1. `\"jdbc API support custom schema\"`, 2. `\"jdbc API custom schema DDL-like strings.\"`, 3. `\"jdbc data source shouldn't have unnecessary metadata in its schema\"`]. ",
        "createdAt" : "2021-01-20T07:29:37Z",
        "updatedAt" : "2021-01-20T07:29:37Z",
        "lastEditedBy" : "4a21ee68-297a-45ce-ad3f-eebc5801c797",
        "tags" : [
        ]
      },
      {
        "id" : "aff02065-b2a1-4d93-9480-7daaa1d9f4c0",
        "parentId" : "94e0b84a-aa72-421c-817f-3330e89d2e09",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "> whereas previously, metadata can be built by the dialect or completely ignored (as default)\r\n\r\nIf it was dialect's responsibility to put things into metadata before, I think this PR should put the fix in `PostgresDialect`.",
        "createdAt" : "2021-01-20T09:00:40Z",
        "updatedAt" : "2021-01-20T09:00:40Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "9b92c4e3-ce6d-4052-9554-c217ac491bae",
        "parentId" : "94e0b84a-aa72-421c-817f-3330e89d2e09",
        "authorId" : "4a21ee68-297a-45ce-ad3f-eebc5801c797",
        "body" : "But the only way `fieldScale` can make it into the dialect is by the field metadata. So it is a very chicken and egg problem. \r\n\r\nEDIT: Postgresql utilize the metadatabuilder to get the scale for array[][] of type numeric for example - cos dataType is `ARRAY ` but the typeName is `_numeric` - note the underscore specific for Postgresql. Whereas MySQL dialect is putting more info into the metadata (like put(\"binarylong\")). The use cases differ.\r\n\r\nMight have to change the interface somehow to let the ResultSetMetadata to be passed or init-ed to the dialect. ",
        "createdAt" : "2021-01-20T09:08:38Z",
        "updatedAt" : "2021-01-20T09:16:18Z",
        "lastEditedBy" : "4a21ee68-297a-45ce-ad3f-eebc5801c797",
        "tags" : [
        ]
      }
    ],
    "commit" : "9fe9769809b177a9967b195ecf78a3717e1cfd8f",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +307,311 @@      val metadata = new MetadataBuilder()\n      // SPARK-33888\n      // - include scale in metadata for only DECIMAL & NUMERIC as well as ARRAY (for Postgres)\n      // - include TIME type metadata\n      // - always build the metadata"
  },
  {
    "id" : "8de16b4c-4b09-4cd2-b356-b9642d7c65a9",
    "prId" : 30902,
    "prUrl" : "https://github.com/apache/spark/pull/30902#pullrequestreview-572985451",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e971ccfc-1107-4e94-8c42-3bdd6b3b5fab",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "@sarutak do you mean what returns here is seconds (with certain precision) from a starting timestamp, while the timestamp is different between databases? I'm a bit surprised if the JDBC protocol was design this way, but if this is true, then this PR doesn't make sense...",
        "createdAt" : "2021-01-21T04:53:33Z",
        "updatedAt" : "2021-01-21T04:53:33Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "9ebd5830-6d06-4c64-b20b-020a9f82d170",
        "parentId" : "e971ccfc-1107-4e94-8c42-3bdd6b3b5fab",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "What returns here is `java.sql.Time`, and its doc says\r\n```\r\nThe date components should be set to the \"zero epoch\"\r\nvalue of January 1, 1970 and should not be accessed.\r\n```\r\nMaybe some databases don't follow the requirement, but it doesn't matter, as we call `rawTime.toLocalTime` which only access the hour:minute:second components.",
        "createdAt" : "2021-01-21T04:58:18Z",
        "updatedAt" : "2021-01-21T04:58:18Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "97b16a01-c1d0-4d20-b627-1847e1824ca5",
        "parentId" : "e971ccfc-1107-4e94-8c42-3bdd6b3b5fab",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "That said, I think reading SQL TIME type as integer has a well-define semantic in Spark (after this PR): the integer represents the milliseconds of the time from 00:00:00.",
        "createdAt" : "2021-01-21T05:00:40Z",
        "updatedAt" : "2021-01-21T05:00:40Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "fdb220fb-0a9b-4124-86bd-c1a216bf81c0",
        "parentId" : "e971ccfc-1107-4e94-8c42-3bdd6b3b5fab",
        "authorId" : "3a227965-84e0-47cf-9974-11293764f028",
        "body" : "O.K, so the test seems wrong.\r\nActually, I'll fix it in another PR.",
        "createdAt" : "2021-01-21T05:02:54Z",
        "updatedAt" : "2021-01-21T05:02:54Z",
        "lastEditedBy" : "3a227965-84e0-47cf-9974-11293764f028",
        "tags" : [
        ]
      }
    ],
    "commit" : "63fad20278c90b55fc311f276008ead5883dba1d",
    "line" : 54,
    "diffHunk" : "@@ -1,1 +429,433 @@    case IntegerType if metadata.contains(\"logical_time_type\") =>\n      (rs: ResultSet, row: InternalRow, pos: Int) => {\n        val rawTime = rs.getTime(pos + 1)\n        if (rawTime != null) {\n          val rawTimeInNano = rawTime.toLocalTime().toNanoOfDay()"
  },
  {
    "id" : "65eb5fc2-a876-4fc2-92dd-1fedd7a8c480",
    "prId" : 30902,
    "prUrl" : "https://github.com/apache/spark/pull/30902#pullrequestreview-583090995",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "96ad31c3-9fec-49a5-96b6-146c9a713a0c",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "After a second thought, why do we pick millisecond precision? Why not microsecond? Is there a standard for it?",
        "createdAt" : "2021-02-03T07:13:46Z",
        "updatedAt" : "2021-02-03T07:13:46Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "438fa754-50bb-4b99-8406-902c14efbdeb",
        "parentId" : "96ad31c3-9fec-49a5-96b6-146c9a713a0c",
        "authorId" : "4a21ee68-297a-45ce-ad3f-eebc5801c797",
        "body" : "Since we are converting to/from java.sql.Time, and according to the javadoc https://docs.oracle.com/javase/8/docs/api/java/sql/Time.html , it supports till milliseconds for constructor.",
        "createdAt" : "2021-02-03T07:20:05Z",
        "updatedAt" : "2021-02-03T07:20:05Z",
        "lastEditedBy" : "4a21ee68-297a-45ce-ad3f-eebc5801c797",
        "tags" : [
        ]
      },
      {
        "id" : "fc917494-1990-497b-98f2-035ce7c5427d",
        "parentId" : "96ad31c3-9fec-49a5-96b6-146c9a713a0c",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "It may confuse Spark users, as Spark timestamp is microsecond precision.\r\n\r\nAfter more thought, it's probably better to return timestamp when reading JDBC time, with a clear rule: we convert the time to timestamp by using \"zero epoch\" as the date part. It's also more useful as users can call `hour` function or similar ones to get some field values. What do you think?",
        "createdAt" : "2021-02-03T07:33:55Z",
        "updatedAt" : "2021-02-03T07:33:55Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "cc82b2eb-b60f-4f20-9c17-b28fa5122cfd",
        "parentId" : "96ad31c3-9fec-49a5-96b6-146c9a713a0c",
        "authorId" : "4a21ee68-297a-45ce-ad3f-eebc5801c797",
        "body" : "Hmm, I agree with you on the user experience part (`hour` function and all). I think it is hard (near impossible) to introduce new DataType (Time - HH:MM:SS.sss display) and another function to convert an int without date portion to Timestamp (most conversion is parse string) as it gonna take through multiple level of approvals and testings.\r\n\r\nAnother reason why I went with Int was that with TimestampType, it breaks compatibility with Avro logical & Spark type converter (https://github.com/apache/spark/blob/master/external/avro/src/main/scala/org/apache/spark/sql/avro/SchemaConverters.scala).\r\n\r\nI'm not sure if a `time_to_timestamp()` helper function would be a better compromise or revert back the 2 MRs?",
        "createdAt" : "2021-02-03T08:01:27Z",
        "updatedAt" : "2021-02-03T08:01:28Z",
        "lastEditedBy" : "4a21ee68-297a-45ce-ad3f-eebc5801c797",
        "tags" : [
        ]
      },
      {
        "id" : "c45d2c62-9bb2-4aeb-ba8c-4a6f8684fa9d",
        "parentId" : "96ad31c3-9fec-49a5-96b6-146c9a713a0c",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "It looks better if the avro schema converter can convert timestamp to time. After reading time column from JDBC, it becomes `IntegerType` and there is no context to indicate that this int comes from JDBC time and means milliseconds. What if the avro logic type is time-micros? With timestamp type, at least we know the precision is microsecond.",
        "createdAt" : "2021-02-03T10:25:24Z",
        "updatedAt" : "2021-02-03T10:25:24Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "1822df23-8f1a-4dae-99fc-45c4b70588f9",
        "parentId" : "96ad31c3-9fec-49a5-96b6-146c9a713a0c",
        "authorId" : "4a21ee68-297a-45ce-ad3f-eebc5801c797",
        "body" : "Your suggestion makes sense. We can also stuff the info into the metadata field of the struct field. Let me find sometimes this weekend or over the new year to try out your suggestion? Will ping you back if I couldn't find time or manage to get a solution so we can revert the MRs. Thanks a lots for helping me out!\r\n\r\nShould I create a new JIRA ticket and cut a new MR or do you prefer to consolidate in here?",
        "createdAt" : "2021-02-03T10:41:18Z",
        "updatedAt" : "2021-02-03T10:41:18Z",
        "lastEditedBy" : "4a21ee68-297a-45ce-ad3f-eebc5801c797",
        "tags" : [
        ]
      },
      {
        "id" : "5dfc1ca4-d891-4928-9686-2186dbd3f6cc",
        "parentId" : "96ad31c3-9fec-49a5-96b6-146c9a713a0c",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Let's create a new JIRA to track it. The PR is merged to master only so we have plenty of time to fix it before the 3.2 release :)",
        "createdAt" : "2021-02-03T16:14:04Z",
        "updatedAt" : "2021-02-03T16:14:04Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "a3e53d3c-fde3-4f31-88d4-127380ec8482",
        "parentId" : "96ad31c3-9fec-49a5-96b6-146c9a713a0c",
        "authorId" : "4a21ee68-297a-45ce-ad3f-eebc5801c797",
        "body" : "@cloud-fan btw, I did a quick check and it seems like if we use TimestampType, the time will always be converted to the JVM system timezone. So for the JDBCSuite test, given `12:34:56` time value, when you do the `.select(hour(\"time\"))` it will always point to my local timezone hour instead of `12`. So I don't know if we should proceed in this case.",
        "createdAt" : "2021-02-04T06:49:03Z",
        "updatedAt" : "2021-02-04T06:49:03Z",
        "lastEditedBy" : "4a21ee68-297a-45ce-ad3f-eebc5801c797",
        "tags" : [
        ]
      },
      {
        "id" : "c21709a6-5d55-463e-8dd6-5c93f2928469",
        "parentId" : "96ad31c3-9fec-49a5-96b6-146c9a713a0c",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "We can do whatever we want. We can use JDBC API `getTime` to get the time value, and construct the timestamp value in a reasonable way. It's under our control.",
        "createdAt" : "2021-02-04T07:13:52Z",
        "updatedAt" : "2021-02-04T07:13:53Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "63fad20278c90b55fc311f276008ead5883dba1d",
    "line" : 50,
    "diffHunk" : "@@ -1,1 +425,429 @@    // SPARK-33888 - sql TIME type represents as physical int in millis\n    // Represents a time of day, with no reference to a particular calendar,\n    // time zone or date, with a precision of one millisecond.\n    // It stores the number of milliseconds after midnight, 00:00:00.000.\n    case IntegerType if metadata.contains(\"logical_time_type\") =>"
  },
  {
    "id" : "969bb044-aa74-4557-9833-83a32d9783eb",
    "prId" : 30154,
    "prUrl" : "https://github.com/apache/spark/pull/30154#pullrequestreview-519378196",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6a996576-6693-4845-a3d7-9e90f038d757",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "can we make comment a parameter of `Option[String]`, instead of options?",
        "createdAt" : "2020-10-29T06:59:09Z",
        "updatedAt" : "2020-11-06T06:50:03Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "ecc9a4e69af4d22a5df51abec0eefd6887ff5beb",
    "line" : 3,
    "diffHunk" : "@@ -1,1 +863,867 @@      schema: StructType,\n      caseSensitive: Boolean,\n      options: JdbcOptionsInWrite): Unit = {\n    val dialect = JdbcDialects.get(options.url)\n    val strSchema = schemaString("
  },
  {
    "id" : "72442f9b-8e67-4278-84d8-957c2d3cdf81",
    "prId" : 30154,
    "prUrl" : "https://github.com/apache/spark/pull/30154#pullrequestreview-522143684",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1d90492a-ad99-4cc0-bd4f-c06a2e806d0d",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "instead of having a dialect API to set table comment, can we have a dialect API to create table?",
        "createdAt" : "2020-11-02T06:38:22Z",
        "updatedAt" : "2020-11-06T06:50:03Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "601fa107-2708-4fa4-a575-39202b841f4c",
        "parentId" : "1d90492a-ad99-4cc0-bd4f-c06a2e806d0d",
        "authorId" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "body" : "Updated the code to have a dialect API to create table. Take a look to see which way you like. \r\nI thought I can have one sql statement for MYSQL because it supports the syntax of appending comment after CREATE TABLE, but I think over, I should still use a separate sql statement for comment because I want to log a Warning if it fails.",
        "createdAt" : "2020-11-03T01:34:31Z",
        "updatedAt" : "2020-11-06T06:50:03Z",
        "lastEditedBy" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "tags" : [
        ]
      }
    ],
    "commit" : "ecc9a4e69af4d22a5df51abec0eefd6887ff5beb",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +877,881 @@      try {\n        executeStatement(\n          conn, options, dialect.getTableCommentQuery(tableName, options.tableComment))\n      } catch {\n        case e: Exception =>"
  },
  {
    "id" : "c78f4381-dce6-436f-9c43-83ffee797783",
    "prId" : 30001,
    "prUrl" : "https://github.com/apache/spark/pull/30001#pullrequestreview-509911454",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "88b2008f-eb2f-414b-90fc-ed117a2ea88c",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Could you pull this check from the loop? I think we don't need to check it in every iterations.",
        "createdAt" : "2020-10-16T00:12:37Z",
        "updatedAt" : "2020-12-09T06:49:51Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "0ebceb01d1bbd30345f4d0a3662f34a51bc965d7",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +677,681 @@            i = i + 1\n          }\n          if (supportsBatchUpdates) {\n            stmt.addBatch()\n            rowCount += 1"
  },
  {
    "id" : "0fea2b0a-2064-4021-a250-a349efadd51c",
    "prId" : 29324,
    "prUrl" : "https://github.com/apache/spark/pull/29324#pullrequestreview-460512864",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9b962b9e-4092-4860-82b0-5f8e76b64407",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Should we record auto commit status and restore back?",
        "createdAt" : "2020-08-04T04:13:36Z",
        "updatedAt" : "2020-08-05T07:11:02Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "48e1aff3-698e-454b-87b7-bdca5330c7ac",
        "parentId" : "9b962b9e-4092-4860-82b0-5f8e76b64407",
        "authorId" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "body" : "I guess we don't need to record the original auto commit status. The default value of autocommit is true.",
        "createdAt" : "2020-08-04T05:50:09Z",
        "updatedAt" : "2020-08-05T07:11:02Z",
        "lastEditedBy" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "tags" : [
        ]
      }
    ],
    "commit" : "ef55b5ee728a16c67c993eabecd37aec2587d0fe",
    "line" : 94,
    "diffHunk" : "@@ -1,1 +924,928 @@        } finally {\n          statement.close()\n          conn.setAutoCommit(true)\n        }\n      }"
  },
  {
    "id" : "22df4ade-e9b1-426f-bcab-acc9b3f1e246",
    "prId" : 29323,
    "prUrl" : "https://github.com/apache/spark/pull/29323#pullrequestreview-459691470",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e9042a35-b575-474d-a711-f21b27a5b8c4",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "@MaxGekk . As you see the change on `PostgreSQL` dialect, this will break the downstream custom dialect. Shall we avoid to change this?",
        "createdAt" : "2020-08-03T00:08:17Z",
        "updatedAt" : "2020-08-03T00:08:18Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "7251ba2de5267683c7d4251877094969f268c888",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +45,49 @@ * Util functions for JDBC tables.\n */\nobject JDBCUtils extends Logging {\n  /**\n   * Returns a factory for creating connections to the given JDBC URL."
  },
  {
    "id" : "8a72f79e-1c72-4a19-811e-42a4b31c91a0",
    "prId" : 29317,
    "prUrl" : "https://github.com/apache/spark/pull/29317#pullrequestreview-459129046",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4a9e0328-3d99-4ded-9136-c70650046681",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Here is the fix - replacing `checkColumnNameDuplication` by `checkSchemaColumnNameDuplication`",
        "createdAt" : "2020-07-31T12:17:36Z",
        "updatedAt" : "2020-07-31T16:23:46Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "5d1dcaf37713b500f85f594bc7d6314a9270a740",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +821,825 @@\n      SchemaUtils.checkSchemaColumnNameDuplication(\n        userSchema,\n        \"in the customSchema option value\",\n        nameEquality)"
  },
  {
    "id" : "d011610d-1ff0-48b5-823d-fe1dce9010eb",
    "prId" : 29317,
    "prUrl" : "https://github.com/apache/spark/pull/29317#pullrequestreview-459296040",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f947591d-85b7-451a-886f-c9f37a2b61d1",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Oh, I see. We need to accept a nested schema in `customSchema`? I checked the original PR https://github.com/apache/spark/pull/18266, but I couldn't find test cases for nested schemas. So, I'm not sure this is an expected behaviour... cc: @wangyum",
        "createdAt" : "2020-07-31T14:02:35Z",
        "updatedAt" : "2020-07-31T16:23:46Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "8b688440-032f-4cfe-a9db-982937138f2a",
        "parentId" : "f947591d-85b7-451a-886f-c9f37a2b61d1",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I don't know which JDBC server supports nested schema. But IIUC this feature is to specify the type, and I think it can be used to specify the data type of nested fields as well.",
        "createdAt" : "2020-07-31T14:14:55Z",
        "updatedAt" : "2020-07-31T16:23:46Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "7c8f5a9a-d691-4808-bffc-ca365bfbba68",
        "parentId" : "f947591d-85b7-451a-886f-c9f37a2b61d1",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Yea, it can be and accepting nested fields looks okay. Either way, I think we need more test cases for `customeSchema` with nested fields, arrays, map, ...",
        "createdAt" : "2020-07-31T14:27:17Z",
        "updatedAt" : "2020-07-31T16:23:46Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "1f6a983c-6e74-4ac5-a057-8790b1ee2bd3",
        "parentId" : "f947591d-85b7-451a-886f-c9f37a2b61d1",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "JDBC spec mentions the STRUCT type, for example https://docs.oracle.com/javase/8/docs/api/java/sql/Types.html#STRUCT.\r\n\r\nAt least, you can access to Spark cluster from another Spark cluster via JDBC ;-)",
        "createdAt" : "2020-07-31T16:14:52Z",
        "updatedAt" : "2020-07-31T16:23:46Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "5d1dcaf37713b500f85f594bc7d6314a9270a740",
    "line" : 1,
    "diffHunk" : "@@ -1,1 +817,821 @@      customSchema: String,\n      nameEquality: Resolver): StructType = {\n    if (null != customSchema && customSchema.nonEmpty) {\n      val userSchema = CatalystSqlParser.parseTableSchema(customSchema)\n"
  },
  {
    "id" : "bad2b3af-6b82-4420-8576-0af898d3a851",
    "prId" : 28953,
    "prUrl" : "https://github.com/apache/spark/pull/28953#pullrequestreview-440489867",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7f6e27fd-0b08-4bcf-91e1-2c70278fce32",
        "parentId" : null,
        "authorId" : "a2fcc15e-a51a-42f0-87a6-137048a28e30",
        "body" : "@moomindani Typically in my experience, in traditional databases, we do the pre and post trigger actions in the same transaction as the main action. I was saying that in spark, it's probably not possible as the main action could be done in a distributed fashion. So if we do this, we probably have to document it telling our users that, when a post action fails, we still can have the pre and post action committed.",
        "createdAt" : "2020-06-30T18:32:24Z",
        "updatedAt" : "2020-08-20T07:17:28Z",
        "lastEditedBy" : "a2fcc15e-a51a-42f0-87a6-137048a28e30",
        "tags" : [
        ]
      },
      {
        "id" : "14c3141c-45f6-4715-9edc-86c331520ead",
        "parentId" : "7f6e27fd-0b08-4bcf-91e1-2c70278fce32",
        "authorId" : "8c4501b6-dbfc-491f-80a5-f615a2311f95",
        "body" : "Makes sense. I have updated the document to include this point.",
        "createdAt" : "2020-07-01T01:31:49Z",
        "updatedAt" : "2020-08-20T07:17:28Z",
        "lastEditedBy" : "8c4501b6-dbfc-491f-80a5-f615a2311f95",
        "tags" : [
        ]
      }
    ],
    "commit" : "3244c301914219abc8721ab23c224ca99f64fbbd",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +123,127 @@  def runQuery(conn: Connection, actions: String, options: JDBCOptions): Unit = {\n    val autoCommit = conn.getAutoCommit\n    conn.setAutoCommit(false)\n\n    val commands = StringUtils.splitSemiColon(actions).asScala"
  },
  {
    "id" : "e663d242-db6e-472c-a0e8-ce46ee035aa9",
    "prId" : 28953,
    "prUrl" : "https://github.com/apache/spark/pull/28953#pullrequestreview-445524166",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c16acbe7-bd96-4421-b564-337e16fd954e",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "How about adding `logInfo` here? https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD.scala#L283",
        "createdAt" : "2020-07-08T07:51:45Z",
        "updatedAt" : "2020-08-20T07:17:28Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "d4195695-c82f-46c3-8945-5876b81b4f25",
        "parentId" : "c16acbe7-bd96-4421-b564-337e16fd954e",
        "authorId" : "8c4501b6-dbfc-491f-80a5-f615a2311f95",
        "body" : "Added `logInfo` in similar way.",
        "createdAt" : "2020-07-09T11:22:40Z",
        "updatedAt" : "2020-08-20T07:17:28Z",
        "lastEditedBy" : "8c4501b6-dbfc-491f-80a5-f615a2311f95",
        "tags" : [
        ]
      }
    ],
    "commit" : "3244c301914219abc8721ab23c224ca99f64fbbd",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +129,133 @@    statement.setQueryTimeout(options.queryTimeout)\n\n    commands.foreach { command =>\n      val sql = command.trim()\n      statement.addBatch(sql)"
  },
  {
    "id" : "8d21aed6-75cf-4ca4-84dd-aa85d2889cc6",
    "prId" : 26301,
    "prUrl" : "https://github.com/apache/spark/pull/26301#pullrequestreview-311474872",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "adae01ed-2eac-4953-9ccc-f2ab90e2ce54",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Just to check, these are pretty standard type names in DBs?",
        "createdAt" : "2019-11-02T13:57:43Z",
        "updatedAt" : "2019-11-13T22:08:53Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "7a6b90bf-3116-4a85-9236-63e5e9d00a50",
        "parentId" : "adae01ed-2eac-4953-9ccc-f2ab90e2ce54",
        "authorId" : "af133a35-c4d5-4a44-81d2-5683a0e3bb07",
        "body" : "Yes. When a DB does not support a particularity type or uses a different type then the respective dialect should be modified. e.g. Postgres does not support tinyint and you 'll see that in the PostfresDialect.scala ByteType also gets mapped to SMALLINT. Have added a sanity test in PostgresIntegrationSuite.scala.",
        "createdAt" : "2019-11-05T02:37:52Z",
        "updatedAt" : "2019-11-13T22:08:53Z",
        "lastEditedBy" : "af133a35-c4d5-4a44-81d2-5683a0e3bb07",
        "tags" : [
        ]
      }
    ],
    "commit" : "06008946964cc7acb8df3ff781513e4577dd2116",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +171,175 @@      case DoubleType => Option(JdbcType(\"DOUBLE PRECISION\", java.sql.Types.DOUBLE))\n      case FloatType => Option(JdbcType(\"REAL\", java.sql.Types.FLOAT))\n      case ShortType => Option(JdbcType(\"SMALLINT\", java.sql.Types.SMALLINT))\n      case ByteType => Option(JdbcType(\"TINYINT\", java.sql.Types.TINYINT))\n      case BooleanType => Option(JdbcType(\"BIT(1)\", java.sql.Types.BIT))"
  }
]