[
  {
    "id" : "61828185-25c3-4a00-afff-b113793cfb66",
    "prId" : 31848,
    "prUrl" : "https://github.com/apache/spark/pull/31848#pullrequestreview-615735662",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1646b0c1-2b1a-4a9f-ba93-8d0711663053",
        "parentId" : null,
        "authorId" : "3d4870da-39a8-4406-b00d-131930d14cd8",
        "body" : "Shall I update `hashCode()` as well? Looks like we can easily come up with a better one...",
        "createdAt" : "2021-03-18T18:05:20Z",
        "updatedAt" : "2021-03-22T13:36:26Z",
        "lastEditedBy" : "3d4870da-39a8-4406-b00d-131930d14cd8",
        "tags" : [
        ]
      },
      {
        "id" : "f4080396-880e-49d3-a0b1-80107fc0adb7",
        "parentId" : "1646b0c1-2b1a-4a9f-ba93-8d0711663053",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Shall we do that separately because it's irrelevant to the correctness issue?\r\nIn general, we expect a performance improvement with that, don't we?\r\nApache Spark doesn't allow to backport performance improvement.",
        "createdAt" : "2021-03-18T18:25:33Z",
        "updatedAt" : "2021-03-22T13:36:26Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "7668deee-6b0f-48cb-85da-ff48770c4721",
        "parentId" : "1646b0c1-2b1a-4a9f-ba93-8d0711663053",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Please file a new JIRA and go for it, @peter-toth ! :)",
        "createdAt" : "2021-03-18T18:26:10Z",
        "updatedAt" : "2021-03-22T13:36:26Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "0d38ac27dec27b77e0057113fbb57de8ea4cd3ca",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +99,103 @@  }\n\n  override def equals(obj: Any): Boolean = obj match {\n    case f: FileScan =>\n      fileIndex == f.fileIndex && readSchema == f.readSchema &&"
  },
  {
    "id" : "2283a719-abc0-4a42-b058-21eccfa0eddf",
    "prId" : 28425,
    "prUrl" : "https://github.com/apache/spark/pull/28425#pullrequestreview-446848884",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "068207b0-87c2-43c8-952d-711619755a60",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Just a question. Is this a new metadata?",
        "createdAt" : "2020-07-11T15:01:06Z",
        "updatedAt" : "2020-07-12T06:12:07Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "a57c7416-5c1f-4152-860f-e23f665fc032",
        "parentId" : "068207b0-87c2-43c8-952d-711619755a60",
        "authorId" : "a2fcc15e-a51a-42f0-87a6-137048a28e30",
        "body" : "@dongjoon-hyun Previously we used to print the scan node class name and in the new format, we print it in its own line. Please see the old output in the pr description. We have it printed as `ParquetScan`.",
        "createdAt" : "2020-07-12T06:07:54Z",
        "updatedAt" : "2020-07-12T06:12:07Z",
        "lastEditedBy" : "a2fcc15e-a51a-42f0-87a6-137048a28e30",
        "tags" : [
        ]
      }
    ],
    "commit" : "e177c2a3a99ce2ed64c0562035f0552a4c25c919",
    "line" : 38,
    "diffHunk" : "@@ -1,1 +113,117 @@        Utils.buildLocationMetadata(fileIndex.rootPaths, maxMetadataValueLength)\n    Map(\n      \"Format\" -> s\"${this.getClass.getSimpleName.replace(\"Scan\", \"\").toLowerCase(Locale.ROOT)}\",\n      \"ReadSchema\" -> readDataSchema.catalogString,\n      \"PartitionFilters\" -> seqToString(partitionFilters),"
  },
  {
    "id" : "b6e0c21e-6ae0-4eda-9ca8-a2535a0d2b5b",
    "prId" : 27157,
    "prUrl" : "https://github.com/apache/spark/pull/27157#pullrequestreview-340896450",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1fa29e5e-e438-4f60-8785-088462675418",
        "parentId" : null,
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "It seems too many methods to implement in `FileScan`.\r\nHow about just\r\n```\r\ndef filters: Seq[Expression]\r\ndef withFilters(filters: Seq[Expression]): FileScan\r\n```\r\nAnd we get the data/partition filters in FileScan.",
        "createdAt" : "2020-01-09T23:28:58Z",
        "updatedAt" : "2020-01-18T05:56:27Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "9c883af5-cd18-42de-9420-b2910d20f0d5",
        "parentId" : "1fa29e5e-e438-4f60-8785-088462675418",
        "authorId" : "fdf14d51-7a8c-4235-b6a2-99d0c173e917",
        "body" : "ok, agree, I will make the necessary changes",
        "createdAt" : "2020-01-09T23:30:41Z",
        "updatedAt" : "2020-01-18T05:56:27Z",
        "lastEditedBy" : "fdf14d51-7a8c-4235-b6a2-99d0c173e917",
        "tags" : [
        ]
      },
      {
        "id" : "0e62085f-aeb9-464d-9018-0ae52debf1f4",
        "parentId" : "1fa29e5e-e438-4f60-8785-088462675418",
        "authorId" : "fdf14d51-7a8c-4235-b6a2-99d0c173e917",
        "body" : "@gengliangwang it's seem to me more straight forward to split the filters to `dataFilters` and `partitionFilters` within the `PruneFileSourcePartitions` rule as there is a need to resolve the partition columns from the relation in order to get the `partitionFilters` and `dataFilters`.\r\n\r\nAlso from the point of view of the `FileScan` trait it seems cleaner to have as a metadata in the description distinction between the `dataFilters` and `partitionFilters`.\r\n\r\nOne option is to have \r\n```\r\ndef withFilters(partitionFilters: Seq[Expression], dataFilters: Seq[Expression]): FileScan\r\n```\r\ninstead of 2 functions one for the `dataFilters` and one for the `partitionFilters`.\r\n\r\nWhat do you think?",
        "createdAt" : "2020-01-09T23:53:12Z",
        "updatedAt" : "2020-01-18T05:56:27Z",
        "lastEditedBy" : "fdf14d51-7a8c-4235-b6a2-99d0c173e917",
        "tags" : [
        ]
      },
      {
        "id" : "a6fe56e5-d59f-4eda-9f6a-907e00f3cde9",
        "parentId" : "1fa29e5e-e438-4f60-8785-088462675418",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "> def withFilters(partitionFilters: Seq[Expression], dataFilters: Seq[Expression]): FileScan\r\n\r\nthis one seems good",
        "createdAt" : "2020-01-10T00:08:58Z",
        "updatedAt" : "2020-01-18T05:56:27Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "44d642e1-7fac-4b43-8b4c-98679829fa37",
        "parentId" : "1fa29e5e-e438-4f60-8785-088462675418",
        "authorId" : "fdf14d51-7a8c-4235-b6a2-99d0c173e917",
        "body" : "@gengliangwang fixed per the comment",
        "createdAt" : "2020-01-10T00:27:59Z",
        "updatedAt" : "2020-01-18T05:56:27Z",
        "lastEditedBy" : "fdf14d51-7a8c-4235-b6a2-99d0c173e917",
        "tags" : [
        ]
      }
    ],
    "commit" : "d181e38a4440c7673b1c14ead96d9d9be2720ea3",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +64,68 @@   * Returns the data filters that can be use for file listing\n   */\n  def dataFilters: Seq[Expression]\n\n  /**"
  },
  {
    "id" : "892a2cf7-cf79-4051-8743-b814589d418c",
    "prId" : 27112,
    "prUrl" : "https://github.com/apache/spark/pull/27112#pullrequestreview-340872312",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e3b351e9-6491-4f0e-bea1-4d629e3e537e",
        "parentId" : null,
        "authorId" : "fdf14d51-7a8c-4235-b6a2-99d0c173e917",
        "body" : "@gengliangwang @cloud-fan continuing the discussion from above (the comment was on the wrong line).\r\nThe `V2ScanRelationPushDown` rule will pushdown the `dataFilters` only to datasources which support pushdown by implementing the `SupportsPushDownFilters` trait.\r\nDatasources such as `csv` and `json` do not implement the `SupportsPushDownFilters` trait. In order to support data skipping uniformly for all file based data sources, we override the `listFiles` method in a FileIndex implementation, which consults external metadata and prunes the list of files.\r\nThe suggestion is to make the necessary changes to have the `dataFilters` passed to the `listFiles` as well.\r\nOtherwise, one would have to create a new datasource implementation in order to support each file based datasource that doesn't have a built in pushdown mechanism.",
        "createdAt" : "2020-01-09T14:09:55Z",
        "updatedAt" : "2020-01-09T14:11:59Z",
        "lastEditedBy" : "fdf14d51-7a8c-4235-b6a2-99d0c173e917",
        "tags" : [
        ]
      },
      {
        "id" : "72d76287-41f8-47ff-b115-a57a53baaa1e",
        "parentId" : "e3b351e9-6491-4f0e-bea1-4d629e3e537e",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "This makes sense to me. @gengliangwang what do you think?\r\n\r\nAt least you can disable v2 file source to bring back this feature.",
        "createdAt" : "2020-01-09T14:31:21Z",
        "updatedAt" : "2020-01-09T14:31:22Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "2487e393-e47f-4eb1-bbfb-8346d2e75261",
        "parentId" : "e3b351e9-6491-4f0e-bea1-4d629e3e537e",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "Yes, it makes sense if there is a `fileIndex` can use the `dataFilters`.\r\n@guykhazma  could you create a PR for this?",
        "createdAt" : "2020-01-09T18:47:32Z",
        "updatedAt" : "2020-01-09T18:47:33Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "daaf3b51-4bfe-4ef9-942b-6d68481946c3",
        "parentId" : "e3b351e9-6491-4f0e-bea1-4d629e3e537e",
        "authorId" : "fdf14d51-7a8c-4235-b6a2-99d0c173e917",
        "body" : "@gengliangwang @cloud-fan sure, thanks.\r\nI have opened this [PR](https://github.com/apache/spark/pull/27157)",
        "createdAt" : "2020-01-09T23:10:04Z",
        "updatedAt" : "2020-01-09T23:10:04Z",
        "lastEditedBy" : "fdf14d51-7a8c-4235-b6a2-99d0c173e917",
        "tags" : [
        ]
      }
    ],
    "commit" : "58a4a07c50fd0ef54ca55fe1883d228eeb8464b2",
    "line" : 84,
    "diffHunk" : "@@ -1,1 +104,108 @@\n  protected def partitions: Seq[FilePartition] = {\n    val selectedPartitions = fileIndex.listFiles(partitionFilters, Seq.empty)\n    val maxSplitBytes = FilePartition.maxSplitBytes(sparkSession, selectedPartitions)\n    val partitionAttributes = fileIndex.partitionSchema.toAttributes"
  }
]