[
  {
    "id" : "4dd11a41-7480-4e8a-8579-e9558bfdd564",
    "prId" : 30473,
    "prUrl" : "https://github.com/apache/spark/pull/30473#pullrequestreview-536746651",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7d8f37d0-ab3b-4dd3-9b27-fd18c55c6c0b",
        "parentId" : null,
        "authorId" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "body" : "Seems to me the only property we can support for name space is schema comment. I don't have a good way to retrieve schema comment, so I will return an empty map for now.",
        "createdAt" : "2020-11-23T18:37:30Z",
        "updatedAt" : "2020-12-03T16:05:02Z",
        "lastEditedBy" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "tags" : [
        ]
      }
    ],
    "commit" : "68838c979de47d35d1df3324b3a92ab80615d053",
    "line" : 88,
    "diffHunk" : "@@ -1,1 +214,218 @@      case Array(db) =>\n        if (!namespaceExists(namespace)) throw new NoSuchNamespaceException(db)\n        mutable.HashMap[String, String]().asJava\n\n      case _ =>"
  },
  {
    "id" : "0b58ffa2-dcec-425a-88e7-8165c9b3cbe6",
    "prId" : 30473,
    "prUrl" : "https://github.com/apache/spark/pull/30473#pullrequestreview-540699064",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ed0679c9-4689-4500-b0a0-733570f3c864",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "shall we fail and say \"action xxx not supported\"?",
        "createdAt" : "2020-11-30T08:48:59Z",
        "updatedAt" : "2020-12-03T16:05:02Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "68838c979de47d35d1df3324b3a92ab80615d053",
    "line" : 151,
    "diffHunk" : "@@ -1,1 +277,281 @@            }\n\n          case _ =>\n            throw new AnalysisException(s\"Unsupported NamespaceChange $changes in JDBC catalog.\")\n        }"
  },
  {
    "id" : "440e1f69-15ca-4999-b7df-6ec274701ce5",
    "prId" : 30473,
    "prUrl" : "https://github.com/apache/spark/pull/30473#pullrequestreview-543288050",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f9d6c938-c572-454a-8efe-f74c7dca156a",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "does it return schemas only from the current catalog?",
        "createdAt" : "2020-12-02T05:40:04Z",
        "updatedAt" : "2020-12-03T16:05:02Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "575de141-bd0c-48c9-8baa-e162c931a075",
        "parentId" : "f9d6c938-c572-454a-8efe-f74c7dca156a",
        "authorId" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "body" : "DB2 jdbc driver implements this to return schemas from the current catalog (the current database jdbc driver connects to). Not sure how other jdbc drivers implement this. I tested with postgres jdbc driver, it also returns schemas from the current catalog. In listTables (https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/jdbc/JDBCTableCatalog.scala#L62), we also assume ```getTables``` with `null` as catalog value returns tables from the current catalog. ",
        "createdAt" : "2020-12-02T22:24:22Z",
        "updatedAt" : "2020-12-03T16:05:02Z",
        "lastEditedBy" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "tags" : [
        ]
      }
    ],
    "commit" : "68838c979de47d35d1df3324b3a92ab80615d053",
    "line" : 53,
    "diffHunk" : "@@ -1,1 +179,183 @@    case Array(db) =>\n      withConnection { conn =>\n        val rs = conn.getMetaData.getSchemas(null, db)\n        while (rs.next()) {\n          if (rs.getString(1) == db) return true;"
  },
  {
    "id" : "c5e67643-99c2-43cb-a8d5-3b775791085e",
    "prId" : 30473,
    "prUrl" : "https://github.com/apache/spark/pull/30473#pullrequestreview-543288111",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "76f7ca37-2b17-48eb-a278-fb4709c4a40e",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "shall we return the input `namespace` here?",
        "createdAt" : "2020-12-02T05:41:28Z",
        "updatedAt" : "2020-12-03T16:05:02Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "77398012-8c8c-4426-979f-d042ed98a100",
        "parentId" : "76f7ca37-2b17-48eb-a278-fb4709c4a40e",
        "authorId" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "body" : "I am following the implementation in `V2SessionCatalog` (https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2SessionCatalog.scala#L211). According to the method definition `List namespaces in a namespace`, the method returns namespaces inside the input `namespace`, which is empty.",
        "createdAt" : "2020-12-02T22:24:28Z",
        "updatedAt" : "2020-12-03T16:05:02Z",
        "lastEditedBy" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "tags" : [
        ]
      }
    ],
    "commit" : "68838c979de47d35d1df3324b3a92ab80615d053",
    "line" : 78,
    "diffHunk" : "@@ -1,1 +204,208 @@        listNamespaces()\n      case Array(_) if namespaceExists(namespace) =>\n        Array()\n      case _ =>\n        throw new NoSuchNamespaceException(namespace)"
  },
  {
    "id" : "14c53971-ed28-4a4b-9319-a52bb2402fc2",
    "prId" : 30154,
    "prUrl" : "https://github.com/apache/spark/pull/30154#pullrequestreview-522143649",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d6090a6c-5799-43f9-92cc-603b7b077a36",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "This is a good idea. We can make the rule a bit more clearer:\r\n1. \"provider\", \"owner\" and \"location\" can't be set. We can ignore \"provider\" for now, and wait for the CREATE TABLE unification PR to be merged.\r\n2. For other options, just add them to the `writeOptions`, as let the dialect to decide. (fail if a dialect doesn't support one of the properties)",
        "createdAt" : "2020-11-02T06:35:38Z",
        "updatedAt" : "2020-11-06T06:50:03Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "40a114d3-6a66-46ed-8009-fe9a47521ad3",
        "parentId" : "d6090a6c-5799-43f9-92cc-603b7b077a36",
        "authorId" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "body" : "Sorry, I need a little bit clarification on this :)\r\nWhen you say \"let the dialect to decide\", do you mean to let the database to decide if fail the CREATE TABLE or not (my current implementation)? Or you mean in each of the XXXDialect classes, I need to check if the table options are valid or not, and fail the CREATE TABLE on Spark side if options are invalid? I prefer the first one.",
        "createdAt" : "2020-11-03T01:34:22Z",
        "updatedAt" : "2020-11-06T06:50:03Z",
        "lastEditedBy" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "tags" : [
        ]
      }
    ],
    "commit" : "ecc9a4e69af4d22a5df51abec0eefd6887ff5beb",
    "line" : 44,
    "diffHunk" : "@@ -1,1 +146,150 @@      // E.g., \"CREATE TABLE t (name string) ENGINE InnoDB DEFAULT CHARACTER SET utf8\"\n      // Spark doesn't check if these table properties are supported by databases. If\n      // table property is invalid, database will fail the table creation.\n      tableOptions = tableOptions + (JDBCOptions.JDBC_CREATE_TABLE_OPTIONS -> tableProperties)\n    }"
  }
]