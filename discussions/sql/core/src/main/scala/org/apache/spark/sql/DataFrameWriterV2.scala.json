[
  {
    "id" : "adb993c4-6338-4d3e-b39d-dc565f041d54",
    "prId" : 29830,
    "prUrl" : "https://github.com/apache/spark/pull/29830#pullrequestreview-493156512",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "87c77925-a4ea-483c-a498-6055b1670530",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Should we write it about `AnalysisException` and update the docs above? maybe at least just for the sake of matching.",
        "createdAt" : "2020-09-22T06:01:33Z",
        "updatedAt" : "2020-09-22T06:01:34Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "bb3a845d-e828-4a0d-a5fa-4663c35e8ac7",
        "parentId" : "87c77925-a4ea-483c-a498-6055b1670530",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "+1 for @HyukjinKwon 's suggestion.",
        "createdAt" : "2020-09-22T06:25:36Z",
        "updatedAt" : "2020-09-22T06:25:36Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "4f1c3545-c240-4ce4-8ea0-67121b0e65b5",
        "parentId" : "87c77925-a4ea-483c-a498-6055b1670530",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "I thought about this, and decided to skip because AnalysisException is thrown for whatever cases even without this case. We couldn't explain all these things. I'll add it if we prefer to explain it at least for explicit case.",
        "createdAt" : "2020-09-22T06:29:58Z",
        "updatedAt" : "2020-09-22T06:30:07Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "2c750627182ea1264d567af531c3e343854479a6",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +204,208 @@   *\n   * @throws org.apache.spark.sql.catalyst.analysis.NoSuchTableException If the table does not exist\n   */\n  @throws(classOf[NoSuchTableException])\n  def overwritePartitions(): Unit = {"
  },
  {
    "id" : "e5e12a85-caea-45cf-9a62-8d8411e67962",
    "prId" : 29830,
    "prUrl" : "https://github.com/apache/spark/pull/29830#pullrequestreview-504106179",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c28dfb06-5e0a-41c1-8f27-d49c6f709143",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I'm thinking of a more aggressive refactor, which creates an unresolved logical plan here and leaves the table/temp view lookup to the analyzer.\r\n\r\nFor example, here we can just create `AppendData.byName(UnresolvedRelation(...))`.\r\n\r\nBy doing this, we can make the framework more clear: the API layer should just create logical plans, other works should be done by the analyzer and query planner.",
        "createdAt" : "2020-09-22T08:37:15Z",
        "updatedAt" : "2020-09-22T08:37:15Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "6334153d-bbb7-4fab-88a6-95095799c2b8",
        "parentId" : "c28dfb06-5e0a-41c1-8f27-d49c6f709143",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "I agree the suggestion is promising, but I'm not sure I understand all spots I need to modify. I could try to find places where `InsertIntoStatement(UnresolvedRelation(...))` is resolved, but AppendData and InsertIntoStatement are not 100% same. In addition, looks like it should be done for each operation.\r\n\r\nI'll take a look at this soon. If you can fix it easily (and you'd like to) please go ahead and I can learn from your PR.",
        "createdAt" : "2020-09-22T12:07:22Z",
        "updatedAt" : "2020-09-22T12:07:23Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "36eb56bc-c566-4c9a-b922-edbc88d2d7bc",
        "parentId" : "c28dfb06-5e0a-41c1-8f27-d49c6f709143",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "done in https://github.com/apache/spark/pull/29970",
        "createdAt" : "2020-10-07T17:24:16Z",
        "updatedAt" : "2020-10-07T17:24:17Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "2c750627182ea1264d567af531c3e343854479a6",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +155,159 @@  def append(): Unit = {\n    assertNoTempView(\"append\")\n    val append = loadTable(catalog, identifier) match {\n      case Some(t) =>\n        AppendData.byName("
  }
]