[
  {
    "id" : "adb993c4-6338-4d3e-b39d-dc565f041d54",
    "prId" : 29830,
    "prUrl" : "https://github.com/apache/spark/pull/29830#pullrequestreview-493156512",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "87c77925-a4ea-483c-a498-6055b1670530",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Should we write it about `AnalysisException` and update the docs above? maybe at least just for the sake of matching.",
        "createdAt" : "2020-09-22T06:01:33Z",
        "updatedAt" : "2020-09-22T06:01:34Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "bb3a845d-e828-4a0d-a5fa-4663c35e8ac7",
        "parentId" : "87c77925-a4ea-483c-a498-6055b1670530",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "+1 for @HyukjinKwon 's suggestion.",
        "createdAt" : "2020-09-22T06:25:36Z",
        "updatedAt" : "2020-09-22T06:25:36Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "4f1c3545-c240-4ce4-8ea0-67121b0e65b5",
        "parentId" : "87c77925-a4ea-483c-a498-6055b1670530",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "I thought about this, and decided to skip because AnalysisException is thrown for whatever cases even without this case. We couldn't explain all these things. I'll add it if we prefer to explain it at least for explicit case.",
        "createdAt" : "2020-09-22T06:29:58Z",
        "updatedAt" : "2020-09-22T06:30:07Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "2c750627182ea1264d567af531c3e343854479a6",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +204,208 @@   *\n   * @throws org.apache.spark.sql.catalyst.analysis.NoSuchTableException If the table does not exist\n   */\n  @throws(classOf[NoSuchTableException])\n  def overwritePartitions(): Unit = {"
  },
  {
    "id" : "e5e12a85-caea-45cf-9a62-8d8411e67962",
    "prId" : 29830,
    "prUrl" : "https://github.com/apache/spark/pull/29830#pullrequestreview-504106179",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c28dfb06-5e0a-41c1-8f27-d49c6f709143",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I'm thinking of a more aggressive refactor, which creates an unresolved logical plan here and leaves the table/temp view lookup to the analyzer.\r\n\r\nFor example, here we can just create `AppendData.byName(UnresolvedRelation(...))`.\r\n\r\nBy doing this, we can make the framework more clear: the API layer should just create logical plans, other works should be done by the analyzer and query planner.",
        "createdAt" : "2020-09-22T08:37:15Z",
        "updatedAt" : "2020-09-22T08:37:15Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "6334153d-bbb7-4fab-88a6-95095799c2b8",
        "parentId" : "c28dfb06-5e0a-41c1-8f27-d49c6f709143",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "I agree the suggestion is promising, but I'm not sure I understand all spots I need to modify. I could try to find places where `InsertIntoStatement(UnresolvedRelation(...))` is resolved, but AppendData and InsertIntoStatement are not 100% same. In addition, looks like it should be done for each operation.\r\n\r\nI'll take a look at this soon. If you can fix it easily (and you'd like to) please go ahead and I can learn from your PR.",
        "createdAt" : "2020-09-22T12:07:22Z",
        "updatedAt" : "2020-09-22T12:07:23Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "36eb56bc-c566-4c9a-b922-edbc88d2d7bc",
        "parentId" : "c28dfb06-5e0a-41c1-8f27-d49c6f709143",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "done in https://github.com/apache/spark/pull/29970",
        "createdAt" : "2020-10-07T17:24:16Z",
        "updatedAt" : "2020-10-07T17:24:17Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "2c750627182ea1264d567af531c3e343854479a6",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +155,159 @@  def append(): Unit = {\n    assertNoTempView(\"append\")\n    val append = loadTable(catalog, identifier) match {\n      case Some(t) =>\n        AppendData.byName("
  },
  {
    "id" : "dca6d30f-1bd4-4cfb-9738-cd9e8ea04e96",
    "prId" : 27992,
    "prUrl" : "https://github.com/apache/spark/pull/27992#pullrequestreview-384111808",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0f04f363-2159-45bc-9ec5-a9b40d1daaba",
        "parentId" : null,
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "This no longer passes the options as write options because the statement plans don't have write options, only table options.\r\n\r\nThis change should add `writeOptions` to the statement and update conversions to pass them through to the exec plans.",
        "createdAt" : "2020-03-23T22:03:16Z",
        "updatedAt" : "2020-03-31T06:29:04Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      },
      {
        "id" : "f2821f1f-d6a1-4979-9259-11fcc3da45d5",
        "parentId" : "0f04f363-2159-45bc-9ec5-a9b40d1daaba",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "The `Create/ReplaceTableAsSelectStatement` has an `options` field, which is used as the write options, see\r\n\r\nhttps://github.com/apache/spark/blob/27d53de10faa907a53a979b66282a457767b33ba/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveCatalogs.scala#L163\r\n\r\nhttps://github.com/apache/spark/blob/27d53de10faa907a53a979b66282a457767b33ba/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveCatalogs.scala#L189\r\n\r\nhttps://github.com/apache/spark/blob/27d53de10faa907a53a979b66282a457767b33ba/sql/core/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveSessionCatalog.scala#L296\r\n\r\nhttps://github.com/apache/spark/blob/27d53de10faa907a53a979b66282a457767b33ba/sql/core/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveSessionCatalog.scala#L335",
        "createdAt" : "2020-03-25T09:19:10Z",
        "updatedAt" : "2020-03-31T06:29:04Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "a77b0a0c-dea7-4b42-b787-5f6a112f7137",
        "parentId" : "0f04f363-2159-45bc-9ec5-a9b40d1daaba",
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "Okay, I see that we reuse options for both write options and table options.\r\n\r\nDo we want to separate those in the v2 API? I think we only mix them in the v1 API by accident. And we set table properties using `tableProperty`. Seems reasonable to separate table options and use `tableOption` or `storageOption` to set them.",
        "createdAt" : "2020-03-27T17:14:56Z",
        "updatedAt" : "2020-03-31T06:29:04Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      },
      {
        "id" : "9baff41c-ec11-4f84-bb33-cc237929f34f",
        "parentId" : "0f04f363-2159-45bc-9ec5-a9b40d1daaba",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I think about this more, and still not sure about the separation. Yes, Hive table needs this separation, but it's not general and AFAIK most databases do not separate properties and options of a table. For example, if I implement a Cassandra catalog, how shall I deal with properties and options of my `CassandraTable`?\r\n\r\nI think another approach can be: slowly deprecate OPTIONS and only use TBLPROPERTIES. We can always set the Hive table serde properties the same as the table properties.\r\n\r\nAnyway I think we don't need to make a decision here in this PR. I'll just add a new `writeOptions` field to the `CreateTableAsSelectStatement`.",
        "createdAt" : "2020-03-30T12:54:50Z",
        "updatedAt" : "2020-03-31T06:29:04Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "eb3b00de-10c6-44bd-b874-8cd279e9c394",
        "parentId" : "0f04f363-2159-45bc-9ec5-a9b40d1daaba",
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "I think it's a good idea to deprecate table options, but that doesn't really matter here. For this PR, we should keep write options separate from table options because they are used for different things.",
        "createdAt" : "2020-03-30T17:51:34Z",
        "updatedAt" : "2020-03-31T06:29:04Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      },
      {
        "id" : "8f2c06b4-d0de-4bff-a1bc-fbae4ea87a7d",
        "parentId" : "0f04f363-2159-45bc-9ec5-a9b40d1daaba",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "yup, so my new change that adds a new `writeOption` field does separate the options of `DataFrameWriter(V2)` from the table options.",
        "createdAt" : "2020-03-30T18:16:19Z",
        "updatedAt" : "2020-03-31T06:29:04Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "8d948739db7d247a80899b91ab13e5a38018cc21",
    "line" : 31,
    "diffHunk" : "@@ -1,1 +119,123 @@  override def create(): Unit = {\n    runCommand(\"create\") {\n      CreateTableAsSelectStatement(\n        tableName,\n        logicalPlan,"
  },
  {
    "id" : "94da9b65-7ff9-4c5d-884f-0fbf16c0a8f6",
    "prId" : 25681,
    "prUrl" : "https://github.com/apache/spark/pull/25681#pullrequestreview-293403070",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "eca924aa-f45f-431c-82ea-ae6cae6cf448",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "This should return `CreateTableWriter`. It doesn't make sense to specify table properties when inserting to an existing table.",
        "createdAt" : "2019-09-23T14:10:07Z",
        "updatedAt" : "2019-09-23T14:10:07Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "d215d8b0-8a4a-431c-bf1a-c5da87712480",
        "parentId" : "eca924aa-f45f-431c-82ea-ae6cae6cf448",
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "I agree.",
        "createdAt" : "2019-09-23T17:18:06Z",
        "updatedAt" : "2019-09-23T17:18:07Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      },
      {
        "id" : "6a7a5bbb-eb0b-426c-9082-fc4fc953add1",
        "parentId" : "eca924aa-f45f-431c-82ea-ae6cae6cf448",
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "Opened SPARK-29249 for this. Should have a PR posted soon.",
        "createdAt" : "2019-09-25T22:42:19Z",
        "updatedAt" : "2019-09-25T22:42:19Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      }
    ],
    "commit" : "ab7c3e89888d5233cefe64a8a91789e8e7a5504f",
    "line" : 96,
    "diffHunk" : "@@ -1,1 +94,98 @@  }\n\n  override def tableProperty(property: String, value: String): DataFrameWriterV2[T] = {\n    this.properties.put(property, value)\n    this"
  }
]