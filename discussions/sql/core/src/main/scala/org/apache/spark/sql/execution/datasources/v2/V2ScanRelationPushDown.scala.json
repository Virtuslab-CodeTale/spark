[
  {
    "id" : "7905affb-0ee8-4bbe-80f1-04bf9a453421",
    "prId" : 33584,
    "prUrl" : "https://github.com/apache/spark/pull/33584#pullrequestreview-719523582",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cdc9ddba-109b-4f7a-8763-56945a3fe8b0",
        "parentId" : null,
        "authorId" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "body" : "Here we have subtracted the `partitionFilters` from the `postScanFilters`. If the file based data source only has partition filters, then the `postScanFilters` should be empty here, and we can push down aggregate for  file based data source.\r\n",
        "createdAt" : "2021-07-30T20:21:20Z",
        "updatedAt" : "2021-07-30T20:21:20Z",
        "lastEditedBy" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "tags" : [
        ]
      },
      {
        "id" : "06e07b57-d5b6-4419-87e7-39faec9fe580",
        "parentId" : "cdc9ddba-109b-4f7a-8763-56945a3fe8b0",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "I'm not sure why we can't do this in the old code, where `filters` contain both partition filters and data filters. Suppose we have a method `DataSourceUtils.getPartitionKeyFiltersAndDataFilters`, then we can basically do:\r\n\r\n```scala\r\nval (partitionFilters, dataFilters) = DataSourceUtils.getPartitionKeyFiltersAndDataFilters(...)\r\nif (dataFilters.isEmpty) {\r\n  // pushdown aggregates\r\n}\r\n```\r\n?",
        "createdAt" : "2021-07-30T23:19:18Z",
        "updatedAt" : "2021-07-30T23:19:19Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "34ea0bac-8ee2-4831-909e-d64f5a1d5b56",
        "parentId" : "cdc9ddba-109b-4f7a-8763-56945a3fe8b0",
        "authorId" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "body" : "Yes, we can do it that way, but since we have already called `DataSourceUtils.getPartitionKeyFiltersAndDataFilters` to separate the partition filters and data filters, we can just push these two filters to the scan builder, so we can completely remove the v2 partition pruning code from `PruneFileSourcePartitions`.",
        "createdAt" : "2021-07-31T00:08:45Z",
        "updatedAt" : "2021-07-31T00:08:45Z",
        "lastEditedBy" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "tags" : [
        ]
      }
    ],
    "commit" : "bbc85db716de1db24bdb67f2ca9075dcaa648ded",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +62,66 @@        .pushPartitionFilters(sHolder.builder, sHolder.relation, normalizedFiltersWithoutSubquery)\n      postScanFilters =\n        (ExpressionSet(postScanFilters) -- partitionFilters.filter(_.references.nonEmpty)).toSeq\n\n      logInfo("
  },
  {
    "id" : "fc8faa02-e040-4c42-a0d9-b2e8d221fddb",
    "prId" : 33584,
    "prUrl" : "https://github.com/apache/spark/pull/33584#pullrequestreview-719634360",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dfc91cf5-5180-4eba-b91a-9f1114b2b468",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Honestly I feel that it is somehow not good to move partial partition pruning from `PruneFileSourcePartitions` to this place mixed with `V2ScanRelationPushDown`.\r\n\r\n",
        "createdAt" : "2021-08-01T08:06:40Z",
        "updatedAt" : "2021-08-01T08:07:03Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "bbc85db716de1db24bdb67f2ca9075dcaa648ded",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +60,64 @@      var postScanFilters = postScanFiltersWithoutSubquery ++ normalizedFiltersWithSubquery\n      val partitionFilters = PushDownUtils\n        .pushPartitionFilters(sHolder.builder, sHolder.relation, normalizedFiltersWithoutSubquery)\n      postScanFilters =\n        (ExpressionSet(postScanFilters) -- partitionFilters.filter(_.references.nonEmpty)).toSeq"
  },
  {
    "id" : "dccb051b-2f3d-4df5-981a-c7afa31c5fd8",
    "prId" : 33352,
    "prUrl" : "https://github.com/apache/spark/pull/33352#pullrequestreview-715301561",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "534e1426-c5a1-4283-b7a2-46f9f39f4d6d",
        "parentId" : null,
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "nit: rename to `pushDownAggregate` to keep it consistent with `pushDownFilters`?",
        "createdAt" : "2021-07-26T21:46:24Z",
        "updatedAt" : "2021-07-26T21:52:10Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      }
    ],
    "commit" : "fae570a68cde1cc25ae60e7ba74651e7972578e3",
    "line" : 64,
    "diffHunk" : "@@ -1,1 +69,73 @@  }\n\n  def pushdownAggregate(plan: LogicalPlan): LogicalPlan = plan.transform {\n    // update the scan builder with agg pushdown and return a new plan with agg pushed\n    case aggNode @ Aggregate(groupingExpressions, resultExpressions, child) =>"
  },
  {
    "id" : "66e00c93-cb0b-4dcc-be72-eaa71bfd6f70",
    "prId" : 32049,
    "prUrl" : "https://github.com/apache/spark/pull/32049#pullrequestreview-703253675",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f00f8566-a3b5-4544-8258-6b66f4c06e37",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "one thing we should take care of is the changing of expr ID. For example,\r\n```\r\nSort(a#1, sum_b#3)\r\n  Aggregate(groupBy=a#1, result=a#1, sum(b#2) as sum_b#3)\r\n    V2Relation(a#1, b#2)\r\n```\r\nAfter pushdown\r\n```\r\n!Sort(a#1, sum_b#3)\r\n  Aggregate(groupBy=a#4, result=a#4, sum(sum(b)#5) as sum_b#3)\r\n    V2Scan(a#4, sum(b)#5)\r\n```\r\n\r\nThe sort operator becomes invalid because `a#1` is missing now. We should find a way to keep the original expr ID, via `Alias(...)(exprId = originalId)`\r\n```\r\nSort(a#1, sum_b#3)\r\n  Aggregate(groupBy=a#4, result=a#4 as a#1, sum(sum(b)#5) as sum_b#3)\r\n    V2Scan(a#4, sum(b)#5)\r\n```",
        "createdAt" : "2021-07-09T17:20:50Z",
        "updatedAt" : "2021-07-09T17:23:10Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "5c2b630a61d8ea9263116c37e154884a5cabd2ca",
    "line" : 142,
    "diffHunk" : "@@ -1,1 +147,151 @@                  var i = 0\n                  val aggOutput = output.drop(groupAttrs.length)\n                  plan.transformExpressions {\n                    case agg: AggregateExpression =>\n                      i += 1"
  },
  {
    "id" : "1ed089a8-7ffd-4e02-b90b-d4550b406de7",
    "prId" : 29695,
    "prUrl" : "https://github.com/apache/spark/pull/29695#pullrequestreview-493980686",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "274490bb-1931-4fa8-885d-4bc3bb2873fd",
        "parentId" : null,
        "authorId" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "body" : "update optimized logical plan \r\nbefore update\r\n```\r\nAggregate [max(id#18) AS max(id)#21, min(id#18) AS min(id)#22]\r\n+- RelationV2[ID#18] test.people\r\n```\r\nafter update\r\n```\r\nAggregate [max(max(ID)#77) AS max(ID)#72, min(min(ID)#78) AS min(ID)#73]\r\n+- RelationV2[max(ID)#77, min(ID)#78] test.people\r\n```",
        "createdAt" : "2020-09-23T02:11:01Z",
        "updatedAt" : "2021-03-27T18:58:02Z",
        "lastEditedBy" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "tags" : [
        ]
      }
    ],
    "commit" : "782a0a827fbb1e60b8673d533cbda618df65dc96",
    "line" : 133,
    "diffHunk" : "@@ -1,1 +132,136 @@                  }\n                  agg.copy(aggregateFunction = aggFunction, filter = None)\n              }\n            }\n          }"
  },
  {
    "id" : "39f9a7bd-dfb5-4db1-ac12-95afd65693a9",
    "prId" : 29695,
    "prUrl" : "https://github.com/apache/spark/pull/29695#pullrequestreview-627388344",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3f754197-a6eb-4374-9854-11d6a74cd851",
        "parentId" : null,
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "pattern matching?",
        "createdAt" : "2021-04-02T22:51:55Z",
        "updatedAt" : "2021-04-03T00:08:31Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      }
    ],
    "commit" : "782a0a827fbb1e60b8673d533cbda618df65dc96",
    "line" : 118,
    "diffHunk" : "@@ -1,1 +117,121 @@                  i += 1\n                  val aggFunction: aggregate.AggregateFunction = {\n                    if (agg.aggregateFunction.isInstanceOf[aggregate.Max]) {\n                      aggregate.Max(aggOutput(i - 1))\n                    } else if (agg.aggregateFunction.isInstanceOf[aggregate.Min]) {"
  },
  {
    "id" : "651dbd73-14f1-47af-a9f4-3dd5d819823f",
    "prId" : 29695,
    "prUrl" : "https://github.com/apache/spark/pull/29695#pullrequestreview-627388344",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "32a3777c-3db0-411a-b7f8-b968d87a5d95",
        "parentId" : null,
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "perhaps we should return the original plan node rather than a new `Aggregate`?",
        "createdAt" : "2021-04-02T23:44:01Z",
        "updatedAt" : "2021-04-03T00:08:31Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      }
    ],
    "commit" : "782a0a827fbb1e60b8673d533cbda618df65dc96",
    "line" : 70,
    "diffHunk" : "@@ -1,1 +69,73 @@          val (pushedFilters, postScanFilters) = pushDownFilter(scanBuilder, newFilters, relation)\n          if (postScanFilters.nonEmpty) {\n            Aggregate(groupingExpressions, resultExpressions, child)\n          } else { // only push down aggregate if all the filers can be push down\n            val aggregation = PushDownUtils.pushAggregates(scanBuilder, aggregates,"
  },
  {
    "id" : "4f799e81-d125-4c7d-accd-db949ad1d270",
    "prId" : 29695,
    "prUrl" : "https://github.com/apache/spark/pull/29695#pullrequestreview-627388344",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "190ef2cd-0441-4351-908b-63237ab691f9",
        "parentId" : null,
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "ditto",
        "createdAt" : "2021-04-02T23:44:18Z",
        "updatedAt" : "2021-04-03T00:08:31Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      }
    ],
    "commit" : "782a0a827fbb1e60b8673d533cbda618df65dc96",
    "line" : 96,
    "diffHunk" : "@@ -1,1 +95,99 @@\n            if (aggregation.aggregateExpressions.isEmpty) {\n              Aggregate(groupingExpressions, resultExpressions, child)\n            } else {\n              val aggOutputBuilder = ArrayBuilder.make[AttributeReference]"
  },
  {
    "id" : "2090b93d-d23f-4d02-8306-561083c964f6",
    "prId" : 29695,
    "prUrl" : "https://github.com/apache/spark/pull/29695#pullrequestreview-627388344",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3f38b41a-2b44-4032-a239-ca80f7c87a4a",
        "parentId" : null,
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "hmm is this correct?",
        "createdAt" : "2021-04-02T23:46:12Z",
        "updatedAt" : "2021-04-03T00:08:31Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      }
    ],
    "commit" : "782a0a827fbb1e60b8673d533cbda618df65dc96",
    "line" : 101,
    "diffHunk" : "@@ -1,1 +100,104 @@              for (i <- 0 until aggregates.length) {\n                aggOutputBuilder += AttributeReference(\n                  aggregation.aggregateExpressions(i).toString, aggregates(i).dataType)()\n              }\n              groupingExpressions.foreach{"
  },
  {
    "id" : "468fd75d-3193-4bd7-b6c9-2ea9ec85efba",
    "prId" : 29695,
    "prUrl" : "https://github.com/apache/spark/pull/29695#pullrequestreview-627388344",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a5d81aa4-afed-4f95-9735-ca822d9b0b01",
        "parentId" : null,
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "This is a little hard to read. Maybe we can better separate the logic for pushing down aggregate with pushing down filters. Also some comments can help.",
        "createdAt" : "2021-04-03T00:08:06Z",
        "updatedAt" : "2021-04-03T00:08:32Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      }
    ],
    "commit" : "782a0a827fbb1e60b8673d533cbda618df65dc96",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +36,40 @@\n  override def apply(plan: LogicalPlan): LogicalPlan = plan transformDown {\n    case Aggregate(groupingExpressions, resultExpressions, child) =>\n      child match {\n        case ScanOperation(project, filters, relation: DataSourceV2Relation) =>"
  },
  {
    "id" : "d8a20594-cf1c-4a96-986d-037ce07e26c7",
    "prId" : 25955,
    "prUrl" : "https://github.com/apache/spark/pull/25955#pullrequestreview-297206140",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2ccb973a-a33f-4e86-b9c3-c7b92ca079b1",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "do we always need to create a `DataSourceV2ScanRelation` even if there is no filter/project above?",
        "createdAt" : "2019-10-02T03:35:17Z",
        "updatedAt" : "2019-10-30T18:21:04Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "2e292e9a-5610-43f2-b3b1-0b3551eb63ba",
        "parentId" : "2ccb973a-a33f-4e86-b9c3-c7b92ca079b1",
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "No, we don't need to. That would allow us to push down different filters later, after the \"dynamic\" predicate pushdown happens.",
        "createdAt" : "2019-10-02T15:38:59Z",
        "updatedAt" : "2019-10-30T18:21:04Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      },
      {
        "id" : "98c6da9a-f6b9-44f1-b299-0ee54823af65",
        "parentId" : "2ccb973a-a33f-4e86-b9c3-c7b92ca079b1",
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "I implemented this, but I didn't push it because it would cause test failures when `computeStats` is called on the `DataSourceV2Relation`.\r\n\r\nBut, that raises another problem with not converting to a scan relation. If stats are used in the optimizer, we have to convert to scan relation to avoid planning the query splits twice. Otherwise, a scan is built for `computeStats` and then another scan is built when converting to the physical plan. Both may have to plan splits. So I think I'll leave this as it is to catch as many cases as possible.",
        "createdAt" : "2019-10-03T23:40:11Z",
        "updatedAt" : "2019-10-30T18:21:04Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      }
    ],
    "commit" : "c223e0537796c1e0a52bc5a655660f89ab539192",
    "line" : 53,
    "diffHunk" : "@@ -1,1 +51,55 @@         \"\"\".stripMargin)\n\n      val scanRelation = DataSourceV2ScanRelation(relation.table, scan, output)\n\n      val filterCondition = postScanFilters.reduceLeftOption(And)"
  },
  {
    "id" : "9577e74b-133d-47e6-9a4f-c56d1c1c2673",
    "prId" : 25955,
    "prUrl" : "https://github.com/apache/spark/pull/25955#pullrequestreview-304682578",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8ca07a53-edae-4373-ba47-aeac75bb0bb2",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Now we only do pushdown in this rule. Do we still need `PushDownUtils`?",
        "createdAt" : "2019-10-21T11:56:23Z",
        "updatedAt" : "2019-10-30T18:21:04Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "c7fe8768-7d94-494e-b147-e37f86142c44",
        "parentId" : "8ca07a53-edae-4373-ba47-aeac75bb0bb2",
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "I'd rather have this separated into utils than keep the methods embedded in the rule.",
        "createdAt" : "2019-10-21T16:15:34Z",
        "updatedAt" : "2019-10-30T18:21:04Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      }
    ],
    "commit" : "c223e0537796c1e0a52bc5a655660f89ab539192",
    "line" : 26,
    "diffHunk" : "@@ -1,1 +24,28 @@import org.apache.spark.sql.execution.datasources.DataSourceStrategy\n\nobject V2ScanRelationPushDown extends Rule[LogicalPlan] {\n  import DataSourceV2Implicits._\n"
  }
]