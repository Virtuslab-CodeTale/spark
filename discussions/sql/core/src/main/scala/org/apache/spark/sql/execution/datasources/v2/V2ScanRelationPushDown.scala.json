[
  {
    "id" : "7905affb-0ee8-4bbe-80f1-04bf9a453421",
    "prId" : 33584,
    "prUrl" : "https://github.com/apache/spark/pull/33584#pullrequestreview-719523582",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cdc9ddba-109b-4f7a-8763-56945a3fe8b0",
        "parentId" : null,
        "authorId" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "body" : "Here we have subtracted the `partitionFilters` from the `postScanFilters`. If the file based data source only has partition filters, then the `postScanFilters` should be empty here, and we can push down aggregate for  file based data source.\r\n",
        "createdAt" : "2021-07-30T20:21:20Z",
        "updatedAt" : "2021-07-30T20:21:20Z",
        "lastEditedBy" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "tags" : [
        ]
      },
      {
        "id" : "06e07b57-d5b6-4419-87e7-39faec9fe580",
        "parentId" : "cdc9ddba-109b-4f7a-8763-56945a3fe8b0",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "I'm not sure why we can't do this in the old code, where `filters` contain both partition filters and data filters. Suppose we have a method `DataSourceUtils.getPartitionKeyFiltersAndDataFilters`, then we can basically do:\r\n\r\n```scala\r\nval (partitionFilters, dataFilters) = DataSourceUtils.getPartitionKeyFiltersAndDataFilters(...)\r\nif (dataFilters.isEmpty) {\r\n  // pushdown aggregates\r\n}\r\n```\r\n?",
        "createdAt" : "2021-07-30T23:19:18Z",
        "updatedAt" : "2021-07-30T23:19:19Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "34ea0bac-8ee2-4831-909e-d64f5a1d5b56",
        "parentId" : "cdc9ddba-109b-4f7a-8763-56945a3fe8b0",
        "authorId" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "body" : "Yes, we can do it that way, but since we have already called `DataSourceUtils.getPartitionKeyFiltersAndDataFilters` to separate the partition filters and data filters, we can just push these two filters to the scan builder, so we can completely remove the v2 partition pruning code from `PruneFileSourcePartitions`.",
        "createdAt" : "2021-07-31T00:08:45Z",
        "updatedAt" : "2021-07-31T00:08:45Z",
        "lastEditedBy" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "tags" : [
        ]
      }
    ],
    "commit" : "bbc85db716de1db24bdb67f2ca9075dcaa648ded",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +62,66 @@        .pushPartitionFilters(sHolder.builder, sHolder.relation, normalizedFiltersWithoutSubquery)\n      postScanFilters =\n        (ExpressionSet(postScanFilters) -- partitionFilters.filter(_.references.nonEmpty)).toSeq\n\n      logInfo("
  },
  {
    "id" : "fc8faa02-e040-4c42-a0d9-b2e8d221fddb",
    "prId" : 33584,
    "prUrl" : "https://github.com/apache/spark/pull/33584#pullrequestreview-719634360",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dfc91cf5-5180-4eba-b91a-9f1114b2b468",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Honestly I feel that it is somehow not good to move partial partition pruning from `PruneFileSourcePartitions` to this place mixed with `V2ScanRelationPushDown`.\r\n\r\n",
        "createdAt" : "2021-08-01T08:06:40Z",
        "updatedAt" : "2021-08-01T08:07:03Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "bbc85db716de1db24bdb67f2ca9075dcaa648ded",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +60,64 @@      var postScanFilters = postScanFiltersWithoutSubquery ++ normalizedFiltersWithSubquery\n      val partitionFilters = PushDownUtils\n        .pushPartitionFilters(sHolder.builder, sHolder.relation, normalizedFiltersWithoutSubquery)\n      postScanFilters =\n        (ExpressionSet(postScanFilters) -- partitionFilters.filter(_.references.nonEmpty)).toSeq"
  },
  {
    "id" : "dccb051b-2f3d-4df5-981a-c7afa31c5fd8",
    "prId" : 33352,
    "prUrl" : "https://github.com/apache/spark/pull/33352#pullrequestreview-715301561",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "534e1426-c5a1-4283-b7a2-46f9f39f4d6d",
        "parentId" : null,
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "nit: rename to `pushDownAggregate` to keep it consistent with `pushDownFilters`?",
        "createdAt" : "2021-07-26T21:46:24Z",
        "updatedAt" : "2021-07-26T21:52:10Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      }
    ],
    "commit" : "fae570a68cde1cc25ae60e7ba74651e7972578e3",
    "line" : 64,
    "diffHunk" : "@@ -1,1 +69,73 @@  }\n\n  def pushdownAggregate(plan: LogicalPlan): LogicalPlan = plan.transform {\n    // update the scan builder with agg pushdown and return a new plan with agg pushed\n    case aggNode @ Aggregate(groupingExpressions, resultExpressions, child) =>"
  },
  {
    "id" : "66e00c93-cb0b-4dcc-be72-eaa71bfd6f70",
    "prId" : 32049,
    "prUrl" : "https://github.com/apache/spark/pull/32049#pullrequestreview-703253675",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f00f8566-a3b5-4544-8258-6b66f4c06e37",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "one thing we should take care of is the changing of expr ID. For example,\r\n```\r\nSort(a#1, sum_b#3)\r\n  Aggregate(groupBy=a#1, result=a#1, sum(b#2) as sum_b#3)\r\n    V2Relation(a#1, b#2)\r\n```\r\nAfter pushdown\r\n```\r\n!Sort(a#1, sum_b#3)\r\n  Aggregate(groupBy=a#4, result=a#4, sum(sum(b)#5) as sum_b#3)\r\n    V2Scan(a#4, sum(b)#5)\r\n```\r\n\r\nThe sort operator becomes invalid because `a#1` is missing now. We should find a way to keep the original expr ID, via `Alias(...)(exprId = originalId)`\r\n```\r\nSort(a#1, sum_b#3)\r\n  Aggregate(groupBy=a#4, result=a#4 as a#1, sum(sum(b)#5) as sum_b#3)\r\n    V2Scan(a#4, sum(b)#5)\r\n```",
        "createdAt" : "2021-07-09T17:20:50Z",
        "updatedAt" : "2021-07-09T17:23:10Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "5c2b630a61d8ea9263116c37e154884a5cabd2ca",
    "line" : 142,
    "diffHunk" : "@@ -1,1 +147,151 @@                  var i = 0\n                  val aggOutput = output.drop(groupAttrs.length)\n                  plan.transformExpressions {\n                    case agg: AggregateExpression =>\n                      i += 1"
  }
]