[
  {
    "id" : "52ccc94a-9c98-4c86-90c7-9c62d15680e4",
    "prId" : 29695,
    "prUrl" : "https://github.com/apache/spark/pull/29695#pullrequestreview-627388344",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "273206a6-f663-48ff-a3a1-5f004f676c0a",
        "parentId" : null,
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "it's a bit strange that this is a DSv1 API but is only used by DSv2 JDBC scan? is it possible that a V1 data source implements this and goes through the V1 code path (i.e., through `DataSourceStrategy`)?",
        "createdAt" : "2021-04-03T00:06:20Z",
        "updatedAt" : "2021-04-03T00:08:32Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      }
    ],
    "commit" : "782a0a827fbb1e60b8673d533cbda618df65dc96",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +277,281 @@ * @since 3.1.0\n */\ntrait PrunedFilteredAggregateScan {\n  def buildScan(\n      requiredColumns: Array[String],"
  },
  {
    "id" : "951f26e6-1884-4e09-b628-2cbba5b20e07",
    "prId" : 28159,
    "prUrl" : "https://github.com/apache/spark/pull/28159#pullrequestreview-395555292",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6d7730fa-e392-4d2b-9fc1-3bfbe83262fe",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I think this works for `TableProvider`, which is used in `DataFrameReader/Writer`. But `TableCatalog` is a different API and it uses a different way to plug into Spark, so this won't work.\r\n\r\ncc @rdblue @brkyvz do you think this is valuable?",
        "createdAt" : "2020-04-17T09:09:42Z",
        "updatedAt" : "2020-04-17T09:09:42Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "9afe389c-8f30-4f8c-bc30-8beccb30bf85",
        "parentId" : "6d7730fa-e392-4d2b-9fc1-3bfbe83262fe",
        "authorId" : "571cff28-2336-450e-b24f-41cdb19b7f97",
        "body" : "This is meant strictly as an improvement to the DataSourceRegister API,\r\nwhich also isn't used by the catalog.",
        "createdAt" : "2020-04-17T15:29:41Z",
        "updatedAt" : "2020-04-17T15:29:42Z",
        "lastEditedBy" : "571cff28-2336-450e-b24f-41cdb19b7f97",
        "tags" : [
        ]
      }
    ],
    "commit" : "ff6b14b5206347d4b2e8a372993440efef95030e",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +60,64 @@ *\n */\ntrait DataSourceRegisterV2 {\n\n  /**"
  },
  {
    "id" : "77d9285c-c96c-4f04-8d55-4039a5931ae4",
    "prId" : 27911,
    "prUrl" : "https://github.com/apache/spark/pull/27911#pullrequestreview-377698488",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2f7a01a9-345b-4db7-8dff-127c2edabb19",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "The source isn't necessarily related to files. It's a bit odd to assume the number of files here. Also, seems like this is what DSv2 should cover?",
        "createdAt" : "2020-03-19T07:06:42Z",
        "updatedAt" : "2020-03-19T07:06:42Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "1ec04178-d5e8-4ffe-9063-b04a90a72c2d",
        "parentId" : "2f7a01a9-345b-4db7-8dff-127c2edabb19",
        "authorId" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "body" : "Does DSv2 have the similar interface? Current Delta Lake is using this command and besides this metrics, we have implemented ensure distribution for InsertIntoDataSource to support bucketing delta table. Does DSv1 accepte no more improvements? It has to move to DSv2?",
        "createdAt" : "2020-03-19T13:05:48Z",
        "updatedAt" : "2020-03-19T13:05:48Z",
        "lastEditedBy" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "tags" : [
        ]
      }
    ],
    "commit" : "bb6738fe9fc5f80836e64d6c44cd1508882ab20a",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +296,300 @@  def insert(data: DataFrame, overwrite: Boolean): Unit = {}\n\n  def insert(data: DataFrame, overwrite: Boolean, metrics: Map[String, SQLMetric]): Unit = {\n    insert(data, overwrite)\n  }"
  },
  {
    "id" : "4b8198a1-0f17-4128-b26a-8a556b007488",
    "prId" : 25558,
    "prUrl" : "https://github.com/apache/spark/pull/25558#pullrequestreview-278462571",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9e1a6e0f-8a02-47ba-8b0c-15a647c07b91",
        "parentId" : null,
        "authorId" : "24e1dd39-ae3f-4bbb-a391-f60afb62d075",
        "body" : "does it make sense to have it Unstable? It  has not changed for over 2 years...",
        "createdAt" : "2019-08-22T14:25:29Z",
        "updatedAt" : "2019-08-27T14:53:45Z",
        "lastEditedBy" : "24e1dd39-ae3f-4bbb-a391-f60afb62d075",
        "tags" : [
        ]
      },
      {
        "id" : "2d5c57c0-82d0-4b42-9635-b8626b6c7023",
        "parentId" : "9e1a6e0f-8a02-47ba-8b0c-15a647c07b91",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Agree, I wasn't sure about removing Unstable, but seems like some that clearly can go. Maybe anything not changed in a long time as you say.",
        "createdAt" : "2019-08-22T14:31:28Z",
        "updatedAt" : "2019-08-27T14:53:45Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      }
    ],
    "commit" : "6e9bb612c3c3d863f19d4019abe31f855783e44d",
    "line" : 33,
    "diffHunk" : "@@ -1,1 +306,310 @@ * @since 1.3.0\n */\n@Unstable\ntrait CatalystScan {\n  def buildScan(requiredColumns: Seq[Attribute], filters: Seq[Expression]): RDD[Row]"
  },
  {
    "id" : "a499600c-2319-4ad3-8df7-66c623a7d9e8",
    "prId" : 25291,
    "prUrl" : "https://github.com/apache/spark/pull/25291#pullrequestreview-278081029",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "22e69de4-d2b5-44c3-9d85-702fe5304639",
        "parentId" : null,
        "authorId" : "1aba4faa-ea5e-4487-baf9-d552ca566126",
        "body" : "consistent with upper case for wording `V1` and `V2`?",
        "createdAt" : "2019-08-21T21:20:02Z",
        "updatedAt" : "2019-08-21T21:27:25Z",
        "lastEditedBy" : "1aba4faa-ea5e-4487-baf9-d552ca566126",
        "tags" : [
        ]
      }
    ],
    "commit" : "8072fc5e4521fd921a5f7fdc7971f289b3439f09",
    "line" : 33,
    "diffHunk" : "@@ -1,1 +329,333 @@/**\n * A special Data Source V2 `Table`, which doesn't need to implement the read/write capabilities.\n * Spark will fallback the read/write requests to the v1 relation.\n *\n * @since 3.0.0"
  }
]