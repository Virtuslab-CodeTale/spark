[
  {
    "id" : "7ee7677c-8501-48d3-b8ad-1ed092de7156",
    "prId" : 31258,
    "prUrl" : "https://github.com/apache/spark/pull/31258#pullrequestreview-583104767",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9e32dce3-fc26-495c-91cd-eeea45886d46",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Let's add parameter doc\r\n```\r\n/**\r\n * ...\r\n * @param id              the query stage id.\r\n * @param plan            the underlying plan.\r\n * @param _canonicalized  the canonicalized plan before applying query stage optimizer rules.\r\n */\r\n```",
        "createdAt" : "2021-02-04T07:38:41Z",
        "updatedAt" : "2021-02-08T13:36:57Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "1e1b097c38ba468b751260b00262c21781892047",
    "line" : 29,
    "diffHunk" : "@@ -1,1 +156,160 @@ * @param plan the underlying plan.\n * @param _canonicalized the canonicalized plan before applying query stage optimizer rules.\n */\ncase class ShuffleQueryStageExec(\n    override val id: Int,"
  },
  {
    "id" : "3d2f1e8a-9f3e-4b80-b5fa-83a1b4268858",
    "prId" : 31258,
    "prUrl" : "https://github.com/apache/spark/pull/31258#pullrequestreview-583104866",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cfb4c413-be52-4a9e-b22e-e249f51b491f",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "ditto",
        "createdAt" : "2021-02-04T07:38:52Z",
        "updatedAt" : "2021-02-08T13:36:57Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "1e1b097c38ba468b751260b00262c21781892047",
    "line" : 57,
    "diffHunk" : "@@ -1,1 +211,215 @@ * @param _canonicalized the canonicalized plan before applying query stage optimizer rules.\n */\ncase class BroadcastQueryStageExec(\n    override val id: Int,\n    override val plan: SparkPlan,"
  },
  {
    "id" : "748e3c14-fa3c-47a0-8220-e717ea8cef9e",
    "prId" : 28250,
    "prUrl" : "https://github.com/apache/spark/pull/28250#pullrequestreview-396516799",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "82225a54-0934-408a-a79c-ebb4924dfcc9",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Cancel the job group as `BroadcastExchangeExec` does?",
        "createdAt" : "2020-04-20T06:12:01Z",
        "updatedAt" : "2020-04-20T14:32:03Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "8549961f-6983-420d-926f-322cf52e871e",
        "parentId" : "82225a54-0934-408a-a79c-ebb4924dfcc9",
        "authorId" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "body" : "This is done in the AQE mechanism already: after the timeout happens, this will become a `StageFailure` event in the AQE event queue, which will trigger a cleanup that calls the `cancel()` routine of each running query stage (including the broadcast stage that has timed out). And a broadcast stage's `cancel()` stops the broadcast thread as well as the job group.",
        "createdAt" : "2020-04-20T14:36:54Z",
        "updatedAt" : "2020-04-20T14:36:54Z",
        "lastEditedBy" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "tags" : [
        ]
      },
      {
        "id" : "fa5ed11a-7b4a-4ee7-922d-4676972523fc",
        "parentId" : "82225a54-0934-408a-a79c-ebb4924dfcc9",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "I see, thanks for your explanation :)",
        "createdAt" : "2020-04-20T14:38:36Z",
        "updatedAt" : "2020-04-20T14:38:36Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "65ee658eb104776156ef670169f2dd2705f2cfbb",
    "line" : 46,
    "diffHunk" : "@@ -1,1 +200,204 @@        promise.tryFailure(new SparkException(s\"Could not execute broadcast in $timeout secs. \" +\n          s\"You can increase the timeout for broadcasts via ${SQLConf.BROADCAST_TIMEOUT.key} or \" +\n          s\"disable broadcast join by setting ${SQLConf.AUTO_BROADCASTJOIN_THRESHOLD.key} to -1\"))\n      }\n    }, timeout, TimeUnit.SECONDS)"
  },
  {
    "id" : "4db1f57b-bb25-4b04-99c2-8e21413936bc",
    "prId" : 26726,
    "prUrl" : "https://github.com/apache/spark/pull/26726#pullrequestreview-325996556",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7df4014d-4599-420d-9b96-ff5530f73821",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "This is a bug of the `dataSize` metrics, not AQE, let's fix the underlying issue.\r\n\r\nCan you take over https://github.com/apache/spark/pull/25095 ?",
        "createdAt" : "2019-12-03T08:41:08Z",
        "updatedAt" : "2019-12-03T08:41:09Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "efc20d1b-5f4d-4571-b917-af075afdf714",
        "parentId" : "7df4014d-4599-420d-9b96-ff5530f73821",
        "authorId" : "31f9c2ba-7775-44e4-b6fa-5e9ab26c8dff",
        "body" : "OK, I'll take over it",
        "createdAt" : "2019-12-03T09:30:09Z",
        "updatedAt" : "2019-12-03T09:30:09Z",
        "lastEditedBy" : "31f9c2ba-7775-44e4-b6fa-5e9ab26c8dff",
        "tags" : [
        ]
      }
    ],
    "commit" : "76cb6904ded6406ddf76c5ce25b0028babce0a9f",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +82,86 @@    // The `dataSize` metric may be a negative number when this spark plan generates empty result,\n    // since SQLMetric use -1 as initial value.\n    Statistics(sizeInBytes = Math.max(plan.metrics(\"dataSize\").value, 0))\n  }\n"
  },
  {
    "id" : "d1890195-633a-4367-b27c-4806a5dab989",
    "prId" : 26400,
    "prUrl" : "https://github.com/apache/spark/pull/26400#pullrequestreview-311956519",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "194b8300-fe65-4ce5-ad58-759d711e383b",
        "parentId" : null,
        "authorId" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "body" : "nit: Is there a better place to put this method in, as well as `updateAttr`, like in `Attribute` or sth., so that they can be shared between `ReusedExchangeExec` and `ReusedQueryStageExec`?",
        "createdAt" : "2019-11-05T18:43:41Z",
        "updatedAt" : "2019-11-06T17:25:30Z",
        "lastEditedBy" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "tags" : [
        ]
      }
    ],
    "commit" : "2b3f1e0433a63c448b02a9abce42c002dd37785e",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +218,222 @@  }\n\n  private[sql] def updatePartitioning(p: Partitioning): Partitioning = p match {\n    case e: Expression => updateAttr(e).asInstanceOf[Partitioning]\n    case other => other"
  },
  {
    "id" : "268e1b87-fc89-464c-8d33-913d26af6921",
    "prId" : 24706,
    "prUrl" : "https://github.com/apache/spark/pull/24706#pullrequestreview-243941595",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0d7c55dc-779f-4e3b-8d79-7ce70f5a0ba3",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "we should mention that both shuffle and broadcast have this metrics",
        "createdAt" : "2019-05-30T18:02:02Z",
        "updatedAt" : "2019-06-15T01:12:56Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "e2651049c408f211b225f40c93adf5b741b14eb4",
    "line" : 82,
    "diffHunk" : "@@ -1,1 +80,84 @@  def computeStats(): Option[Statistics] = resultOption.map { _ =>\n    // Metrics `dataSize` are available in both `ShuffleExchangeExec` and `BroadcastExchangeExec`.\n    Statistics(sizeInBytes = plan.metrics(\"dataSize\").value)\n  }\n"
  },
  {
    "id" : "175d700b-137b-4341-a7e2-c9476e61e05a",
    "prId" : 24706,
    "prUrl" : "https://github.com/apache/spark/pull/24706#pullrequestreview-249991172",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8b5b16cf-83af-450c-a739-97fcff93a62d",
        "parentId" : null,
        "authorId" : "80d631a6-73e8-46a6-a01b-a80d1f1cc6cc",
        "body" : "This forces materialization right? It would be better to if we can check whether it is already running.",
        "createdAt" : "2019-06-12T21:13:57Z",
        "updatedAt" : "2019-06-15T01:12:56Z",
        "lastEditedBy" : "80d631a6-73e8-46a6-a01b-a80d1f1cc6cc",
        "tags" : [
        ]
      },
      {
        "id" : "f32e3216-9d47-4381-98d6-6ee3bca6a64a",
        "parentId" : "8b5b16cf-83af-450c-a739-97fcff93a62d",
        "authorId" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "body" : "At this point, all the existing `QueryStageExec` nodes in the plan has been called \"materialize\" already, so it should not be a concern.",
        "createdAt" : "2019-06-14T16:06:19Z",
        "updatedAt" : "2019-06-15T01:12:56Z",
        "lastEditedBy" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "tags" : [
        ]
      }
    ],
    "commit" : "e2651049c408f211b225f40c93adf5b741b14eb4",
    "line" : 137,
    "diffHunk" : "@@ -1,1 +135,139 @@\n  override def cancel(): Unit = {\n    mapOutputStatisticsFuture match {\n      case action: FutureAction[MapOutputStatistics] if !mapOutputStatisticsFuture.isCompleted =>\n        action.cancel()"
  },
  {
    "id" : "1dde74c5-918a-4afd-ade4-0843563a0369",
    "prId" : 24706,
    "prUrl" : "https://github.com/apache/spark/pull/24706#pullrequestreview-249016694",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3ac2defe-e613-4186-9480-3eb0c2c31b8c",
        "parentId" : null,
        "authorId" : "80d631a6-73e8-46a6-a01b-a80d1f1cc6cc",
        "body" : "This also forces materialization right?",
        "createdAt" : "2019-06-12T21:15:05Z",
        "updatedAt" : "2019-06-15T01:12:56Z",
        "lastEditedBy" : "80d631a6-73e8-46a6-a01b-a80d1f1cc6cc",
        "tags" : [
        ]
      }
    ],
    "commit" : "e2651049c408f211b225f40c93adf5b741b14eb4",
    "line" : 157,
    "diffHunk" : "@@ -1,1 +155,159 @@\n  override def cancel(): Unit = {\n    if (!plan.relationFuture.isDone) {\n      sparkContext.cancelJobGroup(plan.runId.toString)\n      plan.relationFuture.cancel(true)"
  }
]