[
  {
    "id" : "0ec6ce12-7a65-4c6b-bbaa-8ab5bf602eba",
    "prId" : 32828,
    "prUrl" : "https://github.com/apache/spark/pull/32828#pullrequestreview-679220046",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d3268ed2-e378-49aa-9169-e994daba5768",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "The reason I suggested to log a warning message here is to help debugging if the problem arises.\r\n\r\nSuppose we see the log message and would like to understand what's happening. Does it give enough information, with other logs? Can we determine the current key from the log context?",
        "createdAt" : "2021-06-09T01:44:16Z",
        "updatedAt" : "2021-06-09T01:44:16Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "725cac61-00ad-46db-8978-3da3e7217fa3",
        "parentId" : "d3268ed2-e378-49aa-9169-e994daba5768",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Hm, as currentKey is an UnsafeRow, I guess it is hard to get understanding from its string output?",
        "createdAt" : "2021-06-09T02:03:04Z",
        "updatedAt" : "2021-06-09T02:03:04Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "81099070-f460-4d09-8e43-3c27d086011c",
        "parentId" : "d3268ed2-e378-49aa-9169-e994daba5768",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "I can log the currentKey if you think it is important to have.",
        "createdAt" : "2021-06-09T02:25:58Z",
        "updatedAt" : "2021-06-09T02:25:58Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "0eae3629-e61e-45f8-b5ec-e77aed838f14",
        "parentId" : "d3268ed2-e378-49aa-9169-e994daba5768",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Sigh that's the problem... We won't be able to reproduce a key row from string output.\r\n\r\nProbably we need to think more on this. I don't like to swallow the problem we can catch in prior, but I don't also like the way we log cryptic message to end user, and when they come up with the log message we say \"Sorry we don't have enough information from the log.\". Cryptic log message might be acceptable, but at least we should be able to provide the way how to investigate further.\r\n\r\nThe next possible way may be... having state store reader and scan all state store to find null in value? If we are OK with this one, then it might still make sense to leave this log message.\r\n\r\nLess cryptic, or guide to report to Spark community might be better though. Do we have some way to log an internal error so that community can report it?",
        "createdAt" : "2021-06-09T03:20:48Z",
        "updatedAt" : "2021-06-09T03:20:49Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "5bb5861a-bfe2-41e2-b525-8d8805b1eaa0",
        "parentId" : "d3268ed2-e378-49aa-9169-e994daba5768",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Less cryptic approach might be to project unsafe row to internal row. Although it needs extra projection, but I think this is less likely case to happen. So maybe we don't need to worry too much about performance.\r\n\r\nWDYT?",
        "createdAt" : "2021-06-09T03:55:39Z",
        "updatedAt" : "2021-06-09T03:55:39Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "0eec39ed-7590-47d4-81da-b620c3ea8dcb",
        "parentId" : "d3268ed2-e378-49aa-9169-e994daba5768",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Oh that's a brilliant idea. Just to make sure we don't miss anything, how it will be printed out? Probably testing with simple but composite row (like row having String, Int, Timestamp, etc.) would be good.",
        "createdAt" : "2021-06-09T05:12:18Z",
        "updatedAt" : "2021-06-09T05:12:18Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "0f7af67f-7861-4105-82e8-19b9a4d38b82",
        "parentId" : "d3268ed2-e378-49aa-9169-e994daba5768",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Okay, let me do it and add some tests.",
        "createdAt" : "2021-06-09T05:25:30Z",
        "updatedAt" : "2021-06-09T05:25:30Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "14732640b63cad3ab8f50d052c514b98d27f11cc",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +277,281 @@          } else {\n            val projectedKey = getInternalRowOfKeyWithIndex(currentKey)\n            logWarning(s\"`keyWithIndexToValue` returns a null value for index ${numValues - 1} \" +\n              s\"at current key $projectedKey.\")\n          }"
  },
  {
    "id" : "59d4da97-83cf-4c53-91a9-8b9965e82cc0",
    "prId" : 32796,
    "prUrl" : "https://github.com/apache/spark/pull/32796#pullrequestreview-678741544",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dad9c2cd-b0c9-42fa-8521-4f50c994e4bb",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Just wondering, would `valuePairAtMaxIndex == null` happen in normal case? I can only imagine the case the iterator is not fully consumed and somehow numValues is not updated.",
        "createdAt" : "2021-06-08T07:29:37Z",
        "updatedAt" : "2021-06-08T07:29:37Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "60a3ec2e-2ffd-4eb4-a922-c242750594ca",
        "parentId" : "dad9c2cd-b0c9-42fa-8521-4f50c994e4bb",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "I guess no, for normal case. Seems better to keep it.",
        "createdAt" : "2021-06-08T07:49:57Z",
        "updatedAt" : "2021-06-08T07:49:57Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "8e8809c1-b272-4f1a-ab1a-69ea12431f9b",
        "parentId" : "dad9c2cd-b0c9-42fa-8521-4f50c994e4bb",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Yeah my preference is to fail the query (like raising internal error) on the case of `valuePairAtMaxIndex == null`, so that we can indicate the bad case in prior instead of encountering bad case after state is corrupted.\r\n\r\nIf we would only have the case the value is null as error case, still makes sense to fail the query even for the state created before the fix, but for safety, we could log warning message here. I'm just afraid I'm missing some \"normal\" case where the null value is doable.",
        "createdAt" : "2021-06-08T08:15:42Z",
        "updatedAt" : "2021-06-08T08:15:42Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "23c34727-4f09-4f9e-8326-eae3a51f9347",
        "parentId" : "dad9c2cd-b0c9-42fa-8521-4f50c994e4bb",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "ah, missing latest comment. Let me add a log warning message here in a follow-up.",
        "createdAt" : "2021-06-08T16:27:36Z",
        "updatedAt" : "2021-06-08T16:27:37Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "a0a8d8e808b1eff35e9cc3bc31a532ddb2aeae2d",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +272,276 @@        if (index != numValues - 1) {\n          val valuePairAtMaxIndex = keyWithIndexToValue.get(currentKey, numValues - 1)\n          if (valuePairAtMaxIndex != null) {\n            keyWithIndexToValue.put(currentKey, index, valuePairAtMaxIndex.value,\n              valuePairAtMaxIndex.matched)"
  },
  {
    "id" : "d937547d-009a-4ec9-9ac8-787d69cee145",
    "prId" : 30076,
    "prUrl" : "https://github.com/apache/spark/pull/30076#pullrequestreview-514125232",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "67855a6f-3147-46ca-879f-ebb49abab6a1",
        "parentId" : null,
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "It makes more sense to add this filter logic in the `predicate` param(i.e `postJoinFilter` for OneSideHashJoiner) for rightSideJoiner only, corresponding to the comment https://github.com/apache/spark/pull/30076/files#diff-6cd66da710d8d54025c1edf658bbec5230e8b4e748f9f2f884a60b1ba1efed42R264",
        "createdAt" : "2020-10-21T11:08:56Z",
        "updatedAt" : "2020-10-21T22:41:42Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      },
      {
        "id" : "828e6cfa-29c4-4ac7-94ee-05dd95c28f87",
        "parentId" : "67855a6f-3147-46ca-879f-ebb49abab6a1",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "I thought this first and not proposed because current predicate cannot check the condition. We can still do this via adjusting the type of predicate a bit, but I guess the followup PR would try to separate left semi case of performance which lets us to can revert the change here. For the reason I prefer the small change for now.",
        "createdAt" : "2020-10-21T12:07:28Z",
        "updatedAt" : "2020-10-21T22:41:42Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "87553739-776e-4875-9de2-8d1678c5c4dc",
        "parentId" : "67855a6f-3147-46ca-879f-ebb49abab6a1",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "Yes, after taking a further look, the `joinedRow` already dropped the message of `matched`, so it's hard to do now. +1 for the change now.",
        "createdAt" : "2020-10-21T13:21:25Z",
        "updatedAt" : "2020-10-21T22:41:42Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      },
      {
        "id" : "c0e5cdb6-a340-4226-9255-b5211f1e0201",
        "parentId" : "67855a6f-3147-46ca-879f-ebb49abab6a1",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "FYI I created https://issues.apache.org/jira/browse/SPARK-33211 for this followup.",
        "createdAt" : "2020-10-21T20:18:09Z",
        "updatedAt" : "2020-10-21T22:41:42Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      }
    ],
    "commit" : "14871d9d2be6b751687e78dd4d17c2e249b8f205",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +111,115 @@      excludeRowsAlreadyMatched: Boolean = false): Iterator[JoinedRow] = {\n    val numValues = keyToNumValues.get(key)\n    keyWithIndexToValue.getAll(key, numValues).filterNot { keyIdxToValue =>\n      excludeRowsAlreadyMatched && keyIdxToValue.matched\n    }.map { keyIdxToValue =>"
  },
  {
    "id" : "f4aca33a-71cc-46c6-b7a5-a83da60d2d51",
    "prId" : 26108,
    "prUrl" : "https://github.com/apache/spark/pull/26108#pullrequestreview-301138278",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ef63630e-df9c-45bd-a667-7ea997028152",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "This is unavoidable refactoring, as we need to deal with both schemas depending on the state format version.\r\nI didn't let inner join use state format version 1 (though it could be one of optimizations), as the inner join query could be changed to outer join which Spark will not be able to determine the change - correctness issue will occur then.",
        "createdAt" : "2019-10-14T07:32:48Z",
        "updatedAt" : "2019-11-01T02:28:28Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "7213d4266b7bfa2bd05225c68e2184a7abb24469",
    "line" : 276,
    "diffHunk" : "@@ -1,1 +450,454 @@  }\n\n  private trait KeyWithIndexToValueRowConverter {\n    def valueAttributes: Seq[Attribute]\n"
  },
  {
    "id" : "dbcdd85e-e81a-4aac-b05d-cd4e826a2a45",
    "prId" : 26108,
    "prUrl" : "https://github.com/apache/spark/pull/26108#pullrequestreview-301139538",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9d57d3e5-af55-446b-b45c-bbcafa3e0b55",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Here `matched` also has a default value for state format version 1 - the main version of state format version is 2, so focused to optimize for state format version 2. (`Option[Boolean] vs Boolean`) The value is not used in caller if the state format version is 1, so I think this is OK.",
        "createdAt" : "2019-10-14T07:36:22Z",
        "updatedAt" : "2019-11-01T02:28:28Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "7213d4266b7bfa2bd05225c68e2184a7abb24469",
    "line" : 243,
    "diffHunk" : "@@ -1,1 +419,423 @@    var valueIndex: Long = -1,\n    var value: UnsafeRow = null,\n    var matched: Boolean = false) {\n\n    def withNew("
  },
  {
    "id" : "19885b50-036a-4993-b3a1-0fa9664b8da0",
    "prId" : 26108,
    "prUrl" : "https://github.com/apache/spark/pull/26108#pullrequestreview-307426918",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c873dd76-fa36-4735-9a90-943eccb06cc5",
        "parentId" : null,
        "authorId" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "body" : "It would be good to just reference the same values as the config entry.\r\n\r\nIn fact, since the config is internal, you could probably just drop the check there, and rely on the checks in this code to make sure the version if correct.",
        "createdAt" : "2019-10-24T23:23:26Z",
        "updatedAt" : "2019-11-01T02:28:28Z",
        "lastEditedBy" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "tags" : [
        ]
      },
      {
        "id" : "b88aafda-95d3-4574-96e0-d8d6ba8299bc",
        "parentId" : "c873dd76-fa36-4735-9a90-943eccb06cc5",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Same here. Sounds nice, but may want to keep consistency. So I'm in favor of changing all, or leaving it as it is.",
        "createdAt" : "2019-10-25T11:09:00Z",
        "updatedAt" : "2019-11-01T02:28:28Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "01afb2f0-8ff1-45f1-b4cc-597a07b39358",
        "parentId" : "c873dd76-fa36-4735-9a90-943eccb06cc5",
        "authorId" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "body" : "Same comment as before. I'd like the better approach to be used. If you want to go later and fix the other spots in the code, that's up to you.",
        "createdAt" : "2019-10-25T20:08:25Z",
        "updatedAt" : "2019-11-01T02:28:28Z",
        "lastEditedBy" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "tags" : [
        ]
      }
    ],
    "commit" : "7213d4266b7bfa2bd05225c68e2184a7abb24469",
    "line" : 399,
    "diffHunk" : "@@ -1,1 +603,607 @@\nobject SymmetricHashJoinStateManager {\n  val supportedVersions = Seq(1, 2)\n  val legacyVersion = 1\n"
  },
  {
    "id" : "db237e60-8f4f-446d-8bee-3ab2df8eca45",
    "prId" : 26108,
    "prUrl" : "https://github.com/apache/spark/pull/26108#pullrequestreview-308110018",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bfe673f0-0cef-4d32-b21d-3cadc46bfb6c",
        "parentId" : null,
        "authorId" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "body" : "So, while replying to your comment, this came to my mind.\r\n\r\nThe way I see this code, when you start from a v1 state store, you'll also be writing back v1 data.\r\n\r\nShouldn't you write out new data, so that you fix the problem going forward once you start the app with the fixed version of Spark?",
        "createdAt" : "2019-10-25T20:19:16Z",
        "updatedAt" : "2019-11-01T02:28:28Z",
        "lastEditedBy" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "tags" : [
        ]
      },
      {
        "id" : "a9ec3489-4f95-4905-ba4c-f2a7a7baae79",
        "parentId" : "bfe673f0-0cef-4d32-b21d-3cadc46bfb6c",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Even we write v2 data here, we only update part of state instead of full rewrite of state. It will leave v1 / v2 data co-exist and make state store not possible to read the entire rows - individual row doesn't have schema information. That's why we separate versions and isolate them. Yes that's bad, but that's the current state.\r\n\r\nActually we don't even have schema information for state store as well so we would encounter undefined behavior if we do such unsafe thing. SPARK-27237 (#24173) proposes to introduce schema information per state (not per row) and fail the query if schema is not compatible.\r\n\r\nWe can still rewrite the state entirely in a batch query (cannot be online modification) - please refer the README of https://github.com/HeartSaVioR/spark-state-tools. I've already implemented the migration of v1 to v2 for streaming aggregation / flatMapGroupsWithState, and I may try to implement this migration as well. I'm also proposing the project as a part of Apache Spark, but not as it is - I'm proposing batch data source for \"state\" first (SPARK-28190), and once we adopt it, we could also consider to have some tools for helping migration. Flink recently added the state API which denotes the necessity of the data source for \"state\".",
        "createdAt" : "2019-10-26T02:31:21Z",
        "updatedAt" : "2019-11-01T02:28:28Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "f7e33ae1-cd4e-439e-b24e-379297de12ac",
        "parentId" : "bfe673f0-0cef-4d32-b21d-3cadc46bfb6c",
        "authorId" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "body" : "Ok, think I got it. Seems like even if we add some code that would write new data in the v2 format, the state store still may have data in v1 format that can trigger the correctness issue. So it's a small gain in the end, although that old state data would be constrained to state generated before the \"fixed\" Spark was used.\r\n\r\nGiven that I think that failing the query when the bad combination is detected is better (I see you already made that change).\r\n\r\nOne thing is that a \"rewrite\" of the state in this case, at least, is not a good option. The data is missing from the v1 data, so there's no way to generate correct v2 data from it. The only thing you'd achieve by doing that is to mask errors (since the exception you're adding wouldn't be triggered anymore).\r\n\r\nSo unless it's possible to have a \"mixed\" solution here, where the same store can have both v1 and v2 data, erroring out is the best outcome I can think of. I can think of ways to hack it (you could e.g. compare the column count for the `UnsafeRow` read from the store and see if it matches the count in the expected schema, or if it has the extra fields expected by the v2 data), but haven't thoroughly thought about it.",
        "createdAt" : "2019-10-28T18:57:15Z",
        "updatedAt" : "2019-11-01T02:28:28Z",
        "lastEditedBy" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "tags" : [
        ]
      },
      {
        "id" : "86074402-3dcd-444d-895a-7b62a29ee392",
        "parentId" : "bfe673f0-0cef-4d32-b21d-3cadc46bfb6c",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "> One thing is that a \"rewrite\" of the state in this case, at least, is not a good option. The data is missing from the v1 data, so there's no way to generate correct v2 data from it.\r\n\r\nYeah you're right. At least for this case rewrite doesn't work. I forgot what I've already found  months ago. Thanks for reminding.\r\n\r\n> I can think of ways to hack it (you could e.g. compare the column count for the UnsafeRow read from the store and see if it matches the count in the expected schema, or if it has the extra fields expected by the v2 data), but haven't thoroughly thought about it.\r\n\r\nI also had thought about it a bit (and that was maybe one of review comment in previous PR) but it can bring side-effect if the query is changed. If end users change the query to let input of join containing one less column, there's a chance Spark may read \"v1 row\" as \"v2 row\" incorrectly whereas Spark should just fail the query since schema has been changed. Relying on column count is unsafe.\r\n",
        "createdAt" : "2019-10-28T19:20:10Z",
        "updatedAt" : "2019-11-01T02:28:28Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "73f25f20-5d62-43cc-8005-fef5e0cbe299",
        "parentId" : "bfe673f0-0cef-4d32-b21d-3cadc46bfb6c",
        "authorId" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "body" : "You could take that approach if you also had schema validation, as in your other PR. The main advantage is that it would simplify some of the versioning-related changes in this PR (i.e. you would only have to deal with the read path).\r\n\r\nAnyway, it's more important to fix the correctness issue, so that can be done later.",
        "createdAt" : "2019-10-28T19:57:47Z",
        "updatedAt" : "2019-11-01T02:28:28Z",
        "lastEditedBy" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "tags" : [
        ]
      }
    ],
    "commit" : "7213d4266b7bfa2bd05225c68e2184a7abb24469",
    "line" : 379,
    "diffHunk" : "@@ -1,1 +562,566 @@    def put(key: UnsafeRow, valueIndex: Long, value: UnsafeRow, matched: Boolean): Unit = {\n      val keyWithIndex = keyWithIndexRow(key, valueIndex)\n      val valueWithMatched = valueRowConverter.convertToValueRow(value, matched)\n      stateStore.put(keyWithIndex, valueWithMatched)\n    }"
  },
  {
    "id" : "93e5229e-469d-4eb5-9eaa-19cad440ab8a",
    "prId" : 26108,
    "prUrl" : "https://github.com/apache/spark/pull/26108#pullrequestreview-311429178",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c0f6fda4-ce90-47fa-a275-63c94561bfec",
        "parentId" : null,
        "authorId" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "body" : "Can `valuePair` be `null` here? Just like before, `keyWithIndexToValue.get` can return null, but I'm not sure whether in this particular spot it would return null.",
        "createdAt" : "2019-11-04T23:22:52Z",
        "updatedAt" : "2019-11-04T23:25:17Z",
        "lastEditedBy" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "tags" : [
        ]
      },
      {
        "id" : "a487516c-73be-4b24-a70b-3e78fbd1d137",
        "parentId" : "c0f6fda4-ce90-47fa-a275-63c94561bfec",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "I think it would only return null when there's corruption on the state store - `hasMoreValuesForCurrentKey == true` denotes the index is valid (not an out of index), then the return value is not expected to be null.",
        "createdAt" : "2019-11-04T23:42:40Z",
        "updatedAt" : "2019-11-04T23:42:41Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "7213d4266b7bfa2bd05225c68e2184a7abb24469",
    "line" : 174,
    "diffHunk" : "@@ -1,1 +225,229 @@          if (hasMoreValuesForCurrentKey) {\n            // First search the values for the current key.\n            val valuePair = keyWithIndexToValue.get(currentKey, index)\n            if (removalCondition(valuePair.value)) {\n              return valuePair"
  }
]