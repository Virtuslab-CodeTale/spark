[
  {
    "id" : "3edfcc5f-c4ea-4f8b-8ea8-7d2cd3d591da",
    "prId" : 31804,
    "prUrl" : "https://github.com/apache/spark/pull/31804#pullrequestreview-609400754",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "def2e6d2-9bdf-4906-8dfa-42e66d87cce1",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Well, this seems inconsistent with the doc. The current document means `spark.sql.legacy.keepCommandOutputSchema` means the 3.1 or earlier schema, doesn't it?",
        "createdAt" : "2021-03-11T04:54:58Z",
        "updatedAt" : "2021-03-30T07:59:14Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "f2effd52-42c9-4558-91f4-996e3f34bf80",
        "parentId" : "def2e6d2-9bdf-4906-8dfa-42e66d87cce1",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : " Introducing a new `legacy` conf for this behavior change seems kind of trivial and might bring cognition burdens for users. So the config is reused for now and the doc will be updated if it is the right way to go ",
        "createdAt" : "2021-03-11T05:02:45Z",
        "updatedAt" : "2021-03-30T07:59:14Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "0e4c31ef-0b20-4426-882d-48e9ebd2760a",
        "parentId" : "def2e6d2-9bdf-4906-8dfa-42e66d87cce1",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "This is not reused technically. If we reuse the existing conf, this should be `output.length == 4` because it disable this PR and the previous commit simultaneously.\r\n> So the config is reused for now ",
        "createdAt" : "2021-03-11T05:06:57Z",
        "updatedAt" : "2021-03-30T07:59:14Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "b898ded2-5da5-414c-a6e8-8eb73d51e8ff",
        "parentId" : "def2e6d2-9bdf-4906-8dfa-42e66d87cce1",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "> this should be output.length == 4\r\n\r\nindeed, this is true.  `output.head.withName(\"database\") +: output.slice(1, 4)` will cut the `isView` off",
        "createdAt" : "2021-03-11T05:15:30Z",
        "updatedAt" : "2021-03-30T07:59:14Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "3ef5b5f414b68ea149877eaa26618e9787252004",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +367,371 @@        output) =>\n      val newOutput = if (conf.getConf(SQLConf.LEGACY_KEEP_COMMAND_OUTPUT_SCHEMA)) {\n        assert(output.length == 5)\n        ShowTableExtended.getLegacyOutputAttrs\n      } else {"
  },
  {
    "id" : "ff2c3c4e-929f-4170-9d19-2f6d72c690a1",
    "prId" : 31705,
    "prUrl" : "https://github.com/apache/spark/pull/31705#pullrequestreview-635163466",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dd335cb1-d967-4579-a4bd-53d80773e335",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "@AngersZhuuuu  can we add one item in the migration guide for this change?",
        "createdAt" : "2021-04-13T15:58:10Z",
        "updatedAt" : "2021-04-13T15:58:10Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "ca34a2c3-7bb9-4df0-a96b-830ba8b9fbff",
        "parentId" : "dd335cb1-d967-4579-a4bd-53d80773e335",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "Done https://github.com/apache/spark/pull/32155",
        "createdAt" : "2021-04-14T02:23:49Z",
        "updatedAt" : "2021-04-14T02:23:50Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "72345b2351cd447c61b93cc186952e6ae526fdfd",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +181,185 @@\n    case DescribeNamespace(DatabaseInSessionCatalog(db), extended, output) =>\n      val newOutput = if (conf.getConf(SQLConf.LEGACY_KEEP_COMMAND_OUTPUT_SCHEMA)) {\n        assert(output.length == 2)\n        Seq(output.head.withName(\"database_description_item\"),"
  },
  {
    "id" : "c77e215f-4c03-4fc2-822f-f6469de400ad",
    "prId" : 31499,
    "prUrl" : "https://github.com/apache/spark/pull/31499#pullrequestreview-591079091",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a13cc985-2269-4ab1-b9a6-720b170684ff",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Just a question: can we propagate the original `commandName` instead of having `\"MSCK REPAIR TABLE\"`?",
        "createdAt" : "2021-02-15T16:53:13Z",
        "updatedAt" : "2021-02-23T10:22:53Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "4d3623e2-4268-454a-b5b0-e42b79d8311c",
        "parentId" : "a13cc985-2269-4ab1-b9a6-720b170684ff",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Unfortunately, we lost the info in `UnresolvedTable` -> `ResolvedTable` but we can reconstruct the command name from the flags `addPartitions` and `dropPartitions`. Though not the original one because `MSCK REPAIR TABLE table` has `addPartitions = true, dropPartitions  = false` can be re-constructed as `MSCK REPAIR TABLE table ADD PARTITIONS`. Are you ok with that?",
        "createdAt" : "2021-02-16T10:19:25Z",
        "updatedAt" : "2021-02-23T10:22:53Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "fefea57d097e64b5b506dea45aac24c537f27abd",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +382,386 @@        addPartitions,\n        dropPartitions,\n        \"MSCK REPAIR TABLE\")\n\n    case LoadData(ResolvedV1TableIdentifier(ident), path, isLocal, isOverwrite, partition) =>"
  },
  {
    "id" : "8cc9953b-bfc1-436c-83bb-625038dbde37",
    "prId" : 30748,
    "prUrl" : "https://github.com/apache/spark/pull/30748#pullrequestreview-550857954",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ae494e2a-ab16-4018-97e7-10ac853fd634",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "I leaved `retainData` in `AlterTableDropPartitionCommand` because it is set to `true` at:\r\nhttps://github.com/apache/spark/blob/7c59aeeef4c571838bd291079f9b804d6f546487/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand.scala#L158-L161",
        "createdAt" : "2020-12-12T19:59:06Z",
        "updatedAt" : "2020-12-12T21:57:19Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "96064653043a18f7f826218681590730cd753486",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +476,480 @@        ifExists,\n        purge,\n        retainData = false)\n\n    case AlterTableSerDePropertiesStatement(tbl, serdeClassName, serdeProperties, partitionSpec) =>"
  },
  {
    "id" : "3caaa64c-e3a5-4153-82a9-2b961b9f1d37",
    "prId" : 30554,
    "prUrl" : "https://github.com/apache/spark/pull/30554#pullrequestreview-542398594",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9c50c098-ca4e-409e-bd3b-20bb8f95e2b9",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I would even think about printing a warning that creating a Hive table by default is deprecated, and this configuration will be disabled by default in the future because Spark should create a Spark table.",
        "createdAt" : "2020-12-02T00:07:12Z",
        "updatedAt" : "2020-12-02T08:50:59Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "bff924de21b8af2da17f31591768ce02c784bd43",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +640,644 @@      //   1. `LEGACY_CREATE_HIVE_TABLE_BY_DEFAULT` is false, or\n      //   2. It's a CTAS and `conf.convertCTAS` is true.\n      val createHiveTableByDefault = conf.getConf(SQLConf.LEGACY_CREATE_HIVE_TABLE_BY_DEFAULT)\n      if (!createHiveTableByDefault || (ctas && conf.convertCTAS)) {\n        (nonHiveStorageFormat, conf.defaultDataSourceName)"
  },
  {
    "id" : "269ebefc-3aec-469b-9e27-5286d990f694",
    "prId" : 30554,
    "prUrl" : "https://github.com/apache/spark/pull/30554#pullrequestreview-543086657",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d2ab7946-3084-4608-9dcc-0e07b743ea5e",
        "parentId" : null,
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "Should this mark `convertCTAS` as deprecated since it is superseded by the new config?",
        "createdAt" : "2020-12-02T17:37:33Z",
        "updatedAt" : "2020-12-02T17:37:33Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      },
      {
        "id" : "9ddacded-8512-48e4-965a-ab11935dd66b",
        "parentId" : "d2ab7946-3084-4608-9dcc-0e07b743ea5e",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "yea, I can do it in a follow-up.",
        "createdAt" : "2020-12-02T17:44:25Z",
        "updatedAt" : "2020-12-02T17:44:25Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "bff924de21b8af2da17f31591768ce02c784bd43",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +641,645 @@      //   2. It's a CTAS and `conf.convertCTAS` is true.\n      val createHiveTableByDefault = conf.getConf(SQLConf.LEGACY_CREATE_HIVE_TABLE_BY_DEFAULT)\n      if (!createHiveTableByDefault || (ctas && conf.convertCTAS)) {\n        (nonHiveStorageFormat, conf.defaultDataSourceName)\n      } else {"
  },
  {
    "id" : "10734b03-692f-41ef-8960-8e9b3f8f6086",
    "prId" : 30403,
    "prUrl" : "https://github.com/apache/spark/pull/30403#pullrequestreview-533097554",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "128eb3f5-01a4-4bb2-a87b-75bba3d1fb4c",
        "parentId" : null,
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "A refactoring not related to this PR.",
        "createdAt" : "2020-11-18T04:33:54Z",
        "updatedAt" : "2020-11-27T21:30:33Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "7e788cea5c2dd03f71ee30b0106e04d7f036f30f",
    "line" : 26,
    "diffHunk" : "@@ -1,1 +548,552 @@      }\n\n    case ShowTableProperties(ResolvedV1TableOrViewIdentifier(ident), propertyKey) =>\n      ShowTablePropertiesCommand(ident.asTableIdentifier, propertyKey)\n"
  },
  {
    "id" : "75291153-6050-4e40-98a7-bc4b5159aa89",
    "prId" : 30270,
    "prUrl" : "https://github.com/apache/spark/pull/30270#pullrequestreview-524830999",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "55a4727e-8227-4f05-b170-c308c71d2170",
        "parentId" : null,
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "Note that changes related to `RefreshTable`, `DropTable`, and `ShowTableProperties` are to use the new extractor `ResolvedV1TableIdentifier`.",
        "createdAt" : "2020-11-06T03:20:03Z",
        "updatedAt" : "2020-11-09T18:50:13Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "d01a95e11896388f0978d6d102c363f3a45c28a7",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +314,318 @@\n    case RefreshTable(ResolvedV1TableIdentifier(ident)) =>\n      RefreshTableCommand(ident.asTableIdentifier)\n\n    case RefreshTable(r: ResolvedView) =>"
  },
  {
    "id" : "54a7640f-443b-4f61-a0cf-51d0581d8da5",
    "prId" : 30229,
    "prUrl" : "https://github.com/apache/spark/pull/30229#pullrequestreview-522193606",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d34a2d8a-2f3a-43e0-8db1-cbbd1c946ba5",
        "parentId" : null,
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "Applying `ResolvedV1TableOrViewIdentifier` extractor to existing code to simplify.",
        "createdAt" : "2020-11-03T05:13:30Z",
        "updatedAt" : "2020-11-03T18:46:05Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "c80e8eb5828ff68a1ebab4f9ee56b55b852ba7d0",
    "line" : 33,
    "diffHunk" : "@@ -1,1 +260,264 @@\n    case DescribeColumn(ResolvedV1TableOrViewIdentifier(ident), colNameParts, isExtended) =>\n      DescribeColumnCommand(ident.asTableIdentifier, colNameParts, isExtended)\n\n    // For CREATE TABLE [AS SELECT], we should use the v1 command if the catalog is resolved to the"
  },
  {
    "id" : "5d9bfa59-5795-4f54-81de-4d0a06b4d91f",
    "prId" : 30079,
    "prUrl" : "https://github.com/apache/spark/pull/30079#pullrequestreview-518153344",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f869b9ef-cc4b-47ce-b717-b884f13759f3",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "nit: shall we add an assert here to make sure it's not temp view?",
        "createdAt" : "2020-10-26T09:28:59Z",
        "updatedAt" : "2020-10-27T21:04:52Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "c6edfa5a-1e25-4772-a280-b1b660cd1f5d",
        "parentId" : "f869b9ef-cc4b-47ce-b717-b884f13759f3",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "You mean to make sure it's a temp view?\r\n\r\nFor v1, there are few checks for handling views/temp views inside `DropTableCommand`:\r\nhttps://github.com/apache/spark/blob/11bbb130df7b083f42acf0207531efe3912d89eb/sql/core/src/main/scala/org/apache/spark/sql/execution/command/ddl.scala#L226-L229\r\n\r\nDo you want me to copy the checks here?",
        "createdAt" : "2020-10-26T15:24:41Z",
        "updatedAt" : "2020-10-27T21:04:52Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      },
      {
        "id" : "967eed43-5204-4ca3-9ffd-ae92d8061229",
        "parentId" : "f869b9ef-cc4b-47ce-b717-b884f13759f3",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "just `assert(r.isTemp)`",
        "createdAt" : "2020-10-27T15:06:55Z",
        "updatedAt" : "2020-10-27T21:04:52Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "972a61ea-1211-4692-89eb-a6a60e3cbad0",
        "parentId" : "f869b9ef-cc4b-47ce-b717-b884f13759f3",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "If we put the assert here, the output call stack doesn't seem to be very helpful to the user:\r\n```\r\njava.lang.AssertionError: assertion failed\r\n\tat scala.Predef$.assert(Predef.scala:208)\r\n\tat org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog$$anonfun$apply$1.applyOrElse(ResolveSessionCatalog.scala:376)\r\n\tat org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog$$anonfun$apply$1.applyOrElse(ResolveSessionCatalog.scala:47)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$3(AnalysisHelper.scala:90)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)\r\n```\r\n, also there is an existing test around it: \r\nhttps://github.com/apache/spark/blob/3f2a2b5fe6ada37ef86f00737387e6cf2496df74/sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/HiveDDLSuite.scala#L1057\r\n\r\nI am throwing an exception instead similar to how `DropTableCommand` is handling. Please let me know what you think about this.",
        "createdAt" : "2020-10-27T21:09:59Z",
        "updatedAt" : "2020-10-27T21:09:59Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "dfc44923d10fa7cbb5cd6cde4dfcffa19a9e994d",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +373,377 @@\n    // v1 DROP TABLE supports temp view.\n    case DropTable(r: ResolvedView, ifExists, purge) =>\n      if (!r.isTemp) {\n        throw new AnalysisException("
  },
  {
    "id" : "f73dc153-a403-4d9c-8b75-409d6c5c4522",
    "prId" : 30079,
    "prUrl" : "https://github.com/apache/spark/pull/30079#pullrequestreview-518338984",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e0ffc66d-4ce3-4dc4-bd23-b591d1b4e392",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Currently, it's duplicated with the check in `DropTableCommand`. I think it's OK as we want to move to v2 commands eventually.",
        "createdAt" : "2020-10-28T05:44:12Z",
        "updatedAt" : "2020-10-28T05:44:12Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "dfc44923d10fa7cbb5cd6cde4dfcffa19a9e994d",
    "line" : 29,
    "diffHunk" : "@@ -1,1 +374,378 @@    // v1 DROP TABLE supports temp view.\n    case DropTable(r: ResolvedView, ifExists, purge) =>\n      if (!r.isTemp) {\n        throw new AnalysisException(\n          \"Cannot drop a view with DROP TABLE. Please use DROP VIEW instead\")"
  },
  {
    "id" : "b0b99378-30a3-4a74-9347-abb5c46adef6",
    "prId" : 29880,
    "prUrl" : "https://github.com/apache/spark/pull/29880#pullrequestreview-501437926",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6ebbd62c-46b0-40a4-8127-f304211f8f45",
        "parentId" : null,
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "This change is for `DESCRIBE table`, but adding this check for consistency.",
        "createdAt" : "2020-09-26T23:22:41Z",
        "updatedAt" : "2020-10-06T19:46:53Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      },
      {
        "id" : "66aee94d-1b4b-4ee7-aa6d-35b47fa00611",
        "parentId" : "6ebbd62c-46b0-40a4-8127-f304211f8f45",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "hmm, I think only session catalog can return `V1Table`?",
        "createdAt" : "2020-09-29T13:02:13Z",
        "updatedAt" : "2020-10-06T19:46:53Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "a29ed646-81e7-46c5-8b9c-ddb0dd0057dd",
        "parentId" : "6ebbd62c-46b0-40a4-8127-f304211f8f45",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "OK. I will remove the check here and other existing places.",
        "createdAt" : "2020-10-02T20:47:16Z",
        "updatedAt" : "2020-10-06T19:46:53Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "97a4deb99439fb6f5903c44d3ca6b74bae1c2f78",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +258,262 @@\n    case DescribeRelation(r @ ResolvedTable(_, ident, _: V1Table), partitionSpec, isExtended)\n        if isSessionCatalog(r.catalog) =>\n      DescribeTableCommand(ident.asTableIdentifier, partitionSpec, isExtended)\n"
  },
  {
    "id" : "fa874638-d861-4a46-a9a2-5a681ccbc930",
    "prId" : 29880,
    "prUrl" : "https://github.com/apache/spark/pull/29880#pullrequestreview-498443027",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1be3aa4a-0178-4a52-a218-a29a358dbe83",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "ditto",
        "createdAt" : "2020-09-29T13:02:25Z",
        "updatedAt" : "2020-10-06T19:46:53Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "97a4deb99439fb6f5903c44d3ca6b74bae1c2f78",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +266,270 @@\n    case DescribeColumn(r @ ResolvedTable(_, _, _: V1Table), colNameParts, isExtended)\n        if isSessionCatalog(r.catalog) =>\n      DescribeColumnCommand(r.identifier.asTableIdentifier, colNameParts, isExtended)\n"
  },
  {
    "id" : "b08f375e-0d9d-471b-96f2-e8d13cf12675",
    "prId" : 28935,
    "prUrl" : "https://github.com/apache/spark/pull/28935#pullrequestreview-439722350",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1988369f-a8d2-43de-b98e-e1afc7b8dc62",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "we can be more aggressive here: forbid void type in all cases, including hive tables.",
        "createdAt" : "2020-06-30T07:00:10Z",
        "updatedAt" : "2020-06-30T07:00:11Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "3fa76cfaf7aaca5c5d2ca8ad2743bdefae208b61",
    "line" : 47,
    "diffHunk" : "@@ -1,1 +136,140 @@              }\n          }\n          // Add Hive type 'string' and 'void' to metadata.\n          val cleanedDataType =\n            HiveVoidType.replaceVoidType(HiveStringType.replaceCharType(dataType))"
  },
  {
    "id" : "e1326666-6e75-450a-a375-f2fa6b299da8",
    "prId" : 28833,
    "prUrl" : "https://github.com/apache/spark/pull/28833#pullrequestreview-440729404",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8a883cbc-be51-4215-9549-37bdbe3082ad",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "It seems that we have a check for `ReplaceTableStatement`, but not for `ReplaceTableAsSelectStatement`. Is it okay?",
        "createdAt" : "2020-07-01T07:34:54Z",
        "updatedAt" : "2020-07-08T00:27:04Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "f20715e8-5aac-4bf8-8d31-1c5f90356601",
        "parentId" : "8a883cbc-be51-4215-9549-37bdbe3082ad",
        "authorId" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "body" : "Yes. `case CreateTableAsSelectStatement` is not cover either. ",
        "createdAt" : "2020-07-01T09:09:16Z",
        "updatedAt" : "2020-07-08T00:27:04Z",
        "lastEditedBy" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "tags" : [
        ]
      },
      {
        "id" : "83b7332c-f1d7-4483-96a0-e2253a836741",
        "parentId" : "8a883cbc-be51-4215-9549-37bdbe3082ad",
        "authorId" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "body" : "I will check the new REPLACE syntax with UT.",
        "createdAt" : "2020-07-01T09:16:07Z",
        "updatedAt" : "2020-07-08T00:27:04Z",
        "lastEditedBy" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "tags" : [
        ]
      },
      {
        "id" : "5ab10db9-60d5-4048-a256-69a199512754",
        "parentId" : "8a883cbc-be51-4215-9549-37bdbe3082ad",
        "authorId" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "body" : "By my testing, `ReplaceTableAsSelectStatement` should also check the assertion. I will change it.",
        "createdAt" : "2020-07-01T10:01:25Z",
        "updatedAt" : "2020-07-08T00:27:04Z",
        "lastEditedBy" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "tags" : [
        ]
      },
      {
        "id" : "0f8998d6-73f4-4256-a6df-ed0b14e60a13",
        "parentId" : "8a883cbc-be51-4215-9549-37bdbe3082ad",
        "authorId" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "body" : "done",
        "createdAt" : "2020-07-01T10:05:12Z",
        "updatedAt" : "2020-07-08T00:27:04Z",
        "lastEditedBy" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "tags" : [
        ]
      }
    ],
    "commit" : "9ad57d17bac47ea0f801004ec0aba9197e631bc7",
    "line" : 46,
    "diffHunk" : "@@ -1,1 +327,331 @@    case c @ ReplaceTableStatement(\n         SessionCatalogAndTable(catalog, tbl), _, _, _, _, _, _, _, _, _) =>\n      assertNoNullTypeInSchema(c.tableSchema)\n      val provider = c.provider.getOrElse(conf.defaultDataSourceName)\n      if (!isV2Provider(provider)) {"
  },
  {
    "id" : "358542cc-30a5-44dc-8cc0-1902c298feb7",
    "prId" : 28833,
    "prUrl" : "https://github.com/apache/spark/pull/28833#pullrequestreview-635339687",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6d2773c0-ec74-4bee-bd37-c55292a20991",
        "parentId" : null,
        "authorId" : "2d6b46ba-4100-4c4d-9341-fff5e39647ec",
        "body" : "It would be great if we could add a legacy flag for such behavior change in future. This changes the behavior for both v1 and v2 Catalogs in order to fix a compatibility issue with Hive Metastore. But Hive Metastore is not the only Catalog Spark supports since we have opened the Catalog APIs in DSv2.",
        "createdAt" : "2021-04-13T17:50:23Z",
        "updatedAt" : "2021-04-13T17:50:24Z",
        "lastEditedBy" : "2d6b46ba-4100-4c4d-9341-fff5e39647ec",
        "tags" : [
        ]
      },
      {
        "id" : "6c377bfd-1872-4017-b0c2-8b0bae8e215d",
        "parentId" : "6d2773c0-ec74-4bee-bd37-c55292a20991",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I don't know any database that supports creating tables with null/void type column, so this change is not for hive compatibility but for reasonable SQL semantic.\r\n\r\nI agree this is a breaking change that should be at least put in the migration guide. A legacy config can also be added but I can't find a reasonable use case for a null type column.\r\n\r\n",
        "createdAt" : "2021-04-14T03:49:54Z",
        "updatedAt" : "2021-04-14T03:49:55Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "3e53ccca-94a9-4928-82ce-8c71d50f8163",
        "parentId" : "6d2773c0-ec74-4bee-bd37-c55292a20991",
        "authorId" : "a80e0991-f7b1-415a-8d2e-ab615cc6ef4a",
        "body" : "> I don't know any database that supports creating tables with null/void type column, so this change is not for hive compatibility but for reasonable SQL semantic.\r\n> \r\n> I agree this is a breaking change that should be at least put in the migration guide. A legacy config can also be added but I can't find a reasonable use case for a null type column.\r\n\r\nI think the main reason why you would want to support it is when people are using tables / views / temp tables to structure existing workloads. We support NullType type in CTEs, but in the case where people want to reuse the same CTE in multiple queries (i.e., multi-output workloads), they have no choice but to use views or temporary tables. (With DataFrames they'd still be able to reuse the same dataframe for multiple outputs, but in SQL that doesn't work.)\r\n\r\nOne typical use case where you use CTEs to structure your code is if you have multiple sources with different structures that you then UNION ALL together into a single dataset. It is not uncommon for each of the sources to have certain columns that don't apply, and then you write explicit NULLs there. It would be pretty annoying if you had to write explicit casts of those NULLs to the right type in all of those cases.\r\n",
        "createdAt" : "2021-04-14T07:29:21Z",
        "updatedAt" : "2021-04-14T07:29:22Z",
        "lastEditedBy" : "a80e0991-f7b1-415a-8d2e-ab615cc6ef4a",
        "tags" : [
        ]
      },
      {
        "id" : "5cf9935e-9820-4522-96be-c882dcf35ddb",
        "parentId" : "6d2773c0-ec74-4bee-bd37-c55292a20991",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "@bart-samwel this makes sense, shall we also support `CREATE TABLE t(c VOID)`? Your case seems like CTAS only.",
        "createdAt" : "2021-04-14T07:41:00Z",
        "updatedAt" : "2021-04-14T07:41:01Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "2785cebd-f06e-4d35-9fa6-9d396cd9a635",
        "parentId" : "6d2773c0-ec74-4bee-bd37-c55292a20991",
        "authorId" : "a80e0991-f7b1-415a-8d2e-ab615cc6ef4a",
        "body" : "> @bart-samwel this makes sense, shall we also support `CREATE TABLE t(c VOID)`? Your case seems like CTAS only.\r\n\r\nI think the `CREATE TABLE` case with explicit types is not very useful, but it could be useful if there were tools that get a table's schema and then try to recreate it, e.g. for mocking purposes. Probably best to be orthogonal here.",
        "createdAt" : "2021-04-14T08:03:17Z",
        "updatedAt" : "2021-04-14T08:03:18Z",
        "lastEditedBy" : "a80e0991-f7b1-415a-8d2e-ab615cc6ef4a",
        "tags" : [
        ]
      },
      {
        "id" : "58d8c357-e284-4d86-8395-824cda4b9cfc",
        "parentId" : "6d2773c0-ec74-4bee-bd37-c55292a20991",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "@LantaoJin do you have time to fix it? I think we can simply remove the null type check and add a few tests with both in-memory and hive catalog.",
        "createdAt" : "2021-04-14T08:19:05Z",
        "updatedAt" : "2021-04-14T08:19:05Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "9ad57d17bac47ea0f801004ec0aba9197e631bc7",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +272,276 @@    case c @ CreateTableStatement(\n         SessionCatalogAndTable(catalog, tbl), _, _, _, _, _, _, _, _, _) =>\n      assertNoNullTypeInSchema(c.tableSchema)\n      val provider = c.provider.getOrElse(conf.defaultDataSourceName)\n      if (!isV2Provider(provider)) {"
  }
]