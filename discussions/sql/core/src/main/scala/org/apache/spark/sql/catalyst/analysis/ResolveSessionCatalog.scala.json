[
  {
    "id" : "3edfcc5f-c4ea-4f8b-8ea8-7d2cd3d591da",
    "prId" : 31804,
    "prUrl" : "https://github.com/apache/spark/pull/31804#pullrequestreview-609400754",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "def2e6d2-9bdf-4906-8dfa-42e66d87cce1",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Well, this seems inconsistent with the doc. The current document means `spark.sql.legacy.keepCommandOutputSchema` means the 3.1 or earlier schema, doesn't it?",
        "createdAt" : "2021-03-11T04:54:58Z",
        "updatedAt" : "2021-03-30T07:59:14Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "f2effd52-42c9-4558-91f4-996e3f34bf80",
        "parentId" : "def2e6d2-9bdf-4906-8dfa-42e66d87cce1",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : " Introducing a new `legacy` conf for this behavior change seems kind of trivial and might bring cognition burdens for users. So the config is reused for now and the doc will be updated if it is the right way to go ",
        "createdAt" : "2021-03-11T05:02:45Z",
        "updatedAt" : "2021-03-30T07:59:14Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "0e4c31ef-0b20-4426-882d-48e9ebd2760a",
        "parentId" : "def2e6d2-9bdf-4906-8dfa-42e66d87cce1",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "This is not reused technically. If we reuse the existing conf, this should be `output.length == 4` because it disable this PR and the previous commit simultaneously.\r\n> So the config is reused for now ",
        "createdAt" : "2021-03-11T05:06:57Z",
        "updatedAt" : "2021-03-30T07:59:14Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "b898ded2-5da5-414c-a6e8-8eb73d51e8ff",
        "parentId" : "def2e6d2-9bdf-4906-8dfa-42e66d87cce1",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "> this should be output.length == 4\r\n\r\nindeed, this is true.  `output.head.withName(\"database\") +: output.slice(1, 4)` will cut the `isView` off",
        "createdAt" : "2021-03-11T05:15:30Z",
        "updatedAt" : "2021-03-30T07:59:14Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "3ef5b5f414b68ea149877eaa26618e9787252004",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +367,371 @@        output) =>\n      val newOutput = if (conf.getConf(SQLConf.LEGACY_KEEP_COMMAND_OUTPUT_SCHEMA)) {\n        assert(output.length == 5)\n        ShowTableExtended.getLegacyOutputAttrs\n      } else {"
  },
  {
    "id" : "ff2c3c4e-929f-4170-9d19-2f6d72c690a1",
    "prId" : 31705,
    "prUrl" : "https://github.com/apache/spark/pull/31705#pullrequestreview-635163466",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dd335cb1-d967-4579-a4bd-53d80773e335",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "@AngersZhuuuu  can we add one item in the migration guide for this change?",
        "createdAt" : "2021-04-13T15:58:10Z",
        "updatedAt" : "2021-04-13T15:58:10Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "ca34a2c3-7bb9-4df0-a96b-830ba8b9fbff",
        "parentId" : "dd335cb1-d967-4579-a4bd-53d80773e335",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "Done https://github.com/apache/spark/pull/32155",
        "createdAt" : "2021-04-14T02:23:49Z",
        "updatedAt" : "2021-04-14T02:23:50Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "72345b2351cd447c61b93cc186952e6ae526fdfd",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +181,185 @@\n    case DescribeNamespace(DatabaseInSessionCatalog(db), extended, output) =>\n      val newOutput = if (conf.getConf(SQLConf.LEGACY_KEEP_COMMAND_OUTPUT_SCHEMA)) {\n        assert(output.length == 2)\n        Seq(output.head.withName(\"database_description_item\"),"
  },
  {
    "id" : "c77e215f-4c03-4fc2-822f-f6469de400ad",
    "prId" : 31499,
    "prUrl" : "https://github.com/apache/spark/pull/31499#pullrequestreview-591079091",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a13cc985-2269-4ab1-b9a6-720b170684ff",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Just a question: can we propagate the original `commandName` instead of having `\"MSCK REPAIR TABLE\"`?",
        "createdAt" : "2021-02-15T16:53:13Z",
        "updatedAt" : "2021-02-23T10:22:53Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "4d3623e2-4268-454a-b5b0-e42b79d8311c",
        "parentId" : "a13cc985-2269-4ab1-b9a6-720b170684ff",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Unfortunately, we lost the info in `UnresolvedTable` -> `ResolvedTable` but we can reconstruct the command name from the flags `addPartitions` and `dropPartitions`. Though not the original one because `MSCK REPAIR TABLE table` has `addPartitions = true, dropPartitions  = false` can be re-constructed as `MSCK REPAIR TABLE table ADD PARTITIONS`. Are you ok with that?",
        "createdAt" : "2021-02-16T10:19:25Z",
        "updatedAt" : "2021-02-23T10:22:53Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "fefea57d097e64b5b506dea45aac24c537f27abd",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +382,386 @@        addPartitions,\n        dropPartitions,\n        \"MSCK REPAIR TABLE\")\n\n    case LoadData(ResolvedV1TableIdentifier(ident), path, isLocal, isOverwrite, partition) =>"
  },
  {
    "id" : "8cc9953b-bfc1-436c-83bb-625038dbde37",
    "prId" : 30748,
    "prUrl" : "https://github.com/apache/spark/pull/30748#pullrequestreview-550857954",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ae494e2a-ab16-4018-97e7-10ac853fd634",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "I leaved `retainData` in `AlterTableDropPartitionCommand` because it is set to `true` at:\r\nhttps://github.com/apache/spark/blob/7c59aeeef4c571838bd291079f9b804d6f546487/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand.scala#L158-L161",
        "createdAt" : "2020-12-12T19:59:06Z",
        "updatedAt" : "2020-12-12T21:57:19Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "96064653043a18f7f826218681590730cd753486",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +476,480 @@        ifExists,\n        purge,\n        retainData = false)\n\n    case AlterTableSerDePropertiesStatement(tbl, serdeClassName, serdeProperties, partitionSpec) =>"
  },
  {
    "id" : "3caaa64c-e3a5-4153-82a9-2b961b9f1d37",
    "prId" : 30554,
    "prUrl" : "https://github.com/apache/spark/pull/30554#pullrequestreview-542398594",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9c50c098-ca4e-409e-bd3b-20bb8f95e2b9",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I would even think about printing a warning that creating a Hive table by default is deprecated, and this configuration will be disabled by default in the future because Spark should create a Spark table.",
        "createdAt" : "2020-12-02T00:07:12Z",
        "updatedAt" : "2020-12-02T08:50:59Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "bff924de21b8af2da17f31591768ce02c784bd43",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +640,644 @@      //   1. `LEGACY_CREATE_HIVE_TABLE_BY_DEFAULT` is false, or\n      //   2. It's a CTAS and `conf.convertCTAS` is true.\n      val createHiveTableByDefault = conf.getConf(SQLConf.LEGACY_CREATE_HIVE_TABLE_BY_DEFAULT)\n      if (!createHiveTableByDefault || (ctas && conf.convertCTAS)) {\n        (nonHiveStorageFormat, conf.defaultDataSourceName)"
  },
  {
    "id" : "269ebefc-3aec-469b-9e27-5286d990f694",
    "prId" : 30554,
    "prUrl" : "https://github.com/apache/spark/pull/30554#pullrequestreview-543086657",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d2ab7946-3084-4608-9dcc-0e07b743ea5e",
        "parentId" : null,
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "Should this mark `convertCTAS` as deprecated since it is superseded by the new config?",
        "createdAt" : "2020-12-02T17:37:33Z",
        "updatedAt" : "2020-12-02T17:37:33Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      },
      {
        "id" : "9ddacded-8512-48e4-965a-ab11935dd66b",
        "parentId" : "d2ab7946-3084-4608-9dcc-0e07b743ea5e",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "yea, I can do it in a follow-up.",
        "createdAt" : "2020-12-02T17:44:25Z",
        "updatedAt" : "2020-12-02T17:44:25Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "bff924de21b8af2da17f31591768ce02c784bd43",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +641,645 @@      //   2. It's a CTAS and `conf.convertCTAS` is true.\n      val createHiveTableByDefault = conf.getConf(SQLConf.LEGACY_CREATE_HIVE_TABLE_BY_DEFAULT)\n      if (!createHiveTableByDefault || (ctas && conf.convertCTAS)) {\n        (nonHiveStorageFormat, conf.defaultDataSourceName)\n      } else {"
  },
  {
    "id" : "10734b03-692f-41ef-8960-8e9b3f8f6086",
    "prId" : 30403,
    "prUrl" : "https://github.com/apache/spark/pull/30403#pullrequestreview-533097554",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "128eb3f5-01a4-4bb2-a87b-75bba3d1fb4c",
        "parentId" : null,
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "A refactoring not related to this PR.",
        "createdAt" : "2020-11-18T04:33:54Z",
        "updatedAt" : "2020-11-27T21:30:33Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "7e788cea5c2dd03f71ee30b0106e04d7f036f30f",
    "line" : 26,
    "diffHunk" : "@@ -1,1 +548,552 @@      }\n\n    case ShowTableProperties(ResolvedV1TableOrViewIdentifier(ident), propertyKey) =>\n      ShowTablePropertiesCommand(ident.asTableIdentifier, propertyKey)\n"
  },
  {
    "id" : "75291153-6050-4e40-98a7-bc4b5159aa89",
    "prId" : 30270,
    "prUrl" : "https://github.com/apache/spark/pull/30270#pullrequestreview-524830999",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "55a4727e-8227-4f05-b170-c308c71d2170",
        "parentId" : null,
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "Note that changes related to `RefreshTable`, `DropTable`, and `ShowTableProperties` are to use the new extractor `ResolvedV1TableIdentifier`.",
        "createdAt" : "2020-11-06T03:20:03Z",
        "updatedAt" : "2020-11-09T18:50:13Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "d01a95e11896388f0978d6d102c363f3a45c28a7",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +314,318 @@\n    case RefreshTable(ResolvedV1TableIdentifier(ident)) =>\n      RefreshTableCommand(ident.asTableIdentifier)\n\n    case RefreshTable(r: ResolvedView) =>"
  },
  {
    "id" : "54a7640f-443b-4f61-a0cf-51d0581d8da5",
    "prId" : 30229,
    "prUrl" : "https://github.com/apache/spark/pull/30229#pullrequestreview-522193606",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d34a2d8a-2f3a-43e0-8db1-cbbd1c946ba5",
        "parentId" : null,
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "Applying `ResolvedV1TableOrViewIdentifier` extractor to existing code to simplify.",
        "createdAt" : "2020-11-03T05:13:30Z",
        "updatedAt" : "2020-11-03T18:46:05Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "c80e8eb5828ff68a1ebab4f9ee56b55b852ba7d0",
    "line" : 33,
    "diffHunk" : "@@ -1,1 +260,264 @@\n    case DescribeColumn(ResolvedV1TableOrViewIdentifier(ident), colNameParts, isExtended) =>\n      DescribeColumnCommand(ident.asTableIdentifier, colNameParts, isExtended)\n\n    // For CREATE TABLE [AS SELECT], we should use the v1 command if the catalog is resolved to the"
  },
  {
    "id" : "5d9bfa59-5795-4f54-81de-4d0a06b4d91f",
    "prId" : 30079,
    "prUrl" : "https://github.com/apache/spark/pull/30079#pullrequestreview-518153344",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f869b9ef-cc4b-47ce-b717-b884f13759f3",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "nit: shall we add an assert here to make sure it's not temp view?",
        "createdAt" : "2020-10-26T09:28:59Z",
        "updatedAt" : "2020-10-27T21:04:52Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "c6edfa5a-1e25-4772-a280-b1b660cd1f5d",
        "parentId" : "f869b9ef-cc4b-47ce-b717-b884f13759f3",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "You mean to make sure it's a temp view?\r\n\r\nFor v1, there are few checks for handling views/temp views inside `DropTableCommand`:\r\nhttps://github.com/apache/spark/blob/11bbb130df7b083f42acf0207531efe3912d89eb/sql/core/src/main/scala/org/apache/spark/sql/execution/command/ddl.scala#L226-L229\r\n\r\nDo you want me to copy the checks here?",
        "createdAt" : "2020-10-26T15:24:41Z",
        "updatedAt" : "2020-10-27T21:04:52Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      },
      {
        "id" : "967eed43-5204-4ca3-9ffd-ae92d8061229",
        "parentId" : "f869b9ef-cc4b-47ce-b717-b884f13759f3",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "just `assert(r.isTemp)`",
        "createdAt" : "2020-10-27T15:06:55Z",
        "updatedAt" : "2020-10-27T21:04:52Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "972a61ea-1211-4692-89eb-a6a60e3cbad0",
        "parentId" : "f869b9ef-cc4b-47ce-b717-b884f13759f3",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "If we put the assert here, the output call stack doesn't seem to be very helpful to the user:\r\n```\r\njava.lang.AssertionError: assertion failed\r\n\tat scala.Predef$.assert(Predef.scala:208)\r\n\tat org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog$$anonfun$apply$1.applyOrElse(ResolveSessionCatalog.scala:376)\r\n\tat org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog$$anonfun$apply$1.applyOrElse(ResolveSessionCatalog.scala:47)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$3(AnalysisHelper.scala:90)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)\r\n```\r\n, also there is an existing test around it: \r\nhttps://github.com/apache/spark/blob/3f2a2b5fe6ada37ef86f00737387e6cf2496df74/sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/HiveDDLSuite.scala#L1057\r\n\r\nI am throwing an exception instead similar to how `DropTableCommand` is handling. Please let me know what you think about this.",
        "createdAt" : "2020-10-27T21:09:59Z",
        "updatedAt" : "2020-10-27T21:09:59Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "dfc44923d10fa7cbb5cd6cde4dfcffa19a9e994d",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +373,377 @@\n    // v1 DROP TABLE supports temp view.\n    case DropTable(r: ResolvedView, ifExists, purge) =>\n      if (!r.isTemp) {\n        throw new AnalysisException("
  },
  {
    "id" : "f73dc153-a403-4d9c-8b75-409d6c5c4522",
    "prId" : 30079,
    "prUrl" : "https://github.com/apache/spark/pull/30079#pullrequestreview-518338984",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e0ffc66d-4ce3-4dc4-bd23-b591d1b4e392",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Currently, it's duplicated with the check in `DropTableCommand`. I think it's OK as we want to move to v2 commands eventually.",
        "createdAt" : "2020-10-28T05:44:12Z",
        "updatedAt" : "2020-10-28T05:44:12Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "dfc44923d10fa7cbb5cd6cde4dfcffa19a9e994d",
    "line" : 29,
    "diffHunk" : "@@ -1,1 +374,378 @@    // v1 DROP TABLE supports temp view.\n    case DropTable(r: ResolvedView, ifExists, purge) =>\n      if (!r.isTemp) {\n        throw new AnalysisException(\n          \"Cannot drop a view with DROP TABLE. Please use DROP VIEW instead\")"
  },
  {
    "id" : "b0b99378-30a3-4a74-9347-abb5c46adef6",
    "prId" : 29880,
    "prUrl" : "https://github.com/apache/spark/pull/29880#pullrequestreview-501437926",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6ebbd62c-46b0-40a4-8127-f304211f8f45",
        "parentId" : null,
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "This change is for `DESCRIBE table`, but adding this check for consistency.",
        "createdAt" : "2020-09-26T23:22:41Z",
        "updatedAt" : "2020-10-06T19:46:53Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      },
      {
        "id" : "66aee94d-1b4b-4ee7-aa6d-35b47fa00611",
        "parentId" : "6ebbd62c-46b0-40a4-8127-f304211f8f45",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "hmm, I think only session catalog can return `V1Table`?",
        "createdAt" : "2020-09-29T13:02:13Z",
        "updatedAt" : "2020-10-06T19:46:53Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "a29ed646-81e7-46c5-8b9c-ddb0dd0057dd",
        "parentId" : "6ebbd62c-46b0-40a4-8127-f304211f8f45",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "OK. I will remove the check here and other existing places.",
        "createdAt" : "2020-10-02T20:47:16Z",
        "updatedAt" : "2020-10-06T19:46:53Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "97a4deb99439fb6f5903c44d3ca6b74bae1c2f78",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +258,262 @@\n    case DescribeRelation(r @ ResolvedTable(_, ident, _: V1Table), partitionSpec, isExtended)\n        if isSessionCatalog(r.catalog) =>\n      DescribeTableCommand(ident.asTableIdentifier, partitionSpec, isExtended)\n"
  },
  {
    "id" : "fa874638-d861-4a46-a9a2-5a681ccbc930",
    "prId" : 29880,
    "prUrl" : "https://github.com/apache/spark/pull/29880#pullrequestreview-498443027",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1be3aa4a-0178-4a52-a218-a29a358dbe83",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "ditto",
        "createdAt" : "2020-09-29T13:02:25Z",
        "updatedAt" : "2020-10-06T19:46:53Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "97a4deb99439fb6f5903c44d3ca6b74bae1c2f78",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +266,270 @@\n    case DescribeColumn(r @ ResolvedTable(_, _, _: V1Table), colNameParts, isExtended)\n        if isSessionCatalog(r.catalog) =>\n      DescribeColumnCommand(r.identifier.asTableIdentifier, colNameParts, isExtended)\n"
  },
  {
    "id" : "b08f375e-0d9d-471b-96f2-e8d13cf12675",
    "prId" : 28935,
    "prUrl" : "https://github.com/apache/spark/pull/28935#pullrequestreview-439722350",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1988369f-a8d2-43de-b98e-e1afc7b8dc62",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "we can be more aggressive here: forbid void type in all cases, including hive tables.",
        "createdAt" : "2020-06-30T07:00:10Z",
        "updatedAt" : "2020-06-30T07:00:11Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "3fa76cfaf7aaca5c5d2ca8ad2743bdefae208b61",
    "line" : 47,
    "diffHunk" : "@@ -1,1 +136,140 @@              }\n          }\n          // Add Hive type 'string' and 'void' to metadata.\n          val cleanedDataType =\n            HiveVoidType.replaceVoidType(HiveStringType.replaceCharType(dataType))"
  },
  {
    "id" : "e1326666-6e75-450a-a375-f2fa6b299da8",
    "prId" : 28833,
    "prUrl" : "https://github.com/apache/spark/pull/28833#pullrequestreview-440729404",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8a883cbc-be51-4215-9549-37bdbe3082ad",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "It seems that we have a check for `ReplaceTableStatement`, but not for `ReplaceTableAsSelectStatement`. Is it okay?",
        "createdAt" : "2020-07-01T07:34:54Z",
        "updatedAt" : "2020-07-08T00:27:04Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "f20715e8-5aac-4bf8-8d31-1c5f90356601",
        "parentId" : "8a883cbc-be51-4215-9549-37bdbe3082ad",
        "authorId" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "body" : "Yes. `case CreateTableAsSelectStatement` is not cover either. ",
        "createdAt" : "2020-07-01T09:09:16Z",
        "updatedAt" : "2020-07-08T00:27:04Z",
        "lastEditedBy" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "tags" : [
        ]
      },
      {
        "id" : "83b7332c-f1d7-4483-96a0-e2253a836741",
        "parentId" : "8a883cbc-be51-4215-9549-37bdbe3082ad",
        "authorId" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "body" : "I will check the new REPLACE syntax with UT.",
        "createdAt" : "2020-07-01T09:16:07Z",
        "updatedAt" : "2020-07-08T00:27:04Z",
        "lastEditedBy" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "tags" : [
        ]
      },
      {
        "id" : "5ab10db9-60d5-4048-a256-69a199512754",
        "parentId" : "8a883cbc-be51-4215-9549-37bdbe3082ad",
        "authorId" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "body" : "By my testing, `ReplaceTableAsSelectStatement` should also check the assertion. I will change it.",
        "createdAt" : "2020-07-01T10:01:25Z",
        "updatedAt" : "2020-07-08T00:27:04Z",
        "lastEditedBy" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "tags" : [
        ]
      },
      {
        "id" : "0f8998d6-73f4-4256-a6df-ed0b14e60a13",
        "parentId" : "8a883cbc-be51-4215-9549-37bdbe3082ad",
        "authorId" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "body" : "done",
        "createdAt" : "2020-07-01T10:05:12Z",
        "updatedAt" : "2020-07-08T00:27:04Z",
        "lastEditedBy" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "tags" : [
        ]
      }
    ],
    "commit" : "9ad57d17bac47ea0f801004ec0aba9197e631bc7",
    "line" : 46,
    "diffHunk" : "@@ -1,1 +327,331 @@    case c @ ReplaceTableStatement(\n         SessionCatalogAndTable(catalog, tbl), _, _, _, _, _, _, _, _, _) =>\n      assertNoNullTypeInSchema(c.tableSchema)\n      val provider = c.provider.getOrElse(conf.defaultDataSourceName)\n      if (!isV2Provider(provider)) {"
  },
  {
    "id" : "358542cc-30a5-44dc-8cc0-1902c298feb7",
    "prId" : 28833,
    "prUrl" : "https://github.com/apache/spark/pull/28833#pullrequestreview-635339687",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6d2773c0-ec74-4bee-bd37-c55292a20991",
        "parentId" : null,
        "authorId" : "2d6b46ba-4100-4c4d-9341-fff5e39647ec",
        "body" : "It would be great if we could add a legacy flag for such behavior change in future. This changes the behavior for both v1 and v2 Catalogs in order to fix a compatibility issue with Hive Metastore. But Hive Metastore is not the only Catalog Spark supports since we have opened the Catalog APIs in DSv2.",
        "createdAt" : "2021-04-13T17:50:23Z",
        "updatedAt" : "2021-04-13T17:50:24Z",
        "lastEditedBy" : "2d6b46ba-4100-4c4d-9341-fff5e39647ec",
        "tags" : [
        ]
      },
      {
        "id" : "6c377bfd-1872-4017-b0c2-8b0bae8e215d",
        "parentId" : "6d2773c0-ec74-4bee-bd37-c55292a20991",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I don't know any database that supports creating tables with null/void type column, so this change is not for hive compatibility but for reasonable SQL semantic.\r\n\r\nI agree this is a breaking change that should be at least put in the migration guide. A legacy config can also be added but I can't find a reasonable use case for a null type column.\r\n\r\n",
        "createdAt" : "2021-04-14T03:49:54Z",
        "updatedAt" : "2021-04-14T03:49:55Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "3e53ccca-94a9-4928-82ce-8c71d50f8163",
        "parentId" : "6d2773c0-ec74-4bee-bd37-c55292a20991",
        "authorId" : "a80e0991-f7b1-415a-8d2e-ab615cc6ef4a",
        "body" : "> I don't know any database that supports creating tables with null/void type column, so this change is not for hive compatibility but for reasonable SQL semantic.\r\n> \r\n> I agree this is a breaking change that should be at least put in the migration guide. A legacy config can also be added but I can't find a reasonable use case for a null type column.\r\n\r\nI think the main reason why you would want to support it is when people are using tables / views / temp tables to structure existing workloads. We support NullType type in CTEs, but in the case where people want to reuse the same CTE in multiple queries (i.e., multi-output workloads), they have no choice but to use views or temporary tables. (With DataFrames they'd still be able to reuse the same dataframe for multiple outputs, but in SQL that doesn't work.)\r\n\r\nOne typical use case where you use CTEs to structure your code is if you have multiple sources with different structures that you then UNION ALL together into a single dataset. It is not uncommon for each of the sources to have certain columns that don't apply, and then you write explicit NULLs there. It would be pretty annoying if you had to write explicit casts of those NULLs to the right type in all of those cases.\r\n",
        "createdAt" : "2021-04-14T07:29:21Z",
        "updatedAt" : "2021-04-14T07:29:22Z",
        "lastEditedBy" : "a80e0991-f7b1-415a-8d2e-ab615cc6ef4a",
        "tags" : [
        ]
      },
      {
        "id" : "5cf9935e-9820-4522-96be-c882dcf35ddb",
        "parentId" : "6d2773c0-ec74-4bee-bd37-c55292a20991",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "@bart-samwel this makes sense, shall we also support `CREATE TABLE t(c VOID)`? Your case seems like CTAS only.",
        "createdAt" : "2021-04-14T07:41:00Z",
        "updatedAt" : "2021-04-14T07:41:01Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "2785cebd-f06e-4d35-9fa6-9d396cd9a635",
        "parentId" : "6d2773c0-ec74-4bee-bd37-c55292a20991",
        "authorId" : "a80e0991-f7b1-415a-8d2e-ab615cc6ef4a",
        "body" : "> @bart-samwel this makes sense, shall we also support `CREATE TABLE t(c VOID)`? Your case seems like CTAS only.\r\n\r\nI think the `CREATE TABLE` case with explicit types is not very useful, but it could be useful if there were tools that get a table's schema and then try to recreate it, e.g. for mocking purposes. Probably best to be orthogonal here.",
        "createdAt" : "2021-04-14T08:03:17Z",
        "updatedAt" : "2021-04-14T08:03:18Z",
        "lastEditedBy" : "a80e0991-f7b1-415a-8d2e-ab615cc6ef4a",
        "tags" : [
        ]
      },
      {
        "id" : "58d8c357-e284-4d86-8395-824cda4b9cfc",
        "parentId" : "6d2773c0-ec74-4bee-bd37-c55292a20991",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "@LantaoJin do you have time to fix it? I think we can simply remove the null type check and add a few tests with both in-memory and hive catalog.",
        "createdAt" : "2021-04-14T08:19:05Z",
        "updatedAt" : "2021-04-14T08:19:05Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "9ad57d17bac47ea0f801004ec0aba9197e631bc7",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +272,276 @@    case c @ CreateTableStatement(\n         SessionCatalogAndTable(catalog, tbl), _, _, _, _, _, _, _, _, _) =>\n      assertNoNullTypeInSchema(c.tableSchema)\n      val provider = c.provider.getOrElse(conf.defaultDataSourceName)\n      if (!isV2Provider(provider)) {"
  },
  {
    "id" : "bf83d022-7977-42e2-8a00-51c5f781e307",
    "prId" : 27838,
    "prUrl" : "https://github.com/apache/spark/pull/27838#pullrequestreview-371662412",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "494126a3-226e-4fdb-9aa8-20183d8c1e8f",
        "parentId" : null,
        "authorId" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "body" : "We shouldn't do this change. Since the default provider of a catalog may be different",
        "createdAt" : "2020-03-06T19:27:35Z",
        "updatedAt" : "2020-03-06T20:11:23Z",
        "lastEditedBy" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "tags" : [
        ]
      },
      {
        "id" : "64185e6b-6745-40ec-9f04-4f2e6ed6ff1d",
        "parentId" : "494126a3-226e-4fdb-9aa8-20183d8c1e8f",
        "authorId" : "8fec0990-98ca-4639-aa0d-4d79de603820",
        "body" : "but this is the session catalog? Shouldn't it always use `defaultDataSourceName` to be coherent with sql? ",
        "createdAt" : "2020-03-06T19:31:05Z",
        "updatedAt" : "2020-03-06T20:11:23Z",
        "lastEditedBy" : "8fec0990-98ca-4639-aa0d-4d79de603820",
        "tags" : [
        ]
      },
      {
        "id" : "09d841a3-d883-4bc4-afee-736c74462fbe",
        "parentId" : "494126a3-226e-4fdb-9aa8-20183d8c1e8f",
        "authorId" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "body" : "I think that implementation needs to live within `V2SessionCatalog`, not here",
        "createdAt" : "2020-03-06T19:50:10Z",
        "updatedAt" : "2020-03-06T20:11:23Z",
        "lastEditedBy" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "tags" : [
        ]
      },
      {
        "id" : "13990f45-c3d8-446c-8001-e309dcdfa9f2",
        "parentId" : "494126a3-226e-4fdb-9aa8-20183d8c1e8f",
        "authorId" : "8fec0990-98ca-4639-aa0d-4d79de603820",
        "body" : "The create table path is already doing this. Shall I change that too then? https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveSessionCatalog.scala#L281",
        "createdAt" : "2020-03-06T20:11:10Z",
        "updatedAt" : "2020-03-06T20:51:54Z",
        "lastEditedBy" : "8fec0990-98ca-4639-aa0d-4d79de603820",
        "tags" : [
        ]
      },
      {
        "id" : "eb547910-02b1-4814-807e-4b1c29b5b4c9",
        "parentId" : "494126a3-226e-4fdb-9aa8-20183d8c1e8f",
        "authorId" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "body" : "hmm... ðŸ¤” @rdblue thoughts?",
        "createdAt" : "2020-03-06T21:43:33Z",
        "updatedAt" : "2020-03-06T21:43:33Z",
        "lastEditedBy" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "tags" : [
        ]
      },
      {
        "id" : "c794cf95-80ac-4aac-a0e0-e2bde526e013",
        "parentId" : "494126a3-226e-4fdb-9aa8-20183d8c1e8f",
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "I think this should not use Spark's default. The default provider should be determined by the catalog, not by Spark. For example, if my default provider is `parquet` then how should a JDBC catalog interpret this?",
        "createdAt" : "2020-03-06T22:51:31Z",
        "updatedAt" : "2020-03-06T22:51:31Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      },
      {
        "id" : "7a43fbc2-a424-4a1c-be0f-48fd0d12e873",
        "parentId" : "494126a3-226e-4fdb-9aa8-20183d8c1e8f",
        "authorId" : "8fec0990-98ca-4639-aa0d-4d79de603820",
        "body" : "Sounds reasonable, though I think the AST builder currently always requires a provider in the sql query? I can make the change for create and replace here to pass None in the properties if not specified. @cloud-fan thoughts?",
        "createdAt" : "2020-03-06T23:06:52Z",
        "updatedAt" : "2020-03-06T23:06:52Z",
        "lastEditedBy" : "8fec0990-98ca-4639-aa0d-4d79de603820",
        "tags" : [
        ]
      },
      {
        "id" : "5b21bad3-7c75-403d-be7d-37a7557025e3",
        "parentId" : "494126a3-226e-4fdb-9aa8-20183d8c1e8f",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "It's better to put the default provider in `V2SessionCatalog`, but the problem here is: this rule need to know the provider and see if it's v2, and then pick v1 or v2 command.",
        "createdAt" : "2020-03-09T05:06:13Z",
        "updatedAt" : "2020-03-09T05:06:13Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "4bdc6e33-3879-4e29-a144-3e4fc93e6ff4",
        "parentId" : "494126a3-226e-4fdb-9aa8-20183d8c1e8f",
        "authorId" : "8fec0990-98ca-4639-aa0d-4d79de603820",
        "body" : "Maybe `CatalogPlugin` should expose an interface for the default provider? It's not so clear to me what's the relationship between provider and catalog. Should every catalog have a provider?",
        "createdAt" : "2020-03-09T20:13:11Z",
        "updatedAt" : "2020-03-09T20:13:11Z",
        "lastEditedBy" : "8fec0990-98ca-4639-aa0d-4d79de603820",
        "tags" : [
        ]
      },
      {
        "id" : "9709989e-b5db-4a62-9055-09d809d73b36",
        "parentId" : "494126a3-226e-4fdb-9aa8-20183d8c1e8f",
        "authorId" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "body" : "A default provider doesn't make sense for CatalogPlugin, but it can make sense for a `TableCatalog`. V2SessionCatalog then can use the `conf.defaultDataSourceName`. WDYT?\r\n",
        "createdAt" : "2020-03-09T20:59:35Z",
        "updatedAt" : "2020-03-09T20:59:36Z",
        "lastEditedBy" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "tags" : [
        ]
      },
      {
        "id" : "bc80a44a-9c22-419b-8cf7-c87137db9b66",
        "parentId" : "494126a3-226e-4fdb-9aa8-20183d8c1e8f",
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "The provider doesn't determine whether the command should be run using v1 or v2, the table and catalog do. The provider for the session catalog matters, but not for any other catalog so I don't think changing either the `TableCatalog` or `CatalogPlugin` API is a good idea.\r\n\r\nSince this is limited to the v2 session catalog, let's come up with a way to delegate to the v2 session catalog for this. That, or set a default.",
        "createdAt" : "2020-03-09T21:20:17Z",
        "updatedAt" : "2020-03-09T21:20:17Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      },
      {
        "id" : "02e23156-df9e-4d46-961f-cb8506d3fd14",
        "parentId" : "494126a3-226e-4fdb-9aa8-20183d8c1e8f",
        "authorId" : "8fec0990-98ca-4639-aa0d-4d79de603820",
        "body" : "Can we just say that the existing `defaultDataSourceName` is the default provider for v2 session catalog? Or shall I add another conf for that or add another interface for session catalog? ",
        "createdAt" : "2020-03-09T22:34:38Z",
        "updatedAt" : "2020-03-09T22:34:39Z",
        "lastEditedBy" : "8fec0990-98ca-4639-aa0d-4d79de603820",
        "tags" : [
        ]
      },
      {
        "id" : "0302fca0-f277-492d-9348-49b76941776a",
        "parentId" : "494126a3-226e-4fdb-9aa8-20183d8c1e8f",
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "Now that I understand that this problem is scoped to just how we handle the session catalog, I think it's fine to use the v1 default provider here. That's what the v1 and v2 session catalogs would use anyway.",
        "createdAt" : "2020-03-09T22:41:01Z",
        "updatedAt" : "2020-03-09T22:41:01Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      },
      {
        "id" : "ab335ef6-b683-4055-97f1-f6bdad466b24",
        "parentId" : "494126a3-226e-4fdb-9aa8-20183d8c1e8f",
        "authorId" : "8fec0990-98ca-4639-aa0d-4d79de603820",
        "body" : "yup. @brkyvz @cloud-fan Good to go?",
        "createdAt" : "2020-03-09T23:09:30Z",
        "updatedAt" : "2020-03-09T23:09:31Z",
        "lastEditedBy" : "8fec0990-98ca-4639-aa0d-4d79de603820",
        "tags" : [
        ]
      },
      {
        "id" : "e03dd609-c18f-4cef-a031-a53f0a885a38",
        "parentId" : "494126a3-226e-4fdb-9aa8-20183d8c1e8f",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "yea good to go!",
        "createdAt" : "2020-03-10T03:34:02Z",
        "updatedAt" : "2020-03-10T03:34:03Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "ffd7a21ae2735d20fa8b8cac393d51ba37a5f3aa",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +306,310 @@    case c @ ReplaceTableStatement(\n         SessionCatalogAndTable(catalog, tbl), _, _, _, _, _, _, _, _, _) =>\n      val provider = c.provider.getOrElse(conf.defaultDataSourceName)\n      if (!isV2Provider(provider)) {\n        throw new AnalysisException(\"REPLACE TABLE is only supported with v2 tables.\")"
  },
  {
    "id" : "19095ded-5531-4a9d-a38d-3a0f8b8b19a2",
    "prId" : 27838,
    "prUrl" : "https://github.com/apache/spark/pull/27838#pullrequestreview-370571088",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c6af6652-a7e2-4e07-b1e1-5402d710f0c6",
        "parentId" : null,
        "authorId" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "body" : "ditto",
        "createdAt" : "2020-03-06T19:27:42Z",
        "updatedAt" : "2020-03-06T20:11:23Z",
        "lastEditedBy" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "tags" : [
        ]
      }
    ],
    "commit" : "ffd7a21ae2735d20fa8b8cac393d51ba37a5f3aa",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +322,326 @@    case c @ ReplaceTableAsSelectStatement(\n         SessionCatalogAndTable(catalog, tbl), _, _, _, _, _, _, _, _, _) =>\n      val provider = c.provider.getOrElse(conf.defaultDataSourceName)\n      if (!isV2Provider(provider)) {\n        throw new AnalysisException(\"REPLACE TABLE AS SELECT is only supported with v2 tables.\")"
  },
  {
    "id" : "915f9c95-b5cc-450a-956d-561d4068ff62",
    "prId" : 27642,
    "prUrl" : "https://github.com/apache/spark/pull/27642#pullrequestreview-363063697",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fae5e852-2fa1-4435-a76f-189fec072778",
        "parentId" : null,
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "`AnalyzeColumnCommand` actually supports temp view.",
        "createdAt" : "2020-02-21T04:28:17Z",
        "updatedAt" : "2020-02-24T17:15:55Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      },
      {
        "id" : "39f23f48-02d5-4b56-9e08-30ad7a4eaa76",
        "parentId" : "fae5e852-2fa1-4435-a76f-189fec072778",
        "authorId" : "8fec0990-98ca-4639-aa0d-4d79de603820",
        "body" : "Just want to check if I understand this correctly. This reason why we need to make this change is that `parseV1Table` would accidentally add currentName as the namespace for a temp view which is incorrect because temp view shouldn't have a namespace?",
        "createdAt" : "2020-02-22T00:42:33Z",
        "updatedAt" : "2020-02-24T17:15:55Z",
        "lastEditedBy" : "8fec0990-98ca-4639-aa0d-4d79de603820",
        "tags" : [
        ]
      },
      {
        "id" : "1e15889b-ee6a-4028-a083-ebc81b5aa3ff",
        "parentId" : "fae5e852-2fa1-4435-a76f-189fec072778",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "yes, and this command supports temp views.",
        "createdAt" : "2020-02-23T03:42:16Z",
        "updatedAt" : "2020-02-24T17:15:55Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "eb0ebf1990c126d352c61f757d68686faf9c7f4b",
    "line" : 29,
    "diffHunk" : "@@ -1,1 +386,390 @@\n    case AnalyzeColumnStatement(tbl, columnNames, allColumns) =>\n      val v1TableName = parseTempViewOrV1Table(tbl, \"ANALYZE TABLE\")\n      AnalyzeColumnCommand(v1TableName.asTableIdentifier, columnNames, allColumns)\n"
  },
  {
    "id" : "6b4b67aa-b252-4592-8d78-b538090ba276",
    "prId" : 27642,
    "prUrl" : "https://github.com/apache/spark/pull/27642#pullrequestreview-362393338",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b893664f-16a5-4873-b3f8-226771d451ef",
        "parentId" : null,
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "`ShowColumnsCommand` works with temp views as well.",
        "createdAt" : "2020-02-21T04:30:15Z",
        "updatedAt" : "2020-02-24T17:15:55Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "eb0ebf1990c126d352c61f757d68686faf9c7f4b",
    "line" : 61,
    "diffHunk" : "@@ -1,1 +451,455 @@      }\n      val sql = \"SHOW COLUMNS\"\n      val v1TableName = parseTempViewOrV1Table(nameParts, sql).asTableIdentifier\n      val resolver = conf.resolver\n      val db = ns match {"
  },
  {
    "id" : "f7ba962c-69b7-4503-9a15-d822cb421991",
    "prId" : 27642,
    "prUrl" : "https://github.com/apache/spark/pull/27642#pullrequestreview-363136648",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3d995584-53fe-494b-87c6-18e1115a9d08",
        "parentId" : null,
        "authorId" : "8fec0990-98ca-4639-aa0d-4d79de603820",
        "body" : "Can we perhaps add a comment here why this `tbl.length == 1` is necessary?",
        "createdAt" : "2020-02-22T00:35:02Z",
        "updatedAt" : "2020-02-24T17:15:55Z",
        "lastEditedBy" : "8fec0990-98ca-4639-aa0d-4d79de603820",
        "tags" : [
        ]
      },
      {
        "id" : "c09e9fcd-97b6-454d-a414-aed2d729ea86",
        "parentId" : "3d995584-53fe-494b-87c6-18e1115a9d08",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "Added.",
        "createdAt" : "2020-02-23T04:10:58Z",
        "updatedAt" : "2020-02-24T17:15:55Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      },
      {
        "id" : "a0886cf7-37fc-4c7f-b0c2-d958c72863ca",
        "parentId" : "3d995584-53fe-494b-87c6-18e1115a9d08",
        "authorId" : "8fec0990-98ca-4639-aa0d-4d79de603820",
        "body" : "Actually, the part confuses me is when would `tbl.length != 1`. Is it because sometimes we would keep the namespace in the table name? It just feels strange that ns might not represent the actual namespace for the table.",
        "createdAt" : "2020-02-23T23:05:20Z",
        "updatedAt" : "2020-02-24T17:15:55Z",
        "lastEditedBy" : "8fec0990-98ca-4639-aa0d-4d79de603820",
        "tags" : [
        ]
      },
      {
        "id" : "6b9545f7-cd2e-483c-a6b4-e9aa1a953aa3",
        "parentId" : "3d995584-53fe-494b-87c6-18e1115a9d08",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "Yea, you can do the following: `SHOW COLUMNS FROM db1.t1 IN db1`",
        "createdAt" : "2020-02-24T00:26:10Z",
        "updatedAt" : "2020-02-24T17:15:55Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "eb0ebf1990c126d352c61f757d68686faf9c7f4b",
    "line" : 54,
    "diffHunk" : "@@ -1,1 +445,449 @@      // Use namespace only if table name doesn't specify it. If namespace is already specified\n      // in the table name, it's checked against the given namespace below.\n      val nameParts = if (ns.isDefined && tbl.length == 1) {\n        ns.get ++ tbl\n      } else {"
  },
  {
    "id" : "6d78827e-8231-492f-abe0-a00e1a382fbd",
    "prId" : 27550,
    "prUrl" : "https://github.com/apache/spark/pull/27550#pullrequestreview-358691035",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "69933673-2251-4745-a49d-2d63f5d3be55",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I can remove this, but then I need to update many tests slightly to update the error message. We can leave it to 3.1.",
        "createdAt" : "2020-02-14T03:16:34Z",
        "updatedAt" : "2020-02-14T10:56:14Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "f4a3d08d096ee44d2fc7bf5d23f6223f9700865a",
    "line" : 269,
    "diffHunk" : "@@ -1,1 +640,644 @@          // If there is only one name part, it means the current catalog is the session catalog.\n          // Here we return the original name part, to keep the error message unchanged for\n          // v1 commands.\n          Some(catalog -> nameParts)\n        } else {"
  },
  {
    "id" : "c4de6012-c947-488e-8990-c94e418d23e4",
    "prId" : 27550,
    "prUrl" : "https://github.com/apache/spark/pull/27550#pullrequestreview-358708629",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bf512aba-8a15-4418-a649-b41f6a97fe63",
        "parentId" : null,
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "Since `tbl` is from `SessionCatalogAndTable`, `tbl` may have the current namespace? And if the current namespace is not empty, this will always return false?",
        "createdAt" : "2020-02-14T03:41:04Z",
        "updatedAt" : "2020-02-14T10:56:14Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      },
      {
        "id" : "cfe85287-ad38-432b-aaaa-095922a1348f",
        "parentId" : "bf512aba-8a15-4418-a649-b41f6a97fe63",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "You have a good point here. I think this is an existing bug. Here we look up table first (call `loadTable`) then look up temp view. We should look up temp view first to retain the behavior of Spark 2.4. @imback82 can you help fix it later?",
        "createdAt" : "2020-02-14T04:24:22Z",
        "updatedAt" : "2020-02-14T10:56:14Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "9bb46160-2779-430c-8a69-afb4c0da095a",
        "parentId" : "bf512aba-8a15-4418-a649-b41f6a97fe63",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "Yes, will do after this PR.",
        "createdAt" : "2020-02-14T04:38:10Z",
        "updatedAt" : "2020-02-14T10:56:14Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "f4a3d08d096ee44d2fc7bf5d23f6223f9700865a",
    "line" : 40,
    "diffHunk" : "@@ -1,1 +235,239 @@          DescribeColumnCommand(tbl.asTableIdentifier, colNameParts, isExtended)\n      }.getOrElse {\n        if (isTempView(tbl)) {\n          // v1 DESCRIBE COLUMN supports temp view.\n          DescribeColumnCommand(tbl.asTableIdentifier, colNameParts, isExtended)"
  },
  {
    "id" : "51c21454-078b-4103-9b07-2c1b1d6e8ad1",
    "prId" : 27550,
    "prUrl" : "https://github.com/apache/spark/pull/27550#pullrequestreview-358709143",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "648c3084-8861-4d0e-b50b-9a3bd20089be",
        "parentId" : null,
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "Unrelated question, what was the reason to allow `spark_catalog.t` to be `spark_catalog.<cur_db>.t`? (different behavior from v2 catalogs)",
        "createdAt" : "2020-02-14T03:49:55Z",
        "updatedAt" : "2020-02-14T10:56:14Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      },
      {
        "id" : "fd144914-4168-4ded-8ab7-4b138ff3f924",
        "parentId" : "648c3084-8861-4d0e-b50b-9a3bd20089be",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I noticed this well and think this is not intentional. But this is a different topic and we have many tests using `spark_catalog.t`.\r\n\r\nWe can open another PR to forbid it if we think we shouldn't support this feature.",
        "createdAt" : "2020-02-14T04:26:38Z",
        "updatedAt" : "2020-02-14T10:56:14Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "b7a49eca-26d2-4396-8553-0a5e508881b2",
        "parentId" : "648c3084-8861-4d0e-b50b-9a3bd20089be",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "Got it, thanks for the explanation. It makes more sense to forbid it to keep the behavior consistent.",
        "createdAt" : "2020-02-14T04:40:36Z",
        "updatedAt" : "2020-02-14T10:56:14Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "f4a3d08d096ee44d2fc7bf5d23f6223f9700865a",
    "line" : 283,
    "diffHunk" : "@@ -1,1 +654,658 @@      case SessionCatalogAndTable(_, tbl) =>\n        if (nameParts.head == CatalogManager.SESSION_CATALOG_NAME && tbl.length == 1) {\n          // For name parts like `spark_catalog.t`, we need to fill in the default database so\n          // that the caller side won't treat it as a temp view.\n          Some(Seq(catalogManager.v1SessionCatalog.getCurrentDatabase, tbl.head))"
  },
  {
    "id" : "bc54c0d1-1473-4618-bb3f-604ca62bfbd0",
    "prId" : 27550,
    "prUrl" : "https://github.com/apache/spark/pull/27550#pullrequestreview-358865132",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3b05aed7-afd8-4323-b246-b9a4c2050ad3",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "This is consistent with https://github.com/apache/spark/pull/27550/files#diff-2e07be4d73605cb1941153441a0c0c14L536",
        "createdAt" : "2020-02-14T11:01:36Z",
        "updatedAt" : "2020-02-14T11:01:37Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "f4a3d08d096ee44d2fc7bf5d23f6223f9700865a",
    "line" : 177,
    "diffHunk" : "@@ -1,1 +536,540 @@        // temp func doesn't belong to any catalog and we shouldn't resolve catalog in the name.\n        val database = if (nameParts.length > 2) {\n          throw new AnalysisException(s\"Unsupported function name '${nameParts.quoted}'\")\n        } else if (nameParts.length == 2) {\n          Some(nameParts.head)"
  },
  {
    "id" : "c6411716-95b9-499a-aacd-64171ccd5c1f",
    "prId" : 27444,
    "prUrl" : "https://github.com/apache/spark/pull/27444#pullrequestreview-353481315",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fe3556ca-de70-4a4d-aff9-cff0a2f51474",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "`builder.putString(HIVE_TYPE_STRING, dataType)`?",
        "createdAt" : "2020-02-05T05:35:13Z",
        "updatedAt" : "2020-02-06T00:09:08Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "9809556f-59ba-4510-af2f-6ea348d4d9b9",
        "parentId" : "fe3556ca-de70-4a4d-aff9-cff0a2f51474",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "good catch. it should be `builder.putString(HIVE_TYPE_STRING, dataType.catalogString)`",
        "createdAt" : "2020-02-05T05:58:13Z",
        "updatedAt" : "2020-02-06T00:09:08Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "93de5f27906a2292d1749f684184877ed207a9c2",
    "line" : 32,
    "diffHunk" : "@@ -1,1 +103,107 @@          if (dataType != cleanedDataType) {\n            builder.putString(HIVE_TYPE_STRING, dataType.catalogString)\n          }\n          val newColumn = StructField(\n            colName,"
  },
  {
    "id" : "b6367073-5443-4ece-84a6-9bfca76644ed",
    "prId" : 27187,
    "prUrl" : "https://github.com/apache/spark/pull/27187#pullrequestreview-342955112",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "24fc8f89-2397-4b49-86e0-732bdc8c87e8",
        "parentId" : null,
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "looks clean ðŸ‘ ",
        "createdAt" : "2020-01-15T02:59:00Z",
        "updatedAt" : "2020-01-15T04:45:49Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "562232270f24c021c962015b4314dac38e40906b",
    "line" : 35,
    "diffHunk" : "@@ -1,1 +220,224 @@    // Use v1 command to describe (temp) view, as v2 catalog doesn't support view yet.\n    case DescribeRelation(ResolvedView(ident), partitionSpec, isExtended) =>\n      DescribeTableCommand(ident.asTableIdentifier, partitionSpec, isExtended)\n\n    case DescribeColumnStatement("
  },
  {
    "id" : "329d1ede-f5dc-427f-aaad-cd7a6b141684",
    "prId" : 26890,
    "prUrl" : "https://github.com/apache/spark/pull/26890#pullrequestreview-332288406",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ece5f394-2cdf-4a30-a741-72dd8aa9346a",
        "parentId" : null,
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "I would return `FunctionIdentifier` to match the function name, and also using `FunctionIdentifier` makes the intention / usage clear on the caller site.",
        "createdAt" : "2019-12-15T18:59:45Z",
        "updatedAt" : "2020-01-06T19:40:25Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      },
      {
        "id" : "9e4161a2-b84f-4ba1-9857-eb9d3e3e7eda",
        "parentId" : "ece5f394-2cdf-4a30-a741-72dd8aa9346a",
        "authorId" : "51be1be0-7c72-4f32-a21f-91b1707ec363",
        "body" : "oh, yes, it sounds good!",
        "createdAt" : "2019-12-15T21:26:17Z",
        "updatedAt" : "2020-01-06T19:40:25Z",
        "lastEditedBy" : "51be1be0-7c72-4f32-a21f-91b1707ec363",
        "tags" : [
        ]
      }
    ],
    "commit" : "d9eb4410d5c77234746d13132f3cad5aa6092d1d",
    "line" : 75,
    "diffHunk" : "@@ -1,1 +520,524 @@  }\n\n  private def parseSessionCatalogFunctionIdentifier(\n      sql: String,\n      catalog: CatalogPlugin,"
  },
  {
    "id" : "771ab52e-2064-47f4-a698-6685e1a415ee",
    "prId" : 26890,
    "prUrl" : "https://github.com/apache/spark/pull/26890#pullrequestreview-333046216",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cd49f3f1-a749-4639-bfa7-cd7adcf3dfeb",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "This is not caused by this PR. This error message looks too general, and I do not see much info from it.\r\n\r\nMaybe we can make it clearer? Like v1 function name should not have multiple namespaces?",
        "createdAt" : "2019-12-16T22:30:52Z",
        "updatedAt" : "2020-01-06T19:40:25Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "42230108-64d6-4746-a794-4e84f8c78321",
        "parentId" : "cd49f3f1-a749-4639-bfa7-cd7adcf3dfeb",
        "authorId" : "51be1be0-7c72-4f32-a21f-91b1707ec363",
        "body" : "In the show columns statement we decide this message\r\n> Namespace name should have only one part if specified\r\n\r\nHow about the same message or similar? Both are clearer",
        "createdAt" : "2019-12-17T05:56:24Z",
        "updatedAt" : "2020-01-06T19:40:25Z",
        "lastEditedBy" : "51be1be0-7c72-4f32-a21f-91b1707ec363",
        "tags" : [
        ]
      }
    ],
    "commit" : "d9eb4410d5c77234746d13132f3cad5aa6092d1d",
    "line" : 84,
    "diffHunk" : "@@ -1,1 +529,533 @@        case Seq(fn) => FunctionIdentifier(fn, None)\n        case _ =>\n          throw new AnalysisException(s\"Unsupported function name '${functionIdent.quoted}'\")\n      }\n    } else {"
  }
]