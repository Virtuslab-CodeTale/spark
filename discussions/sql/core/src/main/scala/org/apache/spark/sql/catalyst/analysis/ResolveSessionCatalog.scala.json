[
  {
    "id" : "3edfcc5f-c4ea-4f8b-8ea8-7d2cd3d591da",
    "prId" : 31804,
    "prUrl" : "https://github.com/apache/spark/pull/31804#pullrequestreview-609400754",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "def2e6d2-9bdf-4906-8dfa-42e66d87cce1",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Well, this seems inconsistent with the doc. The current document means `spark.sql.legacy.keepCommandOutputSchema` means the 3.1 or earlier schema, doesn't it?",
        "createdAt" : "2021-03-11T04:54:58Z",
        "updatedAt" : "2021-03-30T07:59:14Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "f2effd52-42c9-4558-91f4-996e3f34bf80",
        "parentId" : "def2e6d2-9bdf-4906-8dfa-42e66d87cce1",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : " Introducing a new `legacy` conf for this behavior change seems kind of trivial and might bring cognition burdens for users. So the config is reused for now and the doc will be updated if it is the right way to go ",
        "createdAt" : "2021-03-11T05:02:45Z",
        "updatedAt" : "2021-03-30T07:59:14Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "0e4c31ef-0b20-4426-882d-48e9ebd2760a",
        "parentId" : "def2e6d2-9bdf-4906-8dfa-42e66d87cce1",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "This is not reused technically. If we reuse the existing conf, this should be `output.length == 4` because it disable this PR and the previous commit simultaneously.\r\n> So the config is reused for now ",
        "createdAt" : "2021-03-11T05:06:57Z",
        "updatedAt" : "2021-03-30T07:59:14Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "b898ded2-5da5-414c-a6e8-8eb73d51e8ff",
        "parentId" : "def2e6d2-9bdf-4906-8dfa-42e66d87cce1",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "> this should be output.length == 4\r\n\r\nindeed, this is true.  `output.head.withName(\"database\") +: output.slice(1, 4)` will cut the `isView` off",
        "createdAt" : "2021-03-11T05:15:30Z",
        "updatedAt" : "2021-03-30T07:59:14Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "3ef5b5f414b68ea149877eaa26618e9787252004",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +367,371 @@        output) =>\n      val newOutput = if (conf.getConf(SQLConf.LEGACY_KEEP_COMMAND_OUTPUT_SCHEMA)) {\n        assert(output.length == 5)\n        ShowTableExtended.getLegacyOutputAttrs\n      } else {"
  },
  {
    "id" : "ff2c3c4e-929f-4170-9d19-2f6d72c690a1",
    "prId" : 31705,
    "prUrl" : "https://github.com/apache/spark/pull/31705#pullrequestreview-635163466",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dd335cb1-d967-4579-a4bd-53d80773e335",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "@AngersZhuuuu  can we add one item in the migration guide for this change?",
        "createdAt" : "2021-04-13T15:58:10Z",
        "updatedAt" : "2021-04-13T15:58:10Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "ca34a2c3-7bb9-4df0-a96b-830ba8b9fbff",
        "parentId" : "dd335cb1-d967-4579-a4bd-53d80773e335",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "Done https://github.com/apache/spark/pull/32155",
        "createdAt" : "2021-04-14T02:23:49Z",
        "updatedAt" : "2021-04-14T02:23:50Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "72345b2351cd447c61b93cc186952e6ae526fdfd",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +181,185 @@\n    case DescribeNamespace(DatabaseInSessionCatalog(db), extended, output) =>\n      val newOutput = if (conf.getConf(SQLConf.LEGACY_KEEP_COMMAND_OUTPUT_SCHEMA)) {\n        assert(output.length == 2)\n        Seq(output.head.withName(\"database_description_item\"),"
  },
  {
    "id" : "c77e215f-4c03-4fc2-822f-f6469de400ad",
    "prId" : 31499,
    "prUrl" : "https://github.com/apache/spark/pull/31499#pullrequestreview-591079091",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a13cc985-2269-4ab1-b9a6-720b170684ff",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Just a question: can we propagate the original `commandName` instead of having `\"MSCK REPAIR TABLE\"`?",
        "createdAt" : "2021-02-15T16:53:13Z",
        "updatedAt" : "2021-02-23T10:22:53Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "4d3623e2-4268-454a-b5b0-e42b79d8311c",
        "parentId" : "a13cc985-2269-4ab1-b9a6-720b170684ff",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Unfortunately, we lost the info in `UnresolvedTable` -> `ResolvedTable` but we can reconstruct the command name from the flags `addPartitions` and `dropPartitions`. Though not the original one because `MSCK REPAIR TABLE table` has `addPartitions = true, dropPartitions  = false` can be re-constructed as `MSCK REPAIR TABLE table ADD PARTITIONS`. Are you ok with that?",
        "createdAt" : "2021-02-16T10:19:25Z",
        "updatedAt" : "2021-02-23T10:22:53Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "fefea57d097e64b5b506dea45aac24c537f27abd",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +382,386 @@        addPartitions,\n        dropPartitions,\n        \"MSCK REPAIR TABLE\")\n\n    case LoadData(ResolvedV1TableIdentifier(ident), path, isLocal, isOverwrite, partition) =>"
  },
  {
    "id" : "8cc9953b-bfc1-436c-83bb-625038dbde37",
    "prId" : 30748,
    "prUrl" : "https://github.com/apache/spark/pull/30748#pullrequestreview-550857954",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ae494e2a-ab16-4018-97e7-10ac853fd634",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "I leaved `retainData` in `AlterTableDropPartitionCommand` because it is set to `true` at:\r\nhttps://github.com/apache/spark/blob/7c59aeeef4c571838bd291079f9b804d6f546487/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand.scala#L158-L161",
        "createdAt" : "2020-12-12T19:59:06Z",
        "updatedAt" : "2020-12-12T21:57:19Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "96064653043a18f7f826218681590730cd753486",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +476,480 @@        ifExists,\n        purge,\n        retainData = false)\n\n    case AlterTableSerDePropertiesStatement(tbl, serdeClassName, serdeProperties, partitionSpec) =>"
  },
  {
    "id" : "3caaa64c-e3a5-4153-82a9-2b961b9f1d37",
    "prId" : 30554,
    "prUrl" : "https://github.com/apache/spark/pull/30554#pullrequestreview-542398594",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9c50c098-ca4e-409e-bd3b-20bb8f95e2b9",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I would even think about printing a warning that creating a Hive table by default is deprecated, and this configuration will be disabled by default in the future because Spark should create a Spark table.",
        "createdAt" : "2020-12-02T00:07:12Z",
        "updatedAt" : "2020-12-02T08:50:59Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "bff924de21b8af2da17f31591768ce02c784bd43",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +640,644 @@      //   1. `LEGACY_CREATE_HIVE_TABLE_BY_DEFAULT` is false, or\n      //   2. It's a CTAS and `conf.convertCTAS` is true.\n      val createHiveTableByDefault = conf.getConf(SQLConf.LEGACY_CREATE_HIVE_TABLE_BY_DEFAULT)\n      if (!createHiveTableByDefault || (ctas && conf.convertCTAS)) {\n        (nonHiveStorageFormat, conf.defaultDataSourceName)"
  },
  {
    "id" : "269ebefc-3aec-469b-9e27-5286d990f694",
    "prId" : 30554,
    "prUrl" : "https://github.com/apache/spark/pull/30554#pullrequestreview-543086657",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d2ab7946-3084-4608-9dcc-0e07b743ea5e",
        "parentId" : null,
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "Should this mark `convertCTAS` as deprecated since it is superseded by the new config?",
        "createdAt" : "2020-12-02T17:37:33Z",
        "updatedAt" : "2020-12-02T17:37:33Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      },
      {
        "id" : "9ddacded-8512-48e4-965a-ab11935dd66b",
        "parentId" : "d2ab7946-3084-4608-9dcc-0e07b743ea5e",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "yea, I can do it in a follow-up.",
        "createdAt" : "2020-12-02T17:44:25Z",
        "updatedAt" : "2020-12-02T17:44:25Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "bff924de21b8af2da17f31591768ce02c784bd43",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +641,645 @@      //   2. It's a CTAS and `conf.convertCTAS` is true.\n      val createHiveTableByDefault = conf.getConf(SQLConf.LEGACY_CREATE_HIVE_TABLE_BY_DEFAULT)\n      if (!createHiveTableByDefault || (ctas && conf.convertCTAS)) {\n        (nonHiveStorageFormat, conf.defaultDataSourceName)\n      } else {"
  },
  {
    "id" : "10734b03-692f-41ef-8960-8e9b3f8f6086",
    "prId" : 30403,
    "prUrl" : "https://github.com/apache/spark/pull/30403#pullrequestreview-533097554",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "128eb3f5-01a4-4bb2-a87b-75bba3d1fb4c",
        "parentId" : null,
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "A refactoring not related to this PR.",
        "createdAt" : "2020-11-18T04:33:54Z",
        "updatedAt" : "2020-11-27T21:30:33Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "7e788cea5c2dd03f71ee30b0106e04d7f036f30f",
    "line" : 26,
    "diffHunk" : "@@ -1,1 +548,552 @@      }\n\n    case ShowTableProperties(ResolvedV1TableOrViewIdentifier(ident), propertyKey) =>\n      ShowTablePropertiesCommand(ident.asTableIdentifier, propertyKey)\n"
  },
  {
    "id" : "75291153-6050-4e40-98a7-bc4b5159aa89",
    "prId" : 30270,
    "prUrl" : "https://github.com/apache/spark/pull/30270#pullrequestreview-524830999",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "55a4727e-8227-4f05-b170-c308c71d2170",
        "parentId" : null,
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "Note that changes related to `RefreshTable`, `DropTable`, and `ShowTableProperties` are to use the new extractor `ResolvedV1TableIdentifier`.",
        "createdAt" : "2020-11-06T03:20:03Z",
        "updatedAt" : "2020-11-09T18:50:13Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "d01a95e11896388f0978d6d102c363f3a45c28a7",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +314,318 @@\n    case RefreshTable(ResolvedV1TableIdentifier(ident)) =>\n      RefreshTableCommand(ident.asTableIdentifier)\n\n    case RefreshTable(r: ResolvedView) =>"
  },
  {
    "id" : "54a7640f-443b-4f61-a0cf-51d0581d8da5",
    "prId" : 30229,
    "prUrl" : "https://github.com/apache/spark/pull/30229#pullrequestreview-522193606",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d34a2d8a-2f3a-43e0-8db1-cbbd1c946ba5",
        "parentId" : null,
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "Applying `ResolvedV1TableOrViewIdentifier` extractor to existing code to simplify.",
        "createdAt" : "2020-11-03T05:13:30Z",
        "updatedAt" : "2020-11-03T18:46:05Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "c80e8eb5828ff68a1ebab4f9ee56b55b852ba7d0",
    "line" : 33,
    "diffHunk" : "@@ -1,1 +260,264 @@\n    case DescribeColumn(ResolvedV1TableOrViewIdentifier(ident), colNameParts, isExtended) =>\n      DescribeColumnCommand(ident.asTableIdentifier, colNameParts, isExtended)\n\n    // For CREATE TABLE [AS SELECT], we should use the v1 command if the catalog is resolved to the"
  },
  {
    "id" : "5d9bfa59-5795-4f54-81de-4d0a06b4d91f",
    "prId" : 30079,
    "prUrl" : "https://github.com/apache/spark/pull/30079#pullrequestreview-518153344",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f869b9ef-cc4b-47ce-b717-b884f13759f3",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "nit: shall we add an assert here to make sure it's not temp view?",
        "createdAt" : "2020-10-26T09:28:59Z",
        "updatedAt" : "2020-10-27T21:04:52Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "c6edfa5a-1e25-4772-a280-b1b660cd1f5d",
        "parentId" : "f869b9ef-cc4b-47ce-b717-b884f13759f3",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "You mean to make sure it's a temp view?\r\n\r\nFor v1, there are few checks for handling views/temp views inside `DropTableCommand`:\r\nhttps://github.com/apache/spark/blob/11bbb130df7b083f42acf0207531efe3912d89eb/sql/core/src/main/scala/org/apache/spark/sql/execution/command/ddl.scala#L226-L229\r\n\r\nDo you want me to copy the checks here?",
        "createdAt" : "2020-10-26T15:24:41Z",
        "updatedAt" : "2020-10-27T21:04:52Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      },
      {
        "id" : "967eed43-5204-4ca3-9ffd-ae92d8061229",
        "parentId" : "f869b9ef-cc4b-47ce-b717-b884f13759f3",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "just `assert(r.isTemp)`",
        "createdAt" : "2020-10-27T15:06:55Z",
        "updatedAt" : "2020-10-27T21:04:52Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "972a61ea-1211-4692-89eb-a6a60e3cbad0",
        "parentId" : "f869b9ef-cc4b-47ce-b717-b884f13759f3",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "If we put the assert here, the output call stack doesn't seem to be very helpful to the user:\r\n```\r\njava.lang.AssertionError: assertion failed\r\n\tat scala.Predef$.assert(Predef.scala:208)\r\n\tat org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog$$anonfun$apply$1.applyOrElse(ResolveSessionCatalog.scala:376)\r\n\tat org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog$$anonfun$apply$1.applyOrElse(ResolveSessionCatalog.scala:47)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$3(AnalysisHelper.scala:90)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)\r\n```\r\n, also there is an existing test around it: \r\nhttps://github.com/apache/spark/blob/3f2a2b5fe6ada37ef86f00737387e6cf2496df74/sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/HiveDDLSuite.scala#L1057\r\n\r\nI am throwing an exception instead similar to how `DropTableCommand` is handling. Please let me know what you think about this.",
        "createdAt" : "2020-10-27T21:09:59Z",
        "updatedAt" : "2020-10-27T21:09:59Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      }
    ],
    "commit" : "dfc44923d10fa7cbb5cd6cde4dfcffa19a9e994d",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +373,377 @@\n    // v1 DROP TABLE supports temp view.\n    case DropTable(r: ResolvedView, ifExists, purge) =>\n      if (!r.isTemp) {\n        throw new AnalysisException("
  },
  {
    "id" : "f73dc153-a403-4d9c-8b75-409d6c5c4522",
    "prId" : 30079,
    "prUrl" : "https://github.com/apache/spark/pull/30079#pullrequestreview-518338984",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e0ffc66d-4ce3-4dc4-bd23-b591d1b4e392",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Currently, it's duplicated with the check in `DropTableCommand`. I think it's OK as we want to move to v2 commands eventually.",
        "createdAt" : "2020-10-28T05:44:12Z",
        "updatedAt" : "2020-10-28T05:44:12Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "dfc44923d10fa7cbb5cd6cde4dfcffa19a9e994d",
    "line" : 29,
    "diffHunk" : "@@ -1,1 +374,378 @@    // v1 DROP TABLE supports temp view.\n    case DropTable(r: ResolvedView, ifExists, purge) =>\n      if (!r.isTemp) {\n        throw new AnalysisException(\n          \"Cannot drop a view with DROP TABLE. Please use DROP VIEW instead\")"
  }
]