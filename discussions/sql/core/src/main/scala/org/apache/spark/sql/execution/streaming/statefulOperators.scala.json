[
  {
    "id" : "a3ffcebb-2bdc-46ba-8fe5-5c6a7d223f73",
    "prId" : 33081,
    "prUrl" : "https://github.com/apache/spark/pull/33081#pullrequestreview-707925165",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6a69d51a-3d51-40ad-8b16-ee0a93c0ec0b",
        "parentId" : null,
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "Seems we can simply count the time-spending of `removeByValueCondition` as removalStartTime? Then we can avoid calculating the time manually.",
        "createdAt" : "2021-07-15T15:42:06Z",
        "updatedAt" : "2021-07-15T16:05:46Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      },
      {
        "id" : "744e8cc0-bf6e-48ca-b0e5-bf89c6153401",
        "parentId" : "6a69d51a-3d51-40ad-8b16-ee0a93c0ec0b",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "removeByValueCondition returns \"iterator\" which is evaluated \"lazily\".",
        "createdAt" : "2021-07-16T00:58:38Z",
        "updatedAt" : "2021-07-16T00:58:38Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "bbade3501f16e9437ba9af4feca3e2029785d273",
    "line" : 152,
    "diffHunk" : "@@ -1,1 +652,656 @@          new NextIterator[InternalRow] {\n            private val removedIter = stateManager.removeByValueCondition(\n              store, watermarkPredicateForData.get.eval)\n\n            override protected def getNext(): InternalRow = {"
  },
  {
    "id" : "40c3b78a-bdf4-411f-936c-2cd20b17edb5",
    "prId" : 33081,
    "prUrl" : "https://github.com/apache/spark/pull/33081#pullrequestreview-707925608",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "531be757-acf5-4c46-8693-7cf7e9d87507",
        "parentId" : null,
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "`shouldRunAnotherBatch` is never used?",
        "createdAt" : "2021-07-15T15:50:12Z",
        "updatedAt" : "2021-07-15T16:05:46Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      },
      {
        "id" : "49a19291-2e73-443e-b6c1-140534a652de",
        "parentId" : "531be757-acf5-4c46-8693-7cf7e9d87507",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "It's for Spark to determine whether no-data batch is needed or not.",
        "createdAt" : "2021-07-16T00:59:57Z",
        "updatedAt" : "2021-07-16T00:59:57Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "bbade3501f16e9437ba9af4feca3e2029785d273",
    "line" : 226,
    "diffHunk" : "@@ -1,1 +726,730 @@  }\n\n  override def shouldRunAnotherBatch(newMetadata: OffsetSeqMetadata): Boolean = {\n    (outputMode.contains(Append) || outputMode.contains(Update)) &&\n      eventTimeWatermark.isDefined &&"
  }
]