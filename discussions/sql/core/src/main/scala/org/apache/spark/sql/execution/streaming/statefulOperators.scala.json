[
  {
    "id" : "a3ffcebb-2bdc-46ba-8fe5-5c6a7d223f73",
    "prId" : 33081,
    "prUrl" : "https://github.com/apache/spark/pull/33081#pullrequestreview-707925165",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6a69d51a-3d51-40ad-8b16-ee0a93c0ec0b",
        "parentId" : null,
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "Seems we can simply count the time-spending of `removeByValueCondition` as removalStartTime? Then we can avoid calculating the time manually.",
        "createdAt" : "2021-07-15T15:42:06Z",
        "updatedAt" : "2021-07-15T16:05:46Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      },
      {
        "id" : "744e8cc0-bf6e-48ca-b0e5-bf89c6153401",
        "parentId" : "6a69d51a-3d51-40ad-8b16-ee0a93c0ec0b",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "removeByValueCondition returns \"iterator\" which is evaluated \"lazily\".",
        "createdAt" : "2021-07-16T00:58:38Z",
        "updatedAt" : "2021-07-16T00:58:38Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "bbade3501f16e9437ba9af4feca3e2029785d273",
    "line" : 152,
    "diffHunk" : "@@ -1,1 +652,656 @@          new NextIterator[InternalRow] {\n            private val removedIter = stateManager.removeByValueCondition(\n              store, watermarkPredicateForData.get.eval)\n\n            override protected def getNext(): InternalRow = {"
  },
  {
    "id" : "40c3b78a-bdf4-411f-936c-2cd20b17edb5",
    "prId" : 33081,
    "prUrl" : "https://github.com/apache/spark/pull/33081#pullrequestreview-707925608",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "531be757-acf5-4c46-8693-7cf7e9d87507",
        "parentId" : null,
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "`shouldRunAnotherBatch` is never used?",
        "createdAt" : "2021-07-15T15:50:12Z",
        "updatedAt" : "2021-07-15T16:05:46Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      },
      {
        "id" : "49a19291-2e73-443e-b6c1-140534a652de",
        "parentId" : "531be757-acf5-4c46-8693-7cf7e9d87507",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "It's for Spark to determine whether no-data batch is needed or not.",
        "createdAt" : "2021-07-16T00:59:57Z",
        "updatedAt" : "2021-07-16T00:59:57Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "bbade3501f16e9437ba9af4feca3e2029785d273",
    "line" : 226,
    "diffHunk" : "@@ -1,1 +726,730 @@  }\n\n  override def shouldRunAnotherBatch(newMetadata: OffsetSeqMetadata): Boolean = {\n    (outputMode.contains(Append) || outputMode.contains(Update)) &&\n      eventTimeWatermark.isDefined &&"
  },
  {
    "id" : "8a817e54-9e89-4734-9723-eaacaafe51e2",
    "prId" : 32703,
    "prUrl" : "https://github.com/apache/spark/pull/32703#pullrequestreview-671875958",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "19def0cf-268c-4226-81c8-94480e0a2826",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Hi, @viirya . ~So, is this a fix for the behavior of SPARK-21977 since Apache Spark 2.3.0?~\r\nOh, it seems that it's more older than I expected.",
        "createdAt" : "2021-05-30T23:41:41Z",
        "updatedAt" : "2021-05-30T23:43:46Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "a8135d85a46e48715ce60d8f5e4ac5f4dbf26b36",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +266,270 @@            val outputRows = Option(restoredRow).toSeq :+ row\n            numOutputRows += outputRows.size\n            outputRows\n          }\n        }"
  },
  {
    "id" : "ccc9cbe9-7202-4a12-b542-ea6dd5fb8e4a",
    "prId" : 31570,
    "prUrl" : "https://github.com/apache/spark/pull/31570#pullrequestreview-615112882",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0f47f76f-243a-43cb-88e8-18d69649440b",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "As I said, I don't think this retains the overall order hence requiring additional sort. `savedState` can be injected anywhere in input rows.",
        "createdAt" : "2021-03-18T08:52:41Z",
        "updatedAt" : "2021-03-18T08:52:41Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "04ca039bdf74c3b71be5fe53574906f426f25256",
    "line" : 102,
    "diffHunk" : "@@ -1,1 +512,516 @@          Seq(row)\n        } else {\n          val outputs = savedState :+ row\n          numOutputRows += outputs.size\n          outputs"
  },
  {
    "id" : "bed5f06c-7b13-4973-b025-a33a62f314ff",
    "prId" : 28607,
    "prUrl" : "https://github.com/apache/spark/pull/28607#pullrequestreview-430253338",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ac517257-050c-4406-82dd-df04f4e84f10",
        "parentId" : null,
        "authorId" : "2d6b46ba-4100-4c4d-9341-fff5e39647ec",
        "body" : "nit: maybe name it as `numDroppedRowsByWatermark`? Technically, we don't guarantee to drop all late inputs. So this metric doesn't report the accurate number of late inputs.",
        "createdAt" : "2020-06-14T17:54:47Z",
        "updatedAt" : "2020-06-14T17:54:47Z",
        "lastEditedBy" : "2d6b46ba-4100-4c4d-9341-fff5e39647ec",
        "tags" : [
        ]
      },
      {
        "id" : "e5f807a9-2323-4f84-90a7-ee62a7f579c8",
        "parentId" : "ac517257-050c-4406-82dd-df04f4e84f10",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Thanks for the suggestion. Yeah I guess I tried to explain the behavior but the name seems to be still confusing to others. I agree the suggested name is clearer.\r\n\r\nBtw, would we be better to have accurate number of late inputs? (Just asking because #24936 covers this, and can be applied orthogonally like num of late inputs vs num of dropped inputs.)",
        "createdAt" : "2020-06-14T19:42:49Z",
        "updatedAt" : "2020-06-14T19:42:50Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "10d2c16f-adf8-44e7-8de3-a905813cc618",
        "parentId" : "ac517257-050c-4406-82dd-df04f4e84f10",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "#28828",
        "createdAt" : "2020-06-14T21:40:15Z",
        "updatedAt" : "2020-06-14T21:40:15Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "4216405789c07f7ded54be05d5ecd8797ee05291",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +78,82 @@  override lazy val metrics = Map(\n    \"numOutputRows\" -> SQLMetrics.createMetric(sparkContext, \"number of output rows\"),\n    \"numLateInputs\" -> SQLMetrics.createMetric(sparkContext,\n      \"number of inputs which are later than watermark ('inputs' are relative to operators)\"),\n    \"numTotalStateRows\" -> SQLMetrics.createMetric(sparkContext, \"number of total state rows\"),"
  }
]