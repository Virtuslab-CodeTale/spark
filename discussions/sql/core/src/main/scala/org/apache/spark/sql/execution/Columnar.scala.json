[
  {
    "id" : "4ed64142-dfde-46a8-8035-a362186acafc",
    "prId" : 33140,
    "prUrl" : "https://github.com/apache/spark/pull/33140#pullrequestreview-698193963",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ae4b7ad8-e802-41bc-b302-4397ba9eab95",
        "parentId" : null,
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "`&& !a.isSubquery` ? If you only care about writing then I think you don't need do this in subquery ?",
        "createdAt" : "2021-07-02T06:16:40Z",
        "updatedAt" : "2021-07-02T06:16:40Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      },
      {
        "id" : "8dc6cc50-5913-4b92-a132-b6ed65fc5533",
        "parentId" : "ae4b7ad8-e802-41bc-b302-4397ba9eab95",
        "authorId" : "7e060620-bf02-418e-832f-c68dddfa2611",
        "body" : "Writing to parquet is just one example of where an adaptive plan can be contained in another plan, and just happens to be the one we are most interested in supporting without transitions at the moment. If the plan is columnar then we need to execute it as columnar, regardless of whether it is a subquery or not.",
        "createdAt" : "2021-07-02T13:35:55Z",
        "updatedAt" : "2021-07-02T13:35:56Z",
        "lastEditedBy" : "7e060620-bf02-418e-832f-c68dddfa2611",
        "tags" : [
        ]
      }
    ],
    "commit" : "0988acdafd41581cd02baa4b339497e6e54e7015",
    "line" : 109,
    "diffHunk" : "@@ -1,1 +457,461 @@  override def doExecuteColumnar(): RDD[ColumnarBatch] = {\n    child match {\n      case a: AdaptiveSparkPlanExec if a.finalPlanSupportsColumnar() =>\n        // if the child plan is adaptive and resulted in columnar data\n        // then we can bypass any transition"
  },
  {
    "id" : "ff67dd7f-1a30-479b-b6f4-6d550a790da0",
    "prId" : 33108,
    "prUrl" : "https://github.com/apache/spark/pull/33108#pullrequestreview-694453790",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b3842a95-3bde-43ff-9e5e-4681ded7c1bc",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Ah, I see. Nice catch.",
        "createdAt" : "2021-06-28T06:31:17Z",
        "updatedAt" : "2021-06-28T06:50:00Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "2ba7d5ab-21c2-4067-916b-c464dd0bb466",
        "parentId" : "b3842a95-3bde-43ff-9e5e-4681ded7c1bc",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Shouldn't it be `nullable || at.containsNull`?",
        "createdAt" : "2021-06-28T22:49:30Z",
        "updatedAt" : "2021-06-28T22:49:31Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "380b1d07f17c076ef3a90a4d12282ba79b9612e9",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +265,269 @@      case StringType => StringConverter\n      case CalendarIntervalType => CalendarConverter\n      case at: ArrayType => ArrayConverter(getConverterForType(at.elementType, at.containsNull))\n      case st: StructType => new StructConverter(st.fields.map(\n        (f) => getConverterForType(f.dataType, f.nullable)))"
  },
  {
    "id" : "a5699340-bf05-4441-9823-394eebd912f9",
    "prId" : 33108,
    "prUrl" : "https://github.com/apache/spark/pull/33108#pullrequestreview-694548607",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6c7aaf91-b1d6-4a7f-94c9-d63cce8c0d47",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "ditto, `nullable || mt.valueContainsNull`?",
        "createdAt" : "2021-06-28T22:50:01Z",
        "updatedAt" : "2021-06-28T22:50:02Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "9d38a626-db68-4f49-8427-0d94821ef63b",
        "parentId" : "6c7aaf91-b1d6-4a7f-94c9-d63cce8c0d47",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "if the map itself is null, I think we won't invoke the converter?",
        "createdAt" : "2021-06-29T02:44:56Z",
        "updatedAt" : "2021-06-29T02:44:56Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "a0a0caee-a946-4f58-b75f-2f0b0be0ebb3",
        "parentId" : "6c7aaf91-b1d6-4a7f-94c9-d63cce8c0d47",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "makes sense.",
        "createdAt" : "2021-06-29T03:00:10Z",
        "updatedAt" : "2021-06-29T03:00:10Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "380b1d07f17c076ef3a90a4d12282ba79b9612e9",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +270,274 @@      case dt: DecimalType => new DecimalConverter(dt)\n      case mt: MapType => MapConverter(getConverterForType(mt.keyType, nullable = false),\n        getConverterForType(mt.valueType, mt.valueContainsNull))\n      case unknown => throw new UnsupportedOperationException(\n        s\"Type $unknown not supported\")"
  }
]