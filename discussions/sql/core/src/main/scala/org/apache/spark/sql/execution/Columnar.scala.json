[
  {
    "id" : "4ed64142-dfde-46a8-8035-a362186acafc",
    "prId" : 33140,
    "prUrl" : "https://github.com/apache/spark/pull/33140#pullrequestreview-698193963",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ae4b7ad8-e802-41bc-b302-4397ba9eab95",
        "parentId" : null,
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "`&& !a.isSubquery` ? If you only care about writing then I think you don't need do this in subquery ?",
        "createdAt" : "2021-07-02T06:16:40Z",
        "updatedAt" : "2021-07-02T06:16:40Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      },
      {
        "id" : "8dc6cc50-5913-4b92-a132-b6ed65fc5533",
        "parentId" : "ae4b7ad8-e802-41bc-b302-4397ba9eab95",
        "authorId" : "7e060620-bf02-418e-832f-c68dddfa2611",
        "body" : "Writing to parquet is just one example of where an adaptive plan can be contained in another plan, and just happens to be the one we are most interested in supporting without transitions at the moment. If the plan is columnar then we need to execute it as columnar, regardless of whether it is a subquery or not.",
        "createdAt" : "2021-07-02T13:35:55Z",
        "updatedAt" : "2021-07-02T13:35:56Z",
        "lastEditedBy" : "7e060620-bf02-418e-832f-c68dddfa2611",
        "tags" : [
        ]
      }
    ],
    "commit" : "0988acdafd41581cd02baa4b339497e6e54e7015",
    "line" : 109,
    "diffHunk" : "@@ -1,1 +457,461 @@  override def doExecuteColumnar(): RDD[ColumnarBatch] = {\n    child match {\n      case a: AdaptiveSparkPlanExec if a.finalPlanSupportsColumnar() =>\n        // if the child plan is adaptive and resulted in columnar data\n        // then we can bypass any transition"
  },
  {
    "id" : "ff67dd7f-1a30-479b-b6f4-6d550a790da0",
    "prId" : 33108,
    "prUrl" : "https://github.com/apache/spark/pull/33108#pullrequestreview-694453790",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b3842a95-3bde-43ff-9e5e-4681ded7c1bc",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Ah, I see. Nice catch.",
        "createdAt" : "2021-06-28T06:31:17Z",
        "updatedAt" : "2021-06-28T06:50:00Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "2ba7d5ab-21c2-4067-916b-c464dd0bb466",
        "parentId" : "b3842a95-3bde-43ff-9e5e-4681ded7c1bc",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Shouldn't it be `nullable || at.containsNull`?",
        "createdAt" : "2021-06-28T22:49:30Z",
        "updatedAt" : "2021-06-28T22:49:31Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "380b1d07f17c076ef3a90a4d12282ba79b9612e9",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +265,269 @@      case StringType => StringConverter\n      case CalendarIntervalType => CalendarConverter\n      case at: ArrayType => ArrayConverter(getConverterForType(at.elementType, at.containsNull))\n      case st: StructType => new StructConverter(st.fields.map(\n        (f) => getConverterForType(f.dataType, f.nullable)))"
  },
  {
    "id" : "a5699340-bf05-4441-9823-394eebd912f9",
    "prId" : 33108,
    "prUrl" : "https://github.com/apache/spark/pull/33108#pullrequestreview-694548607",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6c7aaf91-b1d6-4a7f-94c9-d63cce8c0d47",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "ditto, `nullable || mt.valueContainsNull`?",
        "createdAt" : "2021-06-28T22:50:01Z",
        "updatedAt" : "2021-06-28T22:50:02Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "9d38a626-db68-4f49-8427-0d94821ef63b",
        "parentId" : "6c7aaf91-b1d6-4a7f-94c9-d63cce8c0d47",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "if the map itself is null, I think we won't invoke the converter?",
        "createdAt" : "2021-06-29T02:44:56Z",
        "updatedAt" : "2021-06-29T02:44:56Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "a0a0caee-a946-4f58-b75f-2f0b0be0ebb3",
        "parentId" : "6c7aaf91-b1d6-4a7f-94c9-d63cce8c0d47",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "makes sense.",
        "createdAt" : "2021-06-29T03:00:10Z",
        "updatedAt" : "2021-06-29T03:00:10Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "380b1d07f17c076ef3a90a4d12282ba79b9612e9",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +270,274 @@      case dt: DecimalType => new DecimalConverter(dt)\n      case mt: MapType => MapConverter(getConverterForType(mt.keyType, nullable = false),\n        getConverterForType(mt.valueType, mt.valueContainsNull))\n      case unknown => throw new UnsupportedOperationException(\n        s\"Type $unknown not supported\")"
  },
  {
    "id" : "5e9327b4-17fa-4595-97f1-f702131989da",
    "prId" : 26137,
    "prUrl" : "https://github.com/apache/spark/pull/26137#pullrequestreview-306972064",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b7373fe0-2378-40ed-b272-3eaaef1e54d6",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "This bug seems to exist in `branch-2.4`, doesn't it? With a proper UT, we need to backport this to `branch-2.4`. Please update the JIRA issue `Affects Versions` together.",
        "createdAt" : "2019-10-24T17:40:40Z",
        "updatedAt" : "2019-10-24T17:40:41Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "c0495cce-711a-44f8-a0cd-5837e2747750",
        "parentId" : "b7373fe0-2378-40ed-b272-3eaaef1e54d6",
        "authorId" : "a7490dbf-9175-4936-9854-d45214f267dd",
        "body" : "Thanks for your comments. Sorry but I don't find this bug in `branch-2.4`. This part of code is proposed by [https://issues.apache.org/jira/browse/SPARK-27396](url)",
        "createdAt" : "2019-10-25T03:21:45Z",
        "updatedAt" : "2019-10-25T03:22:10Z",
        "lastEditedBy" : "a7490dbf-9175-4936-9854-d45214f267dd",
        "tags" : [
        ]
      },
      {
        "id" : "ccc19026-c1b5-4565-bc94-47f060a5919f",
        "parentId" : "b7373fe0-2378-40ed-b272-3eaaef1e54d6",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "You are right, @rongma1997 . It seems that I misread the commit history last time.",
        "createdAt" : "2019-10-25T03:57:30Z",
        "updatedAt" : "2019-10-25T03:57:30Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "114a743b77c2963967cb5e25a20c792f70aca6e2",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +455,459 @@          override def next(): ColumnarBatch = {\n            cb.setNumRows(0)\n            vectors.foreach(_.reset())\n            var rowCount = 0\n            while (rowCount < numRows && rowIterator.hasNext) {"
  },
  {
    "id" : "36c4c5ce-4a67-41f2-9d94-4d5ad5c728cc",
    "prId" : 25264,
    "prUrl" : "https://github.com/apache/spark/pull/25264#pullrequestreview-269822571",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9f97cab3-39ce-439d-bd19-56ee3df5e6eb",
        "parentId" : null,
        "authorId" : "80d631a6-73e8-46a6-a01b-a80d1f1cc6cc",
        "body" : "+1",
        "createdAt" : "2019-08-01T19:09:58Z",
        "updatedAt" : "2019-08-02T03:59:22Z",
        "lastEditedBy" : "80d631a6-73e8-46a6-a01b-a80d1f1cc6cc",
        "tags" : [
        ]
      }
    ],
    "commit" : "308fc11bfa726143d627abfe37b038cd655f6051",
    "line" : 47,
    "diffHunk" : "@@ -1,1 +81,85 @@    // This avoids calling `output` in the RDD closure, so that we don't need to include the entire\n    // plan (this) in the closure.\n    val localOutput = this.output\n    child.executeColumnar().mapPartitionsInternal { batches =>\n      val toUnsafe = UnsafeProjection.create(localOutput, localOutput)"
  },
  {
    "id" : "d13fc79b-65d7-448e-b778-53460d148480",
    "prId" : 25008,
    "prUrl" : "https://github.com/apache/spark/pull/25008#pullrequestreview-260174227",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f7d0087c-9b5d-4d5c-8754-4eb68dd28e4d",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "`ColumnarToRowExec`'s comment also mentions `ColumnarBatchScan`. If you are like to remove all reference to `ColumnarBatchScan`...",
        "createdAt" : "2019-07-10T15:10:15Z",
        "updatedAt" : "2019-07-10T15:36:54Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "4f801e07-4e41-40b8-aaf2-979ff00123d5",
        "parentId" : "f7d0087c-9b5d-4d5c-8754-4eb68dd28e4d",
        "authorId" : "b19fe247-920f-40ae-83e7-6b8ec9979f6b",
        "body" : "Great catch. I thought I got rid of all of them.  Will grep though again.",
        "createdAt" : "2019-07-10T15:24:43Z",
        "updatedAt" : "2019-07-10T15:36:54Z",
        "lastEditedBy" : "b19fe247-920f-40ae-83e7-6b8ec9979f6b",
        "tags" : [
        ]
      }
    ],
    "commit" : "2cce2fa9e057cb379e628fe01ea2cef280a9b198",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +94,98 @@  }\n\n  /**\n   * Generate [[ColumnVector]] expressions for our parent to consume as rows.\n   * This is called once per [[ColumnVector]] in the batch."
  },
  {
    "id" : "3cd89b12-74fb-4d17-8991-047394a78cbf",
    "prId" : 24795,
    "prUrl" : "https://github.com/apache/spark/pull/24795#pullrequestreview-246777630",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6f665492-9ae3-4a08-82dd-7d4137e95c5c",
        "parentId" : null,
        "authorId" : "99e31844-a5b3-48a2-af71-412edb607a13",
        "body" : "Are `pre` and `post` appropriate naming, in particular, `pre`?  IIUC correctly, `pre` is a mandatory function for columnar processing and `post` is an optional function to optimize columnars or to clean up resources. To me, a pair of `pre` and `post` looks dual. (Of course, I am not a native English speaker).\r\n\r\nIn addition, can you create a test case using `ColunarRule` (for example, a simplified version of [this](https://gist.github.com/revans2/c3cad77075c4fa5d9d271308ee2f1b1d), but use `pre` and `post`)? It would help understanding of reviewers and can ensure the behavior of this API.",
        "createdAt" : "2019-06-06T18:58:48Z",
        "updatedAt" : "2019-06-21T14:22:17Z",
        "lastEditedBy" : "99e31844-a5b3-48a2-af71-412edb607a13",
        "tags" : [
        ]
      },
      {
        "id" : "b79de028-71de-466b-a16f-772cbaeec4c9",
        "parentId" : "6f665492-9ae3-4a08-82dd-7d4137e95c5c",
        "authorId" : "b19fe247-920f-40ae-83e7-6b8ec9979f6b",
        "body" : "For me `pre` and `post` made since in relation to the comments, but perhaps not standalone. I'll see if I can make them more descriptive.  I'll also add in a test.",
        "createdAt" : "2019-06-06T19:44:44Z",
        "updatedAt" : "2019-06-21T14:22:17Z",
        "lastEditedBy" : "b19fe247-920f-40ae-83e7-6b8ec9979f6b",
        "tags" : [
        ]
      }
    ],
    "commit" : "7f1753db2d15186399e5ef9d7ef88b2bc7eedf67",
    "line" : 47,
    "diffHunk" : "@@ -1,1 +45,49 @@ * memory.\n */\nclass ColumnarRule {\n  def preColumnarTransitions: Rule[SparkPlan] = plan => plan\n  def postColumnarTransitions: Rule[SparkPlan] = plan => plan"
  },
  {
    "id" : "4c07664b-6a60-4e98-a4c9-8253260544f6",
    "prId" : 24795,
    "prUrl" : "https://github.com/apache/spark/pull/24795#pullrequestreview-250465701",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "48b6aa06-12fb-4784-85e0-5db6e42461de",
        "parentId" : null,
        "authorId" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "body" : "Could this be used as a generalized replacement for `ArrowWriter` in ArrowWriter.scala, which writes to ArrowColumnVectors?",
        "createdAt" : "2019-06-14T22:55:52Z",
        "updatedAt" : "2019-06-21T14:22:17Z",
        "lastEditedBy" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "tags" : [
        ]
      },
      {
        "id" : "a9123f19-2b52-4e13-81da-dd76246ccb6e",
        "parentId" : "48b6aa06-12fb-4784-85e0-5db6e42461de",
        "authorId" : "b19fe247-920f-40ae-83e7-6b8ec9979f6b",
        "body" : "That is the eventual goal.  In this patch, I wanted the code to not replace any existing code to keep the possible impacts smaller.  \r\n\r\nI am currently working on a separate patch to replace ColumnarBatchScan with ColumnarToRowExec.  The patch is mostly just updating tests to match the new pattern, but I also want to run a lot of benchmark code to be sure that I didn't regress anything.\r\n\r\nAfter that, I was going to start working on the various APIs that use arrow data behind the scenes to process data, R, Python, etc.  That is likely to have a larger impact because the code that does those transitions is spread through a lot more files.  I also don't want to get too far ahead of myself as this patch all of them would be based off of is not even in yet.  ",
        "createdAt" : "2019-06-17T12:34:24Z",
        "updatedAt" : "2019-06-21T14:22:17Z",
        "lastEditedBy" : "b19fe247-920f-40ae-83e7-6b8ec9979f6b",
        "tags" : [
        ]
      }
    ],
    "commit" : "7f1753db2d15186399e5ef9d7ef88b2bc7eedf67",
    "line" : 237,
    "diffHunk" : "@@ -1,1 +235,239 @@ * [[WritableColumnVector]].\n */\nprivate object RowToColumnConverter {\n  private abstract class TypeConverter extends Serializable {\n    def append(row: SpecializedGetters, column: Int, cv: WritableColumnVector): Unit"
  },
  {
    "id" : "19277dcc-a127-46b6-8a52-f393c548eb3e",
    "prId" : 24795,
    "prUrl" : "https://github.com/apache/spark/pull/24795#pullrequestreview-252796146",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "226c5dab-fbfb-4771-bf4b-f703427ff5d5",
        "parentId" : null,
        "authorId" : "a269db99-bf49-44c7-9b33-056e49f21bc5",
        "body" : "I still don't understand - why do we need ColumnarRule, i.e. any rule that's column specific?",
        "createdAt" : "2019-06-21T04:36:36Z",
        "updatedAt" : "2019-06-21T14:22:17Z",
        "lastEditedBy" : "a269db99-bf49-44c7-9b33-056e49f21bc5",
        "tags" : [
        ]
      },
      {
        "id" : "c3bf3d74-6bc5-4dcb-b27c-b08f2be7f253",
        "parentId" : "226c5dab-fbfb-4771-bf4b-f703427ff5d5",
        "authorId" : "b19fe247-920f-40ae-83e7-6b8ec9979f6b",
        "body" : "@rxin \r\nAs I explained [before](https://github.com/apache/spark/pull/24795#issuecomment-499960263) there needs to be at least one rule that operates on the entire physical `SparkPlan` after the exchanges have been inserted into the plan.\r\n\r\nHaving two rules, one that runs prior to inserting columnar transitions and one that runs after it makes it so a plugin does not have to duplicate code for inserting columnar transitions and provides a cleaner API for anyone trying to insert columnar processing, but that is relatively minor.\r\n\r\nIf you want me to split `ColumnarRule` up and rename the individual parts to not be columnar specific I am happy to do it.  If you want me to make it a single rule that runs prior to code generation I can make that work too.  Just let me know which of the two changes you would prefer, and I will make it happen.",
        "createdAt" : "2019-06-21T12:29:38Z",
        "updatedAt" : "2019-06-21T14:22:17Z",
        "lastEditedBy" : "b19fe247-920f-40ae-83e7-6b8ec9979f6b",
        "tags" : [
        ]
      }
    ],
    "commit" : "7f1753db2d15186399e5ef9d7ef88b2bc7eedf67",
    "line" : 37,
    "diffHunk" : "@@ -1,1 +35,39 @@\n/**\n * Holds a user defined rule that can be used to inject columnar implementations of various\n * operators in the plan. The [[preColumnarTransitions]] [[Rule]] can be used to replace\n * [[SparkPlan]] instances with versions that support a columnar implementation. After this"
  }
]