[
  {
    "id" : "4ed64142-dfde-46a8-8035-a362186acafc",
    "prId" : 33140,
    "prUrl" : "https://github.com/apache/spark/pull/33140#pullrequestreview-698193963",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ae4b7ad8-e802-41bc-b302-4397ba9eab95",
        "parentId" : null,
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "`&& !a.isSubquery` ? If you only care about writing then I think you don't need do this in subquery ?",
        "createdAt" : "2021-07-02T06:16:40Z",
        "updatedAt" : "2021-07-02T06:16:40Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      },
      {
        "id" : "8dc6cc50-5913-4b92-a132-b6ed65fc5533",
        "parentId" : "ae4b7ad8-e802-41bc-b302-4397ba9eab95",
        "authorId" : "7e060620-bf02-418e-832f-c68dddfa2611",
        "body" : "Writing to parquet is just one example of where an adaptive plan can be contained in another plan, and just happens to be the one we are most interested in supporting without transitions at the moment. If the plan is columnar then we need to execute it as columnar, regardless of whether it is a subquery or not.",
        "createdAt" : "2021-07-02T13:35:55Z",
        "updatedAt" : "2021-07-02T13:35:56Z",
        "lastEditedBy" : "7e060620-bf02-418e-832f-c68dddfa2611",
        "tags" : [
        ]
      }
    ],
    "commit" : "0988acdafd41581cd02baa4b339497e6e54e7015",
    "line" : 109,
    "diffHunk" : "@@ -1,1 +457,461 @@  override def doExecuteColumnar(): RDD[ColumnarBatch] = {\n    child match {\n      case a: AdaptiveSparkPlanExec if a.finalPlanSupportsColumnar() =>\n        // if the child plan is adaptive and resulted in columnar data\n        // then we can bypass any transition"
  },
  {
    "id" : "ff67dd7f-1a30-479b-b6f4-6d550a790da0",
    "prId" : 33108,
    "prUrl" : "https://github.com/apache/spark/pull/33108#pullrequestreview-694453790",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b3842a95-3bde-43ff-9e5e-4681ded7c1bc",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Ah, I see. Nice catch.",
        "createdAt" : "2021-06-28T06:31:17Z",
        "updatedAt" : "2021-06-28T06:50:00Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "2ba7d5ab-21c2-4067-916b-c464dd0bb466",
        "parentId" : "b3842a95-3bde-43ff-9e5e-4681ded7c1bc",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Shouldn't it be `nullable || at.containsNull`?",
        "createdAt" : "2021-06-28T22:49:30Z",
        "updatedAt" : "2021-06-28T22:49:31Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "380b1d07f17c076ef3a90a4d12282ba79b9612e9",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +265,269 @@      case StringType => StringConverter\n      case CalendarIntervalType => CalendarConverter\n      case at: ArrayType => ArrayConverter(getConverterForType(at.elementType, at.containsNull))\n      case st: StructType => new StructConverter(st.fields.map(\n        (f) => getConverterForType(f.dataType, f.nullable)))"
  },
  {
    "id" : "a5699340-bf05-4441-9823-394eebd912f9",
    "prId" : 33108,
    "prUrl" : "https://github.com/apache/spark/pull/33108#pullrequestreview-694548607",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6c7aaf91-b1d6-4a7f-94c9-d63cce8c0d47",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "ditto, `nullable || mt.valueContainsNull`?",
        "createdAt" : "2021-06-28T22:50:01Z",
        "updatedAt" : "2021-06-28T22:50:02Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "9d38a626-db68-4f49-8427-0d94821ef63b",
        "parentId" : "6c7aaf91-b1d6-4a7f-94c9-d63cce8c0d47",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "if the map itself is null, I think we won't invoke the converter?",
        "createdAt" : "2021-06-29T02:44:56Z",
        "updatedAt" : "2021-06-29T02:44:56Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "a0a0caee-a946-4f58-b75f-2f0b0be0ebb3",
        "parentId" : "6c7aaf91-b1d6-4a7f-94c9-d63cce8c0d47",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "makes sense.",
        "createdAt" : "2021-06-29T03:00:10Z",
        "updatedAt" : "2021-06-29T03:00:10Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "380b1d07f17c076ef3a90a4d12282ba79b9612e9",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +270,274 @@      case dt: DecimalType => new DecimalConverter(dt)\n      case mt: MapType => MapConverter(getConverterForType(mt.keyType, nullable = false),\n        getConverterForType(mt.valueType, mt.valueContainsNull))\n      case unknown => throw new UnsupportedOperationException(\n        s\"Type $unknown not supported\")"
  },
  {
    "id" : "5e9327b4-17fa-4595-97f1-f702131989da",
    "prId" : 26137,
    "prUrl" : "https://github.com/apache/spark/pull/26137#pullrequestreview-306972064",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b7373fe0-2378-40ed-b272-3eaaef1e54d6",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "This bug seems to exist in `branch-2.4`, doesn't it? With a proper UT, we need to backport this to `branch-2.4`. Please update the JIRA issue `Affects Versions` together.",
        "createdAt" : "2019-10-24T17:40:40Z",
        "updatedAt" : "2019-10-24T17:40:41Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "c0495cce-711a-44f8-a0cd-5837e2747750",
        "parentId" : "b7373fe0-2378-40ed-b272-3eaaef1e54d6",
        "authorId" : "a7490dbf-9175-4936-9854-d45214f267dd",
        "body" : "Thanks for your comments. Sorry but I don't find this bug in `branch-2.4`. This part of code is proposed by [https://issues.apache.org/jira/browse/SPARK-27396](url)",
        "createdAt" : "2019-10-25T03:21:45Z",
        "updatedAt" : "2019-10-25T03:22:10Z",
        "lastEditedBy" : "a7490dbf-9175-4936-9854-d45214f267dd",
        "tags" : [
        ]
      },
      {
        "id" : "ccc19026-c1b5-4565-bc94-47f060a5919f",
        "parentId" : "b7373fe0-2378-40ed-b272-3eaaef1e54d6",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "You are right, @rongma1997 . It seems that I misread the commit history last time.",
        "createdAt" : "2019-10-25T03:57:30Z",
        "updatedAt" : "2019-10-25T03:57:30Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "114a743b77c2963967cb5e25a20c792f70aca6e2",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +455,459 @@          override def next(): ColumnarBatch = {\n            cb.setNumRows(0)\n            vectors.foreach(_.reset())\n            var rowCount = 0\n            while (rowCount < numRows && rowIterator.hasNext) {"
  },
  {
    "id" : "36c4c5ce-4a67-41f2-9d94-4d5ad5c728cc",
    "prId" : 25264,
    "prUrl" : "https://github.com/apache/spark/pull/25264#pullrequestreview-269822571",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9f97cab3-39ce-439d-bd19-56ee3df5e6eb",
        "parentId" : null,
        "authorId" : "80d631a6-73e8-46a6-a01b-a80d1f1cc6cc",
        "body" : "+1",
        "createdAt" : "2019-08-01T19:09:58Z",
        "updatedAt" : "2019-08-02T03:59:22Z",
        "lastEditedBy" : "80d631a6-73e8-46a6-a01b-a80d1f1cc6cc",
        "tags" : [
        ]
      }
    ],
    "commit" : "308fc11bfa726143d627abfe37b038cd655f6051",
    "line" : 47,
    "diffHunk" : "@@ -1,1 +81,85 @@    // This avoids calling `output` in the RDD closure, so that we don't need to include the entire\n    // plan (this) in the closure.\n    val localOutput = this.output\n    child.executeColumnar().mapPartitionsInternal { batches =>\n      val toUnsafe = UnsafeProjection.create(localOutput, localOutput)"
  }
]