[
  {
    "id" : "ae10e953-97ca-4bfb-b588-e11cd7fe2e79",
    "prId" : 33751,
    "prUrl" : "https://github.com/apache/spark/pull/33751#pullrequestreview-730731405",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c03644fd-b3ec-4784-a5c6-00f1b9e9316a",
        "parentId" : null,
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "This is following string to date logic in BaseScriptTransformationExec",
        "createdAt" : "2021-08-16T13:43:55Z",
        "updatedAt" : "2021-08-16T13:43:55Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "7a03c24491af59c76744d791d76e06d68fe37b3f",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +226,230 @@      case YearMonthIntervalType(start, end) => wrapperConvertException(\n        data => IntervalUtils.castStringToYMInterval(UTF8String.fromString(data), start, end)\n          .map(IntervalUtils.monthsToPeriod).orNull, converter)\n      case DayTimeIntervalType(start, end) => wrapperConvertException(\n        data => IntervalUtils.microsToDuration("
  },
  {
    "id" : "9d9fa779-61bf-42a3-8dee-3a6518b27cf4",
    "prId" : 33363,
    "prUrl" : "https://github.com/apache/spark/pull/33363#pullrequestreview-709103168",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "85c6fd75-2269-4df4-9125-e2d8d223c3d6",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "nit:\r\n```\r\n        if (data == ioschema.outputRowFormatMap(\"TOK_TABLEROWFORMATNULL\")) {\r\n          null\r\n        } else {\r\n          try {\r\n            f(data)\r\n          } catch {\r\n            case NonFatal(_) => null\r\n          }\r\n        }\r\n```\r\n?",
        "createdAt" : "2021-07-16T02:00:21Z",
        "updatedAt" : "2021-07-16T02:03:48Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "5d9a4b1e-433e-48ca-be67-4d1a6bdc62cd",
        "parentId" : "85c6fd75-2269-4df4-9125-e2d8d223c3d6",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "Done",
        "createdAt" : "2021-07-19T02:14:09Z",
        "updatedAt" : "2021-07-19T02:14:09Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "4f44917dd2b2c789ea46ca365a1b60cd5fa4ee24",
    "line" : 33,
    "diffHunk" : "@@ -1,1 +247,251 @@          } catch {\n            case NonFatal(_) => null\n          }\n        }\n      }"
  },
  {
    "id" : "2df3776d-6899-4a7c-afcb-4e3d5fbac5a1",
    "prId" : 33363,
    "prUrl" : "https://github.com/apache/spark/pull/33363#pullrequestreview-712332671",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4cd124e1-be35-46f3-a424-991c2f59e4bf",
        "parentId" : null,
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "FYI @cloud-fan @maropu For this change, I need to explain is that. \r\nIf we not add parameter -1, \r\n```\r\n\"2@false@Spark SQL@\".split('@') will have three elems\r\n\"2\"\r\n\"false\"\r\n\"Spark SQL\"\r\n```\r\nSince we are strict handle data, we need to add -1 the result should be \r\n```\r\n\"2@false@Spark SQL@\".split('@', -1) will have four elems\r\n\"2\"\r\n\"false\"\r\n\"Spark SQL\"\r\n\"\"\r\n```",
        "createdAt" : "2021-07-22T03:09:14Z",
        "updatedAt" : "2021-07-22T03:09:29Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "4f44917dd2b2c789ea46ca365a1b60cd5fa4ee24",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +119,123 @@        prevLine: String =>\n          new GenericInternalRow(\n            prevLine.split(outputRowFormat, -1).padTo(outputFieldWriters.size, null)\n              .zip(outputFieldWriters)\n              .map { case (data, writer) => writer(data) })"
  },
  {
    "id" : "81df432b-6549-45f6-ac7e-827760cb7264",
    "prId" : 31046,
    "prUrl" : "https://github.com/apache/spark/pull/31046#pullrequestreview-562266780",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d7be9d19-1ab1-4626-b409-ac0854c638c8",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Oh, I don't know why I thought it's more than 5+. It's more than 6+ (https://github.com/databricks/scala-style-guide#imports).",
        "createdAt" : "2021-01-06T00:32:29Z",
        "updatedAt" : "2021-01-06T05:14:36Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "faa18df7-f558-408b-9f7c-cb3e21e286e0",
        "parentId" : "d7be9d19-1ab1-4626-b409-ac0854c638c8",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> Oh, I don't know why I thought it's more than 5+. It's more than 6+ (https://github.com/databricks/scala-style-guide#imports).\r\n\r\nSeems my local ideal's rule is 5, emmm I need to check this.",
        "createdAt" : "2021-01-06T01:29:15Z",
        "updatedAt" : "2021-01-06T05:14:36Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "db12e28a3cd5f3c19810f301a83c5667ff661058",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +18,22 @@package org.apache.spark.sql.execution\n\nimport java.io.{BufferedReader, File, InputStream, InputStreamReader, OutputStream}\nimport java.nio.charset.StandardCharsets\nimport java.util.concurrent.TimeUnit"
  },
  {
    "id" : "602e3c50-b413-4f23-b8f0-9f4bb28274ac",
    "prId" : 30973,
    "prUrl" : "https://github.com/apache/spark/pull/30973#pullrequestreview-560667110",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6e9c3314-2617-4e30-84fc-84113d2a7cff",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@AngersZhuuuu, is this required? In executor side, the root directory is already current working directory IIRC.",
        "createdAt" : "2021-01-03T00:53:14Z",
        "updatedAt" : "2021-01-04T05:07:05Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "902e6d7c-db2a-4431-9fcc-03ead85eaec2",
        "parentId" : "6e9c3314-2617-4e30-84fc-84113d2a7cff",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> @AngersZhuuuu, is this required? In executor side, the root directory is already current working directory IIRC.\r\n\r\nYea, can remove this ",
        "createdAt" : "2021-01-03T01:04:36Z",
        "updatedAt" : "2021-01-04T05:07:05Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "a32d1013-5784-4f97-9d3d-c0ed4538f0b0",
        "parentId" : "6e9c3314-2617-4e30-84fc-84113d2a7cff",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> @AngersZhuuuu, is this required? In executor side, the root directory is already current working directory IIRC.\r\n\r\nBut in Unit Test, driver node's working dir is not `SparkFile.getRootDirectory`. If remove this, test will failed.\r\n```\r\n /usr/bin/python: can't open file 'test-resource8890660714423343799.py': [Errno 2] No such file or directory\r\n```",
        "createdAt" : "2021-01-03T02:16:22Z",
        "updatedAt" : "2021-01-04T05:07:05Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "f65d22dc-1b83-4b89-9a9a-ed0923b185c7",
        "parentId" : "6e9c3314-2617-4e30-84fc-84113d2a7cff",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "That seems like because we're running with `local `cluster. If you run the tests with `local-cluster`, I guess it'd work. But okay, in this way we can cover `local` too.",
        "createdAt" : "2021-01-03T03:20:08Z",
        "updatedAt" : "2021-01-04T05:07:05Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "ca24dd60-30d6-4d41-8120-8048cbb65142",
        "parentId" : "6e9c3314-2617-4e30-84fc-84113d2a7cff",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "Yea,  It seems to have no effect on other tests. Cover local mode maybe better.",
        "createdAt" : "2021-01-03T03:29:01Z",
        "updatedAt" : "2021-01-04T05:07:05Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "7f2991dd-1801-4c6a-a27f-1becb061235a",
        "parentId" : "6e9c3314-2617-4e30-84fc-84113d2a7cff",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "And add `PathFInder` like hive, it cab support search file under all entry of `PATH`, should be better.",
        "createdAt" : "2021-01-03T03:30:28Z",
        "updatedAt" : "2021-01-04T05:07:05Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "91542f9e526af6d9a1f486d7f28240aa4594ac21",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +73,77 @@    val cmd = List(\"/bin/bash\", \"-c\", script)\n    val builder = new ProcessBuilder(cmd.asJava)\n      .directory(new File(SparkFiles.getRootDirectory()))\n    val path = System.getenv(\"PATH\") + File.pathSeparator +\n      SparkFiles.getRootDirectory()"
  },
  {
    "id" : "7439743b-fb6a-43fa-bb32-0b1d0d014e61",
    "prId" : 30973,
    "prUrl" : "https://github.com/apache/spark/pull/30973#pullrequestreview-562074661",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "938a472d-fa36-4a42-894d-224f3e7aa7a1",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "@AngersZhuuuu . nit. Please enumerate all next time.",
        "createdAt" : "2021-01-05T19:35:45Z",
        "updatedAt" : "2021-01-05T19:35:45Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "91542f9e526af6d9a1f486d7f28240aa4594ac21",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +18,22 @@package org.apache.spark.sql.execution\n\nimport java.io._\nimport java.nio.charset.StandardCharsets\nimport java.util.concurrent.TimeUnit"
  },
  {
    "id" : "30499563-d0fe-474d-92f1-9e84fa90de76",
    "prId" : 30957,
    "prUrl" : "https://github.com/apache/spark/pull/30957#pullrequestreview-584419813",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6ec48837-9907-455e-8398-ef2b3cd25232",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "nit format:\r\n```\r\n    input.map { _.dataType match {\r\n      case _: ArrayType | _: MapType | _: StructType =>\r\n        new StructsToJson(in).withTimeZone(conf.sessionLocalTimeZone)\r\n      case _ => Cast(in, StringType).withTimeZone(conf.sessionLocalTimeZone)\r\n    }\r\n```",
        "createdAt" : "2021-02-05T07:49:40Z",
        "updatedAt" : "2021-04-19T03:26:35Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "3680155a-c7f7-43c1-8e67-ea2a5c37252a",
        "parentId" : "6ec48837-9907-455e-8398-ef2b3cd25232",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> nit format:\r\n> \r\n> ```\r\n>     input.map { _.dataType match {\r\n>       case _: ArrayType | _: MapType | _: StructType =>\r\n>         new StructsToJson(in).withTimeZone(conf.sessionLocalTimeZone)\r\n>       case _ => Cast(in, StringType).withTimeZone(conf.sessionLocalTimeZone)\r\n>     }\r\n> ```\r\n\r\nIn this way, we miss value `in`, Hmmmm",
        "createdAt" : "2021-02-05T08:25:04Z",
        "updatedAt" : "2021-04-19T03:26:35Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "60d85cc9-6af3-4075-8081-c6c66612e55b",
        "parentId" : "6ec48837-9907-455e-8398-ef2b3cd25232",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Ah, I see. The current one looks okay.",
        "createdAt" : "2021-02-05T15:02:13Z",
        "updatedAt" : "2021-04-19T03:26:35Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "adf8a6682d6c9b2ff759c456d26e0d4358c0d965",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +48,52 @@\n  protected lazy val inputExpressionsWithoutSerde: Seq[Expression] = {\n    input.map { in =>\n      in.dataType match {\n        case _: ArrayType | _: MapType | _: StructType =>"
  },
  {
    "id" : "0b81bd7a-1407-4fab-898f-2b0ed8a77ac6",
    "prId" : 30156,
    "prUrl" : "https://github.com/apache/spark/pull/30156#pullrequestreview-517314795",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e60bd403-b0c7-4a94-abab-7c74b713fb62",
        "parentId" : null,
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "pass as a func to avoid repeating this logic",
        "createdAt" : "2020-10-27T02:38:17Z",
        "updatedAt" : "2020-10-30T05:00:51Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "357fd27d-a87b-47b7-9442-496a36d34ed4",
        "parentId" : "e60bd403-b0c7-4a94-abab-7c74b713fb62",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Nice.",
        "createdAt" : "2020-10-27T03:01:13Z",
        "updatedAt" : "2020-10-30T05:00:51Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "0541ff50ba75278e984dd3414efed8dc5c339881",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +110,114 @@      } else {\n        (arr: Array[String], size: Int) => arr\n      }\n      val processRowWithoutSerde = if (!ioschema.schemaLess) {\n        prevLine: String =>"
  },
  {
    "id" : "49bfa58b-1a4e-41d8-8893-83233648c203",
    "prId" : 30156,
    "prUrl" : "https://github.com/apache/spark/pull/30156#pullrequestreview-520402882",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0907fe5d-4cca-4612-bd1f-031fb8e3a423",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "The config name sounds like padding is the legacy behavior.",
        "createdAt" : "2020-10-30T06:27:44Z",
        "updatedAt" : "2020-10-30T06:27:45Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "0541ff50ba75278e984dd3414efed8dc5c339881",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +106,110 @@      val outputRowFormat = ioschema.outputRowFormatMap(\"TOK_TABLEROWFORMATFIELD\")\n\n      val padNull = if (conf.legacyPadNullWhenValueLessThenSchema) {\n        (arr: Array[String], size: Int) => arr.padTo(size, null)\n      } else {"
  },
  {
    "id" : "92b6af9c-a914-4289-8870-2d801ac98b72",
    "prId" : 29421,
    "prUrl" : "https://github.com/apache/spark/pull/29421#pullrequestreview-473140284",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dac062f2-cb16-4b07-badb-c6a2336915c8",
        "parentId" : null,
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "@maropu \r\nthis question https://github.com/apache/spark/pull/29421/files#r469889057\r\n fixed by  this line. \r\n\r\nand will rase a new pr for https://issues.apache.org/jira/browse/SPARK-32667",
        "createdAt" : "2020-08-20T11:08:58Z",
        "updatedAt" : "2020-10-26T02:51:45Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "b1528549-bf40-449d-97fe-392449dd968e",
        "parentId" : "dac062f2-cb16-4b07-badb-c6a2336915c8",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "@maropu How about split this fix as a new one, since this fix is very necessary and no disputed ",
        "createdAt" : "2020-08-24T01:11:40Z",
        "updatedAt" : "2020-10-26T02:51:45Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "241a3afd-122a-43b9-81b6-41ba4edc571c",
        "parentId" : "dac062f2-cb16-4b07-badb-c6a2336915c8",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "If it's a trivial fix, IMO its okay to include it in this PR.",
        "createdAt" : "2020-08-24T06:03:13Z",
        "updatedAt" : "2020-10-26T02:51:45Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "e83f6a55e20e7f4edfa11ad91e06115a5eb38c6d",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +120,124 @@          new GenericInternalRow(\n            prevLine.split(outputRowFormat).slice(0, 2).padTo(2, null)\n              .map(kvWriter))\n      }\n"
  },
  {
    "id" : "8dfb6599-0f14-4fac-9757-ad5562b00899",
    "prId" : 29421,
    "prUrl" : "https://github.com/apache/spark/pull/29421#pullrequestreview-478262115",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5e6faebc-1175-4224-9c35-20da02062f8b",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "nit: could you leave comments about why we need `.padTo(2, null)` here.",
        "createdAt" : "2020-08-31T01:20:56Z",
        "updatedAt" : "2020-10-26T02:51:45Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "8c325175-17d7-4b82-85ae-812d8403e471",
        "parentId" : "5e6faebc-1175-4224-9c35-20da02062f8b",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> nit: could you leave comments about why we need `.padTo(2, null)` here.\r\n\r\nSee the latest change, I have changed comment leaved before",
        "createdAt" : "2020-08-31T01:38:48Z",
        "updatedAt" : "2020-10-26T02:51:45Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "e83f6a55e20e7f4edfa11ad91e06115a5eb38c6d",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +119,123 @@        prevLine: String =>\n          new GenericInternalRow(\n            prevLine.split(outputRowFormat).slice(0, 2).padTo(2, null)\n              .map(kvWriter))\n      }"
  },
  {
    "id" : "8266097c-023e-407a-bc36-1a2437d8c3fa",
    "prId" : 29421,
    "prUrl" : "https://github.com/apache/spark/pull/29421#pullrequestreview-516433966",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "00f40ead-a7b3-4ef2-ba44-d99b0571a81c",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@AngersZhuuuu, can you add a configuration to control the legacy behaviour, and add a note to the migration guide since it's arguable to say this is a bug? With that, I think I can leave my sign-off and merge.",
        "createdAt" : "2020-10-26T01:55:33Z",
        "updatedAt" : "2020-10-26T02:51:45Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "fdf3218b-656d-4bc8-9a66-a190c77666da",
        "parentId" : "00f40ead-a7b3-4ef2-ba44-d99b0571a81c",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> @AngersZhuuuu, can you add a configuration to control the legacy behaviour, and add a note to the migration guide since it's arguable to say this is a bug? With that, I think I can leave my sign-off and merge.\r\n\r\nThis suggestion looks good to me, but since we have fix a similar issue.  https://github.com/apache/spark/commit/c75a82794fc6a0f35697f8e1258562d43e860f68#diff-1bdf8d74f6b4f84d5acb1d1490503cde337fe64fe5d21339d7fedb06bd7cabc0\r\n\r\nHow about merge this and  I will follow a pr to add a configuration to control the legacy behavior.  Change these two point together seems to be better?",
        "createdAt" : "2020-10-26T02:17:36Z",
        "updatedAt" : "2020-10-26T02:51:45Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "3e99de3e-54b5-451e-8561-4631b00c89a7",
        "parentId" : "00f40ead-a7b3-4ef2-ba44-d99b0571a81c",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "SGMT",
        "createdAt" : "2020-10-26T02:19:36Z",
        "updatedAt" : "2020-10-26T02:51:45Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "e83f6a55e20e7f4edfa11ad91e06115a5eb38c6d",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +119,123 @@        prevLine: String =>\n          new GenericInternalRow(\n            prevLine.split(outputRowFormat).slice(0, 2).padTo(2, null)\n              .map(kvWriter))\n      }"
  }
]