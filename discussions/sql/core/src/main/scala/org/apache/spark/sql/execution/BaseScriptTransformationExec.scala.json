[
  {
    "id" : "ae10e953-97ca-4bfb-b588-e11cd7fe2e79",
    "prId" : 33751,
    "prUrl" : "https://github.com/apache/spark/pull/33751#pullrequestreview-730731405",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c03644fd-b3ec-4784-a5c6-00f1b9e9316a",
        "parentId" : null,
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "This is following string to date logic in BaseScriptTransformationExec",
        "createdAt" : "2021-08-16T13:43:55Z",
        "updatedAt" : "2021-08-16T13:43:55Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "7a03c24491af59c76744d791d76e06d68fe37b3f",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +226,230 @@      case YearMonthIntervalType(start, end) => wrapperConvertException(\n        data => IntervalUtils.castStringToYMInterval(UTF8String.fromString(data), start, end)\n          .map(IntervalUtils.monthsToPeriod).orNull, converter)\n      case DayTimeIntervalType(start, end) => wrapperConvertException(\n        data => IntervalUtils.microsToDuration("
  },
  {
    "id" : "9d9fa779-61bf-42a3-8dee-3a6518b27cf4",
    "prId" : 33363,
    "prUrl" : "https://github.com/apache/spark/pull/33363#pullrequestreview-709103168",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "85c6fd75-2269-4df4-9125-e2d8d223c3d6",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "nit:\r\n```\r\n        if (data == ioschema.outputRowFormatMap(\"TOK_TABLEROWFORMATNULL\")) {\r\n          null\r\n        } else {\r\n          try {\r\n            f(data)\r\n          } catch {\r\n            case NonFatal(_) => null\r\n          }\r\n        }\r\n```\r\n?",
        "createdAt" : "2021-07-16T02:00:21Z",
        "updatedAt" : "2021-07-16T02:03:48Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "5d9a4b1e-433e-48ca-be67-4d1a6bdc62cd",
        "parentId" : "85c6fd75-2269-4df4-9125-e2d8d223c3d6",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "Done",
        "createdAt" : "2021-07-19T02:14:09Z",
        "updatedAt" : "2021-07-19T02:14:09Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "4f44917dd2b2c789ea46ca365a1b60cd5fa4ee24",
    "line" : 33,
    "diffHunk" : "@@ -1,1 +247,251 @@          } catch {\n            case NonFatal(_) => null\n          }\n        }\n      }"
  },
  {
    "id" : "2df3776d-6899-4a7c-afcb-4e3d5fbac5a1",
    "prId" : 33363,
    "prUrl" : "https://github.com/apache/spark/pull/33363#pullrequestreview-712332671",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4cd124e1-be35-46f3-a424-991c2f59e4bf",
        "parentId" : null,
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "FYI @cloud-fan @maropu For this change, I need to explain is that. \r\nIf we not add parameter -1, \r\n```\r\n\"2@false@Spark SQL@\".split('@') will have three elems\r\n\"2\"\r\n\"false\"\r\n\"Spark SQL\"\r\n```\r\nSince we are strict handle data, we need to add -1 the result should be \r\n```\r\n\"2@false@Spark SQL@\".split('@', -1) will have four elems\r\n\"2\"\r\n\"false\"\r\n\"Spark SQL\"\r\n\"\"\r\n```",
        "createdAt" : "2021-07-22T03:09:14Z",
        "updatedAt" : "2021-07-22T03:09:29Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "4f44917dd2b2c789ea46ca365a1b60cd5fa4ee24",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +119,123 @@        prevLine: String =>\n          new GenericInternalRow(\n            prevLine.split(outputRowFormat, -1).padTo(outputFieldWriters.size, null)\n              .zip(outputFieldWriters)\n              .map { case (data, writer) => writer(data) })"
  },
  {
    "id" : "81df432b-6549-45f6-ac7e-827760cb7264",
    "prId" : 31046,
    "prUrl" : "https://github.com/apache/spark/pull/31046#pullrequestreview-562266780",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d7be9d19-1ab1-4626-b409-ac0854c638c8",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Oh, I don't know why I thought it's more than 5+. It's more than 6+ (https://github.com/databricks/scala-style-guide#imports).",
        "createdAt" : "2021-01-06T00:32:29Z",
        "updatedAt" : "2021-01-06T05:14:36Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "faa18df7-f558-408b-9f7c-cb3e21e286e0",
        "parentId" : "d7be9d19-1ab1-4626-b409-ac0854c638c8",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> Oh, I don't know why I thought it's more than 5+. It's more than 6+ (https://github.com/databricks/scala-style-guide#imports).\r\n\r\nSeems my local ideal's rule is 5, emmm I need to check this.",
        "createdAt" : "2021-01-06T01:29:15Z",
        "updatedAt" : "2021-01-06T05:14:36Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "db12e28a3cd5f3c19810f301a83c5667ff661058",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +18,22 @@package org.apache.spark.sql.execution\n\nimport java.io.{BufferedReader, File, InputStream, InputStreamReader, OutputStream}\nimport java.nio.charset.StandardCharsets\nimport java.util.concurrent.TimeUnit"
  },
  {
    "id" : "602e3c50-b413-4f23-b8f0-9f4bb28274ac",
    "prId" : 30973,
    "prUrl" : "https://github.com/apache/spark/pull/30973#pullrequestreview-560667110",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6e9c3314-2617-4e30-84fc-84113d2a7cff",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@AngersZhuuuu, is this required? In executor side, the root directory is already current working directory IIRC.",
        "createdAt" : "2021-01-03T00:53:14Z",
        "updatedAt" : "2021-01-04T05:07:05Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "902e6d7c-db2a-4431-9fcc-03ead85eaec2",
        "parentId" : "6e9c3314-2617-4e30-84fc-84113d2a7cff",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> @AngersZhuuuu, is this required? In executor side, the root directory is already current working directory IIRC.\r\n\r\nYea, can remove this ",
        "createdAt" : "2021-01-03T01:04:36Z",
        "updatedAt" : "2021-01-04T05:07:05Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "a32d1013-5784-4f97-9d3d-c0ed4538f0b0",
        "parentId" : "6e9c3314-2617-4e30-84fc-84113d2a7cff",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> @AngersZhuuuu, is this required? In executor side, the root directory is already current working directory IIRC.\r\n\r\nBut in Unit Test, driver node's working dir is not `SparkFile.getRootDirectory`. If remove this, test will failed.\r\n```\r\n /usr/bin/python: can't open file 'test-resource8890660714423343799.py': [Errno 2] No such file or directory\r\n```",
        "createdAt" : "2021-01-03T02:16:22Z",
        "updatedAt" : "2021-01-04T05:07:05Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "f65d22dc-1b83-4b89-9a9a-ed0923b185c7",
        "parentId" : "6e9c3314-2617-4e30-84fc-84113d2a7cff",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "That seems like because we're running with `local `cluster. If you run the tests with `local-cluster`, I guess it'd work. But okay, in this way we can cover `local` too.",
        "createdAt" : "2021-01-03T03:20:08Z",
        "updatedAt" : "2021-01-04T05:07:05Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "ca24dd60-30d6-4d41-8120-8048cbb65142",
        "parentId" : "6e9c3314-2617-4e30-84fc-84113d2a7cff",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "Yea,  It seems to have no effect on other tests. Cover local mode maybe better.",
        "createdAt" : "2021-01-03T03:29:01Z",
        "updatedAt" : "2021-01-04T05:07:05Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "7f2991dd-1801-4c6a-a27f-1becb061235a",
        "parentId" : "6e9c3314-2617-4e30-84fc-84113d2a7cff",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "And add `PathFInder` like hive, it cab support search file under all entry of `PATH`, should be better.",
        "createdAt" : "2021-01-03T03:30:28Z",
        "updatedAt" : "2021-01-04T05:07:05Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "91542f9e526af6d9a1f486d7f28240aa4594ac21",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +73,77 @@    val cmd = List(\"/bin/bash\", \"-c\", script)\n    val builder = new ProcessBuilder(cmd.asJava)\n      .directory(new File(SparkFiles.getRootDirectory()))\n    val path = System.getenv(\"PATH\") + File.pathSeparator +\n      SparkFiles.getRootDirectory()"
  },
  {
    "id" : "7439743b-fb6a-43fa-bb32-0b1d0d014e61",
    "prId" : 30973,
    "prUrl" : "https://github.com/apache/spark/pull/30973#pullrequestreview-562074661",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "938a472d-fa36-4a42-894d-224f3e7aa7a1",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "@AngersZhuuuu . nit. Please enumerate all next time.",
        "createdAt" : "2021-01-05T19:35:45Z",
        "updatedAt" : "2021-01-05T19:35:45Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "91542f9e526af6d9a1f486d7f28240aa4594ac21",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +18,22 @@package org.apache.spark.sql.execution\n\nimport java.io._\nimport java.nio.charset.StandardCharsets\nimport java.util.concurrent.TimeUnit"
  },
  {
    "id" : "30499563-d0fe-474d-92f1-9e84fa90de76",
    "prId" : 30957,
    "prUrl" : "https://github.com/apache/spark/pull/30957#pullrequestreview-584419813",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6ec48837-9907-455e-8398-ef2b3cd25232",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "nit format:\r\n```\r\n    input.map { _.dataType match {\r\n      case _: ArrayType | _: MapType | _: StructType =>\r\n        new StructsToJson(in).withTimeZone(conf.sessionLocalTimeZone)\r\n      case _ => Cast(in, StringType).withTimeZone(conf.sessionLocalTimeZone)\r\n    }\r\n```",
        "createdAt" : "2021-02-05T07:49:40Z",
        "updatedAt" : "2021-04-19T03:26:35Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "3680155a-c7f7-43c1-8e67-ea2a5c37252a",
        "parentId" : "6ec48837-9907-455e-8398-ef2b3cd25232",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> nit format:\r\n> \r\n> ```\r\n>     input.map { _.dataType match {\r\n>       case _: ArrayType | _: MapType | _: StructType =>\r\n>         new StructsToJson(in).withTimeZone(conf.sessionLocalTimeZone)\r\n>       case _ => Cast(in, StringType).withTimeZone(conf.sessionLocalTimeZone)\r\n>     }\r\n> ```\r\n\r\nIn this way, we miss value `in`, Hmmmm",
        "createdAt" : "2021-02-05T08:25:04Z",
        "updatedAt" : "2021-04-19T03:26:35Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "60d85cc9-6af3-4075-8081-c6c66612e55b",
        "parentId" : "6ec48837-9907-455e-8398-ef2b3cd25232",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Ah, I see. The current one looks okay.",
        "createdAt" : "2021-02-05T15:02:13Z",
        "updatedAt" : "2021-04-19T03:26:35Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "adf8a6682d6c9b2ff759c456d26e0d4358c0d965",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +48,52 @@\n  protected lazy val inputExpressionsWithoutSerde: Seq[Expression] = {\n    input.map { in =>\n      in.dataType match {\n        case _: ArrayType | _: MapType | _: StructType =>"
  }
]