[
  {
    "id" : "ae10e953-97ca-4bfb-b588-e11cd7fe2e79",
    "prId" : 33751,
    "prUrl" : "https://github.com/apache/spark/pull/33751#pullrequestreview-730731405",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c03644fd-b3ec-4784-a5c6-00f1b9e9316a",
        "parentId" : null,
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "This is following string to date logic in BaseScriptTransformationExec",
        "createdAt" : "2021-08-16T13:43:55Z",
        "updatedAt" : "2021-08-16T13:43:55Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "7a03c24491af59c76744d791d76e06d68fe37b3f",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +226,230 @@      case YearMonthIntervalType(start, end) => wrapperConvertException(\n        data => IntervalUtils.castStringToYMInterval(UTF8String.fromString(data), start, end)\n          .map(IntervalUtils.monthsToPeriod).orNull, converter)\n      case DayTimeIntervalType(start, end) => wrapperConvertException(\n        data => IntervalUtils.microsToDuration("
  },
  {
    "id" : "9d9fa779-61bf-42a3-8dee-3a6518b27cf4",
    "prId" : 33363,
    "prUrl" : "https://github.com/apache/spark/pull/33363#pullrequestreview-709103168",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "85c6fd75-2269-4df4-9125-e2d8d223c3d6",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "nit:\r\n```\r\n        if (data == ioschema.outputRowFormatMap(\"TOK_TABLEROWFORMATNULL\")) {\r\n          null\r\n        } else {\r\n          try {\r\n            f(data)\r\n          } catch {\r\n            case NonFatal(_) => null\r\n          }\r\n        }\r\n```\r\n?",
        "createdAt" : "2021-07-16T02:00:21Z",
        "updatedAt" : "2021-07-16T02:03:48Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "5d9a4b1e-433e-48ca-be67-4d1a6bdc62cd",
        "parentId" : "85c6fd75-2269-4df4-9125-e2d8d223c3d6",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "Done",
        "createdAt" : "2021-07-19T02:14:09Z",
        "updatedAt" : "2021-07-19T02:14:09Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "4f44917dd2b2c789ea46ca365a1b60cd5fa4ee24",
    "line" : 33,
    "diffHunk" : "@@ -1,1 +247,251 @@          } catch {\n            case NonFatal(_) => null\n          }\n        }\n      }"
  },
  {
    "id" : "2df3776d-6899-4a7c-afcb-4e3d5fbac5a1",
    "prId" : 33363,
    "prUrl" : "https://github.com/apache/spark/pull/33363#pullrequestreview-712332671",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4cd124e1-be35-46f3-a424-991c2f59e4bf",
        "parentId" : null,
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "FYI @cloud-fan @maropu For this change, I need to explain is that. \r\nIf we not add parameter -1, \r\n```\r\n\"2@false@Spark SQL@\".split('@') will have three elems\r\n\"2\"\r\n\"false\"\r\n\"Spark SQL\"\r\n```\r\nSince we are strict handle data, we need to add -1 the result should be \r\n```\r\n\"2@false@Spark SQL@\".split('@', -1) will have four elems\r\n\"2\"\r\n\"false\"\r\n\"Spark SQL\"\r\n\"\"\r\n```",
        "createdAt" : "2021-07-22T03:09:14Z",
        "updatedAt" : "2021-07-22T03:09:29Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "4f44917dd2b2c789ea46ca365a1b60cd5fa4ee24",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +119,123 @@        prevLine: String =>\n          new GenericInternalRow(\n            prevLine.split(outputRowFormat, -1).padTo(outputFieldWriters.size, null)\n              .zip(outputFieldWriters)\n              .map { case (data, writer) => writer(data) })"
  },
  {
    "id" : "81df432b-6549-45f6-ac7e-827760cb7264",
    "prId" : 31046,
    "prUrl" : "https://github.com/apache/spark/pull/31046#pullrequestreview-562266780",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d7be9d19-1ab1-4626-b409-ac0854c638c8",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Oh, I don't know why I thought it's more than 5+. It's more than 6+ (https://github.com/databricks/scala-style-guide#imports).",
        "createdAt" : "2021-01-06T00:32:29Z",
        "updatedAt" : "2021-01-06T05:14:36Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "faa18df7-f558-408b-9f7c-cb3e21e286e0",
        "parentId" : "d7be9d19-1ab1-4626-b409-ac0854c638c8",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> Oh, I don't know why I thought it's more than 5+. It's more than 6+ (https://github.com/databricks/scala-style-guide#imports).\r\n\r\nSeems my local ideal's rule is 5, emmm I need to check this.",
        "createdAt" : "2021-01-06T01:29:15Z",
        "updatedAt" : "2021-01-06T05:14:36Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "db12e28a3cd5f3c19810f301a83c5667ff661058",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +18,22 @@package org.apache.spark.sql.execution\n\nimport java.io.{BufferedReader, File, InputStream, InputStreamReader, OutputStream}\nimport java.nio.charset.StandardCharsets\nimport java.util.concurrent.TimeUnit"
  },
  {
    "id" : "602e3c50-b413-4f23-b8f0-9f4bb28274ac",
    "prId" : 30973,
    "prUrl" : "https://github.com/apache/spark/pull/30973#pullrequestreview-560667110",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6e9c3314-2617-4e30-84fc-84113d2a7cff",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@AngersZhuuuu, is this required? In executor side, the root directory is already current working directory IIRC.",
        "createdAt" : "2021-01-03T00:53:14Z",
        "updatedAt" : "2021-01-04T05:07:05Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "902e6d7c-db2a-4431-9fcc-03ead85eaec2",
        "parentId" : "6e9c3314-2617-4e30-84fc-84113d2a7cff",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> @AngersZhuuuu, is this required? In executor side, the root directory is already current working directory IIRC.\r\n\r\nYea, can remove this ",
        "createdAt" : "2021-01-03T01:04:36Z",
        "updatedAt" : "2021-01-04T05:07:05Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "a32d1013-5784-4f97-9d3d-c0ed4538f0b0",
        "parentId" : "6e9c3314-2617-4e30-84fc-84113d2a7cff",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> @AngersZhuuuu, is this required? In executor side, the root directory is already current working directory IIRC.\r\n\r\nBut in Unit Test, driver node's working dir is not `SparkFile.getRootDirectory`. If remove this, test will failed.\r\n```\r\n /usr/bin/python: can't open file 'test-resource8890660714423343799.py': [Errno 2] No such file or directory\r\n```",
        "createdAt" : "2021-01-03T02:16:22Z",
        "updatedAt" : "2021-01-04T05:07:05Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "f65d22dc-1b83-4b89-9a9a-ed0923b185c7",
        "parentId" : "6e9c3314-2617-4e30-84fc-84113d2a7cff",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "That seems like because we're running with `local `cluster. If you run the tests with `local-cluster`, I guess it'd work. But okay, in this way we can cover `local` too.",
        "createdAt" : "2021-01-03T03:20:08Z",
        "updatedAt" : "2021-01-04T05:07:05Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "ca24dd60-30d6-4d41-8120-8048cbb65142",
        "parentId" : "6e9c3314-2617-4e30-84fc-84113d2a7cff",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "Yea,  It seems to have no effect on other tests. Cover local mode maybe better.",
        "createdAt" : "2021-01-03T03:29:01Z",
        "updatedAt" : "2021-01-04T05:07:05Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "7f2991dd-1801-4c6a-a27f-1becb061235a",
        "parentId" : "6e9c3314-2617-4e30-84fc-84113d2a7cff",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "And add `PathFInder` like hive, it cab support search file under all entry of `PATH`, should be better.",
        "createdAt" : "2021-01-03T03:30:28Z",
        "updatedAt" : "2021-01-04T05:07:05Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "91542f9e526af6d9a1f486d7f28240aa4594ac21",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +73,77 @@    val cmd = List(\"/bin/bash\", \"-c\", script)\n    val builder = new ProcessBuilder(cmd.asJava)\n      .directory(new File(SparkFiles.getRootDirectory()))\n    val path = System.getenv(\"PATH\") + File.pathSeparator +\n      SparkFiles.getRootDirectory()"
  },
  {
    "id" : "7439743b-fb6a-43fa-bb32-0b1d0d014e61",
    "prId" : 30973,
    "prUrl" : "https://github.com/apache/spark/pull/30973#pullrequestreview-562074661",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "938a472d-fa36-4a42-894d-224f3e7aa7a1",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "@AngersZhuuuu . nit. Please enumerate all next time.",
        "createdAt" : "2021-01-05T19:35:45Z",
        "updatedAt" : "2021-01-05T19:35:45Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "91542f9e526af6d9a1f486d7f28240aa4594ac21",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +18,22 @@package org.apache.spark.sql.execution\n\nimport java.io._\nimport java.nio.charset.StandardCharsets\nimport java.util.concurrent.TimeUnit"
  },
  {
    "id" : "30499563-d0fe-474d-92f1-9e84fa90de76",
    "prId" : 30957,
    "prUrl" : "https://github.com/apache/spark/pull/30957#pullrequestreview-584419813",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6ec48837-9907-455e-8398-ef2b3cd25232",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "nit format:\r\n```\r\n    input.map { _.dataType match {\r\n      case _: ArrayType | _: MapType | _: StructType =>\r\n        new StructsToJson(in).withTimeZone(conf.sessionLocalTimeZone)\r\n      case _ => Cast(in, StringType).withTimeZone(conf.sessionLocalTimeZone)\r\n    }\r\n```",
        "createdAt" : "2021-02-05T07:49:40Z",
        "updatedAt" : "2021-04-19T03:26:35Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "3680155a-c7f7-43c1-8e67-ea2a5c37252a",
        "parentId" : "6ec48837-9907-455e-8398-ef2b3cd25232",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> nit format:\r\n> \r\n> ```\r\n>     input.map { _.dataType match {\r\n>       case _: ArrayType | _: MapType | _: StructType =>\r\n>         new StructsToJson(in).withTimeZone(conf.sessionLocalTimeZone)\r\n>       case _ => Cast(in, StringType).withTimeZone(conf.sessionLocalTimeZone)\r\n>     }\r\n> ```\r\n\r\nIn this way, we miss value `in`, Hmmmm",
        "createdAt" : "2021-02-05T08:25:04Z",
        "updatedAt" : "2021-04-19T03:26:35Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "60d85cc9-6af3-4075-8081-c6c66612e55b",
        "parentId" : "6ec48837-9907-455e-8398-ef2b3cd25232",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Ah, I see. The current one looks okay.",
        "createdAt" : "2021-02-05T15:02:13Z",
        "updatedAt" : "2021-04-19T03:26:35Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "adf8a6682d6c9b2ff759c456d26e0d4358c0d965",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +48,52 @@\n  protected lazy val inputExpressionsWithoutSerde: Seq[Expression] = {\n    input.map { in =>\n      in.dataType match {\n        case _: ArrayType | _: MapType | _: StructType =>"
  },
  {
    "id" : "0b81bd7a-1407-4fab-898f-2b0ed8a77ac6",
    "prId" : 30156,
    "prUrl" : "https://github.com/apache/spark/pull/30156#pullrequestreview-517314795",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e60bd403-b0c7-4a94-abab-7c74b713fb62",
        "parentId" : null,
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "pass as a func to avoid repeating this logic",
        "createdAt" : "2020-10-27T02:38:17Z",
        "updatedAt" : "2020-10-30T05:00:51Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "357fd27d-a87b-47b7-9442-496a36d34ed4",
        "parentId" : "e60bd403-b0c7-4a94-abab-7c74b713fb62",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Nice.",
        "createdAt" : "2020-10-27T03:01:13Z",
        "updatedAt" : "2020-10-30T05:00:51Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "0541ff50ba75278e984dd3414efed8dc5c339881",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +110,114 @@      } else {\n        (arr: Array[String], size: Int) => arr\n      }\n      val processRowWithoutSerde = if (!ioschema.schemaLess) {\n        prevLine: String =>"
  },
  {
    "id" : "49bfa58b-1a4e-41d8-8893-83233648c203",
    "prId" : 30156,
    "prUrl" : "https://github.com/apache/spark/pull/30156#pullrequestreview-520402882",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0907fe5d-4cca-4612-bd1f-031fb8e3a423",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "The config name sounds like padding is the legacy behavior.",
        "createdAt" : "2020-10-30T06:27:44Z",
        "updatedAt" : "2020-10-30T06:27:45Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "0541ff50ba75278e984dd3414efed8dc5c339881",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +106,110 @@      val outputRowFormat = ioschema.outputRowFormatMap(\"TOK_TABLEROWFORMATFIELD\")\n\n      val padNull = if (conf.legacyPadNullWhenValueLessThenSchema) {\n        (arr: Array[String], size: Int) => arr.padTo(size, null)\n      } else {"
  },
  {
    "id" : "92b6af9c-a914-4289-8870-2d801ac98b72",
    "prId" : 29421,
    "prUrl" : "https://github.com/apache/spark/pull/29421#pullrequestreview-473140284",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dac062f2-cb16-4b07-badb-c6a2336915c8",
        "parentId" : null,
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "@maropu \r\nthis question https://github.com/apache/spark/pull/29421/files#r469889057\r\n fixed by  this line. \r\n\r\nand will rase a new pr for https://issues.apache.org/jira/browse/SPARK-32667",
        "createdAt" : "2020-08-20T11:08:58Z",
        "updatedAt" : "2020-10-26T02:51:45Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "b1528549-bf40-449d-97fe-392449dd968e",
        "parentId" : "dac062f2-cb16-4b07-badb-c6a2336915c8",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "@maropu How about split this fix as a new one, since this fix is very necessary and no disputed ",
        "createdAt" : "2020-08-24T01:11:40Z",
        "updatedAt" : "2020-10-26T02:51:45Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "241a3afd-122a-43b9-81b6-41ba4edc571c",
        "parentId" : "dac062f2-cb16-4b07-badb-c6a2336915c8",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "If it's a trivial fix, IMO its okay to include it in this PR.",
        "createdAt" : "2020-08-24T06:03:13Z",
        "updatedAt" : "2020-10-26T02:51:45Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "e83f6a55e20e7f4edfa11ad91e06115a5eb38c6d",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +120,124 @@          new GenericInternalRow(\n            prevLine.split(outputRowFormat).slice(0, 2).padTo(2, null)\n              .map(kvWriter))\n      }\n"
  },
  {
    "id" : "8dfb6599-0f14-4fac-9757-ad5562b00899",
    "prId" : 29421,
    "prUrl" : "https://github.com/apache/spark/pull/29421#pullrequestreview-478262115",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5e6faebc-1175-4224-9c35-20da02062f8b",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "nit: could you leave comments about why we need `.padTo(2, null)` here.",
        "createdAt" : "2020-08-31T01:20:56Z",
        "updatedAt" : "2020-10-26T02:51:45Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "8c325175-17d7-4b82-85ae-812d8403e471",
        "parentId" : "5e6faebc-1175-4224-9c35-20da02062f8b",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> nit: could you leave comments about why we need `.padTo(2, null)` here.\r\n\r\nSee the latest change, I have changed comment leaved before",
        "createdAt" : "2020-08-31T01:38:48Z",
        "updatedAt" : "2020-10-26T02:51:45Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "e83f6a55e20e7f4edfa11ad91e06115a5eb38c6d",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +119,123 @@        prevLine: String =>\n          new GenericInternalRow(\n            prevLine.split(outputRowFormat).slice(0, 2).padTo(2, null)\n              .map(kvWriter))\n      }"
  },
  {
    "id" : "8266097c-023e-407a-bc36-1a2437d8c3fa",
    "prId" : 29421,
    "prUrl" : "https://github.com/apache/spark/pull/29421#pullrequestreview-516433966",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "00f40ead-a7b3-4ef2-ba44-d99b0571a81c",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@AngersZhuuuu, can you add a configuration to control the legacy behaviour, and add a note to the migration guide since it's arguable to say this is a bug? With that, I think I can leave my sign-off and merge.",
        "createdAt" : "2020-10-26T01:55:33Z",
        "updatedAt" : "2020-10-26T02:51:45Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "fdf3218b-656d-4bc8-9a66-a190c77666da",
        "parentId" : "00f40ead-a7b3-4ef2-ba44-d99b0571a81c",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> @AngersZhuuuu, can you add a configuration to control the legacy behaviour, and add a note to the migration guide since it's arguable to say this is a bug? With that, I think I can leave my sign-off and merge.\r\n\r\nThis suggestion looks good to me, but since we have fix a similar issue.  https://github.com/apache/spark/commit/c75a82794fc6a0f35697f8e1258562d43e860f68#diff-1bdf8d74f6b4f84d5acb1d1490503cde337fe64fe5d21339d7fedb06bd7cabc0\r\n\r\nHow about merge this and  I will follow a pr to add a configuration to control the legacy behavior.  Change these two point together seems to be better?",
        "createdAt" : "2020-10-26T02:17:36Z",
        "updatedAt" : "2020-10-26T02:51:45Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "3e99de3e-54b5-451e-8561-4631b00c89a7",
        "parentId" : "00f40ead-a7b3-4ef2-ba44-d99b0571a81c",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "SGMT",
        "createdAt" : "2020-10-26T02:19:36Z",
        "updatedAt" : "2020-10-26T02:51:45Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "e83f6a55e20e7f4edfa11ad91e06115a5eb38c6d",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +119,123 @@        prevLine: String =>\n          new GenericInternalRow(\n            prevLine.split(outputRowFormat).slice(0, 2).padTo(2, null)\n              .map(kvWriter))\n      }"
  },
  {
    "id" : "ed12852e-336a-4dca-9aef-27a90975b4c0",
    "prId" : 29085,
    "prUrl" : "https://github.com/apache/spark/pull/29085#pullrequestreview-448718877",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "49dacb6a-5620-477b-9685-52c43d001dd0",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "nit: we don't need line breaks?\r\n```\r\n  def inputRowFormat: Seq[(String, String)]\r\n  def outputRowFormat: Seq[(String, String)]\r\n  def inputSerdeClass: Option[String]\r\n  def outputSerdeClass: Option[String]\r\n  def inputSerdeProps: Seq[(String, String)]\r\n  def outputSerdeProps: Seq[(String, String)]\r\n  def recordReaderClass: Option[String]\r\n  def recordWriterClass: Option[String]\r\n  def schemaLess: Boolean\r\n```",
        "createdAt" : "2020-07-15T04:17:00Z",
        "updatedAt" : "2020-07-23T03:08:36Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "450c1122-5d81-4680-b496-3fef2b7ddd76",
        "parentId" : "49dacb6a-5620-477b-9685-52c43d001dd0",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "Not need, without blank line looks better.",
        "createdAt" : "2020-07-15T08:02:22Z",
        "updatedAt" : "2020-07-23T03:08:36Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "03d3409f6ca641a4f10fdc2ac71479445220f676",
    "line" : 215,
    "diffHunk" : "@@ -1,1 +247,251 @@  def stderrBuffer: CircularBuffer\n  def taskContext: TaskContext\n  def conf: Configuration\n\n  setDaemon(true)"
  },
  {
    "id" : "84e33c1d-43c8-4fbd-a223-0f0a67b1760d",
    "prId" : 29085,
    "prUrl" : "https://github.com/apache/spark/pull/29085#pullrequestreview-450528133",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fb0eee51-f2aa-4159-b136-82d35e9627f1",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "nit:\r\n```\r\n  protected def processOutputWithoutSerde(prevLine: String, reader: BufferedReader): InternalRow = {\r\n    val limit = if (ioschema.schemaLess) 2 else 0\r\n    new GenericInternalRow(\r\n      prevLine.split(ioschema.outputRowFormatMap(\"TOK_TABLEROWFORMATFIELD\"), limit)\r\n        .zip(fieldWriters)\r\n        .map { case (data, writer) => writer(data) })\r\n  }\r\n```\r\n?",
        "createdAt" : "2020-07-17T08:34:19Z",
        "updatedAt" : "2020-07-23T03:08:36Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "8ef590ef-62f3-40dd-9640-262518bd9584",
        "parentId" : "fb0eee51-f2aa-4159-b136-82d35e9627f1",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "Done",
        "createdAt" : "2020-07-17T09:54:58Z",
        "updatedAt" : "2020-07-23T03:08:36Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "03d3409f6ca641a4f10fdc2ac71479445220f676",
    "line" : 131,
    "diffHunk" : "@@ -1,1 +152,156 @@        processRowWithoutSerde(prevLine)\n      }\n    }\n  }\n"
  },
  {
    "id" : "65364448-eb82-473b-84f2-64c7c79fa1e3",
    "prId" : 29085,
    "prUrl" : "https://github.com/apache/spark/pull/29085#pullrequestreview-451032984",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1f822298-5a01-4064-a3cd-f59ca4211433",
        "parentId" : null,
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "@maropu  Here I change for support schema less mode.\r\n\r\nIn test case I choose not to use sql since hive serde can't support schemaless mode well in spark's way.\r\n```\r\n[info] - SPARK-25990: TRANSFORM should handle schema less correctly *** FAILED *** (360 milliseconds)\r\n[info]   Results do not match for Spark plan:\r\n[info]    HiveScriptTransformation [a#86, b#87, c#88, d#89, e#90], python /Users/angerszhu/Documents/project/AngersZhu/spark/sql/core/target/test-classes/test_script.py, [key#96, value#97], ScriptTransformationIOSchema(List(),List(),Some(org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe),Some(org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe),List((field.delim,\t)),List((field.delim,\t)),Some(org.apache.hadoop.hive.ql.exec.TextRecordReader),Some(org.apache.hadoop.hive.ql.exec.TextRecordWriter),true)\r\n[info]   +- Project [_1#75 AS a#86, _2#76 AS b#87, _3#77 AS c#88, _4#78 AS d#89, _5#79 AS e#90]\r\n[info]      +- LocalTableScan [_1#75, _2#76, _3#77, _4#78, _5#79]\r\n[info]\r\n[info]\r\n[info]    == Results ==\r\n[info]    !== Expected Answer - 3 ==                                == Actual Answer - 3 ==\r\n[info]   ![1,1\t1.0\t1.000000000000000000\t1969-12-31 16:00:00.001]   [1,1]\r\n[info]   ![2,2\t2.0\t2.000000000000000000\t1969-12-31 16:00:00.002]   [2,2]\r\n[info]   ![3,3\t3.0\t3.000000000000000000\t1969-12-31 16:00:00.003]   [3,3] (SparkPlanTest.scala:96)\r\n[info]   org.scalatest.exceptions.TestFailedException:\r\n[info]   at org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:530)\r\n[info]   at org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:529)\r\n[info]   at org.scalatest.FunSuite.newAssertionFailedException(FunSuite.scala:1560)\r\n[i\r\n```",
        "createdAt" : "2020-07-18T05:24:27Z",
        "updatedAt" : "2020-07-23T03:08:36Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "03d3409f6ca641a4f10fdc2ac71479445220f676",
    "line" : 102,
    "diffHunk" : "@@ -1,1 +123,127 @@              .map(kvWriter))\n      }\n\n      override def hasNext: Boolean = {\n        try {"
  },
  {
    "id" : "af01caa3-8685-439f-8394-668fab696a59",
    "prId" : 29085,
    "prUrl" : "https://github.com/apache/spark/pull/29085#pullrequestreview-451098996",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "31d2a0c7-5df0-4fa9-a7f4-346453602c56",
        "parentId" : null,
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "@marope Add type convert for BooleanType and BinaryType, and I have add it in UT `test(\"SPARK-32106: TRANSFORM should support more data types (no serde)\")`",
        "createdAt" : "2020-07-19T05:02:54Z",
        "updatedAt" : "2020-07-23T03:08:36Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "03d3409f6ca641a4f10fdc2ac71479445220f676",
    "line" : 150,
    "diffHunk" : "@@ -1,1 +191,195 @@      case BinaryType =>\n        wrapperConvertException(data => UTF8String.fromString(data).getBytes, converter)\n      case IntegerType => wrapperConvertException(data => data.toInt, converter)\n      case ShortType => wrapperConvertException(data => data.toShort, converter)\n      case LongType => wrapperConvertException(data => data.toLong, converter)"
  },
  {
    "id" : "1cf2933c-b61e-4460-aea7-df07ae7d9445",
    "prId" : 29085,
    "prUrl" : "https://github.com/apache/spark/pull/29085#pullrequestreview-453001905",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "19bc4729-ae5f-4609-b1f6-4f3362f01d18",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Could you add tests in `BaseScriptTransformationSuite.scala ` with this flag enabled/disabled?",
        "createdAt" : "2020-07-22T02:06:05Z",
        "updatedAt" : "2020-07-23T03:08:36Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "51c1140d-814c-4ce3-837f-71737a531462",
        "parentId" : "19bc4729-ae5f-4609-b1f6-4f3362f01d18",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> Could you add tests in `BaseScriptTransformationSuite.scala ` with this flag enabled/disabled?\r\n\r\nDone, see\r\n```\r\n test(\"SPARK-32106: TRANSFORM should support all data types as input (no serde)\") {\r\n    assume(TestUtils.testCommandAvailable(\"python\"))\r\n    Array(false, true).foreach { java8AapiEnable =>\r\n      withSQLConf(SQLConf.DATETIME_JAVA8API_ENABLED.key -> java8AapiEnable.toString) {\r\n        withTempView(\"v\") {\r\n               ......\r\n               .....\r\n       }\r\n     }\r\n   }\r\n}\r\n```",
        "createdAt" : "2020-07-22T05:21:30Z",
        "updatedAt" : "2020-07-23T03:08:36Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "03d3409f6ca641a4f10fdc2ac71479445220f676",
    "line" : 156,
    "diffHunk" : "@@ -1,1 +197,201 @@      case DoubleType => wrapperConvertException(data => data.toDouble, converter)\n      case _: DecimalType => wrapperConvertException(data => BigDecimal(data), converter)\n      case DateType if conf.datetimeJava8ApiEnabled =>\n        wrapperConvertException(data => DateTimeUtils.stringToDate(\n          UTF8String.fromString(data),"
  },
  {
    "id" : "0c0aa086-6e0a-414d-a64a-acd950d041a0",
    "prId" : 29085,
    "prUrl" : "https://github.com/apache/spark/pull/29085#pullrequestreview-452939092",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6a1f8801-94cd-409c-a497-1b3ba9f17e1e",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "ditto",
        "createdAt" : "2020-07-22T02:06:14Z",
        "updatedAt" : "2020-07-23T03:08:36Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "03d3409f6ca641a4f10fdc2ac71479445220f676",
    "line" : 165,
    "diffHunk" : "@@ -1,1 +206,210 @@        DateTimeUtils.getZoneId(conf.sessionLocalTimeZone))\n        .map(DateTimeUtils.toJavaDate).orNull, converter)\n      case TimestampType if conf.datetimeJava8ApiEnabled =>\n         wrapperConvertException(data => DateTimeUtils.stringToTimestamp(\n          UTF8String.fromString(data),"
  }
]