[
  {
    "id" : "50cec823-3ced-4571-8680-08fba445cc60",
    "prId" : 32933,
    "prUrl" : "https://github.com/apache/spark/pull/32933#pullrequestreview-692425879",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6d80c40d-e8da-4bea-97da-c86233b68320",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Is this the change for review in this PR?",
        "createdAt" : "2021-06-23T01:45:57Z",
        "updatedAt" : "2021-06-23T01:45:58Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "ab8545fb-f3b8-4810-a02d-9c753b8aadaa",
        "parentId" : "6d80c40d-e8da-4bea-97da-c86233b68320",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "That's right.",
        "createdAt" : "2021-06-25T03:44:39Z",
        "updatedAt" : "2021-06-25T03:44:39Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "ea949836c26be0124e37df0eb641be806628d94d",
    "line" : 35,
    "diffHunk" : "@@ -1,1 +223,227 @@   *   set of SST files.\n   */\n  def deleteOldVersions(numVersionsToRetain: Int): Unit = {\n    val path = new Path(dfsRootDir)\n"
  },
  {
    "id" : "c1506b51-9165-49cd-98cd-63d3ed83bbb1",
    "prId" : 32933,
    "prUrl" : "https://github.com/apache/spark/pull/32933#pullrequestreview-695921325",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8859dbe4-e175-4d4c-9bd7-ba3cd4902e55",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "What the second case for? When it will happen?",
        "createdAt" : "2021-06-23T01:50:13Z",
        "updatedAt" : "2021-06-23T01:50:13Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "1825e3d2-ae49-4022-957e-d3532b635bf8",
        "parentId" : "8859dbe4-e175-4d4c-9bd7-ba3cd4902e55",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "If I understand correctly, it would occur with reattempt of same micro-batch.",
        "createdAt" : "2021-06-28T09:36:58Z",
        "updatedAt" : "2021-06-28T12:04:27Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "65d747e9-d791-43af-a6cd-9a5f2cf7d067",
        "parentId" : "8859dbe4-e175-4d4c-9bd7-ba3cd4902e55",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "Yep, that's right.",
        "createdAt" : "2021-06-30T09:49:22Z",
        "updatedAt" : "2021-06-30T09:49:22Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "ea949836c26be0124e37df0eb641be806628d94d",
    "line" : 33,
    "diffHunk" : "@@ -1,1 +221,225 @@   * - Partially written SST files\n   * - SST files that were used in a version, but that version got overwritten with a different\n   *   set of SST files.\n   */\n  def deleteOldVersions(numVersionsToRetain: Int): Unit = {"
  },
  {
    "id" : "2dcf4962-7cd1-4418-8849-d44aaad9b92a",
    "prId" : 32933,
    "prUrl" : "https://github.com/apache/spark/pull/32933#pullrequestreview-696193810",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bb238ce0-c763-4228-bb04-e4793229407d",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Is localTempDir used only here? Just to make sure deleting the directory won't break anything.",
        "createdAt" : "2021-06-28T09:58:22Z",
        "updatedAt" : "2021-06-28T12:04:27Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "fbe54132-4dc1-4b3d-9316-8b0c6157617c",
        "parentId" : "bb238ce0-c763-4228-bb04-e4793229407d",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "Yes, the `localTempDir` is only used for unzip files.",
        "createdAt" : "2021-06-30T13:54:14Z",
        "updatedAt" : "2021-06-30T13:54:15Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "ea949836c26be0124e37df0eb641be806628d94d",
    "line" : 114,
    "diffHunk" : "@@ -1,1 +392,396 @@  /** Get the SST files required for a version from the version zip file in DFS */\n  private def getImmutableFilesFromVersionZip(version: Long): Seq[RocksDBImmutableFile] = {\n    Utils.deleteRecursively(localTempDir)\n    localTempDir.mkdirs()\n    Utils.unzipFilesFromFile(fs, dfsBatchZipFile(version), localTempDir)"
  },
  {
    "id" : "bf060d2d-8a7d-42e3-b1e8-85ee93266f23",
    "prId" : 32767,
    "prUrl" : "https://github.com/apache/spark/pull/32767#pullrequestreview-690430800",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2a010acf-753c-4eb4-9cf1-d5c753926702",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "When it is possible to have existing file in local dir which has same file name but not the same file in DFS?",
        "createdAt" : "2021-06-22T06:47:26Z",
        "updatedAt" : "2021-06-22T06:51:07Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "ac7291d7-c552-4ad2-a400-7158000ca68b",
        "parentId" : "2a010acf-753c-4eb4-9cf1-d5c753926702",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Or just a safer guard?",
        "createdAt" : "2021-06-22T06:47:40Z",
        "updatedAt" : "2021-06-22T06:51:07Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "d6f4101b-5d13-4cf7-9dbc-ab5942350253",
        "parentId" : "2a010acf-753c-4eb4-9cf1-d5c753926702",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "A safer guard for checking both file names and file size.",
        "createdAt" : "2021-06-23T09:38:18Z",
        "updatedAt" : "2021-06-23T09:38:18Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "7279d434ddba27a93924577458f8dedf6bf340c5",
    "line" : 89,
    "diffHunk" : "@@ -1,1 +263,267 @@          existingFile.delete()\n          logInfo(s\"Deleted local file $existingFile\")\n        }\n      }\n"
  },
  {
    "id" : "ca84fd02-a693-4592-ab5c-d3406261b338",
    "prId" : 32767,
    "prUrl" : "https://github.com/apache/spark/pull/32767#pullrequestreview-690437137",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2882ef77-386b-4f9d-adbb-24e867d85ba6",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Is this only used by tests?",
        "createdAt" : "2021-06-22T06:55:27Z",
        "updatedAt" : "2021-06-22T06:55:28Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "b8c9152e-4468-4c98-bab0-02e73ea3cb5f",
        "parentId" : "2882ef77-386b-4f9d-adbb-24e867d85ba6",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "Yes.",
        "createdAt" : "2021-06-23T09:44:47Z",
        "updatedAt" : "2021-06-23T09:44:47Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "7279d434ddba27a93924577458f8dedf6bf340c5",
    "line" : 54,
    "diffHunk" : "@@ -1,1 +188,192 @@\n  /** Get the latest version available in the DFS directory. If no data present, it returns 0. */\n  def getLatestVersion(): Long = {\n    val path = new Path(dfsRootDir)\n    if (fm.exists(path)) {"
  },
  {
    "id" : "8913b6de-e2eb-42d4-bcc1-7c0208d89d02",
    "prId" : 32767,
    "prUrl" : "https://github.com/apache/spark/pull/32767#pullrequestreview-690452537",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "04ad81aa-48e8-43f3-9fc5-d0ecda1e0d95",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "What if we just remove the all files in localDir? Just would like to know the reason we don't clear the directory but just remove the specific files. Would we need to leverage some remaining files?",
        "createdAt" : "2021-06-22T07:31:47Z",
        "updatedAt" : "2021-06-22T07:44:26Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "46c0d5a7-2c41-465e-a226-b18ee9c7cdb2",
        "parentId" : "04ad81aa-48e8-43f3-9fc5-d0ecda1e0d95",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "Yes. The consideration here is mainly for immutable files like sst/log files. We can avoid IO for the immutable files shared among different versions.",
        "createdAt" : "2021-06-23T10:00:13Z",
        "updatedAt" : "2021-06-23T10:00:13Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "7279d434ddba27a93924577458f8dedf6bf340c5",
    "line" : 36,
    "diffHunk" : "@@ -1,1 +170,174 @@      RocksDBCheckpointMetadata(Seq.empty, 0)\n    } else {\n      // Delete all non-immutable files in local dir, and unzip new ones from DFS commit file\n      listRocksDBFiles(localDir)._2.foreach(_.delete())\n      Utils.unzipFilesFromFile(fs, dfsBatchZipFile(version), localDir)"
  },
  {
    "id" : "4c6c5352-d52f-45b9-b36f-b875b83fd05b",
    "prId" : 32582,
    "prUrl" : "https://github.com/apache/spark/pull/32582#pullrequestreview-662202504",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "eee0dee8-c674-40af-856d-6e7fbd00d2d1",
        "parentId" : null,
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "Per https://github.com/apache/spark/pull/32272#discussion_r624388116, we only use prettyJson in the log.",
        "createdAt" : "2021-05-18T14:59:51Z",
        "updatedAt" : "2021-05-18T14:59:52Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "c05b140f73f855a148ccedaf753841fb94a4ab1d",
    "line" : 127,
    "diffHunk" : "@@ -1,1 +140,144 @@    val metadataFile = localMetadataFile(checkpointDir)\n    metadata.writeToFile(metadataFile)\n    logInfo(s\"Written metadata for version $version:\\n${metadata.prettyJson}\")\n\n    if (version <= 1 && numKeys == 0) {"
  },
  {
    "id" : "969cdaaf-cf6c-495b-a9be-d0e11ec2ca34",
    "prId" : 32582,
    "prUrl" : "https://github.com/apache/spark/pull/32582#pullrequestreview-674051699",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4b701ee1-4b35-4a5e-8856-117fa2099f37",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Just to confirm, could I safely assume versionToRocksDBFiles will be loaded when RocksDBFileManager is initialized with existing checkpoint in further PR?",
        "createdAt" : "2021-05-30T05:50:45Z",
        "updatedAt" : "2021-05-30T07:54:27Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "921bbe5a-e651-4686-a34d-d244c16ad6a1",
        "parentId" : "4b701ee1-4b35-4a5e-8856-117fa2099f37",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "That's right. `versionToRocksDBFiles` was touched in the following 3 places:\r\n- saveCheckpointToDfs (this PR)\r\n- deleteOldVersions\r\n- loadCheckpointFromDfs",
        "createdAt" : "2021-06-02T11:01:32Z",
        "updatedAt" : "2021-06-02T11:01:32Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "c05b140f73f855a148ccedaf753841fb94a4ab1d",
    "line" : 115,
    "diffHunk" : "@@ -1,1 +128,132 @@  import RocksDBImmutableFile._\n\n  private val versionToRocksDBFiles = new ConcurrentHashMap[Long, Seq[RocksDBImmutableFile]]\n  private lazy val fm = CheckpointFileManager.create(new Path(dfsRootDir), hadoopConf)\n  private val fs = new Path(dfsRootDir).getFileSystem(hadoopConf)"
  },
  {
    "id" : "04c4f026-260e-48ca-9543-3e61db0acd62",
    "prId" : 32582,
    "prUrl" : "https://github.com/apache/spark/pull/32582#pullrequestreview-674053690",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9775dfa7-2f30-442e-b43c-7d60be35ab26",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "We count `filesReused` but not leaving the information here. Can we add it in the log message as well?",
        "createdAt" : "2021-05-30T05:58:41Z",
        "updatedAt" : "2021-05-30T07:54:27Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "467307a2-5b55-4975-930c-fc349b06cbf2",
        "parentId" : "9775dfa7-2f30-442e-b43c-7d60be35ab26",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "Sure, done in 3800c51",
        "createdAt" : "2021-06-02T11:03:52Z",
        "updatedAt" : "2021-06-02T11:03:52Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "c05b140f73f855a148ccedaf753841fb94a4ab1d",
    "line" : 181,
    "diffHunk" : "@@ -1,1 +194,198 @@        }\n    }\n    logInfo(s\"Copied $filesCopied files ($bytesCopied bytes) from local to\" +\n      s\" DFS for version $version. $filesReused files reused without copying.\")\n    versionToRocksDBFiles.put(version, immutableFiles)"
  },
  {
    "id" : "3dcf7d1e-d7e7-424b-9f98-09ef5cd5a2c3",
    "prId" : 32582,
    "prUrl" : "https://github.com/apache/spark/pull/32582#pullrequestreview-674054483",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8567cb9f-dd11-4745-bc32-c6e4f0e0530d",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "It'd be nice to note here again the SST files and LOG files are not included to the zip file. I was mistaken as it contains all files including data as well, and concerned about the overhead of creating and sending zip file due to the size.",
        "createdAt" : "2021-05-30T06:59:22Z",
        "updatedAt" : "2021-05-30T07:54:27Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "62d69712-6a90-40e0-a88a-9aff533edb21",
        "parentId" : "8567cb9f-dd11-4745-bc32-c6e4f0e0530d",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "Make sense, add the comment in 3800c51",
        "createdAt" : "2021-06-02T11:04:47Z",
        "updatedAt" : "2021-06-02T11:04:47Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "c05b140f73f855a148ccedaf753841fb94a4ab1d",
    "line" : 88,
    "diffHunk" : "@@ -1,1 +101,105 @@ *\n * Note the following.\n * - Each [version].zip is a complete description of all the data and metadata needed to recover\n *   a RocksDB instance at the corresponding version. The SST files and log files are not included\n *   in the zip files, they can be shared cross different versions. This is unlike the"
  },
  {
    "id" : "650a7d75-47a8-479d-b9fc-b6ce84b9c206",
    "prId" : 32582,
    "prUrl" : "https://github.com/apache/spark/pull/32582#pullrequestreview-675127987",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3ecfae0a-e24f-4f94-bb93-75cd8133c628",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "The zipped file is `dfsZipFile`, right? why `filesStr` here?",
        "createdAt" : "2021-06-03T04:21:31Z",
        "updatedAt" : "2021-06-03T04:26:18Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "dc4e521a-b4f0-4961-bbac-fde0c4e44132",
        "parentId" : "3ecfae0a-e24f-4f94-bb93-75cd8133c628",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "the `fileStr` contains the `dfsZipFile` name. The log format here is `${dfsZipFile} \\n ${listing all the file names}`",
        "createdAt" : "2021-06-03T10:31:42Z",
        "updatedAt" : "2021-06-03T10:31:42Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "c05b140f73f855a148ccedaf753841fb94a4ab1d",
    "line" : 208,
    "diffHunk" : "@@ -1,1 +221,225 @@      }\n      zout.close()  // so that any error in closing also cancels the output stream\n      logInfo(s\"Zipped $totalBytes bytes (before compression) to $filesStr\")\n    } catch {\n      case e: Exception =>"
  },
  {
    "id" : "2a1beac2-16ef-46e3-8358-c90de995ca59",
    "prId" : 32582,
    "prUrl" : "https://github.com/apache/spark/pull/32582#pullrequestreview-675122445",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7cdc51eb-3685-4c23-859c-7f48abb03101",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Can you talk more about there this claim (thread-safe) applies? Where we delete old files?\r\n\r\nDo you mean `versionToRocksDBFiles`? When we prepare files for new version, there is another thread (maintenance thread) deleting old files?",
        "createdAt" : "2021-06-03T04:22:38Z",
        "updatedAt" : "2021-06-03T04:26:18Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "ea24cbd4-b0a8-4acc-9aef-f9e2ae407f00",
        "parentId" : "7cdc51eb-3685-4c23-859c-7f48abb03101",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "Yes. It refers to delete the files contained in the old versions. Here's the description of the deleteOldVersions method of RocksDBFileManager, which will be called in RocksDBStateStoreProvider.doMaintenance. As we did before, I'll also refer this comment when the PR for delete path submitted.\r\n```\r\n   * Delete old versions by deleting the associated version and SST files.\r\n   * At a high-level, this method finds which versions to delete, and which SST files that were\r\n   * last used in those versions. Its safe to delete these SST files because a SST file can\r\n   * be reused only in successive versions. Therefore, if a SST file F was last used in version\r\n   * V, then it wont be used in version V+1 or later, and if version V can be deleted, then\r\n   * F can safely be deleted as well.\r\n```",
        "createdAt" : "2021-06-03T10:24:46Z",
        "updatedAt" : "2021-06-03T10:24:46Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "c05b140f73f855a148ccedaf753841fb94a4ab1d",
    "line" : 99,
    "diffHunk" : "@@ -1,1 +112,116 @@ * - Immutable files can be reused only across adjacent checkpoints/versions.\n * - This class is thread-safe. Specifically, it is safe to concurrently delete old files from a\n *   different thread than the task thread saving files.\n *\n * @param dfsRootDir  Directory where the [version].zip files will be stored"
  },
  {
    "id" : "7802d45d-0ee1-4a41-aa58-c57388e6797b",
    "prId" : 32272,
    "prUrl" : "https://github.com/apache/spark/pull/32272#pullrequestreview-658743489",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1fbec635-1f6f-4ada-abbc-bf6efc0c6ca9",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "why we need to avoid it?",
        "createdAt" : "2021-04-22T04:40:26Z",
        "updatedAt" : "2021-04-22T04:41:24Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "60676102-4afe-4a61-b585-2f388186b0a7",
        "parentId" : "1fbec635-1f6f-4ada-abbc-bf6efc0c6ca9",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "It's related to the usage for RocksDB, we don't always have log files. But we must have sst files.",
        "createdAt" : "2021-04-25T09:46:17Z",
        "updatedAt" : "2021-04-25T09:46:17Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      },
      {
        "id" : "c3a357fa-bcd7-44b7-88c0-08da9c3b5c31",
        "parentId" : "1fbec635-1f6f-4ada-abbc-bf6efc0c6ca9",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "I think the point here is excluding empty field (correct?) vs leaving empty field with `[]`. Seems like a small optimization.",
        "createdAt" : "2021-05-01T02:47:11Z",
        "updatedAt" : "2021-05-01T02:47:11Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "44051136-305f-4ab4-b088-91b154cddc7d",
        "parentId" : "1fbec635-1f6f-4ada-abbc-bf6efc0c6ca9",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "Yes, the `logFiles` field not always has value.",
        "createdAt" : "2021-05-13T09:38:06Z",
        "updatedAt" : "2021-05-13T09:38:07Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "f52adac64ae66a3d01da23bee69106bd17b1a4e7",
    "line" : 44,
    "diffHunk" : "@@ -1,1 +42,46 @@  def json: String = {\n    // We turn this field into a null to avoid write a empty logFiles field in the json.\n    val nullified = if (logFiles.isEmpty) this.copy(logFiles = null) else this\n    mapper.writeValueAsString(nullified)\n  }"
  },
  {
    "id" : "69335993-117e-4b2a-bf50-ed268d44c639",
    "prId" : 32272,
    "prUrl" : "https://github.com/apache/spark/pull/32272#pullrequestreview-644118451",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4c56ff84-206c-4ab1-a3d4-115f3c74f82c",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Does it mean that a DFS copy can be mapped to more than one local file names?",
        "createdAt" : "2021-04-22T04:44:04Z",
        "updatedAt" : "2021-04-22T04:44:04Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "5d94aa28-84c3-4525-bf3b-2dc2e7780207",
        "parentId" : "4c56ff84-206c-4ab1-a3d4-115f3c74f82c",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "When do we reuse the DFS copies?",
        "createdAt" : "2021-04-22T04:44:40Z",
        "updatedAt" : "2021-04-22T04:44:40Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "2c245167-17b9-4d64-9f64-27b10ff1fe8f",
        "parentId" : "4c56ff84-206c-4ab1-a3d4-115f3c74f82c",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "Yes. Can be mapped to more than one local file but for different tasks. The most common scenario is task/stage retry.",
        "createdAt" : "2021-04-25T09:53:25Z",
        "updatedAt" : "2021-04-25T09:53:25Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "f52adac64ae66a3d01da23bee69106bd17b1a4e7",
    "line" : 102,
    "diffHunk" : "@@ -1,1 +100,104 @@/**\n * A RocksDBImmutableFile maintains a mapping between a local RocksDB file name and the name of\n * its copy on DFS. Since these files are immutable, their DFS copies can be reused.\n */\nsealed trait RocksDBImmutableFile {"
  },
  {
    "id" : "6015e768-a96a-4a5f-b8b0-f6789596f4e1",
    "prId" : 32272,
    "prUrl" : "https://github.com/apache/spark/pull/32272#pullrequestreview-644118833",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bedc63ec-d588-4b2e-85b4-cfb8fb63be62",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "If a DFS copy can be mapped to more than one local file names, shouldn't two local file names the same one even their local file names are different, if their DFS file names are the same?",
        "createdAt" : "2021-04-22T04:45:56Z",
        "updatedAt" : "2021-04-22T04:45:57Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "59c590b6-3b64-4089-863f-a47894869ca5",
        "parentId" : "bedc63ec-d588-4b2e-85b4-cfb8fb63be62",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "The DFS file name contains UUID, it shouldn't be the same. Normally we use the local file name to filter whether the file is existing locally.",
        "createdAt" : "2021-04-25T09:57:28Z",
        "updatedAt" : "2021-04-25T09:57:28Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "f52adac64ae66a3d01da23bee69106bd17b1a4e7",
    "line" : 115,
    "diffHunk" : "@@ -1,1 +113,117 @@  def isSameFile(otherFile: File): Boolean = {\n    otherFile.getName == localFileName && otherFile.length() == sizeBytes\n  }\n}\n"
  },
  {
    "id" : "d650b9da-b3cb-4655-a656-1333a31f9a25",
    "prId" : 32272,
    "prUrl" : "https://github.com/apache/spark/pull/32272#pullrequestreview-663906958",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "58cd9a42-4611-48b0-bdc9-4a1516697de6",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Would it produce same output with `json`? Since this doesn't manipulate empty logFiles field. Otherwise is it by intention to handle json and prettyJson differently?",
        "createdAt" : "2021-05-01T02:49:56Z",
        "updatedAt" : "2021-05-01T03:20:38Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "05a120ab-9dcb-44aa-a429-dccaf7a21c85",
        "parentId" : "58cd9a42-4611-48b0-bdc9-4a1516697de6",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "The only difference is the `logFiles` fields. Actually the `prettyJson` field is for providing a readable string for log. `json` field is for files writing.",
        "createdAt" : "2021-05-13T09:37:10Z",
        "updatedAt" : "2021-05-13T09:37:10Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      },
      {
        "id" : "94d14e22-65b9-40e0-89fe-f0b325bb9176",
        "parentId" : "58cd9a42-4611-48b0-bdc9-4a1516697de6",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "OK I see where it is used. Just for logging - got it.",
        "createdAt" : "2021-05-20T03:32:13Z",
        "updatedAt" : "2021-05-20T03:32:14Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "f52adac64ae66a3d01da23bee69106bd17b1a4e7",
    "line" : 48,
    "diffHunk" : "@@ -1,1 +46,50 @@  }\n\n  def prettyJson: String = Serialization.writePretty(this)(RocksDBCheckpointMetadata.format)\n\n  def writeToFile(metadataFile: File): Unit = {"
  },
  {
    "id" : "8fb30d2b-8992-4f93-9a0e-b5fc59f4803d",
    "prId" : 32272,
    "prUrl" : "https://github.com/apache/spark/pull/32272#pullrequestreview-660314256",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2c48bdb1-8eb6-4866-8e56-a9100d53d732",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "<del>About backward-compatibility, shall we introduce version number?</del>\r\n\r\nOh, I saw it below. I also forgot I saw it before...",
        "createdAt" : "2021-05-15T07:06:02Z",
        "updatedAt" : "2021-05-15T07:08:39Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "f52adac64ae66a3d01da23bee69106bd17b1a4e7",
    "line" : 34,
    "diffHunk" : "@@ -1,1 +32,36 @@/**\n * Classes to represent metadata of checkpoints saved to DFS. Since this is converted to JSON, any\n * changes to this MUST be backward-compatible.\n */\ncase class RocksDBCheckpointMetadata("
  },
  {
    "id" : "8f0d40be-ab78-49b8-b100-a3ca29312488",
    "prId" : 32272,
    "prUrl" : "https://github.com/apache/spark/pull/32272#pullrequestreview-670894767",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "00e14914-c09e-423e-99bc-74effb2e7108",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Hmm, so if a log file not archived, it is not immutable? But `RocksDBLogFile` is actually `RocksDBImmutableFile`?",
        "createdAt" : "2021-05-15T07:13:50Z",
        "updatedAt" : "2021-05-15T07:14:25Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "612322b6-3beb-4a3e-a71c-9fa14e1cd82a",
        "parentId" : "00e14914-c09e-423e-99bc-74effb2e7108",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "That's right. This is because we only generate the RocksDBLogFile object in the operation of checkpointing. Let's keep this comment open and I'll link this with the caller side for full context. (The caller side is the next PR as my plan - the save path of RocksDBFileManager).",
        "createdAt" : "2021-05-17T08:13:16Z",
        "updatedAt" : "2021-05-17T08:13:17Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      },
      {
        "id" : "0e450cac-f1a0-4b45-b72f-c08e4371e746",
        "parentId" : "00e14914-c09e-423e-99bc-74effb2e7108",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "I read the save path, but still a bit confused. Is there a log file which is not archived log file? I mean why only archived log file is immutable? `isArchivedLogFile` considers a log file is archived by looking at its parent dir, so when we move a log file to `LOG_FILES_LOCAL_SUBDIR` and make it immutable?",
        "createdAt" : "2021-05-23T01:58:16Z",
        "updatedAt" : "2021-05-23T01:58:16Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "ac0ea0b4-9109-4781-be8c-fd25028eb414",
        "parentId" : "00e14914-c09e-423e-99bc-74effb2e7108",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "```\r\nIs there a log file which is not archived log file?...so when we move a log file to LOG_FILES_LOCAL_SUBDIR and make it immutable?\r\n```\r\nYes, per the [example](https://github.com/facebook/rocksdb/wiki/Write-Ahead-Log#life-cycle-of-a-wal), the archival is triggered when the flushing happens. The save path is used for syncing the necessary files to DFS, and the save path is called in the commit operation of a state store provider.",
        "createdAt" : "2021-05-24T17:03:10Z",
        "updatedAt" : "2021-05-24T17:03:10Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      },
      {
        "id" : "fa4f0492-dc28-4b82-9621-d0fad7a07c67",
        "parentId" : "00e14914-c09e-423e-99bc-74effb2e7108",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Yea, this is somehow confusing part to me. Do we have a change to touch log files before they are archived?",
        "createdAt" : "2021-05-24T17:07:21Z",
        "updatedAt" : "2021-05-24T17:07:22Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "927e7d5b-4ccf-418b-8e66-4de386d61699",
        "parentId" : "00e14914-c09e-423e-99bc-74effb2e7108",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "No, we won't change any files generated by Rocksdb, just sync them with DFS. ",
        "createdAt" : "2021-05-24T17:13:59Z",
        "updatedAt" : "2021-05-24T17:14:00Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      },
      {
        "id" : "735edda8-2b2e-4f38-8f0e-8162d4c292ad",
        "parentId" : "00e14914-c09e-423e-99bc-74effb2e7108",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "So there won't be any non-archived log files touched by this. Why we need to distinguish archived log files?",
        "createdAt" : "2021-05-24T17:27:40Z",
        "updatedAt" : "2021-05-24T17:27:40Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "bbda27ab-2f72-4540-8837-988db413744b",
        "parentId" : "00e14914-c09e-423e-99bc-74effb2e7108",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Do you mean we sync up non-archived log files in `localOtherFiles` to DFS?\r\n\r\n```scala\r\nval (localImmutableFiles, localOtherFiles) = allCheckpointFiles.partition(isImmutableFile)\r\n...\r\nzipToDfsFile(localOtherFiles :+ metadataFile, dfsBatchZipFile(version))\r\n```",
        "createdAt" : "2021-05-24T17:29:28Z",
        "updatedAt" : "2021-05-24T17:29:29Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "2f70156f-cdc7-4377-8edf-6c2611360238",
        "parentId" : "00e14914-c09e-423e-99bc-74effb2e7108",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "```\r\nWhy we need to distinguish archived log files?\r\n```\r\n\r\nBecause we'll build a mapping between archived log files and versions. https://github.com/apache/spark/pull/32582/files#diff-e3d3914d0398d61fdd299b1f8d3e869ec6a86e97606677c724969e421c9bf44eR135\r\n\r\n```\r\nDo you mean we sync up non-archived log files in localOtherFiles to DFS?\r\n```\r\nYes. The DFS directory structure was explained in the doc and code comment [here](https://github.com/apache/spark/pull/32582/files#diff-e3d3914d0398d61fdd299b1f8d3e869ec6a86e97606677c724969e421c9bf44eR72)",
        "createdAt" : "2021-05-25T01:04:56Z",
        "updatedAt" : "2021-05-25T01:04:56Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      },
      {
        "id" : "169ad1d0-bebe-4286-83ea-48323646071e",
        "parentId" : "00e14914-c09e-423e-99bc-74effb2e7108",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "The DFS directory structure only explains SST files, but doesn't mention log files. Does `metadata` also  contain mapping between `archive/00008.log` and `logs/00008-[uuid3].log`? Are archived log files also shared across checkpoint versions?",
        "createdAt" : "2021-05-25T06:41:31Z",
        "updatedAt" : "2021-05-25T06:41:31Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "b271e373-35a9-4096-a2f2-f036d0683d5a",
        "parentId" : "00e14914-c09e-423e-99bc-74effb2e7108",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Looks they are shared too as they are immutable files. Can you also update the code comment there?",
        "createdAt" : "2021-05-25T06:56:31Z",
        "updatedAt" : "2021-05-25T06:56:31Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "aa64e43d-ece0-47e8-b6ad-75b9d4a06127",
        "parentId" : "00e14914-c09e-423e-99bc-74effb2e7108",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "```\r\nLooks they are shared too as they are immutable files. Can you also update the code comment there?\r\n```\r\nYes that's right. All the immutable files (sst and log files) are contained in checkpoint metadata and shared across checkpoint versions. The un-archived log files are only contained in the ${version}.zip and mapped with a specific checkpoint version.\r\n\r\nI think the structure already contains log files. Do you mean I need to emphasize it in the comments [here](https://github.com/apache/spark/pull/32582/files#diff-e3d3914d0398d61fdd299b1f8d3e869ec6a86e97606677c724969e421c9bf44eR74-R77)?",
        "createdAt" : "2021-05-25T07:19:00Z",
        "updatedAt" : "2021-05-25T07:19:20Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      },
      {
        "id" : "06d71614-85e4-4273-a8d1-167eaea384b1",
        "parentId" : "00e14914-c09e-423e-99bc-74effb2e7108",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "I saw archived log files in the structure, but the comment only mentions SST files. So I thought if it is different than SST files. I think it is better to  mention archived log files along with SST files in the comments.",
        "createdAt" : "2021-05-25T07:26:10Z",
        "updatedAt" : "2021-05-25T07:26:10Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "2e1e9927-a839-407d-88c2-3dcfdf01a319",
        "parentId" : "00e14914-c09e-423e-99bc-74effb2e7108",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "Ah yeah. I know why you feel confused. Make sense, I'll emphasize the \"sst files and log files\" together in the comments to avoid making others confuse. Thanks!",
        "createdAt" : "2021-05-25T07:33:39Z",
        "updatedAt" : "2021-05-25T07:33:39Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      },
      {
        "id" : "a5908bf4-0664-4dff-a52c-0d8b1cf9f179",
        "parentId" : "00e14914-c09e-423e-99bc-74effb2e7108",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "The comment changed in https://github.com/apache/spark/pull/32582/files#diff-e3d3914d0398d61fdd299b1f8d3e869ec6a86e97606677c724969e421c9bf44eR77-R80",
        "createdAt" : "2021-05-28T03:08:56Z",
        "updatedAt" : "2021-05-28T03:08:57Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "f52adac64ae66a3d01da23bee69106bd17b1a4e7",
    "line" : 164,
    "diffHunk" : "@@ -1,1 +162,166 @@    isLogFile(file.getName) && file.getParentFile.getName == LOG_FILES_LOCAL_SUBDIR\n\n  def isImmutableFile(file: File): Boolean = isSstFile(file.getName) || isArchivedLogFile(file)\n}"
  }
]