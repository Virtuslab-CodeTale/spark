[
  {
    "id" : "5a415096-6f32-4ad2-8e03-531f4e32ef62",
    "prId" : 33310,
    "prUrl" : "https://github.com/apache/spark/pull/33310#pullrequestreview-705991230",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "da783f6b-eefc-4ea0-bc27-e246b7b60e30",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I'm wondering that if we should have a more meticulous algorithm.\r\n\r\nLet's say that there are 3 mappers and 2 reducers, so 6 shuffle blocks in total: `(M0, R0), (M0, R1), (M1, R0), (M1, R1), (M2, R0), (M2, R1)`. If the expected parallelism is 2, I think each task should read 3 blocks:\r\ntask 0: `(M0, R0), (M0, R1), (M1, R0)`\r\ntask1: `(M1, R1), (M2, R0), (M2, R1)`\r\n\r\nSo one task can read some entire mappers and some partial mapper.",
        "createdAt" : "2021-07-14T08:23:54Z",
        "updatedAt" : "2021-07-14T08:24:09Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "b4b6b15b8841bc6e9e2452861db74440e5dcbcd2",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +87,91 @@      (0 until 1).flatMap { _ =>\n        (splitPoints :+ numMappers).sliding(2).map {\n          case Seq(start, end) => CoalescedMapperPartitionSpec(start, end, numReducers)\n        }\n      }"
  }
]