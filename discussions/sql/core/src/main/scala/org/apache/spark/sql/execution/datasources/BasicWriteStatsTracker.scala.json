[
  {
    "id" : "b2c44f78-b6b7-4860-b965-731801a0cc9b",
    "prId" : 32459,
    "prUrl" : "https://github.com/apache/spark/pull/32459#pullrequestreview-653964275",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1948fcf4-52dd-4990-95df-ecf01e89bc87",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "From Apache Side codebase, this is no-op unused parameter and there is no test coverage in this PR. Do you think we can have a sample custom `WriteTaskStatsTracker` test case to prevent a future regression, @cloud-fan ?\r\n> To not break some custom WriteTaskStatsTracker implementations.",
        "createdAt" : "2021-05-06T23:11:00Z",
        "updatedAt" : "2021-05-06T23:12:49Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "8e9f6cb8d5b19792fc408c7b9fe9bcc77a4a56d7",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +152,156 @@  }\n\n  override def newRow(filePath: String, row: InternalRow): Unit = {\n    numRows += 1\n  }"
  },
  {
    "id" : "5d98d5a4-5a89-4e05-8428-cf40f3ceb7d5",
    "prId" : 31522,
    "prUrl" : "https://github.com/apache/spark/pull/31522#pullrequestreview-710601224",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "86eebdb5-88b6-4d02-aa2a-9634cdc603e1",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "nit: seems we can simplify the caller side if we add a new constructor in this class\r\n```\r\ndef this(\r\n    serializableHadoopConf: SerializableConfiguration,\r\n    metrics: Map[String, SQLMetric]): WriteJobStatsTracker {\r\n  this(serializableHadoopConf, metrics - TASK_COMMIT_TIME, metrics(TASK_COMMIT_TIME))\r\n}\r\n```",
        "createdAt" : "2021-07-20T13:01:29Z",
        "updatedAt" : "2021-07-20T13:01:29Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "ccee1cc6-1738-4a4b-947a-289e04ff7356",
        "parentId" : "86eebdb5-88b6-4d02-aa2a-9634cdc603e1",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "Done",
        "createdAt" : "2021-07-20T13:48:24Z",
        "updatedAt" : "2021-07-20T13:48:24Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "b5c9d6338c53c52724c0a21d61a093194745b98f",
    "line" : 43,
    "diffHunk" : "@@ -1,1 +190,194 @@    @transient val driverSideMetrics: Map[String, SQLMetric],\n    taskCommitTimeMetric: SQLMetric)\n  extends WriteJobStatsTracker {\n\n  def this("
  },
  {
    "id" : "bacf3b13-eaf9-4514-883c-040879c2e93a",
    "prId" : 30714,
    "prUrl" : "https://github.com/apache/spark/pull/30714#pullrequestreview-550003267",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "59cec564-58f9-42df-b5b3-a5c5000e1bae",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "`HADOOP-17414` seems to be not committed yet and is still targeting 3.3.1.\r\nShall we hold one this PR until we have an official Hadoop release?",
        "createdAt" : "2020-12-10T21:28:31Z",
        "updatedAt" : "2021-02-17T11:36:20Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "b19537d7-b547-48ae-9c86-1413e9f5f44f",
        "parentId" : "59cec564-58f9-42df-b5b3-a5c5000e1bae",
        "authorId" : "224991ce-ad69-410b-8143-bf394b6b5c59",
        "body" : "I'm developing them in sync so they can be done end to end. Yes, it makes sense for the Hadoop one to be committed,  first -but if reviewing the spark one finds changes which need to be done there (is the header a good name?) then an initial review of this PR will make sure that they are lined up",
        "createdAt" : "2020-12-11T11:05:50Z",
        "updatedAt" : "2021-02-17T11:36:20Z",
        "lastEditedBy" : "224991ce-ad69-410b-8143-bf394b6b5c59",
        "tags" : [
        ]
      }
    ],
    "commit" : "81c0a5244fa5a17184b59df9f08aed0d20868276",
    "line" : 88,
    "diffHunk" : "@@ -1,1 +224,228 @@  private val NUM_OUTPUT_ROWS_KEY = \"numOutputRows\"\n  private val NUM_PARTS_KEY = \"numParts\"\n  /** XAttr key of the data length header added in HADOOP-17414. */\n  val FILE_LENGTH_XATTR = \"header.x-hadoop-s3a-magic-data-length\"\n"
  },
  {
    "id" : "98773530-24fb-4e9b-8800-131be4f8c0bf",
    "prId" : 30714,
    "prUrl" : "https://github.com/apache/spark/pull/30714#pullrequestreview-579304040",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7081f748-5ec9-4079-8a22-a51673054cf0",
        "parentId" : null,
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "do we want this warn instead or perhaps just extend saying file stats may be wrong?",
        "createdAt" : "2021-01-28T14:52:12Z",
        "updatedAt" : "2021-02-17T11:36:20Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "19275032-6c3d-4149-92a9-be9b28c783ba",
        "parentId" : "7081f748-5ec9-4079-8a22-a51673054cf0",
        "authorId" : "224991ce-ad69-410b-8143-bf394b6b5c59",
        "body" : "OK. I'd say at info as warn seems overkill for progress report issues.",
        "createdAt" : "2021-01-29T14:26:58Z",
        "updatedAt" : "2021-02-17T11:36:20Z",
        "lastEditedBy" : "224991ce-ad69-410b-8143-bf394b6b5c59",
        "tags" : [
        ]
      }
    ],
    "commit" : "81c0a5244fa5a17184b59df9f08aed0d20868276",
    "line" : 70,
    "diffHunk" : "@@ -1,1 +117,121 @@      case e: NumberFormatException =>\n        // warn but don't dump the whole stack\n        logInfo(s\"Failed to parse\" +\n          s\" ${BasicWriteJobStatsTracker.FILE_LENGTH_XATTR}:$e;\" +\n          s\" bytes written may be under-reported\");"
  }
]