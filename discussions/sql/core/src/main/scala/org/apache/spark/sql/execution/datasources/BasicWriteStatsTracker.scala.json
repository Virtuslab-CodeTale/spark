[
  {
    "id" : "b2c44f78-b6b7-4860-b965-731801a0cc9b",
    "prId" : 32459,
    "prUrl" : "https://github.com/apache/spark/pull/32459#pullrequestreview-653964275",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1948fcf4-52dd-4990-95df-ecf01e89bc87",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "From Apache Side codebase, this is no-op unused parameter and there is no test coverage in this PR. Do you think we can have a sample custom `WriteTaskStatsTracker` test case to prevent a future regression, @cloud-fan ?\r\n> To not break some custom WriteTaskStatsTracker implementations.",
        "createdAt" : "2021-05-06T23:11:00Z",
        "updatedAt" : "2021-05-06T23:12:49Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "8e9f6cb8d5b19792fc408c7b9fe9bcc77a4a56d7",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +152,156 @@  }\n\n  override def newRow(filePath: String, row: InternalRow): Unit = {\n    numRows += 1\n  }"
  },
  {
    "id" : "5d98d5a4-5a89-4e05-8428-cf40f3ceb7d5",
    "prId" : 31522,
    "prUrl" : "https://github.com/apache/spark/pull/31522#pullrequestreview-710601224",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "86eebdb5-88b6-4d02-aa2a-9634cdc603e1",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "nit: seems we can simplify the caller side if we add a new constructor in this class\r\n```\r\ndef this(\r\n    serializableHadoopConf: SerializableConfiguration,\r\n    metrics: Map[String, SQLMetric]): WriteJobStatsTracker {\r\n  this(serializableHadoopConf, metrics - TASK_COMMIT_TIME, metrics(TASK_COMMIT_TIME))\r\n}\r\n```",
        "createdAt" : "2021-07-20T13:01:29Z",
        "updatedAt" : "2021-07-20T13:01:29Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "ccee1cc6-1738-4a4b-947a-289e04ff7356",
        "parentId" : "86eebdb5-88b6-4d02-aa2a-9634cdc603e1",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "Done",
        "createdAt" : "2021-07-20T13:48:24Z",
        "updatedAt" : "2021-07-20T13:48:24Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "b5c9d6338c53c52724c0a21d61a093194745b98f",
    "line" : 43,
    "diffHunk" : "@@ -1,1 +190,194 @@    @transient val driverSideMetrics: Map[String, SQLMetric],\n    taskCommitTimeMetric: SQLMetric)\n  extends WriteJobStatsTracker {\n\n  def this("
  },
  {
    "id" : "bacf3b13-eaf9-4514-883c-040879c2e93a",
    "prId" : 30714,
    "prUrl" : "https://github.com/apache/spark/pull/30714#pullrequestreview-550003267",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "59cec564-58f9-42df-b5b3-a5c5000e1bae",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "`HADOOP-17414` seems to be not committed yet and is still targeting 3.3.1.\r\nShall we hold one this PR until we have an official Hadoop release?",
        "createdAt" : "2020-12-10T21:28:31Z",
        "updatedAt" : "2021-02-17T11:36:20Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "b19537d7-b547-48ae-9c86-1413e9f5f44f",
        "parentId" : "59cec564-58f9-42df-b5b3-a5c5000e1bae",
        "authorId" : "224991ce-ad69-410b-8143-bf394b6b5c59",
        "body" : "I'm developing them in sync so they can be done end to end. Yes, it makes sense for the Hadoop one to be committed,  first -but if reviewing the spark one finds changes which need to be done there (is the header a good name?) then an initial review of this PR will make sure that they are lined up",
        "createdAt" : "2020-12-11T11:05:50Z",
        "updatedAt" : "2021-02-17T11:36:20Z",
        "lastEditedBy" : "224991ce-ad69-410b-8143-bf394b6b5c59",
        "tags" : [
        ]
      }
    ],
    "commit" : "81c0a5244fa5a17184b59df9f08aed0d20868276",
    "line" : 88,
    "diffHunk" : "@@ -1,1 +224,228 @@  private val NUM_OUTPUT_ROWS_KEY = \"numOutputRows\"\n  private val NUM_PARTS_KEY = \"numParts\"\n  /** XAttr key of the data length header added in HADOOP-17414. */\n  val FILE_LENGTH_XATTR = \"header.x-hadoop-s3a-magic-data-length\"\n"
  },
  {
    "id" : "98773530-24fb-4e9b-8800-131be4f8c0bf",
    "prId" : 30714,
    "prUrl" : "https://github.com/apache/spark/pull/30714#pullrequestreview-579304040",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7081f748-5ec9-4079-8a22-a51673054cf0",
        "parentId" : null,
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "do we want this warn instead or perhaps just extend saying file stats may be wrong?",
        "createdAt" : "2021-01-28T14:52:12Z",
        "updatedAt" : "2021-02-17T11:36:20Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "19275032-6c3d-4149-92a9-be9b28c783ba",
        "parentId" : "7081f748-5ec9-4079-8a22-a51673054cf0",
        "authorId" : "224991ce-ad69-410b-8143-bf394b6b5c59",
        "body" : "OK. I'd say at info as warn seems overkill for progress report issues.",
        "createdAt" : "2021-01-29T14:26:58Z",
        "updatedAt" : "2021-02-17T11:36:20Z",
        "lastEditedBy" : "224991ce-ad69-410b-8143-bf394b6b5c59",
        "tags" : [
        ]
      }
    ],
    "commit" : "81c0a5244fa5a17184b59df9f08aed0d20868276",
    "line" : 70,
    "diffHunk" : "@@ -1,1 +117,121 @@      case e: NumberFormatException =>\n        // warn but don't dump the whole stack\n        logInfo(s\"Failed to parse\" +\n          s\" ${BasicWriteJobStatsTracker.FILE_LENGTH_XATTR}:$e;\" +\n          s\" bytes written may be under-reported\");"
  },
  {
    "id" : "c420a713-a71b-4329-b6ed-e6dca20ee6c8",
    "prId" : 30026,
    "prUrl" : "https://github.com/apache/spark/pull/30026#pullrequestreview-508905216",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "99207e71-81d8-4247-bbb3-39cbc0523b31",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "This increases the data size we need to transfer between executors and the driver. Do we have a microbenchmark to verify the impact?",
        "createdAt" : "2020-10-13T13:20:44Z",
        "updatedAt" : "2020-10-22T10:50:19Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "993351a5-d59a-4236-a239-07be4a1da5fb",
        "parentId" : "99207e71-81d8-4247-bbb3-39cbc0523b31",
        "authorId" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "body" : "A benchmark for latency or memory usage? I can try it ~",
        "createdAt" : "2020-10-13T13:28:04Z",
        "updatedAt" : "2020-10-22T10:50:19Z",
        "lastEditedBy" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "tags" : [
        ]
      },
      {
        "id" : "7de3ad57-a328-4744-9dac-f26053a6c0ef",
        "parentId" : "99207e71-81d8-4247-bbb3-39cbc0523b31",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "end-to-end performance of an INSERT query, with a partitioned table with 1 or 2 or 3 partitioned columns.",
        "createdAt" : "2020-10-13T15:45:34Z",
        "updatedAt" : "2020-10-22T10:50:19Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "5b5f2776-9c11-47df-902b-1bc90c6277ea",
        "parentId" : "99207e71-81d8-4247-bbb3-39cbc0523b31",
        "authorId" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "body" : "ok ~ busy with my own work, will give feedback later",
        "createdAt" : "2020-10-15T03:05:49Z",
        "updatedAt" : "2020-10-22T10:50:19Z",
        "lastEditedBy" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "tags" : [
        ]
      }
    ],
    "commit" : "4f47e205f2d130cf432b1a6c42589f919f2852e5",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +38,42 @@ */\ncase class BasicWriteTaskStats(\n    partitions: Seq[InternalRow],\n    numFiles: Int,\n    numBytes: Long,"
  },
  {
    "id" : "d3c247ba-4bd4-4524-814c-cc95f852073f",
    "prId" : 30026,
    "prUrl" : "https://github.com/apache/spark/pull/30026#pullrequestreview-510241585",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "240b4f0d-152f-446a-9bc5-ffd39e90e69f",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "ditto, `partitionsSet.addAll(summary.partitions)`",
        "createdAt" : "2020-10-16T07:14:37Z",
        "updatedAt" : "2020-10-22T10:50:19Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "96bfc413-aaef-483d-a89d-78eb0d3ba55b",
        "parentId" : "240b4f0d-152f-446a-9bc5-ffd39e90e69f",
        "authorId" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "body" : "ditto, `partitionsSet.addAll(summary.partitions)` can only be used in Scala 2.13 too.",
        "createdAt" : "2020-10-16T08:05:00Z",
        "updatedAt" : "2020-10-22T10:50:19Z",
        "lastEditedBy" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "tags" : [
        ]
      }
    ],
    "commit" : "4f47e205f2d130cf432b1a6c42589f919f2852e5",
    "line" : 59,
    "diffHunk" : "@@ -1,1 +150,154 @@\n    basicStats.foreach { summary =>\n      partitionsSet ++= summary.partitions\n      numFiles += summary.numFiles\n      totalNumBytes += summary.numBytes"
  },
  {
    "id" : "16c3c80b-d9e8-40fd-9150-7cb6e46e4bea",
    "prId" : 33332,
    "prUrl" : "https://github.com/apache/spark/pull/33332#pullrequestreview-706437950",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "60134bf5-20be-4758-97e0-fcc44138da06",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "WDYT @steveloughran ?",
        "createdAt" : "2021-07-14T04:17:23Z",
        "updatedAt" : "2021-07-14T04:17:23Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "fedfbf02-0669-4bfb-b0ac-a89c4ecb7c93",
        "parentId" : "60134bf5-20be-4758-97e0-fcc44138da06",
        "authorId" : "224991ce-ad69-410b-8143-bf394b6b5c59",
        "body" : "This used to happen sometimes on code writing files to S3 where a cached HEAD-generated 404 was being returned even after the upload.\r\n* Not going to be an issue on S3 any more, never was on GCS/Azure. \r\n* s3a magic committer creates marker files for this code\r\n* Are there any FileOutputFormat's which don't write zero byte files if the client didn't write the output? ",
        "createdAt" : "2021-07-14T15:44:16Z",
        "updatedAt" : "2021-07-14T15:44:16Z",
        "lastEditedBy" : "224991ce-ad69-410b-8143-bf394b6b5c59",
        "tags" : [
        ]
      }
    ],
    "commit" : "c082ccb1959c64389dfd69f45c838a727fe7eb16",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +167,171 @@\n    if (numSubmittedFiles != numFiles) {\n      logWarning(s\"Expected $numSubmittedFiles files, but only saw $numFiles. \" +\n        \"This could be due to the output format not writing empty files, \" +\n        \"or files being not immediately visible in the filesystem.\")"
  }
]