[
  {
    "id" : "29847216-bcfa-4654-8005-b11bffeff70a",
    "prId" : 31958,
    "prUrl" : "https://github.com/apache/spark/pull/31958#pullrequestreview-622899247",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0e3a8453-ac23-4c0d-b525-142e28bbe6e3",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Can we do the same thing for Parquet, @c21 ?",
        "createdAt" : "2021-03-28T19:06:53Z",
        "updatedAt" : "2021-03-31T20:09:47Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "a7cbb227-a1a5-4aa6-b94f-e7893f463495",
        "parentId" : "0e3a8453-ac23-4c0d-b525-142e28bbe6e3",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "@dongjoon-hyun - do you mean implementing Parquet vectorized reader for nested column? I created https://issues.apache.org/jira/browse/SPARK-34863 and plan to do it after this one, thanks.",
        "createdAt" : "2021-03-29T04:59:38Z",
        "updatedAt" : "2021-03-31T20:09:47Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      },
      {
        "id" : "38f2e162-19c5-47c0-9f75-f58d790f8cef",
        "parentId" : "0e3a8453-ac23-4c0d-b525-142e28bbe6e3",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Yes, thank you for creating SPARK-34863.",
        "createdAt" : "2021-03-29T06:31:46Z",
        "updatedAt" : "2021-03-31T20:09:47Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "44feaccadc8fa81ad9f885686685431f3976f537",
    "line" : 25,
    "diffHunk" : "@@ -1,1 +152,156 @@      schema.forall(s => supportDataType(s.dataType) &&\n        !s.dataType.isInstanceOf[UserDefinedType[_]]) &&\n      supportBatchForNestedColumn(sparkSession, schema)\n  }\n"
  },
  {
    "id" : "ec008010-a9f2-4f6b-a0e5-5d1abb33e022",
    "prId" : 30707,
    "prUrl" : "https://github.com/apache/spark/pull/30707#pullrequestreview-549775069",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "658138c4-e5c0-4fec-933e-39c09c08ce38",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I would prefer to avoid relying on try-catch whenever possible.\r\n\r\nAlso, a space in a URI doesn't look correct. HDFS itself has a complicated behaviour arond path and URIs so let;s don't follow the behaviour unless it's known as a legitimate behaviour (is it?).`file.filePath` should be a URI so the behaviour `new Path(new URI(file.filePath))` seems correct.\r\n",
        "createdAt" : "2020-12-11T03:22:15Z",
        "updatedAt" : "2020-12-11T03:22:16Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "f7f72ea7054bce3592ab3c07405eb29d3f37b78c",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +172,176 @@\n      var path: Option[Path] = None\n      import scala.util.Try\n      Try {\n        path = Some(new Path(new URI(file.filePath)))"
  }
]