[
  {
    "id" : "29847216-bcfa-4654-8005-b11bffeff70a",
    "prId" : 31958,
    "prUrl" : "https://github.com/apache/spark/pull/31958#pullrequestreview-622899247",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0e3a8453-ac23-4c0d-b525-142e28bbe6e3",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Can we do the same thing for Parquet, @c21 ?",
        "createdAt" : "2021-03-28T19:06:53Z",
        "updatedAt" : "2021-03-31T20:09:47Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "a7cbb227-a1a5-4aa6-b94f-e7893f463495",
        "parentId" : "0e3a8453-ac23-4c0d-b525-142e28bbe6e3",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "@dongjoon-hyun - do you mean implementing Parquet vectorized reader for nested column? I created https://issues.apache.org/jira/browse/SPARK-34863 and plan to do it after this one, thanks.",
        "createdAt" : "2021-03-29T04:59:38Z",
        "updatedAt" : "2021-03-31T20:09:47Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      },
      {
        "id" : "38f2e162-19c5-47c0-9f75-f58d790f8cef",
        "parentId" : "0e3a8453-ac23-4c0d-b525-142e28bbe6e3",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Yes, thank you for creating SPARK-34863.",
        "createdAt" : "2021-03-29T06:31:46Z",
        "updatedAt" : "2021-03-31T20:09:47Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "44feaccadc8fa81ad9f885686685431f3976f537",
    "line" : 25,
    "diffHunk" : "@@ -1,1 +152,156 @@      schema.forall(s => supportDataType(s.dataType) &&\n        !s.dataType.isInstanceOf[UserDefinedType[_]]) &&\n      supportBatchForNestedColumn(sparkSession, schema)\n  }\n"
  },
  {
    "id" : "ec008010-a9f2-4f6b-a0e5-5d1abb33e022",
    "prId" : 30707,
    "prUrl" : "https://github.com/apache/spark/pull/30707#pullrequestreview-549775069",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "658138c4-e5c0-4fec-933e-39c09c08ce38",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I would prefer to avoid relying on try-catch whenever possible.\r\n\r\nAlso, a space in a URI doesn't look correct. HDFS itself has a complicated behaviour arond path and URIs so let;s don't follow the behaviour unless it's known as a legitimate behaviour (is it?).`file.filePath` should be a URI so the behaviour `new Path(new URI(file.filePath))` seems correct.\r\n",
        "createdAt" : "2020-12-11T03:22:15Z",
        "updatedAt" : "2020-12-11T03:22:16Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "f7f72ea7054bce3592ab3c07405eb29d3f37b78c",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +172,176 @@\n      var path: Option[Path] = None\n      import scala.util.Try\n      Try {\n        path = Some(new Path(new URI(file.filePath)))"
  },
  {
    "id" : "ca8ea986-3135-4346-aacf-2f4f90d2be8e",
    "prId" : 30663,
    "prUrl" : "https://github.com/apache/spark/pull/30663#pullrequestreview-556763757",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fc7f9588-edd1-4a9d-9a95-e1d723838a09",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@LuciferYang, but I think ORC here is a valid case, though.",
        "createdAt" : "2020-12-22T01:33:51Z",
        "updatedAt" : "2020-12-22T02:25:42Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "6d392ad9-4540-42c4-8629-7968226c3970",
        "parentId" : "fc7f9588-edd1-4a9d-9a95-e1d723838a09",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "cc @dongjoon-hyun FYI",
        "createdAt" : "2020-12-22T01:34:00Z",
        "updatedAt" : "2020-12-22T02:25:42Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "dc04cff134136561e6e5822d25758356cdd653e3",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +185,189 @@      } else {\n        // ORC predicate pushdown\n        if (orcFilterPushDown && filters.nonEmpty) {\n          OrcUtils.readCatalystSchema(filePath, conf, ignoreCorruptFiles).foreach { fileSchema =>\n            OrcFilters.createFilter(fileSchema, filters).foreach { f =>"
  },
  {
    "id" : "8c78cfd9-fc0f-4d3b-a912-a5deb3943f33",
    "prId" : 29457,
    "prUrl" : "https://github.com/apache/spark/pull/29457#pullrequestreview-471846530",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "20cc8af9-1b53-42f1-a437-3fe36814ab35",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "AFAIK the input `schema` is only used to build the `dataTypeMap`, can we directly pass the physical ORC schema here? Then we don't need to convert the ORC schema to catalyst schema which is consistent with the parquet side.",
        "createdAt" : "2020-08-20T09:31:24Z",
        "updatedAt" : "2020-08-20T09:31:25Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "cb026453-4cf1-402c-8ac4-40f407cdbec2",
        "parentId" : "20cc8af9-1b53-42f1-a437-3fe36814ab35",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Our ORC pushdown code, e.g., `buildLeafSearchArgument`, uses catalyst schema in many places. I think it is still possible to use ORC's schema there and remove catalyst schema, but it looks like a big change. If we want to do it, I'd suggest to do it in another PR. This diff is already not small now.",
        "createdAt" : "2020-08-20T17:45:35Z",
        "updatedAt" : "2020-08-20T17:45:35Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "090747d4a9d1540c4b65e45f960c926a23d76b84",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +187,191 @@        if (orcFilterPushDown) {\n          OrcUtils.readCatalystSchema(filePath, conf, ignoreCorruptFiles).map { fileSchema =>\n            OrcFilters.createFilter(fileSchema, filters).foreach { f =>\n              OrcInputFormat.setSearchArgument(conf, f, fileSchema.fieldNames)\n            }"
  }
]