[
  {
    "id" : "29847216-bcfa-4654-8005-b11bffeff70a",
    "prId" : 31958,
    "prUrl" : "https://github.com/apache/spark/pull/31958#pullrequestreview-622899247",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0e3a8453-ac23-4c0d-b525-142e28bbe6e3",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Can we do the same thing for Parquet, @c21 ?",
        "createdAt" : "2021-03-28T19:06:53Z",
        "updatedAt" : "2021-03-31T20:09:47Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "a7cbb227-a1a5-4aa6-b94f-e7893f463495",
        "parentId" : "0e3a8453-ac23-4c0d-b525-142e28bbe6e3",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "@dongjoon-hyun - do you mean implementing Parquet vectorized reader for nested column? I created https://issues.apache.org/jira/browse/SPARK-34863 and plan to do it after this one, thanks.",
        "createdAt" : "2021-03-29T04:59:38Z",
        "updatedAt" : "2021-03-31T20:09:47Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      },
      {
        "id" : "38f2e162-19c5-47c0-9f75-f58d790f8cef",
        "parentId" : "0e3a8453-ac23-4c0d-b525-142e28bbe6e3",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Yes, thank you for creating SPARK-34863.",
        "createdAt" : "2021-03-29T06:31:46Z",
        "updatedAt" : "2021-03-31T20:09:47Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "44feaccadc8fa81ad9f885686685431f3976f537",
    "line" : 25,
    "diffHunk" : "@@ -1,1 +152,156 @@      schema.forall(s => supportDataType(s.dataType) &&\n        !s.dataType.isInstanceOf[UserDefinedType[_]]) &&\n      supportBatchForNestedColumn(sparkSession, schema)\n  }\n"
  },
  {
    "id" : "ec008010-a9f2-4f6b-a0e5-5d1abb33e022",
    "prId" : 30707,
    "prUrl" : "https://github.com/apache/spark/pull/30707#pullrequestreview-549775069",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "658138c4-e5c0-4fec-933e-39c09c08ce38",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I would prefer to avoid relying on try-catch whenever possible.\r\n\r\nAlso, a space in a URI doesn't look correct. HDFS itself has a complicated behaviour arond path and URIs so let;s don't follow the behaviour unless it's known as a legitimate behaviour (is it?).`file.filePath` should be a URI so the behaviour `new Path(new URI(file.filePath))` seems correct.\r\n",
        "createdAt" : "2020-12-11T03:22:15Z",
        "updatedAt" : "2020-12-11T03:22:16Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "f7f72ea7054bce3592ab3c07405eb29d3f37b78c",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +172,176 @@\n      var path: Option[Path] = None\n      import scala.util.Try\n      Try {\n        path = Some(new Path(new URI(file.filePath)))"
  },
  {
    "id" : "ca8ea986-3135-4346-aacf-2f4f90d2be8e",
    "prId" : 30663,
    "prUrl" : "https://github.com/apache/spark/pull/30663#pullrequestreview-556763757",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fc7f9588-edd1-4a9d-9a95-e1d723838a09",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@LuciferYang, but I think ORC here is a valid case, though.",
        "createdAt" : "2020-12-22T01:33:51Z",
        "updatedAt" : "2020-12-22T02:25:42Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "6d392ad9-4540-42c4-8629-7968226c3970",
        "parentId" : "fc7f9588-edd1-4a9d-9a95-e1d723838a09",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "cc @dongjoon-hyun FYI",
        "createdAt" : "2020-12-22T01:34:00Z",
        "updatedAt" : "2020-12-22T02:25:42Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "dc04cff134136561e6e5822d25758356cdd653e3",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +185,189 @@      } else {\n        // ORC predicate pushdown\n        if (orcFilterPushDown && filters.nonEmpty) {\n          OrcUtils.readCatalystSchema(filePath, conf, ignoreCorruptFiles).foreach { fileSchema =>\n            OrcFilters.createFilter(fileSchema, filters).foreach { f =>"
  },
  {
    "id" : "8c78cfd9-fc0f-4d3b-a912-a5deb3943f33",
    "prId" : 29457,
    "prUrl" : "https://github.com/apache/spark/pull/29457#pullrequestreview-471846530",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "20cc8af9-1b53-42f1-a437-3fe36814ab35",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "AFAIK the input `schema` is only used to build the `dataTypeMap`, can we directly pass the physical ORC schema here? Then we don't need to convert the ORC schema to catalyst schema which is consistent with the parquet side.",
        "createdAt" : "2020-08-20T09:31:24Z",
        "updatedAt" : "2020-08-20T09:31:25Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "cb026453-4cf1-402c-8ac4-40f407cdbec2",
        "parentId" : "20cc8af9-1b53-42f1-a437-3fe36814ab35",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Our ORC pushdown code, e.g., `buildLeafSearchArgument`, uses catalyst schema in many places. I think it is still possible to use ORC's schema there and remove catalyst schema, but it looks like a big change. If we want to do it, I'd suggest to do it in another PR. This diff is already not small now.",
        "createdAt" : "2020-08-20T17:45:35Z",
        "updatedAt" : "2020-08-20T17:45:35Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "090747d4a9d1540c4b65e45f960c926a23d76b84",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +187,191 @@        if (orcFilterPushDown) {\n          OrcUtils.readCatalystSchema(filePath, conf, ignoreCorruptFiles).map { fileSchema =>\n            OrcFilters.createFilter(fileSchema, filters).foreach { f =>\n              OrcInputFormat.setSearchArgument(conf, f, fileSchema.fieldNames)\n            }"
  },
  {
    "id" : "29e60add-6ef1-4761-8cb5-8ead4483bf4b",
    "prId" : 26826,
    "prUrl" : "https://github.com/apache/spark/pull/26826#pullrequestreview-330734524",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c13c0b77-5de2-4819-b874-1a949c9c00a0",
        "parentId" : null,
        "authorId" : "79d368bd-8aec-43ce-bcd3-764c21bb93d5",
        "body" : "Hey @srowen, I was going through the changes here. I am not sure, what this change means? ",
        "createdAt" : "2019-12-10T06:42:44Z",
        "updatedAt" : "2019-12-11T17:33:36Z",
        "lastEditedBy" : "79d368bd-8aec-43ce-bcd3-764c21bb93d5",
        "tags" : [
        ]
      },
      {
        "id" : "c971c49f-32b5-4b87-9f26-c62a778301b0",
        "parentId" : "c13c0b77-5de2-4819-b874-1a949c9c00a0",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Ah right, the issue is there is also an org.apache.org.OrcUtils, in addition to the one in this class's package. The code below wants to use the latter, the Spark OrcUtils. In 2.12, the local package class wins, but in 2.13 it doesn't, so the wrong one is imported.\r\n\r\nThis import syntax says \"don't import OrcUtils from org.apache.orc, but import everything else\"",
        "createdAt" : "2019-12-10T12:55:36Z",
        "updatedAt" : "2019-12-11T17:33:36Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "c564a230-8e8c-4dc1-bce8-6c2c8d226d3d",
        "parentId" : "c13c0b77-5de2-4819-b874-1a949c9c00a0",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Yes. We should use `Spark's OrcUtils`.",
        "createdAt" : "2019-12-11T18:02:10Z",
        "updatedAt" : "2019-12-11T18:02:10Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "d24ed2171587ddaf3a5986d4bdfd8779116c1d2a",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +27,31 @@import org.apache.hadoop.mapreduce.lib.input.FileSplit\nimport org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl\nimport org.apache.orc.{OrcUtils => _, _}\nimport org.apache.orc.OrcConf.{COMPRESS, MAPRED_OUTPUT_SCHEMA}\nimport org.apache.orc.mapred.OrcStruct"
  },
  {
    "id" : "775c37c5-eb65-43ae-aaeb-b4aad217960b",
    "prId" : 25006,
    "prUrl" : "https://github.com/apache/spark/pull/25006#pullrequestreview-256024421",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ff40b789-e15c-4e9b-82fd-ebbe6c051558",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Can we do:\r\n\r\n```scala\r\n      val requestedColIdsOrEmptyFile = {\r\n        Utils.tryWithResource(OrcFile.createReader(filePath, readerOptions)) { reader =>\r\n          OrcUtils.requestedColumnIds(\r\n            isCaseSensitive, dataSchema, requiredSchema, reader, conf)\r\n        }\r\n      }\r\n```\r\n\r\nor, `try` and `finally`?\r\n\r\n?",
        "createdAt" : "2019-06-29T05:17:39Z",
        "updatedAt" : "2019-09-16T18:33:54Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "24088cb8443fd011890670452f57067dea20d9c0",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +185,189 @@      val requestedColIdsOrEmptyFile = OrcUtils.requestedColumnIds(\n        isCaseSensitive, dataSchema, requiredSchema, reader, conf)\n      reader.close()\n\n      if (requestedColIdsOrEmptyFile.isEmpty) {"
  }
]