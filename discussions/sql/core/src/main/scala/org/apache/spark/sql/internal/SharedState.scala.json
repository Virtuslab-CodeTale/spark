[
  {
    "id" : "7d4b7950-fb88-4465-8895-5a8374bc4f5b",
    "prId" : 31868,
    "prUrl" : "https://github.com/apache/spark/pull/31868#pullrequestreview-614537056",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "195b6ea7-6b3d-4e87-aae5-aa9e28240c41",
        "parentId" : null,
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "I don't quite get this change. IIUC, only `spark.hadoop` and `spark.hive` prefixed ones get resolved and extracted into Hadoop configuration. After this, it seems that a `hadoop.xyz` has higher priority than `spark.hadoop.hadoop.xyz`.",
        "createdAt" : "2021-03-17T16:10:26Z",
        "updatedAt" : "2021-03-18T07:52:17Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "52936c81-48bf-437b-9b8d-8d79e700ab06",
        "parentId" : "195b6ea7-6b3d-4e87-aae5-aa9e28240c41",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "This is to follow `session.sessionState.newHadoopConf()`:\r\n1. The SQLConf contains all the configs from SparkConf, see https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/internal/BaseSessionStateBuilder.scala#L89\r\n2. The Hadoop conf contains all the configs from SQLConf, see https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/internal/SessionState.scala#L127",
        "createdAt" : "2021-03-17T16:17:01Z",
        "updatedAt" : "2021-03-18T07:52:17Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "05a4299a-996d-438a-94b6-9723f1e00e06",
        "parentId" : "195b6ea7-6b3d-4e87-aae5-aa9e28240c41",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "Make sense to me ",
        "createdAt" : "2021-03-17T16:30:27Z",
        "updatedAt" : "2021-03-18T07:52:17Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "e1425964ebdec196c4de1f6f25d36a53072aa1cc",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +60,64 @@    val hadoopConfClone = new Configuration(sparkContext.hadoopConfiguration)\n    // Extract entries from `SparkConf` and put them in the Hadoop conf.\n    confClone.getAll.foreach { case (k, v) =>\n      if (v ne null) hadoopConfClone.set(k, v)\n    }"
  }
]