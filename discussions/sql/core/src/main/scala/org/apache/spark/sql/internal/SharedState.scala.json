[
  {
    "id" : "7d4b7950-fb88-4465-8895-5a8374bc4f5b",
    "prId" : 31868,
    "prUrl" : "https://github.com/apache/spark/pull/31868#pullrequestreview-614537056",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "195b6ea7-6b3d-4e87-aae5-aa9e28240c41",
        "parentId" : null,
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "I don't quite get this change. IIUC, only `spark.hadoop` and `spark.hive` prefixed ones get resolved and extracted into Hadoop configuration. After this, it seems that a `hadoop.xyz` has higher priority than `spark.hadoop.hadoop.xyz`.",
        "createdAt" : "2021-03-17T16:10:26Z",
        "updatedAt" : "2021-03-18T07:52:17Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "52936c81-48bf-437b-9b8d-8d79e700ab06",
        "parentId" : "195b6ea7-6b3d-4e87-aae5-aa9e28240c41",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "This is to follow `session.sessionState.newHadoopConf()`:\r\n1. The SQLConf contains all the configs from SparkConf, see https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/internal/BaseSessionStateBuilder.scala#L89\r\n2. The Hadoop conf contains all the configs from SQLConf, see https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/internal/SessionState.scala#L127",
        "createdAt" : "2021-03-17T16:17:01Z",
        "updatedAt" : "2021-03-18T07:52:17Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "05a4299a-996d-438a-94b6-9723f1e00e06",
        "parentId" : "195b6ea7-6b3d-4e87-aae5-aa9e28240c41",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "Make sense to me ",
        "createdAt" : "2021-03-17T16:30:27Z",
        "updatedAt" : "2021-03-18T07:52:17Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "e1425964ebdec196c4de1f6f25d36a53072aa1cc",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +60,64 @@    val hadoopConfClone = new Configuration(sparkContext.hadoopConfiguration)\n    // Extract entries from `SparkConf` and put them in the Hadoop conf.\n    confClone.getAll.foreach { case (k, v) =>\n      if (v ne null) hadoopConfClone.set(k, v)\n    }"
  },
  {
    "id" : "be9845e0-f504-4897-a6a6-31b886d2c1bb",
    "prId" : 31671,
    "prUrl" : "https://github.com/apache/spark/pull/31671#pullrequestreview-600412330",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bceccf2f-053b-42cf-8e04-a77e730b5bfd",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "We still need this `logInfo` even after the code in L256 is removed?",
        "createdAt" : "2021-03-01T05:27:44Z",
        "updatedAt" : "2021-03-02T02:59:42Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "1aa8b351-97cc-4ab8-9ded-c7d7242309a9",
        "parentId" : "bceccf2f-053b-42cf-8e04-a77e730b5bfd",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "These log messages themselves look not very useful to me personally before or after this change. But we do not vary the logging logic with this change, I am not sure if there is anyone who relies on these outputs",
        "createdAt" : "2021-03-01T06:08:01Z",
        "updatedAt" : "2021-03-02T02:59:42Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "c4b4cdae144aa7c9cb1da58d9773b4d5096baa45",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +254,258 @@      // If hive.metastore.warehouse.dir is set and spark.sql.warehouse.dir is not set,\n      // we will respect the value of hive.metastore.warehouse.dir.\n      logInfo(s\"${WAREHOUSE_PATH.key} is not set, but $hiveWarehouseKey is set. Setting\" +\n        s\" ${WAREHOUSE_PATH.key} to the value of $hiveWarehouseKey.\")\n      hiveWarehouseDir"
  },
  {
    "id" : "cfaac972-b6d0-4022-a3b0-0c3b6c2840be",
    "prId" : 31671,
    "prUrl" : "https://github.com/apache/spark/pull/31671#pullrequestreview-600398801",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3efaa3cb-b21b-48c0-8aee-e4c792408a66",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "ditto",
        "createdAt" : "2021-03-01T05:27:59Z",
        "updatedAt" : "2021-03-02T02:59:42Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "c4b4cdae144aa7c9cb1da58d9773b4d5096baa45",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +263,267 @@      // we will set hive.metastore.warehouse.dir to the default value of spark.sql.warehouse.dir.\n      val sparkWarehouseDir = sparkWarehouseOption.getOrElse(WAREHOUSE_PATH.defaultValueString)\n      logInfo(s\"Setting $hiveWarehouseKey ('$hiveWarehouseDir') to the value of \" +\n        s\"${WAREHOUSE_PATH.key}.\")\n      sparkWarehouseDir"
  },
  {
    "id" : "e63e6d9d-5e18-4f13-8847-182b8456b397",
    "prId" : 31671,
    "prUrl" : "https://github.com/apache/spark/pull/31671#pullrequestreview-600423930",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "69bc8d26-e691-4392-aa8b-6978a6dd3952",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "ditto",
        "createdAt" : "2021-03-01T06:38:37Z",
        "updatedAt" : "2021-03-02T02:59:42Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "c4b4cdae144aa7c9cb1da58d9773b4d5096baa45",
    "line" : 27,
    "diffHunk" : "@@ -1,1 +264,268 @@      val sparkWarehouseDir = sparkWarehouseOption.getOrElse(WAREHOUSE_PATH.defaultValueString)\n      logInfo(s\"Setting $hiveWarehouseKey ('$hiveWarehouseDir') to the value of \" +\n        s\"${WAREHOUSE_PATH.key}.\")\n      sparkWarehouseDir\n    }"
  },
  {
    "id" : "a2307822-7bf9-4fbf-a659-02ef1d6ef384",
    "prId" : 30709,
    "prUrl" : "https://github.com/apache/spark/pull/30709#pullrequestreview-549720742",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "97028f60-2db4-4956-843e-5590025b4bad",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "nit: It seems there is no test for the condition `(key.startsWith(\"hive\") && sparkConf.contains(\"spark.\" + key))`?",
        "createdAt" : "2020-12-11T00:37:49Z",
        "updatedAt" : "2020-12-11T00:37:50Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "c333672499123dd3e8a5b657da3c999f609ce253",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +232,236 @@    def containsInSparkConf(key: String): Boolean = {\n      sparkConf.contains(key) || sparkConf.contains(\"spark.hadoop.\" + key) ||\n        (key.startsWith(\"hive\") && sparkConf.contains(\"spark.\" + key))\n    }\n"
  },
  {
    "id" : "309e5459-92b8-433b-9ed2-1f47647825fb",
    "prId" : 30709,
    "prUrl" : "https://github.com/apache/spark/pull/30709#pullrequestreview-549722809",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f23daeda-f8bd-4670-980a-ae60f8773dd3",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "(This is not related to this PR though) The string prefix `\"spark.hadoop.\"` seems to be used in many places, so how about defining a global value for it somewhere? I think that the refactoring could make it easy to track the positions where the prefix is used.\r\n",
        "createdAt" : "2020-12-11T00:43:46Z",
        "updatedAt" : "2020-12-11T00:43:46Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "c333672499123dd3e8a5b657da3c999f609ce253",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +231,235 @@\n    def containsInSparkConf(key: String): Boolean = {\n      sparkConf.contains(key) || sparkConf.contains(\"spark.hadoop.\" + key) ||\n        (key.startsWith(\"hive\") && sparkConf.contains(\"spark.\" + key))\n    }"
  }
]