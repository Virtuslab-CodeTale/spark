[
  {
    "id" : "7d4b7950-fb88-4465-8895-5a8374bc4f5b",
    "prId" : 31868,
    "prUrl" : "https://github.com/apache/spark/pull/31868#pullrequestreview-614537056",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "195b6ea7-6b3d-4e87-aae5-aa9e28240c41",
        "parentId" : null,
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "I don't quite get this change. IIUC, only `spark.hadoop` and `spark.hive` prefixed ones get resolved and extracted into Hadoop configuration. After this, it seems that a `hadoop.xyz` has higher priority than `spark.hadoop.hadoop.xyz`.",
        "createdAt" : "2021-03-17T16:10:26Z",
        "updatedAt" : "2021-03-18T07:52:17Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "52936c81-48bf-437b-9b8d-8d79e700ab06",
        "parentId" : "195b6ea7-6b3d-4e87-aae5-aa9e28240c41",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "This is to follow `session.sessionState.newHadoopConf()`:\r\n1. The SQLConf contains all the configs from SparkConf, see https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/internal/BaseSessionStateBuilder.scala#L89\r\n2. The Hadoop conf contains all the configs from SQLConf, see https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/internal/SessionState.scala#L127",
        "createdAt" : "2021-03-17T16:17:01Z",
        "updatedAt" : "2021-03-18T07:52:17Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "05a4299a-996d-438a-94b6-9723f1e00e06",
        "parentId" : "195b6ea7-6b3d-4e87-aae5-aa9e28240c41",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "Make sense to me ",
        "createdAt" : "2021-03-17T16:30:27Z",
        "updatedAt" : "2021-03-18T07:52:17Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "e1425964ebdec196c4de1f6f25d36a53072aa1cc",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +60,64 @@    val hadoopConfClone = new Configuration(sparkContext.hadoopConfiguration)\n    // Extract entries from `SparkConf` and put them in the Hadoop conf.\n    confClone.getAll.foreach { case (k, v) =>\n      if (v ne null) hadoopConfClone.set(k, v)\n    }"
  },
  {
    "id" : "be9845e0-f504-4897-a6a6-31b886d2c1bb",
    "prId" : 31671,
    "prUrl" : "https://github.com/apache/spark/pull/31671#pullrequestreview-600412330",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bceccf2f-053b-42cf-8e04-a77e730b5bfd",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "We still need this `logInfo` even after the code in L256 is removed?",
        "createdAt" : "2021-03-01T05:27:44Z",
        "updatedAt" : "2021-03-02T02:59:42Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "1aa8b351-97cc-4ab8-9ded-c7d7242309a9",
        "parentId" : "bceccf2f-053b-42cf-8e04-a77e730b5bfd",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "These log messages themselves look not very useful to me personally before or after this change. But we do not vary the logging logic with this change, I am not sure if there is anyone who relies on these outputs",
        "createdAt" : "2021-03-01T06:08:01Z",
        "updatedAt" : "2021-03-02T02:59:42Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "c4b4cdae144aa7c9cb1da58d9773b4d5096baa45",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +254,258 @@      // If hive.metastore.warehouse.dir is set and spark.sql.warehouse.dir is not set,\n      // we will respect the value of hive.metastore.warehouse.dir.\n      logInfo(s\"${WAREHOUSE_PATH.key} is not set, but $hiveWarehouseKey is set. Setting\" +\n        s\" ${WAREHOUSE_PATH.key} to the value of $hiveWarehouseKey.\")\n      hiveWarehouseDir"
  },
  {
    "id" : "cfaac972-b6d0-4022-a3b0-0c3b6c2840be",
    "prId" : 31671,
    "prUrl" : "https://github.com/apache/spark/pull/31671#pullrequestreview-600398801",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3efaa3cb-b21b-48c0-8aee-e4c792408a66",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "ditto",
        "createdAt" : "2021-03-01T05:27:59Z",
        "updatedAt" : "2021-03-02T02:59:42Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "c4b4cdae144aa7c9cb1da58d9773b4d5096baa45",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +263,267 @@      // we will set hive.metastore.warehouse.dir to the default value of spark.sql.warehouse.dir.\n      val sparkWarehouseDir = sparkWarehouseOption.getOrElse(WAREHOUSE_PATH.defaultValueString)\n      logInfo(s\"Setting $hiveWarehouseKey ('$hiveWarehouseDir') to the value of \" +\n        s\"${WAREHOUSE_PATH.key}.\")\n      sparkWarehouseDir"
  },
  {
    "id" : "e63e6d9d-5e18-4f13-8847-182b8456b397",
    "prId" : 31671,
    "prUrl" : "https://github.com/apache/spark/pull/31671#pullrequestreview-600423930",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "69bc8d26-e691-4392-aa8b-6978a6dd3952",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "ditto",
        "createdAt" : "2021-03-01T06:38:37Z",
        "updatedAt" : "2021-03-02T02:59:42Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "c4b4cdae144aa7c9cb1da58d9773b4d5096baa45",
    "line" : 27,
    "diffHunk" : "@@ -1,1 +264,268 @@      val sparkWarehouseDir = sparkWarehouseOption.getOrElse(WAREHOUSE_PATH.defaultValueString)\n      logInfo(s\"Setting $hiveWarehouseKey ('$hiveWarehouseDir') to the value of \" +\n        s\"${WAREHOUSE_PATH.key}.\")\n      sparkWarehouseDir\n    }"
  },
  {
    "id" : "a2307822-7bf9-4fbf-a659-02ef1d6ef384",
    "prId" : 30709,
    "prUrl" : "https://github.com/apache/spark/pull/30709#pullrequestreview-549720742",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "97028f60-2db4-4956-843e-5590025b4bad",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "nit: It seems there is no test for the condition `(key.startsWith(\"hive\") && sparkConf.contains(\"spark.\" + key))`?",
        "createdAt" : "2020-12-11T00:37:49Z",
        "updatedAt" : "2020-12-11T00:37:50Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "c333672499123dd3e8a5b657da3c999f609ce253",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +232,236 @@    def containsInSparkConf(key: String): Boolean = {\n      sparkConf.contains(key) || sparkConf.contains(\"spark.hadoop.\" + key) ||\n        (key.startsWith(\"hive\") && sparkConf.contains(\"spark.\" + key))\n    }\n"
  },
  {
    "id" : "309e5459-92b8-433b-9ed2-1f47647825fb",
    "prId" : 30709,
    "prUrl" : "https://github.com/apache/spark/pull/30709#pullrequestreview-549722809",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f23daeda-f8bd-4670-980a-ae60f8773dd3",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "(This is not related to this PR though) The string prefix `\"spark.hadoop.\"` seems to be used in many places, so how about defining a global value for it somewhere? I think that the refactoring could make it easy to track the positions where the prefix is used.\r\n",
        "createdAt" : "2020-12-11T00:43:46Z",
        "updatedAt" : "2020-12-11T00:43:46Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "c333672499123dd3e8a5b657da3c999f609ce253",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +231,235 @@\n    def containsInSparkConf(key: String): Boolean = {\n      sparkConf.contains(key) || sparkConf.contains(\"spark.hadoop.\" + key) ||\n        (key.startsWith(\"hive\") && sparkConf.contains(\"spark.\" + key))\n    }"
  },
  {
    "id" : "340c15ad-b6b5-4169-9c83-c234ff7fe5f5",
    "prId" : 30045,
    "prUrl" : "https://github.com/apache/spark/pull/30045#pullrequestreview-510413246",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ea68c728-bcd9-42b4-b44a-41f445860411",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "how is it related to RESET?",
        "createdAt" : "2020-10-16T11:10:11Z",
        "updatedAt" : "2020-10-22T15:38:55Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "8790ced3-74ef-49df-8b60-b444ccb36438",
        "parentId" : "ea68c728-bcd9-42b4-b44a-41f445860411",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "This is kind of a hidden bug here, If we create a SparkSession like this: \r\n```scala\r\nSparkSession.builder.sparkContext(sc).config(\"spark.sql.warehouse.dir\", \"abc\").getOrCreate()\r\n```\r\nThe `\"spark.sql.warehouse.dir\", \"abc\"` is used by the SessionCatalog correctly, but we do not set it in the cloned conf in SharedState. It causes the default database may use a wrong path here https://github.com/apache/spark/blob/f253fad00c14376e950804849481fa6252cd8154/sql/core/src/main/scala/org/apache/spark/sql/internal/SharedState.scala#L135\r\nAnd If we want to use it for RESET, we also need this config to be kept here.",
        "createdAt" : "2020-10-16T12:13:37Z",
        "updatedAt" : "2020-10-22T15:38:55Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "e98c176f-b164-4d2b-a071-4772fc92ba10",
        "parentId" : "ea68c728-bcd9-42b4-b44a-41f445860411",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "I updated the test case a bit, FYI https://github.com/yaooqinn/sugar/blob/master/src/main/scala/com/netease/mammut/spark/training/sql/WarehouseSCBeforeSS.scala#L67\r\n\r\n'data2' is expected",
        "createdAt" : "2020-10-16T12:16:00Z",
        "updatedAt" : "2020-10-22T15:38:55Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "d1947961a366cd76086b9996c4d9687b7c63f3f7",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +60,64 @@    // both spark conf and hadoop conf avoiding be affected by any SparkSession level options\n    SharedState.loadHiveConfFile(\n      sparkContext.conf, sparkContext.hadoopConfiguration, initialConfigs)\n    val confClone = sparkContext.conf.clone()\n    val hadoopConfClone = new Configuration(sparkContext.hadoopConfiguration)"
  },
  {
    "id" : "445921f4-4617-4ef7-a38c-b6e52ec80c08",
    "prId" : 28529,
    "prUrl" : "https://github.com/apache/spark/pull/28529#pullrequestreview-412038678",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b120f27f-3b4a-42b9-beff-682f9d061c39",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Ur, is it the same with branch-3.0?",
        "createdAt" : "2020-05-14T17:32:53Z",
        "updatedAt" : "2020-05-14T17:42:11Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "22f314fd-ab85-4549-9ba1-ec5de4587a33",
        "parentId" : "b120f27f-3b4a-42b9-beff-682f9d061c39",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Is this because we don't have a configuration patch in branch-2.4?",
        "createdAt" : "2020-05-14T17:34:19Z",
        "updatedAt" : "2020-05-14T17:42:11Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "ad09bdb5-862f-4926-abaa-9f3673c82b63",
        "parentId" : "b120f27f-3b4a-42b9-beff-682f9d061c39",
        "authorId" : "f3166ab8-4dba-4d22-b10b-31a984dfa2ad",
        "body" : "Yes, we dont have the configuration patch",
        "createdAt" : "2020-05-14T17:38:00Z",
        "updatedAt" : "2020-05-14T17:42:11Z",
        "lastEditedBy" : "f3166ab8-4dba-4d22-b10b-31a984dfa2ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "c96d6b0e4bd2effbf8ddc2f2d7acd0890dae168a",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +158,162 @@object SharedState extends Logging {\n  try {\n    SparkSession.getActiveSession match {\n      case Some(spark) =>\n        // scalastyle:off hadoopconfiguration"
  },
  {
    "id" : "f3342188-b6ba-439f-a85b-f3774fd4838b",
    "prId" : 27969,
    "prUrl" : "https://github.com/apache/spark/pull/27969#pullrequestreview-381050334",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "00d3948e-360c-4ae1-83a6-5766bff819ab",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "we should document the return value.",
        "createdAt" : "2020-03-25T10:46:34Z",
        "updatedAt" : "2020-03-26T16:27:37Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "d03e400e-84c7-4f9a-9249-1e663bb6dc33",
        "parentId" : "00d3948e-360c-4ae1-83a6-5766bff819ab",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "do we really need the return value?",
        "createdAt" : "2020-03-25T10:49:11Z",
        "updatedAt" : "2020-03-26T16:27:37Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "60bfba4c-205a-4699-93ed-9f6c041ff42e",
        "parentId" : "00d3948e-360c-4ae1-83a6-5766bff819ab",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "ah. I forgot this",
        "createdAt" : "2020-03-25T11:10:05Z",
        "updatedAt" : "2020-03-26T16:27:37Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "6dddf02bfbba6ce71fea0b64c8aaf1e6c30a1a63",
    "line" : 83,
    "diffHunk" : "@@ -1,1 +229,233 @@   * the config from both hive and Spark SQL. Finally set the warehouse config value to sparkConf.\n   */\n  def loadHiveConfFile(\n      sparkConf: SparkConf,\n      hadoopConf: Configuration): Unit = {"
  },
  {
    "id" : "93e9f731-fc76-4e86-8122-427b9e458a57",
    "prId" : 26530,
    "prUrl" : "https://github.com/apache/spark/pull/26530#pullrequestreview-318064222",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1d89cddc-7f43-4ebc-be0f-268cbc796fdf",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "To keep the original behavior, we don't print `java.lang.Error: factory already defined`.",
        "createdAt" : "2019-11-18T01:25:51Z",
        "updatedAt" : "2019-11-18T04:18:46Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "755c9c0384d7cf016999fdd499800189e77ee733",
    "line" : 38,
    "diffHunk" : "@@ -1,1 +207,211 @@          } catch {\n            case NonFatal(_) =>\n              logWarning(\"URL.setURLStreamHandlerFactory failed to set FsUrlStreamHandlerFactory\")\n          }\n        }"
  }
]