[
  {
    "id" : "0a79628a-c2a8-4474-bbb3-e6b4e8b7516d",
    "prId" : 30642,
    "prUrl" : "https://github.com/apache/spark/pull/30642#pullrequestreview-552631911",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c051401a-2cc0-47f7-a7ab-c3a09875d63e",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "why we don't look at `options` now?",
        "createdAt" : "2020-12-15T15:36:16Z",
        "updatedAt" : "2020-12-16T12:50:56Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "8db2427f-9dd6-4ea2-902a-00a35f328d0f",
        "parentId" : "c051401a-2cc0-47f7-a7ab-c3a09875d63e",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "it is same as session.initialSessionOptions",
        "createdAt" : "2020-12-15T16:20:04Z",
        "updatedAt" : "2020-12-16T12:50:56Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "dad24543aa7bb7cc81d2a8522112eb797b015633",
    "line" : 41,
    "diffHunk" : "@@ -1,1 +91,95 @@      // the later added configs to spark conf shall be respected too\n      SQLConf.mergeNonStaticSQLConfigs(conf, session.sparkContext.conf.getAll.toMap)\n      SQLConf.mergeNonStaticSQLConfigs(conf, session.initialSessionOptions)\n      conf\n    }"
  },
  {
    "id" : "79848884-9fe9-4075-a1ce-5baca7a8f787",
    "prId" : 30642,
    "prUrl" : "https://github.com/apache/spark/pull/30642#pullrequestreview-553372793",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fba67e64-e659-4734-8ef5-aba29a4d2733",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "shall we use `sharedState.initialConfigs` to make it clear?",
        "createdAt" : "2020-12-16T05:18:36Z",
        "updatedAt" : "2020-12-16T12:50:56Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "dd23c3ac-367a-4542-9faa-0a65b1bcad3c",
        "parentId" : "fba67e64-e659-4734-8ef5-aba29a4d2733",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "the warehouse directory in it may be not final, so the `session.sharedState.conf` shall be used here",
        "createdAt" : "2020-12-16T05:45:09Z",
        "updatedAt" : "2020-12-16T12:50:56Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "dad24543aa7bb7cc81d2a8522112eb797b015633",
    "line" : 38,
    "diffHunk" : "@@ -1,1 +88,92 @@    }.getOrElse {\n      val conf = new SQLConf\n      SQLConf.mergeSparkConf(conf, session.sharedState.conf)\n      // the later added configs to spark conf shall be respected too\n      SQLConf.mergeNonStaticSQLConfigs(conf, session.sparkContext.conf.getAll.toMap)"
  },
  {
    "id" : "6cc0e8e0-d58b-47a6-9ce6-76eaba5b1b75",
    "prId" : 30558,
    "prUrl" : "https://github.com/apache/spark/pull/30558#pullrequestreview-542487720",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5a71db9f-07dd-476e-92f8-ab6dcc504200",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "This name does not explain the goal. IMO, this is misleading.  Let us make the API name more general. It is not related to the data sources. \r\n",
        "createdAt" : "2020-12-02T04:19:34Z",
        "updatedAt" : "2020-12-02T04:21:54Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      }
    ],
    "commit" : "11f07cd7c44bb92543dc534a30eac491a1553954",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +273,277 @@   * Note that this may NOT depend on the `optimizer` function.\n   */\n  protected def customDataSourceRewriteRules: Seq[Rule[LogicalPlan]] = Nil\n\n  /**"
  },
  {
    "id" : "a4969408-7b3b-4fd1-b3b0-03910f11db9b",
    "prId" : 25019,
    "prUrl" : "https://github.com/apache/spark/pull/25019#pullrequestreview-263988470",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "24b64b0e-bab1-4888-b000-ef15e43e42f7",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Can we resolve this issue in the `InsertIntoDataSourceDirCommand` side?",
        "createdAt" : "2019-07-05T02:02:27Z",
        "updatedAt" : "2019-07-05T02:02:38Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "d39f5ba7-a05f-45dc-837e-59d79d372b09",
        "parentId" : "24b64b0e-bab1-4888-b000-ef15e43e42f7",
        "authorId" : "2e5e2a86-4faf-484a-9251-577e584e1564",
        "body" : "@maropu  Yes, I agree that resolving this issue inside `InsertIntoDataSourceDirCommand` is better, but as I have already questioned is the jira: https://issues.apache.org/jira/browse/SPARK-28195, I'm not sure whether there are some special consideration for making the children of `InsertIntoDataSourceDirCommand` to empty and use innerChildren instead.",
        "createdAt" : "2019-07-05T03:25:17Z",
        "updatedAt" : "2019-07-05T03:25:18Z",
        "lastEditedBy" : "2e5e2a86-4faf-484a-9251-577e584e1564",
        "tags" : [
        ]
      },
      {
        "id" : "487d9426-9d3f-4123-a124-a57c6ead45ff",
        "parentId" : "24b64b0e-bab1-4888-b000-ef15e43e42f7",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "You meant this issue only happened when `spark.sql.runSQLOnFiles`=true? What if `spark.sql.runSQLOnFiles`=false?",
        "createdAt" : "2019-07-07T03:02:23Z",
        "updatedAt" : "2019-07-07T03:02:23Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "72ef50cd-928e-49ab-982b-ee6a6ed40641",
        "parentId" : "24b64b0e-bab1-4888-b000-ef15e43e42f7",
        "authorId" : "2e5e2a86-4faf-484a-9251-577e584e1564",
        "body" : "@maropu This issue affect more aspects, what I point out in the jira is just for the case \"table or view not found\", actually, this issue may cause many other problems.\r\nThe root cause is that the the children of InsertIntoDataSourceDirCommand is empty, thus many analysis rules after InsertIntoDataSourceDirCommand being inserted(In DataSourceAnalysis rule) may not be effective and so was it for  `CheckAnalysis`.\r\nI think we can fix it better inside InsertIntoDataSourceDirCommand, but I should first make it clear that why we set it's children to empty, but use innerChildren instead? Is there any PR or issue for that?",
        "createdAt" : "2019-07-10T10:45:01Z",
        "updatedAt" : "2019-07-10T10:48:33Z",
        "lastEditedBy" : "2e5e2a86-4faf-484a-9251-577e584e1564",
        "tags" : [
        ]
      },
      {
        "id" : "b719f871-72ff-4897-b81d-817be10315a7",
        "parentId" : "24b64b0e-bab1-4888-b000-ef15e43e42f7",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I think you should investigate it by yourself.  I don't like the current approach too. Appeartly issue is minor but the fix is pretty invasive. I doesn't need to touch SessionStateBuilder side at all.",
        "createdAt" : "2019-07-19T02:25:03Z",
        "updatedAt" : "2019-07-19T02:25:03Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "ec711af6008f8b0178e5aa435b53556630b26d28",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +197,201 @@        case _ => plan\n      }\n      super.checkAnalysis(planToCheck)\n    }\n  }"
  }
]