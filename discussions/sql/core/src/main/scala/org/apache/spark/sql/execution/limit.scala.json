[
  {
    "id" : "234a1aa5-13c4-4363-a416-f32feaf7e44f",
    "prId" : 31468,
    "prUrl" : "https://github.com/apache/spark/pull/31468#pullrequestreview-593887447",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "34f76067-a729-4fcd-bd8f-4250b1d0133c",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "EmptyRDD?",
        "createdAt" : "2021-02-10T18:53:08Z",
        "updatedAt" : "2021-02-10T18:53:08Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "68ab0ae2-54b3-4628-93c9-d985c434e22c",
        "parentId" : "34f76067-a729-4fcd-bd8f-4250b1d0133c",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "kindly ping @zhenglaizhang ",
        "createdAt" : "2021-02-18T01:51:44Z",
        "updatedAt" : "2021-02-18T01:51:44Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "ed7a4738-39b3-4a9c-bf49-2c1dee920205",
        "parentId" : "34f76067-a729-4fcd-bd8f-4250b1d0133c",
        "authorId" : "685f805b-e4fa-4f21-b066-58afcecf9ce6",
        "body" : "@viirya @maropu  we can not set the number of partitions in an `EmptyRDD`, it always has zero partitions.\r\n",
        "createdAt" : "2021-02-19T03:48:28Z",
        "updatedAt" : "2021-02-19T03:48:28Z",
        "lastEditedBy" : "685f805b-e4fa-4f21-b066-58afcecf9ce6",
        "tags" : [
        ]
      },
      {
        "id" : "ad353b15-6ca9-4cbf-aecf-55de45d1fe06",
        "parentId" : "34f76067-a729-4fcd-bd8f-4250b1d0133c",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Isn't the `childRDD` zero partitions?",
        "createdAt" : "2021-02-19T04:02:56Z",
        "updatedAt" : "2021-02-19T04:02:56Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "a4dfe101-eda8-41b7-946f-daca325055ec",
        "parentId" : "34f76067-a729-4fcd-bd8f-4250b1d0133c",
        "authorId" : "685f805b-e4fa-4f21-b066-58afcecf9ce6",
        "body" : "`new ParallelCollectionRDD(sparkContext, Seq.empty[InternalRow], 1, Map.empty)` is a empty rdd with single partition",
        "createdAt" : "2021-02-19T04:04:37Z",
        "updatedAt" : "2021-02-19T04:04:37Z",
        "lastEditedBy" : "685f805b-e4fa-4f21-b066-58afcecf9ce6",
        "tags" : [
        ]
      },
      {
        "id" : "39cb08f5-ca35-4f3e-8d6a-2f186d1e06cf",
        "parentId" : "34f76067-a729-4fcd-bd8f-4250b1d0133c",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Oh I see. I am not sure if `CollectLimitExec` must be single partition, but this looks minor. I'm okay with `ParallelCollectionRDD`. Thanks.",
        "createdAt" : "2021-02-19T04:17:39Z",
        "updatedAt" : "2021-02-19T04:17:40Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "455ba56b-135c-4c52-939e-1199ee8a0992",
        "parentId" : "34f76067-a729-4fcd-bd8f-4250b1d0133c",
        "authorId" : "685f805b-e4fa-4f21-b066-58afcecf9ce6",
        "body" : "@viirya the `outputPartitioning` of `CollectLimitExec` is  `SinglePartition`, so the output rdd should have single partition.",
        "createdAt" : "2021-02-19T05:36:59Z",
        "updatedAt" : "2021-02-19T05:36:59Z",
        "lastEditedBy" : "685f805b-e4fa-4f21-b066-58afcecf9ce6",
        "tags" : [
        ]
      },
      {
        "id" : "899b0ead-15d5-43db-84d4-653171aad2ab",
        "parentId" : "34f76067-a729-4fcd-bd8f-4250b1d0133c",
        "authorId" : "685f805b-e4fa-4f21-b066-58afcecf9ce6",
        "body" : "or we can use `EmptyRDDWithPartitions` defined in `CoalesceExec`?\r\nmaybe we can make `EmptyRDD` support number of partition, so we can use it in both `CollectLimitExec` and `CoalesceExec`. But I am not sure whether we should do this.",
        "createdAt" : "2021-02-19T05:44:58Z",
        "updatedAt" : "2021-02-19T05:44:59Z",
        "lastEditedBy" : "685f805b-e4fa-4f21-b066-58afcecf9ce6",
        "tags" : [
        ]
      },
      {
        "id" : "c1b05913-acf4-45e3-b94c-f39d4b4c5fd4",
        "parentId" : "34f76067-a729-4fcd-bd8f-4250b1d0133c",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "> @viirya the `outputPartitioning` of `CollectLimitExec` is `SinglePartition`, so the output rdd should have single partition.\r\n\r\nOh, my previous comment was confusing. I mean I am not sure if its `outputPartitioning` must be ` SinglePartition`.",
        "createdAt" : "2021-02-19T06:09:31Z",
        "updatedAt" : "2021-02-19T06:09:31Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "058a6810-0dee-41f3-9e4b-4170407ab11e",
        "parentId" : "34f76067-a729-4fcd-bd8f-4250b1d0133c",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "> or we can use `EmptyRDDWithPartitions` defined in `CoalesceExec`?\r\n> maybe we can make `EmptyRDD` support number of partition, so we can use it in both `CollectLimitExec` and `CoalesceExec`. But I am not sure whether we should do this.\r\n\r\nI think it sounds over-engineering. At least for now I don't think we need it.",
        "createdAt" : "2021-02-19T06:10:28Z",
        "updatedAt" : "2021-02-19T06:10:28Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "75c4d92dc7d24b6698b2eb7e5d103e95e925247a",
    "line" : 33,
    "diffHunk" : "@@ -1,1 +56,60 @@    val childRDD = child.execute()\n    if (childRDD.getNumPartitions == 0) {\n      new ParallelCollectionRDD(sparkContext, Seq.empty[InternalRow], 1, Map.empty)\n    } else {\n      val singlePartitionRDD = if (childRDD.getNumPartitions == 1) {"
  },
  {
    "id" : "07b40a30-29fc-4b3f-9cdc-b6ef84179130",
    "prId" : 26809,
    "prUrl" : "https://github.com/apache/spark/pull/26809#pullrequestreview-329530911",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b1db9335-68a2-4d0b-ab91-97e1a4c5ae9c",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "looks like `executeTail` scans partitions from the end until returning last `n` rows? This `doExecute` looks like different to it?",
        "createdAt" : "2019-12-10T00:11:57Z",
        "updatedAt" : "2019-12-30T04:46:12Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "bd62238d-62c2-4286-b852-cb113e159c74",
        "parentId" : "b1db9335-68a2-4d0b-ab91-97e1a4c5ae9c",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Yes, I noticed `executeTake` vs `doExecute` in `Limit` too. It's a bit different.\r\n\r\n`executeTail` and `doExecute` here do the same thing but in a different way. I think it's similar with `limit(...).collect()` vs  `limit(...).foreach()`.\r\n\r\nFor `tail` case, actually, we can even remove all such plans at all (because we will currently only have one API `tail(...)` alone unlike `limit` for now). This was a pain point when I worked on this PR. \r\n\r\nI just decided to be consistent with `Limit` considering the case we push down such plans into DataSource V2 later or we expose this plan as another API (or use it in another API). I can remove this if any of you feels strongly.",
        "createdAt" : "2019-12-10T00:56:21Z",
        "updatedAt" : "2019-12-30T04:46:12Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "738d3e1307144504ed8977a7758ce40a602b6cf3",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +76,80 @@  override def outputPartitioning: Partitioning = SinglePartition\n  override def executeCollect(): Array[InternalRow] = child.executeTail(limit)\n  protected override def doExecute(): RDD[InternalRow] = {\n    // This is a bit hacky way to avoid a shuffle and scanning all data when it performs\n    // at `Dataset.tail`."
  },
  {
    "id" : "5d771753-a10e-4179-89cb-4e3072da0504",
    "prId" : 26809,
    "prUrl" : "https://github.com/apache/spark/pull/26809#pullrequestreview-329820124",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "12589ea0-6623-45fa-8993-db09e8209f83",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "For now maybe a simple solution is to just call `executeCollect` and build a new RDD from local data.",
        "createdAt" : "2019-12-10T13:09:23Z",
        "updatedAt" : "2019-12-30T04:46:12Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "738d3e1307144504ed8977a7758ce40a602b6cf3",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +76,80 @@  override def outputPartitioning: Partitioning = SinglePartition\n  override def executeCollect(): Array[InternalRow] = child.executeTail(limit)\n  protected override def doExecute(): RDD[InternalRow] = {\n    // This is a bit hacky way to avoid a shuffle and scanning all data when it performs\n    // at `Dataset.tail`."
  }
]