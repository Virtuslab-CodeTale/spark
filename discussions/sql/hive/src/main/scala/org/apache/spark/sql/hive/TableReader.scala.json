[
  {
    "id" : "f72229c9-22e9-4fe4-a712-8aebe9ee83a2",
    "prId" : 33406,
    "prUrl" : "https://github.com/apache/spark/pull/33406#pullrequestreview-709203867",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cfc52f6d-a6fd-4b2a-9a96-6b43bf8fbba8",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Can we call `createNewHadoopRDD(tableDesc: TableDesc, path: String)` here?",
        "createdAt" : "2021-07-19T06:51:25Z",
        "updatedAt" : "2021-07-19T06:51:25Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "3ed9a5d217c513afd2f7d48a9bf4bf5d057799ff",
    "line" : 78,
    "diffHunk" : "@@ -1,1 +378,382 @@  private def createNewHadoopRDD(partDesc: PartitionDesc, path: String): RDD[Writable] = {\n    val newJobConf = new JobConf(hadoopConf)\n    HadoopTableReader.initializeLocalJobConfFunc(path, partDesc.getTableDesc)(newJobConf)\n    val inputFormatClass = partDesc.getInputFileFormatClass\n      .asInstanceOf[Class[newInputClass[Writable, Writable]]]"
  },
  {
    "id" : "27ee5dd9-f87c-4ca8-b7d6-25566a534c4d",
    "prId" : 33406,
    "prUrl" : "https://github.com/apache/spark/pull/33406#pullrequestreview-709206064",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d90b0abb-819e-4103-a4cd-22839b64969d",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Can we call `createOldHadoopRDD(tableDesc: TableDesc, path: String) ` here?",
        "createdAt" : "2021-07-19T06:52:01Z",
        "updatedAt" : "2021-07-19T06:52:01Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "5571fd90-8b20-43fe-be85-bb456bf16d51",
        "parentId" : "d90b0abb-819e-4103-a4cd-22839b64969d",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "`partitionDesc` is still needed here to `getInputFileFormatClass `",
        "createdAt" : "2021-07-19T06:54:57Z",
        "updatedAt" : "2021-07-19T06:54:57Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "3ed9a5d217c513afd2f7d48a9bf4bf5d057799ff",
    "line" : 57,
    "diffHunk" : "@@ -1,1 +342,346 @@  private def createOldHadoopRDD(partitionDesc: PartitionDesc, path: String): RDD[Writable] = {\n    val initializeJobConfFunc =\n      HadoopTableReader.initializeLocalJobConfFunc(path, partitionDesc.getTableDesc) _\n    val inputFormatClass = partitionDesc.getInputFileFormatClass\n      .asInstanceOf[Class[oldInputClass[Writable, Writable]]]"
  },
  {
    "id" : "b1f063dc-077f-411a-8d24-de3e61c44364",
    "prId" : 31302,
    "prUrl" : "https://github.com/apache/spark/pull/31302#pullrequestreview-574742257",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b4344ad5-0eb6-4e37-b69d-518da7ef216d",
        "parentId" : null,
        "authorId" : "6a8d0007-eb0f-4236-a65a-2174a14ac689",
        "body" : "Can you add a test to suite for this change and explain the error?",
        "createdAt" : "2021-01-23T06:51:29Z",
        "updatedAt" : "2021-01-23T06:59:16Z",
        "lastEditedBy" : "6a8d0007-eb0f-4236-a65a-2174a14ac689",
        "tags" : [
        ]
      },
      {
        "id" : "d179ae8c-89a4-4dd1-b1fe-27e248717509",
        "parentId" : "b4344ad5-0eb6-4e37-b69d-518da7ef216d",
        "authorId" : "255b7267-adfb-4674-91dc-9fad9539c433",
        "body" : "OK, let me see how to write it",
        "createdAt" : "2021-01-23T06:57:12Z",
        "updatedAt" : "2021-01-23T06:57:13Z",
        "lastEditedBy" : "255b7267-adfb-4674-91dc-9fad9539c433",
        "tags" : [
        ]
      }
    ],
    "commit" : "787c1df87bde6f914ff5ca37ff5954503ff29dec",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +301,305 @@    val inputFormatClazz = localTableDesc.getInputFileFormatClass\n    if (classOf[oldInputClass[_, _]].isAssignableFrom(inputFormatClazz)) {\n      createOldHadoopRDD(localTableDesc, inputPathStr)\n    } else {\n      createNewHadoopRDD(localTableDesc, inputPathStr)"
  },
  {
    "id" : "93e03296-c9b7-445c-b6ef-5e2c2d6713d0",
    "prId" : 31133,
    "prUrl" : "https://github.com/apache/spark/pull/31133#pullrequestreview-583832957",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6d1f70a8-3c11-4dbe-aa65-5c33f0e85325",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "~`&& tableProperties.containsKey(k)` looks risky to me and beyond this PR's test coverage. According to the test case, it looks like `k == AvroTableProperties.SCHEMA_LITERAL.getPropName()` is enough to pass the test case. Did I understand correctly?~ My bad. Never mind.",
        "createdAt" : "2021-02-04T21:34:53Z",
        "updatedAt" : "2021-02-05T07:01:10Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "20dc1676da8514708a139f8e4ac66c3034ed7b5e",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +257,261 @@        val props = new Properties(tableProperties)\n        partProps.asScala.filterNot { case (k, _) =>\n          k == AvroTableProperties.SCHEMA_LITERAL.getPropName() && tableProperties.containsKey(k)\n        }.foreach {\n          case (key, value) => props.setProperty(key, value)"
  },
  {
    "id" : "ff547774-023a-4c57-a596-ada7c5a4f042",
    "prId" : 29178,
    "prUrl" : "https://github.com/apache/spark/pull/29178#pullrequestreview-452956209",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3c1bfecc-1c28-4ad5-9cf9-a51f0df12b32",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Do you think we can have a test case for this?",
        "createdAt" : "2020-07-21T15:26:53Z",
        "updatedAt" : "2020-07-21T15:26:53Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "96b93266-cbcf-42e7-8091-143b374ecf23",
        "parentId" : "3c1bfecc-1c28-4ad5-9cf9-a51f0df12b32",
        "authorId" : "e797987b-f2b0-4d36-b6ea-8d0323789423",
        "body" : "I try Spark2.4.3 can work well.\r\nspark3.0.0 will createNewHadoopRDD, and call 'val allRowSplits = inputFormat.getSplits(new JobContextImpl(_conf, jobId)).asScala' in method getPartitions, this will call TableInputFormatBase.getSplits(JobContext context), but the variable table is null, so throw the execption; \r\n\r\n",
        "createdAt" : "2020-07-22T02:41:05Z",
        "updatedAt" : "2020-07-22T02:41:06Z",
        "lastEditedBy" : "e797987b-f2b0-4d36-b6ea-8d0323789423",
        "tags" : [
        ]
      }
    ],
    "commit" : "d170e2463cc7865f2907a861f6d77bbd3e92ca23",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +302,306 @@    if (!inputFormatClazz.getName.\n      equalsIgnoreCase(\"org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat\")\n      && classOf[newInputClass[_, _]].isAssignableFrom(inputFormatClazz)) {\n      createNewHadoopRDD(localTableDesc, inputPathStr)\n    } else {"
  },
  {
    "id" : "18e7f65a-a2e1-4fda-87c3-db7edb25c5f4",
    "prId" : 29178,
    "prUrl" : "https://github.com/apache/spark/pull/29178#pullrequestreview-564167318",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1a514e9c-871c-4c88-8772-d701fd16f9d7",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Do you know why `org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat` implements new Hadoop inputformat interface but doesn't work?",
        "createdAt" : "2020-07-22T04:32:00Z",
        "updatedAt" : "2020-07-22T04:32:00Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "8d89d5fb-3b61-4849-bf1e-1aeb88327d2e",
        "parentId" : "1a514e9c-871c-4c88-8772-d701fd16f9d7",
        "authorId" : "01edfaee-e1bc-40b2-9868-ae21812c44cf",
        "body" : "It looks like the new MapReduce API (`org.apache.hadoop.mapreduce`) used when creating NewHadoopRDD , but The getsplits method of HiveHBaseTableInputFormat is implemented by org.apache.hadoop.mapred API，so some initialization operations (Table、connection) are not done,so the obtained variable table is null.And when using methord createOldHadoopRDD will use the org.apache.hadoop.mapred API,and some initialization operations (Table、connection) are doing,so it can work well.",
        "createdAt" : "2021-01-08T10:47:31Z",
        "updatedAt" : "2021-01-08T10:47:31Z",
        "lastEditedBy" : "01edfaee-e1bc-40b2-9868-ae21812c44cf",
        "tags" : [
        ]
      }
    ],
    "commit" : "d170e2463cc7865f2907a861f6d77bbd3e92ca23",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +301,305 @@    val inputFormatClazz = localTableDesc.getInputFileFormatClass\n    if (!inputFormatClazz.getName.\n      equalsIgnoreCase(\"org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat\")\n      && classOf[newInputClass[_, _]].isAssignableFrom(inputFormatClazz)) {\n      createNewHadoopRDD(localTableDesc, inputPathStr)"
  },
  {
    "id" : "2b93f0c0-09b1-4033-a841-21e88e3ac738",
    "prId" : 26895,
    "prUrl" : "https://github.com/apache/spark/pull/26895#pullrequestreview-332285870",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a20b068e-70a2-449c-b22d-584082659e42",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Is there any other way to guard against this besides a global lock? that could introduce contention. `deserializer` is a local object here; it accesses some non-thread-safe shared state?",
        "createdAt" : "2019-12-15T14:02:39Z",
        "updatedAt" : "2019-12-18T19:32:29Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "c4aa2fbc-5dad-4984-bd46-63e8ae85d893",
        "parentId" : "a20b068e-70a2-449c-b22d-584082659e42",
        "authorId" : "119f7711-a251-4127-b455-51a922a097f1",
        "body" : "I don't think there will be a lot of contention. We call `deserializer.initialize` once for each partition.\r\nThe race is actually in `HCatRecordObjectInspectorFactory.getHCatRecordObjectInspector` which is called by `JsonSerDe#initialize` (the factory uses a `HashMap` instead of , e.g., a `ConcurrentHashMap` to maintain a cache). We want the same `ObjectInspector` to be returned by the factory cache and set in each `JsonSerDe` instance as its `ObjectInspector`.\r\nOur objective is that there not be more than one task from each executor calling `JsonSerDe#initialize` at the same time.",
        "createdAt" : "2019-12-15T19:43:50Z",
        "updatedAt" : "2019-12-18T19:32:29Z",
        "lastEditedBy" : "119f7711-a251-4127-b455-51a922a097f1",
        "tags" : [
        ]
      },
      {
        "id" : "e5c2cb28-56e4-496d-a464-6cec6af9e635",
        "parentId" : "a20b068e-70a2-449c-b22d-584082659e42",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Fair enough. Is there any way to fix this in the Hadoop implementation? or is it already fixed but we have to deal with it here? \r\nCan we push the global lock down further rather than making callers know to use it?",
        "createdAt" : "2019-12-15T19:50:51Z",
        "updatedAt" : "2019-12-18T19:32:29Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "e733ed1d-b290-4ce6-8f01-9640c0ec92ee",
        "parentId" : "a20b068e-70a2-449c-b22d-584082659e42",
        "authorId" : "119f7711-a251-4127-b455-51a922a097f1",
        "body" : "HIVE-15773 and HIVE-21752 offer two different fixes for the `HCatRecordObjectInspectorFactory` bug. HIVE-21752 has been merged in Hive 4.0.0, which is not released yet. Even when it is, not many users will be on Hive 4.0.0 for some time, so I think we need to deal with it here.",
        "createdAt" : "2019-12-15T20:08:09Z",
        "updatedAt" : "2019-12-18T19:32:29Z",
        "lastEditedBy" : "119f7711-a251-4127-b455-51a922a097f1",
        "tags" : [
        ]
      },
      {
        "id" : "6c47ffa3-0190-431a-ad9d-efd4d8a948f2",
        "parentId" : "a20b068e-70a2-449c-b22d-584082659e42",
        "authorId" : "119f7711-a251-4127-b455-51a922a097f1",
        "body" : "On the question of other callers needing to use this global lock:\r\nAFAIK, the only reported problem in Spark with the `HCatRecordObjectInspectorFactory` bug is in reading partitioned `JsonSerDe` tables. In `HadoopTableReader#makeRDDForTable`, we can even do without using the lock; we only really need to use the lock in `HadoopTableReader#makeRDDForPartitionedTable`.\r\nFrom my search, there are two other places in Spark where we call `Deserializer#initialize`:\r\n1. `HiveTableScanExec`, in initializing a `HadoopTableReader` instance, before `HadoopTableReader#makeRDDForTable` or `HadoopTableReader#makeRDDForPartitionedTable` even get called in `doExec`.\r\n2. `HiveScriptIOSchema`.\r\n\r\nI don't think we need to use the lock for 1.\r\nI don't know about 2., but if no problem has been reported due to the race, we can also leave it alone.\r\nIn other words, I'm not proposing we guard against the race in the Hive bug everywhere, just in this known case.",
        "createdAt" : "2019-12-15T20:39:08Z",
        "updatedAt" : "2019-12-18T19:32:29Z",
        "lastEditedBy" : "119f7711-a251-4127-b455-51a922a097f1",
        "tags" : [
        ]
      }
    ],
    "commit" : "2d23e43ac823ce0562c480071a656d099b2c32b9",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +133,137 @@      val hconf = broadcastedHadoopConf.value.value\n      val deserializer = deserializerClass.getConstructor().newInstance()\n      DeserializerLock.synchronized {\n        deserializer.initialize(hconf, localTableDesc.getProperties)\n      }"
  },
  {
    "id" : "001a7cfa-3caa-4197-871e-7b16af8e0dc0",
    "prId" : 26895,
    "prUrl" : "https://github.com/apache/spark/pull/26895#pullrequestreview-333465630",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ca75ac3b-0bd4-4bf9-9e71-eede71298e40",
        "parentId" : null,
        "authorId" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "body" : "What you have is ok but you can completely avoid this second serde instance; in the common case both `localDeserializer` and `localTableDesc.getDeserializerClass` will be the same.",
        "createdAt" : "2019-12-17T00:09:21Z",
        "updatedAt" : "2019-12-18T19:32:29Z",
        "lastEditedBy" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "tags" : [
        ]
      },
      {
        "id" : "aef23e67-d132-4121-87be-fcdd483b1fd8",
        "parentId" : "ca75ac3b-0bd4-4bf9-9e71-eede71298e40",
        "authorId" : "119f7711-a251-4127-b455-51a922a097f1",
        "body" : "Yes, I did find that to be the case in my repro, that the two were the same class (JsonSerDe). However, the initialize calls on deserializer and on tableSerDe are with potentially different properties (props and tableProperties could differ), so I think I should initialize tableSerDe even if it has the same class as deserializer.",
        "createdAt" : "2019-12-17T07:02:43Z",
        "updatedAt" : "2019-12-18T19:32:29Z",
        "lastEditedBy" : "119f7711-a251-4127-b455-51a922a097f1",
        "tags" : [
        ]
      },
      {
        "id" : "7c92ed46-1998-466f-9da7-8c933750911d",
        "parentId" : "ca75ac3b-0bd4-4bf9-9e71-eede71298e40",
        "authorId" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "body" : "Ah, ok, missed that when reading the code.",
        "createdAt" : "2019-12-17T17:53:03Z",
        "updatedAt" : "2019-12-18T19:32:29Z",
        "lastEditedBy" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "tags" : [
        ]
      }
    ],
    "commit" : "2d23e43ac823ce0562c480071a656d099b2c32b9",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +261,265 @@        val tableSerDe = localTableDesc.getDeserializerClass.getConstructor().newInstance()\n        DeserializerLock.synchronized {\n          tableSerDe.initialize(hconf, tableProperties)\n        }\n"
  }
]