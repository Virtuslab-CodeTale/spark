[
  {
    "id" : "f72229c9-22e9-4fe4-a712-8aebe9ee83a2",
    "prId" : 33406,
    "prUrl" : "https://github.com/apache/spark/pull/33406#pullrequestreview-709203867",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cfc52f6d-a6fd-4b2a-9a96-6b43bf8fbba8",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Can we call `createNewHadoopRDD(tableDesc: TableDesc, path: String)` here?",
        "createdAt" : "2021-07-19T06:51:25Z",
        "updatedAt" : "2021-07-19T06:51:25Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "3ed9a5d217c513afd2f7d48a9bf4bf5d057799ff",
    "line" : 78,
    "diffHunk" : "@@ -1,1 +378,382 @@  private def createNewHadoopRDD(partDesc: PartitionDesc, path: String): RDD[Writable] = {\n    val newJobConf = new JobConf(hadoopConf)\n    HadoopTableReader.initializeLocalJobConfFunc(path, partDesc.getTableDesc)(newJobConf)\n    val inputFormatClass = partDesc.getInputFileFormatClass\n      .asInstanceOf[Class[newInputClass[Writable, Writable]]]"
  },
  {
    "id" : "27ee5dd9-f87c-4ca8-b7d6-25566a534c4d",
    "prId" : 33406,
    "prUrl" : "https://github.com/apache/spark/pull/33406#pullrequestreview-709206064",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d90b0abb-819e-4103-a4cd-22839b64969d",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Can we call `createOldHadoopRDD(tableDesc: TableDesc, path: String) ` here?",
        "createdAt" : "2021-07-19T06:52:01Z",
        "updatedAt" : "2021-07-19T06:52:01Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "5571fd90-8b20-43fe-be85-bb456bf16d51",
        "parentId" : "d90b0abb-819e-4103-a4cd-22839b64969d",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "`partitionDesc` is still needed here to `getInputFileFormatClass `",
        "createdAt" : "2021-07-19T06:54:57Z",
        "updatedAt" : "2021-07-19T06:54:57Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "3ed9a5d217c513afd2f7d48a9bf4bf5d057799ff",
    "line" : 57,
    "diffHunk" : "@@ -1,1 +342,346 @@  private def createOldHadoopRDD(partitionDesc: PartitionDesc, path: String): RDD[Writable] = {\n    val initializeJobConfFunc =\n      HadoopTableReader.initializeLocalJobConfFunc(path, partitionDesc.getTableDesc) _\n    val inputFormatClass = partitionDesc.getInputFileFormatClass\n      .asInstanceOf[Class[oldInputClass[Writable, Writable]]]"
  },
  {
    "id" : "b1f063dc-077f-411a-8d24-de3e61c44364",
    "prId" : 31302,
    "prUrl" : "https://github.com/apache/spark/pull/31302#pullrequestreview-574742257",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b4344ad5-0eb6-4e37-b69d-518da7ef216d",
        "parentId" : null,
        "authorId" : "6a8d0007-eb0f-4236-a65a-2174a14ac689",
        "body" : "Can you add a test to suite for this change and explain the error?",
        "createdAt" : "2021-01-23T06:51:29Z",
        "updatedAt" : "2021-01-23T06:59:16Z",
        "lastEditedBy" : "6a8d0007-eb0f-4236-a65a-2174a14ac689",
        "tags" : [
        ]
      },
      {
        "id" : "d179ae8c-89a4-4dd1-b1fe-27e248717509",
        "parentId" : "b4344ad5-0eb6-4e37-b69d-518da7ef216d",
        "authorId" : "255b7267-adfb-4674-91dc-9fad9539c433",
        "body" : "OK, let me see how to write it",
        "createdAt" : "2021-01-23T06:57:12Z",
        "updatedAt" : "2021-01-23T06:57:13Z",
        "lastEditedBy" : "255b7267-adfb-4674-91dc-9fad9539c433",
        "tags" : [
        ]
      }
    ],
    "commit" : "787c1df87bde6f914ff5ca37ff5954503ff29dec",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +301,305 @@    val inputFormatClazz = localTableDesc.getInputFileFormatClass\n    if (classOf[oldInputClass[_, _]].isAssignableFrom(inputFormatClazz)) {\n      createOldHadoopRDD(localTableDesc, inputPathStr)\n    } else {\n      createNewHadoopRDD(localTableDesc, inputPathStr)"
  },
  {
    "id" : "93e03296-c9b7-445c-b6ef-5e2c2d6713d0",
    "prId" : 31133,
    "prUrl" : "https://github.com/apache/spark/pull/31133#pullrequestreview-583832957",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6d1f70a8-3c11-4dbe-aa65-5c33f0e85325",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "~`&& tableProperties.containsKey(k)` looks risky to me and beyond this PR's test coverage. According to the test case, it looks like `k == AvroTableProperties.SCHEMA_LITERAL.getPropName()` is enough to pass the test case. Did I understand correctly?~ My bad. Never mind.",
        "createdAt" : "2021-02-04T21:34:53Z",
        "updatedAt" : "2021-02-05T07:01:10Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "20dc1676da8514708a139f8e4ac66c3034ed7b5e",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +257,261 @@        val props = new Properties(tableProperties)\n        partProps.asScala.filterNot { case (k, _) =>\n          k == AvroTableProperties.SCHEMA_LITERAL.getPropName() && tableProperties.containsKey(k)\n        }.foreach {\n          case (key, value) => props.setProperty(key, value)"
  },
  {
    "id" : "ff547774-023a-4c57-a596-ada7c5a4f042",
    "prId" : 29178,
    "prUrl" : "https://github.com/apache/spark/pull/29178#pullrequestreview-452956209",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3c1bfecc-1c28-4ad5-9cf9-a51f0df12b32",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Do you think we can have a test case for this?",
        "createdAt" : "2020-07-21T15:26:53Z",
        "updatedAt" : "2020-07-21T15:26:53Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "96b93266-cbcf-42e7-8091-143b374ecf23",
        "parentId" : "3c1bfecc-1c28-4ad5-9cf9-a51f0df12b32",
        "authorId" : "e797987b-f2b0-4d36-b6ea-8d0323789423",
        "body" : "I try Spark2.4.3 can work well.\r\nspark3.0.0 will createNewHadoopRDD, and call 'val allRowSplits = inputFormat.getSplits(new JobContextImpl(_conf, jobId)).asScala' in method getPartitions, this will call TableInputFormatBase.getSplits(JobContext context), but the variable table is null, so throw the execption; \r\n\r\n",
        "createdAt" : "2020-07-22T02:41:05Z",
        "updatedAt" : "2020-07-22T02:41:06Z",
        "lastEditedBy" : "e797987b-f2b0-4d36-b6ea-8d0323789423",
        "tags" : [
        ]
      }
    ],
    "commit" : "d170e2463cc7865f2907a861f6d77bbd3e92ca23",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +302,306 @@    if (!inputFormatClazz.getName.\n      equalsIgnoreCase(\"org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat\")\n      && classOf[newInputClass[_, _]].isAssignableFrom(inputFormatClazz)) {\n      createNewHadoopRDD(localTableDesc, inputPathStr)\n    } else {"
  },
  {
    "id" : "18e7f65a-a2e1-4fda-87c3-db7edb25c5f4",
    "prId" : 29178,
    "prUrl" : "https://github.com/apache/spark/pull/29178#pullrequestreview-564167318",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1a514e9c-871c-4c88-8772-d701fd16f9d7",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Do you know why `org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat` implements new Hadoop inputformat interface but doesn't work?",
        "createdAt" : "2020-07-22T04:32:00Z",
        "updatedAt" : "2020-07-22T04:32:00Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "8d89d5fb-3b61-4849-bf1e-1aeb88327d2e",
        "parentId" : "1a514e9c-871c-4c88-8772-d701fd16f9d7",
        "authorId" : "01edfaee-e1bc-40b2-9868-ae21812c44cf",
        "body" : "It looks like the new MapReduce API (`org.apache.hadoop.mapreduce`) used when creating NewHadoopRDD , but The getsplits method of HiveHBaseTableInputFormat is implemented by org.apache.hadoop.mapred API，so some initialization operations (Table、connection) are not done,so the obtained variable table is null.And when using methord createOldHadoopRDD will use the org.apache.hadoop.mapred API,and some initialization operations (Table、connection) are doing,so it can work well.",
        "createdAt" : "2021-01-08T10:47:31Z",
        "updatedAt" : "2021-01-08T10:47:31Z",
        "lastEditedBy" : "01edfaee-e1bc-40b2-9868-ae21812c44cf",
        "tags" : [
        ]
      }
    ],
    "commit" : "d170e2463cc7865f2907a861f6d77bbd3e92ca23",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +301,305 @@    val inputFormatClazz = localTableDesc.getInputFileFormatClass\n    if (!inputFormatClazz.getName.\n      equalsIgnoreCase(\"org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat\")\n      && classOf[newInputClass[_, _]].isAssignableFrom(inputFormatClazz)) {\n      createNewHadoopRDD(localTableDesc, inputPathStr)"
  }
]