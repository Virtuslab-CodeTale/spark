[
  {
    "id" : "f72229c9-22e9-4fe4-a712-8aebe9ee83a2",
    "prId" : 33406,
    "prUrl" : "https://github.com/apache/spark/pull/33406#pullrequestreview-709203867",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cfc52f6d-a6fd-4b2a-9a96-6b43bf8fbba8",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Can we call `createNewHadoopRDD(tableDesc: TableDesc, path: String)` here?",
        "createdAt" : "2021-07-19T06:51:25Z",
        "updatedAt" : "2021-07-19T06:51:25Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "3ed9a5d217c513afd2f7d48a9bf4bf5d057799ff",
    "line" : 78,
    "diffHunk" : "@@ -1,1 +378,382 @@  private def createNewHadoopRDD(partDesc: PartitionDesc, path: String): RDD[Writable] = {\n    val newJobConf = new JobConf(hadoopConf)\n    HadoopTableReader.initializeLocalJobConfFunc(path, partDesc.getTableDesc)(newJobConf)\n    val inputFormatClass = partDesc.getInputFileFormatClass\n      .asInstanceOf[Class[newInputClass[Writable, Writable]]]"
  },
  {
    "id" : "27ee5dd9-f87c-4ca8-b7d6-25566a534c4d",
    "prId" : 33406,
    "prUrl" : "https://github.com/apache/spark/pull/33406#pullrequestreview-709206064",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d90b0abb-819e-4103-a4cd-22839b64969d",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Can we call `createOldHadoopRDD(tableDesc: TableDesc, path: String) ` here?",
        "createdAt" : "2021-07-19T06:52:01Z",
        "updatedAt" : "2021-07-19T06:52:01Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "5571fd90-8b20-43fe-be85-bb456bf16d51",
        "parentId" : "d90b0abb-819e-4103-a4cd-22839b64969d",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "`partitionDesc` is still needed here to `getInputFileFormatClass `",
        "createdAt" : "2021-07-19T06:54:57Z",
        "updatedAt" : "2021-07-19T06:54:57Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "3ed9a5d217c513afd2f7d48a9bf4bf5d057799ff",
    "line" : 57,
    "diffHunk" : "@@ -1,1 +342,346 @@  private def createOldHadoopRDD(partitionDesc: PartitionDesc, path: String): RDD[Writable] = {\n    val initializeJobConfFunc =\n      HadoopTableReader.initializeLocalJobConfFunc(path, partitionDesc.getTableDesc) _\n    val inputFormatClass = partitionDesc.getInputFileFormatClass\n      .asInstanceOf[Class[oldInputClass[Writable, Writable]]]"
  },
  {
    "id" : "b1f063dc-077f-411a-8d24-de3e61c44364",
    "prId" : 31302,
    "prUrl" : "https://github.com/apache/spark/pull/31302#pullrequestreview-574742257",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b4344ad5-0eb6-4e37-b69d-518da7ef216d",
        "parentId" : null,
        "authorId" : "6a8d0007-eb0f-4236-a65a-2174a14ac689",
        "body" : "Can you add a test to suite for this change and explain the error?",
        "createdAt" : "2021-01-23T06:51:29Z",
        "updatedAt" : "2021-01-23T06:59:16Z",
        "lastEditedBy" : "6a8d0007-eb0f-4236-a65a-2174a14ac689",
        "tags" : [
        ]
      },
      {
        "id" : "d179ae8c-89a4-4dd1-b1fe-27e248717509",
        "parentId" : "b4344ad5-0eb6-4e37-b69d-518da7ef216d",
        "authorId" : "255b7267-adfb-4674-91dc-9fad9539c433",
        "body" : "OK, let me see how to write it",
        "createdAt" : "2021-01-23T06:57:12Z",
        "updatedAt" : "2021-01-23T06:57:13Z",
        "lastEditedBy" : "255b7267-adfb-4674-91dc-9fad9539c433",
        "tags" : [
        ]
      }
    ],
    "commit" : "787c1df87bde6f914ff5ca37ff5954503ff29dec",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +301,305 @@    val inputFormatClazz = localTableDesc.getInputFileFormatClass\n    if (classOf[oldInputClass[_, _]].isAssignableFrom(inputFormatClazz)) {\n      createOldHadoopRDD(localTableDesc, inputPathStr)\n    } else {\n      createNewHadoopRDD(localTableDesc, inputPathStr)"
  },
  {
    "id" : "93e03296-c9b7-445c-b6ef-5e2c2d6713d0",
    "prId" : 31133,
    "prUrl" : "https://github.com/apache/spark/pull/31133#pullrequestreview-583832957",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6d1f70a8-3c11-4dbe-aa65-5c33f0e85325",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "~`&& tableProperties.containsKey(k)` looks risky to me and beyond this PR's test coverage. According to the test case, it looks like `k == AvroTableProperties.SCHEMA_LITERAL.getPropName()` is enough to pass the test case. Did I understand correctly?~ My bad. Never mind.",
        "createdAt" : "2021-02-04T21:34:53Z",
        "updatedAt" : "2021-02-05T07:01:10Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "20dc1676da8514708a139f8e4ac66c3034ed7b5e",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +257,261 @@        val props = new Properties(tableProperties)\n        partProps.asScala.filterNot { case (k, _) =>\n          k == AvroTableProperties.SCHEMA_LITERAL.getPropName() && tableProperties.containsKey(k)\n        }.foreach {\n          case (key, value) => props.setProperty(key, value)"
  }
]