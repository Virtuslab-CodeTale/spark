[
  {
    "id" : "fb1e698c-1fd8-47e4-a196-f360d19e25ce",
    "prId" : 30097,
    "prUrl" : "https://github.com/apache/spark/pull/30097#pullrequestreview-528864488",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "79816822-344a-48f5-9248-ea18f5f73878",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "ditto",
        "createdAt" : "2020-11-12T08:47:20Z",
        "updatedAt" : "2020-11-12T08:47:20Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "7612695c78456155a95ad4f7d54ef70e53f88921",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +42,46 @@private[sql] class PruneHiveTablePartitions\n\nprivate[sql] object PruneHiveTablePartitions\n  extends Rule[LogicalPlan] with CastSupport with PredicateHelper {\n"
  },
  {
    "id" : "19139252-cbe7-474d-be7a-d0b88a270155",
    "prId" : 29075,
    "prUrl" : "https://github.com/apache/spark/pull/29075#pullrequestreview-446952184",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e00f114c-8c43-414c-b895-b0fbac47da9d",
        "parentId" : null,
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "I am confused that seems CNFConversion won't change references, You don't need to call a `splitConjunctivePredicates` to each  expr in `remainingFilterInCnf` to extract more predicate?",
        "createdAt" : "2020-07-13T01:42:55Z",
        "updatedAt" : "2020-07-13T08:58:41Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "1fdc6e83-2ded-484a-ac44-157ab56d5e9a",
        "parentId" : "e00f114c-8c43-414c-b895-b0fbac47da9d",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "The `filters` here is already processed with `splitConjunctivePredicates` in `PhysicalOperation.unapply`. That's why the original code before #28805 doesn't call `splitConjunctivePredicates` either.",
        "createdAt" : "2020-07-13T02:54:00Z",
        "updatedAt" : "2020-07-13T08:58:41Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "6fe106ce84e83641d66d572c45050b25761bbf3d",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +63,67 @@    val extraPartitionFilters = remainingFilterInCnf.filter(f =>\n      !f.references.isEmpty && f.references.subsetOf(partitionColumnSet))\n    ExpressionSet(partitionFilters ++ extraPartitionFilters)\n  }\n"
  },
  {
    "id" : "937fc343-c531-4038-a022-09d5ba3f90ad",
    "prId" : 27129,
    "prUrl" : "https://github.com/apache/spark/pull/27129#pullrequestreview-351459100",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "af4c0e3a-2122-482c-8443-056d4f7254d8",
        "parentId" : null,
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "why do we need to do calculation here? I think it introduces extra cost.",
        "createdAt" : "2020-01-31T04:43:42Z",
        "updatedAt" : "2020-01-31T15:57:27Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "64f1dbd5-10dc-4e7c-a9e2-8c415cc7692f",
        "parentId" : "af4c0e3a-2122-482c-8443-056d4f7254d8",
        "authorId" : "f06b291a-bc5c-4fd8-9bac-e5a6d035891b",
        "body" : "you mean just leave the tableMeta unchanged (which is table level meta without partition pruning) if there is at least one partition whose size is not available in meta store ?",
        "createdAt" : "2020-01-31T04:55:24Z",
        "updatedAt" : "2020-01-31T15:57:27Z",
        "lastEditedBy" : "f06b291a-bc5c-4fd8-9bac-e5a6d035891b",
        "tags" : [
        ]
      },
      {
        "id" : "823402a3-f052-436d-b67e-5f111f9bcbdd",
        "parentId" : "af4c0e3a-2122-482c-8443-056d4f7254d8",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "yes",
        "createdAt" : "2020-01-31T06:29:28Z",
        "updatedAt" : "2020-01-31T15:57:27Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "807e4094-99b3-48ee-b5c4-fdb878477c45",
        "parentId" : "af4c0e3a-2122-482c-8443-056d4f7254d8",
        "authorId" : "f06b291a-bc5c-4fd8-9bac-e5a6d035891b",
        "body" : "some discussion about this was in https://github.com/apache/spark/pull/26805#discussion_r366133849",
        "createdAt" : "2020-01-31T11:57:51Z",
        "updatedAt" : "2020-01-31T15:57:27Z",
        "lastEditedBy" : "f06b291a-bc5c-4fd8-9bac-e5a6d035891b",
        "tags" : [
        ]
      }
    ],
    "commit" : "8b14ce4435dfa6fb5114385bb122ec9a86ca7213",
    "line" : 32,
    "diffHunk" : "@@ -1,1 +89,93 @@      val sizeInBytes = partitionsWithSize.map(_._2).sum\n      tableMeta.copy(stats = Some(CatalogStatistics(sizeInBytes = BigInt(sizeInBytes))))\n    } else if (partitionsWithSize.count(_._2 == 0) <= conf.maxPartNumForStatsCalculateViaFS) {\n      val sizeInBytes =\n        partitionsWithSize.map(pair => {"
  },
  {
    "id" : "7113d88e-9b68-4721-89aa-098f059bd25c",
    "prId" : 27129,
    "prUrl" : "https://github.com/apache/spark/pull/27129#pullrequestreview-360103556",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2e85aa7c-a2d4-497e-8e13-1a2009b48df0",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@fuwhu, are you're proposing a configuration to automatically calculate the size? why don't you just manually run analyze comment to calculate the stats? It's weird to do this based on the number of partitions.",
        "createdAt" : "2020-02-18T06:55:06Z",
        "updatedAt" : "2020-02-18T06:55:06Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "8b14ce4435dfa6fb5114385bb122ec9a86ca7213",
    "line" : 32,
    "diffHunk" : "@@ -1,1 +89,93 @@      val sizeInBytes = partitionsWithSize.map(_._2).sum\n      tableMeta.copy(stats = Some(CatalogStatistics(sizeInBytes = BigInt(sizeInBytes))))\n    } else if (partitionsWithSize.count(_._2 == 0) <= conf.maxPartNumForStatsCalculateViaFS) {\n      val sizeInBytes =\n        partitionsWithSize.map(pair => {"
  }
]