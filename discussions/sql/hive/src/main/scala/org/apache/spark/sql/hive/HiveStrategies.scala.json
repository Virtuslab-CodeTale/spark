[
  {
    "id" : "cca9581e-b3d8-49a2-aefa-f3629d608024",
    "prId" : 30097,
    "prUrl" : "https://github.com/apache/spark/pull/30097#pullrequestreview-528863912",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "67dd34d3-80b1-4908-88cb-076740fba098",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "ditto, this access the catalog: `val dbName = t.identifier.database.getOrElse(session.catalog.currentDatabase)`",
        "createdAt" : "2020-11-12T08:46:34Z",
        "updatedAt" : "2020-11-12T08:46:34Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "7612695c78456155a95ad4f7d54ef70e53f88921",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +42,46 @@ * properties.\n */\nobject ResolveHiveSerdeTable extends Rule[LogicalPlan] {\n  private def determineHiveSerde(table: CatalogTable): CatalogTable = {\n    if (table.storage.serde.nonEmpty) {"
  },
  {
    "id" : "65549edc-4259-405d-b49e-d56597449175",
    "prId" : 29831,
    "prUrl" : "https://github.com/apache/spark/pull/29831#pullrequestreview-506401362",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1e6cc07a-9008-439e-83d8-56621ed788db",
        "parentId" : null,
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Also curious why we don't check if a filter contains subquery expression - we used to check it in the actual partition filtering (e.g., in `PruneFileSourcePartitions` and `PruneHiveTablePartitions`).",
        "createdAt" : "2020-10-09T23:47:13Z",
        "updatedAt" : "2020-10-19T04:41:28Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "c81bf62b-c01e-42c2-9c99-ac50807c38cd",
        "parentId" : "1e6cc07a-9008-439e-83d8-56621ed788db",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "`PruneFileSourcePartitions` is still in the logical phase, and subquery is not planned yet, so we must filter subquery out as we can't execute it.",
        "createdAt" : "2020-10-12T08:28:28Z",
        "updatedAt" : "2020-10-19T04:41:28Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "8ac7e5aba2f0e75cb7717eeb561903c9acda9830",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +262,266 @@        val partitionKeyIds = AttributeSet(relation.partitionCols)\n        val normalizedFilters = DataSourceStrategy.normalizeExprs(\n          filters.filter(_.deterministic), relation.output)\n\n        val partitionKeyFilters = DataSourceStrategy.getPushedDownFilters(relation.partitionCols,"
  },
  {
    "id" : "09e6cfe9-f86f-44b9-875e-cd81113189b7",
    "prId" : 29831,
    "prUrl" : "https://github.com/apache/spark/pull/29831#pullrequestreview-507989772",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a15d3a9b-409e-4743-853a-5c56a3ffeca0",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "`f.references.isEmpty` means it's true or false literal?",
        "createdAt" : "2020-10-13T09:23:39Z",
        "updatedAt" : "2020-10-19T04:41:28Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "ec37816c-1877-445f-addf-ae56a612e142",
        "parentId" : "a15d3a9b-409e-4743-853a-5c56a3ffeca0",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "or a scalar subquery?",
        "createdAt" : "2020-10-13T09:24:06Z",
        "updatedAt" : "2020-10-19T04:41:28Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "24e6a408-cb8e-4395-a22c-c43823e0c74a",
        "parentId" : "a15d3a9b-409e-4743-853a-5c56a3ffeca0",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "In `FileSourceStrategy`, scalar subquery is put in partition filters, not data filters, shall we follow it?",
        "createdAt" : "2020-10-13T09:25:07Z",
        "updatedAt" : "2020-10-19T04:41:28Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "aa399586-7b1a-41ce-9bd5-a5ba09bad48b",
        "parentId" : "a15d3a9b-409e-4743-853a-5c56a3ffeca0",
        "authorId" : "a6465a0a-3afb-45b9-b60f-b4381e348b5a",
        "body" : "I am actually just following the original behavior of HiveStrategies,\r\n\r\nregarding scalar subquery, is it in partition filter , I missed something in `FileSourceStrategy `?",
        "createdAt" : "2020-10-13T23:04:10Z",
        "updatedAt" : "2020-10-19T04:41:28Z",
        "lastEditedBy" : "a6465a0a-3afb-45b9-b60f-b4381e348b5a",
        "tags" : [
        ]
      },
      {
        "id" : "b23e139e-dec7-48c8-ba8b-7b073805457b",
        "parentId" : "a15d3a9b-409e-4743-853a-5c56a3ffeca0",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "It's OK for now to be the same as before, but it's better to make it consistent with `FileSourceStrategy`. We can do it in followup.",
        "createdAt" : "2020-10-14T04:41:38Z",
        "updatedAt" : "2020-10-19T04:41:28Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "8ac7e5aba2f0e75cb7717eeb561903c9acda9830",
    "line" : 31,
    "diffHunk" : "@@ -1,1 +269,273 @@        pruneFilterProject(\n          projectList,\n          filters.filter(f => f.references.isEmpty || !f.references.subsetOf(partitionKeyIds)),\n          identity[Seq[Expression]],\n          HiveTableScanExec(_, relation, partitionKeyFilters.toSeq)(sparkSession)) :: Nil"
  },
  {
    "id" : "17bde5c5-985a-478e-83da-fe1c3c1510f7",
    "prId" : 27055,
    "prUrl" : "https://github.com/apache/spark/pull/27055#pullrequestreview-354500942",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5fa98067-2b88-480a-be8e-e60f5c50f744",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "I think we just need this change that skips `DetermineTableStats when Hive will be converted later.",
        "createdAt" : "2020-02-03T19:31:45Z",
        "updatedAt" : "2020-02-03T19:31:45Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "1bfddf8c-cf98-40d1-84e4-0b044e61b736",
        "parentId" : "5fa98067-2b88-480a-be8e-e60f5c50f744",
        "authorId" : "2e5e2a86-4faf-484a-9251-577e584e1564",
        "body" : "@viirya I agree that doing size estimation on demand and disregarding catalog statistics is expensive, what I actually want to do is skip the `fallBackToHdfs` code path for datasource tables and do size estimation in `HadoopFsRelation`. \r\nMaybe we should also add a config similar to `fallBackToHdfs` in `HadoopFsRelation`? we just do real scans when the config is true. Otherwise, we just use the stats in CatalogTable to compute the `sizeInBytes`.\r\nWhy I think we should move this size estimation to `HadoopFsRelation` for datasource table is that we can do better estimation for datasource table like parquet. see  [SPARK-30712](https://issues.apache.org/jira/browse/SPARK-30712)",
        "createdAt" : "2020-02-06T14:41:56Z",
        "updatedAt" : "2020-02-06T14:41:57Z",
        "lastEditedBy" : "2e5e2a86-4faf-484a-9251-577e584e1564",
        "tags" : [
        ]
      }
    ],
    "commit" : "c5297d48fce32b584718c04b85dd06a293bc2c2b",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +141,145 @@    case relation: HiveTableRelation\n      if DDLUtils.isHiveTable(relation.tableMeta) && relation.tableMeta.stats.isEmpty &&\n        !RelationConversions.isConvertible(relation) =>\n      hiveTableWithStats(relation)\n"
  },
  {
    "id" : "258f396a-04dc-4b65-a5d7-2587e2e6fc8d",
    "prId" : 25919,
    "prUrl" : "https://github.com/apache/spark/pull/25919#pullrequestreview-294676175",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "695fc7f3-49d4-4b30-b150-dbc412e17369",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "so we list partitions here just to get statistic, and we will list partitions again at runtime?",
        "createdAt" : "2019-09-24T16:42:33Z",
        "updatedAt" : "2019-10-26T04:14:03Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "d55538e2-2de8-4032-a642-6784820336bd",
        "parentId" : "695fc7f3-49d4-4b30-b150-dbc412e17369",
        "authorId" : "8e87861a-4202-49a2-baf7-6d51f6aaa5a2",
        "body" : "Currently, yes. And I didn't find an easy way to avoid the list partitions operation.",
        "createdAt" : "2019-09-25T02:23:57Z",
        "updatedAt" : "2019-10-26T04:14:03Z",
        "lastEditedBy" : "8e87861a-4202-49a2-baf7-6d51f6aaa5a2",
        "tags" : [
        ]
      },
      {
        "id" : "3c869164-89df-4cdd-ac37-b9fda5610327",
        "parentId" : "695fc7f3-49d4-4b30-b150-dbc412e17369",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Then this may make the performance worse. How about we keep the listed partitions in `HiveTableRelation`?\r\n\r\n",
        "createdAt" : "2019-09-26T06:08:20Z",
        "updatedAt" : "2019-10-26T04:14:03Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "d7c9050a-7a2d-483e-99a7-b3b36b5aaa4e",
        "parentId" : "695fc7f3-49d4-4b30-b150-dbc412e17369",
        "authorId" : "8e87861a-4202-49a2-baf7-6d51f6aaa5a2",
        "body" : "> How about we keep the listed partitions in HiveTableRelation?\r\n\r\nThis is a good one. However, we may have to add two fields: `pruningFilters: Seq[Expression]` and `prunedPartitions: Seq[CatalogTablePartition]`, and I believe they are complicating the `HiveTableRelation`. Another things is that `HiveTableRelation` might be copied multiple times, we may lost the `prunedPartitions` field by accident if not copied carefully. \r\n\r\nIf that's not the problem, storing listed partitions in `HiveTableRelation` is a good choice. \r\nWDYT @cloud-fan?",
        "createdAt" : "2019-09-29T09:44:50Z",
        "updatedAt" : "2019-10-26T04:14:03Z",
        "lastEditedBy" : "8e87861a-4202-49a2-baf7-6d51f6aaa5a2",
        "tags" : [
        ]
      }
    ],
    "commit" : "ecfbe4d975aeee1da38240753c99a6927a8aa690",
    "line" : 45,
    "diffHunk" : "@@ -1,1 +255,259 @@      val conf = session.sessionState.conf\n      if (conf.metastorePartitionPruning && pruningPredicates.nonEmpty && !hasScalarSubquery) {\n        val prunedPartitions = session.sharedState.externalCatalog.listPartitionsByFilter(\n          relation.tableMeta.database,\n          relation.tableMeta.identifier.table,"
  },
  {
    "id" : "4d39ed02-c85e-4989-b54e-083cd1d842a2",
    "prId" : 25919,
    "prUrl" : "https://github.com/apache/spark/pull/25919#pullrequestreview-325043885",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ad900b90-cb75-4ae1-9bd2-dddf519a3ef1",
        "parentId" : null,
        "authorId" : "f06b291a-bc5c-4fd8-9bac-e5a6d035891b",
        "body" : "It skips all subqueries instead of scalar subqueries.",
        "createdAt" : "2019-12-02T06:59:11Z",
        "updatedAt" : "2019-12-02T09:12:27Z",
        "lastEditedBy" : "f06b291a-bc5c-4fd8-9bac-e5a6d035891b",
        "tags" : [
        ]
      }
    ],
    "commit" : "ecfbe4d975aeee1da38240753c99a6927a8aa690",
    "line" : 42,
    "diffHunk" : "@@ -1,1 +252,256 @@      }\n      // SPARK-24085: scalar subquery should be skipped for partition pruning\n      val hasScalarSubquery = pruningPredicates.exists(SubqueryExpression.hasSubquery)\n      val conf = session.sessionState.conf\n      if (conf.metastorePartitionPruning && pruningPredicates.nonEmpty && !hasScalarSubquery) {"
  },
  {
    "id" : "fe3f577d-60d9-4e35-af03-51eceabbbfe3",
    "prId" : 25919,
    "prUrl" : "https://github.com/apache/spark/pull/25919#pullrequestreview-325068410",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9ab90a4e-6506-4350-a7ec-bfb92e0b7e73",
        "parentId" : null,
        "authorId" : "f06b291a-bc5c-4fd8-9bac-e5a6d035891b",
        "body" : "Per the doc of the conf \"spark.sql.statistics.fallBackToHdfs\", it is only for non-partitioned hive table : \r\n\"This flag is effective only for non-partitioned Hive tables.\"",
        "createdAt" : "2019-12-02T08:13:11Z",
        "updatedAt" : "2019-12-02T08:13:11Z",
        "lastEditedBy" : "f06b291a-bc5c-4fd8-9bac-e5a6d035891b",
        "tags" : [
        ]
      }
    ],
    "commit" : "ecfbe4d975aeee1da38240753c99a6927a8aa690",
    "line" : 58,
    "diffHunk" : "@@ -1,1 +268,272 @@            } else if (totalSize.isDefined && totalSize.get > 0L) {\n              totalSize.get\n            } else if (conf.fallBackToHdfsForStatsEnabled) {\n              CommandUtils.calculateLocationSize(\n                session.sessionState, relation.tableMeta.identifier, part.storage.locationUri)"
  },
  {
    "id" : "26e0aee8-1c39-49bb-a9a5-2c4dca129cc9",
    "prId" : 25584,
    "prUrl" : "https://github.com/apache/spark/pull/25584#pullrequestreview-279928227",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cdaa72f1-fbc2-48ee-9aed-ea07df12b8e5",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "can we add some comment to explain why we should skip partitioned tables?",
        "createdAt" : "2019-08-27T01:56:46Z",
        "updatedAt" : "2019-08-27T07:51:42Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "89e10970-2b30-4350-bae0-0472d6317a0b",
        "parentId" : "cdaa72f1-fbc2-48ee-9aed-ea07df12b8e5",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "Done",
        "createdAt" : "2019-08-27T02:36:31Z",
        "updatedAt" : "2019-08-27T07:51:42Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      }
    ],
    "commit" : "cfcd0aafedbcaede70f072c7ba156ce6465942b0",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +120,124 @@      // For partitioned tables, the partition directory may be outside of the table directory.\n      // Which is expensive to get table size. Please see how we implemented it in the AnalyzeTable.\n      val sizeInBytes = if (conf.fallBackToHdfsForStatsEnabled && partitionCols.isEmpty) {\n        try {\n          val hadoopConf = session.sessionState.newHadoopConf()"
  }
]