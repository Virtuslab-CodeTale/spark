[
  {
    "id" : "cca9581e-b3d8-49a2-aefa-f3629d608024",
    "prId" : 30097,
    "prUrl" : "https://github.com/apache/spark/pull/30097#pullrequestreview-528863912",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "67dd34d3-80b1-4908-88cb-076740fba098",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "ditto, this access the catalog: `val dbName = t.identifier.database.getOrElse(session.catalog.currentDatabase)`",
        "createdAt" : "2020-11-12T08:46:34Z",
        "updatedAt" : "2020-11-12T08:46:34Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "7612695c78456155a95ad4f7d54ef70e53f88921",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +42,46 @@ * properties.\n */\nobject ResolveHiveSerdeTable extends Rule[LogicalPlan] {\n  private def determineHiveSerde(table: CatalogTable): CatalogTable = {\n    if (table.storage.serde.nonEmpty) {"
  },
  {
    "id" : "65549edc-4259-405d-b49e-d56597449175",
    "prId" : 29831,
    "prUrl" : "https://github.com/apache/spark/pull/29831#pullrequestreview-506401362",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1e6cc07a-9008-439e-83d8-56621ed788db",
        "parentId" : null,
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Also curious why we don't check if a filter contains subquery expression - we used to check it in the actual partition filtering (e.g., in `PruneFileSourcePartitions` and `PruneHiveTablePartitions`).",
        "createdAt" : "2020-10-09T23:47:13Z",
        "updatedAt" : "2020-10-19T04:41:28Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "c81bf62b-c01e-42c2-9c99-ac50807c38cd",
        "parentId" : "1e6cc07a-9008-439e-83d8-56621ed788db",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "`PruneFileSourcePartitions` is still in the logical phase, and subquery is not planned yet, so we must filter subquery out as we can't execute it.",
        "createdAt" : "2020-10-12T08:28:28Z",
        "updatedAt" : "2020-10-19T04:41:28Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "8ac7e5aba2f0e75cb7717eeb561903c9acda9830",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +262,266 @@        val partitionKeyIds = AttributeSet(relation.partitionCols)\n        val normalizedFilters = DataSourceStrategy.normalizeExprs(\n          filters.filter(_.deterministic), relation.output)\n\n        val partitionKeyFilters = DataSourceStrategy.getPushedDownFilters(relation.partitionCols,"
  },
  {
    "id" : "09e6cfe9-f86f-44b9-875e-cd81113189b7",
    "prId" : 29831,
    "prUrl" : "https://github.com/apache/spark/pull/29831#pullrequestreview-507989772",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a15d3a9b-409e-4743-853a-5c56a3ffeca0",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "`f.references.isEmpty` means it's true or false literal?",
        "createdAt" : "2020-10-13T09:23:39Z",
        "updatedAt" : "2020-10-19T04:41:28Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "ec37816c-1877-445f-addf-ae56a612e142",
        "parentId" : "a15d3a9b-409e-4743-853a-5c56a3ffeca0",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "or a scalar subquery?",
        "createdAt" : "2020-10-13T09:24:06Z",
        "updatedAt" : "2020-10-19T04:41:28Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "24e6a408-cb8e-4395-a22c-c43823e0c74a",
        "parentId" : "a15d3a9b-409e-4743-853a-5c56a3ffeca0",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "In `FileSourceStrategy`, scalar subquery is put in partition filters, not data filters, shall we follow it?",
        "createdAt" : "2020-10-13T09:25:07Z",
        "updatedAt" : "2020-10-19T04:41:28Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "aa399586-7b1a-41ce-9bd5-a5ba09bad48b",
        "parentId" : "a15d3a9b-409e-4743-853a-5c56a3ffeca0",
        "authorId" : "a6465a0a-3afb-45b9-b60f-b4381e348b5a",
        "body" : "I am actually just following the original behavior of HiveStrategies,\r\n\r\nregarding scalar subquery, is it in partition filter , I missed something in `FileSourceStrategy `?",
        "createdAt" : "2020-10-13T23:04:10Z",
        "updatedAt" : "2020-10-19T04:41:28Z",
        "lastEditedBy" : "a6465a0a-3afb-45b9-b60f-b4381e348b5a",
        "tags" : [
        ]
      },
      {
        "id" : "b23e139e-dec7-48c8-ba8b-7b073805457b",
        "parentId" : "a15d3a9b-409e-4743-853a-5c56a3ffeca0",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "It's OK for now to be the same as before, but it's better to make it consistent with `FileSourceStrategy`. We can do it in followup.",
        "createdAt" : "2020-10-14T04:41:38Z",
        "updatedAt" : "2020-10-19T04:41:28Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "8ac7e5aba2f0e75cb7717eeb561903c9acda9830",
    "line" : 31,
    "diffHunk" : "@@ -1,1 +269,273 @@        pruneFilterProject(\n          projectList,\n          filters.filter(f => f.references.isEmpty || !f.references.subsetOf(partitionKeyIds)),\n          identity[Seq[Expression]],\n          HiveTableScanExec(_, relation, partitionKeyFilters.toSeq)(sparkSession)) :: Nil"
  },
  {
    "id" : "17bde5c5-985a-478e-83da-fe1c3c1510f7",
    "prId" : 27055,
    "prUrl" : "https://github.com/apache/spark/pull/27055#pullrequestreview-354500942",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5fa98067-2b88-480a-be8e-e60f5c50f744",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "I think we just need this change that skips `DetermineTableStats when Hive will be converted later.",
        "createdAt" : "2020-02-03T19:31:45Z",
        "updatedAt" : "2020-02-03T19:31:45Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "1bfddf8c-cf98-40d1-84e4-0b044e61b736",
        "parentId" : "5fa98067-2b88-480a-be8e-e60f5c50f744",
        "authorId" : "2e5e2a86-4faf-484a-9251-577e584e1564",
        "body" : "@viirya I agree that doing size estimation on demand and disregarding catalog statistics is expensive, what I actually want to do is skip the `fallBackToHdfs` code path for datasource tables and do size estimation in `HadoopFsRelation`. \r\nMaybe we should also add a config similar to `fallBackToHdfs` in `HadoopFsRelation`? we just do real scans when the config is true. Otherwise, we just use the stats in CatalogTable to compute the `sizeInBytes`.\r\nWhy I think we should move this size estimation to `HadoopFsRelation` for datasource table is that we can do better estimation for datasource table like parquet. see  [SPARK-30712](https://issues.apache.org/jira/browse/SPARK-30712)",
        "createdAt" : "2020-02-06T14:41:56Z",
        "updatedAt" : "2020-02-06T14:41:57Z",
        "lastEditedBy" : "2e5e2a86-4faf-484a-9251-577e584e1564",
        "tags" : [
        ]
      }
    ],
    "commit" : "c5297d48fce32b584718c04b85dd06a293bc2c2b",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +141,145 @@    case relation: HiveTableRelation\n      if DDLUtils.isHiveTable(relation.tableMeta) && relation.tableMeta.stats.isEmpty &&\n        !RelationConversions.isConvertible(relation) =>\n      hiveTableWithStats(relation)\n"
  }
]