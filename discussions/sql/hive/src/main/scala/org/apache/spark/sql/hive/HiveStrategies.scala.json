[
  {
    "id" : "cca9581e-b3d8-49a2-aefa-f3629d608024",
    "prId" : 30097,
    "prUrl" : "https://github.com/apache/spark/pull/30097#pullrequestreview-528863912",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "67dd34d3-80b1-4908-88cb-076740fba098",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "ditto, this access the catalog: `val dbName = t.identifier.database.getOrElse(session.catalog.currentDatabase)`",
        "createdAt" : "2020-11-12T08:46:34Z",
        "updatedAt" : "2020-11-12T08:46:34Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "7612695c78456155a95ad4f7d54ef70e53f88921",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +42,46 @@ * properties.\n */\nobject ResolveHiveSerdeTable extends Rule[LogicalPlan] {\n  private def determineHiveSerde(table: CatalogTable): CatalogTable = {\n    if (table.storage.serde.nonEmpty) {"
  },
  {
    "id" : "65549edc-4259-405d-b49e-d56597449175",
    "prId" : 29831,
    "prUrl" : "https://github.com/apache/spark/pull/29831#pullrequestreview-506401362",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1e6cc07a-9008-439e-83d8-56621ed788db",
        "parentId" : null,
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Also curious why we don't check if a filter contains subquery expression - we used to check it in the actual partition filtering (e.g., in `PruneFileSourcePartitions` and `PruneHiveTablePartitions`).",
        "createdAt" : "2020-10-09T23:47:13Z",
        "updatedAt" : "2020-10-19T04:41:28Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "c81bf62b-c01e-42c2-9c99-ac50807c38cd",
        "parentId" : "1e6cc07a-9008-439e-83d8-56621ed788db",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "`PruneFileSourcePartitions` is still in the logical phase, and subquery is not planned yet, so we must filter subquery out as we can't execute it.",
        "createdAt" : "2020-10-12T08:28:28Z",
        "updatedAt" : "2020-10-19T04:41:28Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "8ac7e5aba2f0e75cb7717eeb561903c9acda9830",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +262,266 @@        val partitionKeyIds = AttributeSet(relation.partitionCols)\n        val normalizedFilters = DataSourceStrategy.normalizeExprs(\n          filters.filter(_.deterministic), relation.output)\n\n        val partitionKeyFilters = DataSourceStrategy.getPushedDownFilters(relation.partitionCols,"
  },
  {
    "id" : "09e6cfe9-f86f-44b9-875e-cd81113189b7",
    "prId" : 29831,
    "prUrl" : "https://github.com/apache/spark/pull/29831#pullrequestreview-507989772",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a15d3a9b-409e-4743-853a-5c56a3ffeca0",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "`f.references.isEmpty` means it's true or false literal?",
        "createdAt" : "2020-10-13T09:23:39Z",
        "updatedAt" : "2020-10-19T04:41:28Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "ec37816c-1877-445f-addf-ae56a612e142",
        "parentId" : "a15d3a9b-409e-4743-853a-5c56a3ffeca0",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "or a scalar subquery?",
        "createdAt" : "2020-10-13T09:24:06Z",
        "updatedAt" : "2020-10-19T04:41:28Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "24e6a408-cb8e-4395-a22c-c43823e0c74a",
        "parentId" : "a15d3a9b-409e-4743-853a-5c56a3ffeca0",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "In `FileSourceStrategy`, scalar subquery is put in partition filters, not data filters, shall we follow it?",
        "createdAt" : "2020-10-13T09:25:07Z",
        "updatedAt" : "2020-10-19T04:41:28Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "aa399586-7b1a-41ce-9bd5-a5ba09bad48b",
        "parentId" : "a15d3a9b-409e-4743-853a-5c56a3ffeca0",
        "authorId" : "a6465a0a-3afb-45b9-b60f-b4381e348b5a",
        "body" : "I am actually just following the original behavior of HiveStrategies,\r\n\r\nregarding scalar subquery, is it in partition filter , I missed something in `FileSourceStrategy `?",
        "createdAt" : "2020-10-13T23:04:10Z",
        "updatedAt" : "2020-10-19T04:41:28Z",
        "lastEditedBy" : "a6465a0a-3afb-45b9-b60f-b4381e348b5a",
        "tags" : [
        ]
      },
      {
        "id" : "b23e139e-dec7-48c8-ba8b-7b073805457b",
        "parentId" : "a15d3a9b-409e-4743-853a-5c56a3ffeca0",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "It's OK for now to be the same as before, but it's better to make it consistent with `FileSourceStrategy`. We can do it in followup.",
        "createdAt" : "2020-10-14T04:41:38Z",
        "updatedAt" : "2020-10-19T04:41:28Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "8ac7e5aba2f0e75cb7717eeb561903c9acda9830",
    "line" : 31,
    "diffHunk" : "@@ -1,1 +269,273 @@        pruneFilterProject(\n          projectList,\n          filters.filter(f => f.references.isEmpty || !f.references.subsetOf(partitionKeyIds)),\n          identity[Seq[Expression]],\n          HiveTableScanExec(_, relation, partitionKeyFilters.toSeq)(sparkSession)) :: Nil"
  },
  {
    "id" : "17bde5c5-985a-478e-83da-fe1c3c1510f7",
    "prId" : 27055,
    "prUrl" : "https://github.com/apache/spark/pull/27055#pullrequestreview-354500942",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5fa98067-2b88-480a-be8e-e60f5c50f744",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "I think we just need this change that skips `DetermineTableStats when Hive will be converted later.",
        "createdAt" : "2020-02-03T19:31:45Z",
        "updatedAt" : "2020-02-03T19:31:45Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "1bfddf8c-cf98-40d1-84e4-0b044e61b736",
        "parentId" : "5fa98067-2b88-480a-be8e-e60f5c50f744",
        "authorId" : "2e5e2a86-4faf-484a-9251-577e584e1564",
        "body" : "@viirya I agree that doing size estimation on demand and disregarding catalog statistics is expensive, what I actually want to do is skip the `fallBackToHdfs` code path for datasource tables and do size estimation in `HadoopFsRelation`. \r\nMaybe we should also add a config similar to `fallBackToHdfs` in `HadoopFsRelation`? we just do real scans when the config is true. Otherwise, we just use the stats in CatalogTable to compute the `sizeInBytes`.\r\nWhy I think we should move this size estimation to `HadoopFsRelation` for datasource table is that we can do better estimation for datasource table like parquet. see  [SPARK-30712](https://issues.apache.org/jira/browse/SPARK-30712)",
        "createdAt" : "2020-02-06T14:41:56Z",
        "updatedAt" : "2020-02-06T14:41:57Z",
        "lastEditedBy" : "2e5e2a86-4faf-484a-9251-577e584e1564",
        "tags" : [
        ]
      }
    ],
    "commit" : "c5297d48fce32b584718c04b85dd06a293bc2c2b",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +141,145 @@    case relation: HiveTableRelation\n      if DDLUtils.isHiveTable(relation.tableMeta) && relation.tableMeta.stats.isEmpty &&\n        !RelationConversions.isConvertible(relation) =>\n      hiveTableWithStats(relation)\n"
  },
  {
    "id" : "258f396a-04dc-4b65-a5d7-2587e2e6fc8d",
    "prId" : 25919,
    "prUrl" : "https://github.com/apache/spark/pull/25919#pullrequestreview-294676175",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "695fc7f3-49d4-4b30-b150-dbc412e17369",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "so we list partitions here just to get statistic, and we will list partitions again at runtime?",
        "createdAt" : "2019-09-24T16:42:33Z",
        "updatedAt" : "2019-10-26T04:14:03Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "d55538e2-2de8-4032-a642-6784820336bd",
        "parentId" : "695fc7f3-49d4-4b30-b150-dbc412e17369",
        "authorId" : "8e87861a-4202-49a2-baf7-6d51f6aaa5a2",
        "body" : "Currently, yes. And I didn't find an easy way to avoid the list partitions operation.",
        "createdAt" : "2019-09-25T02:23:57Z",
        "updatedAt" : "2019-10-26T04:14:03Z",
        "lastEditedBy" : "8e87861a-4202-49a2-baf7-6d51f6aaa5a2",
        "tags" : [
        ]
      },
      {
        "id" : "3c869164-89df-4cdd-ac37-b9fda5610327",
        "parentId" : "695fc7f3-49d4-4b30-b150-dbc412e17369",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Then this may make the performance worse. How about we keep the listed partitions in `HiveTableRelation`?\r\n\r\n",
        "createdAt" : "2019-09-26T06:08:20Z",
        "updatedAt" : "2019-10-26T04:14:03Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "d7c9050a-7a2d-483e-99a7-b3b36b5aaa4e",
        "parentId" : "695fc7f3-49d4-4b30-b150-dbc412e17369",
        "authorId" : "8e87861a-4202-49a2-baf7-6d51f6aaa5a2",
        "body" : "> How about we keep the listed partitions in HiveTableRelation?\r\n\r\nThis is a good one. However, we may have to add two fields: `pruningFilters: Seq[Expression]` and `prunedPartitions: Seq[CatalogTablePartition]`, and I believe they are complicating the `HiveTableRelation`. Another things is that `HiveTableRelation` might be copied multiple times, we may lost the `prunedPartitions` field by accident if not copied carefully. \r\n\r\nIf that's not the problem, storing listed partitions in `HiveTableRelation` is a good choice. \r\nWDYT @cloud-fan?",
        "createdAt" : "2019-09-29T09:44:50Z",
        "updatedAt" : "2019-10-26T04:14:03Z",
        "lastEditedBy" : "8e87861a-4202-49a2-baf7-6d51f6aaa5a2",
        "tags" : [
        ]
      }
    ],
    "commit" : "ecfbe4d975aeee1da38240753c99a6927a8aa690",
    "line" : 45,
    "diffHunk" : "@@ -1,1 +255,259 @@      val conf = session.sessionState.conf\n      if (conf.metastorePartitionPruning && pruningPredicates.nonEmpty && !hasScalarSubquery) {\n        val prunedPartitions = session.sharedState.externalCatalog.listPartitionsByFilter(\n          relation.tableMeta.database,\n          relation.tableMeta.identifier.table,"
  },
  {
    "id" : "4d39ed02-c85e-4989-b54e-083cd1d842a2",
    "prId" : 25919,
    "prUrl" : "https://github.com/apache/spark/pull/25919#pullrequestreview-325043885",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ad900b90-cb75-4ae1-9bd2-dddf519a3ef1",
        "parentId" : null,
        "authorId" : "f06b291a-bc5c-4fd8-9bac-e5a6d035891b",
        "body" : "It skips all subqueries instead of scalar subqueries.",
        "createdAt" : "2019-12-02T06:59:11Z",
        "updatedAt" : "2019-12-02T09:12:27Z",
        "lastEditedBy" : "f06b291a-bc5c-4fd8-9bac-e5a6d035891b",
        "tags" : [
        ]
      }
    ],
    "commit" : "ecfbe4d975aeee1da38240753c99a6927a8aa690",
    "line" : 42,
    "diffHunk" : "@@ -1,1 +252,256 @@      }\n      // SPARK-24085: scalar subquery should be skipped for partition pruning\n      val hasScalarSubquery = pruningPredicates.exists(SubqueryExpression.hasSubquery)\n      val conf = session.sessionState.conf\n      if (conf.metastorePartitionPruning && pruningPredicates.nonEmpty && !hasScalarSubquery) {"
  },
  {
    "id" : "fe3f577d-60d9-4e35-af03-51eceabbbfe3",
    "prId" : 25919,
    "prUrl" : "https://github.com/apache/spark/pull/25919#pullrequestreview-325068410",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9ab90a4e-6506-4350-a7ec-bfb92e0b7e73",
        "parentId" : null,
        "authorId" : "f06b291a-bc5c-4fd8-9bac-e5a6d035891b",
        "body" : "Per the doc of the conf \"spark.sql.statistics.fallBackToHdfs\", it is only for non-partitioned hive table : \r\n\"This flag is effective only for non-partitioned Hive tables.\"",
        "createdAt" : "2019-12-02T08:13:11Z",
        "updatedAt" : "2019-12-02T08:13:11Z",
        "lastEditedBy" : "f06b291a-bc5c-4fd8-9bac-e5a6d035891b",
        "tags" : [
        ]
      }
    ],
    "commit" : "ecfbe4d975aeee1da38240753c99a6927a8aa690",
    "line" : 58,
    "diffHunk" : "@@ -1,1 +268,272 @@            } else if (totalSize.isDefined && totalSize.get > 0L) {\n              totalSize.get\n            } else if (conf.fallBackToHdfsForStatsEnabled) {\n              CommandUtils.calculateLocationSize(\n                session.sessionState, relation.tableMeta.identifier, part.storage.locationUri)"
  },
  {
    "id" : "26e0aee8-1c39-49bb-a9a5-2c4dca129cc9",
    "prId" : 25584,
    "prUrl" : "https://github.com/apache/spark/pull/25584#pullrequestreview-279928227",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cdaa72f1-fbc2-48ee-9aed-ea07df12b8e5",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "can we add some comment to explain why we should skip partitioned tables?",
        "createdAt" : "2019-08-27T01:56:46Z",
        "updatedAt" : "2019-08-27T07:51:42Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "89e10970-2b30-4350-bae0-0472d6317a0b",
        "parentId" : "cdaa72f1-fbc2-48ee-9aed-ea07df12b8e5",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "Done",
        "createdAt" : "2019-08-27T02:36:31Z",
        "updatedAt" : "2019-08-27T07:51:42Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      }
    ],
    "commit" : "cfcd0aafedbcaede70f072c7ba156ce6465942b0",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +120,124 @@      // For partitioned tables, the partition directory may be outside of the table directory.\n      // Which is expensive to get table size. Please see how we implemented it in the AnalyzeTable.\n      val sizeInBytes = if (conf.fallBackToHdfsForStatsEnabled && partitionCols.isEmpty) {\n        try {\n          val hadoopConf = session.sessionState.newHadoopConf()"
  },
  {
    "id" : "ba8b878e-e80f-445c-9bf2-0a6038c60e0e",
    "prId" : 24741,
    "prUrl" : "https://github.com/apache/spark/pull/24741#pullrequestreview-246153659",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "63016f6f-a02b-4a6d-a4e5-e110a239c8d4",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "This sounds like a separate issue. Could we submit a separate PR? ",
        "createdAt" : "2019-06-04T22:01:14Z",
        "updatedAt" : "2019-06-13T01:41:28Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "03bccd6f-ea24-4666-8f78-0c784e164d80",
        "parentId" : "63016f6f-a02b-4a6d-a4e5-e110a239c8d4",
        "authorId" : "f5b3f57a-75a6-496c-af94-c32580ada13a",
        "body" : "This PR prevents lookupTableFromCatalog from throwing NoSuchTableException right away. Instead, it relies on checkAnalysis to throw an exception for UnresolvedRelation.\r\n\r\nThe test hive.SQLQuerySuite.\"double nested data\" would fail in the following sql without this change:\r\n```\r\nCREATE TABLE test_ctas_1234 AS SELECT * from notexists\r\n```\r\nHiveAnalysis gets to run before checkAnalysis, thus exposing this bug where query.output is used before query is resolved.\r\n\r\nSo wouldn't say it is a totally separate issue.\r\nIn addition, outside of this PR, it'd be hard to write a unit test.\r\n",
        "createdAt" : "2019-06-05T05:18:05Z",
        "updatedAt" : "2019-06-13T01:41:28Z",
        "lastEditedBy" : "f5b3f57a-75a6-496c-af94-c32580ada13a",
        "tags" : [
        ]
      },
      {
        "id" : "f4358e2c-c4b5-4e1d-9519-e8ffc8c04e08",
        "parentId" : "63016f6f-a02b-4a6d-a4e5-e110a239c8d4",
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "I agree with John. This is needed as a consequence of fixing the `ResolveRelations` rule to no longer throw `AnalysisException` if it can't resolve the name and doesn't think that `ResolveSQLOnFile` would either.",
        "createdAt" : "2019-06-05T17:34:35Z",
        "updatedAt" : "2019-06-13T01:41:28Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      }
    ],
    "commit" : "b8cdf6c22172585b3b3a9452d5e4d2d591ece88e",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +155,159 @@\n    case CreateTable(tableDesc, mode, Some(query))\n        if DDLUtils.isHiveTable(tableDesc) && query.resolved =>\n      CreateHiveTableAsSelectCommand(tableDesc, query, query.output.map(_.name), mode)\n"
  },
  {
    "id" : "7814e2e5-d313-4bb1-97b0-7c93419b8ee8",
    "prId" : 24741,
    "prUrl" : "https://github.com/apache/spark/pull/24741#pullrequestreview-249070493",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7de02dc6-f38f-4c50-a628-99988e385f21",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "@jzhuge Like `CREATE`, we need `&& child.resolved` here for `INSERT OVERWRITE LOCAL DIRECTORY`. Please add the same test case accordingly.",
        "createdAt" : "2019-06-12T23:46:25Z",
        "updatedAt" : "2019-06-13T01:41:28Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "b8cdf6c22172585b3b3a9452d5e4d2d591ece88e",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +158,162 @@      CreateHiveTableAsSelectCommand(tableDesc, query, query.output.map(_.name), mode)\n\n    case InsertIntoDir(isLocal, storage, provider, child, overwrite)\n        if DDLUtils.isHiveTable(provider) && child.resolved =>\n      val outputPath = new Path(storage.locationUri.get)"
  },
  {
    "id" : "c49d45aa-7d03-4805-87f7-94c745851e3f",
    "prId" : 24712,
    "prUrl" : "https://github.com/apache/spark/pull/24712#pullrequestreview-242072663",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5237b4c6-085a-4f95-9353-2e60042d02da",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Looks if getting size is failed, previously, it used `defaultSizeInBytes` but now `0`. Can you check if it still behaves identically?",
        "createdAt" : "2019-05-27T02:09:40Z",
        "updatedAt" : "2019-05-27T02:09:40Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "0dc68b937ceebc79c855104270d3a5f46c96feb4",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +119,123 @@      val table = relation.tableMeta\n      val sizeInBytes = if (session.sessionState.conf.fallBackToHdfsForStatsEnabled) {\n        CommandUtils.calculateTotalSize(session, table)\n      } else {\n        BigInt(session.sessionState.conf.defaultSizeInBytes)"
  },
  {
    "id" : "0aa09270-96e0-4a45-b933-1c5a1763088c",
    "prId" : 24551,
    "prUrl" : "https://github.com/apache/spark/pull/24551#pullrequestreview-234874334",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d1e64f4c-78dc-426c-9109-3447a1fbc084",
        "parentId" : null,
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "Set `isJoin = false`: Do not wait for this thread to die.",
        "createdAt" : "2019-05-08T05:48:35Z",
        "updatedAt" : "2019-05-08T05:48:35Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      }
    ],
    "commit" : "a0afbf21a3efeda2a098e41fd18a74db71931beb",
    "line" : 25,
    "diffHunk" : "@@ -1,1 +129,133 @@          if (sessionState.conf.persistentStatsAfterFallBackEnabled) {\n            val threadName = s\"update-${table.identifier.unquotedString}-stats\"\n            ThreadUtils.runInNewThread(threadName, isJoin = false) {\n              val newStats = CatalogStatistics(sizeInBytes = newSize)\n              sessionState.catalog.alterTableStats(table.identifier, Some(newStats))"
  }
]