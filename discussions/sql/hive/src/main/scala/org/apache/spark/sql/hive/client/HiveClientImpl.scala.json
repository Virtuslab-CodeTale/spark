[
  {
    "id" : "4ace1b81-f4b7-4940-8b87-77476a05ccee",
    "prId" : 32446,
    "prUrl" : "https://github.com/apache/spark/pull/32446#pullrequestreview-654087281",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "802f825b-74cd-406b-8ae5-57e9d6418ff6",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Here, line 303 claims that we need `the side-effect of Hive.get(conf)`.\r\nCould you confirm that `shim.getHive(conf)` doesn't break the side-effect assumption?",
        "createdAt" : "2021-05-07T05:14:03Z",
        "updatedAt" : "2021-05-07T05:14:03Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "83f1ef44-b4c8-4187-bd1c-33dd93652c59",
        "parentId" : "802f825b-74cd-406b-8ae5-57e9d6418ff6",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "yeah it shouldn't - both function are doing the same in this case by updating the `Hive` object's config with provided `conf`.",
        "createdAt" : "2021-05-07T05:16:22Z",
        "updatedAt" : "2021-05-07T05:16:23Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      }
    ],
    "commit" : "88697a43ba63963a1951f8d99a697fab4ca5692f",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +304,308 @@    // See discussion in https://github.com/apache/spark/pull/16826/files#r104606859\n    // for more details.\n    shim.getHive(conf)\n    // setCurrentSessionState will use the classLoader associated\n    // with the HiveConf in `state` to override the context class loader of the current"
  },
  {
    "id" : "23d4bb52-1a1b-475a-89d3-162a7145e886",
    "prId" : 32410,
    "prUrl" : "https://github.com/apache/spark/pull/32410#pullrequestreview-679603176",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0c147e18-0a17-4b84-9ed9-b276612db912",
        "parentId" : null,
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "@HyukjinKwon @sunchao I changed it to `shim.setCurrentSessionState(state)` to make it simpler.",
        "createdAt" : "2021-05-14T06:05:24Z",
        "updatedAt" : "2021-05-14T06:05:24Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      },
      {
        "id" : "39068d0f-fa49-4f5b-b965-6fb1eb586a21",
        "parentId" : "0c147e18-0a17-4b84-9ed9-b276612db912",
        "authorId" : "2821208d-43c5-4475-94ad-36cbf84c14d8",
        "body" : "@wangyum I wonder if this is a safe change. And why do you think this is making it simpler? ",
        "createdAt" : "2021-06-07T19:58:50Z",
        "updatedAt" : "2021-06-07T19:58:51Z",
        "lastEditedBy" : "2821208d-43c5-4475-94ad-36cbf84c14d8",
        "tags" : [
        ]
      },
      {
        "id" : "ab32acde-9137-41ed-8227-bb7a892ec651",
        "parentId" : "0c147e18-0a17-4b84-9ed9-b276612db912",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "Sorry for late replay. It is two questions.\r\n\r\n> if this is a safe change\r\n\r\nYes. The difference is `sessionState.start` creating too many directories and Spark SQL do not need it.\r\n\r\n> why do you think this is making it simpler?\r\n\r\n`shim.setCurrentSessionState(state)` is simpler than `if (version != hive.v12) SessionState.setCurrentSessionState(state) else   SessionState.start(state) `.",
        "createdAt" : "2021-06-09T12:45:42Z",
        "updatedAt" : "2021-06-09T12:45:42Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      }
    ],
    "commit" : "4bca8ecaec066ef19d04a12e134ba830320a2e0f",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +191,195 @@    // got changed. We reset it to clientLoader.ClassLoader here.\n    state.getConf.setClassLoader(clientLoader.classLoader)\n    shim.setCurrentSessionState(state)\n    state.out = new PrintStream(outputBuffer, true, UTF_8.name())\n    state.err = new PrintStream(outputBuffer, true, UTF_8.name())"
  },
  {
    "id" : "74bb2218-3400-4d14-b8fe-a369175442bc",
    "prId" : 32410,
    "prUrl" : "https://github.com/apache/spark/pull/32410#pullrequestreview-682750212",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4c0257a7-d08d-48d5-8a5d-251174890531",
        "parentId" : null,
        "authorId" : "2821208d-43c5-4475-94ad-36cbf84c14d8",
        "body" : "@HyukjinKwon @wangyum should this also be replaced with `shim.setCurrentSessionState(state)`?",
        "createdAt" : "2021-06-14T09:22:38Z",
        "updatedAt" : "2021-06-14T09:22:38Z",
        "lastEditedBy" : "2821208d-43c5-4475-94ad-36cbf84c14d8",
        "tags" : [
        ]
      },
      {
        "id" : "996662bd-ed01-4b44-a731-04df2158eb27",
        "parentId" : "4c0257a7-d08d-48d5-8a5d-251174890531",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I think it's because it executes some Hive queries e.g.) `sparkSession.metadataHive.runSqlHive(\"SELECT * FROM t\")` at `sql/hive/src/test/scala/org/apache/spark/sql/hive/HiveMetastoreCatalogSuite.scala`",
        "createdAt" : "2021-06-14T09:37:20Z",
        "updatedAt" : "2021-06-14T09:37:20Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "f754a457-6405-4327-8df7-ed47d1b62765",
        "parentId" : "4c0257a7-d08d-48d5-8a5d-251174890531",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "Yes. Hive query needs to start `SessionState`.",
        "createdAt" : "2021-06-14T10:26:12Z",
        "updatedAt" : "2021-06-14T10:26:12Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      }
    ],
    "commit" : "4bca8ecaec066ef19d04a12e134ba830320a2e0f",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +830,834 @@\n    // Hive query needs to start SessionState.\n    SessionState.start(state)\n    logDebug(s\"Running hiveql '$cmd'\")\n    if (cmd.toLowerCase(Locale.ROOT).startsWith(\"set\")) { logDebug(s\"Changing config: $cmd\") }"
  },
  {
    "id" : "b4f9dcc5-3775-4b08-8bf9-da9acff611c6",
    "prId" : 31850,
    "prUrl" : "https://github.com/apache/spark/pull/31850#pullrequestreview-613129858",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c5d220f8-a5c9-47e1-84fa-969b82c2a28f",
        "parentId" : null,
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "this bypasses the hook canceling here\r\n\r\n```java\r\n// org.apache.hadoop.hive.common.FileUtils#deleteTmpFile\r\n\r\n  /**\r\n   * delete a temporary file and remove it from delete-on-exit hook.\r\n   */\r\n  public static boolean deleteTmpFile(File tempFile) {\r\n    if (tempFile != null) {\r\n      tempFile.delete();\r\n      ShutdownHookManager.cancelDeleteOnExit(tempFile);\r\n      return true;\r\n    }\r\n    return false;\r\n  }\r\n```\r\nfor `org.apache.hadoop.hive.ql.session.SessionState#deleteTmpOutputFile` and `deleteTmpErrOutputFile`",
        "createdAt" : "2021-03-16T11:28:01Z",
        "updatedAt" : "2021-03-16T13:10:34Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "b454fef8442d3c57d08622bc6323256485e0d9a8",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +164,168 @@    if (state.getTmpOutputFile != null) {\n      state.getTmpOutputFile.delete()\n      state.setTmpOutputFile(null)\n    }\n    if (state.getTmpErrOutputFile != null) {"
  },
  {
    "id" : "c95e79f4-4cd1-4527-957d-cdd91ed9b6fd",
    "prId" : 31271,
    "prUrl" : "https://github.com/apache/spark/pull/31271#pullrequestreview-574729794",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1fd3c087-a80c-4eb4-b6e3-6d5523677412",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "hmmm, I thought `org.apache.spark.sql.hive` is already a private package, no? cc @viirya @dongjoon-hyun ",
        "createdAt" : "2021-01-21T06:01:53Z",
        "updatedAt" : "2021-01-25T02:32:33Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "5dce089d-0d46-4081-809d-ab7a9b04e7fa",
        "parentId" : "1fd3c087-a80c-4eb4-b6e3-6d5523677412",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "I think so.",
        "createdAt" : "2021-01-21T06:03:26Z",
        "updatedAt" : "2021-01-25T02:32:33Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "7a33cdc0-bf80-4403-add4-1a93d9075266",
        "parentId" : "1fd3c087-a80c-4eb4-b6e3-6d5523677412",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Nah, it's exposed. https://spark.apache.org/docs/3.0.1/api/java/org/apache/spark/sql/hive/package-summary.html. We should better explicitly make it private",
        "createdAt" : "2021-01-21T09:07:46Z",
        "updatedAt" : "2021-01-25T02:32:33Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "2af98f7b-897e-493d-b93f-76da9fba69e3",
        "parentId" : "1fd3c087-a80c-4eb4-b6e3-6d5523677412",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "Got it, will fix it in the next commit.",
        "createdAt" : "2021-01-22T08:55:28Z",
        "updatedAt" : "2021-01-25T02:32:33Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      },
      {
        "id" : "e8d5545c-1a51-4563-b47d-e77a85c45163",
        "parentId" : "1fd3c087-a80c-4eb4-b6e3-6d5523677412",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Thank you for pinging me. +1 for making it private.",
        "createdAt" : "2021-01-23T03:07:34Z",
        "updatedAt" : "2021-01-25T02:32:33Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "1b89ea39a6393d478225bd05142fca655f16f4a5",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1269,1273 @@}\n\nprivate[hive] case object HiveVoidType extends DataType {\n  override def defaultSize: Int = 1\n  override def asNullable: DataType = HiveVoidType"
  },
  {
    "id" : "45cb58a2-0144-40c6-a58a-4b2578a121c8",
    "prId" : 30866,
    "prUrl" : "https://github.com/apache/spark/pull/30866#pullrequestreview-556304036",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9bf1334c-5ba0-4f62-95bb-091087bff406",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@MaxGekk, are there more places like to fix in this file? I think we can fix in one go instead of fixing one by one.",
        "createdAt" : "2020-12-21T10:52:32Z",
        "updatedAt" : "2020-12-21T10:52:32Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "6c1bdea6-9d46-4d2f-8c37-7bf7044a7f8b",
        "parentId" : "9bf1334c-5ba0-4f62-95bb-091087bff406",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Maybe, there are more places here but I am not fixing them somehow randomly. I find those issues while unifying tests for DSv2, DSv1 In-Memory and Hive Table (SPARK-33381). More tests we will unify, more behavior differences we will find, I guess.",
        "createdAt" : "2020-12-21T11:09:50Z",
        "updatedAt" : "2020-12-21T11:09:50Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "bcc0f1a5-d34b-4c54-8f90-75462fd52237",
        "parentId" : "9bf1334c-5ba0-4f62-95bb-091087bff406",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "In particular, this fix is needed for https://github.com/apache/spark/pull/30863#discussion_r546630850",
        "createdAt" : "2020-12-21T11:10:54Z",
        "updatedAt" : "2020-12-21T11:10:55Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "ab696330-fb88-4a99-b2d4-fafd91d8cb09",
        "parentId" : "9bf1334c-5ba0-4f62-95bb-091087bff406",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "ðŸ‘Œ ",
        "createdAt" : "2020-12-21T11:12:33Z",
        "updatedAt" : "2020-12-21T11:12:34Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "1c478e888c5bd2d3f63f5ff9a92fb2d6318ad491",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +666,670 @@    val hiveTable = toHiveTable(catalogTable, Some(userName))\n    specs.zip(newSpecs).foreach { case (oldSpec, newSpec) =>\n      if (client.getPartition(hiveTable, newSpec.asJava, false) != null) {\n        throw new PartitionAlreadyExistsException(db, table, newSpec)\n      }"
  },
  {
    "id" : "557decfb-2a56-4908-9518-aef74eef73cd",
    "prId" : 29936,
    "prUrl" : "https://github.com/apache/spark/pull/29936#pullrequestreview-501703891",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "31a4ec39-8d89-4f23-9d6f-7c097931bfc5",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@dongjoon-hyun should we remove `HiveUtils.isHive23` and related changes? I see it was added when we add Hive 2.3 support at https://github.com/apache/spark/commit/33f3c48cac087e079b9c7e342c2e58b16eaaa681#diff-842e3447fc453de26c706db1cac8f2c4R59. cc @wangyum FYI",
        "createdAt" : "2020-10-05T02:25:35Z",
        "updatedAt" : "2020-10-05T22:12:19Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "ca774720-fa3e-494a-aebd-1ab5b19bc865",
        "parentId" : "31a4ec39-8d89-4f23-9d6f-7c097931bfc5",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@AngersZhuuuu should we remove https://github.com/apache/spark/blob/55ce49ed28a58e0047a9680fa63f4dff54c11be4/sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/HiveScriptTransformationSuite.scala#L41 too?",
        "createdAt" : "2020-10-05T02:26:06Z",
        "updatedAt" : "2020-10-05T22:12:19Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "963e0260-732a-4b9f-9394-ce83cd4af290",
        "parentId" : "31a4ec39-8d89-4f23-9d6f-7c097931bfc5",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "+1, we can remove `HiveUtils.isHive23` and related changes.",
        "createdAt" : "2020-10-05T03:28:23Z",
        "updatedAt" : "2020-10-05T22:12:19Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      }
    ],
    "commit" : "c855260761af80b287caaea0ab071dbd15319265",
    "line" : 2,
    "diffHunk" : "@@ -1,1 +295,299 @@    } catch {\n      case e: NoClassDefFoundError\n        if HiveUtils.isHive23 && e.getMessage.contains(\"org/apache/hadoop/hive/serde2/SerDe\") =>\n        throw new ClassNotFoundException(\"The SerDe interface removed since Hive 2.3(HIVE-15167).\" +\n          \" Please migrate your custom SerDes to Hive 2.3. See HIVE-15167 for more details.\", e)"
  },
  {
    "id" : "baf2c54f-3efa-41fa-b6f9-472e74a0b61e",
    "prId" : 29363,
    "prUrl" : "https://github.com/apache/spark/pull/29363#pullrequestreview-461853455",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "11631d95-64ae-45c3-baf4-1a604a20b881",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "+1",
        "createdAt" : "2020-08-05T16:58:28Z",
        "updatedAt" : "2020-08-05T16:58:29Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      }
    ],
    "commit" : "9236cc6db5b2a381baa16f4e464a9338baa349b4",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +760,764 @@      pattern: String,\n      tableType: CatalogTableType): Seq[String] = withHiveState {\n    val hiveTableType = toHiveTableType(tableType)\n    try {\n      // Try with Hive API getTablesByType first, it's supported from Hive 2.3+."
  },
  {
    "id" : "790e8032-cced-43c7-aad5-39a12dac5a65",
    "prId" : 28647,
    "prUrl" : "https://github.com/apache/spark/pull/28647#pullrequestreview-440530813",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "30764732-26e3-455c-9e1a-5eb0075e4d37",
        "parentId" : null,
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "It also affect `createTable`, but seems fine.",
        "createdAt" : "2020-07-01T04:02:41Z",
        "updatedAt" : "2020-11-24T09:28:18Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      }
    ],
    "commit" : "c45489ad5b8ddd53d5e81fbba4cd08c0b4fd9850",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +1066,1070 @@    // Hive only retain the useful properties through serde class annotation.\n    // For better compatible with Hive, we remove the metastore properties.\n    val hiveProperties = table.properties -- HIVE_METASTORE_GENERATED_PROPERTIES\n    hiveProperties.foreach { case (k, v) => hiveTable.setProperty(k, v) }\n    table.comment.foreach { c => hiveTable.setProperty(\"comment\", c) }"
  },
  {
    "id" : "52d4ca66-4fdb-4103-8c16-7d8a090a16a1",
    "prId" : 28647,
    "prUrl" : "https://github.com/apache/spark/pull/28647#pullrequestreview-446046675",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "80b3b7bd-4897-4cce-9d5e-f48c8ad1af53",
        "parentId" : null,
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "After check Hive1.2 and Hive2.3 again, I think we can just remove 3 properties so that we can reduce the scope of influence.",
        "createdAt" : "2020-07-08T01:15:47Z",
        "updatedAt" : "2020-11-24T09:28:18Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      },
      {
        "id" : "63297559-718e-48ef-84c3-18655df9eb25",
        "parentId" : "80b3b7bd-4897-4cce-9d5e-f48c8ad1af53",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Does this include all the properties in https://github.com/apache/spark/pull/28647/files#diff-b7094baa12601424a5d19cb930e3402fL1534 ?",
        "createdAt" : "2020-07-09T15:24:18Z",
        "updatedAt" : "2020-11-24T09:28:18Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "78050762-56c9-4d21-86f6-c85b48d8e296",
        "parentId" : "80b3b7bd-4897-4cce-9d5e-f48c8ad1af53",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "Yes, I checked all the properties and make sure hive just modify the three properties.",
        "createdAt" : "2020-07-10T00:12:30Z",
        "updatedAt" : "2020-11-24T09:28:18Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      }
    ],
    "commit" : "c45489ad5b8ddd53d5e81fbba4cd08c0b4fd9850",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +1227,1231 @@\n  // Visible for testing.\n  private[hive] val HIVE_METASTORE_GENERATED_PROPERTIES: Set[String] = Set(\n    hive_metastoreConstants.DDL_TIME,\n    // at org.apache.hadoop.hive.ql.exec.DDLTask.updateModifiedParameters()"
  },
  {
    "id" : "89e6926d-97ba-44f7-bae2-e812894d159f",
    "prId" : 27041,
    "prUrl" : "https://github.com/apache/spark/pull/27041#pullrequestreview-342803015",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c1596c24-b503-4795-824a-192f054546c6",
        "parentId" : null,
        "authorId" : "56b136f7-d485-4c54-b8df-91c9cec8c43d",
        "body" : "Seems we should update https://github.com/apache/spark/pull/27041/files#diff-6fd847124f8eae45ba2de1cf7d6296feR170-R179 and also explain why extraConfig is at the end.",
        "createdAt" : "2020-01-14T19:26:03Z",
        "updatedAt" : "2020-02-18T15:07:02Z",
        "lastEditedBy" : "56b136f7-d485-4c54-b8df-91c9cec8c43d",
        "tags" : [
        ]
      },
      {
        "id" : "9570fd2b-0a3d-4622-96f3-8f8583604c5b",
        "parentId" : "c1596c24-b503-4795-824a-192f054546c6",
        "authorId" : "2c1bd8aa-c665-4159-a689-57743063347c",
        "body" : "@yhuai Sure, I have updated the PR with reasonable pointers for the order. Does it suffice it now.?",
        "createdAt" : "2020-01-14T19:38:39Z",
        "updatedAt" : "2020-02-18T15:07:02Z",
        "lastEditedBy" : "2c1bd8aa-c665-4159-a689-57743063347c",
        "tags" : [
        ]
      },
      {
        "id" : "cb16b834-b329-4c57-9c6e-c7e204b69f3c",
        "parentId" : "c1596c24-b503-4795-824a-192f054546c6",
        "authorId" : "56b136f7-d485-4c54-b8df-91c9cec8c43d",
        "body" : "Thank you. As getConfSystemProperties will get all of hive confs that are in the system properties, it is possible that we will pull in a config that is not set by `--hiveconf`. Seems we are introducing a behavior change? Can you explain the impact of this change and why this change is fine?",
        "createdAt" : "2020-01-14T19:55:37Z",
        "updatedAt" : "2020-02-18T15:07:02Z",
        "lastEditedBy" : "56b136f7-d485-4c54-b8df-91c9cec8c43d",
        "tags" : [
        ]
      },
      {
        "id" : "5d7045b5-5e4d-4e7c-a37e-5556e9874b38",
        "parentId" : "c1596c24-b503-4795-824a-192f054546c6",
        "authorId" : "2c1bd8aa-c665-4159-a689-57743063347c",
        "body" : "Sure, even without my changes in this PR, the `HiveConf` always considers the hive confs in system properties which were not set via `--hiveconf` as part of `HiveConf` constructor i.e Refer https://github.com/apache/hive/blob/rel/release-2.3.5/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java#L4079 (same behaviour for hive 1.2.1 as well) hence, this do not change any flow",
        "createdAt" : "2020-01-14T20:01:55Z",
        "updatedAt" : "2020-02-18T15:07:02Z",
        "lastEditedBy" : "2c1bd8aa-c665-4159-a689-57743063347c",
        "tags" : [
        ]
      }
    ],
    "commit" : "3718df989c02641f864432b246313a158a11b9e6",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +191,195 @@    val overriddenHiveProps = HiveConf.getConfSystemProperties.asScala\n    val confMap = (hadoopConf.iterator().asScala.map(kv => kv.getKey -> kv.getValue) ++\n      sparkConf.getAll.toMap ++ overriddenHiveProps ++ extraConfig).toMap\n    confMap.foreach { case (k, v) => hiveConf.set(k, v) }\n    SQLConf.get.redactOptions(confMap).foreach { case (k, v) =>"
  },
  {
    "id" : "30a2d6b3-df28-49e6-9404-475363254566",
    "prId" : 27041,
    "prUrl" : "https://github.com/apache/spark/pull/27041#pullrequestreview-360415248",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f073b573-fbf1-491d-b136-a4917f43ec0e",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "so we totally ignore the --hive-conf previously?",
        "createdAt" : "2020-02-18T05:26:29Z",
        "updatedAt" : "2020-02-18T15:07:02Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "2d1f08bb-2dfc-4811-8b9d-3b0d11416dd2",
        "parentId" : "f073b573-fbf1-491d-b136-a4917f43ec0e",
        "authorId" : "2c1bd8aa-c665-4159-a689-57743063347c",
        "body" : "it was handled as --hiveconf as part of HiveConf constructor i.e Refer https://github.com/apache/hive/blob/rel/release-2.3.5/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java#L4079 , \r\ni.e first it loads hive-site and then it adds --hiveconf  properties on top. \r\n\r\nBut in spark we again add hadoopConf on top of it hence overwriting HiveConf order",
        "createdAt" : "2020-02-18T05:31:20Z",
        "updatedAt" : "2020-02-18T15:07:02Z",
        "lastEditedBy" : "2c1bd8aa-c665-4159-a689-57743063347c",
        "tags" : [
        ]
      },
      {
        "id" : "9a63eca8-0050-4f45-b0f3-db9fd920007d",
        "parentId" : "f073b573-fbf1-491d-b136-a4917f43ec0e",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Can you add more code comment to convince people that `HiveConf.getConfSystemProperties` contains only the --hiveconf?",
        "createdAt" : "2020-02-18T05:46:05Z",
        "updatedAt" : "2020-02-18T15:07:02Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "34ce4b74-01bc-4bfd-bdfe-4a7f6312c9e3",
        "parentId" : "f073b573-fbf1-491d-b136-a4917f43ec0e",
        "authorId" : "2c1bd8aa-c665-4159-a689-57743063347c",
        "body" : "@cloud-fan updated. is the comment adequate now?",
        "createdAt" : "2020-02-18T15:08:27Z",
        "updatedAt" : "2020-02-18T15:08:40Z",
        "lastEditedBy" : "2c1bd8aa-c665-4159-a689-57743063347c",
        "tags" : [
        ]
      }
    ],
    "commit" : "3718df989c02641f864432b246313a158a11b9e6",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +189,193 @@    // which were considered on creation of HiveConf constructor\n    // Refer: org.apache.hadoop.hive.conf.HiveConf#applySystemProperties\n    val overriddenHiveProps = HiveConf.getConfSystemProperties.asScala\n    val confMap = (hadoopConf.iterator().asScala.map(kv => kv.getKey -> kv.getValue) ++\n      sparkConf.getAll.toMap ++ overriddenHiveProps ++ extraConfig).toMap"
  },
  {
    "id" : "926c3153-9b56-46cc-9ac4-e3e0e2095121",
    "prId" : 26892,
    "prUrl" : "https://github.com/apache/spark/pull/26892#pullrequestreview-332247845",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "54d33319-1880-46a2-8d3c-fba2f2f0d63c",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "This is a logical patch, but there is no evidence when this happens.\r\nCould you give me a reproducible procedure which you met?",
        "createdAt" : "2019-12-15T03:17:04Z",
        "updatedAt" : "2019-12-15T03:17:04Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "25853aa7-4e4e-4628-be50-5d47474f99dc",
        "parentId" : "54d33319-1880-46a2-8d3c-fba2f2f0d63c",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "I'm wondering if you are hitting the other system's bug. Then, you had better fix the root cause.\r\n> This is a related to the robustness of the code and may lead to unexpected exception in some unpredictable situation.Here is the case:",
        "createdAt" : "2019-12-15T03:40:23Z",
        "updatedAt" : "2019-12-15T03:40:23Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "f2fadba3-3a18-4b86-b23a-79bda7f55c77",
        "parentId" : "54d33319-1880-46a2-8d3c-fba2f2f0d63c",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Also, can you add tests for that?",
        "createdAt" : "2019-12-15T07:46:12Z",
        "updatedAt" : "2019-12-15T07:46:12Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "aff4a337190f4ec4ccd69ea11a7f1c9ae338e104",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +1043,1047 @@    val rawDataSize = properties.get(StatsSetupConst.RAW_DATA_SIZE).filter(_.nonEmpty)\n      .map(BigInt(_))\n    val rowCount = properties.get(StatsSetupConst.ROW_COUNT).filter(_.nonEmpty).map(BigInt(_))\n    // NOTE: getting `totalSize` directly from params is kind of hacky, but this should be\n    // relatively cheap if parameters for the table are populated into the metastore."
  },
  {
    "id" : "f882f528-3a9c-46c9-8b52-7b8ac531e32f",
    "prId" : 26422,
    "prUrl" : "https://github.com/apache/spark/pull/26422#pullrequestreview-381734581",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9380a86d-9da9-4cff-8a01-4f3b9f870661",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "is this how hive resolve the problem?",
        "createdAt" : "2020-03-02T06:33:27Z",
        "updatedAt" : "2020-05-29T08:32:32Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "d059d16d-78f5-42a5-8882-28343bf47ac0",
        "parentId" : "9380a86d-9da9-4cff-8a01-4f3b9f870661",
        "authorId" : "1a7af33a-9000-4d4e-9729-ac32a9505710",
        "body" : "> is this how hive resolve the problem?\r\n\r\nYes, It's the same method as Hive uses.",
        "createdAt" : "2020-03-17T03:12:36Z",
        "updatedAt" : "2020-05-29T08:32:32Z",
        "lastEditedBy" : "1a7af33a-9000-4d4e-9729-ac32a9505710",
        "tags" : [
        ]
      },
      {
        "id" : "eae3dd6d-0158-4bc6-9f09-75aeddf5c797",
        "parentId" : "9380a86d-9da9-4cff-8a01-4f3b9f870661",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Isn't it bad for performance? i.e. you call `fs.exists` and `fs.listStatus` for each partition.",
        "createdAt" : "2020-03-17T04:07:07Z",
        "updatedAt" : "2020-05-29T08:32:32Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "42368d2a-8832-4baf-aa1c-4d8ed2a8c95c",
        "parentId" : "9380a86d-9da9-4cff-8a01-4f3b9f870661",
        "authorId" : "1a7af33a-9000-4d4e-9729-ac32a9505710",
        "body" : "> Isn't it bad for performance? i.e. you call `fs.exists` and `fs.listStatus` for each partition.\r\n\r\nYes, but only affect `drop partitions`. I think it's necessary and won't take much time to do the check while dropping.",
        "createdAt" : "2020-03-17T08:06:36Z",
        "updatedAt" : "2020-05-29T08:32:32Z",
        "lastEditedBy" : "1a7af33a-9000-4d4e-9729-ac32a9505710",
        "tags" : [
        ]
      },
      {
        "id" : "ebe59bf8-582c-4249-a9bc-cccc7a3fec0d",
        "parentId" : "9380a86d-9da9-4cff-8a01-4f3b9f870661",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Can you point to the Hive source code that does the same thing? i.e. create a dummy directory before dropping the partition.",
        "createdAt" : "2020-03-23T08:32:32Z",
        "updatedAt" : "2020-05-29T08:32:32Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "dad5d7e4-8dc4-4da8-83f2-f0d7948d8fd7",
        "parentId" : "9380a86d-9da9-4cff-8a01-4f3b9f870661",
        "authorId" : "1a7af33a-9000-4d4e-9729-ac32a9505710",
        "body" : "> Can you point to the Hive source code that does the same thing? i.e. create a dummy directory before dropping the partition.\r\n\r\nIn Hive 1.x, it's like [this](https://github.com/apache/hive/blob/6002c510113d9a6aa87159c7386f2a8a4747405b/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java#L3415-L3420).",
        "createdAt" : "2020-03-24T02:34:03Z",
        "updatedAt" : "2020-05-29T08:32:32Z",
        "lastEditedBy" : "1a7af33a-9000-4d4e-9729-ac32a9505710",
        "tags" : [
        ]
      },
      {
        "id" : "65675bdf-d21a-4cd1-b9c7-a4c7bbdb1662",
        "parentId" : "9380a86d-9da9-4cff-8a01-4f3b9f870661",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "is it for DROP PARTITION?",
        "createdAt" : "2020-03-24T09:32:24Z",
        "updatedAt" : "2020-05-29T08:32:32Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "33fc3732-dc0c-4583-b11b-849e2ab6459d",
        "parentId" : "9380a86d-9da9-4cff-8a01-4f3b9f870661",
        "authorId" : "1a7af33a-9000-4d4e-9729-ac32a9505710",
        "body" : "> is it for DROP PARTITION?\r\n\r\nNo, it will check every query before executing. Maybe it's better to do the check before all queries?",
        "createdAt" : "2020-03-24T11:25:02Z",
        "updatedAt" : "2020-05-29T08:32:32Z",
        "lastEditedBy" : "1a7af33a-9000-4d4e-9729-ac32a9505710",
        "tags" : [
        ]
      },
      {
        "id" : "f34db7d4-19e9-456d-8558-48909dce0520",
        "parentId" : "9380a86d-9da9-4cff-8a01-4f3b9f870661",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Does Spark have a problem to do table scan when partition directory not exist?",
        "createdAt" : "2020-03-24T12:08:15Z",
        "updatedAt" : "2020-05-29T08:32:32Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "56645cec-7b5b-4da7-9886-f156d3715288",
        "parentId" : "9380a86d-9da9-4cff-8a01-4f3b9f870661",
        "authorId" : "1a7af33a-9000-4d4e-9729-ac32a9505710",
        "body" : "> Does Spark have a problem to do table scan when partition directory not exist?\r\n\r\nIt's related to [#24668](https://github.com/apache/spark/pull/24668), and controlled by `spark.sql.files.ignoreMissingFiles`.\r\nSpark will check it when listing leaf files.",
        "createdAt" : "2020-03-26T06:41:48Z",
        "updatedAt" : "2020-05-29T08:32:32Z",
        "lastEditedBy" : "1a7af33a-9000-4d4e-9729-ac32a9505710",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8b96de283f692781568903f0c27b1532808ecc4",
    "line" : 29,
    "diffHunk" : "@@ -1,1 +646,650 @@        }\n        // Check whether the partition we are going to drop is empty.\n        // We make a dummy one for the empty partition. See [SPARK-29786] for more details.\n        parts.foreach { partition =>\n          val partPath = partition.getPath.head"
  },
  {
    "id" : "0dd4cfd7-1b8a-4923-9282-2aa641d81a07",
    "prId" : 26080,
    "prUrl" : "https://github.com/apache/spark/pull/26080#pullrequestreview-325957165",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "89c00e4e-d5d0-4bc3-a045-c69796c3097f",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "shall we create a `toHiveDatabase` method for the code here? to remove duplicated code",
        "createdAt" : "2019-12-03T08:13:47Z",
        "updatedAt" : "2019-12-04T15:37:37Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "5eba948f-b8c7-470a-b0a8-3fe6cd803fc8",
        "parentId" : "89c00e4e-d5d0-4bc3-a045-c69796c3097f",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "ok",
        "createdAt" : "2019-12-03T08:19:32Z",
        "updatedAt" : "2019-12-04T15:37:37Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "e92c93fc0002f0a2c6d14fe3ebc9563e356d4663",
    "line" : 45,
    "diffHunk" : "@@ -1,1 +381,385 @@  private def toHiveDatabase(database: CatalogDatabase, isCreate: Boolean): HiveDatabase = {\n    val props = database.properties\n    val hiveDb = new HiveDatabase(\n      database.name,\n      database.description,"
  },
  {
    "id" : "3df53d76-1a16-4422-8c1a-f01b6272ed88",
    "prId" : 26068,
    "prUrl" : "https://github.com/apache/spark/pull/26068#pullrequestreview-306957553",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ae39cf3c-adc8-4937-a983-6fb5fa1f6eca",
        "parentId" : null,
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "Could we move this change to [HiveClientImpl.scala#L1043](https://github.com/apache/spark/blob/a6721ca93857acea1bfbc34046b967a39f8a5fa3/sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveClientImpl.scala#L1043)?\r\n```scala\r\nOption(table.owner).filter(_.nonEmpty).orElse(userName).foreach(hiveTable.setOwner)\r\n```",
        "createdAt" : "2019-10-14T14:58:25Z",
        "updatedAt" : "2019-10-17T09:09:28Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      },
      {
        "id" : "328a5573-fe08-4f08-bfb5-4311356ba483",
        "parentId" : "ae39cf3c-adc8-4937-a983-6fb5fa1f6eca",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "It sounds like a bigger deal than it is. Indeed,  I am going to add `alter table owner` syntax, which may make this kind of change back and forth",
        "createdAt" : "2019-10-15T03:13:50Z",
        "updatedAt" : "2019-10-17T09:09:28Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "2ebbaebb-d3f0-43be-9aa5-17758779368c",
        "parentId" : "ae39cf3c-adc8-4937-a983-6fb5fa1f6eca",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Hm, yea I think @wangyum's suggestion is more correct.",
        "createdAt" : "2019-10-24T09:49:24Z",
        "updatedAt" : "2019-10-24T09:49:24Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "720f8b37-35cb-43d1-9281-185ca2198b1e",
        "parentId" : "ae39cf3c-adc8-4937-a983-6fb5fa1f6eca",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "Thank you @HyukjinKwon Fixed by https://github.com/apache/spark/pull/26160.",
        "createdAt" : "2019-10-25T02:43:28Z",
        "updatedAt" : "2019-10-25T02:43:29Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      }
    ],
    "commit" : "10cb13bc846f5ad207e3a4f417483ded1b9179ef",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +574,578 @@    // these user-specified values.\n    verifyColumnDataType(table.dataSchema)\n    val owner = Option(table.owner).filter(_.nonEmpty).getOrElse(userName)\n    val hiveTable = toHiveTable(\n      table.copy(properties = table.ignoredProperties ++ table.properties), Some(owner))"
  },
  {
    "id" : "38e14f3d-bfb5-49f9-add8-abcf10d2ea9b",
    "prId" : 26016,
    "prUrl" : "https://github.com/apache/spark/pull/26016#pullrequestreview-324909133",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8c7e7d62-5f8f-4e74-b4a5-27cf5b24f4de",
        "parentId" : null,
        "authorId" : "934976a5-cca7-44a7-8578-4c05ccf3cb53",
        "body" : "If you've run `analyze table compute statistics` in Hive, rawDataSize can be a pretty big number, e.g.:\r\n<pre>\r\nparameters:{totalSize=8124889, numRows=2000000, rawDataSize=212750000, EXTERNAL=TRUE, etc.,,,\r\n</pre>\r\nI'm not sure how you'd end up with a rawDataSize but no totalSize (which is how you'd end up in this else/if block), but the value of rawDataSize is 26 times bigger than totalSize (which is nearly the same as your calculated deserFactor in this particular case).\r\n\r\nHowever, rawSize is not always big like that (for example, in Hive 1.2, it appears to simply be columnCount*rowCount).",
        "createdAt" : "2019-10-08T00:09:11Z",
        "updatedAt" : "2020-03-17T14:17:29Z",
        "lastEditedBy" : "934976a5-cca7-44a7-8578-4c05ccf3cb53",
        "tags" : [
        ]
      },
      {
        "id" : "4d3ec440-94b8-4f28-ac90-d426696ea717",
        "parentId" : "8c7e7d62-5f8f-4e74-b4a5-27cf5b24f4de",
        "authorId" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "body" : "In this case (when only `rawDataSize` is defined) I will set the `deserFactor` to `None` to  avoid the extra scaling as `rawDataSize` is already the \"approximate size of data in memory\". \r\n\r\nThe Hive 1.2 value you are referring to is probably a hive bug.  ",
        "createdAt" : "2019-11-20T19:43:14Z",
        "updatedAt" : "2020-03-17T14:17:29Z",
        "lastEditedBy" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "tags" : [
        ]
      },
      {
        "id" : "17de1b1a-2f41-4212-bfec-1c326a96cd7a",
        "parentId" : "8c7e7d62-5f8f-4e74-b4a5-27cf5b24f4de",
        "authorId" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "body" : "Resolved in a5469c9eb4cce673e42b0a7f3043a0d3fb767d82",
        "createdAt" : "2019-11-30T21:23:16Z",
        "updatedAt" : "2020-03-17T14:17:29Z",
        "lastEditedBy" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "tags" : [
        ]
      }
    ],
    "commit" : "42933f2e3f6eb699745e399a1fc9f05288603d79",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +1222,1226 @@    } else if (rawDataSize.isDefined && rawDataSize.get > 0) {\n      Some(CatalogStatistics(\n        sizeInBytes = rawDataSize.get,\n        deserFactor = None,\n        rowCount = rowCount.filter(_ > 0)))"
  }
]