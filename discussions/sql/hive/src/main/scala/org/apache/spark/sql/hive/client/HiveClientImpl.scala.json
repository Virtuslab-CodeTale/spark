[
  {
    "id" : "4ace1b81-f4b7-4940-8b87-77476a05ccee",
    "prId" : 32446,
    "prUrl" : "https://github.com/apache/spark/pull/32446#pullrequestreview-654087281",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "802f825b-74cd-406b-8ae5-57e9d6418ff6",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Here, line 303 claims that we need `the side-effect of Hive.get(conf)`.\r\nCould you confirm that `shim.getHive(conf)` doesn't break the side-effect assumption?",
        "createdAt" : "2021-05-07T05:14:03Z",
        "updatedAt" : "2021-05-07T05:14:03Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "83f1ef44-b4c8-4187-bd1c-33dd93652c59",
        "parentId" : "802f825b-74cd-406b-8ae5-57e9d6418ff6",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "yeah it shouldn't - both function are doing the same in this case by updating the `Hive` object's config with provided `conf`.",
        "createdAt" : "2021-05-07T05:16:22Z",
        "updatedAt" : "2021-05-07T05:16:23Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      }
    ],
    "commit" : "88697a43ba63963a1951f8d99a697fab4ca5692f",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +304,308 @@    // See discussion in https://github.com/apache/spark/pull/16826/files#r104606859\n    // for more details.\n    shim.getHive(conf)\n    // setCurrentSessionState will use the classLoader associated\n    // with the HiveConf in `state` to override the context class loader of the current"
  },
  {
    "id" : "23d4bb52-1a1b-475a-89d3-162a7145e886",
    "prId" : 32410,
    "prUrl" : "https://github.com/apache/spark/pull/32410#pullrequestreview-679603176",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0c147e18-0a17-4b84-9ed9-b276612db912",
        "parentId" : null,
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "@HyukjinKwon @sunchao I changed it to `shim.setCurrentSessionState(state)` to make it simpler.",
        "createdAt" : "2021-05-14T06:05:24Z",
        "updatedAt" : "2021-05-14T06:05:24Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      },
      {
        "id" : "39068d0f-fa49-4f5b-b965-6fb1eb586a21",
        "parentId" : "0c147e18-0a17-4b84-9ed9-b276612db912",
        "authorId" : "2821208d-43c5-4475-94ad-36cbf84c14d8",
        "body" : "@wangyum I wonder if this is a safe change. And why do you think this is making it simpler? ",
        "createdAt" : "2021-06-07T19:58:50Z",
        "updatedAt" : "2021-06-07T19:58:51Z",
        "lastEditedBy" : "2821208d-43c5-4475-94ad-36cbf84c14d8",
        "tags" : [
        ]
      },
      {
        "id" : "ab32acde-9137-41ed-8227-bb7a892ec651",
        "parentId" : "0c147e18-0a17-4b84-9ed9-b276612db912",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "Sorry for late replay. It is two questions.\r\n\r\n> if this is a safe change\r\n\r\nYes. The difference is `sessionState.start` creating too many directories and Spark SQL do not need it.\r\n\r\n> why do you think this is making it simpler?\r\n\r\n`shim.setCurrentSessionState(state)` is simpler than `if (version != hive.v12) SessionState.setCurrentSessionState(state) else   SessionState.start(state) `.",
        "createdAt" : "2021-06-09T12:45:42Z",
        "updatedAt" : "2021-06-09T12:45:42Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      }
    ],
    "commit" : "4bca8ecaec066ef19d04a12e134ba830320a2e0f",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +191,195 @@    // got changed. We reset it to clientLoader.ClassLoader here.\n    state.getConf.setClassLoader(clientLoader.classLoader)\n    shim.setCurrentSessionState(state)\n    state.out = new PrintStream(outputBuffer, true, UTF_8.name())\n    state.err = new PrintStream(outputBuffer, true, UTF_8.name())"
  },
  {
    "id" : "74bb2218-3400-4d14-b8fe-a369175442bc",
    "prId" : 32410,
    "prUrl" : "https://github.com/apache/spark/pull/32410#pullrequestreview-682750212",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4c0257a7-d08d-48d5-8a5d-251174890531",
        "parentId" : null,
        "authorId" : "2821208d-43c5-4475-94ad-36cbf84c14d8",
        "body" : "@HyukjinKwon @wangyum should this also be replaced with `shim.setCurrentSessionState(state)`?",
        "createdAt" : "2021-06-14T09:22:38Z",
        "updatedAt" : "2021-06-14T09:22:38Z",
        "lastEditedBy" : "2821208d-43c5-4475-94ad-36cbf84c14d8",
        "tags" : [
        ]
      },
      {
        "id" : "996662bd-ed01-4b44-a731-04df2158eb27",
        "parentId" : "4c0257a7-d08d-48d5-8a5d-251174890531",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I think it's because it executes some Hive queries e.g.) `sparkSession.metadataHive.runSqlHive(\"SELECT * FROM t\")` at `sql/hive/src/test/scala/org/apache/spark/sql/hive/HiveMetastoreCatalogSuite.scala`",
        "createdAt" : "2021-06-14T09:37:20Z",
        "updatedAt" : "2021-06-14T09:37:20Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "f754a457-6405-4327-8df7-ed47d1b62765",
        "parentId" : "4c0257a7-d08d-48d5-8a5d-251174890531",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "Yes. Hive query needs to start `SessionState`.",
        "createdAt" : "2021-06-14T10:26:12Z",
        "updatedAt" : "2021-06-14T10:26:12Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      }
    ],
    "commit" : "4bca8ecaec066ef19d04a12e134ba830320a2e0f",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +830,834 @@\n    // Hive query needs to start SessionState.\n    SessionState.start(state)\n    logDebug(s\"Running hiveql '$cmd'\")\n    if (cmd.toLowerCase(Locale.ROOT).startsWith(\"set\")) { logDebug(s\"Changing config: $cmd\") }"
  },
  {
    "id" : "b4f9dcc5-3775-4b08-8bf9-da9acff611c6",
    "prId" : 31850,
    "prUrl" : "https://github.com/apache/spark/pull/31850#pullrequestreview-613129858",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c5d220f8-a5c9-47e1-84fa-969b82c2a28f",
        "parentId" : null,
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "this bypasses the hook canceling here\r\n\r\n```java\r\n// org.apache.hadoop.hive.common.FileUtils#deleteTmpFile\r\n\r\n  /**\r\n   * delete a temporary file and remove it from delete-on-exit hook.\r\n   */\r\n  public static boolean deleteTmpFile(File tempFile) {\r\n    if (tempFile != null) {\r\n      tempFile.delete();\r\n      ShutdownHookManager.cancelDeleteOnExit(tempFile);\r\n      return true;\r\n    }\r\n    return false;\r\n  }\r\n```\r\nfor `org.apache.hadoop.hive.ql.session.SessionState#deleteTmpOutputFile` and `deleteTmpErrOutputFile`",
        "createdAt" : "2021-03-16T11:28:01Z",
        "updatedAt" : "2021-03-16T13:10:34Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "b454fef8442d3c57d08622bc6323256485e0d9a8",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +164,168 @@    if (state.getTmpOutputFile != null) {\n      state.getTmpOutputFile.delete()\n      state.setTmpOutputFile(null)\n    }\n    if (state.getTmpErrOutputFile != null) {"
  },
  {
    "id" : "c95e79f4-4cd1-4527-957d-cdd91ed9b6fd",
    "prId" : 31271,
    "prUrl" : "https://github.com/apache/spark/pull/31271#pullrequestreview-574729794",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1fd3c087-a80c-4eb4-b6e3-6d5523677412",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "hmmm, I thought `org.apache.spark.sql.hive` is already a private package, no? cc @viirya @dongjoon-hyun ",
        "createdAt" : "2021-01-21T06:01:53Z",
        "updatedAt" : "2021-01-25T02:32:33Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "5dce089d-0d46-4081-809d-ab7a9b04e7fa",
        "parentId" : "1fd3c087-a80c-4eb4-b6e3-6d5523677412",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "I think so.",
        "createdAt" : "2021-01-21T06:03:26Z",
        "updatedAt" : "2021-01-25T02:32:33Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "7a33cdc0-bf80-4403-add4-1a93d9075266",
        "parentId" : "1fd3c087-a80c-4eb4-b6e3-6d5523677412",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Nah, it's exposed. https://spark.apache.org/docs/3.0.1/api/java/org/apache/spark/sql/hive/package-summary.html. We should better explicitly make it private",
        "createdAt" : "2021-01-21T09:07:46Z",
        "updatedAt" : "2021-01-25T02:32:33Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "2af98f7b-897e-493d-b93f-76da9fba69e3",
        "parentId" : "1fd3c087-a80c-4eb4-b6e3-6d5523677412",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "Got it, will fix it in the next commit.",
        "createdAt" : "2021-01-22T08:55:28Z",
        "updatedAt" : "2021-01-25T02:32:33Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      },
      {
        "id" : "e8d5545c-1a51-4563-b47d-e77a85c45163",
        "parentId" : "1fd3c087-a80c-4eb4-b6e3-6d5523677412",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Thank you for pinging me. +1 for making it private.",
        "createdAt" : "2021-01-23T03:07:34Z",
        "updatedAt" : "2021-01-25T02:32:33Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "1b89ea39a6393d478225bd05142fca655f16f4a5",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1269,1273 @@}\n\nprivate[hive] case object HiveVoidType extends DataType {\n  override def defaultSize: Int = 1\n  override def asNullable: DataType = HiveVoidType"
  },
  {
    "id" : "45cb58a2-0144-40c6-a58a-4b2578a121c8",
    "prId" : 30866,
    "prUrl" : "https://github.com/apache/spark/pull/30866#pullrequestreview-556304036",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9bf1334c-5ba0-4f62-95bb-091087bff406",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@MaxGekk, are there more places like to fix in this file? I think we can fix in one go instead of fixing one by one.",
        "createdAt" : "2020-12-21T10:52:32Z",
        "updatedAt" : "2020-12-21T10:52:32Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "6c1bdea6-9d46-4d2f-8c37-7bf7044a7f8b",
        "parentId" : "9bf1334c-5ba0-4f62-95bb-091087bff406",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Maybe, there are more places here but I am not fixing them somehow randomly. I find those issues while unifying tests for DSv2, DSv1 In-Memory and Hive Table (SPARK-33381). More tests we will unify, more behavior differences we will find, I guess.",
        "createdAt" : "2020-12-21T11:09:50Z",
        "updatedAt" : "2020-12-21T11:09:50Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "bcc0f1a5-d34b-4c54-8f90-75462fd52237",
        "parentId" : "9bf1334c-5ba0-4f62-95bb-091087bff406",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "In particular, this fix is needed for https://github.com/apache/spark/pull/30863#discussion_r546630850",
        "createdAt" : "2020-12-21T11:10:54Z",
        "updatedAt" : "2020-12-21T11:10:55Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "ab696330-fb88-4a99-b2d4-fafd91d8cb09",
        "parentId" : "9bf1334c-5ba0-4f62-95bb-091087bff406",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "ðŸ‘Œ ",
        "createdAt" : "2020-12-21T11:12:33Z",
        "updatedAt" : "2020-12-21T11:12:34Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "1c478e888c5bd2d3f63f5ff9a92fb2d6318ad491",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +666,670 @@    val hiveTable = toHiveTable(catalogTable, Some(userName))\n    specs.zip(newSpecs).foreach { case (oldSpec, newSpec) =>\n      if (client.getPartition(hiveTable, newSpec.asJava, false) != null) {\n        throw new PartitionAlreadyExistsException(db, table, newSpec)\n      }"
  },
  {
    "id" : "557decfb-2a56-4908-9518-aef74eef73cd",
    "prId" : 29936,
    "prUrl" : "https://github.com/apache/spark/pull/29936#pullrequestreview-501703891",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "31a4ec39-8d89-4f23-9d6f-7c097931bfc5",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@dongjoon-hyun should we remove `HiveUtils.isHive23` and related changes? I see it was added when we add Hive 2.3 support at https://github.com/apache/spark/commit/33f3c48cac087e079b9c7e342c2e58b16eaaa681#diff-842e3447fc453de26c706db1cac8f2c4R59. cc @wangyum FYI",
        "createdAt" : "2020-10-05T02:25:35Z",
        "updatedAt" : "2020-10-05T22:12:19Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "ca774720-fa3e-494a-aebd-1ab5b19bc865",
        "parentId" : "31a4ec39-8d89-4f23-9d6f-7c097931bfc5",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@AngersZhuuuu should we remove https://github.com/apache/spark/blob/55ce49ed28a58e0047a9680fa63f4dff54c11be4/sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/HiveScriptTransformationSuite.scala#L41 too?",
        "createdAt" : "2020-10-05T02:26:06Z",
        "updatedAt" : "2020-10-05T22:12:19Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "963e0260-732a-4b9f-9394-ce83cd4af290",
        "parentId" : "31a4ec39-8d89-4f23-9d6f-7c097931bfc5",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "+1, we can remove `HiveUtils.isHive23` and related changes.",
        "createdAt" : "2020-10-05T03:28:23Z",
        "updatedAt" : "2020-10-05T22:12:19Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      }
    ],
    "commit" : "c855260761af80b287caaea0ab071dbd15319265",
    "line" : 2,
    "diffHunk" : "@@ -1,1 +295,299 @@    } catch {\n      case e: NoClassDefFoundError\n        if HiveUtils.isHive23 && e.getMessage.contains(\"org/apache/hadoop/hive/serde2/SerDe\") =>\n        throw new ClassNotFoundException(\"The SerDe interface removed since Hive 2.3(HIVE-15167).\" +\n          \" Please migrate your custom SerDes to Hive 2.3. See HIVE-15167 for more details.\", e)"
  },
  {
    "id" : "baf2c54f-3efa-41fa-b6f9-472e74a0b61e",
    "prId" : 29363,
    "prUrl" : "https://github.com/apache/spark/pull/29363#pullrequestreview-461853455",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "11631d95-64ae-45c3-baf4-1a604a20b881",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "+1",
        "createdAt" : "2020-08-05T16:58:28Z",
        "updatedAt" : "2020-08-05T16:58:29Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      }
    ],
    "commit" : "9236cc6db5b2a381baa16f4e464a9338baa349b4",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +760,764 @@      pattern: String,\n      tableType: CatalogTableType): Seq[String] = withHiveState {\n    val hiveTableType = toHiveTableType(tableType)\n    try {\n      // Try with Hive API getTablesByType first, it's supported from Hive 2.3+."
  },
  {
    "id" : "790e8032-cced-43c7-aad5-39a12dac5a65",
    "prId" : 28647,
    "prUrl" : "https://github.com/apache/spark/pull/28647#pullrequestreview-440530813",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "30764732-26e3-455c-9e1a-5eb0075e4d37",
        "parentId" : null,
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "It also affect `createTable`, but seems fine.",
        "createdAt" : "2020-07-01T04:02:41Z",
        "updatedAt" : "2020-11-24T09:28:18Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      }
    ],
    "commit" : "c45489ad5b8ddd53d5e81fbba4cd08c0b4fd9850",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +1066,1070 @@    // Hive only retain the useful properties through serde class annotation.\n    // For better compatible with Hive, we remove the metastore properties.\n    val hiveProperties = table.properties -- HIVE_METASTORE_GENERATED_PROPERTIES\n    hiveProperties.foreach { case (k, v) => hiveTable.setProperty(k, v) }\n    table.comment.foreach { c => hiveTable.setProperty(\"comment\", c) }"
  },
  {
    "id" : "52d4ca66-4fdb-4103-8c16-7d8a090a16a1",
    "prId" : 28647,
    "prUrl" : "https://github.com/apache/spark/pull/28647#pullrequestreview-446046675",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "80b3b7bd-4897-4cce-9d5e-f48c8ad1af53",
        "parentId" : null,
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "After check Hive1.2 and Hive2.3 again, I think we can just remove 3 properties so that we can reduce the scope of influence.",
        "createdAt" : "2020-07-08T01:15:47Z",
        "updatedAt" : "2020-11-24T09:28:18Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      },
      {
        "id" : "63297559-718e-48ef-84c3-18655df9eb25",
        "parentId" : "80b3b7bd-4897-4cce-9d5e-f48c8ad1af53",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Does this include all the properties in https://github.com/apache/spark/pull/28647/files#diff-b7094baa12601424a5d19cb930e3402fL1534 ?",
        "createdAt" : "2020-07-09T15:24:18Z",
        "updatedAt" : "2020-11-24T09:28:18Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "78050762-56c9-4d21-86f6-c85b48d8e296",
        "parentId" : "80b3b7bd-4897-4cce-9d5e-f48c8ad1af53",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "Yes, I checked all the properties and make sure hive just modify the three properties.",
        "createdAt" : "2020-07-10T00:12:30Z",
        "updatedAt" : "2020-11-24T09:28:18Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      }
    ],
    "commit" : "c45489ad5b8ddd53d5e81fbba4cd08c0b4fd9850",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +1227,1231 @@\n  // Visible for testing.\n  private[hive] val HIVE_METASTORE_GENERATED_PROPERTIES: Set[String] = Set(\n    hive_metastoreConstants.DDL_TIME,\n    // at org.apache.hadoop.hive.ql.exec.DDLTask.updateModifiedParameters()"
  },
  {
    "id" : "89e6926d-97ba-44f7-bae2-e812894d159f",
    "prId" : 27041,
    "prUrl" : "https://github.com/apache/spark/pull/27041#pullrequestreview-342803015",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c1596c24-b503-4795-824a-192f054546c6",
        "parentId" : null,
        "authorId" : "56b136f7-d485-4c54-b8df-91c9cec8c43d",
        "body" : "Seems we should update https://github.com/apache/spark/pull/27041/files#diff-6fd847124f8eae45ba2de1cf7d6296feR170-R179 and also explain why extraConfig is at the end.",
        "createdAt" : "2020-01-14T19:26:03Z",
        "updatedAt" : "2020-02-18T15:07:02Z",
        "lastEditedBy" : "56b136f7-d485-4c54-b8df-91c9cec8c43d",
        "tags" : [
        ]
      },
      {
        "id" : "9570fd2b-0a3d-4622-96f3-8f8583604c5b",
        "parentId" : "c1596c24-b503-4795-824a-192f054546c6",
        "authorId" : "2c1bd8aa-c665-4159-a689-57743063347c",
        "body" : "@yhuai Sure, I have updated the PR with reasonable pointers for the order. Does it suffice it now.?",
        "createdAt" : "2020-01-14T19:38:39Z",
        "updatedAt" : "2020-02-18T15:07:02Z",
        "lastEditedBy" : "2c1bd8aa-c665-4159-a689-57743063347c",
        "tags" : [
        ]
      },
      {
        "id" : "cb16b834-b329-4c57-9c6e-c7e204b69f3c",
        "parentId" : "c1596c24-b503-4795-824a-192f054546c6",
        "authorId" : "56b136f7-d485-4c54-b8df-91c9cec8c43d",
        "body" : "Thank you. As getConfSystemProperties will get all of hive confs that are in the system properties, it is possible that we will pull in a config that is not set by `--hiveconf`. Seems we are introducing a behavior change? Can you explain the impact of this change and why this change is fine?",
        "createdAt" : "2020-01-14T19:55:37Z",
        "updatedAt" : "2020-02-18T15:07:02Z",
        "lastEditedBy" : "56b136f7-d485-4c54-b8df-91c9cec8c43d",
        "tags" : [
        ]
      },
      {
        "id" : "5d7045b5-5e4d-4e7c-a37e-5556e9874b38",
        "parentId" : "c1596c24-b503-4795-824a-192f054546c6",
        "authorId" : "2c1bd8aa-c665-4159-a689-57743063347c",
        "body" : "Sure, even without my changes in this PR, the `HiveConf` always considers the hive confs in system properties which were not set via `--hiveconf` as part of `HiveConf` constructor i.e Refer https://github.com/apache/hive/blob/rel/release-2.3.5/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java#L4079 (same behaviour for hive 1.2.1 as well) hence, this do not change any flow",
        "createdAt" : "2020-01-14T20:01:55Z",
        "updatedAt" : "2020-02-18T15:07:02Z",
        "lastEditedBy" : "2c1bd8aa-c665-4159-a689-57743063347c",
        "tags" : [
        ]
      }
    ],
    "commit" : "3718df989c02641f864432b246313a158a11b9e6",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +191,195 @@    val overriddenHiveProps = HiveConf.getConfSystemProperties.asScala\n    val confMap = (hadoopConf.iterator().asScala.map(kv => kv.getKey -> kv.getValue) ++\n      sparkConf.getAll.toMap ++ overriddenHiveProps ++ extraConfig).toMap\n    confMap.foreach { case (k, v) => hiveConf.set(k, v) }\n    SQLConf.get.redactOptions(confMap).foreach { case (k, v) =>"
  },
  {
    "id" : "30a2d6b3-df28-49e6-9404-475363254566",
    "prId" : 27041,
    "prUrl" : "https://github.com/apache/spark/pull/27041#pullrequestreview-360415248",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f073b573-fbf1-491d-b136-a4917f43ec0e",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "so we totally ignore the --hive-conf previously?",
        "createdAt" : "2020-02-18T05:26:29Z",
        "updatedAt" : "2020-02-18T15:07:02Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "2d1f08bb-2dfc-4811-8b9d-3b0d11416dd2",
        "parentId" : "f073b573-fbf1-491d-b136-a4917f43ec0e",
        "authorId" : "2c1bd8aa-c665-4159-a689-57743063347c",
        "body" : "it was handled as --hiveconf as part of HiveConf constructor i.e Refer https://github.com/apache/hive/blob/rel/release-2.3.5/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java#L4079 , \r\ni.e first it loads hive-site and then it adds --hiveconf  properties on top. \r\n\r\nBut in spark we again add hadoopConf on top of it hence overwriting HiveConf order",
        "createdAt" : "2020-02-18T05:31:20Z",
        "updatedAt" : "2020-02-18T15:07:02Z",
        "lastEditedBy" : "2c1bd8aa-c665-4159-a689-57743063347c",
        "tags" : [
        ]
      },
      {
        "id" : "9a63eca8-0050-4f45-b0f3-db9fd920007d",
        "parentId" : "f073b573-fbf1-491d-b136-a4917f43ec0e",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Can you add more code comment to convince people that `HiveConf.getConfSystemProperties` contains only the --hiveconf?",
        "createdAt" : "2020-02-18T05:46:05Z",
        "updatedAt" : "2020-02-18T15:07:02Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "34ce4b74-01bc-4bfd-bdfe-4a7f6312c9e3",
        "parentId" : "f073b573-fbf1-491d-b136-a4917f43ec0e",
        "authorId" : "2c1bd8aa-c665-4159-a689-57743063347c",
        "body" : "@cloud-fan updated. is the comment adequate now?",
        "createdAt" : "2020-02-18T15:08:27Z",
        "updatedAt" : "2020-02-18T15:08:40Z",
        "lastEditedBy" : "2c1bd8aa-c665-4159-a689-57743063347c",
        "tags" : [
        ]
      }
    ],
    "commit" : "3718df989c02641f864432b246313a158a11b9e6",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +189,193 @@    // which were considered on creation of HiveConf constructor\n    // Refer: org.apache.hadoop.hive.conf.HiveConf#applySystemProperties\n    val overriddenHiveProps = HiveConf.getConfSystemProperties.asScala\n    val confMap = (hadoopConf.iterator().asScala.map(kv => kv.getKey -> kv.getValue) ++\n      sparkConf.getAll.toMap ++ overriddenHiveProps ++ extraConfig).toMap"
  },
  {
    "id" : "926c3153-9b56-46cc-9ac4-e3e0e2095121",
    "prId" : 26892,
    "prUrl" : "https://github.com/apache/spark/pull/26892#pullrequestreview-332247845",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "54d33319-1880-46a2-8d3c-fba2f2f0d63c",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "This is a logical patch, but there is no evidence when this happens.\r\nCould you give me a reproducible procedure which you met?",
        "createdAt" : "2019-12-15T03:17:04Z",
        "updatedAt" : "2019-12-15T03:17:04Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "25853aa7-4e4e-4628-be50-5d47474f99dc",
        "parentId" : "54d33319-1880-46a2-8d3c-fba2f2f0d63c",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "I'm wondering if you are hitting the other system's bug. Then, you had better fix the root cause.\r\n> This is a related to the robustness of the code and may lead to unexpected exception in some unpredictable situation.Here is the case:",
        "createdAt" : "2019-12-15T03:40:23Z",
        "updatedAt" : "2019-12-15T03:40:23Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "f2fadba3-3a18-4b86-b23a-79bda7f55c77",
        "parentId" : "54d33319-1880-46a2-8d3c-fba2f2f0d63c",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Also, can you add tests for that?",
        "createdAt" : "2019-12-15T07:46:12Z",
        "updatedAt" : "2019-12-15T07:46:12Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "aff4a337190f4ec4ccd69ea11a7f1c9ae338e104",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +1043,1047 @@    val rawDataSize = properties.get(StatsSetupConst.RAW_DATA_SIZE).filter(_.nonEmpty)\n      .map(BigInt(_))\n    val rowCount = properties.get(StatsSetupConst.ROW_COUNT).filter(_.nonEmpty).map(BigInt(_))\n    // NOTE: getting `totalSize` directly from params is kind of hacky, but this should be\n    // relatively cheap if parameters for the table are populated into the metastore."
  },
  {
    "id" : "f882f528-3a9c-46c9-8b52-7b8ac531e32f",
    "prId" : 26422,
    "prUrl" : "https://github.com/apache/spark/pull/26422#pullrequestreview-381734581",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9380a86d-9da9-4cff-8a01-4f3b9f870661",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "is this how hive resolve the problem?",
        "createdAt" : "2020-03-02T06:33:27Z",
        "updatedAt" : "2020-05-29T08:32:32Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "d059d16d-78f5-42a5-8882-28343bf47ac0",
        "parentId" : "9380a86d-9da9-4cff-8a01-4f3b9f870661",
        "authorId" : "1a7af33a-9000-4d4e-9729-ac32a9505710",
        "body" : "> is this how hive resolve the problem?\r\n\r\nYes, It's the same method as Hive uses.",
        "createdAt" : "2020-03-17T03:12:36Z",
        "updatedAt" : "2020-05-29T08:32:32Z",
        "lastEditedBy" : "1a7af33a-9000-4d4e-9729-ac32a9505710",
        "tags" : [
        ]
      },
      {
        "id" : "eae3dd6d-0158-4bc6-9f09-75aeddf5c797",
        "parentId" : "9380a86d-9da9-4cff-8a01-4f3b9f870661",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Isn't it bad for performance? i.e. you call `fs.exists` and `fs.listStatus` for each partition.",
        "createdAt" : "2020-03-17T04:07:07Z",
        "updatedAt" : "2020-05-29T08:32:32Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "42368d2a-8832-4baf-aa1c-4d8ed2a8c95c",
        "parentId" : "9380a86d-9da9-4cff-8a01-4f3b9f870661",
        "authorId" : "1a7af33a-9000-4d4e-9729-ac32a9505710",
        "body" : "> Isn't it bad for performance? i.e. you call `fs.exists` and `fs.listStatus` for each partition.\r\n\r\nYes, but only affect `drop partitions`. I think it's necessary and won't take much time to do the check while dropping.",
        "createdAt" : "2020-03-17T08:06:36Z",
        "updatedAt" : "2020-05-29T08:32:32Z",
        "lastEditedBy" : "1a7af33a-9000-4d4e-9729-ac32a9505710",
        "tags" : [
        ]
      },
      {
        "id" : "ebe59bf8-582c-4249-a9bc-cccc7a3fec0d",
        "parentId" : "9380a86d-9da9-4cff-8a01-4f3b9f870661",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Can you point to the Hive source code that does the same thing? i.e. create a dummy directory before dropping the partition.",
        "createdAt" : "2020-03-23T08:32:32Z",
        "updatedAt" : "2020-05-29T08:32:32Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "dad5d7e4-8dc4-4da8-83f2-f0d7948d8fd7",
        "parentId" : "9380a86d-9da9-4cff-8a01-4f3b9f870661",
        "authorId" : "1a7af33a-9000-4d4e-9729-ac32a9505710",
        "body" : "> Can you point to the Hive source code that does the same thing? i.e. create a dummy directory before dropping the partition.\r\n\r\nIn Hive 1.x, it's like [this](https://github.com/apache/hive/blob/6002c510113d9a6aa87159c7386f2a8a4747405b/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java#L3415-L3420).",
        "createdAt" : "2020-03-24T02:34:03Z",
        "updatedAt" : "2020-05-29T08:32:32Z",
        "lastEditedBy" : "1a7af33a-9000-4d4e-9729-ac32a9505710",
        "tags" : [
        ]
      },
      {
        "id" : "65675bdf-d21a-4cd1-b9c7-a4c7bbdb1662",
        "parentId" : "9380a86d-9da9-4cff-8a01-4f3b9f870661",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "is it for DROP PARTITION?",
        "createdAt" : "2020-03-24T09:32:24Z",
        "updatedAt" : "2020-05-29T08:32:32Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "33fc3732-dc0c-4583-b11b-849e2ab6459d",
        "parentId" : "9380a86d-9da9-4cff-8a01-4f3b9f870661",
        "authorId" : "1a7af33a-9000-4d4e-9729-ac32a9505710",
        "body" : "> is it for DROP PARTITION?\r\n\r\nNo, it will check every query before executing. Maybe it's better to do the check before all queries?",
        "createdAt" : "2020-03-24T11:25:02Z",
        "updatedAt" : "2020-05-29T08:32:32Z",
        "lastEditedBy" : "1a7af33a-9000-4d4e-9729-ac32a9505710",
        "tags" : [
        ]
      },
      {
        "id" : "f34db7d4-19e9-456d-8558-48909dce0520",
        "parentId" : "9380a86d-9da9-4cff-8a01-4f3b9f870661",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Does Spark have a problem to do table scan when partition directory not exist?",
        "createdAt" : "2020-03-24T12:08:15Z",
        "updatedAt" : "2020-05-29T08:32:32Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "56645cec-7b5b-4da7-9886-f156d3715288",
        "parentId" : "9380a86d-9da9-4cff-8a01-4f3b9f870661",
        "authorId" : "1a7af33a-9000-4d4e-9729-ac32a9505710",
        "body" : "> Does Spark have a problem to do table scan when partition directory not exist?\r\n\r\nIt's related to [#24668](https://github.com/apache/spark/pull/24668), and controlled by `spark.sql.files.ignoreMissingFiles`.\r\nSpark will check it when listing leaf files.",
        "createdAt" : "2020-03-26T06:41:48Z",
        "updatedAt" : "2020-05-29T08:32:32Z",
        "lastEditedBy" : "1a7af33a-9000-4d4e-9729-ac32a9505710",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8b96de283f692781568903f0c27b1532808ecc4",
    "line" : 29,
    "diffHunk" : "@@ -1,1 +646,650 @@        }\n        // Check whether the partition we are going to drop is empty.\n        // We make a dummy one for the empty partition. See [SPARK-29786] for more details.\n        parts.foreach { partition =>\n          val partPath = partition.getPath.head"
  },
  {
    "id" : "0dd4cfd7-1b8a-4923-9282-2aa641d81a07",
    "prId" : 26080,
    "prUrl" : "https://github.com/apache/spark/pull/26080#pullrequestreview-325957165",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "89c00e4e-d5d0-4bc3-a045-c69796c3097f",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "shall we create a `toHiveDatabase` method for the code here? to remove duplicated code",
        "createdAt" : "2019-12-03T08:13:47Z",
        "updatedAt" : "2019-12-04T15:37:37Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "5eba948f-b8c7-470a-b0a8-3fe6cd803fc8",
        "parentId" : "89c00e4e-d5d0-4bc3-a045-c69796c3097f",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "ok",
        "createdAt" : "2019-12-03T08:19:32Z",
        "updatedAt" : "2019-12-04T15:37:37Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "e92c93fc0002f0a2c6d14fe3ebc9563e356d4663",
    "line" : 45,
    "diffHunk" : "@@ -1,1 +381,385 @@  private def toHiveDatabase(database: CatalogDatabase, isCreate: Boolean): HiveDatabase = {\n    val props = database.properties\n    val hiveDb = new HiveDatabase(\n      database.name,\n      database.description,"
  },
  {
    "id" : "3df53d76-1a16-4422-8c1a-f01b6272ed88",
    "prId" : 26068,
    "prUrl" : "https://github.com/apache/spark/pull/26068#pullrequestreview-306957553",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ae39cf3c-adc8-4937-a983-6fb5fa1f6eca",
        "parentId" : null,
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "Could we move this change to [HiveClientImpl.scala#L1043](https://github.com/apache/spark/blob/a6721ca93857acea1bfbc34046b967a39f8a5fa3/sql/hive/src/main/scala/org/apache/spark/sql/hive/client/HiveClientImpl.scala#L1043)?\r\n```scala\r\nOption(table.owner).filter(_.nonEmpty).orElse(userName).foreach(hiveTable.setOwner)\r\n```",
        "createdAt" : "2019-10-14T14:58:25Z",
        "updatedAt" : "2019-10-17T09:09:28Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      },
      {
        "id" : "328a5573-fe08-4f08-bfb5-4311356ba483",
        "parentId" : "ae39cf3c-adc8-4937-a983-6fb5fa1f6eca",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "It sounds like a bigger deal than it is. Indeed,  I am going to add `alter table owner` syntax, which may make this kind of change back and forth",
        "createdAt" : "2019-10-15T03:13:50Z",
        "updatedAt" : "2019-10-17T09:09:28Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "2ebbaebb-d3f0-43be-9aa5-17758779368c",
        "parentId" : "ae39cf3c-adc8-4937-a983-6fb5fa1f6eca",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Hm, yea I think @wangyum's suggestion is more correct.",
        "createdAt" : "2019-10-24T09:49:24Z",
        "updatedAt" : "2019-10-24T09:49:24Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "720f8b37-35cb-43d1-9281-185ca2198b1e",
        "parentId" : "ae39cf3c-adc8-4937-a983-6fb5fa1f6eca",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "Thank you @HyukjinKwon Fixed by https://github.com/apache/spark/pull/26160.",
        "createdAt" : "2019-10-25T02:43:28Z",
        "updatedAt" : "2019-10-25T02:43:29Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      }
    ],
    "commit" : "10cb13bc846f5ad207e3a4f417483ded1b9179ef",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +574,578 @@    // these user-specified values.\n    verifyColumnDataType(table.dataSchema)\n    val owner = Option(table.owner).filter(_.nonEmpty).getOrElse(userName)\n    val hiveTable = toHiveTable(\n      table.copy(properties = table.ignoredProperties ++ table.properties), Some(owner))"
  },
  {
    "id" : "38e14f3d-bfb5-49f9-add8-abcf10d2ea9b",
    "prId" : 26016,
    "prUrl" : "https://github.com/apache/spark/pull/26016#pullrequestreview-324909133",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8c7e7d62-5f8f-4e74-b4a5-27cf5b24f4de",
        "parentId" : null,
        "authorId" : "934976a5-cca7-44a7-8578-4c05ccf3cb53",
        "body" : "If you've run `analyze table compute statistics` in Hive, rawDataSize can be a pretty big number, e.g.:\r\n<pre>\r\nparameters:{totalSize=8124889, numRows=2000000, rawDataSize=212750000, EXTERNAL=TRUE, etc.,,,\r\n</pre>\r\nI'm not sure how you'd end up with a rawDataSize but no totalSize (which is how you'd end up in this else/if block), but the value of rawDataSize is 26 times bigger than totalSize (which is nearly the same as your calculated deserFactor in this particular case).\r\n\r\nHowever, rawSize is not always big like that (for example, in Hive 1.2, it appears to simply be columnCount*rowCount).",
        "createdAt" : "2019-10-08T00:09:11Z",
        "updatedAt" : "2020-03-17T14:17:29Z",
        "lastEditedBy" : "934976a5-cca7-44a7-8578-4c05ccf3cb53",
        "tags" : [
        ]
      },
      {
        "id" : "4d3ec440-94b8-4f28-ac90-d426696ea717",
        "parentId" : "8c7e7d62-5f8f-4e74-b4a5-27cf5b24f4de",
        "authorId" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "body" : "In this case (when only `rawDataSize` is defined) I will set the `deserFactor` to `None` to  avoid the extra scaling as `rawDataSize` is already the \"approximate size of data in memory\". \r\n\r\nThe Hive 1.2 value you are referring to is probably a hive bug.  ",
        "createdAt" : "2019-11-20T19:43:14Z",
        "updatedAt" : "2020-03-17T14:17:29Z",
        "lastEditedBy" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "tags" : [
        ]
      },
      {
        "id" : "17de1b1a-2f41-4212-bfec-1c326a96cd7a",
        "parentId" : "8c7e7d62-5f8f-4e74-b4a5-27cf5b24f4de",
        "authorId" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "body" : "Resolved in a5469c9eb4cce673e42b0a7f3043a0d3fb767d82",
        "createdAt" : "2019-11-30T21:23:16Z",
        "updatedAt" : "2020-03-17T14:17:29Z",
        "lastEditedBy" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "tags" : [
        ]
      }
    ],
    "commit" : "42933f2e3f6eb699745e399a1fc9f05288603d79",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +1222,1226 @@    } else if (rawDataSize.isDefined && rawDataSize.get > 0) {\n      Some(CatalogStatistics(\n        sizeInBytes = rawDataSize.get,\n        deserFactor = None,\n        rowCount = rowCount.filter(_ > 0)))"
  },
  {
    "id" : "f39c7750-b65d-472e-874c-53a98369b546",
    "prId" : 25883,
    "prUrl" : "https://github.com/apache/spark/pull/25883#pullrequestreview-293660177",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e8c033fd-c76d-4d73-b100-a740777e398d",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "[HIVE-8472](https://issues.apache.org/jira/browse/HIVE-8472)'s `Fixed Versions` are 3.0.0, 2.4.0, and 2.2.1, aren't they? I didn't test them by myself.",
        "createdAt" : "2019-09-22T22:56:24Z",
        "updatedAt" : "2019-09-26T11:28:40Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "f5d777a3-753a-41bf-9ce4-6081617260b7",
        "parentId" : "e8c033fd-c76d-4d73-b100-a740777e398d",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "Hive 3.0:\r\n```\r\nhive> select version();\r\nOK\r\n3.0.0 rce61711a5fa54ab34fc74d86d521ecaeea6b072a\r\nTime taken: 2.505 seconds, Fetched: 1 row(s)\r\nhive> create database db1;\r\nOK\r\nTime taken: 0.051 seconds\r\nhive> desc database db1;\r\nOK\r\ndb1\t\tfile:/user/hive/warehouse/db1.db\troot\tUSER\r\nTime taken: 0.019 seconds, Fetched: 1 row(s)\r\nhive> ALTER DATABASE db1 SET LOCATION 'file:/tmp/default';\r\nOK\r\nTime taken: 0.043 seconds\r\nhive> desc database db1;\r\nOK\r\ndb1\t\tfile:/tmp/default\troot\tUSER\r\nTime taken: 0.01 seconds, Fetched: 1 row(s)\r\n```\r\n\r\nHive 3.1:\r\n```\r\nhive> select version();\r\nOK\r\n3.1.1 rf4e0529634b6231a0072295da48af466cf2f10b7\r\nTime taken: 0.096 seconds, Fetched: 1 row(s)\r\nhive> create database db1;\r\nOK\r\nTime taken: 0.014 seconds\r\nhive> desc database db1;\r\nOK\r\ndb1\t\tfile:/user/hive/warehouse/db1.db\troot\tUSER\r\nTime taken: 0.01 seconds, Fetched: 1 row(s)\r\nhive> ALTER DATABASE db1 SET LOCATION 'file:/tmp/default';\r\nOK\r\nTime taken: 0.016 seconds\r\nhive> desc database db1;\r\nOK\r\ndb1\t\tfile:/tmp/default\troot\tUSER\r\nTime taken: 0.011 seconds, Fetched: 1 row(s)\r\n```",
        "createdAt" : "2019-09-23T03:03:13Z",
        "updatedAt" : "2019-09-26T11:28:40Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      },
      {
        "id" : "df35d0c5-73da-46d9-b5a5-db37dbdf6c2b",
        "parentId" : "e8c033fd-c76d-4d73-b100-a740777e398d",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "? @wangyum . I asked 2.4.0 and 2.2.1. Why do you show the result on 3.0 and 3.1?",
        "createdAt" : "2019-09-23T22:53:26Z",
        "updatedAt" : "2019-09-26T11:28:40Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "7a559df8-43d9-4169-ae65-3c1beeb9f97b",
        "parentId" : "e8c033fd-c76d-4d73-b100-a740777e398d",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "2.4.0 and 2.2.1 haven't released.",
        "createdAt" : "2019-09-24T01:11:44Z",
        "updatedAt" : "2019-09-26T11:28:40Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      },
      {
        "id" : "3191967f-a0b5-4fcd-8433-952e2063df23",
        "parentId" : "e8c033fd-c76d-4d73-b100-a740777e398d",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Usually, we need IDed TODO here.",
        "createdAt" : "2019-09-25T04:39:08Z",
        "updatedAt" : "2019-09-26T11:28:40Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "c5752478-818c-4bc8-bcb8-ecb6942aeb63",
        "parentId" : "e8c033fd-c76d-4d73-b100-a740777e398d",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "Done. https://issues.apache.org/jira/browse/SPARK-29260",
        "createdAt" : "2019-09-26T11:30:08Z",
        "updatedAt" : "2019-09-26T11:30:08Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      }
    ],
    "commit" : "43fe00208f00da456f09c6080b8f50e72b5584b5",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +366,370 @@    if (!getDatabase(database.name).locationUri.equals(database.locationUri)) {\n      // SPARK-29260: Enable supported versions once it support altering database location.\n      if (!(version.equals(hive.v3_0) || version.equals(hive.v3_1))) {\n        throw new AnalysisException(\n          s\"Hive ${version.fullVersion} does not support altering database location\")"
  },
  {
    "id" : "ec68c1c7-ae3c-4bba-b020-77878493caac",
    "prId" : 25775,
    "prUrl" : "https://github.com/apache/spark/pull/25775#pullrequestreview-294094446",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "889b84c0-9706-4406-bdb9-814fa796a4c3",
        "parentId" : null,
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "@AngersZhuuuu I think we need to add another test case similar to [`HiveSparkSubmitSuite#SPARK-8368: includes jars passed in through --jars`](https://github.com/apache/spark/blob/016e1b491c9099063df8e2e76bc58d64fc369490/sql/hive/src/test/scala/org/apache/spark/sql/hive/HiveSparkSubmitSuite.scala#L110-L127) but with `spark.sql.hive.metastore.version=3.1` and `spark.sql.hive.metastore.jars=maven`.",
        "createdAt" : "2019-09-26T02:04:57Z",
        "updatedAt" : "2019-09-26T02:31:08Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      },
      {
        "id" : "a3b27f68-3b41-48df-acc8-9201312c5789",
        "parentId" : "889b84c0-9706-4406-bdb9-814fa796a4c3",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "Sorry. It's another bug: https://issues.apache.org/jira/browse/SPARK-29254",
        "createdAt" : "2019-09-26T05:44:08Z",
        "updatedAt" : "2019-09-26T05:44:08Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      },
      {
        "id" : "8fc2720f-e241-474f-b9ed-411ee8420b39",
        "parentId" : "889b84c0-9706-4406-bdb9-814fa796a4c3",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "@wangyum are you saying this change shouldn't be merged yet, or there is a different issue, and this change is OK?",
        "createdAt" : "2019-09-26T12:26:36Z",
        "updatedAt" : "2019-09-26T12:26:36Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "06ec529d-d571-425d-8712-86215fd51d51",
        "parentId" : "889b84c0-9706-4406-bdb9-814fa796a4c3",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "SPARK-29254 is a different issue.",
        "createdAt" : "2019-09-27T03:01:17Z",
        "updatedAt" : "2019-09-27T03:01:18Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      }
    ],
    "commit" : "f37335cda7602934271695c878584292cf6c9251",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +198,202 @@      state.getConf.setClassLoader(clientLoader.classLoader)\n    }\n    SessionState.start(state)\n    state.out = new PrintStream(outputBuffer, true, UTF_8.name())\n    state.err = new PrintStream(outputBuffer, true, UTF_8.name())"
  },
  {
    "id" : "b67cd8e1-cefc-4cd4-b01b-3c2adf1460ca",
    "prId" : 25775,
    "prUrl" : "https://github.com/apache/spark/pull/25775#pullrequestreview-293587598",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5950599d-dc50-40f6-9488-36315bcf8657",
        "parentId" : null,
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "@gatorsmile @srowen  @HyukjinKwon @dongjoon-hyun @juliuszsompolski Do you have more comments?",
        "createdAt" : "2019-09-26T05:52:21Z",
        "updatedAt" : "2019-09-26T05:52:22Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      },
      {
        "id" : "3d62c8c1-9092-4d9d-b578-566d2ad8366b",
        "parentId" : "5950599d-dc50-40f6-9488-36315bcf8657",
        "authorId" : "e5beb795-ac0e-4bd6-ad66-708215b8ae58",
        "body" : "No opinion from me, I know too little context of how these classloaders should be used to review.",
        "createdAt" : "2019-09-26T09:12:43Z",
        "updatedAt" : "2019-09-26T09:12:44Z",
        "lastEditedBy" : "e5beb795-ac0e-4bd6-ad66-708215b8ae58",
        "tags" : [
        ]
      }
    ],
    "commit" : "f37335cda7602934271695c878584292cf6c9251",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +197,201 @@    if (HiveUtils.isHive23) {\n      state.getConf.setClassLoader(clientLoader.classLoader)\n    }\n    SessionState.start(state)\n    state.out = new PrintStream(outputBuffer, true, UTF_8.name())"
  },
  {
    "id" : "87ff3142-ab71-4518-b15b-8467eca8d83e",
    "prId" : 25775,
    "prUrl" : "https://github.com/apache/spark/pull/25775#pullrequestreview-578161658",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8292d37a-7dd7-4815-90b8-baf5ad5395cf",
        "parentId" : null,
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "Why not set the classLoader blindly to `clientLoader.classLoader`? HIVE-11878 got merged into hive 1.3.0 and 2.0.0? This issue can still exist in the production environment when we try to use non-builtin hive libraries.\r\n\r\ncc @ulysses-you @cloud-fan @wangyum @AngersZhuuuu ",
        "createdAt" : "2021-01-28T09:57:35Z",
        "updatedAt" : "2021-01-28T09:57:35Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "f37335cda7602934271695c878584292cf6c9251",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +195,199 @@    // For this reason we cannot load the jars added by ADDJarCommand because of class loader\n    // got changed. We reset it to clientLoader.ClassLoader here.\n    if (HiveUtils.isHive23) {\n      state.getConf.setClassLoader(clientLoader.classLoader)\n    }"
  },
  {
    "id" : "f67a525a-6b9d-4258-99c9-33bedac85cbb",
    "prId" : 25696,
    "prUrl" : "https://github.com/apache/spark/pull/25696#pullrequestreview-288990258",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "91a434fb-f117-4abc-a2c3-8fb9d1557d20",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Can we change user name at runtime? If we can then it should be `def` instead of `val`.",
        "createdAt" : "2019-09-17T02:11:52Z",
        "updatedAt" : "2019-09-17T02:30:11Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "76af311d-bedc-4a52-a431-80e5bcb43435",
        "parentId" : "91a434fb-f117-4abc-a2c3-8fb9d1557d20",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "I'm not 100% sure, but we have been defining userName as `val` from Spark 2.2.0, starting from this commit https://github.com/apache/spark/commit/344f38b04b271b5f3ec2748b34db4e52d54da1bc\r\n\r\nChanging it to def is safer anyway (can handle both cases - whether user name is changed or not) so please let me know if we would like to make change.",
        "createdAt" : "2019-09-17T02:22:14Z",
        "updatedAt" : "2019-09-17T02:30:11Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "e6405d0e-cb70-41e4-8238-2594d24979ab",
        "parentId" : "91a434fb-f117-4abc-a2c3-8fb9d1557d20",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Btw, that would be better to be handled via separate PR if necessary, to isolate two different things \"adding test\" and \"fixing logic\".",
        "createdAt" : "2019-09-17T02:24:20Z",
        "updatedAt" : "2019-09-17T02:30:11Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "015815d3-906e-4bd2-b35c-4c706ec8f15a",
        "parentId" : "91a434fb-f117-4abc-a2c3-8fb9d1557d20",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "yea, we can investigate it in a separated pr",
        "createdAt" : "2019-09-17T02:28:30Z",
        "updatedAt" : "2019-09-17T02:30:11Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "ef220efecf44a4b569de9e5a8a8147170692954f",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +223,227 @@  }\n\n  override val userName = UserGroupInformation.getCurrentUser.getShortUserName\n\n  override def getConf(key: String, defaultValue: String): String = {"
  },
  {
    "id" : "0f9f5b7f-3a2a-4794-abbf-c4da608eabd5",
    "prId" : 25197,
    "prUrl" : "https://github.com/apache/spark/pull/25197#pullrequestreview-269499885",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6bcc6b7c-7605-4b91-8090-1851d4bd77bc",
        "parentId" : null,
        "authorId" : "1a7af33a-9000-4d4e-9729-ac32a9505710",
        "body" : "If we set `fs.hdfs.impl.disable.cache=true` in Hadoop conf ([PR#25058](https://github.com/apache/spark/pull/25058)), I think results are the same. FileSystem will be created when we get a new session.\r\nSo maybe as @jerryshao commented, it's unnecessary to do code changes in Spark?",
        "createdAt" : "2019-07-19T09:50:55Z",
        "updatedAt" : "2019-07-19T09:52:21Z",
        "lastEditedBy" : "1a7af33a-9000-4d4e-9729-ac32a9505710",
        "tags" : [
        ]
      },
      {
        "id" : "86db031a-4209-4d36-b879-ce74521dfa28",
        "parentId" : "6bcc6b7c-7605-4b91-8090-1851d4bd77bc",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Is this the right place for this? HDFS can be used without `hive` module.",
        "createdAt" : "2019-07-19T17:31:21Z",
        "updatedAt" : "2019-07-19T17:31:21Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "b9372026-ecaf-4530-a9ae-8431ff161bf8",
        "parentId" : "6bcc6b7c-7605-4b91-8090-1851d4bd77bc",
        "authorId" : "8afcf008-6988-44a5-92fa-2bef54bb4058",
        "body" : "@Deegue , if `fs.hdfs.impl.disable.cache=true`, this issue(SPARK-21067) would never happen, and this code has nothing effect.\r\nPls correct me if i'm wrong.",
        "createdAt" : "2019-08-01T09:18:07Z",
        "updatedAt" : "2019-08-01T09:18:07Z",
        "lastEditedBy" : "8afcf008-6988-44a5-92fa-2bef54bb4058",
        "tags" : [
        ]
      },
      {
        "id" : "f9c2e09b-9b97-4d2a-b5b4-3ba4c96e65de",
        "parentId" : "6bcc6b7c-7605-4b91-8090-1851d4bd77bc",
        "authorId" : "8afcf008-6988-44a5-92fa-2bef54bb4058",
        "body" : "@dongjoon-hyun , ehmm, I think it's the \"right\" place. The issue is because spark hive module doesn't use the HdfsEncryptionShim **carefully**. As for other modules, that's another thing.",
        "createdAt" : "2019-08-01T09:27:22Z",
        "updatedAt" : "2019-08-01T09:27:22Z",
        "lastEditedBy" : "8afcf008-6988-44a5-92fa-2bef54bb4058",
        "tags" : [
        ]
      }
    ],
    "commit" : "af6db068743072d79ba399f370a8e2242e24f54c",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +197,201 @@    // auto-closable, which would be closed when the session is closed. The subsequent sessions use\n    // the same FS will throw \"Filesystem closed Exception\".\n    state.getHdfsEncryptionShim\n    state.out = new PrintStream(outputBuffer, true, \"UTF-8\")\n    state.err = new PrintStream(outputBuffer, true, \"UTF-8\")"
  },
  {
    "id" : "bfb90d76-c20b-4051-b0e6-24a855c992ef",
    "prId" : 25085,
    "prUrl" : "https://github.com/apache/spark/pull/25085#pullrequestreview-293466255",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "aa061881-04df-4c0f-be7c-4e230659aaed",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I believe this is the fix we need. But I'm curious about when can this happen if hive forbids defining void type columns.",
        "createdAt" : "2019-09-17T06:32:33Z",
        "updatedAt" : "2019-09-30T03:10:42Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "d118bf01-45e8-4ad1-854b-8cfe71f62b4b",
        "parentId" : "aa061881-04df-4c0f-be7c-4e230659aaed",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "Fix this point,  hive void type will be converted to null type. \r\nSuch as, t1 have a void type column c1, when execute `show create table $tbl`.\r\nSpark:\r\n`create table t1 (c1 null)`\r\nHive:\r\n`create table t1 (c1 void)`\r\n\r\nAnd without fix this point, spark will throw an exception.",
        "createdAt" : "2019-09-25T05:36:50Z",
        "updatedAt" : "2019-09-30T03:10:42Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      },
      {
        "id" : "497a0e2a-174e-41c5-8e6a-24fd4e07a3f3",
        "parentId" : "aa061881-04df-4c0f-be7c-4e230659aaed",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "> But I'm curious about when can this happen if hive forbids defining void type columns.\r\n\r\nCan hive do `create table t1 (c1 void)`?",
        "createdAt" : "2019-09-25T07:47:03Z",
        "updatedAt" : "2019-09-30T03:10:42Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "80c52168-ba33-46d8-8cc6-48b52f81bfd7",
        "parentId" : "aa061881-04df-4c0f-be7c-4e230659aaed",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "No, hive can not but hive can do `create table t1 as select null as c1`.",
        "createdAt" : "2019-09-25T08:01:52Z",
        "updatedAt" : "2019-09-30T03:10:42Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      },
      {
        "id" : "67414504-700e-477d-8eee-ac21dee00060",
        "parentId" : "aa061881-04df-4c0f-be7c-4e230659aaed",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "can we make this fix surgical and only keep this change?",
        "createdAt" : "2019-09-25T08:10:51Z",
        "updatedAt" : "2019-09-30T03:10:42Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "27563c72-33e3-4a0a-8c82-b55bcadbeced",
        "parentId" : "aa061881-04df-4c0f-be7c-4e230659aaed",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "How about add code to check and change null to void at `ShowCreateTableCommand.showCreateHiveTable()` ?",
        "createdAt" : "2019-09-25T09:02:53Z",
        "updatedAt" : "2019-09-30T03:10:42Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      },
      {
        "id" : "96c7da1a-1c18-40f9-8c2b-bdc66fc64c01",
        "parentId" : "aa061881-04df-4c0f-be7c-4e230659aaed",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Does it have to be in this PR? If it has to, I'm ok with it.",
        "createdAt" : "2019-09-25T14:42:18Z",
        "updatedAt" : "2019-09-30T03:10:42Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "f6ff3193-6c8b-41ff-aeb9-d0a2b2ed2f24",
        "parentId" : "aa061881-04df-4c0f-be7c-4e230659aaed",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "Yeah~ If you have a time, you could review the latest commit.",
        "createdAt" : "2019-09-26T03:23:55Z",
        "updatedAt" : "2019-09-30T03:10:42Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      }
    ],
    "commit" : "aa0865815d3e0ef54175b5e598b175b77a5562e8",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +982,986 @@      hc.getType match {\n        // SPARK-28313 compatible hive void type\n        case \"void\" => NullType\n        case other => CatalystSqlParser.parseDataType(other)\n      }"
  },
  {
    "id" : "ce779798-cd30-40c5-9c42-fbc913c6ec79",
    "prId" : 24592,
    "prUrl" : "https://github.com/apache/spark/pull/24592#pullrequestreview-238375565",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "252f79bb-9463-432f-94d6-dedeb620ad2d",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "Do we have the same issue for Hive indexes? Also, we normally also added a test case in the Version Suite. You can do it by using the method  `runSqlHive(sql: String)` to create materialized views. ",
        "createdAt" : "2019-05-15T23:30:03Z",
        "updatedAt" : "2019-05-15T23:30:03Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "10edd1c4-8e04-419f-9be9-c7d34fe9b7f8",
        "parentId" : "252f79bb-9463-432f-94d6-dedeb620ad2d",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "Test in another PR: https://github.com/apache/spark/pull/23992",
        "createdAt" : "2019-05-15T23:35:57Z",
        "updatedAt" : "2019-05-15T23:35:58Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      },
      {
        "id" : "c1279b3a-05f8-4c2f-aac5-39e18b9ef7e8",
        "parentId" : "252f79bb-9463-432f-94d6-dedeb620ad2d",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "This issue only occurs with `materialized views` in Hive 3.1.\r\nI have tested the `index` and the `view`.",
        "createdAt" : "2019-05-16T13:01:39Z",
        "updatedAt" : "2019-05-16T13:01:39Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      }
    ],
    "commit" : "953194d0f23ddfe370bcb7888377f6c60824fe4c",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +861,865 @@      .partition(_.getTableType.toString.equals(\"MATERIALIZED_VIEW\"))\n\n    // Remove materialized view first, otherwise caused a violation of foreign key constraint.\n    mvs.foreach { table =>\n      val t = table.getTableName"
  }
]