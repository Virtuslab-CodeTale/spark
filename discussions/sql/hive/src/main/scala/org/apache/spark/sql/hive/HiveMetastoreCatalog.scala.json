[
  {
    "id" : "da6ab290-2e49-4b26-bced-2905a94419f4",
    "prId" : 26499,
    "prUrl" : "https://github.com/apache/spark/pull/26499#pullrequestreview-316105305",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "eb51e24b-dfa5-4939-aa7a-8e05874dcff6",
        "parentId" : null,
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "Thank you @LantaoJin Could we add a test case?",
        "createdAt" : "2019-11-13T08:58:44Z",
        "updatedAt" : "2019-11-19T03:11:44Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      },
      {
        "id" : "233a406e-c660-47df-a103-5b66581e1f66",
        "parentId" : "eb51e24b-dfa5-4939-aa7a-8e05874dcff6",
        "authorId" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "body" : "`spark.sql.sources.*` can not add in SQL of `CREATE TABLE` in current Spark version. Any other way to go to this invalid code path?",
        "createdAt" : "2019-11-13T09:28:14Z",
        "updatedAt" : "2019-11-19T03:11:44Z",
        "lastEditedBy" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "tags" : [
        ]
      },
      {
        "id" : "9840eb86-f6cc-4b31-924f-dd5acbb5a67f",
        "parentId" : "eb51e24b-dfa5-4939-aa7a-8e05874dcff6",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "`sparkSession.metadataHive.runSqlHive`?\r\nhttps://github.com/apache/spark/blob/96179732aad28418d486de1a365cd5d68c3db910/sql/hive/src/test/scala/org/apache/spark/sql/hive/HiveMetastoreCatalogSuite.scala#L314-L320",
        "createdAt" : "2019-11-13T09:35:01Z",
        "updatedAt" : "2019-11-19T03:11:44Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      }
    ],
    "commit" : "3288ffddf66d0db058dcf7190992f4a0bf49445f",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +271,275 @@          s\"Set ${HiveUtils.CONVERT_METASTORE_PARQUET.key} to false, \" +\n          s\"or recreate table ${relation.tableMeta.identifier} to workaround.\")\n    }\n    val newOutput = result.output.zip(relation.output).map {\n      case (a1, a2) => a1.withExprId(a2.exprId)"
  },
  {
    "id" : "971cc0de-fc6d-4af4-8d17-3fb67353b2bd",
    "prId" : 25306,
    "prUrl" : "https://github.com/apache/spark/pull/25306#pullrequestreview-271737199",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0e1520dd-0ed9-4455-bb98-6d932edacd34",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "I'm not sure about this part. Why it should always use dynamic? Don't we need to respect `spark.sql.sources.partitionOverwriteMode`?",
        "createdAt" : "2019-08-07T05:19:09Z",
        "updatedAt" : "2019-09-02T09:27:30Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "d946ad2d-04e8-4d8c-b968-c913f5c643a4",
        "parentId" : "0e1520dd-0ed9-4455-bb98-6d932edacd34",
        "authorId" : "8e87861a-4202-49a2-baf7-6d51f6aaa5a2",
        "body" : "Please refer https://issues.apache.org/jira/browse/SPARK-20236.\r\nTL;DR version: dynamic follows hive's style, static is the spark style which is different with hive. When converting InsertIntoHiveTable, we definitely want the hive behaviour. So it takes preference over `spark.sql.sources.partitionOverwriteMode`",
        "createdAt" : "2019-08-07T05:46:24Z",
        "updatedAt" : "2019-09-02T09:27:30Z",
        "lastEditedBy" : "8e87861a-4202-49a2-baf7-6d51f6aaa5a2",
        "tags" : [
        ]
      }
    ],
    "commit" : "b80166badf5f63d0dffa7dd9eee57c0ff7b6c9f0",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +212,216 @@          // Spark SQL's data source table now support static and dynamic partition insert. Source\n          // table converted from Hive table should always use dynamic.\n          val enableDynamicPartition = options.updated(\"partitionOverwriteMode\", \"dynamic\")\n          val fsRelation = HadoopFsRelation(\n            location = fileIndex,"
  }
]