[
  {
    "id" : "7815e32d-c2b9-4e83-906a-e6ca510e1f54",
    "prId" : 32373,
    "prUrl" : "https://github.com/apache/spark/pull/32373#pullrequestreview-647757646",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e821bdbf-939a-4581-acbc-bc59487c342a",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Should we guard the change by `spark.driver.userClassPathFirst`?",
        "createdAt" : "2021-04-28T01:53:57Z",
        "updatedAt" : "2021-04-28T06:18:14Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "89845b77-86f9-431d-9678-d6a0c762f12c",
        "parentId" : "e821bdbf-939a-4581-acbc-bc59487c342a",
        "authorId" : "d63ec90d-34b5-4fb0-8626-865510da3afa",
        "body" : "It's already implied by ChildFirstURLClassLoader. Do you want to me check again in URLClassLoader?",
        "createdAt" : "2021-04-28T06:16:45Z",
        "updatedAt" : "2021-04-28T06:18:14Z",
        "lastEditedBy" : "d63ec90d-34b5-4fb0-8626-865510da3afa",
        "tags" : [
        ]
      },
      {
        "id" : "c49d112d-3415-4984-af84-8578fc813a0c",
        "parentId" : "e821bdbf-939a-4581-acbc-bc59487c342a",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Ohh okay I got what you mean. ",
        "createdAt" : "2021-04-29T01:20:49Z",
        "updatedAt" : "2021-04-29T01:20:49Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "cf169b1676759422d14390c9e32e0784efc3b298",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +399,403 @@          childFirst.getURLs() ++ allJars(Utils.getSparkClassLoader)\n        case urlClassLoader: URLClassLoader =>\n          allJars(urlClassLoader.getParent) ++ urlClassLoader.getURLs\n        case other => allJars(other.getParent)\n      }"
  },
  {
    "id" : "06e0aa12-0769-49e9-8d12-cd474ae2956f",
    "prId" : 32373,
    "prUrl" : "https://github.com/apache/spark/pull/32373#pullrequestreview-648813028",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7fc57359-b41b-4f28-9657-6050a04afae2",
        "parentId" : null,
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "I am 100% sure this change will 'guarantee' the class loading order. Because in `ChildFirstURLClassLoader`, we re-constructor it by setting the parent classloader to null and load classes with an order we explicitly set.",
        "createdAt" : "2021-04-30T02:20:35Z",
        "updatedAt" : "2021-04-30T02:21:23Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "cf169b1676759422d14390c9e32e0784efc3b298",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +399,403 @@          childFirst.getURLs() ++ allJars(Utils.getSparkClassLoader)\n        case urlClassLoader: URLClassLoader =>\n          allJars(urlClassLoader.getParent) ++ urlClassLoader.getURLs\n        case other => allJars(other.getParent)\n      }"
  },
  {
    "id" : "2a18f37a-fbe2-45e1-a984-2561ebdf10c5",
    "prId" : 31317,
    "prUrl" : "https://github.com/apache/spark/pull/31317#pullrequestreview-575978395",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "82a52666-a122-4f33-ae93-bd10d608e74e",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Is it a typo above?\r\n\r\n```\r\nThis property can be one of four options: \"\r\n```\r\n\r\n->\r\n\r\n```\r\nThis property can be one of four options:\r\n```\r\n?",
        "createdAt" : "2021-01-26T02:42:12Z",
        "updatedAt" : "2021-01-26T03:09:00Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "d9c64f94-3cdc-4f76-9860-e5a812c90d4f",
        "parentId" : "82a52666-a122-4f33-ae93-bd10d608e74e",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "<img width=\"753\" alt=\"截屏2021-01-26 上午10 59 28\" src=\"https://user-images.githubusercontent.com/46485123/105794249-981a7980-5fc5-11eb-8490-6fdd9e0fb17c.png\">\r\n\r\nYea..a typo",
        "createdAt" : "2021-01-26T03:00:00Z",
        "updatedAt" : "2021-01-26T03:09:00Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "66006722-ef63-4fb7-91d8-c4bc94e007b3",
        "parentId" : "82a52666-a122-4f33-ae93-bd10d608e74e",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "Fixed",
        "createdAt" : "2021-01-26T03:12:22Z",
        "updatedAt" : "2021-01-26T03:12:22Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "75bbdd871b414b00118e9c9fc6348f10bd626af4",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +87,91 @@      |   Use Hive jars of specified version downloaded from Maven repositories.\n      | 3. \"path\"\n      |   Use Hive jars configured by `spark.sql.hive.metastore.jars.path`\n      |   in comma separated format. Support both local or remote paths.The provided jars\n      |   should be the same version as ${HIVE_METASTORE_VERSION}."
  },
  {
    "id" : "f5dbb919-7086-4d49-be5c-d01590cc7dc4",
    "prId" : 31030,
    "prUrl" : "https://github.com/apache/spark/pull/31030#pullrequestreview-562308368",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6e22753b-a761-42c8-bc16-1dc5627f0e46",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@dh20, which case are you trying to elabourate? ",
        "createdAt" : "2021-01-05T09:37:49Z",
        "updatedAt" : "2021-01-12T03:39:55Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "fd72ed94-0fee-424f-9d8c-ba1a9910003c",
        "parentId" : "6e22753b-a761-42c8-bc16-1dc5627f0e46",
        "authorId" : "727a0ed3-3ae9-4a2e-bf11-94858ae239f3",
        "body" : "@HyukjinKwon When I query an orc format hive table and overwrite the query result into this orc table, sparksql will throw this error -> [org.apache.spark.sql.AnalysisException: Cannot overwrite a path that is also being read from], when I set this parameter to false, Hql will execute normally",
        "createdAt" : "2021-01-05T09:46:51Z",
        "updatedAt" : "2021-01-12T03:39:55Z",
        "lastEditedBy" : "727a0ed3-3ae9-4a2e-bf11-94858ae239f3",
        "tags" : [
        ]
      },
      {
        "id" : "cf0408a3-a635-4e9e-8203-e1ee7cae16e2",
        "parentId" : "6e22753b-a761-42c8-bc16-1dc5627f0e46",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I think it's weird to overwrite a path is being read from. Is it something officially supported from Hive?",
        "createdAt" : "2021-01-05T10:06:28Z",
        "updatedAt" : "2021-01-12T03:39:55Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "9bfbf1f5-3a6f-479d-9ce2-aa7d1b9e1715",
        "parentId" : "6e22753b-a761-42c8-bc16-1dc5627f0e46",
        "authorId" : "727a0ed3-3ae9-4a2e-bf11-94858ae239f3",
        "body" : "@HyukjinKwon Yes, Hive officially supported this function. This SQL was originally run on hive, and I switched sparksql before this problem appeared",
        "createdAt" : "2021-01-06T00:47:29Z",
        "updatedAt" : "2021-01-12T03:39:55Z",
        "lastEditedBy" : "727a0ed3-3ae9-4a2e-bf11-94858ae239f3",
        "tags" : [
        ]
      },
      {
        "id" : "b3887832-586e-4456-a3b0-d1441bc61c5b",
        "parentId" : "6e22753b-a761-42c8-bc16-1dc5627f0e46",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I think it's fine enough to say we use Spark's when `spark.sql.hive.convertMetastoreOrc` is enabled. Otherwise, we will have to list up every behaviour differences between Spark and Hive in this configuration doc.",
        "createdAt" : "2021-01-06T00:49:12Z",
        "updatedAt" : "2021-01-12T03:39:55Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "25b7c12f-0c9d-4fab-9c67-7f9bc2bb4d10",
        "parentId" : "6e22753b-a761-42c8-bc16-1dc5627f0e46",
        "authorId" : "727a0ed3-3ae9-4a2e-bf11-94858ae239f3",
        "body" : "@HyukjinKwon Yes, I think so, but now the official explanation of spark for this parameter is aimed at the difference between spark Orc vector readers, so I think its explanation should be improved",
        "createdAt" : "2021-01-06T01:09:10Z",
        "updatedAt" : "2021-01-12T03:39:55Z",
        "lastEditedBy" : "727a0ed3-3ae9-4a2e-bf11-94858ae239f3",
        "tags" : [
        ]
      },
      {
        "id" : "4dd44c53-187d-4d38-bac2-b4eb72afa6b7",
        "parentId" : "6e22753b-a761-42c8-bc16-1dc5627f0e46",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "The doc already says that \"the built-in ORC reader\" vs \"Hive serde\". I think it's fine.",
        "createdAt" : "2021-01-06T01:11:39Z",
        "updatedAt" : "2021-01-12T03:39:55Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "275a5e87-ab4e-4edb-b1ca-5a8c720396cb",
        "parentId" : "6e22753b-a761-42c8-bc16-1dc5627f0e46",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "@dh20 . It seems that you want to describe one of the differences between Spark data source tables and Hive serde tables. May I ask a few questions?\r\n\r\n- Is the situation different from `Parquet`? Parquet also has the same conf `spark.sql.hive.convertMetastoreParquet`.\r\n\r\n- Is the added statement correct always? Specifically, the following statement looks wrong in some situations. For example, did you try to use `PartitionOverwriteMode.DYNAMIC`?\r\n> When I query an orc format hive table and overwrite the query result into this orc table, sparksql will throw this error -> [org.apache.spark.sql.AnalysisException: Cannot overwrite a path that is also being read from], when I set this parameter to false, Hql will execute normally\r\n\r\n- Which Apache Spark version are you using? The situation will be changed according to the Spark versions. For example,\r\n  - SPARK-30112 implemented `Allow insert overwrite same table if using dynamic partition overwrite` at Apache Spark 3.0.0. \r\n  - SPARK-33887 is also trying to implement `Allow insert overwrite same table with static partition if using dynamic partition overwrite mode` at Apache Spark 3.2.0.",
        "createdAt" : "2021-01-06T01:48:19Z",
        "updatedAt" : "2021-01-12T03:39:55Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "f52a1d6a-c4fe-43fd-9ace-94323e584d31",
        "parentId" : "6e22753b-a761-42c8-bc16-1dc5627f0e46",
        "authorId" : "727a0ed3-3ae9-4a2e-bf11-94858ae239f3",
        "body" : "@dongjoon-hyun Hello, I'll answer your questions one by one.\r\n\r\n- 1.When the format of the table is parquet, there is no problem to insert and overlay the same table, no matter whether this parameter is added or not【 spark.sql.hive .convertMetastoreParquet=false】\r\n\r\n- 2.[PartitionOverwriteMode.DYNAMIC] It can't solve the problem\r\n\r\n- 3.I use spark 3.0.0, spark-30112 does not solve this problem.\r\n\r\n\r\n.",
        "createdAt" : "2021-01-06T03:18:01Z",
        "updatedAt" : "2021-01-12T03:39:55Z",
        "lastEditedBy" : "727a0ed3-3ae9-4a2e-bf11-94858ae239f3",
        "tags" : [
        ]
      },
      {
        "id" : "6799a778-d3a1-4c9c-ae29-cd646f42a845",
        "parentId" : "6e22753b-a761-42c8-bc16-1dc5627f0e46",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Could you file a JIRA for this with your specific procedure? This one is a valid bug.\r\n> When the format of the table is parquet, there is no problem to insert and overlay the same table, no matter whether this parameter is added or not【 spark.sql.hive .convertMetastoreParquet=false】\r\n\r\nFor this one, I think your statement in this PR is too broader because we have a unit test coverage with Dynamic which was added by SPARK-30112. When you say `this problem`, you should be specific to avoid ambiguity.\r\n> [PartitionOverwriteMode.DYNAMIC] It can't solve the problem\r\n> I use spark 3.0.0, spark-30112 does not solve this problem.",
        "createdAt" : "2021-01-06T03:48:56Z",
        "updatedAt" : "2021-01-12T03:39:55Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "bf56322679c1d15649663828bdb1c5e270ab49e2",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +133,137 @@      \"ORC tables created by using the HiveQL syntax, instead of Hive serde.\" +\n      \"Enabling this parameter will cause insert overwrite when reading the table,\" +\n      \"and the file format of this table is ORC This error will be thrown[], \" +\n      \"Users can set spark.sql.hive.convertMetastoreOrc=false to solve this error.\")\n    .version(\"2.0.0\")"
  },
  {
    "id" : "87bf25ad-b86b-4e82-8a62-f531c18df458",
    "prId" : 30407,
    "prUrl" : "https://github.com/apache/spark/pull/30407#pullrequestreview-533185015",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c286524c-6b19-4982-b38d-3c9b925aa108",
        "parentId" : null,
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "LGTM",
        "createdAt" : "2020-11-18T08:05:09Z",
        "updatedAt" : "2020-11-18T08:14:06Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "45922bda6290dd7f34b4d719c18a5cc577df5cb4",
    "line" : 26,
    "diffHunk" : "@@ -1,1 +108,112 @@      | 1. file://path/to/jar/*,file://path2/to/jar/*/*.jar\n      | 2. hdfs://nameservice/path/to/jar/*,hdfs://nameservice2/path/to/jar/*/*.jar\n      \"\"\".stripMargin)\n    .version(\"3.1.0\")\n    .stringConf"
  },
  {
    "id" : "c2afcc66-a4e8-418a-8765-bbe1e90dbc02",
    "prId" : 29881,
    "prUrl" : "https://github.com/apache/spark/pull/29881#pullrequestreview-508045826",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "67078280-79e3-4afe-9620-f421dd60f5a0",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "`This property can be one of three options` should be updated to use `four options`",
        "createdAt" : "2020-10-14T06:44:16Z",
        "updatedAt" : "2020-10-22T07:29:23Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "b48174d9-118a-4550-88db-3100edeb5f8f",
        "parentId" : "67078280-79e3-4afe-9620-f421dd60f5a0",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "Done",
        "createdAt" : "2020-10-14T06:57:58Z",
        "updatedAt" : "2020-10-22T07:29:23Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "ea9ef2bbfd8c2314605ae8e4daa12d2b77b31e62",
    "line" : 25,
    "diffHunk" : "@@ -1,1 +93,97 @@      |   Use Hive jars configured by `spark.sql.hive.metastore.jars.path`\n      |   in comma separated format. Support both local or remote paths.\n      | 4. A classpath in the standard format for both Hive and Hadoop.\n      \"\"\".stripMargin)\n    .version(\"1.4.0\")"
  },
  {
    "id" : "491d29b2-dba8-4d98-b1df-16b7abf4aac3",
    "prId" : 29881,
    "prUrl" : "https://github.com/apache/spark/pull/29881#pullrequestreview-508046859",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "708743af-d20a-43d3-9fae-2786b75008ad",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "So `IsolatedClientLoader` already supports remote JAR paths?",
        "createdAt" : "2020-10-14T06:53:02Z",
        "updatedAt" : "2020-10-22T07:29:23Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "c49cb919-a405-4bf7-9632-735ef47dd79d",
        "parentId" : "708743af-d20a-43d3-9fae-2786b75008ad",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> So `IsolatedClientLoader` already supports remote JAR paths?\r\n\r\nadd new config reason is https://github.com/apache/spark/pull/29881#discussion_r496661990",
        "createdAt" : "2020-10-14T06:59:45Z",
        "updatedAt" : "2020-10-22T07:29:23Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "ea9ef2bbfd8c2314605ae8e4daa12d2b77b31e62",
    "line" : 122,
    "diffHunk" : "@@ -1,1 +464,468 @@        sparkConf = conf,\n        hadoopConf = hadoopConf,\n        execJars = jars.toSeq,\n        config = configurations,\n        isolationOn = true,"
  },
  {
    "id" : "dcabc071-04c7-47d7-a326-ab0b7d048cae",
    "prId" : 29881,
    "prUrl" : "https://github.com/apache/spark/pull/29881#pullrequestreview-509239914",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8ef3c0a4-a429-4739-8ab6-81f12185bad9",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I don't know windows very well. cc @HyukjinKwon ",
        "createdAt" : "2020-10-15T06:53:42Z",
        "updatedAt" : "2020-10-22T07:29:23Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "44e2dcbe-c1a1-495d-9ecc-5e81000ed951",
        "parentId" : "8ef3c0a4-a429-4739-8ab6-81f12185bad9",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> I don't know windows very well. cc @HyukjinKwon\r\n\r\nsee https://github.com/apache/spark/blob/77a8efbc05cb4ecc40dd050c363429e71a9f23c1/core/src/main/scala/org/apache/spark/SparkContext.scala#L1902",
        "createdAt" : "2020-10-15T06:57:32Z",
        "updatedAt" : "2020-10-22T07:29:23Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "81ebfd54-2d19-487b-b537-c9ca7579b683",
        "parentId" : "8ef3c0a4-a429-4739-8ab6-81f12185bad9",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Yeah, this is fine.",
        "createdAt" : "2020-10-15T10:33:36Z",
        "updatedAt" : "2020-10-22T07:29:23Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "ea9ef2bbfd8c2314605ae8e4daa12d2b77b31e62",
    "line" : 103,
    "diffHunk" : "@@ -1,1 +445,449 @@        HiveUtils.hiveMetastoreJarsPath(sqlConf)\n          .flatMap {\n            case path if path.contains(\"\\\\\") && Utils.isWindows =>\n              addLocalHiveJars(new File(path))\n            case path =>"
  },
  {
    "id" : "414f7f56-0b4d-413c-851d-b0c7e7736ce5",
    "prId" : 28042,
    "prUrl" : "https://github.com/apache/spark/pull/28042#pullrequestreview-382592046",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c597362e-91f8-47a7-a166-c96484fd0af5",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-6908, commit ID: 05454fd8aef75b129cbbd0288f5089c5259f4a15#diff-ff50aea397a607b79df9bec6f2a841db",
        "createdAt" : "2020-03-27T05:42:24Z",
        "updatedAt" : "2020-03-27T05:42:25Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "3b3cb8c1cae88575c7c0bcf98a20d5d06f479267",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +66,70 @@        \"<code>0.12.0</code> through <code>2.3.6</code> and \" +\n        \"<code>3.0.0</code> through <code>3.1.2</code>.\")\n    .version(\"1.4.0\")\n    .stringConf\n    .createWithDefault(builtinHiveVersion)"
  },
  {
    "id" : "f2bc1347-1689-4ec7-b1e4-6467ca3d2427",
    "prId" : 28042,
    "prUrl" : "https://github.com/apache/spark/pull/28042#pullrequestreview-382592163",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5e5b5bc8-afdf-43cd-881a-233aa117c4d5",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-3971, commit ID: 64945f868443fbc59cb34b34c16d782dda0fb63d#diff-12fa2178364a810b3262b30d8d48aa2d",
        "createdAt" : "2020-03-27T05:42:48Z",
        "updatedAt" : "2020-03-27T05:42:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "3b3cb8c1cae88575c7c0bcf98a20d5d06f479267",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +75,79 @@  val FAKE_HIVE_VERSION = buildConf(\"spark.sql.hive.version\")\n    .doc(s\"deprecated, please use ${HIVE_METASTORE_VERSION.key} to get the Hive version in Spark.\")\n    .version(\"1.1.1\")\n    .stringConf\n    .createWithDefault(builtinHiveVersion)"
  },
  {
    "id" : "2a3ba60a-fc3d-4db0-ba99-78226abe3172",
    "prId" : 28042,
    "prUrl" : "https://github.com/apache/spark/pull/28042#pullrequestreview-382592244",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ea282585-f1c3-4e7d-bbd2-eeb317a6b039",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-6908, commit ID: 05454fd8aef75b129cbbd0288f5089c5259f4a15#diff-ff50aea397a607b79df9bec6f2a841db",
        "createdAt" : "2020-03-27T05:43:08Z",
        "updatedAt" : "2020-03-27T05:43:08Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "3b3cb8c1cae88575c7c0bcf98a20d5d06f479267",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +92,96 @@      | 3. A classpath in the standard format for both Hive and Hadoop.\n      \"\"\".stripMargin)\n    .version(\"1.4.0\")\n    .stringConf\n    .createWithDefault(\"builtin\")"
  },
  {
    "id" : "17322802-09f7-4b64-a5cf-c513b42bf852",
    "prId" : 28042,
    "prUrl" : "https://github.com/apache/spark/pull/28042#pullrequestreview-382592346",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "29a7af7f-20bc-48ef-b532-e487f6f5a070",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-2406, commit ID: cc4015d2fa3785b92e6ab079b3abcf17627f7c56#diff-ff50aea397a607b79df9bec6f2a841db",
        "createdAt" : "2020-03-27T05:43:29Z",
        "updatedAt" : "2020-03-27T05:43:29Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "3b3cb8c1cae88575c7c0bcf98a20d5d06f479267",
    "line" : 27,
    "diffHunk" : "@@ -1,1 +99,103 @@    .doc(\"When set to true, the built-in Parquet reader and writer are used to process \" +\n      \"parquet tables created by using the HiveQL syntax, instead of Hive serde.\")\n    .version(\"1.1.1\")\n    .booleanConf\n    .createWithDefault(true)"
  },
  {
    "id" : "966c6a8b-7364-40ba-9fce-5d4062c1faa5",
    "prId" : 28042,
    "prUrl" : "https://github.com/apache/spark/pull/28042#pullrequestreview-382592440",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4bcf62f7-f8dd-4f62-8150-b8973572f163",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-6575, commit ID: 778c87686af0c04df9dfe144b8f744f271a988ad#diff-ff50aea397a607b79df9bec6f2a841db",
        "createdAt" : "2020-03-27T05:43:50Z",
        "updatedAt" : "2020-03-27T05:43:51Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "3b3cb8c1cae88575c7c0bcf98a20d5d06f479267",
    "line" : 35,
    "diffHunk" : "@@ -1,1 +108,112 @@        \"different Parquet data files. This configuration is only effective \" +\n        \"when \\\"spark.sql.hive.convertMetastoreParquet\\\" is true.\")\n      .version(\"1.3.1\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "c86a931e-43be-4539-8dfd-586ea4c88e44",
    "prId" : 28042,
    "prUrl" : "https://github.com/apache/spark/pull/28042#pullrequestreview-382592544",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1b06ee40-18d1-4d49-8098-88fc04fad6e3",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-14070, commit ID: 1e886159849e3918445d3fdc3c4cef86c6c1a236#diff-ff50aea397a607b79df9bec6f2a841db",
        "createdAt" : "2020-03-27T05:44:10Z",
        "updatedAt" : "2020-03-27T05:44:10Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "3b3cb8c1cae88575c7c0bcf98a20d5d06f479267",
    "line" : 42,
    "diffHunk" : "@@ -1,1 +115,119 @@    .doc(\"When set to true, the built-in ORC reader and writer are used to process \" +\n      \"ORC tables created by using the HiveQL syntax, instead of Hive serde.\")\n    .version(\"2.0.0\")\n    .booleanConf\n    .createWithDefault(true)"
  },
  {
    "id" : "a51c311c-062d-4ff9-9446-539d688aaa4f",
    "prId" : 28042,
    "prUrl" : "https://github.com/apache/spark/pull/28042#pullrequestreview-382592649",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e0ec8d4b-265f-4d81-b361-35e8b0f784fc",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-28573, commit ID: d5688dc732890923c326f272b0c18c329a69459a#diff-842e3447fc453de26c706db1cac8f2c4",
        "createdAt" : "2020-03-27T05:44:31Z",
        "updatedAt" : "2020-03-27T05:44:32Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "3b3cb8c1cae88575c7c0bcf98a20d5d06f479267",
    "line" : 50,
    "diffHunk" : "@@ -1,1 +125,129 @@        \"to process inserting into partitioned ORC/Parquet tables created by using the HiveSQL \" +\n        \"syntax.\")\n      .version(\"3.0.0\")\n      .booleanConf\n      .createWithDefault(true)"
  },
  {
    "id" : "ffb2ec48-ce71-4556-adbb-56ce550fbf6f",
    "prId" : 28042,
    "prUrl" : "https://github.com/apache/spark/pull/28042#pullrequestreview-382592763",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a8a7006b-2f18-4a4e-b965-7eb7e675acd2",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-25271, commit ID: 5ad03607d1487e7ab3e3b6d00eef9c4028ed4975#diff-842e3447fc453de26c706db1cac8f2c4",
        "createdAt" : "2020-03-27T05:44:54Z",
        "updatedAt" : "2020-03-27T05:44:54Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "3b3cb8c1cae88575c7c0bcf98a20d5d06f479267",
    "line" : 58,
    "diffHunk" : "@@ -1,1 +134,138 @@      \"`spark.sql.hive.convertMetastoreParquet` or `spark.sql.hive.convertMetastoreOrc` is \" +\n      \"enabled respectively for Parquet and ORC formats\")\n    .version(\"3.0.0\")\n    .booleanConf\n    .createWithDefault(true)"
  },
  {
    "id" : "18d2deb9-0112-4b73-94e4-e8dea7092a1c",
    "prId" : 28042,
    "prUrl" : "https://github.com/apache/spark/pull/28042#pullrequestreview-382592878",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bb12153c-d1be-456c-b0ec-4a312c2bb4a5",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-7491, commit ID: a8556086d33cb993fab0ae2751e31455e6c664ab#diff-ff50aea397a607b79df9bec6f2a841db",
        "createdAt" : "2020-03-27T05:45:17Z",
        "updatedAt" : "2020-03-27T05:45:17Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "3b3cb8c1cae88575c7c0bcf98a20d5d06f479267",
    "line" : 66,
    "diffHunk" : "@@ -1,1 +144,148 @@      \"classes that need to be shared are those that interact with classes that are already \" +\n      \"shared. For example, custom appenders that are used by log4j.\")\n    .version(\"1.4.0\")\n    .stringConf\n    .toSequence"
  },
  {
    "id" : "a210751d-c834-42c7-968a-796ab1c3508c",
    "prId" : 28042,
    "prUrl" : "https://github.com/apache/spark/pull/28042#pullrequestreview-382592907",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "724059ad-4009-470f-8049-74cecd23fd2e",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-7491, commit ID: a8556086d33cb993fab0ae2751e31455e6c664ab#diff-ff50aea397a607b79df9bec6f2a841db",
        "createdAt" : "2020-03-27T05:45:25Z",
        "updatedAt" : "2020-03-27T05:45:25Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "3b3cb8c1cae88575c7c0bcf98a20d5d06f479267",
    "line" : 74,
    "diffHunk" : "@@ -1,1 +156,160 @@      \"version of Hive that Spark SQL is communicating with. For example, Hive UDFs that are \" +\n      \"declared in a prefix that typically would be shared (i.e. <code>org.apache.spark.*</code>).\")\n    .version(\"1.4.0\")\n    .stringConf\n    .toSequence"
  },
  {
    "id" : "038891a8-a7d9-4949-84e5-0e12633185fc",
    "prId" : 28042,
    "prUrl" : "https://github.com/apache/spark/pull/28042#pullrequestreview-382592991",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ea7054bf-c33a-4bcd-a4c2-8a536d322e24",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-6964, commit ID: eb19d3f75cbd002f7e72ce02017a8de67f562792#diff-ff50aea397a607b79df9bec6f2a841db",
        "createdAt" : "2020-03-27T05:45:42Z",
        "updatedAt" : "2020-03-27T05:45:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "3b3cb8c1cae88575c7c0bcf98a20d5d06f479267",
    "line" : 81,
    "diffHunk" : "@@ -1,1 +163,167 @@  val HIVE_THRIFT_SERVER_ASYNC = buildConf(\"spark.sql.hive.thriftServer.async\")\n    .doc(\"When set to true, Hive Thrift server executes SQL queries in an asynchronous way.\")\n    .version(\"1.5.0\")\n    .booleanConf\n    .createWithDefault(true)"
  },
  {
    "id" : "0be3c65b-5f60-4bd0-990d-b0cdd5a72f9a",
    "prId" : 26461,
    "prUrl" : "https://github.com/apache/spark/pull/26461#pullrequestreview-314712287",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fbc2b13b-2f63-4ab5-8e58-3d96de05cf5a",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "is this really useful? The parallelism should depend on data size, and it's a hard job to tune this config.",
        "createdAt" : "2019-11-11T05:32:29Z",
        "updatedAt" : "2019-11-11T05:32:29Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "5c2326b2-b0a7-43e6-8c07-e665af6def0a",
        "parentId" : "fbc2b13b-2f63-4ab5-8e58-3d96de05cf5a",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "When reading a Hive partitioned table, users could get an unreasonable number of partitions like dozens of thousands. \r\n\r\nHive Scan node returns a UnionRDD of Hive table partitions. Each Hive table partition is read as a HadoopRDD. For each Hive table partition, the parallelism depends on data size. But final UnionRDD sums up all number of parallelism of all Hive table partitions.\r\n\r\n\r\n\r\n",
        "createdAt" : "2019-11-11T05:48:08Z",
        "updatedAt" : "2019-11-11T05:50:34Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "5ba9e78b-1f0f-4663-bab0-042c7ac01721",
        "parentId" : "fbc2b13b-2f63-4ab5-8e58-3d96de05cf5a",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "is it possible to get the size of each hadoop RDD and do coalesce automatically?",
        "createdAt" : "2019-11-11T05:57:26Z",
        "updatedAt" : "2019-11-11T05:57:26Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "43d79c9b-9814-4388-b3ca-9c9f44e83c25",
        "parentId" : "fbc2b13b-2f63-4ab5-8e58-3d96de05cf5a",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "I think we can get split size and total number of splits of a Hadoop RDD.",
        "createdAt" : "2019-11-11T06:20:55Z",
        "updatedAt" : "2019-11-11T06:20:55Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "2b7615c79ad83b4078c01d0864cdb9989e411624",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +156,160 @@    .createWithDefault(true)\n\n  val HIVE_TABLE_SCAN_MAX_PARALLELISM = buildConf(\"spark.sql.hive.tableScan.maxParallelism\")\n    .doc(\"When reading Hive partitioned table, the default parallelism is the sum of Hive \" +\n      \"partition RDDs' parallelism. For Hive table of many partitions with many files, \" +"
  },
  {
    "id" : "e19ffa85-ebcc-4b4c-97bc-a81ef5f1514a",
    "prId" : 26461,
    "prUrl" : "https://github.com/apache/spark/pull/26461#pullrequestreview-315110263",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bf5e3ead-a842-48bf-a734-bcb855964c0b",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Do we need a default value?",
        "createdAt" : "2019-11-11T19:15:59Z",
        "updatedAt" : "2019-11-11T19:15:59Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "07a6b61f-92c5-49a3-a5ca-dc0f2fe99c43",
        "parentId" : "bf5e3ead-a842-48bf-a734-bcb855964c0b",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "The reason I leave it optional is to allow us keep current behavior.",
        "createdAt" : "2019-11-11T19:32:46Z",
        "updatedAt" : "2019-11-11T19:32:46Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "2b7615c79ad83b4078c01d0864cdb9989e411624",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +164,168 @@      \"number by doing a coalesce on the RDD.\")\n    .intConf\n    .createOptional\n\n  /**"
  },
  {
    "id" : "02dcaed7-89c1-4e4e-9736-02155994fe97",
    "prId" : 25977,
    "prUrl" : "https://github.com/apache/spark/pull/25977#pullrequestreview-300598308",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f2c80318-500a-4cef-9adc-cbecea6c0ef6",
        "parentId" : null,
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "@yaooqinn Please take a look. `isCliSessionState` added by you.",
        "createdAt" : "2019-10-01T03:04:59Z",
        "updatedAt" : "2019-10-11T11:01:28Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      },
      {
        "id" : "c0aa414c-bc7d-4a78-a035-c87bc7be31b5",
        "parentId" : "f2c80318-500a-4cef-9adc-cbecea6c0ef6",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "The isolation for non built-in hive shoud be on actually to isolate with those built in ones，right？",
        "createdAt" : "2019-10-01T09:26:59Z",
        "updatedAt" : "2019-10-11T11:01:28Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "d5d56201-f015-4dbe-b1b2-924a3665dc7f",
        "parentId" : "f2c80318-500a-4cef-9adc-cbecea6c0ef6",
        "authorId" : "f2e3e1f3-82a3-4b68-a0c2-2f7fdf58cf35",
        "body" : "sorry, I am unable to understand your mean, could you please elaborate me on this ",
        "createdAt" : "2019-10-01T09:35:58Z",
        "updatedAt" : "2019-10-11T11:01:28Z",
        "lastEditedBy" : "f2e3e1f3-82a3-4b68-a0c2-2f7fdf58cf35",
        "tags" : [
        ]
      },
      {
        "id" : "1387e5cf-ed16-4cf9-b3ac-33b94e612294",
        "parentId" : "f2c80318-500a-4cef-9adc-cbecea6c0ef6",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "If you are using !built-in hive jars and turn off the isolation, which means using the SparkContxetClassLoader to load them directly , then you probably get hive class or  jar conflicts with the spark built-in ones during the runtime",
        "createdAt" : "2019-10-01T11:10:35Z",
        "updatedAt" : "2019-10-11T11:01:28Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "70e809ec-0959-4db8-98ac-8a6ccb346f1c",
        "parentId" : "f2c80318-500a-4cef-9adc-cbecea6c0ef6",
        "authorId" : "f2e3e1f3-82a3-4b68-a0c2-2f7fdf58cf35",
        "body" : "okay I will recheck this part of the problem",
        "createdAt" : "2019-10-03T10:31:10Z",
        "updatedAt" : "2019-10-11T11:01:28Z",
        "lastEditedBy" : "f2e3e1f3-82a3-4b68-a0c2-2f7fdf58cf35",
        "tags" : [
        ]
      },
      {
        "id" : "0638c581-19cc-451d-9e33-6657ddb43139",
        "parentId" : "f2c80318-500a-4cef-9adc-cbecea6c0ef6",
        "authorId" : "f2e3e1f3-82a3-4b68-a0c2-2f7fdf58cf35",
        "body" : "@wangyum if user is using  !built-in hive  then isolation should not be turned off, so this issue has to solved in different ways . \r\n\r\n1. Use in memory derby database. Configure hive-site.xml as below\r\n```\r\n<property>\r\n<name>javax.jdo.option.ConnectionURL</name>\r\n<value>jdbc:derby:memory:/opt/BigdataTools/spark-2.4.4-bin-hadoop2.7/metastore_db;create=true</value>\r\n<description>JDBC connect string for a JDBC metastore</description>\r\n</property>\r\n```\r\n\r\n2. Stop the derby instance started by [SparkSQLCLIDriver](https://github.com/apache/spark/blob/master/sql/hive-thriftserver/src/main/scala/org/apache/spark/sql/hive/thriftserver/SparkSQLCLIDriver.scala#L136) Something like below\r\n![image](https://user-images.githubusercontent.com/35216143/66289178-793c3380-e8f9-11e9-99a9-4c1098918548.png)\r\n\r\n\r\n@yaooqinn  what's your suggesstion\r\n\r\n\r\n\r\n",
        "createdAt" : "2019-10-07T06:26:37Z",
        "updatedAt" : "2019-10-11T11:01:28Z",
        "lastEditedBy" : "f2e3e1f3-82a3-4b68-a0c2-2f7fdf58cf35",
        "tags" : [
        ]
      },
      {
        "id" : "9d5c2778-3d62-4b97-98b7-ca9f6ddd71bd",
        "parentId" : "f2c80318-500a-4cef-9adc-cbecea6c0ef6",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "IMO,  SparkSQLCLIDriver already load and initiate hive classes in the main classloader, then we shouldn't use !builtin hive jars. Maybe just throw exception when using spark-sql stript with spark.sql.hive.metastore.jars",
        "createdAt" : "2019-10-11T09:09:57Z",
        "updatedAt" : "2019-10-11T11:01:28Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "8c134f0b-9a4d-400e-9a0b-c197cc82f705",
        "parentId" : "f2c80318-500a-4cef-9adc-cbecea6c0ef6",
        "authorId" : "f2e3e1f3-82a3-4b68-a0c2-2f7fdf58cf35",
        "body" : "there are 2 approach\r\n\r\n1.based on `isCliSessionState`  Hive class should be loaded ( This PR fix)\r\n2. Throw exception at the start of spark-sql if user used !builtin. By doing this there might be a problem, because currently this problem exists for Derby, if the user uses some other Database there will be a impact",
        "createdAt" : "2019-10-11T10:28:00Z",
        "updatedAt" : "2019-10-11T11:01:28Z",
        "lastEditedBy" : "f2e3e1f3-82a3-4b68-a0c2-2f7fdf58cf35",
        "tags" : [
        ]
      }
    ],
    "commit" : "ce0f4dec7e5c4852033f6cb584c3e554d15e1e57",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +416,420 @@        execJars = jars.toSeq,\n        config = configurations,\n        isolationOn = !isCliSessionState(),\n        barrierPrefixes = hiveMetastoreBarrierPrefixes,\n        sharedPrefixes = hiveMetastoreSharedPrefixes)"
  }
]