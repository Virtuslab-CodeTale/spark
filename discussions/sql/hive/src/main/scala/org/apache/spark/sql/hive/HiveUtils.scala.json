[
  {
    "id" : "7815e32d-c2b9-4e83-906a-e6ca510e1f54",
    "prId" : 32373,
    "prUrl" : "https://github.com/apache/spark/pull/32373#pullrequestreview-647757646",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e821bdbf-939a-4581-acbc-bc59487c342a",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Should we guard the change by `spark.driver.userClassPathFirst`?",
        "createdAt" : "2021-04-28T01:53:57Z",
        "updatedAt" : "2021-04-28T06:18:14Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "89845b77-86f9-431d-9678-d6a0c762f12c",
        "parentId" : "e821bdbf-939a-4581-acbc-bc59487c342a",
        "authorId" : "d63ec90d-34b5-4fb0-8626-865510da3afa",
        "body" : "It's already implied by ChildFirstURLClassLoader. Do you want to me check again in URLClassLoader?",
        "createdAt" : "2021-04-28T06:16:45Z",
        "updatedAt" : "2021-04-28T06:18:14Z",
        "lastEditedBy" : "d63ec90d-34b5-4fb0-8626-865510da3afa",
        "tags" : [
        ]
      },
      {
        "id" : "c49d112d-3415-4984-af84-8578fc813a0c",
        "parentId" : "e821bdbf-939a-4581-acbc-bc59487c342a",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Ohh okay I got what you mean. ",
        "createdAt" : "2021-04-29T01:20:49Z",
        "updatedAt" : "2021-04-29T01:20:49Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "cf169b1676759422d14390c9e32e0784efc3b298",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +399,403 @@          childFirst.getURLs() ++ allJars(Utils.getSparkClassLoader)\n        case urlClassLoader: URLClassLoader =>\n          allJars(urlClassLoader.getParent) ++ urlClassLoader.getURLs\n        case other => allJars(other.getParent)\n      }"
  },
  {
    "id" : "06e0aa12-0769-49e9-8d12-cd474ae2956f",
    "prId" : 32373,
    "prUrl" : "https://github.com/apache/spark/pull/32373#pullrequestreview-648813028",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7fc57359-b41b-4f28-9657-6050a04afae2",
        "parentId" : null,
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "I am 100% sure this change will 'guarantee' the class loading order. Because in `ChildFirstURLClassLoader`, we re-constructor it by setting the parent classloader to null and load classes with an order we explicitly set.",
        "createdAt" : "2021-04-30T02:20:35Z",
        "updatedAt" : "2021-04-30T02:21:23Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "cf169b1676759422d14390c9e32e0784efc3b298",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +399,403 @@          childFirst.getURLs() ++ allJars(Utils.getSparkClassLoader)\n        case urlClassLoader: URLClassLoader =>\n          allJars(urlClassLoader.getParent) ++ urlClassLoader.getURLs\n        case other => allJars(other.getParent)\n      }"
  },
  {
    "id" : "2a18f37a-fbe2-45e1-a984-2561ebdf10c5",
    "prId" : 31317,
    "prUrl" : "https://github.com/apache/spark/pull/31317#pullrequestreview-575978395",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "82a52666-a122-4f33-ae93-bd10d608e74e",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Is it a typo above?\r\n\r\n```\r\nThis property can be one of four options: \"\r\n```\r\n\r\n->\r\n\r\n```\r\nThis property can be one of four options:\r\n```\r\n?",
        "createdAt" : "2021-01-26T02:42:12Z",
        "updatedAt" : "2021-01-26T03:09:00Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "d9c64f94-3cdc-4f76-9860-e5a812c90d4f",
        "parentId" : "82a52666-a122-4f33-ae93-bd10d608e74e",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "<img width=\"753\" alt=\"截屏2021-01-26 上午10 59 28\" src=\"https://user-images.githubusercontent.com/46485123/105794249-981a7980-5fc5-11eb-8490-6fdd9e0fb17c.png\">\r\n\r\nYea..a typo",
        "createdAt" : "2021-01-26T03:00:00Z",
        "updatedAt" : "2021-01-26T03:09:00Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "66006722-ef63-4fb7-91d8-c4bc94e007b3",
        "parentId" : "82a52666-a122-4f33-ae93-bd10d608e74e",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "Fixed",
        "createdAt" : "2021-01-26T03:12:22Z",
        "updatedAt" : "2021-01-26T03:12:22Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "75bbdd871b414b00118e9c9fc6348f10bd626af4",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +87,91 @@      |   Use Hive jars of specified version downloaded from Maven repositories.\n      | 3. \"path\"\n      |   Use Hive jars configured by `spark.sql.hive.metastore.jars.path`\n      |   in comma separated format. Support both local or remote paths.The provided jars\n      |   should be the same version as ${HIVE_METASTORE_VERSION}."
  },
  {
    "id" : "f5dbb919-7086-4d49-be5c-d01590cc7dc4",
    "prId" : 31030,
    "prUrl" : "https://github.com/apache/spark/pull/31030#pullrequestreview-562308368",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6e22753b-a761-42c8-bc16-1dc5627f0e46",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@dh20, which case are you trying to elabourate? ",
        "createdAt" : "2021-01-05T09:37:49Z",
        "updatedAt" : "2021-01-12T03:39:55Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "fd72ed94-0fee-424f-9d8c-ba1a9910003c",
        "parentId" : "6e22753b-a761-42c8-bc16-1dc5627f0e46",
        "authorId" : "727a0ed3-3ae9-4a2e-bf11-94858ae239f3",
        "body" : "@HyukjinKwon When I query an orc format hive table and overwrite the query result into this orc table, sparksql will throw this error -> [org.apache.spark.sql.AnalysisException: Cannot overwrite a path that is also being read from], when I set this parameter to false, Hql will execute normally",
        "createdAt" : "2021-01-05T09:46:51Z",
        "updatedAt" : "2021-01-12T03:39:55Z",
        "lastEditedBy" : "727a0ed3-3ae9-4a2e-bf11-94858ae239f3",
        "tags" : [
        ]
      },
      {
        "id" : "cf0408a3-a635-4e9e-8203-e1ee7cae16e2",
        "parentId" : "6e22753b-a761-42c8-bc16-1dc5627f0e46",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I think it's weird to overwrite a path is being read from. Is it something officially supported from Hive?",
        "createdAt" : "2021-01-05T10:06:28Z",
        "updatedAt" : "2021-01-12T03:39:55Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "9bfbf1f5-3a6f-479d-9ce2-aa7d1b9e1715",
        "parentId" : "6e22753b-a761-42c8-bc16-1dc5627f0e46",
        "authorId" : "727a0ed3-3ae9-4a2e-bf11-94858ae239f3",
        "body" : "@HyukjinKwon Yes, Hive officially supported this function. This SQL was originally run on hive, and I switched sparksql before this problem appeared",
        "createdAt" : "2021-01-06T00:47:29Z",
        "updatedAt" : "2021-01-12T03:39:55Z",
        "lastEditedBy" : "727a0ed3-3ae9-4a2e-bf11-94858ae239f3",
        "tags" : [
        ]
      },
      {
        "id" : "b3887832-586e-4456-a3b0-d1441bc61c5b",
        "parentId" : "6e22753b-a761-42c8-bc16-1dc5627f0e46",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I think it's fine enough to say we use Spark's when `spark.sql.hive.convertMetastoreOrc` is enabled. Otherwise, we will have to list up every behaviour differences between Spark and Hive in this configuration doc.",
        "createdAt" : "2021-01-06T00:49:12Z",
        "updatedAt" : "2021-01-12T03:39:55Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "25b7c12f-0c9d-4fab-9c67-7f9bc2bb4d10",
        "parentId" : "6e22753b-a761-42c8-bc16-1dc5627f0e46",
        "authorId" : "727a0ed3-3ae9-4a2e-bf11-94858ae239f3",
        "body" : "@HyukjinKwon Yes, I think so, but now the official explanation of spark for this parameter is aimed at the difference between spark Orc vector readers, so I think its explanation should be improved",
        "createdAt" : "2021-01-06T01:09:10Z",
        "updatedAt" : "2021-01-12T03:39:55Z",
        "lastEditedBy" : "727a0ed3-3ae9-4a2e-bf11-94858ae239f3",
        "tags" : [
        ]
      },
      {
        "id" : "4dd44c53-187d-4d38-bac2-b4eb72afa6b7",
        "parentId" : "6e22753b-a761-42c8-bc16-1dc5627f0e46",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "The doc already says that \"the built-in ORC reader\" vs \"Hive serde\". I think it's fine.",
        "createdAt" : "2021-01-06T01:11:39Z",
        "updatedAt" : "2021-01-12T03:39:55Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "275a5e87-ab4e-4edb-b1ca-5a8c720396cb",
        "parentId" : "6e22753b-a761-42c8-bc16-1dc5627f0e46",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "@dh20 . It seems that you want to describe one of the differences between Spark data source tables and Hive serde tables. May I ask a few questions?\r\n\r\n- Is the situation different from `Parquet`? Parquet also has the same conf `spark.sql.hive.convertMetastoreParquet`.\r\n\r\n- Is the added statement correct always? Specifically, the following statement looks wrong in some situations. For example, did you try to use `PartitionOverwriteMode.DYNAMIC`?\r\n> When I query an orc format hive table and overwrite the query result into this orc table, sparksql will throw this error -> [org.apache.spark.sql.AnalysisException: Cannot overwrite a path that is also being read from], when I set this parameter to false, Hql will execute normally\r\n\r\n- Which Apache Spark version are you using? The situation will be changed according to the Spark versions. For example,\r\n  - SPARK-30112 implemented `Allow insert overwrite same table if using dynamic partition overwrite` at Apache Spark 3.0.0. \r\n  - SPARK-33887 is also trying to implement `Allow insert overwrite same table with static partition if using dynamic partition overwrite mode` at Apache Spark 3.2.0.",
        "createdAt" : "2021-01-06T01:48:19Z",
        "updatedAt" : "2021-01-12T03:39:55Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "f52a1d6a-c4fe-43fd-9ace-94323e584d31",
        "parentId" : "6e22753b-a761-42c8-bc16-1dc5627f0e46",
        "authorId" : "727a0ed3-3ae9-4a2e-bf11-94858ae239f3",
        "body" : "@dongjoon-hyun Hello, I'll answer your questions one by one.\r\n\r\n- 1.When the format of the table is parquet, there is no problem to insert and overlay the same table, no matter whether this parameter is added or not【 spark.sql.hive .convertMetastoreParquet=false】\r\n\r\n- 2.[PartitionOverwriteMode.DYNAMIC] It can't solve the problem\r\n\r\n- 3.I use spark 3.0.0, spark-30112 does not solve this problem.\r\n\r\n\r\n.",
        "createdAt" : "2021-01-06T03:18:01Z",
        "updatedAt" : "2021-01-12T03:39:55Z",
        "lastEditedBy" : "727a0ed3-3ae9-4a2e-bf11-94858ae239f3",
        "tags" : [
        ]
      },
      {
        "id" : "6799a778-d3a1-4c9c-ae29-cd646f42a845",
        "parentId" : "6e22753b-a761-42c8-bc16-1dc5627f0e46",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Could you file a JIRA for this with your specific procedure? This one is a valid bug.\r\n> When the format of the table is parquet, there is no problem to insert and overlay the same table, no matter whether this parameter is added or not【 spark.sql.hive .convertMetastoreParquet=false】\r\n\r\nFor this one, I think your statement in this PR is too broader because we have a unit test coverage with Dynamic which was added by SPARK-30112. When you say `this problem`, you should be specific to avoid ambiguity.\r\n> [PartitionOverwriteMode.DYNAMIC] It can't solve the problem\r\n> I use spark 3.0.0, spark-30112 does not solve this problem.",
        "createdAt" : "2021-01-06T03:48:56Z",
        "updatedAt" : "2021-01-12T03:39:55Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "bf56322679c1d15649663828bdb1c5e270ab49e2",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +133,137 @@      \"ORC tables created by using the HiveQL syntax, instead of Hive serde.\" +\n      \"Enabling this parameter will cause insert overwrite when reading the table,\" +\n      \"and the file format of this table is ORC This error will be thrown[], \" +\n      \"Users can set spark.sql.hive.convertMetastoreOrc=false to solve this error.\")\n    .version(\"2.0.0\")"
  },
  {
    "id" : "87bf25ad-b86b-4e82-8a62-f531c18df458",
    "prId" : 30407,
    "prUrl" : "https://github.com/apache/spark/pull/30407#pullrequestreview-533185015",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c286524c-6b19-4982-b38d-3c9b925aa108",
        "parentId" : null,
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "LGTM",
        "createdAt" : "2020-11-18T08:05:09Z",
        "updatedAt" : "2020-11-18T08:14:06Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "45922bda6290dd7f34b4d719c18a5cc577df5cb4",
    "line" : 26,
    "diffHunk" : "@@ -1,1 +108,112 @@      | 1. file://path/to/jar/*,file://path2/to/jar/*/*.jar\n      | 2. hdfs://nameservice/path/to/jar/*,hdfs://nameservice2/path/to/jar/*/*.jar\n      \"\"\".stripMargin)\n    .version(\"3.1.0\")\n    .stringConf"
  }
]