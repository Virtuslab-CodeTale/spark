[
  {
    "id" : "ecd5568a-43d3-41a4-a5c5-88fd933e6065",
    "prId" : 28129,
    "prUrl" : "https://github.com/apache/spark/pull/28129#pullrequestreview-387909969",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c3d099b4-bfde-4321-beb2-6567ad52bd86",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "If the description is true, we should check if `spark.speculation` is true or false here?",
        "createdAt" : "2020-04-06T01:49:31Z",
        "updatedAt" : "2020-04-06T01:49:31Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "5ecf5519-b769-45ec-96d8-2adc97af0eac",
        "parentId" : "c3d099b4-bfde-4321-beb2-6567ad52bd86",
        "authorId" : "0f4ef4e8-09be-436f-b743-bf8fbc343490",
        "body" : "@maropu Thanks for reply. In my scenario it was indeed caused by `spark.speculation`, but I don’t know if there are any other scenarios also happened in this.\r\nIf it only happened with `spark.speculation`, then we can just check `spark.speculation` instead of adding new configuration.",
        "createdAt" : "2020-04-06T02:52:30Z",
        "updatedAt" : "2020-04-06T02:52:31Z",
        "lastEditedBy" : "0f4ef4e8-09be-436f-b743-bf8fbc343490",
        "tags" : [
        ]
      },
      {
        "id" : "31823c2b-93ad-4c88-b167-c308f1de7da1",
        "parentId" : "c3d099b4-bfde-4321-beb2-6567ad52bd86",
        "authorId" : "0f4ef4e8-09be-436f-b743-bf8fbc343490",
        "body" : "It is happened when running tasks is killed after Job committed, maybe the better way to solve this is checking the job's tasks status. However it seems it is impossible to check status here, thus I add new configuration to this.",
        "createdAt" : "2020-04-06T03:05:59Z",
        "updatedAt" : "2020-04-06T03:05:59Z",
        "lastEditedBy" : "0f4ef4e8-09be-436f-b743-bf8fbc343490",
        "tags" : [
        ]
      }
    ],
    "commit" : "2f9dc05e7dbf5f1c6724407741137fcc6153e7d0",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +144,148 @@        // Sometimes (e.g., when speculative task is enabled), temporary directories may be\n        // left uncleaned, confirmTempDirDeleted can confirm deleteOnExit.\n        if (fs.delete(path, true) && !SQLConf.get.confirmTempDirDeleted) {\n          // If we successfully delete the staging directory, remove it from FileSystem's cache.\n          fs.cancelDeleteOnExit(path)"
  },
  {
    "id" : "cb46f502-d347-46a1-98f4-5d7357e31547",
    "prId" : 28129,
    "prUrl" : "https://github.com/apache/spark/pull/28129#pullrequestreview-388807802",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2cd3aed2-80f0-41ff-a362-a70f8d248f7e",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Do you mean even if we delete the temp dir here, some tasks may re-create it later?",
        "createdAt" : "2020-04-07T05:46:45Z",
        "updatedAt" : "2020-04-07T05:46:45Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "5b251510-78f4-4867-8d66-fd66997a948b",
        "parentId" : "2cd3aed2-80f0-41ff-a362-a70f8d248f7e",
        "authorId" : "0f4ef4e8-09be-436f-b743-bf8fbc343490",
        "body" : "Yeap, the speculative tasks are maybe still running after Job committed.",
        "createdAt" : "2020-04-07T06:07:26Z",
        "updatedAt" : "2020-04-07T06:07:26Z",
        "lastEditedBy" : "0f4ef4e8-09be-436f-b743-bf8fbc343490",
        "tags" : [
        ]
      }
    ],
    "commit" : "2f9dc05e7dbf5f1c6724407741137fcc6153e7d0",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +143,147 @@        val fs = path.getFileSystem(hadoopConf)\n        // Sometimes (e.g., when speculative task is enabled), temporary directories may be\n        // left uncleaned, confirmTempDirDeleted can confirm deleteOnExit.\n        if (fs.delete(path, true) && !SQLConf.get.confirmTempDirDeleted) {\n          // If we successfully delete the staging directory, remove it from FileSystem's cache."
  },
  {
    "id" : "117ab1b9-7232-435c-aeba-2398bcde3e29",
    "prId" : 27690,
    "prUrl" : "https://github.com/apache/spark/pull/27690#pullrequestreview-410633294",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e187fd0f-62a5-4b17-a30a-16cba762d325",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Looks this logging is similar to those inside `getMRTmpPath`.\r\nhttps://github.com/apache/spark/pull/27690/files#diff-e08e0d262de1e7582ddee984936d93d4R118\r\nhttps://github.com/apache/spark/pull/27690/files#diff-e08e0d262de1e7582ddee984936d93d4R121\r\nCould you merge them in a single place? I think It is hard to read the current logs.",
        "createdAt" : "2020-05-13T00:08:24Z",
        "updatedAt" : "2020-07-17T08:22:50Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "6660a682-7455-4e39-9eb3-84ae0bf8f9a3",
        "parentId" : "e187fd0f-62a5-4b17-a30a-16cba762d325",
        "authorId" : "8c4501b6-dbfc-491f-80a5-f615a2311f95",
        "body" : "Deleted the debug logging.",
        "createdAt" : "2020-05-13T06:52:07Z",
        "updatedAt" : "2020-07-17T08:22:50Z",
        "lastEditedBy" : "8c4501b6-dbfc-491f-80a5-f615a2311f95",
        "tags" : [
        ]
      }
    ],
    "commit" : "44ce61eb4cc129c285131525b99b20f82774d107",
    "line" : 81,
    "diffHunk" : "@@ -1,1 +178,182 @@        val sessionScratchDir = externalCatalog.unwrapped.asInstanceOf[HiveExternalCatalog]\n          .client.getConf(HDFS_SESSION_PATH_KEY, \"\")\n        logDebug(s\"session scratch dir '$sessionScratchDir' is used\")\n        getNonBlobTmpPath(hadoopConf, sessionScratchDir, scratchDir)\n      } else {"
  },
  {
    "id" : "4e642daa-7ac9-40b7-abe8-d0b7744179b2",
    "prId" : 27690,
    "prUrl" : "https://github.com/apache/spark/pull/27690#pullrequestreview-410683784",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "833dff85-8b36-4109-9113-7c0ca19c4dc8",
        "parentId" : null,
        "authorId" : "a2fcc15e-a51a-42f0-87a6-137048a28e30",
        "body" : "Question: We don't need to do .toLowercase on supportedBlobSchemes ? Also, are the supported schemes always in lower case ?",
        "createdAt" : "2020-05-13T07:57:30Z",
        "updatedAt" : "2020-07-17T08:22:50Z",
        "lastEditedBy" : "a2fcc15e-a51a-42f0-87a6-137048a28e30",
        "tags" : [
        ]
      },
      {
        "id" : "0fab152e-56d7-4142-b316-bea4be29acea",
        "parentId" : "833dff85-8b36-4109-9113-7c0ca19c4dc8",
        "authorId" : "8c4501b6-dbfc-491f-80a5-f615a2311f95",
        "body" : "I agreed with you, to be honest I am not sure if there are such schemes not in lower case or not. I remove `.toLowercase` for now.",
        "createdAt" : "2020-05-13T08:06:40Z",
        "updatedAt" : "2020-07-17T08:22:50Z",
        "lastEditedBy" : "8c4501b6-dbfc-491f-80a5-f615a2311f95",
        "tags" : [
        ]
      }
    ],
    "commit" : "44ce61eb4cc129c285131525b99b20f82774d107",
    "line" : 45,
    "diffHunk" : "@@ -1,1 +131,135 @@      val supportedBlobSchemes = SQLConf.get.supportedSchemesToUseNonBlobstore\n      val scheme = Option(path.toUri.getScheme).getOrElse(\"\")\n      Utils.stringToSeq(supportedBlobSchemes).contains(scheme.toLowerCase(Locale.ROOT))\n    }\n  }"
  },
  {
    "id" : "c2c589cf-04f6-413e-a284-5630616ac3c9",
    "prId" : 27690,
    "prUrl" : "https://github.com/apache/spark/pull/27690#pullrequestreview-410691126",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b4ff6d4d-31a7-46b3-b38f-f81fb5414946",
        "parentId" : null,
        "authorId" : "a2fcc15e-a51a-42f0-87a6-137048a28e30",
        "body" : "Super Nit: Do we want to logDebug the sessionScratchDir ? ",
        "createdAt" : "2020-05-13T08:01:20Z",
        "updatedAt" : "2020-07-17T08:22:50Z",
        "lastEditedBy" : "a2fcc15e-a51a-42f0-87a6-137048a28e30",
        "tags" : [
        ]
      },
      {
        "id" : "b3481157-f9fc-4dd2-b06b-4cbeaf81ca09",
        "parentId" : "b4ff6d4d-31a7-46b3-b38f-f81fb5414946",
        "authorId" : "8c4501b6-dbfc-491f-80a5-f615a2311f95",
        "body" : "I removed it right before, but added it again for future troubleshooting.",
        "createdAt" : "2020-05-13T08:16:18Z",
        "updatedAt" : "2020-07-17T08:22:50Z",
        "lastEditedBy" : "8c4501b6-dbfc-491f-80a5-f615a2311f95",
        "tags" : [
        ]
      }
    ],
    "commit" : "44ce61eb4cc129c285131525b99b20f82774d107",
    "line" : 79,
    "diffHunk" : "@@ -1,1 +176,180 @@        // Hive sets session_path as HDFS_SESSION_PATH_KEY(_hive.hdfs.session.path) in hive config\n        val HDFS_SESSION_PATH_KEY = \"_hive.hdfs.session.path\"\n        val sessionScratchDir = externalCatalog.unwrapped.asInstanceOf[HiveExternalCatalog]\n          .client.getConf(HDFS_SESSION_PATH_KEY, \"\")\n        logDebug(s\"session scratch dir '$sessionScratchDir' is used\")"
  },
  {
    "id" : "f3963e55-f7cf-4bcc-8bca-bc53c237cdb7",
    "prId" : 27690,
    "prUrl" : "https://github.com/apache/spark/pull/27690#pullrequestreview-430111125",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "10385ae0-df98-4ce3-95cc-f8c924447fda",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Any benefit to use sessionScratchDir here except for following Hive?",
        "createdAt" : "2020-06-12T20:15:41Z",
        "updatedAt" : "2020-07-17T08:22:50Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "4cf9e0df-836e-4e10-babe-dc429e98ec8e",
        "parentId" : "10385ae0-df98-4ce3-95cc-f8c924447fda",
        "authorId" : "8c4501b6-dbfc-491f-80a5-f615a2311f95",
        "body" : "Several HDFS scratch directories are created during start SessionState.\r\nIf the session scratch directory is created in the path specified in `_hive.hdfs.session.path`, the directory should be used. If it is not specified, then we just use scratchDir.",
        "createdAt" : "2020-06-13T03:55:30Z",
        "updatedAt" : "2020-07-17T08:22:50Z",
        "lastEditedBy" : "8c4501b6-dbfc-491f-80a5-f615a2311f95",
        "tags" : [
        ]
      }
    ],
    "commit" : "44ce61eb4cc129c285131525b99b20f82774d107",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +114,118 @@    // Here it uses session_path unless it's emtpy, otherwise uses scratchDir.\n    val sessionPath = if (!sessionScratchDir.isEmpty) sessionScratchDir else scratchDir\n    val mrScratchDir = oldVersionExternalTempPath(new Path(sessionPath), hadoopConf, sessionPath)\n    logDebug(s\"MR scratch dir '$mrScratchDir/-mr-10000' is used\")\n    val path = new Path(mrScratchDir, \"-mr-10000\")"
  },
  {
    "id" : "eb318626-d934-464b-8b8d-2423395662b0",
    "prId" : 27690,
    "prUrl" : "https://github.com/apache/spark/pull/27690#pullrequestreview-430306110",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "22a65ff4-20b7-40b9-ae71-9a23e06a8201",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Why this feature only applies on `hiveVersionsUsingNewExternalTempPath` cases? How about `hiveVersionsUsingOldExternalTempPath`?",
        "createdAt" : "2020-06-12T20:24:44Z",
        "updatedAt" : "2020-07-17T08:22:50Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "f6682e03-2d82-41ec-a414-18dfafc89c79",
        "parentId" : "22a65ff4-20b7-40b9-ae71-9a23e06a8201",
        "authorId" : "8c4501b6-dbfc-491f-80a5-f615a2311f95",
        "body" : "It is because old versions do not support data copy between different type of DFSs.",
        "createdAt" : "2020-06-13T03:35:29Z",
        "updatedAt" : "2020-07-17T08:22:50Z",
        "lastEditedBy" : "8c4501b6-dbfc-491f-80a5-f615a2311f95",
        "tags" : [
        ]
      },
      {
        "id" : "e0b7703a-8ab3-4fb0-b97f-f0035824adad",
        "parentId" : "22a65ff4-20b7-40b9-ae71-9a23e06a8201",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "You mean old Hive versions doesn't support it? If so, I think we should at least document it clearly.",
        "createdAt" : "2020-06-13T07:33:56Z",
        "updatedAt" : "2020-07-17T08:22:50Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "da8279c1-05cf-4020-955d-06f270ebe7ec",
        "parentId" : "22a65ff4-20b7-40b9-ae71-9a23e06a8201",
        "authorId" : "8c4501b6-dbfc-491f-80a5-f615a2311f95",
        "body" : "Got it. I added the description \"This option is supported in Hive 2.0 or later.\" in SQLConf.scala.\r\nhttps://github.com/apache/spark/pull/27690/files#diff-9a6b543db706f1a90f790783d6930a13R849",
        "createdAt" : "2020-06-15T03:48:48Z",
        "updatedAt" : "2020-07-17T08:22:50Z",
        "lastEditedBy" : "8c4501b6-dbfc-491f-80a5-f615a2311f95",
        "tags" : [
        ]
      }
    ],
    "commit" : "44ce61eb4cc129c285131525b99b20f82774d107",
    "line" : 71,
    "diffHunk" : "@@ -1,1 +169,173 @@    if (hiveVersionsUsingOldExternalTempPath.contains(hiveVersion)) {\n      oldVersionExternalTempPath(path, hadoopConf, scratchDir)\n    } else if (hiveVersionsUsingNewExternalTempPath.contains(hiveVersion)) {\n      // SPARK-21514: Write temporary data to HDFS when doing inserts on tables located on S3\n      // Copied from Context.java#getTempDirForPath of Hive 2.3."
  },
  {
    "id" : "13d58451-0d08-486a-97b5-c88a285f000e",
    "prId" : 27690,
    "prUrl" : "https://github.com/apache/spark/pull/27690#pullrequestreview-438753786",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "79c5854d-4a2b-4817-aa5d-8b26f1915413",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "`oldVersionExternalTempPath` will use the first `Path` parameter's scheme to create the temp path.\r\n\r\nDoes it guarantee `Path(sessionPath)` always uses HDFS scheme here?\r\n\r\nIf `sessionPath` is a HDFS path, it is ok. But if we use `scratchDir` as `sessionPath` here, it will be something like `Path(\"/tmp/hive\")`, is it still HDFS scheme?",
        "createdAt" : "2020-06-27T04:58:43Z",
        "updatedAt" : "2020-07-17T08:22:50Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "85812677-9214-4f23-8309-e3f8e64031df",
        "parentId" : "79c5854d-4a2b-4817-aa5d-8b26f1915413",
        "authorId" : "8c4501b6-dbfc-491f-80a5-f615a2311f95",
        "body" : "Hive has two kinds of scratch dir accordingly, one in local, the other in hdfs.\r\nhttps://mingyue.me/2018/11/17/hive-scratch-working-directory/\r\n\r\nIn this pull-request, the latter one, `hive.exec.scratchdir` is used. In my recognition, we can assume HDFS schema in most cases.",
        "createdAt" : "2020-06-27T11:49:58Z",
        "updatedAt" : "2020-07-17T08:22:50Z",
        "lastEditedBy" : "8c4501b6-dbfc-491f-80a5-f615a2311f95",
        "tags" : [
        ]
      },
      {
        "id" : "c8eb4106-a688-4854-92a1-a08f04ea3bca",
        "parentId" : "79c5854d-4a2b-4817-aa5d-8b26f1915413",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "But as you see we obtain scratch dir like `val scratchDir = hadoopConf.get(\"hive.exec.scratchdir\", \"/tmp/hive\")`, it is possible that we use `\"/tmp/hive\"`. I think in the case we will use local scheme as the temp path.",
        "createdAt" : "2020-06-27T16:01:01Z",
        "updatedAt" : "2020-07-17T08:22:50Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "9ccdd44e-bc41-462e-9466-e99e38046b40",
        "parentId" : "79c5854d-4a2b-4817-aa5d-8b26f1915413",
        "authorId" : "8c4501b6-dbfc-491f-80a5-f615a2311f95",
        "body" : "Here, no specific scheme is configured in `hive.exec.scratchdir` as default value. The default is just `/tmp/hive` without scheme. It means that the default scheme determined by `fs.default.name` is used. \r\nIf `fs.default.name` is a local scheme like `file://` then `file://` will be used. If `fs.default.name` is a hdfs scheme like `hdfs://host:port/` then `hdfs://host:port/` will be used. I confirmed this behavior in my end to make sure it.\r\n\r\nIn most cases, `fs.default.name` is a hdfs scheme. \r\nEven if local scheme is used here, it should work because we can assume local scheme can be used in such environment (e.g. unit testing).\n\nOf course, users have a full control on `hive.exec.scratchdir` and `fs.default.name`, so I believe it won’t be a problem.",
        "createdAt" : "2020-06-28T01:45:43Z",
        "updatedAt" : "2020-07-17T08:22:50Z",
        "lastEditedBy" : "8c4501b6-dbfc-491f-80a5-f615a2311f95",
        "tags" : [
        ]
      },
      {
        "id" : "3c01dba3-797f-4612-aafd-fe7978cd65f4",
        "parentId" : "79c5854d-4a2b-4817-aa5d-8b26f1915413",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "When this new feature is enabled, it is possible that a scheme which doesn't work for this feature is used, e.g. local scheme. If it is happened and causes some error, end-users might not know how to deal with it.\r\n\r\nBecause we don't know if every scheme supports this feature, we use a list of schemes as config value, instead of a boolean config. Similarly, I think we should not reply on an assumption that `fs.default.name` always works for this feature. Can we just restrict this feature to HDFS only? ",
        "createdAt" : "2020-06-28T03:16:01Z",
        "updatedAt" : "2020-07-17T08:22:50Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "1cf4bf31-9df1-4cfb-bfb8-5ba4a2e3f772",
        "parentId" : "79c5854d-4a2b-4817-aa5d-8b26f1915413",
        "authorId" : "8c4501b6-dbfc-491f-80a5-f615a2311f95",
        "body" : "Currently we just rely on `hive.exec.scratchdir` (not directly on `fs.default.name`), and it works in most use cases even if `hive.exec.scratchdir` is not configured explicitly. \r\nI do not want to restrict this feature to HDFS only because I have seen some clusters which do not have HDFS. I want to let end-users choose any scheme where they want to store temporary data.",
        "createdAt" : "2020-06-28T05:47:16Z",
        "updatedAt" : "2020-07-17T08:22:50Z",
        "lastEditedBy" : "8c4501b6-dbfc-491f-80a5-f615a2311f95",
        "tags" : [
        ]
      },
      {
        "id" : "ca496b5b-387d-49c3-aad8-4bdba92c7a97",
        "parentId" : "79c5854d-4a2b-4817-aa5d-8b26f1915413",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "If any one scheme doesn't work? Do we have clear error message to tell users what happens and how to fix it?",
        "createdAt" : "2020-06-28T06:05:45Z",
        "updatedAt" : "2020-07-17T08:22:50Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "7f6b91e9-b691-4f87-b4c6-2e8f9dccee5b",
        "parentId" : "79c5854d-4a2b-4817-aa5d-8b26f1915413",
        "authorId" : "8c4501b6-dbfc-491f-80a5-f615a2311f95",
        "body" : "It makes sense to me. Thank you for the suggestion.\r\nI added an error message to suggest end-users to configure `hive.exec.scratchdir` if the scheme of the temporary directory is `file`.\r\nCan you check if it is clear enough for end-users?\r\n\r\n```\r\n    val path = new Path(mrScratchDir, \"-mr-10000\")\r\n    val scheme = Option(path.toUri.getScheme).getOrElse(\"\")\r\n    if (schema == \"file\") {\r\n      logWarning(s\"Temporary data will be written into a local disk \" +\r\n        s\"(scheme: '$scheme', path: '$mrScratchDir'). \" +\r\n        s\"You need to configure 'hive.exec.scratchdir' to use accessible location \" +\r\n        s\"(e.g. HDFS path) from any executors in the cluster.\")\r\n    }\r\n```\r\n\r\nNote: I also added one test case for HDFS.",
        "createdAt" : "2020-06-28T07:46:34Z",
        "updatedAt" : "2020-07-17T08:22:50Z",
        "lastEditedBy" : "8c4501b6-dbfc-491f-80a5-f615a2311f95",
        "tags" : [
        ]
      }
    ],
    "commit" : "44ce61eb4cc129c285131525b99b20f82774d107",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +114,118 @@    // Here it uses session_path unless it's emtpy, otherwise uses scratchDir.\n    val sessionPath = if (!sessionScratchDir.isEmpty) sessionScratchDir else scratchDir\n    val mrScratchDir = oldVersionExternalTempPath(new Path(sessionPath), hadoopConf, sessionPath)\n    logDebug(s\"MR scratch dir '$mrScratchDir/-mr-10000' is used\")\n    val path = new Path(mrScratchDir, \"-mr-10000\")"
  },
  {
    "id" : "38936317-4e20-451b-b2cf-defff30cf4d4",
    "prId" : 24886,
    "prUrl" : "https://github.com/apache/spark/pull/24886#pullrequestreview-254364455",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b976095c-23cd-4419-a559-88d059c1e6b4",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "indent. ",
        "createdAt" : "2019-06-26T02:25:37Z",
        "updatedAt" : "2019-06-26T02:25:37Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "e94fa1e4-96af-4c1d-829a-81700654ba4b",
        "parentId" : "b976095c-23cd-4419-a559-88d059c1e6b4",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "oops..will fix in a follow-up. Thanks.",
        "createdAt" : "2019-06-26T03:19:18Z",
        "updatedAt" : "2019-06-26T03:30:10Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "1aec7f83292269757bb4c5e863e1d439cd26dd02",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +90,94 @@    // scalastyle:off caselocale\n    val hiveCompatiblePartitionColumns = partitionAttributes.map { attr =>\n     attr.withName(attr.name.toLowerCase)\n    }\n    // scalastyle:on caselocale"
  }
]