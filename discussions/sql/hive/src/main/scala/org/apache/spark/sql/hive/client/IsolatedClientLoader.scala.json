[
  {
    "id" : "f16cb032-528c-4584-9f20-d5707656b835",
    "prId" : 31203,
    "prUrl" : "https://github.com/apache/spark/pull/31203#pullrequestreview-574635797",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "66105305-2cee-40ec-8b15-3ba4c5a56415",
        "parentId" : null,
        "authorId" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "body" : "maybe\r\n```\r\ncase (maj, _, _) if maj > 3 => true\r\ncase (3, min, _) if min > 2 => true\r\ncase (3, 2, patch) if patch >=2 => true\r\n```\r\nSeems like we can reasonably assume that future versions of Hadoop will support the shaded client?",
        "createdAt" : "2021-01-22T20:51:00Z",
        "updatedAt" : "2021-01-26T21:30:32Z",
        "lastEditedBy" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "tags" : [
        ]
      },
      {
        "id" : "fe9e94cc-d6fd-4023-81da-ae44312526f9",
        "parentId" : "66105305-2cee-40ec-8b15-3ba4c5a56415",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "I think we'd better to wait until the future versions come out before changing this (so that we can verify firs). For instance, Hadoop 3.3.0 currently doesn't support shaded client (due to the hadoop-aws issue). But yeah the Hadoop 3.2.2+ should support the shaded client assuming there's no regression.",
        "createdAt" : "2021-01-22T20:59:10Z",
        "updatedAt" : "2021-01-26T21:30:32Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "baf13b43-48e4-4def-ac61-fccc39988a3e",
        "parentId" : "66105305-2cee-40ec-8b15-3ba4c5a56415",
        "authorId" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "body" : "Interesting... I just worry about forgetting to update this if/when we bump the Hadoop version in the future and causing a regression. Has the `hadoop-aws` fix made it to be targeted for Hadoop 3.3.1? If so, can we reasonably assume that 3.2.2+, 3.3.1+, and 3.4.0+ will have it?\r\n\r\nIt seems you're more tied into what's happening in the Hadoop world than I am these days so I'll take your word in either direction. If we decide _not_ to future-proof it, can we create a follow-up JIRA to revisit it once some future release is out at which time we would be confident in putting a wildcard?",
        "createdAt" : "2021-01-22T21:05:19Z",
        "updatedAt" : "2021-01-26T21:30:32Z",
        "lastEditedBy" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "tags" : [
        ]
      },
      {
        "id" : "8805ee37-0e77-4043-a382-eb9327493d01",
        "parentId" : "66105305-2cee-40ec-8b15-3ba4c5a56415",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "I think your concern is valid. One thing we can do is perhaps adding a test to make sure that the built-in Hadoop version is always compatible with the shaded client. So that in future if we upgrade Hadoop version & forget to do this, the test will break.",
        "createdAt" : "2021-01-22T21:23:33Z",
        "updatedAt" : "2021-01-26T21:30:32Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "09c32e0b-aa0c-4bc5-b390-5a6a1c7dd937",
        "parentId" : "66105305-2cee-40ec-8b15-3ba4c5a56415",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "And yes we can assume that 3.2.2+, 3.3.1+ and 3.4.0+ will all have the fix.",
        "createdAt" : "2021-01-22T21:26:17Z",
        "updatedAt" : "2021-01-26T21:30:32Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "1839b0bf-5e84-4272-a94f-24b8110dd7da",
        "parentId" : "66105305-2cee-40ec-8b15-3ba4c5a56415",
        "authorId" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "body" : "Excellent idea on adding a compatibility test for the built-in Hadoop version!",
        "createdAt" : "2021-01-22T21:28:21Z",
        "updatedAt" : "2021-01-26T21:30:32Z",
        "lastEditedBy" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "tags" : [
        ]
      }
    ],
    "commit" : "43367c51cc85c2893d663bc248ee95d4d15cdbde",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +111,115 @@    VersionUtils.majorMinorPatchVersion(hadoopVersion).exists {\n      case (3, 2, v) if v >= 2 => true\n      case _ => false\n    }\n  }"
  },
  {
    "id" : "8bef2c51-34e9-4b80-b33c-d81a554f33b4",
    "prId" : 31203,
    "prUrl" : "https://github.com/apache/spark/pull/31203#pullrequestreview-576768190",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "37deb64b-ace5-4370-b1ad-847fa4043269",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Got it. I agree that we need to change this again.\r\n> And yes we can assume that 3.2.2+, 3.3.1+ and 3.4.0+ will all have the fix.",
        "createdAt" : "2021-01-26T21:16:39Z",
        "updatedAt" : "2021-01-26T21:30:32Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "43367c51cc85c2893d663bc248ee95d4d15cdbde",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +111,115 @@    VersionUtils.majorMinorPatchVersion(hadoopVersion).exists {\n      case (3, 2, v) if v >= 2 => true\n      case _ => false\n    }\n  }"
  },
  {
    "id" : "7636e8dd-d39d-40d5-aa60-dee142188c5a",
    "prId" : 30701,
    "prUrl" : "https://github.com/apache/spark/pull/30701#pullrequestreview-569550740",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "85348cd8-148e-49bd-8759-66f7b787442e",
        "parentId" : null,
        "authorId" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "body" : "Will this break if `hadoopVersion` is 3.1.0, 3.2.1, etc. (due to the previous issues with the shaded client JARs)?",
        "createdAt" : "2021-01-15T18:38:27Z",
        "updatedAt" : "2021-01-15T18:41:06Z",
        "lastEditedBy" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "tags" : [
        ]
      },
      {
        "id" : "c632114d-6ecd-4e1a-9650-5df002e5e16c",
        "parentId" : "85348cd8-148e-49bd-8759-66f7b787442e",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Do you mean [HADOOP-16080](https://issues.apache.org/jira/browse/HADOOP-16080)? yes things could still break in the following cases.\r\n\r\n1. users build Spark without `-Phadoop-cloud` AND use a version doesn't have the fix in HADOOP-16080, such as:\r\n```\r\n$ bin/spark-shell --packages org.apache.hadoop:hadoop-aws:3.2.0,org.apache.hadoop:hadoop-common:3.2.0\r\n```\r\nHowever I think we should recommend users to stick to the same version used by Spark, i.e., 3.2.2\r\n\r\n2. users build Spark with custom Hadoop version such as 3.1.0/3.2.1 you mentioned via the `hadoop.version` property, and use this to talk to cloud storage like S3. \r\n\r\nTo enable these use cases we may have to introduce another Maven property to switch back to non-shaded client, and update here as well.\r\n",
        "createdAt" : "2021-01-15T19:02:50Z",
        "updatedAt" : "2021-01-15T19:02:51Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "ec81333c-5497-48e2-bc18-6319ed8f5466",
        "parentId" : "85348cd8-148e-49bd-8759-66f7b787442e",
        "authorId" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "body" : "Yah I think (2) is my primary concern. We support building against a custom version using `-Dhadoop.version`, but right now it will break if you use `-Phadoop-3 -Dhadoop.version=3.1.0`. This one I believe you can at least work around by changing the `-Dhadoop-client-{runtime,api,minicluster}.artifact` properties, but here there's no way to work around it.",
        "createdAt" : "2021-01-15T19:14:40Z",
        "updatedAt" : "2021-01-15T19:14:40Z",
        "lastEditedBy" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "tags" : [
        ]
      },
      {
        "id" : "5972792f-4561-4ae4-b1e9-e9c3c99f0c89",
        "parentId" : "85348cd8-148e-49bd-8759-66f7b787442e",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Got it. Yes we can be more precise here, such as using a version map. I can update this.",
        "createdAt" : "2021-01-15T19:17:20Z",
        "updatedAt" : "2021-01-15T19:17:21Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      }
    ],
    "commit" : "cccb021189418f4be31b057931f40ef69fcccc4c",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +113,117 @@      ivyPath: Option[String],\n      remoteRepos: String): Seq[URL] = {\n    val hadoopJarNames = if (hadoopVersion.startsWith(\"3\")) {\n      Seq(s\"org.apache.hadoop:hadoop-client-api:$hadoopVersion\",\n        s\"org.apache.hadoop:hadoop-client-runtime:$hadoopVersion\")"
  },
  {
    "id" : "d56b9ef7-c5e1-4c6b-87e3-0c9a87dd78a7",
    "prId" : 30701,
    "prUrl" : "https://github.com/apache/spark/pull/30701#pullrequestreview-569550200",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7170793b-b630-481b-9027-9b2a35f42acf",
        "parentId" : null,
        "authorId" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "body" : "Doesn't Hive pull in other non-shaded Hadoop dependencies that could cause issues? ",
        "createdAt" : "2021-01-15T18:39:48Z",
        "updatedAt" : "2021-01-15T18:41:06Z",
        "lastEditedBy" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "tags" : [
        ]
      },
      {
        "id" : "33d13869-8d89-4f14-8523-4185bc417cc3",
        "parentId" : "7170793b-b630-481b-9027-9b2a35f42acf",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Actually this code is no longer required after #30284. Currently Spark will always load Hadoop classes from the built-in Hadoop version.",
        "createdAt" : "2021-01-15T19:11:21Z",
        "updatedAt" : "2021-01-15T19:11:21Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "ad41fe2e-77d6-47ba-b766-ed5410fdab2d",
        "parentId" : "7170793b-b630-481b-9027-9b2a35f42acf",
        "authorId" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "body" : "Right, I forgot about that one. Thanks for the clarification.",
        "createdAt" : "2021-01-15T19:16:26Z",
        "updatedAt" : "2021-01-15T19:16:26Z",
        "lastEditedBy" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "tags" : [
        ]
      }
    ],
    "commit" : "cccb021189418f4be31b057931f40ef69fcccc4c",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +127,131 @@      // this introduced from lower version of Hive could conflict with jars in Hadoop 3.2+, so\n      // exclude here in favor of the ones in Hadoop 3.2+\n      Seq(\"org.apache.hadoop:hadoop-auth\")\n    } else {\n      Seq.empty"
  }
]