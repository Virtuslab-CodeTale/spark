[
  {
    "id" : "f16cb032-528c-4584-9f20-d5707656b835",
    "prId" : 31203,
    "prUrl" : "https://github.com/apache/spark/pull/31203#pullrequestreview-574635797",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "66105305-2cee-40ec-8b15-3ba4c5a56415",
        "parentId" : null,
        "authorId" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "body" : "maybe\r\n```\r\ncase (maj, _, _) if maj > 3 => true\r\ncase (3, min, _) if min > 2 => true\r\ncase (3, 2, patch) if patch >=2 => true\r\n```\r\nSeems like we can reasonably assume that future versions of Hadoop will support the shaded client?",
        "createdAt" : "2021-01-22T20:51:00Z",
        "updatedAt" : "2021-01-26T21:30:32Z",
        "lastEditedBy" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "tags" : [
        ]
      },
      {
        "id" : "fe9e94cc-d6fd-4023-81da-ae44312526f9",
        "parentId" : "66105305-2cee-40ec-8b15-3ba4c5a56415",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "I think we'd better to wait until the future versions come out before changing this (so that we can verify firs). For instance, Hadoop 3.3.0 currently doesn't support shaded client (due to the hadoop-aws issue). But yeah the Hadoop 3.2.2+ should support the shaded client assuming there's no regression.",
        "createdAt" : "2021-01-22T20:59:10Z",
        "updatedAt" : "2021-01-26T21:30:32Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "baf13b43-48e4-4def-ac61-fccc39988a3e",
        "parentId" : "66105305-2cee-40ec-8b15-3ba4c5a56415",
        "authorId" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "body" : "Interesting... I just worry about forgetting to update this if/when we bump the Hadoop version in the future and causing a regression. Has the `hadoop-aws` fix made it to be targeted for Hadoop 3.3.1? If so, can we reasonably assume that 3.2.2+, 3.3.1+, and 3.4.0+ will have it?\r\n\r\nIt seems you're more tied into what's happening in the Hadoop world than I am these days so I'll take your word in either direction. If we decide _not_ to future-proof it, can we create a follow-up JIRA to revisit it once some future release is out at which time we would be confident in putting a wildcard?",
        "createdAt" : "2021-01-22T21:05:19Z",
        "updatedAt" : "2021-01-26T21:30:32Z",
        "lastEditedBy" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "tags" : [
        ]
      },
      {
        "id" : "8805ee37-0e77-4043-a382-eb9327493d01",
        "parentId" : "66105305-2cee-40ec-8b15-3ba4c5a56415",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "I think your concern is valid. One thing we can do is perhaps adding a test to make sure that the built-in Hadoop version is always compatible with the shaded client. So that in future if we upgrade Hadoop version & forget to do this, the test will break.",
        "createdAt" : "2021-01-22T21:23:33Z",
        "updatedAt" : "2021-01-26T21:30:32Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "09c32e0b-aa0c-4bc5-b390-5a6a1c7dd937",
        "parentId" : "66105305-2cee-40ec-8b15-3ba4c5a56415",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "And yes we can assume that 3.2.2+, 3.3.1+ and 3.4.0+ will all have the fix.",
        "createdAt" : "2021-01-22T21:26:17Z",
        "updatedAt" : "2021-01-26T21:30:32Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "1839b0bf-5e84-4272-a94f-24b8110dd7da",
        "parentId" : "66105305-2cee-40ec-8b15-3ba4c5a56415",
        "authorId" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "body" : "Excellent idea on adding a compatibility test for the built-in Hadoop version!",
        "createdAt" : "2021-01-22T21:28:21Z",
        "updatedAt" : "2021-01-26T21:30:32Z",
        "lastEditedBy" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "tags" : [
        ]
      }
    ],
    "commit" : "43367c51cc85c2893d663bc248ee95d4d15cdbde",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +111,115 @@    VersionUtils.majorMinorPatchVersion(hadoopVersion).exists {\n      case (3, 2, v) if v >= 2 => true\n      case _ => false\n    }\n  }"
  },
  {
    "id" : "8bef2c51-34e9-4b80-b33c-d81a554f33b4",
    "prId" : 31203,
    "prUrl" : "https://github.com/apache/spark/pull/31203#pullrequestreview-576768190",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "37deb64b-ace5-4370-b1ad-847fa4043269",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Got it. I agree that we need to change this again.\r\n> And yes we can assume that 3.2.2+, 3.3.1+ and 3.4.0+ will all have the fix.",
        "createdAt" : "2021-01-26T21:16:39Z",
        "updatedAt" : "2021-01-26T21:30:32Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "43367c51cc85c2893d663bc248ee95d4d15cdbde",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +111,115 @@    VersionUtils.majorMinorPatchVersion(hadoopVersion).exists {\n      case (3, 2, v) if v >= 2 => true\n      case _ => false\n    }\n  }"
  },
  {
    "id" : "7636e8dd-d39d-40d5-aa60-dee142188c5a",
    "prId" : 30701,
    "prUrl" : "https://github.com/apache/spark/pull/30701#pullrequestreview-569550740",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "85348cd8-148e-49bd-8759-66f7b787442e",
        "parentId" : null,
        "authorId" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "body" : "Will this break if `hadoopVersion` is 3.1.0, 3.2.1, etc. (due to the previous issues with the shaded client JARs)?",
        "createdAt" : "2021-01-15T18:38:27Z",
        "updatedAt" : "2021-01-15T18:41:06Z",
        "lastEditedBy" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "tags" : [
        ]
      },
      {
        "id" : "c632114d-6ecd-4e1a-9650-5df002e5e16c",
        "parentId" : "85348cd8-148e-49bd-8759-66f7b787442e",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Do you mean [HADOOP-16080](https://issues.apache.org/jira/browse/HADOOP-16080)? yes things could still break in the following cases.\r\n\r\n1. users build Spark without `-Phadoop-cloud` AND use a version doesn't have the fix in HADOOP-16080, such as:\r\n```\r\n$ bin/spark-shell --packages org.apache.hadoop:hadoop-aws:3.2.0,org.apache.hadoop:hadoop-common:3.2.0\r\n```\r\nHowever I think we should recommend users to stick to the same version used by Spark, i.e., 3.2.2\r\n\r\n2. users build Spark with custom Hadoop version such as 3.1.0/3.2.1 you mentioned via the `hadoop.version` property, and use this to talk to cloud storage like S3. \r\n\r\nTo enable these use cases we may have to introduce another Maven property to switch back to non-shaded client, and update here as well.\r\n",
        "createdAt" : "2021-01-15T19:02:50Z",
        "updatedAt" : "2021-01-15T19:02:51Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "ec81333c-5497-48e2-bc18-6319ed8f5466",
        "parentId" : "85348cd8-148e-49bd-8759-66f7b787442e",
        "authorId" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "body" : "Yah I think (2) is my primary concern. We support building against a custom version using `-Dhadoop.version`, but right now it will break if you use `-Phadoop-3 -Dhadoop.version=3.1.0`. This one I believe you can at least work around by changing the `-Dhadoop-client-{runtime,api,minicluster}.artifact` properties, but here there's no way to work around it.",
        "createdAt" : "2021-01-15T19:14:40Z",
        "updatedAt" : "2021-01-15T19:14:40Z",
        "lastEditedBy" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "tags" : [
        ]
      },
      {
        "id" : "5972792f-4561-4ae4-b1e9-e9c3c99f0c89",
        "parentId" : "85348cd8-148e-49bd-8759-66f7b787442e",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Got it. Yes we can be more precise here, such as using a version map. I can update this.",
        "createdAt" : "2021-01-15T19:17:20Z",
        "updatedAt" : "2021-01-15T19:17:21Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      }
    ],
    "commit" : "cccb021189418f4be31b057931f40ef69fcccc4c",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +113,117 @@      ivyPath: Option[String],\n      remoteRepos: String): Seq[URL] = {\n    val hadoopJarNames = if (hadoopVersion.startsWith(\"3\")) {\n      Seq(s\"org.apache.hadoop:hadoop-client-api:$hadoopVersion\",\n        s\"org.apache.hadoop:hadoop-client-runtime:$hadoopVersion\")"
  },
  {
    "id" : "d56b9ef7-c5e1-4c6b-87e3-0c9a87dd78a7",
    "prId" : 30701,
    "prUrl" : "https://github.com/apache/spark/pull/30701#pullrequestreview-569550200",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7170793b-b630-481b-9027-9b2a35f42acf",
        "parentId" : null,
        "authorId" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "body" : "Doesn't Hive pull in other non-shaded Hadoop dependencies that could cause issues? ",
        "createdAt" : "2021-01-15T18:39:48Z",
        "updatedAt" : "2021-01-15T18:41:06Z",
        "lastEditedBy" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "tags" : [
        ]
      },
      {
        "id" : "33d13869-8d89-4f14-8523-4185bc417cc3",
        "parentId" : "7170793b-b630-481b-9027-9b2a35f42acf",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Actually this code is no longer required after #30284. Currently Spark will always load Hadoop classes from the built-in Hadoop version.",
        "createdAt" : "2021-01-15T19:11:21Z",
        "updatedAt" : "2021-01-15T19:11:21Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "ad41fe2e-77d6-47ba-b766-ed5410fdab2d",
        "parentId" : "7170793b-b630-481b-9027-9b2a35f42acf",
        "authorId" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "body" : "Right, I forgot about that one. Thanks for the clarification.",
        "createdAt" : "2021-01-15T19:16:26Z",
        "updatedAt" : "2021-01-15T19:16:26Z",
        "lastEditedBy" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "tags" : [
        ]
      }
    ],
    "commit" : "cccb021189418f4be31b057931f40ef69fcccc4c",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +127,131 @@      // this introduced from lower version of Hive could conflict with jars in Hadoop 3.2+, so\n      // exclude here in favor of the ones in Hadoop 3.2+\n      Seq(\"org.apache.hadoop:hadoop-auth\")\n    } else {\n      Seq.empty"
  },
  {
    "id" : "b54835e7-11d3-4e31-a184-3ac3a624dd5e",
    "prId" : 29843,
    "prUrl" : "https://github.com/apache/spark/pull/29843#pullrequestreview-514216719",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a5d32d0d-b0f6-47e0-bade-e382b57f9cc4",
        "parentId" : null,
        "authorId" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "body" : "Why is `hadoop-auth` the only one excluded here? Won't Hive pull in a variety of the non-shaded Hadoop JARs?",
        "createdAt" : "2020-10-21T21:10:32Z",
        "updatedAt" : "2020-10-21T23:44:36Z",
        "lastEditedBy" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "tags" : [
        ]
      },
      {
        "id" : "cde4c98a-6868-4102-a70a-14a7b14aeb25",
        "parentId" : "a5d32d0d-b0f6-47e0-bade-e382b57f9cc4",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "I only excluded this one because it caused issues, but yes we could exclude more.\r\n\r\nA bit more context, I think currently the way it works is a bit messed up: it pulls hadoop dependencies from both Hive and the explicit `hadoop-client` dependency, while the latter uses the Hadoop version in Spark (which is 3.x in default, as opposed to 2.x in Hive). As result, one can end up with two versions of various Hadoop jars in the class path and conflicts can happen unexpectedly. This only happens when we are not sharing Hadoop classes though (which is rare): in the other path Spark always loads its own Hadoop classes first, then Hive's.\r\n\r\nThis was amplified when we replace hadoop-client with hadoop-client-api, since latter does not include other jars such as hadoop-common: it just put all classes in a single jar. ",
        "createdAt" : "2020-10-21T21:16:41Z",
        "updatedAt" : "2020-10-21T23:44:36Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "62bb0b69-721f-4b56-b90f-d8aae9fe472b",
        "parentId" : "a5d32d0d-b0f6-47e0-bade-e382b57f9cc4",
        "authorId" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "body" : "I thought that Ivy would do dependency conflict resolution to resolve such issues? Do you actually get two versions of the same module (e.g. `hadoop-common` 2.7 and 3.2) or mixed Hadoop modules (like `hadoop-common` 2.7 but `hadoop-auth` 3.2)? I was under the impression that the former should be resolved automatically.\r\n\r\nIn any case -- this seems like a good opportunity to clean it up. If we exclude everything _except_ `hadoop-client-api` and `hadoop-client-runtime`, that should cover excluding all of Hive's JARs, right?",
        "createdAt" : "2020-10-21T21:22:58Z",
        "updatedAt" : "2020-10-21T23:44:36Z",
        "lastEditedBy" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "tags" : [
        ]
      },
      {
        "id" : "5c2f6108-ad69-4481-a86f-9cc639d784ac",
        "parentId" : "a5d32d0d-b0f6-47e0-bade-e382b57f9cc4",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "No not same artifacts with different versions, but different artifacts with different versions (e.g., `hadoop-common` 2.7 but `hadoop-auth` 3.2).\r\n\r\nYes we could do a cleanup and remove all other dependencies here, although somewhat related I'm wondering whether we do need to support not-sharing classes at all. It is currently used only in tests and in a rare case where it can't find [the specified hadoop version](https://github.com/apache/spark/blob/master/sql/hive/src/main/scala/org/apache/spark/sql/hive/client/IsolatedClientLoader.scala#L78) (I don't fully understand why it switch to not using shared classes in the latter). ",
        "createdAt" : "2020-10-21T21:31:17Z",
        "updatedAt" : "2020-10-21T23:44:36Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "369b6f2c-5cad-4d84-aa67-48f84ebf4521",
        "parentId" : "a5d32d0d-b0f6-47e0-bade-e382b57f9cc4",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "It's also related to the definition of \"not sharing Hadoop classes\": does it mean Hadoop classes from Hive dependency take precedence over those from Spark itself? if that's true then we probably shouldn't exclude those.",
        "createdAt" : "2020-10-21T21:36:01Z",
        "updatedAt" : "2020-10-21T23:44:36Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "6e10d1f8-383a-42fd-9603-6f657639ba75",
        "parentId" : "a5d32d0d-b0f6-47e0-bade-e382b57f9cc4",
        "authorId" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "body" : "True... I've recently been debugging some test issues in our environment that are caused by this behavior, and am also confused by its existence.\r\n\r\nGiven that we explicitly specify a Hadoop version to `downloadVersion` (which can be different from the one the rest of Spark is using), it seems that should take precedence over the ones Hive pulls in.",
        "createdAt" : "2020-10-21T21:40:33Z",
        "updatedAt" : "2020-10-21T23:44:36Z",
        "lastEditedBy" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "tags" : [
        ]
      },
      {
        "id" : "d4a54a52-8091-41c3-990e-7f36a0f2bb2c",
        "parentId" : "a5d32d0d-b0f6-47e0-bade-e382b57f9cc4",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Yes it's confusing. I'll probably leave this as it is and open another JIRA specifically for this issue, and see what the broader community think.",
        "createdAt" : "2020-10-21T21:58:28Z",
        "updatedAt" : "2020-10-21T23:44:36Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      }
    ],
    "commit" : "0359047cd02ed7051ade3002c84ab5f3255a67cb",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +133,137 @@      // this introduced from lower version of Hive could conflict with jars in Hadoop 3.2+, so\n      // exclude here in favor of the ones in Hadoop 3.2+\n      Seq(\"org.apache.hadoop:hadoop-auth\")\n    } else {\n      Seq.empty"
  }
]