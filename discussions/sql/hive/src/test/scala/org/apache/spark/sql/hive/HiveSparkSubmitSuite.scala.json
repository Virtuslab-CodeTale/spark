[
  {
    "id" : "1a1597fa-8c76-4bb3-bda3-d8f7bd5a7215",
    "prId" : 25426,
    "prUrl" : "https://github.com/apache/spark/pull/25426#pullrequestreview-274041775",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "879b26f4-67d7-4f6a-af1e-fe4807ba26de",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "We don't have `spark.sql.hive.metastore.version=0.12` in this test case. Could you add more explanation about this into the PR description? \r\n> spark.sql.hive.metastore.version of these tests is lower than 2.0",
        "createdAt" : "2019-08-13T02:57:56Z",
        "updatedAt" : "2019-08-13T03:09:49Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "032cf57d-6858-49dd-a3a3-5dae083c91b3",
        "parentId" : "879b26f4-67d7-4f6a-af1e-fe4807ba26de",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "https://github.com/apache/spark/blob/d7b7a1ac02659940f0e70e5cfb23c2bc34fb8775/sql/hive/src/test/scala/org/apache/spark/sql/hive/HiveSparkSubmitSuite.scala#L674-L685",
        "createdAt" : "2019-08-13T02:59:39Z",
        "updatedAt" : "2019-08-13T03:09:49Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      },
      {
        "id" : "e0d49ba4-ffd2-41c4-8bfd-b7210a7f7f6a",
        "parentId" : "879b26f4-67d7-4f6a-af1e-fe4807ba26de",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Thanks. Got it!",
        "createdAt" : "2019-08-13T03:02:20Z",
        "updatedAt" : "2019-08-13T03:09:49Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "23c721009299539f3b416523ad14d91311416c01",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +166,170 @@\n  test(\"SPARK-9757 Persist Parquet relation with decimal column\") {\n    assume(!SystemUtils.isJavaVersionAtLeast(JavaVersion.JAVA_9))\n    val unusedJar = TestUtils.createJarWithClasses(Seq.empty)\n    val args = Seq("
  },
  {
    "id" : "7dbbf2a0-fc1c-4216-97b1-5543531dbe0d",
    "prId" : 24751,
    "prUrl" : "https://github.com/apache/spark/pull/24751#pullrequestreview-284036275",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f6cb9d5b-a33d-44cd-b0a1-9b735dc0992e",
        "parentId" : null,
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "It should throw an `AnalysisException`  if we remove jar3 and jar4. But it is successful now:\r\n```\r\n[info]   2019-09-04 22:54:30.372 - stderr> 19/09/04 22:54:30 WARN Master: App app-20190904225430-0000 requires more resource than any of Workers could have.\r\n[info]   2019-09-04 22:54:37.434 - stderr> Exception in thread \"main\" org.apache.spark.sql.AnalysisException: Can not load class 'org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMax' when registering the function 'example_max', please make sure it is on the classpath;\r\n[info]   2019-09-04 22:54:37.435 - stderr>\r\n```",
        "createdAt" : "2019-09-05T06:14:52Z",
        "updatedAt" : "2019-09-05T06:14:53Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      }
    ],
    "commit" : "addb9087b34bfb83aec9c300f473b88a08b670d9",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +111,115 @@    val jar3 = HiveTestUtils.getHiveContribJar.getCanonicalPath\n    val jar4 = HiveTestUtils.getHiveHcatalogCoreJar.getCanonicalPath\n    val jarsString = Seq(jar1, jar2, jar3, jar4).map(j => j.toString).mkString(\",\")\n    val args = Seq(\n      \"--class\", SparkSubmitClassLoaderTest.getClass.getName.stripSuffix(\"$\"),"
  }
]