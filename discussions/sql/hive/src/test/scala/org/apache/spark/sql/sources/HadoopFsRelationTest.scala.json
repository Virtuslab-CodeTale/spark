[
  {
    "id" : "e3c6b402-4b0b-4dee-a1cb-a600d5ced8d4",
    "prId" : 28481,
    "prUrl" : "https://github.com/apache/spark/pull/28481#pullrequestreview-408453124",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "19f8221e-7689-4b5c-99cb-30fc2fce45f8",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "I bumped the number to have the same amount of random values as before the changes.",
        "createdAt" : "2020-05-08T18:57:40Z",
        "updatedAt" : "2020-05-10T19:47:28Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "a78e18b2631ed29703e4791cc850d62e3c58faaa",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +160,164 @@              .add(\"col\", dataType, nullable = true)\n            val rdd =\n              spark.sparkContext.parallelize((1 to 20).map(i => Row(i, dataGenerator())))\n            val df = spark.createDataFrame(rdd, schema).orderBy(\"index\").coalesce(1)\n"
  }
]