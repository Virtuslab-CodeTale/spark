[
  {
    "id" : "01ab72d8-af30-4de6-b717-9fa7d17f7b42",
    "prId" : 30665,
    "prUrl" : "https://github.com/apache/spark/pull/30665#pullrequestreview-548187606",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "70cb0253-f2e0-4223-9451-dd8f633f1941",
        "parentId" : null,
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "How about?\r\n```scala\r\ncheckAnswer(\r\n  sql(\"SELECT udtf_stack2(2, 'A', 10, date '2015-01-01', 'B', 20, date '2016-01-01')\"),\r\n  Seq(Row(\"A\", 10, java.sql.Date.valueOf(\"2015-01-01\")),\r\n    Row(\"B\", 20, java.sql.Date.valueOf(\"2016-01-01\"))))\r\n```",
        "createdAt" : "2020-12-09T13:35:30Z",
        "updatedAt" : "2020-12-09T13:35:31Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      },
      {
        "id" : "539219d7-8656-44c3-9c51-ce77bdd270b4",
        "parentId" : "70cb0253-f2e0-4223-9451-dd8f633f1941",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "Could you update test name?",
        "createdAt" : "2020-12-09T13:36:19Z",
        "updatedAt" : "2020-12-09T13:36:19Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      }
    ],
    "commit" : "0e3dca1ea3228747a43d573075bc6162cc76d475",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +2135,2139 @@      val num =\n        sql(\"SELECT udtf_stack2(2, 'A', 10, date '2015-01-01', 'B', 20, date '2016-01-01')\").count()\n      assert(num === 2)\n    }\n  }"
  },
  {
    "id" : "18eece97-a7cd-44bb-8cb5-9d5a73479caa",
    "prId" : 29761,
    "prUrl" : "https://github.com/apache/spark/pull/29761#pullrequestreview-490241510",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1ca557df-aad8-4b7e-bb9e-02059469e848",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Oh, I missed that you meant to change this.",
        "createdAt" : "2020-09-17T04:52:53Z",
        "updatedAt" : "2020-09-17T06:34:53Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "7d63fd96462fda9b820873a679464fe2662e6869",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +2207,2211 @@  }\n\n  test(\"SPARK-21912 Parquet table should not create invalid column names\") {\n    Seq(\" \", \",\", \";\", \"{\", \"}\", \"(\", \")\", \"\\n\", \"\\t\", \"=\").foreach { name =>\n      val source = \"PARQUET\""
  },
  {
    "id" : "7b8b4fc6-4b54-4f1a-a204-beeece52ac42",
    "prId" : 29672,
    "prUrl" : "https://github.com/apache/spark/pull/29672#pullrequestreview-485668578",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7d609a8b-59bb-4da6-ad8e-75d8ad8e9533",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Does this test fail w/o this patch? Looks it can pass even in master.",
        "createdAt" : "2020-09-10T05:20:27Z",
        "updatedAt" : "2020-09-10T05:26:05Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "f8613cfd-818b-460e-a821-75d48dea85dc",
        "parentId" : "7d609a8b-59bb-4da6-ad8e-75d8ad8e9533",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> Does this test fail w/o this patch? Looks it can pass even in master.\r\n\r\nI miss the point that SQLConf.get is related to current thread's SparkSession.",
        "createdAt" : "2020-09-10T07:50:12Z",
        "updatedAt" : "2020-09-10T07:50:12Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "fc221f5f5c9a73b4627ba24517c925393acf21ba",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +2562,2566 @@  }\n\n  test(\"SPARK-32818: make metastore convert session level configurable\") {\n    withSQLConf(HiveUtils.CONVERT_METASTORE_ORC.key -> \"true\") {\n      withTable(\"t\") {"
  },
  {
    "id" : "28464d48-b11e-4b7e-bcf9-8ccc679ddddc",
    "prId" : 29672,
    "prUrl" : "https://github.com/apache/spark/pull/29672#pullrequestreview-485589576",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a1f0ee65-f2c4-4019-b997-80c290b8eb2b",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "nit: `SparkPlan` not used.",
        "createdAt" : "2020-09-10T05:20:38Z",
        "updatedAt" : "2020-09-10T05:26:05Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "fc221f5f5c9a73b4627ba24517c925393acf21ba",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +34,38 @@import org.apache.spark.sql.catalyst.parser.ParseException\nimport org.apache.spark.sql.catalyst.plans.logical.{LogicalPlan, SubqueryAlias}\nimport org.apache.spark.sql.execution.{FileSourceScanExec, SparkPlan, TestUncaughtExceptionHandler}\nimport org.apache.spark.sql.execution.adaptive.{DisableAdaptiveExecutionSuite, EnableAdaptiveExecutionSuite}\nimport org.apache.spark.sql.execution.command.{FunctionsCommand, LoadDataCommand}"
  },
  {
    "id" : "fc3a844f-c122-4af4-9d74-198507973e62",
    "prId" : 29672,
    "prUrl" : "https://github.com/apache/spark/pull/29672#pullrequestreview-485589576",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c6430fe8-1d91-49bb-a76e-d0dc6016bece",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "I think we don't need the partition definition for this test and how about writing it like this;\r\n```\r\nsql(\"CREATE TABLE t (i INT) USING hive\")\r\nsql(\"SELECT * FROM t\")\r\n```",
        "createdAt" : "2020-09-10T05:22:02Z",
        "updatedAt" : "2020-09-10T05:26:05Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "fc221f5f5c9a73b4627ba24517c925393acf21ba",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +2566,2570 @@      withTable(\"t\") {\n        sql(\"CREATE TABLE t (i INT) PARTITIONED BY (p STRING) STORED AS ORC\")\n        sql(\"INSERT INTO t PARTITION(p='20200901') SELECT 1\")\n\n        val query ="
  },
  {
    "id" : "bf37afe7-ea9d-498c-bef4-a169ebe1bde7",
    "prId" : 29156,
    "prUrl" : "https://github.com/apache/spark/pull/29156#pullrequestreview-451173158",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5b8e32fb-e26c-4a61-b404-c9004a26ba65",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Just in case, could you check that the hist is correctly applied?",
        "createdAt" : "2020-07-19T22:58:34Z",
        "updatedAt" : "2020-07-19T22:58:34Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "75b7ac61-f4c9-4e12-9e87-4bcac82e11be",
        "parentId" : "5b8e32fb-e26c-4a61-b404-c9004a26ba65",
        "authorId" : "919368dc-8bac-4d8f-9fa6-be962263fc1e",
        "body" : "Thank you, I need to check the hist, seems something wrong with the patch.",
        "createdAt" : "2020-07-19T23:33:01Z",
        "updatedAt" : "2020-07-19T23:33:01Z",
        "lastEditedBy" : "919368dc-8bac-4d8f-9fa6-be962263fc1e",
        "tags" : [
        ]
      }
    ],
    "commit" : "be0711aa5582ea27d3d1f6b89a712ba7edcea057",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +2563,2567 @@    withTempView(\"t\") {\n      sql(\"create temporary view t as select 1 as id\")\n      sql(\"with cte as (select /*+ BROADCAST(id) */ id from t) select id from cte\")\n      sql(\"with cte as (select /*+ COALESCE(3) */ id from t) select id from cte\")\n    }"
  },
  {
    "id" : "268da623-1f29-4659-89dc-606680ec898f",
    "prId" : 28765,
    "prUrl" : "https://github.com/apache/spark/pull/28765#pullrequestreview-426938804",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "967f93f5-9572-47f9-99e0-0cd7e2bfa354",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "nit: add a blank line between test cases",
        "createdAt" : "2020-06-09T09:06:52Z",
        "updatedAt" : "2020-06-09T09:08:47Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "d94906541099dd5eda4e026526ee1c317247bdd0",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +2546,2550 @@  }\n\n  test(\"SPARK-29295: dynamic partition map parsed from partition path should be case insensitive\") {\n    withTable(\"t\") {\n      withSQLConf(\"hive.exec.dynamic.partition\" -> \"true\","
  },
  {
    "id" : "207bd1c6-45ab-4a56-a482-a76148de75ae",
    "prId" : 27025,
    "prUrl" : "https://github.com/apache/spark/pull/27025#pullrequestreview-337008977",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f759ef4f-0304-4961-9142-3b13fff33542",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "without this patch, which step will we fail in this test?",
        "createdAt" : "2019-12-30T06:10:26Z",
        "updatedAt" : "2019-12-30T11:12:11Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "9cd8e15e-bafd-4694-9406-fa50df325788",
        "parentId" : "f759ef4f-0304-4961-9142-3b13fff33542",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Second run of query execution will fail. It succeeds on first run of query execution, because the actual loading for jar happens on first reference (lazy loading). CREATE FUNCTION doesn't seem to load the jar at that time.",
        "createdAt" : "2019-12-30T06:13:06Z",
        "updatedAt" : "2019-12-30T11:12:11Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "0bdf9d8a-3e40-4d21-b70d-cec1f11517f8",
        "parentId" : "f759ef4f-0304-4961-9142-3b13fff33542",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "FYI: without the patch, this test throws below exception and fails:\r\n\r\n```\r\nNo handler for UDF/UDAF/UDTF 'org.apache.hadoop.hive.contrib.udtf.example.GenericUDTFCount3': java.lang.ClassNotFoundException: org.apache.hadoop.hive.contrib.udtf.example.GenericUDTFCount3\r\nPlease make sure your function overrides `public StructObjectInspector initialize(ObjectInspector[] args)`.; line 1 pos 7\r\norg.apache.spark.sql.AnalysisException: No handler for UDF/UDAF/UDTF 'org.apache.hadoop.hive.contrib.udtf.example.GenericUDTFCount3': java.lang.ClassNotFoundException: org.apache.hadoop.hive.contrib.udtf.example.GenericUDTFCount3\r\nPlease make sure your function overrides `public StructObjectInspector initialize(ObjectInspector[] args)`.; line 1 pos 7\r\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:382)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\r\n\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\r\n\tat org.apache.spark.sql.hive.HiveShim$HiveFunctionWrapper.createFunction(HiveShim.scala:245)\r\n\tat org.apache.spark.sql.hive.HiveGenericUDTF.function$lzycompute(hiveUDFs.scala:207)\r\n\tat org.apache.spark.sql.hive.HiveGenericUDTF.function(hiveUDFs.scala:206)\r\n\tat org.apache.spark.sql.hive.HiveGenericUDTF.outputInspector$lzycompute(hiveUDFs.scala:216)\r\n\tat org.apache.spark.sql.hive.HiveGenericUDTF.outputInspector(hiveUDFs.scala:216)\r\n\tat org.apache.spark.sql.hive.HiveGenericUDTF.elementSchema$lzycompute(hiveUDFs.scala:224)\r\n\tat org.apache.spark.sql.hive.HiveGenericUDTF.elementSchema(hiveUDFs.scala:224)\r\n\tat org.apache.spark.sql.hive.HiveSessionCatalog.$anonfun$makeFunctionExpression$2(HiveSessionCatalog.scala:96)\r\n\tat scala.util.Failure.getOrElse(Try.scala:222)\r\n\tat org.apache.spark.sql.hive.HiveSessionCatalog.makeFunctionExpression(HiveSessionCatalog.scala:72)\r\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.$anonfun$makeFunctionBuilder$1(SessionCatalog.scala:1247)\r\n\tat org.apache.spark.sql.catalyst.analysis.SimpleFunctionRegistry.lookupFunction(FunctionRegistry.scala:121)\r\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.lookupFunction(SessionCatalog.scala:1415)\r\n\tat org.apache.spark.sql.hive.HiveSessionCatalog.super$lookupFunction(HiveSessionCatalog.scala:135)\r\n\tat org.apache.spark.sql.hive.HiveSessionCatalog.$anonfun$lookupFunction0$2(HiveSessionCatalog.scala:135)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.sql.hive.HiveSessionCatalog.lookupFunction0(HiveSessionCatalog.scala:135)\r\n\tat org.apache.spark.sql.hive.HiveSessionCatalog.lookupFunction(HiveSessionCatalog.scala:128)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$15$$anonfun$applyOrElse$96.$anonfun$applyOrElse$99(Analyzer.scala:1739)\r\n\tat org.apache.spark.sql.catalyst.analysis.package$.withPosition(package.scala:53)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$15$$anonfun$applyOrElse$96.applyOrElse(Analyzer.scala:1739)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$15$$anonfun$applyOrElse$96.applyOrElse(Analyzer.scala:1722)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:286)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:72)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:286)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$3(TreeNode.scala:291)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:376)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:214)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:374)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:327)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:291)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsDown$1(QueryPlan.scala:87)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:109)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:72)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:109)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:120)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$3(QueryPlan.scala:125)\r\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\r\n\tat scala.collection.immutable.List.foreach(List.scala:392)\r\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\r\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\r\n\tat scala.collection.immutable.List.map(List.scala:298)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:125)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:130)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:214)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:130)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsDown(QueryPlan.scala:87)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressions(QueryPlan.scala:78)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$15.applyOrElse(Analyzer.scala:1722)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$15.applyOrElse(Analyzer.scala:1720)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$3(AnalysisHelper.scala:90)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:72)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:90)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$.apply(Analyzer.scala:1720)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$.apply(Analyzer.scala:1719)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:130)\r\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\r\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\r\n\tat scala.collection.immutable.List.foldLeft(List.scala:89)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:127)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:119)\r\n\tat scala.collection.immutable.List.foreach(List.scala:392)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:119)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:168)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:162)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:122)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:98)\r\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:98)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:146)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:145)\r\n\tat org.apache.spark.sql.hive.test.TestHiveQueryExecution.analyzed$lzycompute(TestHive.scala:606)\r\n\tat org.apache.spark.sql.hive.test.TestHiveQueryExecution.analyzed(TestHive.scala:589)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:58)\r\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:87)\r\n\tat org.apache.spark.sql.hive.test.TestHiveSparkSession.sql(TestHive.scala:238)\r\n\tat org.apache.spark.sql.test.SQLTestUtilsBase.$anonfun$sql$1(SQLTestUtils.scala:216)\r\n\tat org.apache.spark.sql.hive.execution.SQLQuerySuite.$anonfun$new$488(SQLQuerySuite.scala:2533)\r\n\tat org.apache.spark.sql.QueryTest.checkAnswer(QueryTest.scala:139)\r\n\tat org.apache.spark.sql.hive.execution.SQLQuerySuite.$anonfun$new$486(SQLQuerySuite.scala:2534)\r\n\tat org.apache.spark.sql.test.SQLTestUtilsBase.withUserDefinedFunction(SQLTestUtils.scala:239)\r\n\tat org.apache.spark.sql.test.SQLTestUtilsBase.withUserDefinedFunction$(SQLTestUtils.scala:237)\r\n\tat org.apache.spark.sql.hive.execution.SQLQuerySuite.withUserDefinedFunction(SQLQuerySuite.scala:70)\r\n\tat org.apache.spark.sql.hive.execution.SQLQuerySuite.$anonfun$new$485(SQLQuerySuite.scala:2501)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:221)\r\n\tat org.apache.spark.sql.hive.execution.SQLQuerySuite.$anonfun$new$484(SQLQuerySuite.scala:2501)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)\r\n\tat org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)\r\n\tat org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)\r\n\tat org.scalatest.Transformer.apply(Transformer.scala:22)\r\n\tat org.scalatest.Transformer.apply(Transformer.scala:20)\r\n\tat org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)\r\n\tat org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:149)\r\n\tat org.scalatest.FunSuiteLike.invokeWithFixture$1(FunSuiteLike.scala:184)\r\n\tat org.scalatest.FunSuiteLike.$anonfun$runTest$1(FunSuiteLike.scala:196)\r\n\tat org.scalatest.SuperEngine.runTestImpl(Engine.scala:286)\r\n\tat org.scalatest.FunSuiteLike.runTest(FunSuiteLike.scala:196)\r\n\tat org.scalatest.FunSuiteLike.runTest$(FunSuiteLike.scala:178)\r\n\tat org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(SparkFunSuite.scala:56)\r\n\tat org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:221)\r\n\tat org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:214)\r\n\tat org.apache.spark.SparkFunSuite.runTest(SparkFunSuite.scala:56)\r\n\tat org.scalatest.FunSuiteLike.$anonfun$runTests$1(FunSuiteLike.scala:229)\r\n\tat org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:393)\r\n\tat scala.collection.immutable.List.foreach(List.scala:392)\r\n\tat org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:381)\r\n\tat org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:376)\r\n\tat org.scalatest.SuperEngine.runTestsImpl(Engine.scala:458)\r\n\tat org.scalatest.FunSuiteLike.runTests(FunSuiteLike.scala:229)\r\n\tat org.scalatest.FunSuiteLike.runTests$(FunSuiteLike.scala:228)\r\n\tat org.scalatest.FunSuite.runTests(FunSuite.scala:1560)\r\n\tat org.scalatest.Suite.run(Suite.scala:1124)\r\n\tat org.scalatest.Suite.run$(Suite.scala:1106)\r\n\tat org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1560)\r\n\tat org.scalatest.FunSuiteLike.$anonfun$run$1(FunSuiteLike.scala:233)\r\n\tat org.scalatest.SuperEngine.runImpl(Engine.scala:518)\r\n\tat org.scalatest.FunSuiteLike.run(FunSuiteLike.scala:233)\r\n\tat org.scalatest.FunSuiteLike.run$(FunSuiteLike.scala:232)\r\n\tat org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:56)\r\n\tat org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)\r\n\tat org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)\r\n\tat org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)\r\n\tat org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:56)\r\n\tat org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:45)\r\n\tat org.scalatest.tools.Runner$.$anonfun$doRunRunRunDaDoRunRun$13(Runner.scala:1349)\r\n\tat org.scalatest.tools.Runner$.$anonfun$doRunRunRunDaDoRunRun$13$adapted(Runner.scala:1343)\r\n\tat scala.collection.immutable.List.foreach(List.scala:392)\r\n\tat org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:1343)\r\n\tat org.scalatest.tools.Runner$.$anonfun$runOptionallyWithPassFailReporter$24(Runner.scala:1033)\r\n\tat org.scalatest.tools.Runner$.$anonfun$runOptionallyWithPassFailReporter$24$adapted(Runner.scala:1011)\r\n\tat org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:1509)\r\n\tat org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1011)\r\n\tat org.scalatest.tools.Runner$.run(Runner.scala:850)\r\n\tat org.scalatest.tools.Runner.run(Runner.scala)\r\n\tat org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.runScalaTest2(ScalaTestRunner.java:133)\r\n\tat org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.main(ScalaTestRunner.java:27)\r\n```",
        "createdAt" : "2019-12-30T06:28:09Z",
        "updatedAt" : "2019-12-30T11:12:11Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "39a2171d361f38b54640c07ccb8990aa4c204238",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +2494,2498 @@  }\n\n  test(\"SPARK-26560 Spark should be able to run Hive UDF using jar regardless of \" +\n    \"current thread context classloader\") {\n    // force to use Spark classloader as other test (even in other test suites) may change the"
  },
  {
    "id" : "cabeda7e-7472-4e4a-9d8c-9ace38867d76",
    "prId" : 27025,
    "prUrl" : "https://github.com/apache/spark/pull/27025#pullrequestreview-337073617",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e6eb7b68-23e4-48ff-94e6-8d62597a97ea",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Do you mean even if we set back the class loader, this test still fails without this patch?",
        "createdAt" : "2019-12-30T07:25:12Z",
        "updatedAt" : "2019-12-30T11:12:11Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "95291de1-19e1-46f2-9f79-f27d4f0460b1",
        "parentId" : "e6eb7b68-23e4-48ff-94e6-8d62597a97ea",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Yes, because the jar is loaded in jarClassloader. (Not \"even if\". It's needed to fail the test: commenting this line would make the test pass.)\r\n\r\nIf we don't change the thread context classloader manually, current thread context classloader might be still likely jarClassloader, but it can be changed (one example is spark-shell which will roll back context classloader per prompt if I understand correctly). Here we mimic the behavior of `spark-shell`.",
        "createdAt" : "2019-12-30T07:44:05Z",
        "updatedAt" : "2019-12-30T11:12:11Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "a87589d3-5702-4840-a7fe-376c8abf13e8",
        "parentId" : "e6eb7b68-23e4-48ff-94e6-8d62597a97ea",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "got it. Can we add some comments to explain it?",
        "createdAt" : "2019-12-30T10:14:25Z",
        "updatedAt" : "2019-12-30T11:12:11Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "a6c9ba79-e0dd-466d-b5eb-dcaeeca35290",
        "parentId" : "e6eb7b68-23e4-48ff-94e6-8d62597a97ea",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Sure! Will update it.",
        "createdAt" : "2019-12-30T11:04:29Z",
        "updatedAt" : "2019-12-30T11:12:11Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "cb9a6abb-27d3-4ad9-af7c-223cbb8cdd95",
        "parentId" : "e6eb7b68-23e4-48ff-94e6-8d62597a97ea",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Just updated.",
        "createdAt" : "2019-12-30T11:15:25Z",
        "updatedAt" : "2019-12-30T11:15:25Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "39a2171d361f38b54640c07ccb8990aa4c204238",
    "line" : 52,
    "diffHunk" : "@@ -1,1 +2533,2537 @@        // example is spark-shell, which thread context classloader rolls back automatically. This\n        // mimics the behavior of spark-shell.\n        Thread.currentThread().setContextClassLoader(sparkClassLoader)\n        checkAnswer(\n          sql(\"SELECT udtf_count3(a) FROM (SELECT 1 AS a FROM src LIMIT 3) t\"),"
  }
]