[
  {
    "id" : "823520cf-90ae-4773-8678-c87e06fe6151",
    "prId" : 33488,
    "prUrl" : "https://github.com/apache/spark/pull/33488#pullrequestreview-714892385",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "81804bb2-9468-4f58-a938-fd367f3126d0",
        "parentId" : null,
        "authorId" : "29c89154-22f7-40ee-b295-115a55734d6c",
        "body" : "parquet doesn't support null(spark)/void(hive) type",
        "createdAt" : "2021-07-26T14:09:57Z",
        "updatedAt" : "2021-07-26T14:09:58Z",
        "lastEditedBy" : "29c89154-22f7-40ee-b295-115a55734d6c",
        "tags" : [
        ]
      }
    ],
    "commit" : "c833c86da7778988376b20ccf2b70e5dec8ae1bd",
    "line" : 39,
    "diffHunk" : "@@ -1,1 +2403,2407 @@      assertAnalysisError(\n        \"CREATE TABLE t2 STORED AS PARQUET AS SELECT null as null_col\",\n        \"Unknown field type: void\")\n\n      sql(\"CREATE TABLE t3 AS SELECT NULL AS null_col\")"
  },
  {
    "id" : "c9de3730-618c-497e-8e16-6ee4a45a7944",
    "prId" : 31639,
    "prUrl" : "https://github.com/apache/spark/pull/31639#pullrequestreview-599538123",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "071021a1-631b-46ee-b94a-26d825f21609",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Ah, I see. Since we don't allow duplicate names in a top-level, it seems we need to follow it in this case, too. cc: @cloud-fan \r\n```\r\nscala> spark.sql(\"CREATE VIEW v1 AS SELECT 'a' AS `a`, 1 AS b\")\r\nscala> spark.sql(\"ALTER VIEW v1 AS SELECT 'a' AS `b`, 1 AS b\")\r\norg.apache.spark.sql.AnalysisException: Found duplicate column(s) in the view definition: `b`\r\n```",
        "createdAt" : "2021-02-25T02:10:56Z",
        "updatedAt" : "2021-03-01T18:08:49Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "4780a611-2fcd-4202-b92f-1140959c789c",
        "parentId" : "071021a1-631b-46ee-b94a-26d825f21609",
        "authorId" : "94083dd5-86b7-47ec-bcb5-1b0831a32179",
        "body" : "So you want me to add this test ?",
        "createdAt" : "2021-02-25T23:05:53Z",
        "updatedAt" : "2021-03-01T18:08:49Z",
        "lastEditedBy" : "94083dd5-86b7-47ec-bcb5-1b0831a32179",
        "tags" : [
        ]
      },
      {
        "id" : "8e692e89-7cca-4fec-81e1-104ddb23a8e8",
        "parentId" : "071021a1-631b-46ee-b94a-26d825f21609",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "This is out-of-scope in this PR. Do you wanna work on it? If so, please feel free to file jira for it.",
        "createdAt" : "2021-02-26T12:41:49Z",
        "updatedAt" : "2021-03-01T18:08:49Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "a458128a4089a381ca2c9a7339cfd99e635c66e8",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +206,210 @@      checkAnswer(spark.table(\"v\"), Row(Row(\"a\", 1)) :: Nil)\n\n      spark.sql(\"ALTER VIEW v AS SELECT STRUCT('a' AS `c`, 1 AS b) q1\")\n      val df = spark.table(\"v\")\n      assert(\"q1\".equals(df.schema.fields(0).name))"
  },
  {
    "id" : "ce3251af-8f89-4410-9866-31538045eb11",
    "prId" : 31133,
    "prUrl" : "https://github.com/apache/spark/pull/31133#pullrequestreview-566035682",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2b1d9e5e-053d-4a3b-91cd-7bb6030c6dbb",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Just a question because it's a little counter-intuitive. Is there a reason why we start with `col2` and add `col1` later? Usually, I thought we start with `col1` and add `col2` later.",
        "createdAt" : "2021-01-11T22:17:40Z",
        "updatedAt" : "2021-02-05T07:01:10Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "296f6a0d-70b5-45b0-aaf6-deacca943502",
        "parentId" : "2b1d9e5e-053d-4a3b-91cd-7bb6030c6dbb",
        "authorId" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "body" : "Yes, regarding schema evolution rules you can add a field to an arbitrary position, see the first example here:\r\nhttps://docs.oracle.com/database/nosql-12.1.3.0/GettingStartedGuide/schemaevolution.html\r\n \r\nMy intention with this example and field naming was to illustrate the worst case (the column mismatch error) and emphasize its root cause. And even by adding new fields at the end one just decrease the scope of the problem as there will be still wrong values (null) for the new fields. \r\n\r\nMoreover when an existing field is removed the column mismatch usually cannot be avoided, example:\r\n\r\n```\r\nsql(\"\"\"\r\n  CREATE TABLE t PARTITIONED BY (ds string)\r\n  ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe'\r\n  WITH SERDEPROPERTIES ('avro.schema.literal'='\r\n  {\r\n    \"namespace\": \"test\",\r\n    \"name\": \"some_schema\",\r\n    \"type\": \"record\",\r\n    \"fields\": [\r\n      {\r\n        \"name\": \"col1\",\r\n        \"type\": \"string\",\r\n        \"default\": \"col1_default\"\r\n      },\r\n      {\r\n        \"name\": \"col2\",\r\n        \"type\": \"string\"\r\n      }\r\n    ]\r\n  }')\r\n  STORED AS\r\n  INPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'\r\n  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'\r\n\"\"\")\r\n\r\nsql(\"\"\"\r\n  INSERT INTO t partition (ds='1981-01-07') VALUES ('col1_value', 'col2_value')\r\n\"\"\")\r\n\r\nsql(\"\"\"\r\n  ALTER TABLE t SET SERDEPROPERTIES ('avro.schema.literal'='\r\n  {\r\n    \"namespace\": \"test\",\r\n    \"name\": \"some_schema\",\r\n    \"type\": \"record\",\r\n    \"fields\": [\r\n      {\r\n        \"name\": \"col2\",\r\n        \"type\": \"string\"\r\n      }\r\n    ]\r\n  }')\r\n\"\"\")\r\n\r\nsql(\"\"\"\r\n  INSERT INTO t partition (ds='1983-04-27') VALUES ('col2_value')\r\n\"\"\")\r\n\r\nsql(\"\"\"\r\n  select * from t\r\n\"\"\").show()\r\n```\r\n\r\nWithout this PR:\r\n```\r\n+------------+----------+\r\n|        col2|        ds|\r\n+------------+----------+\r\n|col1_default|1981-01-07|\r\n|  col2_value|1983-04-27|\r\n+------------+----------+\r\n```\r\n\r\nWith the fix:\r\n```\r\n+----------+----------+\r\n|      col2|        ds|\r\n+----------+----------+\r\n|col2_value|1981-01-07|\r\n|col2_value|1983-04-27|\r\n+----------+----------+\r\n```\r\n\r\n\r\n",
        "createdAt" : "2021-01-12T08:32:01Z",
        "updatedAt" : "2021-02-05T07:01:10Z",
        "lastEditedBy" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "tags" : [
        ]
      }
    ],
    "commit" : "20dc1676da8514708a139f8e4ac66c3034ed7b5e",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +1894,1898 @@          |  \"fields\": [\n          |    {\n          |      \"name\": \"col2\",\n          |      \"type\": \"string\"\n          |    }"
  },
  {
    "id" : "6e9c0410-a051-4996-8cfd-2d81358cc43b",
    "prId" : 29796,
    "prUrl" : "https://github.com/apache/spark/pull/29796#pullrequestreview-491134667",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6c18152f-efb8-421f-a933-9506b7b71756",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "this too",
        "createdAt" : "2020-09-18T03:56:08Z",
        "updatedAt" : "2020-09-18T05:27:44Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "71a046515efd306f36e4894069abe234271b99d2",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1193,1197 @@      Map.empty))\n    // the database directory was created\n    assert(fs.exists(dbPath) && fs.getFileStatus(dbPath).isDirectory)\n    sql(s\"USE $dbName\")\n"
  },
  {
    "id" : "1d8cfbe1-0dc1-48c8-9bea-84b2c9fd09be",
    "prId" : 29152,
    "prUrl" : "https://github.com/apache/spark/pull/29152#pullrequestreview-451563730",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "934155fc-950d-4470-afbc-ae246b6f5ed2",
        "parentId" : null,
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "changed old error msg `tables` -> `tables/views`.",
        "createdAt" : "2020-07-20T12:34:54Z",
        "updatedAt" : "2020-07-24T04:34:29Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      }
    ],
    "commit" : "d02986cffe43150d6184915eeec1741c51eace83",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +2315,2319 @@\n  test(\"SPARK-20680: do not support for null column datatype\") {\n    val errMsg = s\"Cannot create tables/views with ${NullType.simpleString} type.\"\n    withTable(\"t\") {\n      withView(\"tabNullType\") {"
  },
  {
    "id" : "632ef7f9-538b-4c5d-ac38-503974a99141",
    "prId" : 28833,
    "prUrl" : "https://github.com/apache/spark/pull/28833#pullrequestreview-444204485",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cad1ac72-122f-4c86-a7c9-8221fe15c421",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "For this line and the following, it looks correct because we parse `VOID` in AstBuilder.",
        "createdAt" : "2020-07-07T19:55:22Z",
        "updatedAt" : "2020-07-08T00:27:04Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "9ad57d17bac47ea0f801004ec0aba9197e631bc7",
    "line" : 52,
    "diffHunk" : "@@ -1,1 +2351,2355 @@    }\n\n    // Forbid creating table with VOID type in Spark\n    withTable(\"t1\", \"t2\", \"t3\", \"t4\") {\n      val e1 = intercept[AnalysisException] {"
  }
]