[
  {
    "id" : "823520cf-90ae-4773-8678-c87e06fe6151",
    "prId" : 33488,
    "prUrl" : "https://github.com/apache/spark/pull/33488#pullrequestreview-714892385",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "81804bb2-9468-4f58-a938-fd367f3126d0",
        "parentId" : null,
        "authorId" : "29c89154-22f7-40ee-b295-115a55734d6c",
        "body" : "parquet doesn't support null(spark)/void(hive) type",
        "createdAt" : "2021-07-26T14:09:57Z",
        "updatedAt" : "2021-07-26T14:09:58Z",
        "lastEditedBy" : "29c89154-22f7-40ee-b295-115a55734d6c",
        "tags" : [
        ]
      }
    ],
    "commit" : "c833c86da7778988376b20ccf2b70e5dec8ae1bd",
    "line" : 39,
    "diffHunk" : "@@ -1,1 +2403,2407 @@      assertAnalysisError(\n        \"CREATE TABLE t2 STORED AS PARQUET AS SELECT null as null_col\",\n        \"Unknown field type: void\")\n\n      sql(\"CREATE TABLE t3 AS SELECT NULL AS null_col\")"
  },
  {
    "id" : "c9de3730-618c-497e-8e16-6ee4a45a7944",
    "prId" : 31639,
    "prUrl" : "https://github.com/apache/spark/pull/31639#pullrequestreview-599538123",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "071021a1-631b-46ee-b94a-26d825f21609",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Ah, I see. Since we don't allow duplicate names in a top-level, it seems we need to follow it in this case, too. cc: @cloud-fan \r\n```\r\nscala> spark.sql(\"CREATE VIEW v1 AS SELECT 'a' AS `a`, 1 AS b\")\r\nscala> spark.sql(\"ALTER VIEW v1 AS SELECT 'a' AS `b`, 1 AS b\")\r\norg.apache.spark.sql.AnalysisException: Found duplicate column(s) in the view definition: `b`\r\n```",
        "createdAt" : "2021-02-25T02:10:56Z",
        "updatedAt" : "2021-03-01T18:08:49Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "4780a611-2fcd-4202-b92f-1140959c789c",
        "parentId" : "071021a1-631b-46ee-b94a-26d825f21609",
        "authorId" : "94083dd5-86b7-47ec-bcb5-1b0831a32179",
        "body" : "So you want me to add this test ?",
        "createdAt" : "2021-02-25T23:05:53Z",
        "updatedAt" : "2021-03-01T18:08:49Z",
        "lastEditedBy" : "94083dd5-86b7-47ec-bcb5-1b0831a32179",
        "tags" : [
        ]
      },
      {
        "id" : "8e692e89-7cca-4fec-81e1-104ddb23a8e8",
        "parentId" : "071021a1-631b-46ee-b94a-26d825f21609",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "This is out-of-scope in this PR. Do you wanna work on it? If so, please feel free to file jira for it.",
        "createdAt" : "2021-02-26T12:41:49Z",
        "updatedAt" : "2021-03-01T18:08:49Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "a458128a4089a381ca2c9a7339cfd99e635c66e8",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +206,210 @@      checkAnswer(spark.table(\"v\"), Row(Row(\"a\", 1)) :: Nil)\n\n      spark.sql(\"ALTER VIEW v AS SELECT STRUCT('a' AS `c`, 1 AS b) q1\")\n      val df = spark.table(\"v\")\n      assert(\"q1\".equals(df.schema.fields(0).name))"
  },
  {
    "id" : "ce3251af-8f89-4410-9866-31538045eb11",
    "prId" : 31133,
    "prUrl" : "https://github.com/apache/spark/pull/31133#pullrequestreview-566035682",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2b1d9e5e-053d-4a3b-91cd-7bb6030c6dbb",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Just a question because it's a little counter-intuitive. Is there a reason why we start with `col2` and add `col1` later? Usually, I thought we start with `col1` and add `col2` later.",
        "createdAt" : "2021-01-11T22:17:40Z",
        "updatedAt" : "2021-02-05T07:01:10Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "296f6a0d-70b5-45b0-aaf6-deacca943502",
        "parentId" : "2b1d9e5e-053d-4a3b-91cd-7bb6030c6dbb",
        "authorId" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "body" : "Yes, regarding schema evolution rules you can add a field to an arbitrary position, see the first example here:\r\nhttps://docs.oracle.com/database/nosql-12.1.3.0/GettingStartedGuide/schemaevolution.html\r\n \r\nMy intention with this example and field naming was to illustrate the worst case (the column mismatch error) and emphasize its root cause. And even by adding new fields at the end one just decrease the scope of the problem as there will be still wrong values (null) for the new fields. \r\n\r\nMoreover when an existing field is removed the column mismatch usually cannot be avoided, example:\r\n\r\n```\r\nsql(\"\"\"\r\n  CREATE TABLE t PARTITIONED BY (ds string)\r\n  ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe'\r\n  WITH SERDEPROPERTIES ('avro.schema.literal'='\r\n  {\r\n    \"namespace\": \"test\",\r\n    \"name\": \"some_schema\",\r\n    \"type\": \"record\",\r\n    \"fields\": [\r\n      {\r\n        \"name\": \"col1\",\r\n        \"type\": \"string\",\r\n        \"default\": \"col1_default\"\r\n      },\r\n      {\r\n        \"name\": \"col2\",\r\n        \"type\": \"string\"\r\n      }\r\n    ]\r\n  }')\r\n  STORED AS\r\n  INPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'\r\n  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'\r\n\"\"\")\r\n\r\nsql(\"\"\"\r\n  INSERT INTO t partition (ds='1981-01-07') VALUES ('col1_value', 'col2_value')\r\n\"\"\")\r\n\r\nsql(\"\"\"\r\n  ALTER TABLE t SET SERDEPROPERTIES ('avro.schema.literal'='\r\n  {\r\n    \"namespace\": \"test\",\r\n    \"name\": \"some_schema\",\r\n    \"type\": \"record\",\r\n    \"fields\": [\r\n      {\r\n        \"name\": \"col2\",\r\n        \"type\": \"string\"\r\n      }\r\n    ]\r\n  }')\r\n\"\"\")\r\n\r\nsql(\"\"\"\r\n  INSERT INTO t partition (ds='1983-04-27') VALUES ('col2_value')\r\n\"\"\")\r\n\r\nsql(\"\"\"\r\n  select * from t\r\n\"\"\").show()\r\n```\r\n\r\nWithout this PR:\r\n```\r\n+------------+----------+\r\n|        col2|        ds|\r\n+------------+----------+\r\n|col1_default|1981-01-07|\r\n|  col2_value|1983-04-27|\r\n+------------+----------+\r\n```\r\n\r\nWith the fix:\r\n```\r\n+----------+----------+\r\n|      col2|        ds|\r\n+----------+----------+\r\n|col2_value|1981-01-07|\r\n|col2_value|1983-04-27|\r\n+----------+----------+\r\n```\r\n\r\n\r\n",
        "createdAt" : "2021-01-12T08:32:01Z",
        "updatedAt" : "2021-02-05T07:01:10Z",
        "lastEditedBy" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "tags" : [
        ]
      }
    ],
    "commit" : "20dc1676da8514708a139f8e4ac66c3034ed7b5e",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +1894,1898 @@          |  \"fields\": [\n          |    {\n          |      \"name\": \"col2\",\n          |      \"type\": \"string\"\n          |    }"
  },
  {
    "id" : "6e9c0410-a051-4996-8cfd-2d81358cc43b",
    "prId" : 29796,
    "prUrl" : "https://github.com/apache/spark/pull/29796#pullrequestreview-491134667",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6c18152f-efb8-421f-a933-9506b7b71756",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "this too",
        "createdAt" : "2020-09-18T03:56:08Z",
        "updatedAt" : "2020-09-18T05:27:44Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "71a046515efd306f36e4894069abe234271b99d2",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1193,1197 @@      Map.empty))\n    // the database directory was created\n    assert(fs.exists(dbPath) && fs.getFileStatus(dbPath).isDirectory)\n    sql(s\"USE $dbName\")\n"
  },
  {
    "id" : "1d8cfbe1-0dc1-48c8-9bea-84b2c9fd09be",
    "prId" : 29152,
    "prUrl" : "https://github.com/apache/spark/pull/29152#pullrequestreview-451563730",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "934155fc-950d-4470-afbc-ae246b6f5ed2",
        "parentId" : null,
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "changed old error msg `tables` -> `tables/views`.",
        "createdAt" : "2020-07-20T12:34:54Z",
        "updatedAt" : "2020-07-24T04:34:29Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      }
    ],
    "commit" : "d02986cffe43150d6184915eeec1741c51eace83",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +2315,2319 @@\n  test(\"SPARK-20680: do not support for null column datatype\") {\n    val errMsg = s\"Cannot create tables/views with ${NullType.simpleString} type.\"\n    withTable(\"t\") {\n      withView(\"tabNullType\") {"
  },
  {
    "id" : "632ef7f9-538b-4c5d-ac38-503974a99141",
    "prId" : 28833,
    "prUrl" : "https://github.com/apache/spark/pull/28833#pullrequestreview-444204485",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cad1ac72-122f-4c86-a7c9-8221fe15c421",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "For this line and the following, it looks correct because we parse `VOID` in AstBuilder.",
        "createdAt" : "2020-07-07T19:55:22Z",
        "updatedAt" : "2020-07-08T00:27:04Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "9ad57d17bac47ea0f801004ec0aba9197e631bc7",
    "line" : 52,
    "diffHunk" : "@@ -1,1 +2351,2355 @@    }\n\n    // Forbid creating table with VOID type in Spark\n    withTable(\"t1\", \"t2\", \"t3\", \"t4\") {\n      val e1 = intercept[AnalysisException] {"
  },
  {
    "id" : "6fe8248f-23c3-4841-bded-8a344829301d",
    "prId" : 28686,
    "prUrl" : "https://github.com/apache/spark/pull/28686#pullrequestreview-441157092",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0c5a0925-8e73-49be-948e-959cae83d7a5",
        "parentId" : null,
        "authorId" : "f3166ab8-4dba-4d22-b10b-31a984dfa2ad",
        "body" : "What would be right place to add this test? @viirya @maropu \r\nI will  move the table create/drop to `beforeAll` and `afterAll` when moving this test to the right file  ",
        "createdAt" : "2020-07-01T19:56:48Z",
        "updatedAt" : "2020-07-01T21:31:44Z",
        "lastEditedBy" : "f3166ab8-4dba-4d22-b10b-31a984dfa2ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "3ce6fbb20204b77d20043709a7fd05c72a8b98bd",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +172,176 @@  }\n\n  test(\"DetermineTableStats should not cause any plan changes\" +\n    \" if it can be converted to Datasource table\") {\n    withSQLConf((SQLConf.ENABLE_FALL_BACK_TO_HDFS_FOR_STATS.key, \"true\")) {"
  },
  {
    "id" : "5140bc35-2dbf-4cf1-a1ae-b4804cc1e9b8",
    "prId" : 28647,
    "prUrl" : "https://github.com/apache/spark/pull/28647#pullrequestreview-420595282",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "92971dd1-c9d3-4a49-b39a-5ca083ecfdbd",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Could you check if the metastore props are filtered out here?",
        "createdAt" : "2020-05-29T01:02:50Z",
        "updatedAt" : "2020-11-24T09:28:18Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "6d58eb9e-76ee-4346-917e-81c0f716ba75",
        "parentId" : "92971dd1-c9d3-4a49-b39a-5ca083ecfdbd",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "Sure.",
        "createdAt" : "2020-05-29T01:25:10Z",
        "updatedAt" : "2020-11-24T09:28:18Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      }
    ],
    "commit" : "c45489ad5b8ddd53d5e81fbba4cd08c0b4fd9850",
    "line" : 65,
    "diffHunk" : "@@ -1,1 +2854,2858 @@      val t3 = catalog.getTableMetadata(TableIdentifier(\"t3\"))\n      assert(t3.properties(\"k1\") == \"v1\")\n      assert(t3.properties(\"k2\") == \"v2\")\n      sql(\n        \"\"\""
  },
  {
    "id" : "8c1818c8-0e87-44ac-8677-29f35e6de36d",
    "prId" : 27538,
    "prUrl" : "https://github.com/apache/spark/pull/27538#pullrequestreview-357934712",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "862c67c1-8cd5-4c05-b33a-7f887086ea77",
        "parentId" : null,
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "Add another assert?\r\n```scala\r\ncheckAnswer(sql(\"SHOW PARTITIONS sc_part\"), Row(\"ts=10\") :: Nil) or assert(spark.sql(\"SHOW PARTITIONS sc_part\").count() == 1)\r\n```",
        "createdAt" : "2020-02-13T03:21:13Z",
        "updatedAt" : "2020-02-18T13:18:49Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      },
      {
        "id" : "3937b8f4-d01f-4770-a6a0-959f77b270f7",
        "parentId" : "862c67c1-8cd5-4c05-b33a-7f887086ea77",
        "authorId" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "body" : "Sure. I think you mean `SHOW PARTITIONS ta_part`",
        "createdAt" : "2020-02-13T03:30:06Z",
        "updatedAt" : "2020-02-18T13:18:49Z",
        "lastEditedBy" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "tags" : [
        ]
      }
    ],
    "commit" : "cf28b073351798338cd8db8adb2c9433945de198",
    "line" : 30,
    "diffHunk" : "@@ -1,1 +2744,2748 @@      sql(\"ALTER TABLE ta_part ADD PARTITION (ts=10)\") // no exception\n      checkAnswer(sql(\"SHOW PARTITIONS ta_part\"), Row(\"ts=10\") :: Nil)\n    }\n  }\n}"
  },
  {
    "id" : "c3f9286b-b3a6-427f-ad9a-8972875fa16a",
    "prId" : 27538,
    "prUrl" : "https://github.com/apache/spark/pull/27538#pullrequestreview-359295934",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5b72d822-b7cf-4a31-a933-ea4503f08d46",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "does this test fail without this patch? AFAIK `tracksPartitionsInCatalog` is true by default.",
        "createdAt" : "2020-02-14T08:37:36Z",
        "updatedAt" : "2020-02-18T13:18:49Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "b7fadbb1-c677-4e6f-9a43-58f7be5c2bdc",
        "parentId" : "5b72d822-b7cf-4a31-a933-ea4503f08d46",
        "authorId" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "body" : "Correct, the test fails without this patch.",
        "createdAt" : "2020-02-15T01:51:03Z",
        "updatedAt" : "2020-02-18T13:18:49Z",
        "lastEditedBy" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "tags" : [
        ]
      },
      {
        "id" : "c75ff74c-9003-4e6a-b8b2-a006161265f6",
        "parentId" : "5b72d822-b7cf-4a31-a933-ea4503f08d46",
        "authorId" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "body" : "Check the PR description. ",
        "createdAt" : "2020-02-15T01:54:13Z",
        "updatedAt" : "2020-02-18T13:18:49Z",
        "lastEditedBy" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "tags" : [
        ]
      },
      {
        "id" : "0022c4b5-2553-4d12-90dd-8fe39ee329c7",
        "parentId" : "5b72d822-b7cf-4a31-a933-ea4503f08d46",
        "authorId" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "body" : "This issue only exists in DS partitioned table.",
        "createdAt" : "2020-02-15T01:56:05Z",
        "updatedAt" : "2020-02-18T13:18:49Z",
        "lastEditedBy" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "tags" : [
        ]
      }
    ],
    "commit" : "cf28b073351798338cd8db8adb2c9433945de198",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +2732,2736 @@  }\n\n  test(\"SPARK-30785: create table like a partitioned table\") {\n    val catalog = spark.sessionState.catalog\n    withTable(\"sc_part\", \"ta_part\") {"
  }
]