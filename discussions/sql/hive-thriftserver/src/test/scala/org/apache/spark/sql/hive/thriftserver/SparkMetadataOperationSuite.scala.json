[
  {
    "id" : "855b7f21-d540-4e8a-bfc1-abf9f4828a37",
    "prId" : 32949,
    "prUrl" : "https://github.com/apache/spark/pull/32949#pullrequestreview-704267163",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e494a552-cde1-4123-b123-2eb7c08b44fd",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "We should update this test to check the new interval types. @yaooqinn can you help with this later in a followup PR?",
        "createdAt" : "2021-07-12T15:40:03Z",
        "updatedAt" : "2021-07-12T15:40:03Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "b9d29165-3c1a-45e3-aa36-9362ae38ad87",
        "parentId" : "e494a552-cde1-4123-b123-2eb7c08b44fd",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "OK",
        "createdAt" : "2021-07-12T15:42:24Z",
        "updatedAt" : "2021-07-12T15:42:24Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "48f080fc5b3bb2a2d704a0cc6bddb8d6be56f328",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +357,361 @@\n    withJdbcStatement(viewName) { statement =>\n      statement.execute(s\"SET ${SQLConf.LEGACY_INTERVAL_ENABLED.key}=true\")\n      statement.execute(ddl)\n      val data = statement.getConnection.getMetaData"
  },
  {
    "id" : "9d2aea02-dd28-41b6-bd4b-2f2cbe6d3194",
    "prId" : 32176,
    "prUrl" : "https://github.com/apache/spark/pull/32176#pullrequestreview-636374676",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d944da7a-f31b-4538-aa66-f8b561b71757",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Opened SPARK-35085 to have the same tests for ANSI intervals",
        "createdAt" : "2021-04-15T07:53:14Z",
        "updatedAt" : "2021-04-16T11:24:15Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "7668405ebd902cea698b76c373d0c06b1e759426",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +358,362 @@    withJdbcStatement(viewName) { statement =>\n      val legacyIntervalEnabled = SQLConf.get.legacyIntervalEnabled\n      statement.execute(s\"SET ${SQLConf.LEGACY_INTERVAL_ENABLED.key}=true\")\n      statement.execute(ddl)\n      val data = statement.getConnection.getMetaData"
  },
  {
    "id" : "24c737dc-987d-4dd3-a844-d04f5c9de109",
    "prId" : 30101,
    "prUrl" : "https://github.com/apache/spark/pull/30101#pullrequestreview-513363314",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8bcd893c-7833-4006-b042-22719c8d1a61",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "This comment looks a bit confusing. btw, could we fix this in followup activities? If we can, could you file it in jira?",
        "createdAt" : "2020-10-21T00:21:01Z",
        "updatedAt" : "2020-10-22T07:43:16Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "f08cfa29-2ee1-4f36-9cc9-970c21ca05aa",
        "parentId" : "8bcd893c-7833-4006-b042-22719c8d1a61",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "we can't, it belongs to the upstream hive module where defines the hive behavior.  Here we just highlight the difference and make it perspective in future changes",
        "createdAt" : "2020-10-21T02:35:31Z",
        "updatedAt" : "2020-10-22T07:43:16Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "43f034c5-ea51-4051-bd80-0806682b04d7",
        "parentId" : "8bcd893c-7833-4006-b042-22719c8d1a61",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Ah, I see...",
        "createdAt" : "2020-10-21T06:54:09Z",
        "updatedAt" : "2020-10-22T07:43:16Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "2b77e61f-a653-4447-9980-d2434b34ffc3",
        "parentId" : "8bcd893c-7833-4006-b042-22719c8d1a61",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "If so, could you write it in the comment like that?",
        "createdAt" : "2020-10-21T06:55:37Z",
        "updatedAt" : "2020-10-22T07:43:16Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "8e2bd998-c9c4-4f06-9191-f50a76e8b3ec",
        "parentId" : "8bcd893c-7833-4006-b042-22719c8d1a61",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "yea~",
        "createdAt" : "2020-10-21T07:04:34Z",
        "updatedAt" : "2020-10-22T07:43:16Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "7b1fa5049b6f3579a1f72a52a09c0d8fd38c72be",
    "line" : 73,
    "diffHunk" : "@@ -1,1 +452,456 @@        () => metaData.supportsSubqueriesInIns,\n        () => metaData.supportsSubqueriesInQuantifieds,\n        // Spark support this, see https://issues.apache.org/jira/browse/SPARK-18455\n        () => metaData.supportsCorrelatedSubqueries,\n        () => metaData.supportsOpenCursorsAcrossCommit,"
  },
  {
    "id" : "19c08a0c-20ea-486a-8c54-1079a36f7662",
    "prId" : 29834,
    "prUrl" : "https://github.com/apache/spark/pull/29834#pullrequestreview-493341517",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "89862f53-233b-46ca-ba90-c8ca1a13253c",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Does this operation work for permanent views?",
        "createdAt" : "2020-09-22T10:51:50Z",
        "updatedAt" : "2020-09-22T10:51:50Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "1c785fbc-c978-4401-8624-db162b331b2c",
        "parentId" : "89862f53-233b-46ca-ba90-c8ca1a13253c",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "this is DB layer API, we need to fix other type of operations too",
        "createdAt" : "2020-09-22T10:55:23Z",
        "updatedAt" : "2020-09-22T10:55:33Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "e79c1a57db504bb1804b616db37231088e68c1ba",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +29,33 @@  override def mode: ServerMode.Value = ServerMode.binary\n\n  test(\"Spark's own GetSchemasOperation(SparkGetSchemasOperation)\") {\n    def checkResult(rs: ResultSet, dbNames: Seq[String]): Unit = {\n      val expected = dbNames.iterator"
  },
  {
    "id" : "733e3341-8ff2-4b0a-9149-413d1b651ff3",
    "prId" : 29539,
    "prUrl" : "https://github.com/apache/spark/pull/29539#pullrequestreview-476354679",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "12cb6c2a-cc87-43b9-8a32-f9a9a35ca612",
        "parentId" : null,
        "authorId" : "2821208d-43c5-4475-94ad-36cbf84c14d8",
        "body" : "Nit: Can we also test an extract query of the table with calendar types?",
        "createdAt" : "2020-08-26T11:48:20Z",
        "updatedAt" : "2020-08-27T02:57:56Z",
        "lastEditedBy" : "2821208d-43c5-4475-94ad-36cbf84c14d8",
        "tags" : [
        ]
      },
      {
        "id" : "f51e47c2-f94c-439e-860f-59e0434d8057",
        "parentId" : "12cb6c2a-cc87-43b9-8a32-f9a9a35ca612",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "This PR fixes SparkGetColumnsOperation. As a meta operation, it is not related to extract-queries that executed by SparkExecuteStatementOperation. BTW, I notice that test cases for extracting interval values in VIEWS through the thrift server might be missing in the current codebase. Maybe we can make another PR to improve these, WDYT, @cloud-fan Or I can just add some UTs here https://github.com/yaooqinn/spark/blob/d24d27f1bc39e915df23d65f8fda0d83e716b308/sql/hive-thriftserver/src/test/scala/org/apache/spark/sql/hive/thriftserver/HiveThriftServer2Suites.scala#L674",
        "createdAt" : "2020-08-26T15:05:27Z",
        "updatedAt" : "2020-08-27T02:57:56Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "ece2694b-962d-4ea3-b851-7fc1e77a39d4",
        "parentId" : "12cb6c2a-cc87-43b9-8a32-f9a9a35ca612",
        "authorId" : "2821208d-43c5-4475-94ad-36cbf84c14d8",
        "body" : "That's why I think we should add a small check here to make sure they actually work.",
        "createdAt" : "2020-08-26T15:24:42Z",
        "updatedAt" : "2020-08-27T02:57:56Z",
        "lastEditedBy" : "2821208d-43c5-4475-94ad-36cbf84c14d8",
        "tags" : [
        ]
      },
      {
        "id" : "fe4a7b44-49f7-4e32-b68c-e0d8f05a06ae",
        "parentId" : "12cb6c2a-cc87-43b9-8a32-f9a9a35ca612",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I'm OK to add the missing test in this PR.",
        "createdAt" : "2020-08-26T16:09:12Z",
        "updatedAt" : "2020-08-27T02:57:56Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "7198e2b9-ded0-4cb4-a2ce-6b9d97bbd4b6",
        "parentId" : "12cb6c2a-cc87-43b9-8a32-f9a9a35ca612",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "done, thank you guys for the check~",
        "createdAt" : "2020-08-27T03:08:54Z",
        "updatedAt" : "2020-08-27T03:08:54Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "d3888d45ef8c5cedd5a15944be7100b5fcdb9dd8",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +335,339 @@  }\n\n  test(\"get columns operation should handle interval column properly\") {\n    val viewName = \"view_interval\"\n    val ddl = s\"CREATE GLOBAL TEMP VIEW $viewName as select interval 1 day as i\""
  },
  {
    "id" : "f11c349f-f2e2-41fb-9f77-4a9d433c8e61",
    "prId" : 29539,
    "prUrl" : "https://github.com/apache/spark/pull/29539#pullrequestreview-476493986",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4e278f4a-de2a-4970-a94e-b60d9e553b00",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "if we treat it as string type, can we still report the `TYPE_NAME` as `INTERVAL`?",
        "createdAt" : "2020-08-27T08:07:07Z",
        "updatedAt" : "2020-08-27T08:07:07Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "58766237-8021-486f-86b5-d70311e873bb",
        "parentId" : "4e278f4a-de2a-4970-a94e-b60d9e553b00",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "I guess that all the meta operations should do nothing else but specifically describe how all the objects(database/table /columns etc) stored in the Spark system.\r\n\r\n![image](https://user-images.githubusercontent.com/8326978/91415230-50f65400-e880-11ea-9edd-dc26a2ed77cb.png)\r\n\r\nIf we change the column meta here from INTERVAL to STRING, so the column here becomes orderable and comparable?(JDBC users may guess), they may have no idea about what they are dealing with and get a completely different awareness of the meta-information comparing to those users who use spark-sql self-contained applications. ",
        "createdAt" : "2020-08-27T08:23:24Z",
        "updatedAt" : "2020-08-27T08:24:02Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "d3888d45ef8c5cedd5a15944be7100b5fcdb9dd8",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +349,353 @@        assert(rowSet.getString(\"COLUMN_NAME\") === \"i\")\n        assert(rowSet.getInt(\"DATA_TYPE\") === java.sql.Types.OTHER)\n        assert(rowSet.getString(\"TYPE_NAME\").equalsIgnoreCase(CalendarIntervalType.sql))\n        assert(rowSet.getInt(\"COLUMN_SIZE\") === CalendarIntervalType.defaultSize)\n        assert(rowSet.getInt(\"DECIMAL_DIGITS\") === 0)"
  }
]