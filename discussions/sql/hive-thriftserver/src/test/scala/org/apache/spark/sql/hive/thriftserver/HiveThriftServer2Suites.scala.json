[
  {
    "id" : "7db295dc-7343-4e3c-9dc5-a073353d2d3d",
    "prId" : 32176,
    "prUrl" : "https://github.com/apache/spark/pull/32176#pullrequestreview-636375475",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f4e41bad-a86f-4cc1-a9c3-4d3c956e93b1",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Opened SPARK-35086 for ANSI intervals",
        "createdAt" : "2021-04-15T07:54:09Z",
        "updatedAt" : "2021-04-16T11:24:15Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "7668405ebd902cea698b76c373d0c06b1e759426",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +666,670 @@    withJdbcStatement() { statement =>\n      val legacyIntervalEnabled = SQLConf.get.legacyIntervalEnabled\n      statement.execute(s\"SET ${SQLConf.LEGACY_INTERVAL_ENABLED.key}=true\")\n      val rs = statement.executeQuery(\"SELECT interval 3 months 1 hours\")\n      assert(rs.next())"
  },
  {
    "id" : "98821655-c000-41dd-baf7-6f777ef95d14",
    "prId" : 32176,
    "prUrl" : "https://github.com/apache/spark/pull/32176#pullrequestreview-636375580",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "75d3a011-c9ee-4e76-80ea-56046b20d775",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Opened SPARK-35086 for ANSI intervals",
        "createdAt" : "2021-04-15T07:54:17Z",
        "updatedAt" : "2021-04-16T11:24:15Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "7668405ebd902cea698b76c373d0c06b1e759426",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +688,692 @@    withJdbcStatement(viewName1, viewName2) { statement =>\n      val legacyIntervalEnabled = SQLConf.get.legacyIntervalEnabled\n      statement.execute(s\"SET ${SQLConf.LEGACY_INTERVAL_ENABLED.key}=true\")\n      statement.executeQuery(ddl1)\n      statement.executeQuery(ddl2)"
  },
  {
    "id" : "087e55ca-631c-4c2d-90e5-91d337dbfa28",
    "prId" : 29539,
    "prUrl" : "https://github.com/apache/spark/pull/29539#pullrequestreview-476548385",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "832218ca-aaea-4a36-848f-f5b4cc86d0a9",
        "parentId" : null,
        "authorId" : "e5beb795-ac0e-4bd6-ad66-708215b8ae58",
        "body" : "What will be returned by rs.getMetadata.getColumnType for the interval column?\r\nI believe that will be a string, because of how we just make it a string in the output schema in https://github.com/apache/spark/blob/master/sql/hive-thriftserver/src/main/scala/org/apache/spark/sql/hive/thriftserver/SparkExecuteStatementOperation.scala#L362\r\n\r\nThere was a discussion last year in https://github.com/apache/spark/pull/25694#discussion_r321972811, that because the interval is just returned as string in the query output schema, and not compliant to standard SQL interval types, it should not be listed by SparkGetTypeInfoOperation, and be treated just as strings.\r\n\r\nIf we want to make it be returned as a java.sql.Types.OTHER, to make this complete, it should be returned as such in result set metadata as well, and listed in SparkGetTypeInfoOperation.\r\nOr we could map it to string.",
        "createdAt" : "2020-08-27T07:31:39Z",
        "updatedAt" : "2020-08-27T07:38:49Z",
        "lastEditedBy" : "e5beb795-ac0e-4bd6-ad66-708215b8ae58",
        "tags" : [
        ]
      },
      {
        "id" : "130af5c3-1c16-4a7c-8cd4-d5de83544248",
        "parentId" : "832218ca-aaea-4a36-848f-f5b4cc86d0a9",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "the ResultSetMetadata describes the return types of the result set, the GetColumnOperation retrieves the metadata of the original table or view. IMHO,  they do not have to be the same.",
        "createdAt" : "2020-08-27T07:56:05Z",
        "updatedAt" : "2020-08-27T07:56:05Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "3b10d295-2719-47b4-b4e4-88898600b8ae",
        "parentId" : "832218ca-aaea-4a36-848f-f5b4cc86d0a9",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "I will make a followup to add it to SparkGetTypeInfoOperation",
        "createdAt" : "2020-08-27T07:58:30Z",
        "updatedAt" : "2020-08-27T07:58:30Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "a4a02154-5f69-4810-9d01-e4ebb66918ad",
        "parentId" : "832218ca-aaea-4a36-848f-f5b4cc86d0a9",
        "authorId" : "e5beb795-ac0e-4bd6-ad66-708215b8ae58",
        "body" : "It would be the most consistent if the table columns metadata, and metadata of the result of `select * from table` had consistent types. I don't really have a preference whether it should be string or interval. Last year the discussion leaned towards string, because of the intervals not really being standard compliant, but indeed there might be value in having it marked as interval in the schema, so the application might try to parse it as such... Though, e.g. between Spark 2.4 and Spark 3.0 the format of the returned interval changed, see https://issues.apache.org/jira/browse/SPARK-32132, which might have broken how anyone would have parsed it, so Spark is not really giving any guarantees as to the stability of the format of this type...\r\n\r\nUltimately, I think this is very rarely used, as intervals didn't even work at all until https://github.com/apache/spark/pull/25277 and it was not raised earlier...",
        "createdAt" : "2020-08-27T08:21:49Z",
        "updatedAt" : "2020-08-27T08:21:50Z",
        "lastEditedBy" : "e5beb795-ac0e-4bd6-ad66-708215b8ae58",
        "tags" : [
        ]
      },
      {
        "id" : "316fd654-7ba0-490f-9ea9-c4b1001f59a3",
        "parentId" : "832218ca-aaea-4a36-848f-f5b4cc86d0a9",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "The ResultSetMetadata is used by the client-side user to do the right work on the result set what they getï¼Œwhen the interval values become strings, they need to follow the rules of String operations in their future activities.\r\n\r\nThe GetColumnOperation tells the client-side users about how the data is stored and formed at the server-side, then if the JDBC-users know column A is an interval type, then they should notice that they can not perform string functions, comparators, etc on column A.",
        "createdAt" : "2020-08-27T08:51:26Z",
        "updatedAt" : "2020-08-27T08:56:19Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "8770ebfc-c7d7-4040-b2f8-71cce6767360",
        "parentId" : "832218ca-aaea-4a36-848f-f5b4cc86d0a9",
        "authorId" : "e5beb795-ac0e-4bd6-ad66-708215b8ae58",
        "body" : "In ResultSetMetadata would the client-side user not learn that they should follow string rules by using getColumnClassName, which should return String, while it would be still valuable to them to know that that column represents an interval?",
        "createdAt" : "2020-08-27T09:34:18Z",
        "updatedAt" : "2020-08-27T09:34:18Z",
        "lastEditedBy" : "e5beb795-ac0e-4bd6-ad66-708215b8ae58",
        "tags" : [
        ]
      }
    ],
    "commit" : "d3888d45ef8c5cedd5a15944be7100b5fcdb9dd8",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +695,699 @@      statement.executeQuery(ddl1)\n      statement.executeQuery(ddl2)\n      val rs = statement.executeQuery(s\"SELECT v1.i as a, v2.i as b FROM global_temp.$viewName1\" +\n        s\" v1 join $viewName2 v2 on date_part('DAY', v1.i) = date_part('DAY', v2.i)\")\n      while (rs.next()) {"
  },
  {
    "id" : "180ba60c-ada1-491d-834d-37a4800691e4",
    "prId" : 28991,
    "prUrl" : "https://github.com/apache/spark/pull/28991#pullrequestreview-442948786",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "de81a58f-edb5-4cc1-9120-d003d53c4b28",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Could you check if it can throw an exception (when hive1.2 used internally) by using `HiveUtils.builtinHiveVersion` or the other related variables?\r\n\r\nhttps://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/125039/testReport/org.apache.spark.sql.hive.thriftserver/HiveThriftBinaryServerSuite/SPARK_26533__Support_query_auto_timeout_cancel_on_thriftserver/\r\n```\r\nError Message\r\njava.sql.SQLException: Method not supported\r\nStacktrace\r\nsbt.ForkMain$ForkError: java.sql.SQLException: Method not supported\r\n\tat org.apache.hive.jdbc.HiveStatement.setQueryTimeout(HiveStatement.java:739)\r\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftBinaryServerSuite.$anonfun$new$97(HiveThriftServer2Suites.scala:880)\r\n\tat \r\n```",
        "createdAt" : "2020-07-06T08:18:49Z",
        "updatedAt" : "2020-08-11T02:48:03Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "d4453109-2ccf-4005-a4f8-ab81a0138821",
        "parentId" : "de81a58f-edb5-4cc1-9120-d003d53c4b28",
        "authorId" : "7f1185d9-3900-4812-b576-80ba5f410d6b",
        "body" : "fixed, thanks",
        "createdAt" : "2020-07-06T10:13:59Z",
        "updatedAt" : "2020-08-11T02:48:03Z",
        "lastEditedBy" : "7f1185d9-3900-4812-b576-80ba5f410d6b",
        "tags" : [
        ]
      }
    ],
    "commit" : "da76e1a7ba25ca96495cbb2ce56377983ee9f74f",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +876,880 @@  }\n\n  test(\"SPARK-26533: Support query auto timeout cancel on thriftserver\") {\n    withJdbcStatement() { statement =>\n      if (HiveUtils.isHive23) {"
  },
  {
    "id" : "2e9e5e21-476d-4245-bf2d-94d7119e1d24",
    "prId" : 28671,
    "prUrl" : "https://github.com/apache/spark/pull/28671#pullrequestreview-425128264",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "642e1935-7283-46f4-a4e8-cffabeb0fe64",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "@juliuszsompolski Did you use this nested `st` intentionally? Looks like `statement` is not used.",
        "createdAt" : "2020-06-04T20:03:54Z",
        "updatedAt" : "2020-06-04T20:03:58Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "c0b775d8-7c2f-4336-ab85-19d9e40b3b39",
        "parentId" : "642e1935-7283-46f4-a4e8-cffabeb0fe64",
        "authorId" : "e5beb795-ac0e-4bd6-ad66-708215b8ae58",
        "body" : ".... this was not intentional, looks like a copy paste error on my part. I raised https://github.com/apache/spark/pull/28735/ to fix it.",
        "createdAt" : "2020-06-05T09:09:53Z",
        "updatedAt" : "2020-06-05T09:09:54Z",
        "lastEditedBy" : "e5beb795-ac0e-4bd6-ad66-708215b8ae58",
        "tags" : [
        ]
      }
    ],
    "commit" : "988a29dff1cfce61cc846a51c6105a484e164bdd",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +815,819 @@  test(\"SPARK-31859 Thriftserver works with spark.sql.datetime.java8API.enabled=true\") {\n    withJdbcStatement() { statement =>\n      withJdbcStatement() { st =>\n        st.execute(\"set spark.sql.datetime.java8API.enabled=true\")\n        val rs = st.executeQuery(\"select date '2020-05-28', timestamp '2020-05-28 00:00:00'\")"
  },
  {
    "id" : "00bce3e2-558f-4fc1-8a35-75ffc5c76654",
    "prId" : 26014,
    "prUrl" : "https://github.com/apache/spark/pull/26014#pullrequestreview-299488177",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e39610fc-f931-4807-b980-684566b47dd0",
        "parentId" : null,
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "Could we change this  assert to?\r\n```scala\r\nrows.iterator.asScala.zip((start until end).iterator).foreach { case (row, v2) =>\r\n  assert(row(0).asInstanceOf[Long] === v2)\r\n}\r\n```",
        "createdAt" : "2019-10-09T15:11:27Z",
        "updatedAt" : "2019-10-10T15:12:03Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      }
    ],
    "commit" : "62e1f98aa9ad92d958e85e7b499da6ab0ffd4119",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +693,697 @@      rows.iterator.asScala.zip((start until end).iterator).foreach { case (row, v) =>\n        assert(row(0).asInstanceOf[Long] === v)\n      }\n    }\n"
  },
  {
    "id" : "1c1ac9e0-8607-4c60-a744-07040c03baba",
    "prId" : 26014,
    "prUrl" : "https://github.com/apache/spark/pull/26014#pullrequestreview-300062624",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9b027426-f92c-477a-bf91-1090722899aa",
        "parentId" : null,
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "The `startOffset` is `5` at this time. If we do FETCH_PRIOR one row, it is 4. But PostgreSQL returns `5`. Is this what we expect?\r\n\r\n```sql\r\npostgres=# begin;\r\nBEGIN\r\npostgres=# declare rf_cur scroll cursor for select * from generate_series(0, 9);\r\nDECLARE CURSOR\r\npostgres=# FETCH 5 from rf_cur;\r\n generate_series\r\n-----------------\r\n               0\r\n               1\r\n               2\r\n               3\r\n               4\r\n(5 rows)\r\n\r\npostgres=# FETCH 2 from rf_cur;\r\n generate_series\r\n-----------------\r\n               5\r\n               6\r\n(2 rows)\r\n\r\npostgres=# FETCH PRIOR from rf_cur;\r\n generate_series\r\n-----------------\r\n               5\r\n(1 row)\r\n\r\npostgres=# commit;\r\nCOMMIT\r\n```\r\n\r\nhttps://www.postgresql.org/docs/11/sql-fetch.html",
        "createdAt" : "2019-10-10T02:48:48Z",
        "updatedAt" : "2019-10-10T15:12:03Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      },
      {
        "id" : "85b5bbdc-d2f3-44be-8b82-3a677896a426",
        "parentId" : "9b027426-f92c-477a-bf91-1090722899aa",
        "authorId" : "e5beb795-ac0e-4bd6-ad66-708215b8ae58",
        "body" : "@wangyum this is expected.\r\n`FETCH_PRIOR` of the Thriftserver is not the same as FETCH PRIOR in the cursor of the client.\r\nFetch in Thriftserver operates in batches of rows, and the cursor in the client caches these batches and returns results row by row. Let's say it's batching by maxRows=100, and we returned row 99. The client has rows [0, 100) from the first batch and is at row 99. The next FETCH NEXT on the cursor will have to call FETCH_NEXT to the Thriftserver to get a batch of rows [100, 200) and return row 100 to the client. Another FETCH NEXT will return row 101 from the batch without having to call FETCH_NEXT on the Thriftserver. Another FETCH NEXT on the cursor will return row 102. Then FETCH PRIOR will return row 101 again. Then FETCH PRIOR will return row 100. Only then, another FETCH PRIOR should return row 99, but the cursor doesn't have its current batch. Then it has to call FETCH_PRIOR on Thriftserver to get rows [0, 99) again.",
        "createdAt" : "2019-10-10T09:16:18Z",
        "updatedAt" : "2019-10-10T15:12:03Z",
        "lastEditedBy" : "e5beb795-ac0e-4bd6-ad66-708215b8ae58",
        "tags" : [
        ]
      },
      {
        "id" : "cb004f2b-35eb-4814-ba99-0f2d1c30d615",
        "parentId" : "9b027426-f92c-477a-bf91-1090722899aa",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "Thank you @juliuszsompolski",
        "createdAt" : "2019-10-10T13:24:07Z",
        "updatedAt" : "2019-10-10T15:12:03Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      }
    ],
    "commit" : "62e1f98aa9ad92d958e85e7b499da6ab0ffd4119",
    "line" : 52,
    "diffHunk" : "@@ -1,1 +717,721 @@      checkResult(rows, 5, 7) // fetched [5, 7)\n\n      // FETCH_PRIOR 3 rows\n      rows = client.fetchResults(\n        operationHandle, FetchOrientation.FETCH_PRIOR, 3, FetchType.QUERY_OUTPUT)"
  },
  {
    "id" : "08d5ec21-46d1-413a-a69f-814c9a7f344f",
    "prId" : 25217,
    "prUrl" : "https://github.com/apache/spark/pull/25217#pullrequestreview-267312452",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3efb8895-05b7-45e7-9800-abf900ba761c",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "That also indicates our thrift server might have various issues we might not catch. \r\n\r\nCould you bulid a test framework that can directly re-run all the tests in SQLQueryTestSuite via Thrift Server? We do not need to run them always, since they could be very time consuming.",
        "createdAt" : "2019-07-26T17:25:12Z",
        "updatedAt" : "2019-07-26T17:25:12Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "2b85ee4b-8439-4f1c-9018-e84be0805818",
        "parentId" : "3efb8895-05b7-45e7-9800-abf900ba761c",
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "https://issues.apache.org/jira/browse/SPARK-28527",
        "createdAt" : "2019-07-26T17:29:15Z",
        "updatedAt" : "2019-07-26T17:29:16Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      }
    ],
    "commit" : "93a083c334a9a6808dca03f32bd76e497e614f00",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +656,660 @@  }\n\n  test(\"SPARK-28463: Thriftserver throws BigDecimal incompatible with HiveDecimal\") {\n    withJdbcStatement() { statement =>\n      val rs = statement.executeQuery(\"SELECT CAST(1 AS decimal(38, 18))\")"
  }
]