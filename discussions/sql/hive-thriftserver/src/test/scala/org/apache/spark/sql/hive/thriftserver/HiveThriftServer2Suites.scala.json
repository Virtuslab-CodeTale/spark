[
  {
    "id" : "7db295dc-7343-4e3c-9dc5-a073353d2d3d",
    "prId" : 32176,
    "prUrl" : "https://github.com/apache/spark/pull/32176#pullrequestreview-636375475",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f4e41bad-a86f-4cc1-a9c3-4d3c956e93b1",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Opened SPARK-35086 for ANSI intervals",
        "createdAt" : "2021-04-15T07:54:09Z",
        "updatedAt" : "2021-04-16T11:24:15Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "7668405ebd902cea698b76c373d0c06b1e759426",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +666,670 @@    withJdbcStatement() { statement =>\n      val legacyIntervalEnabled = SQLConf.get.legacyIntervalEnabled\n      statement.execute(s\"SET ${SQLConf.LEGACY_INTERVAL_ENABLED.key}=true\")\n      val rs = statement.executeQuery(\"SELECT interval 3 months 1 hours\")\n      assert(rs.next())"
  },
  {
    "id" : "98821655-c000-41dd-baf7-6f777ef95d14",
    "prId" : 32176,
    "prUrl" : "https://github.com/apache/spark/pull/32176#pullrequestreview-636375580",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "75d3a011-c9ee-4e76-80ea-56046b20d775",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Opened SPARK-35086 for ANSI intervals",
        "createdAt" : "2021-04-15T07:54:17Z",
        "updatedAt" : "2021-04-16T11:24:15Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "7668405ebd902cea698b76c373d0c06b1e759426",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +688,692 @@    withJdbcStatement(viewName1, viewName2) { statement =>\n      val legacyIntervalEnabled = SQLConf.get.legacyIntervalEnabled\n      statement.execute(s\"SET ${SQLConf.LEGACY_INTERVAL_ENABLED.key}=true\")\n      statement.executeQuery(ddl1)\n      statement.executeQuery(ddl2)"
  },
  {
    "id" : "087e55ca-631c-4c2d-90e5-91d337dbfa28",
    "prId" : 29539,
    "prUrl" : "https://github.com/apache/spark/pull/29539#pullrequestreview-476548385",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "832218ca-aaea-4a36-848f-f5b4cc86d0a9",
        "parentId" : null,
        "authorId" : "e5beb795-ac0e-4bd6-ad66-708215b8ae58",
        "body" : "What will be returned by rs.getMetadata.getColumnType for the interval column?\r\nI believe that will be a string, because of how we just make it a string in the output schema in https://github.com/apache/spark/blob/master/sql/hive-thriftserver/src/main/scala/org/apache/spark/sql/hive/thriftserver/SparkExecuteStatementOperation.scala#L362\r\n\r\nThere was a discussion last year in https://github.com/apache/spark/pull/25694#discussion_r321972811, that because the interval is just returned as string in the query output schema, and not compliant to standard SQL interval types, it should not be listed by SparkGetTypeInfoOperation, and be treated just as strings.\r\n\r\nIf we want to make it be returned as a java.sql.Types.OTHER, to make this complete, it should be returned as such in result set metadata as well, and listed in SparkGetTypeInfoOperation.\r\nOr we could map it to string.",
        "createdAt" : "2020-08-27T07:31:39Z",
        "updatedAt" : "2020-08-27T07:38:49Z",
        "lastEditedBy" : "e5beb795-ac0e-4bd6-ad66-708215b8ae58",
        "tags" : [
        ]
      },
      {
        "id" : "130af5c3-1c16-4a7c-8cd4-d5de83544248",
        "parentId" : "832218ca-aaea-4a36-848f-f5b4cc86d0a9",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "the ResultSetMetadata describes the return types of the result set, the GetColumnOperation retrieves the metadata of the original table or view. IMHO,  they do not have to be the same.",
        "createdAt" : "2020-08-27T07:56:05Z",
        "updatedAt" : "2020-08-27T07:56:05Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "3b10d295-2719-47b4-b4e4-88898600b8ae",
        "parentId" : "832218ca-aaea-4a36-848f-f5b4cc86d0a9",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "I will make a followup to add it to SparkGetTypeInfoOperation",
        "createdAt" : "2020-08-27T07:58:30Z",
        "updatedAt" : "2020-08-27T07:58:30Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "a4a02154-5f69-4810-9d01-e4ebb66918ad",
        "parentId" : "832218ca-aaea-4a36-848f-f5b4cc86d0a9",
        "authorId" : "e5beb795-ac0e-4bd6-ad66-708215b8ae58",
        "body" : "It would be the most consistent if the table columns metadata, and metadata of the result of `select * from table` had consistent types. I don't really have a preference whether it should be string or interval. Last year the discussion leaned towards string, because of the intervals not really being standard compliant, but indeed there might be value in having it marked as interval in the schema, so the application might try to parse it as such... Though, e.g. between Spark 2.4 and Spark 3.0 the format of the returned interval changed, see https://issues.apache.org/jira/browse/SPARK-32132, which might have broken how anyone would have parsed it, so Spark is not really giving any guarantees as to the stability of the format of this type...\r\n\r\nUltimately, I think this is very rarely used, as intervals didn't even work at all until https://github.com/apache/spark/pull/25277 and it was not raised earlier...",
        "createdAt" : "2020-08-27T08:21:49Z",
        "updatedAt" : "2020-08-27T08:21:50Z",
        "lastEditedBy" : "e5beb795-ac0e-4bd6-ad66-708215b8ae58",
        "tags" : [
        ]
      },
      {
        "id" : "316fd654-7ba0-490f-9ea9-c4b1001f59a3",
        "parentId" : "832218ca-aaea-4a36-848f-f5b4cc86d0a9",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "The ResultSetMetadata is used by the client-side user to do the right work on the result set what they getï¼Œwhen the interval values become strings, they need to follow the rules of String operations in their future activities.\r\n\r\nThe GetColumnOperation tells the client-side users about how the data is stored and formed at the server-side, then if the JDBC-users know column A is an interval type, then they should notice that they can not perform string functions, comparators, etc on column A.",
        "createdAt" : "2020-08-27T08:51:26Z",
        "updatedAt" : "2020-08-27T08:56:19Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "8770ebfc-c7d7-4040-b2f8-71cce6767360",
        "parentId" : "832218ca-aaea-4a36-848f-f5b4cc86d0a9",
        "authorId" : "e5beb795-ac0e-4bd6-ad66-708215b8ae58",
        "body" : "In ResultSetMetadata would the client-side user not learn that they should follow string rules by using getColumnClassName, which should return String, while it would be still valuable to them to know that that column represents an interval?",
        "createdAt" : "2020-08-27T09:34:18Z",
        "updatedAt" : "2020-08-27T09:34:18Z",
        "lastEditedBy" : "e5beb795-ac0e-4bd6-ad66-708215b8ae58",
        "tags" : [
        ]
      }
    ],
    "commit" : "d3888d45ef8c5cedd5a15944be7100b5fcdb9dd8",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +695,699 @@      statement.executeQuery(ddl1)\n      statement.executeQuery(ddl2)\n      val rs = statement.executeQuery(s\"SELECT v1.i as a, v2.i as b FROM global_temp.$viewName1\" +\n        s\" v1 join $viewName2 v2 on date_part('DAY', v1.i) = date_part('DAY', v2.i)\")\n      while (rs.next()) {"
  },
  {
    "id" : "180ba60c-ada1-491d-834d-37a4800691e4",
    "prId" : 28991,
    "prUrl" : "https://github.com/apache/spark/pull/28991#pullrequestreview-442948786",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "de81a58f-edb5-4cc1-9120-d003d53c4b28",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Could you check if it can throw an exception (when hive1.2 used internally) by using `HiveUtils.builtinHiveVersion` or the other related variables?\r\n\r\nhttps://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/125039/testReport/org.apache.spark.sql.hive.thriftserver/HiveThriftBinaryServerSuite/SPARK_26533__Support_query_auto_timeout_cancel_on_thriftserver/\r\n```\r\nError Message\r\njava.sql.SQLException: Method not supported\r\nStacktrace\r\nsbt.ForkMain$ForkError: java.sql.SQLException: Method not supported\r\n\tat org.apache.hive.jdbc.HiveStatement.setQueryTimeout(HiveStatement.java:739)\r\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftBinaryServerSuite.$anonfun$new$97(HiveThriftServer2Suites.scala:880)\r\n\tat \r\n```",
        "createdAt" : "2020-07-06T08:18:49Z",
        "updatedAt" : "2020-08-11T02:48:03Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "d4453109-2ccf-4005-a4f8-ab81a0138821",
        "parentId" : "de81a58f-edb5-4cc1-9120-d003d53c4b28",
        "authorId" : "7f1185d9-3900-4812-b576-80ba5f410d6b",
        "body" : "fixed, thanks",
        "createdAt" : "2020-07-06T10:13:59Z",
        "updatedAt" : "2020-08-11T02:48:03Z",
        "lastEditedBy" : "7f1185d9-3900-4812-b576-80ba5f410d6b",
        "tags" : [
        ]
      }
    ],
    "commit" : "da76e1a7ba25ca96495cbb2ce56377983ee9f74f",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +876,880 @@  }\n\n  test(\"SPARK-26533: Support query auto timeout cancel on thriftserver\") {\n    withJdbcStatement() { statement =>\n      if (HiveUtils.isHive23) {"
  },
  {
    "id" : "2e9e5e21-476d-4245-bf2d-94d7119e1d24",
    "prId" : 28671,
    "prUrl" : "https://github.com/apache/spark/pull/28671#pullrequestreview-425128264",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "642e1935-7283-46f4-a4e8-cffabeb0fe64",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "@juliuszsompolski Did you use this nested `st` intentionally? Looks like `statement` is not used.",
        "createdAt" : "2020-06-04T20:03:54Z",
        "updatedAt" : "2020-06-04T20:03:58Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "c0b775d8-7c2f-4336-ab85-19d9e40b3b39",
        "parentId" : "642e1935-7283-46f4-a4e8-cffabeb0fe64",
        "authorId" : "e5beb795-ac0e-4bd6-ad66-708215b8ae58",
        "body" : ".... this was not intentional, looks like a copy paste error on my part. I raised https://github.com/apache/spark/pull/28735/ to fix it.",
        "createdAt" : "2020-06-05T09:09:53Z",
        "updatedAt" : "2020-06-05T09:09:54Z",
        "lastEditedBy" : "e5beb795-ac0e-4bd6-ad66-708215b8ae58",
        "tags" : [
        ]
      }
    ],
    "commit" : "988a29dff1cfce61cc846a51c6105a484e164bdd",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +815,819 @@  test(\"SPARK-31859 Thriftserver works with spark.sql.datetime.java8API.enabled=true\") {\n    withJdbcStatement() { statement =>\n      withJdbcStatement() { st =>\n        st.execute(\"set spark.sql.datetime.java8API.enabled=true\")\n        val rs = st.executeQuery(\"select date '2020-05-28', timestamp '2020-05-28 00:00:00'\")"
  }
]