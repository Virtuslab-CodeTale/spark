[
  {
    "id" : "b1c1c755-ec9f-4176-8d13-c667281afffd",
    "prId" : 32959,
    "prUrl" : "https://github.com/apache/spark/pull/32959#pullrequestreview-697903239",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "947bb207-ddf0-4e5b-b770-d0dd23db70be",
        "parentId" : null,
        "authorId" : "29c89154-22f7-40ee-b295-115a55734d6c",
        "body" : "same reason to \"date.sql\" that thriftserver couldn't handle negative year",
        "createdAt" : "2021-07-02T07:31:13Z",
        "updatedAt" : "2021-07-02T07:31:14Z",
        "lastEditedBy" : "29c89154-22f7-40ee-b295-115a55734d6c",
        "tags" : [
        ]
      }
    ],
    "commit" : "538463a405c2eac1ddfccba8d82ca846f81e5a22",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +86,90 @@    \"datetime.sql\",\n    \"datetime-legacy.sql\",\n    \"ansi/datetime.sql\",\n    // SPARK-28620\n    \"postgreSQL/float4.sql\","
  },
  {
    "id" : "3a7bbcf6-f5ec-4ef8-a8a5-2a0b81ff8c85",
    "prId" : 32648,
    "prUrl" : "https://github.com/apache/spark/pull/32648#pullrequestreview-666454378",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e1ed1b21-3f10-4432-9ee6-e3350e5b7b95",
        "parentId" : null,
        "authorId" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "body" : "```\r\n/spark/sql/hive-thriftserver/src/test/scala/org/apache/spark/sql/hive/thriftserver/ThriftServerQueryTestSuite.scala:110:15\r\nThe outer reference in this type test cannot be checked at run time.\r\n        case _: PgSQLTest =>\r\n```",
        "createdAt" : "2021-05-24T07:07:00Z",
        "updatedAt" : "2021-05-24T07:07:00Z",
        "lastEditedBy" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "tags" : [
        ]
      }
    ],
    "commit" : "2a867a6277c1c6c4f1a6497973588304f907bede",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +108,112 @@\n      testCase match {\n        case _: SQLQueryTestSuite#PgSQLTest =>\n          statement.execute(s\"SET ${SQLConf.ANSI_ENABLED.key} = true\")\n          statement.execute(s\"SET ${SQLConf.LEGACY_INTERVAL_ENABLED.key} = true\")"
  },
  {
    "id" : "6ac50485-8a37-44f3-917a-c9d7c6889f9e",
    "prId" : 32648,
    "prUrl" : "https://github.com/apache/spark/pull/32648#pullrequestreview-666454787",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "62e345fa-3a5f-47e1-a409-3caaa3d15612",
        "parentId" : null,
        "authorId" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "body" : "```\r\n/spark/sql/hive-thriftserver/src/test/scala/org/apache/spark/sql/hive/thriftserver/ThriftServerQueryTestSuite.scala:113:15\r\nThe outer reference in this type test cannot be checked at run time.\r\n        case _: AnsiTest =>\r\n```",
        "createdAt" : "2021-05-24T07:07:41Z",
        "updatedAt" : "2021-05-24T07:07:41Z",
        "lastEditedBy" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "tags" : [
        ]
      }
    ],
    "commit" : "2a867a6277c1c6c4f1a6497973588304f907bede",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +111,115 @@          statement.execute(s\"SET ${SQLConf.ANSI_ENABLED.key} = true\")\n          statement.execute(s\"SET ${SQLConf.LEGACY_INTERVAL_ENABLED.key} = true\")\n        case _: SQLQueryTestSuite#AnsiTest =>\n          statement.execute(s\"SET ${SQLConf.ANSI_ENABLED.key} = true\")\n        case _ =>"
  },
  {
    "id" : "099a7e37-048d-4fd3-b730-e40383673cfb",
    "prId" : 32442,
    "prUrl" : "https://github.com/apache/spark/pull/32442#pullrequestreview-654946228",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "020336a5-a96e-4504-900c-269318b9b4b1",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "why it doesn't work in thriftserver?",
        "createdAt" : "2021-05-07T08:39:58Z",
        "updatedAt" : "2021-05-10T08:41:36Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "383adefd-5c3e-42ef-8bd2-f636d4f63c06",
        "parentId" : "020336a5-a96e-4504-900c-269318b9b4b1",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "Because the output schema of hive is different from spark",
        "createdAt" : "2021-05-07T08:50:57Z",
        "updatedAt" : "2021-05-10T08:41:36Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      },
      {
        "id" : "cdc0f736-f85e-456e-96d2-9c4ac88c8518",
        "parentId" : "020336a5-a96e-4504-900c-269318b9b4b1",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "because Hive doesn't support this syntax?",
        "createdAt" : "2021-05-07T13:08:06Z",
        "updatedAt" : "2021-05-10T08:41:36Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "3c94b925-e04f-4c51-986f-3d8954198312",
        "parentId" : "020336a5-a96e-4504-900c-269318b9b4b1",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "how about adding a comment here to explain?",
        "createdAt" : "2021-05-07T17:55:41Z",
        "updatedAt" : "2021-05-10T08:41:36Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "e55982e9-cb48-4fc8-be5b-2644fca8eeb0",
        "parentId" : "020336a5-a96e-4504-900c-269318b9b4b1",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "> how about adding a comment here to explain?\r\n\r\nOK",
        "createdAt" : "2021-05-08T02:25:25Z",
        "updatedAt" : "2021-05-10T08:41:36Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      },
      {
        "id" : "2a320bfe-4843-42bd-b1b8-ffb976ff0bb6",
        "parentId" : "020336a5-a96e-4504-900c-269318b9b4b1",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "Because the output schema of some `DDL` in `Hive` is differing from `Spark SQL,` we exclude it.\r\nFor example, the output schema of `SHOW TABLES` is `(namespace, tableName, isTemporary)` in `Hive`, but `(tableName)` in Spark SQL.",
        "createdAt" : "2021-05-08T02:25:48Z",
        "updatedAt" : "2021-05-10T08:41:36Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "3312ac00fad514d9fa68e9b6999f7037cccf15b6",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +89,93 @@    // For example, the output schema of SHOW TABLES is (namespace, tableName, isTemporary) in Hive,\n    // but (tableName) in Spark SQL.\n    \"cte-ddl.sql\",\n    // SPARK-28636\n    \"decimalArithmeticOperations.sql\","
  },
  {
    "id" : "a90a7283-8aba-4872-87af-c1493439c23b",
    "prId" : 32141,
    "prUrl" : "https://github.com/apache/spark/pull/32141#pullrequestreview-634188306",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c1385d69-c758-43a5-b557-53a8a25cff4e",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Should we specify hive: `-Phive` or `-Phive-2.3`? I usually do.",
        "createdAt" : "2021-04-13T05:09:21Z",
        "updatedAt" : "2021-04-13T06:33:35Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "68954235-10e3-43a7-926a-4ef7b0bf7deb",
        "parentId" : "c1385d69-c758-43a5-b557-53a8a25cff4e",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "`-Phive-thriftserver` is enough. Let's make it simple here.",
        "createdAt" : "2021-04-13T05:36:50Z",
        "updatedAt" : "2021-04-13T06:33:35Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "590632cc8486dd2610302aa11584a73981bf0342",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +43,47 @@ * To run the entire test suite:\n * {{{\n *   build/sbt -Phive-thriftserver \"hive-thriftserver/testOnly *ThriftServerQueryTestSuite\"\n * }}}\n *"
  },
  {
    "id" : "73c78514-825a-4faf-9079-b6bbe0cf21ef",
    "prId" : 32141,
    "prUrl" : "https://github.com/apache/spark/pull/32141#pullrequestreview-634189511",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3fa23520-302c-43b7-b95a-30aaa1cb5d6f",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "**\"a single test\"** - Such command can run multiple tests, for example:\r\n```\r\nSPARK_GENERATE_GOLDEN_FILES=1 build/sbt \"sql/testOnly *SQLQueryTestSuite -- -z datetime.sql\"\r\n```\r\nupdates:\r\n```\r\n./sql/core/src/test/resources/sql-tests/inputs/ansi/datetime.sql\r\n./sql/core/src/test/resources/sql-tests/inputs/datetime.sql\r\n```",
        "createdAt" : "2021-04-13T05:27:23Z",
        "updatedAt" : "2021-04-13T06:33:35Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "1cfbc3ac-1847-4408-b99a-cdb0e5e817de",
        "parentId" : "3fa23520-302c-43b7-b95a-30aaa1cb5d6f",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "Yeah I know. This is copied from `SQLQueryTestSuite`.\r\nIf the developer really wants to update the single test only, he/she has to use the exact test case name. Otherwise, \"-z e.sql\" can update multiple test outputs too.",
        "createdAt" : "2021-04-13T05:39:49Z",
        "updatedAt" : "2021-04-13T06:33:35Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "590632cc8486dd2610302aa11584a73981bf0342",
    "line" : 27,
    "diffHunk" : "@@ -1,1 +56,60 @@ * }}}\n *\n * To re-generate golden file for a single test, run:\n * {{{\n *   SPARK_GENERATE_GOLDEN_FILES=1 build/sbt \"sql/testOnly *SQLQueryTestSuite -- -z describe.sql\""
  },
  {
    "id" : "1e406207-42e7-45a5-a76b-90c660ed7f6f",
    "prId" : 28186,
    "prUrl" : "https://github.com/apache/spark/pull/28186#pullrequestreview-391764126",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ca0c06a5-bb29-457f-9e83-5c3b8cf451e5",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "oh, I totally missed the failures on Maven env... Thanks, @dongjoon-hyun .\r\nBtw, why the tests passed on sbt env only?",
        "createdAt" : "2020-04-11T06:19:42Z",
        "updatedAt" : "2020-04-11T06:19:42Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "e77d5903-c9c4-4686-907f-012c8036d863",
        "parentId" : "ca0c06a5-bb29-457f-9e83-5c3b8cf451e5",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "SBT test framework seems to get the correct resource path instead of the embedded resource path.",
        "createdAt" : "2020-04-11T06:22:33Z",
        "updatedAt" : "2020-04-11T06:22:34Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "d69ebb1e-518b-44ec-bdb1-28312aedb02d",
        "parentId" : "ca0c06a5-bb29-457f-9e83-5c3b8cf451e5",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "hm, I see. Thanks. ",
        "createdAt" : "2020-04-11T06:29:03Z",
        "updatedAt" : "2020-04-11T06:29:03Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "29836a72-11a3-457c-a959-685cc1406350",
        "parentId" : "ca0c06a5-bb29-457f-9e83-5c3b8cf451e5",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "@dongjoon-hyun Thanks for your fix. If we could copy files from class path to temp file. This indicates that the resource file exists in the jar package. So why Maven can't find the path?",
        "createdAt" : "2020-04-11T12:48:42Z",
        "updatedAt" : "2020-04-11T12:48:43Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      },
      {
        "id" : "c329e09d-6562-4d39-ab52-755d0eed74d4",
        "parentId" : "ca0c06a5-bb29-457f-9e83-5c3b8cf451e5",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "Btw, could we add a judge about env(e.g sbt) to avoid copying files ?",
        "createdAt" : "2020-04-11T12:55:24Z",
        "updatedAt" : "2020-04-11T12:55:24Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "bb7ea514ceddf6e95eb8b347de968aa18f540037",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +56,60 @@\n  override protected def testFile(fileName: String): String = {\n    val url = Thread.currentThread().getContextClassLoader.getResource(fileName)\n    // Copy to avoid URISyntaxException during accessing the resources in `sql/core`\n    val file = File.createTempFile(\"thriftserver-test\", \".data\")"
  },
  {
    "id" : "5551d7f2-f56e-48fc-bd5e-6291c36a6446",
    "prId" : 28186,
    "prUrl" : "https://github.com/apache/spark/pull/28186#pullrequestreview-391741283",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fbe62186-b794-4f94-a2a7-e9c4e987028c",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "nit: Any reason not to use `Utils.createTempDir` here?",
        "createdAt" : "2020-04-11T06:28:50Z",
        "updatedAt" : "2020-04-11T06:28:50Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "fdd43a30-935e-4d6f-a6f1-8ab1dafb170c",
        "parentId" : "fbe62186-b794-4f94-a2a7-e9c4e987028c",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "This is a file, not a directory~",
        "createdAt" : "2020-04-11T06:32:32Z",
        "updatedAt" : "2020-04-11T06:32:32Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "bb7ea514ceddf6e95eb8b347de968aa18f540037",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +58,62 @@    val url = Thread.currentThread().getContextClassLoader.getResource(fileName)\n    // Copy to avoid URISyntaxException during accessing the resources in `sql/core`\n    val file = File.createTempFile(\"thriftserver-test\", \".data\")\n    file.deleteOnExit()\n    FileUtils.copyURLToFile(url, file)"
  },
  {
    "id" : "b9af7c6e-9f32-49a1-95ee-6fcc8fa5d441",
    "prId" : 26543,
    "prUrl" : "https://github.com/apache/spark/pull/26543#pullrequestreview-320887261",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5a015b40-d736-4db4-93f1-e0ec721fd650",
        "parentId" : null,
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "Could we move this test to a new test suite because it is used to test these files:\r\nhttps://github.com/apache/spark/tree/master/sql/core/src/test/resources/sql-tests",
        "createdAt" : "2019-11-21T07:48:48Z",
        "updatedAt" : "2019-11-21T07:48:48Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      },
      {
        "id" : "49ffefd2-79da-48cf-9754-a2db3f139768",
        "parentId" : "5a015b40-d736-4db4-93f1-e0ec721fd650",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Ah, that's a good point, I missed that. @LantaoJin what about a follow up for these two small items?",
        "createdAt" : "2019-11-21T12:40:15Z",
        "updatedAt" : "2019-11-21T12:40:15Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "4b8bc4fb-a1ef-4a9b-b990-2600cdbb7a86",
        "parentId" : "5a015b40-d736-4db4-93f1-e0ec721fd650",
        "authorId" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "body" : "Sure. I will create a follow up issue",
        "createdAt" : "2019-11-21T13:39:15Z",
        "updatedAt" : "2019-11-21T13:39:16Z",
        "lastEditedBy" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "tags" : [
        ]
      }
    ],
    "commit" : "fa3e0dc2f509b35a69df99d8be7b539fe34359dd",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +255,259 @@  }\n\n  test(\"SPARK-29911: Uncache cached tables when session closed\") {\n    val cacheManager = spark.sharedState.cacheManager\n    val globalTempDB = spark.sharedState.globalTempViewManager.database"
  }
]