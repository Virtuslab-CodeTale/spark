[
  {
    "id" : "b1c1c755-ec9f-4176-8d13-c667281afffd",
    "prId" : 32959,
    "prUrl" : "https://github.com/apache/spark/pull/32959#pullrequestreview-697903239",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "947bb207-ddf0-4e5b-b770-d0dd23db70be",
        "parentId" : null,
        "authorId" : "29c89154-22f7-40ee-b295-115a55734d6c",
        "body" : "same reason to \"date.sql\" that thriftserver couldn't handle negative year",
        "createdAt" : "2021-07-02T07:31:13Z",
        "updatedAt" : "2021-07-02T07:31:14Z",
        "lastEditedBy" : "29c89154-22f7-40ee-b295-115a55734d6c",
        "tags" : [
        ]
      }
    ],
    "commit" : "538463a405c2eac1ddfccba8d82ca846f81e5a22",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +86,90 @@    \"datetime.sql\",\n    \"datetime-legacy.sql\",\n    \"ansi/datetime.sql\",\n    // SPARK-28620\n    \"postgreSQL/float4.sql\","
  },
  {
    "id" : "3a7bbcf6-f5ec-4ef8-a8a5-2a0b81ff8c85",
    "prId" : 32648,
    "prUrl" : "https://github.com/apache/spark/pull/32648#pullrequestreview-666454378",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e1ed1b21-3f10-4432-9ee6-e3350e5b7b95",
        "parentId" : null,
        "authorId" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "body" : "```\r\n/spark/sql/hive-thriftserver/src/test/scala/org/apache/spark/sql/hive/thriftserver/ThriftServerQueryTestSuite.scala:110:15\r\nThe outer reference in this type test cannot be checked at run time.\r\n        case _: PgSQLTest =>\r\n```",
        "createdAt" : "2021-05-24T07:07:00Z",
        "updatedAt" : "2021-05-24T07:07:00Z",
        "lastEditedBy" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "tags" : [
        ]
      }
    ],
    "commit" : "2a867a6277c1c6c4f1a6497973588304f907bede",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +108,112 @@\n      testCase match {\n        case _: SQLQueryTestSuite#PgSQLTest =>\n          statement.execute(s\"SET ${SQLConf.ANSI_ENABLED.key} = true\")\n          statement.execute(s\"SET ${SQLConf.LEGACY_INTERVAL_ENABLED.key} = true\")"
  },
  {
    "id" : "6ac50485-8a37-44f3-917a-c9d7c6889f9e",
    "prId" : 32648,
    "prUrl" : "https://github.com/apache/spark/pull/32648#pullrequestreview-666454787",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "62e345fa-3a5f-47e1-a409-3caaa3d15612",
        "parentId" : null,
        "authorId" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "body" : "```\r\n/spark/sql/hive-thriftserver/src/test/scala/org/apache/spark/sql/hive/thriftserver/ThriftServerQueryTestSuite.scala:113:15\r\nThe outer reference in this type test cannot be checked at run time.\r\n        case _: AnsiTest =>\r\n```",
        "createdAt" : "2021-05-24T07:07:41Z",
        "updatedAt" : "2021-05-24T07:07:41Z",
        "lastEditedBy" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "tags" : [
        ]
      }
    ],
    "commit" : "2a867a6277c1c6c4f1a6497973588304f907bede",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +111,115 @@          statement.execute(s\"SET ${SQLConf.ANSI_ENABLED.key} = true\")\n          statement.execute(s\"SET ${SQLConf.LEGACY_INTERVAL_ENABLED.key} = true\")\n        case _: SQLQueryTestSuite#AnsiTest =>\n          statement.execute(s\"SET ${SQLConf.ANSI_ENABLED.key} = true\")\n        case _ =>"
  },
  {
    "id" : "099a7e37-048d-4fd3-b730-e40383673cfb",
    "prId" : 32442,
    "prUrl" : "https://github.com/apache/spark/pull/32442#pullrequestreview-654946228",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "020336a5-a96e-4504-900c-269318b9b4b1",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "why it doesn't work in thriftserver?",
        "createdAt" : "2021-05-07T08:39:58Z",
        "updatedAt" : "2021-05-10T08:41:36Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "383adefd-5c3e-42ef-8bd2-f636d4f63c06",
        "parentId" : "020336a5-a96e-4504-900c-269318b9b4b1",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "Because the output schema of hive is different from spark",
        "createdAt" : "2021-05-07T08:50:57Z",
        "updatedAt" : "2021-05-10T08:41:36Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      },
      {
        "id" : "cdc0f736-f85e-456e-96d2-9c4ac88c8518",
        "parentId" : "020336a5-a96e-4504-900c-269318b9b4b1",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "because Hive doesn't support this syntax?",
        "createdAt" : "2021-05-07T13:08:06Z",
        "updatedAt" : "2021-05-10T08:41:36Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "3c94b925-e04f-4c51-986f-3d8954198312",
        "parentId" : "020336a5-a96e-4504-900c-269318b9b4b1",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "how about adding a comment here to explain?",
        "createdAt" : "2021-05-07T17:55:41Z",
        "updatedAt" : "2021-05-10T08:41:36Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "e55982e9-cb48-4fc8-be5b-2644fca8eeb0",
        "parentId" : "020336a5-a96e-4504-900c-269318b9b4b1",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "> how about adding a comment here to explain?\r\n\r\nOK",
        "createdAt" : "2021-05-08T02:25:25Z",
        "updatedAt" : "2021-05-10T08:41:36Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      },
      {
        "id" : "2a320bfe-4843-42bd-b1b8-ffb976ff0bb6",
        "parentId" : "020336a5-a96e-4504-900c-269318b9b4b1",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "Because the output schema of some `DDL` in `Hive` is differing from `Spark SQL,` we exclude it.\r\nFor example, the output schema of `SHOW TABLES` is `(namespace, tableName, isTemporary)` in `Hive`, but `(tableName)` in Spark SQL.",
        "createdAt" : "2021-05-08T02:25:48Z",
        "updatedAt" : "2021-05-10T08:41:36Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "3312ac00fad514d9fa68e9b6999f7037cccf15b6",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +89,93 @@    // For example, the output schema of SHOW TABLES is (namespace, tableName, isTemporary) in Hive,\n    // but (tableName) in Spark SQL.\n    \"cte-ddl.sql\",\n    // SPARK-28636\n    \"decimalArithmeticOperations.sql\","
  },
  {
    "id" : "a90a7283-8aba-4872-87af-c1493439c23b",
    "prId" : 32141,
    "prUrl" : "https://github.com/apache/spark/pull/32141#pullrequestreview-634188306",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c1385d69-c758-43a5-b557-53a8a25cff4e",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Should we specify hive: `-Phive` or `-Phive-2.3`? I usually do.",
        "createdAt" : "2021-04-13T05:09:21Z",
        "updatedAt" : "2021-04-13T06:33:35Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "68954235-10e3-43a7-926a-4ef7b0bf7deb",
        "parentId" : "c1385d69-c758-43a5-b557-53a8a25cff4e",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "`-Phive-thriftserver` is enough. Let's make it simple here.",
        "createdAt" : "2021-04-13T05:36:50Z",
        "updatedAt" : "2021-04-13T06:33:35Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "590632cc8486dd2610302aa11584a73981bf0342",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +43,47 @@ * To run the entire test suite:\n * {{{\n *   build/sbt -Phive-thriftserver \"hive-thriftserver/testOnly *ThriftServerQueryTestSuite\"\n * }}}\n *"
  },
  {
    "id" : "73c78514-825a-4faf-9079-b6bbe0cf21ef",
    "prId" : 32141,
    "prUrl" : "https://github.com/apache/spark/pull/32141#pullrequestreview-634189511",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3fa23520-302c-43b7-b95a-30aaa1cb5d6f",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "**\"a single test\"** - Such command can run multiple tests, for example:\r\n```\r\nSPARK_GENERATE_GOLDEN_FILES=1 build/sbt \"sql/testOnly *SQLQueryTestSuite -- -z datetime.sql\"\r\n```\r\nupdates:\r\n```\r\n./sql/core/src/test/resources/sql-tests/inputs/ansi/datetime.sql\r\n./sql/core/src/test/resources/sql-tests/inputs/datetime.sql\r\n```",
        "createdAt" : "2021-04-13T05:27:23Z",
        "updatedAt" : "2021-04-13T06:33:35Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "1cfbc3ac-1847-4408-b99a-cdb0e5e817de",
        "parentId" : "3fa23520-302c-43b7-b95a-30aaa1cb5d6f",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "Yeah I know. This is copied from `SQLQueryTestSuite`.\r\nIf the developer really wants to update the single test only, he/she has to use the exact test case name. Otherwise, \"-z e.sql\" can update multiple test outputs too.",
        "createdAt" : "2021-04-13T05:39:49Z",
        "updatedAt" : "2021-04-13T06:33:35Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "590632cc8486dd2610302aa11584a73981bf0342",
    "line" : 27,
    "diffHunk" : "@@ -1,1 +56,60 @@ * }}}\n *\n * To re-generate golden file for a single test, run:\n * {{{\n *   SPARK_GENERATE_GOLDEN_FILES=1 build/sbt \"sql/testOnly *SQLQueryTestSuite -- -z describe.sql\""
  },
  {
    "id" : "1e406207-42e7-45a5-a76b-90c660ed7f6f",
    "prId" : 28186,
    "prUrl" : "https://github.com/apache/spark/pull/28186#pullrequestreview-391764126",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ca0c06a5-bb29-457f-9e83-5c3b8cf451e5",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "oh, I totally missed the failures on Maven env... Thanks, @dongjoon-hyun .\r\nBtw, why the tests passed on sbt env only?",
        "createdAt" : "2020-04-11T06:19:42Z",
        "updatedAt" : "2020-04-11T06:19:42Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "e77d5903-c9c4-4686-907f-012c8036d863",
        "parentId" : "ca0c06a5-bb29-457f-9e83-5c3b8cf451e5",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "SBT test framework seems to get the correct resource path instead of the embedded resource path.",
        "createdAt" : "2020-04-11T06:22:33Z",
        "updatedAt" : "2020-04-11T06:22:34Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "d69ebb1e-518b-44ec-bdb1-28312aedb02d",
        "parentId" : "ca0c06a5-bb29-457f-9e83-5c3b8cf451e5",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "hm, I see. Thanks. ",
        "createdAt" : "2020-04-11T06:29:03Z",
        "updatedAt" : "2020-04-11T06:29:03Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "29836a72-11a3-457c-a959-685cc1406350",
        "parentId" : "ca0c06a5-bb29-457f-9e83-5c3b8cf451e5",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "@dongjoon-hyun Thanks for your fix. If we could copy files from class path to temp file. This indicates that the resource file exists in the jar package. So why Maven can't find the path?",
        "createdAt" : "2020-04-11T12:48:42Z",
        "updatedAt" : "2020-04-11T12:48:43Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      },
      {
        "id" : "c329e09d-6562-4d39-ab52-755d0eed74d4",
        "parentId" : "ca0c06a5-bb29-457f-9e83-5c3b8cf451e5",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "Btw, could we add a judge about env(e.g sbt) to avoid copying files ?",
        "createdAt" : "2020-04-11T12:55:24Z",
        "updatedAt" : "2020-04-11T12:55:24Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "bb7ea514ceddf6e95eb8b347de968aa18f540037",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +56,60 @@\n  override protected def testFile(fileName: String): String = {\n    val url = Thread.currentThread().getContextClassLoader.getResource(fileName)\n    // Copy to avoid URISyntaxException during accessing the resources in `sql/core`\n    val file = File.createTempFile(\"thriftserver-test\", \".data\")"
  },
  {
    "id" : "5551d7f2-f56e-48fc-bd5e-6291c36a6446",
    "prId" : 28186,
    "prUrl" : "https://github.com/apache/spark/pull/28186#pullrequestreview-391741283",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fbe62186-b794-4f94-a2a7-e9c4e987028c",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "nit: Any reason not to use `Utils.createTempDir` here?",
        "createdAt" : "2020-04-11T06:28:50Z",
        "updatedAt" : "2020-04-11T06:28:50Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "fdd43a30-935e-4d6f-a6f1-8ab1dafb170c",
        "parentId" : "fbe62186-b794-4f94-a2a7-e9c4e987028c",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "This is a file, not a directory~",
        "createdAt" : "2020-04-11T06:32:32Z",
        "updatedAt" : "2020-04-11T06:32:32Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "bb7ea514ceddf6e95eb8b347de968aa18f540037",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +58,62 @@    val url = Thread.currentThread().getContextClassLoader.getResource(fileName)\n    // Copy to avoid URISyntaxException during accessing the resources in `sql/core`\n    val file = File.createTempFile(\"thriftserver-test\", \".data\")\n    file.deleteOnExit()\n    FileUtils.copyURLToFile(url, file)"
  },
  {
    "id" : "b9af7c6e-9f32-49a1-95ee-6fcc8fa5d441",
    "prId" : 26543,
    "prUrl" : "https://github.com/apache/spark/pull/26543#pullrequestreview-320887261",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5a015b40-d736-4db4-93f1-e0ec721fd650",
        "parentId" : null,
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "Could we move this test to a new test suite because it is used to test these files:\r\nhttps://github.com/apache/spark/tree/master/sql/core/src/test/resources/sql-tests",
        "createdAt" : "2019-11-21T07:48:48Z",
        "updatedAt" : "2019-11-21T07:48:48Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      },
      {
        "id" : "49ffefd2-79da-48cf-9754-a2db3f139768",
        "parentId" : "5a015b40-d736-4db4-93f1-e0ec721fd650",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Ah, that's a good point, I missed that. @LantaoJin what about a follow up for these two small items?",
        "createdAt" : "2019-11-21T12:40:15Z",
        "updatedAt" : "2019-11-21T12:40:15Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "4b8bc4fb-a1ef-4a9b-b990-2600cdbb7a86",
        "parentId" : "5a015b40-d736-4db4-93f1-e0ec721fd650",
        "authorId" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "body" : "Sure. I will create a follow up issue",
        "createdAt" : "2019-11-21T13:39:15Z",
        "updatedAt" : "2019-11-21T13:39:16Z",
        "lastEditedBy" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "tags" : [
        ]
      }
    ],
    "commit" : "fa3e0dc2f509b35a69df99d8be7b539fe34359dd",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +255,259 @@  }\n\n  test(\"SPARK-29911: Uncache cached tables when session closed\") {\n    val cacheManager = spark.sharedState.cacheManager\n    val globalTempDB = spark.sharedState.globalTempViewManager.database"
  },
  {
    "id" : "ea65cb92-782e-48de-a462-ec2bfe0b554e",
    "prId" : 26418,
    "prUrl" : "https://github.com/apache/spark/pull/26418#pullrequestreview-318487111",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7931e714-6b8f-48d8-89bb-05dd34dd9973",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Was it only because we couldn't set the configurations? If Thrift server does not respect configuration like:\r\n\r\n```\r\n--SET spark.sql.intervalOutputStyle = ISO_8601\r\n```\r\n\r\nit looks a bug to me. @wangyum ",
        "createdAt" : "2019-11-18T09:42:39Z",
        "updatedAt" : "2019-11-18T09:42:39Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "1b3f3136-d781-4e1c-ab58-5934b9fa0f86",
        "parentId" : "7931e714-6b8f-48d8-89bb-05dd34dd9973",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "We can remove this config:\r\nhttps://github.com/apache/spark/blob/3163b6b43b99ca02642cf935d885ed2d0f98d633/sql/hive-thriftserver/src/test/scala/org/apache/spark/sql/hive/thriftserver/ThriftServerQueryTestSuite.scala#L78",
        "createdAt" : "2019-11-18T17:02:41Z",
        "updatedAt" : "2019-11-18T17:02:42Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      }
    ],
    "commit" : "0f54af89b8641ee7df80991dcca63e183a03d5ff",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +95,99 @@    \"subquery/in-subquery/in-order-by.sql\",\n    \"subquery/in-subquery/in-set-operations.sql\",\n    // SPARK-29783: need to set conf\n    \"interval-display-iso_8601.sql\",\n    \"interval-display-sql_standard.sql\""
  },
  {
    "id" : "58d1a28a-de35-472f-8f0a-04d351a20047",
    "prId" : 25868,
    "prUrl" : "https://github.com/apache/spark/pull/25868#pullrequestreview-303966874",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9b9a2c93-1ede-4d0e-892b-c5cf6749b1f3",
        "parentId" : null,
        "authorId" : "2d6b46ba-4100-4c4d-9341-fff5e39647ec",
        "body" : "Does this mean this is broken and the user should also turn if off? If so, should we change the default value? Otherwise, our tests are actually testing something that's rarely used.\r\n\r\n> Hive Thrift server should not executes SQL queries in an asynchronous way because we may set session configuration.\r\n\r\nCould you clarify what's the exact issue? Is it because the background thread is missing some thread-local variables because threads are reused? Can we copy them from the parent thread here? https://github.com/apache/spark/blob/master/sql/hive-thriftserver/src/main/scala/org/apache/spark/sql/hive/thriftserver/SparkExecuteStatementOperation.scala#L186",
        "createdAt" : "2019-09-24T09:08:54Z",
        "updatedAt" : "2019-09-24T09:10:14Z",
        "lastEditedBy" : "2d6b46ba-4100-4c4d-9341-fff5e39647ec",
        "tags" : [
        ]
      },
      {
        "id" : "572aac8a-8159-4649-b5f3-9a14b609ff2b",
        "parentId" : "9b9a2c93-1ede-4d0e-892b-c5cf6749b1f3",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Ur, this suite is not aiming *concurrency* stress test. It's just targeting SQL execution one by one.",
        "createdAt" : "2019-09-24T23:54:54Z",
        "updatedAt" : "2019-09-24T23:54:54Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "ef6f0c33-7f0e-44bf-85f6-a2fab64ac0a3",
        "parentId" : "9b9a2c93-1ede-4d0e-892b-c5cf6749b1f3",
        "authorId" : "2d6b46ba-4100-4c4d-9341-fff5e39647ec",
        "body" : "But it exposes some bugs in the default mode. Seems worth to fix the bug. ",
        "createdAt" : "2019-09-25T00:18:32Z",
        "updatedAt" : "2019-09-25T00:18:32Z",
        "lastEditedBy" : "2d6b46ba-4100-4c4d-9341-fff5e39647ec",
        "tags" : [
        ]
      },
      {
        "id" : "386d7083-386e-4785-b0e7-5a28d5e3113a",
        "parentId" : "9b9a2c93-1ede-4d0e-892b-c5cf6749b1f3",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "For me, that doesn't imply a bug in the default mode. That means we want to run one by one simply in this test suite.",
        "createdAt" : "2019-09-25T01:22:23Z",
        "updatedAt" : "2019-09-25T01:22:23Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "363d6674-da14-4d4f-b91c-f4376faf2d9f",
        "parentId" : "9b9a2c93-1ede-4d0e-892b-c5cf6749b1f3",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Of course, if there is a bug, definitely we should fix it. But, let's not enable that in this test suite. That is completely a separate issue, isn't it? With a separate UT and a patch, that should be handled.",
        "createdAt" : "2019-09-25T01:23:45Z",
        "updatedAt" : "2019-09-25T01:24:43Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "92dbe7b5-613e-441a-a365-cfdd40984583",
        "parentId" : "9b9a2c93-1ede-4d0e-892b-c5cf6749b1f3",
        "authorId" : "e5beb795-ac0e-4bd6-ad66-708215b8ae58",
        "body" : "With `HiveUtils.HIVE_THRIFT_SERVER_ASYNC` enabled the Thriftserver will still execute queries one by one. The difference is that it will not block the request:\r\n- With HIVE_THRIFT_SERVER_ASYNC=false client sends a query in an TExecuteStatementReq. The query executes, and only after it finishes the server responds with a TExecuteStatementResp. Then the client calls TGetOperationStatusReq to see if the result was a success or failure, and then potentially continues fetching results...\r\n- With HIVE_THRIFT_SERVER_ASYNC=true client sends a query in an TExecuteStatementReq, and the server starts it in a background thread and immediately returns a handle in the response. Then the client periodically polls with TGetOperationStatusReq until the query is finished, an then potentailly continues fetching results...\r\n\r\nIn both cases, the Hive JDBC driver executes one query at once and there is no concurrency.\r\n\r\nI think this setting does not need to be set.",
        "createdAt" : "2019-10-18T15:26:02Z",
        "updatedAt" : "2019-10-18T15:34:42Z",
        "lastEditedBy" : "e5beb795-ac0e-4bd6-ad66-708215b8ae58",
        "tags" : [
        ]
      }
    ],
    "commit" : "2eac612a33320e64a464df7ba9da683f36aa6b05",
    "line" : 58,
    "diffHunk" : "@@ -1,1 +79,83 @@    // Hive Thrift server should not executes SQL queries in an asynchronous way\n    // because we may set session configuration.\n    .set(HiveUtils.HIVE_THRIFT_SERVER_ASYNC, false)\n\n  override val isTestWithConfigSets = false"
  },
  {
    "id" : "bcfabe3b-1cea-4210-9a92-fb47f4278c0b",
    "prId" : 25868,
    "prUrl" : "https://github.com/apache/spark/pull/25868#pullrequestreview-304196619",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "db0147f4-ba63-4bce-af18-cb316f51850a",
        "parentId" : null,
        "authorId" : "e5beb795-ac0e-4bd6-ad66-708215b8ae58",
        "body" : "In an earlier PR I commented that the flakiness may be because of async issues.\r\nI meant that calling `startThriftServer` actually starts some things asynchronously in the background before the server becomes ready.\r\nMoving it to `beforeAll` instead of `beforeEach` should help the flakiness by having this race only at the start of the suite and not before every test, but I think adding a 3 or 5 s sleep would make sure it never happens.",
        "createdAt" : "2019-10-18T15:34:17Z",
        "updatedAt" : "2019-10-18T15:34:42Z",
        "lastEditedBy" : "e5beb795-ac0e-4bd6-ad66-708215b8ae58",
        "tags" : [
        ]
      },
      {
        "id" : "c8a5a520-0014-429b-8a28-c1562d54ce42",
        "parentId" : "db0147f4-ba63-4bce-af18-cb316f51850a",
        "authorId" : "e5beb795-ac0e-4bd6-ad66-708215b8ae58",
        "body" : "I seem to be hitting it in the first test of the suite quite often:\r\n```\r\n[info] ThriftServerQueryTestSuite:\r\n[info] - query_regex_column.sql *** FAILED *** (184 milliseconds)\r\n[info]   java.sql.SQLException: Could not open client transport with JDBC Uri: jdbc:hive2://localhost:15421: java.net.ConnectException: Connection refused (Connection refused)\r\n```",
        "createdAt" : "2019-10-18T19:21:50Z",
        "updatedAt" : "2019-10-18T19:21:50Z",
        "lastEditedBy" : "e5beb795-ac0e-4bd6-ad66-708215b8ae58",
        "tags" : [
        ]
      },
      {
        "id" : "c3b155f9-f104-4d98-b019-975fbadf8cb1",
        "parentId" : "db0147f4-ba63-4bce-af18-cb316f51850a",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "`pgSQL/text.sql` should fail if `spark.sql.hive.thriftServer.async` is enabled: https://github.com/apache/spark/pull/25567#issuecomment-524507309",
        "createdAt" : "2019-10-19T05:16:31Z",
        "updatedAt" : "2019-10-19T05:16:31Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      },
      {
        "id" : "b55fe791-2658-4402-9ffe-59bbe4f38e48",
        "parentId" : "db0147f4-ba63-4bce-af18-cb316f51850a",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "Anyway, let's try to enable it: https://github.com/apache/spark/pull/26172",
        "createdAt" : "2019-10-19T05:17:41Z",
        "updatedAt" : "2019-10-19T05:17:41Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      }
    ],
    "commit" : "2eac612a33320e64a464df7ba9da683f36aa6b05",
    "line" : 37,
    "diffHunk" : "@@ -1,1 +51,55 @@\n  override def beforeAll(): Unit = {\n    super.beforeAll()\n    // Chooses a random port between 10000 and 19999\n    var listeningPort = 10000 + Random.nextInt(10000)"
  },
  {
    "id" : "6318548c-f84e-438a-9a21-2bd5041c8844",
    "prId" : 25567,
    "prUrl" : "https://github.com/apache/spark/pull/25567#pullrequestreview-280562654",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "44b1d4de-96cf-46ab-b12f-67ddcfe81d4a",
        "parentId" : null,
        "authorId" : "e5beb795-ac0e-4bd6-ad66-708215b8ae58",
        "body" : "@wangyum \r\nI think this is the cause of flakiness.\r\n\r\nThis goes down to https://github.com/apache/spark/blob/master/sql/hive-thriftserver/v1.2.1/src/main/java/org/apache/hive/service/cli/thrift/ThriftCLIService.java#L178, which starts the service in an asynchronous background thread. It can race and try to connect in the test before the server actually gets initialized.\r\nMoving it to `beforeAll()` and adding a few seconds sleep should help.\r\nOr we could transplant the whole way from `HiveThriftServer2Test` that starts uses the start-thriftserver.sh script and monitors it's log until its started. Would it be possible to make `HiveThriftServer2Test` a trait that can be mixed in here?",
        "createdAt" : "2019-08-27T14:12:40Z",
        "updatedAt" : "2019-08-27T14:12:40Z",
        "lastEditedBy" : "e5beb795-ac0e-4bd6-ad66-708215b8ae58",
        "tags" : [
        ]
      },
      {
        "id" : "27c4b8d1-b599-4f62-8470-eb94a00f7b20",
        "parentId" : "44b1d4de-96cf-46ab-b12f-67ddcfe81d4a",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Thank you so much for the investigation, @juliuszsompolski !",
        "createdAt" : "2019-08-27T16:45:10Z",
        "updatedAt" : "2019-08-27T16:45:10Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "3b0698bb-c1f7-4adc-a786-f86389ec483e",
        "parentId" : "44b1d4de-96cf-46ab-b12f-67ddcfe81d4a",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "Move it to `beforeAll()` also unstable: https://github.com/apache/spark/pull/25567#issuecomment-524507309\r\n\r\nAnyway, I will try to move it to `beforeAll()` again.",
        "createdAt" : "2019-08-28T02:35:32Z",
        "updatedAt" : "2019-08-28T02:35:32Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      }
    ],
    "commit" : "2254809a4de19030ecf3b5561aafaeb5e1557b4b",
    "line" : 57,
    "diffHunk" : "@@ -1,1 +55,59 @@      started.orElse {\n        listeningPort += 1\n        Try(startThriftServer(listeningPort, attempt))\n      }\n    }.recover {"
  },
  {
    "id" : "bceaf247-e6a7-4463-be68-069e9bedb879",
    "prId" : 25567,
    "prUrl" : "https://github.com/apache/spark/pull/25567#pullrequestreview-318734079",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "897f383b-1448-48ba-93aa-8bf40f140086",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "I am trying to troubleshoot an jenkins build failure on this this, but the regular command which I use to run `SQLQueryTestSuite` \r\n```\r\nSPARK_GENERATE_GOLDEN_FILES=1 build/sbt \"sql/test-only *ThriftServerQueryTestSuite -- -z interval.sql\"\r\n```\r\nCould someone help me to run `ThriftServerQueryTestSuite`?",
        "createdAt" : "2019-11-15T16:48:43Z",
        "updatedAt" : "2019-11-15T16:48:44Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "69efc673-1b1c-48b0-9c9a-bcb0abb784ea",
        "parentId" : "897f383b-1448-48ba-93aa-8bf40f140086",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@wangyum, can you write some guides in the documentation about how we run each tests?",
        "createdAt" : "2019-11-18T04:03:02Z",
        "updatedAt" : "2019-11-18T04:03:02Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "d5b3b583-ef32-4667-87ee-63ebab313e21",
        "parentId" : "897f383b-1448-48ba-93aa-8bf40f140086",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "https://github.com/apache/spark/pull/26587",
        "createdAt" : "2019-11-19T01:24:46Z",
        "updatedAt" : "2019-11-19T01:24:46Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      }
    ],
    "commit" : "2254809a4de19030ecf3b5561aafaeb5e1557b4b",
    "line" : 45,
    "diffHunk" : "@@ -1,1 +43,47 @@ *   3. Support SHOW command.\n */\nclass ThriftServerQueryTestSuite extends SQLQueryTestSuite {\n\n  private var hiveServer2: HiveThriftServer2 = _"
  },
  {
    "id" : "88dc9f84-b303-4011-b233-8670864efa3d",
    "prId" : 25373,
    "prUrl" : "https://github.com/apache/spark/pull/25373#pullrequestreview-274198528",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f05e944e-fec9-42ad-8a6b-37134fc7eed8",
        "parentId" : null,
        "authorId" : "e5beb795-ac0e-4bd6-ad66-708215b8ae58",
        "body" : "Could we keep the connection open, to not have to start a sessopm amd reload the test data into temp views each time?\r\nWe could open the connection with `conn = DriverManager.getConnection(jdbcUri, user, \"\")` in beforeAll, load the test data there, and then have `withJDBCStatement` just create new statements, finally closing the connection in afterAll.\r\n\r\nHowever, it seems that opening new connection/session may be by design here, for test isolation. Then we'll have to leave it as is, but I think we should still be able to avoid starting the ThriftServer beforeEach.",
        "createdAt" : "2019-08-12T10:14:30Z",
        "updatedAt" : "2019-08-13T11:01:13Z",
        "lastEditedBy" : "e5beb795-ac0e-4bd6-ad66-708215b8ae58",
        "tags" : [
        ]
      },
      {
        "id" : "2696b787-e222-48c5-90b6-810650903524",
        "parentId" : "f05e944e-fec9-42ad-8a6b-37134fc7eed8",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "If move `loadTestData` to `beforeAll`. Some tests will fail:\r\n![image](https://user-images.githubusercontent.com/5399861/62935611-2747c780-bdfa-11e9-8974-4ac79696783b.png)\r\n",
        "createdAt" : "2019-08-13T10:44:02Z",
        "updatedAt" : "2019-08-13T11:01:13Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      }
    ],
    "commit" : "580288897aac3394894ab3674dedfd7c4bd3ad26",
    "line" : 265,
    "diffHunk" : "@@ -1,1 +263,267 @@  }\n\n  private def withJdbcStatement(fs: (Statement => Unit)*) {\n    val user = System.getProperty(\"user.name\")\n"
  }
]