[
  {
    "id" : "ba4d06d6-11d2-4797-875b-3bc21c1069b0",
    "prId" : 31054,
    "prUrl" : "https://github.com/apache/spark/pull/31054#pullrequestreview-563166445",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cda895f2-bbc2-416d-a7e2-21fbd4c41304",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "How about `statementBegin` -> `statementInProgress` and `includingStatement` -> `isStatement`?",
        "createdAt" : "2021-01-07T01:28:11Z",
        "updatedAt" : "2021-01-07T07:54:33Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "98d4bf1f-01f5-446f-bb53-ceb9295fc11b",
        "parentId" : "cda895f2-bbc2-416d-a7e2-21fbd4c41304",
        "authorId" : "b6393788-12cc-4066-9294-857f4baec092",
        "body" : "ok",
        "createdAt" : "2021-01-07T02:12:47Z",
        "updatedAt" : "2021-01-07T07:54:33Z",
        "lastEditedBy" : "b6393788-12cc-4066-9294-857f4baec092",
        "tags" : [
        ]
      }
    ],
    "commit" : "d348489f93cfad39d6a7102261ff7846d3254765",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +538,542 @@    def insideComment: Boolean = insideSimpleComment || insideBracketedComment\n    def statementInProgress(index: Int): Boolean = isStatement || (!insideComment &&\n      index > beginIndex && !s\"${line.charAt(index)}\".trim.isEmpty)\n\n    for (index <- 0 until line.length) {"
  },
  {
    "id" : "ab5f0562-7c8e-4d8d-b397-b99964b5a443",
    "prId" : 31054,
    "prUrl" : "https://github.com/apache/spark/pull/31054#pullrequestreview-563197250",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2568f820-6358-483d-a851-fc5d64679b10",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "nit:\r\n```\r\n// Decrements `bracketedCommentLevel` at the beginning of the next loop\r\nleavingBracketedComment = true\r\n```",
        "createdAt" : "2021-01-07T04:04:56Z",
        "updatedAt" : "2021-01-07T07:54:33Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "d348489f93cfad39d6a7102261ff7846d3254765",
    "line" : 48,
    "diffHunk" : "@@ -1,1 +596,600 @@        } else if (insideBracketedComment && line.charAt(index - 1) == '*' ) {\n          // Decrements `bracketedCommentLevel` at the beginning of the next loop\n          leavingBracketedComment = true\n        } else if (hasNext && !insideBracketedComment && line.charAt(index + 1) == '*') {\n          bracketedCommentLevel += 1"
  },
  {
    "id" : "142381d6-213a-47df-b49c-7d78dce8ed9c",
    "prId" : 29982,
    "prUrl" : "https://github.com/apache/spark/pull/29982#pullrequestreview-513764501",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "53c425e6-13f7-4698-8839-4e55fe93cfa5",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Ah, ... I notice that handling bracketed comments is more complicated than I thought. For example, `SELECT /*+ COALESCE(3) */ * FROM t`. In this case, we need to pass the comment into the Spark parser.",
        "createdAt" : "2020-10-21T12:37:03Z",
        "updatedAt" : "2021-01-04T15:06:22Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "6a71d55f-ea93-4c3d-ab95-ff8c02cca1e8",
        "parentId" : "53c425e6-13f7-4698-8839-4e55fe93cfa5",
        "authorId" : "b6393788-12cc-4066-9294-857f4baec092",
        "body" : "`SELECT /*+ COALESCE(3) */ * FROM t` would be passed to spark parser with this pr.\r\n\r\nOnly the comment without any statement content would not be passed to spark parser.",
        "createdAt" : "2020-10-21T14:40:32Z",
        "updatedAt" : "2021-01-04T15:06:22Z",
        "lastEditedBy" : "b6393788-12cc-4066-9294-857f4baec092",
        "tags" : [
        ]
      }
    ],
    "commit" : "f7f803034ae1ffa713b7015f214d63412e101389",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +531,535 @@    val ret = new JArrayList[String]\n\n    def insideBracketedComment: Boolean = bracketedCommentLevel > 0\n    def insideComment: Boolean = insideSimpleComment || insideBracketedComment\n    def statementBegin(index: Int): Boolean = includingStatement || (!insideComment &&"
  },
  {
    "id" : "82f98a26-a195-46ba-a6a5-f7ea2abb8cfa",
    "prId" : 28499,
    "prUrl" : "https://github.com/apache/spark/pull/28499#pullrequestreview-409756251",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d81a1647-9783-419b-9f17-17a77fbaa0d3",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Printing this stacktrace via logger (with `ERROR` levels) is not enough for your usecase? I feel that printing to a shell is a bit verbose.",
        "createdAt" : "2020-05-12T06:35:03Z",
        "updatedAt" : "2020-05-12T12:18:20Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "99d02a03-90b5-4cc7-af86-377bebf8e2bb",
        "parentId" : "d81a1647-9783-419b-9f17-17a77fbaa0d3",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "For AnalysisException, we won't get the stacktrace via logger. It's in the `DEBUG` level",
        "createdAt" : "2020-05-12T06:42:00Z",
        "updatedAt" : "2020-05-12T12:18:20Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "5bb1bd03-43c4-4086-8e91-54516f831403",
        "parentId" : "d81a1647-9783-419b-9f17-17a77fbaa0d3",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "FYI, https://github.com/apache/spark/blob/1fac06c4307c8c7a5a48a50952d48ee5b9ebccb2/sql/hive-thriftserver/src/main/scala/org/apache/spark/sql/hive/thriftserver/SparkSQLDriver.scala#L71",
        "createdAt" : "2020-05-12T06:43:00Z",
        "updatedAt" : "2020-05-12T12:18:20Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "66609c23-fa29-4935-83d7-a437c9666622",
        "parentId" : "d81a1647-9783-419b-9f17-17a77fbaa0d3",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "Or do you mean just use `logError`?",
        "createdAt" : "2020-05-12T06:45:05Z",
        "updatedAt" : "2020-05-12T12:18:20Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "ebaa7002-fa9c-411f-954a-cf3176583a0d",
        "parentId" : "d81a1647-9783-419b-9f17-17a77fbaa0d3",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Ah, I see. But, I'm still not sure that end users want to see stacktraces of analysis exceptions for debugging use.... Could we add a new internal config to turn on/off this error info in a server-side?",
        "createdAt" : "2020-05-12T06:51:08Z",
        "updatedAt" : "2020-05-12T12:18:20Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "fd075a23-0583-4c58-930d-f009d86928ab",
        "parentId" : "d81a1647-9783-419b-9f17-17a77fbaa0d3",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "Just for clarification, although this class is under the `thriftserver` package, but it has nothing to do with the `thriftserver` and there is no such `client/server` thing here. It is only a single `spark-sql` shell process. \r\n\r\nFor the config thing,  maybe we just need `sessionState.getIsSilent` and just like\r\n\r\n```scala\r\ncase e: AnalysisException => e.cause match {\r\n                case Some(_) if !sessionState.getIsSilent =>\r\n                  err.println(\r\n                    s\"\"\"Error in query: ${e.getMessage}\r\n                       |${org.apache.hadoop.util.StringUtils.stringifyException(e)}\r\n                     \"\"\".stripMargin)\r\n             case ....\r\n```",
        "createdAt" : "2020-05-12T07:05:05Z",
        "updatedAt" : "2020-05-12T12:18:20Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "368bd41d-c68b-4cd6-b5be-c764c699fe4b",
        "parentId" : "d81a1647-9783-419b-9f17-17a77fbaa0d3",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Yea, I personally think the proposed one above looks better than that in the current PR.",
        "createdAt" : "2020-05-12T07:09:37Z",
        "updatedAt" : "2020-05-12T12:18:20Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "6fcd4d2fe5f1b63640b1a2ca8c1efda5453c9d77",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +387,391 @@                    s\"\"\"Error in query: ${e.getMessage}\n                       |${org.apache.hadoop.util.StringUtils.stringifyException(e)}\n                     \"\"\".stripMargin)\n                // For analysis exceptions in silent mode or simple ones that only related to the\n                // query itself, such as `NoSuchDatabaseException`, only the error is printed out"
  },
  {
    "id" : "c91901f9-e596-473e-a6eb-a201dc578cea",
    "prId" : 28393,
    "prUrl" : "https://github.com/apache/spark/pull/28393#pullrequestreview-403263644",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "255aed03-683d-42f3-9353-3e3501ff553c",
        "parentId" : null,
        "authorId" : "a2fcc15e-a51a-42f0-87a6-137048a28e30",
        "body" : "@adrian-wang Should we update the comment to reflect the newly added condition?",
        "createdAt" : "2020-04-30T06:57:17Z",
        "updatedAt" : "2020-05-06T02:24:59Z",
        "lastEditedBy" : "a2fcc15e-a51a-42f0-87a6-137048a28e30",
        "tags" : [
        ]
      },
      {
        "id" : "e0745840-76ba-41e7-9a53-957a35bc9b90",
        "parentId" : "255aed03-683d-42f3-9353-3e3501ff553c",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "Yep, maybe we can rephrase [this comment](https://github.com/apache/spark/pull/28393/files#diff-f7aac41bf732c1ba1edbac436d331a55R510) here.",
        "createdAt" : "2020-04-30T07:23:14Z",
        "updatedAt" : "2020-05-06T02:24:59Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "d185264ae4acc585855b70b2ec3caa0e63c83043",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +522,526 @@    for (index <- 0 until line.length) {\n      if (line.charAt(index) == '\\'' && !insideComment) {\n        // take a look to see if it is escaped\n        // See the comment above about SPARK-31595\n        if (!escape && !insideDoubleQuote) {"
  },
  {
    "id" : "ac01f750-5ffa-46a4-a41e-e83f6cff8163",
    "prId" : 28393,
    "prUrl" : "https://github.com/apache/spark/pull/28393#pullrequestreview-403248890",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "791966c9-50ec-46f0-9956-38dffb911eca",
        "parentId" : null,
        "authorId" : "a2fcc15e-a51a-42f0-87a6-137048a28e30",
        "body" : "@adrian-wang Same.",
        "createdAt" : "2020-04-30T06:57:48Z",
        "updatedAt" : "2020-05-06T02:24:59Z",
        "lastEditedBy" : "a2fcc15e-a51a-42f0-87a6-137048a28e30",
        "tags" : [
        ]
      }
    ],
    "commit" : "d185264ae4acc585855b70b2ec3caa0e63c83043",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +529,533 @@        }\n      } else if (line.charAt(index) == '\\\"' && !insideComment) {\n        // take a look to see if it is escaped\n        // See the comment above about SPARK-31595\n        if (!escape && !insideSingleQuote) {"
  },
  {
    "id" : "2b0681ec-47ea-49e2-a61d-4acf9a9fb6df",
    "prId" : 27969,
    "prUrl" : "https://github.com/apache/spark/pull/27969#pullrequestreview-381898116",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "17cb4c74-cb11-47ec-a2a2-f4fa53cd4148",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "can we add some comments to explain why we skip `hive.metastore.warehouse.dir`?",
        "createdAt" : "2020-03-26T10:58:15Z",
        "updatedAt" : "2020-03-26T16:27:37Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "6dddf02bfbba6ce71fea0b64c8aaf1e6c30a1a63",
    "line" : 25,
    "diffHunk" : "@@ -1,1 +194,198 @@    // [[SharedState.loadHiveConfFile]] based on the user specified or default values of\n    // spark.sql.warehouse.dir and hive.metastore.warehouse.dir.\n    for ((k, v) <- newHiveConf if k != \"hive.metastore.warehouse.dir\") {\n      SparkSQLEnv.sqlContext.setConf(k, v)\n    }"
  }
]