[
  {
    "id" : "c694fc84-2720-4d52-92be-254b77ea14a6",
    "prId" : 32345,
    "prUrl" : "https://github.com/apache/spark/pull/32345#pullrequestreview-646576536",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "28ac3e3b-5d79-427a-873b-ee8a30bf3d6d",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Size of what does it return? `CalendarIntervalType`, `YearMonthIntervalType`, `DayTimeIntervalType` are returned as strings in rowSets.",
        "createdAt" : "2021-04-27T14:34:57Z",
        "updatedAt" : "2021-04-28T04:07:35Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "1049e845-7f6f-4863-8f7d-dd3bd86d3d77",
        "parentId" : "28ac3e3b-5d79-427a-873b-ee8a30bf3d6d",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "`CalendarIntervalType` return it's `defaultSize` (4 + 4 + 8 = 16).\r\nIt seems `YearMonthIntervalType` and `DayTimeIntervalType` should return `defaultSize` too.",
        "createdAt" : "2021-04-28T04:03:11Z",
        "updatedAt" : "2021-04-28T04:07:35Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "4c594bb749f67fa50042f626668a94fe151e40c2",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +132,136 @@  private def getColumnSize(typ: DataType): Option[Int] = typ match {\n    case dt @ (BooleanType | _: NumericType | DateType | TimestampType |\n               CalendarIntervalType | NullType | YearMonthIntervalType | DayTimeIntervalType) =>\n      Some(dt.defaultSize)\n    case CharType(n) => Some(n)"
  },
  {
    "id" : "de7a4092-614f-43c4-9179-5ca50db6c53f",
    "prId" : 31217,
    "prUrl" : "https://github.com/apache/spark/pull/31217#pullrequestreview-574312556",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "be241c21-8891-4519-a67a-d43d5c9c15f3",
        "parentId" : null,
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "It is by design. May be we should fix SPARK-31634 first. Another related issue: https://issues.apache.org/jira/browse/HIVE-23359",
        "createdAt" : "2021-01-18T00:40:22Z",
        "updatedAt" : "2021-01-18T04:51:21Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      },
      {
        "id" : "4ab659f9-4d80-4560-b106-c7132c435183",
        "parentId" : "be241c21-8891-4519-a67a-d43d5c9c15f3",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "cc @yaooqinn ",
        "createdAt" : "2021-01-18T01:05:58Z",
        "updatedAt" : "2021-01-18T04:51:21Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      },
      {
        "id" : "69f05278-c6df-4a2a-af16-46afc43a7077",
        "parentId" : "be241c21-8891-4519-a67a-d43d5c9c15f3",
        "authorId" : "031289b4-137a-4567-9185-75ac4065cf22",
        "body" : "Thanks for review @wangyum. SPARK-31634 talks about sql command, but this change affect thrift meta api. Can you clarify the relation between them?",
        "createdAt" : "2021-01-18T01:10:50Z",
        "updatedAt" : "2021-01-18T04:51:21Z",
        "lastEditedBy" : "031289b4-137a-4567-9185-75ac4065cf22",
        "tags" : [
        ]
      },
      {
        "id" : "017d2cb8-d1f4-4787-bf91-87a7ffbda739",
        "parentId" : "be241c21-8891-4519-a67a-d43d5c9c15f3",
        "authorId" : "031289b4-137a-4567-9185-75ac4065cf22",
        "body" : "@yaooqinn Any suggestions?",
        "createdAt" : "2021-01-22T14:33:41Z",
        "updatedAt" : "2021-01-22T14:33:41Z",
        "lastEditedBy" : "031289b4-137a-4567-9185-75ac4065cf22",
        "tags" : [
        ]
      }
    ],
    "commit" : "8d5c62870fa8c3ea5798f63affeba12d6f675074",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +105,109 @@      if (schemaName == null || schemaName.isEmpty || schemaName == \"*\"\n        || Pattern.compile(CLIServiceUtils.patternToRegex(schemaName))\n        .matcher(globalTempViewDb).matches()) {\n        catalog.globalTempViewManager.listViewNames(tablePattern).foreach { globalTempView =>\n          catalog.getGlobalTempView(globalTempView).foreach { plan =>"
  },
  {
    "id" : "2f040719-f847-4898-90f2-d00f6df6f734",
    "prId" : 29687,
    "prUrl" : "https://github.com/apache/spark/pull/29687#pullrequestreview-485537375",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c44382c4-1cc6-4775-bd79-53cd83d462dc",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "so it's not a test only PR. Can we remove [TEST] from PR title?",
        "createdAt" : "2020-09-10T03:18:05Z",
        "updatedAt" : "2020-09-10T03:18:06Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "f9b08063-ba8b-4c0c-83c8-ba9f218f6447",
        "parentId" : "c44382c4-1cc6-4775-bd79-53cd83d462dc",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "OK~",
        "createdAt" : "2020-09-10T03:18:48Z",
        "updatedAt" : "2020-09-10T03:18:48Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "2e6914ded7582cc02d215d7d435d76b9bf90a720",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +132,136 @@  private def getColumnSize(typ: DataType): Option[Int] = typ match {\n    case dt @ (BooleanType | _: NumericType | DateType | TimestampType |\n               CalendarIntervalType | NullType) =>\n      Some(dt.defaultSize)\n    case StructType(fields) =>"
  },
  {
    "id" : "b49e67d4-c70a-452d-bf27-ed64a9b36937",
    "prId" : 29303,
    "prUrl" : "https://github.com/apache/spark/pull/29303#pullrequestreview-459042406",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9192e3ea-bcb5-44f0-9c14-0d9124d5ab77",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "how do we pick these numbers?",
        "createdAt" : "2020-07-31T08:37:34Z",
        "updatedAt" : "2020-08-03T09:33:46Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "6fba9ea1-4c32-47ed-8e5c-56c340d2077d",
        "parentId" : "9192e3ea-bcb5-44f0-9c14-0d9124d5ab77",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "They are from hive's `TypeDescriptor` and I also checked the results, which are same from hive and spark, e.g. `select 0.12345678901234567890D`",
        "createdAt" : "2020-07-31T08:50:21Z",
        "updatedAt" : "2020-08-03T09:33:46Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "bff65186-f104-4d93-84ed-5bad1eddfd2c",
        "parentId" : "9192e3ea-bcb5-44f0-9c14-0d9124d5ab77",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "the result is `0.12345678901234568` which have 17 decimal digits but hive says it's 15 for double...",
        "createdAt" : "2020-07-31T08:52:05Z",
        "updatedAt" : "2020-08-03T09:33:46Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "2261667c-674c-4b2f-8120-77504d7128db",
        "parentId" : "9192e3ea-bcb5-44f0-9c14-0d9124d5ab77",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "It  seems to follow `IEEE 754` https://en.wikipedia.org/wiki/IEEE_754",
        "createdAt" : "2020-07-31T08:57:49Z",
        "updatedAt" : "2020-08-03T09:33:46Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "828fe698-a1c2-4822-9fb7-44ef279fe2ff",
        "parentId" : "9192e3ea-bcb5-44f0-9c14-0d9124d5ab77",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Can we add a code comment to explain it?",
        "createdAt" : "2020-07-31T09:22:43Z",
        "updatedAt" : "2020-08-03T09:33:46Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "9c544d82-459a-4748-80f0-c44482f3ecc2",
        "parentId" : "9192e3ea-bcb5-44f0-9c14-0d9124d5ab77",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "OK",
        "createdAt" : "2020-07-31T09:34:18Z",
        "updatedAt" : "2020-08-03T09:33:46Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "cf34aa3fced16d61b75e362a418d8c67a7de7023",
    "line" : 55,
    "diffHunk" : "@@ -1,1 +154,158 @@    case BooleanType | _: IntegerType => Some(0)\n    case FloatType => Some(7)\n    case DoubleType => Some(15)\n    case d: DecimalType => Some(d.scale)\n    case TimestampType => Some(6)"
  },
  {
    "id" : "4c76346b-f456-411a-b349-a7a9b0b9b883",
    "prId" : 29303,
    "prUrl" : "https://github.com/apache/spark/pull/29303#pullrequestreview-459021479",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "29e3b84b-bcd8-4d1b-b7cf-8c97bc4e1706",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Could you make the title and description clearer? e.g., which field does this PR add (COLUMN_SIZE, DECIMAL_DIGITS, and NUM_PREC_RADIX, and ORDINAL_POSITION?)",
        "createdAt" : "2020-07-31T08:43:08Z",
        "updatedAt" : "2020-08-03T09:33:46Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "1ed8a25b-963e-4a88-a3e4-805cf0d22a7a",
        "parentId" : "29e3b84b-bcd8-4d1b-b7cf-8c97bc4e1706",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "OK, thanks Takeshi ~",
        "createdAt" : "2020-07-31T08:58:58Z",
        "updatedAt" : "2020-08-03T09:33:46Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "cf34aa3fced16d61b75e362a418d8c67a7de7023",
    "line" : 81,
    "diffHunk" : "@@ -1,1 +180,184 @@          toJavaSQLType(column.dataType.sql).asInstanceOf[AnyRef], // DATA_TYPE\n          column.dataType.sql, // TYPE_NAME\n          getColumnSize(column.dataType).map(_.asInstanceOf[AnyRef]).orNull, // COLUMN_SIZE\n          null, // BUFFER_LENGTH, unused\n          getDecimalDigits(column.dataType).map(_.asInstanceOf[AnyRef]).orNull, // DECIMAL_DIGITS"
  },
  {
    "id" : "383fd944-3f3a-4f9a-a138-37450bc14c01",
    "prId" : 29303,
    "prUrl" : "https://github.com/apache/spark/pull/29303#pullrequestreview-459716541",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7e37c070-2d09-4741-bed7-5b1d82b9de5e",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Hive returns the almost same values for column size?\r\nhttps://github.com/apache/hive/blob/3e5e99eae154ceb8f9aa4e4ec71e6b05310e98e4/service/src/java/org/apache/hive/service/cli/operation/GetColumnsOperation.java#L187-L211",
        "createdAt" : "2020-07-31T13:10:03Z",
        "updatedAt" : "2020-08-03T09:33:46Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "f02c24bc-53cb-45fd-954a-2cb152a20bff",
        "parentId" : "7e37c070-2d09-4741-bed7-5b1d82b9de5e",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "Hive does not return same result for each type",
        "createdAt" : "2020-08-03T02:35:39Z",
        "updatedAt" : "2020-08-03T09:33:46Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "cf34aa3fced16d61b75e362a418d8c67a7de7023",
    "line" : 32,
    "diffHunk" : "@@ -1,1 +131,135 @@   * For array, map, string, and binaries, the column size is variable, return null as unknown.\n   */\n  private def getColumnSize(typ: DataType): Option[Int] = typ match {\n    case dt @ (BooleanType | _: NumericType | DateType | TimestampType) => Some(dt.defaultSize)\n    case StructType(fields) =>"
  },
  {
    "id" : "e238391d-2cb3-43c4-8acb-f76468cb4cd4",
    "prId" : 29303,
    "prUrl" : "https://github.com/apache/spark/pull/29303#pullrequestreview-459986776",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2a6ff7fd-189e-47fe-a990-9320c8ad3045",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Should this respect `column.nullable`?",
        "createdAt" : "2020-08-03T12:24:37Z",
        "updatedAt" : "2020-08-03T12:24:38Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "8f5e7493-9dca-404b-97c6-3205873fdee1",
        "parentId" : "2a6ff7fd-189e-47fe-a990-9320c8ad3045",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "IS_NULLABLE is related to PK, \"YES\" here is more specific as we don't have such thing implemented",
        "createdAt" : "2020-08-03T12:32:04Z",
        "updatedAt" : "2020-08-03T12:32:04Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "cf34aa3fced16d61b75e362a418d8c67a7de7023",
    "line" : 95,
    "diffHunk" : "@@ -1,1 +191,195 @@          null, // CHAR_OCTET_LENGTH\n          pos.asInstanceOf[AnyRef], // ORDINAL_POSITION\n          \"YES\", // IS_NULLABLE\n          null, // SCOPE_CATALOG\n          null, // SCOPE_SCHEMA"
  },
  {
    "id" : "851333d3-1475-4b1c-b22c-e1a8ec6f229a",
    "prId" : 24906,
    "prUrl" : "https://github.com/apache/spark/pull/24906#pullrequestreview-252882928",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "491abb04-d894-4909-9489-d6cc98b0661d",
        "parentId" : null,
        "authorId" : "e5beb795-ac0e-4bd6-ad66-708215b8ae58",
        "body" : "BTW: I believe that this doesn't take the global temp views into account.\r\nI think it could be solved by\r\n```\r\n    val matchingDbs = {\r\n      val databases = catalog.listDatabases(schemaPattern)\r\n      // Add global temp view schema if matches\r\n      val databasePattern = Pattern.compile(CLIServiceUtils.patternToRegex(sparkSchemaName))\r\n      if (databasePattern.matcher(catalog.globalTempViewManager.database).matches()) {\r\n        databases :+ catalog.globalTempViewManager.database\r\n      } else {\r\n        databases\r\n      }\r\n    }\r\n```\r\n(same problem seems to exist in SparkGetTablesOperation and SparkGetSchemasOperation)",
        "createdAt" : "2019-06-20T08:59:06Z",
        "updatedAt" : "2019-06-21T15:33:29Z",
        "lastEditedBy" : "e5beb795-ac0e-4bd6-ad66-708215b8ae58",
        "tags" : [
        ]
      },
      {
        "id" : "9b68e960-de0a-432c-8b27-86c82807c6bc",
        "parentId" : "491abb04-d894-4909-9489-d6cc98b0661d",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "Thank you @juliuszsompolski It seems need more changes:\r\n```java\r\n19/06/21 22:51:53 WARN ThriftCLIService: Error getting tables: \r\njava.lang.RuntimeException: org.apache.spark.sql.AnalysisException: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to fetch tables of db global_temp;\r\n        at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:83)\r\n        at org.apache.hive.service.cli.session.HiveSessionProxy.access$000(HiveSessionProxy.java:36)\r\n        at org.apache.hive.service.cli.session.HiveSessionProxy$1.run(HiveSessionProxy.java:63)\r\n        at java.security.AccessController.doPrivileged(Native Method)\r\n        at javax.security.auth.Subject.doAs(Subject.java:422)\r\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746)\r\n        at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:59)\r\n        at com.sun.proxy.$Proxy23.getTables(Unknown Source)\r\n        at org.apache.hive.service.cli.CLIService.getTables(CLIService.java:326)\r\n        at org.apache.hive.service.cli.thrift.ThriftCLIService.GetTables(ThriftCLIService.java:495)\r\n        at org.apache.hive.service.cli.thrift.TCLIService$Processor$GetTables.getResult(TCLIService.java:1393)\r\n        at org.apache.hive.service.cli.thrift.TCLIService$Processor$GetTables.getResult(TCLIService.java:1378)\r\n        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\r\n        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\r\n        at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:53)\r\n        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n        at java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.sql.AnalysisException: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to fetch tables of db global_temp;\r\n        at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:109)\r\n        at org.apache.spark.sql.hive.HiveExternalCatalog.getTablesByName(HiveExternalCatalog.scala:710)\r\n        at org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.getTablesByName(ExternalCatalogWithListener.scala:142)\r\n        at org.apache.spark.sql.catalyst.catalog.SessionCatalog.getTablesByName(SessionCatalog.scala:459)\r\n        at org.apache.spark.sql.hive.thriftserver.SparkGetTablesOperation.$anonfun$runInternal$1(SparkGetTablesOperation.scala:89)\r\n        at org.apache.spark.sql.hive.thriftserver.SparkGetTablesOperation.$anonfun$runInternal$1$adapted(SparkGetTablesOperation.scala:87)\r\n        at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n        at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n        at org.apache.spark.sql.hive.thriftserver.SparkGetTablesOperation.runInternal(SparkGetTablesOperation.scala:87)\r\n        at org.apache.hive.service.cli.operation.Operation.run(Operation.java:257)\r\n        at org.apache.hive.service.cli.session.HiveSessionImpl.getTables(HiveSessionImpl.java:556)\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n        at java.lang.reflect.Method.invoke(Method.java:498)\r\n        at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:78)\r\n        ... 18 more\r\nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to fetch tables of db global_temp\r\n        at org.apache.spark.sql.hive.client.HiveClientImpl.getRawTablesByName(HiveClientImpl.scala:401)\r\n        at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$getTablesByName$1(HiveClientImpl.scala:412)\r\n        at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:310)\r\n        at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:244)\r\n        at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:243)\r\n        at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:293)\r\n        at org.apache.spark.sql.hive.client.HiveClientImpl.getTablesByName(HiveClientImpl.scala:412)\r\n        at org.apache.spark.sql.hive.HiveExternalCatalog.getRawTablesByNames(HiveExternalCatalog.scala:124)\r\n        at org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$getTablesByName$1(HiveExternalCatalog.scala:710)\r\n        at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:99)\r\n        ... 34 more\r\nCaused by: UnknownDBException(message:Could not find database global_temp)\r\n        at org.apache.hadoop.hive.metastore.ObjectStore.getTableObjectsByName(ObjectStore.java:1022)\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n        at java.lang.reflect.Method.invoke(Method.java:498)\r\n        at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:114)\r\n        at com.sun.proxy.$Proxy13.getTableObjectsByName(Unknown Source)\r\n        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_table_objects_by_name(HiveMetaStore.java:1854)\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n        at java.lang.reflect.Method.invoke(Method.java:498)\r\n        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)\r\n        at com.sun.proxy.$Proxy15.get_table_objects_by_name(Unknown Source)\r\n        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getTableObjectsByName(HiveMetaStoreClient.java:1223)\r\n        at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.getTableObjectsByName(SessionHiveMetaStoreClient.java:197)\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n        at java.lang.reflect.Method.invoke(Method.java:498)\r\n        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:156)\r\n        at com.sun.proxy.$Proxy16.getTableObjectsByName(Unknown Source)\r\n        at org.apache.spark.sql.hive.client.HiveClientImpl.getRawTablesByName(HiveClientImpl.scala:397)\r\n        ... 43 more\r\n```\r\n\r\nHow about support `GLOBAL TEMP VIEW` and `TEMPORARY VIEW` in the next PR?",
        "createdAt" : "2019-06-21T14:59:44Z",
        "updatedAt" : "2019-06-21T15:33:29Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      },
      {
        "id" : "89682f5f-80b8-4ddb-87c0-b62f2575bee6",
        "parentId" : "491abb04-d894-4909-9489-d6cc98b0661d",
        "authorId" : "e5beb795-ac0e-4bd6-ad66-708215b8ae58",
        "body" : "Sure, makes sense; to then add it to all three operations.",
        "createdAt" : "2019-06-21T15:11:54Z",
        "updatedAt" : "2019-06-21T15:33:29Z",
        "lastEditedBy" : "e5beb795-ac0e-4bd6-ad66-708215b8ae58",
        "tags" : [
        ]
      },
      {
        "id" : "e9810e1a-3144-4024-86c3-6b5e91e02a12",
        "parentId" : "491abb04-d894-4909-9489-d6cc98b0661d",
        "authorId" : "e5beb795-ac0e-4bd6-ad66-708215b8ae58",
        "body" : "Seems `getTablesByName` needs a similar check like `listTables` https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/catalog/SessionCatalog.scala#L788",
        "createdAt" : "2019-06-21T15:15:45Z",
        "updatedAt" : "2019-06-21T15:33:29Z",
        "lastEditedBy" : "e5beb795-ac0e-4bd6-ad66-708215b8ae58",
        "tags" : [
        ]
      }
    ],
    "commit" : "b00d4a2da2f7c69cc5fc48403d4e07386d85ce16",
    "line" : 81,
    "diffHunk" : "@@ -1,1 +79,83 @@    val db2Tabs = catalog.listDatabases(schemaPattern).map { dbName =>\n      (dbName, catalog.listTables(dbName, tablePattern))\n    }.toMap\n\n    if (isAuthV2Enabled) {"
  },
  {
    "id" : "38e48482-bbeb-487a-ac4e-d66a9f7c258f",
    "prId" : 24906,
    "prUrl" : "https://github.com/apache/spark/pull/24906#pullrequestreview-253723030",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f6bd4e41-a5e6-457d-ae67-6a1b586ff476",
        "parentId" : null,
        "authorId" : "e5beb795-ac0e-4bd6-ad66-708215b8ae58",
        "body" : "Actually, currently the metadata operations are not visible in the SparkUI. We could log this same string as `statement` for `HiveThriftServer2.listener.onStatementStart`, here and also in GetTables and GetSchemas operation.\r\n@wangyum @gatorsmile do you think it would be a good followup?",
        "createdAt" : "2019-06-24T11:26:54Z",
        "updatedAt" : "2019-06-24T11:26:55Z",
        "lastEditedBy" : "e5beb795-ac0e-4bd6-ad66-708215b8ae58",
        "tags" : [
        ]
      },
      {
        "id" : "038dbf77-174a-4bdd-8dec-e4af0f76455f",
        "parentId" : "f6bd4e41-a5e6-457d-ae67-6a1b586ff476",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "+1",
        "createdAt" : "2019-06-25T00:43:14Z",
        "updatedAt" : "2019-06-25T00:43:15Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      }
    ],
    "commit" : "b00d4a2da2f7c69cc5fc48403d4e07386d85ce16",
    "line" : 64,
    "diffHunk" : "@@ -1,1 +62,66 @@    val cmdStr = s\"catalog : $catalogName, schemaPattern : $schemaName, tablePattern : $tableName\" +\n      s\", columnName : $columnName\"\n    logInfo(s\"GetColumnsOperation: $cmdStr\")\n\n    setState(OperationState.RUNNING)"
  }
]