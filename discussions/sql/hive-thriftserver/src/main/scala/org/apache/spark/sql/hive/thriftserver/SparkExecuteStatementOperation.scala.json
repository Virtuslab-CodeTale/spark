[
  {
    "id" : "66f1836f-c3d1-431d-99e7-fbc0c54487a4",
    "prId" : 32995,
    "prUrl" : "https://github.com/apache/spark/pull/32995#pullrequestreview-688658903",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6e681a88-5e05-4930-8413-6bac726cb435",
        "parentId" : null,
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "It seems that there is not better choice from: https://github.com/apache/hive/blob/master/serde/src/java/org/apache/hadoop/hive/serde2/thrift/Type.java#L230",
        "createdAt" : "2021-06-21T12:55:31Z",
        "updatedAt" : "2021-06-21T12:55:31Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "2c8e4c50-954d-474a-b56d-25569a02a8ab",
        "parentId" : "6e681a88-5e05-4930-8413-6bac726cb435",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "We should probably map the existing timestamp to `TIMESTAMPLOCALTZ`, we can discuss this in an individual PR.",
        "createdAt" : "2021-06-21T16:41:17Z",
        "updatedAt" : "2021-06-21T16:41:38Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "88c59477-7252-4566-828c-6428a894adc7",
        "parentId" : "6e681a88-5e05-4930-8413-6bac726cb435",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "+1",
        "createdAt" : "2021-06-21T16:45:24Z",
        "updatedAt" : "2021-06-21T16:45:24Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "d5afd6abdb6eb56e91b2d95b0fecad6468d2a64b",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +380,384 @@        case _: YearMonthIntervalType => \"interval_year_month\"\n        case _: DayTimeIntervalType => \"interval_day_time\"\n        case _: TimestampWithoutTZType => \"timestamp\"\n        case other => other.catalogString\n      }"
  },
  {
    "id" : "c335a386-8a8a-4c75-8a2c-971830276d72",
    "prId" : 29933,
    "prUrl" : "https://github.com/apache/spark/pull/29933#pullrequestreview-507948419",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b33772a6-1964-44fd-bdf4-886f5bf27be0",
        "parentId" : null,
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "Maybe we need to call shutdown for this executor ?\r\n\r\nhttps://github.com/apache/hive/blob/940ee46d3a7a9300a47e5beae7c3ca6eb32fd759/service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java#L170-L184",
        "createdAt" : "2020-10-14T02:20:39Z",
        "updatedAt" : "2020-10-14T23:22:41Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "5e3c44060567d2234dd79d222f77bc58983bb805",
    "line" : 55,
    "diffHunk" : "@@ -1,1 +230,234 @@      }, timeout, TimeUnit.SECONDS)\n    }\n\n    if (!runInBackground) {\n      execute()"
  },
  {
    "id" : "4fb9aee1-be56-42c7-8c1c-9bea485d0fa8",
    "prId" : 29204,
    "prUrl" : "https://github.com/apache/spark/pull/29204#pullrequestreview-454606113",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dffaa272-cd36-48cb-a365-a04d26cb6aec",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "is `log.info` and `logInfo` the same?",
        "createdAt" : "2020-07-23T14:15:01Z",
        "updatedAt" : "2020-07-27T08:47:59Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "7c599fc7-d0f1-4d14-9b61-82f5d686b56f",
        "parentId" : "dffaa272-cd36-48cb-a365-a04d26cb6aec",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "`logInfo` will check `log.isInfoEnabled`",
        "createdAt" : "2020-07-24T02:31:19Z",
        "updatedAt" : "2020-07-27T08:47:59Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "50113145de4d9c7247d2a8af6e1e4f1087d19548",
    "line" : 34,
    "diffHunk" : "@@ -1,1 +179,183 @@      }\n      previousFetchEndOffset = resultOffset\n      logInfo(s\"Returning result set with ${curRow} rows from offsets \" +\n        s\"[$previousFetchStartOffset, $previousFetchEndOffset) with $statementId\")\n      resultRowSet"
  },
  {
    "id" : "9713bcee-728d-416e-b039-128492689c4d",
    "prId" : 28991,
    "prUrl" : "https://github.com/apache/spark/pull/28991#pullrequestreview-442605674",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "11b598c4-8787-40e2-9bc9-b0e067cfcc16",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "nit: `logInfo` is located just before `HiveThriftServer2.eventManager.onXXX`?\r\n```\r\n        cleanup()\r\n        setState(OperationState.TIMEDOUT)\r\n        logInfo(s\"Timeout and Cancel query with $statementId \")\r\n        HiveThriftServer2.eventManager.onStatementCanceled(statementId)\r\n```\r\nhttps://github.com/apache/spark/blob/42f01e314b4874236544cc8b94bef766269385ee/sql/hive-thriftserver/src/main/scala/org/apache/spark/sql/hive/thriftserver/SparkOperation.scala#L51-L52",
        "createdAt" : "2020-07-03T08:22:15Z",
        "updatedAt" : "2020-08-11T02:48:03Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "bfd7ec1a-d1e0-4eef-94e1-5b705abb3992",
        "parentId" : "11b598c4-8787-40e2-9bc9-b0e067cfcc16",
        "authorId" : "7f1185d9-3900-4812-b576-80ba5f410d6b",
        "body" : "thanks, fixed",
        "createdAt" : "2020-07-03T09:17:18Z",
        "updatedAt" : "2020-08-11T02:48:03Z",
        "lastEditedBy" : "7f1185d9-3900-4812-b576-80ba5f410d6b",
        "tags" : [
        ]
      },
      {
        "id" : "75ac05d6-81ba-46c8-95fd-98eb937780f9",
        "parentId" : "11b598c4-8787-40e2-9bc9-b0e067cfcc16",
        "authorId" : "e5beb795-ac0e-4bd6-ad66-708215b8ae58",
        "body" : "Please setState before cleanup. It's an open bug, see https://github.com/apache/spark/pull/28912",
        "createdAt" : "2020-07-03T13:57:32Z",
        "updatedAt" : "2020-08-11T02:48:03Z",
        "lastEditedBy" : "e5beb795-ac0e-4bd6-ad66-708215b8ae58",
        "tags" : [
        ]
      },
      {
        "id" : "4facb138-f6a8-4db2-847a-5f7eebb6b578",
        "parentId" : "11b598c4-8787-40e2-9bc9-b0e067cfcc16",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "> Please setState before cleanup. It's an open bug, see #28912\r\n\r\nYea, nice suggestion! Might be better to add tests for that case, too.",
        "createdAt" : "2020-07-03T22:18:30Z",
        "updatedAt" : "2020-08-11T02:48:03Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "5038ec04-6819-4f2e-b2d0-42afb1ab69b3",
        "parentId" : "11b598c4-8787-40e2-9bc9-b0e067cfcc16",
        "authorId" : "7f1185d9-3900-4812-b576-80ba5f410d6b",
        "body" : "> Please setState before cleanup. It's an open bug, see #28912\r\n\r\nFixed, thanks for your review",
        "createdAt" : "2020-07-04T11:54:02Z",
        "updatedAt" : "2020-08-11T02:48:03Z",
        "lastEditedBy" : "7f1185d9-3900-4812-b576-80ba5f410d6b",
        "tags" : [
        ]
      }
    ],
    "commit" : "da76e1a7ba25ca96495cbb2ce56377983ee9f74f",
    "line" : 38,
    "diffHunk" : "@@ -1,1 +314,318 @@    synchronized {\n      if (!getStatus.getState.isTerminal) {\n        setState(OperationState.TIMEDOUT)\n        cleanup()\n        logInfo(s\"Timeout and Cancel query with $statementId \")"
  },
  {
    "id" : "a8b12201-6129-4660-9ef8-db0a5f10919e",
    "prId" : 27552,
    "prUrl" : "https://github.com/apache/spark/pull/27552#pullrequestreview-359489834",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "244743e4-2b19-4b24-9aac-df9bb8b104fc",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "I have to convert Java 8 classes to `java.sql.Timestamp`/`Date` for correct formatting. I cannot use `TimestampFormatter` here because the conversions to strings can be performed in `ColumnBuffer` in `hive-serde`.",
        "createdAt" : "2020-02-13T09:24:33Z",
        "updatedAt" : "2020-02-17T12:50:08Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "652f19c7-eebb-4eef-ac15-311391506674",
        "parentId" : "244743e4-2b19-4b24-9aac-df9bb8b104fc",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "can you add a code comment? Say that the results are passed to Hive and Hive only accepts  `Timestamp`/`Date`",
        "createdAt" : "2020-02-17T05:01:51Z",
        "updatedAt" : "2020-02-17T12:50:08Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "880b1dec95595b58688a3d5f7a2e0637a090cb47",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +182,186 @@        // Convert date-time instances to types that are acceptable by Hive libs\n        // used in conversions to strings.\n        val resultRow = row.map {\n          case i: Instant => Timestamp.from(i)\n          case ld: LocalDate => Date.valueOf(ld)"
  },
  {
    "id" : "65d166a7-90ef-4c44-bb24-afc84c274f44",
    "prId" : 27552,
    "prUrl" : "https://github.com/apache/spark/pull/27552#pullrequestreview-366155848",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1fa070e4-9e62-43c7-8f81-3a89cd1b5581",
        "parentId" : null,
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "There seems no java8 datetime values to be add to the `row` buffer here by `SparkExecuteStatementOperation#addNonNullColumnValue `\r\n\r\nhttps://github.com/apache/spark/pull/27552/files#diff-72dcd8f81a51c8a815159fdf0332acdcR84-R116",
        "createdAt" : "2020-02-28T03:38:18Z",
        "updatedAt" : "2020-02-28T03:38:33Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "24979ec9-e45a-44d0-827b-adcd46bd53a4",
        "parentId" : "1fa070e4-9e62-43c7-8f81-3a89cd1b5581",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "can you help fix it? I think we should output java8 datetime values if the config is enabled.",
        "createdAt" : "2020-02-28T03:53:02Z",
        "updatedAt" : "2020-02-28T03:53:03Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "55c361bf-3e71-405a-baa6-f460d0447781",
        "parentId" : "1fa070e4-9e62-43c7-8f81-3a89cd1b5581",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "We are limited by `hive-jdbc` module, see https://github.com/apache/hive/blob/a7e704c679a00db68db9b9f921d133d79a32cfcc/jdbc/src/java/org/apache/hive/jdbc/HiveBaseResultSet.java#L427-L457, we might need our own jdbc driver implementation to achieve this",
        "createdAt" : "2020-02-28T04:32:13Z",
        "updatedAt" : "2020-02-28T04:32:14Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "880b1dec95595b58688a3d5f7a2e0637a090cb47",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +183,187 @@        // used in conversions to strings.\n        val resultRow = row.map {\n          case i: Instant => Timestamp.from(i)\n          case ld: LocalDate => Date.valueOf(ld)\n          case other => other"
  },
  {
    "id" : "2f5cc61d-cb19-4f94-99f0-ce7a78720c6b",
    "prId" : 26357,
    "prUrl" : "https://github.com/apache/spark/pull/26357#pullrequestreview-310892043",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "be43733a-5a47-46cd-8833-75d5bd48d96c",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "this configuration seems only affecting thriftserver. it should have an effect in regular Spark session as well. But more importantly, why don't you just restart or start another one for debugging purpose? I don't think this configuration is worth much.",
        "createdAt" : "2019-11-04T05:59:07Z",
        "updatedAt" : "2019-11-04T05:59:08Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "47bf8bc6-790a-4676-adff-f606071db54e",
        "parentId" : "be43733a-5a47-46cd-8833-75d5bd48d96c",
        "authorId" : "cb93ab63-5789-483a-864c-9467d1506bf5",
        "body" : "We just want to change the log level of thriftserver when our application has already deployed. If it doesn't support dynamic log level, we should change the log4j file and restart the thrift server every time. It is not convenient.",
        "createdAt" : "2019-11-04T06:28:05Z",
        "updatedAt" : "2019-11-04T06:28:05Z",
        "lastEditedBy" : "cb93ab63-5789-483a-864c-9467d1506bf5",
        "tags" : [
        ]
      }
    ],
    "commit" : "a26dd2b38a4ca3a6c27732efa591a9e06fc4dc34",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +377,381 @@\n  private def prepareLogLevel(): Unit = {\n    val debugClasses = sqlContext.conf.debugClasses()\n    if (debugClasses == null || debugClasses == \"\") {\n      SparkUtils.restoreLogLevel()"
  },
  {
    "id" : "7af1bbbf-eb83-45be-aecb-5ecda8b5d63f",
    "prId" : 26141,
    "prUrl" : "https://github.com/apache/spark/pull/26141#pullrequestreview-397295255",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4994f32f-aadc-4ad2-8a64-914c93ac3a4e",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "do we need to set it back?",
        "createdAt" : "2020-01-06T05:29:38Z",
        "updatedAt" : "2020-04-29T05:08:09Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "6e74000e-bbfa-45f4-bf90-6507c7a634dc",
        "parentId" : "4994f32f-aadc-4ad2-8a64-914c93ac3a4e",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> do we need to set it back?\r\n\r\nNo need, in our env, just changed like this works fine. if we set it back after `execute()`, \r\nmay have problem when re-compute.\r\n\r\nThis class loader is not used in thrift server side code. \r\nin `hive` about module, use `executionHiveClassLoader ` is also reasonable.",
        "createdAt" : "2020-01-06T06:14:25Z",
        "updatedAt" : "2020-04-29T05:08:09Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "bd92a7f2-eb05-42ab-83f5-fecdb1f53b71",
        "parentId" : "4994f32f-aadc-4ad2-8a64-914c93ac3a4e",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "\"about module\"?",
        "createdAt" : "2020-04-21T11:30:30Z",
        "updatedAt" : "2020-04-29T05:08:09Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "93e26e7e-ef46-4aad-9c3a-0aaf1725ec68",
        "parentId" : "4994f32f-aadc-4ad2-8a64-914c93ac3a4e",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> \"about module\"?\r\n\r\nemmmm, module about hive..",
        "createdAt" : "2020-04-21T12:49:06Z",
        "updatedAt" : "2020-04-29T05:08:09Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "baa9f0602e0d5fed526cef59eb7936b676d869a6",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +277,281 @@      // Always set the session state classloader to `executionHiveClassLoader` even for sync mode\n      if (!runInBackground) {\n        parentSession.getSessionState.getConf.setClassLoader(executionHiveClassLoader)\n      }\n"
  },
  {
    "id" : "70ff59a7-b728-4b13-8664-a35eb528e49b",
    "prId" : 26141,
    "prUrl" : "https://github.com/apache/spark/pull/26141#pullrequestreview-397296849",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "04eda80d-7599-45c7-ac7e-d642906ce96e",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Even in sync mode, we don't use the classloader of the current thread?",
        "createdAt" : "2020-04-21T11:27:58Z",
        "updatedAt" : "2020-04-29T05:08:09Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "6e7b3cfe-98cd-4103-8cbd-19f4ff9665f7",
        "parentId" : "04eda80d-7599-45c7-ac7e-d642906ce96e",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> Even in sync mode, we don't use the classloader of the current thread?\r\n\r\nIn code I mentioned in description, it will use SessionState's HiveConf's ClassLoader.\r\nThat's the problem, when it execute in background thread, it will use HiveClientImpl 's SessionState, and the corresponding classLoader is `executionHiveClassLoader`",
        "createdAt" : "2020-04-21T12:51:06Z",
        "updatedAt" : "2020-04-29T05:08:09Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "baa9f0602e0d5fed526cef59eb7936b676d869a6",
    "line" : 2,
    "diffHunk" : "@@ -1,1 +273,277 @@      // Always use the latest class loader provided by executionHive's state.\n      val executionHiveClassLoader = sqlContext.sharedState.jarClassLoader\n      Thread.currentThread().setContextClassLoader(executionHiveClassLoader)\n\n      // Always set the session state classloader to `executionHiveClassLoader` even for sync mode"
  },
  {
    "id" : "bd89106c-31cd-40c6-b04e-a0081ef666ce",
    "prId" : 26141,
    "prUrl" : "https://github.com/apache/spark/pull/26141#pullrequestreview-399674043",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bb303f51-68f5-4678-bf10-61419f1a6df7",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Can we add a comment?\r\n```\r\n// Always set the session state classloader to `executionHiveClassLoader` even for sync mode\r\n```",
        "createdAt" : "2020-04-24T06:34:04Z",
        "updatedAt" : "2020-04-29T05:08:09Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "baa9f0602e0d5fed526cef59eb7936b676d869a6",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +276,280 @@\n      // Always set the session state classloader to `executionHiveClassLoader` even for sync mode\n      if (!runInBackground) {\n        parentSession.getSessionState.getConf.setClassLoader(executionHiveClassLoader)\n      }"
  },
  {
    "id" : "d60eecac-24e8-4fc3-8f39-d4d379861b28",
    "prId" : 25743,
    "prUrl" : "https://github.com/apache/spark/pull/25743#pullrequestreview-287428703",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "31ab346a-eaeb-474e-9408-0a89acff9629",
        "parentId" : null,
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "Could we add a comment explaining why we need this change?",
        "createdAt" : "2019-09-12T13:29:41Z",
        "updatedAt" : "2019-09-16T09:17:10Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      }
    ],
    "commit" : "232ef816de0dba7690ff9959800ba94ec8f24a5e",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +272,276 @@        // task interrupted, it may have start some spark job, so we need to cancel again to\n        // make sure job was cancelled when background thread was interrupted\n        if (statementId != null) {\n          sqlContext.sparkContext.cancelJobGroup(statementId)\n        }"
  },
  {
    "id" : "8fab1435-9ca8-4502-adcc-e7fa1dd181b3",
    "prId" : 25611,
    "prUrl" : "https://github.com/apache/spark/pull/25611#pullrequestreview-282766054",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e71d3299-b1af-4196-a87c-b5d1147240d7",
        "parentId" : null,
        "authorId" : "e5beb795-ac0e-4bd6-ad66-708215b8ae58",
        "body" : "ocd nit: `if (getStatus.getState.isTerminal)` would make it consistent with other places in the file, and adding the `val currentState` now is not needed, as it's accessed only once anyway.",
        "createdAt" : "2019-09-02T12:20:58Z",
        "updatedAt" : "2019-09-03T22:46:22Z",
        "lastEditedBy" : "e5beb795-ac0e-4bd6-ad66-708215b8ae58",
        "tags" : [
        ]
      },
      {
        "id" : "da80bbb7-42cc-4f43-8799-3f195343eb69",
        "parentId" : "e71d3299-b1af-4196-a87c-b5d1147240d7",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> ocd nit: `if (getStatus.getState.isTerminal)` would make it consistent with other places in the file, and adding the `val currentState` now is not needed, as it's accessed only once anyway.\r\n\r\nWe should show currentState in \r\n![image](https://user-images.githubusercontent.com/46485123/64146210-a5721980-ce4e-11e9-8209-f8bd62013baf.png)\r\n",
        "createdAt" : "2019-09-03T05:28:09Z",
        "updatedAt" : "2019-09-03T22:46:22Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "536756be89b77d37219b97bbf4358722a214f76d",
    "line" : 90,
    "diffHunk" : "@@ -1,1 +266,270 @@      case e: Throwable =>\n        val currentState = getStatus().getState()\n        if (currentState.isTerminal) {\n          // This may happen if the execution was cancelled, and then closed from another thread.\n          logWarning(s\"Ignore exception in terminal state with $statementId: $e\")"
  },
  {
    "id" : "a2c475e6-e85d-4d49-9313-ecf0e953b008",
    "prId" : 25611,
    "prUrl" : "https://github.com/apache/spark/pull/25611#pullrequestreview-282995828",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9a939c40-86fb-43e8-a8c7-cdfb84e4827b",
        "parentId" : null,
        "authorId" : "e5beb795-ac0e-4bd6-ad66-708215b8ae58",
        "body" : "Could you add `logInfo(s\"Submitting query '$statement' with $statementId\")` here?",
        "createdAt" : "2019-09-02T13:45:58Z",
        "updatedAt" : "2019-09-03T22:46:22Z",
        "lastEditedBy" : "e5beb795-ac0e-4bd6-ad66-708215b8ae58",
        "tags" : [
        ]
      },
      {
        "id" : "e18c6b5d-72d5-452d-b432-756d56ffe82f",
        "parentId" : "9a939c40-86fb-43e8-a8c7-cdfb84e4827b",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> Could you add `logInfo(s\"Submitting query '$statement' with $statementId\")` here?\r\n\r\ndone",
        "createdAt" : "2019-09-03T10:43:09Z",
        "updatedAt" : "2019-09-03T22:46:22Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "4bbd88a7-808e-4a78-acd8-aa8b4efbdff0",
        "parentId" : "9a939c40-86fb-43e8-a8c7-cdfb84e4827b",
        "authorId" : "e5beb795-ac0e-4bd6-ad66-708215b8ae58",
        "body" : "ocd formatting nit: move `setHasResultSet(true)` below statementid, logInfo and onStatementStart, to do all the initialization bookkeeping first.",
        "createdAt" : "2019-09-03T13:31:31Z",
        "updatedAt" : "2019-09-03T22:46:22Z",
        "lastEditedBy" : "e5beb795-ac0e-4bd6-ad66-708215b8ae58",
        "tags" : [
        ]
      }
    ],
    "commit" : "536756be89b77d37219b97bbf4358722a214f76d",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +161,165 @@    statementId = UUID.randomUUID().toString\n    logInfo(s\"Submitting query '$statement' with $statementId\")\n    HiveThriftServer2.listener.onStatementStart(\n      statementId,\n      parentSession.getSessionHandle.getSessionId.toString,"
  },
  {
    "id" : "6b8d0913-d93e-44f3-94b0-64d1c744e723",
    "prId" : 25611,
    "prUrl" : "https://github.com/apache/spark/pull/25611#pullrequestreview-282765026",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5fc8e1f6-225c-45c1-b764-7a435cd41788",
        "parentId" : null,
        "authorId" : "e5beb795-ac0e-4bd6-ad66-708215b8ae58",
        "body" : "nit: I think this could become a `finally { synchronized {` block; the if check will make sure that it doesn't go to finished after another state.",
        "createdAt" : "2019-09-02T14:19:53Z",
        "updatedAt" : "2019-09-03T22:46:22Z",
        "lastEditedBy" : "e5beb795-ac0e-4bd6-ad66-708215b8ae58",
        "tags" : [
        ]
      },
      {
        "id" : "ca92d4b1-c2d2-40f8-990d-4454e6af4cc4",
        "parentId" : "5fc8e1f6-225c-45c1-b764-7a435cd41788",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> nit: I think this could become a `finally { synchronized {` block; the if check will make sure that it doesn't go to finished after another state.\r\n\r\nReasonable, I add too much control.",
        "createdAt" : "2019-09-03T05:22:49Z",
        "updatedAt" : "2019-09-03T22:46:22Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "536756be89b77d37219b97bbf4358722a214f76d",
    "line" : 120,
    "diffHunk" : "@@ -1,1 +286,290 @@          HiveThriftServer2.listener.onStatementFinish(statementId)\n        }\n      }\n      sqlContext.sparkContext.clearJobGroup()\n    }"
  },
  {
    "id" : "53f2badc-460e-433b-b52d-19a574db7a29",
    "prId" : 25611,
    "prUrl" : "https://github.com/apache/spark/pull/25611#pullrequestreview-282903041",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d0563ea0-afdb-4201-9044-28f62070b167",
        "parentId" : null,
        "authorId" : "e5beb795-ac0e-4bd6-ad66-708215b8ae58",
        "body" : "collateral fix: please move `sqlContext.sparkContext.clearJobGroup()` from close() to this finally block (outside synchronized) - setJobGroup is called from this thread, so clearJobGroup should also be called from here.",
        "createdAt" : "2019-09-03T09:49:44Z",
        "updatedAt" : "2019-09-03T22:46:22Z",
        "lastEditedBy" : "e5beb795-ac0e-4bd6-ad66-708215b8ae58",
        "tags" : [
        ]
      },
      {
        "id" : "375fa604-7c9b-4f59-881d-885a92a734d9",
        "parentId" : "d0563ea0-afdb-4201-9044-28f62070b167",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> collateral fix: please move `sqlContext.sparkContext.clearJobGroup()` from close() to this finally block (outside synchronized) - setJobGroup is called from this thread, so clearJobGroup should also be called from here.\r\n\r\nMiss this point , fix it.",
        "createdAt" : "2019-09-03T10:30:31Z",
        "updatedAt" : "2019-09-03T22:46:22Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "536756be89b77d37219b97bbf4358722a214f76d",
    "line" : 115,
    "diffHunk" : "@@ -1,1 +281,285 @@        }\n    } finally {\n      synchronized {\n        if (!getStatus.getState.isTerminal) {\n          setState(OperationState.FINISHED)"
  },
  {
    "id" : "db560d1b-b1e1-46d5-a356-bf9d98c5ed50",
    "prId" : 25611,
    "prUrl" : "https://github.com/apache/spark/pull/25611#pullrequestreview-285152595",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1a08530d-9a47-4071-8c3d-8d16bb165b98",
        "parentId" : null,
        "authorId" : "e5beb795-ac0e-4bd6-ad66-708215b8ae58",
        "body" : "@AngersZhuuuu \r\nI think I found one more problem:\r\nIf `cancel()` and `close()` is called very quickly after the query is started, then they may both call `cleanup()` before Spark Jobs are started. Then `sqlContext.sparkContext.cancelJobGroup(statementId)` does nothing.\r\nBut then the `execute` thread can start the jobs, and only then get interrupted and exit through here. But then it will exit here, and no-one will cancel these jobs and they will keep running even though this execution has exited.\r\n\r\nI think it can be fixed by:\r\n```\r\n      case e: Throwable =>\r\n        // In any case, cancel any remaining running jobs.\r\n        // E.g. a cancel() operation could have called cleanup() which canceled the Jobs before\r\n        // they started.\r\n        if (statementId != null) {\r\n          sqlContext.sparkContext.cancelJobGroup(statementId)\r\n        }\r\n```",
        "createdAt" : "2019-09-06T21:43:37Z",
        "updatedAt" : "2019-09-06T21:43:38Z",
        "lastEditedBy" : "e5beb795-ac0e-4bd6-ad66-708215b8ae58",
        "tags" : [
        ]
      },
      {
        "id" : "d4782a05-955e-42af-b4d7-51479f932f5a",
        "parentId" : "1a08530d-9a47-4071-8c3d-8d16bb165b98",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "Got you point. \r\nWhen cleanup().\r\nSparkContext haven't setup jobGroup.\r\nBut execute thread start execute and setup jobGroup.\r\n`cleanup()` can cancel background thread task but can't promise `cancelJobGroup` since it may be called before `sparkContext setupJobGroup` \r\n\r\nBut  cancelJobGroup here seem can't stop `execute()` method run.\r\n",
        "createdAt" : "2019-09-07T02:42:04Z",
        "updatedAt" : "2019-09-07T02:42:04Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "536756be89b77d37219b97bbf4358722a214f76d",
    "line" : 88,
    "diffHunk" : "@@ -1,1 +264,268 @@      // Actually do need to catch Throwable as some failures don't inherit from Exception and\n      // HiveServer will silently swallow them.\n      case e: Throwable =>\n        val currentState = getStatus().getState()\n        if (currentState.isTerminal) {"
  },
  {
    "id" : "3424bc03-4f94-4e91-aa80-e5b873c1b4a6",
    "prId" : 25611,
    "prUrl" : "https://github.com/apache/spark/pull/25611#pullrequestreview-285152569",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f57312ca-bd7a-4d6a-a82f-be0b56567105",
        "parentId" : null,
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "@juliuszsompolski  call cancel here for run statement in sync mode. seems can't stop task running .",
        "createdAt" : "2019-09-07T02:40:56Z",
        "updatedAt" : "2019-09-07T02:40:56Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "536756be89b77d37219b97bbf4358722a214f76d",
    "line" : 74,
    "diffHunk" : "@@ -1,1 +239,243 @@      val executionHiveClassLoader = sqlContext.sharedState.jarClassLoader\n      Thread.currentThread().setContextClassLoader(executionHiveClassLoader)\n\n      sqlContext.sparkContext.setJobGroup(statementId, statement)\n      result = sqlContext.sql(statement)"
  },
  {
    "id" : "4c096e66-dc6c-4682-9a4a-97ccbb064e02",
    "prId" : 25611,
    "prUrl" : "https://github.com/apache/spark/pull/25611#pullrequestreview-285605804",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a1748b8f-03a4-488c-bc39-cfbdf6f04bbd",
        "parentId" : null,
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "How about move judgement to this place .  I think `setState(RUNNING)` after `setJobGroup` is ok.\r\nSince we  can think setClassLoader and setJobGroup is prepare work  for running\r\n```\r\nsqlContext.sparkContext.setJobGroup(statementId, statement)\r\n  synchronized {\r\n        if (getStatus.getState.isTerminal) {\r\n          logInfo(s\"Query with $statementId in terminal state before it started running\")\r\n          return\r\n        } else {\r\n          logInfo(s\"Running query with $statementId\")\r\n          setState(OperationState.RUNNING)\r\n        }\r\n      }\r\n```",
        "createdAt" : "2019-09-07T02:45:26Z",
        "updatedAt" : "2019-09-07T02:45:26Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "02e5c317-b70f-4209-83de-da1fce1e6e42",
        "parentId" : "a1748b8f-03a4-488c-bc39-cfbdf6f04bbd",
        "authorId" : "e5beb795-ac0e-4bd6-ad66-708215b8ae58",
        "body" : "@AngersZhuuuu \r\nI don't think it will help. It's not just about setJobGroup, but about actually starting the job. You can setJobGroup, then cancelJobGroup, and then start running Jobs in that Job Group.\r\nWhat cancelJobGroup does is only cancel Jobs that are currently running in the Job group, it doesn't prevent further jobs being started. So I think it can still go past here, and then be cancelled before it actually starts the Jobs.\r\nI think it would be safer to handle it from the catch block.",
        "createdAt" : "2019-09-09T14:45:23Z",
        "updatedAt" : "2019-09-09T14:45:23Z",
        "lastEditedBy" : "e5beb795-ac0e-4bd6-ad66-708215b8ae58",
        "tags" : [
        ]
      },
      {
        "id" : "84a356a8-ea8c-463e-8644-032b315e9224",
        "parentId" : "a1748b8f-03a4-488c-bc39-cfbdf6f04bbd",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "But when\r\n`You can setJobGroup, then cancelJobGroup, and then start running Jobs in that Job Group.` \r\nThe job is under no JobGroup since when call cancelJobGroup, jobGroup in localProperties has been cleared",
        "createdAt" : "2019-09-09T14:58:01Z",
        "updatedAt" : "2019-09-09T14:58:02Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "9175bc0d-1d6d-4e88-80a2-2c8f2bad93f0",
        "parentId" : "a1748b8f-03a4-488c-bc39-cfbdf6f04bbd",
        "authorId" : "e5beb795-ac0e-4bd6-ad66-708215b8ae58",
        "body" : "Let's have\r\n1. Thread1 be in `execute()` after this synchronized block, but before the Jobs have started\r\n2. Thread2 be a user connection calling `cancel()`. It cancels all jobs in the job group, and notifies Thread1\r\n3. But before Thread1 gets the notification and throws InterruptedException, it starts some Jobs.\r\n4. Then Thread1 gets InterruptedException, and exits through the catch and finally block. It does a clearJobGroup in the finally, but that doesn't cancel the Jobs started in 3. These Jobs keep running after Thread1 exits, and nobody cancels them.\r\n\r\nI think that the only way to prevent this, is to call another cancelJobGroup from the catch block when an exception comes.",
        "createdAt" : "2019-09-09T15:24:07Z",
        "updatedAt" : "2019-09-09T15:24:08Z",
        "lastEditedBy" : "e5beb795-ac0e-4bd6-ad66-708215b8ae58",
        "tags" : [
        ]
      },
      {
        "id" : "6e784fa1-bd86-48f2-80d3-91bacb337883",
        "parentId" : "a1748b8f-03a4-488c-bc39-cfbdf6f04bbd",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "Get your point.\r\n`Cancel ` before  `setJobGroup`, but there will be an interval between `cancel` and execute thread been interrupted.  Then job won't be canceled after execute thread get into catch block.",
        "createdAt" : "2019-09-09T15:47:11Z",
        "updatedAt" : "2019-09-09T15:47:11Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "536756be89b77d37219b97bbf4358722a214f76d",
    "line" : 75,
    "diffHunk" : "@@ -1,1 +240,244 @@      Thread.currentThread().setContextClassLoader(executionHiveClassLoader)\n\n      sqlContext.sparkContext.setJobGroup(statementId, statement)\n      result = sqlContext.sql(statement)\n      logDebug(result.queryExecution.toString())"
  }
]