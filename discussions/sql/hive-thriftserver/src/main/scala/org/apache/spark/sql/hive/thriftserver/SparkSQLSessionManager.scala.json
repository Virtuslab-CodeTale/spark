[
  {
    "id" : "64c206c1-9587-4a81-96da-259958c6ab63",
    "prId" : 30536,
    "prUrl" : "https://github.com/apache/spark/pull/30536#pullrequestreview-540558598",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e361c148-850c-4ef7-af0b-f203c7da44ca",
        "parentId" : null,
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "The logic is the same: https://github.com/apache/spark/blob/008a2ad1f836ff04fafd51a9c94c355ef35f1692/sql/hive-thriftserver/src/main/java/org/apache/hive/service/cli/session/SessionManager.java#L273-L282",
        "createdAt" : "2020-11-30T02:06:08Z",
        "updatedAt" : "2020-11-30T02:07:17Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      }
    ],
    "commit" : "090dad4e57b73b7c56cd746b48a1bf930ab6a4fb",
    "line" : 61,
    "diffHunk" : "@@ -1,1 +81,85 @@            logWarning(\"Error closing session\", t)\n        }\n        throw new HiveSQLException(\"Failed to open new session: \" + e, e)\n    }\n  }"
  },
  {
    "id" : "b8b160c8-978d-48fd-84ed-b5a3afd4a020",
    "prId" : 30536,
    "prUrl" : "https://github.com/apache/spark/pull/30536#pullrequestreview-550699166",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c5e8fa12-ea0d-416f-9976-071d45a95723",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "NonFatal ?",
        "createdAt" : "2020-12-12T07:36:53Z",
        "updatedAt" : "2020-12-12T07:36:53Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      }
    ],
    "commit" : "090dad4e57b73b7c56cd746b48a1bf930ab6a4fb",
    "line" : 58,
    "diffHunk" : "@@ -1,1 +78,82 @@          closeSession(sessionHandle)\n        } catch {\n          case t: Throwable =>\n            logWarning(\"Error closing session\", t)\n        }"
  },
  {
    "id" : "33659a59-6601-4521-8f20-1ba7c452180b",
    "prId" : 30536,
    "prUrl" : "https://github.com/apache/spark/pull/30536#pullrequestreview-550700821",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fe523cc3-a3d8-4917-bcfc-8f9ad9bcfc51",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "NonFatal ?",
        "createdAt" : "2020-12-12T07:38:16Z",
        "updatedAt" : "2020-12-12T07:38:16Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "51efc6a8-772c-4cb3-a2ab-48b23c6550e2",
        "parentId" : "fe523cc3-a3d8-4917-bcfc-8f9ad9bcfc51",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "https://github.com/apache/spark/pull/30744",
        "createdAt" : "2020-12-12T08:09:24Z",
        "updatedAt" : "2020-12-12T08:09:25Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      }
    ],
    "commit" : "090dad4e57b73b7c56cd746b48a1bf930ab6a4fb",
    "line" : 54,
    "diffHunk" : "@@ -1,1 +74,78 @@      sessionHandle\n    } catch {\n      case e: Exception =>\n        try {\n          closeSession(sessionHandle)"
  },
  {
    "id" : "6ad0c5cf-2938-4b78-ab44-9e4f19c4ae5d",
    "prId" : 29656,
    "prUrl" : "https://github.com/apache/spark/pull/29656#pullrequestreview-484196840",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ed147027-633e-4521-a690-04a295a658bd",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "just to be clear, is this just a small refactoring?",
        "createdAt" : "2020-09-07T06:16:21Z",
        "updatedAt" : "2020-09-07T06:16:22Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "02a11b84-c985-477e-8f81-566245692c5d",
        "parentId" : "ed147027-633e-4521-a690-04a295a658bd",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> just to be clear, is this just a small refactoring?\r\n\r\nIn this way we can avoid a lot of process part, but seems I have miss a important problem that user may write a db name wrong such as `aa.cc`",
        "createdAt" : "2020-09-07T06:33:34Z",
        "updatedAt" : "2020-09-07T06:33:34Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "d85ed65d-4141-4020-86f1-70d125e66a9a",
        "parentId" : "ed147027-633e-4521-a690-04a295a658bd",
        "authorId" : "2821208d-43c5-4475-94ad-36cbf84c14d8",
        "body" : "Can we add extra tests to verify the behaviour in case of failures? Is it the same error thrown? Are we skipping some checks along the way by calling the catalog functions?  ",
        "createdAt" : "2020-09-08T14:27:29Z",
        "updatedAt" : "2020-09-08T14:27:29Z",
        "lastEditedBy" : "2821208d-43c5-4475-94ad-36cbf84c14d8",
        "tags" : [
        ]
      }
    ],
    "commit" : "1f2fa6cfc6dd841bda5c82fa8f428a54e1dfa8a5",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +70,74 @@    setConfMap(ctx, hiveSessionState.getHiveVariables)\n    if (sessionConf != null && sessionConf.containsKey(\"use:database\")) {\n      ctx.sessionState.catalog.setCurrentDatabase(sessionConf.get(\"use:database\"))\n    }\n    sparkSqlOperationManager.sessionToContexts.put(sessionHandle, ctx)"
  },
  {
    "id" : "e3fc660f-1ac4-432a-a74b-6060343db823",
    "prId" : 26543,
    "prUrl" : "https://github.com/apache/spark/pull/26543#pullrequestreview-319866272",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "18cfce19-8b1d-4985-9dff-d1a2cd24b119",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Oh, I meant call uncacheTable inside clearTempTables. Is it possible?",
        "createdAt" : "2019-11-15T14:10:56Z",
        "updatedAt" : "2019-11-19T01:53:39Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "73da0256-4ee1-4a30-8bb3-122ed5e60691",
        "parentId" : "18cfce19-8b1d-4985-9dff-d1a2cd24b119",
        "authorId" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "body" : "I think it can not call `uncacheTable ` inside `clearTempTables`. There is no sqlContext reference in `SessionCatalog`",
        "createdAt" : "2019-11-17T14:04:50Z",
        "updatedAt" : "2019-11-19T01:53:39Z",
        "lastEditedBy" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "tags" : [
        ]
      },
      {
        "id" : "4eb7a939-c869-4670-b39a-cdb0e80b685d",
        "parentId" : "18cfce19-8b1d-4985-9dff-d1a2cd24b119",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Ah right. The `clearTempTables()` call doesn't really matter then. Maybe revert that part just to keep the change simple.\r\n\r\nIt seems like a good thing to fix and this is straightforward, I just keep wondering if there's a slightly better place to manage it, or whether there are more cases like this that need to be addressed. Eh, @cloud-fan do you have an opinion or is this a fine way to do it?",
        "createdAt" : "2019-11-17T14:40:38Z",
        "updatedAt" : "2019-11-19T01:53:39Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "b33138bc-625e-4af4-b22a-b9ebc8f23cdb",
        "parentId" : "18cfce19-8b1d-4985-9dff-d1a2cd24b119",
        "authorId" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "body" : "reverted",
        "createdAt" : "2019-11-17T14:52:09Z",
        "updatedAt" : "2019-11-19T01:53:39Z",
        "lastEditedBy" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "tags" : [
        ]
      },
      {
        "id" : "0cdf0398-e758-49d5-9678-163986f8d0b9",
        "parentId" : "18cfce19-8b1d-4985-9dff-d1a2cd24b119",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "shall we do it in `SparkSesson.stop`? The code path here seems thrift server specific.",
        "createdAt" : "2019-11-18T08:17:38Z",
        "updatedAt" : "2019-11-19T01:53:39Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "fea72c3a-7931-4849-a1a1-429ef6d8846a",
        "parentId" : "18cfce19-8b1d-4985-9dff-d1a2cd24b119",
        "authorId" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "body" : "`SparkSesson.stop` will stop underlying `SparkContext`. I don't think it's the right place.\r\nhttps://github.com/apache/spark/blob/882f54b0a323fb5cd827d600b3c3332e1fcdf65a/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala#L672",
        "createdAt" : "2019-11-19T01:34:22Z",
        "updatedAt" : "2019-11-19T01:53:39Z",
        "lastEditedBy" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "tags" : [
        ]
      },
      {
        "id" : "3d13b90d-fbaf-49e1-993e-f608d498c830",
        "parentId" : "18cfce19-8b1d-4985-9dff-d1a2cd24b119",
        "authorId" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "body" : "What's more, SQL session close in thrift server doesn't reach `SparkSesson.stop`.",
        "createdAt" : "2019-11-19T01:49:59Z",
        "updatedAt" : "2019-11-19T01:53:39Z",
        "lastEditedBy" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "tags" : [
        ]
      },
      {
        "id" : "08b9b739-7057-491a-915c-cad81f508474",
        "parentId" : "18cfce19-8b1d-4985-9dff-d1a2cd24b119",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "hmm the behavior is really weird. I think we should track all the active sessions and only stop the spark context if the session being closed is the last one.",
        "createdAt" : "2019-11-19T07:00:27Z",
        "updatedAt" : "2019-11-19T07:00:27Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "a296aa66-e048-4130-b613-0f492a85d3ae",
        "parentId" : "18cfce19-8b1d-4985-9dff-d1a2cd24b119",
        "authorId" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "body" : "I can work that in another ticket. In current `SparkSQLSessionManager` implementation, each hive session connection will create a new SparkSession and store in the map `SparkSQLOperationManager.sessionToContexts`. When a session close, this session will be removed from this map and GC later. `SparkSesson.stop` won't be invoked at all.",
        "createdAt" : "2019-11-19T08:35:45Z",
        "updatedAt" : "2019-11-19T08:37:05Z",
        "lastEditedBy" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "tags" : [
        ]
      },
      {
        "id" : "3698aa3c-6404-451e-bfcd-fb495cddf7e7",
        "parentId" : "18cfce19-8b1d-4985-9dff-d1a2cd24b119",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "@cloud-fan is that a somewhat different point? I'm not an expert here. Seems like this is about unpersisting cached temp views in the session when the session is done, not stopping Spark when all active sessions are closed. As in, is it valid to re-start a new session in the same context?",
        "createdAt" : "2019-11-19T15:03:10Z",
        "updatedAt" : "2019-11-19T15:03:11Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "ad34770c-6c7b-4f83-9207-2e85c2fde5e4",
        "parentId" : "18cfce19-8b1d-4985-9dff-d1a2cd24b119",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Ideally we should uncache the temp views when the session is closed, but seems we don't have such an action called \"close session\" in Spark SQL. `SparkSession.close` closes the SparkContext and thus closes all sessions.\r\n\r\nIf we have \"close session\" action in thrift server, I think it's corrected to put the logic here.",
        "createdAt" : "2019-11-20T08:02:13Z",
        "updatedAt" : "2019-11-20T08:02:13Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "a30e83fa-9c4a-46a3-83db-17c99441b33e",
        "parentId" : "18cfce19-8b1d-4985-9dff-d1a2cd24b119",
        "authorId" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "body" : "Thanks @cloud-fan and @srowen . So the patch is correct so far, right?",
        "createdAt" : "2019-11-20T13:56:41Z",
        "updatedAt" : "2019-11-20T13:56:41Z",
        "lastEditedBy" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "tags" : [
        ]
      },
      {
        "id" : "298bde47-f3c8-454b-bdb3-35ad347d5869",
        "parentId" : "18cfce19-8b1d-4985-9dff-d1a2cd24b119",
        "authorId" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "body" : "Since there is no place to trace all live spark sessions in a non-thrift-server application. For example spark session can be stored in thread local variable and be garbage collection when user's thread exit. So it still has memory leak problem in an application. I think I can open a new ticket to manage/trace all created spark session. For now, could we accept this due to thrift server is official implemented multiple session application.",
        "createdAt" : "2019-11-20T14:06:01Z",
        "updatedAt" : "2019-11-20T14:06:02Z",
        "lastEditedBy" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "tags" : [
        ]
      },
      {
        "id" : "673a608a-2b06-4eea-a73c-a09d308bb370",
        "parentId" : "18cfce19-8b1d-4985-9dff-d1a2cd24b119",
        "authorId" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "body" : "https://issues.apache.org/jira/browse/SPARK-29972",
        "createdAt" : "2019-11-20T14:10:56Z",
        "updatedAt" : "2019-11-20T14:10:57Z",
        "lastEditedBy" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "tags" : [
        ]
      }
    ],
    "commit" : "fa3e0dc2f509b35a69df99d8be7b539fe34359dd",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +77,81 @@    HiveThriftServer2.listener.onSessionClosed(sessionHandle.getSessionId.toString)\n    val ctx = sparkSqlOperationManager.sessionToContexts.getOrDefault(sessionHandle, sqlContext)\n    ctx.sparkSession.sessionState.catalog.getTempViewNames().foreach(ctx.uncacheTable)\n    super.closeSession(sessionHandle)\n    sparkSqlOperationManager.sessionToActivePool.remove(sessionHandle)"
  },
  {
    "id" : "0f1179b4-0948-453d-a91c-135aeb8f5709",
    "prId" : 25201,
    "prUrl" : "https://github.com/apache/spark/pull/25201#pullrequestreview-362361792",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "393e699a-fc55-4c36-888a-68f6676b7678",
        "parentId" : null,
        "authorId" : "f6c00e82-9133-4e97-854c-9790ac284535",
        "body" : "Hi @AngersZhuuuu, while applying your patch I found a bug here, `session.getSessionHandle` will have an exception because `session` is `null`.\r\n\r\nThen I moved `session = HiveSessionProxy.getProxy(sessionWithUGI, sessionWithUGI.getSessionUgi)` to before the if statement will solve this issue.",
        "createdAt" : "2020-02-20T23:00:11Z",
        "updatedAt" : "2020-02-20T23:00:11Z",
        "lastEditedBy" : "f6c00e82-9133-4e97-854c-9790ac284535",
        "tags" : [
        ]
      },
      {
        "id" : "88ed12fc-7ac7-403a-8dd5-4623ff39bd54",
        "parentId" : "393e699a-fc55-4c36-888a-68f6676b7678",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> Hi @AngersZhuuuu, while applying your patch I found a bug here, `session.getSessionHandle` will have an exception because `session` is `null`.\r\n> \r\n> Then I moved `session = HiveSessionProxy.getProxy(sessionWithUGI, sessionWithUGI.getSessionUgi)` to before the if statement will solve this issue.\r\n\r\nyeaï¼Œ some mistake, here is just a way to implement this. If you have interesting in this way , you can see this \r\nhttps://github.com/spark-thriftserver/spark-thriftserver/pull/53",
        "createdAt" : "2020-02-21T02:13:55Z",
        "updatedAt" : "2020-02-21T02:13:56Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "424dca304aec99c7c1012197e5369c54b8700a01",
    "line" : 81,
    "diffHunk" : "@@ -1,1 +87,91 @@          existing.mergeAll(originalCreds)\n          ugi.addCredentials(existing)\n          sparkSqlOperationManager.sessionToTokens.put(session.getSessionHandle, tokens)\n        } catch {\n          case e: Exception =>"
  },
  {
    "id" : "aac2a014-f989-4b79-a781-aa715aa8279f",
    "prId" : 25179,
    "prUrl" : "https://github.com/apache/spark/pull/25179#pullrequestreview-263010652",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d08f02b9-dd6c-43c1-be00-c679fa081ddf",
        "parentId" : null,
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "Should consider more about token expire.",
        "createdAt" : "2019-07-17T13:04:13Z",
        "updatedAt" : "2019-07-17T14:40:24Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "5ecb8a761bfbe87cfc67934d773a33cc3a172580",
    "line" : 41,
    "diffHunk" : "@@ -1,1 +62,66 @@      val originalCreds = ugi.getCredentials\n      val creds = new Credentials()\n      ThriftServerHadoopUtil.doAs(ugi)(() => hadoopTokenProvider.obtainDelegationTokens(creds, username))\n      ugi.addCredentials(creds)\n      val existing = ugi.getCredentials()"
  },
  {
    "id" : "22d078f1-9486-45dc-a605-27e3adac53d5",
    "prId" : 24659,
    "prUrl" : "https://github.com/apache/spark/pull/24659#pullrequestreview-243917162",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ed55510a-b80d-48bb-9403-be09f4208aa8",
        "parentId" : null,
        "authorId" : "e5beb795-ac0e-4bd6-ad66-708215b8ae58",
        "body" : "I think setting it in a thread local in open session is not enough, as I think that each request in the session can be handled by different threads of the connection threadpool.",
        "createdAt" : "2019-05-30T17:15:12Z",
        "updatedAt" : "2019-05-30T17:15:13Z",
        "lastEditedBy" : "e5beb795-ac0e-4bd6-ad66-708215b8ae58",
        "tags" : [
        ]
      }
    ],
    "commit" : "5c31fae26a49e024b3932c5ae1b7747b19ba7644",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +62,66 @@    }\n    sqlContext.sparkContext.setLocalProperty(\n      SQLContext.SESSION_ID_KEY, sessionHandle.getSessionId.toString)\n    ctx.setConf(HiveUtils.FAKE_HIVE_VERSION.key, HiveUtils.builtinHiveVersion)\n    if (sessionConf != null && sessionConf.containsKey(\"use:database\")) {"
  }
]