[
  {
    "id" : "bdeeeb58-7561-4b48-b26b-63554bbfc41c",
    "prId" : 32779,
    "prUrl" : "https://github.com/apache/spark/pull/32779#pullrequestreview-676697625",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e581f834-eaef-4376-b756-cc5592d06393",
        "parentId" : null,
        "authorId" : "43998e22-6c2f-401d-9914-8cecf6fad929",
        "body" : "Should pin the version?",
        "createdAt" : "2021-06-04T22:12:46Z",
        "updatedAt" : "2021-06-04T22:20:03Z",
        "lastEditedBy" : "43998e22-6c2f-401d-9914-8cecf6fad929",
        "tags" : [
        ]
      }
    ],
    "commit" : "08bd5aa1e60d313d3d16929b5fd8a4f7f24fb058",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +35,39 @@\n# pandas API on Spark Code formatter.\nblack"
  },
  {
    "id" : "3624e5bd-8d1a-47b7-adf9-acf685440a17",
    "prId" : 29491,
    "prUrl" : "https://github.com/apache/spark/pull/29491#pullrequestreview-472915523",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5af9c72f-4214-42d2-b15e-c3a900689d7e",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Are there other implications to this, like, our python packaging now requires ipython when installing pyspark? (sorry ignorant question)",
        "createdAt" : "2020-08-20T15:39:01Z",
        "updatedAt" : "2020-08-25T07:51:55Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "f37decfe-3d58-486c-bd53-728b3d017720",
        "parentId" : "5af9c72f-4214-42d2-b15e-c3a900689d7e",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Nope, it means nothing special and this file is used nowhere within Spark. It's just for some dev people but I doubt if this is actually often used. We should leverage this file and standardize the dependencies somehow like @nchammas and @dongjoon-hyun tried before but .. I currently don't have a good idea about how to handle it.",
        "createdAt" : "2020-08-21T04:31:56Z",
        "updatedAt" : "2020-08-25T07:51:55Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "15aac718-f7cd-413e-b0ba-dd46c5b29cb5",
        "parentId" : "5af9c72f-4214-42d2-b15e-c3a900689d7e",
        "authorId" : "e29a7794-ac2e-4e6f-a690-737e83e1bace",
        "body" : "The packages that are being installed, when installing pyspark are defined in the setup.py: https://github.com/apache/spark/blob/master/python/setup.py#L206\r\n\r\nThe defacto way is to add an `extras_require` called `dev` where you add the development requirements. Preferably with a version attached to it, so you know that Jenkins and the local dev environments are the same.\r\n\r\nHappy to migrate this to the setup.py if you like.",
        "createdAt" : "2020-08-22T09:46:17Z",
        "updatedAt" : "2020-08-25T07:51:55Z",
        "lastEditedBy" : "e29a7794-ac2e-4e6f-a690-737e83e1bace",
        "tags" : [
        ]
      }
    ],
    "commit" : "0e114b6ac47cde46467d18dce2ca8e1ac0650a35",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +5,9 @@sphinx\npydata_sphinx_theme\nipython\nnbsphinx"
  }
]