[
  {
    "id" : "bdeeeb58-7561-4b48-b26b-63554bbfc41c",
    "prId" : 32779,
    "prUrl" : "https://github.com/apache/spark/pull/32779#pullrequestreview-676697625",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e581f834-eaef-4376-b756-cc5592d06393",
        "parentId" : null,
        "authorId" : "43998e22-6c2f-401d-9914-8cecf6fad929",
        "body" : "Should pin the version?",
        "createdAt" : "2021-06-04T22:12:46Z",
        "updatedAt" : "2021-06-04T22:20:03Z",
        "lastEditedBy" : "43998e22-6c2f-401d-9914-8cecf6fad929",
        "tags" : [
        ]
      }
    ],
    "commit" : "08bd5aa1e60d313d3d16929b5fd8a4f7f24fb058",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +35,39 @@\n# pandas API on Spark Code formatter.\nblack"
  },
  {
    "id" : "3624e5bd-8d1a-47b7-adf9-acf685440a17",
    "prId" : 29491,
    "prUrl" : "https://github.com/apache/spark/pull/29491#pullrequestreview-472915523",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5af9c72f-4214-42d2-b15e-c3a900689d7e",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Are there other implications to this, like, our python packaging now requires ipython when installing pyspark? (sorry ignorant question)",
        "createdAt" : "2020-08-20T15:39:01Z",
        "updatedAt" : "2020-08-25T07:51:55Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "f37decfe-3d58-486c-bd53-728b3d017720",
        "parentId" : "5af9c72f-4214-42d2-b15e-c3a900689d7e",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Nope, it means nothing special and this file is used nowhere within Spark. It's just for some dev people but I doubt if this is actually often used. We should leverage this file and standardize the dependencies somehow like @nchammas and @dongjoon-hyun tried before but .. I currently don't have a good idea about how to handle it.",
        "createdAt" : "2020-08-21T04:31:56Z",
        "updatedAt" : "2020-08-25T07:51:55Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "15aac718-f7cd-413e-b0ba-dd46c5b29cb5",
        "parentId" : "5af9c72f-4214-42d2-b15e-c3a900689d7e",
        "authorId" : "e29a7794-ac2e-4e6f-a690-737e83e1bace",
        "body" : "The packages that are being installed, when installing pyspark are defined in the setup.py: https://github.com/apache/spark/blob/master/python/setup.py#L206\r\n\r\nThe defacto way is to add an `extras_require` called `dev` where you add the development requirements. Preferably with a version attached to it, so you know that Jenkins and the local dev environments are the same.\r\n\r\nHappy to migrate this to the setup.py if you like.",
        "createdAt" : "2020-08-22T09:46:17Z",
        "updatedAt" : "2020-08-25T07:51:55Z",
        "lastEditedBy" : "e29a7794-ac2e-4e6f-a690-737e83e1bace",
        "tags" : [
        ]
      }
    ],
    "commit" : "0e114b6ac47cde46467d18dce2ca8e1ac0650a35",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +5,9 @@sphinx\npydata_sphinx_theme\nipython\nnbsphinx"
  },
  {
    "id" : "0c752d25-787f-4520-bf07-35b2eeb2597e",
    "prId" : 29188,
    "prUrl" : "https://github.com/apache/spark/pull/29188#pullrequestreview-453803574",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "312bf6bf-da1b-4e59-99af-96d582b068ee",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Maybe, is `pydata_sphinx_theme==0.3.1` better to prevent accidental HTML corruption in the future because HTML code and CSS style is impossible to verify automatically.",
        "createdAt" : "2020-07-23T00:40:42Z",
        "updatedAt" : "2020-07-24T01:12:25Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "f38e5037-62b9-4998-b6d6-f417be288403",
        "parentId" : "312bf6bf-da1b-4e59-99af-96d582b068ee",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "This PR is already pinning it in `dev/create-release/spark-rm/Dockerfile` for release.",
        "createdAt" : "2020-07-23T00:41:22Z",
        "updatedAt" : "2020-07-24T01:12:25Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "90829e5e-3f76-419a-bfbe-2e9fa6e83707",
        "parentId" : "312bf6bf-da1b-4e59-99af-96d582b068ee",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I thought about this for a while actually especially given the discussion we had before.\r\n`pydata_sphinx_theme` is actually relatively very new and it has [frequent releases](https://github.com/pandas-dev/pydata-sphinx-theme/tags). I would like to not pin the versions for now to promote dev people to keep it up to date.. maybe we could see how it goes if that sounds fine to you as well.",
        "createdAt" : "2020-07-23T02:30:18Z",
        "updatedAt" : "2020-07-24T01:12:25Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "d6d011737aba77e7eca41c1b65d6588764856797",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +4,8 @@Unidecode==0.04.19\nsphinx\npydata_sphinx_theme"
  }
]