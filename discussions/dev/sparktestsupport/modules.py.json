[
  {
    "id" : "3fcbb356-346e-44a6-8fa0-b0793b5f3d99",
    "prId" : 33484,
    "prUrl" : "https://github.com/apache/spark/pull/33484#pullrequestreview-713616894",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0d948abb-0128-4a88-a3ea-21dce5c34aee",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "let's put this in PySpark documentation as well https://github.com/apache/spark/blob/master/python/docs/source/reference/pyspark.sql.rst",
        "createdAt" : "2021-07-23T10:24:21Z",
        "updatedAt" : "2021-07-23T10:24:22Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "18ec8259-6164-468c-a5aa-fbdfd36274b0",
        "parentId" : "0d948abb-0128-4a88-a3ea-21dce5c34aee",
        "authorId" : "c4ecf4e4-1fda-4fed-9980-aed6c3b3e8e4",
        "body" : "so many magic places... done",
        "createdAt" : "2021-07-23T10:58:51Z",
        "updatedAt" : "2021-07-23T10:58:51Z",
        "lastEditedBy" : "c4ecf4e4-1fda-4fed-9980-aed6c3b3e8e4",
        "tags" : [
        ]
      }
    ],
    "commit" : "90675b0d300a299a047f97274e9d0814b2366a37",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +438,442 @@        \"pyspark.sql.pandas.typehints\",\n        \"pyspark.sql.pandas.utils\",\n        \"pyspark.sql.observation\",\n        # unittests\n        \"pyspark.sql.tests.test_arrow\","
  },
  {
    "id" : "217be488-0053-4199-bb4f-e31c5b310bd6",
    "prId" : 33125,
    "prUrl" : "https://github.com/apache/spark/pull/33125#pullrequestreview-694610519",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5dbfe0e9-6c8c-4d57-b41b-b7cc8ece3b9f",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "hm, the problem is that the tests will still be skipped because `ENABLE_DOCKER_INTEGRATION_TESTS` is set only when explicitly there are some changes in `docker_integration_tests` module alone. (e.g., when `sql` module is picked alone, `ENABLE_DOCKER_INTEGRATION_TESTS` is not set, and the related tests are skipped).\r\n\r\nThis logic was followed from the Kinesis one but maybe we should just remove `ENABLE_DOCKER_INTEGRATION_TESTS` away and treat it like other test cases.\r\n\r\nThis env approach was used because of the concern of flakiness IIRC but actually the test seems pretty stable (?).",
        "createdAt" : "2021-06-29T05:38:45Z",
        "updatedAt" : "2021-06-29T05:38:46Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "2b7b2c07-c471-4b02-8272-bd8e970ff1e0",
        "parentId" : "5dbfe0e9-6c8c-4d57-b41b-b7cc8ece3b9f",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "alternatively we can copy:\r\n\r\n```\r\n    environ=None if \"GITHUB_ACTIONS\" not in os.environ else {\r\n        \"ENABLE_DOCKER_INTEGRATION_TESTS\": \"1\"\r\n    },\r\n```\r\n\r\nto `sql` module too to be extra conservative.",
        "createdAt" : "2021-06-29T05:39:48Z",
        "updatedAt" : "2021-06-29T05:39:48Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "78948284-8952-4953-ab1e-d7f7919dacaa",
        "parentId" : "5dbfe0e9-6c8c-4d57-b41b-b7cc8ece3b9f",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Got it.",
        "createdAt" : "2021-06-29T05:43:11Z",
        "updatedAt" : "2021-06-29T05:43:20Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "241b6b30-c047-4bca-8f5d-00f3c9ac3ff9",
        "parentId" : "5dbfe0e9-6c8c-4d57-b41b-b7cc8ece3b9f",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Then, it will happen at `catalyst` module change, too. I'll put it to both `catalyst` and `sql` module.",
        "createdAt" : "2021-06-29T05:44:21Z",
        "updatedAt" : "2021-06-29T05:44:21Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "06ff73f0-c6f5-45c5-bf2c-906e2453c501",
        "parentId" : "5dbfe0e9-6c8c-4d57-b41b-b7cc8ece3b9f",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Yeah, that should be fine for now ðŸ‘ ",
        "createdAt" : "2021-06-29T05:44:49Z",
        "updatedAt" : "2021-06-29T05:44:49Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "3bd4962c8fe0f2ad4dc02f74613ea874d1862ad9",
    "line" : 25,
    "diffHunk" : "@@ -1,1 +776,780 @@docker_integration_tests = Module(\n    name=\"docker-integration-tests\",\n    dependencies=[sql],\n    build_profile_flags=[\"-Pdocker-integration-tests\"],\n    source_file_regexes=[\"external/docker-integration-tests\"],"
  },
  {
    "id" : "50053fe7-e78b-40a4-806d-00e38f79c0d6",
    "prId" : 32867,
    "prUrl" : "https://github.com/apache/spark/pull/32867#pullrequestreview-687542735",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "89e82358-38f8-4396-8b11-cbce601ce43b",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "BTW, do you plan to do it for doctests later? Doing it in a separate PR is fine. I'm just curious about your plan.",
        "createdAt" : "2021-06-18T11:27:38Z",
        "updatedAt" : "2021-06-18T11:27:38Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "11a1a255-2d17-48df-baae-9d906ae2e86f",
        "parentId" : "89e82358-38f8-4396-8b11-cbce601ce43b",
        "authorId" : "c13f1277-0254-494b-8e7a-bd7c261b2e9f",
        "body" : "Frankly, not yet, but I'd like to start to do some investigation. This unittestPR is started when I forgot many times to add the unittests in module. ^_^\r\n\r\nAnd the `doctests` discover is a bit special, it's not subclass of doctest (just like the unitest), so looks like there are 2 potential solution:\r\n\r\n1. Flag based. Add some flag like `_doc_test = True` in every test we want to test.\r\n2. Special name based. Discover which module contains `_doctest` method (we should rename _test to _doctest to make doc test name unique).",
        "createdAt" : "2021-06-18T15:41:04Z",
        "updatedAt" : "2021-06-18T15:41:05Z",
        "lastEditedBy" : "c13f1277-0254-494b-8e7a-bd7c261b2e9f",
        "tags" : [
        ]
      },
      {
        "id" : "64f69ddf-5f61-4365-a1a6-d03c4d77434a",
        "parentId" : "89e82358-38f8-4396-8b11-cbce601ce43b",
        "authorId" : "c13f1277-0254-494b-8e7a-bd7c261b2e9f",
        "body" : "BTW, I'm not sure here is the good place to discussion. : )\r\n\r\nDo you think the footer method (excute doctest or unitests in __main__ with repeat code in many files) in python code is a good way enough? Do we consider do some re-arch (or to say, already have some work) on testing framework?",
        "createdAt" : "2021-06-18T15:45:18Z",
        "updatedAt" : "2021-06-18T15:46:47Z",
        "lastEditedBy" : "c13f1277-0254-494b-8e7a-bd7c261b2e9f",
        "tags" : [
        ]
      }
    ],
    "commit" : "9f6b08f3d9ddaf1087c74d4a9f808697063922db",
    "line" : 148,
    "diffHunk" : "@@ -1,1 +507,511 @@    python_test_goals=[\n        # doctests\n        \"pyspark.streaming.util\",\n    ] + _discover_python_unittests(\"pyspark/streaming/tests\"),\n)"
  },
  {
    "id" : "f3c7aeeb-9724-4c2e-a192-76676aea037c",
    "prId" : 32867,
    "prUrl" : "https://github.com/apache/spark/pull/32867#pullrequestreview-694556739",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4593b721-2644-4bb4-998b-be64f9ee1f88",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "It is possible to add a simple doctests at https://github.com/apache/spark/blob/master/dev/run-tests.py? The doctests there are ran before running the script.",
        "createdAt" : "2021-06-28T02:22:44Z",
        "updatedAt" : "2021-06-28T02:22:44Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "b3f94044-77ab-4aaf-abf3-a252d23425bf",
        "parentId" : "4593b721-2644-4bb4-998b-be64f9ee1f88",
        "authorId" : "c13f1277-0254-494b-8e7a-bd7c261b2e9f",
        "body" : "Emm, looks like it a diffcult to add this doctest, the doctest requires real import of pyspark, so it would be failed when running the testcase in run-test.py.",
        "createdAt" : "2021-06-28T13:43:54Z",
        "updatedAt" : "2021-06-28T13:58:17Z",
        "lastEditedBy" : "c13f1277-0254-494b-8e7a-bd7c261b2e9f",
        "tags" : [
        ]
      },
      {
        "id" : "47ec49aa-c821-43e0-b9e3-53dd786de170",
        "parentId" : "4593b721-2644-4bb4-998b-be64f9ee1f88",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "kk that's fine",
        "createdAt" : "2021-06-29T03:23:08Z",
        "updatedAt" : "2021-06-29T03:23:08Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "9f6b08f3d9ddaf1087c74d4a9f808697063922db",
    "line" : 39,
    "diffHunk" : "@@ -1,1 +51,55 @@    Returns\n    -------\n    A set of complete test module name discovered under specified paths\n    \"\"\"\n"
  },
  {
    "id" : "05ae7019-d5ff-43b7-8ca9-51379a8bf102",
    "prId" : 32631,
    "prUrl" : "https://github.com/apache/spark/pull/32631#pullrequestreview-666163063",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f258e780-e25a-4a16-a86c-8b99aa89a424",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "I thought this test (& the k8s integration one) was intentionally excluded from this file because it is heavy. So, I tried to simply run the docker integration tests via `sbt` in my PR.",
        "createdAt" : "2021-05-22T08:09:00Z",
        "updatedAt" : "2021-05-22T08:09:00Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "03b9fd4c-8a1d-4de2-8d6a-0afeb2524f7b",
        "parentId" : "f258e780-e25a-4a16-a86c-8b99aa89a424",
        "authorId" : "3a227965-84e0-47cf-9974-11293764f028",
        "body" : ">I thought this test (& the k8s integration one) was intentionally excluded from this file because it is heavy. \r\n\r\nI think so. So I changed `run-tests.py` not to run the integration tests by default.",
        "createdAt" : "2021-05-22T08:11:15Z",
        "updatedAt" : "2021-05-22T08:11:15Z",
        "lastEditedBy" : "3a227965-84e0-47cf-9974-11293764f028",
        "tags" : [
        ]
      }
    ],
    "commit" : "692d95d1458993cbb9cbd47014202e84cd6aa328",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +745,749 @@)\n\ndocker_integration_tests = Module(\n    name=\"docker-integration-tests\",\n    dependencies=[],"
  },
  {
    "id" : "659f52eb-22dd-4964-8e22-ace9bfb7a4fc",
    "prId" : 32139,
    "prUrl" : "https://github.com/apache/spark/pull/32139#pullrequestreview-695261294",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1b1ccd72-7437-42c1-a630-d99f24c529d3",
        "parentId" : null,
        "authorId" : "c13f1277-0254-494b-8e7a-bd7c261b2e9f",
        "body" : "Looks like we could add some testcase discover mechanism on it in follow up.",
        "createdAt" : "2021-04-15T14:30:33Z",
        "updatedAt" : "2021-04-15T18:35:29Z",
        "lastEditedBy" : "c13f1277-0254-494b-8e7a-bd7c261b2e9f",
        "tags" : [
        ]
      },
      {
        "id" : "3896780d-6156-4079-b397-da63c61bc0be",
        "parentId" : "1b1ccd72-7437-42c1-a630-d99f24c529d3",
        "authorId" : "8194e9fe-1d06-4199-937e-9633d15a18cb",
        "body" : "Sounds good! That would reduce maintenance costs.",
        "createdAt" : "2021-04-15T16:18:08Z",
        "updatedAt" : "2021-04-15T18:35:29Z",
        "lastEditedBy" : "8194e9fe-1d06-4199-937e-9633d15a18cb",
        "tags" : [
        ]
      },
      {
        "id" : "ebcbf63c-0f1d-42b4-afc4-f345f9503678",
        "parentId" : "1b1ccd72-7437-42c1-a630-d99f24c529d3",
        "authorId" : "c13f1277-0254-494b-8e7a-bd7c261b2e9f",
        "body" : "Finally, it is merged in https://github.com/apache/spark/pull/32867 , : )",
        "createdAt" : "2021-06-29T09:01:21Z",
        "updatedAt" : "2021-06-29T09:01:21Z",
        "lastEditedBy" : "c13f1277-0254-494b-8e7a-bd7c261b2e9f",
        "tags" : [
        ]
      },
      {
        "id" : "c88e7903-9d88-4b00-ab2e-2dfaf37a4e86",
        "parentId" : "1b1ccd72-7437-42c1-a630-d99f24c529d3",
        "authorId" : "8194e9fe-1d06-4199-937e-9633d15a18cb",
        "body" : "Thanks for making it happen! :)",
        "createdAt" : "2021-06-29T16:34:55Z",
        "updatedAt" : "2021-06-29T16:34:55Z",
        "lastEditedBy" : "8194e9fe-1d06-4199-937e-9633d15a18cb",
        "tags" : [
        ]
      }
    ],
    "commit" : "a209a9f506824c49fa81f460f43ec37cd8b4f8e2",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +612,616 @@        \"pyspark.pandas.typedef.typehints\",\n        # unittests\n        \"pyspark.pandas.tests.indexes.test_base\",\n        \"pyspark.pandas.tests.indexes.test_category\",\n        \"pyspark.pandas.tests.indexes.test_datetime\","
  },
  {
    "id" : "f91bc0ac-b898-4ab3-be02-7ba8034df4d7",
    "prId" : 32137,
    "prUrl" : "https://github.com/apache/spark/pull/32137#pullrequestreview-634769914",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c807998d-523a-4f08-b833-2a94e16c6708",
        "parentId" : null,
        "authorId" : "8194e9fe-1d06-4199-937e-9633d15a18cb",
        "body" : "May I sort these tests in my last PR? That would make resolving other PRs' conflicts much easier.",
        "createdAt" : "2021-04-13T16:00:30Z",
        "updatedAt" : "2021-04-13T21:54:06Z",
        "lastEditedBy" : "8194e9fe-1d06-4199-937e-9633d15a18cb",
        "tags" : [
        ]
      }
    ],
    "commit" : "cf58ab2f0a9ca4e553a636489e332faa01ec491c",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +613,617 @@        # unittests\n        \"pyspark.pandas.tests.test_dataframe\",\n        \"pyspark.pandas.tests.test_config\",\n        \"pyspark.pandas.tests.test_default_index\",\n        \"pyspark.pandas.tests.test_extension\","
  },
  {
    "id" : "9c066972-478d-4996-b950-b619c01e4d44",
    "prId" : 27165,
    "prUrl" : "https://github.com/apache/spark/pull/27165#pullrequestreview-341450144",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3efcbe28-69ef-44c7-a1f5-480932420739",
        "parentId" : null,
        "authorId" : "981b170c-729a-429c-b115-0350ea50b32b",
        "body" : "Note: It looks like this contains only internal components. It wouldn't hurt to add leading underscore (probably applies to modules included with [SPARK-30434]).",
        "createdAt" : "2020-01-10T14:31:43Z",
        "updatedAt" : "2020-01-22T01:53:52Z",
        "lastEditedBy" : "981b170c-729a-429c-b115-0350ea50b32b",
        "tags" : [
        ]
      },
      {
        "id" : "64c25f7d-c7d4-4117-a5c0-f365a31b70a3",
        "parentId" : "3efcbe28-69ef-44c7-a1f5-480932420739",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Let me keep it consistent in the current codebase for now. We could do that separately since there are already smilar private modules without underscore.",
        "createdAt" : "2020-01-10T22:04:43Z",
        "updatedAt" : "2020-01-22T01:53:52Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "f3c4d9b2-198c-446e-8838-e526c223147b",
        "parentId" : "3efcbe28-69ef-44c7-a1f5-480932420739",
        "authorId" : "981b170c-729a-429c-b115-0350ea50b32b",
        "body" : "Makes sense, though we should probably keep that mind in the future. Latest refactoring of `pyspark.ml` moved things in that direction, so it would be nice to keep things consistent (maybe some official guidelines?).",
        "createdAt" : "2020-01-10T22:12:22Z",
        "updatedAt" : "2020-01-22T01:53:52Z",
        "lastEditedBy" : "981b170c-729a-429c-b115-0350ea50b32b",
        "tags" : [
        ]
      }
    ],
    "commit" : "76e09092f983782f0986c0c55c34165de62435bd",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +369,373 @@        \"pyspark.sql.pandas.types\",\n        \"pyspark.sql.pandas.serializers\",\n        \"pyspark.sql.pandas.typehints\",\n        \"pyspark.sql.pandas.utils\",\n        # unittests"
  },
  {
    "id" : "32663777-7d28-40d7-b8ed-e13778d496fd",
    "prId" : 27109,
    "prUrl" : "https://github.com/apache/spark/pull/27109#pullrequestreview-339595317",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f0610d58-7f5a-41be-a2c3-424c4e731f04",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "BTW @BryanCutler, @icexelloss, @ueshin, please feel free to change package/module/class names separately if you don't like it. It won't likely cause no or less conflicts with the works I'm doing.",
        "createdAt" : "2020-01-08T00:50:27Z",
        "updatedAt" : "2020-01-08T00:50:28Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "c63c271d95e7864b69ca477c51e4e2d22b7d9029",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +363,367 @@        \"pyspark.sql.window\",\n        \"pyspark.sql.avro.functions\",\n        \"pyspark.sql.pandas.conversion\",\n        \"pyspark.sql.pandas.map_ops\",\n        \"pyspark.sql.pandas.functions\","
  },
  {
    "id" : "28c58afb-9ab5-4936-9f08-8157218819f7",
    "prId" : 24644,
    "prUrl" : "https://github.com/apache/spark/pull/24644#pullrequestreview-239550396",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "aa610eb6-8f1e-4dd0-a183-2280c1cd391a",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Similarily, I tested `all_modules`. This is just to move the codes to remove later in single place.",
        "createdAt" : "2019-05-20T02:37:37Z",
        "updatedAt" : "2019-05-20T02:46:27Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "95c267a4-552c-4712-8b74-9641254e9cf8",
        "parentId" : "aa610eb6-8f1e-4dd0-a183-2280c1cd391a",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "This just does the same thing as before, right?",
        "createdAt" : "2019-05-20T04:01:29Z",
        "updatedAt" : "2019-05-20T04:01:30Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "4b012f55-36cf-48ad-8de9-c8d303ff544c",
        "parentId" : "aa610eb6-8f1e-4dd0-a183-2280c1cd391a",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "yea I just moved.",
        "createdAt" : "2019-05-20T04:53:24Z",
        "updatedAt" : "2019-05-20T04:53:24Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "5d272ed9-77f3-44e9-9bf4-3f9c8d3c13a4",
        "parentId" : "aa610eb6-8f1e-4dd0-a183-2280c1cd391a",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Got it. Thank you.",
        "createdAt" : "2019-05-20T14:52:01Z",
        "updatedAt" : "2019-05-20T14:52:01Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "92abff88ea626812c2b6e161122bc4d0766be4a7",
    "line" : 37,
    "diffHunk" : "@@ -1,1 +566,570 @@if hadoop_version == \"hadoop3.2\":\n    print(\"[info] Skip unsupported module:\", \"hive-thriftserver\")\n    all_modules = [m for m in all_modules if m.name != \"hive-thriftserver\"]\n\n# The root module is a dummy module which is used to run all of the tests."
  },
  {
    "id" : "6b819ff9-10fa-4a82-bca1-8aa826c19e06",
    "prId" : 24391,
    "prUrl" : "https://github.com/apache/spark/pull/24391#pullrequestreview-236002524",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "469db0e1-961a-47aa-81a8-6ba441df90ba",
        "parentId" : null,
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "Used to skip hive-thriftserver module for hadoop-3.2.  Will revert this change once we can merge.",
        "createdAt" : "2019-04-17T06:55:43Z",
        "updatedAt" : "2019-05-13T05:48:48Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      },
      {
        "id" : "12927e9d-65e9-4112-9d8f-e54b93fd3635",
        "parentId" : "469db0e1-961a-47aa-81a8-6ba441df90ba",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Maybe leave a TODO here just to try to make sure that doesn't get lost",
        "createdAt" : "2019-05-10T09:12:10Z",
        "updatedAt" : "2019-05-13T05:48:48Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      }
    ],
    "commit" : "27e5b6e73fd3ede76219c477270b56e0f849fac2",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +82,86 @@            dep.dependent_modules.add(self)\n        # TODO: Skip hive-thriftserver module for hadoop-3.2. remove this once hadoop-3.2 support it\n        if name == \"hive-thriftserver\" and hadoop_version == \"hadoop3.2\":\n            print(\"[info] Skip unsupported module:\", name)\n        else:"
  },
  {
    "id" : "b9b1413a-26be-4136-8d62-5e6c911d51c4",
    "prId" : 24391,
    "prUrl" : "https://github.com/apache/spark/pull/24391#pullrequestreview-235940568",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a77c82da-b6a6-4a3a-a465-875dd282bc7d",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "We should have a log message to show which profile we are using; otherwise, it is hard for us to know which profile is activated. ",
        "createdAt" : "2019-05-06T17:01:20Z",
        "updatedAt" : "2019-05-13T05:48:48Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "89c79c2d-2e07-4674-8d8e-681b170f04c3",
        "parentId" : "a77c82da-b6a6-4a3a-a465-875dd282bc7d",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "Yes. we already have these log message:\r\n![image](https://user-images.githubusercontent.com/5399861/57261352-07399700-709a-11e9-9074-f6faf2097f7d.png)\r\nhttps://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/105106/consoleFull",
        "createdAt" : "2019-05-06T23:32:20Z",
        "updatedAt" : "2019-05-13T05:48:48Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      },
      {
        "id" : "5d2983af-583e-46d0-9f22-511c6144be2d",
        "parentId" : "a77c82da-b6a6-4a3a-a465-875dd282bc7d",
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "What I mean, we should have a log message to show which profile is being actually used. \r\n\r\nJust relying on the command parameter lists does not sound very reliable",
        "createdAt" : "2019-05-10T04:38:33Z",
        "updatedAt" : "2019-05-13T05:48:48Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "182feeff-2a84-484e-827b-2e620604463b",
        "parentId" : "a77c82da-b6a6-4a3a-a465-875dd282bc7d",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "OK. updated to:\r\n![image](https://user-images.githubusercontent.com/5399861/57506520-9d85eb00-732e-11e9-8a7e-05ef7319d359.png)\r\n\r\n",
        "createdAt" : "2019-05-10T06:19:48Z",
        "updatedAt" : "2019-05-13T05:48:48Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      }
    ],
    "commit" : "27e5b6e73fd3ede76219c477270b56e0f849fac2",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +29,33 @@\nall_modules = []\n\n\n@total_ordering"
  },
  {
    "id" : "26eed6e7-c60f-4ab0-912d-b56edc9ca34f",
    "prId" : 24391,
    "prUrl" : "https://github.com/apache/spark/pull/24391#pullrequestreview-239274615",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f5201a44-e67c-478b-8084-db26af26553d",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@wangyum, this will shows the info every time this modules is imported. why did we do this here?",
        "createdAt" : "2019-05-20T02:02:46Z",
        "updatedAt" : "2019-05-20T02:02:46Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "9a234f9a-9ca2-4dde-85c0-20df99fd143b",
        "parentId" : "f5201a44-e67c-478b-8084-db26af26553d",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "okay. it's a temp fix so I'm fine. I will make a followup to handle https://github.com/apache/spark/pull/24639/files",
        "createdAt" : "2019-05-20T02:12:23Z",
        "updatedAt" : "2019-05-20T02:12:24Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "7297edf0-0c03-4428-9322-af88f8b17ffd",
        "parentId" : "f5201a44-e67c-478b-8084-db26af26553d",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "Skip the `hive-thriftserver` module when running the hadoop-3.2 test. Will remove it in another PR: https://github.com/apache/spark/pull/24628/files",
        "createdAt" : "2019-05-20T02:12:57Z",
        "updatedAt" : "2019-05-20T02:12:57Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      },
      {
        "id" : "08b181a4-77f7-4455-aad6-46f5c5cd5b7a",
        "parentId" : "f5201a44-e67c-478b-8084-db26af26553d",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "Thank you @HyukjinKwon",
        "createdAt" : "2019-05-20T02:22:17Z",
        "updatedAt" : "2019-05-20T02:22:17Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      }
    ],
    "commit" : "27e5b6e73fd3ede76219c477270b56e0f849fac2",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +22,26 @@import os\n\nif os.environ.get(\"AMPLAB_JENKINS\"):\n    hadoop_version = os.environ.get(\"AMPLAB_JENKINS_BUILD_PROFILE\", \"hadoop2.7\")\nelse:"
  }
]