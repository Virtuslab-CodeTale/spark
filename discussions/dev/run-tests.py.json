[
  {
    "id" : "1211f521-78ca-4df4-bfcf-9e032b2b86c2",
    "prId" : 33410,
    "prUrl" : "https://github.com/apache/spark/pull/33410#pullrequestreview-712198205",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "68f7c89d-e59a-41bd-bb48-9ddea38096c1",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Ah the changes here should be backported too. I'll make a pr later today.",
        "createdAt" : "2021-07-21T21:15:22Z",
        "updatedAt" : "2021-07-21T21:15:22Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "48bfb39f0d3a8d15614998297ba56addf3b756b5",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +694,698 @@    excluded_tags = []\n    if should_only_test_modules:\n        # We're likely in the forked repository\n        is_apache_spark_ref = os.environ.get(\"APACHE_SPARK_REF\", \"\") != \"\"\n        # We're likely in the main repo build."
  },
  {
    "id" : "fa08efbf-cb78-4095-91a9-aa1767a5c67b",
    "prId" : 29306,
    "prUrl" : "https://github.com/apache/spark/pull/29306#pullrequestreview-458479278",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "04254514-2435-4f83-a147-4cb2eed60c3b",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I piggyback this change together. We don't need to install SparkR when `--modules` is specified except that it includes `sparkr`.",
        "createdAt" : "2020-07-30T14:52:32Z",
        "updatedAt" : "2020-07-30T15:54:34Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "8ff181e0fd0433fbc721c99e6f5aefbf2de58d6d",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +616,620 @@        test_modules = [m for m in modules.all_modules if m.name in str_test_modules]\n\n    if not should_only_test_modules or modules.sparkr in test_modules:\n        # If tests modules are specified, we will not run R linter.\n        # SparkR needs the manual SparkR installation."
  },
  {
    "id" : "8b870240-3404-4657-8972-0792c0225daa",
    "prId" : 29086,
    "prUrl" : "https://github.com/apache/spark/pull/29086#pullrequestreview-447360590",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9c7a440b-1c9c-41b5-afff-723f5c19294c",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "`GITHUB_BASE_REF` is only set in the PR builder; otherwise, it's an empty string.",
        "createdAt" : "2020-07-13T11:07:40Z",
        "updatedAt" : "2020-07-13T11:07:41Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "5de6b34d-f529-43ca-9196-d12a3c051ec8",
        "parentId" : "9c7a440b-1c9c-41b5-afff-723f5c19294c",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "~Do you mean Jenkins PR Builder?~",
        "createdAt" : "2020-07-13T15:30:08Z",
        "updatedAt" : "2020-07-13T15:30:27Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "9d8555a44d8157791e8140236a6eba9c09b2d5d8",
    "line" : 108,
    "diffHunk" : "@@ -1,1 +654,658 @@        # only the affected modules.\n        if test_env == \"github_actions\":\n            if os.environ[\"GITHUB_BASE_REF\"] != \"\":\n                # Pull requests\n                changed_files = identify_changed_files_from_git_commits("
  },
  {
    "id" : "ac08107b-ba50-4e88-991e-6180799366f9",
    "prId" : 29086,
    "prUrl" : "https://github.com/apache/spark/pull/29086#pullrequestreview-447846572",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c2eec916-629a-4ac9-901c-b7aaaffc54e9",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "`GITHUB_SHA` is the commit in PR. `GITHUB_BASE_REF` is the target repo's branch.",
        "createdAt" : "2020-07-13T11:08:52Z",
        "updatedAt" : "2020-07-13T11:08:52Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "fa1ba9f3-7919-4b11-98b6-513730329729",
        "parentId" : "c2eec916-629a-4ac9-901c-b7aaaffc54e9",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "For many commits in a PR, each time `changed_files` is different? If a previous commit changed two modules but later one changes only one, will this only run test for the later module?",
        "createdAt" : "2020-07-14T06:33:01Z",
        "updatedAt" : "2020-07-14T06:33:02Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "842394c0-806d-43eb-b8a2-590c916d7830",
        "parentId" : "c2eec916-629a-4ac9-901c-b7aaaffc54e9",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "From my testing, it combines all commits and `changed_files` is based on all commits always.\r\n\r\nSeems like `GITHUB_SHA` became a merge commit internally in Github Actions, and `GITHUB_BASE_REF` becomes the branch which PR opens against.\r\n\r\nFor example, if a PR has commits 1, 2 and 3, GitHub Actions looks creates a merge commit 4 for commits 1, 2 and 3. And, it calculates the diff from the branch, for example, `master` in the original repo, Apache Spark.\r\n\r\n```bash\r\ngit diff 4 master\r\n```\r\n",
        "createdAt" : "2020-07-14T06:58:39Z",
        "updatedAt" : "2020-07-14T06:58:39Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "d7cff1bc-26c3-410e-9d5d-1aeda70d899b",
        "parentId" : "c2eec916-629a-4ac9-901c-b7aaaffc54e9",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "This is an example of the merge commit: https://github.com/HyukjinKwon/spark/commit/8f36ec455e19dbfb10195d872a9ccaeb2de8ceca at https://github.com/HyukjinKwon/spark/pull/7",
        "createdAt" : "2020-07-14T07:04:36Z",
        "updatedAt" : "2020-07-14T07:04:36Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "9f588b27-3a3a-4808-bc8d-88a021955379",
        "parentId" : "c2eec916-629a-4ac9-901c-b7aaaffc54e9",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Okay. Thanks for clarifying. Looks good.",
        "createdAt" : "2020-07-14T07:14:29Z",
        "updatedAt" : "2020-07-14T07:14:30Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "9d8555a44d8157791e8140236a6eba9c09b2d5d8",
    "line" : 111,
    "diffHunk" : "@@ -1,1 +657,661 @@                # Pull requests\n                changed_files = identify_changed_files_from_git_commits(\n                    os.environ[\"GITHUB_SHA\"], target_branch=os.environ[\"GITHUB_BASE_REF\"])\n            else:\n                # Build for each commit."
  },
  {
    "id" : "338fa973-6623-460c-b287-62a324713c79",
    "prId" : 29086,
    "prUrl" : "https://github.com/apache/spark/pull/29086#pullrequestreview-447361273",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "036d1845-e7c0-4d94-ae51-f30359571c45",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Thanks. This looks safe.",
        "createdAt" : "2020-07-13T15:30:53Z",
        "updatedAt" : "2020-07-13T15:30:53Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "9d8555a44d8157791e8140236a6eba9c09b2d5d8",
    "line" : 107,
    "diffHunk" : "@@ -1,1 +653,657 @@        # If we're running the tests in Github Actions, attempt to detect and test\n        # only the affected modules.\n        if test_env == \"github_actions\":\n            if os.environ[\"GITHUB_BASE_REF\"] != \"\":\n                # Pull requests"
  },
  {
    "id" : "2e7e1b88-0b9c-469c-896e-c407a0a31887",
    "prId" : 28578,
    "prUrl" : "https://github.com/apache/spark/pull/28578#pullrequestreview-414158195",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e31015bd-59d0-46ad-8d86-519c7471d96a",
        "parentId" : null,
        "authorId" : "3a227965-84e0-47cf-9974-11293764f028",
        "body" : "For environments where Chrome and Chrome driver is not installed, including CI environment, tests tagged as `ChromeUITest` is filtered out when tests are run through `run-tests.py`.",
        "createdAt" : "2020-05-19T07:09:01Z",
        "updatedAt" : "2020-05-21T08:31:41Z",
        "lastEditedBy" : "3a227965-84e0-47cf-9974-11293764f028",
        "tags" : [
        ]
      }
    ],
    "commit" : "6ccbc391067995205127425ef59c4a9f1fe40520",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +610,614 @@          \", \".join(x.name for x in changed_modules))\n\n    excluded_tags.extend(always_excluded_tags)\n\n    # setup environment variables"
  },
  {
    "id" : "16971068-7fb3-4688-a392-78c52ee52577",
    "prId" : 26796,
    "prUrl" : "https://github.com/apache/spark/pull/26796#pullrequestreview-328602716",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6aedca36-61c6-4634-90bd-2f41ec084d7d",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Hm.. I avoided this because it will slow down doc gen since it re-uses complied ones from the regular build. How long does it increase?",
        "createdAt" : "2019-12-08T11:24:03Z",
        "updatedAt" : "2019-12-09T00:57:47Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "238d6f03-24a6-4577-9f51-c351b0577688",
        "parentId" : "6aedca36-61c6-4634-90bd-2f41ec084d7d",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Also, if I am not wrong, this wont verify the documentation in test codes anymore. That's fine as documentation in the test codes wont be the part of our official documentation though. Cc @JoshRosen I think I talked about this with you before somewhere.",
        "createdAt" : "2019-12-08T11:30:11Z",
        "updatedAt" : "2019-12-09T00:57:47Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "5acd9c00cbb5052e12b68d48f54f50ffeb6d23cd",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +343,347 @@    # Enable all of the profiles for the build:\n    build_profiles = extra_profiles + modules.root.build_profile_flags\n    sbt_goals = [\"clean\", \"unidoc\"]\n    profiles_and_goals = build_profiles + sbt_goals\n"
  },
  {
    "id" : "88f755b6-2d07-4333-8dd7-46a39d8dc93b",
    "prId" : 26695,
    "prUrl" : "https://github.com/apache/spark/pull/26695#pullrequestreview-324500606",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c855c02a-f701-4150-9e59-cc3ee79efa03",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Can we do it consistently with what we handle Hadoop versions? (e.g., https://github.com/apache/spark/commit/8ab13065f626a12d18f6ac14a4dbf3d258919825#diff-3c9f4fccf7d30ce2e8fa86db2ad1fdad)",
        "createdAt" : "2019-11-29T00:59:22Z",
        "updatedAt" : "2019-11-29T00:59:22Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "543cf73e-bd3e-432c-b6c3-12ed3c30f583",
        "parentId" : "c855c02a-f701-4150-9e59-cc3ee79efa03",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "and .. seems like we should change https://github.com/apache/spark/commit/c98e5eb3396a6db92f2420e743afa9ddff319ca2#diff-180360612c6b8c4ed830919bbb4dd459R56 too if this PR also targets to change default Hive profile to 2.3.",
        "createdAt" : "2019-11-29T02:44:14Z",
        "updatedAt" : "2019-11-29T02:44:15Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "264e64cb74b17ee53666d5f168d5fb91710965c1",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +289,293 @@\n    if hadoop_version in sbt_maven_hadoop_profiles:\n        if (\"ghprbPullTitle\" in os.environ and\n                \"test-hive1.2\" in os.environ[\"ghprbPullTitle\"].lower()):\n            return sbt_maven_hadoop_profiles[hadoop_version] + [\"-Phive-1.2\"]"
  },
  {
    "id" : "a693bec0-648b-4e24-aa9c-4433b4250a2f",
    "prId" : 26619,
    "prUrl" : "https://github.com/apache/spark/pull/26619#pullrequestreview-321299143",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "41f59ca3-17fd-4caa-8da5-5d90194d0ae7",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Hm, @dongjoon-hyun, can you create JIRAs to follow up? Seems we will change `-Phive-2.3` as a default profile as [discussed in the mailing list](http://apache-spark-developers-list.1001551.n3.nabble.com/Removing-the-usage-of-forked-hive-in-Apache-Spark-3-0-SPARK-20202-td28347.html).\r\n\r\n1. Verify *Hadoop 2.7 and Hive 2.3 combination in both JDK 8 and [JDK 11 (WIP)](https://github.com/apache/spark/pull/26533) as it will be the default\r\n2. Setting a Jenkins job for `-Phadoop-2.7 -Phive-2.3`.\r\n3. We will need to be able to configure Hive version and also Hadoop version in the PR builder.\r\n4. Change the default profile to Hive 2.3.\r\n5. https://github.com/apache/spark/pull/26619#issuecomment-556926064\r\n6. Release script updates.\r\n7. ... (other more .. ?)\r\n\r\n*Hadoop 2 will be default at this moment. it's [being discussed in the mailing list](http://apache-spark-developers-list.1001551.n3.nabble.com/Use-Hadoop-3-2-as-a-default-Hadoop-profile-in-3-0-0-td28198.html)\r\n\r\nMany things are going on so it looks very easy to lose the track.",
        "createdAt" : "2019-11-21T05:52:18Z",
        "updatedAt" : "2019-11-21T05:52:18Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "3ca69567-365a-4105-b39a-6a58934c7947",
        "parentId" : "41f59ca3-17fd-4caa-8da5-5d90194d0ae7",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Sure!",
        "createdAt" : "2019-11-21T16:09:18Z",
        "updatedAt" : "2019-11-21T16:09:19Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "1544c6c7-375e-4e6e-abed-cb6f12f413e1",
        "parentId" : "41f59ca3-17fd-4caa-8da5-5d90194d0ae7",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "BTW, the thread is independent from JDK11 or Hadoop 3. For the following, only JDK8 is the target of the follow-up of this PR. And, for JDK11, I believe @wangyum will handle it at his on-going work.\r\n> Verify *Hadoop 2.7 and Hive 2.3 combination in both JDK 8 and JDK 11 (WIP) as it will be the default\r\n\r\nFor (4), `Hive 2.3` is already default for `Hadoop-3`. And, this PR makes `Hive 2.3` as a default in the pom files. So, I guess the remains of (4) is equal to (2) and (3).",
        "createdAt" : "2019-11-21T16:46:11Z",
        "updatedAt" : "2019-11-21T18:53:23Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "52f1d267-6cbe-407c-903b-304927edbe99",
        "parentId" : "41f59ca3-17fd-4caa-8da5-5d90194d0ae7",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "The follow-up issues are created and mentioned in the PR description, @HyukjinKwon .",
        "createdAt" : "2019-11-21T18:40:45Z",
        "updatedAt" : "2019-11-21T18:40:45Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "10e067d2-2620-4279-9e5b-0a8c3939352c",
        "parentId" : "41f59ca3-17fd-4caa-8da5-5d90194d0ae7",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Ah, sure, thanks for clarification. @wangyum, can you verify the new JDK 11 combination and take a following action?",
        "createdAt" : "2019-11-22T01:49:46Z",
        "updatedAt" : "2019-11-22T01:49:46Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "27bff221-8c3a-4e0a-9bae-9adf41fde795",
        "parentId" : "41f59ca3-17fd-4caa-8da5-5d90194d0ae7",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "@HyukjinKwon I will do it later.",
        "createdAt" : "2019-11-22T02:03:47Z",
        "updatedAt" : "2019-11-22T02:03:47Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      }
    ],
    "commit" : "f93742a311c594cecbf404c495aed23afb22c381",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +285,289 @@    sbt_maven_hadoop_profiles = {\n        \"hadoop2.7\": [\"-Phadoop-2.7\", \"-Phive-1.2\"],\n        \"hadoop3.2\": [\"-Phadoop-3.2\", \"-Phive-2.3\"],\n    }\n"
  },
  {
    "id" : "611012ad-eaa4-4ed8-bddf-9f252ac2ac90",
    "prId" : 26556,
    "prUrl" : "https://github.com/apache/spark/pull/26556#pullrequestreview-318089797",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "540d8cc7-18ae-41bd-9520-d4477ac8c5ca",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "no big deal but it is usually:\r\n\r\n```python\r\n    >>> [x.name for x in determine_modules_for_files(\r\n    ...     [\".github/workflows/master.yml\", \"appveyor.yml\"])]\r\n```\r\n",
        "createdAt" : "2019-11-18T03:59:47Z",
        "updatedAt" : "2019-11-18T03:59:47Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "f64d060b76592fd2b7436647a1a2a1fbd1f4887a",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +51,55 @@    ['root']\n    >>> [x.name for x in determine_modules_for_files( \\\n            [\".github/workflows/master.yml\", \"appveyor.yml\"])]\n    []\n    \"\"\""
  },
  {
    "id" : "06e99b7f-f48d-42b8-b37a-34c7ddffe084",
    "prId" : 25423,
    "prUrl" : "https://github.com/apache/spark/pull/25423#pullrequestreview-279875356",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d81b9625-7acf-46d4-b106-9e8006c3edd2",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Can we try to set this in python tests too? Seems like Java gateway has to use JDK 11 as well.",
        "createdAt" : "2019-08-25T05:20:45Z",
        "updatedAt" : "2019-08-25T05:20:54Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "f854aa6d-5923-41a7-a37e-b5b0615fa2c0",
        "parentId" : "d81b9625-7acf-46d4-b106-9e8006c3edd2",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "It should use Java 11 if the path provides Java 11 and the test harness that runs Python tests does too. At least I don't know how else one would tell pyspark what to use!\r\n\r\nIn fact I'm pretty sure the test failure here shows that it is using JDK 11. From JPMML: `java.lang.ClassNotFoundException: com.sun.xml.internal.bind.v2.ContextFactory` This would be caused by JDK 11 changes. However, I don't get why all the other non-Python tests don't fail.\r\n\r\nGiven the weird problem in https://github.com/apache/spark/pull/24651 I am wondering if we have some subtle classpath issues with how the Pyspark tests are run.\r\n\r\nThis one however might be more directly solvable by figuring out what is suggesting to use this old Sun JAXB implementation. I'll start digging around META-INF",
        "createdAt" : "2019-08-26T15:12:13Z",
        "updatedAt" : "2019-08-26T15:20:00Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "a044d472-bdcd-488d-9d1a-77270709fb0e",
        "parentId" : "d81b9625-7acf-46d4-b106-9e8006c3edd2",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Hm, and why does https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-master-test-maven-hadoop-3.2-jdk-11/ pass then? it is doing the same thing in the Jenkins config. (OK I think I answered my own question below)\r\n\r\nEDIT: Oh, because it doesn't run Pyspark tests?",
        "createdAt" : "2019-08-26T15:21:17Z",
        "updatedAt" : "2019-08-26T15:22:39Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "10a9e6e6-a23a-4dfe-97fb-778a1adcb252",
        "parentId" : "d81b9625-7acf-46d4-b106-9e8006c3edd2",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "No, actually you're right. Yes, seems after Scala tests here, the PATH and JAVA_HOME still set as are.\r\n\r\nI thought:\r\n\r\nhttps://github.com/apache/spark/blob/209b9361ac8a4410ff797cff1115e1888e2f7e66/python/pyspark/java_gateway.py#L45-L60\r\n\r\nhttps://github.com/apache/spark/blob/3cb82047f2f51af553df09b9323796af507d36f8/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala#L425\r\n\r\nHere somehow happened to use JDK 8.\r\n\r\nActually the PySpark tests and SparkR tests passed at https://github.com/apache/spark/pull/25443#issuecomment-522206143\r\n\r\nSo, the issue persists here .. but I guess yes we can do it separately since at least this PR _seems_ setting JDK 11 correctly, and it virtually doesn't affect any main or test code (if this title is not used).",
        "createdAt" : "2019-08-26T15:45:10Z",
        "updatedAt" : "2019-08-26T15:45:44Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "d5170063-31e5-45bb-8240-fe35ff1e5439",
        "parentId" : "d81b9625-7acf-46d4-b106-9e8006c3edd2",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "It's interesting. Thank you for the investigation, @srowen and @HyukjinKwon ",
        "createdAt" : "2019-08-26T17:56:52Z",
        "updatedAt" : "2019-08-26T17:56:52Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "f9570d88-a88f-49f5-ae95-7bcae328ce75",
        "parentId" : "d81b9625-7acf-46d4-b106-9e8006c3edd2",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Do we have a JIRA issue for this?",
        "createdAt" : "2019-08-26T22:12:48Z",
        "updatedAt" : "2019-08-26T22:12:48Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "019e5fc3-0065-44d1-baa3-be30316531cc",
        "parentId" : "d81b9625-7acf-46d4-b106-9e8006c3edd2",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "We probably need one, yeah, regardless of the cause. I'll file one to track.",
        "createdAt" : "2019-08-26T22:34:39Z",
        "updatedAt" : "2019-08-26T22:34:39Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "14741564-1af4-43db-88c2-3fd0b67b7300",
        "parentId" : "d81b9625-7acf-46d4-b106-9e8006c3edd2",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "https://issues.apache.org/jira/browse/SPARK-28877",
        "createdAt" : "2019-08-26T22:43:03Z",
        "updatedAt" : "2019-08-26T22:43:03Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      }
    ],
    "commit" : "bae6524d7d1965dbf30eb4146c1278e18af13801",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +409,413 @@        os.environ[\"JAVA_HOME\"] = \"/usr/java/jdk-11.0.1\"\n        os.environ[\"PATH\"] = \"%s/bin:%s\" % (os.environ[\"JAVA_HOME\"], os.environ[\"PATH\"])\n        test_profiles += ['-Djava.version=11']\n\n    if build_tool == \"maven\":"
  },
  {
    "id" : "af977702-dff3-4432-9c6f-a8667f394583",
    "prId" : 25289,
    "prUrl" : "https://github.com/apache/spark/pull/25289#pullrequestreview-271750637",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f464af1e-dfd8-4270-ab70-a8b6bee55fae",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "I know the intention, but this is worse historically.\r\nIf we are ready, `python3` might be a better choice.",
        "createdAt" : "2019-07-30T01:18:37Z",
        "updatedAt" : "2020-11-16T00:31:58Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "4addbb55-1ae6-497a-90ed-a71c39235755",
        "parentId" : "f464af1e-dfd8-4270-ab70-a8b6bee55fae",
        "authorId" : "7e665d8b-d739-4edf-88c8-7379ff8585c2",
        "body" : "If so, should we also change this line in `dev/run-test` ?\r\n```\r\nexec python -u ./dev/run-tests.py \"$@\"\r\n```",
        "createdAt" : "2019-07-30T01:27:46Z",
        "updatedAt" : "2019-08-07T10:42:31Z",
        "lastEditedBy" : "7e665d8b-d739-4edf-88c8-7379ff8585c2",
        "tags" : [
        ]
      },
      {
        "id" : "ed90f74f-8d14-4c90-a0ad-c6e1cd60cf26",
        "parentId" : "f464af1e-dfd8-4270-ab70-a8b6bee55fae",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "For this one, could you make a decision first like https://github.com/apache/spark/pull/25289#pullrequestreview-271749485?",
        "createdAt" : "2019-08-07T06:25:58Z",
        "updatedAt" : "2019-08-07T10:42:31Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "d26140d5ab67231c1f36578c08f9cef62a3d69b9",
    "line" : 2,
    "diffHunk" : "@@ -1,1 +-1,3 @@#!/usr/bin/env python\n\n#"
  }
]