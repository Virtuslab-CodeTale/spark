[
  {
    "id" : "9c7f39a7-b5b1-4213-b1bc-30f25967fd24",
    "prId" : 27989,
    "prUrl" : "https://github.com/apache/spark/pull/27989#pullrequestreview-379363495",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b631bf53-3cea-4914-aa6a-45451de7898c",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-19968, commit ID: f6730a70cb47ebb3df7f42209df7b076aece1093#diff-ac8844e8d791a75aaee3d0d10bfc1f2a",
        "createdAt" : "2020-03-23T12:00:59Z",
        "updatedAt" : "2020-03-23T12:00:59Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "738b704b36156e5eb606838bbb9184021602f0ec",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +30,34 @@    ConfigBuilder(\"spark.kafka.producer.cache.timeout\")\n      .doc(\"The expire time to remove the unused producers.\")\n      .version(\"2.2.1\")\n      .timeConf(TimeUnit.MILLISECONDS)\n      .createWithDefaultString(\"10m\")"
  },
  {
    "id" : "b9e5c658-479a-4ba8-984c-d1e7552503d9",
    "prId" : 27989,
    "prUrl" : "https://github.com/apache/spark/pull/27989#pullrequestreview-379363813",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5690afb4-ae4a-489f-adde-7d5b559c50a6",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-21869, commit ID: 7bff2db9ed803e05a43c2d875c1dea819d81248a#diff-ea8349d528fe8d1b0a8ffa2840ff4bcd",
        "createdAt" : "2020-03-23T12:01:28Z",
        "updatedAt" : "2020-03-23T12:01:28Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "738b704b36156e5eb606838bbb9184021602f0ec",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +38,42 @@      .doc(\"The interval of time between runs of the idle evictor thread for producer pool. \" +\n        \"When non-positive, no idle evictor thread will be run.\")\n      .version(\"3.0.0\")\n      .timeConf(TimeUnit.MILLISECONDS)\n      .createWithDefaultString(\"1m\")"
  },
  {
    "id" : "4ad9407f-12c1-4fe5-89e9-3480860103e0",
    "prId" : 27989,
    "prUrl" : "https://github.com/apache/spark/pull/27989#pullrequestreview-379364251",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "99aa6161-56cf-4b4d-b548-ed3ad3e8a990",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-27687, commit ID: efa303581ac61d6f517aacd08883da2d01530bd2#diff-ea8349d528fe8d1b0a8ffa2840ff4bcd",
        "createdAt" : "2020-03-23T12:02:07Z",
        "updatedAt" : "2020-03-23T12:02:07Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "738b704b36156e5eb606838bbb9184021602f0ec",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +46,50 @@      .doc(\"The maximum number of consumers cached. Please note it's a soft limit\" +\n        \" (check Structured Streaming Kafka integration guide for further details).\")\n      .version(\"3.0.0\")\n      .intConf\n      .createWithDefault(64)"
  },
  {
    "id" : "3a50a07f-1d2e-4e23-8aa7-2452ac9a4058",
    "prId" : 27989,
    "prUrl" : "https://github.com/apache/spark/pull/27989#pullrequestreview-379364590",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9c2e1c01-be2e-46cb-a7e0-0bffa30aae6e",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-25151, commit ID: 594c9c5a3ece0e913949c7160bb4925e5d289e44#diff-ea8349d528fe8d1b0a8ffa2840ff4bcd",
        "createdAt" : "2020-03-23T12:02:38Z",
        "updatedAt" : "2020-03-23T12:02:38Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "738b704b36156e5eb606838bbb9184021602f0ec",
    "line" : 27,
    "diffHunk" : "@@ -1,1 +53,57 @@    ConfigBuilder(\"spark.kafka.consumer.cache.jmx.enable\")\n      .doc(\"Enable or disable JMX for pools created with this configuration instance.\")\n      .version(\"3.0.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "7d562ac4-21ca-4257-917a-837932b4ee8e",
    "prId" : 27989,
    "prUrl" : "https://github.com/apache/spark/pull/27989#pullrequestreview-379364796",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5151aead-f091-412e-b5c2-524974678288",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-25151, commit ID: 594c9c5a3ece0e913949c7160bb4925e5d289e44#diff-ea8349d528fe8d1b0a8ffa2840ff4bcd",
        "createdAt" : "2020-03-23T12:02:56Z",
        "updatedAt" : "2020-03-23T12:02:57Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "738b704b36156e5eb606838bbb9184021602f0ec",
    "line" : 35,
    "diffHunk" : "@@ -1,1 +62,66 @@        \"it is eligible for eviction by the evictor. \" +\n        \"When non-positive, no consumers will be evicted from the pool due to idle time alone.\")\n      .version(\"3.0.0\")\n      .timeConf(TimeUnit.MILLISECONDS)\n      .createWithDefaultString(\"5m\")"
  },
  {
    "id" : "b6e71c98-93b9-4d1c-9739-d3ccf05e500a",
    "prId" : 27989,
    "prUrl" : "https://github.com/apache/spark/pull/27989#pullrequestreview-379365020",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9d8f939a-7415-42d9-9d4a-5d13cbba3e75",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-25151, commit ID: 594c9c5a3ece0e913949c7160bb4925e5d289e44#diff-ea8349d528fe8d1b0a8ffa2840ff4bcd",
        "createdAt" : "2020-03-23T12:03:15Z",
        "updatedAt" : "2020-03-23T12:03:15Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "738b704b36156e5eb606838bbb9184021602f0ec",
    "line" : 43,
    "diffHunk" : "@@ -1,1 +70,74 @@      .doc(\"The interval of time between runs of the idle evictor thread for consumer pool. \" +\n        \"When non-positive, no idle evictor thread will be run.\")\n      .version(\"3.0.0\")\n      .timeConf(TimeUnit.MILLISECONDS)\n      .createWithDefaultString(\"1m\")"
  },
  {
    "id" : "a61ee964-cd0f-4083-a1a1-da3cff1fb569",
    "prId" : 27989,
    "prUrl" : "https://github.com/apache/spark/pull/27989#pullrequestreview-379365241",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a4b619ab-d382-4823-9090-5ff571df42a2",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-25151, commit ID: 594c9c5a3ece0e913949c7160bb4925e5d289e44#diff-ea8349d528fe8d1b0a8ffa2840ff4bcd",
        "createdAt" : "2020-03-23T12:03:36Z",
        "updatedAt" : "2020-03-23T12:03:36Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "738b704b36156e5eb606838bbb9184021602f0ec",
    "line" : 51,
    "diffHunk" : "@@ -1,1 +79,83 @@        \"it is eligible for eviction by the evictor. \" +\n        \"When non-positive, no fetched data will be evicted from the pool due to idle time alone.\")\n      .version(\"3.0.0\")\n      .timeConf(TimeUnit.MILLISECONDS)\n      .createWithDefaultString(\"5m\")"
  },
  {
    "id" : "af3b7f84-ccc2-46e3-83b3-79e239e39ea8",
    "prId" : 27989,
    "prUrl" : "https://github.com/apache/spark/pull/27989#pullrequestreview-379365311",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "de604660-eafb-4bdd-84a5-3da271831ff1",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-25151, commit ID: 594c9c5a3ece0e913949c7160bb4925e5d289e44#diff-ea8349d528fe8d1b0a8ffa2840ff4bcd",
        "createdAt" : "2020-03-23T12:03:42Z",
        "updatedAt" : "2020-03-23T12:03:43Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "738b704b36156e5eb606838bbb9184021602f0ec",
    "line" : 59,
    "diffHunk" : "@@ -1,1 +87,91 @@      .doc(\"The interval of time between runs of the idle evictor thread for fetched data pool. \" +\n        \"When non-positive, no idle evictor thread will be run.\")\n      .version(\"3.0.0\")\n      .timeConf(TimeUnit.MILLISECONDS)\n      .createWithDefaultString(\"1m\")"
  },
  {
    "id" : "435a221d-7865-4471-ab8a-87f009b67d4b",
    "prId" : 24590,
    "prUrl" : "https://github.com/apache/spark/pull/24590#pullrequestreview-237970467",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "42652890-6e3a-4ad6-837f-9bd0c5955a55",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Thank you for new doc, @gaborgsomogyi .\r\nSince this is used since Spark 2.0.x, shall we add a little comment on the migration doc?",
        "createdAt" : "2019-05-13T16:24:36Z",
        "updatedAt" : "2019-05-14T06:20:10Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "5db668d6-8689-42c7-9c4b-7f67c731d994",
        "parentId" : "42652890-6e3a-4ad6-837f-9bd0c5955a55",
        "authorId" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "body" : "Originally I've taken a look at the related migration guide (sql-migration-guide-upgrade) but there I haven't seen any parameter rename entry. Thought it's the way because of the following:\r\n* The parameter just deprecated and will work properly in 3.0 as well\r\n* The Spark config system will give a warning message which tells what is the new parameter\r\n\r\nIf my thoughts are not correct please correct it and I'll comment on the migration doc.\r\n",
        "createdAt" : "2019-05-14T06:26:16Z",
        "updatedAt" : "2019-05-14T06:26:16Z",
        "lastEditedBy" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "tags" : [
        ]
      },
      {
        "id" : "78c79465-da07-484a-b4c6-6000681f7926",
        "parentId" : "42652890-6e3a-4ad6-837f-9bd0c5955a55",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Yeah this one doesn't require migration per se. We'd definitely mention it if the old config went away. Could mention it now too. My only hesitation is overloading the already huge 3.0 migration guide with something that doesn't actually require action now and will be flagged to the user.",
        "createdAt" : "2019-05-14T13:48:15Z",
        "updatedAt" : "2019-05-14T13:48:16Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "3e9c70d6-2634-43b8-a231-2d16176e8be6",
        "parentId" : "42652890-6e3a-4ad6-837f-9bd0c5955a55",
        "authorId" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "body" : "Same understanding.",
        "createdAt" : "2019-05-14T15:17:10Z",
        "updatedAt" : "2019-05-14T15:17:10Z",
        "lastEditedBy" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "tags" : [
        ]
      },
      {
        "id" : "0b39e760-d974-4e83-a199-26450945ab13",
        "parentId" : "42652890-6e3a-4ad6-837f-9bd0c5955a55",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Got it. Thanks!",
        "createdAt" : "2019-05-15T17:37:58Z",
        "updatedAt" : "2019-05-15T17:37:58Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "38bc96dcdbba7961ac68f09cbdb74dd719568f79",
    "line" : 3,
    "diffHunk" : "@@ -1,1 +33,37 @@      .createWithDefaultString(\"10m\")\n\n  private[kafka010] val CONSUMER_CACHE_CAPACITY =\n    ConfigBuilder(\"spark.kafka.consumer.cache.capacity\")\n      .doc(\"The maximum number of consumers cached. Please note it's a soft limit\" +"
  }
]