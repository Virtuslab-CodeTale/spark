[
  {
    "id" : "bb4b0c90-2299-47bb-986c-b32c4e5b3868",
    "prId" : 26632,
    "prUrl" : "https://github.com/apache/spark/pull/26632#pullrequestreview-323515822",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a018d7bc-ee52-4f22-8e41-ad56327049a8",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "It looks less intuitive for me. I'm not sure how much this makes us be bothered (the message is quite serious one so I'd be even OK if it's a bit bothering), but if it's really bugging, then I'd add an explicit flag to check it.",
        "createdAt" : "2019-11-25T07:22:18Z",
        "updatedAt" : "2019-11-25T07:22:18Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "8832e94a-7098-46c4-864d-bb35d28fb797",
        "parentId" : "a018d7bc-ee52-4f22-8e41-ad56327049a8",
        "authorId" : "ba6a59c9-dd69-47c0-bfc8-6de96af270ce",
        "body" : "@HeartSaVioR In micro-batch application, every mini-batch job will generate two logs per task in executor log file. Since micro-batch application is long running, there will be massive warning log.",
        "createdAt" : "2019-11-27T08:59:32Z",
        "updatedAt" : "2019-11-27T08:59:32Z",
        "lastEditedBy" : "ba6a59c9-dd69-47c0-bfc8-6de96af270ce",
        "tags" : [
        ]
      },
      {
        "id" : "b782d92e-32f8-4150-b50e-368ad5eb7397",
        "parentId" : "a018d7bc-ee52-4f22-8e41-ad56327049a8",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "The thing is, the thread should have been UninterruptibleThread; if we found the other case we should file an issue and try to fix.",
        "createdAt" : "2019-11-27T09:04:43Z",
        "updatedAt" : "2019-11-27T09:04:44Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "02db803bfff3834271c645804a85eedb9252012c",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +588,592 @@      ut.runUninterruptibly(body)\n    case _ =>\n      val warned = _consumer.isDefined\n      if (!warned) {\n        logWarning(\"KafkaDataConsumer is not running in UninterruptibleThread. \" +"
  },
  {
    "id" : "13834460-8340-4cd3-9cc7-04f20a7e3c5d",
    "prId" : 26470,
    "prUrl" : "https://github.com/apache/spark/pull/26470#pullrequestreview-316211413",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "eb49847d-8cdd-4d71-b58e-e6cd0beaf52d",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Once we add above log, comment is being redundant. I'd make the code lines also symmetric with producer side - two lines without no newline, logDebug -> invalidateKey.",
        "createdAt" : "2019-11-13T00:18:22Z",
        "updatedAt" : "2019-11-13T12:34:37Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "bd1b000e-4f3b-484e-9cc9-79946c215f86",
        "parentId" : "eb49847d-8cdd-4d71-b58e-e6cd0beaf52d",
        "authorId" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "body" : "Makes sense, changed.",
        "createdAt" : "2019-11-13T12:34:58Z",
        "updatedAt" : "2019-11-13T12:34:58Z",
        "lastEditedBy" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "tags" : [
        ]
      }
    ],
    "commit" : "d3843244c467487f255a4895feb9368c2199a8b3",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +629,633 @@      val cacheKey = new CacheKey(topicPartition, kafkaParams)\n      logDebug(s\"Invalidating key $cacheKey\")\n\n      // If this is reattempt at running the task, then invalidate cached consumer if any.\n      consumerPool.invalidateKey(cacheKey)"
  },
  {
    "id" : "972fd0ca-72ad-4946-be95-683869227269",
    "prId" : 25853,
    "prUrl" : "https://github.com/apache/spark/pull/25853#pullrequestreview-292246898",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c5a924c3-181e-4730-889a-77c301bcc794",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Let's call `fetchedDataPool.reset()` as well.",
        "createdAt" : "2019-09-21T03:25:39Z",
        "updatedAt" : "2019-11-07T13:51:09Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "3cec3f43-03ca-4df5-a5dc-43f8b7b55761",
        "parentId" : "c5a924c3-181e-4730-889a-77c301bcc794",
        "authorId" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "body" : "Added.",
        "createdAt" : "2019-09-24T08:16:13Z",
        "updatedAt" : "2019-11-07T13:51:09Z",
        "lastEditedBy" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "tags" : [
        ]
      }
    ],
    "commit" : "ebe9708c96e8127b9ef5c43fc26cff2811a79d82",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +641,645 @@\n  private[kafka010] def clear(): Unit = {\n    consumerPool.reset()\n    fetchedDataPool.reset()\n  }"
  },
  {
    "id" : "139b3b84-6809-4e81-a862-5ff9b6225955",
    "prId" : 25760,
    "prUrl" : "https://github.com/apache/spark/pull/25760#pullrequestreview-294832739",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5bcfd2d2-69c9-4fbc-99d7-1bf44c05a892",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Seems like missed to pass clusterConfig here, in `setAuthenticationConfigIfNeeded()`.",
        "createdAt" : "2019-09-28T23:57:55Z",
        "updatedAt" : "2019-10-03T06:39:49Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "699b8bff-5157-41ab-bf40-e9edc1d83efa",
        "parentId" : "5bcfd2d2-69c9-4fbc-99d7-1bf44c05a892",
        "authorId" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "body" : "Nice catch, fixed.",
        "createdAt" : "2019-09-30T08:54:32Z",
        "updatedAt" : "2019-10-03T06:39:49Z",
        "lastEditedBy" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "tags" : [
        ]
      }
    ],
    "commit" : "b1b22878d00b250e5706aeb6dc9c92f34f9b02a7",
    "line" : 35,
    "diffHunk" : "@@ -1,1 +115,119 @@  /** Create a KafkaConsumer to fetch records for `topicPartition` */\n  private def createConsumer(): KafkaConsumer[Array[Byte], Array[Byte]] = {\n    kafkaParamsWithSecurity = KafkaConfigUpdater(\"executor\", kafkaParams.asScala.toMap)\n      .setAuthenticationConfigIfNeeded(clusterConfig)\n      .build()"
  },
  {
    "id" : "4fa928b8-86ab-46f6-84c2-f6f05cd6aec0",
    "prId" : 25582,
    "prUrl" : "https://github.com/apache/spark/pull/25582#pullrequestreview-279640679",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b7977c00-e64d-4eb4-973d-2102306c4eb4",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "That's technically `private[kafka010]` as class scope so seems OK.",
        "createdAt" : "2019-08-26T14:54:47Z",
        "updatedAt" : "2019-08-26T14:55:30Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "d6602ece506469bc706f74d8b0e4eda4dc4d229b",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +79,83 @@\n  /** Reference to the internal implementation that this wrapper delegates to */\n  def internalConsumer: InternalKafkaConsumer\n}\n"
  }
]