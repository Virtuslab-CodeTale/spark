[
  {
    "id" : "bb4b0c90-2299-47bb-986c-b32c4e5b3868",
    "prId" : 26632,
    "prUrl" : "https://github.com/apache/spark/pull/26632#pullrequestreview-323515822",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a018d7bc-ee52-4f22-8e41-ad56327049a8",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "It looks less intuitive for me. I'm not sure how much this makes us be bothered (the message is quite serious one so I'd be even OK if it's a bit bothering), but if it's really bugging, then I'd add an explicit flag to check it.",
        "createdAt" : "2019-11-25T07:22:18Z",
        "updatedAt" : "2019-11-25T07:22:18Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "8832e94a-7098-46c4-864d-bb35d28fb797",
        "parentId" : "a018d7bc-ee52-4f22-8e41-ad56327049a8",
        "authorId" : "ba6a59c9-dd69-47c0-bfc8-6de96af270ce",
        "body" : "@HeartSaVioR In micro-batch application, every mini-batch job will generate two logs per task in executor log file. Since micro-batch application is long running, there will be massive warning log.",
        "createdAt" : "2019-11-27T08:59:32Z",
        "updatedAt" : "2019-11-27T08:59:32Z",
        "lastEditedBy" : "ba6a59c9-dd69-47c0-bfc8-6de96af270ce",
        "tags" : [
        ]
      },
      {
        "id" : "b782d92e-32f8-4150-b50e-368ad5eb7397",
        "parentId" : "a018d7bc-ee52-4f22-8e41-ad56327049a8",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "The thing is, the thread should have been UninterruptibleThread; if we found the other case we should file an issue and try to fix.",
        "createdAt" : "2019-11-27T09:04:43Z",
        "updatedAt" : "2019-11-27T09:04:44Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "02db803bfff3834271c645804a85eedb9252012c",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +588,592 @@      ut.runUninterruptibly(body)\n    case _ =>\n      val warned = _consumer.isDefined\n      if (!warned) {\n        logWarning(\"KafkaDataConsumer is not running in UninterruptibleThread. \" +"
  },
  {
    "id" : "13834460-8340-4cd3-9cc7-04f20a7e3c5d",
    "prId" : 26470,
    "prUrl" : "https://github.com/apache/spark/pull/26470#pullrequestreview-316211413",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "eb49847d-8cdd-4d71-b58e-e6cd0beaf52d",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Once we add above log, comment is being redundant. I'd make the code lines also symmetric with producer side - two lines without no newline, logDebug -> invalidateKey.",
        "createdAt" : "2019-11-13T00:18:22Z",
        "updatedAt" : "2019-11-13T12:34:37Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "bd1b000e-4f3b-484e-9cc9-79946c215f86",
        "parentId" : "eb49847d-8cdd-4d71-b58e-e6cd0beaf52d",
        "authorId" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "body" : "Makes sense, changed.",
        "createdAt" : "2019-11-13T12:34:58Z",
        "updatedAt" : "2019-11-13T12:34:58Z",
        "lastEditedBy" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "tags" : [
        ]
      }
    ],
    "commit" : "d3843244c467487f255a4895feb9368c2199a8b3",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +629,633 @@      val cacheKey = new CacheKey(topicPartition, kafkaParams)\n      logDebug(s\"Invalidating key $cacheKey\")\n\n      // If this is reattempt at running the task, then invalidate cached consumer if any.\n      consumerPool.invalidateKey(cacheKey)"
  },
  {
    "id" : "972fd0ca-72ad-4946-be95-683869227269",
    "prId" : 25853,
    "prUrl" : "https://github.com/apache/spark/pull/25853#pullrequestreview-292246898",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c5a924c3-181e-4730-889a-77c301bcc794",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Let's call `fetchedDataPool.reset()` as well.",
        "createdAt" : "2019-09-21T03:25:39Z",
        "updatedAt" : "2019-11-07T13:51:09Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "3cec3f43-03ca-4df5-a5dc-43f8b7b55761",
        "parentId" : "c5a924c3-181e-4730-889a-77c301bcc794",
        "authorId" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "body" : "Added.",
        "createdAt" : "2019-09-24T08:16:13Z",
        "updatedAt" : "2019-11-07T13:51:09Z",
        "lastEditedBy" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "tags" : [
        ]
      }
    ],
    "commit" : "ebe9708c96e8127b9ef5c43fc26cff2811a79d82",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +641,645 @@\n  private[kafka010] def clear(): Unit = {\n    consumerPool.reset()\n    fetchedDataPool.reset()\n  }"
  }
]