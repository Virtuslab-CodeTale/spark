[
  {
    "id" : "d755d9e2-8732-45de-aaf1-e36321a20e75",
    "prId" : 31323,
    "prUrl" : "https://github.com/apache/spark/pull/31323#pullrequestreview-576042977",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "eb0bfda4-2ca7-4b1f-9ef6-0758aad8b6a9",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@LuciferYang, can you fix this line to `Utils.tryWithResource(Source.fromFile(kdc.getKrb5conf, \"UTF-8\")) { krb5Conf  =>`? that will reduce the diff  a lot.",
        "createdAt" : "2021-01-26T04:49:54Z",
        "updatedAt" : "2021-01-26T05:44:52Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "38fa1173-70e1-4270-8d4c-82af9c410a12",
        "parentId" : "eb0bfda4-2ca7-4b1f-9ef6-0758aad8b6a9",
        "authorId" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "body" : "replace with  `Utils.tryWithResource(Source.fromFile(kdc.getKrb5conf, \"UTF-8\"))(_.getLines().toList)`, only 1 line changed",
        "createdAt" : "2021-01-26T05:46:56Z",
        "updatedAt" : "2021-01-26T05:46:56Z",
        "lastEditedBy" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "tags" : [
        ]
      }
    ],
    "commit" : "245d8446e831043a140b8c16a31a9710328a67a7",
    "line" : 3,
    "diffHunk" : "@@ -1,1 +167,171 @@   * In this method we rewrite krb5.conf to make kdc and client use the same enctypes\n   */\n  private def rewriteKrb5Conf(): Unit = {\n    val krb5Conf = Utils\n      .tryWithResource(Source.fromFile(kdc.getKrb5conf, \"UTF-8\"))(_.getLines().toList)"
  },
  {
    "id" : "8a7ccd0b-9d4a-4a62-9b1c-72c19a6d0d25",
    "prId" : 30939,
    "prUrl" : "https://github.com/apache/spark/pull/30939#pullrequestreview-558962501",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a727f98f-123b-4ba0-8bb0-36c58c0beeea",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "https://github.com/apache/kafka/pull/8931",
        "createdAt" : "2020-12-27T22:31:55Z",
        "updatedAt" : "2020-12-28T05:31:54Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "91a6ccd0ac017b7ab8122103aa183969f6e0c91d",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +575,579 @@    // ensure that logs from all replicas are deleted if delete topic is marked successful\n    assert(servers.forall(server => topicAndPartitions.forall(tp =>\n      server.getLogManager.getLog(tp).isEmpty)),\n      s\"topic $topic still exists in log manager\")\n    // ensure that topic is removed from all cleaner offsets"
  },
  {
    "id" : "92c81916-ce1f-4b3b-8169-d44f98d39df5",
    "prId" : 29386,
    "prUrl" : "https://github.com/apache/spark/pull/29386#pullrequestreview-463527416",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "63c6545e-7ba4-46fb-948f-92a767ed3c28",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "This change is required to compile.",
        "createdAt" : "2020-08-07T18:42:49Z",
        "updatedAt" : "2020-08-07T18:42:49Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "3001355177ff581e25b8cded79d4ef79a0e48200",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +396,400 @@\n  def getAllTopicsAndPartitionSize(): Seq[(String, Int)] = {\n    zkClient.getPartitionsForTopics(zkClient.getAllTopicsInCluster()).mapValues(_.size).toSeq\n  }\n"
  },
  {
    "id" : "4faea20a-d908-4432-9422-5eca76236375",
    "prId" : 26960,
    "prUrl" : "https://github.com/apache/spark/pull/26960#pullrequestreview-335080548",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "90e79bb7-fea7-41cf-bd85-bdbe3c028988",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "The return type is changed - it no longer returns Option, but includes None in its return type.",
        "createdAt" : "2019-12-20T06:01:42Z",
        "updatedAt" : "2019-12-20T15:24:58Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "4cca9d27f08f595c1e1dd8df6bdbf8024ec6e59d",
    "line" : 118,
    "diffHunk" : "@@ -1,1 +583,587 @@    // ensure that the topic-partition has been deleted from all brokers' replica managers\n    assert(servers.forall(server => topicAndPartitions.forall(tp =>\n      server.replicaManager.getPartition(tp) == HostedPartition.None)),\n      s\"topic $topic still exists in the replica manager\")\n    // ensure that logs from all replicas are deleted if delete topic is marked successful"
  },
  {
    "id" : "11e7d625-2eae-41c2-8d54-56c56dc38b11",
    "prId" : 26960,
    "prUrl" : "https://github.com/apache/spark/pull/26960#pullrequestreview-335615867",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ff69e005-6525-4d37-b0a5-510fb9740414",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "partitionState.basePartitionState no longer exists. partitionState.basePartitionState.XXX seem to be moved to partitionState.XXX, at least for these we use",
        "createdAt" : "2019-12-20T06:03:24Z",
        "updatedAt" : "2019-12-20T15:24:58Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "bef983ba-1e14-442b-8ab9-940771b4467e",
        "parentId" : "ff69e005-6525-4d37-b0a5-510fb9740414",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Got it.",
        "createdAt" : "2019-12-21T18:07:03Z",
        "updatedAt" : "2019-12-21T18:07:04Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "4cca9d27f08f595c1e1dd8df6bdbf8024ec6e59d",
    "line" : 145,
    "diffHunk" : "@@ -1,1 +626,630 @@      case Some(partitionState) =>\n        zkClient.getLeaderForPartition(new TopicPartition(topic, partition)).isDefined &&\n          Request.isValidBrokerId(partitionState.leader) &&\n          !partitionState.replicas.isEmpty\n"
  },
  {
    "id" : "84711be9-49a6-402a-aa18-f6580d7f2585",
    "prId" : 26594,
    "prUrl" : "https://github.com/apache/spark/pull/26594#pullrequestreview-322959252",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "369a6fac-c8a4-4571-884b-e09c3456ca81",
        "parentId" : null,
        "authorId" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "body" : "`refreshKrb5Config` is safe to add.",
        "createdAt" : "2019-11-26T11:13:39Z",
        "updatedAt" : "2019-12-06T04:28:15Z",
        "lastEditedBy" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "tags" : [
        ]
      },
      {
        "id" : "8a9307ab-41ff-4925-acd9-09fe5b924718",
        "parentId" : "369a6fac-c8a4-4571-884b-e09c3456ca81",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> `refreshKrb5Config` is safe to add.\r\n\r\nYea, it's necessary in this case.",
        "createdAt" : "2019-11-26T12:59:41Z",
        "updatedAt" : "2019-12-06T04:28:15Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "223533fa13551b54f3ac10b666babb379f8601a9",
    "line" : 57,
    "diffHunk" : "@@ -1,1 +208,212 @@      |  storeKey=true\n      |  useTicketCache=false\n      |  refreshKrb5Config=true\n      |  keyTab=\"${zkServerKeytabFile.getAbsolutePath()}\"\n      |  principal=\"$zkServerUser@$realm\";"
  },
  {
    "id" : "813d1a4b-cb65-48be-afd2-e0d2cf6a7760",
    "prId" : 26594,
    "prUrl" : "https://github.com/apache/spark/pull/26594#pullrequestreview-324660987",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4c5c0bc6-b9a7-424f-a84c-3a20673d47a7",
        "parentId" : null,
        "authorId" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "body" : "Maybe we can extract this functionality into a function and this way simple tests can be added.",
        "createdAt" : "2019-11-26T11:16:35Z",
        "updatedAt" : "2019-12-06T04:28:15Z",
        "lastEditedBy" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "tags" : [
        ]
      },
      {
        "id" : "98c999e0-a31b-469a-bc04-9ae12a156a87",
        "parentId" : "4c5c0bc6-b9a7-424f-a84c-3a20673d47a7",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> Maybe we can extract this functionality into a function and this way simple tests can be added.\r\n\r\nHere I want to show add `default_tkt_enctypes` and `default_tgs_enctypes` is useful and it can solve this problem. \r\n\r\nFor me, I prefer to build spark's MiniKDC like Kafka and make `krb5.conf` customize. \r\nHadoop's MiniKDC  is not so good.",
        "createdAt" : "2019-11-26T13:06:14Z",
        "updatedAt" : "2019-12-06T04:28:15Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "74dd07fa-6aab-4c3a-b243-3c57e3e828ba",
        "parentId" : "4c5c0bc6-b9a7-424f-a84c-3a20673d47a7",
        "authorId" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "body" : "> build spark's MiniKDC like Kafka\r\n\r\nYou mean implement a new SparkKDC?",
        "createdAt" : "2019-11-26T16:04:07Z",
        "updatedAt" : "2019-12-06T04:28:15Z",
        "lastEditedBy" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "tags" : [
        ]
      },
      {
        "id" : "3219ae24-7802-44de-9b9d-a6740647cec6",
        "parentId" : "4c5c0bc6-b9a7-424f-a84c-3a20673d47a7",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> You mean implement a new SparkKDC?\r\n\r\nYea, I think it's better.",
        "createdAt" : "2019-11-29T10:05:15Z",
        "updatedAt" : "2019-12-06T04:28:15Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "43bc3f50-758e-44be-b86f-490e03d936b4",
        "parentId" : "4c5c0bc6-b9a7-424f-a84c-3a20673d47a7",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> You mean implement a new SparkKDC?\r\n\r\nBetter to do this.",
        "createdAt" : "2019-11-29T10:09:43Z",
        "updatedAt" : "2019-12-06T04:28:15Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "8087e1f4-6463-4f6a-885d-411d0cfa238a",
        "parentId" : "4c5c0bc6-b9a7-424f-a84c-3a20673d47a7",
        "authorId" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "body" : "Kafka implemented this because it's not using any Hadoop deps. Here we have a different situation but I think it's not the scope of this PR. On another Jira this can be discussed...\r\n\r\nHowever my previous comment still stands. This can be tested but no tests added. I can personally imagine 2 tests (but maybe one have better idea):\r\n* `s.contains(\"libdefaults\")`\r\n* `!s.contains(\"libdefaults\")`\r\n",
        "createdAt" : "2019-11-29T11:20:07Z",
        "updatedAt" : "2019-12-06T04:28:15Z",
        "lastEditedBy" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "tags" : [
        ]
      }
    ],
    "commit" : "223533fa13551b54f3ac10b666babb379f8601a9",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +148,152 @@   */\n  private def rewriteKrb5Conf(): Unit = {\n    val krb5Conf = Source.fromFile(kdc.getKrb5conf, \"UTF-8\").getLines()\n    var rewritten = false\n    val addedConfig ="
  },
  {
    "id" : "86296147-e5d4-4479-8fca-7c99133ac6f4",
    "prId" : 26594,
    "prUrl" : "https://github.com/apache/spark/pull/26594#pullrequestreview-328028337",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "eeb8b745-f02e-4fbd-85d0-b46a4db3dee2",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Hi, @AngersZhuuuu . Maybe, I missed the discussion history.\r\nI'm wondering if we can put the config to `kdcConf` before creating `MiniKdc`? ",
        "createdAt" : "2019-12-05T16:07:54Z",
        "updatedAt" : "2019-12-06T04:28:15Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "a918f925-de2e-40f2-8353-9fdc3e9b5e86",
        "parentId" : "eeb8b745-f02e-4fbd-85d0-b46a4db3dee2",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> Hi, @AngersZhuuuu . Maybe, I missed the discussion history.\r\n> I'm wondering if we can put the config to `kdcConf` before creating `MiniKdc`?\r\n\r\nNow we use hadoop's MiniKDC, `krb5.conf` generated in MiniKDC.\r\nIf we want to put krb5conf before creating `MiniKDC`, we should build our own `MiniKDC`\r\nhttps://github.com/apache/spark/pull/26594#discussion_r350561381",
        "createdAt" : "2019-12-06T04:02:17Z",
        "updatedAt" : "2019-12-06T04:28:15Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "b1b73c65-d157-4ae2-b741-3ca2a6d23330",
        "parentId" : "eeb8b745-f02e-4fbd-85d0-b46a4db3dee2",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Got it.",
        "createdAt" : "2019-12-06T07:11:13Z",
        "updatedAt" : "2019-12-06T07:11:14Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "223533fa13551b54f3ac10b666babb379f8601a9",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +136,140 @@    val kdcConf = MiniKdc.createConf()\n    kdcConf.setProperty(MiniKdc.DEBUG, \"true\")\n    kdc = new MiniKdc(kdcConf, kdcDir)\n    kdc.start()\n    // TODO https://issues.apache.org/jira/browse/SPARK-30037"
  }
]