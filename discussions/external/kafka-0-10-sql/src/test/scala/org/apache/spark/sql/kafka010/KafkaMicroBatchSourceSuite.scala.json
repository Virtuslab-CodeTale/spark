[
  {
    "id" : "12045917-ff35-4cf7-9812-198dd96458d4",
    "prId" : 32747,
    "prUrl" : "https://github.com/apache/spark/pull/32747#pullrequestreview-688003785",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "349345e2-0e3b-4ee5-b999-72d33a549ac0",
        "parentId" : null,
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "If we have to stick with checking general exception, let's add more check on the error messages.",
        "createdAt" : "2021-06-17T16:20:23Z",
        "updatedAt" : "2021-06-17T16:35:02Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      },
      {
        "id" : "897653d1-3345-424d-8359-9bd9f97fde9e",
        "parentId" : "349345e2-0e3b-4ee5-b999-72d33a549ac0",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "In continuous mode, the error message is completely different than we expect. This is a log message I get from `exc.getMessage()`:\r\n\r\n```\r\n21/06/18 22:10:20.680 ScalaTest-run-running-KafkaContinuousSourceSuite WARN KafkaContinuousSourceSuite: \r\nIncrementalExecution was not created: The code passed to eventually never returned normally. Attempted 1794 times over 30.009420817000002 seconds. Last failure message: null equaled null.\r\n```\r\n\r\nThe actual exception is logged, but we can't intercept the original exception here, hence can't find the relevant message from the exception being intercepted.\r\n\r\n```\r\n21/06/18 22:09:53.747 stream execution thread for [id = bc35e8a2-077e-4949-b8b6-47670d1c057f, runId = d528f706-e553-4603-b022-fc0fe7cc0432] ERROR ContinuousExecution: Query [id = bc35e8a2-077e-4949-b8b6-47670d1c057f, runId = d528f706-e553-4603-b022-fc0fe7cc0432] terminated with error\r\njava.lang.AssertionError: No offset matched from request of topic-partition topic-31-4 and timestamp 1624021786438.\r\n\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderConsumer.$anonfun$readTimestampOffsets$1(KafkaOffsetReaderConsumer.scala:280)\r\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\r\n\tat scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:400)\r\n\tat scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:728)\r\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\r\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\r\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\r\n\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderConsumer.readTimestampOffsets(KafkaOffsetReaderConsumer.scala:271)\r\n\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderConsumer.$anonfun$fetchSpecificTimestampBasedOffsets$4(KafkaOffsetReaderConsumer.scala:227)\r\n\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderConsumer.$anonfun$fetchSpecificOffsets0$1(KafkaOffsetReaderConsumer.scala:305)\r\n\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderConsumer.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReaderConsumer.scala:561)\r\n\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderConsumer.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReaderConsumer.scala:594)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\r\n\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderConsumer.withRetriesWithoutInterrupt(KafkaOffsetReaderConsumer.scala:593)\r\n\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderConsumer.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReaderConsumer.scala:547)\r\n\tat org.apache.spark.util.UninterruptibleThreadRunner.runUninterruptibly(UninterruptibleThreadRunner.scala:48)\r\n\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderConsumer.partitionsAssignedToConsumer(KafkaOffsetReaderConsumer.scala:547)\r\n\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderConsumer.fetchSpecificOffsets0(KafkaOffsetReaderConsumer.scala:301)\r\n\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderConsumer.fetchSpecificTimestampBasedOffsets(KafkaOffsetReaderConsumer.scala:233)\r\n\tat org.apache.spark.sql.kafka010.KafkaContinuousStream.initialOffset(KafkaContinuousStream.scala:74)\r\n\tat org.apache.spark.sql.execution.streaming.continuous.ContinuousExecution$$anonfun$3.$anonfun$applyOrElse$4(ContinuousExecution.scala:170)\r\n...\r\n```",
        "createdAt" : "2021-06-18T13:20:17Z",
        "updatedAt" : "2021-06-18T13:39:32Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "722fa4b2-8125-465e-a117-c9acedbfefc9",
        "parentId" : "349345e2-0e3b-4ee5-b999-72d33a549ac0",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "Ah, got it.",
        "createdAt" : "2021-06-21T03:36:23Z",
        "updatedAt" : "2021-06-21T03:36:23Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "962778544ad79b3ded45ed2f510ad5cda544088a",
    "line" : 160,
    "diffHunk" : "@@ -1,1 +1692,1696 @@    // In continuous mode, the origin exception is not caught here unfortunately, so we have to\n    // stick with checking general exception instead of verifying IllegalArgumentException.\n    intercept[Exception] {\n      testStream(df)(makeSureGetOffsetCalled)\n    }"
  },
  {
    "id" : "2096f49d-35b4-4c5b-8624-0665d75d84a2",
    "prId" : 32653,
    "prUrl" : "https://github.com/apache/spark/pull/32653#pullrequestreview-675231045",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1f89428d-2be4-48ad-8cee-428ef1dcca04",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Same comments here.",
        "createdAt" : "2021-06-01T06:54:26Z",
        "updatedAt" : "2021-06-01T06:54:34Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "110e7178-84bd-451b-9547-abe70df07cee",
        "parentId" : "1f89428d-2be4-48ad-8cee-428ef1dcca04",
        "authorId" : "8b34e239-9405-49c6-8310-4dd110416932",
        "body" : "Updated",
        "createdAt" : "2021-06-03T12:41:25Z",
        "updatedAt" : "2021-06-03T12:41:25Z",
        "lastEditedBy" : "8b34e239-9405-49c6-8310-4dd110416932",
        "tags" : [
        ]
      }
    ],
    "commit" : "046d751c005376aad290d96d80478c64da8c4c29",
    "line" : 164,
    "diffHunk" : "@@ -1,1 +419,423 @@  }\n\n  test(\"compositeReadLimit\") {\n    val topic = newTopic()\n    testUtils.createTopic(topic, partitions = 3)"
  },
  {
    "id" : "77e98b21-af90-49ec-80a9-8cd40aaf7587",
    "prId" : 31944,
    "prUrl" : "https://github.com/apache/spark/pull/31944#pullrequestreview-638516665",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1d8d20fe-9663-45cc-8afb-7d3af3c3fd86",
        "parentId" : null,
        "authorId" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "body" : "Nit: -1 newline needed.",
        "createdAt" : "2021-04-15T11:42:11Z",
        "updatedAt" : "2021-05-04T23:33:00Z",
        "lastEditedBy" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "tags" : [
        ]
      },
      {
        "id" : "cd333437-4af3-4437-9491-56e7029e4b24",
        "parentId" : "1d8d20fe-9663-45cc-8afb-7d3af3c3fd86",
        "authorId" : "135e1e15-9613-4c1f-bf11-b9ec5cf2a369",
        "body" : "Removed.",
        "createdAt" : "2021-04-19T06:58:57Z",
        "updatedAt" : "2021-05-04T23:33:00Z",
        "lastEditedBy" : "135e1e15-9613-4c1f-bf11-b9ec5cf2a369",
        "tags" : [
        ]
      }
    ],
    "commit" : "2f13f28d7bee15bed657858c53dd8495ba8f1cbc",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +1298,1302 @@    )\n  }\n\n  test(\"test custom metrics - with rate limit\") {\n    import testImplicits._"
  },
  {
    "id" : "0f060594-f470-4a0e-b915-8074437b7562",
    "prId" : 29729,
    "prUrl" : "https://github.com/apache/spark/pull/29729#pullrequestreview-486831057",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "abd860af-6df0-478d-bb49-b23d9c1423e5",
        "parentId" : null,
        "authorId" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "body" : "This needed because we don't use `KafkaConsumer` and `AdminClient` doesn't provide auto topic creation. Please see doc for the rationale.",
        "createdAt" : "2020-09-11T14:03:36Z",
        "updatedAt" : "2020-12-01T07:55:03Z",
        "lastEditedBy" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "tags" : [
        ]
      }
    ],
    "commit" : "203e60c9b7abeac0464ad3acc256121a5f900e40",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +611,615 @@        // Re-create topic since Kafka auto topic creation is not supported by Spark\n        KafkaSourceSuite.globalTestUtils.deleteTopic(topic)\n        KafkaSourceSuite.globalTestUtils.createTopic(topic)\n        true\n      }"
  }
]