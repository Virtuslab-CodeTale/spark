[
  {
    "id" : "87f9ae20-c388-45c1-bd97-e87a89fac36d",
    "prId" : 26292,
    "prUrl" : "https://github.com/apache/spark/pull/26292#pullrequestreview-308356805",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3a0b35e2-bab4-4546-b5e9-a07a3a904fe4",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Copied from KafkaSinkStreamingSuite as it's only relevant to micro-batch.",
        "createdAt" : "2019-10-29T09:06:31Z",
        "updatedAt" : "2019-11-06T21:04:53Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "e4e36ed2488889be370167d25fc90f9c6f2893ed",
    "line" : 322,
    "diffHunk" : "@@ -1,1 +300,304 @@  override protected def defaultTrigger: Option[Trigger] = None\n\n  test(\"streaming - sink progress is produced\") {\n    /* ensure sink progress is correctly produced. */\n    val input = MemoryStream[String]"
  },
  {
    "id" : "7f2278e2-8f22-4aad-8107-226b03ca4657",
    "prId" : 26292,
    "prUrl" : "https://github.com/apache/spark/pull/26292#pullrequestreview-308357680",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "445fe4cc-aceb-4959-987a-bd96a20086a1",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Copied from KafkaContinuousSinkSuite. I guess it's not necessarily needed to be put here, but would like to leave it as it was.",
        "createdAt" : "2019-10-29T09:07:58Z",
        "updatedAt" : "2019-11-06T21:04:53Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "e4e36ed2488889be370167d25fc90f9c6f2893ed",
    "line" : 366,
    "diffHunk" : "@@ -1,1 +344,348 @@  override protected def defaultTrigger: Option[Trigger] = Some(Trigger.Continuous(1000))\n\n  test(\"generic - write big data with small producer buffer\") {\n    /* This test ensures that we understand the semantics of Kafka when\n    * is comes to blocking on a call to send when the send buffer is full."
  },
  {
    "id" : "438fabbb-ae4e-47f8-beab-2ad6f4750c00",
    "prId" : 26153,
    "prUrl" : "https://github.com/apache/spark/pull/26153#pullrequestreview-303854792",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "22803fd7-c677-47b6-8ed7-27ed6d22dbd0",
        "parentId" : null,
        "authorId" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "body" : "I know this copy-paste started earlier but it's a good opportunity to reduce it.\r\nAs I see only the select expression and the message is changing here, right?",
        "createdAt" : "2019-10-18T10:07:58Z",
        "updatedAt" : "2019-10-22T14:50:13Z",
        "lastEditedBy" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "tags" : [
        ]
      },
      {
        "id" : "40e7ef0d-5746-4068-8cc0-d538a9e94ad2",
        "parentId" : "22803fd7-c677-47b6-8ed7-27ed6d22dbd0",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Actually I just submitted a patch #26158 to handle refactor to old ones: depending on which one to merge first, we can apply it.",
        "createdAt" : "2019-10-18T11:31:35Z",
        "updatedAt" : "2019-10-22T14:50:13Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "d176742b-641b-4936-8903-f3bfba0bc8b7",
        "parentId" : "22803fd7-c677-47b6-8ed7-27ed6d22dbd0",
        "authorId" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "body" : "OK, started to review it...",
        "createdAt" : "2019-10-18T12:27:37Z",
        "updatedAt" : "2019-10-22T14:50:13Z",
        "lastEditedBy" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "tags" : [
        ]
      }
    ],
    "commit" : "96a8e9f24fe7446ae444621cdb5c16e8d7c86a54",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +297,301 @@      \"key attribute type must be a string or binary\"))\n\n    try {\n      ex = intercept[StreamingQueryException] {\n        /* partition field wrong type */"
  },
  {
    "id" : "b03e29b4-bf21-454f-a2b9-13925e65bc41",
    "prId" : 26153,
    "prUrl" : "https://github.com/apache/spark/pull/26153#pullrequestreview-305063352",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1fd3a69b-8925-4e7f-835b-13b9b1649d3c",
        "parentId" : null,
        "authorId" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "body" : "I was thinking a bit different test split but this is also fine. It covers all the priority scenarios, good job. In this case I agree with your view about not having `test(\"batch - partition column sets partition in kafka writes\")` since it just duplicates things. From my view we can remove it.",
        "createdAt" : "2019-10-21T10:51:37Z",
        "updatedAt" : "2019-10-22T14:50:13Z",
        "lastEditedBy" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "tags" : [
        ]
      },
      {
        "id" : "00f3371f-11be-4607-9f50-95995e61c4df",
        "parentId" : "1fd3a69b-8925-4e7f-835b-13b9b1649d3c",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Looks like this test covers everything. Great.",
        "createdAt" : "2019-10-22T09:19:14Z",
        "updatedAt" : "2019-10-22T14:50:13Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "96a8e9f24fe7446ae444621cdb5c16e8d7c86a54",
    "line" : 63,
    "diffHunk" : "@@ -1,1 +455,459 @@  }\n\n  test(\"batch - partition column and partitioner priorities\") {\n    val nrPartitions = 4\n    val topic1 = newTopic()"
  }
]