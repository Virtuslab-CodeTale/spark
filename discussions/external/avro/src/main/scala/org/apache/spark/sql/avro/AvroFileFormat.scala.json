[
  {
    "id" : "76569945-cb2a-47df-bcfd-e2d504e6e96c",
    "prId" : 30869,
    "prUrl" : "https://github.com/apache/spark/pull/30869#pullrequestreview-706417000",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cc2c9177-1ede-465e-8b48-93b96ca87972",
        "parentId" : null,
        "authorId" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "body" : "Looks like this code is directly copied from `Schema.validateName`. At minimum, we need to provide a copyright attribution here. It's a shame that the function isn't exposed publicly.\r\n\r\nRather than duplicating the validation logic here, I'm wondering if we should work around it by creating a placeholder schema with the fields from `checkFieldNames` (the types can be arbitrary) and asking Avro to parse it, and treating the field names as valid if Avro doesn't throw a `SchemaParseException`, e.g.:\r\n```\r\n        val emptyBuilder = SchemaBuilder.record(\"foo\").fields()\r\n        val fullBuilder = names.foldLeft(emptyBuilder)((builder, name) => builder.requiredInt(name))\r\n        fullBuilder.endRecord()\r\n```",
        "createdAt" : "2021-07-14T15:26:42Z",
        "updatedAt" : "2021-07-14T15:29:37Z",
        "lastEditedBy" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "tags" : [
        ]
      }
    ],
    "commit" : "706938c3c970225dfbfcd673d1e58faca4c344d0",
    "line" : 34,
    "diffHunk" : "@@ -1,1 +177,181 @@          i += 1\n        }\n      }\n    }\n  }"
  },
  {
    "id" : "2b371977-7f7c-42ca-99d4-16829d468902",
    "prId" : 24753,
    "prUrl" : "https://github.com/apache/spark/pull/24753#pullrequestreview-244585555",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ec22728c-42f1-479d-ba74-2c63008bd23b",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "haha. you know what? I believe this was from Hadoop (https://github.com/apache/hadoop/search?q=IsSplitable&unscoped_q=IsSplitable)\r\n\r\nand although it's meant to be private, `FileFormat` one is pretty used here and there. Strictly we don't need to but might be good to keep the compatibility there. ",
        "createdAt" : "2019-05-31T11:43:09Z",
        "updatedAt" : "2019-05-31T11:43:10Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "f5e70650-de3f-43b7-be2e-015be27c20ac",
        "parentId" : "ec22728c-42f1-479d-ba74-2c63008bd23b",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Yeah I recognize this typo from Hadoop. I think we should just leave it.",
        "createdAt" : "2019-05-31T11:48:58Z",
        "updatedAt" : "2019-05-31T11:48:58Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "28c50703-5320-43f3-8cd3-38427a8bbf1d",
        "parentId" : "ec22728c-42f1-479d-ba74-2c63008bd23b",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "+1 for just leaving it",
        "createdAt" : "2019-05-31T11:55:18Z",
        "updatedAt" : "2019-05-31T11:55:18Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "bed4f992-5bdf-4fb6-ab45-377de53297e0",
        "parentId" : "ec22728c-42f1-479d-ba74-2c63008bd23b",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "I just thought that Spark 3.0 would be suitable time to make such kind of changes in API otherwise the typo will stay here forever.",
        "createdAt" : "2019-05-31T12:02:24Z",
        "updatedAt" : "2019-05-31T12:02:25Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "80705fb5-5ece-46ce-a795-b515369635fb",
        "parentId" : "ec22728c-42f1-479d-ba74-2c63008bd23b",
        "authorId" : "99156419-7ce7-4671-b858-d0b5c4711f88",
        "body" : "hmm, this is breaking change every single spark app that calls this\r\n",
        "createdAt" : "2019-06-01T07:02:14Z",
        "updatedAt" : "2019-06-01T07:03:46Z",
        "lastEditedBy" : "99156419-7ce7-4671-b858-d0b5c4711f88",
        "tags" : [
        ]
      },
      {
        "id" : "9e52d0cf-7caa-4d14-b423-9f2a38f79472",
        "parentId" : "ec22728c-42f1-479d-ba74-2c63008bd23b",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Apps won't call this directly, but they might extend the class. We can make breaking changes in Spark 3, but yes I'd leave this.",
        "createdAt" : "2019-06-01T12:47:31Z",
        "updatedAt" : "2019-06-01T12:47:31Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      }
    ],
    "commit" : "42868e9507a1d9efc1146739578e5589538eb26d",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +131,135 @@  override def toString(): String = \"Avro\"\n\n  override def isSplittable(\n      sparkSession: SparkSession,\n      options: Map[String, String],"
  },
  {
    "id" : "bcc15c3a-a193-45f4-93a6-b9efb7bc8c84",
    "prId" : 24518,
    "prUrl" : "https://github.com/apache/spark/pull/24518#pullrequestreview-234671113",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "31e22f61-95e3-4fa3-b046-0efddc3c6b62",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "nit `CaseInsensitiveMap(options).contains(\"ignoreExtension\")`",
        "createdAt" : "2019-05-07T12:30:17Z",
        "updatedAt" : "2019-05-07T18:03:42Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "d467497b-b6c9-423b-ba71-83051619472c",
        "parentId" : "31e22f61-95e3-4fa3-b046-0efddc3c6b62",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "The option here is already case insensitive.",
        "createdAt" : "2019-05-07T17:57:21Z",
        "updatedAt" : "2019-05-07T18:03:42Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "d8f8420d9d3c97f96c1e09855e008ece3f275ad3",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +58,62 @@      files: Seq[FileStatus]): Option[StructType] = {\n    val conf = spark.sessionState.newHadoopConf()\n    if (options.contains(\"ignoreExtension\")) {\n      logWarning(s\"Option ${AvroOptions.ignoreExtensionKey} is deprecated. Please use the \" +\n        \"general data source option pathGlobFilter for filtering file names.\")"
  },
  {
    "id" : "912d0f2c-28a4-47fc-87b1-ac115fbea2f8",
    "prId" : 24518,
    "prUrl" : "https://github.com/apache/spark/pull/24518#pullrequestreview-339682285",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a886fc36-ac82-45cc-867b-5bb3c718d841",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "`\"ignoreExtension \"` -> `AvroOptions.ignoreExtensionKey`",
        "createdAt" : "2020-01-08T07:23:54Z",
        "updatedAt" : "2020-01-08T07:23:55Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "d8f8420d9d3c97f96c1e09855e008ece3f275ad3",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +58,62 @@      files: Seq[FileStatus]): Option[StructType] = {\n    val conf = spark.sessionState.newHadoopConf()\n    if (options.contains(\"ignoreExtension\")) {\n      logWarning(s\"Option ${AvroOptions.ignoreExtensionKey} is deprecated. Please use the \" +\n        \"general data source option pathGlobFilter for filtering file names.\")"
  }
]