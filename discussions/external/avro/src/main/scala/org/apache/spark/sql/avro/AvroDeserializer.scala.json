[
  {
    "id" : "c0c05c9c-def3-4fc9-941e-4a95fc5321b3",
    "prId" : 33413,
    "prUrl" : "https://github.com/apache/spark/pull/33413#pullrequestreview-719837270",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "460188a6-387e-470c-8ae0-15d667c99d09",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "do we have backport compatibility issues here? I think we should fail when we read AVRO long as TS NTZ type.",
        "createdAt" : "2021-07-30T04:55:57Z",
        "updatedAt" : "2021-07-30T04:55:57Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "ca7d4e39-8920-4720-8145-98c44aa87d18",
        "parentId" : "460188a6-387e-470c-8ae0-15d667c99d09",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "I think @beliefer is trying to make it consistent with the conversion between `TimestampType` and `LongType`. \r\nFor older Avro version, there are no logical types. So previously I make it convertible for `TimestampType` and `LongType`.\r\nI don't have a good idea about this either.",
        "createdAt" : "2021-08-02T03:42:11Z",
        "updatedAt" : "2021-08-02T03:42:11Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "33bc7b1d-8175-4c9a-8c44-6d2ae79c5ec7",
        "parentId" : "460188a6-387e-470c-8ae0-15d667c99d09",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "Yes. As @gengliangwang said, I think we should follows the way treat `null` as timestamp without time zone type with millisecond precision.",
        "createdAt" : "2021-08-02T04:26:40Z",
        "updatedAt" : "2021-08-02T04:26:40Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      },
      {
        "id" : "572af118-9fc4-4da3-81f4-1548b9a5c51b",
        "parentId" : "460188a6-387e-470c-8ae0-15d667c99d09",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "then can we at least update the comment? it's nothing about backward compatibility, but to keep consistent with TS LTZ.",
        "createdAt" : "2021-08-02T04:32:18Z",
        "updatedAt" : "2021-08-02T04:32:24Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "75ea9abf-c574-49a0-9416-104660475d57",
        "parentId" : "460188a6-387e-470c-8ae0-15d667c99d09",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "OK.",
        "createdAt" : "2021-08-02T06:31:57Z",
        "updatedAt" : "2021-08-02T06:31:57Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "e18b8b2b56b8f1b2ac83c0a9a8ebf3d42d37ffde",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +149,153 @@\n      case (LONG, TimestampNTZType) => avroType.getLogicalType match {\n        // For backward compatibility, if the Avro type is Long and it is not logical type\n        // (the `null` case), the value is processed as timestamp without time zone type\n        // with millisecond precision."
  },
  {
    "id" : "7c34317d-af74-49d9-8dca-164e1a8163d1",
    "prId" : 31329,
    "prUrl" : "https://github.com/apache/spark/pull/31329#pullrequestreview-576094754",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4a9bcab7-616f-492d-b991-20ba2a138b0d",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "So here we create the Decimal of precision/scale for the logical type of avro, not for Catalyst type?",
        "createdAt" : "2021-01-25T23:11:19Z",
        "updatedAt" : "2021-01-25T23:11:19Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "32dcafee-17eb-4eda-8640-0d1fc11eb824",
        "parentId" : "4a9bcab7-616f-492d-b991-20ba2a138b0d",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Yes, it should be because `createDecimal` do `Decimal(decimal.unscaledValue().longValue(), precision, scale)`.",
        "createdAt" : "2021-01-25T23:17:27Z",
        "updatedAt" : "2021-01-25T23:17:27Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "02822d0b-2da5-4d22-9d33-185763adf96c",
        "parentId" : "4a9bcab7-616f-492d-b991-20ba2a138b0d",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Will we cast the decimal value to the catalyst decimal type precision and scale? It looks risky if the value inside `InternalRow` doesn't match the data type.\r\n\r\nI'm also curious about behaviors when the precision from avro file schema is much larger than the catalyst decimal tyoe, do we truncate the value?",
        "createdAt" : "2021-01-26T05:33:05Z",
        "updatedAt" : "2021-01-26T05:33:06Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "b85e879f-925b-496d-af28-ea44690a7d8c",
        "parentId" : "4a9bcab7-616f-492d-b991-20ba2a138b0d",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "For now, we don't cast here yet. Are you suggesting it?\r\n> Will we cast the decimal value to the catalyst decimal type precision and scale? It looks risky if the value inside InternalRow doesn't match the data type.\r\n\r\nFor the following, could you give me some example? `much larger` is ambiguous to me. I can take a look.\r\n> I'm also curious about behaviors when the precision from avro file schema is much larger than the catalyst decimal tyoe, do we truncate the value?\r\n\r\nIn general, the following is the expected behavior for decimal range. \r\n```scala\r\nscala> spark.read.format(\"avro\").load(\"/tmp/avro\").show\r\n+----+\r\n|   a|\r\n+----+\r\n|3.14|\r\n+----+\r\n\r\n\r\nscala> spark.read.format(\"avro\").load(\"/tmp/avro\").printSchema\r\nroot\r\n |-- a: decimal(3,2) (nullable = true)\r\n\r\n\r\nscala> spark.read.schema(\"a DECIMAL(2, 1)\").format(\"avro\").load(\"/tmp/avro\").show()\r\n+---+\r\n|  a|\r\n+---+\r\n|3.1|\r\n+---+\r\n\r\n\r\nscala> spark.read.schema(\"a DECIMAL(4, 3)\").format(\"avro\").load(\"/tmp/avro\").show()\r\n+-----+\r\n|    a|\r\n+-----+\r\n|3.140|\r\n+-----+\r\n```",
        "createdAt" : "2021-01-26T07:47:56Z",
        "updatedAt" : "2021-01-26T07:47:56Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "9fdeb4b0-3c02-43de-a528-18156ff55c3d",
        "parentId" : "4a9bcab7-616f-492d-b991-20ba2a138b0d",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "BTW, @cloud-fan . I want to make it sure the following.\r\n1. This PR have no side-effect when the physical schema matches the catalyst schema.\r\n2. This PR fixes the incorrect result when the physical schema mismatches the catalyst schema.\r\n\r\nThis is much better than before. And, we can fix the inconsistency in the upper-layer still because we have the original decimal's precision and scale still.",
        "createdAt" : "2021-01-26T07:52:07Z",
        "updatedAt" : "2021-01-26T07:52:07Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "19b68385-0f5f-42b4-a281-a8bc9e31d323",
        "parentId" : "4a9bcab7-616f-492d-b991-20ba2a138b0d",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "```\r\nscala> spark.read.schema(\"a DECIMAL(2, 1)\").format(\"avro\").load(\"/tmp/avro\").show()\r\n+---+\r\n|  a|\r\n+---+\r\n|3.1|\r\n+---+\r\n```\r\n\r\nThis answers my question. So truncation happens, but I don't know where it happens yet.\r\n\r\nYea it's better than before, I just want to make sure there are no more correctness bugs here.",
        "createdAt" : "2021-01-26T07:56:25Z",
        "updatedAt" : "2021-01-26T07:56:25Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "d474f3555a31ec62a2336f91fb75b04654438d74",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +173,177 @@        val d = avroType.getLogicalType.asInstanceOf[LogicalTypes.Decimal]\n        val bigDecimal = decimalConversions.fromFixed(value.asInstanceOf[GenericFixed], avroType, d)\n        val decimal = createDecimal(bigDecimal, d.getPrecision, d.getScale)\n        updater.setDecimal(ordinal, decimal)\n"
  },
  {
    "id" : "ad088278-d974-4e13-94bc-4a3403d049cd",
    "prId" : 30323,
    "prUrl" : "https://github.com/apache/spark/pull/30323#pullrequestreview-528227082",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0e414405-08b5-41d8-8a8d-86f579fa06d9",
        "parentId" : null,
        "authorId" : "55608632-4c33-461c-886f-b7602b7c2ec9",
        "body" : "I'm really uncertain about this -- maybe `crete` is a thing??",
        "createdAt" : "2020-11-11T03:02:39Z",
        "updatedAt" : "2020-12-09T00:34:56Z",
        "lastEditedBy" : "55608632-4c33-461c-886f-b7602b7c2ec9",
        "tags" : [
        ]
      },
      {
        "id" : "9624f3b5-a2df-42d8-a3cb-747488fba1b6",
        "parentId" : "0e414405-08b5-41d8-8a8d-86f579fa06d9",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "LOL I'm sure it is not. Just like the POSIX function `creat` was a mistake so many years ago.",
        "createdAt" : "2020-11-11T15:43:45Z",
        "updatedAt" : "2020-12-09T00:34:56Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      }
    ],
    "commit" : "9e48d42723a52eb313d204efd737dfbead535e70",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +62,66 @@    datetimeRebaseMode, \"Avro\")\n\n  private val timestampRebaseFunc = DataSourceUtils.createTimestampRebaseFuncInRead(\n    datetimeRebaseMode, \"Avro\")\n"
  },
  {
    "id" : "12f53196-4ee8-4451-86cf-1fad0507dd38",
    "prId" : 29145,
    "prUrl" : "https://github.com/apache/spark/pull/29145#pullrequestreview-452472736",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d1abc2b8-458d-4322-8609-70c238e9596d",
        "parentId" : null,
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "Nit: add comment to explain why it is `_ => false` here",
        "createdAt" : "2020-07-21T09:42:29Z",
        "updatedAt" : "2020-07-22T14:29:30Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "41c47055-341d-467e-94f3-63415b703d64",
        "parentId" : "d1abc2b8-458d-4322-8609-70c238e9596d",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Added a comment",
        "createdAt" : "2020-07-21T13:46:12Z",
        "updatedAt" : "2020-07-22T14:29:30Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "f6b2a9ae8f0fd9426f96cd214c8dc1f2f636ef98",
    "line" : 76,
    "diffHunk" : "@@ -1,1 +185,189 @@        // Avro datasource doesn't accept filters with nested attributes. See SPARK-32328.\n        // We can always return `false` from `applyFilters` for nested records.\n        val writeRecord = getRecordWriter(avroType, st, path, applyFilters = _ => false)\n        (updater, ordinal, value) =>\n          val row = new SpecificInternalRow(st)"
  }
]