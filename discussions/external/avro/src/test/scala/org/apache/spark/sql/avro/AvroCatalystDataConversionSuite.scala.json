[
  {
    "id" : "3aa4d951-538b-4edd-9691-bb55ea9c2c3a",
    "prId" : 29145,
    "prUrl" : "https://github.com/apache/spark/pull/29145#pullrequestreview-457568943",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fd36e144-47ff-4071-b523-b6612e9f1760",
        "parentId" : null,
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "It would be great to add more rows here.",
        "createdAt" : "2020-07-29T13:13:43Z",
        "updatedAt" : "2020-07-29T13:13:43Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "9d4ae105-5c1e-4dc0-aad3-78aa5c7644e4",
        "parentId" : "fd36e144-47ff-4071-b523-b6612e9f1760",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "This test checks Avro deserializer which accepts ONE avro record and return None or Some(InternalRow). So, the test covers both cases. I am just wondering what do you mean by `more rows`?",
        "createdAt" : "2020-07-29T14:18:39Z",
        "updatedAt" : "2020-07-29T14:18:39Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "f6b2a9ae8f0fd9426f96cd214c8dc1f2f636ef98",
    "line" : 90,
    "diffHunk" : "@@ -1,1 +342,346 @@    val sqlSchema = new StructType().add(\"Age\", \"int\").add(\"Name\", \"string\")\n    val data = new GenericRecordBuilder(avroSchema)\n      .set(\"Age\", 39)\n      .set(\"Name\", \"Maxim\")\n      .build()"
  },
  {
    "id" : "469e0d45-9f38-461f-ade6-37e1ea17b9c3",
    "prId" : 26907,
    "prUrl" : "https://github.com/apache/spark/pull/26907#pullrequestreview-333264859",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "14bae0ea-44c6-495c-9f7d-660b82d2848e",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "You can do it in a bit more neater way:\r\n\r\n```scala\r\n  test(s\"array of nested schema with seed\") {\r\n    val seed = scala.util.Random.nextLong()\r\n    val rand = new scala.util.Random(seed)\r\n    val schema = StructType(\r\n      StructField(\"a\",\r\n        ArrayType(\r\n          RandomDataGenerator.randomNestedSchema(rand, 10, testingTypes), containsNull = false),\r\n          nullable = false) :: Nil\r\n    )\r\n\r\n    withClue(s\"Nested schema: $schema\\nseed: $seed\") {\r\n      val data = RandomDataGenerator.randomRow(rand, schema)\r\n      val converter = CatalystTypeConverters.createToCatalystConverter(schema)\r\n      val input = Literal.create(converter(data), schema)\r\n      roundTripTest(input)\r\n    }\r\n  }\r\n```",
        "createdAt" : "2019-12-17T12:34:23Z",
        "updatedAt" : "2020-01-03T08:12:07Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "6d0ed8b3-aae5-4ee6-b764-a538554b28d1",
        "parentId" : "14bae0ea-44c6-495c-9f7d-660b82d2848e",
        "authorId" : "5d6473f1-5b91-44f3-b617-3e17eba22aab",
        "body" : "Thanks, updated the test like that.",
        "createdAt" : "2019-12-17T13:17:32Z",
        "updatedAt" : "2020-01-03T08:12:07Z",
        "lastEditedBy" : "5d6473f1-5b91-44f3-b617-3e17eba22aab",
        "tags" : [
        ]
      }
    ],
    "commit" : "e52c1ea550033404a322c461b3ee34ccd282d63f",
    "line" : 35,
    "diffHunk" : "@@ -1,1 +151,155 @@      roundTripTest(input)\n    }\n  }\n\n  test(\"read int as string\") {"
  },
  {
    "id" : "0b754311-8da2-4705-a041-75a8889650fd",
    "prId" : 26907,
    "prUrl" : "https://github.com/apache/spark/pull/26907#pullrequestreview-333265982",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b3b48892-a8f3-4b9e-9945-8487b9a9d227",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "nit, I am sure this style is correct:\r\n\r\n```scala\r\n    val schema = StructType(\r\n      StructField(\"a\",\r\n        ArrayType(\r\n          RandomDataGenerator.randomNestedSchema(rand, 10, testingTypes), containsNull = false),\r\n          nullable = false) :: Nil\r\n    )\r\n```",
        "createdAt" : "2019-12-17T13:19:23Z",
        "updatedAt" : "2020-01-03T08:12:07Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "e52c1ea550033404a322c461b3ee34ccd282d63f",
    "line" : 27,
    "diffHunk" : "@@ -1,1 +143,147 @@        nullable = false\n      ) :: Nil\n    )\n\n    withClue(s\"Schema: $schema\\nseed: $seed\") {"
  },
  {
    "id" : "2cba4b80-916b-4588-abfa-810e7aae1e44",
    "prId" : 25419,
    "prUrl" : "https://github.com/apache/spark/pull/25419#pullrequestreview-274044775",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9815c74b-2e75-45a1-a80a-e3d9b059b0da",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Also, if we have the default value, we don't need to change line 41 and 46.",
        "createdAt" : "2019-08-12T19:23:41Z",
        "updatedAt" : "2019-08-13T05:29:09Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "5ef52fc3-0cf2-404e-bb17-2fc42729496f",
        "parentId" : "9815c74b-2e75-45a1-a80a-e3d9b059b0da",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "See my comment in https://github.com/apache/spark/pull/25419#discussion_r313206434",
        "createdAt" : "2019-08-13T03:19:55Z",
        "updatedAt" : "2019-08-13T05:29:09Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "6d7520bf991d3002f246c3c603cbe4edd3ee2143",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +44,48 @@\n  protected def checkUnsupportedRead(data: Literal, schema: String): Unit = {\n    val binary = CatalystDataToAvro(data, None)\n    intercept[Exception] {\n      AvroDataToCatalyst(binary, schema, Map(\"mode\" -> \"FAILFAST\")).eval()"
  },
  {
    "id" : "deaae1cc-d2d5-46b8-a647-23092a7c64d0",
    "prId" : 25419,
    "prUrl" : "https://github.com/apache/spark/pull/25419#pullrequestreview-274050595",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "421165eb-8637-42f1-aaa7-5e52d7d86815",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "In this PR, `CatalystDataToAvro` ignores the given scheme in case of `None`, doesn't it? For me, this error seems to come from `AvroDataToCatalyst` instead of `CatalystDataToAvro`.",
        "createdAt" : "2019-08-12T19:31:18Z",
        "updatedAt" : "2019-08-13T05:29:09Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "0c05f2cc-5f40-4b85-833c-da7a3881b26a",
        "parentId" : "421165eb-8637-42f1-aaa7-5e52d7d86815",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "If this error comes from `AvroDataToCatalyst`, this test coverage is misleading. For example, we had better have a test coverage for\r\n- a test whether `CatalystDataToAvro(data, None)` successfully ignores `None` without any exception.\r\n- a test whether `CatalystDataToAvro(data, \"\")` fails with that error message (?)\r\n\r\nHow do you think about that, @gengliangwang ?",
        "createdAt" : "2019-08-12T19:34:40Z",
        "updatedAt" : "2019-08-13T05:29:09Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "64b0481d-1e26-419f-aa14-c2ee9a230ef6",
        "parentId" : "421165eb-8637-42f1-aaa7-5e52d7d86815",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "Here `AvroDataToCatalyst` is just to check the Avro schema of `CatalystDataToAvro`.\r\n1. When `jsonFormatSchema` is provided in `CatalystDataToAvro`, the output Avro schema is `enum` type, and we validate it with `AvroDataToCatalyst`. This proves that the provided schema works.\r\n2. When the `jsonFormatSchema` is None, the output Avro schema is `string` type, and it can't be parsed as `enum` type.\r\n\r\nI will change the order of the two checks in the case and add a new test case for invalid user-specified schema ",
        "createdAt" : "2019-08-13T03:34:23Z",
        "updatedAt" : "2019-08-13T05:29:09Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "3f23bad7-f0e7-42a3-a07e-1c2d72ba6660",
        "parentId" : "421165eb-8637-42f1-aaa7-5e52d7d86815",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "+1. Thanks, @gengliangwang .",
        "createdAt" : "2019-08-13T03:54:30Z",
        "updatedAt" : "2019-08-13T05:29:09Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "6d7520bf991d3002f246c3c603cbe4edd3ee2143",
    "line" : 47,
    "diffHunk" : "@@ -1,1 +229,233 @@        options = Map.empty).eval()\n    }.getMessage\n    assert(message.contains(\"Malformed records are detected in record parsing.\"))\n\n    checkEvaluation("
  }
]