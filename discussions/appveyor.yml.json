[
  {
    "id" : "64e0906f-04a8-4231-81be-7945a11cb5bf",
    "prId" : 29251,
    "prUrl" : "https://github.com/apache/spark/pull/29251#pullrequestreview-455414493",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ffded24d-4ccf-41c3-ae0c-6e1565c9e4db",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "I piggy-backed this because I want to make AppVeyor success in this PR.",
        "createdAt" : "2020-07-27T00:56:42Z",
        "updatedAt" : "2020-07-27T00:56:42Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "395ef0bae5a8138b42fe441018f87e1ca3957f93",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +61,65 @@  R_REMOTES_NO_ERRORS_FROM_WARNINGS: true\n  # AppVeyor doesn't have python3 yet\n  PYSPARK_PYTHON: python\n\ntest_script:"
  },
  {
    "id" : "1d183b25-b2fe-400d-91d9-6537b56d3697",
    "prId" : 29040,
    "prUrl" : "https://github.com/apache/spark/pull/29040#pullrequestreview-444848890",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "186656ab-2a94-4d79-9b03-f07c96d4e761",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Can we upgrade to @steveloughran 's Hadoop 3.0 instead of using Hadoop 2.7.1 here?\r\n- https://github.com/apache/spark/blob/master/dev/appveyor-install-dependencies.ps1#L98\r\n```\r\n# ========================== Hadoop bin package\r\n# This must match the version at https://github.com/steveloughran/winutils/tree/master/hadoop-2.7.1\r\n$hadoopVer = \"2.7.1\"\r\n```\r\n\r\nHe already provides Hadoop 3 also.\r\n- https://github.com/steveloughran/winutils/tree/master/hadoop-3.0.0/bin",
        "createdAt" : "2020-07-08T14:43:56Z",
        "updatedAt" : "2020-07-08T14:46:02Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "9ca68da7-35b4-4293-8222-989d96a02106",
        "parentId" : "186656ab-2a94-4d79-9b03-f07c96d4e761",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Ah, sure. Let me try!",
        "createdAt" : "2020-07-08T14:50:13Z",
        "updatedAt" : "2020-07-08T14:50:14Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "f2bf31a3-4f0c-48ed-97fb-658b76b37694",
        "parentId" : "186656ab-2a94-4d79-9b03-f07c96d4e761",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I made a separate PR for that https://github.com/apache/spark/pull/29042. If that passes the tests, we can merge that one and close this.",
        "createdAt" : "2020-07-08T14:54:25Z",
        "updatedAt" : "2020-07-08T14:54:26Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "b4d1c6cebe51fb192c92bbb435ab9443c9f24762",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +50,54 @@  # See SPARK-28759.\n  # Ideally we should check the tests related to Hive in SparkR as well (SPARK-31745).\n  - cmd: mvn -DskipTests -Phadoop-2.7 -Psparkr -Djna.nosys=true package\n\nenvironment:"
  }
]