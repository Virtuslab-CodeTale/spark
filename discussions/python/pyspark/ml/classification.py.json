[
  {
    "id" : "d64ee47a-4eb3-4ef9-93fb-55bf4785364c",
    "prId" : 32245,
    "prUrl" : "https://github.com/apache/spark/pull/32245#pullrequestreview-639521093",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9e67d255-895a-4fbe-8db5-8b46bdeae86c",
        "parentId" : null,
        "authorId" : "d345f781-1a19-4d7a-abe4-e55ff1851cbf",
        "body" : "Can we just cast the `accColName` column (type: `ArrayType(DoubleType())`) to `VectorUDT()` here?\r\n\r\n```suggestion\r\n            aggregatedDataset = aggregatedDataset.withColumn(\r\n                self.getRawPredictionCol(), aggregatedDataset[accColName].cast(VectorUDT()))\r\n```",
        "createdAt" : "2021-04-20T03:08:05Z",
        "updatedAt" : "2021-04-21T03:49:41Z",
        "lastEditedBy" : "d345f781-1a19-4d7a-abe4-e55ff1851cbf",
        "tags" : [
        ]
      },
      {
        "id" : "9908711d-523b-4600-9c05-a5886d6bab6a",
        "parentId" : "9e67d255-895a-4fbe-8db5-8b46bdeae86c",
        "authorId" : "7e665d8b-d739-4edf-88c8-7379ff8585c2",
        "body" : "We should use `udf(func, VectorUDT())`.",
        "createdAt" : "2021-04-20T05:33:38Z",
        "updatedAt" : "2021-04-21T03:49:41Z",
        "lastEditedBy" : "7e665d8b-d739-4edf-88c8-7379ff8585c2",
        "tags" : [
        ]
      },
      {
        "id" : "af093c4b-ea85-41ec-9dbb-9ce8117972cd",
        "parentId" : "9e67d255-895a-4fbe-8db5-8b46bdeae86c",
        "authorId" : "7e665d8b-d739-4edf-88c8-7379ff8585c2",
        "body" : "@HyukjinKwon \r\nI want to know if no udf return type specified, how does the return type inferring work ? Check all rows udf return type ?\r\nThe master code failed in some cases and the return column type in schema become \"String\".",
        "createdAt" : "2021-04-20T05:34:53Z",
        "updatedAt" : "2021-04-21T03:49:41Z",
        "lastEditedBy" : "7e665d8b-d739-4edf-88c8-7379ff8585c2",
        "tags" : [
        ]
      },
      {
        "id" : "ede82f19-0729-43f8-a009-e864e4addd27",
        "parentId" : "9e67d255-895a-4fbe-8db5-8b46bdeae86c",
        "authorId" : "d345f781-1a19-4d7a-abe4-e55ff1851cbf",
        "body" : "Got it.",
        "createdAt" : "2021-04-20T05:38:45Z",
        "updatedAt" : "2021-04-21T03:49:41Z",
        "lastEditedBy" : "d345f781-1a19-4d7a-abe4-e55ff1851cbf",
        "tags" : [
        ]
      }
    ],
    "commit" : "ed26d2cef4d321b0c5fee7a2a851f9535beb12c9",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +3154,3158 @@            rawPredictionUDF = udf(func, VectorUDT())\n            aggregatedDataset = aggregatedDataset.withColumn(\n                self.getRawPredictionCol(), rawPredictionUDF(aggregatedDataset[accColName]))\n\n        if self.getPredictionCol():"
  },
  {
    "id" : "5710567c-3c59-4715-8051-481a77397f87",
    "prId" : 32124,
    "prUrl" : "https://github.com/apache/spark/pull/32124#pullrequestreview-637254151",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a88d29e9-c44f-4074-940f-d7481d5c6158",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Just checking that these _should_ change for reasons you highlighted in the R comments above? these are more correct answers?",
        "createdAt" : "2021-04-15T13:56:15Z",
        "updatedAt" : "2021-04-15T13:56:15Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "d14051af-a51f-40ab-be07-05c6b6de2c37",
        "parentId" : "a88d29e9-c44f-4074-940f-d7481d5c6158",
        "authorId" : "685f805b-e4fa-4f21-b066-58afcecf9ce6",
        "body" : "Firstly, this dataset only contains two instances, so I think the result maybe not reliable:\r\n\r\nR:\r\n```r\r\n> library(e1071)\r\n> label <- factor((c(1.0, 0.0)))\r\n> features <- as.matrix(data.frame(c(1.0, 1.0), c(1.0, 2.0), c(1.0, 3.0)))\r\n> C <- 2.0 / 2 / 0.01\r\n> model <- svm(features, label, type='C', kernel='linear', cost=C, scale=F, tolerance=1e-4)\r\n> w <- -t(model$coefs) %*% model$SV\r\n> w\r\n     c.1..1. c.1..2. c.1..3.\r\n[1,]       0     0.4     0.8\r\n> model$rho\r\n[1] -2.2\r\n> predict(model, features)\r\n1 2 \r\n1 0 \r\nLevels: 0 1\r\n```\r\n\r\n\r\nmaster:\r\n```\r\n>>> df = sc.parallelize([Row(label=1.0, features=Vectors.dense(1.0, 1.0, 1.0)), Row(label=0.0, features=Vectors.dense(1.0, 2.0, 3.0))]).toDF()\r\n>>> svm = LinearSVC()\r\n>>> svm.setRegParam(0.01)\r\nLinearSVC_c0eb3d7e4ecb\r\n>>> model = svm.fit(df)\r\n21/04/16 09:17:11 ERROR OWLQN: Failure! Resetting history: breeze.optimize.NaNHistory: \r\n>>> model.summary().objectiveHistory\r\n[1.0, 0.8483595765125, 0.7130226960596805, 0.6959063640335401, 0.6931614154979393, 0.677666741843437, 0.5879414534831688, 0.0930620718342175, 0.09048677587921486, 0.07102532249029628, 0.04374349680010344, 0.023954120035139716, 0.023830521501263628, 0.02320692346265251, 0.0215105944133274, 0.010552738391597573, 0.01051515768008, 0.009931999904022892, 0.008750399708406655, 0.0069553439215890535, 0.006868546522886791, 0.006515326943672154, 0.006347452798458171, 0.006187248462236068, 0.006164709258006091, 0.006053452700831725, 0.0060260493716176936, 0.005887552134192266, 0.005887136407436091, 0.005880225305895262, 0.005859805832943488, 0.005843641492521756, 0.005843641492521756, 0.00584486187964547, 0.005843267361152545, 0.005843241969094577, 0.005843234493660629, 0.0058432258624429665, 0.005843208081880606, 0.005843193072290253, 0.005843171629631225, 0.005843141001368335, 0.005843114614745189, 0.005843059249359493, 0.005843017763481445, 0.005842944768449553, 0.005842902553628102, 0.005842759959850206, 0.0058426880723804935, 0.0058424738985869045, 0.005842333714732286, 0.005842029622612579, 0.005841764628607474, 0.005841335619271196, 0.005840857505437478, 0.005840248258184164, 0.0058394259571752215, 0.005838527516717635, 0.00583821354404327, 0.005837212690021832, 0.005837208597515555, 0.005836810329205446, 0.005836776694348312, 0.00583640540121474, 0.00583635673616978, 0.005836167737106047, 0.005836194033268012, 0.00583616729576746, 0.0058361672269734615, 0.0058392815360454285, 0.005837679815125598, 0.005836401523490996, 0.005837908002584265, 0.005836192352808301, 0.005836670387508997, 0.005836202175777219, 0.005836200250798619, 0.005838876113322196]\r\n>>> model.summary().totalIterations\r\n77\r\n>>> model.transform(df).show()\r\n+-----+-------------+--------------------+----------+\r\n|label|     features|       rawPrediction|prediction|\r\n+-----+-------------+--------------------+----------+\r\n|  1.0|[1.0,1.0,1.0]|[-1.0000039068924...|       1.0|\r\n|  0.0|[1.0,2.0,3.0]|[0.99999460575938...|       0.0|\r\n+-----+-------------+--------------------+----------+\r\n```\r\n\r\n\r\nthis PR:\r\n```\r\n>>> from pyspark.sql import Row\r\n>>> from pyspark.ml.linalg import Vectors\r\n>>> from pyspark.ml.classification import LinearSVC\r\n>>> \r\n>>> df = sc.parallelize([Row(label=1.0, features=Vectors.dense(1.0, 1.0, 1.0)), Row(label=0.0, features=Vectors.dense(1.0, 2.0, 3.0))]).toDF()\r\n>>> svm = LinearSVC()\r\n>>> svm.setRegParam(0.01)\r\nLinearSVC_09863ffadcb7\r\n>>> model = svm.fit(df)\r\n>>> model.summary().objectiveHistory\r\n[1.0, 0.02084985122275127, 0.013524781160976402, 0.007607689403049228, 0.005824637199209566, 0.005323658700671529, 0.01838013981356318, 0.005089779695828527, 0.009036497849295026, 0.005023675663196612, 0.006332321775223654, 0.005004910245203961, 0.005002395070241166, 0.005000981376776186, 0.005042760733060569, 0.005000267637684085, 0.005013305173582782, 0.005000064609216319, 0.005004925699118321, 0.0050000068554171585, 0.005000088146444176, 0.005000002943794836, 0.005000000740400182, 0.005001814975687912, 0.005000001493803021, 0.0050001206863792644, 0.005000000123809999, 0.005000005933099882, 0.005000000031021147, 0.005000002103382519, 0.005000000004626093, 0.005000000001086049, 0.005001888167096368, 0.005000000754488572, 0.005000000108570436, 0.005001877526710762, 0.005000000861973192, 0.005000000216054848, 0.005001866885777133]\r\n>>> model.summary().totalIterations\r\n38\r\n>>> model.transform(df).show()\r\n+-----+-------------+--------------------+----------+\r\n|label|     features|       rawPrediction|prediction|\r\n+-----+-------------+--------------------+----------+\r\n|  1.0|[1.0,1.0,1.0]|[-0.9999981142568...|       1.0|\r\n|  0.0|[1.0,2.0,3.0]|[0.99999811425680...|       0.0|\r\n+-----+-------------+--------------------+----------+\r\n```\r\n\r\nunfortunately, spark's solutions are not like R's, but between the two solution, this PR convergen faster than existing one (which has a `OWLQN: Failure` warning)\r\n",
        "createdAt" : "2021-04-16T01:22:44Z",
        "updatedAt" : "2021-04-16T01:22:45Z",
        "lastEditedBy" : "685f805b-e4fa-4f21-b066-58afcecf9ce6",
        "tags" : [
        ]
      }
    ],
    "commit" : "5857a52f9cb9fe787b22dfdfd647e04b801224db",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +572,576 @@    0.0\n    >>> model.coefficients\n    DenseVector([0.0, -1.0319, -0.5159])\n    >>> model.intercept\n    2.579645978780695"
  },
  {
    "id" : "04f3e8d1-92c3-4b68-a34d-372c60b093c8",
    "prId" : 29616,
    "prUrl" : "https://github.com/apache/spark/pull/29616#pullrequestreview-480265270",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1779ebb3-50bf-44ea-8fad-5e78ca9bf9d9",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Ohhhh sorry it was my mistake. It doesn't crash because `uuid ` is imported via `from pyspark.ml.util import *`.",
        "createdAt" : "2020-09-02T01:06:56Z",
        "updatedAt" : "2020-09-02T01:06:56Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "d1575a669b7d32843b4cf24bdec45a1e0a4b9695",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +18,22 @@import operator\nimport sys\nimport uuid\n\nfrom multiprocessing.pool import ThreadPool"
  },
  {
    "id" : "817d275b-624b-40a9-9600-8323a12d6914",
    "prId" : 29563,
    "prUrl" : "https://github.com/apache/spark/pull/29563#pullrequestreview-477048152",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2b084423-da02-42f5-a5c2-395ae79057c3",
        "parentId" : null,
        "authorId" : "e29a7794-ac2e-4e6f-a690-737e83e1bace",
        "body" : "Both `sys` and `uuid` are used in the class, but aren't being imported.\r\n\r\nuuid: https://github.com/apache/spark/blob/master/python/pyspark/ml/classification.py#L3027\r\nsys: https://github.com/apache/spark/blob/master/python/pyspark/ml/classification.py#L3421",
        "createdAt" : "2020-08-27T20:14:59Z",
        "updatedAt" : "2020-08-29T18:33:37Z",
        "lastEditedBy" : "e29a7794-ac2e-4e6f-a690-737e83e1bace",
        "tags" : [
        ]
      }
    ],
    "commit" : "06480a7b4e8c5106ac7f7ea0fa9cecd6ea09e0bb",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +17,21 @@\nimport operator\nimport sys\nimport uuid\nimport warnings"
  },
  {
    "id" : "ee0ed29e-0553-45aa-829d-d8d4426b8298",
    "prId" : 29563,
    "prUrl" : "https://github.com/apache/spark/pull/29563#pullrequestreview-480268455",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fc1ccfd9-418a-47fb-86af-58dbd668ea01",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@Fokko, do you mind opening a minor PR to add this `uuid` to all other branches - looks like it wasn't there from the very first place https://github.com/Fokko/spark/commit/90b46e014a60069bd18754b02fce056d8f4d1b3e? \r\n\r\nI checked other occurrences fixed here, and other ones look fine to not fix in other branches.",
        "createdAt" : "2020-08-31T02:30:20Z",
        "updatedAt" : "2020-08-31T02:30:20Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "b340f5d4-5115-4f7b-8929-a2b494a525c2",
        "parentId" : "fc1ccfd9-418a-47fb-86af-58dbd668ea01",
        "authorId" : "e29a7794-ac2e-4e6f-a690-737e83e1bace",
        "body" : "Sure, I've created two PR's:\r\n\r\n- 3.0 branch: https://github.com/apache/spark/pull/29615\r\n- 2.4 branch: https://github.com/apache/spark/pull/29616",
        "createdAt" : "2020-09-01T19:16:54Z",
        "updatedAt" : "2020-09-01T19:16:55Z",
        "lastEditedBy" : "e29a7794-ac2e-4e6f-a690-737e83e1bace",
        "tags" : [
        ]
      },
      {
        "id" : "5698e4e3-3d69-40a7-b504-8f27d71fe441",
        "parentId" : "fc1ccfd9-418a-47fb-86af-58dbd668ea01",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Oh @Fokko sorry I misread. It was fine because we're using wildcard imports at `from pyspark.ml.util import *` and `util` imports `uuid`. So it didn't crash before ..  Let's just don't port this back alone :-)..",
        "createdAt" : "2020-09-02T01:08:32Z",
        "updatedAt" : "2020-09-02T01:08:32Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "06480a7b4e8c5106ac7f7ea0fa9cecd6ea09e0bb",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +18,22 @@import operator\nimport sys\nimport uuid\nimport warnings\nfrom abc import ABCMeta, abstractmethod, abstractproperty"
  },
  {
    "id" : "7f2e4357-d8c2-4e6f-b254-79f2c81104e5",
    "prId" : 29250,
    "prUrl" : "https://github.com/apache/spark/pull/29250#pullrequestreview-456046814",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6a116b5e-a0f5-4663-b372-8d37f17d9a7b",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@huaxingao, are these going to be documented? Might be good to list at `pyspark.ml.rst` too if so. The documentation PR (https://github.com/apache/spark/pull/29188) was merged. ",
        "createdAt" : "2020-07-27T08:55:30Z",
        "updatedAt" : "2020-07-27T18:48:58Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "d853597e-15c1-41e8-8d89-bc9c39f0e7ac",
        "parentId" : "6a116b5e-a0f5-4663-b372-8d37f17d9a7b",
        "authorId" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "body" : "@HyukjinKwon updated `pyspark.ml.rst`. Thanks!",
        "createdAt" : "2020-07-27T18:50:44Z",
        "updatedAt" : "2020-07-27T18:50:45Z",
        "lastEditedBy" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "tags" : [
        ]
      }
    ],
    "commit" : "328797cac590fff0db13848ef8e7ab3461817b45",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +53,57 @@           'MultilayerPerceptronClassifier', 'MultilayerPerceptronClassificationModel',\n           'MultilayerPerceptronClassificationSummary',\n           'MultilayerPerceptronClassificationTrainingSummary',\n           'OneVsRest', 'OneVsRestModel',\n           'FMClassifier', 'FMClassificationModel', 'FMClassificationSummary',"
  },
  {
    "id" : "40c1b98f-e44b-4c93-af73-5059af1fe4c9",
    "prId" : 26575,
    "prUrl" : "https://github.com/apache/spark/pull/26575#pullrequestreview-318926681",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0cd2b209-ee99-49c8-b7db-c2356be849b7",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Does this need any additional doc changes in Pyspark to show the new type that's allowed, or a doctest example?",
        "createdAt" : "2019-11-18T14:56:15Z",
        "updatedAt" : "2019-11-19T06:50:18Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "c6b73e53-551d-4279-8701-b03800fb8349",
        "parentId" : "0cd2b209-ee99-49c8-b7db-c2356be849b7",
        "authorId" : "685f805b-e4fa-4f21-b066-58afcecf9ce6",
        "body" : "@srowen It seem that we do not need to make other doc changes. The getter and setter has no doc about supported options, I guess adding a doctest is enough.",
        "createdAt" : "2019-11-19T10:24:12Z",
        "updatedAt" : "2019-11-19T10:24:13Z",
        "lastEditedBy" : "685f805b-e4fa-4f21-b066-58afcecf9ce6",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad36b5be386300cf259f30f87d9ef480d4020bf9",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +1914,1918 @@    the model's coefficients. The inventors of Complement NB show empirically that the parameter\n    estimates for CNB are more stable than those for Multinomial NB. Like Multinomial NB, the\n    input feature values for Complement NB must be nonnegative.\n    Since 3.0.0, it also supports Gaussian NB\n    <https://en.wikipedia.org/wiki/Naive_Bayes_classifier#Gaussian_naive_Bayes>`_."
  }
]