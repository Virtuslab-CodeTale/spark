[
  {
    "id" : "d64ee47a-4eb3-4ef9-93fb-55bf4785364c",
    "prId" : 32245,
    "prUrl" : "https://github.com/apache/spark/pull/32245#pullrequestreview-639521093",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9e67d255-895a-4fbe-8db5-8b46bdeae86c",
        "parentId" : null,
        "authorId" : "d345f781-1a19-4d7a-abe4-e55ff1851cbf",
        "body" : "Can we just cast the `accColName` column (type: `ArrayType(DoubleType())`) to `VectorUDT()` here?\r\n\r\n```suggestion\r\n            aggregatedDataset = aggregatedDataset.withColumn(\r\n                self.getRawPredictionCol(), aggregatedDataset[accColName].cast(VectorUDT()))\r\n```",
        "createdAt" : "2021-04-20T03:08:05Z",
        "updatedAt" : "2021-04-21T03:49:41Z",
        "lastEditedBy" : "d345f781-1a19-4d7a-abe4-e55ff1851cbf",
        "tags" : [
        ]
      },
      {
        "id" : "9908711d-523b-4600-9c05-a5886d6bab6a",
        "parentId" : "9e67d255-895a-4fbe-8db5-8b46bdeae86c",
        "authorId" : "7e665d8b-d739-4edf-88c8-7379ff8585c2",
        "body" : "We should use `udf(func, VectorUDT())`.",
        "createdAt" : "2021-04-20T05:33:38Z",
        "updatedAt" : "2021-04-21T03:49:41Z",
        "lastEditedBy" : "7e665d8b-d739-4edf-88c8-7379ff8585c2",
        "tags" : [
        ]
      },
      {
        "id" : "af093c4b-ea85-41ec-9dbb-9ce8117972cd",
        "parentId" : "9e67d255-895a-4fbe-8db5-8b46bdeae86c",
        "authorId" : "7e665d8b-d739-4edf-88c8-7379ff8585c2",
        "body" : "@HyukjinKwon \r\nI want to know if no udf return type specified, how does the return type inferring work ? Check all rows udf return type ?\r\nThe master code failed in some cases and the return column type in schema become \"String\".",
        "createdAt" : "2021-04-20T05:34:53Z",
        "updatedAt" : "2021-04-21T03:49:41Z",
        "lastEditedBy" : "7e665d8b-d739-4edf-88c8-7379ff8585c2",
        "tags" : [
        ]
      },
      {
        "id" : "ede82f19-0729-43f8-a009-e864e4addd27",
        "parentId" : "9e67d255-895a-4fbe-8db5-8b46bdeae86c",
        "authorId" : "d345f781-1a19-4d7a-abe4-e55ff1851cbf",
        "body" : "Got it.",
        "createdAt" : "2021-04-20T05:38:45Z",
        "updatedAt" : "2021-04-21T03:49:41Z",
        "lastEditedBy" : "d345f781-1a19-4d7a-abe4-e55ff1851cbf",
        "tags" : [
        ]
      }
    ],
    "commit" : "ed26d2cef4d321b0c5fee7a2a851f9535beb12c9",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +3154,3158 @@            rawPredictionUDF = udf(func, VectorUDT())\n            aggregatedDataset = aggregatedDataset.withColumn(\n                self.getRawPredictionCol(), rawPredictionUDF(aggregatedDataset[accColName]))\n\n        if self.getPredictionCol():"
  },
  {
    "id" : "5710567c-3c59-4715-8051-481a77397f87",
    "prId" : 32124,
    "prUrl" : "https://github.com/apache/spark/pull/32124#pullrequestreview-637254151",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a88d29e9-c44f-4074-940f-d7481d5c6158",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Just checking that these _should_ change for reasons you highlighted in the R comments above? these are more correct answers?",
        "createdAt" : "2021-04-15T13:56:15Z",
        "updatedAt" : "2021-04-15T13:56:15Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "d14051af-a51f-40ab-be07-05c6b6de2c37",
        "parentId" : "a88d29e9-c44f-4074-940f-d7481d5c6158",
        "authorId" : "685f805b-e4fa-4f21-b066-58afcecf9ce6",
        "body" : "Firstly, this dataset only contains two instances, so I think the result maybe not reliable:\r\n\r\nR:\r\n```r\r\n> library(e1071)\r\n> label <- factor((c(1.0, 0.0)))\r\n> features <- as.matrix(data.frame(c(1.0, 1.0), c(1.0, 2.0), c(1.0, 3.0)))\r\n> C <- 2.0 / 2 / 0.01\r\n> model <- svm(features, label, type='C', kernel='linear', cost=C, scale=F, tolerance=1e-4)\r\n> w <- -t(model$coefs) %*% model$SV\r\n> w\r\n     c.1..1. c.1..2. c.1..3.\r\n[1,]       0     0.4     0.8\r\n> model$rho\r\n[1] -2.2\r\n> predict(model, features)\r\n1 2 \r\n1 0 \r\nLevels: 0 1\r\n```\r\n\r\n\r\nmaster:\r\n```\r\n>>> df = sc.parallelize([Row(label=1.0, features=Vectors.dense(1.0, 1.0, 1.0)), Row(label=0.0, features=Vectors.dense(1.0, 2.0, 3.0))]).toDF()\r\n>>> svm = LinearSVC()\r\n>>> svm.setRegParam(0.01)\r\nLinearSVC_c0eb3d7e4ecb\r\n>>> model = svm.fit(df)\r\n21/04/16 09:17:11 ERROR OWLQN: Failure! Resetting history: breeze.optimize.NaNHistory: \r\n>>> model.summary().objectiveHistory\r\n[1.0, 0.8483595765125, 0.7130226960596805, 0.6959063640335401, 0.6931614154979393, 0.677666741843437, 0.5879414534831688, 0.0930620718342175, 0.09048677587921486, 0.07102532249029628, 0.04374349680010344, 0.023954120035139716, 0.023830521501263628, 0.02320692346265251, 0.0215105944133274, 0.010552738391597573, 0.01051515768008, 0.009931999904022892, 0.008750399708406655, 0.0069553439215890535, 0.006868546522886791, 0.006515326943672154, 0.006347452798458171, 0.006187248462236068, 0.006164709258006091, 0.006053452700831725, 0.0060260493716176936, 0.005887552134192266, 0.005887136407436091, 0.005880225305895262, 0.005859805832943488, 0.005843641492521756, 0.005843641492521756, 0.00584486187964547, 0.005843267361152545, 0.005843241969094577, 0.005843234493660629, 0.0058432258624429665, 0.005843208081880606, 0.005843193072290253, 0.005843171629631225, 0.005843141001368335, 0.005843114614745189, 0.005843059249359493, 0.005843017763481445, 0.005842944768449553, 0.005842902553628102, 0.005842759959850206, 0.0058426880723804935, 0.0058424738985869045, 0.005842333714732286, 0.005842029622612579, 0.005841764628607474, 0.005841335619271196, 0.005840857505437478, 0.005840248258184164, 0.0058394259571752215, 0.005838527516717635, 0.00583821354404327, 0.005837212690021832, 0.005837208597515555, 0.005836810329205446, 0.005836776694348312, 0.00583640540121474, 0.00583635673616978, 0.005836167737106047, 0.005836194033268012, 0.00583616729576746, 0.0058361672269734615, 0.0058392815360454285, 0.005837679815125598, 0.005836401523490996, 0.005837908002584265, 0.005836192352808301, 0.005836670387508997, 0.005836202175777219, 0.005836200250798619, 0.005838876113322196]\r\n>>> model.summary().totalIterations\r\n77\r\n>>> model.transform(df).show()\r\n+-----+-------------+--------------------+----------+\r\n|label|     features|       rawPrediction|prediction|\r\n+-----+-------------+--------------------+----------+\r\n|  1.0|[1.0,1.0,1.0]|[-1.0000039068924...|       1.0|\r\n|  0.0|[1.0,2.0,3.0]|[0.99999460575938...|       0.0|\r\n+-----+-------------+--------------------+----------+\r\n```\r\n\r\n\r\nthis PR:\r\n```\r\n>>> from pyspark.sql import Row\r\n>>> from pyspark.ml.linalg import Vectors\r\n>>> from pyspark.ml.classification import LinearSVC\r\n>>> \r\n>>> df = sc.parallelize([Row(label=1.0, features=Vectors.dense(1.0, 1.0, 1.0)), Row(label=0.0, features=Vectors.dense(1.0, 2.0, 3.0))]).toDF()\r\n>>> svm = LinearSVC()\r\n>>> svm.setRegParam(0.01)\r\nLinearSVC_09863ffadcb7\r\n>>> model = svm.fit(df)\r\n>>> model.summary().objectiveHistory\r\n[1.0, 0.02084985122275127, 0.013524781160976402, 0.007607689403049228, 0.005824637199209566, 0.005323658700671529, 0.01838013981356318, 0.005089779695828527, 0.009036497849295026, 0.005023675663196612, 0.006332321775223654, 0.005004910245203961, 0.005002395070241166, 0.005000981376776186, 0.005042760733060569, 0.005000267637684085, 0.005013305173582782, 0.005000064609216319, 0.005004925699118321, 0.0050000068554171585, 0.005000088146444176, 0.005000002943794836, 0.005000000740400182, 0.005001814975687912, 0.005000001493803021, 0.0050001206863792644, 0.005000000123809999, 0.005000005933099882, 0.005000000031021147, 0.005000002103382519, 0.005000000004626093, 0.005000000001086049, 0.005001888167096368, 0.005000000754488572, 0.005000000108570436, 0.005001877526710762, 0.005000000861973192, 0.005000000216054848, 0.005001866885777133]\r\n>>> model.summary().totalIterations\r\n38\r\n>>> model.transform(df).show()\r\n+-----+-------------+--------------------+----------+\r\n|label|     features|       rawPrediction|prediction|\r\n+-----+-------------+--------------------+----------+\r\n|  1.0|[1.0,1.0,1.0]|[-0.9999981142568...|       1.0|\r\n|  0.0|[1.0,2.0,3.0]|[0.99999811425680...|       0.0|\r\n+-----+-------------+--------------------+----------+\r\n```\r\n\r\nunfortunately, spark's solutions are not like R's, but between the two solution, this PR convergen faster than existing one (which has a `OWLQN: Failure` warning)\r\n",
        "createdAt" : "2021-04-16T01:22:44Z",
        "updatedAt" : "2021-04-16T01:22:45Z",
        "lastEditedBy" : "685f805b-e4fa-4f21-b066-58afcecf9ce6",
        "tags" : [
        ]
      }
    ],
    "commit" : "5857a52f9cb9fe787b22dfdfd647e04b801224db",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +572,576 @@    0.0\n    >>> model.coefficients\n    DenseVector([0.0, -1.0319, -0.5159])\n    >>> model.intercept\n    2.579645978780695"
  },
  {
    "id" : "04f3e8d1-92c3-4b68-a34d-372c60b093c8",
    "prId" : 29616,
    "prUrl" : "https://github.com/apache/spark/pull/29616#pullrequestreview-480265270",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1779ebb3-50bf-44ea-8fad-5e78ca9bf9d9",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Ohhhh sorry it was my mistake. It doesn't crash because `uuid ` is imported via `from pyspark.ml.util import *`.",
        "createdAt" : "2020-09-02T01:06:56Z",
        "updatedAt" : "2020-09-02T01:06:56Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "d1575a669b7d32843b4cf24bdec45a1e0a4b9695",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +18,22 @@import operator\nimport sys\nimport uuid\n\nfrom multiprocessing.pool import ThreadPool"
  },
  {
    "id" : "817d275b-624b-40a9-9600-8323a12d6914",
    "prId" : 29563,
    "prUrl" : "https://github.com/apache/spark/pull/29563#pullrequestreview-477048152",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2b084423-da02-42f5-a5c2-395ae79057c3",
        "parentId" : null,
        "authorId" : "e29a7794-ac2e-4e6f-a690-737e83e1bace",
        "body" : "Both `sys` and `uuid` are used in the class, but aren't being imported.\r\n\r\nuuid: https://github.com/apache/spark/blob/master/python/pyspark/ml/classification.py#L3027\r\nsys: https://github.com/apache/spark/blob/master/python/pyspark/ml/classification.py#L3421",
        "createdAt" : "2020-08-27T20:14:59Z",
        "updatedAt" : "2020-08-29T18:33:37Z",
        "lastEditedBy" : "e29a7794-ac2e-4e6f-a690-737e83e1bace",
        "tags" : [
        ]
      }
    ],
    "commit" : "06480a7b4e8c5106ac7f7ea0fa9cecd6ea09e0bb",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +17,21 @@\nimport operator\nimport sys\nimport uuid\nimport warnings"
  },
  {
    "id" : "ee0ed29e-0553-45aa-829d-d8d4426b8298",
    "prId" : 29563,
    "prUrl" : "https://github.com/apache/spark/pull/29563#pullrequestreview-480268455",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fc1ccfd9-418a-47fb-86af-58dbd668ea01",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@Fokko, do you mind opening a minor PR to add this `uuid` to all other branches - looks like it wasn't there from the very first place https://github.com/Fokko/spark/commit/90b46e014a60069bd18754b02fce056d8f4d1b3e? \r\n\r\nI checked other occurrences fixed here, and other ones look fine to not fix in other branches.",
        "createdAt" : "2020-08-31T02:30:20Z",
        "updatedAt" : "2020-08-31T02:30:20Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "b340f5d4-5115-4f7b-8929-a2b494a525c2",
        "parentId" : "fc1ccfd9-418a-47fb-86af-58dbd668ea01",
        "authorId" : "e29a7794-ac2e-4e6f-a690-737e83e1bace",
        "body" : "Sure, I've created two PR's:\r\n\r\n- 3.0 branch: https://github.com/apache/spark/pull/29615\r\n- 2.4 branch: https://github.com/apache/spark/pull/29616",
        "createdAt" : "2020-09-01T19:16:54Z",
        "updatedAt" : "2020-09-01T19:16:55Z",
        "lastEditedBy" : "e29a7794-ac2e-4e6f-a690-737e83e1bace",
        "tags" : [
        ]
      },
      {
        "id" : "5698e4e3-3d69-40a7-b504-8f27d71fe441",
        "parentId" : "fc1ccfd9-418a-47fb-86af-58dbd668ea01",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Oh @Fokko sorry I misread. It was fine because we're using wildcard imports at `from pyspark.ml.util import *` and `util` imports `uuid`. So it didn't crash before ..  Let's just don't port this back alone :-)..",
        "createdAt" : "2020-09-02T01:08:32Z",
        "updatedAt" : "2020-09-02T01:08:32Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "06480a7b4e8c5106ac7f7ea0fa9cecd6ea09e0bb",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +18,22 @@import operator\nimport sys\nimport uuid\nimport warnings\nfrom abc import ABCMeta, abstractmethod, abstractproperty"
  },
  {
    "id" : "7f2e4357-d8c2-4e6f-b254-79f2c81104e5",
    "prId" : 29250,
    "prUrl" : "https://github.com/apache/spark/pull/29250#pullrequestreview-456046814",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6a116b5e-a0f5-4663-b372-8d37f17d9a7b",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@huaxingao, are these going to be documented? Might be good to list at `pyspark.ml.rst` too if so. The documentation PR (https://github.com/apache/spark/pull/29188) was merged. ",
        "createdAt" : "2020-07-27T08:55:30Z",
        "updatedAt" : "2020-07-27T18:48:58Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "d853597e-15c1-41e8-8d89-bc9c39f0e7ac",
        "parentId" : "6a116b5e-a0f5-4663-b372-8d37f17d9a7b",
        "authorId" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "body" : "@HyukjinKwon updated `pyspark.ml.rst`. Thanks!",
        "createdAt" : "2020-07-27T18:50:44Z",
        "updatedAt" : "2020-07-27T18:50:45Z",
        "lastEditedBy" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "tags" : [
        ]
      }
    ],
    "commit" : "328797cac590fff0db13848ef8e7ab3461817b45",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +53,57 @@           'MultilayerPerceptronClassifier', 'MultilayerPerceptronClassificationModel',\n           'MultilayerPerceptronClassificationSummary',\n           'MultilayerPerceptronClassificationTrainingSummary',\n           'OneVsRest', 'OneVsRestModel',\n           'FMClassifier', 'FMClassificationModel', 'FMClassificationSummary',"
  },
  {
    "id" : "40c1b98f-e44b-4c93-af73-5059af1fe4c9",
    "prId" : 26575,
    "prUrl" : "https://github.com/apache/spark/pull/26575#pullrequestreview-318926681",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0cd2b209-ee99-49c8-b7db-c2356be849b7",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Does this need any additional doc changes in Pyspark to show the new type that's allowed, or a doctest example?",
        "createdAt" : "2019-11-18T14:56:15Z",
        "updatedAt" : "2019-11-19T06:50:18Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "c6b73e53-551d-4279-8701-b03800fb8349",
        "parentId" : "0cd2b209-ee99-49c8-b7db-c2356be849b7",
        "authorId" : "685f805b-e4fa-4f21-b066-58afcecf9ce6",
        "body" : "@srowen It seem that we do not need to make other doc changes. The getter and setter has no doc about supported options, I guess adding a doctest is enough.",
        "createdAt" : "2019-11-19T10:24:12Z",
        "updatedAt" : "2019-11-19T10:24:13Z",
        "lastEditedBy" : "685f805b-e4fa-4f21-b066-58afcecf9ce6",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad36b5be386300cf259f30f87d9ef480d4020bf9",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +1914,1918 @@    the model's coefficients. The inventors of Complement NB show empirically that the parameter\n    estimates for CNB are more stable than those for Multinomial NB. Like Multinomial NB, the\n    input feature values for Complement NB must be nonnegative.\n    Since 3.0.0, it also supports Gaussian NB\n    <https://en.wikipedia.org/wiki/Naive_Bayes_classifier#Gaussian_naive_Bayes>`_."
  },
  {
    "id" : "6b4bcdf6-7bbd-45fe-93d8-540b4aea1ab3",
    "prId" : 26142,
    "prUrl" : "https://github.com/apache/spark/pull/26142#pullrequestreview-302725781",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4cc18324-04cc-4755-a21a-2c7786fc3f91",
        "parentId" : null,
        "authorId" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "body" : "it's a little strange to have ```setThreshold/Thresholds``` in the XXXParams class, but scala ```LogisticRegressionParams``` does this way, so I just do the same to be consistent with scala side. ",
        "createdAt" : "2019-10-16T16:52:09Z",
        "updatedAt" : "2019-10-18T05:13:43Z",
        "lastEditedBy" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "tags" : [
        ]
      }
    ],
    "commit" : "cdf87c78731747c183d71e5d5fa7c044d1269231",
    "line" : 156,
    "diffHunk" : "@@ -1,1 +375,379 @@        self._set(thresholds=value)\n        self.clear(self.threshold)\n        return self\n\n    @since(\"1.5.0\")"
  },
  {
    "id" : "e95f40eb-1749-435c-be1c-7b27bb45e775",
    "prId" : 25776,
    "prUrl" : "https://github.com/apache/spark/pull/25776#pullrequestreview-289289211",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b0aa339d-c5b4-4f03-bf2a-4645d4e699a8",
        "parentId" : null,
        "authorId" : "685f805b-e4fa-4f21-b066-58afcecf9ce6",
        "body" : "can we add a since annotation?",
        "createdAt" : "2019-09-16T07:11:56Z",
        "updatedAt" : "2019-09-18T16:26:19Z",
        "lastEditedBy" : "685f805b-e4fa-4f21-b066-58afcecf9ce6",
        "tags" : [
        ]
      },
      {
        "id" : "bae6798c-4cef-4aee-8a6a-011fb767c786",
        "parentId" : "b0aa339d-c5b4-4f03-bf2a-4645d4e699a8",
        "authorId" : "685f805b-e4fa-4f21-b066-58afcecf9ce6",
        "body" : "`HasRawPredictionCol` in `JavaClassifierParams` already support this method, right?\r\nMy previous PR https://github.com/apache/spark/pull/25662/files added this method because the setters generated by code_gen were removed in it.",
        "createdAt" : "2019-09-16T07:23:50Z",
        "updatedAt" : "2019-09-18T16:26:19Z",
        "lastEditedBy" : "685f805b-e4fa-4f21-b066-58afcecf9ce6",
        "tags" : [
        ]
      },
      {
        "id" : "64951fb7-bc32-4218-b613-c88295b0d15f",
        "parentId" : "b0aa339d-c5b4-4f03-bf2a-4645d4e699a8",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "As a broad comment, I'd say retain `@Since` information if it already existed. Did this exist already? if it was added in 3.0 we can mark it as such. If it replaces several definitions in subclasses that have inconsistent \"since\" versions, hm, I think we should label it as \"since\" the latest version of any of them.",
        "createdAt" : "2019-09-16T16:25:51Z",
        "updatedAt" : "2019-09-18T16:26:19Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "3288f855-f08b-460e-a377-a526e51e3bd8",
        "parentId" : "b0aa339d-c5b4-4f03-bf2a-4645d4e699a8",
        "authorId" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "body" : "@zhengruifeng Thanks for your comments. \r\nYou are right, ```HasRawPredictionCol``` already has this ```setRawPredictionCol```. I am a little hesitated to remove this setter, because I don't want to have too many no-ops class in this PR.  Since we will need to add it back after removing the setters from code_gen, I guess maybe just leave the method here?",
        "createdAt" : "2019-09-16T23:38:01Z",
        "updatedAt" : "2019-09-18T16:26:19Z",
        "lastEditedBy" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "tags" : [
        ]
      },
      {
        "id" : "0b251dbd-2b20-40cb-8d69-9156fc29fd6c",
        "parentId" : "b0aa339d-c5b4-4f03-bf2a-4645d4e699a8",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "I think it can stay as is",
        "createdAt" : "2019-09-17T14:07:04Z",
        "updatedAt" : "2019-09-18T16:26:19Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      }
    ],
    "commit" : "bc1d9e1ef4a27c9c619c10378d2a48ae0cad7836",
    "line" : 32,
    "diffHunk" : "@@ -1,1 +63,67 @@\n    @since(\"3.0.0\")\n    def setRawPredictionCol(self, value):\n        \"\"\"\n        Sets the value of :py:attr:`rawPredictionCol`."
  },
  {
    "id" : "34b13203-0e0b-4209-854b-7557c7e39ed6",
    "prId" : 25776,
    "prUrl" : "https://github.com/apache/spark/pull/25776#pullrequestreview-288444369",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "eaa638d2-321c-4897-bf94-7c0af3056f1a",
        "parentId" : null,
        "authorId" : "685f805b-e4fa-4f21-b066-58afcecf9ce6",
        "body" : "ditto",
        "createdAt" : "2019-09-16T07:14:05Z",
        "updatedAt" : "2019-09-18T16:26:19Z",
        "lastEditedBy" : "685f805b-e4fa-4f21-b066-58afcecf9ce6",
        "tags" : [
        ]
      }
    ],
    "commit" : "bc1d9e1ef4a27c9c619c10378d2a48ae0cad7836",
    "line" : 48,
    "diffHunk" : "@@ -1,1 +79,83 @@\n    @since(\"3.0.0\")\n    def setRawPredictionCol(self, value):\n        \"\"\"\n        Sets the value of :py:attr:`rawPredictionCol`."
  },
  {
    "id" : "1eddfd10-50d8-4afc-bf24-405473c35f99",
    "prId" : 25776,
    "prUrl" : "https://github.com/apache/spark/pull/25776#pullrequestreview-288444369",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "086442f0-71f9-4db9-8202-acc54daba616",
        "parentId" : null,
        "authorId" : "685f805b-e4fa-4f21-b066-58afcecf9ce6",
        "body" : "ditto",
        "createdAt" : "2019-09-16T07:28:05Z",
        "updatedAt" : "2019-09-18T16:26:19Z",
        "lastEditedBy" : "685f805b-e4fa-4f21-b066-58afcecf9ce6",
        "tags" : [
        ]
      }
    ],
    "commit" : "bc1d9e1ef4a27c9c619c10378d2a48ae0cad7836",
    "line" : 85,
    "diffHunk" : "@@ -1,1 +115,119 @@\n    @since(\"3.0.0\")\n    def setThresholds(self, value):\n        \"\"\"\n        Sets the value of :py:attr:`thresholds`."
  },
  {
    "id" : "3ed89c96-1113-427d-a616-651690708206",
    "prId" : 25776,
    "prUrl" : "https://github.com/apache/spark/pull/25776#pullrequestreview-289079081",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1078c372-4ae1-421d-8276-4625b8ad843a",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "I'm OK with adding this class for parity, though indeed they are no-ops in Python. In Scala they only contain some internal logic. I'd be OK removing these too, but it's OK to have if it's not costing much of anything.",
        "createdAt" : "2019-09-16T16:22:45Z",
        "updatedAt" : "2019-09-18T16:26:19Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "fc495ed6-b012-45b6-bbbb-0fbcc295d1ed",
        "parentId" : "1078c372-4ae1-421d-8276-4625b8ad843a",
        "authorId" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "body" : "@srowen Thanks for your comments.\r\nI did a quick test using ```print (time.clock() - start_time, \"seconds\")``` in ```LogisticRegressionTest```to compare the difference between with and without this no-op class. I didn't see any time degradation with this no-op class. \r\nAlso, I prefer to keep this no-op class so I don't need to make both ```JavaProbabilisticClassifier``` and ```JavaProbabilisticClassificationModel``` to implement ```HasProbabilityCol, HasThresholds```. I guess we just keep this class?",
        "createdAt" : "2019-09-16T23:38:27Z",
        "updatedAt" : "2019-09-18T16:26:19Z",
        "lastEditedBy" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "tags" : [
        ]
      },
      {
        "id" : "5fa366cb-5ec4-4785-a10e-b7bf551c8bb7",
        "parentId" : "1078c372-4ae1-421d-8276-4625b8ad843a",
        "authorId" : "685f805b-e4fa-4f21-b066-58afcecf9ce6",
        "body" : "This no-op class will help maintain the codebase if it's not costing much.",
        "createdAt" : "2019-09-17T07:55:13Z",
        "updatedAt" : "2019-09-18T16:26:19Z",
        "lastEditedBy" : "685f805b-e4fa-4f21-b066-58afcecf9ce6",
        "tags" : [
        ]
      }
    ],
    "commit" : "bc1d9e1ef4a27c9c619c10378d2a48ae0cad7836",
    "line" : 65,
    "diffHunk" : "@@ -1,1 +98,102 @@    (Private) Java Probabilistic Classifier Params for classification tasks.\n    \"\"\"\n    pass\n\n"
  },
  {
    "id" : "8af38b78-2cd8-4b1d-8efb-8edb77c9018e",
    "prId" : 25776,
    "prUrl" : "https://github.com/apache/spark/pull/25776#pullrequestreview-289082205",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "59056699-f4a3-4c28-86f5-257e019f9d5b",
        "parentId" : null,
        "authorId" : "685f805b-e4fa-4f21-b066-58afcecf9ce6",
        "body" : "What about adding some simple tests for it?",
        "createdAt" : "2019-09-17T08:01:37Z",
        "updatedAt" : "2019-09-18T16:26:19Z",
        "lastEditedBy" : "685f805b-e4fa-4f21-b066-58afcecf9ce6",
        "tags" : [
        ]
      }
    ],
    "commit" : "bc1d9e1ef4a27c9c619c10378d2a48ae0cad7836",
    "line" : 249,
    "diffHunk" : "@@ -1,1 +1162,1166 @@class RandomForestClassifier(JavaProbabilisticClassifier, HasSeed, RandomForestParams,\n                             TreeClassifierParams, HasCheckpointInterval,\n                             JavaMLWritable, JavaMLReadable):\n    \"\"\"\n    `Random Forest <http://en.wikipedia.org/wiki/Random_forest>`_"
  },
  {
    "id" : "06da00b7-5439-48d3-8424-8cbdb4eeb549",
    "prId" : 25715,
    "prUrl" : "https://github.com/apache/spark/pull/25715#pullrequestreview-286360023",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c30e5de8-87c5-4546-a9a5-7c877e96efce",
        "parentId" : null,
        "authorId" : "a2fcc15e-a51a-42f0-87a6-137048a28e30",
        "body" : "@huaxingao keeping it at 2.0.0 is okay ?",
        "createdAt" : "2019-09-10T18:00:10Z",
        "updatedAt" : "2019-09-12T17:38:16Z",
        "lastEditedBy" : "a2fcc15e-a51a-42f0-87a6-137048a28e30",
        "tags" : [
        ]
      },
      {
        "id" : "a394bf82-f1b9-4043-8241-dea97e81f60b",
        "parentId" : "c30e5de8-87c5-4546-a9a5-7c877e96efce",
        "authorId" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "body" : "@dilipbiswal \r\nYes, it's ok to keep 2.0.0. The method was originally in 2.0.0 in the subclass of `OneVsRest`. Now it was moved to `OneVsRest`.",
        "createdAt" : "2019-09-10T18:32:41Z",
        "updatedAt" : "2019-09-12T17:38:16Z",
        "lastEditedBy" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "tags" : [
        ]
      },
      {
        "id" : "8015bdef-05e7-4288-bda1-72042c9f7128",
        "parentId" : "c30e5de8-87c5-4546-a9a5-7c877e96efce",
        "authorId" : "a2fcc15e-a51a-42f0-87a6-137048a28e30",
        "body" : "@huaxingao OK.. ",
        "createdAt" : "2019-09-10T18:45:54Z",
        "updatedAt" : "2019-09-12T17:38:16Z",
        "lastEditedBy" : "a2fcc15e-a51a-42f0-87a6-137048a28e30",
        "tags" : [
        ]
      }
    ],
    "commit" : "4ee12ef8a3416ed92188c5274ab8531d0f1706da",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +1951,1955 @@        return self._set(**kwargs)\n\n    @since(\"2.0.0\")\n    def setClassifier(self, value):\n        \"\"\""
  }
]