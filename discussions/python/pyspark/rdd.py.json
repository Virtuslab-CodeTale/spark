[
  {
    "id" : "5b8d944b-ea71-41c9-be20-722228a8c089",
    "prId" : 32667,
    "prUrl" : "https://github.com/apache/spark/pull/32667#pullrequestreview-668731587",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4f7880c4-bc54-4821-bb22-f3559c3cf4ef",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "should we use `sys.float_info.max` instead, @nolanliou? Hm, btw just realised that it's funny that it can reach to the maximum of float ...",
        "createdAt" : "2021-05-26T05:20:02Z",
        "updatedAt" : "2021-05-26T05:20:02Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "c4b53dbe-faaf-47b6-b500-158a1dbd05a5",
        "parentId" : "4f7880c4-bc54-4821-bb22-f3559c3cf4ef",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Anyway avoiding failure on the batch size makes sense.",
        "createdAt" : "2021-05-26T05:23:57Z",
        "updatedAt" : "2021-05-26T05:23:57Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "1365fe55-addd-46ba-9378-dd3a8f2e66aa",
        "parentId" : "4f7880c4-bc54-4821-bb22-f3559c3cf4ef",
        "authorId" : "4b0337ac-9e2b-41b3-be5a-60c61b374bc6",
        "body" : "I think `sys.maxsize`  is ok. \r\n\r\nItâ€™s not easy to encounter this problem, but I ran into it...",
        "createdAt" : "2021-05-26T06:10:36Z",
        "updatedAt" : "2021-05-26T06:10:36Z",
        "lastEditedBy" : "4b0337ac-9e2b-41b3-be5a-60c61b374bc6",
        "tags" : [
        ]
      },
      {
        "id" : "349c46e6-30f0-46e4-842f-8e733baff1ef",
        "parentId" : "4f7880c4-bc54-4821-bb22-f3559c3cf4ef",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "okay, `sys.maxsize` batch size already doesn't make much sense anyway. ",
        "createdAt" : "2021-05-26T08:22:16Z",
        "updatedAt" : "2021-05-26T08:22:17Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "67e6d719db88b480be527bcd15af7e021bcff141",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +2068,2072 @@                    # let 1M < avg < 10M\n                    if avg < 1:\n                        batch = min(sys.maxsize, batch * 1.5)\n                    elif avg > 10:\n                        batch = max(int(batch / 1.5), 1)"
  },
  {
    "id" : "f9bacccd-c65e-4f6a-90f4-b6530cd24e65",
    "prId" : 29023,
    "prUrl" : "https://github.com/apache/spark/pull/29023#pullrequestreview-445216295",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "26175370-2561-4b32-b69b-c1cf3fda0ee8",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "This was correctly ported back at https://github.com/apache/spark/pull/25593. There's no such code you pointed out in Spark 2.4.4: https://github.com/apache/spark/blob/v2.4.4/python/pyspark/sql/dataframe.py#L2182.\r\n\r\nAre you using your own fork or mixing the Spark versions? Your error message seems from https://github.com/apache/spark/blob/v2.4.3/python/pyspark/sql/dataframe.py#L2179 which is Spark 2.4.3. Spark 2.4.3 does not have this change.\r\n\r\n",
        "createdAt" : "2020-07-08T00:46:36Z",
        "updatedAt" : "2020-07-08T00:46:37Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "3dcaae70-d976-490f-8a77-230701c4799a",
        "parentId" : "26175370-2561-4b32-b69b-c1cf3fda0ee8",
        "authorId" : "ebe419f4-c44a-4311-bf58-d394b5472320",
        "body" : "@HyukjinKwon You are linking to `dataframe.py` but I patched `rdd.py`\r\nThe code is there:\r\nhttps://github.com/apache/spark/blob/v2.4.4/python/pyspark/rdd.py#L144\r\n\r\nSo `(*sock_info)` is used instead of passing individual touple values.",
        "createdAt" : "2020-07-08T17:55:04Z",
        "updatedAt" : "2020-07-08T17:55:05Z",
        "lastEditedBy" : "ebe419f4-c44a-4311-bf58-d394b5472320",
        "tags" : [
        ]
      },
      {
        "id" : "334d157d-5f05-4bf5-983d-9b284d759c7d",
        "parentId" : "26175370-2561-4b32-b69b-c1cf3fda0ee8",
        "authorId" : "ebe419f4-c44a-4311-bf58-d394b5472320",
        "body" : "Ok I see now. It is possible that that I messed something while using pipenv. \r\nAnyway I feel that using `(*sock_info)` is inherently unsafe and prone to such errors.",
        "createdAt" : "2020-07-08T18:10:02Z",
        "updatedAt" : "2020-07-08T18:10:02Z",
        "lastEditedBy" : "ebe419f4-c44a-4311-bf58-d394b5472320",
        "tags" : [
        ]
      },
      {
        "id" : "cce1a579-edd8-4540-a977-7b072c7886e7",
        "parentId" : "26175370-2561-4b32-b69b-c1cf3fda0ee8",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Yeah, I actually already pointed it out it's error-prone in the previous PR. Feel free to open another PR to fix it.",
        "createdAt" : "2020-07-09T00:37:09Z",
        "updatedAt" : "2020-07-09T00:37:20Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "2e2534f1ec4323b96df966b88cdb006609ca892b",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +144,148 @@    port = sock_info[0]\n    auth_secret = sock_info[1]\n    (sockfile, sock) = local_connect_and_auth(port, auth_secret)\n    # The RDD materialization time is unpredicable, if we set a timeout for socket reading\n    # operation, it will very possibly fail. See SPARK-18281."
  },
  {
    "id" : "1f646ddc-25f3-4cfd-baf4-155ec986e23e",
    "prId" : 28395,
    "prUrl" : "https://github.com/apache/spark/pull/28395#pullrequestreview-403111027",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5700c821-5f2d-4651-8d12-b63c4b12b3fd",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Do we target this at 3.0.0?\r\ncc @gatorsmile ",
        "createdAt" : "2020-04-29T18:07:10Z",
        "updatedAt" : "2020-04-30T07:24:39Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "d7ae2141-2d4b-4e7c-8138-0e973b2bbfff",
        "parentId" : "5700c821-5f2d-4651-8d12-b63c4b12b3fd",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "If then, it would be great if we have @HyukjinKwon's [background explanation](https://github.com/apache/spark/pull/28395#pullrequestreview-402300959) on the PR description.",
        "createdAt" : "2020-04-29T18:08:21Z",
        "updatedAt" : "2020-04-30T07:24:39Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "9b09cbe7-0497-4c12-86c6-d58496a06106",
        "parentId" : "5700c821-5f2d-4651-8d12-b63c4b12b3fd",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "BTW, we can remove this API since this is Experimental? cc @marmbrus \r\n> I will target to deprecate this API in Spark 3.1.",
        "createdAt" : "2020-04-29T18:09:24Z",
        "updatedAt" : "2020-04-30T07:24:39Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "d726e83f-4878-4010-9c8e-1b89d33870c0",
        "parentId" : "5700c821-5f2d-4651-8d12-b63c4b12b3fd",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I don't think we will ignore the existing definitions of API stability tags. Might be best to stick to what's documented there in tags ...\r\n\r\nOf course we should take it considering the ammended semver but I think it's a rather rubric to consider.\r\n\r\nI kind of strongly think we should deprecate/remove it to stick to the standard approach for this one specifically later once we enable the pin-thread mode by default. I also agree with having this APIs as a step to migrate meanwhile.",
        "createdAt" : "2020-04-29T23:34:54Z",
        "updatedAt" : "2020-04-30T07:24:39Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "bdd77fecc886a7b94a66c8c3dfd27f8923c22015",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +884,888 @@        When collect rdd, use this method to specify job group.\n\n        .. versionadded:: 3.0.0\n        \"\"\"\n        with SCCallSiteSync(self.context) as css:"
  },
  {
    "id" : "864c137c-ccc0-42f1-814f-312f7fc32b9d",
    "prId" : 28085,
    "prUrl" : "https://github.com/apache/spark/pull/28085#pullrequestreview-388405452",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1c726358-2899-409b-936a-5a869ae6f06d",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@tgravescs, can you clarify if we're going to make `getResourceProfile` and `withResources` as APIs or not first? I strongly think this is a bad idea to mix private and API declarations\r\n\r\nhttps://github.com/apache/spark/blob/5d76b12e9b2ca0eb090c3c5145eee4cf78caba13/core/src/main/scala/org/apache/spark/rdd/RDD.scala#L1740-L1743",
        "createdAt" : "2020-04-05T04:41:42Z",
        "updatedAt" : "2020-04-22T14:03:45Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "f8a0254c-b49d-41f3-94e0-3c3eb04b872a",
        "parentId" : "1c726358-2899-409b-936a-5a869ae6f06d",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "I don't know what you mean, we are going with these apis. This is only in master branch, not in branch-3.0.  https://issues.apache.org/jira/browse/SPARK-29150 is to make the RDD ones public. At this point the core feature is complete so making the python ones and java ones public doesn't hurt anything and they will be only be in 3.1.0. Sorry for the confusion there, its just timing on when things got merged and it was across the 3.0 and 3.1 boundary.",
        "createdAt" : "2020-04-06T16:13:01Z",
        "updatedAt" : "2020-04-22T14:03:45Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "354fb0c09ff9ed6be985f0ca7a8b6fae835303a2",
    "line" : 76,
    "diffHunk" : "@@ -1,1 +2497,2501 @@        return self\n\n    def getResourceProfile(self):\n        \"\"\"\n        .. note:: Experimental"
  },
  {
    "id" : "1115ad5b-d943-4a51-bdfd-e1e37ffe6c50",
    "prId" : 26148,
    "prUrl" : "https://github.com/apache/spark/pull/26148#pullrequestreview-303961719",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "03a50894-8568-4648-b786-1b3d189dd223",
        "parentId" : null,
        "authorId" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "body" : "maybe add a python test for this newly added API?",
        "createdAt" : "2019-10-18T15:17:46Z",
        "updatedAt" : "2019-10-22T05:50:01Z",
        "lastEditedBy" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "tags" : [
        ]
      }
    ],
    "commit" : "437aadfcd972347a5b5a017271a5ad7f09a7cd4c",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +2549,2553 @@        \"\"\"\n        return PipelinedRDD(self.rdd, f, preservesPartitioning, isFromBarrier=True)\n\n\nclass PipelinedRDD(RDD):"
  },
  {
    "id" : "ca223a6e-e83e-41de-a2ba-7fecbffa54b4",
    "prId" : 24834,
    "prUrl" : "https://github.com/apache/spark/pull/24834#pullrequestreview-253550265",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1f745b28-4f6c-4f04-8c87-b4b27e6f3f04",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@BryanCutler, what does `sock_info` expect to be? Seems it can be both 2-tuple and 3-tuple (with server).",
        "createdAt" : "2019-06-24T04:35:30Z",
        "updatedAt" : "2019-06-24T17:59:28Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "ca73c5e1-0f82-49db-ad8f-38227f5d8219",
        "parentId" : "1f745b28-4f6c-4f04-8c87-b4b27e6f3f04",
        "authorId" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "body" : "Uggh, yeah I'm not too happy with this. Java returns a 3-tuple with (port, auth_secret, server) and most places only use the first 2, such as `_load_from_socket`.  It gets a little confusing, so I thought it might be better to expand the values returned by java for `serveToStream` etc., but it ended up with a lot of changes where the third value is ignored like this\r\n\r\n```python\r\nport, auth_secret, _ = ...\r\n```\r\nand I don't think it really made things clearer.  I'll try to think of something better and maybe do a followup.",
        "createdAt" : "2019-06-24T17:23:27Z",
        "updatedAt" : "2019-06-24T17:59:28Z",
        "lastEditedBy" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "tags" : [
        ]
      }
    ],
    "commit" : "20eb7487962a49c928003167a8d7e3cfaff54dbc",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +156,160 @@\n\ndef _load_from_socket(sock_info, serializer):\n    \"\"\"\n    Connect to a local socket described by sock_info and use the given serializer to yield data"
  }
]