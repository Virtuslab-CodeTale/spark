[
  {
    "id" : "7371f422-a2d5-48a1-a1cd-3be80f48c996",
    "prId" : 29858,
    "prUrl" : "https://github.com/apache/spark/pull/29858#pullrequestreview-495223647",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "455f5a29-00b4-45eb-98a4-5a733d22d39f",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "cc @gatorsmile ",
        "createdAt" : "2020-09-24T02:47:02Z",
        "updatedAt" : "2020-09-24T02:47:02Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "d0a233d8-0385-4428-8a84-654dfe5561f2",
        "parentId" : "455f5a29-00b4-45eb-98a4-5a733d22d39f",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I checked offline with Xiao. Seems fine.",
        "createdAt" : "2020-09-24T05:10:49Z",
        "updatedAt" : "2020-09-24T05:10:49Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "41e83f3932d7680cb077bdd5c0b909b2c112fe76",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +27,31 @@DEFAULT_HIVE = \"hive2.3\"\nSUPPORTED_HADOOP_VERSIONS = [\"hadoop2.7\", \"hadoop3.2\", \"without-hadoop\"]\nSUPPORTED_HIVE_VERSIONS = [\"hive2.3\"]\nUNSUPPORTED_COMBINATIONS = [\n]"
  },
  {
    "id" : "d280ea79-25ca-424c-aeb0-0b0b3ddbb4a4",
    "prId" : 29703,
    "prUrl" : "https://github.com/apache/spark/pull/29703#pullrequestreview-485558011",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3224424d-9b1a-4d82-b644-52b6e48609e5",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I basically referred to https://github.com/apache/spark/blob/b84ed4146d93b37adb2b83ca642c7978a1ac853e/sql/hive/src/test/scala/org/apache/spark/sql/hive/HiveExternalCatalogVersionsSuite.scala#L70-L111\r\nand\r\nhttps://github.com/apache/spark/blob/f53d8c63e80172295e2fbc805c0c391bdececcaa/R/pkg/R/install.R#L68-L161",
        "createdAt" : "2020-09-10T04:32:18Z",
        "updatedAt" : "2020-09-22T04:29:30Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "20491e0cdd5b1fae207cf20d8091d4c456728b39",
    "line" : 89,
    "diffHunk" : "@@ -1,1 +87,91 @@\n\ndef install_spark(dest, spark_version, hadoop_version, hive_version):\n    \"\"\"\n    Installs Spark that corresponds to the given Hadoop version in the current"
  },
  {
    "id" : "ed48c116-c815-403c-b100-598786e150a0",
    "prId" : 29703,
    "prUrl" : "https://github.com/apache/spark/pull/29703#pullrequestreview-485568659",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "520cd95f-f275-41e8-9322-cc1e98fc27b9",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "The purpose of showing the progress are two:\r\n\r\n- The print out will be seen when `pip install ... -v`.\r\n- The output from plan `pip` without `-v` shows like it's in progress (otherwise it looks like it hangs). For example:\r\n\r\n    ```\r\n      Building wheel for pyspark (setup.py) ... -\r\n      Building wheel for pyspark (setup.py) ... \\\r\n      Building wheel for pyspark (setup.py) ... |\r\n    ``` \r\n",
        "createdAt" : "2020-09-10T04:54:00Z",
        "updatedAt" : "2020-09-22T04:29:30Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "20491e0cdd5b1fae207cf20d8091d4c456728b39",
    "line" : 173,
    "diffHunk" : "@@ -1,1 +171,175 @@                bytes_so_far,\n                total_size,\n                round(float(bytes_so_far) / total_size * 100, 2)))"
  },
  {
    "id" : "7795aff3-72b6-4b66-b53f-76c81ad62257",
    "prId" : 29703,
    "prUrl" : "https://github.com/apache/spark/pull/29703#pullrequestreview-486412038",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "691741c4-55b4-4981-b745-9a8a58275409",
        "parentId" : null,
        "authorId" : "fd6ebc48-7da4-490b-8d41-2e8530d92720",
        "body" : "All non-current versions of Spark will hit the archive, since the mirrors only maintain the latest version. I don't think the archive will be able to handle the volume of traffic that will eventually come its way from various people downloading (and re-downloading) Spark, e.g. as part of CI setup.",
        "createdAt" : "2020-09-11T00:56:09Z",
        "updatedAt" : "2020-09-22T04:29:30Z",
        "lastEditedBy" : "fd6ebc48-7da4-490b-8d41-2e8530d92720",
        "tags" : [
        ]
      },
      {
        "id" : "56043388-6488-4287-b0ac-b1ffd63ebcc3",
        "parentId" : "691741c4-55b4-4981-b745-9a8a58275409",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "When they install, I think it will likely be the latest in most of cases. I guess that's the reason why we moved the old versions into these archive and keep the the latest versions in the mirrors.\r\n\r\nPeople are already using this to download old versions or to setup the CI. This PR just makes it easier to do it.",
        "createdAt" : "2020-09-11T01:21:40Z",
        "updatedAt" : "2020-09-22T04:29:30Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "20491e0cdd5b1fae207cf20d8091d4c456728b39",
    "line" : 155,
    "diffHunk" : "@@ -1,1 +153,157 @@\n    default_sites = [\n        \"https://archive.apache.org/dist\", \"https://dist.apache.org/repos/dist/release\"]\n    return list(set(mirror_urls)) + default_sites\n"
  },
  {
    "id" : "d5bb765d-24f8-47d8-a81c-1f593eb0cd1e",
    "prId" : 29703,
    "prUrl" : "https://github.com/apache/spark/pull/29703#pullrequestreview-490220921",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5d44d70b-c27b-4ff0-82a2-8382247ee5f7",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Is \"without-hadoop\" also supported as special keyword? Seems not see it is matched here?",
        "createdAt" : "2020-09-17T03:15:59Z",
        "updatedAt" : "2020-09-22T04:29:30Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "2e532959-b776-428f-b38a-0351374079a9",
        "parentId" : "5d44d70b-c27b-4ff0-82a2-8382247ee5f7",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "It is verified below `if hadoop_version not in SUPPORTED_HADOOP_VERSIONS:` later.  There's a test case here: https://github.com/apache/spark/pull/29703/files/033a33ee515b95342e8c5a74e63054d915661579#diff-e23af4eb5cc3bf6af4bc26cb801b7e84R69 and https://github.com/apache/spark/pull/29703/files/033a33ee515b95342e8c5a74e63054d915661579#diff-e23af4eb5cc3bf6af4bc26cb801b7e84R88\r\n\r\nUsers can specify the Hadoop and Hive versions such as `hadoop3.2` and `hive2.3` as well but I didn't document this. These keywords are actually ported from SparkR as are `SparkR::install.spark`.",
        "createdAt" : "2020-09-17T03:40:53Z",
        "updatedAt" : "2020-09-22T04:29:30Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "20491e0cdd5b1fae207cf20d8091d4c456728b39",
    "line" : 64,
    "diffHunk" : "@@ -1,1 +62,66 @@\n    if hadoop_version == \"without\":\n        hadoop_version = \"without-hadoop\"\n    elif re.match(\"^[0-9]+\\\\.[0-9]+$\", hadoop_version):\n        hadoop_version = \"hadoop%s\" % hadoop_version"
  }
]