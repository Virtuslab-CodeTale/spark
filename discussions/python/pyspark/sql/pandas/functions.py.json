[
  {
    "id" : "d58a3906-0314-4727-b0ce-1a6e0f00b744",
    "prId" : 28052,
    "prUrl" : "https://github.com/apache/spark/pull/28052#pullrequestreview-383373335",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a879a05d-ca0e-4044-85d6-e61ebb4e3a3e",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Shall we show warn messaging in these cases that type hints are not supported yet?",
        "createdAt" : "2020-03-27T17:06:57Z",
        "updatedAt" : "2020-03-27T17:06:58Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "c546cbdc-9088-4ea2-876a-08bb3b86e918",
        "parentId" : "a879a05d-ca0e-4044-85d6-e61ebb4e3a3e",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Let me don't add the warning for now .. I am not yet sure if we should add the support of type hints in these pandas Function APIs .. Hopefully the documentation in the site could clarify that type hints are only supported in pandas UDFs.",
        "createdAt" : "2020-03-29T04:56:51Z",
        "updatedAt" : "2020-03-29T04:56:51Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "40586d45-f8e0-4c30-ab56-74601eb17801",
        "parentId" : "a879a05d-ca0e-4044-85d6-e61ebb4e3a3e",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "It implies a good point - there's a mismatch about how pandas UDF uses Python type hints because we force to set the Python type hints but the type hints are supposed to be completely optional ...",
        "createdAt" : "2020-03-29T04:58:46Z",
        "updatedAt" : "2020-03-29T04:58:46Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "371b8ebfc0d354539a96e3c0f4f6a776dbb212ce",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +393,397 @@            # evaluation type will always be set.\n            pass\n        elif len(argspec.annotations) > 0:\n            evalType = infer_eval_type(signature(f))\n            assert evalType is not None"
  },
  {
    "id" : "d5dfee32-5fc8-41bc-ad19-17ebd79f6ef9",
    "prId" : 27466,
    "prUrl" : "https://github.com/apache/spark/pull/27466#pullrequestreview-355094186",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "edec5760-4f02-4d2c-be4d-ef21fadf9ed3",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "duplicated?",
        "createdAt" : "2020-02-07T11:26:27Z",
        "updatedAt" : "2020-02-11T08:35:36Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "626ff3cb245ccd305f9a32d9c99f774ef154ebe1",
    "line" : 331,
    "diffHunk" : "@@ -1,1 +83,87 @@    type via `functionType` which will be deprecated in the future releases.\n\n    Note that the type hint should use `pandas.Series` in all cases but there is one variant\n    that `pandas.DataFrame` should be used for its input or output type hint instead when the input\n    or output column is of :class:`pyspark.sql.types.StructType`. The following example shows"
  },
  {
    "id" : "25868635-ca2e-4258-8a7e-88d946f6f8fd",
    "prId" : 27466,
    "prUrl" : "https://github.com/apache/spark/pull/27466#pullrequestreview-355902150",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3a449d89-a0e6-46ba-a2b9-7ff596bf6250",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I don't review it carefully and assume most of it are coped from the md file.",
        "createdAt" : "2020-02-10T12:11:04Z",
        "updatedAt" : "2020-02-11T08:35:36Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "14c4f734-b11c-46c2-86a9-f60b122f0338",
        "parentId" : "3a449d89-a0e6-46ba-a2b9-7ff596bf6250",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Yeah... it happened to be almost a copy ...",
        "createdAt" : "2020-02-10T12:35:52Z",
        "updatedAt" : "2020-02-11T08:35:36Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "626ff3cb245ccd305f9a32d9c99f774ef154ebe1",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +46,50 @@    Creates a pandas user defined function (a.k.a. vectorized user defined function).\n\n    Pandas UDFs are user defined functions that are executed by Spark using Arrow to transfer\n    data and Pandas to work with the data, which allows vectorized operations. A Pandas UDF\n    is defined using the `pandas_udf` as a decorator or to wrap the function, and no"
  },
  {
    "id" : "f9ddacb2-b4dd-43af-ba16-0858bfbccda9",
    "prId" : 27466,
    "prUrl" : "https://github.com/apache/spark/pull/27466#pullrequestreview-355889223",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d99f9521-522b-45b3-b96c-3f9fe36a1916",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "shall we introduce the new API first and legacy API later to promote the new API?",
        "createdAt" : "2020-02-10T12:12:24Z",
        "updatedAt" : "2020-02-11T08:35:36Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "626ff3cb245ccd305f9a32d9c99f774ef154ebe1",
    "line" : 320,
    "diffHunk" : "@@ -1,1 +72,76 @@    ...     return s.str.len()\n\n    Prior to Spark 3.0, the pandas UDF used `functionType` to decide the execution type as below:\n\n    >>> from pyspark.sql.functions import PandasUDFType"
  },
  {
    "id" : "ddaf0a0c-642c-4cb9-9ba2-f7281bee05a9",
    "prId" : 27466,
    "prUrl" : "https://github.com/apache/spark/pull/27466#pullrequestreview-355899784",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "83a50bbc-4306-4e28-ad4a-1367609f5a65",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "what about input types?",
        "createdAt" : "2020-02-10T12:13:05Z",
        "updatedAt" : "2020-02-11T08:35:36Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "7245ee71-4ebe-449b-a01f-935b81f7544a",
        "parentId" : "83a50bbc-4306-4e28-ad4a-1367609f5a65",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "They are same.",
        "createdAt" : "2020-02-10T12:31:39Z",
        "updatedAt" : "2020-02-11T08:35:36Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "626ff3cb245ccd305f9a32d9c99f774ef154ebe1",
    "line" : 532,
    "diffHunk" : "@@ -1,1 +283,287 @@        :class:`pyspark.sql.types.ArrayType` of :class:`pyspark.sql.types.TimestampType` and\n        nested :class:`pyspark.sql.types.StructType`\n        are currently not supported as output types.\n\n    .. seealso:: :meth:`pyspark.sql.DataFrame.mapInPandas`"
  },
  {
    "id" : "8568b53e-77a3-4618-8661-400e3da1c46d",
    "prId" : 27165,
    "prUrl" : "https://github.com/apache/spark/pull/27165#pullrequestreview-345628669",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "69384847-0b71-4b9f-bd65-d867c6531240",
        "parentId" : null,
        "authorId" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "body" : "I'm not sure about this warning unless we are positive that we will be requiring type hints in the future. Plus, it seems to warn on `mapInPandas` etc, which isn't right..",
        "createdAt" : "2020-01-21T00:36:24Z",
        "updatedAt" : "2020-01-22T01:53:52Z",
        "lastEditedBy" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "tags" : [
        ]
      },
      {
        "id" : "72c5721d-2fb1-43f2-9a47-288ffeaf5eab",
        "parentId" : "69384847-0b71-4b9f-bd65-d867c6531240",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I actually would like to choose one side and encourage users to give feedbacks. Given my interactions here, using type hints seems preferred. Let me try to fix the condition.",
        "createdAt" : "2020-01-21T04:25:15Z",
        "updatedAt" : "2020-01-22T01:53:52Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "76e09092f983782f0986c0c55c34165de62435bd",
    "line" : 197,
    "diffHunk" : "@@ -1,1 +445,449 @@                        PythonEvalType.SQL_SCALAR_PANDAS_ITER_UDF,\n                        PythonEvalType.SQL_GROUPED_AGG_PANDAS_UDF]:\n            warnings.warn(\n                \"In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for \"\n                \"pandas UDF instead of specifying pandas UDF type which will be deprecated \""
  },
  {
    "id" : "5ecfb02d-33ab-4d1f-b452-bf7be9ba6de1",
    "prId" : 27165,
    "prUrl" : "https://github.com/apache/spark/pull/27165#pullrequestreview-345669535",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "88c0aac6-9ec4-4426-9738-604dfd109b25",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Shall we check `evalType` is `None` here?",
        "createdAt" : "2020-01-21T07:11:55Z",
        "updatedAt" : "2020-01-22T01:53:52Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "76e09092f983782f0986c0c55c34165de62435bd",
    "line" : 202,
    "diffHunk" : "@@ -1,1 +450,454 @@                \"in the future releases. See SPARK-28264 for more details.\", UserWarning)\n        elif len(argspec.annotations) > 0:\n            evalType = infer_eval_type(signature(f))\n            assert evalType is not None\n"
  },
  {
    "id" : "4e40f8d7-ac95-47f1-bc5c-47a998fa02ae",
    "prId" : 27109,
    "prUrl" : "https://github.com/apache/spark/pull/27109#pullrequestreview-339613356",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "360773b8-b768-4c06-8d7f-73605cb46463",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Do we still need to import?",
        "createdAt" : "2020-01-08T02:03:39Z",
        "updatedAt" : "2020-01-08T02:03:39Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "4a791e63-5d85-477d-89a3-7cd7063701d4",
        "parentId" : "360773b8-b768-4c06-8d7f-73605cb46463",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "<del>There are similar imports below.</del>",
        "createdAt" : "2020-01-08T02:05:51Z",
        "updatedAt" : "2020-01-08T02:09:17Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "1ac8e3d3-bbaa-4950-a6f4-c2870eb6220f",
        "parentId" : "360773b8-b768-4c06-8d7f-73605cb46463",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I think maybe no need to import. Just wanted to keep the examples as are.",
        "createdAt" : "2020-01-08T02:07:29Z",
        "updatedAt" : "2020-01-08T02:07:30Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "c63c271d95e7864b69ca477c51e4e2d22b7d9029",
    "line" : 67,
    "diffHunk" : "@@ -1,1 +65,69 @@       :meth:`pyspark.sql.DataFrame.select`.\n\n       >>> from pyspark.sql.functions import pandas_udf, PandasUDFType\n       >>> from pyspark.sql.types import IntegerType, StringType\n       >>> slen = pandas_udf(lambda s: s.str.len(), IntegerType())  # doctest: +SKIP"
  },
  {
    "id" : "2a282650-e8b7-4e35-b925-5ef0142845d0",
    "prId" : 27109,
    "prUrl" : "https://github.com/apache/spark/pull/27109#pullrequestreview-339613651",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8d07fb67-6316-41ef-a89b-2bc64c5fbb9d",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "sql.pandas.functions.pandas_udf?",
        "createdAt" : "2020-01-08T02:04:22Z",
        "updatedAt" : "2020-01-08T02:04:22Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "29186002-f90c-4f5d-8a25-9b166344118b",
        "parentId" : "8d07fb67-6316-41ef-a89b-2bc64c5fbb9d",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "It still keeps the import as is. I intend to keep `sql.pandas` as rather a private package. I documented at `pandas/__init__.py`.\r\n\r\nI double checked this still links currently to the `pandas_udf` in API documentation.",
        "createdAt" : "2020-01-08T02:06:06Z",
        "updatedAt" : "2020-01-08T02:06:06Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "3268abab-a665-4bd3-bbcd-941112df96b9",
        "parentId" : "8d07fb67-6316-41ef-a89b-2bc64c5fbb9d",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Oh I see. Thanks!",
        "createdAt" : "2020-01-08T02:08:48Z",
        "updatedAt" : "2020-01-08T02:08:48Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "c63c271d95e7864b69ca477c51e4e2d22b7d9029",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +26,30 @@\nclass PandasUDFType(object):\n    \"\"\"Pandas UDF Types. See :meth:`pyspark.sql.functions.pandas_udf`.\n    \"\"\"\n    SCALAR = PythonEvalType.SQL_SCALAR_PANDAS_UDF"
  },
  {
    "id" : "b2039a9e-3381-425d-a3fd-a2dd1a45ead5",
    "prId" : 27109,
    "prUrl" : "https://github.com/apache/spark/pull/27109#pullrequestreview-339612800",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "92a4ce74-9368-4bfa-9ad4-792d664e7aa6",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "sql.pandas.functions. PandasUDFType?",
        "createdAt" : "2020-01-08T02:04:46Z",
        "updatedAt" : "2020-01-08T02:08:04Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "c63c271d95e7864b69ca477c51e4e2d22b7d9029",
    "line" : 51,
    "diffHunk" : "@@ -1,1 +49,53 @@    :param returnType: the return type of the user-defined function. The value can be either a\n        :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.\n    :param functionType: an enum value in :class:`pyspark.sql.functions.PandasUDFType`.\n                         Default: SCALAR.\n"
  }
]