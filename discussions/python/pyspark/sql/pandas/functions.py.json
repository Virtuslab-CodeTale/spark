[
  {
    "id" : "d58a3906-0314-4727-b0ce-1a6e0f00b744",
    "prId" : 28052,
    "prUrl" : "https://github.com/apache/spark/pull/28052#pullrequestreview-383373335",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a879a05d-ca0e-4044-85d6-e61ebb4e3a3e",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Shall we show warn messaging in these cases that type hints are not supported yet?",
        "createdAt" : "2020-03-27T17:06:57Z",
        "updatedAt" : "2020-03-27T17:06:58Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "c546cbdc-9088-4ea2-876a-08bb3b86e918",
        "parentId" : "a879a05d-ca0e-4044-85d6-e61ebb4e3a3e",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Let me don't add the warning for now .. I am not yet sure if we should add the support of type hints in these pandas Function APIs .. Hopefully the documentation in the site could clarify that type hints are only supported in pandas UDFs.",
        "createdAt" : "2020-03-29T04:56:51Z",
        "updatedAt" : "2020-03-29T04:56:51Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "40586d45-f8e0-4c30-ab56-74601eb17801",
        "parentId" : "a879a05d-ca0e-4044-85d6-e61ebb4e3a3e",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "It implies a good point - there's a mismatch about how pandas UDF uses Python type hints because we force to set the Python type hints but the type hints are supposed to be completely optional ...",
        "createdAt" : "2020-03-29T04:58:46Z",
        "updatedAt" : "2020-03-29T04:58:46Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "371b8ebfc0d354539a96e3c0f4f6a776dbb212ce",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +393,397 @@            # evaluation type will always be set.\n            pass\n        elif len(argspec.annotations) > 0:\n            evalType = infer_eval_type(signature(f))\n            assert evalType is not None"
  },
  {
    "id" : "d5dfee32-5fc8-41bc-ad19-17ebd79f6ef9",
    "prId" : 27466,
    "prUrl" : "https://github.com/apache/spark/pull/27466#pullrequestreview-355094186",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "edec5760-4f02-4d2c-be4d-ef21fadf9ed3",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "duplicated?",
        "createdAt" : "2020-02-07T11:26:27Z",
        "updatedAt" : "2020-02-11T08:35:36Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "626ff3cb245ccd305f9a32d9c99f774ef154ebe1",
    "line" : 331,
    "diffHunk" : "@@ -1,1 +83,87 @@    type via `functionType` which will be deprecated in the future releases.\n\n    Note that the type hint should use `pandas.Series` in all cases but there is one variant\n    that `pandas.DataFrame` should be used for its input or output type hint instead when the input\n    or output column is of :class:`pyspark.sql.types.StructType`. The following example shows"
  },
  {
    "id" : "25868635-ca2e-4258-8a7e-88d946f6f8fd",
    "prId" : 27466,
    "prUrl" : "https://github.com/apache/spark/pull/27466#pullrequestreview-355902150",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3a449d89-a0e6-46ba-a2b9-7ff596bf6250",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I don't review it carefully and assume most of it are coped from the md file.",
        "createdAt" : "2020-02-10T12:11:04Z",
        "updatedAt" : "2020-02-11T08:35:36Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "14c4f734-b11c-46c2-86a9-f60b122f0338",
        "parentId" : "3a449d89-a0e6-46ba-a2b9-7ff596bf6250",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Yeah... it happened to be almost a copy ...",
        "createdAt" : "2020-02-10T12:35:52Z",
        "updatedAt" : "2020-02-11T08:35:36Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "626ff3cb245ccd305f9a32d9c99f774ef154ebe1",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +46,50 @@    Creates a pandas user defined function (a.k.a. vectorized user defined function).\n\n    Pandas UDFs are user defined functions that are executed by Spark using Arrow to transfer\n    data and Pandas to work with the data, which allows vectorized operations. A Pandas UDF\n    is defined using the `pandas_udf` as a decorator or to wrap the function, and no"
  },
  {
    "id" : "f9ddacb2-b4dd-43af-ba16-0858bfbccda9",
    "prId" : 27466,
    "prUrl" : "https://github.com/apache/spark/pull/27466#pullrequestreview-355889223",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d99f9521-522b-45b3-b96c-3f9fe36a1916",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "shall we introduce the new API first and legacy API later to promote the new API?",
        "createdAt" : "2020-02-10T12:12:24Z",
        "updatedAt" : "2020-02-11T08:35:36Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "626ff3cb245ccd305f9a32d9c99f774ef154ebe1",
    "line" : 320,
    "diffHunk" : "@@ -1,1 +72,76 @@    ...     return s.str.len()\n\n    Prior to Spark 3.0, the pandas UDF used `functionType` to decide the execution type as below:\n\n    >>> from pyspark.sql.functions import PandasUDFType"
  },
  {
    "id" : "ddaf0a0c-642c-4cb9-9ba2-f7281bee05a9",
    "prId" : 27466,
    "prUrl" : "https://github.com/apache/spark/pull/27466#pullrequestreview-355899784",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "83a50bbc-4306-4e28-ad4a-1367609f5a65",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "what about input types?",
        "createdAt" : "2020-02-10T12:13:05Z",
        "updatedAt" : "2020-02-11T08:35:36Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "7245ee71-4ebe-449b-a01f-935b81f7544a",
        "parentId" : "83a50bbc-4306-4e28-ad4a-1367609f5a65",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "They are same.",
        "createdAt" : "2020-02-10T12:31:39Z",
        "updatedAt" : "2020-02-11T08:35:36Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "626ff3cb245ccd305f9a32d9c99f774ef154ebe1",
    "line" : 532,
    "diffHunk" : "@@ -1,1 +283,287 @@        :class:`pyspark.sql.types.ArrayType` of :class:`pyspark.sql.types.TimestampType` and\n        nested :class:`pyspark.sql.types.StructType`\n        are currently not supported as output types.\n\n    .. seealso:: :meth:`pyspark.sql.DataFrame.mapInPandas`"
  }
]