[
  {
    "id" : "354f65e2-db1e-416c-8053-e04c03ca53a2",
    "prId" : 32026,
    "prUrl" : "https://github.com/apache/spark/pull/32026#pullrequestreview-626807546",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2020e626-eb9b-4494-8ec6-b02f91eefab6",
        "parentId" : null,
        "authorId" : "f6398a50-7f9b-4c64-9d3a-f384404338b3",
        "body" : "mypy lint\r\n```\r\npython/pyspark/sql/pandas/serializers.py:23: error: Module 'pyspark.sql.types' has no attribute '_has_udt'\r\n```\r\nI think it is wrongly reported by mypy. I don't know how to make it happy.",
        "createdAt" : "2021-04-02T06:17:55Z",
        "updatedAt" : "2021-04-24T07:13:10Z",
        "lastEditedBy" : "f6398a50-7f9b-4c64-9d3a-f384404338b3",
        "tags" : [
        ]
      }
    ],
    "commit" : "5845ee2419515b9243ea4396005f731eb294b6a9",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +22,26 @@from pyspark.serializers import Serializer, read_int, write_int, UTF8Deserializer\nfrom pyspark.sql.types import DataType, UserDefinedType, StructType, \\\n    _has_udt\nfrom pyspark.sql.pandas.types import to_arrow_type, _serialize_pandas_with_udt\n"
  },
  {
    "id" : "ca4fc47b-3a51-4d4c-a6ab-b08233881049",
    "prId" : 29951,
    "prUrl" : "https://github.com/apache/spark/pull/29951#pullrequestreview-502639956",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5d2925f6-c107-4ca4-b35c-499b82cdfdb9",
        "parentId" : null,
        "authorId" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "body" : "errors during safe conversion will be `ArrowInvalid`, which subclasses ValueError",
        "createdAt" : "2020-10-06T07:08:23Z",
        "updatedAt" : "2020-10-06T07:08:23Z",
        "lastEditedBy" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "tags" : [
        ]
      }
    ],
    "commit" : "e7a09e9dce49f6246957cb1965c26dbe846acbe8",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +154,158 @@            try:\n                array = pa.Array.from_pandas(s, mask=mask, type=t, safe=self._safecheck)\n            except ValueError as e:\n                if self._safecheck:\n                    error_msg = \"Exception thrown when converting pandas.Series (%s) to \" + \\"
  },
  {
    "id" : "24168b92-6a19-42f1-87c7-6dc1df82213c",
    "prId" : 29951,
    "prUrl" : "https://github.com/apache/spark/pull/29951#pullrequestreview-503441125",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9b9d3f62-3872-403b-adce-0de01e8eadd1",
        "parentId" : null,
        "authorId" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "body" : "Now that we dropped Python 2, this seems more appropriate",
        "createdAt" : "2020-10-06T07:08:26Z",
        "updatedAt" : "2020-10-06T07:08:27Z",
        "lastEditedBy" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "tags" : [
        ]
      },
      {
        "id" : "17c86eaa-3a0e-4737-b7a5-035d0750bbad",
        "parentId" : "9b9d3f62-3872-403b-adce-0de01e8eadd1",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "In `branch-3.0`.\r\n\r\n```\r\n  File \"/home/jenkins/workspace/spark-branch-3.0-test-sbt-hadoop-2.7-hive-1.2/python/pyspark/sql/pandas/serializers.py\", line 166\r\n    raise ValueError(error_msg % (s.dtype, t)) from e\r\n                                              ^\r\nSyntaxError: invalid syntax\r\n```",
        "createdAt" : "2020-10-07T00:07:32Z",
        "updatedAt" : "2020-10-07T00:07:32Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "e7a09e9dce49f6246957cb1965c26dbe846acbe8",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +161,165 @@                                \"can be disabled by using SQL config \" + \\\n                                \"`spark.sql.execution.pandas.convertToArrowArraySafely`.\"\n                    raise ValueError(error_msg % (s.dtype, t)) from e\n                else:\n                    raise e"
  },
  {
    "id" : "ffcbfaba-55b1-4a26-b5ab-134374f0f3c9",
    "prId" : 28957,
    "prUrl" : "https://github.com/apache/spark/pull/28957#pullrequestreview-447019474",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "71fa6757-17ba-4245-9a93-56194993e8c8",
        "parentId" : null,
        "authorId" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "body" : "We might want to think about removing this as an option as a followup. It was mostly added because dataframe constructed with python < 3.6 could not guarantee the order of columns, but now it should match the given schema.",
        "createdAt" : "2020-07-13T02:53:51Z",
        "updatedAt" : "2020-07-13T09:52:43Z",
        "lastEditedBy" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "tags" : [
        ]
      },
      {
        "id" : "c0e764a6-cafe-4160-b07b-7aea790bd5ee",
        "parentId" : "71fa6757-17ba-4245-9a93-56194993e8c8",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Ah, right. sounds good!",
        "createdAt" : "2020-07-13T06:56:11Z",
        "updatedAt" : "2020-07-13T09:52:43Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "46b3476b-9944-464c-aafd-8f334ae3badc",
        "parentId" : "71fa6757-17ba-4245-9a93-56194993e8c8",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "and yes, maybe it should better be in a separate PR.",
        "createdAt" : "2020-07-13T06:58:56Z",
        "updatedAt" : "2020-07-13T09:52:43Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "f2356c80a105c0b74e578152282013c1d3b9c647",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +174,178 @@                    arrs_names = [(pa.array([], type=field.type), field.name) for field in t]\n                # Assign result columns by schema name if user labeled with strings\n                elif self._assign_cols_by_name and any(isinstance(name, str)\n                                                       for name in s.columns):\n                    arrs_names = [(create_array(s[field.name], field.type), field.name)"
  },
  {
    "id" : "4748650e-5241-4032-b5c3-ef00b3ab0780",
    "prId" : 28793,
    "prUrl" : "https://github.com/apache/spark/pull/28793#pullrequestreview-428522070",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "77c8a1a9-fbe5-4be7-9bdc-1c58843532a0",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Oh, nice!",
        "createdAt" : "2020-06-11T00:52:32Z",
        "updatedAt" : "2020-06-11T00:52:32Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "d2030cf4629a640b3b4b24e7ebe21dc32e645f36",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +144,148 @@        import pyarrow as pa\n        from pyspark.sql.pandas.types import _check_series_convert_timestamps_internal\n        from pandas.api.types import is_categorical\n        # Make input conform to [(series1, type1), (series2, type2), ...]\n        if not isinstance(series, (list, tuple)) or \\"
  }
]