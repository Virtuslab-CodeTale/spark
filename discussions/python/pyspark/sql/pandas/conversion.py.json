[
  {
    "id" : "1946ec7f-8961-4dfe-9c0b-8ad755f44744",
    "prId" : 32026,
    "prUrl" : "https://github.com/apache/spark/pull/32026#pullrequestreview-626740372",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2e152242-471f-453c-a02b-47e40043630f",
        "parentId" : null,
        "authorId" : "4a97d995-edb8-416d-ba1b-29e030885f7e",
        "body" : "this seems to be only a rename? should we leave them as they are to reduce the size of PR?",
        "createdAt" : "2021-04-01T16:17:52Z",
        "updatedAt" : "2021-04-24T07:13:10Z",
        "lastEditedBy" : "4a97d995-edb8-416d-ba1b-29e030885f7e",
        "tags" : [
        ]
      },
      {
        "id" : "eef730ad-75bd-4c77-9031-7235ddf08f0d",
        "parentId" : "2e152242-471f-453c-a02b-47e40043630f",
        "authorId" : "f6398a50-7f9b-4c64-9d3a-f384404338b3",
        "body" : "Yes. It is renamed to indicate it is arrow data with datatype (Spark SQL DataType or Arrow DataType).\r\n\r\nIn `serializers.py`, `dt` is for Spark SQL DataType, `pdt` is for pyarrow DataType.",
        "createdAt" : "2021-04-02T01:16:57Z",
        "updatedAt" : "2021-04-24T07:13:10Z",
        "lastEditedBy" : "f6398a50-7f9b-4c64-9d3a-f384404338b3",
        "tags" : [
        ]
      },
      {
        "id" : "cbb5613c-d981-4506-86aa-30177be8a4f1",
        "parentId" : "2e152242-471f-453c-a02b-47e40043630f",
        "authorId" : "f6398a50-7f9b-4c64-9d3a-f384404338b3",
        "body" : "Well, I should use `adt` or `padt` for PyArrow Data Type and `pdt` for Pandas DataType.",
        "createdAt" : "2021-04-02T01:25:02Z",
        "updatedAt" : "2021-04-24T07:13:10Z",
        "lastEditedBy" : "f6398a50-7f9b-4c64-9d3a-f384404338b3",
        "tags" : [
        ]
      }
    ],
    "commit" : "5845ee2419515b9243ea4396005f731eb294b6a9",
    "line" : 64,
    "diffHunk" : "@@ -1,1 +475,479 @@        # Create list of Arrow (columns, type) for serializer dump_stream\n        # Type can be Spark SQL Data Type or Arrow Data Type\n        arrow_data_with_t = [\n            [(c, t) for (_, c), t in zip(pdf_slice.iteritems(), data_types)]\n            for pdf_slice in pdf_slices"
  },
  {
    "id" : "5a0bb66a-0d73-443a-b114-8674c0e931f3",
    "prId" : 32026,
    "prUrl" : "https://github.com/apache/spark/pull/32026#pullrequestreview-626739916",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9fbbc1c4-1133-4c9c-ad95-62f4881b72cb",
        "parentId" : null,
        "authorId" : "4a97d995-edb8-416d-ba1b-29e030885f7e",
        "body" : "these two `_` prefixed functions are used outside their files. should we remove the `_` prefix, because they are not private anymore.",
        "createdAt" : "2021-04-01T16:19:17Z",
        "updatedAt" : "2021-04-24T07:13:10Z",
        "lastEditedBy" : "4a97d995-edb8-416d-ba1b-29e030885f7e",
        "tags" : [
        ]
      },
      {
        "id" : "e913ffa8-665a-48da-9e80-1a441ccc6c7b",
        "parentId" : "9fbbc1c4-1133-4c9c-ad95-62f4881b72cb",
        "authorId" : "f6398a50-7f9b-4c64-9d3a-f384404338b3",
        "body" : "`git grep _make_type_verifier`, there are other use cases which a function starts with `_` but is used outside where they are defined.",
        "createdAt" : "2021-04-02T01:23:05Z",
        "updatedAt" : "2021-04-24T07:13:10Z",
        "lastEditedBy" : "f6398a50-7f9b-4c64-9d3a-f384404338b3",
        "tags" : [
        ]
      }
    ],
    "commit" : "5845ee2419515b9243ea4396005f731eb294b6a9",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +24,28 @@    DoubleType, BooleanType, MapType, TimestampType, StructType, DataType, \\\n    IntegralType, _has_udt\nfrom pyspark.sql.pandas.types import _deserialize_pandas_with_udt\nfrom pyspark.traceback_utils import SCCallSiteSync\n"
  },
  {
    "id" : "047faf84-49a1-4ac4-a592-e2a9f53b7e6e",
    "prId" : 29818,
    "prUrl" : "https://github.com/apache/spark/pull/29818#pullrequestreview-509029779",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1f5eae89-a23b-47d6-aafc-a8ef3b448e47",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Yeah, I think it's better. In which case should we disable? Maybe we should enable it by default.",
        "createdAt" : "2020-09-21T13:58:23Z",
        "updatedAt" : "2021-01-28T20:44:29Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "c1100329-bbbc-4fa3-b34d-19b08d73d6b5",
        "parentId" : "1f5eae89-a23b-47d6-aafc-a8ef3b448e47",
        "authorId" : "d1541f17-4fdb-40b8-a545-d0d79a158f71",
        "body" : "The worry is with running into things like https://github.com/pandas-dev/pandas/issues/35530 in which case the user may appreciate an escape hatch.",
        "createdAt" : "2020-09-21T14:20:32Z",
        "updatedAt" : "2021-01-28T20:44:29Z",
        "lastEditedBy" : "d1541f17-4fdb-40b8-a545-d0d79a158f71",
        "tags" : [
        ]
      },
      {
        "id" : "ab600920-3512-4048-a163-e33143b4df9b",
        "parentId" : "1f5eae89-a23b-47d6-aafc-a8ef3b448e47",
        "authorId" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "body" : "yeah, I think this option can lead to other side effects, best to disable by default I think.",
        "createdAt" : "2020-10-15T06:34:39Z",
        "updatedAt" : "2021-01-28T20:44:29Z",
        "lastEditedBy" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "tags" : [
        ]
      }
    ],
    "commit" : "64d03012616c9bc56b97693d1fdf8132493deb0e",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +128,132 @@                                'use_threads': False,\n                            })\n                        pdf = table.to_pandas(**pandas_options)\n                        # Rename back to the original column names.\n                        pdf.columns = self.columns"
  },
  {
    "id" : "74a43e3c-3310-4fc3-b963-346eedef0743",
    "prId" : 29818,
    "prUrl" : "https://github.com/apache/spark/pull/29818#pullrequestreview-508827943",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "067659ce-b190-4e90-abee-578f3e17b216",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Would you mind leaving a comment on the codes about this set of parameter configurations?",
        "createdAt" : "2020-10-14T04:55:33Z",
        "updatedAt" : "2021-01-28T20:44:29Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "12079a15-8f86-4210-8326-1ae8187ff83a",
        "parentId" : "067659ce-b190-4e90-abee-578f3e17b216",
        "authorId" : "d1541f17-4fdb-40b8-a545-d0d79a158f71",
        "body" : "Done.",
        "createdAt" : "2020-10-14T22:57:46Z",
        "updatedAt" : "2021-01-28T20:44:29Z",
        "lastEditedBy" : "d1541f17-4fdb-40b8-a545-d0d79a158f71",
        "tags" : [
        ]
      }
    ],
    "commit" : "64d03012616c9bc56b97693d1fdf8132493deb0e",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +124,128 @@                            # use_threads - convert one column at a time\n                            pandas_options.update({\n                                'self_destruct': True,\n                                'split_blocks': True,\n                                'use_threads': False,"
  },
  {
    "id" : "b7156777-4b6e-453e-a762-ab04f84ecf31",
    "prId" : 29818,
    "prUrl" : "https://github.com/apache/spark/pull/29818#pullrequestreview-510624981",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2c83b733-03a1-4fc0-b4d1-44a41af8c02f",
        "parentId" : null,
        "authorId" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "body" : "does this have any bearing on the buffers self destructing? is it taking into account how many reference counts there are before destructing?",
        "createdAt" : "2020-10-15T06:33:12Z",
        "updatedAt" : "2021-01-28T20:44:29Z",
        "lastEditedBy" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "tags" : [
        ]
      },
      {
        "id" : "ea22e122-6a93-4a78-ac79-131a70c19382",
        "parentId" : "2c83b733-03a1-4fc0-b4d1-44a41af8c02f",
        "authorId" : "d1541f17-4fdb-40b8-a545-d0d79a158f71",
        "body" : "Yes - we don't want to hold on to any other references to the buffers, we want the Table to be the only owner. I'll clarify this part here.",
        "createdAt" : "2020-10-16T16:27:58Z",
        "updatedAt" : "2021-01-28T20:44:29Z",
        "lastEditedBy" : "d1541f17-4fdb-40b8-a545-d0d79a158f71",
        "tags" : [
        ]
      }
    ],
    "commit" : "64d03012616c9bc56b97693d1fdf8132493deb0e",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +113,117 @@                        # Ensure only the table has a reference to the batches, so that\n                        # self_destruct (if enabled) is effective\n                        del batches\n                        # Pandas DataFrame created from PyArrow uses datetime64[ns] for date type\n                        # values, but we should use datetime.date to match the behavior with when"
  },
  {
    "id" : "51113549-6959-4cdf-abae-4588332b837c",
    "prId" : 29818,
    "prUrl" : "https://github.com/apache/spark/pull/29818#pullrequestreview-510626387",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d2a889df-661a-4c5f-8690-968a3e5ea02a",
        "parentId" : null,
        "authorId" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "body" : "Is this necessary to set with `self_destruct`? It might lead to Pandas doing more memory allocation later, I believe.",
        "createdAt" : "2020-10-15T06:44:03Z",
        "updatedAt" : "2021-01-28T20:44:29Z",
        "lastEditedBy" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "tags" : [
        ]
      },
      {
        "id" : "d95c0371-4a1b-4101-af2a-216450ce909a",
        "parentId" : "d2a889df-661a-4c5f-8690-968a3e5ea02a",
        "authorId" : "d1541f17-4fdb-40b8-a545-d0d79a158f71",
        "body" : "Not quite _necessary_ but good to have, else we may end up allocating a large block if there are a lot of columns of the same type, defeating the point. Pandas may reconsolidate later but that's part of the issue of using Pandas.",
        "createdAt" : "2020-10-16T16:29:54Z",
        "updatedAt" : "2021-01-28T20:44:29Z",
        "lastEditedBy" : "d1541f17-4fdb-40b8-a545-d0d79a158f71",
        "tags" : [
        ]
      }
    ],
    "commit" : "64d03012616c9bc56b97693d1fdf8132493deb0e",
    "line" : 25,
    "diffHunk" : "@@ -1,1 +125,129 @@                            pandas_options.update({\n                                'self_destruct': True,\n                                'split_blocks': True,\n                                'use_threads': False,\n                            })"
  },
  {
    "id" : "ca68f3db-6e63-4eac-a303-9603311e7c86",
    "prId" : 29818,
    "prUrl" : "https://github.com/apache/spark/pull/29818#pullrequestreview-573550773",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6eacd9b5-4bf7-4184-9796-ac468df30700",
        "parentId" : null,
        "authorId" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "body" : "could you make a note that this is to reallocate the array?",
        "createdAt" : "2021-01-21T17:12:49Z",
        "updatedAt" : "2021-01-28T20:44:29Z",
        "lastEditedBy" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "tags" : [
        ]
      }
    ],
    "commit" : "64d03012616c9bc56b97693d1fdf8132493deb0e",
    "line" : 69,
    "diffHunk" : "@@ -1,1 +276,280 @@                        batch_or_indices = pa.RecordBatch.from_arrays([\n                            # This call actually reallocates the array\n                            pa.concat_arrays([array])\n                            for array in batch_or_indices\n                        ], schema=batch_or_indices.schema)"
  },
  {
    "id" : "b6ff2285-1228-4a65-b093-023014654d1c",
    "prId" : 29284,
    "prUrl" : "https://github.com/apache/spark/pull/29284#pullrequestreview-457994401",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "adce8e7c-e3df-4601-827d-e6c6fa54d252",
        "parentId" : null,
        "authorId" : "7e665d8b-d739-4edf-88c8-7379ff8585c2",
        "body" : "let's use\r\n```\r\nstart_end_pairs = ((i * length // num_slices, (i + 1) * length // num_slices) for i in range(0, num_slices))\r\npdf_slices = (pdf.iloc[start, end] for start, end in start_end_pairs if start < end)\r\n```\r\nto filter out empty slices (it will happen when len(pdf) <  num_slices, which my leads to error). ",
        "createdAt" : "2020-07-30T00:59:29Z",
        "updatedAt" : "2020-07-30T01:00:19Z",
        "lastEditedBy" : "7e665d8b-d739-4edf-88c8-7379ff8585c2",
        "tags" : [
        ]
      }
    ],
    "commit" : "ef9f646ca795af99b7b395290e2aec1e3b1dd36b",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +409,413 @@        pdf_slices = (pdf.iloc[i * length // num_slices: (i + 1) * length // num_slices]\n                      for i in range(0, num_slices))\n\n        # Create list of Arrow (columns, type) for serializer dump_stream\n        arrow_data = [[(c, t) for (_, c), t in zip(pdf_slice.iteritems(), arrow_types)]"
  },
  {
    "id" : "c7020104-d6de-40bb-8a0c-cad7384d483b",
    "prId" : 28928,
    "prUrl" : "https://github.com/apache/spark/pull/28928#pullrequestreview-438024896",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8e80bfb6-2d4a-4de9-bac1-2270b811b0e0",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "Thank you for fixing this! \r\n\r\n> While standard Python / Numpy expressions for selecting and setting are intuitive and come in handy for interactive work, for production code, we recommend the optimized pandas data access methods, .at, .iat, .loc and .iloc.\r\n\r\nIs it the only place? ",
        "createdAt" : "2020-06-26T05:36:06Z",
        "updatedAt" : "2020-06-26T05:36:07Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "09052352-6349-40c8-ae20-faf2989e0ba5",
        "parentId" : "8e80bfb6-2d4a-4de9-bac1-2270b811b0e0",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "As far as I can tell, yes.",
        "createdAt" : "2020-06-26T05:37:38Z",
        "updatedAt" : "2020-06-26T05:37:39Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "612426e34e29229aa2187e8775ee0e453288c75d",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +414,418 @@        # Slice the DataFrame to be batched\n        step = -(-len(pdf) // self.sparkContext.defaultParallelism)  # round int up\n        pdf_slices = (pdf.iloc[start:start + step] for start in xrange(0, len(pdf), step))\n\n        # Create list of Arrow (columns, type) for serializer dump_stream"
  },
  {
    "id" : "fd061c76-66e8-4467-9386-3fdad2bf1430",
    "prId" : 28743,
    "prUrl" : "https://github.com/apache/spark/pull/28743#pullrequestreview-427959183",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "061f2d0c-cdf3-41bd-a912-2453e118c09a",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Why don't we follow nullability anymore?",
        "createdAt" : "2020-06-10T03:46:05Z",
        "updatedAt" : "2020-06-11T14:37:02Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "6d1210ff-8570-4936-8662-faea72ae52a2",
        "parentId" : "061f2d0c-cdf3-41bd-a912-2453e118c09a",
        "authorId" : "19b2f27e-11d4-4e35-89e0-140cc4b3f40b",
        "body" : "`infer_type` only returns a type, not a `field`, which would supposedly have nullability information. But it appears that in the implementation of `Schema.from_pandas` ([link](https://github.com/apache/arrow/blob/b058cf0d1c26ad7984c104bb84322cc7dcc66f00/python/pyarrow/types.pxi#L1328)), inferring nullability was not actually done and the default `nullable=True` would always be returned. So this change is just following the existing behaviour of `Schema.from_pandas`.",
        "createdAt" : "2020-06-10T04:50:08Z",
        "updatedAt" : "2020-06-11T14:37:02Z",
        "lastEditedBy" : "19b2f27e-11d4-4e35-89e0-140cc4b3f40b",
        "tags" : [
        ]
      },
      {
        "id" : "6ea059db-6554-4c44-9532-fa039f5dcaed",
        "parentId" : "061f2d0c-cdf3-41bd-a912-2453e118c09a",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Let's add a comment here to explain it?",
        "createdAt" : "2020-06-10T06:28:58Z",
        "updatedAt" : "2020-06-11T14:37:02Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "47fcdcf8-d28d-47df-9d98-e6b791e00efe",
        "parentId" : "061f2d0c-cdf3-41bd-a912-2453e118c09a",
        "authorId" : "19b2f27e-11d4-4e35-89e0-140cc4b3f40b",
        "body" : "Sounds good, will update with a comment.\r\n\r\nAlternatively, `any(s.isna())` could be checked if we wanted to actively infer nullability here. This would change existing behavior as well as being inconsistent with the non-Arrow path, though, which similarly defaults to inferred types being nullable: https://github.com/apache/spark/blob/43063e2db2bf7469f985f1954d8615b95cf5c578/python/pyspark/sql/types.py#L1069",
        "createdAt" : "2020-06-10T11:26:30Z",
        "updatedAt" : "2020-06-11T14:37:02Z",
        "lastEditedBy" : "19b2f27e-11d4-4e35-89e0-140cc4b3f40b",
        "tags" : [
        ]
      }
    ],
    "commit" : "01fb6a4c25f830ec6a52d1030b2a6790a3dbae4f",
    "line" : 31,
    "diffHunk" : "@@ -1,1 +410,414 @@                # nullability is not determined on types inferred by Arrow or\n                # by the non-Arrow conversion path, so default to nullable\n                struct.add(name, from_arrow_type(t), nullable=True)\n            schema = struct\n"
  },
  {
    "id" : "0a6161e8-a7f3-4102-94ed-aa63ced240c6",
    "prId" : 27358,
    "prUrl" : "https://github.com/apache/spark/pull/27358#pullrequestreview-348424168",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8ae43e15-e2e9-4557-8442-f1cef72eea0b",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Is it different? Doesn't this also assign the series back to the DataFrame?",
        "createdAt" : "2020-01-27T00:31:04Z",
        "updatedAt" : "2020-01-27T00:31:04Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "ec6dd328-d6d0-4a7d-a5f6-30908b5f8bd1",
        "parentId" : "8ae43e15-e2e9-4557-8442-f1cef72eea0b",
        "authorId" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "body" : "Yeah, for the case of timestamps making a copy is unavailable. This is just to prevent non-timestamp columns that were also causing a copy when assigned back to the DataFrame",
        "createdAt" : "2020-01-27T00:59:33Z",
        "updatedAt" : "2020-01-27T00:59:33Z",
        "lastEditedBy" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "tags" : [
        ]
      },
      {
        "id" : "497a1123-4ef6-4c6c-ad07-64b17e9d73b8",
        "parentId" : "8ae43e15-e2e9-4557-8442-f1cef72eea0b",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "ok. looks good then. thanks!",
        "createdAt" : "2020-01-27T01:52:40Z",
        "updatedAt" : "2020-01-27T01:52:40Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "bfed1518-2654-4681-bc87-58e41ab02ed3",
        "parentId" : "8ae43e15-e2e9-4557-8442-f1cef72eea0b",
        "authorId" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "body" : "Thanks @viirya !",
        "createdAt" : "2020-01-27T02:25:12Z",
        "updatedAt" : "2020-01-27T02:25:12Z",
        "lastEditedBy" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "tags" : [
        ]
      }
    ],
    "commit" : "3a61dd132950ddd1163957dc0a24b3368994d251",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +112,116 @@                        for field in self.schema:\n                            if isinstance(field.dataType, TimestampType):\n                                pdf[field.name] = \\\n                                    _check_series_localize_timestamps(pdf[field.name], timezone)\n                        return pdf"
  },
  {
    "id" : "6c5ac0e0-6f63-403d-b013-8ccd581d9b01",
    "prId" : 27109,
    "prUrl" : "https://github.com/apache/spark/pull/27109#pullrequestreview-339592526",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7f50f116-82e8-451b-a9f3-1c518244409d",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Here, I piggyback a nit fix - I made this as a staticmethod. It was a function at module.",
        "createdAt" : "2020-01-08T00:39:35Z",
        "updatedAt" : "2020-01-08T00:39:35Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "c63c271d95e7864b69ca477c51e4e2d22b7d9029",
    "line" : 166,
    "diffHunk" : "@@ -1,1 +164,168 @@\n    @staticmethod\n    def _to_corrected_pandas_type(dt):\n        \"\"\"\n        When converting Spark SQL records to Pandas :class:`DataFrame`, the inferred data type"
  },
  {
    "id" : "9f5f8562-31bf-4406-b119-192262bc4583",
    "prId" : 27109,
    "prUrl" : "https://github.com/apache/spark/pull/27109#pullrequestreview-339592725",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f91c4079-bba1-46e0-900f-73f9705abba6",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Here, I piggyback a nit fix. It was `_collectAsArrow`. Unless it's to match with Scala side, it has to follow snake_naming per PEP 8. ",
        "createdAt" : "2020-01-08T00:40:18Z",
        "updatedAt" : "2020-01-08T00:40:19Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "c63c271d95e7864b69ca477c51e4e2d22b7d9029",
    "line" : 192,
    "diffHunk" : "@@ -1,1 +190,194 @@            return None\n\n    def _collect_as_arrow(self):\n        \"\"\"\n        Returns all records as a list of ArrowRecordBatches, pyarrow must be installed"
  },
  {
    "id" : "43bc399e-04fe-48b5-a43d-ac1f32f3750a",
    "prId" : 27109,
    "prUrl" : "https://github.com/apache/spark/pull/27109#pullrequestreview-339592864",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fd5f3798-e803-4cf9-988e-9aefa42d229f",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Here, this assert was added. IDE can detect and know `self` is `DataFrame`.",
        "createdAt" : "2020-01-08T00:40:51Z",
        "updatedAt" : "2020-01-08T00:40:51Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "c63c271d95e7864b69ca477c51e4e2d22b7d9029",
    "line" : 201,
    "diffHunk" : "@@ -1,1 +199,203 @@        from pyspark.sql.dataframe import DataFrame\n\n        assert isinstance(self, DataFrame)\n\n        with SCCallSiteSync(self._sc):"
  },
  {
    "id" : "01ebc983-726d-4a82-8e6b-2c1a9a7c107a",
    "prId" : 27109,
    "prUrl" : "https://github.com/apache/spark/pull/27109#pullrequestreview-339594693",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0e0a3a63-d09d-4b8b-a89c-d16684cf9a96",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I piggyback another change here - `doctest.REPORT_NDIFF` was added. Some files have it and some files don't. I decided to consistently add.",
        "createdAt" : "2020-01-08T00:48:01Z",
        "updatedAt" : "2020-01-08T00:48:01Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "c63c271d95e7864b69ca477c51e4e2d22b7d9029",
    "line" : 424,
    "diffHunk" : "@@ -1,1 +422,426 @@    (failure_count, test_count) = doctest.testmod(\n        pyspark.sql.pandas.conversion, globs=globs,\n        optionflags=doctest.ELLIPSIS | doctest.NORMALIZE_WHITESPACE | doctest.REPORT_NDIFF)\n    spark.stop()\n    if failure_count:"
  },
  {
    "id" : "e9302594-11c2-4bef-be57-4259720ae808",
    "prId" : 27109,
    "prUrl" : "https://github.com/apache/spark/pull/27109#pullrequestreview-343864816",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a599a92e-7e95-4807-9228-7d8e62388234",
        "parentId" : null,
        "authorId" : "981b170c-729a-429c-b115-0350ea50b32b",
        "body" : "Instead of shading `SparkSession.createDataFrame` and later [having to disambiguate](https://github.com/HyukjinKwon/spark/blob/c63c271d95e7864b69ca477c51e4e2d22b7d9029/python/pyspark/sql/session.py#L600-L601) it would be more idiomatic to just name mangle [like this](https://github.com/zero323/spark/blob/228c1f3896f871aae303eb306d6233bcf28edf19/python/pyspark/sql/pandas/conversion.py#L226)\r\n\r\n```python\r\ndef __createDataFrame(self, data, schema=None, samplingRatio=None, verifySchema=True):\r\n```\r\n\r\n[and later](https://github.com/zero323/spark/blob/228c1f3896f871aae303eb306d6233bcf28edf19/python/pyspark/sql/session.py#L600)\r\n\r\n```python\r\nreturn self._SparkConversionMixin__createDataFrame(\r\n```\r\n\r\nIt would be also easier to add new mixins later, if that proves to be useful for some reason ([here is possible diff](https://github.com/apache/spark/compare/master...zero323:SPARK-30434-mangle?expand=1)).\r\n\r\nAdditionally we could limit  the responsibility to data and schema preparation only ([diff for this](https://github.com/apache/spark/compare/master...zero323:SPARK-30434-prepare?expand=1)). Additionally to benefits mentioned above, it allow us to decouple mix in from `SparkSession` (as we'd only need `_wrapped._conf`).\r\n\r\n ",
        "createdAt" : "2020-01-15T15:58:02Z",
        "updatedAt" : "2020-01-15T15:58:03Z",
        "lastEditedBy" : "981b170c-729a-429c-b115-0350ea50b32b",
        "tags" : [
        ]
      },
      {
        "id" : "b136439e-a154-428e-a8b6-f282fa49d03b",
        "parentId" : "a599a92e-7e95-4807-9228-7d8e62388234",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I did the current way so that `SparkConversionMixin.createDataFrame` can work alone by itself. I didn't like the second way because it's weird to return both `schema` and `data`.",
        "createdAt" : "2020-01-16T02:50:04Z",
        "updatedAt" : "2020-01-16T02:50:04Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "0a0d4255-dfd2-4149-9295-0a9945cc43b0",
        "parentId" : "a599a92e-7e95-4807-9228-7d8e62388234",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "`_SparkConversionMixin__createDataFrame` looks fine in a way that it can distinguish which mixin is used but we don't really use mangle name yet in the current codebase too.",
        "createdAt" : "2020-01-16T03:01:23Z",
        "updatedAt" : "2020-01-16T03:01:24Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "e816e781-cb14-403e-a610-e99f9bd631cc",
        "parentId" : "a599a92e-7e95-4807-9228-7d8e62388234",
        "authorId" : "981b170c-729a-429c-b115-0350ea50b32b",
        "body" : "> I did the current way so that SparkConversionMixin.createDataFrame can work alone by itself\r\n\r\nBut I cannot, can it? It still has to be mixed-in into something that has these methods.\r\n\r\n>  I didn't like the second way because it's weird to return both schema and data.\r\n\r\nMatter of taste I guess.\r\n\r\n> _SparkConversionMixin__createDataFrame looks fine in a way that it can distinguish which mixin is used but we don't really use mangle name yet in the current codebase too.\r\n\r\nThat's true, though I am not aware of  case might occur. We use mixins for `Params`, but there conflicts are either unlikely or irrelevant. Here both methods are used, and it is theoretically possible that other similar might follow. Let's say `pypark.sql.ray.conversions` or ``pypark.sql.dask.conversions`, either here, or in the 3rd party extensions. In such cases we might prefer to have consistent interface.\r\n\r\nAnyway, I don't have very strong opinion about this. I've mentioned this primarily because corresponding type hints looked funky.\r\n\r\nThere is however  one  immediate side effect of the current approach â€’ it  makes passing methods values a bit unreliable (and by extension methods that depend on it, like monkey patching).",
        "createdAt" : "2020-01-16T12:05:15Z",
        "updatedAt" : "2020-01-16T12:05:15Z",
        "lastEditedBy" : "981b170c-729a-429c-b115-0350ea50b32b",
        "tags" : [
        ]
      }
    ],
    "commit" : "c63c271d95e7864b69ca477c51e4e2d22b7d9029",
    "line" : 226,
    "diffHunk" : "@@ -1,1 +224,228 @@    can use this class.\n    \"\"\"\n    def createDataFrame(self, data, schema=None, samplingRatio=None, verifySchema=True):\n        from pyspark.sql import SparkSession\n"
  },
  {
    "id" : "73b79bd7-4459-41aa-a499-13938826268c",
    "prId" : 27109,
    "prUrl" : "https://github.com/apache/spark/pull/27109#pullrequestreview-343649109",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6fd30248-0d0e-48f3-b8f9-fd42df6c0bc3",
        "parentId" : null,
        "authorId" : "981b170c-729a-429c-b115-0350ea50b32b",
        "body" : "Why `..., samplingRatio, samplingRatio)`? Should it be `..., verifySchema, samplingRatio)`?",
        "createdAt" : "2020-01-15T15:59:11Z",
        "updatedAt" : "2020-01-15T15:59:11Z",
        "lastEditedBy" : "981b170c-729a-429c-b115-0350ea50b32b",
        "tags" : [
        ]
      },
      {
        "id" : "c4af73ff-8d6e-4888-9875-e0f665012ec3",
        "parentId" : "6fd30248-0d0e-48f3-b8f9-fd42df6c0bc3",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Thanks for pointing this out. I made a PR - https://github.com/apache/spark/pull/27225",
        "createdAt" : "2020-01-16T02:59:10Z",
        "updatedAt" : "2020-01-16T02:59:11Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "c63c271d95e7864b69ca477c51e4e2d22b7d9029",
    "line" : 270,
    "diffHunk" : "@@ -1,1 +268,272 @@                    raise\n        data = self._convert_from_pandas(data, schema, timezone)\n        return self._create_dataframe(data, schema, samplingRatio, samplingRatio)\n\n    def _convert_from_pandas(self, pdf, schema, timezone):"
  }
]