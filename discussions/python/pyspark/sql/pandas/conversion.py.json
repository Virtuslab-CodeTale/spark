[
  {
    "id" : "1946ec7f-8961-4dfe-9c0b-8ad755f44744",
    "prId" : 32026,
    "prUrl" : "https://github.com/apache/spark/pull/32026#pullrequestreview-626740372",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2e152242-471f-453c-a02b-47e40043630f",
        "parentId" : null,
        "authorId" : "4a97d995-edb8-416d-ba1b-29e030885f7e",
        "body" : "this seems to be only a rename? should we leave them as they are to reduce the size of PR?",
        "createdAt" : "2021-04-01T16:17:52Z",
        "updatedAt" : "2021-04-24T07:13:10Z",
        "lastEditedBy" : "4a97d995-edb8-416d-ba1b-29e030885f7e",
        "tags" : [
        ]
      },
      {
        "id" : "eef730ad-75bd-4c77-9031-7235ddf08f0d",
        "parentId" : "2e152242-471f-453c-a02b-47e40043630f",
        "authorId" : "f6398a50-7f9b-4c64-9d3a-f384404338b3",
        "body" : "Yes. It is renamed to indicate it is arrow data with datatype (Spark SQL DataType or Arrow DataType).\r\n\r\nIn `serializers.py`, `dt` is for Spark SQL DataType, `pdt` is for pyarrow DataType.",
        "createdAt" : "2021-04-02T01:16:57Z",
        "updatedAt" : "2021-04-24T07:13:10Z",
        "lastEditedBy" : "f6398a50-7f9b-4c64-9d3a-f384404338b3",
        "tags" : [
        ]
      },
      {
        "id" : "cbb5613c-d981-4506-86aa-30177be8a4f1",
        "parentId" : "2e152242-471f-453c-a02b-47e40043630f",
        "authorId" : "f6398a50-7f9b-4c64-9d3a-f384404338b3",
        "body" : "Well, I should use `adt` or `padt` for PyArrow Data Type and `pdt` for Pandas DataType.",
        "createdAt" : "2021-04-02T01:25:02Z",
        "updatedAt" : "2021-04-24T07:13:10Z",
        "lastEditedBy" : "f6398a50-7f9b-4c64-9d3a-f384404338b3",
        "tags" : [
        ]
      }
    ],
    "commit" : "5845ee2419515b9243ea4396005f731eb294b6a9",
    "line" : 64,
    "diffHunk" : "@@ -1,1 +475,479 @@        # Create list of Arrow (columns, type) for serializer dump_stream\n        # Type can be Spark SQL Data Type or Arrow Data Type\n        arrow_data_with_t = [\n            [(c, t) for (_, c), t in zip(pdf_slice.iteritems(), data_types)]\n            for pdf_slice in pdf_slices"
  },
  {
    "id" : "5a0bb66a-0d73-443a-b114-8674c0e931f3",
    "prId" : 32026,
    "prUrl" : "https://github.com/apache/spark/pull/32026#pullrequestreview-626739916",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9fbbc1c4-1133-4c9c-ad95-62f4881b72cb",
        "parentId" : null,
        "authorId" : "4a97d995-edb8-416d-ba1b-29e030885f7e",
        "body" : "these two `_` prefixed functions are used outside their files. should we remove the `_` prefix, because they are not private anymore.",
        "createdAt" : "2021-04-01T16:19:17Z",
        "updatedAt" : "2021-04-24T07:13:10Z",
        "lastEditedBy" : "4a97d995-edb8-416d-ba1b-29e030885f7e",
        "tags" : [
        ]
      },
      {
        "id" : "e913ffa8-665a-48da-9e80-1a441ccc6c7b",
        "parentId" : "9fbbc1c4-1133-4c9c-ad95-62f4881b72cb",
        "authorId" : "f6398a50-7f9b-4c64-9d3a-f384404338b3",
        "body" : "`git grep _make_type_verifier`, there are other use cases which a function starts with `_` but is used outside where they are defined.",
        "createdAt" : "2021-04-02T01:23:05Z",
        "updatedAt" : "2021-04-24T07:13:10Z",
        "lastEditedBy" : "f6398a50-7f9b-4c64-9d3a-f384404338b3",
        "tags" : [
        ]
      }
    ],
    "commit" : "5845ee2419515b9243ea4396005f731eb294b6a9",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +24,28 @@    DoubleType, BooleanType, MapType, TimestampType, StructType, DataType, \\\n    IntegralType, _has_udt\nfrom pyspark.sql.pandas.types import _deserialize_pandas_with_udt\nfrom pyspark.traceback_utils import SCCallSiteSync\n"
  },
  {
    "id" : "047faf84-49a1-4ac4-a592-e2a9f53b7e6e",
    "prId" : 29818,
    "prUrl" : "https://github.com/apache/spark/pull/29818#pullrequestreview-509029779",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1f5eae89-a23b-47d6-aafc-a8ef3b448e47",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Yeah, I think it's better. In which case should we disable? Maybe we should enable it by default.",
        "createdAt" : "2020-09-21T13:58:23Z",
        "updatedAt" : "2021-01-28T20:44:29Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "c1100329-bbbc-4fa3-b34d-19b08d73d6b5",
        "parentId" : "1f5eae89-a23b-47d6-aafc-a8ef3b448e47",
        "authorId" : "d1541f17-4fdb-40b8-a545-d0d79a158f71",
        "body" : "The worry is with running into things like https://github.com/pandas-dev/pandas/issues/35530 in which case the user may appreciate an escape hatch.",
        "createdAt" : "2020-09-21T14:20:32Z",
        "updatedAt" : "2021-01-28T20:44:29Z",
        "lastEditedBy" : "d1541f17-4fdb-40b8-a545-d0d79a158f71",
        "tags" : [
        ]
      },
      {
        "id" : "ab600920-3512-4048-a163-e33143b4df9b",
        "parentId" : "1f5eae89-a23b-47d6-aafc-a8ef3b448e47",
        "authorId" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "body" : "yeah, I think this option can lead to other side effects, best to disable by default I think.",
        "createdAt" : "2020-10-15T06:34:39Z",
        "updatedAt" : "2021-01-28T20:44:29Z",
        "lastEditedBy" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "tags" : [
        ]
      }
    ],
    "commit" : "64d03012616c9bc56b97693d1fdf8132493deb0e",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +128,132 @@                                'use_threads': False,\n                            })\n                        pdf = table.to_pandas(**pandas_options)\n                        # Rename back to the original column names.\n                        pdf.columns = self.columns"
  },
  {
    "id" : "74a43e3c-3310-4fc3-b963-346eedef0743",
    "prId" : 29818,
    "prUrl" : "https://github.com/apache/spark/pull/29818#pullrequestreview-508827943",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "067659ce-b190-4e90-abee-578f3e17b216",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Would you mind leaving a comment on the codes about this set of parameter configurations?",
        "createdAt" : "2020-10-14T04:55:33Z",
        "updatedAt" : "2021-01-28T20:44:29Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "12079a15-8f86-4210-8326-1ae8187ff83a",
        "parentId" : "067659ce-b190-4e90-abee-578f3e17b216",
        "authorId" : "d1541f17-4fdb-40b8-a545-d0d79a158f71",
        "body" : "Done.",
        "createdAt" : "2020-10-14T22:57:46Z",
        "updatedAt" : "2021-01-28T20:44:29Z",
        "lastEditedBy" : "d1541f17-4fdb-40b8-a545-d0d79a158f71",
        "tags" : [
        ]
      }
    ],
    "commit" : "64d03012616c9bc56b97693d1fdf8132493deb0e",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +124,128 @@                            # use_threads - convert one column at a time\n                            pandas_options.update({\n                                'self_destruct': True,\n                                'split_blocks': True,\n                                'use_threads': False,"
  },
  {
    "id" : "b7156777-4b6e-453e-a762-ab04f84ecf31",
    "prId" : 29818,
    "prUrl" : "https://github.com/apache/spark/pull/29818#pullrequestreview-510624981",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2c83b733-03a1-4fc0-b4d1-44a41af8c02f",
        "parentId" : null,
        "authorId" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "body" : "does this have any bearing on the buffers self destructing? is it taking into account how many reference counts there are before destructing?",
        "createdAt" : "2020-10-15T06:33:12Z",
        "updatedAt" : "2021-01-28T20:44:29Z",
        "lastEditedBy" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "tags" : [
        ]
      },
      {
        "id" : "ea22e122-6a93-4a78-ac79-131a70c19382",
        "parentId" : "2c83b733-03a1-4fc0-b4d1-44a41af8c02f",
        "authorId" : "d1541f17-4fdb-40b8-a545-d0d79a158f71",
        "body" : "Yes - we don't want to hold on to any other references to the buffers, we want the Table to be the only owner. I'll clarify this part here.",
        "createdAt" : "2020-10-16T16:27:58Z",
        "updatedAt" : "2021-01-28T20:44:29Z",
        "lastEditedBy" : "d1541f17-4fdb-40b8-a545-d0d79a158f71",
        "tags" : [
        ]
      }
    ],
    "commit" : "64d03012616c9bc56b97693d1fdf8132493deb0e",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +113,117 @@                        # Ensure only the table has a reference to the batches, so that\n                        # self_destruct (if enabled) is effective\n                        del batches\n                        # Pandas DataFrame created from PyArrow uses datetime64[ns] for date type\n                        # values, but we should use datetime.date to match the behavior with when"
  },
  {
    "id" : "51113549-6959-4cdf-abae-4588332b837c",
    "prId" : 29818,
    "prUrl" : "https://github.com/apache/spark/pull/29818#pullrequestreview-510626387",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d2a889df-661a-4c5f-8690-968a3e5ea02a",
        "parentId" : null,
        "authorId" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "body" : "Is this necessary to set with `self_destruct`? It might lead to Pandas doing more memory allocation later, I believe.",
        "createdAt" : "2020-10-15T06:44:03Z",
        "updatedAt" : "2021-01-28T20:44:29Z",
        "lastEditedBy" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "tags" : [
        ]
      },
      {
        "id" : "d95c0371-4a1b-4101-af2a-216450ce909a",
        "parentId" : "d2a889df-661a-4c5f-8690-968a3e5ea02a",
        "authorId" : "d1541f17-4fdb-40b8-a545-d0d79a158f71",
        "body" : "Not quite _necessary_ but good to have, else we may end up allocating a large block if there are a lot of columns of the same type, defeating the point. Pandas may reconsolidate later but that's part of the issue of using Pandas.",
        "createdAt" : "2020-10-16T16:29:54Z",
        "updatedAt" : "2021-01-28T20:44:29Z",
        "lastEditedBy" : "d1541f17-4fdb-40b8-a545-d0d79a158f71",
        "tags" : [
        ]
      }
    ],
    "commit" : "64d03012616c9bc56b97693d1fdf8132493deb0e",
    "line" : 25,
    "diffHunk" : "@@ -1,1 +125,129 @@                            pandas_options.update({\n                                'self_destruct': True,\n                                'split_blocks': True,\n                                'use_threads': False,\n                            })"
  },
  {
    "id" : "ca68f3db-6e63-4eac-a303-9603311e7c86",
    "prId" : 29818,
    "prUrl" : "https://github.com/apache/spark/pull/29818#pullrequestreview-573550773",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6eacd9b5-4bf7-4184-9796-ac468df30700",
        "parentId" : null,
        "authorId" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "body" : "could you make a note that this is to reallocate the array?",
        "createdAt" : "2021-01-21T17:12:49Z",
        "updatedAt" : "2021-01-28T20:44:29Z",
        "lastEditedBy" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "tags" : [
        ]
      }
    ],
    "commit" : "64d03012616c9bc56b97693d1fdf8132493deb0e",
    "line" : 69,
    "diffHunk" : "@@ -1,1 +276,280 @@                        batch_or_indices = pa.RecordBatch.from_arrays([\n                            # This call actually reallocates the array\n                            pa.concat_arrays([array])\n                            for array in batch_or_indices\n                        ], schema=batch_or_indices.schema)"
  }
]