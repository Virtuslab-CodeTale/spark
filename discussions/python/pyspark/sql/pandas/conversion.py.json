[
  {
    "id" : "1946ec7f-8961-4dfe-9c0b-8ad755f44744",
    "prId" : 32026,
    "prUrl" : "https://github.com/apache/spark/pull/32026#pullrequestreview-626740372",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2e152242-471f-453c-a02b-47e40043630f",
        "parentId" : null,
        "authorId" : "4a97d995-edb8-416d-ba1b-29e030885f7e",
        "body" : "this seems to be only a rename? should we leave them as they are to reduce the size of PR?",
        "createdAt" : "2021-04-01T16:17:52Z",
        "updatedAt" : "2021-04-24T07:13:10Z",
        "lastEditedBy" : "4a97d995-edb8-416d-ba1b-29e030885f7e",
        "tags" : [
        ]
      },
      {
        "id" : "eef730ad-75bd-4c77-9031-7235ddf08f0d",
        "parentId" : "2e152242-471f-453c-a02b-47e40043630f",
        "authorId" : "f6398a50-7f9b-4c64-9d3a-f384404338b3",
        "body" : "Yes. It is renamed to indicate it is arrow data with datatype (Spark SQL DataType or Arrow DataType).\r\n\r\nIn `serializers.py`, `dt` is for Spark SQL DataType, `pdt` is for pyarrow DataType.",
        "createdAt" : "2021-04-02T01:16:57Z",
        "updatedAt" : "2021-04-24T07:13:10Z",
        "lastEditedBy" : "f6398a50-7f9b-4c64-9d3a-f384404338b3",
        "tags" : [
        ]
      },
      {
        "id" : "cbb5613c-d981-4506-86aa-30177be8a4f1",
        "parentId" : "2e152242-471f-453c-a02b-47e40043630f",
        "authorId" : "f6398a50-7f9b-4c64-9d3a-f384404338b3",
        "body" : "Well, I should use `adt` or `padt` for PyArrow Data Type and `pdt` for Pandas DataType.",
        "createdAt" : "2021-04-02T01:25:02Z",
        "updatedAt" : "2021-04-24T07:13:10Z",
        "lastEditedBy" : "f6398a50-7f9b-4c64-9d3a-f384404338b3",
        "tags" : [
        ]
      }
    ],
    "commit" : "5845ee2419515b9243ea4396005f731eb294b6a9",
    "line" : 64,
    "diffHunk" : "@@ -1,1 +475,479 @@        # Create list of Arrow (columns, type) for serializer dump_stream\n        # Type can be Spark SQL Data Type or Arrow Data Type\n        arrow_data_with_t = [\n            [(c, t) for (_, c), t in zip(pdf_slice.iteritems(), data_types)]\n            for pdf_slice in pdf_slices"
  },
  {
    "id" : "5a0bb66a-0d73-443a-b114-8674c0e931f3",
    "prId" : 32026,
    "prUrl" : "https://github.com/apache/spark/pull/32026#pullrequestreview-626739916",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9fbbc1c4-1133-4c9c-ad95-62f4881b72cb",
        "parentId" : null,
        "authorId" : "4a97d995-edb8-416d-ba1b-29e030885f7e",
        "body" : "these two `_` prefixed functions are used outside their files. should we remove the `_` prefix, because they are not private anymore.",
        "createdAt" : "2021-04-01T16:19:17Z",
        "updatedAt" : "2021-04-24T07:13:10Z",
        "lastEditedBy" : "4a97d995-edb8-416d-ba1b-29e030885f7e",
        "tags" : [
        ]
      },
      {
        "id" : "e913ffa8-665a-48da-9e80-1a441ccc6c7b",
        "parentId" : "9fbbc1c4-1133-4c9c-ad95-62f4881b72cb",
        "authorId" : "f6398a50-7f9b-4c64-9d3a-f384404338b3",
        "body" : "`git grep _make_type_verifier`, there are other use cases which a function starts with `_` but is used outside where they are defined.",
        "createdAt" : "2021-04-02T01:23:05Z",
        "updatedAt" : "2021-04-24T07:13:10Z",
        "lastEditedBy" : "f6398a50-7f9b-4c64-9d3a-f384404338b3",
        "tags" : [
        ]
      }
    ],
    "commit" : "5845ee2419515b9243ea4396005f731eb294b6a9",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +24,28 @@    DoubleType, BooleanType, MapType, TimestampType, StructType, DataType, \\\n    IntegralType, _has_udt\nfrom pyspark.sql.pandas.types import _deserialize_pandas_with_udt\nfrom pyspark.traceback_utils import SCCallSiteSync\n"
  }
]