[
  {
    "id" : "cc629e38-e75e-4b1a-bd01-6bb6a4447a69",
    "prId" : 27466,
    "prUrl" : "https://github.com/apache/spark/pull/27466#pullrequestreview-353666702",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b7560375-96da-49e4-9f61-a752f66ee191",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Information ported from GROUPED MAP in `pandas_udf`.",
        "createdAt" : "2020-02-05T12:07:57Z",
        "updatedAt" : "2020-02-11T08:35:36Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "626ff3cb245ccd305f9a32d9c99f774ef154ebe1",
    "line" : 45,
    "diffHunk" : "@@ -1,1 +120,124 @@        +---+-------------------+\n\n        Alternatively, the user can pass a function that takes two arguments.\n        In this case, the grouping key(s) will be passed as the first argument and the data will\n        be passed as the second argument. The grouping key(s) will be passed as a tuple of numpy"
  },
  {
    "id" : "c4f2e80e-ded5-4461-9d6b-1056b2ef4a77",
    "prId" : 27165,
    "prUrl" : "https://github.com/apache/spark/pull/27165#pullrequestreview-341000309",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ae929ac3-dca3-4f41-8031-7356b9974999",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I piggyback this change while I'm here.",
        "createdAt" : "2020-01-10T07:59:10Z",
        "updatedAt" : "2020-01-22T01:53:52Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "76e09092f983782f0986c0c55c34165de62435bd",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +30,34 @@    \"\"\"\n\n    @since(2.3)\n    def apply(self, udf):\n        \"\"\""
  },
  {
    "id" : "845c776e-d432-4341-a4d4-605335ec76ce",
    "prId" : 27165,
    "prUrl" : "https://github.com/apache/spark/pull/27165#pullrequestreview-346229868",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "539b80f1-c317-4412-9b37-3b1802c9ffd5",
        "parentId" : null,
        "authorId" : "43998e22-6c2f-401d-9914-8cecf6fad929",
        "body" : "Seems trying to always show this warning even when using `applyInPandas`?\r\nMaybe we should move the following logic into `applyInPandas` and call it from here after showing the warning?\r\n\r\n```py\r\n>>> df.groupby(\"id\").applyInPandas(normalize, schema=\"id long, v double\").show()\r\n/.../pyspark/sql/pandas/group_ops.py:85: UserWarning: It is preferred to use 'applyInPandas' over this API. This API will be deprecated in the future releases. See SPARK-28264 for more details.\r\n...\r\n```",
        "createdAt" : "2020-01-21T23:14:00Z",
        "updatedAt" : "2020-01-22T01:53:52Z",
        "lastEditedBy" : "43998e22-6c2f-401d-9914-8cecf6fad929",
        "tags" : [
        ]
      }
    ],
    "commit" : "76e09092f983782f0986c0c55c34165de62435bd",
    "line" : 56,
    "diffHunk" : "@@ -1,1 +74,78 @@            \"It is preferred to use 'applyInPandas' over this \"\n            \"API. This API will be deprecated in the future releases. See SPARK-28264 for \"\n            \"more details.\", UserWarning)\n\n        return self.applyInPandas(udf.func, schema=udf.returnType)"
  },
  {
    "id" : "9fc57888-d2ff-48da-aa72-70264b292632",
    "prId" : 27109,
    "prUrl" : "https://github.com/apache/spark/pull/27109#pullrequestreview-339612800",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9dba085a-21bf-436a-9904-f855883bb35b",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "ditto.",
        "createdAt" : "2020-01-08T02:07:02Z",
        "updatedAt" : "2020-01-08T02:08:04Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "c63c271d95e7864b69ca477c51e4e2d22b7d9029",
    "line" : 31,
    "diffHunk" : "@@ -1,1 +49,53 @@            :func:`pyspark.sql.functions.pandas_udf`.\n\n        >>> from pyspark.sql.functions import pandas_udf, PandasUDFType\n        >>> df = spark.createDataFrame(\n        ...     [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],"
  }
]