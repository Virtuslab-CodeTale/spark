[
  {
    "id" : "4a01fac6-78f7-47e8-97fe-063c789f046b",
    "prId" : 33484,
    "prUrl" : "https://github.com/apache/spark/pull/33484#pullrequestreview-714300427",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5a06afcd-6bc8-49e2-a1bb-eacc45b9cae7",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Let's move this under \"A user can retrieve ...\"",
        "createdAt" : "2021-07-25T01:44:52Z",
        "updatedAt" : "2021-07-25T01:44:52Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "2518baf2-2f40-4fa7-8f8e-1d0e809dadba",
        "parentId" : "5a06afcd-6bc8-49e2-a1bb-eacc45b9cae7",
        "authorId" : "c4ecf4e4-1fda-4fed-9980-aed6c3b3e8e4",
        "body" : "moved",
        "createdAt" : "2021-07-25T10:16:42Z",
        "updatedAt" : "2021-07-25T10:16:42Z",
        "lastEditedBy" : "c4ecf4e4-1fda-4fed-9980-aed6c3b3e8e4",
        "tags" : [
        ]
      }
    ],
    "commit" : "90675b0d300a299a047f97274e9d0814b2366a37",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +1837,1841 @@        A user can retrieve the metrics by accessing `Observation.get`.\n\n        .. versionadded:: 3.3.0\n\n        Parameters"
  },
  {
    "id" : "b8d0bc44-0b78-4c34-82fe-8eedf93ca3a5",
    "prId" : 33364,
    "prUrl" : "https://github.com/apache/spark/pull/33364#pullrequestreview-708067922",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5c54ec25-f7d1-49da-9182-99f4b447da16",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "```suggestion\r\n            raise TypeError(\"Parameter 'subset' must be a list of columns or a single column\")\r\n```",
        "createdAt" : "2021-07-16T07:05:13Z",
        "updatedAt" : "2021-07-16T07:05:14Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "2c720e2d8b4edcab3f52f611849373b18377829f",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +1984,1988 @@        if subset is not None and (\n                not isinstance(subset, Iterable) or isinstance(subset, str)):\n            raise TypeError(\"Parameter 'subset' must be a list of columns\")\n\n        if subset is None:"
  },
  {
    "id" : "37660dd0-ff7a-4b61-b6a6-b98335be1675",
    "prId" : 33054,
    "prUrl" : "https://github.com/apache/spark/pull/33054#pullrequestreview-692124029",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "59865833-d09c-4470-8427-e818873fc6a0",
        "parentId" : null,
        "authorId" : "43998e22-6c2f-401d-9914-8cecf6fad929",
        "body" : "We need to add the signature in `dataframe.pyi` file?",
        "createdAt" : "2021-06-24T19:03:13Z",
        "updatedAt" : "2021-06-24T19:03:51Z",
        "lastEditedBy" : "43998e22-6c2f-401d-9914-8cecf6fad929",
        "tags" : [
        ]
      }
    ],
    "commit" : "e3349906c1b6586bbc850557bc1ce087c34c22e5",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +2696,2700 @@        return DataFrameWriterV2(self, table)\n\n    def to_pandas_on_spark(self, index_col=None):\n        \"\"\"\n        Converts the existing DataFrame into a pandas-on-Spark DataFrame."
  },
  {
    "id" : "739aac11-68c0-4638-8c05-9fb2f1e9e3cb",
    "prId" : 32555,
    "prUrl" : "https://github.com/apache/spark/pull/32555#pullrequestreview-660398123",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2cd4baf9-4797-49a5-af19-c9315e17a75e",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "While we're here, would you mind fix the docs above `truncate : bool, optional` -> `truncate : bool or int, optional`.",
        "createdAt" : "2021-05-16T00:59:17Z",
        "updatedAt" : "2021-05-16T00:59:17Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "a30729a1d66bb357ffc2c7909f6a9889fa1513d3",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +480,484 @@         name | Alice\n        -RECORD 1-----\n         age  | 5\n         name | Bob\n        \"\"\""
  },
  {
    "id" : "fc165313-05bc-46cc-ab7f-7b75697cceb3",
    "prId" : 32555,
    "prUrl" : "https://github.com/apache/spark/pull/32555#pullrequestreview-660462900",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d1c6e1ad-4f26-404d-b212-47f7d7cb9e7a",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "why do we need to check if `n` is `bool`? bool inherits `int` anyway so it's no-op to remove",
        "createdAt" : "2021-05-16T01:05:22Z",
        "updatedAt" : "2021-05-16T01:05:22Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "18941085-a936-438b-9641-6404d1f3ed54",
        "parentId" : "d1c6e1ad-4f26-404d-b212-47f7d7cb9e7a",
        "authorId" : "e7dcdd7d-a86f-415f-875c-a80eec9bc0f6",
        "body" : "if we pass `n=False` then ` not isinstance(n, int)` is `False` and with this alone the error would not be raised. We need to have an explicit check to reject the `bool`-typed `n`",
        "createdAt" : "2021-05-16T06:05:42Z",
        "updatedAt" : "2021-05-16T06:05:58Z",
        "lastEditedBy" : "e7dcdd7d-a86f-415f-875c-a80eec9bc0f6",
        "tags" : [
        ]
      },
      {
        "id" : "81ebb0ff-c828-4a38-861b-6411cd42a0f4",
        "parentId" : "d1c6e1ad-4f26-404d-b212-47f7d7cb9e7a",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Is this trying to catch the case where a user calls `.show(False)` for example, meaning `.show(truncate=False)`? I get it. While I think we could equally check so many other methods like this, maybe this is worth checking more thoroughly here.",
        "createdAt" : "2021-05-16T14:13:18Z",
        "updatedAt" : "2021-05-16T14:16:51Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "1f31d52d-01c4-4906-a777-862a8d138194",
        "parentId" : "d1c6e1ad-4f26-404d-b212-47f7d7cb9e7a",
        "authorId" : "e7dcdd7d-a86f-415f-875c-a80eec9bc0f6",
        "body" : "Maybe we can come up with a generic mechanism  to tackle  non-existing method exceptions from`ReflectionEngine.getMethod`",
        "createdAt" : "2021-05-16T20:14:24Z",
        "updatedAt" : "2021-05-16T20:14:24Z",
        "lastEditedBy" : "e7dcdd7d-a86f-415f-875c-a80eec9bc0f6",
        "tags" : [
        ]
      }
    ],
    "commit" : "a30729a1d66bb357ffc2c7909f6a9889fa1513d3",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +484,488 @@        \"\"\"\n\n        if not isinstance(n, int) or isinstance(n, bool):\n            raise TypeError(\"Parameter 'n' (number of rows) must be an int\")\n"
  },
  {
    "id" : "31d68372-0afc-4d1e-a99c-5ff78dccaec3",
    "prId" : 32276,
    "prUrl" : "https://github.com/apache/spark/pull/32276#pullrequestreview-641189944",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cd06a248-4430-41df-b47c-6849be6ccfec",
        "parentId" : null,
        "authorId" : "c13f1277-0254-494b-8e7a-bd7c261b2e9f",
        "body" : "Note that there are some wrong usage on ValueError in some other methods, I add an issue on https://issues.apache.org/jira/browse/SPARK-35176",
        "createdAt" : "2021-04-21T14:49:28Z",
        "updatedAt" : "2021-04-21T14:49:28Z",
        "lastEditedBy" : "c13f1277-0254-494b-8e7a-bd7c261b2e9f",
        "tags" : [
        ]
      }
    ],
    "commit" : "b2dbe9b6c9225267b7a3160e59be13530c7b7f52",
    "line" : 32,
    "diffHunk" : "@@ -1,1 +2459,2463 @@        \"\"\"\n        if not isinstance(colName, (str, list, tuple)):\n            raise TypeError(\"colName must be string or list/tuple of column names.\")\n        if not isinstance(col, (Column, list, tuple)):\n            raise TypeError(\"col must be a column or list/tuple of columns.\")"
  },
  {
    "id" : "bada8e78-8579-4d41-a934-b03aea0bab75",
    "prId" : 32276,
    "prUrl" : "https://github.com/apache/spark/pull/32276#pullrequestreview-641197434",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8aee84b2-e8a8-47dc-8ec1-c6926179983c",
        "parentId" : null,
        "authorId" : "c13f1277-0254-494b-8e7a-bd7c261b2e9f",
        "body" : "Notice that I use the `withColumns` in here which is a [private method](https://github.com/apache/spark/blob/b5241c97b17a1139a4ff719bfce7f68aef094d95/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L2402) in scala. The scala private [withColumns API](https://github.com/apache/spark/blob/b5241c97b17a1139a4ff719bfce7f68aef094d95/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L2396) can receive multiple columns now.",
        "createdAt" : "2021-04-21T14:55:21Z",
        "updatedAt" : "2021-04-22T02:01:50Z",
        "lastEditedBy" : "c13f1277-0254-494b-8e7a-bd7c261b2e9f",
        "tags" : [
        ]
      }
    ],
    "commit" : "b2dbe9b6c9225267b7a3160e59be13530c7b7f52",
    "line" : 45,
    "diffHunk" : "@@ -1,1 +2472,2476 @@\n        return DataFrame(\n            self._jdf.withColumns(_to_seq(self._sc, col_names), self._jcols(col)),\n            self.sql_ctx\n        )"
  },
  {
    "id" : "3163c8c3-d0e7-4215-8986-2f06781f33dd",
    "prId" : 32122,
    "prUrl" : "https://github.com/apache/spark/pull/32122#pullrequestreview-632974485",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3e1c2be6-d719-4aad-baca-fd414e8c4758",
        "parentId" : null,
        "authorId" : "c13f1277-0254-494b-8e7a-bd7c261b2e9f",
        "body" : "The scalar eventTime is also `str`\r\nhttps://github.com/apache/spark/blob/61d038f26e1beeeb39638fdf9703d2e86d058342/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L753\r\nand python implements is also validated as string\r\nhttps://github.com/apache/spark/blob/ff1fc5ed4b685b4f5f83d5f600b24f089dd4522e/python/pyspark/sql/dataframe.py#L608-L609\r\n\r\nbut wrong doc on the param and typehints",
        "createdAt" : "2021-04-11T05:41:45Z",
        "updatedAt" : "2021-04-12T12:01:34Z",
        "lastEditedBy" : "c13f1277-0254-494b-8e7a-bd7c261b2e9f",
        "tags" : [
        ]
      }
    ],
    "commit" : "a45bdbb9bf4137cc9028e973e7b042d5af40b792",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +589,593 @@        Parameters\n        ----------\n        eventTime : str\n            the name of the column that contains the event time of the row.\n        delayThreshold : str"
  },
  {
    "id" : "54acadb7-72bf-4e41-96c4-bfd163377f57",
    "prId" : 30181,
    "prUrl" : "https://github.com/apache/spark/pull/30181#pullrequestreview-520377841",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "24ed1ef7-66ea-4d29-813f-7461b2f6c38a",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Is this `versionchanged` put among parameters intentionally?",
        "createdAt" : "2020-10-30T01:11:39Z",
        "updatedAt" : "2020-11-03T00:49:21Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "6332c051-5ef2-43b0-83cb-e67a2ae68cef",
        "parentId" : "24ed1ef7-66ea-4d29-813f-7461b2f6c38a",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Yeah because it was the change into the specific parameter `col` (SPARK-25381). Looks like at least this is what pandas does.",
        "createdAt" : "2020-10-30T05:00:59Z",
        "updatedAt" : "2020-11-03T00:49:21Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "40bc04996ab651411656c08849b5ec0495cd8468",
    "line" : 791,
    "diffHunk" : "@@ -1,1 +1104,1108 @@\n            .. versionchanged:: 3.0\n               Added sampling by a column of :class:`Column`\n        fractions : dict\n            sampling fraction for each stratum. If a stratum is not"
  },
  {
    "id" : "eab09d52-823a-4aa5-aadb-49450d2aa66c",
    "prId" : 30181,
    "prUrl" : "https://github.com/apache/spark/pull/30181#pullrequestreview-520264562",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2a04a7ff-34df-4603-a629-935cf4dd0a8f",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : ":class:`Row`?",
        "createdAt" : "2020-10-30T01:16:05Z",
        "updatedAt" : "2020-11-03T00:49:21Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "40bc04996ab651411656c08849b5ec0495cd8468",
    "line" : 1132,
    "diffHunk" : "@@ -1,1 +1574,1578 @@        -------\n        If n is greater than 1, return a list of :class:`Row`.\n        If n is 1, return a single Row.\n\n        Examples"
  },
  {
    "id" : "aabf2cb3-9018-4e9f-b906-70be14f78a07",
    "prId" : 30181,
    "prUrl" : "https://github.com/apache/spark/pull/30181#pullrequestreview-520264562",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "41e16cd9-f232-453d-88e7-322865fe363f",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "`list` is removed?",
        "createdAt" : "2020-10-30T01:17:44Z",
        "updatedAt" : "2020-11-03T00:49:21Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "40bc04996ab651411656c08849b5ec0495cd8468",
    "line" : 1473,
    "diffHunk" : "@@ -1,1 +2104,2108 @@            If the value is a dict, then `value` is ignored or can be omitted, and `to_replace`\n            must be a mapping between a value and a replacement.\n        value : bool, int, float, string or None, optional\n            The replacement value must be a bool, int, float, string or None. If `value` is a\n            list, `value` should be of the same length and type as `to_replace`."
  },
  {
    "id" : "3f07c91e-340d-4851-a6a0-9e0ba79f6cae",
    "prId" : 30060,
    "prUrl" : "https://github.com/apache/spark/pull/30060#pullrequestreview-509923390",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d381c93a-1275-4b80-8db6-ccb0aa54c41a",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Thank you for your contribution. Is this the last one? Could you check the other examples in this file, please?",
        "createdAt" : "2020-10-15T19:49:59Z",
        "updatedAt" : "2020-10-15T19:50:00Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "de403bd6-79fd-4904-9114-c544b89f489e",
        "parentId" : "d381c93a-1275-4b80-8db6-ccb0aa54c41a",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "cc @HyukjinKwon ",
        "createdAt" : "2020-10-15T19:50:19Z",
        "updatedAt" : "2020-10-15T19:50:20Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "9021f2dc-687d-4749-95ec-4a0bdb80834b",
        "parentId" : "d381c93a-1275-4b80-8db6-ccb0aa54c41a",
        "authorId" : "19f11a93-e55a-4dc1-a32c-961e80fbe837",
        "body" : "> Thank you for your contribution. Is this the last one? Could you check the other examples in this file, please?\r\n\r\nMy pleasure. This is the only instance in the current version branch.",
        "createdAt" : "2020-10-15T20:00:23Z",
        "updatedAt" : "2020-10-15T20:00:23Z",
        "lastEditedBy" : "19f11a93-e55a-4dc1-a32c-961e80fbe837",
        "tags" : [
        ]
      },
      {
        "id" : "259d05bb-307f-447b-84a3-feb583ebfd27",
        "parentId" : "d381c93a-1275-4b80-8db6-ccb0aa54c41a",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Thanks!",
        "createdAt" : "2020-10-16T00:22:16Z",
        "updatedAt" : "2020-10-16T00:22:17Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "1614635b4fe00ecd32a7c15181cd7d06d5190112",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1515,1519 @@    def agg(self, *exprs):\n        \"\"\" Aggregate on the entire :class:`DataFrame` without groups\n        (shorthand for ``df.groupBy().agg()``).\n\n        >>> df.agg({\"age\": \"max\"}).collect()"
  },
  {
    "id" : "95679d59-bea7-49a4-8663-38d8f8654a01",
    "prId" : 29242,
    "prUrl" : "https://github.com/apache/spark/pull/29242#pullrequestreview-483254520",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9bcafb4a-7fff-47a7-a70a-687b59fca45e",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Now I see the confusion. In Scala, `MEMORY_AND_DISK` means `deserialized=true`, while in Python, `MEMORY_AND_DISK` means `deserialized=false`.",
        "createdAt" : "2020-09-07T05:44:53Z",
        "updatedAt" : "2020-09-08T13:56:44Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "ba3c2ef8a90dc21df08d59a40b23c33dfaae6920",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +679,683 @@\n    @since(1.3)\n    def persist(self, storageLevel=StorageLevel.MEMORY_AND_DISK_DESER):\n        \"\"\"Sets the storage level to persist the contents of the :class:`DataFrame` across\n        operations after the first time it is computed. This can only be used to assign"
  },
  {
    "id" : "4b6e8d51-cbee-4e2b-948f-5ee90de3f3ad",
    "prId" : 29214,
    "prUrl" : "https://github.com/apache/spark/pull/29214#pullrequestreview-454802408",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "96cfe229-b131-4ff5-aa4e-4661ba7373c8",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Can you update the migration guide at https://github.com/apache/spark/blob/master/docs/pyspark-migration-guide.md? You can create a new title \"Upgrading from PySpark 3.0 to 3.1\"",
        "createdAt" : "2020-07-24T05:54:31Z",
        "updatedAt" : "2020-07-24T11:05:35Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "b574b785-e48a-4a83-9dbb-437185828410",
        "parentId" : "96cfe229-b131-4ff5-aa4e-4661ba7373c8",
        "authorId" : "84d37f6c-0769-44e9-80b2-42fe4b57dce5",
        "body" : "done",
        "createdAt" : "2020-07-24T11:05:49Z",
        "updatedAt" : "2020-07-24T11:05:50Z",
        "lastEditedBy" : "84d37f6c-0769-44e9-80b2-42fe4b57dce5",
        "tags" : [
        ]
      }
    ],
    "commit" : "2a3be294950f9a59c61cd16f2e52e1c342e20f53",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +1334,1338 @@        if n is None:\n            rs = self.head(1)\n            return rs[0] if rs else []\n        return self.take(n)\n"
  },
  {
    "id" : "7cfcad14-e9fe-41b9-bb5e-dcdb75a8db8c",
    "prId" : 28957,
    "prUrl" : "https://github.com/apache/spark/pull/28957#pullrequestreview-442361693",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "94c0e54a-f5ab-4abe-a06e-0af8293f6261",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "So this is also because of https://github.com/apache/spark/pull/28957/files#r449475633",
        "createdAt" : "2020-07-03T11:54:02Z",
        "updatedAt" : "2020-07-13T09:52:43Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "f2356c80a105c0b74e578152282013c1d3b9c647",
    "line" : 806,
    "diffHunk" : "@@ -1,1 +2336,2340 @@    globs['df5'] = sc.parallelize([Row(age=10, name='Alice', spy=False),\n                                   Row(age=5, name='Bob', spy=None),\n                                   Row(age=None, name='Mallory', spy=True)]).toDF()\n    globs['sdf'] = sc.parallelize([Row(name='Tom', time=1479441846),\n                                   Row(name='Bob', time=1479442946)]).toDF()"
  },
  {
    "id" : "9f7b9269-36a7-4ae0-8530-29fe5ec5c753",
    "prId" : 28559,
    "prUrl" : "https://github.com/apache/spark/pull/28559#pullrequestreview-413268640",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2d286102-05c2-4e7b-8944-d49d175c7e4e",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Could you fix `classification.py` and `regression.py`, too?\r\n```\r\npyspark/ml/classification.py:    To be mixed in with class:`pyspark.ml.JavaModel`\r\npyspark/ml/regression.py:    To be mixed in with class:`pyspark.ml.JavaModel`\r\n```",
        "createdAt" : "2020-05-18T04:46:30Z",
        "updatedAt" : "2020-05-18T10:54:43Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "239bf9c0d1ee20deb1705ce4c0658b64596a8e1e",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +2153,2157 @@        \"\"\"Returns a new :class:`DataFrame`. Concise syntax for chaining custom transformations.\n\n        :param func: a function that takes and returns a :class:`DataFrame`.\n\n        >>> from pyspark.sql.functions import col"
  },
  {
    "id" : "8fe348b1-e969-4b9d-902c-0433c63b4714",
    "prId" : 28238,
    "prUrl" : "https://github.com/apache/spark/pull/28238#pullrequestreview-395885455",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7060fe93-4aae-4087-a916-61be9dc5cdff",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "BTW, we had to add [`versionchanged` directive](https://www.sphinx-doc.org/en/master/usage/restructuredtext/directives.html#directive-versionchanged) in the docstring too.\r\n",
        "createdAt" : "2020-04-17T08:09:23Z",
        "updatedAt" : "2020-04-17T08:09:23Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "59f1c37f-79c3-4055-a13f-ae09d5a2bc1e",
        "parentId" : "7060fe93-4aae-4087-a916-61be9dc5cdff",
        "authorId" : "e29a7794-ac2e-4e6f-a690-737e83e1bace",
        "body" : "It doesn't really change behavior since it is a bug. Does this also apply for bugfixes? Let me know.",
        "createdAt" : "2020-04-18T06:04:33Z",
        "updatedAt" : "2020-04-18T06:04:33Z",
        "lastEditedBy" : "e29a7794-ac2e-4e6f-a690-737e83e1bace",
        "tags" : [
        ]
      }
    ],
    "commit" : "0b8b844a57eb78743ed76405b77a05dcb595b110",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +507,511 @@            raise TypeError(\"name should be provided as str, got {0}\".format(type(name)))\n\n        allowed_types = (basestring, list, float, int)\n        for p in parameters:\n            if not isinstance(p, allowed_types):"
  },
  {
    "id" : "69703378-623e-4d1a-98ce-22bdc053fd83",
    "prId" : 27565,
    "prUrl" : "https://github.com/apache/spark/pull/27565#pullrequestreview-358687125",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "54de14c6-76b2-4ac1-a721-21c6044916b6",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Can you add a doctest too?",
        "createdAt" : "2020-02-14T02:58:59Z",
        "updatedAt" : "2020-02-17T10:44:06Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "0e749403b4bb127341849827cc95313752b6c715",
    "line" : 27,
    "diffHunk" : "@@ -1,1 +2177,2181 @@        >>> df1.withColumn(\"col1\", df1.id * 2).sameSemantics(df2.withColumn(\"col0\", df2.id * 2))\n        True\n        \"\"\"\n        if not isinstance(other, DataFrame):\n            raise ValueError(\"other parameter should be of DataFrame; however, got %s\""
  },
  {
    "id" : "d54074e7-644b-4672-98ff-aef2877874dc",
    "prId" : 27565,
    "prUrl" : "https://github.com/apache/spark/pull/27565#pullrequestreview-358687184",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "90f9adfb-8567-4025-9772-0e43b01cf3bd",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Doc seems missing. Shall we add it with doctest?",
        "createdAt" : "2020-02-14T02:59:12Z",
        "updatedAt" : "2020-02-17T10:44:06Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "0e749403b4bb127341849827cc95313752b6c715",
    "line" : 35,
    "diffHunk" : "@@ -1,1 +2185,2189 @@    @since(3.1)\n    def semanticHash(self):\n        \"\"\"\n        Returns a hash code of the logical query plan against this :class:`DataFrame`.\n"
  },
  {
    "id" : "ca98af92-8960-4e0b-bf77-ad028eee59ee",
    "prId" : 27565,
    "prUrl" : "https://github.com/apache/spark/pull/27565#pullrequestreview-358844948",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2aa42de3-3f21-4727-a3d4-14339797c5ac",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "The documentation seems mismatched with Scala side. I would suggest:\r\n\r\n```\r\nReturns `True` when the logical query plans inside both :class:`DataFrame`\\\\s are equal and\r\ntherefore return same results.\r\n\r\n.. note:: The equality comparison here is simplified by tolerating the cosmetic differences\r\n    such as attribute names.\r\n\r\n.. note::This API can compare both :class:`DataFrame`\\\\s very fast but can still return `False` on\r\n    the :class:`DataFrame` that return the same results, for instance, from different plans. Such\r\n    false negative semantic can be useful when caching as an example.\r\n```",
        "createdAt" : "2020-02-14T10:26:41Z",
        "updatedAt" : "2020-02-17T10:44:06Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "0e749403b4bb127341849827cc95313752b6c715",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +2156,2160 @@    @since(3.1)\n    def sameSemantics(self, other):\n        \"\"\"\n        Returns `True` when the logical query plans inside both :class:`DataFrame`\\\\s are equal and\n        therefore return same results."
  },
  {
    "id" : "b1fd3b74-b078-4e05-9452-32858baf2538",
    "prId" : 27565,
    "prUrl" : "https://github.com/apache/spark/pull/27565#pullrequestreview-359495912",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1917f7ae-24c6-44cc-bf8a-ca29fe941781",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "nit:\r\n\r\n```python\r\n        >>> df1 = spark.range(10)\r\n        >>> df2 = spark.range(10)\r\n        >>> df1.withColumn(\"col1\", df1.id * 2).sameSemantics(df2.withColumn(\"col1\", df2.id * 2))\r\n        True\r\n        >>> df1.withColumn(\"col1\", df1.id * 2).sameSemantics(df2.withColumn(\"col1\", df2.id + 2))\r\n        False\r\n        >>> df1.withColumn(\"col1\", df1.id * 2).sameSemantics(df2.withColumn(\"col0\", df2.id * 2))\r\n        True\r\n```",
        "createdAt" : "2020-02-17T05:31:05Z",
        "updatedAt" : "2020-02-17T10:44:06Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "0e749403b4bb127341849827cc95313752b6c715",
    "line" : 27,
    "diffHunk" : "@@ -1,1 +2177,2181 @@        >>> df1.withColumn(\"col1\", df1.id * 2).sameSemantics(df2.withColumn(\"col0\", df2.id * 2))\n        True\n        \"\"\"\n        if not isinstance(other, DataFrame):\n            raise ValueError(\"other parameter should be of DataFrame; however, got %s\""
  },
  {
    "id" : "43f1a73e-0fd7-4112-8a05-fe3f3ddcb80d",
    "prId" : 27109,
    "prUrl" : "https://github.com/apache/spark/pull/27109#pullrequestreview-339367137",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1c4202d9-a371-4536-beab-b7548c98922f",
        "parentId" : null,
        "authorId" : "981b170c-729a-429c-b115-0350ea50b32b",
        "body" : "Could you explain why use mixins here? Since these are never used outside this context nor (as far as I can tell) mimic the Scala counterpart, the approach seems a bit counterintuitive.\r\n\r\nWouldn't make more sense to use proper instances, same way as `DataFrameNaFunctions` or `DataFrameStatFunctions`?\r\n\r\nThis question also applies to `GroupedData` and its mxin.",
        "createdAt" : "2020-01-06T14:22:38Z",
        "updatedAt" : "2020-01-07T13:30:30Z",
        "lastEditedBy" : "981b170c-729a-429c-b115-0350ea50b32b",
        "tags" : [
        ]
      },
      {
        "id" : "a206058d-2731-47c8-80f2-9119ee18470a",
        "parentId" : "1c4202d9-a371-4536-beab-b7548c98922f",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Ah you mean API usages like:\r\n\r\n```python\r\ndf.pandas.mapInPandas(...)\r\n```\r\n\r\nvia defining an instance like `DataFrameNaFunctions` or `DataFrameStatFunctions`? I actually considered this approach first. If I could write it from the first place, I think I could consider this option without concerning about backward compatibility.\r\n\r\nIf you mean defining `DataFrameNaFunctions` or `DataFrameStatFunctions` and calling directly at API calls of `DataFrame` or `SparkSession`, the API names have to be exposed at at `DataFrame` or `SparkSession` level .. which is ugly.\r\n\r\nAlso, Scala guys are pretty used to mixins. If we think self type trait, I think it fits to the purpose.\r\n\r\nLastly (trivial but still), I wanted to make the codes a bit more pandas friendly by mimicking in case we can have some more pandas dev guys.",
        "createdAt" : "2020-01-06T14:53:44Z",
        "updatedAt" : "2020-01-07T13:30:30Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "ac349ebc-1629-4eae-a55a-6c7a522eaf70",
        "parentId" : "1c4202d9-a371-4536-beab-b7548c98922f",
        "authorId" : "981b170c-729a-429c-b115-0350ea50b32b",
        "body" : "In general I am trying to get a better feeling of overall purpose of such refactoring.\r\n\r\nAs for now there is no indication that any of these mixins will be ever used outside the current context (`DataFrame` and `GroupedData`). That impression is further enforced by explicit type checks ([here](https://github.com/apache/spark/blob/cfd78393e76f454503e7cf5416f6d56f1efffd0a/python/pyspark/sql/pandas/group_ops.py#L96) and [here](https://github.com/apache/spark/blob/cfd78393e76f454503e7cf5416f6d56f1efffd0a/python/pyspark/sql/pandas/map_ops.py#L64)). So that doesn't really seem like a canonical use of mixin, especially when base core `DataFrame` is not designed for extensiblity. \r\n\r\n> Ah you mean API usages like:\r\n>     \r\n>    df.pandas.mapInPandas(...)\r\n\r\nThat's one possible approach though not the one I was thinking about. I assumed (though I am not sure, as the amount of code moved, excluding docs, message and some static stuff is negligible, and  tightly coupled with `DataFrame` anyway) that the point is maintainability.\r\n\r\nSo possible approach is either direct \r\n\r\n        def __init__(self, ...):\r\n            ...\r\n            self._pandasMapOpsMixin = PandasMapOpsMixin(self)\r\n            ...\r\n\r\n        def mapInPandas(self, udf):\r\n            return self._pandasMapOpsMixin.mapInPandas(udf)\r\n\r\nor indirect (by overwriting `__geattr__`).\r\n\r\n\r\n",
        "createdAt" : "2020-01-06T20:40:15Z",
        "updatedAt" : "2020-01-07T13:30:30Z",
        "lastEditedBy" : "981b170c-729a-429c-b115-0350ea50b32b",
        "tags" : [
        ]
      },
      {
        "id" : "7993a72c-b15d-48d0-a2a3-79f24ce9b028",
        "parentId" : "1c4202d9-a371-4536-beab-b7548c98922f",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Yeah, \r\n\r\n```python\r\n    def __init__(self, ...):\r\n        ...\r\n        self._pandasMapOpsMixin = PandasMapOpsMixin(self)\r\n        ...\r\n\r\n    def mapInPandas(self, udf):\r\n        return self._pandasMapOpsMixin.mapInPandas(udf)\r\n```\r\n\r\nThis one, the `DataFrame` or `SparkSession` have to expose the method themselves in their own classes. So, if you want to add some new pandas APIs, then it should be added into `PandasMapOpsMixin` and then also `DataFrame`. Using `__getattr__` looks to me overkill and a bit odd.\r\n\r\nThe point I wanted to say is, the current `DataFrame` with mixin approaches is closer to the Scala side's `Dataset` since it does not expose such APIs.\r\n\r\nAlso, I believe what I am doing is what self type trait is supposed to be doing. It's coupled to specific type and other types cannot implement this trait.",
        "createdAt" : "2020-01-07T01:18:09Z",
        "updatedAt" : "2020-01-07T13:30:30Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "aae9661b-5755-4060-b1db-56bd962dbbce",
        "parentId" : "1c4202d9-a371-4536-beab-b7548c98922f",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I meant:\r\n\r\n\r\n```python\r\nclass PandasMapOps(object):\r\n    def mapInPandas(self, ...):\r\n        ...\r\n        return ...\r\n\r\n    # other Pandas <> PySpark APIs\r\n```\r\n\r\n\r\n```python\r\nclass DataFrame(object):\r\n    def __init__(self, ...):\r\n        ...\r\n        self.pandas_map_ops = PandasMapOps(self)\r\n\r\n    # other DataFrame APIs equivalent to Scala side.\r\n\r\n    def mapInPandas(self, ...):\r\n        return pandas_map_ops.mapInPandas(...)\r\n```\r\n\r\nvs\r\n\r\n\r\n```python\r\nclass PandasMapOpsMixin(object):\r\n    def mapInPandas(self, ...):\r\n        ...\r\n        return ...\r\n\r\n    # other Pandas <> PySpark APIs\r\n```\r\n\r\n```python\r\nclass DataFrame(PandasMapOpsMixin):\r\n\r\n    # other DataFrame APIs equivalent to Scala side.\r\n\r\n```\r\n\r\nI thought the latter is better.\r\n",
        "createdAt" : "2020-01-07T01:50:34Z",
        "updatedAt" : "2020-01-07T13:30:30Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "df527aee-c97f-4ced-bd69-6a36acf5dfe1",
        "parentId" : "1c4202d9-a371-4536-beab-b7548c98922f",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I think both are fine. It's internal so we can change it later. To save the effort, let's keep what it is in this PR, i.e. the second option.",
        "createdAt" : "2020-01-07T05:17:35Z",
        "updatedAt" : "2020-01-07T13:30:30Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "01808eec-5406-4f41-a530-b7fd8abcdea9",
        "parentId" : "1c4202d9-a371-4536-beab-b7548c98922f",
        "authorId" : "981b170c-729a-429c-b115-0350ea50b32b",
        "body" : "@cloud-fan \r\n\r\n> I think both are fine. It's internal so we can change it later.\r\n\r\nIt is hardly internal, considering that as mixed classes are \"public\" (as much as it is meaningful to say about access control in Python) and, in contrast to other changes proposed here, all shifted non-static methods are part of external API. The impact so far is small though, if that's what you mean.\r\n\r\n@HyukjinKwon \r\n\r\n> Also, I believe what I am doing is what self type trait is supposed to be doing. It's coupled to specific type and other types cannot implement this trait.\r\n\r\nSuch patterns, or its closes Python equivalents (see for example Django mixins for class-based views) typically indicate two things:\r\n\r\n- Potential for inheritance, which is clearly not the case, given both Spark API and design of the `DataFrame` class.\r\n- Non-obligatory character, which once again is not the case.\r\n\r\nSo I guess the question I am trying to ask is - \"what future planned changes justify such move\" - as for now it seems mostly obsolete, and less effort path, given implied time pressure, would be to keep things as-is.",
        "createdAt" : "2020-01-07T13:39:33Z",
        "updatedAt" : "2020-01-07T13:39:33Z",
        "lastEditedBy" : "981b170c-729a-429c-b115-0350ea50b32b",
        "tags" : [
        ]
      },
      {
        "id" : "1df57abb-ddfe-445d-ab28-f3336ac4ad3d",
        "parentId" : "1c4202d9-a371-4536-beab-b7548c98922f",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "> It is hardly internal, considering that as mixed classes are \"public\" (as much as it is meaningful to say about access control in Python) and, in contrast to other changes proposed here, all shifted non-static methods are part of external API. The impact so far is small though, if that's what you mean.\r\n\r\nEither way the same APIs are exposed in the same class. It will be also easily able to switch to each other with minimised change - I actually roughly already tested.\r\n\r\n",
        "createdAt" : "2020-01-07T13:53:54Z",
        "updatedAt" : "2020-01-07T13:57:20Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "ef2edf2f-3040-4ded-b986-9fb75ad39345",
        "parentId" : "1c4202d9-a371-4536-beab-b7548c98922f",
        "authorId" : "981b170c-729a-429c-b115-0350ea50b32b",
        "body" : "> Either way the same APIs are exposed in the same class. It's also easily switch to each other with minimised change.\r\n\r\nWhile it is true, it is not equivalent from perspective of type checkers. And since a lot of related discussions circles around typing, it might be something to consider. \r\n\r\nNonetheless, I can take a hint.",
        "createdAt" : "2020-01-07T14:02:24Z",
        "updatedAt" : "2020-01-07T14:02:24Z",
        "lastEditedBy" : "981b170c-729a-429c-b115-0350ea50b32b",
        "tags" : [
        ]
      },
      {
        "id" : "22098ae9-3650-478d-94fa-c64d326676d9",
        "parentId" : "1c4202d9-a371-4536-beab-b7548c98922f",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "What I am trying to do is basically separate pandas-related APIs, and Scala-equivalent APIs.\r\n\r\nSo, if you want to make some changes in pandas-related APIs,  you make changes in `PandasMapOpsMixin`.\r\nIf you need to make some changes in Scala-equivalent APIs, you make changes at `DataFrame`.\r\nBoth are completely separate in the current way. Given the suggestion, it will mix up.",
        "createdAt" : "2020-01-07T14:14:49Z",
        "updatedAt" : "2020-01-07T14:20:08Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "65cec6e2-910c-440e-b4a6-539edddc6a58",
        "parentId" : "1c4202d9-a371-4536-beab-b7548c98922f",
        "authorId" : "0c812942-02cb-4975-9748-394d1387affa",
        "body" : "+1 on @HyukjinKwon 's suggestion.",
        "createdAt" : "2020-01-07T16:52:37Z",
        "updatedAt" : "2020-01-07T16:52:37Z",
        "lastEditedBy" : "0c812942-02cb-4975-9748-394d1387affa",
        "tags" : [
        ]
      }
    ],
    "commit" : "c63c271d95e7864b69ca477c51e4e2d22b7d9029",
    "line" : 25,
    "diffHunk" : "@@ -1,1 +48,52 @@\n\nclass DataFrame(PandasMapOpsMixin, PandasConversionMixin):\n    \"\"\"A distributed collection of data grouped into named columns.\n"
  },
  {
    "id" : "44d30cf8-0e77-4797-aaf1-4feb2d92c389",
    "prId" : 27109,
    "prUrl" : "https://github.com/apache/spark/pull/27109#pullrequestreview-339612546",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e8d6c026-f538-4173-8a68-92205463a00c",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "nit: Is it possible to move these `Pandas compatibility` too?",
        "createdAt" : "2020-01-08T01:58:14Z",
        "updatedAt" : "2020-01-08T01:58:14Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "17150c9e-dcdf-43cb-bda6-ace8425d0eea",
        "parentId" : "e8d6c026-f538-4173-8a68-92205463a00c",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Actually .. I would like to deprecate and remove those away. I don't think we will add such compatibilities anymore in Spark itself because there are so many differences.\r\n\r\nLet me take an action separately for the three APIs after this PR.",
        "createdAt" : "2020-01-08T02:03:33Z",
        "updatedAt" : "2020-01-08T02:03:33Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "c63c271d95e7864b69ca477c51e4e2d22b7d9029",
    "line" : 221,
    "diffHunk" : "@@ -1,1 +2137,2141 @@\n    ##########################################################################################\n    # Pandas compatibility\n    ##########################################################################################\n"
  }
]