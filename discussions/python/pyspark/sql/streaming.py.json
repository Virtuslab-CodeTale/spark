[
  {
    "id" : "1c1f4267-ea13-4368-904d-c927f7d4baaa",
    "prId" : 32204,
    "prUrl" : "https://github.com/apache/spark/pull/32204#pullrequestreview-665013786",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b5986ff5-0dd7-4440-a980-853e3f69ed7d",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Shall we remove this too?",
        "createdAt" : "2021-05-21T01:07:22Z",
        "updatedAt" : "2021-05-21T01:07:22Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "a10586c3d2887463de16984adb72d205f85f3796",
    "line" : 147,
    "diffHunk" : "@@ -1,1 +486,490 @@            For the extra options, refer to\n            `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-json.html#data-source-option>`_  # noqa\n            in the version you use.\n\n        Notes"
  },
  {
    "id" : "cc117e4f-e3ca-4bae-acb1-d2a38bf1e927",
    "prId" : 32161,
    "prUrl" : "https://github.com/apache/spark/pull/32161#pullrequestreview-639445766",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b2011331-c1bf-4939-a855-160c98bf5996",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "I didn't get why you leave parameters here, for instance `mergeSchema` but removed it in other places?",
        "createdAt" : "2021-04-19T13:19:13Z",
        "updatedAt" : "2021-05-10T23:57:49Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "65f4e299-db0b-41ee-9087-92bf3e2658f8",
        "parentId" : "b2011331-c1bf-4939-a855-160c98bf5996",
        "authorId" : "2d8ced12-ada1-43e1-9240-bf1c11a01e4c",
        "body" : "Good point, @MaxGekk . this might be a kinda confusing part.\r\n\r\nI'd say the root cause is the parameter definitions of existing Python functions are not consistent.\r\nFor example, there is case where the `mergeSchema` is not explicitly defined but just included in `**options`, whereas there is case where the parameter name is explicitly indicated as `mergeSchema` even though it is the same `parquet` function.\r\n\r\n```python\r\ndef parquet(self, *paths, **options):\r\n```\r\n\r\n```python\r\ndef parquet(self, path, mergeSchema=None, pathGlobFilter=None, recursiveFileLookup=None,\r\n    datetimeRebaseMode=None, int96RebaseMode=None):\r\n```\r\n\r\nSo, I basically addressed the Python docstring depending on the several rules below:\r\n- If parameter is not specified but included `**options` or `**kwargs`, remove the description and replace to the `Other Parameters`.\r\n- If parameter is explicitly specified with its name, just leave it as is in the `Parameters`.\r\n\r\nI fully understand this is confusing.\r\nDo you think maybe it would be better to fix the inconsistency of function definitions in this PR as well?\r\n\r\nalso cc @HyukjinKwon ",
        "createdAt" : "2021-04-20T01:44:21Z",
        "updatedAt" : "2021-05-10T23:57:49Z",
        "lastEditedBy" : "2d8ced12-ada1-43e1-9240-bf1c11a01e4c",
        "tags" : [
        ]
      },
      {
        "id" : "b968fb55-16b4-44a4-bb21-c3aa2bfde4f8",
        "parentId" : "b2011331-c1bf-4939-a855-160c98bf5996",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Can you remove the parameters such as `mergeSchema`? It duplicates the docs. the point of this PR is to deduplicate the documentation.",
        "createdAt" : "2021-04-20T01:48:09Z",
        "updatedAt" : "2021-05-10T23:57:49Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "806d4fb6-f3f8-45a6-933c-ad1a2d00eff0",
        "parentId" : "b2011331-c1bf-4939-a855-160c98bf5996",
        "authorId" : "2d8ced12-ada1-43e1-9240-bf1c11a01e4c",
        "body" : "Sounds good, let me address it. Thanks, @HyukjinKwon !",
        "createdAt" : "2021-04-20T02:15:29Z",
        "updatedAt" : "2021-05-10T23:57:50Z",
        "lastEditedBy" : "2d8ced12-ada1-43e1-9240-bf1c11a01e4c",
        "tags" : [
        ]
      }
    ],
    "commit" : "d6417a8124eb61390089313d108eff18fd89e412",
    "line" : 44,
    "diffHunk" : "@@ -1,1 +680,684 @@            the path in any Hadoop supported file system\n\n        Other Parameters\n        ----------------\n        Extra options"
  },
  {
    "id" : "854d03fd-52d5-4b20-a284-4ea4479c62d6",
    "prId" : 30835,
    "prUrl" : "https://github.com/apache/spark/pull/30835#pullrequestreview-555508177",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f8f80b10-1eee-4691-ab14-f5b8fc949c4a",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I was thinking something like ... :\r\n\r\n```python\r\n        \"\"\"\r\n        Starts the execution of the streaming query, which will continually output results to the given\r\n        table as new data arrives. A new table will be created if the table not exists. The returned\r\n        :class:`StreamingQuery` object can be used to interact with the stream.\r\n   \r\n        .. versionadded:: 3.2.0\r\n\r\n        Parameters\r\n        ----------\r\n        tableName : str\r\n            string, for the name of the table.\r\n        format : str, optional\r\n            the format used to save.\r\n        outputMode : str, optional\r\n            specifies how data of a streaming DataFrame/Dataset is written to a\r\n            streaming sink.\r\n\r\n            * `append`: Only the new rows in the streaming DataFrame/Dataset will be written to the\r\n              sink\r\n            * `complete`: All the rows in the streaming DataFrame/Dataset will be written to the\r\n              sink every time these is some updates\r\n            * `update`: only the rows that were updated in the streaming DataFrame/Dataset will be\r\n              written to the sink every time there are some updates. If the query doesn't contain\r\n              aggregations, it will be equivalent to `append` mode.\r\n        partitionBy : str or list, optional\r\n            names of partitioning columns\r\n        queryName : str, optional\r\n            unique name for the query\r\n        **options : dict\r\n            All other string options. You may want to provide a `checkpointLocation`.\r\n\r\n        Notes\r\n        -----\r\n        This API is evolving.\r\n\r\n        Examples\r\n        --------\r\n        >>> sq = sdf.writeStream.format('parquet').queryName('this_query').option(\r\n        ...      'checkpointLocation', '/tmp/checkpoint').toTable(output_table_name)\r\n        >>> sq.isActive\r\n        True\r\n        >>> sq.name\r\n        'this_query'\r\n        >>> sq.stop()\r\n        >>> sq.isActive\r\n        False\r\n        >>> sq = sdf.writeStream.trigger(processingTime='5 seconds').toTable(\r\n        ...     output_table_name, queryName='that_query', outputMode=\"append\", format='parquet',\r\n        ...     checkpointLocation='/tmp/checkpoint')\r\n        >>> sq.name\r\n        'that_query'\r\n        >>> sq.isActive\r\n        True\r\n        >>> sq.stop()\r\n        \"\"\"\r\n```\r\n\r\nor .. \r\n\r\n```python\r\n        \"\"\"\r\n        Starts the execution of the streaming query, which will continually output results to the given\r\n        table as new data arrives.\r\n   \r\n        A new table will be created if the table not exists. The returned\r\n        :class:`StreamingQuery` object can be used to interact with the stream.\r\n   \r\n        .. versionadded:: 3.2.0\r\n\r\n        Parameters\r\n        ----------\r\n        tableName : str\r\n            string, for the name of the table.\r\n        format : str, optional\r\n            the format used to save.\r\n        outputMode : str, optional\r\n            specifies how data of a streaming DataFrame/Dataset is written to a\r\n            streaming sink.\r\n\r\n            * `append`: Only the new rows in the streaming DataFrame/Dataset will be written to the\r\n              sink\r\n            * `complete`: All the rows in the streaming DataFrame/Dataset will be written to the\r\n              sink every time these is some updates\r\n            * `update`: only the rows that were updated in the streaming DataFrame/Dataset will be\r\n              written to the sink every time there are some updates. If the query doesn't contain\r\n              aggregations, it will be equivalent to `append` mode.\r\n        partitionBy : str or list, optional\r\n            names of partitioning columns\r\n        queryName : str, optional\r\n            unique name for the query\r\n        **options : dict\r\n            All other string options. You may want to provide a `checkpointLocation`.\r\n\r\n        Notes\r\n        -----\r\n        This API is evolving.\r\n\r\n        Examples\r\n        --------\r\n        >>> sq = sdf.writeStream.format('parquet').queryName('this_query').option(\r\n        ...      'checkpointLocation', '/tmp/checkpoint').toTable(output_table_name)\r\n        >>> sq.isActive\r\n        True\r\n        >>> sq.name\r\n        'this_query'\r\n        >>> sq.stop()\r\n        >>> sq.isActive\r\n        False\r\n        >>> sq = sdf.writeStream.trigger(processingTime='5 seconds').toTable(\r\n        ...     output_table_name, queryName='that_query', outputMode=\"append\", format='parquet',\r\n        ...     checkpointLocation='/tmp/checkpoint')\r\n        >>> sq.name\r\n        'that_query'\r\n        >>> sq.isActive\r\n        True\r\n        >>> sq.stop()\r\n        \"\"\"\r\n```\r\n\r\nEither way should be fine. There's no strict rule on the docstrings currently (except https://numpydoc.readthedocs.io/en/latest/format.html). It's just my suggestion :-).",
        "createdAt" : "2020-12-18T14:03:01Z",
        "updatedAt" : "2020-12-20T03:49:08Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "56ced09ea84d9ad89f01daaf5c197b9969314fad",
    "line" : 117,
    "diffHunk" : "@@ -1,1 +1553,1557 @@        True\n        >>> sq.stop() # doctest: +SKIP\n        \"\"\"\n        # TODO(SPARK-33659): document the current behavior for DataStreamWriter.toTable API\n        self.options(**options)"
  },
  {
    "id" : "272f8d4f-f48c-48cd-a12f-97aae949f352",
    "prId" : 29563,
    "prUrl" : "https://github.com/apache/spark/pull/29563#pullrequestreview-477597720",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2d45ea88-6b3a-4599-bf8a-dc9f2d7644be",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "This was failing?",
        "createdAt" : "2020-08-28T08:44:09Z",
        "updatedAt" : "2020-08-29T18:33:37Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "d1305015-2772-4f69-b558-dbcdcef3bf35",
        "parentId" : "2d45ea88-6b3a-4599-bf8a-dc9f2d7644be",
        "authorId" : "e29a7794-ac2e-4e6f-a690-737e83e1bace",
        "body" : "I tried to run at, and it seems to run:\r\n```\r\nMacBook-Pro-van-Fokko:spark fokkodriesprong$ python3 ./python/pyspark/sql/streaming.py\r\n20/08/28 11:41:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\nUsing Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\r\nSetting default log level to \"WARN\".\r\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\r\n20/08/28 11:41:33 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /private/var/folders/km/xypq2kxs4ys3dt6bwtd4fbj00000gn/T/temporary-ca2e7725-cb1f-4ac9-adc6-5533ab36db58. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\r\n20/08/28 11:41:34 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /private/var/folders/km/xypq2kxs4ys3dt6bwtd4fbj00000gn/T/temporary-0c6bf4bd-795f-4a82-ae4c-72f2cd7751ab. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\r\n20/08/28 11:41:34 WARN Shell: Interrupted while joining on: Thread[Thread-48,5,]\r\njava.lang.InterruptedException\r\n\tat java.lang.Object.wait(Native Method)\r\n\tat java.lang.Thread.join(Thread.java:1252)\r\n\tat java.lang.Thread.join(Thread.java:1326)\r\n\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:629)\r\n\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:580)\r\n\tat org.apache.hadoop.util.Shell.run(Shell.java:482)\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:776)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:869)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:852)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)\r\n\tat org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1017)\r\n\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:100)\r\n\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\r\n\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\r\n\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:596)\r\n\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:686)\r\n\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:682)\r\n\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\r\n\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:688)\r\n\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:310)\r\n\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:133)\r\n\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:136)\r\n\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:316)\r\n\tat org.apache.spark.sql.execution.streaming.HDFSMetadataLog.writeBatchToFile(HDFSMetadataLog.scala:131)\r\n\tat org.apache.spark.sql.execution.streaming.HDFSMetadataLog.$anonfun$add$3(HDFSMetadataLog.scala:120)\r\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.execution.streaming.HDFSMetadataLog.add(HDFSMetadataLog.scala:118)\r\n\tat org.apache.spark.sql.execution.streaming.CompactibleFileStreamLog.add(CompactibleFileStreamLog.scala:158)\r\n\tat org.apache.spark.sql.execution.streaming.FileStreamSourceLog.add(FileStreamSourceLog.scala:69)\r\n\tat org.apache.spark.sql.execution.streaming.FileStreamSource.fetchMaxOffset(FileStreamSource.scala:150)\r\n\tat org.apache.spark.sql.execution.streaming.FileStreamSource.latestOffset(FileStreamSource.scala:286)\r\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$3(MicroBatchExecution.scala:380)\r\n\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:352)\r\n\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:350)\r\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\r\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:371)\r\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\r\n\tat scala.collection.immutable.Map$Map1.foreach(Map.scala:128)\r\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\r\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\r\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\r\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:368)\r\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\r\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:598)\r\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:364)\r\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:208)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:352)\r\n\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:350)\r\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\r\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:191)\r\n\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:57)\r\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:185)\r\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:334)\r\n\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:245)\r\n20/08/28 11:41:36 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /private/var/folders/km/xypq2kxs4ys3dt6bwtd4fbj00000gn/T/temporary-3c27fa5e-7644-437d-9832-cac5168d4af5. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\r\n20/08/28 11:41:36 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /private/var/folders/km/xypq2kxs4ys3dt6bwtd4fbj00000gn/T/temporary-5d2c3db7-4d59-4e0b-953e-429a9d398ef2. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\r\n20/08/28 11:41:36 WARN Shell: Interrupted while joining on: Thread[Thread-110,5,]\r\njava.lang.InterruptedException\r\n\tat java.lang.Object.wait(Native Method)\r\n\tat java.lang.Thread.join(Thread.java:1252)\r\n\tat java.lang.Thread.join(Thread.java:1326)\r\n\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:629)\r\n\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:580)\r\n\tat org.apache.hadoop.util.Shell.run(Shell.java:482)\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:776)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:869)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:852)\r\n\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:160)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:837)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:826)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:797)\r\n\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileLinkStatus(DelegateToFileSystem.java:131)\r\n\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:717)\r\n\tat org.apache.hadoop.fs.FilterFs.renameInternal(FilterFs.java:240)\r\n\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:690)\r\n\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:958)\r\n\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:329)\r\n\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:147)\r\n\tat org.apache.spark.sql.execution.streaming.HDFSMetadataLog.writeBatchToFile(HDFSMetadataLog.scala:134)\r\n\tat org.apache.spark.sql.execution.streaming.HDFSMetadataLog.$anonfun$add$3(HDFSMetadataLog.scala:120)\r\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.execution.streaming.HDFSMetadataLog.add(HDFSMetadataLog.scala:118)\r\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$12(MicroBatchExecution.scala:418)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:352)\r\n\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:350)\r\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\r\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:416)\r\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\r\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:598)\r\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:364)\r\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:208)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:352)\r\n\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:350)\r\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\r\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:191)\r\n\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:57)\r\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:185)\r\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:334)\r\n\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:245)\r\n20/08/28 11:41:37 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /private/var/folders/km/xypq2kxs4ys3dt6bwtd4fbj00000gn/T/temporary-be1a0476-b88e-4b75-ac37-0486dbfe5a2e. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\r\nMacBook-Pro-van-Fokko:spark fokkodriesprong$ echo $?\r\n0\r\n```\r\n\r\nI was under the impression that the `py4j` and `sc` were included using the line above, that is was being pulled from some global context, but this isn't the case. It seems to run, but maybe it doesn't hit the `except`. It looks like some lingering test-code, maybe move this to the tests directory?\r\n\r\ncc @zsxwing @tdas ",
        "createdAt" : "2020-08-28T09:51:30Z",
        "updatedAt" : "2020-08-29T18:33:37Z",
        "lastEditedBy" : "e29a7794-ac2e-4e6f-a690-737e83e1bace",
        "tags" : [
        ]
      }
    ],
    "commit" : "06480a7b4e8c5106ac7f7ea0fa9cecd6ea09e0bb",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +1240,1244 @@    try:\n        spark = SparkSession.builder.getOrCreate()\n    except py4j.protocol.Py4JError:  # noqa: F821\n        spark = SparkSession(sc)  # noqa: F821\n"
  },
  {
    "id" : "34098565-521d-44ca-b401-c06af6fd5ea2",
    "prId" : 27956,
    "prUrl" : "https://github.com/apache/spark/pull/27956#pullrequestreview-380880024",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d23d2787-b579-4fba-96d3-12a3160eefda",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "So this is wrong right? @HyukjinKwon can you check the python doc and see if the link is corrected?",
        "createdAt" : "2020-03-20T03:42:53Z",
        "updatedAt" : "2020-03-20T06:24:36Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "fae4b795-b823-4b0b-b2fc-bee3adc1765d",
        "parentId" : "d23d2787-b579-4fba-96d3-12a3160eefda",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "This is fine - it's a way of reusing a link. It's a relative link",
        "createdAt" : "2020-03-25T05:01:28Z",
        "updatedAt" : "2020-03-25T05:02:04Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "d1f98531-f1fd-4b6e-affb-6e41c73ee338",
        "parentId" : "d23d2787-b579-4fba-96d3-12a3160eefda",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Sorry for my late response.",
        "createdAt" : "2020-03-25T05:01:45Z",
        "updatedAt" : "2020-03-25T05:01:45Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "cba81c65-01ec-4a77-9056-87a2019cfafd",
        "parentId" : "d23d2787-b579-4fba-96d3-12a3160eefda",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "the links are malformed, please check  http://spark.apache.org/docs/3.0.0-preview2/api/python/pyspark.sql.html#module-pyspark.sql.streaming with `partition discovery` content",
        "createdAt" : "2020-03-25T05:05:36Z",
        "updatedAt" : "2020-03-25T05:05:36Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "6f8c1d6b-406f-4537-9e94-b67d40977f8d",
        "parentId" : "d23d2787-b579-4fba-96d3-12a3160eefda",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Oh, right. The root directory is different when it's released. Sure, seems like it's best to stick to the absolute links then. The syntax itself is fine as a relative link",
        "createdAt" : "2020-03-25T05:12:45Z",
        "updatedAt" : "2020-03-25T05:12:46Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "3d9b17ac-f7b6-4e24-baae-2ab0594b1f18",
        "parentId" : "d23d2787-b579-4fba-96d3-12a3160eefda",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@yaooqinn can you make a PR to fix these?",
        "createdAt" : "2020-03-25T06:10:35Z",
        "updatedAt" : "2020-03-25T06:10:35Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "298c2359-cfc6-4544-a8df-0297d9101379",
        "parentId" : "d23d2787-b579-4fba-96d3-12a3160eefda",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "OK, I'll check it",
        "createdAt" : "2020-03-25T06:16:20Z",
        "updatedAt" : "2020-03-25T06:16:21Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "187bd7096432b0f8c565670dbab6383787139804",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +490,494 @@                                    disables `partition discovery`_.\n\n        .. _partition discovery: /sql-data-sources-parquet.html#partition-discovery\n        .. _datetime pattern: https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n"
  },
  {
    "id" : "aeb86415-06ee-455f-acbd-eb3b215c7a52",
    "prId" : 27553,
    "prUrl" : "https://github.com/apache/spark/pull/27553#pullrequestreview-358672332",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fae17557-2686-4c04-bbf1-572160912611",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Why `<pyspark.sql.DataFrame>` is needed?",
        "createdAt" : "2020-02-13T01:07:48Z",
        "updatedAt" : "2020-02-13T01:07:48Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "5da8f8b7-f880-4a54-be47-6c10810d2b7f",
        "parentId" : "fae17557-2686-4c04-bbf1-572160912611",
        "authorId" : "48393283-384a-40f1-96e3-dbfacb17fa96",
        "body" : "Writing `` :class:`DataFrame` `` results in documentation that does not link to the class, and makes Sphinx output the following warning when running Sphinx in nit-picky mode (`-n` on the command line or `nitpicky = True` in the Sphinx config):\r\n> spark/python/pyspark/sql/streaming.py:docstring of pyspark.sql.streaming.DataStreamReader:1: WARNING: py:class reference target not found: DataFrame\r\n\r\nOne could also just write `` :class:`pyspark.sql.DataFrame` `` to fix the warning and documentation linkage, but then the docs are rendered with the full `pyspark.sql.DataFrame` name. I figured rendering only `DataFrame` by writing `` :class:`DataFrame <pyspark.sql.DataFrame>` `` was more fitting with the rest of the docs, but I can of course change this is you'd prefer.",
        "createdAt" : "2020-02-13T20:11:31Z",
        "updatedAt" : "2020-02-13T20:11:31Z",
        "lastEditedBy" : "48393283-384a-40f1-96e3-dbfacb17fa96",
        "tags" : [
        ]
      },
      {
        "id" : "15111113-6cc7-4dec-b442-2c502d1a8057",
        "parentId" : "fae17557-2686-4c04-bbf1-572160912611",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "okie",
        "createdAt" : "2020-02-14T01:59:54Z",
        "updatedAt" : "2020-02-14T01:59:55Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "6d0f6edbcadc83092eb0e9e7000de1f133fe8fa1",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +277,281 @@class DataStreamReader(OptionUtils):\n    \"\"\"\n    Interface used to load a streaming :class:`DataFrame <pyspark.sql.DataFrame>` from external\n    storage systems (e.g. file systems, key-value stores, etc).\n    Use :attr:`SparkSession.readStream <pyspark.sql.SparkSession.readStream>` to access this."
  },
  {
    "id" : "b86e6fb2-0014-4985-9036-0a209371172b",
    "prId" : 27553,
    "prUrl" : "https://github.com/apache/spark/pull/27553#pullrequestreview-358518755",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "63084d48-68fa-4944-ae58-ee30f23f431a",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "ditto - technically this should be `meth`",
        "createdAt" : "2020-02-13T01:08:17Z",
        "updatedAt" : "2020-02-13T01:08:18Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "c90faa2f-903e-488f-8a6f-a5a1e21b0b6a",
        "parentId" : "63084d48-68fa-4944-ae58-ee30f23f431a",
        "authorId" : "48393283-384a-40f1-96e3-dbfacb17fa96",
        "body" : "See above -- `meth` would render like `func`, and wrongly suggest that one can call `readStream()`.",
        "createdAt" : "2020-02-13T20:12:36Z",
        "updatedAt" : "2020-02-13T20:12:36Z",
        "lastEditedBy" : "48393283-384a-40f1-96e3-dbfacb17fa96",
        "tags" : [
        ]
      }
    ],
    "commit" : "6d0f6edbcadc83092eb0e9e7000de1f133fe8fa1",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +279,283 @@    Interface used to load a streaming :class:`DataFrame <pyspark.sql.DataFrame>` from external\n    storage systems (e.g. file systems, key-value stores, etc).\n    Use :attr:`SparkSession.readStream <pyspark.sql.SparkSession.readStream>` to access this.\n\n    .. note:: Evolving."
  },
  {
    "id" : "26cfa876-0a33-4162-b780-36e57c9662d9",
    "prId" : 26299,
    "prUrl" : "https://github.com/apache/spark/pull/26299#pullrequestreview-309494035",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c7ce7e89-659e-41ed-9a21-bf8f9d54ab12",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Nice catch!",
        "createdAt" : "2019-10-30T20:02:31Z",
        "updatedAt" : "2019-11-01T15:27:53Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "fa3e5f88-4a2d-4bda-81e7-d0cc229ce741",
        "parentId" : "c7ce7e89-659e-41ed-9a21-bf8f9d54ab12",
        "authorId" : "2cab227a-d299-49d0-bb77-cdfa6ff6748e",
        "body" : "Thanks!",
        "createdAt" : "2019-10-30T20:19:09Z",
        "updatedAt" : "2019-11-01T15:27:53Z",
        "lastEditedBy" : "2cab227a-d299-49d0-bb77-cdfa6ff6748e",
        "tags" : [
        ]
      }
    ],
    "commit" : "4e909e2241feedc6506d5396b533d1d3faf66d8d",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +1066,1070 @@        ...     batch_df.collect()\n        ...\n        >>> writer = sdf.writeStream.foreachBatch(func)\n        \"\"\"\n"
  }
]