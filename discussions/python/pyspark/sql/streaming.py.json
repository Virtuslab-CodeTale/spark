[
  {
    "id" : "1c1f4267-ea13-4368-904d-c927f7d4baaa",
    "prId" : 32204,
    "prUrl" : "https://github.com/apache/spark/pull/32204#pullrequestreview-665013786",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b5986ff5-0dd7-4440-a980-853e3f69ed7d",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Shall we remove this too?",
        "createdAt" : "2021-05-21T01:07:22Z",
        "updatedAt" : "2021-05-21T01:07:22Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "a10586c3d2887463de16984adb72d205f85f3796",
    "line" : 147,
    "diffHunk" : "@@ -1,1 +486,490 @@            For the extra options, refer to\n            `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-json.html#data-source-option>`_  # noqa\n            in the version you use.\n\n        Notes"
  },
  {
    "id" : "cc117e4f-e3ca-4bae-acb1-d2a38bf1e927",
    "prId" : 32161,
    "prUrl" : "https://github.com/apache/spark/pull/32161#pullrequestreview-639445766",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b2011331-c1bf-4939-a855-160c98bf5996",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "I didn't get why you leave parameters here, for instance `mergeSchema` but removed it in other places?",
        "createdAt" : "2021-04-19T13:19:13Z",
        "updatedAt" : "2021-05-10T23:57:49Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "65f4e299-db0b-41ee-9087-92bf3e2658f8",
        "parentId" : "b2011331-c1bf-4939-a855-160c98bf5996",
        "authorId" : "2d8ced12-ada1-43e1-9240-bf1c11a01e4c",
        "body" : "Good point, @MaxGekk . this might be a kinda confusing part.\r\n\r\nI'd say the root cause is the parameter definitions of existing Python functions are not consistent.\r\nFor example, there is case where the `mergeSchema` is not explicitly defined but just included in `**options`, whereas there is case where the parameter name is explicitly indicated as `mergeSchema` even though it is the same `parquet` function.\r\n\r\n```python\r\ndef parquet(self, *paths, **options):\r\n```\r\n\r\n```python\r\ndef parquet(self, path, mergeSchema=None, pathGlobFilter=None, recursiveFileLookup=None,\r\n    datetimeRebaseMode=None, int96RebaseMode=None):\r\n```\r\n\r\nSo, I basically addressed the Python docstring depending on the several rules below:\r\n- If parameter is not specified but included `**options` or `**kwargs`, remove the description and replace to the `Other Parameters`.\r\n- If parameter is explicitly specified with its name, just leave it as is in the `Parameters`.\r\n\r\nI fully understand this is confusing.\r\nDo you think maybe it would be better to fix the inconsistency of function definitions in this PR as well?\r\n\r\nalso cc @HyukjinKwon ",
        "createdAt" : "2021-04-20T01:44:21Z",
        "updatedAt" : "2021-05-10T23:57:49Z",
        "lastEditedBy" : "2d8ced12-ada1-43e1-9240-bf1c11a01e4c",
        "tags" : [
        ]
      },
      {
        "id" : "b968fb55-16b4-44a4-bb21-c3aa2bfde4f8",
        "parentId" : "b2011331-c1bf-4939-a855-160c98bf5996",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Can you remove the parameters such as `mergeSchema`? It duplicates the docs. the point of this PR is to deduplicate the documentation.",
        "createdAt" : "2021-04-20T01:48:09Z",
        "updatedAt" : "2021-05-10T23:57:49Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "806d4fb6-f3f8-45a6-933c-ad1a2d00eff0",
        "parentId" : "b2011331-c1bf-4939-a855-160c98bf5996",
        "authorId" : "2d8ced12-ada1-43e1-9240-bf1c11a01e4c",
        "body" : "Sounds good, let me address it. Thanks, @HyukjinKwon !",
        "createdAt" : "2021-04-20T02:15:29Z",
        "updatedAt" : "2021-05-10T23:57:50Z",
        "lastEditedBy" : "2d8ced12-ada1-43e1-9240-bf1c11a01e4c",
        "tags" : [
        ]
      }
    ],
    "commit" : "d6417a8124eb61390089313d108eff18fd89e412",
    "line" : 44,
    "diffHunk" : "@@ -1,1 +680,684 @@            the path in any Hadoop supported file system\n\n        Other Parameters\n        ----------------\n        Extra options"
  },
  {
    "id" : "854d03fd-52d5-4b20-a284-4ea4479c62d6",
    "prId" : 30835,
    "prUrl" : "https://github.com/apache/spark/pull/30835#pullrequestreview-555508177",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f8f80b10-1eee-4691-ab14-f5b8fc949c4a",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I was thinking something like ... :\r\n\r\n```python\r\n        \"\"\"\r\n        Starts the execution of the streaming query, which will continually output results to the given\r\n        table as new data arrives. A new table will be created if the table not exists. The returned\r\n        :class:`StreamingQuery` object can be used to interact with the stream.\r\n   \r\n        .. versionadded:: 3.2.0\r\n\r\n        Parameters\r\n        ----------\r\n        tableName : str\r\n            string, for the name of the table.\r\n        format : str, optional\r\n            the format used to save.\r\n        outputMode : str, optional\r\n            specifies how data of a streaming DataFrame/Dataset is written to a\r\n            streaming sink.\r\n\r\n            * `append`: Only the new rows in the streaming DataFrame/Dataset will be written to the\r\n              sink\r\n            * `complete`: All the rows in the streaming DataFrame/Dataset will be written to the\r\n              sink every time these is some updates\r\n            * `update`: only the rows that were updated in the streaming DataFrame/Dataset will be\r\n              written to the sink every time there are some updates. If the query doesn't contain\r\n              aggregations, it will be equivalent to `append` mode.\r\n        partitionBy : str or list, optional\r\n            names of partitioning columns\r\n        queryName : str, optional\r\n            unique name for the query\r\n        **options : dict\r\n            All other string options. You may want to provide a `checkpointLocation`.\r\n\r\n        Notes\r\n        -----\r\n        This API is evolving.\r\n\r\n        Examples\r\n        --------\r\n        >>> sq = sdf.writeStream.format('parquet').queryName('this_query').option(\r\n        ...      'checkpointLocation', '/tmp/checkpoint').toTable(output_table_name)\r\n        >>> sq.isActive\r\n        True\r\n        >>> sq.name\r\n        'this_query'\r\n        >>> sq.stop()\r\n        >>> sq.isActive\r\n        False\r\n        >>> sq = sdf.writeStream.trigger(processingTime='5 seconds').toTable(\r\n        ...     output_table_name, queryName='that_query', outputMode=\"append\", format='parquet',\r\n        ...     checkpointLocation='/tmp/checkpoint')\r\n        >>> sq.name\r\n        'that_query'\r\n        >>> sq.isActive\r\n        True\r\n        >>> sq.stop()\r\n        \"\"\"\r\n```\r\n\r\nor .. \r\n\r\n```python\r\n        \"\"\"\r\n        Starts the execution of the streaming query, which will continually output results to the given\r\n        table as new data arrives.\r\n   \r\n        A new table will be created if the table not exists. The returned\r\n        :class:`StreamingQuery` object can be used to interact with the stream.\r\n   \r\n        .. versionadded:: 3.2.0\r\n\r\n        Parameters\r\n        ----------\r\n        tableName : str\r\n            string, for the name of the table.\r\n        format : str, optional\r\n            the format used to save.\r\n        outputMode : str, optional\r\n            specifies how data of a streaming DataFrame/Dataset is written to a\r\n            streaming sink.\r\n\r\n            * `append`: Only the new rows in the streaming DataFrame/Dataset will be written to the\r\n              sink\r\n            * `complete`: All the rows in the streaming DataFrame/Dataset will be written to the\r\n              sink every time these is some updates\r\n            * `update`: only the rows that were updated in the streaming DataFrame/Dataset will be\r\n              written to the sink every time there are some updates. If the query doesn't contain\r\n              aggregations, it will be equivalent to `append` mode.\r\n        partitionBy : str or list, optional\r\n            names of partitioning columns\r\n        queryName : str, optional\r\n            unique name for the query\r\n        **options : dict\r\n            All other string options. You may want to provide a `checkpointLocation`.\r\n\r\n        Notes\r\n        -----\r\n        This API is evolving.\r\n\r\n        Examples\r\n        --------\r\n        >>> sq = sdf.writeStream.format('parquet').queryName('this_query').option(\r\n        ...      'checkpointLocation', '/tmp/checkpoint').toTable(output_table_name)\r\n        >>> sq.isActive\r\n        True\r\n        >>> sq.name\r\n        'this_query'\r\n        >>> sq.stop()\r\n        >>> sq.isActive\r\n        False\r\n        >>> sq = sdf.writeStream.trigger(processingTime='5 seconds').toTable(\r\n        ...     output_table_name, queryName='that_query', outputMode=\"append\", format='parquet',\r\n        ...     checkpointLocation='/tmp/checkpoint')\r\n        >>> sq.name\r\n        'that_query'\r\n        >>> sq.isActive\r\n        True\r\n        >>> sq.stop()\r\n        \"\"\"\r\n```\r\n\r\nEither way should be fine. There's no strict rule on the docstrings currently (except https://numpydoc.readthedocs.io/en/latest/format.html). It's just my suggestion :-).",
        "createdAt" : "2020-12-18T14:03:01Z",
        "updatedAt" : "2020-12-20T03:49:08Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "56ced09ea84d9ad89f01daaf5c197b9969314fad",
    "line" : 117,
    "diffHunk" : "@@ -1,1 +1553,1557 @@        True\n        >>> sq.stop() # doctest: +SKIP\n        \"\"\"\n        # TODO(SPARK-33659): document the current behavior for DataStreamWriter.toTable API\n        self.options(**options)"
  }
]