[
  {
    "id" : "1c1f4267-ea13-4368-904d-c927f7d4baaa",
    "prId" : 32204,
    "prUrl" : "https://github.com/apache/spark/pull/32204#pullrequestreview-665013786",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b5986ff5-0dd7-4440-a980-853e3f69ed7d",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Shall we remove this too?",
        "createdAt" : "2021-05-21T01:07:22Z",
        "updatedAt" : "2021-05-21T01:07:22Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "a10586c3d2887463de16984adb72d205f85f3796",
    "line" : 147,
    "diffHunk" : "@@ -1,1 +486,490 @@            For the extra options, refer to\n            `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-json.html#data-source-option>`_  # noqa\n            in the version you use.\n\n        Notes"
  },
  {
    "id" : "cc117e4f-e3ca-4bae-acb1-d2a38bf1e927",
    "prId" : 32161,
    "prUrl" : "https://github.com/apache/spark/pull/32161#pullrequestreview-639445766",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b2011331-c1bf-4939-a855-160c98bf5996",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "I didn't get why you leave parameters here, for instance `mergeSchema` but removed it in other places?",
        "createdAt" : "2021-04-19T13:19:13Z",
        "updatedAt" : "2021-05-10T23:57:49Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "65f4e299-db0b-41ee-9087-92bf3e2658f8",
        "parentId" : "b2011331-c1bf-4939-a855-160c98bf5996",
        "authorId" : "2d8ced12-ada1-43e1-9240-bf1c11a01e4c",
        "body" : "Good point, @MaxGekk . this might be a kinda confusing part.\r\n\r\nI'd say the root cause is the parameter definitions of existing Python functions are not consistent.\r\nFor example, there is case where the `mergeSchema` is not explicitly defined but just included in `**options`, whereas there is case where the parameter name is explicitly indicated as `mergeSchema` even though it is the same `parquet` function.\r\n\r\n```python\r\ndef parquet(self, *paths, **options):\r\n```\r\n\r\n```python\r\ndef parquet(self, path, mergeSchema=None, pathGlobFilter=None, recursiveFileLookup=None,\r\n    datetimeRebaseMode=None, int96RebaseMode=None):\r\n```\r\n\r\nSo, I basically addressed the Python docstring depending on the several rules below:\r\n- If parameter is not specified but included `**options` or `**kwargs`, remove the description and replace to the `Other Parameters`.\r\n- If parameter is explicitly specified with its name, just leave it as is in the `Parameters`.\r\n\r\nI fully understand this is confusing.\r\nDo you think maybe it would be better to fix the inconsistency of function definitions in this PR as well?\r\n\r\nalso cc @HyukjinKwon ",
        "createdAt" : "2021-04-20T01:44:21Z",
        "updatedAt" : "2021-05-10T23:57:49Z",
        "lastEditedBy" : "2d8ced12-ada1-43e1-9240-bf1c11a01e4c",
        "tags" : [
        ]
      },
      {
        "id" : "b968fb55-16b4-44a4-bb21-c3aa2bfde4f8",
        "parentId" : "b2011331-c1bf-4939-a855-160c98bf5996",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Can you remove the parameters such as `mergeSchema`? It duplicates the docs. the point of this PR is to deduplicate the documentation.",
        "createdAt" : "2021-04-20T01:48:09Z",
        "updatedAt" : "2021-05-10T23:57:49Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "806d4fb6-f3f8-45a6-933c-ad1a2d00eff0",
        "parentId" : "b2011331-c1bf-4939-a855-160c98bf5996",
        "authorId" : "2d8ced12-ada1-43e1-9240-bf1c11a01e4c",
        "body" : "Sounds good, let me address it. Thanks, @HyukjinKwon !",
        "createdAt" : "2021-04-20T02:15:29Z",
        "updatedAt" : "2021-05-10T23:57:50Z",
        "lastEditedBy" : "2d8ced12-ada1-43e1-9240-bf1c11a01e4c",
        "tags" : [
        ]
      }
    ],
    "commit" : "d6417a8124eb61390089313d108eff18fd89e412",
    "line" : 44,
    "diffHunk" : "@@ -1,1 +680,684 @@            the path in any Hadoop supported file system\n\n        Other Parameters\n        ----------------\n        Extra options"
  },
  {
    "id" : "854d03fd-52d5-4b20-a284-4ea4479c62d6",
    "prId" : 30835,
    "prUrl" : "https://github.com/apache/spark/pull/30835#pullrequestreview-555508177",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f8f80b10-1eee-4691-ab14-f5b8fc949c4a",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I was thinking something like ... :\r\n\r\n```python\r\n        \"\"\"\r\n        Starts the execution of the streaming query, which will continually output results to the given\r\n        table as new data arrives. A new table will be created if the table not exists. The returned\r\n        :class:`StreamingQuery` object can be used to interact with the stream.\r\n   \r\n        .. versionadded:: 3.2.0\r\n\r\n        Parameters\r\n        ----------\r\n        tableName : str\r\n            string, for the name of the table.\r\n        format : str, optional\r\n            the format used to save.\r\n        outputMode : str, optional\r\n            specifies how data of a streaming DataFrame/Dataset is written to a\r\n            streaming sink.\r\n\r\n            * `append`: Only the new rows in the streaming DataFrame/Dataset will be written to the\r\n              sink\r\n            * `complete`: All the rows in the streaming DataFrame/Dataset will be written to the\r\n              sink every time these is some updates\r\n            * `update`: only the rows that were updated in the streaming DataFrame/Dataset will be\r\n              written to the sink every time there are some updates. If the query doesn't contain\r\n              aggregations, it will be equivalent to `append` mode.\r\n        partitionBy : str or list, optional\r\n            names of partitioning columns\r\n        queryName : str, optional\r\n            unique name for the query\r\n        **options : dict\r\n            All other string options. You may want to provide a `checkpointLocation`.\r\n\r\n        Notes\r\n        -----\r\n        This API is evolving.\r\n\r\n        Examples\r\n        --------\r\n        >>> sq = sdf.writeStream.format('parquet').queryName('this_query').option(\r\n        ...      'checkpointLocation', '/tmp/checkpoint').toTable(output_table_name)\r\n        >>> sq.isActive\r\n        True\r\n        >>> sq.name\r\n        'this_query'\r\n        >>> sq.stop()\r\n        >>> sq.isActive\r\n        False\r\n        >>> sq = sdf.writeStream.trigger(processingTime='5 seconds').toTable(\r\n        ...     output_table_name, queryName='that_query', outputMode=\"append\", format='parquet',\r\n        ...     checkpointLocation='/tmp/checkpoint')\r\n        >>> sq.name\r\n        'that_query'\r\n        >>> sq.isActive\r\n        True\r\n        >>> sq.stop()\r\n        \"\"\"\r\n```\r\n\r\nor .. \r\n\r\n```python\r\n        \"\"\"\r\n        Starts the execution of the streaming query, which will continually output results to the given\r\n        table as new data arrives.\r\n   \r\n        A new table will be created if the table not exists. The returned\r\n        :class:`StreamingQuery` object can be used to interact with the stream.\r\n   \r\n        .. versionadded:: 3.2.0\r\n\r\n        Parameters\r\n        ----------\r\n        tableName : str\r\n            string, for the name of the table.\r\n        format : str, optional\r\n            the format used to save.\r\n        outputMode : str, optional\r\n            specifies how data of a streaming DataFrame/Dataset is written to a\r\n            streaming sink.\r\n\r\n            * `append`: Only the new rows in the streaming DataFrame/Dataset will be written to the\r\n              sink\r\n            * `complete`: All the rows in the streaming DataFrame/Dataset will be written to the\r\n              sink every time these is some updates\r\n            * `update`: only the rows that were updated in the streaming DataFrame/Dataset will be\r\n              written to the sink every time there are some updates. If the query doesn't contain\r\n              aggregations, it will be equivalent to `append` mode.\r\n        partitionBy : str or list, optional\r\n            names of partitioning columns\r\n        queryName : str, optional\r\n            unique name for the query\r\n        **options : dict\r\n            All other string options. You may want to provide a `checkpointLocation`.\r\n\r\n        Notes\r\n        -----\r\n        This API is evolving.\r\n\r\n        Examples\r\n        --------\r\n        >>> sq = sdf.writeStream.format('parquet').queryName('this_query').option(\r\n        ...      'checkpointLocation', '/tmp/checkpoint').toTable(output_table_name)\r\n        >>> sq.isActive\r\n        True\r\n        >>> sq.name\r\n        'this_query'\r\n        >>> sq.stop()\r\n        >>> sq.isActive\r\n        False\r\n        >>> sq = sdf.writeStream.trigger(processingTime='5 seconds').toTable(\r\n        ...     output_table_name, queryName='that_query', outputMode=\"append\", format='parquet',\r\n        ...     checkpointLocation='/tmp/checkpoint')\r\n        >>> sq.name\r\n        'that_query'\r\n        >>> sq.isActive\r\n        True\r\n        >>> sq.stop()\r\n        \"\"\"\r\n```\r\n\r\nEither way should be fine. There's no strict rule on the docstrings currently (except https://numpydoc.readthedocs.io/en/latest/format.html). It's just my suggestion :-).",
        "createdAt" : "2020-12-18T14:03:01Z",
        "updatedAt" : "2020-12-20T03:49:08Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "56ced09ea84d9ad89f01daaf5c197b9969314fad",
    "line" : 117,
    "diffHunk" : "@@ -1,1 +1553,1557 @@        True\n        >>> sq.stop() # doctest: +SKIP\n        \"\"\"\n        # TODO(SPARK-33659): document the current behavior for DataStreamWriter.toTable API\n        self.options(**options)"
  },
  {
    "id" : "272f8d4f-f48c-48cd-a12f-97aae949f352",
    "prId" : 29563,
    "prUrl" : "https://github.com/apache/spark/pull/29563#pullrequestreview-477597720",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2d45ea88-6b3a-4599-bf8a-dc9f2d7644be",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "This was failing?",
        "createdAt" : "2020-08-28T08:44:09Z",
        "updatedAt" : "2020-08-29T18:33:37Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "d1305015-2772-4f69-b558-dbcdcef3bf35",
        "parentId" : "2d45ea88-6b3a-4599-bf8a-dc9f2d7644be",
        "authorId" : "e29a7794-ac2e-4e6f-a690-737e83e1bace",
        "body" : "I tried to run at, and it seems to run:\r\n```\r\nMacBook-Pro-van-Fokko:spark fokkodriesprong$ python3 ./python/pyspark/sql/streaming.py\r\n20/08/28 11:41:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\nUsing Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\r\nSetting default log level to \"WARN\".\r\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\r\n20/08/28 11:41:33 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /private/var/folders/km/xypq2kxs4ys3dt6bwtd4fbj00000gn/T/temporary-ca2e7725-cb1f-4ac9-adc6-5533ab36db58. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\r\n20/08/28 11:41:34 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /private/var/folders/km/xypq2kxs4ys3dt6bwtd4fbj00000gn/T/temporary-0c6bf4bd-795f-4a82-ae4c-72f2cd7751ab. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\r\n20/08/28 11:41:34 WARN Shell: Interrupted while joining on: Thread[Thread-48,5,]\r\njava.lang.InterruptedException\r\n\tat java.lang.Object.wait(Native Method)\r\n\tat java.lang.Thread.join(Thread.java:1252)\r\n\tat java.lang.Thread.join(Thread.java:1326)\r\n\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:629)\r\n\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:580)\r\n\tat org.apache.hadoop.util.Shell.run(Shell.java:482)\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:776)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:869)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:852)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)\r\n\tat org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1017)\r\n\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:100)\r\n\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\r\n\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\r\n\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:596)\r\n\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:686)\r\n\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:682)\r\n\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\r\n\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:688)\r\n\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:310)\r\n\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:133)\r\n\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:136)\r\n\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:316)\r\n\tat org.apache.spark.sql.execution.streaming.HDFSMetadataLog.writeBatchToFile(HDFSMetadataLog.scala:131)\r\n\tat org.apache.spark.sql.execution.streaming.HDFSMetadataLog.$anonfun$add$3(HDFSMetadataLog.scala:120)\r\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.execution.streaming.HDFSMetadataLog.add(HDFSMetadataLog.scala:118)\r\n\tat org.apache.spark.sql.execution.streaming.CompactibleFileStreamLog.add(CompactibleFileStreamLog.scala:158)\r\n\tat org.apache.spark.sql.execution.streaming.FileStreamSourceLog.add(FileStreamSourceLog.scala:69)\r\n\tat org.apache.spark.sql.execution.streaming.FileStreamSource.fetchMaxOffset(FileStreamSource.scala:150)\r\n\tat org.apache.spark.sql.execution.streaming.FileStreamSource.latestOffset(FileStreamSource.scala:286)\r\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$3(MicroBatchExecution.scala:380)\r\n\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:352)\r\n\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:350)\r\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\r\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:371)\r\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\r\n\tat scala.collection.immutable.Map$Map1.foreach(Map.scala:128)\r\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\r\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\r\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\r\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:368)\r\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\r\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:598)\r\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:364)\r\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:208)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:352)\r\n\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:350)\r\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\r\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:191)\r\n\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:57)\r\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:185)\r\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:334)\r\n\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:245)\r\n20/08/28 11:41:36 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /private/var/folders/km/xypq2kxs4ys3dt6bwtd4fbj00000gn/T/temporary-3c27fa5e-7644-437d-9832-cac5168d4af5. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\r\n20/08/28 11:41:36 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /private/var/folders/km/xypq2kxs4ys3dt6bwtd4fbj00000gn/T/temporary-5d2c3db7-4d59-4e0b-953e-429a9d398ef2. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\r\n20/08/28 11:41:36 WARN Shell: Interrupted while joining on: Thread[Thread-110,5,]\r\njava.lang.InterruptedException\r\n\tat java.lang.Object.wait(Native Method)\r\n\tat java.lang.Thread.join(Thread.java:1252)\r\n\tat java.lang.Thread.join(Thread.java:1326)\r\n\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:629)\r\n\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:580)\r\n\tat org.apache.hadoop.util.Shell.run(Shell.java:482)\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:776)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:869)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:852)\r\n\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:160)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:837)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:826)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:797)\r\n\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileLinkStatus(DelegateToFileSystem.java:131)\r\n\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:717)\r\n\tat org.apache.hadoop.fs.FilterFs.renameInternal(FilterFs.java:240)\r\n\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:690)\r\n\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:958)\r\n\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:329)\r\n\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:147)\r\n\tat org.apache.spark.sql.execution.streaming.HDFSMetadataLog.writeBatchToFile(HDFSMetadataLog.scala:134)\r\n\tat org.apache.spark.sql.execution.streaming.HDFSMetadataLog.$anonfun$add$3(HDFSMetadataLog.scala:120)\r\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.execution.streaming.HDFSMetadataLog.add(HDFSMetadataLog.scala:118)\r\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$12(MicroBatchExecution.scala:418)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:352)\r\n\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:350)\r\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\r\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:416)\r\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\r\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:598)\r\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:364)\r\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:208)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:352)\r\n\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:350)\r\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\r\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:191)\r\n\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:57)\r\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:185)\r\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:334)\r\n\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:245)\r\n20/08/28 11:41:37 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /private/var/folders/km/xypq2kxs4ys3dt6bwtd4fbj00000gn/T/temporary-be1a0476-b88e-4b75-ac37-0486dbfe5a2e. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\r\nMacBook-Pro-van-Fokko:spark fokkodriesprong$ echo $?\r\n0\r\n```\r\n\r\nI was under the impression that the `py4j` and `sc` were included using the line above, that is was being pulled from some global context, but this isn't the case. It seems to run, but maybe it doesn't hit the `except`. It looks like some lingering test-code, maybe move this to the tests directory?\r\n\r\ncc @zsxwing @tdas ",
        "createdAt" : "2020-08-28T09:51:30Z",
        "updatedAt" : "2020-08-29T18:33:37Z",
        "lastEditedBy" : "e29a7794-ac2e-4e6f-a690-737e83e1bace",
        "tags" : [
        ]
      }
    ],
    "commit" : "06480a7b4e8c5106ac7f7ea0fa9cecd6ea09e0bb",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +1240,1244 @@    try:\n        spark = SparkSession.builder.getOrCreate()\n    except py4j.protocol.Py4JError:  # noqa: F821\n        spark = SparkSession(sc)  # noqa: F821\n"
  }
]