[
  {
    "id" : "13580018-00fc-431e-878a-187fe6938e86",
    "prId" : 32723,
    "prUrl" : "https://github.com/apache/spark/pull/32723#pullrequestreview-672661480",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b6bf0f35-a779-49d2-8991-c1853dbc832c",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I think we can remove `lowerBound`, `upperBound`, and `numPartitions`.\r\nAnd, fix the description of `column` to something like:\r\n\r\nAlias of `partitionColumn` option. Refer to `partitionColumn` in `Data Source Option <...>`_ in the version you use.",
        "createdAt" : "2021-06-01T05:20:58Z",
        "updatedAt" : "2021-06-01T05:20:58Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "bd91a48a90fe1647a45f4ccb1ab7ec395d2ea4eb",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +481,485 @@        table : str\n            the name of the table\n        column : str, optional\n            alias of ``partitionColumn`` option. Refer to ``partitionColumn`` in\n            `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html#data-source-option>`_"
  },
  {
    "id" : "6546b3a0-6bde-45fb-8957-5c2039b0da01",
    "prId" : 32546,
    "prUrl" : "https://github.com/apache/spark/pull/32546#pullrequestreview-665066114",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8deff871-698d-4265-8b66-9b76a661e6f8",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Ditto. Can we have a more robust link here?",
        "createdAt" : "2021-05-21T03:46:33Z",
        "updatedAt" : "2021-05-21T03:46:33Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "043d3087c838c2f6439d798bdc8e889d6a728ff7",
    "line" : 30,
    "diffHunk" : "@@ -1,1 +799,803 @@        Extra options\n            For the extra options, refer to\n            `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-orc.html#data-source-option>`_  # noqa\n            in the version you use.\n"
  },
  {
    "id" : "f845d483-300a-454f-929b-986cbaf1f057",
    "prId" : 32204,
    "prUrl" : "https://github.com/apache/spark/pull/32204#pullrequestreview-663976337",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d6e2dde7-848a-44f6-a357-73d55a8f0c63",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Here you didn't link generic options but you did for Parquet. What's diff?",
        "createdAt" : "2021-05-20T06:12:59Z",
        "updatedAt" : "2021-05-20T06:12:59Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "a10586c3d2887463de16984adb72d205f85f3796",
    "line" : 256,
    "diffHunk" : "@@ -1,1 +1038,1042 @@            For the extra options, refer to\n            `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-json.html#data-source-option>`_  # noqa\n            in the version you use.\n\n        Examples"
  },
  {
    "id" : "1c1594fb-fba9-43e1-bd7f-cf3ffdd92f53",
    "prId" : 32204,
    "prUrl" : "https://github.com/apache/spark/pull/32204#pullrequestreview-665013814",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "167674da-e009-4faa-8af7-d0a7f6515e31",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Shall we remove this too?",
        "createdAt" : "2021-05-21T01:07:28Z",
        "updatedAt" : "2021-05-21T01:07:28Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "a10586c3d2887463de16984adb72d205f85f3796",
    "line" : 175,
    "diffHunk" : "@@ -1,1 +197,201 @@            For the extra options, refer to\n            `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-json.html#data-source-option>`_  # noqa\n            in the version you use.\n\n        Examples"
  },
  {
    "id" : "204e8de3-e5bd-4122-be93-324d50064052",
    "prId" : 32161,
    "prUrl" : "https://github.com/apache/spark/pull/32161#pullrequestreview-660398853",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "99a1d1f1-9f71-4364-be21-d734a12c00b6",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I think you can say like:\r\n\r\n```\r\nFor the extra options, refer to \r\n`Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-parquet.html#data-source-option>`_  # noqa\r\nand\r\n`Generic File Source Options <https://spark.apache.org/docs/latest/sql-data-sources-generic-options.html`>_  # noqa\r\nin the version you use.\r\n```",
        "createdAt" : "2021-05-16T01:18:08Z",
        "updatedAt" : "2021-05-16T01:18:08Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "d6417a8124eb61390089313d108eff18fd89e412",
    "line" : 52,
    "diffHunk" : "@@ -1,1 +418,422 @@        ----------------\n        **options\n            For the extra options, refer to\n            `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-parquet.html#data-source-option>`_  # noqa\n            in the version you use."
  },
  {
    "id" : "dcf0d112-9dca-4cd1-a06e-383f29c9f9d2",
    "prId" : 28841,
    "prUrl" : "https://github.com/apache/spark/pull/28841#pullrequestreview-495506549",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "26491131-6d04-474e-8b3f-75d5a7597872",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Probably better not to change the order. I think such huge number of parameters end users will use named parameter almost every time, but just to be sure.",
        "createdAt" : "2020-09-24T12:15:57Z",
        "updatedAt" : "2020-11-05T10:34:16Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "bf2a66535c634ec68d667f3435a22220f38d29b5",
    "line" : 31,
    "diffHunk" : "@@ -1,1 +216,220 @@             multiLine=None, allowUnquotedControlChars=None, lineSep=None, samplingRatio=None,\n             dropFieldIfAllNull=None, encoding=None, locale=None, pathGlobFilter=None,\n             recursiveFileLookup=None, modifiedBefore=None, modifiedAfter=None,\n             allowNonNumericNumbers=None):\n        \"\"\""
  },
  {
    "id" : "ca6cc2c5-249b-457a-bb13-d7a5fe005712",
    "prId" : 28841,
    "prUrl" : "https://github.com/apache/spark/pull/28841#pullrequestreview-523329997",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dbb44689-1afb-4422-b06a-e962ea8e7748",
        "parentId" : null,
        "authorId" : "e0be6955-58e9-4e7f-8132-27fdb08564c9",
        "body" : "Net difference is addition of lines 325-330",
        "createdAt" : "2020-11-04T12:51:12Z",
        "updatedAt" : "2020-11-05T10:34:16Z",
        "lastEditedBy" : "e0be6955-58e9-4e7f-8132-27fdb08564c9",
        "tags" : [
        ]
      }
    ],
    "commit" : "bf2a66535c634ec68d667f3435a22220f38d29b5",
    "line" : 40,
    "diffHunk" : "@@ -1,1 +323,327 @@            It does not change the behavior of\n            `partition discovery <https://spark.apache.org/docs/latest/sql-data-sources-parquet.html#partition-discovery>`_.  # noqa\n        modifiedBefore : an optional timestamp to only include files with\n            modification times occurring before the specified time. The provided timestamp\n            must be in the following format: YYYY-MM-DDTHH:mm:ss (e.g. 2020-06-01T13:00:00)"
  },
  {
    "id" : "ebaa7a61-b8a0-4516-b93f-8def0529c26d",
    "prId" : 28841,
    "prUrl" : "https://github.com/apache/spark/pull/28841#pullrequestreview-523330391",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d703a751-c6b6-4a14-9780-8dd3a0a3adda",
        "parentId" : null,
        "authorId" : "e0be6955-58e9-4e7f-8132-27fdb08564c9",
        "body" : "Net difference is addition of lines 432-437",
        "createdAt" : "2020-11-04T12:51:46Z",
        "updatedAt" : "2020-11-05T10:34:16Z",
        "lastEditedBy" : "e0be6955-58e9-4e7f-8132-27fdb08564c9",
        "tags" : [
        ]
      }
    ],
    "commit" : "bf2a66535c634ec68d667f3435a22220f38d29b5",
    "line" : 68,
    "diffHunk" : "@@ -1,1 +438,442 @@        modifiedAfter : an optional timestamp to only include files with\n            modification times occurring after the specified time. The provided timestamp\n            must be in the following format: YYYY-MM-DDTHH:mm:ss (e.g. 2020-06-01T13:00:00)\n\n        Examples"
  },
  {
    "id" : "d34d77b2-5b62-4cad-8dfd-7eb354a76a2c",
    "prId" : 28841,
    "prUrl" : "https://github.com/apache/spark/pull/28841#pullrequestreview-523330822",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "87b66ed4-88f3-4cc8-acd9-f6fe38921324",
        "parentId" : null,
        "authorId" : "e0be6955-58e9-4e7f-8132-27fdb08564c9",
        "body" : "Net difference is addition of lines 484-489",
        "createdAt" : "2020-11-04T12:52:20Z",
        "updatedAt" : "2020-11-05T10:34:16Z",
        "lastEditedBy" : "e0be6955-58e9-4e7f-8132-27fdb08564c9",
        "tags" : [
        ]
      }
    ],
    "commit" : "bf2a66535c634ec68d667f3435a22220f38d29b5",
    "line" : 100,
    "diffHunk" : "@@ -1,1 +490,494 @@            modification times occurring before the specified time. The provided timestamp\n            must be in the following format: YYYY-MM-DDTHH:mm:ss (e.g. 2020-06-01T13:00:00)\n        modifiedBefore : an optional timestamp to only include files with\n            modification times occurring before the specified time. The provided timestamp\n            must be in the following format: YYYY-MM-DDTHH:mm:ss (e.g. 2020-06-01T13:00:00)"
  },
  {
    "id" : "0a5c4494-4476-47a5-ae89-ddfbf06d5f73",
    "prId" : 27553,
    "prUrl" : "https://github.com/apache/spark/pull/27553#pullrequestreview-358672293",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d0d11062-92f0-44c5-a272-ec9049c2afae",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Why is this `attr`? Technically this should be `meth`. Can you double check the generated documentation is correct? You can check via `cd python/docs && make clean html`",
        "createdAt" : "2020-02-13T01:07:19Z",
        "updatedAt" : "2020-02-13T01:07:19Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "b31205ef-a5d9-47ce-abab-1ae47a52b909",
        "parentId" : "d0d11062-92f0-44c5-a272-ec9049c2afae",
        "authorId" : "48393283-384a-40f1-96e3-dbfacb17fa96",
        "body" : "`DataFrame.write` is defined as a function decorated with `@property`, and as such behaves as an attribute, rather than a function. Indeed, one needs to write e.g. `df.write.saveAsTable('foo')` rather than `df.write().saveAsTable('foo')`; the latter will not work.\r\n\r\nChanging `func` to `attr` makes Sphinx render `DataFrame.write` correctly, instead of misleadingly rendering `DataFrame.write()` in the documentation.\r\n\r\nI've double-checked the resulting documentation, looks fine to me.\r\n\r\nEdit: Thanks for reviewing! :)",
        "createdAt" : "2020-02-13T20:02:43Z",
        "updatedAt" : "2020-02-13T20:14:45Z",
        "lastEditedBy" : "48393283-384a-40f1-96e3-dbfacb17fa96",
        "tags" : [
        ]
      },
      {
        "id" : "925e6a93-acbe-4fba-b1b3-1241f48fb709",
        "parentId" : "d0d11062-92f0-44c5-a272-ec9049c2afae",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Okay.",
        "createdAt" : "2020-02-14T01:59:48Z",
        "updatedAt" : "2020-02-14T01:59:48Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "6d0f6edbcadc83092eb0e9e7000de1f133fe8fa1",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +617,621 @@    \"\"\"\n    Interface used to write a :class:`DataFrame` to external storage systems\n    (e.g. file systems, key-value stores, etc). Use :attr:`DataFrame.write`\n    to access this.\n"
  },
  {
    "id" : "596f61d6-17ad-4d57-93d1-13c57f76521b",
    "prId" : 27331,
    "prUrl" : "https://github.com/apache/spark/pull/27331#pullrequestreview-436970377",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "36c3f65c-5f23-49f4-bc51-2d22ecdc6529",
        "parentId" : null,
        "authorId" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "body" : "I think we're missing table properties?",
        "createdAt" : "2020-06-24T20:10:36Z",
        "updatedAt" : "2020-07-19T15:47:59Z",
        "lastEditedBy" : "471c3dfe-259c-447a-9abd-f7fdecccd9a7",
        "tags" : [
        ]
      }
    ],
    "commit" : "9197c8411e6484b9df2b8626a6a77a5af18d89b9",
    "line" : 36,
    "diffHunk" : "@@ -1,1 +1099,1103 @@\n    @since(3.1)\n    def option(self, key, value):\n        \"\"\"\n        Add a write option."
  },
  {
    "id" : "ed38972d-1dfc-4e1a-85cc-e0d943bbfb15",
    "prId" : 27331,
    "prUrl" : "https://github.com/apache/spark/pull/27331#pullrequestreview-437953012",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "46bde515-a2b5-455c-ab22-7784395f9eb6",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Maybe it's important to describe what are expected for `col`. Only columns and the partition transform functions are allowed, not the regular Spark Column.\r\n\r\nI still don't like it we made this API looks like it takes regular Spark Columns - they are mutually exclusive given the last discussion in the dev mailing list, this was one of the reason why Pandas UDFs were redesigned and separate into two separate groups .. let's at least clarify it.",
        "createdAt" : "2020-06-25T02:10:20Z",
        "updatedAt" : "2020-07-19T15:47:59Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "bac40ce8-5002-446d-ba68-ff4966c012bc",
        "parentId" : "46bde515-a2b5-455c-ab22-7784395f9eb6",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@rdblue, @brkyvz, @cloud-fan, Should we maybe at least use a different class for these partition column expressions such as `PartitioningColumn` like we do for `TypedColumn`, and add `asPartitioningColumn` to `Column`?\r\n\r\nI remember we basically want to remove these partitioning specific expressions at [[DISCUSS] Revert and revisit the public custom expression API for partition (a.k.a. Transform API)\r\n](http://apache-spark-developers-list.1001551.n3.nabble.com/DISCUSS-Revert-and-revisit-the-public-custom-expression-API-for-partition-a-k-a-Transform-API-td28683.html) if we find a better way to do it.\r\n\r\nI suspect doing `PartitioningColumn ` is a-okay as a temporary solution (?) because we can guard it by typing, and we can move these partitioning-specific expressions into a separate package. I think this way make them distinguished at least. I can work as well on it if this sounds fine to you guys.",
        "createdAt" : "2020-06-25T12:14:22Z",
        "updatedAt" : "2020-07-19T15:47:59Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "2bc1bcc0-640b-4f36-99a2-a65cde43a611",
        "parentId" : "46bde515-a2b5-455c-ab22-7784395f9eb6",
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "I don't see the need for separation here that doesn't exist in Scala.",
        "createdAt" : "2020-06-25T20:49:01Z",
        "updatedAt" : "2020-07-19T15:47:59Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      },
      {
        "id" : "1c929886-6512-494c-99c9-6e0e8edae4f7",
        "parentId" : "46bde515-a2b5-455c-ab22-7784395f9eb6",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@rdblue, I don't mean to we should do that here. I mean to suggest/discuss to make the separation in the Scala first because that propagates the confusion to PySpark API side as well.\r\n\r\nThey are different things so I am suggesting to make it different. I hope we can more focus on the discussion itself.",
        "createdAt" : "2020-06-26T01:02:33Z",
        "updatedAt" : "2020-07-19T15:47:59Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "9197c8411e6484b9df2b8626a6a77a5af18d89b9",
    "line" : 61,
    "diffHunk" : "@@ -1,1 +1124,1128 @@\n    @since(3.1)\n    def partitionedBy(self, col, *cols):\n        \"\"\"\n        Partition the output table created by `create`, `createOrReplace`, or `replace` using"
  },
  {
    "id" : "c718813e-78b3-422b-a89a-47cea0d1a973",
    "prId" : 27331,
    "prUrl" : "https://github.com/apache/spark/pull/27331#pullrequestreview-451142901",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1d216c1f-6611-47b2-984e-fa2ab827238d",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "shall we move it to a new file?",
        "createdAt" : "2020-06-29T08:07:37Z",
        "updatedAt" : "2020-07-19T15:47:59Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "b40b8d90-9f0a-4d48-bce4-43987663cc3c",
        "parentId" : "1d216c1f-6611-47b2-984e-fa2ab827238d",
        "authorId" : "981b170c-729a-429c-b115-0350ea50b32b",
        "body" : "If you think that's better approach. I don't have strong opinion, though feature is small and unlikely to be used directly.",
        "createdAt" : "2020-07-19T15:49:47Z",
        "updatedAt" : "2020-07-19T15:49:47Z",
        "lastEditedBy" : "981b170c-729a-429c-b115-0350ea50b32b",
        "tags" : [
        ]
      }
    ],
    "commit" : "9197c8411e6484b9df2b8626a6a77a5af18d89b9",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +1076,1080 @@\n\nclass DataFrameWriterV2(object):\n    \"\"\"\n    Interface used to write a class:`pyspark.sql.dataframe.DataFrame`"
  },
  {
    "id" : "d3ec9292-3c9e-4d0d-99c5-d51b017e336a",
    "prId" : 26958,
    "prUrl" : "https://github.com/apache/spark/pull/26958#pullrequestreview-335580091",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cd47febb-3541-4e9c-b410-ff07b7dc4a93",
        "parentId" : null,
        "authorId" : "fd6ebc48-7da4-490b-8d41-2e8530d92720",
        "body" : "Style nit: This does make the indentation more consistent with the rest of the file, but I wish we could start moving the indentation style to fit the pre-change indentation.\r\n\r\nRight now these doc strings waste a lot of leading whitespace and have wildly varying indentation that depends on the length of the parameter name. It would be better if we just indented continuing lines with a consistent 4 spaces, like how `mergeSchema`'s docstring looks.",
        "createdAt" : "2019-12-20T04:34:42Z",
        "updatedAt" : "2019-12-20T05:37:24Z",
        "lastEditedBy" : "fd6ebc48-7da4-490b-8d41-2e8530d92720",
        "tags" : [
        ]
      },
      {
        "id" : "e781668f-8a22-48ba-9840-cd3a4f1e4003",
        "parentId" : "cd47febb-3541-4e9c-b410-ff07b7dc4a93",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Yeah, valid point. Let fix them consistent for now, and maybe we can handle them in a batch separately.",
        "createdAt" : "2019-12-20T05:11:07Z",
        "updatedAt" : "2019-12-20T05:37:24Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "6fc283b4-7873-42c6-b2e7-6950f8584efd",
        "parentId" : "cd47febb-3541-4e9c-b410-ff07b7dc4a93",
        "authorId" : "fd6ebc48-7da4-490b-8d41-2e8530d92720",
        "body" : "Would you be open to a batch update of all docstrings to tweak the indentation? Or would it be considered as adding too much noise to the git history? I assumed we didn't want to make sweeping changes like that, so my idea was to update the indentation bit by bit as part of more significant changes, which is what I did with `recursiveFileLookup` and `mergeSchema` when I added those in.",
        "createdAt" : "2019-12-20T16:03:57Z",
        "updatedAt" : "2019-12-20T16:03:58Z",
        "lastEditedBy" : "fd6ebc48-7da4-490b-8d41-2e8530d92720",
        "tags" : [
        ]
      },
      {
        "id" : "5cde3cac-9b22-49a4-897b-a8510c747c62",
        "parentId" : "cd47febb-3541-4e9c-b410-ff07b7dc4a93",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Since we're going ahead for Spark 3, we wont likely backport many things that cause conflicts. So I was thinking it's a feasible option.\r\n\r\nAlso, I think we might have to document this first that virtical alignment isn't preferred. I think virtical alignment is still valid per PEP 8 and PEP 257.\r\n\r\nOne downside of doing bit by bit is a confusion by mixed style. Considering that we wont likely add many new docstrings, mixed style exists in a long term.\r\n\r\n",
        "createdAt" : "2019-12-21T01:19:36Z",
        "updatedAt" : "2019-12-21T01:19:36Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "49adc3ac-fc38-4d13-9026-a0023fb2ff33",
        "parentId" : "cd47febb-3541-4e9c-b410-ff07b7dc4a93",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "So .. if you dont mind, I would like this run that separately :-)..",
        "createdAt" : "2019-12-21T01:19:54Z",
        "updatedAt" : "2019-12-21T01:19:54Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "dde76ab5360a3fb8f3920916f267a7095edc2126",
    "line" : 129,
    "diffHunk" : "@@ -1,1 +549,553 @@                               It does not change the behavior of `partition discovery`_.\n        :param recursiveFileLookup: recursively scan a directory for files. Using this option\n                                    disables `partition discovery`_.\n\n        >>> df = spark.read.orc('python/test_support/sql/orc_partitioned')"
  },
  {
    "id" : "cf0696b3-6723-4d96-bacb-82ae2bb7ed56",
    "prId" : 26718,
    "prUrl" : "https://github.com/apache/spark/pull/26718#pullrequestreview-324876533",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2e3fb547-ac92-45a1-aa13-5bc57b3eea58",
        "parentId" : null,
        "authorId" : "fd6ebc48-7da4-490b-8d41-2e8530d92720",
        "body" : "To support Python 2, we need to say `**options` instead of `recursiveFileLookup=None` because [Python 2 doesn't support keyword-only arguments](https://stackoverflow.com/q/22435992/877069).",
        "createdAt" : "2019-11-29T22:41:10Z",
        "updatedAt" : "2019-11-30T18:04:29Z",
        "lastEditedBy" : "fd6ebc48-7da4-490b-8d41-2e8530d92720",
        "tags" : [
        ]
      },
      {
        "id" : "944e077f-e386-402b-ab23-68aebcd39f5c",
        "parentId" : "2e3fb547-ac92-45a1-aa13-5bc57b3eea58",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "seems fine.",
        "createdAt" : "2019-11-30T04:17:57Z",
        "updatedAt" : "2019-11-30T18:04:29Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "72f33da4d17e9e48336fcf2cb2f98fa5e6138484",
    "line" : 34,
    "diffHunk" : "@@ -1,1 +305,309 @@\n    @since(1.4)\n    def parquet(self, *paths, **options):\n        \"\"\"Loads Parquet files, returning the result as a :class:`DataFrame`.\n"
  },
  {
    "id" : "37997e4f-db2b-4791-abe3-1e2685727731",
    "prId" : 26227,
    "prUrl" : "https://github.com/apache/spark/pull/26227#pullrequestreview-306299588",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "07639b77-cfd9-41c7-a436-6568b12edd87",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "This seems to be beyond `add doc`. Could you update the PR title, @stczwd ?",
        "createdAt" : "2019-10-24T02:32:17Z",
        "updatedAt" : "2019-10-24T08:40:08Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "7d06bfdf-a144-4fe1-b3bb-30d36d54d6f7",
        "parentId" : "07639b77-cfd9-41c7-a436-6568b12edd87",
        "authorId" : "0f4ef4e8-09be-436f-b743-bf8fbc343490",
        "body" : "Okey",
        "createdAt" : "2019-10-24T03:17:23Z",
        "updatedAt" : "2019-10-24T08:40:08Z",
        "lastEditedBy" : "0f4ef4e8-09be-436f-b743-bf8fbc343490",
        "tags" : [
        ]
      }
    ],
    "commit" : "40bb5151015191d975a141daaedf4bc1afcd0486",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +789,793 @@    @since(1.4)\n    def json(self, path, mode=None, compression=None, dateFormat=None, timestampFormat=None,\n             lineSep=None, encoding=None, ignoreNullFields=None):\n        \"\"\"Saves the content of the :class:`DataFrame` in JSON format\n        (`JSON Lines text format or newline-delimited JSON <http://jsonlines.org/>`_) at the"
  }
]