[
  {
    "id" : "636d9a05-17a8-4fac-86a7-e2c8b3412b61",
    "prId" : 25939,
    "prUrl" : "https://github.com/apache/spark/pull/25939#pullrequestreview-293563046",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a67b44ea-3080-4597-bfb6-a4081af13fe6",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "indentation nit",
        "createdAt" : "2019-09-26T08:31:46Z",
        "updatedAt" : "2019-09-29T20:32:31Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "4cdb2fa27d51f56808d5d929ce98310451f73550",
    "line" : 58,
    "diffHunk" : "@@ -1,1 +92,96 @@        ... def asof_join(k, l, r):\n        ...     if k == (1,):\n        ...         return pd.merge_asof(l, r, on=\"time\", by=\"id\")\n        ...     else:\n        ...         return pd.DataFrame(columns=['time', 'id', 'v1', 'v2'])"
  },
  {
    "id" : "80bb8ec9-1889-48fe-b995-0d19270b393f",
    "prId" : 24981,
    "prUrl" : "https://github.com/apache/spark/pull/24981#pullrequestreview-260947083",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0a4ca367-5fcb-4ea6-9c19-17e3b3726ed9",
        "parentId" : null,
        "authorId" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "body" : "should this import be at the top?",
        "createdAt" : "2019-07-10T21:52:35Z",
        "updatedAt" : "2019-09-15T08:33:31Z",
        "lastEditedBy" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "tags" : [
        ]
      },
      {
        "id" : "9a45a692-1041-4a01-9e96-cf96df1c927f",
        "parentId" : "0a4ca367-5fcb-4ea6-9c19-17e3b3726ed9",
        "authorId" : "627d34e9-eba3-4504-aa67-ffc2f967c413",
        "body" : "yes it should!",
        "createdAt" : "2019-07-11T20:50:40Z",
        "updatedAt" : "2019-09-15T08:33:31Z",
        "lastEditedBy" : "627d34e9-eba3-4504-aa67-ffc2f967c413",
        "tags" : [
        ]
      }
    ],
    "commit" : "1b966fda46c5334cf7963bae0bece159c9568622",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +17,21 @@\nfrom pyspark import since\nfrom pyspark.rdd import PythonEvalType\nfrom pyspark.sql.column import Column\nfrom pyspark.sql.dataframe import DataFrame"
  },
  {
    "id" : "83525a88-7e6e-4194-ad26-abfdef64cb4d",
    "prId" : 24981,
    "prUrl" : "https://github.com/apache/spark/pull/24981#pullrequestreview-260947160",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2c41f99f-3e80-4dfd-a871-f083580d57d0",
        "parentId" : null,
        "authorId" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "body" : "please add pydocs",
        "createdAt" : "2019-07-10T21:56:24Z",
        "updatedAt" : "2019-09-15T08:33:31Z",
        "lastEditedBy" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "tags" : [
        ]
      },
      {
        "id" : "a1f1a07b-3bee-461f-beaf-339d5434de8d",
        "parentId" : "2c41f99f-3e80-4dfd-a871-f083580d57d0",
        "authorId" : "627d34e9-eba3-4504-aa67-ffc2f967c413",
        "body" : "done",
        "createdAt" : "2019-07-11T20:50:49Z",
        "updatedAt" : "2019-09-15T08:33:31Z",
        "lastEditedBy" : "627d34e9-eba3-4504-aa67-ffc2f967c413",
        "tags" : [
        ]
      }
    ],
    "commit" : "1b966fda46c5334cf7963bae0bece159c9568622",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +22,26 @@\n\nclass CoGroupedData(object):\n    \"\"\"\n    A logical grouping of two :class:`GroupedData`,"
  },
  {
    "id" : "76263e75-9975-408e-a1ce-e974acda1bf3",
    "prId" : 24981,
    "prUrl" : "https://github.com/apache/spark/pull/24981#pullrequestreview-268602580",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f8bab495-6caa-4315-b7bf-848043c26ddf",
        "parentId" : null,
        "authorId" : "46f89797-5e90-4b09-9cd5-3af0cfeca74a",
        "body" : "Probably want \\`\\`pandas.DataFrame\\`\\` (two backticks) for correct reStructuredText rendering).",
        "createdAt" : "2019-07-30T18:59:16Z",
        "updatedAt" : "2019-09-15T08:33:31Z",
        "lastEditedBy" : "46f89797-5e90-4b09-9cd5-3af0cfeca74a",
        "tags" : [
        ]
      }
    ],
    "commit" : "1b966fda46c5334cf7963bae0bece159c9568622",
    "line" : 46,
    "diffHunk" : "@@ -1,1 +44,48 @@\n        The user-defined function should take two `pandas.DataFrame` and return another\n        `pandas.DataFrame`. For each side of the cogroup, all columns are passed together\n        as a `pandas.DataFrame` to the user-function and the returned `pandas.DataFrame`\n        are combined as a :class:`DataFrame`."
  },
  {
    "id" : "2ca6c5a4-de89-497f-91c2-445b6ddd7ec5",
    "prId" : 24981,
    "prUrl" : "https://github.com/apache/spark/pull/24981#pullrequestreview-291521925",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1ab63472-fa43-44f7-9a68-7b94e1bfc6d8",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "We should skip this test and run the doctests\r\n\r\n1. add this to `dev/sparktestsupport/modules.py` at `pyspark_sql`\r\n2. add:\r\n\r\n    ```python\r\n    def main():\r\n        doctest.testmod(...)\r\n        ...\r\n    ```",
        "createdAt" : "2019-09-22T08:49:02Z",
        "updatedAt" : "2019-09-22T08:58:24Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "d838b590-6081-4f41-8a69-9c4efba9cdee",
        "parentId" : "1ab63472-fa43-44f7-9a68-7b94e1bfc6d8",
        "authorId" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "body" : "We are currently skipping all doctests for Pandas UDFs right? We could add the module but then need to skip each test individually, which might be more consistent with the rest of PySpark.",
        "createdAt" : "2019-09-22T18:48:18Z",
        "updatedAt" : "2019-09-22T18:48:18Z",
        "lastEditedBy" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "tags" : [
        ]
      }
    ],
    "commit" : "1b966fda46c5334cf7963bae0bece159c9568622",
    "line" : 69,
    "diffHunk" : "@@ -1,1 +67,71 @@        ... [(20000101, 1, \"x\"), (20000101, 2, \"y\")],\n        ... (\"time\", \"id\", \"v2\"))\n        >>> @pandas_udf(\"time int, id int, v1 double, v2 string\", PandasUDFType.COGROUPED_MAP)\n        ... def asof_join(l, r):\n        ...     return pd.merge_asof(l, r, on=\"time\", by=\"id\")"
  },
  {
    "id" : "d16bcf37-a0cd-4f82-974a-fc76ae2f3b88",
    "prId" : 24981,
    "prUrl" : "https://github.com/apache/spark/pull/24981#pullrequestreview-291496607",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8d3f6583-3e1b-47b0-8f4e-85f61c5f2495",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "indentation nit",
        "createdAt" : "2019-09-22T08:49:08Z",
        "updatedAt" : "2019-09-22T08:58:24Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "1b966fda46c5334cf7963bae0bece159c9568622",
    "line" : 68,
    "diffHunk" : "@@ -1,1 +66,70 @@        >>> df2 = spark.createDataFrame(\n        ... [(20000101, 1, \"x\"), (20000101, 2, \"y\")],\n        ... (\"time\", \"id\", \"v2\"))\n        >>> @pandas_udf(\"time int, id int, v1 double, v2 string\", PandasUDFType.COGROUPED_MAP)\n        ... def asof_join(l, r):"
  },
  {
    "id" : "59cc222f-63b6-4344-87e1-1f46fe13d1ad",
    "prId" : 24981,
    "prUrl" : "https://github.com/apache/spark/pull/24981#pullrequestreview-291496607",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "af4c0c99-6d07-4d3a-b55c-ccbd4bade829",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Can we document when arguments are three? (when it includes the grouping key)",
        "createdAt" : "2019-09-22T08:56:02Z",
        "updatedAt" : "2019-09-22T08:58:24Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "1b966fda46c5334cf7963bae0bece159c9568622",
    "line" : 69,
    "diffHunk" : "@@ -1,1 +67,71 @@        ... [(20000101, 1, \"x\"), (20000101, 2, \"y\")],\n        ... (\"time\", \"id\", \"v2\"))\n        >>> @pandas_udf(\"time int, id int, v1 double, v2 string\", PandasUDFType.COGROUPED_MAP)\n        ... def asof_join(l, r):\n        ...     return pd.merge_asof(l, r, on=\"time\", by=\"id\")"
  },
  {
    "id" : "ad0cc4ad-8e82-4067-a380-983999e50269",
    "prId" : 24981,
    "prUrl" : "https://github.com/apache/spark/pull/24981#pullrequestreview-291522438",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "538a6802-c31c-4916-ad49-0082e865fcb4",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Seems like we don't generate documentation for this:\r\n\r\n![Screen Shot 2019-09-22 at 9 41 48 PM](https://user-images.githubusercontent.com/6477701/65387377-d96f9900-dd81-11e9-928f-de9929571355.png)\r\n\r\ncannot click.\r\n\r\nIt should be either documented at `python/docs/pyspark.sql.rst` or imported at `pyspark/sql/__init__.py` with including it at `__all__`.",
        "createdAt" : "2019-09-22T12:43:49Z",
        "updatedAt" : "2019-09-22T12:43:49Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "c458bbd7-fdbe-486e-90cd-be0d0e3a8132",
        "parentId" : "538a6802-c31c-4916-ad49-0082e865fcb4",
        "authorId" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "body" : "+1 adding it to `pyspark/sql/__init__.py` with including it at `__all__` since this is what `group.py` does",
        "createdAt" : "2019-09-22T19:05:00Z",
        "updatedAt" : "2019-09-22T19:05:00Z",
        "lastEditedBy" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "tags" : [
        ]
      }
    ],
    "commit" : "1b966fda46c5334cf7963bae0bece159c9568622",
    "line" : 1,
    "diffHunk" : "@@ -1,1 +-1,3 @@#\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with"
  }
]