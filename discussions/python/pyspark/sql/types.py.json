[
  {
    "id" : "eb24461f-04d4-4d9f-80f2-bf5edc9da2b6",
    "prId" : 33214,
    "prUrl" : "https://github.com/apache/spark/pull/33214#pullrequestreview-700519288",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bbe08729-7044-4bc1-99b3-f7de6cc3b20f",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Do we need to log warning if inferred value types are not inconsistent? We can recommend users to use the config.",
        "createdAt" : "2021-07-07T01:11:35Z",
        "updatedAt" : "2021-07-07T01:12:16Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "7b233d5e-2ce5-4f6a-b70a-92bb63e97531",
        "parentId" : "bbe08729-7044-4bc1-99b3-f7de6cc3b20f",
        "authorId" : "2d8ced12-ada1-43e1-9240-bf1c11a01e4c",
        "body" : "Thanks for the comment! :)\r\nActually PySpark merging one only handles null cases only (that's called out here) at \r\nhttps://github.com/apache/spark/blob/52a9a70fa3e5b720b41e2ff4e9177a5d201b471f/python/pyspark/sql/types.py#L1096-L1133\r\n\r\nIt actually fails for different types (unlike JSON or CSV type inference).\r\nI am not sure what's the ideal behavior for the null case pointed out here though.\r\nLet me separate it from this PR in any event if you're fine.",
        "createdAt" : "2021-07-07T02:20:47Z",
        "updatedAt" : "2021-07-07T02:20:47Z",
        "lastEditedBy" : "2d8ced12-ada1-43e1-9240-bf1c11a01e4c",
        "tags" : [
        ]
      }
    ],
    "commit" : "52a9a70fa3e5b720b41e2ff4e9177a5d201b471f",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +1032,1036 @@                    return MapType(_infer_type(key, infer_dict_as_struct),\n                                   _infer_type(value, infer_dict_as_struct), True)\n            return MapType(NullType(), NullType(), True)\n    elif isinstance(obj, list):\n        for v in obj:"
  },
  {
    "id" : "c17f186a-eb13-42e7-a780-3ebe49acd516",
    "prId" : 29935,
    "prUrl" : "https://github.com/apache/spark/pull/29935#pullrequestreview-573860703",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c444cd6c-4b48-48b0-a6fd-e7ae63a9a85d",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "There have been a lot of discussions about exposing interval type in other language APIs but I lost the track. @yaooqinn and @cloud-fan, are we going to make internal as a proper exposed type? Or only support it in some contexts?",
        "createdAt" : "2020-10-03T04:06:34Z",
        "updatedAt" : "2020-12-01T11:47:10Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "c57e9793-f2a9-4f83-a94d-e43cb72da85b",
        "parentId" : "c444cd6c-4b48-48b0-a6fd-e7ae63a9a85d",
        "authorId" : "981b170c-729a-429c-b115-0350ea50b32b",
        "body" : "Yes, I glanced over a few threads and couldn't really figure out where it is going, hence a limited scope of this PR. However, if the type is supported in multiple contexts, current behavior doesn't seem like an intended one.",
        "createdAt" : "2020-10-03T06:01:58Z",
        "updatedAt" : "2020-12-01T11:47:10Z",
        "lastEditedBy" : "981b170c-729a-429c-b115-0350ea50b32b",
        "tags" : [
        ]
      },
      {
        "id" : "a79ab2cb-8936-44bb-9953-55042e8d8f19",
        "parentId" : "c444cd6c-4b48-48b0-a6fd-e7ae63a9a85d",
        "authorId" : "fd6ebc48-7da4-490b-8d41-2e8530d92720",
        "body" : "Doesn't @zero323's example from the PR description show that Spark already exposes this type? \r\n\r\n```python\r\nspark.sql(\"SELECT current_date() - current_date()\")\r\n```\r\n\r\nFor the record, btw, Postgres supports [an `interval` type](https://www.postgresql.org/docs/current/datatype-datetime.html) and done so since at least [version 7.1](https://www.postgresql.org/docs/7.1/datatype-datetime.html), which was released in 2001. (I mention this since Postgres often comes up as a reference for whether Spark SQL should support a feature or not.)",
        "createdAt" : "2021-01-21T20:36:30Z",
        "updatedAt" : "2021-01-21T20:40:34Z",
        "lastEditedBy" : "fd6ebc48-7da4-490b-8d41-2e8530d92720",
        "tags" : [
        ]
      },
      {
        "id" : "cd7f48b0-51e4-4384-bca4-1ab78f2ff1e0",
        "parentId" : "c444cd6c-4b48-48b0-a6fd-e7ae63a9a85d",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "The problem is that it has been half-exposed so far. There have been many discussions up to which context we should support. e.g.) `CalendarInterval` is marked as `Unstable`.",
        "createdAt" : "2021-01-22T01:03:19Z",
        "updatedAt" : "2021-01-22T01:03:19Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "c61d5fcb-79e0-4426-bc14-939c34a03e82",
        "parentId" : "c444cd6c-4b48-48b0-a6fd-e7ae63a9a85d",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "BTW, I personally agree with adding this in particular due to @nchammas point https://github.com/apache/spark/pull/29935#discussion_r562175621 here",
        "createdAt" : "2021-01-22T01:04:56Z",
        "updatedAt" : "2021-01-22T01:04:56Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "69438b418aafa76d6d23f6ffbb146ac2b6e10019",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +187,191 @@\n\nclass CalendarIntervalType(DataType, metaclass=DataTypeSingleton):\n    \"\"\"Calendar Interval type\n    \"\"\""
  },
  {
    "id" : "556e1fb6-491f-4214-8001-92d181710580",
    "prId" : 29935,
    "prUrl" : "https://github.com/apache/spark/pull/29935#pullrequestreview-573760710",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2f00a6df-33eb-4379-9a14-46d2f3f8c00f",
        "parentId" : null,
        "authorId" : "fd6ebc48-7da4-490b-8d41-2e8530d92720",
        "body" : "I suppose in the future if we want to support conversion of Python's [`datetime.timedelta`](https://docs.python.org/3/library/datetime.html#datetime.timedelta), it would happen here, right?",
        "createdAt" : "2021-01-21T20:27:07Z",
        "updatedAt" : "2021-01-21T20:38:42Z",
        "lastEditedBy" : "fd6ebc48-7da4-490b-8d41-2e8530d92720",
        "tags" : [
        ]
      },
      {
        "id" : "261f589c-94ea-4343-ac16-9b73d227511d",
        "parentId" : "2f00a6df-33eb-4379-9a14-46d2f3f8c00f",
        "authorId" : "981b170c-729a-429c-b115-0350ea50b32b",
        "body" : "For full support we might need both Python and JVM component. If I recall correctly `timedelta` has razrovine mapping to their internal `net.razorvine.pickle.objects.TimeDelta`.\r\n\r\nIn the opposite direction we could, if I am not mistaken, start with making `CalendarInterval` bean compatible, but there is compatibility issue â€’ we'd have to map from Spark's months to Python's days.\r\n\r\n",
        "createdAt" : "2021-01-21T21:43:43Z",
        "updatedAt" : "2021-01-21T21:43:44Z",
        "lastEditedBy" : "981b170c-729a-429c-b115-0350ea50b32b",
        "tags" : [
        ]
      }
    ],
    "commit" : "69438b418aafa76d6d23f6ffbb146ac2b6e10019",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +203,207 @@        raise NotImplementedError(\n            \"Conversion from external Python types to interval not supported\"\n        )\n\n    def fromInternal(self, v):"
  }
]