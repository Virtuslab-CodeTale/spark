[
  {
    "id" : "886804c1-59f5-47f3-9b7e-bd296c4beba9",
    "prId" : 24981,
    "prUrl" : "https://github.com/apache/spark/pull/24981#pullrequestreview-260369861",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1df3fa1d-bf2a-4176-a4e8-151cdf009e24",
        "parentId" : null,
        "authorId" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "body" : "I don't think you need a newline here?",
        "createdAt" : "2019-07-10T22:02:02Z",
        "updatedAt" : "2019-09-15T08:33:31Z",
        "lastEditedBy" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "tags" : [
        ]
      }
    ],
    "commit" : "1b966fda46c5334cf7963bae0bece159c9568622",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +18,22 @@import unittest\nimport sys\n\nfrom pyspark.sql.functions import array, explode, col, lit, udf, sum, pandas_udf, PandasUDFType\nfrom pyspark.sql.types import DoubleType, StructType, StructField"
  },
  {
    "id" : "6c8841ce-e351-4219-b580-8d7a58baf47f",
    "prId" : 24981,
    "prUrl" : "https://github.com/apache/spark/pull/24981#pullrequestreview-260948825",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "64d5b502-23f2-491c-b653-c9d599c1f25e",
        "parentId" : null,
        "authorId" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "body" : "So is this filtering out half of the groups in the left?",
        "createdAt" : "2019-07-10T22:16:00Z",
        "updatedAt" : "2019-09-15T08:33:31Z",
        "lastEditedBy" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "tags" : [
        ]
      },
      {
        "id" : "6937d44f-355a-461d-843e-d98231ab5170",
        "parentId" : "64d5b502-23f2-491c-b653-c9d599c1f25e",
        "authorId" : "627d34e9-eba3-4504-aa67-ffc2f967c413",
        "body" : "yes, by default `self.data1` and `self.data2` are set up so that all cogroups will have data present on both the left and right sides.  Some of these tests filter out half the data on one side so we can test that everything works correctly if we have data on one side of the cogroup but not the other.",
        "createdAt" : "2019-07-11T20:54:00Z",
        "updatedAt" : "2019-09-15T08:33:31Z",
        "lastEditedBy" : "627d34e9-eba3-4504-aa67-ffc2f967c413",
        "tags" : [
        ]
      }
    ],
    "commit" : "1b966fda46c5334cf7963bae0bece159c9568622",
    "line" : 162,
    "diffHunk" : "@@ -1,1 +160,164 @@\n    def test_with_key_left_group_empty(self):\n        left = self.data1.where(col(\"id\") % 2 == 0)\n        self._test_with_key(left, self.data1, isLeft=True)\n"
  },
  {
    "id" : "d16b96fe-8644-4a95-a364-46702cd55f7a",
    "prId" : 24981,
    "prUrl" : "https://github.com/apache/spark/pull/24981#pullrequestreview-280435754",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b102de6c-c1fc-4131-bfa1-538469a35a85",
        "parentId" : null,
        "authorId" : "43998e22-6c2f-401d-9914-8cecf6fad929",
        "body" : "Could you reorganize imports? Seems like there are unused imports.",
        "createdAt" : "2019-08-27T20:11:14Z",
        "updatedAt" : "2019-09-15T08:33:31Z",
        "lastEditedBy" : "43998e22-6c2f-401d-9914-8cecf6fad929",
        "tags" : [
        ]
      }
    ],
    "commit" : "1b966fda46c5334cf7963bae0bece159c9568622",
    "line" : 32,
    "diffHunk" : "@@ -1,1 +30,34 @@\nif have_pyarrow:\n    import pyarrow as pa\n\n"
  },
  {
    "id" : "039f0a2a-fa84-4b89-9117-9335c8ce5400",
    "prId" : 24981,
    "prUrl" : "https://github.com/apache/spark/pull/24981#pullrequestreview-291496607",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6c19d8b6-998d-43c9-a7e4-516657354d58",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I think we should leave this as a comment, not a string. Since it's not the top of the module currently, it's not docstring either.",
        "createdAt" : "2019-09-22T08:51:13Z",
        "updatedAt" : "2019-09-22T08:58:24Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "1b966fda46c5334cf7963bae0bece159c9568622",
    "line" : 37,
    "diffHunk" : "@@ -1,1 +35,39 @@\"\"\"\nTests below use pd.DataFrame.assign that will infer mixed types (unicode/str) for column names\nfrom kwargs w/ Python 2, so need to set check_column_type=False and avoid this check\n\"\"\"\nif sys.version < '3':"
  },
  {
    "id" : "c171b963-1f49-4cc4-9398-0b69e82f294b",
    "prId" : 24981,
    "prUrl" : "https://github.com/apache/spark/pull/24981#pullrequestreview-291496607",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f29c3efa-579b-4d8a-b456-a606226c6a23",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "nit\r\n\r\n```python\r\n_check_column_type = sys.version >= '3'\r\n```",
        "createdAt" : "2019-09-22T08:51:51Z",
        "updatedAt" : "2019-09-22T08:58:24Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "1b966fda46c5334cf7963bae0bece159c9568622",
    "line" : 40,
    "diffHunk" : "@@ -1,1 +38,42 @@\"\"\"\nif sys.version < '3':\n    _check_column_type = False\nelse:\n    _check_column_type = True"
  },
  {
    "id" : "cd33812c-6e62-4e2b-9c03-89c8928bcd87",
    "prId" : 24981,
    "prUrl" : "https://github.com/apache/spark/pull/24981#pullrequestreview-291506350",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d05795f0-1049-4249-9091-4cef1a571e56",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "This file should be added into `dev/sparktestsupport/modules.py`; otherwise, we don't actually run the tests ...",
        "createdAt" : "2019-09-22T08:53:50Z",
        "updatedAt" : "2019-09-22T08:58:24Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "a2059abd-5c87-4b58-8320-eb070969f34d",
        "parentId" : "d05795f0-1049-4249-9091-4cef1a571e56",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "addressed at https://github.com/apache/spark/pull/25890",
        "createdAt" : "2019-09-22T12:44:16Z",
        "updatedAt" : "2019-09-22T12:44:17Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "1b966fda46c5334cf7963bae0bece159c9568622",
    "line" : 272,
    "diffHunk" : "@@ -1,1 +270,274 @@\n\nif __name__ == \"__main__\":\n    from pyspark.sql.tests.test_pandas_udf_cogrouped_map import *\n"
  },
  {
    "id" : "36a442e4-a15c-4d5c-927d-9d3a8a95bfba",
    "prId" : 24965,
    "prUrl" : "https://github.com/apache/spark/pull/24965#pullrequestreview-256750648",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3e321d4a-25ab-4e3f-a30e-c54f2c26350b",
        "parentId" : null,
        "authorId" : "d7c0483a-91e4-4c4c-a990-e3d5abb179f5",
        "body" : "Hi @d80tb7, I work with Li and am also interested in cogroup.\r\n\r\nCan I ask how you were able to get your test to run? I wasn't able to run it without the following snippet:\r\n```\r\nif __name__ == \"__main__\":\r\n    from pyspark.sql.tests.test_pandas_udf_grouped_map import *\r\n\r\n    try:\r\n        import xmlrunner\r\n        testRunner = xmlrunner.XMLTestRunner(output='target/test-reports', verbosity=2)\r\n    except ImportError:\r\n        testRunner = None\r\n    unittest.main(testRunner=testRunner, verbosity=2)\r\n```\r\ntaken from the other similar tests like [test_pandas_udf_grouped_map](https://github.com/apache/spark/blob/master/python/pyspark/sql/tests/test_pandas_udf_grouped_map.py#L520). ",
        "createdAt" : "2019-07-01T21:17:06Z",
        "updatedAt" : "2019-07-02T08:59:40Z",
        "lastEditedBy" : "d7c0483a-91e4-4c4c-a990-e3d5abb179f5",
        "tags" : [
        ]
      },
      {
        "id" : "66888974-1d3c-4d01-ac83-fc0561f3b043",
        "parentId" : "3e321d4a-25ab-4e3f-a30e-c54f2c26350b",
        "authorId" : "627d34e9-eba3-4504-aa67-ffc2f967c413",
        "body" : "Hi @hjoo \r\n\r\nSo far I've just been running it via PyCharm's unit test runner under python 3.  I suspect the problem you had was that the iterator I added wasn't compatible with python 2 (sorry!).  I've fixed the iterator and added a similar snippet to the one you provided above.  Now I can run using `python/run-tests --testnames pyspark.sql.tests.test_pandas_udf_cogrouped_map`\r\n\r\nIf you still have problems let me know the error you get and I'll take a look.",
        "createdAt" : "2019-07-02T08:50:45Z",
        "updatedAt" : "2019-07-02T08:59:40Z",
        "lastEditedBy" : "627d34e9-eba3-4504-aa67-ffc2f967c413",
        "tags" : [
        ]
      }
    ],
    "commit" : "d15dabbf71ad3007ea0c37e71c997e6fa1799e51",
    "line" : 92,
    "diffHunk" : "@@ -1,1 +90,94 @@\n        assert_frame_equal(expected, result, check_column_type=_check_column_type)\n\nif __name__ == \"__main__\":\n    from pyspark.sql.tests.test_pandas_udf_cogrouped_map import *"
  }
]