[
  {
    "id" : "aa8e871d-56a4-4037-bc13-341d757f868a",
    "prId" : 32026,
    "prUrl" : "https://github.com/apache/spark/pull/32026#pullrequestreview-626791568",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b71da49c-13c9-4334-8428-7134fa1b7cb2",
        "parentId" : null,
        "authorId" : "4a97d995-edb8-416d-ba1b-29e030885f7e",
        "body" : "In the original PR, there is a new UDT of which the sqlType is a structtype, instead of `ArrayType` here for `ExamplePoint`.   Should we also test that?\r\n\r\nAlso , is it possible a UDT's sqlTypes are primitive types? might want to add them in tests as well.",
        "createdAt" : "2021-04-01T16:12:39Z",
        "updatedAt" : "2021-04-24T07:13:10Z",
        "lastEditedBy" : "4a97d995-edb8-416d-ba1b-29e030885f7e",
        "tags" : [
        ]
      },
      {
        "id" : "adee968c-7555-44e3-be70-df1a4ca3fecc",
        "parentId" : "b71da49c-13c9-4334-8428-7134fa1b7cb2",
        "authorId" : "f6398a50-7f9b-4c64-9d3a-f384404338b3",
        "body" : "See `_deserialize_pandas_with_udt`, support for StructType is postponed in later PRs. ",
        "createdAt" : "2021-04-02T02:06:37Z",
        "updatedAt" : "2021-04-24T07:13:10Z",
        "lastEditedBy" : "f6398a50-7f9b-4c64-9d3a-f384404338b3",
        "tags" : [
        ]
      },
      {
        "id" : "b543ebcc-a201-4dce-b731-747f81d20ba8",
        "parentId" : "b71da49c-13c9-4334-8428-7134fa1b7cb2",
        "authorId" : "f6398a50-7f9b-4c64-9d3a-f384404338b3",
        "body" : "I thought udt is for complex datatype. For udt which is actually primitive typeï¼Œlet me add unit tests.",
        "createdAt" : "2021-04-02T02:20:30Z",
        "updatedAt" : "2021-04-24T07:13:10Z",
        "lastEditedBy" : "f6398a50-7f9b-4c64-9d3a-f384404338b3",
        "tags" : [
        ]
      },
      {
        "id" : "8a303310-7382-49ef-97c7-8effadfab444",
        "parentId" : "b71da49c-13c9-4334-8428-7134fa1b7cb2",
        "authorId" : "f6398a50-7f9b-4c64-9d3a-f384404338b3",
        "body" : "For primitive data type, it is not a good practice to wrap it in UDT. As a result, I do not think we should spend too much time on support UDT which is actually primitive data type. This part can be postponed.",
        "createdAt" : "2021-04-02T05:16:39Z",
        "updatedAt" : "2021-04-24T07:13:10Z",
        "lastEditedBy" : "f6398a50-7f9b-4c64-9d3a-f384404338b3",
        "tags" : [
        ]
      }
    ],
    "commit" : "5845ee2419515b9243ea4396005f731eb294b6a9",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +212,216 @@    def test_array_udt_roundtrip(self):\n        pdf = pd.DataFrame({'points': pd.Series([\n            [ExamplePoint(1.0, 1.0), ExamplePoint(1.0, 2.0), ExamplePoint(1.0, 3.0)],\n            [ExamplePoint(2.0, 1.0), ExamplePoint(2.0, 2.0), ExamplePoint(2.0, 3.0)]\n        ])})"
  },
  {
    "id" : "55eb7f7c-308f-4cf3-9d1d-9f436c72fa79",
    "prId" : 29951,
    "prUrl" : "https://github.com/apache/spark/pull/29951#pullrequestreview-502641435",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f87b18d3-407c-443b-aad0-eb640cf03800",
        "parentId" : null,
        "authorId" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "body" : "The error here is a `TypeError`, but in case it gets changed to an `ArrowInvalid`, we do not want to have the original error chained because the assert does not include it when checking",
        "createdAt" : "2020-10-06T07:10:40Z",
        "updatedAt" : "2020-10-06T07:10:41Z",
        "lastEditedBy" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "tags" : [
        ]
      }
    ],
    "commit" : "e7a09e9dce49f6246957cb1965c26dbe846acbe8",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +267,271 @@        fields[5], fields[6] = fields[6], fields[5]  # swap decimal with date\n        wrong_schema = StructType(fields)\n        with self.sql_conf({\"spark.sql.execution.pandas.convertToArrowArraySafely\": False}):\n            with QuietTest(self.sc):\n                with self.assertRaisesRegexp(Exception, \"[D|d]ecimal.*got.*date\"):"
  },
  {
    "id" : "cd02c442-18fc-4763-bcd8-83339b57783c",
    "prId" : 29818,
    "prUrl" : "https://github.com/apache/spark/pull/29818#pullrequestreview-573550773",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a0d6f8c1-13ed-49c5-8e00-20c02adef418",
        "parentId" : null,
        "authorId" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "body" : "This block isn't quite what I was suggesting. We should compare the Pandas DataFrame resulting from `df.toPandas()` with `spark.sql.execution.arrow.pyspark.selfDestruct.enabled` False to `pdf_split`. Also, we will need to call `df.toPandas()` with `spark.sql.execution.arrow.pyspark.selfDestruct.enabled` True and compare that as well.",
        "createdAt" : "2021-01-21T18:43:46Z",
        "updatedAt" : "2021-01-28T20:44:29Z",
        "lastEditedBy" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "tags" : [
        ]
      }
    ],
    "commit" : "64d03012616c9bc56b97693d1fdf8132493deb0e",
    "line" : 31,
    "diffHunk" : "@@ -1,1 +215,219 @@        self.assertLessEqual(difference, 1.1 * expected_bytes)\n\n        with self.sql_conf({\"spark.sql.execution.arrow.pyspark.selfDestruct.enabled\": False}):\n            no_self_destruct_pdf = df.toPandas()\n            # Note while memory usage is 2x data size here (both table and pdf hold on to"
  },
  {
    "id" : "294ffd01-817e-411b-a6cd-34d7028b8ab7",
    "prId" : 29057,
    "prUrl" : "https://github.com/apache/spark/pull/29057#pullrequestreview-445661181",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2f85a88f-21ea-40dd-b6b2-0559df46a847",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "This is rather a followup of SPARK-25351. We didn't drop Python 2 yet.",
        "createdAt" : "2020-07-09T14:17:19Z",
        "updatedAt" : "2020-07-11T15:14:33Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "c3834b71918845a7fb8688bf9179c1e13798e17d",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +444,448 @@        self.assertEqual(arrow_type, 'string')\n        self.assertIsInstance(arrow_first_category_element, basestring)\n        self.assertIsInstance(spark_first_category_element, basestring)\n\n    def test_createDataFrame_with_float_index(self):"
  },
  {
    "id" : "33bf4c81-68a0-4e24-ac10-9e3086ab1176",
    "prId" : 26585,
    "prUrl" : "https://github.com/apache/spark/pull/26585#pullrequestreview-417312367",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4a2fa25f-cef3-430f-bb72-214ee68b72fa",
        "parentId" : null,
        "authorId" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "body" : "could you add an assert that the Spark DataFrame has column \"B\" as a string type?",
        "createdAt" : "2020-05-22T19:40:36Z",
        "updatedAt" : "2020-05-23T22:55:40Z",
        "lastEditedBy" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "tags" : [
        ]
      },
      {
        "id" : "4f4fcc53-6a8f-47a0-958f-ffc1cdf8e86c",
        "parentId" : "4a2fa25f-cef3-430f-bb72-214ee68b72fa",
        "authorId" : "0cfab362-97a8-4db7-9914-48f37816e889",
        "body" : "Done, move other test checks here too.",
        "createdAt" : "2020-05-23T22:11:11Z",
        "updatedAt" : "2020-05-23T22:55:40Z",
        "lastEditedBy" : "0cfab362-97a8-4db7-9914-48f37816e889",
        "tags" : [
        ]
      }
    ],
    "commit" : "7aa9fcd3a07e9e11d4f23b60b09db8b29c610a1a",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +434,438 @@\n        assert_frame_equal(result_spark, result_arrow)\n\n        # ensure original category elements are string\n        assert isinstance(category_first_element, str)"
  },
  {
    "id" : "160cdd26-3f68-4162-a98d-f691013b09a5",
    "prId" : 25594,
    "prUrl" : "https://github.com/apache/spark/pull/25594#pullrequestreview-280020787",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2bbdf393-e83e-453c-aca8-490b1bc1ffc8",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Oh, do we need these for test cases? This are opposite to the default values. In the reported JIRA's scenario, we didn't change the default configuration. Did we change the default values for those configurations in 3.0.0?",
        "createdAt" : "2019-08-27T07:55:20Z",
        "updatedAt" : "2019-08-27T07:56:36Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "ddbf9c3e-d6f1-47c2-b50a-067b726508f6",
        "parentId" : "2bbdf393-e83e-453c-aca8-490b1bc1ffc8",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "`spark.sql.execution.arrow.enabled` now has an alias `spark.sql.execution.arrow.pyspark.enabled` as of https://github.com/apache/spark/commit/d6632d185e147fcbe6724545488ad80dce20277e\r\n\r\nAnd, `spark.sql.execution.arrow.pyspark.fallback.enabled` was just set to narrow down test scope. Correct behaviour should be a failure in Arrow optimized code path (although the JIRAs' case, it fails in Arrow optimized path first, and then fails again in non-Arrow optimized path).",
        "createdAt" : "2019-08-27T08:01:52Z",
        "updatedAt" : "2019-08-27T08:01:52Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "b39a2845-f5d6-4ea3-b84c-03175103cedc",
        "parentId" : "2bbdf393-e83e-453c-aca8-490b1bc1ffc8",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I think it's better to have a test that fails with default settings, for branch 2.4.",
        "createdAt" : "2019-08-27T08:02:28Z",
        "updatedAt" : "2019-08-27T08:02:28Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "5e8d8e9c-09c4-4fb3-8e50-ac2b29d827e6",
        "parentId" : "2bbdf393-e83e-453c-aca8-490b1bc1ffc8",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "(the issue `SPARK-28881` happened because it passed in Arrow optimized code path, and it returned partial data or empty data).",
        "createdAt" : "2019-08-27T08:02:51Z",
        "updatedAt" : "2019-08-27T08:02:51Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "002f783d-8f95-442f-9572-432350923bfc",
        "parentId" : "2bbdf393-e83e-453c-aca8-490b1bc1ffc8",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I am going to not use aliases when I port to branch-2.4 because techincally `spark.sql.execution.arrow.enabled` and `spark.sql.execution.arrow.fallback.enabled` are supposed to be removed out in the future.",
        "createdAt" : "2019-08-27T08:04:26Z",
        "updatedAt" : "2019-08-27T08:04:26Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "c85f0cc5-3a09-4331-b942-0ebcf1806e17",
        "parentId" : "2bbdf393-e83e-453c-aca8-490b1bc1ffc8",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "These configurations just narrow down the testing scope, and `test_arrow.py` is supposed to test this scope (as can be seen `ArrowTests`).",
        "createdAt" : "2019-08-27T08:05:31Z",
        "updatedAt" : "2019-08-27T08:05:31Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "87840b81e9a4a2ff7f595376bd1d20f086c5b384",
    "line" : 30,
    "diffHunk" : "@@ -1,1 +439,443 @@        # Explicitly enable Arrow and disable fallback.\n        cls.spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n        cls.spark.conf.set(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"false\")\n\n    @classmethod"
  },
  {
    "id" : "d0a9529e-8af9-4adf-86ee-894f64d8b798",
    "prId" : 24867,
    "prUrl" : "https://github.com/apache/spark/pull/24867#pullrequestreview-249665741",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "deaa18da-a667-46a7-9eeb-0c369c795c84",
        "parentId" : null,
        "authorId" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "body" : "removing the workaround changed this error message and it seemed more clear for the test to swap int field instead of timestamp",
        "createdAt" : "2019-06-14T00:32:22Z",
        "updatedAt" : "2019-06-14T21:26:25Z",
        "lastEditedBy" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "tags" : [
        ]
      }
    ],
    "commit" : "13b6ed448d7323f509792184b78df4bdec04d917",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +272,276 @@        wrong_schema = StructType(fields)\n        with QuietTest(self.sc):\n            with self.assertRaisesRegexp(Exception, \"integer.*required.*got.*str\"):\n                self.spark.createDataFrame(pdf, schema=wrong_schema)\n"
  },
  {
    "id" : "cda09834-54c1-4fcc-b967-65e4cb1f20a1",
    "prId" : 24650,
    "prUrl" : "https://github.com/apache/spark/pull/24650#pullrequestreview-239791287",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d027da50-85c1-4db0-8740-0944b44cc0e0",
        "parentId" : null,
        "authorId" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "body" : "Could you also add an assert that the column name is correct?",
        "createdAt" : "2019-05-20T22:35:26Z",
        "updatedAt" : "2019-05-22T00:38:26Z",
        "lastEditedBy" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "tags" : [
        ]
      },
      {
        "id" : "8bb85b1b-e362-480d-bb47-da2aa5d00ea1",
        "parentId" : "d027da50-85c1-4db0-8740-0944b44cc0e0",
        "authorId" : "e5821ab5-ed30-4285-b919-301e171bb4f8",
        "body" : "done",
        "createdAt" : "2019-05-21T01:42:02Z",
        "updatedAt" : "2019-05-22T00:38:26Z",
        "lastEditedBy" : "e5821ab5-ed30-4285-b919-301e171bb4f8",
        "tags" : [
        ]
      }
    ],
    "commit" : "db6f4b1233895083b60a68a40685e50881143dcf",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +188,192 @@        df = self.spark.createDataFrame(self.sc.emptyRDD(), schema)\n        pdf = df.toPandas()\n        self.assertEqual(len(pdf.columns), 1)\n        self.assertEqual(pdf.columns[0], \"field1\")\n        self.assertTrue(pdf.empty)"
  }
]