[
  {
    "id" : "aa8e871d-56a4-4037-bc13-341d757f868a",
    "prId" : 32026,
    "prUrl" : "https://github.com/apache/spark/pull/32026#pullrequestreview-626791568",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b71da49c-13c9-4334-8428-7134fa1b7cb2",
        "parentId" : null,
        "authorId" : "4a97d995-edb8-416d-ba1b-29e030885f7e",
        "body" : "In the original PR, there is a new UDT of which the sqlType is a structtype, instead of `ArrayType` here for `ExamplePoint`.   Should we also test that?\r\n\r\nAlso , is it possible a UDT's sqlTypes are primitive types? might want to add them in tests as well.",
        "createdAt" : "2021-04-01T16:12:39Z",
        "updatedAt" : "2021-04-24T07:13:10Z",
        "lastEditedBy" : "4a97d995-edb8-416d-ba1b-29e030885f7e",
        "tags" : [
        ]
      },
      {
        "id" : "adee968c-7555-44e3-be70-df1a4ca3fecc",
        "parentId" : "b71da49c-13c9-4334-8428-7134fa1b7cb2",
        "authorId" : "f6398a50-7f9b-4c64-9d3a-f384404338b3",
        "body" : "See `_deserialize_pandas_with_udt`, support for StructType is postponed in later PRs. ",
        "createdAt" : "2021-04-02T02:06:37Z",
        "updatedAt" : "2021-04-24T07:13:10Z",
        "lastEditedBy" : "f6398a50-7f9b-4c64-9d3a-f384404338b3",
        "tags" : [
        ]
      },
      {
        "id" : "b543ebcc-a201-4dce-b731-747f81d20ba8",
        "parentId" : "b71da49c-13c9-4334-8428-7134fa1b7cb2",
        "authorId" : "f6398a50-7f9b-4c64-9d3a-f384404338b3",
        "body" : "I thought udt is for complex datatype. For udt which is actually primitive typeï¼Œlet me add unit tests.",
        "createdAt" : "2021-04-02T02:20:30Z",
        "updatedAt" : "2021-04-24T07:13:10Z",
        "lastEditedBy" : "f6398a50-7f9b-4c64-9d3a-f384404338b3",
        "tags" : [
        ]
      },
      {
        "id" : "8a303310-7382-49ef-97c7-8effadfab444",
        "parentId" : "b71da49c-13c9-4334-8428-7134fa1b7cb2",
        "authorId" : "f6398a50-7f9b-4c64-9d3a-f384404338b3",
        "body" : "For primitive data type, it is not a good practice to wrap it in UDT. As a result, I do not think we should spend too much time on support UDT which is actually primitive data type. This part can be postponed.",
        "createdAt" : "2021-04-02T05:16:39Z",
        "updatedAt" : "2021-04-24T07:13:10Z",
        "lastEditedBy" : "f6398a50-7f9b-4c64-9d3a-f384404338b3",
        "tags" : [
        ]
      }
    ],
    "commit" : "5845ee2419515b9243ea4396005f731eb294b6a9",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +212,216 @@    def test_array_udt_roundtrip(self):\n        pdf = pd.DataFrame({'points': pd.Series([\n            [ExamplePoint(1.0, 1.0), ExamplePoint(1.0, 2.0), ExamplePoint(1.0, 3.0)],\n            [ExamplePoint(2.0, 1.0), ExamplePoint(2.0, 2.0), ExamplePoint(2.0, 3.0)]\n        ])})"
  },
  {
    "id" : "55eb7f7c-308f-4cf3-9d1d-9f436c72fa79",
    "prId" : 29951,
    "prUrl" : "https://github.com/apache/spark/pull/29951#pullrequestreview-502641435",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f87b18d3-407c-443b-aad0-eb640cf03800",
        "parentId" : null,
        "authorId" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "body" : "The error here is a `TypeError`, but in case it gets changed to an `ArrowInvalid`, we do not want to have the original error chained because the assert does not include it when checking",
        "createdAt" : "2020-10-06T07:10:40Z",
        "updatedAt" : "2020-10-06T07:10:41Z",
        "lastEditedBy" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "tags" : [
        ]
      }
    ],
    "commit" : "e7a09e9dce49f6246957cb1965c26dbe846acbe8",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +267,271 @@        fields[5], fields[6] = fields[6], fields[5]  # swap decimal with date\n        wrong_schema = StructType(fields)\n        with self.sql_conf({\"spark.sql.execution.pandas.convertToArrowArraySafely\": False}):\n            with QuietTest(self.sc):\n                with self.assertRaisesRegexp(Exception, \"[D|d]ecimal.*got.*date\"):"
  },
  {
    "id" : "cd02c442-18fc-4763-bcd8-83339b57783c",
    "prId" : 29818,
    "prUrl" : "https://github.com/apache/spark/pull/29818#pullrequestreview-573550773",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a0d6f8c1-13ed-49c5-8e00-20c02adef418",
        "parentId" : null,
        "authorId" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "body" : "This block isn't quite what I was suggesting. We should compare the Pandas DataFrame resulting from `df.toPandas()` with `spark.sql.execution.arrow.pyspark.selfDestruct.enabled` False to `pdf_split`. Also, we will need to call `df.toPandas()` with `spark.sql.execution.arrow.pyspark.selfDestruct.enabled` True and compare that as well.",
        "createdAt" : "2021-01-21T18:43:46Z",
        "updatedAt" : "2021-01-28T20:44:29Z",
        "lastEditedBy" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "tags" : [
        ]
      }
    ],
    "commit" : "64d03012616c9bc56b97693d1fdf8132493deb0e",
    "line" : 31,
    "diffHunk" : "@@ -1,1 +215,219 @@        self.assertLessEqual(difference, 1.1 * expected_bytes)\n\n        with self.sql_conf({\"spark.sql.execution.arrow.pyspark.selfDestruct.enabled\": False}):\n            no_self_destruct_pdf = df.toPandas()\n            # Note while memory usage is 2x data size here (both table and pdf hold on to"
  },
  {
    "id" : "294ffd01-817e-411b-a6cd-34d7028b8ab7",
    "prId" : 29057,
    "prUrl" : "https://github.com/apache/spark/pull/29057#pullrequestreview-445661181",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2f85a88f-21ea-40dd-b6b2-0559df46a847",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "This is rather a followup of SPARK-25351. We didn't drop Python 2 yet.",
        "createdAt" : "2020-07-09T14:17:19Z",
        "updatedAt" : "2020-07-11T15:14:33Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "c3834b71918845a7fb8688bf9179c1e13798e17d",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +444,448 @@        self.assertEqual(arrow_type, 'string')\n        self.assertIsInstance(arrow_first_category_element, basestring)\n        self.assertIsInstance(spark_first_category_element, basestring)\n\n    def test_createDataFrame_with_float_index(self):"
  },
  {
    "id" : "33bf4c81-68a0-4e24-ac10-9e3086ab1176",
    "prId" : 26585,
    "prUrl" : "https://github.com/apache/spark/pull/26585#pullrequestreview-417312367",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4a2fa25f-cef3-430f-bb72-214ee68b72fa",
        "parentId" : null,
        "authorId" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "body" : "could you add an assert that the Spark DataFrame has column \"B\" as a string type?",
        "createdAt" : "2020-05-22T19:40:36Z",
        "updatedAt" : "2020-05-23T22:55:40Z",
        "lastEditedBy" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "tags" : [
        ]
      },
      {
        "id" : "4f4fcc53-6a8f-47a0-958f-ffc1cdf8e86c",
        "parentId" : "4a2fa25f-cef3-430f-bb72-214ee68b72fa",
        "authorId" : "0cfab362-97a8-4db7-9914-48f37816e889",
        "body" : "Done, move other test checks here too.",
        "createdAt" : "2020-05-23T22:11:11Z",
        "updatedAt" : "2020-05-23T22:55:40Z",
        "lastEditedBy" : "0cfab362-97a8-4db7-9914-48f37816e889",
        "tags" : [
        ]
      }
    ],
    "commit" : "7aa9fcd3a07e9e11d4f23b60b09db8b29c610a1a",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +434,438 @@\n        assert_frame_equal(result_spark, result_arrow)\n\n        # ensure original category elements are string\n        assert isinstance(category_first_element, str)"
  }
]