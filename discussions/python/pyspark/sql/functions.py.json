[
  {
    "id" : "15b1b11e-947e-4fa6-af0a-375cb0d479c7",
    "prId" : 32566,
    "prUrl" : "https://github.com/apache/spark/pull/32566#pullrequestreview-660544882",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "65d4772d-74bd-4cb8-a2ac-a062801c172a",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "```suggestion\r\n    Examples\r\n    --------\r\n```",
        "createdAt" : "2021-05-17T04:40:14Z",
        "updatedAt" : "2021-05-17T04:44:05Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "3b05e99de665e785ad05a37717d3d2f761cce30a",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +2698,2702 @@        a country of the locale\n\n    Examples\n    --------\n    >>> df = spark.createDataFrame([[\"This is an example sentence.\"]], [\"string\"])"
  },
  {
    "id" : "6e9d8983-ad31-482a-b1fd-a171bca187aa",
    "prId" : 32566,
    "prUrl" : "https://github.com/apache/spark/pull/32566#pullrequestreview-660544882",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6e28cb60-238e-48be-980b-119141fdd36c",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Can we also add:\r\n\r\n```\r\n    Parameters\r\n    ----------\r\n    str: ...\r\n```\r\n\r\n(see also https://numpydoc.readthedocs.io/en/latest/format.html)",
        "createdAt" : "2021-05-17T04:40:27Z",
        "updatedAt" : "2021-05-17T04:44:05Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "3b05e99de665e785ad05a37717d3d2f761cce30a",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +2688,2692 @@\n    .. versionadded:: 3.2.0\n\n    Parameters\n    ----------"
  },
  {
    "id" : "27182c9c-1993-44cb-872d-331a3ad835f8",
    "prId" : 31207,
    "prUrl" : "https://github.com/apache/spark/pull/31207#pullrequestreview-578361157",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6d839666-65be-490e-a00f-cc3b98360e45",
        "parentId" : null,
        "authorId" : "eb588fd2-21dd-4dea-a8f9-bcf2e48148eb",
        "body" : "Worth mentioning the newly supported types in the docstring",
        "createdAt" : "2021-01-26T22:26:37Z",
        "updatedAt" : "2021-01-29T13:23:21Z",
        "lastEditedBy" : "eb588fd2-21dd-4dea-a8f9-bcf2e48148eb",
        "tags" : [
        ]
      },
      {
        "id" : "fd5ee94c-ef81-4e32-bc02-db32937f5c6d",
        "parentId" : "6d839666-65be-490e-a00f-cc3b98360e45",
        "authorId" : "981b170c-729a-429c-b115-0350ea50b32b",
        "body" : "Added a short description of the input parameters @LielBach. ",
        "createdAt" : "2021-01-28T14:02:12Z",
        "updatedAt" : "2021-01-29T13:23:21Z",
        "lastEditedBy" : "981b170c-729a-429c-b115-0350ea50b32b",
        "tags" : [
        ]
      }
    ],
    "commit" : "dade2b17152baf3f02553060bc8bbdbf96744756",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +90,94 @@def lit(col):\n    \"\"\"\n    Creates a :class:`Column` of literal value.\n\n    .. versionadded:: 1.3.0"
  },
  {
    "id" : "7938009a-a1a8-4adf-962e-e19495a288a1",
    "prId" : 31207,
    "prUrl" : "https://github.com/apache/spark/pull/31207#pullrequestreview-578996182",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f739d86d-4a50-4e15-b499-e09b4e0c0164",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "One thing that makes me worry is this isn't technically a literal ... although it's foldable. The results and optimization might be differently applied compared to Scala side because they are not literals. For example, we have some optimization rules that only applies to literals here and there.\r\n\r\nAlso looks like Scala side doesn't support map and struct (due to both the limitation of typing and the fact that `Row` doesn't hold field names, etc.).",
        "createdAt" : "2021-01-27T04:54:27Z",
        "updatedAt" : "2021-01-29T13:23:21Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "f9b172ae-88bb-4669-be96-bb9965e1d05b",
        "parentId" : "f739d86d-4a50-4e15-b499-e09b4e0c0164",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Hm, okay but these will be literals after constant folding though. @cloud-fan, WDYT about supporting complex type at literals in this way? I think we can do similar thing in Scala side as well to allow map and struct literals.",
        "createdAt" : "2021-01-27T05:03:55Z",
        "updatedAt" : "2021-01-29T13:23:21Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "108fc6a3-22ac-4f39-aa6a-fd855135703e",
        "parentId" : "f739d86d-4a50-4e15-b499-e09b4e0c0164",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "cc @viirya too",
        "createdAt" : "2021-01-27T05:04:54Z",
        "updatedAt" : "2021-01-29T13:23:21Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "b9ad4387-11bf-4920-9e3b-4ebe513ca226",
        "parentId" : "f739d86d-4a50-4e15-b499-e09b4e0c0164",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Seems okay because there is constant folding optimization. So if they are foldable, they will be optimized as literals during query optimization.",
        "createdAt" : "2021-01-27T06:15:09Z",
        "updatedAt" : "2021-01-29T13:23:21Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "ced0a754-a7bc-4eec-9625-a82bf3690190",
        "parentId" : "f739d86d-4a50-4e15-b499-e09b4e0c0164",
        "authorId" : "981b170c-729a-429c-b115-0350ea50b32b",
        "body" : "> Also looks like Scala side doesn't support map and struct (due to both the limitation of typing and the fact that `Row` doesn't hold field names, etc.).\r\n\r\nActually, `typedLit` supports `Map` objects:\r\n\r\n```scala\r\nscala> typedLit(Map(\"a\" -> 1, \"b\" -> 2))\r\nres7: org.apache.spark.sql.Column = keys: [a,b], values: [1,2]\r\n```\r\n\r\nand `Products`\r\n\r\n```scala\r\nscala> case class FooBar(foo: Int, bar: String)\r\ndefined class FooBar\r\n\r\nscala> typedLit(FooBar(1, \"a\"))\r\nres8: org.apache.spark.sql.Column = [1,a]\r\n```\r\n\r\n`Row` would be specific to PySpark, but since it is just a tuple, it seemed reasonable to handle it with names.\r\n\r\n> Hm, okay but these will be literals after constant folding though.\r\n\r\nThe biggest difference compared to Scala is that it we handle mixed constants and column objects:\r\n\r\n```python\r\n>>> spark.createDataFrame([(1, 2)], (\"a\", \"b\")).select(lit({\"a\": col(\"a\"), \"b\": col(\"b\")})).show()      \r\n+----------------+\r\n| map(a, a, b, b)|\r\n+----------------+\r\n|{a -> 1, b -> 2}|\r\n+----------------+\r\n```\r\n\r\nSo there is a case where `lit` results in a non-`Literal`.\r\n\r\n\r\n",
        "createdAt" : "2021-01-27T14:09:55Z",
        "updatedAt" : "2021-01-29T13:23:21Z",
        "lastEditedBy" : "981b170c-729a-429c-b115-0350ea50b32b",
        "tags" : [
        ]
      },
      {
        "id" : "94ddad4e-b9e0-4598-b438-f5504ea991df",
        "parentId" : "f739d86d-4a50-4e15-b499-e09b4e0c0164",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "It seems fine, but we might be able to convert these exprs for complex types into literals in advance via `PythonSQLUtils`.",
        "createdAt" : "2021-01-29T07:24:08Z",
        "updatedAt" : "2021-01-29T13:23:21Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "dade2b17152baf3f02553060bc8bbdbf96744756",
    "line" : 53,
    "diffHunk" : "@@ -1,1 +132,136 @@        )\n\n        return struct(*[\n            lit(x).alias(a)\n            for x, a in zip(col, fields)"
  },
  {
    "id" : "fa4df137-eaa9-451e-a6c3-c0d314d6d669",
    "prId" : 31207,
    "prUrl" : "https://github.com/apache/spark/pull/31207#pullrequestreview-579964625",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ca5e55c9-b09a-4eae-a8ac-daf79a043819",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "\"Creates a :class:`Column` of literal value.\"?\r\n\r\nhttps://github.com/apache/spark/blob/72b7f8abfb60d0008f1f9bed94ce1c367a7d7cce/sql/core/src/main/scala/org/apache/spark/sql/functions.scala#L105",
        "createdAt" : "2021-01-29T04:18:08Z",
        "updatedAt" : "2021-01-29T13:23:21Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "33aa20cb-3987-486e-a97e-f52c9359b28b",
        "parentId" : "ca5e55c9-b09a-4eae-a8ac-daf79a043819",
        "authorId" : "981b170c-729a-429c-b115-0350ea50b32b",
        "body" : "I am not sure if I understand what you mean. We already have matching sentence in the description block\r\n\r\nhttps://github.com/apache/spark/blob/6198e0aca0d30d6bd6ba9f29a0fe60a62016caed/python/pyspark/sql/functions.py#L92\r\n\r\nCould you clarify please?",
        "createdAt" : "2021-01-29T13:26:35Z",
        "updatedAt" : "2021-01-29T13:26:35Z",
        "lastEditedBy" : "981b170c-729a-429c-b115-0350ea50b32b",
        "tags" : [
        ]
      },
      {
        "id" : "781168ab-0fc8-4a46-bb4c-8b78cf8f294e",
        "parentId" : "ca5e55c9-b09a-4eae-a8ac-daf79a043819",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Ah, nvm. I missed it.",
        "createdAt" : "2021-02-01T01:31:14Z",
        "updatedAt" : "2021-02-01T01:31:14Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "dade2b17152baf3f02553060bc8bbdbf96744756",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +99,103 @@    ----------\n    col : bool, float, int, str, datetime.date, datetime.datetime, dict, list, tuple\n        Object to be converted into :class:`Column`.\n\n        If it is a collection, conversion will be applied recursively. In such case,"
  },
  {
    "id" : "6820c7af-44ad-402e-a0dd-2b9b420f7637",
    "prId" : 31207,
    "prUrl" : "https://github.com/apache/spark/pull/31207#pullrequestreview-579854094",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3b6dd29f-9397-4cfd-91ba-00eca1cb83aa",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@zero323, can we implement these logics in Scala side, and we simply call `lit` here? I think that's more what @maropu meant.\r\n\r\nI think the only special case we need to implement in Python side is the struct case because 1. `Row` class cannot be passed to JVM side and 2. both `list` and `tuple` are converted into `ArrayList` so JVM cannot tell the origin. We will probably have to add some comments on that case.",
        "createdAt" : "2021-01-31T04:47:50Z",
        "updatedAt" : "2021-01-31T04:47:50Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "dade2b17152baf3f02553060bc8bbdbf96744756",
    "line" : 43,
    "diffHunk" : "@@ -1,1 +122,126 @@        return array(*[lit(x) for x in col])\n\n    elif isinstance(col, tuple):\n        fields = (\n            # Named tuple"
  },
  {
    "id" : "c3ebcc66-9c3a-4817-b331-4c5365239eb8",
    "prId" : 31207,
    "prUrl" : "https://github.com/apache/spark/pull/31207#pullrequestreview-579976168",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d88d7f2c-d3eb-4dae-b3fe-bcda44fdd114",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "One thing I am a bit worried is that this will lazily check the type (with type coercion) unlike Scala side ...",
        "createdAt" : "2021-02-01T02:24:23Z",
        "updatedAt" : "2021-02-01T02:24:24Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "dade2b17152baf3f02553060bc8bbdbf96744756",
    "line" : 41,
    "diffHunk" : "@@ -1,1 +120,124 @@\n    elif isinstance(col, list):\n        return array(*[lit(x) for x in col])\n\n    elif isinstance(col, tuple):"
  },
  {
    "id" : "7197c45c-7aaa-4ef7-9323-481684fbdfb7",
    "prId" : 30745,
    "prUrl" : "https://github.com/apache/spark/pull/30745#pullrequestreview-550915908",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "74206255-79ab-4a68-89b9-acfd8e89cf9b",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Let's also add some doctests here.",
        "createdAt" : "2020-12-13T02:17:42Z",
        "updatedAt" : "2021-03-01T17:18:18Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "da436b34-3f09-414a-96ac-e2980ee8173a",
        "parentId" : "74206255-79ab-4a68-89b9-acfd8e89cf9b",
        "authorId" : "dc7a33cf-bba6-4512-a5a1-248c3cac6eae",
        "body" : "I agree that having some documentation, with examples, would be very helpful and I've added these following your advice. However, it seems like the automated build tests are insanely restrictive in their validation of these doctests, causing multiple building failures when the pydoc examples differ by inconsequential whitespace or formatting changes. For example, even when I have directly copy-pasted example outputs from a pyspark session, these were rejected by the build-system.\r\n\r\nCan I suggest that someone investigates whether the build-system can make these documentation checks more tolerant of whitespace variations, which I'd expect will trip-up many developers and will create obstacles for producing documentation that would benefit Spark users?",
        "createdAt" : "2020-12-13T12:32:35Z",
        "updatedAt" : "2021-03-01T17:18:18Z",
        "lastEditedBy" : "dc7a33cf-bba6-4512-a5a1-248c3cac6eae",
        "tags" : [
        ]
      }
    ],
    "commit" : "1cdabcbdab2d54c67f09139f3b6cea1b2c09e04e",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +247,251 @@    +----+-------+\n\n    \"\"\"\n    return _invoke_function_over_column(\"product\", col)\n"
  },
  {
    "id" : "1ba3b16c-1115-4062-bd27-2bd1c3f181fc",
    "prId" : 30501,
    "prUrl" : "https://github.com/apache/spark/pull/30501#pullrequestreview-538933079",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e4a55b5a-2f0e-43b9-8a8b-041f9cca0317",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Let's also update `spark/python/docs/source/reference/pyspark.sql.rst`",
        "createdAt" : "2020-11-26T01:17:05Z",
        "updatedAt" : "2020-11-26T11:01:55Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "17451c7a0bb58cd5e4f9ee29e62370f2208726d3",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +221,225 @@\n\ndef acosh(col):\n    \"\"\"\n    Computes inverse hyperbolic cosine of the input column."
  },
  {
    "id" : "9a861d58-a2b0-4d24-b123-114a2d553bd4",
    "prId" : 30201,
    "prUrl" : "https://github.com/apache/spark/pull/30201#pullrequestreview-521211584",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2da07c8d-c79a-4c3a-9cc7-cc25aeb4ae6d",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "```suggestion\r\n    >>> schema =  (\r\n    ...     '{\"type\" : \"map\", \"keyType\" : \"string\",'\r\n    ...     '\"valueType\" : \"integer\", \"valueContainsNull\" : true}')\r\n    >>> df.select(from_json(df.value, schema).alias(\"json\")).collect()\r\n    [Row(json={'a': 1})]\r\n```",
        "createdAt" : "2020-11-01T10:34:07Z",
        "updatedAt" : "2020-11-01T10:34:08Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "88cd2a3354e587861f3214b94d0cadc8bc163747",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +2864,2868 @@     '{\"type\" : \"map\", \"keyType\" : \"string\", \"valueType\" : \"integer\", \"valueContainsNull\" : true}')\\\n     .alias(\"json\")).collect()\n    [Row(json={'a': 1})]\n    >>> data = [(1, '''[{\"a\": 1}]''')]\n    >>> schema = ArrayType(StructType([StructField(\"a\", IntegerType())]))"
  },
  {
    "id" : "15a96970-9c56-4eda-ae39-b1addaa9fb00",
    "prId" : 30181,
    "prUrl" : "https://github.com/apache/spark/pull/30181#pullrequestreview-519650717",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8d666f11-87b5-4a7e-a486-dee7be306173",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "There are a bit of trivial changes in the doc to make it more sense with the new format. One example is here.",
        "createdAt" : "2020-10-29T13:15:48Z",
        "updatedAt" : "2020-11-03T00:49:21Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "40bc04996ab651411656c08849b5ec0495cd8468",
    "line" : 786,
    "diffHunk" : "@@ -1,1 +1277,1281 @@    ----------\n    cols : list, set, str or :class:`Column`\n        column names or :class:`Column`\\\\s to contain in the output struct.\n\n    Examples"
  },
  {
    "id" : "49112491-aef9-49be-ac15-ab26746d2504",
    "prId" : 30181,
    "prUrl" : "https://github.com/apache/spark/pull/30181#pullrequestreview-520264562",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f3e6bbd4-0de7-4813-bbd9-1ce30619c297",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "1.4.0?",
        "createdAt" : "2020-10-30T01:22:31Z",
        "updatedAt" : "2020-11-03T00:49:21Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "40bc04996ab651411656c08849b5ec0495cd8468",
    "line" : 38,
    "diffHunk" : "@@ -1,1 +217,221 @@def asin(col):\n    \"\"\"\n    .. versionadded:: 1.3.0\n\n"
  },
  {
    "id" : "171795c9-1640-4b99-a02a-23f1239bc29d",
    "prId" : 30181,
    "prUrl" : "https://github.com/apache/spark/pull/30181#pullrequestreview-520393963",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5c0bb303-365b-46a5-8905-c172cf0ca4ef",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Not to change to `.. versionadded::`?",
        "createdAt" : "2020-10-30T01:24:27Z",
        "updatedAt" : "2020-11-03T00:49:21Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "1d68f281-9576-459a-bbfa-6ab3c2e5b2d5",
        "parentId" : "5c0bb303-365b-46a5-8905-c172cf0ca4ef",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Oh yeah. This one no need to change because `.. deprecated::` is still [reST style directive](https://www.sphinx-doc.org/en/master/usage/restructuredtext/directives.html). So, this has no problem about formatting error because it does not use NumPy documentation specific syntax.",
        "createdAt" : "2020-10-30T05:59:47Z",
        "updatedAt" : "2020-11-03T00:49:21Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "40bc04996ab651411656c08849b5ec0495cd8468",
    "line" : 192,
    "diffHunk" : "@@ -1,1 +424,428 @@\n\n@since(1.4)\ndef toDegrees(col):\n    \"\"\""
  },
  {
    "id" : "d0f4e33c-3e90-465e-aa7a-24d0f2c47ee2",
    "prId" : 30181,
    "prUrl" : "https://github.com/apache/spark/pull/30181#pullrequestreview-520264562",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "553d1a47-a6c3-4b9e-b607-05204b9ce76b",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "ditto",
        "createdAt" : "2020-10-30T01:24:37Z",
        "updatedAt" : "2020-11-03T00:49:21Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "40bc04996ab651411656c08849b5ec0495cd8468",
    "line" : 202,
    "diffHunk" : "@@ -1,1 +434,438 @@\n\n@since(1.4)\ndef toRadians(col):\n    \"\"\""
  },
  {
    "id" : "914f0863-ad32-41ba-bc37-9c919e4734c5",
    "prId" : 30181,
    "prUrl" : "https://github.com/apache/spark/pull/30181#pullrequestreview-520264562",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c1da1fce-c398-46cd-9886-4ceffa1092cc",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "this?",
        "createdAt" : "2020-10-30T01:25:35Z",
        "updatedAt" : "2020-11-03T00:49:21Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "40bc04996ab651411656c08849b5ec0495cd8468",
    "line" : 338,
    "diffHunk" : "@@ -1,1 +731,735 @@\n\n@since(1.3)\ndef approxCountDistinct(col, rsd=None):\n    \"\"\""
  },
  {
    "id" : "eee94655-e73e-45b6-9132-703ffac3bbcc",
    "prId" : 30143,
    "prUrl" : "https://github.com/apache/spark/pull/30143#pullrequestreview-517512188",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4d1d5434-8d55-4608-b1c1-453478949a44",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@zero323, looks like we should call `col.asc if isinstance(col, Column) else _invoke_function(\"asc\", col)` instead to make it support both string and columns in PySpark side (Scala side does not have the Column signature).\r\n\r\nIt was decided to support both string/column instances where we can in PySpark at SPARK-26979.\r\n\r\nIdeally we should do this in a separate JIRA and PR but I don't mind fixing it here while we're here. I will leave it to you.",
        "createdAt" : "2020-10-27T01:44:36Z",
        "updatedAt" : "2020-10-27T02:05:28Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "21a83119-3276-4e19-b391-c50b38e4e503",
        "parentId" : "4d1d5434-8d55-4608-b1c1-453478949a44",
        "authorId" : "981b170c-729a-429c-b115-0350ea50b32b",
        "body" : "I wonder if it makes more sense, for the sake of consistency ,to add `Column` signatures on Scala side. ",
        "createdAt" : "2020-10-27T08:14:04Z",
        "updatedAt" : "2020-10-27T08:14:05Z",
        "lastEditedBy" : "981b170c-729a-429c-b115-0350ea50b32b",
        "tags" : [
        ]
      },
      {
        "id" : "bb07998a-15e7-4cd1-9ba5-3ce5109a7191",
        "parentId" : "4d1d5434-8d55-4608-b1c1-453478949a44",
        "authorId" : "981b170c-729a-429c-b115-0350ea50b32b",
        "body" : "Let's handle this in SPARK-33257",
        "createdAt" : "2020-10-27T09:44:35Z",
        "updatedAt" : "2020-10-27T09:44:36Z",
        "lastEditedBy" : "981b170c-729a-429c-b115-0350ea50b32b",
        "tags" : [
        ]
      }
    ],
    "commit" : "2c99821f3c8630ece8c2cc149ca43667a4be8b52",
    "line" : 218,
    "diffHunk" : "@@ -1,1 +117,121 @@    Returns a sort expression based on the ascending order of the given column name.\n    \"\"\"\n    return _invoke_function(\"asc\", col)\n\n"
  },
  {
    "id" : "937e5049-c38b-44c2-b657-009e0d8481d0",
    "prId" : 30143,
    "prUrl" : "https://github.com/apache/spark/pull/30143#pullrequestreview-517288922",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d953e129-c367-435e-975e-7554e3623bb4",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "This one would have to be same as `asc` above.",
        "createdAt" : "2020-10-27T01:47:38Z",
        "updatedAt" : "2020-10-27T02:05:28Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "2c99821f3c8630ece8c2cc149ca43667a4be8b52",
    "line" : 226,
    "diffHunk" : "@@ -1,1 +125,129 @@    Returns a sort expression based on the descending order of the given column name.\n    \"\"\"\n    return _invoke_function(\"desc\", col)\n\n"
  },
  {
    "id" : "82a890ce-1e66-4a19-b756-df6d82913d9d",
    "prId" : 30143,
    "prUrl" : "https://github.com/apache/spark/pull/30143#pullrequestreview-517288922",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "25c21f71-de4b-483c-9ef2-ad85cad732f6",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "`asc_nulls_first`, `asc_nulls_last`, `desc_nulls_first` and `desc_nulls_last` look the same case above.",
        "createdAt" : "2020-10-27T01:54:24Z",
        "updatedAt" : "2020-10-27T02:05:29Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "2c99821f3c8630ece8c2cc149ca43667a4be8b52",
    "line" : 494,
    "diffHunk" : "@@ -1,1 +393,397 @@    column name, and null values return before non-null values.\n    \"\"\"\n    return _invoke_function(\"asc_nulls_first\", col)\n\n"
  },
  {
    "id" : "5154cda2-4198-4fdc-9b4d-3dc0bbd2a5a5",
    "prId" : 30143,
    "prUrl" : "https://github.com/apache/spark/pull/30143#pullrequestreview-517288922",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b5bc3f74-a0e6-42c5-9789-39882d8deb93",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Much more neat!",
        "createdAt" : "2020-10-27T01:58:49Z",
        "updatedAt" : "2020-10-27T02:05:29Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "2c99821f3c8630ece8c2cc149ca43667a4be8b52",
    "line" : 109,
    "diffHunk" : "@@ -1,1 +77,81 @@        # if they are not columns or strings.\n        _to_java_column(col1) if isinstance(col1, (str, Column)) else float(col1),\n        _to_java_column(col2) if isinstance(col2, (str, Column)) else float(col2)\n    )\n"
  },
  {
    "id" : "057d7a7f-ed90-422b-831c-81bb41621801",
    "prId" : 29947,
    "prUrl" : "https://github.com/apache/spark/pull/29947#pullrequestreview-503467714",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "47ae8080-a046-4799-9fba-1555223a6837",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Now we should also add it into type hints and documentations at https://github.com/apache/spark/blob/master/python/docs/source/reference/pyspark.sql.rst#functions and https://github.com/apache/spark/blob/master/python/pyspark/sql/functions.pyi. cc @zero323 FYI\r\n\r\n",
        "createdAt" : "2020-10-07T01:38:56Z",
        "updatedAt" : "2020-10-08T03:03:41Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "5b93fd49313152356a3fd2baf6c21123170c4037",
    "line" : 3,
    "diffHunk" : "@@ -1,1 +1592,1596 @@    return Column(jc)\n\n\n@since(3.1)\ndef assert_true(col, errMsg=None):"
  },
  {
    "id" : "e15dcb42-173c-4712-bf50-b41e409023e2",
    "prId" : 29899,
    "prUrl" : "https://github.com/apache/spark/pull/29899#pullrequestreview-498028725",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "59cae1b3-7e93-4448-9c8f-d5bf1505e173",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Ideally we should also do the type check. But let me just match it with other window functions for now, and do it separately together with other window functions.",
        "createdAt" : "2020-09-29T02:53:52Z",
        "updatedAt" : "2020-09-29T04:09:24Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "98b04a2546418be49e2a105d8909043bc3113f7b",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +952,956 @@    \"\"\"\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.nth_value(_to_java_column(col), offset, ignoreNulls))\n\n"
  },
  {
    "id" : "9349f75c-87c6-4f94-9900-5da7b8c3cd7b",
    "prId" : 29899,
    "prUrl" : "https://github.com/apache/spark/pull/29899#pullrequestreview-498075659",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "16482747-d29f-444a-a834-25623921e17c",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "`\\\\th`?",
        "createdAt" : "2020-09-29T03:25:53Z",
        "updatedAt" : "2020-09-29T04:09:24Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      },
      {
        "id" : "38d30c22-92b8-47dd-8b6b-0da1f285967f",
        "parentId" : "16482747-d29f-444a-a834-25623921e17c",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Yeah, it should have `\\\\` to distinguish `th` from the previous backquote.",
        "createdAt" : "2020-09-29T04:06:12Z",
        "updatedAt" : "2020-09-29T04:09:24Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "98b04a2546418be49e2a105d8909043bc3113f7b",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +938,942 @@def nth_value(col, offset, ignoreNulls=False):\n    \"\"\"\n    Window function: returns the value that is the `offset`\\\\th row of the window frame\n    (counting from 1), and `null` if the size of window frame is less than `offset` rows.\n"
  },
  {
    "id" : "e7835ea5-11ca-4597-9108-e906491babd6",
    "prId" : 29188,
    "prUrl" : "https://github.com/apache/spark/pull/29188#pullrequestreview-453868345",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "69053ac7-abf9-4e7b-811d-f0fd287fb041",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "This is needed because we're now creating each page for each API. For example, see https://hyukjin-spark.readthedocs.io/en/stable/reference/api/pyspark.sql.functions.to_date.html",
        "createdAt" : "2020-07-23T06:42:41Z",
        "updatedAt" : "2020-07-24T01:12:25Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "d6d011737aba77e7eca41c1b65d6588764856797",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +1197,1201 @@    is omitted. Equivalent to ``col.cast(\"date\")``.\n\n    .. _datetime pattern: https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n\n    >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])"
  },
  {
    "id" : "e8d0190d-a623-4df3-8e14-f726c9a7462d",
    "prId" : 29122,
    "prUrl" : "https://github.com/apache/spark/pull/29122#pullrequestreview-513113743",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "02ab968c-6cdf-4916-ae33-64698f129d99",
        "parentId" : null,
        "authorId" : "981b170c-729a-429c-b115-0350ea50b32b",
        "body" : "Same as for `sql.avro.functions`, we'll need here `Optional[Dict[str, str]]` (applies also to the remaining functions).",
        "createdAt" : "2020-10-20T20:38:12Z",
        "updatedAt" : "2020-12-04T10:23:56Z",
        "lastEditedBy" : "981b170c-729a-429c-b115-0350ea50b32b",
        "tags" : [
        ]
      }
    ],
    "commit" : "11f3790ac3ccc15efd7944e0fc1a3a2e3e8e3247",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +3457,3461 @@\n\ndef from_json(col, schema, options=None):\n    \"\"\"\n    Parses a column containing a JSON string into a :class:`MapType` with :class:`StringType`"
  },
  {
    "id" : "83608055-76b2-4e34-ba1c-054f3867b560",
    "prId" : 29122,
    "prUrl" : "https://github.com/apache/spark/pull/29122#pullrequestreview-537428053",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "afc658f1-d16a-46e0-97c5-d269a73b7492",
        "parentId" : null,
        "authorId" : "981b170c-729a-429c-b115-0350ea50b32b",
        "body" : "Seems like we still have to modify a few annotations, right?\r\n\r\nProbably something like [functions.pyi.patch.txt](https://github.com/apache/spark/files/5583207/functions.pyi.patch.txt)\r\n",
        "createdAt" : "2020-11-23T13:22:25Z",
        "updatedAt" : "2020-12-04T10:23:56Z",
        "lastEditedBy" : "981b170c-729a-429c-b115-0350ea50b32b",
        "tags" : [
        ]
      },
      {
        "id" : "72280984-0531-49bb-9c70-b05c29689720",
        "parentId" : "afc658f1-d16a-46e0-97c5-d269a73b7492",
        "authorId" : "e29a7794-ac2e-4e6f-a690-737e83e1bace",
        "body" : "Good catch, I've added them",
        "createdAt" : "2020-11-24T12:08:19Z",
        "updatedAt" : "2020-12-04T10:23:56Z",
        "lastEditedBy" : "e29a7794-ac2e-4e6f-a690-737e83e1bace",
        "tags" : [
        ]
      },
      {
        "id" : "e2a7eb8a-6ecb-4c40-b216-7522ba125277",
        "parentId" : "afc658f1-d16a-46e0-97c5-d269a73b7492",
        "authorId" : "e29a7794-ac2e-4e6f-a690-737e83e1bace",
        "body" : "Not sure why implicit optional is still allowed ðŸ¤” ",
        "createdAt" : "2020-11-24T12:08:39Z",
        "updatedAt" : "2020-12-04T10:23:56Z",
        "lastEditedBy" : "e29a7794-ac2e-4e6f-a690-737e83e1bace",
        "tags" : [
        ]
      }
    ],
    "commit" : "11f3790ac3ccc15efd7944e0fc1a3a2e3e8e3247",
    "line" : 27,
    "diffHunk" : "@@ -1,1 +3513,3517 @@\n\ndef to_json(col, options=None):\n    \"\"\"\n    Converts a column containing a :class:`StructType`, :class:`ArrayType` or a :class:`MapType`"
  }
]