[
  {
    "id" : "15b1b11e-947e-4fa6-af0a-375cb0d479c7",
    "prId" : 32566,
    "prUrl" : "https://github.com/apache/spark/pull/32566#pullrequestreview-660544882",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "65d4772d-74bd-4cb8-a2ac-a062801c172a",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "```suggestion\r\n    Examples\r\n    --------\r\n```",
        "createdAt" : "2021-05-17T04:40:14Z",
        "updatedAt" : "2021-05-17T04:44:05Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "3b05e99de665e785ad05a37717d3d2f761cce30a",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +2698,2702 @@        a country of the locale\n\n    Examples\n    --------\n    >>> df = spark.createDataFrame([[\"This is an example sentence.\"]], [\"string\"])"
  },
  {
    "id" : "6e9d8983-ad31-482a-b1fd-a171bca187aa",
    "prId" : 32566,
    "prUrl" : "https://github.com/apache/spark/pull/32566#pullrequestreview-660544882",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6e28cb60-238e-48be-980b-119141fdd36c",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Can we also add:\r\n\r\n```\r\n    Parameters\r\n    ----------\r\n    str: ...\r\n```\r\n\r\n(see also https://numpydoc.readthedocs.io/en/latest/format.html)",
        "createdAt" : "2021-05-17T04:40:27Z",
        "updatedAt" : "2021-05-17T04:44:05Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "3b05e99de665e785ad05a37717d3d2f761cce30a",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +2688,2692 @@\n    .. versionadded:: 3.2.0\n\n    Parameters\n    ----------"
  },
  {
    "id" : "27182c9c-1993-44cb-872d-331a3ad835f8",
    "prId" : 31207,
    "prUrl" : "https://github.com/apache/spark/pull/31207#pullrequestreview-578361157",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6d839666-65be-490e-a00f-cc3b98360e45",
        "parentId" : null,
        "authorId" : "eb588fd2-21dd-4dea-a8f9-bcf2e48148eb",
        "body" : "Worth mentioning the newly supported types in the docstring",
        "createdAt" : "2021-01-26T22:26:37Z",
        "updatedAt" : "2021-01-29T13:23:21Z",
        "lastEditedBy" : "eb588fd2-21dd-4dea-a8f9-bcf2e48148eb",
        "tags" : [
        ]
      },
      {
        "id" : "fd5ee94c-ef81-4e32-bc02-db32937f5c6d",
        "parentId" : "6d839666-65be-490e-a00f-cc3b98360e45",
        "authorId" : "981b170c-729a-429c-b115-0350ea50b32b",
        "body" : "Added a short description of the input parameters @LielBach. ",
        "createdAt" : "2021-01-28T14:02:12Z",
        "updatedAt" : "2021-01-29T13:23:21Z",
        "lastEditedBy" : "981b170c-729a-429c-b115-0350ea50b32b",
        "tags" : [
        ]
      }
    ],
    "commit" : "dade2b17152baf3f02553060bc8bbdbf96744756",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +90,94 @@def lit(col):\n    \"\"\"\n    Creates a :class:`Column` of literal value.\n\n    .. versionadded:: 1.3.0"
  },
  {
    "id" : "7938009a-a1a8-4adf-962e-e19495a288a1",
    "prId" : 31207,
    "prUrl" : "https://github.com/apache/spark/pull/31207#pullrequestreview-578996182",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f739d86d-4a50-4e15-b499-e09b4e0c0164",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "One thing that makes me worry is this isn't technically a literal ... although it's foldable. The results and optimization might be differently applied compared to Scala side because they are not literals. For example, we have some optimization rules that only applies to literals here and there.\r\n\r\nAlso looks like Scala side doesn't support map and struct (due to both the limitation of typing and the fact that `Row` doesn't hold field names, etc.).",
        "createdAt" : "2021-01-27T04:54:27Z",
        "updatedAt" : "2021-01-29T13:23:21Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "f9b172ae-88bb-4669-be96-bb9965e1d05b",
        "parentId" : "f739d86d-4a50-4e15-b499-e09b4e0c0164",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Hm, okay but these will be literals after constant folding though. @cloud-fan, WDYT about supporting complex type at literals in this way? I think we can do similar thing in Scala side as well to allow map and struct literals.",
        "createdAt" : "2021-01-27T05:03:55Z",
        "updatedAt" : "2021-01-29T13:23:21Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "108fc6a3-22ac-4f39-aa6a-fd855135703e",
        "parentId" : "f739d86d-4a50-4e15-b499-e09b4e0c0164",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "cc @viirya too",
        "createdAt" : "2021-01-27T05:04:54Z",
        "updatedAt" : "2021-01-29T13:23:21Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "b9ad4387-11bf-4920-9e3b-4ebe513ca226",
        "parentId" : "f739d86d-4a50-4e15-b499-e09b4e0c0164",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Seems okay because there is constant folding optimization. So if they are foldable, they will be optimized as literals during query optimization.",
        "createdAt" : "2021-01-27T06:15:09Z",
        "updatedAt" : "2021-01-29T13:23:21Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "ced0a754-a7bc-4eec-9625-a82bf3690190",
        "parentId" : "f739d86d-4a50-4e15-b499-e09b4e0c0164",
        "authorId" : "981b170c-729a-429c-b115-0350ea50b32b",
        "body" : "> Also looks like Scala side doesn't support map and struct (due to both the limitation of typing and the fact that `Row` doesn't hold field names, etc.).\r\n\r\nActually, `typedLit` supports `Map` objects:\r\n\r\n```scala\r\nscala> typedLit(Map(\"a\" -> 1, \"b\" -> 2))\r\nres7: org.apache.spark.sql.Column = keys: [a,b], values: [1,2]\r\n```\r\n\r\nand `Products`\r\n\r\n```scala\r\nscala> case class FooBar(foo: Int, bar: String)\r\ndefined class FooBar\r\n\r\nscala> typedLit(FooBar(1, \"a\"))\r\nres8: org.apache.spark.sql.Column = [1,a]\r\n```\r\n\r\n`Row` would be specific to PySpark, but since it is just a tuple, it seemed reasonable to handle it with names.\r\n\r\n> Hm, okay but these will be literals after constant folding though.\r\n\r\nThe biggest difference compared to Scala is that it we handle mixed constants and column objects:\r\n\r\n```python\r\n>>> spark.createDataFrame([(1, 2)], (\"a\", \"b\")).select(lit({\"a\": col(\"a\"), \"b\": col(\"b\")})).show()      \r\n+----------------+\r\n| map(a, a, b, b)|\r\n+----------------+\r\n|{a -> 1, b -> 2}|\r\n+----------------+\r\n```\r\n\r\nSo there is a case where `lit` results in a non-`Literal`.\r\n\r\n\r\n",
        "createdAt" : "2021-01-27T14:09:55Z",
        "updatedAt" : "2021-01-29T13:23:21Z",
        "lastEditedBy" : "981b170c-729a-429c-b115-0350ea50b32b",
        "tags" : [
        ]
      },
      {
        "id" : "94ddad4e-b9e0-4598-b438-f5504ea991df",
        "parentId" : "f739d86d-4a50-4e15-b499-e09b4e0c0164",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "It seems fine, but we might be able to convert these exprs for complex types into literals in advance via `PythonSQLUtils`.",
        "createdAt" : "2021-01-29T07:24:08Z",
        "updatedAt" : "2021-01-29T13:23:21Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "dade2b17152baf3f02553060bc8bbdbf96744756",
    "line" : 53,
    "diffHunk" : "@@ -1,1 +132,136 @@        )\n\n        return struct(*[\n            lit(x).alias(a)\n            for x, a in zip(col, fields)"
  },
  {
    "id" : "fa4df137-eaa9-451e-a6c3-c0d314d6d669",
    "prId" : 31207,
    "prUrl" : "https://github.com/apache/spark/pull/31207#pullrequestreview-579964625",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ca5e55c9-b09a-4eae-a8ac-daf79a043819",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "\"Creates a :class:`Column` of literal value.\"?\r\n\r\nhttps://github.com/apache/spark/blob/72b7f8abfb60d0008f1f9bed94ce1c367a7d7cce/sql/core/src/main/scala/org/apache/spark/sql/functions.scala#L105",
        "createdAt" : "2021-01-29T04:18:08Z",
        "updatedAt" : "2021-01-29T13:23:21Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "33aa20cb-3987-486e-a97e-f52c9359b28b",
        "parentId" : "ca5e55c9-b09a-4eae-a8ac-daf79a043819",
        "authorId" : "981b170c-729a-429c-b115-0350ea50b32b",
        "body" : "I am not sure if I understand what you mean. We already have matching sentence in the description block\r\n\r\nhttps://github.com/apache/spark/blob/6198e0aca0d30d6bd6ba9f29a0fe60a62016caed/python/pyspark/sql/functions.py#L92\r\n\r\nCould you clarify please?",
        "createdAt" : "2021-01-29T13:26:35Z",
        "updatedAt" : "2021-01-29T13:26:35Z",
        "lastEditedBy" : "981b170c-729a-429c-b115-0350ea50b32b",
        "tags" : [
        ]
      },
      {
        "id" : "781168ab-0fc8-4a46-bb4c-8b78cf8f294e",
        "parentId" : "ca5e55c9-b09a-4eae-a8ac-daf79a043819",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Ah, nvm. I missed it.",
        "createdAt" : "2021-02-01T01:31:14Z",
        "updatedAt" : "2021-02-01T01:31:14Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "dade2b17152baf3f02553060bc8bbdbf96744756",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +99,103 @@    ----------\n    col : bool, float, int, str, datetime.date, datetime.datetime, dict, list, tuple\n        Object to be converted into :class:`Column`.\n\n        If it is a collection, conversion will be applied recursively. In such case,"
  },
  {
    "id" : "6820c7af-44ad-402e-a0dd-2b9b420f7637",
    "prId" : 31207,
    "prUrl" : "https://github.com/apache/spark/pull/31207#pullrequestreview-579854094",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3b6dd29f-9397-4cfd-91ba-00eca1cb83aa",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@zero323, can we implement these logics in Scala side, and we simply call `lit` here? I think that's more what @maropu meant.\r\n\r\nI think the only special case we need to implement in Python side is the struct case because 1. `Row` class cannot be passed to JVM side and 2. both `list` and `tuple` are converted into `ArrayList` so JVM cannot tell the origin. We will probably have to add some comments on that case.",
        "createdAt" : "2021-01-31T04:47:50Z",
        "updatedAt" : "2021-01-31T04:47:50Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "dade2b17152baf3f02553060bc8bbdbf96744756",
    "line" : 43,
    "diffHunk" : "@@ -1,1 +122,126 @@        return array(*[lit(x) for x in col])\n\n    elif isinstance(col, tuple):\n        fields = (\n            # Named tuple"
  },
  {
    "id" : "c3ebcc66-9c3a-4817-b331-4c5365239eb8",
    "prId" : 31207,
    "prUrl" : "https://github.com/apache/spark/pull/31207#pullrequestreview-579976168",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d88d7f2c-d3eb-4dae-b3fe-bcda44fdd114",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "One thing I am a bit worried is that this will lazily check the type (with type coercion) unlike Scala side ...",
        "createdAt" : "2021-02-01T02:24:23Z",
        "updatedAt" : "2021-02-01T02:24:24Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "dade2b17152baf3f02553060bc8bbdbf96744756",
    "line" : 41,
    "diffHunk" : "@@ -1,1 +120,124 @@\n    elif isinstance(col, list):\n        return array(*[lit(x) for x in col])\n\n    elif isinstance(col, tuple):"
  },
  {
    "id" : "7197c45c-7aaa-4ef7-9323-481684fbdfb7",
    "prId" : 30745,
    "prUrl" : "https://github.com/apache/spark/pull/30745#pullrequestreview-550915908",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "74206255-79ab-4a68-89b9-acfd8e89cf9b",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Let's also add some doctests here.",
        "createdAt" : "2020-12-13T02:17:42Z",
        "updatedAt" : "2021-03-01T17:18:18Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "da436b34-3f09-414a-96ac-e2980ee8173a",
        "parentId" : "74206255-79ab-4a68-89b9-acfd8e89cf9b",
        "authorId" : "dc7a33cf-bba6-4512-a5a1-248c3cac6eae",
        "body" : "I agree that having some documentation, with examples, would be very helpful and I've added these following your advice. However, it seems like the automated build tests are insanely restrictive in their validation of these doctests, causing multiple building failures when the pydoc examples differ by inconsequential whitespace or formatting changes. For example, even when I have directly copy-pasted example outputs from a pyspark session, these were rejected by the build-system.\r\n\r\nCan I suggest that someone investigates whether the build-system can make these documentation checks more tolerant of whitespace variations, which I'd expect will trip-up many developers and will create obstacles for producing documentation that would benefit Spark users?",
        "createdAt" : "2020-12-13T12:32:35Z",
        "updatedAt" : "2021-03-01T17:18:18Z",
        "lastEditedBy" : "dc7a33cf-bba6-4512-a5a1-248c3cac6eae",
        "tags" : [
        ]
      }
    ],
    "commit" : "1cdabcbdab2d54c67f09139f3b6cea1b2c09e04e",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +247,251 @@    +----+-------+\n\n    \"\"\"\n    return _invoke_function_over_column(\"product\", col)\n"
  },
  {
    "id" : "1ba3b16c-1115-4062-bd27-2bd1c3f181fc",
    "prId" : 30501,
    "prUrl" : "https://github.com/apache/spark/pull/30501#pullrequestreview-538933079",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e4a55b5a-2f0e-43b9-8a8b-041f9cca0317",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Let's also update `spark/python/docs/source/reference/pyspark.sql.rst`",
        "createdAt" : "2020-11-26T01:17:05Z",
        "updatedAt" : "2020-11-26T11:01:55Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "17451c7a0bb58cd5e4f9ee29e62370f2208726d3",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +221,225 @@\n\ndef acosh(col):\n    \"\"\"\n    Computes inverse hyperbolic cosine of the input column."
  },
  {
    "id" : "9a861d58-a2b0-4d24-b123-114a2d553bd4",
    "prId" : 30201,
    "prUrl" : "https://github.com/apache/spark/pull/30201#pullrequestreview-521211584",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2da07c8d-c79a-4c3a-9cc7-cc25aeb4ae6d",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "```suggestion\r\n    >>> schema =  (\r\n    ...     '{\"type\" : \"map\", \"keyType\" : \"string\",'\r\n    ...     '\"valueType\" : \"integer\", \"valueContainsNull\" : true}')\r\n    >>> df.select(from_json(df.value, schema).alias(\"json\")).collect()\r\n    [Row(json={'a': 1})]\r\n```",
        "createdAt" : "2020-11-01T10:34:07Z",
        "updatedAt" : "2020-11-01T10:34:08Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "88cd2a3354e587861f3214b94d0cadc8bc163747",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +2864,2868 @@     '{\"type\" : \"map\", \"keyType\" : \"string\", \"valueType\" : \"integer\", \"valueContainsNull\" : true}')\\\n     .alias(\"json\")).collect()\n    [Row(json={'a': 1})]\n    >>> data = [(1, '''[{\"a\": 1}]''')]\n    >>> schema = ArrayType(StructType([StructField(\"a\", IntegerType())]))"
  },
  {
    "id" : "15a96970-9c56-4eda-ae39-b1addaa9fb00",
    "prId" : 30181,
    "prUrl" : "https://github.com/apache/spark/pull/30181#pullrequestreview-519650717",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8d666f11-87b5-4a7e-a486-dee7be306173",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "There are a bit of trivial changes in the doc to make it more sense with the new format. One example is here.",
        "createdAt" : "2020-10-29T13:15:48Z",
        "updatedAt" : "2020-11-03T00:49:21Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "40bc04996ab651411656c08849b5ec0495cd8468",
    "line" : 786,
    "diffHunk" : "@@ -1,1 +1277,1281 @@    ----------\n    cols : list, set, str or :class:`Column`\n        column names or :class:`Column`\\\\s to contain in the output struct.\n\n    Examples"
  },
  {
    "id" : "49112491-aef9-49be-ac15-ab26746d2504",
    "prId" : 30181,
    "prUrl" : "https://github.com/apache/spark/pull/30181#pullrequestreview-520264562",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f3e6bbd4-0de7-4813-bbd9-1ce30619c297",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "1.4.0?",
        "createdAt" : "2020-10-30T01:22:31Z",
        "updatedAt" : "2020-11-03T00:49:21Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "40bc04996ab651411656c08849b5ec0495cd8468",
    "line" : 38,
    "diffHunk" : "@@ -1,1 +217,221 @@def asin(col):\n    \"\"\"\n    .. versionadded:: 1.3.0\n\n"
  },
  {
    "id" : "171795c9-1640-4b99-a02a-23f1239bc29d",
    "prId" : 30181,
    "prUrl" : "https://github.com/apache/spark/pull/30181#pullrequestreview-520393963",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5c0bb303-365b-46a5-8905-c172cf0ca4ef",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Not to change to `.. versionadded::`?",
        "createdAt" : "2020-10-30T01:24:27Z",
        "updatedAt" : "2020-11-03T00:49:21Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "1d68f281-9576-459a-bbfa-6ab3c2e5b2d5",
        "parentId" : "5c0bb303-365b-46a5-8905-c172cf0ca4ef",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Oh yeah. This one no need to change because `.. deprecated::` is still [reST style directive](https://www.sphinx-doc.org/en/master/usage/restructuredtext/directives.html). So, this has no problem about formatting error because it does not use NumPy documentation specific syntax.",
        "createdAt" : "2020-10-30T05:59:47Z",
        "updatedAt" : "2020-11-03T00:49:21Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "40bc04996ab651411656c08849b5ec0495cd8468",
    "line" : 192,
    "diffHunk" : "@@ -1,1 +424,428 @@\n\n@since(1.4)\ndef toDegrees(col):\n    \"\"\""
  },
  {
    "id" : "d0f4e33c-3e90-465e-aa7a-24d0f2c47ee2",
    "prId" : 30181,
    "prUrl" : "https://github.com/apache/spark/pull/30181#pullrequestreview-520264562",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "553d1a47-a6c3-4b9e-b607-05204b9ce76b",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "ditto",
        "createdAt" : "2020-10-30T01:24:37Z",
        "updatedAt" : "2020-11-03T00:49:21Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "40bc04996ab651411656c08849b5ec0495cd8468",
    "line" : 202,
    "diffHunk" : "@@ -1,1 +434,438 @@\n\n@since(1.4)\ndef toRadians(col):\n    \"\"\""
  },
  {
    "id" : "914f0863-ad32-41ba-bc37-9c919e4734c5",
    "prId" : 30181,
    "prUrl" : "https://github.com/apache/spark/pull/30181#pullrequestreview-520264562",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c1da1fce-c398-46cd-9886-4ceffa1092cc",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "this?",
        "createdAt" : "2020-10-30T01:25:35Z",
        "updatedAt" : "2020-11-03T00:49:21Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "40bc04996ab651411656c08849b5ec0495cd8468",
    "line" : 338,
    "diffHunk" : "@@ -1,1 +731,735 @@\n\n@since(1.3)\ndef approxCountDistinct(col, rsd=None):\n    \"\"\""
  },
  {
    "id" : "eee94655-e73e-45b6-9132-703ffac3bbcc",
    "prId" : 30143,
    "prUrl" : "https://github.com/apache/spark/pull/30143#pullrequestreview-517512188",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4d1d5434-8d55-4608-b1c1-453478949a44",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@zero323, looks like we should call `col.asc if isinstance(col, Column) else _invoke_function(\"asc\", col)` instead to make it support both string and columns in PySpark side (Scala side does not have the Column signature).\r\n\r\nIt was decided to support both string/column instances where we can in PySpark at SPARK-26979.\r\n\r\nIdeally we should do this in a separate JIRA and PR but I don't mind fixing it here while we're here. I will leave it to you.",
        "createdAt" : "2020-10-27T01:44:36Z",
        "updatedAt" : "2020-10-27T02:05:28Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "21a83119-3276-4e19-b391-c50b38e4e503",
        "parentId" : "4d1d5434-8d55-4608-b1c1-453478949a44",
        "authorId" : "981b170c-729a-429c-b115-0350ea50b32b",
        "body" : "I wonder if it makes more sense, for the sake of consistency ,to add `Column` signatures on Scala side. ",
        "createdAt" : "2020-10-27T08:14:04Z",
        "updatedAt" : "2020-10-27T08:14:05Z",
        "lastEditedBy" : "981b170c-729a-429c-b115-0350ea50b32b",
        "tags" : [
        ]
      },
      {
        "id" : "bb07998a-15e7-4cd1-9ba5-3ce5109a7191",
        "parentId" : "4d1d5434-8d55-4608-b1c1-453478949a44",
        "authorId" : "981b170c-729a-429c-b115-0350ea50b32b",
        "body" : "Let's handle this in SPARK-33257",
        "createdAt" : "2020-10-27T09:44:35Z",
        "updatedAt" : "2020-10-27T09:44:36Z",
        "lastEditedBy" : "981b170c-729a-429c-b115-0350ea50b32b",
        "tags" : [
        ]
      }
    ],
    "commit" : "2c99821f3c8630ece8c2cc149ca43667a4be8b52",
    "line" : 218,
    "diffHunk" : "@@ -1,1 +117,121 @@    Returns a sort expression based on the ascending order of the given column name.\n    \"\"\"\n    return _invoke_function(\"asc\", col)\n\n"
  },
  {
    "id" : "937e5049-c38b-44c2-b657-009e0d8481d0",
    "prId" : 30143,
    "prUrl" : "https://github.com/apache/spark/pull/30143#pullrequestreview-517288922",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d953e129-c367-435e-975e-7554e3623bb4",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "This one would have to be same as `asc` above.",
        "createdAt" : "2020-10-27T01:47:38Z",
        "updatedAt" : "2020-10-27T02:05:28Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "2c99821f3c8630ece8c2cc149ca43667a4be8b52",
    "line" : 226,
    "diffHunk" : "@@ -1,1 +125,129 @@    Returns a sort expression based on the descending order of the given column name.\n    \"\"\"\n    return _invoke_function(\"desc\", col)\n\n"
  },
  {
    "id" : "82a890ce-1e66-4a19-b756-df6d82913d9d",
    "prId" : 30143,
    "prUrl" : "https://github.com/apache/spark/pull/30143#pullrequestreview-517288922",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "25c21f71-de4b-483c-9ef2-ad85cad732f6",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "`asc_nulls_first`, `asc_nulls_last`, `desc_nulls_first` and `desc_nulls_last` look the same case above.",
        "createdAt" : "2020-10-27T01:54:24Z",
        "updatedAt" : "2020-10-27T02:05:29Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "2c99821f3c8630ece8c2cc149ca43667a4be8b52",
    "line" : 494,
    "diffHunk" : "@@ -1,1 +393,397 @@    column name, and null values return before non-null values.\n    \"\"\"\n    return _invoke_function(\"asc_nulls_first\", col)\n\n"
  },
  {
    "id" : "5154cda2-4198-4fdc-9b4d-3dc0bbd2a5a5",
    "prId" : 30143,
    "prUrl" : "https://github.com/apache/spark/pull/30143#pullrequestreview-517288922",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b5bc3f74-a0e6-42c5-9789-39882d8deb93",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Much more neat!",
        "createdAt" : "2020-10-27T01:58:49Z",
        "updatedAt" : "2020-10-27T02:05:29Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "2c99821f3c8630ece8c2cc149ca43667a4be8b52",
    "line" : 109,
    "diffHunk" : "@@ -1,1 +77,81 @@        # if they are not columns or strings.\n        _to_java_column(col1) if isinstance(col1, (str, Column)) else float(col1),\n        _to_java_column(col2) if isinstance(col2, (str, Column)) else float(col2)\n    )\n"
  },
  {
    "id" : "057d7a7f-ed90-422b-831c-81bb41621801",
    "prId" : 29947,
    "prUrl" : "https://github.com/apache/spark/pull/29947#pullrequestreview-503467714",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "47ae8080-a046-4799-9fba-1555223a6837",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Now we should also add it into type hints and documentations at https://github.com/apache/spark/blob/master/python/docs/source/reference/pyspark.sql.rst#functions and https://github.com/apache/spark/blob/master/python/pyspark/sql/functions.pyi. cc @zero323 FYI\r\n\r\n",
        "createdAt" : "2020-10-07T01:38:56Z",
        "updatedAt" : "2020-10-08T03:03:41Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "5b93fd49313152356a3fd2baf6c21123170c4037",
    "line" : 3,
    "diffHunk" : "@@ -1,1 +1592,1596 @@    return Column(jc)\n\n\n@since(3.1)\ndef assert_true(col, errMsg=None):"
  },
  {
    "id" : "e15dcb42-173c-4712-bf50-b41e409023e2",
    "prId" : 29899,
    "prUrl" : "https://github.com/apache/spark/pull/29899#pullrequestreview-498028725",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "59cae1b3-7e93-4448-9c8f-d5bf1505e173",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Ideally we should also do the type check. But let me just match it with other window functions for now, and do it separately together with other window functions.",
        "createdAt" : "2020-09-29T02:53:52Z",
        "updatedAt" : "2020-09-29T04:09:24Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "98b04a2546418be49e2a105d8909043bc3113f7b",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +952,956 @@    \"\"\"\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.nth_value(_to_java_column(col), offset, ignoreNulls))\n\n"
  },
  {
    "id" : "9349f75c-87c6-4f94-9900-5da7b8c3cd7b",
    "prId" : 29899,
    "prUrl" : "https://github.com/apache/spark/pull/29899#pullrequestreview-498075659",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "16482747-d29f-444a-a834-25623921e17c",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "`\\\\th`?",
        "createdAt" : "2020-09-29T03:25:53Z",
        "updatedAt" : "2020-09-29T04:09:24Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      },
      {
        "id" : "38d30c22-92b8-47dd-8b6b-0da1f285967f",
        "parentId" : "16482747-d29f-444a-a834-25623921e17c",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Yeah, it should have `\\\\` to distinguish `th` from the previous backquote.",
        "createdAt" : "2020-09-29T04:06:12Z",
        "updatedAt" : "2020-09-29T04:09:24Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "98b04a2546418be49e2a105d8909043bc3113f7b",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +938,942 @@def nth_value(col, offset, ignoreNulls=False):\n    \"\"\"\n    Window function: returns the value that is the `offset`\\\\th row of the window frame\n    (counting from 1), and `null` if the size of window frame is less than `offset` rows.\n"
  },
  {
    "id" : "e7835ea5-11ca-4597-9108-e906491babd6",
    "prId" : 29188,
    "prUrl" : "https://github.com/apache/spark/pull/29188#pullrequestreview-453868345",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "69053ac7-abf9-4e7b-811d-f0fd287fb041",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "This is needed because we're now creating each page for each API. For example, see https://hyukjin-spark.readthedocs.io/en/stable/reference/api/pyspark.sql.functions.to_date.html",
        "createdAt" : "2020-07-23T06:42:41Z",
        "updatedAt" : "2020-07-24T01:12:25Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "d6d011737aba77e7eca41c1b65d6588764856797",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +1197,1201 @@    is omitted. Equivalent to ``col.cast(\"date\")``.\n\n    .. _datetime pattern: https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n\n    >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])"
  },
  {
    "id" : "e8d0190d-a623-4df3-8e14-f726c9a7462d",
    "prId" : 29122,
    "prUrl" : "https://github.com/apache/spark/pull/29122#pullrequestreview-513113743",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "02ab968c-6cdf-4916-ae33-64698f129d99",
        "parentId" : null,
        "authorId" : "981b170c-729a-429c-b115-0350ea50b32b",
        "body" : "Same as for `sql.avro.functions`, we'll need here `Optional[Dict[str, str]]` (applies also to the remaining functions).",
        "createdAt" : "2020-10-20T20:38:12Z",
        "updatedAt" : "2020-12-04T10:23:56Z",
        "lastEditedBy" : "981b170c-729a-429c-b115-0350ea50b32b",
        "tags" : [
        ]
      }
    ],
    "commit" : "11f3790ac3ccc15efd7944e0fc1a3a2e3e8e3247",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +3457,3461 @@\n\ndef from_json(col, schema, options=None):\n    \"\"\"\n    Parses a column containing a JSON string into a :class:`MapType` with :class:`StringType`"
  },
  {
    "id" : "83608055-76b2-4e34-ba1c-054f3867b560",
    "prId" : 29122,
    "prUrl" : "https://github.com/apache/spark/pull/29122#pullrequestreview-537428053",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "afc658f1-d16a-46e0-97c5-d269a73b7492",
        "parentId" : null,
        "authorId" : "981b170c-729a-429c-b115-0350ea50b32b",
        "body" : "Seems like we still have to modify a few annotations, right?\r\n\r\nProbably something like [functions.pyi.patch.txt](https://github.com/apache/spark/files/5583207/functions.pyi.patch.txt)\r\n",
        "createdAt" : "2020-11-23T13:22:25Z",
        "updatedAt" : "2020-12-04T10:23:56Z",
        "lastEditedBy" : "981b170c-729a-429c-b115-0350ea50b32b",
        "tags" : [
        ]
      },
      {
        "id" : "72280984-0531-49bb-9c70-b05c29689720",
        "parentId" : "afc658f1-d16a-46e0-97c5-d269a73b7492",
        "authorId" : "e29a7794-ac2e-4e6f-a690-737e83e1bace",
        "body" : "Good catch, I've added them",
        "createdAt" : "2020-11-24T12:08:19Z",
        "updatedAt" : "2020-12-04T10:23:56Z",
        "lastEditedBy" : "e29a7794-ac2e-4e6f-a690-737e83e1bace",
        "tags" : [
        ]
      },
      {
        "id" : "e2a7eb8a-6ecb-4c40-b216-7522ba125277",
        "parentId" : "afc658f1-d16a-46e0-97c5-d269a73b7492",
        "authorId" : "e29a7794-ac2e-4e6f-a690-737e83e1bace",
        "body" : "Not sure why implicit optional is still allowed ðŸ¤” ",
        "createdAt" : "2020-11-24T12:08:39Z",
        "updatedAt" : "2020-12-04T10:23:56Z",
        "lastEditedBy" : "e29a7794-ac2e-4e6f-a690-737e83e1bace",
        "tags" : [
        ]
      }
    ],
    "commit" : "11f3790ac3ccc15efd7944e0fc1a3a2e3e8e3247",
    "line" : 27,
    "diffHunk" : "@@ -1,1 +3513,3517 @@\n\ndef to_json(col, options=None):\n    \"\"\"\n    Converts a column containing a :class:`StructType`, :class:`ArrayType` or a :class:`MapType`"
  },
  {
    "id" : "1657f3c1-2812-426b-9e85-143b6909e235",
    "prId" : 28959,
    "prUrl" : "https://github.com/apache/spark/pull/28959#pullrequestreview-440518051",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "314d7a9e-4218-47cc-8e21-7fb136e2c179",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "do we set the timezone in pyspark doctest, like the scala side test?",
        "createdAt" : "2020-07-01T02:49:04Z",
        "updatedAt" : "2020-07-01T02:49:04Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "a0d38dfe-056b-43c7-8e0f-e4ab769d4e4c",
        "parentId" : "314d7a9e-4218-47cc-8e21-7fb136e2c179",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I'm wondering if this is the common approach we do for all timestamp related pyspark doc tests.",
        "createdAt" : "2020-07-01T02:49:47Z",
        "updatedAt" : "2020-07-01T02:49:48Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "b3789721-0f0e-4db2-ab44-cd4dacd1dcca",
        "parentId" : "314d7a9e-4218-47cc-8e21-7fb136e2c179",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Yeah, we can do that too. Let's set it when we face another similar issue. It's not difficult to do it.",
        "createdAt" : "2020-07-01T03:12:12Z",
        "updatedAt" : "2020-07-01T03:12:12Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "aa6a0e110d0759a1030439c7d6a83480f3503cac",
    "line" : 2,
    "diffHunk" : "@@ -1,1 +1432,1436 @@    \"\"\"\n    >>> from pyspark.sql.functions import timestamp_seconds\n    >>> spark.conf.set(\"spark.sql.session.timeZone\", \"America/Los_Angeles\")\n    >>> time_df = spark.createDataFrame([(1230219000,)], ['unix_time'])\n    >>> time_df.select(timestamp_seconds(time_df.unix_time).alias('ts')).show()"
  },
  {
    "id" : "6228d85a-fd49-49d2-ad61-4679a549677b",
    "prId" : 28593,
    "prUrl" : "https://github.com/apache/spark/pull/28593#pullrequestreview-427898701",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "280e639b-f933-4fd3-b775-2f95cf2a966a",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I vaguely remember that pyspark can access scala SQL functions automatically. cc @HyukjinKwon ",
        "createdAt" : "2020-06-08T12:24:13Z",
        "updatedAt" : "2020-06-15T17:23:35Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "ace5b849-d3df-4c67-be38-f9724385eff1",
        "parentId" : "280e639b-f933-4fd3-b775-2f95cf2a966a",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "There are two (automatic-ish?) ways:\r\n- Using `expr(...)`. I think we usually recommend this way when the SQL functions are not existent in `functions.scala`.\r\n- We can simply add this into [this dictionary](https://github.com/apache/spark/blob/c7f2a9b323c5354c5dab1354c9a9bda19274dcdc/python/pyspark/sql/functions.py#L130-L136). This way is currently kind of discouraged. I was discussed in the mailing list before, and we should convert that dictionary into each function definition for better static analysis and IDE support.",
        "createdAt" : "2020-06-10T09:18:43Z",
        "updatedAt" : "2020-06-15T17:23:35Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "f190f0f5-b4c6-48e4-89a3-487cbb488395",
        "parentId" : "280e639b-f933-4fd3-b775-2f95cf2a966a",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Since we're here, how common is the function? We shouldn't add if it's expected to be less common per:\r\nhttps://github.com/apache/spark/blob/df2a1fe131476aac128d63df9b06ec4bca0c2c07/sql/core/src/main/scala/org/apache/spark/sql/functions.scala#L45\r\n",
        "createdAt" : "2020-06-10T09:24:54Z",
        "updatedAt" : "2020-06-15T17:23:36Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "32e52bbc-797e-4fd3-994d-59d20dbb35d4",
        "parentId" : "280e639b-f933-4fd3-b775-2f95cf2a966a",
        "authorId" : "a67e5600-3f69-4b96-9c53-73795f48a0f6",
        "body" : "@HyukjinKwon I just count the usage,there are 47 places where the function was called. ",
        "createdAt" : "2020-06-10T09:37:19Z",
        "updatedAt" : "2020-06-15T17:23:36Z",
        "lastEditedBy" : "a67e5600-3f69-4b96-9c53-73795f48a0f6",
        "tags" : [
        ]
      },
      {
        "id" : "2a18ba73-b6f2-4f44-a03a-712dc4dc6ede",
        "parentId" : "280e639b-f933-4fd3-b775-2f95cf2a966a",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I see, so we need this func here.",
        "createdAt" : "2020-06-10T09:55:25Z",
        "updatedAt" : "2020-06-15T17:23:36Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "12b42396f058569354040d466962904794fa5c5e",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1429,1433 @@\n@since(3.1)\ndef timestamp_seconds(col):\n    \"\"\"\n    >>> from pyspark.sql.functions import timestamp_seconds"
  },
  {
    "id" : "cb96c4ce-909b-45f9-abe6-445903414f50",
    "prId" : 28593,
    "prUrl" : "https://github.com/apache/spark/pull/28593#pullrequestreview-438118418",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c3ae5981-6327-403e-bb26-4e2b05e26f65",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Can you set the session timezone? It caused SPARK-32088",
        "createdAt" : "2020-06-26T07:50:53Z",
        "updatedAt" : "2020-06-26T07:50:53Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "ad091ba3-8b96-4131-9983-f42a5163fe58",
        "parentId" : "c3ae5981-6327-403e-bb26-4e2b05e26f65",
        "authorId" : "a67e5600-3f69-4b96-9c53-73795f48a0f6",
        "body" : "Thanks HyukjinKwon, will fix soon",
        "createdAt" : "2020-06-26T08:42:07Z",
        "updatedAt" : "2020-06-26T08:42:07Z",
        "lastEditedBy" : "a67e5600-3f69-4b96-9c53-73795f48a0f6",
        "tags" : [
        ]
      }
    ],
    "commit" : "12b42396f058569354040d466962904794fa5c5e",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +1431,1435 @@def timestamp_seconds(col):\n    \"\"\"\n    >>> from pyspark.sql.functions import timestamp_seconds\n    >>> time_df = spark.createDataFrame([(1230219000,)], ['unix_time'])\n    >>> time_df.select(timestamp_seconds(time_df.unix_time).alias('ts')).collect()"
  },
  {
    "id" : "4db7dd8a-5085-4592-8020-d7ed0a517797",
    "prId" : 27406,
    "prUrl" : "https://github.com/apache/spark/pull/27406#pullrequestreview-351094602",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d89578e4-1ce0-477a-9864-966c304c3ac5",
        "parentId" : null,
        "authorId" : "981b170c-729a-429c-b115-0350ea50b32b",
        "body" : "If SPARK-27052 is won't fix, we could be more proactive and inspect the closure here:\r\n\r\n```python\r\nimport inspect\r\nfrom itertools import chain\r\nfrom pyspark.sql.udf import UserDefinedFunction\r\n\r\n\r\ndef _is_udf(f):\r\n    return isinstance(f, UserDefinedFunction) or (\r\n        hasattr(f, \"returnType\")\r\n        and hasattr(f, \"evalType\")\r\n        and f.__closure__\r\n        and isinstance(f.__closure__[0].cell_contents, UserDefinedFunction)\r\n    )\r\n\r\n\r\ndef _check_conatins_udf(f):\r\n    closurevars = inspect.getclosurevars(f)\r\n    for name, f in chain.from_iterable(\r\n        [closurevars.nonlocals.items(), closurevars.globals.items()]\r\n    ):\r\n        if _is_udf(f):\r\n\r\n            raise ValueError(\r\n                \"Higher order functions cannot use UserDefinedFunctions \",\r\n                \"Detected following udf {}: {}\".format(name, f),\r\n            )\r\n```\r\n\r\nor throw an exception at the analyzer level",
        "createdAt" : "2020-01-30T19:45:50Z",
        "updatedAt" : "2020-02-04T01:19:26Z",
        "lastEditedBy" : "981b170c-729a-429c-b115-0350ea50b32b",
        "tags" : [
        ]
      }
    ],
    "commit" : "05741c8422364a77389d13ba0a85b7ad92392be6",
    "line" : 139,
    "diffHunk" : "@@ -1,1 +2976,2980 @@        :py:mod:`pyspark.sql.functions` and Scala ``UserDefinedFunctions``.\n        Python ``UserDefinedFunctions`` are not supported\n        (`SPARK-27052 <https://issues.apache.org/jira/browse/SPARK-27052>`__).\n\n    :return: a :class:`pyspark.sql.Column`"
  },
  {
    "id" : "bb26be34-efa7-4332-a01a-33dde04f4690",
    "prId" : 27406,
    "prUrl" : "https://github.com/apache/spark/pull/27406#pullrequestreview-351115778",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e5fb741c-f306-4f39-9aef-7c0ac08aa0cd",
        "parentId" : null,
        "authorId" : "fd6ebc48-7da4-490b-8d41-2e8530d92720",
        "body" : "Just curious: Do we have doc tests, and do they import all the functions for this to work?",
        "createdAt" : "2020-01-30T19:52:04Z",
        "updatedAt" : "2020-02-04T01:19:26Z",
        "lastEditedBy" : "fd6ebc48-7da4-490b-8d41-2e8530d92720",
        "tags" : [
        ]
      },
      {
        "id" : "36d71279-a07d-43a7-ad75-cce765cafd20",
        "parentId" : "e5fb741c-f306-4f39-9aef-7c0ac08aa0cd",
        "authorId" : "981b170c-729a-429c-b115-0350ea50b32b",
        "body" : "We do. As far as I remember all modules invoke doctests when executed as `__main__`. He's relevant part of `functions.py`:\r\n\r\nhttps://github.com/apache/spark/blob/f59685acaa3e9c227f14fe4d8f9e94a1ac664b05/python/pyspark/sql/functions.py#L2939-L2961",
        "createdAt" : "2020-01-30T20:21:31Z",
        "updatedAt" : "2020-02-04T01:19:26Z",
        "lastEditedBy" : "981b170c-729a-429c-b115-0350ea50b32b",
        "tags" : [
        ]
      }
    ],
    "commit" : "05741c8422364a77389d13ba0a85b7ad92392be6",
    "line" : 242,
    "diffHunk" : "@@ -1,1 +3079,3083 @@    ... )\n    >>> def after_second_quarter(x):\n    ...     return month(to_date(x)) > 6\n    >>> df.select(\n    ...     filter(\"values\", after_second_quarter).alias(\"after_second_quarter\")"
  },
  {
    "id" : "46c794e3-7bea-4fec-824b-1492ac93ba5a",
    "prId" : 27406,
    "prUrl" : "https://github.com/apache/spark/pull/27406#pullrequestreview-352197991",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4bf8e763-8ec3-4d49-86de-62ab626d5268",
        "parentId" : null,
        "authorId" : "981b170c-729a-429c-b115-0350ea50b32b",
        "body" : "Now, when this targets 3.1, this shouldn't be necessary anymore, though we still have to wait for official resolution of SPARK-29909.",
        "createdAt" : "2020-02-03T11:49:17Z",
        "updatedAt" : "2020-02-04T01:19:27Z",
        "lastEditedBy" : "981b170c-729a-429c-b115-0350ea50b32b",
        "tags" : [
        ]
      }
    ],
    "commit" : "05741c8422364a77389d13ba0a85b7ad92392be6",
    "line" : 53,
    "diffHunk" : "@@ -1,1 +2890,2894 @@\n\ndef _get_lambda_parameters_legacy(f):\n    # TODO (SPARK-29909) Remove once 2.7 support is dropped\n    import inspect"
  },
  {
    "id" : "46cc3a32-1cbb-4f70-bc3b-fc643cd4768a",
    "prId" : 27406,
    "prUrl" : "https://github.com/apache/spark/pull/27406#pullrequestreview-355156172",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "173d8d34-7305-437b-a02e-45f310904069",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@zero323, can we remove `_get_lambda_parameters` and `_get_lambda_parameters_legacy` here too for now? Let's discuss and figure out a better way in the next PR about this.",
        "createdAt" : "2020-02-06T03:42:15Z",
        "updatedAt" : "2020-02-06T03:42:15Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "0320e706-7f3d-4b40-a8a7-ec17534dc545",
        "parentId" : "173d8d34-7305-437b-a02e-45f310904069",
        "authorId" : "981b170c-729a-429c-b115-0350ea50b32b",
        "body" : "Sorry, but I don't see this.\r\n\r\n- Argument type validation is not covered by planner, and some combinations of argument types will either lead to cryptic exceptions or ambiguous behavior. How would you generate placeholders for example for `*args` (or similarly `...` in R).\r\n- Having consistent placeholders with Scala counterpart is good. And even to get to `x1`, x2`, `...,`  cannot be done without inspecting signature. And even if we did only that (which still keeps most of the complexity, calling things `_(**kwargs)` or `_(*args)` or different keyword variants would be rather sloppy.\r\n",
        "createdAt" : "2020-02-06T08:50:21Z",
        "updatedAt" : "2020-02-06T08:50:21Z",
        "lastEditedBy" : "981b170c-729a-429c-b115-0350ea50b32b",
        "tags" : [
        ]
      },
      {
        "id" : "d22a4fc8-75f1-406c-8420-4e4429d860ba",
        "parentId" : "173d8d34-7305-437b-a02e-45f310904069",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I am not particularly against this codes. I just thought it's easier to logically separate two PRs. This PR just to add higher order functions relying on JVM check. The other PR to add argument checking in Python (or R) sides.\r\n\r\nFor the former case, I am totally supportive and I can sign-off. For the latter I am not completely sure. I would like to collect some feedback from some more people. So .. asking to remove it is just to make it rolling quicker :-).",
        "createdAt" : "2020-02-07T10:45:50Z",
        "updatedAt" : "2020-02-07T10:45:50Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "39fb9861-1af4-4672-846e-df8f2d0ce015",
        "parentId" : "173d8d34-7305-437b-a02e-45f310904069",
        "authorId" : "981b170c-729a-429c-b115-0350ea50b32b",
        "body" : "I get you point but right now I consider this core logic. We can shuffle things around and shave a line or two, but I don't see it improving overall \"reviewability\".\r\n\r\nSomewhere here we have to decide out how many placeholders  should be provided (if it wasn't for polymorphic behavior of some variants, we could just assume â€’ if we for example had `transform` and `transform_with_index`. But now have to do it at least for a few of functions, and there might be more to come). So we can only push things around and\r\n\r\n```python\r\n    import inspect\r\n\r\n    signature = inspect.signature(f)\r\n    parameters = signature.parameters.values()\r\n```\r\n\r\nas well as the legacy part (I hope it is going to be dropped before 3.1 release anyway) are here to stay (could be inlined, but personally I think it would makes things worse not better).\r\n\r\nIf we wanted to condense things as much as possible, it could be condensed into something like\r\n\r\n```python\r\n\r\n    args = [\r\n        _unresolved_named_lambda_variable(f\"x{i}\") for i, _ in \r\n        enumerate(\r\n          inspect.signature(f).parameters if sys.version_info >= (3, 3) \r\n          else inspect.getargspec(f) \r\n        )\r\n    ]\r\n\r\n```\r\n\r\nbut I don't think that it makes things any clearer, do you?  Also leaves the below.\r\n\r\n\r\nIf we keep just this part and leave variadic argument validation out, I'd be concerned that it will to all kinds of confusing or potentially incorrect behaviors. Users are extremely imaginative and delayed resolution doesn't help*. \r\n\r\nAnd since it is neither heavy in logic nor duplicates (for some definition of duplication) JVM logic, it shouldn't be controversial. \r\n\r\n\r\n\r\n\r\n----------------\r\n\\*  We can be pretty sure that  attempts to apply some-general-purpose-library in func will happen, if this is merged. \r\n\r\nAnd let's be honest â€’ higher order functions are already pretty hard to work with. There are sensitive to minor type mismatches (numeric precision and such) and corresponding expressions are pretty large.\r\n\r\n",
        "createdAt" : "2020-02-07T12:00:22Z",
        "updatedAt" : "2020-02-07T12:00:23Z",
        "lastEditedBy" : "981b170c-729a-429c-b115-0350ea50b32b",
        "tags" : [
        ]
      },
      {
        "id" : "1e2aec44-1c62-4f97-8922-42b82a9f9e5f",
        "parentId" : "173d8d34-7305-437b-a02e-45f310904069",
        "authorId" : "981b170c-729a-429c-b115-0350ea50b32b",
        "body" : "Just to be clear â€’ my reservations are now purely practical (in contrast to my opposition to removal of function specific validation). Certain things have to be done one way or another, some parts of the logic will be removed soon so it is better to keep these clearly separated and preventing ambiguity while keeping things || with Scala  is just basic sanity :)",
        "createdAt" : "2020-02-07T13:32:10Z",
        "updatedAt" : "2020-02-07T13:32:10Z",
        "lastEditedBy" : "981b170c-729a-429c-b115-0350ea50b32b",
        "tags" : [
        ]
      }
    ],
    "commit" : "05741c8422364a77389d13ba0a85b7ad92392be6",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +2858,2862 @@\n\ndef _get_lambda_parameters(f):\n    import inspect\n"
  },
  {
    "id" : "56c0df52-938c-4ddb-8a28-f378ce78797f",
    "prId" : 27331,
    "prUrl" : "https://github.com/apache/spark/pull/27331#pullrequestreview-437123441",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f257fd8c-a757-4681-91b2-076595deb6be",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Let's also clarify this expression should only with `partitionedBy` in DSv2 APIs.",
        "createdAt" : "2020-06-25T02:11:39Z",
        "updatedAt" : "2020-07-19T15:47:59Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "9197c8411e6484b9df2b8626a6a77a5af18d89b9",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +3328,3332 @@def years(col):\n    \"\"\"\n    Partition transform function: A transform for timestamps and dates\n    to partition data into years.\n"
  },
  {
    "id" : "3face5a5-0622-4e0e-ae23-1bed1e9dcda2",
    "prId" : 27278,
    "prUrl" : "https://github.com/apache/spark/pull/27278#pullrequestreview-359598242",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5ac65ee6-e51a-4058-bc47-e377a1b1a29a",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "`accuracy` here too. Let's make it able to take both int and a column instance. Should be able to do it easily by calling `lit` on it.",
        "createdAt" : "2020-02-17T09:39:18Z",
        "updatedAt" : "2020-03-16T04:47:34Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "b6bd2d46ceb2539dd8e99e07268ce9e8fd6e6558",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +586,590 @@\n@since(3.1)\ndef percentile_approx(col, percentage, accuracy=10000):\n    \"\"\"Returns the approximate percentile value of numeric column col at the given percentage.\n    The value of percentage must be between 0.0 and 1.0."
  },
  {
    "id" : "47815f78-2566-4ca1-802f-a92157ec9343",
    "prId" : 26896,
    "prUrl" : "https://github.com/apache/spark/pull/26896#pullrequestreview-332323840",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2c76c1db-0a19-404a-8e4a-749b5a2658c6",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Okay, that's fine but can you at least update R and Scala APIs too to be consistent? You can add new params for start and length at https://github.com/apache/spark/blob/master/R/pkg/R/functions.R#L213 and add `@param`s at https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/functions.scala#L3267-L3272",
        "createdAt" : "2019-12-15T13:48:12Z",
        "updatedAt" : "2019-12-16T03:17:06Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "5b3ebb6a-ef8c-4766-a8e8-8fe017c49df2",
        "parentId" : "2c76c1db-0a19-404a-8e4a-749b5a2658c6",
        "authorId" : "fa174810-4768-44fb-a405-b54fee44c3c3",
        "body" : "Ok! will update shortly",
        "createdAt" : "2019-12-15T13:49:16Z",
        "updatedAt" : "2019-12-16T03:17:06Z",
        "lastEditedBy" : "fa174810-4768-44fb-a405-b54fee44c3c3",
        "tags" : [
        ]
      },
      {
        "id" : "eab8c6da-e046-4347-9728-d60935338b2f",
        "parentId" : "2c76c1db-0a19-404a-8e4a-749b5a2658c6",
        "authorId" : "fa174810-4768-44fb-a405-b54fee44c3c3",
        "body" : "I added the scala params. \r\n\r\nMaybe I am mistaken, but it seems the R API already has this documented at https://github.com/apache/spark/blob/a9fbd310300e57ed58818d7347f3c3172701c491/R/pkg/R/functions.R#L3623\r\n\r\nThe rendering seems to work fine too on https://spark.apache.org/docs/latest/api/R/index.html so I am leaving the R docs alone.",
        "createdAt" : "2019-12-15T14:22:43Z",
        "updatedAt" : "2019-12-16T03:17:06Z",
        "lastEditedBy" : "fa174810-4768-44fb-a405-b54fee44c3c3",
        "tags" : [
        ]
      },
      {
        "id" : "8d9bedb9-787b-4022-93b2-b30d4ca80f2c",
        "parentId" : "2c76c1db-0a19-404a-8e4a-749b5a2658c6",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Thanks. Can you match the wording while we're here?",
        "createdAt" : "2019-12-16T02:47:14Z",
        "updatedAt" : "2019-12-16T03:17:06Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "64f69c05-7d4c-4f9f-8950-39db488aeaf4",
        "parentId" : "2c76c1db-0a19-404a-8e4a-749b5a2658c6",
        "authorId" : "fa174810-4768-44fb-a405-b54fee44c3c3",
        "body" : "Fixed.",
        "createdAt" : "2019-12-16T03:17:16Z",
        "updatedAt" : "2019-12-16T03:17:16Z",
        "lastEditedBy" : "fa174810-4768-44fb-a405-b54fee44c3c3",
        "tags" : [
        ]
      }
    ],
    "commit" : "006327875576edbbf9b6f8e37cb8e5a5a424a1d7",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +1977,1981 @@    :param x: the array to be sliced\n    :param start: the starting index\n    :param length: the length of the slice\n\n    >>> df = spark.createDataFrame([([1, 2, 3],), ([4, 5],)], ['x'])"
  },
  {
    "id" : "61914dfd-c69f-4c99-a413-066c62ed5fa3",
    "prId" : 26435,
    "prUrl" : "https://github.com/apache/spark/pull/26435#pullrequestreview-314660334",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ae011d4c-a5b8-4ac0-814f-a9f0ea77f607",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "This seems already working.",
        "createdAt" : "2019-11-11T01:25:44Z",
        "updatedAt" : "2019-11-11T01:25:45Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "61994d37-3f73-4a63-a96c-9097c86202b3",
        "parentId" : "ae011d4c-a5b8-4ac0-814f-a9f0ea77f607",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "```python\r\n>>> from pyspark.sql.functions import isnan\r\n>>> df = spark.createDataFrame([(1.0, float('nan')), (float('nan'), 2.0)], (\"a\", \"b\"))\r\n>>> df.select(isnan(\"a\")).collect()\r\n[Row(isnan(a)=False), Row(isnan(a)=True)]\r\n```",
        "createdAt" : "2019-11-11T01:26:19Z",
        "updatedAt" : "2019-11-11T01:26:20Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "f8b1424d2f51bc8a5b500c70742be8a9dfffa1df",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +514,518 @@    \"\"\"\n    sc = SparkContext._active_spark_context\n    if type(col) is str:\n        return Column(sc._jvm.functions.isnan(col))\n    return Column(sc._jvm.functions.isnan(_to_java_column(col)))"
  }
]