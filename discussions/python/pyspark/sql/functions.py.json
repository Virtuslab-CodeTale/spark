[
  {
    "id" : "15b1b11e-947e-4fa6-af0a-375cb0d479c7",
    "prId" : 32566,
    "prUrl" : "https://github.com/apache/spark/pull/32566#pullrequestreview-660544882",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "65d4772d-74bd-4cb8-a2ac-a062801c172a",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "```suggestion\r\n    Examples\r\n    --------\r\n```",
        "createdAt" : "2021-05-17T04:40:14Z",
        "updatedAt" : "2021-05-17T04:44:05Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "3b05e99de665e785ad05a37717d3d2f761cce30a",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +2698,2702 @@        a country of the locale\n\n    Examples\n    --------\n    >>> df = spark.createDataFrame([[\"This is an example sentence.\"]], [\"string\"])"
  },
  {
    "id" : "6e9d8983-ad31-482a-b1fd-a171bca187aa",
    "prId" : 32566,
    "prUrl" : "https://github.com/apache/spark/pull/32566#pullrequestreview-660544882",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6e28cb60-238e-48be-980b-119141fdd36c",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Can we also add:\r\n\r\n```\r\n    Parameters\r\n    ----------\r\n    str: ...\r\n```\r\n\r\n(see also https://numpydoc.readthedocs.io/en/latest/format.html)",
        "createdAt" : "2021-05-17T04:40:27Z",
        "updatedAt" : "2021-05-17T04:44:05Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "3b05e99de665e785ad05a37717d3d2f761cce30a",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +2688,2692 @@\n    .. versionadded:: 3.2.0\n\n    Parameters\n    ----------"
  },
  {
    "id" : "27182c9c-1993-44cb-872d-331a3ad835f8",
    "prId" : 31207,
    "prUrl" : "https://github.com/apache/spark/pull/31207#pullrequestreview-578361157",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6d839666-65be-490e-a00f-cc3b98360e45",
        "parentId" : null,
        "authorId" : "eb588fd2-21dd-4dea-a8f9-bcf2e48148eb",
        "body" : "Worth mentioning the newly supported types in the docstring",
        "createdAt" : "2021-01-26T22:26:37Z",
        "updatedAt" : "2021-01-29T13:23:21Z",
        "lastEditedBy" : "eb588fd2-21dd-4dea-a8f9-bcf2e48148eb",
        "tags" : [
        ]
      },
      {
        "id" : "fd5ee94c-ef81-4e32-bc02-db32937f5c6d",
        "parentId" : "6d839666-65be-490e-a00f-cc3b98360e45",
        "authorId" : "981b170c-729a-429c-b115-0350ea50b32b",
        "body" : "Added a short description of the input parameters @LielBach. ",
        "createdAt" : "2021-01-28T14:02:12Z",
        "updatedAt" : "2021-01-29T13:23:21Z",
        "lastEditedBy" : "981b170c-729a-429c-b115-0350ea50b32b",
        "tags" : [
        ]
      }
    ],
    "commit" : "dade2b17152baf3f02553060bc8bbdbf96744756",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +90,94 @@def lit(col):\n    \"\"\"\n    Creates a :class:`Column` of literal value.\n\n    .. versionadded:: 1.3.0"
  },
  {
    "id" : "7938009a-a1a8-4adf-962e-e19495a288a1",
    "prId" : 31207,
    "prUrl" : "https://github.com/apache/spark/pull/31207#pullrequestreview-578996182",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f739d86d-4a50-4e15-b499-e09b4e0c0164",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "One thing that makes me worry is this isn't technically a literal ... although it's foldable. The results and optimization might be differently applied compared to Scala side because they are not literals. For example, we have some optimization rules that only applies to literals here and there.\r\n\r\nAlso looks like Scala side doesn't support map and struct (due to both the limitation of typing and the fact that `Row` doesn't hold field names, etc.).",
        "createdAt" : "2021-01-27T04:54:27Z",
        "updatedAt" : "2021-01-29T13:23:21Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "f9b172ae-88bb-4669-be96-bb9965e1d05b",
        "parentId" : "f739d86d-4a50-4e15-b499-e09b4e0c0164",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Hm, okay but these will be literals after constant folding though. @cloud-fan, WDYT about supporting complex type at literals in this way? I think we can do similar thing in Scala side as well to allow map and struct literals.",
        "createdAt" : "2021-01-27T05:03:55Z",
        "updatedAt" : "2021-01-29T13:23:21Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "108fc6a3-22ac-4f39-aa6a-fd855135703e",
        "parentId" : "f739d86d-4a50-4e15-b499-e09b4e0c0164",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "cc @viirya too",
        "createdAt" : "2021-01-27T05:04:54Z",
        "updatedAt" : "2021-01-29T13:23:21Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "b9ad4387-11bf-4920-9e3b-4ebe513ca226",
        "parentId" : "f739d86d-4a50-4e15-b499-e09b4e0c0164",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Seems okay because there is constant folding optimization. So if they are foldable, they will be optimized as literals during query optimization.",
        "createdAt" : "2021-01-27T06:15:09Z",
        "updatedAt" : "2021-01-29T13:23:21Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "ced0a754-a7bc-4eec-9625-a82bf3690190",
        "parentId" : "f739d86d-4a50-4e15-b499-e09b4e0c0164",
        "authorId" : "981b170c-729a-429c-b115-0350ea50b32b",
        "body" : "> Also looks like Scala side doesn't support map and struct (due to both the limitation of typing and the fact that `Row` doesn't hold field names, etc.).\r\n\r\nActually, `typedLit` supports `Map` objects:\r\n\r\n```scala\r\nscala> typedLit(Map(\"a\" -> 1, \"b\" -> 2))\r\nres7: org.apache.spark.sql.Column = keys: [a,b], values: [1,2]\r\n```\r\n\r\nand `Products`\r\n\r\n```scala\r\nscala> case class FooBar(foo: Int, bar: String)\r\ndefined class FooBar\r\n\r\nscala> typedLit(FooBar(1, \"a\"))\r\nres8: org.apache.spark.sql.Column = [1,a]\r\n```\r\n\r\n`Row` would be specific to PySpark, but since it is just a tuple, it seemed reasonable to handle it with names.\r\n\r\n> Hm, okay but these will be literals after constant folding though.\r\n\r\nThe biggest difference compared to Scala is that it we handle mixed constants and column objects:\r\n\r\n```python\r\n>>> spark.createDataFrame([(1, 2)], (\"a\", \"b\")).select(lit({\"a\": col(\"a\"), \"b\": col(\"b\")})).show()      \r\n+----------------+\r\n| map(a, a, b, b)|\r\n+----------------+\r\n|{a -> 1, b -> 2}|\r\n+----------------+\r\n```\r\n\r\nSo there is a case where `lit` results in a non-`Literal`.\r\n\r\n\r\n",
        "createdAt" : "2021-01-27T14:09:55Z",
        "updatedAt" : "2021-01-29T13:23:21Z",
        "lastEditedBy" : "981b170c-729a-429c-b115-0350ea50b32b",
        "tags" : [
        ]
      },
      {
        "id" : "94ddad4e-b9e0-4598-b438-f5504ea991df",
        "parentId" : "f739d86d-4a50-4e15-b499-e09b4e0c0164",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "It seems fine, but we might be able to convert these exprs for complex types into literals in advance via `PythonSQLUtils`.",
        "createdAt" : "2021-01-29T07:24:08Z",
        "updatedAt" : "2021-01-29T13:23:21Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "dade2b17152baf3f02553060bc8bbdbf96744756",
    "line" : 53,
    "diffHunk" : "@@ -1,1 +132,136 @@        )\n\n        return struct(*[\n            lit(x).alias(a)\n            for x, a in zip(col, fields)"
  },
  {
    "id" : "fa4df137-eaa9-451e-a6c3-c0d314d6d669",
    "prId" : 31207,
    "prUrl" : "https://github.com/apache/spark/pull/31207#pullrequestreview-579964625",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ca5e55c9-b09a-4eae-a8ac-daf79a043819",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "\"Creates a :class:`Column` of literal value.\"?\r\n\r\nhttps://github.com/apache/spark/blob/72b7f8abfb60d0008f1f9bed94ce1c367a7d7cce/sql/core/src/main/scala/org/apache/spark/sql/functions.scala#L105",
        "createdAt" : "2021-01-29T04:18:08Z",
        "updatedAt" : "2021-01-29T13:23:21Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "33aa20cb-3987-486e-a97e-f52c9359b28b",
        "parentId" : "ca5e55c9-b09a-4eae-a8ac-daf79a043819",
        "authorId" : "981b170c-729a-429c-b115-0350ea50b32b",
        "body" : "I am not sure if I understand what you mean. We already have matching sentence in the description block\r\n\r\nhttps://github.com/apache/spark/blob/6198e0aca0d30d6bd6ba9f29a0fe60a62016caed/python/pyspark/sql/functions.py#L92\r\n\r\nCould you clarify please?",
        "createdAt" : "2021-01-29T13:26:35Z",
        "updatedAt" : "2021-01-29T13:26:35Z",
        "lastEditedBy" : "981b170c-729a-429c-b115-0350ea50b32b",
        "tags" : [
        ]
      },
      {
        "id" : "781168ab-0fc8-4a46-bb4c-8b78cf8f294e",
        "parentId" : "ca5e55c9-b09a-4eae-a8ac-daf79a043819",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Ah, nvm. I missed it.",
        "createdAt" : "2021-02-01T01:31:14Z",
        "updatedAt" : "2021-02-01T01:31:14Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "dade2b17152baf3f02553060bc8bbdbf96744756",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +99,103 @@    ----------\n    col : bool, float, int, str, datetime.date, datetime.datetime, dict, list, tuple\n        Object to be converted into :class:`Column`.\n\n        If it is a collection, conversion will be applied recursively. In such case,"
  },
  {
    "id" : "6820c7af-44ad-402e-a0dd-2b9b420f7637",
    "prId" : 31207,
    "prUrl" : "https://github.com/apache/spark/pull/31207#pullrequestreview-579854094",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3b6dd29f-9397-4cfd-91ba-00eca1cb83aa",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@zero323, can we implement these logics in Scala side, and we simply call `lit` here? I think that's more what @maropu meant.\r\n\r\nI think the only special case we need to implement in Python side is the struct case because 1. `Row` class cannot be passed to JVM side and 2. both `list` and `tuple` are converted into `ArrayList` so JVM cannot tell the origin. We will probably have to add some comments on that case.",
        "createdAt" : "2021-01-31T04:47:50Z",
        "updatedAt" : "2021-01-31T04:47:50Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "dade2b17152baf3f02553060bc8bbdbf96744756",
    "line" : 43,
    "diffHunk" : "@@ -1,1 +122,126 @@        return array(*[lit(x) for x in col])\n\n    elif isinstance(col, tuple):\n        fields = (\n            # Named tuple"
  },
  {
    "id" : "c3ebcc66-9c3a-4817-b331-4c5365239eb8",
    "prId" : 31207,
    "prUrl" : "https://github.com/apache/spark/pull/31207#pullrequestreview-579976168",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d88d7f2c-d3eb-4dae-b3fe-bcda44fdd114",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "One thing I am a bit worried is that this will lazily check the type (with type coercion) unlike Scala side ...",
        "createdAt" : "2021-02-01T02:24:23Z",
        "updatedAt" : "2021-02-01T02:24:24Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "dade2b17152baf3f02553060bc8bbdbf96744756",
    "line" : 41,
    "diffHunk" : "@@ -1,1 +120,124 @@\n    elif isinstance(col, list):\n        return array(*[lit(x) for x in col])\n\n    elif isinstance(col, tuple):"
  },
  {
    "id" : "7197c45c-7aaa-4ef7-9323-481684fbdfb7",
    "prId" : 30745,
    "prUrl" : "https://github.com/apache/spark/pull/30745#pullrequestreview-550915908",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "74206255-79ab-4a68-89b9-acfd8e89cf9b",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Let's also add some doctests here.",
        "createdAt" : "2020-12-13T02:17:42Z",
        "updatedAt" : "2021-03-01T17:18:18Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "da436b34-3f09-414a-96ac-e2980ee8173a",
        "parentId" : "74206255-79ab-4a68-89b9-acfd8e89cf9b",
        "authorId" : "dc7a33cf-bba6-4512-a5a1-248c3cac6eae",
        "body" : "I agree that having some documentation, with examples, would be very helpful and I've added these following your advice. However, it seems like the automated build tests are insanely restrictive in their validation of these doctests, causing multiple building failures when the pydoc examples differ by inconsequential whitespace or formatting changes. For example, even when I have directly copy-pasted example outputs from a pyspark session, these were rejected by the build-system.\r\n\r\nCan I suggest that someone investigates whether the build-system can make these documentation checks more tolerant of whitespace variations, which I'd expect will trip-up many developers and will create obstacles for producing documentation that would benefit Spark users?",
        "createdAt" : "2020-12-13T12:32:35Z",
        "updatedAt" : "2021-03-01T17:18:18Z",
        "lastEditedBy" : "dc7a33cf-bba6-4512-a5a1-248c3cac6eae",
        "tags" : [
        ]
      }
    ],
    "commit" : "1cdabcbdab2d54c67f09139f3b6cea1b2c09e04e",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +247,251 @@    +----+-------+\n\n    \"\"\"\n    return _invoke_function_over_column(\"product\", col)\n"
  },
  {
    "id" : "1ba3b16c-1115-4062-bd27-2bd1c3f181fc",
    "prId" : 30501,
    "prUrl" : "https://github.com/apache/spark/pull/30501#pullrequestreview-538933079",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e4a55b5a-2f0e-43b9-8a8b-041f9cca0317",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Let's also update `spark/python/docs/source/reference/pyspark.sql.rst`",
        "createdAt" : "2020-11-26T01:17:05Z",
        "updatedAt" : "2020-11-26T11:01:55Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "17451c7a0bb58cd5e4f9ee29e62370f2208726d3",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +221,225 @@\n\ndef acosh(col):\n    \"\"\"\n    Computes inverse hyperbolic cosine of the input column."
  },
  {
    "id" : "9a861d58-a2b0-4d24-b123-114a2d553bd4",
    "prId" : 30201,
    "prUrl" : "https://github.com/apache/spark/pull/30201#pullrequestreview-521211584",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2da07c8d-c79a-4c3a-9cc7-cc25aeb4ae6d",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "```suggestion\r\n    >>> schema =  (\r\n    ...     '{\"type\" : \"map\", \"keyType\" : \"string\",'\r\n    ...     '\"valueType\" : \"integer\", \"valueContainsNull\" : true}')\r\n    >>> df.select(from_json(df.value, schema).alias(\"json\")).collect()\r\n    [Row(json={'a': 1})]\r\n```",
        "createdAt" : "2020-11-01T10:34:07Z",
        "updatedAt" : "2020-11-01T10:34:08Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "88cd2a3354e587861f3214b94d0cadc8bc163747",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +2864,2868 @@     '{\"type\" : \"map\", \"keyType\" : \"string\", \"valueType\" : \"integer\", \"valueContainsNull\" : true}')\\\n     .alias(\"json\")).collect()\n    [Row(json={'a': 1})]\n    >>> data = [(1, '''[{\"a\": 1}]''')]\n    >>> schema = ArrayType(StructType([StructField(\"a\", IntegerType())]))"
  },
  {
    "id" : "15a96970-9c56-4eda-ae39-b1addaa9fb00",
    "prId" : 30181,
    "prUrl" : "https://github.com/apache/spark/pull/30181#pullrequestreview-519650717",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8d666f11-87b5-4a7e-a486-dee7be306173",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "There are a bit of trivial changes in the doc to make it more sense with the new format. One example is here.",
        "createdAt" : "2020-10-29T13:15:48Z",
        "updatedAt" : "2020-11-03T00:49:21Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "40bc04996ab651411656c08849b5ec0495cd8468",
    "line" : 786,
    "diffHunk" : "@@ -1,1 +1277,1281 @@    ----------\n    cols : list, set, str or :class:`Column`\n        column names or :class:`Column`\\\\s to contain in the output struct.\n\n    Examples"
  },
  {
    "id" : "49112491-aef9-49be-ac15-ab26746d2504",
    "prId" : 30181,
    "prUrl" : "https://github.com/apache/spark/pull/30181#pullrequestreview-520264562",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f3e6bbd4-0de7-4813-bbd9-1ce30619c297",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "1.4.0?",
        "createdAt" : "2020-10-30T01:22:31Z",
        "updatedAt" : "2020-11-03T00:49:21Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "40bc04996ab651411656c08849b5ec0495cd8468",
    "line" : 38,
    "diffHunk" : "@@ -1,1 +217,221 @@def asin(col):\n    \"\"\"\n    .. versionadded:: 1.3.0\n\n"
  },
  {
    "id" : "171795c9-1640-4b99-a02a-23f1239bc29d",
    "prId" : 30181,
    "prUrl" : "https://github.com/apache/spark/pull/30181#pullrequestreview-520393963",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5c0bb303-365b-46a5-8905-c172cf0ca4ef",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Not to change to `.. versionadded::`?",
        "createdAt" : "2020-10-30T01:24:27Z",
        "updatedAt" : "2020-11-03T00:49:21Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "1d68f281-9576-459a-bbfa-6ab3c2e5b2d5",
        "parentId" : "5c0bb303-365b-46a5-8905-c172cf0ca4ef",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Oh yeah. This one no need to change because `.. deprecated::` is still [reST style directive](https://www.sphinx-doc.org/en/master/usage/restructuredtext/directives.html). So, this has no problem about formatting error because it does not use NumPy documentation specific syntax.",
        "createdAt" : "2020-10-30T05:59:47Z",
        "updatedAt" : "2020-11-03T00:49:21Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "40bc04996ab651411656c08849b5ec0495cd8468",
    "line" : 192,
    "diffHunk" : "@@ -1,1 +424,428 @@\n\n@since(1.4)\ndef toDegrees(col):\n    \"\"\""
  },
  {
    "id" : "d0f4e33c-3e90-465e-aa7a-24d0f2c47ee2",
    "prId" : 30181,
    "prUrl" : "https://github.com/apache/spark/pull/30181#pullrequestreview-520264562",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "553d1a47-a6c3-4b9e-b607-05204b9ce76b",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "ditto",
        "createdAt" : "2020-10-30T01:24:37Z",
        "updatedAt" : "2020-11-03T00:49:21Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "40bc04996ab651411656c08849b5ec0495cd8468",
    "line" : 202,
    "diffHunk" : "@@ -1,1 +434,438 @@\n\n@since(1.4)\ndef toRadians(col):\n    \"\"\""
  },
  {
    "id" : "914f0863-ad32-41ba-bc37-9c919e4734c5",
    "prId" : 30181,
    "prUrl" : "https://github.com/apache/spark/pull/30181#pullrequestreview-520264562",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c1da1fce-c398-46cd-9886-4ceffa1092cc",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "this?",
        "createdAt" : "2020-10-30T01:25:35Z",
        "updatedAt" : "2020-11-03T00:49:21Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "40bc04996ab651411656c08849b5ec0495cd8468",
    "line" : 338,
    "diffHunk" : "@@ -1,1 +731,735 @@\n\n@since(1.3)\ndef approxCountDistinct(col, rsd=None):\n    \"\"\""
  }
]