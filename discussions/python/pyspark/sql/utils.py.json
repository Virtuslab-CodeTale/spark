[
  {
    "id" : "6e3b3c95-42f1-474e-b374-a1ce37340c09",
    "prId" : 28661,
    "prUrl" : "https://github.com/apache/spark/pull/28661#pullrequestreview-419798512",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dc4f810c-8ce6-448d-afda-e290ce9c896d",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "This way, actually I mimicked [`six`](https://github.com/benjaminp/six/blob/c0be8815d13df45b6ae471c4c436cce8c192245d/six.py#L729-L738)",
        "createdAt" : "2020-05-28T06:00:53Z",
        "updatedAt" : "2020-05-29T07:58:03Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "28c2a516eec5ffbd527b9e62869b279a162458e4",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +28,32 @@def raise_from(e):\n    raise e from None\n\"\"\")\nelse:\n    def raise_from(e):"
  },
  {
    "id" : "ebde38ef-172d-4773-bf0b-79663074a6c8",
    "prId" : 28661,
    "prUrl" : "https://github.com/apache/spark/pull/28661#pullrequestreview-419869248",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ca8292c0-48ca-4b93-bab5-b08399e609e6",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Not sure about this part, why need the change? ",
        "createdAt" : "2020-05-28T06:14:50Z",
        "updatedAt" : "2020-05-29T07:58:03Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "7d8cbf9a-4f67-4fa0-a1c8-e87ebc89405e",
        "parentId" : "ca8292c0-48ca-4b93-bab5-b08399e609e6",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Previous stacktrace wasn't actually quite correct. It hid the stacktrace from executor side before. Now, this PR handles an exception from executor so I needed to change this.",
        "createdAt" : "2020-05-28T06:35:11Z",
        "updatedAt" : "2020-05-29T07:58:03Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "3cf7233b-1661-49fb-a22b-9c97af95043b",
        "parentId" : "ca8292c0-48ca-4b93-bab5-b08399e609e6",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "hmm, isn't `getStackTrace` content equivalent to `printStackTrace`?",
        "createdAt" : "2020-05-28T06:58:50Z",
        "updatedAt" : "2020-05-29T07:58:03Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "b73d7e23-794e-4aab-9a35-001b6688a48d",
        "parentId" : "ca8292c0-48ca-4b93-bab5-b08399e609e6",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Seems different. This is what I get from `getStackTrace`:\r\n\r\n```\r\norg.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2117)\r\n\t at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2066)\r\n\t at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2065)\r\n\t at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\t at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\t at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\t at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2065)\r\n\t at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1021)\r\n\t at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1021)\r\n\t at scala.Option.foreach(Option.scala:407)\r\n\t at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1021)\r\n\t at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2297)\r\n\t at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2246)\r\n\t at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2235)\r\n\t at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\t at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:823)\r\n\t at org.apache.spark.SparkContext.runJob(SparkContext.scala:2108)\r\n\t at org.apache.spark.SparkContext.runJob(SparkContext.scala:2129)\r\n\t at org.apache.spark.SparkContext.runJob(SparkContext.scala:2148)\r\n\t at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:467)\r\n\t at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:420)\r\n\t at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\r\n\t at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3653)\r\n\t at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2695)\r\n\t at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3644)\r\n\t at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\t at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\t at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\t at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)\r\n\t at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\t at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3642)\r\n\t at org.apache.spark.sql.Dataset.head(Dataset.scala:2695)\r\n\t at org.apache.spark.sql.Dataset.take(Dataset.scala:2902)\r\n\t at org.apache.spark.sql.Dataset.getRows(Dataset.scala:300)\r\n\t at org.apache.spark.sql.Dataset.showString(Dataset.scala:337)\r\n\t at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\t at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\t at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\t at java.lang.reflect.Method.invoke(Method.java:498)\r\n\t at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\t at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\t at py4j.Gateway.invoke(Gateway.java:282)\r\n\t at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\t at py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\t at py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\t at java.lang.Thread.run(Thread.java:748)\r\n```\r\n\r\nthis is what I get from `printStackTrace`\r\n\r\n```\r\norg.apache.spark.SparkException: Job aborted due to stage failure: Task 10 in stage 2.0 failed 4 times, most recent failure: Lost task 10.3 in stage 2.0 (TID 18, 192.168.35.193, executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\r\n  File \"/.../spark/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\r\n    process()\r\n  File \"/.../spark/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in process\r\n    serializer.dump_stream(out_iter, outfile)\r\n  File \"/.../spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 223, in dump_stream\r\n    self.serializer.dump_stream(self._batched(iterator), stream)\r\n  File \"/.../spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\r\n    for obj in iterator:\r\n  File \"/.../spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 212, in _batched\r\n    for item in iterator:\r\n  File \"/.../spark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in mapper\r\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\r\n  File \"/.../spark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in <genexpr>\r\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\r\n  File \"/.../spark/python/lib/pyspark.zip/pyspark/worker.py\", line 90, in <lambda>\r\n    return lambda *a: f(*a)\r\n  File \"/.../spark/python/lib/pyspark.zip/pyspark/util.py\", line 107, in wrapper\r\n    return f(*args, **kwargs)\r\n  File \"<stdin>\", line 3, in divide_by_zero\r\nZeroDivisionError: division by zero\r\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:516)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:81)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:64)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:469)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:753)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:340)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:469)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:472)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\r\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2117)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2066)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2065)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2065)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1021)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1021)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1021)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2297)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2246)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2235)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:823)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2108)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2129)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2148)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:467)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:420)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3653)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2695)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3644)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3642)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2695)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2902)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:300)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:337)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\r\n  File \"/.../spark/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\r\n    process()\r\n  File \"/.../spark/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in process\r\n    serializer.dump_stream(out_iter, outfile)\r\n  File \"/.../spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 223, in dump_stream\r\n    self.serializer.dump_stream(self._batched(iterator), stream)\r\n  File \"/.../spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\r\n    for obj in iterator:\r\n  File \"/.../spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 212, in _batched\r\n    for item in iterator:\r\n  File \"/.../spark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in mapper\r\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\r\n  File \"/.../spark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in <genexpr>\r\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\r\n  File \"/.../spark/python/lib/pyspark.zip/pyspark/worker.py\", line 90, in <lambda>\r\n    return lambda *a: f(*a)\r\n  File \"/.../spark/python/lib/pyspark.zip/pyspark/util.py\", line 107, in wrapper\r\n    return f(*args, **kwargs)\r\n  File \"<stdin>\", line 3, in divide_by_zero\r\nZeroDivisionError: division by zero\r\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:516)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:81)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:64)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:469)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:753)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:340)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:469)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:472)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n```\r\n",
        "createdAt" : "2020-05-28T07:27:39Z",
        "updatedAt" : "2020-05-29T07:58:03Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "4267e31c-cf62-4d67-bfb9-0d880e02d692",
        "parentId" : "ca8292c0-48ca-4b93-bab5-b08399e609e6",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Seems like `getStackTrace` doesn't show the cause whereas `printStackTrace` shows it too. It's best to make it same as shown in the JVM anyway :-).",
        "createdAt" : "2020-05-28T08:03:56Z",
        "updatedAt" : "2020-05-29T07:58:03Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "28c2a516eec5ffbd527b9e62869b279a162458e4",
    "line" : 55,
    "diffHunk" : "@@ -1,1 +102,106 @@    jwriter = jvm.java.io.StringWriter()\n    e.printStackTrace(jvm.java.io.PrintWriter(jwriter))\n    stacktrace = jwriter.toString()\n    if s.startswith('org.apache.spark.sql.AnalysisException: '):\n        return AnalysisException(s.split(': ', 1)[1], stacktrace, c)"
  },
  {
    "id" : "7b6c0266-05bc-45db-8a70-edf25113684d",
    "prId" : 28661,
    "prUrl" : "https://github.com/apache/spark/pull/28661#pullrequestreview-419804274",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "66efbb78-de8f-4996-8f6f-cdb919b84d51",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "So `raise_from` is used to cut the exception chain from JVM?",
        "createdAt" : "2020-05-28T06:23:57Z",
        "updatedAt" : "2020-05-29T07:58:03Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "28c2a516eec5ffbd527b9e62869b279a162458e4",
    "line" : 94,
    "diffHunk" : "@@ -1,1 +135,139 @@                # Hide where the exception came from that shows a non-Pythonic\n                # JVM exception message.\n                raise_from(converted)\n            else:\n                raise"
  },
  {
    "id" : "4a052c35-6081-44e3-97d0-14ec347b9b97",
    "prId" : 28661,
    "prUrl" : "https://github.com/apache/spark/pull/28661#pullrequestreview-419818742",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c2d6069d-8488-4394-a80d-e102a375d1e2",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "So seems PEP 3134 is only for 3.0+, we don't cut exception chaining in Python 2.7 with this `raise_from`?",
        "createdAt" : "2020-05-28T06:27:56Z",
        "updatedAt" : "2020-05-29T07:58:03Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "bf154419-c750-4718-9382-b515854c0acf",
        "parentId" : "c2d6069d-8488-4394-a80d-e102a375d1e2",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "re: https://github.com/apache/spark/pull/28661#discussion_r431606605 too.\r\n\r\nYeah. In Python 2, there is no chaining. This is kind of a new feature in Python 3. \r\n\r\ne.g.) in the current master:\r\nPython 2:\r\n\r\n```python\r\n>>> sql(\"a\")\r\n```\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/.../spark/python/pyspark/sql/session.py\", line 646, in sql\r\n    return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)\r\n  File \"/.../spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1305, in __call__\r\n  File \"/.../spark/python/pyspark/sql/utils.py\", line 102, in deco\r\n    raise converted\r\npyspark.sql.utils.ParseException:\r\nmismatched input 'a' expecting {'(', 'ADD', 'ALTER', 'ANALYZE', 'CACHE', 'CLEAR', 'COMMENT', 'COMMIT', 'CREATE', 'DELETE', 'DESC', 'DESCRIBE', 'DFS', 'DROP', 'EXPLAIN', 'EXPORT', 'FROM', 'GRANT', 'IMPORT', 'INSERT', 'LIST', 'LOAD', 'LOCK', 'MAP', 'MERGE', 'MSCK', 'REDUCE', 'REFRESH', 'REPLACE', 'RESET', 'REVOKE', 'ROLLBACK', 'SELECT', 'SET', 'SHOW', 'START', 'TABLE', 'TRUNCATE', 'UNCACHE', 'UNLOCK', 'UPDATE', 'USE', 'VALUES', 'WITH'}(line 1, pos 0)\r\n\r\n== SQL ==\r\na\r\n^^^\r\n```\r\n\r\nPython 3:\r\n\r\n```python\r\n>>> sql(\"a\")\r\n```\r\n```\r\nTraceback (most recent call last):\r\n  File \"/.../spark/python/pyspark/sql/utils.py\", line 98, in deco\r\n    return f(*a, **kw)\r\n  File \"/.../spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\", line 328, in get_return_value\r\npy4j.protocol.Py4JJavaError: An error occurred while calling o25.sql.\r\n: org.apache.spark.sql.catalyst.parser.ParseException:\r\nmismatched input 'a' expecting {'(', 'ADD', 'ALTER', 'ANALYZE', 'CACHE', 'CLEAR', 'COMMENT', 'COMMIT', 'CREATE', 'DELETE', 'DESC', 'DESCRIBE', 'DFS', 'DROP', 'EXPLAIN', 'EXPORT', 'FROM', 'GRANT', 'IMPORT', 'INSERT', 'LIST', 'LOAD', 'LOCK', 'MAP', 'MERGE', 'MSCK', 'REDUCE', 'REFRESH', 'REPLACE', 'RESET', 'REVOKE', 'ROLLBACK', 'SELECT', 'SET', 'SHOW', 'START', 'TABLE', 'TRUNCATE', 'UNCACHE', 'UNLOCK', 'UPDATE', 'USE', 'VALUES', 'WITH'}(line 1, pos 0)\r\n\r\n== SQL ==\r\na\r\n^^^\r\n\r\n\tat org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:266)\r\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:133)\r\n\tat org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:49)\r\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:81)\r\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$2(SparkSession.scala:604)\r\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\r\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:604)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)\r\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:601)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/.../spark/python/pyspark/sql/session.py\", line 646, in sql\r\n    return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)\r\n  File \"/.../spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1305, in __call__\r\n  File \"/.../spark/python/pyspark/sql/utils.py\", line 102, in deco\r\n    raise converted\r\npyspark.sql.utils.ParseException:\r\nmismatched input 'a' expecting {'(', 'ADD', 'ALTER', 'ANALYZE', 'CACHE', 'CLEAR', 'COMMENT', 'COMMIT', 'CREATE', 'DELETE', 'DESC', 'DESCRIBE', 'DFS', 'DROP', 'EXPLAIN', 'EXPORT', 'FROM', 'GRANT', 'IMPORT', 'INSERT', 'LIST', 'LOAD', 'LOCK', 'MAP', 'MERGE', 'MSCK', 'REDUCE', 'REFRESH', 'REPLACE', 'RESET', 'REVOKE', 'ROLLBACK', 'SELECT', 'SET', 'SHOW', 'START', 'TABLE', 'TRUNCATE', 'UNCACHE', 'UNLOCK', 'UPDATE', 'USE', 'VALUES', 'WITH'}(line 1, pos 0)\r\n\r\n== SQL ==\r\na\r\n^^^\r\n\r\n```\r\n",
        "createdAt" : "2020-05-28T06:44:58Z",
        "updatedAt" : "2020-05-29T07:58:03Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "28c2a516eec5ffbd527b9e62869b279a162458e4",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +32,36 @@    def raise_from(e):\n        raise e\n\n\nclass CapturedException(Exception):"
  },
  {
    "id" : "1a975c96-427b-489b-b333-ff80d368f02f",
    "prId" : 28661,
    "prUrl" : "https://github.com/apache/spark/pull/28661#pullrequestreview-419848195",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "21dcffbd-0ea6-42e5-916d-a78dc388e195",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "For some exceptions, like `ParseException`, it should be happened in JVM, right? Is it good to show JVM stacktrace by default?\r\n\r\n",
        "createdAt" : "2020-05-28T07:03:17Z",
        "updatedAt" : "2020-05-29T07:58:03Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "a4dd3f70-4a3e-4bdc-8e1a-6bc0dafbe9a1",
        "parentId" : "21dcffbd-0ea6-42e5-916d-a78dc388e195",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I think `ParseException` at least shows a meaningful error message to the end user such as:\r\n\r\n```\r\n: org.apache.spark.sql.catalyst.parser.ParseException:\r\nmismatched input 'a' expecting {'(', 'ADD', 'ALTER', 'ANALYZE', 'CACHE', 'CLEAR', 'COMMENT', 'COMMIT', 'CREATE', 'DELETE', 'DESC', 'DESCRIBE', 'DFS', 'DROP', 'EXPLAIN', 'EXPORT', 'FROM', 'GRANT', 'IMPORT', 'INSERT', 'LIST', 'LOAD', 'LOCK', 'MAP', 'MERGE', 'MSCK', 'REDUCE', 'REFRESH', 'REPLACE', 'RESET', 'REVOKE', 'ROLLBACK', 'SELECT', 'SET', 'SHOW', 'START', 'TABLE', 'TRUNCATE', 'UNCACHE', 'UNLOCK', 'UPDATE', 'USE', 'VALUES', 'WITH'}(line 1, pos 0)\r\n\r\n== SQL ==\r\na\r\n^^^\r\n```\r\n\r\nIf developers want to debug, they can enable `spark.sql.pyspark.jvmStacktrace.enabled`.",
        "createdAt" : "2020-05-28T07:34:16Z",
        "updatedAt" : "2020-05-29T07:58:03Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "28c2a516eec5ffbd527b9e62869b279a162458e4",
    "line" : 64,
    "diffHunk" : "@@ -1,1 +108,112 @@        return AnalysisException(s.split(': ', 1)[1], stacktrace, c)\n    if s.startswith('org.apache.spark.sql.catalyst.parser.ParseException: '):\n        return ParseException(s.split(': ', 1)[1], stacktrace, c)\n    if s.startswith('org.apache.spark.sql.streaming.StreamingQueryException: '):\n        return StreamingQueryException(s.split(': ', 1)[1], stacktrace, c)"
  },
  {
    "id" : "7b802ff8-982e-4224-a09d-08c0400e1c2b",
    "prId" : 25814,
    "prUrl" : "https://github.com/apache/spark/pull/25814#pullrequestreview-289916062",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bb7a5f64-6142-412b-b568-e19005314f0b",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Let's remove this once we drop Python 2, which will be right away after Spark 3.0 release.",
        "createdAt" : "2019-09-18T11:16:12Z",
        "updatedAt" : "2019-09-18T13:24:47Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "26c7ed08-c208-455e-a5cb-d45b5e8ced2c",
        "parentId" : "bb7a5f64-6142-412b-b568-e19005314f0b",
        "authorId" : "8e87861a-4202-49a2-baf7-6d51f6aaa5a2",
        "body" : "OK.",
        "createdAt" : "2019-09-18T13:25:56Z",
        "updatedAt" : "2019-09-18T13:25:57Z",
        "lastEditedBy" : "8e87861a-4202-49a2-baf7-6d51f6aaa5a2",
        "tags" : [
        ]
      }
    ],
    "commit" : "9e4e7e98cf4cca4248afb81c34168eef63fbd5cf",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +30,34 @@        # encode unicode instance for python2 for human readable description\n        if sys.version_info.major < 3 and isinstance(desc, unicode):\n            return str(desc.encode('utf-8'))\n        else:\n            return str(desc)"
  }
]