[
  {
    "id" : "5939e231-0bcb-4d5f-9598-877970de1609",
    "prId" : 29703,
    "prUrl" : "https://github.com/apache/spark/pull/29703#pullrequestreview-493076094",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "49066a09-c647-4073-aa7e-c58d3d81706b",
        "parentId" : null,
        "authorId" : "43998e22-6c2f-401d-9914-8cecf6fad929",
        "body" : "We should insert this entry before `os.path.dirname(os.path.realpath(__file__))`, which is the same as the module home if pip is used for the installation; otherwise, the module home will be the spark home and the distribution under `spark-distribution` is not used.",
        "createdAt" : "2020-09-22T00:27:08Z",
        "updatedAt" : "2020-09-22T04:29:30Z",
        "lastEditedBy" : "43998e22-6c2f-401d-9914-8cecf6fad929",
        "tags" : [
        ]
      },
      {
        "id" : "a24bf3d2-b2e1-4f88-92d8-95365b711551",
        "parentId" : "49066a09-c647-4073-aa7e-c58d3d81706b",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "`os.path.dirname(os.path.realpath(__file__))` is usually in a script directory because `find_spark_home.py` is included as a script at `setup.py`: https://github.com/apache/spark/blob/058e61ae143a3619fc13fb0c316044a75f7bc15f/python/setup.py#L187\r\n\r\nfor example, it will be user's `bin`.\r\n\r\nI thought `os.path.dirname(os.path.realpath(__file__))` is more just for dev purpose. I checked that it correctly points out the newly installed Spark home.\r\n\r\nHowever, sure. why not be safer :-) I will update.",
        "createdAt" : "2020-09-22T01:00:35Z",
        "updatedAt" : "2020-09-22T04:29:30Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "1251c1a3-3b7b-42a1-ab4d-f8d19b334c13",
        "parentId" : "49066a09-c647-4073-aa7e-c58d3d81706b",
        "authorId" : "43998e22-6c2f-401d-9914-8cecf6fad929",
        "body" : "The function in the file `find_spark_home.py` is called from `launch_gateway`, so if the users initialize Spark by themselves in their Python repr, the path will be the module home?\r\n\r\n```py\r\n$ python\r\n>>> from pyspark.sql import SparkSession\r\n>>> spark = SparkSession.builder.getOrCreate()\r\n```",
        "createdAt" : "2020-09-22T01:13:08Z",
        "updatedAt" : "2020-09-22T04:29:30Z",
        "lastEditedBy" : "43998e22-6c2f-401d-9914-8cecf6fad929",
        "tags" : [
        ]
      }
    ],
    "commit" : "20491e0cdd5b1fae207cf20d8091d4c456728b39",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +51,55 @@    try:\n        module_home = os.path.dirname(find_spec(\"pyspark\").origin)\n        paths.append(os.path.join(module_home, spark_dist_dir))\n        paths.append(module_home)\n        # If we are installed in edit mode also look two dirs up"
  }
]