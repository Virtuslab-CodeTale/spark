[
  {
    "id" : "3bb540fa-16bf-4c6a-9f70-43cd89e04327",
    "prId" : 31768,
    "prUrl" : "https://github.com/apache/spark/pull/31768#pullrequestreview-608961960",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e9ab9f44-ca9f-485e-b88b-eb00025b4cac",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "It exposes a Py4J instance as is which we should avoid. Defining a Hadoop configuration class in PySpark doesn't make sense either. I am not sure which approach is the best. We could probably leave it and let advanced-users to use `sc. _jsc.hadoopConfiguration` for now.",
        "createdAt" : "2021-03-07T04:56:24Z",
        "updatedAt" : "2021-03-07T04:56:24Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "fc7cdb9e-a525-4257-a307-97fc80998fee",
        "parentId" : "e9ab9f44-ca9f-485e-b88b-eb00025b4cac",
        "authorId" : "2d00fbfd-70c3-44f4-ae26-f5fbabeb7d0b",
        "body" : "I had faced this issue when tried to setup s3a properties in pyspark job. The way how to do it was not present anywhere in the documentation but found a solution on [Stackoverflow.](https://stackoverflow.com/questions/28844631/how-to-set-hadoop-configuration-values-from-pyspark) Digging deeper I found a similar ticket opened in [Jira](https://issues.apache.org/jira/browse/SPARK-33436). That's why this PR was created. If we really do not need it, it can be closed. But in my mind, it is logical to have the same sc API I Scala and Python.",
        "createdAt" : "2021-03-07T20:22:08Z",
        "updatedAt" : "2021-03-07T20:22:08Z",
        "lastEditedBy" : "2d00fbfd-70c3-44f4-ae26-f5fbabeb7d0b",
        "tags" : [
        ]
      },
      {
        "id" : "17de83de-0f9f-4977-a2cb-75fa5d6fcc77",
        "parentId" : "e9ab9f44-ca9f-485e-b88b-eb00025b4cac",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "If the point is to set the Hadoop configurations, you can add `spark.hadoop` prefix and set to Spark configurations.",
        "createdAt" : "2021-03-07T23:47:51Z",
        "updatedAt" : "2021-03-07T23:47:52Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "e397b383-3cc1-4cc5-8b30-10b359a139f3",
        "parentId" : "e9ab9f44-ca9f-485e-b88b-eb00025b4cac",
        "authorId" : "2d00fbfd-70c3-44f4-ae26-f5fbabeb7d0b",
        "body" : "What the purpose of this [ticket](https://issues.apache.org/jira/browse/SPARK-33436) in this case? @nchammas could you please clarify?",
        "createdAt" : "2021-03-08T14:34:52Z",
        "updatedAt" : "2021-03-08T14:38:16Z",
        "lastEditedBy" : "2d00fbfd-70c3-44f4-ae26-f5fbabeb7d0b",
        "tags" : [
        ]
      },
      {
        "id" : "6ff473c3-de63-471b-b8ff-20007d047f50",
        "parentId" : "e9ab9f44-ca9f-485e-b88b-eb00025b4cac",
        "authorId" : "fd6ebc48-7da4-490b-8d41-2e8530d92720",
        "body" : "The purpose of the ticket is as [you described](https://github.com/apache/spark/pull/31768#discussion_r589082773). \r\n\r\nThe story I relate in the ticket description is basically the same as yours: I wanted to set some Hadoop configs related to S3A; I couldn't find a direct way to do that; the best approach I could find went through `_.jsc`, so I thought that merited an improvement to PySpark so it had a direct means of doing the same thing.",
        "createdAt" : "2021-03-08T15:42:36Z",
        "updatedAt" : "2021-03-08T15:42:36Z",
        "lastEditedBy" : "fd6ebc48-7da4-490b-8d41-2e8530d92720",
        "tags" : [
        ]
      },
      {
        "id" : "1ea965a8-ded9-496f-b515-0e456bfa5432",
        "parentId" : "e9ab9f44-ca9f-485e-b88b-eb00025b4cac",
        "authorId" : "2d00fbfd-70c3-44f4-ae26-f5fbabeb7d0b",
        "body" : "Ok, closing PR. I'd suggest abandoning the Jira ticket also, not to confuse people in the future.",
        "createdAt" : "2021-03-10T16:39:05Z",
        "updatedAt" : "2021-03-10T16:39:06Z",
        "lastEditedBy" : "2d00fbfd-70c3-44f4-ae26-f5fbabeb7d0b",
        "tags" : [
        ]
      },
      {
        "id" : "06b8340c-be3a-4ebe-b46a-fe2277d8cd56",
        "parentId" : "e9ab9f44-ca9f-485e-b88b-eb00025b4cac",
        "authorId" : "fd6ebc48-7da4-490b-8d41-2e8530d92720",
        "body" : "I'm not following why the ticket should be abandoned.\r\n\r\nIf there is an alternative solution for PySpark users who want to set S3A configs -- something that a) does not require them to go through `._jsc`, and b) is tested and works -- then please post a clear example of that on the ticket. That would serve as a useful reference to others.\r\n\r\nOtherwise, I think the ticket is valid and should remain open.",
        "createdAt" : "2021-03-10T17:11:28Z",
        "updatedAt" : "2021-03-10T17:11:29Z",
        "lastEditedBy" : "fd6ebc48-7da4-490b-8d41-2e8530d92720",
        "tags" : [
        ]
      }
    ],
    "commit" : "707dd09e5b5f882c006648476d4134b3bebaa048",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +1264,1268 @@        Return :class:`Configuration` object\n        \"\"\"\n        return self._jsc.hadoopConfiguration()\n\n    @property"
  }
]