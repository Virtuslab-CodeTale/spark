[
  {
    "id" : "3bb540fa-16bf-4c6a-9f70-43cd89e04327",
    "prId" : 31768,
    "prUrl" : "https://github.com/apache/spark/pull/31768#pullrequestreview-608961960",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e9ab9f44-ca9f-485e-b88b-eb00025b4cac",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "It exposes a Py4J instance as is which we should avoid. Defining a Hadoop configuration class in PySpark doesn't make sense either. I am not sure which approach is the best. We could probably leave it and let advanced-users to use `sc. _jsc.hadoopConfiguration` for now.",
        "createdAt" : "2021-03-07T04:56:24Z",
        "updatedAt" : "2021-03-07T04:56:24Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "fc7cdb9e-a525-4257-a307-97fc80998fee",
        "parentId" : "e9ab9f44-ca9f-485e-b88b-eb00025b4cac",
        "authorId" : "2d00fbfd-70c3-44f4-ae26-f5fbabeb7d0b",
        "body" : "I had faced this issue when tried to setup s3a properties in pyspark job. The way how to do it was not present anywhere in the documentation but found a solution on [Stackoverflow.](https://stackoverflow.com/questions/28844631/how-to-set-hadoop-configuration-values-from-pyspark) Digging deeper I found a similar ticket opened in [Jira](https://issues.apache.org/jira/browse/SPARK-33436). That's why this PR was created. If we really do not need it, it can be closed. But in my mind, it is logical to have the same sc API I Scala and Python.",
        "createdAt" : "2021-03-07T20:22:08Z",
        "updatedAt" : "2021-03-07T20:22:08Z",
        "lastEditedBy" : "2d00fbfd-70c3-44f4-ae26-f5fbabeb7d0b",
        "tags" : [
        ]
      },
      {
        "id" : "17de83de-0f9f-4977-a2cb-75fa5d6fcc77",
        "parentId" : "e9ab9f44-ca9f-485e-b88b-eb00025b4cac",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "If the point is to set the Hadoop configurations, you can add `spark.hadoop` prefix and set to Spark configurations.",
        "createdAt" : "2021-03-07T23:47:51Z",
        "updatedAt" : "2021-03-07T23:47:52Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "e397b383-3cc1-4cc5-8b30-10b359a139f3",
        "parentId" : "e9ab9f44-ca9f-485e-b88b-eb00025b4cac",
        "authorId" : "2d00fbfd-70c3-44f4-ae26-f5fbabeb7d0b",
        "body" : "What the purpose of this [ticket](https://issues.apache.org/jira/browse/SPARK-33436) in this case? @nchammas could you please clarify?",
        "createdAt" : "2021-03-08T14:34:52Z",
        "updatedAt" : "2021-03-08T14:38:16Z",
        "lastEditedBy" : "2d00fbfd-70c3-44f4-ae26-f5fbabeb7d0b",
        "tags" : [
        ]
      },
      {
        "id" : "6ff473c3-de63-471b-b8ff-20007d047f50",
        "parentId" : "e9ab9f44-ca9f-485e-b88b-eb00025b4cac",
        "authorId" : "fd6ebc48-7da4-490b-8d41-2e8530d92720",
        "body" : "The purpose of the ticket is as [you described](https://github.com/apache/spark/pull/31768#discussion_r589082773). \r\n\r\nThe story I relate in the ticket description is basically the same as yours: I wanted to set some Hadoop configs related to S3A; I couldn't find a direct way to do that; the best approach I could find went through `_.jsc`, so I thought that merited an improvement to PySpark so it had a direct means of doing the same thing.",
        "createdAt" : "2021-03-08T15:42:36Z",
        "updatedAt" : "2021-03-08T15:42:36Z",
        "lastEditedBy" : "fd6ebc48-7da4-490b-8d41-2e8530d92720",
        "tags" : [
        ]
      },
      {
        "id" : "1ea965a8-ded9-496f-b515-0e456bfa5432",
        "parentId" : "e9ab9f44-ca9f-485e-b88b-eb00025b4cac",
        "authorId" : "2d00fbfd-70c3-44f4-ae26-f5fbabeb7d0b",
        "body" : "Ok, closing PR. I'd suggest abandoning the Jira ticket also, not to confuse people in the future.",
        "createdAt" : "2021-03-10T16:39:05Z",
        "updatedAt" : "2021-03-10T16:39:06Z",
        "lastEditedBy" : "2d00fbfd-70c3-44f4-ae26-f5fbabeb7d0b",
        "tags" : [
        ]
      },
      {
        "id" : "06b8340c-be3a-4ebe-b46a-fe2277d8cd56",
        "parentId" : "e9ab9f44-ca9f-485e-b88b-eb00025b4cac",
        "authorId" : "fd6ebc48-7da4-490b-8d41-2e8530d92720",
        "body" : "I'm not following why the ticket should be abandoned.\r\n\r\nIf there is an alternative solution for PySpark users who want to set S3A configs -- something that a) does not require them to go through `._jsc`, and b) is tested and works -- then please post a clear example of that on the ticket. That would serve as a useful reference to others.\r\n\r\nOtherwise, I think the ticket is valid and should remain open.",
        "createdAt" : "2021-03-10T17:11:28Z",
        "updatedAt" : "2021-03-10T17:11:29Z",
        "lastEditedBy" : "fd6ebc48-7da4-490b-8d41-2e8530d92720",
        "tags" : [
        ]
      }
    ],
    "commit" : "707dd09e5b5f882c006648476d4134b3bebaa048",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +1264,1268 @@        Return :class:`Configuration` object\n        \"\"\"\n        return self._jsc.hadoopConfiguration()\n\n    @property"
  },
  {
    "id" : "f7dbd972-c291-48dd-8b4b-ca37286ecab1",
    "prId" : 29918,
    "prUrl" : "https://github.com/apache/spark/pull/29918#pullrequestreview-501362666",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "93a084ff-114a-44b9-8ed2-59011a46d46a",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Let's also decorate this with `@since(3.1)`.",
        "createdAt" : "2020-10-02T04:39:49Z",
        "updatedAt" : "2020-10-04T19:21:07Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "b54eb388-f87f-46ac-87bb-7e99d9c1209a",
        "parentId" : "93a084ff-114a-44b9-8ed2-59011a46d46a",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Should also add the corresponding type hint below https://github.com/apache/spark/blob/master/python/pyspark/context.pyi#L154 line. cc @zero323 FYI.",
        "createdAt" : "2020-10-02T04:40:38Z",
        "updatedAt" : "2020-10-04T19:21:07Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "434d8a54-a4d5-42d3-a46e-55a1e821a6d7",
        "parentId" : "93a084ff-114a-44b9-8ed2-59011a46d46a",
        "authorId" : "981b170c-729a-429c-b115-0350ea50b32b",
        "body" : "Just for the reference â€’ it will be just\r\n\r\n```python\r\n    def getCheckpointDir(self) -> Optional[str]: ...\r\n```",
        "createdAt" : "2020-10-02T07:09:15Z",
        "updatedAt" : "2020-10-04T19:21:07Z",
        "lastEditedBy" : "981b170c-729a-429c-b115-0350ea50b32b",
        "tags" : [
        ]
      },
      {
        "id" : "ea2dab3a-96d0-46a9-9be1-07ef31c670bc",
        "parentId" : "93a084ff-114a-44b9-8ed2-59011a46d46a",
        "authorId" : "95062d94-8b3b-411d-adc4-5537465c09a9",
        "body" : "Thanks for the review. Do you know how to properly import ``since`` into ``python/pyspark/context.py``? ``since`` is defined in the ``python/pyspark/__init__.py`` and I haven't been able to import it into ``python/pyspark/context.py`` without getting errors.",
        "createdAt" : "2020-10-02T17:54:11Z",
        "updatedAt" : "2020-10-04T19:21:07Z",
        "lastEditedBy" : "95062d94-8b3b-411d-adc4-5537465c09a9",
        "tags" : [
        ]
      },
      {
        "id" : "b2bb795f-a4c3-4f84-9f84-da17e83bd7e5",
        "parentId" : "93a084ff-114a-44b9-8ed2-59011a46d46a",
        "authorId" : "981b170c-729a-429c-b115-0350ea50b32b",
        "body" : "That's because of circular dependency, right?\r\n\r\nI think the only way to make it work is to move \r\n\r\nhttps://github.com/apache/spark/blob/82721ce00b6cf535abd3d9cd66445e452554d15d/python/pyspark/__init__.py#L53\r\n\r\nsomewhere after `since` definition, for example here:\r\n\r\nhttps://github.com/apache/spark/blob/82721ce00b6cf535abd3d9cd66445e452554d15d/python/pyspark/__init__.py#L116-L117\r\n\r\nI am not sure if we want to go this way though.",
        "createdAt" : "2020-10-02T18:35:28Z",
        "updatedAt" : "2020-10-04T19:21:07Z",
        "lastEditedBy" : "981b170c-729a-429c-b115-0350ea50b32b",
        "tags" : [
        ]
      }
    ],
    "commit" : "7ff4e88627505e092281ec50f6237b3ae994638d",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +958,962 @@\n    @since(3.1)\n    def getCheckpointDir(self):\n        \"\"\"\n        Return the directory where RDDs are checkpointed. Returns None if no"
  },
  {
    "id" : "55578ea3-c643-488e-814b-91353bb89d55",
    "prId" : 28648,
    "prUrl" : "https://github.com/apache/spark/pull/28648#pullrequestreview-420179860",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2c28f751-3b77-477f-9d24-0bd4bc290e27",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "So if the first one is a JavaPairRDD, but another is a JavaRDD, this will fail I think when the array is created on the JVM side? do we want to check whether _all_ are a pair or double RDD? I haven't thought through that but it occurred to me.",
        "createdAt" : "2020-05-28T14:10:05Z",
        "updatedAt" : "2020-05-29T08:03:10Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "c6faa275-9490-4919-a535-11de078ef5a5",
        "parentId" : "2c28f751-3b77-477f-9d24-0bd4bc290e27",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "It will still work:\r\n\r\n```python\r\n>>> rdd1 = sc.parallelize([1,2,3,4,5])\r\n>>> rdd2 = sc.parallelize([6,7,8,9,10])\r\n>>> pairRDD1 = rdd1.zip(rdd2)\r\n>>> sc.union([rdd1, pairRDD1]).collect()\r\n[1, 2, 3, 4, 5, (1, 6), (2, 7), (3, 8), (4, 9), (5, 10)]\r\n>>> sc.union([pairRDD1, rdd1]).collect()\r\n[(1, 6), (2, 7), (3, 8), (4, 9), (5, 10), 1, 2, 3, 4, 5]\r\n```\r\n\r\nbecause it converts all to `JavaRDD` via `self._reserialize()` when they are different types.",
        "createdAt" : "2020-05-28T14:25:42Z",
        "updatedAt" : "2020-05-29T08:03:10Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "79e54cf2-54a2-4ae9-a5b7-0809ef7adb72",
        "parentId" : "2c28f751-3b77-477f-9d24-0bd4bc290e27",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "And .. actually I think `JavaDoubleRDD` can't be exposed in PySpark if I am not mistaken. It might be unreachable codes but It's just to restore the previous behaviour as is to be extra safe like https://github.com/apache/spark/pull/28648#discussion_r430825553.",
        "createdAt" : "2020-05-28T14:38:16Z",
        "updatedAt" : "2020-05-29T08:03:10Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "a7b2af405e6050372930c41181ccb974210f262f",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +873,877 @@        if is_instance_of(gw, rdds[0]._jrdd, jrdd_cls):\n            cls = jrdd_cls\n        elif is_instance_of(gw, rdds[0]._jrdd, jpair_rdd_cls):\n            cls = jpair_rdd_cls\n        elif is_instance_of(gw, rdds[0]._jrdd, jdouble_rdd_cls):"
  },
  {
    "id" : "7e031db5-1447-4911-807e-9d7cdde1aa08",
    "prId" : 28648,
    "prUrl" : "https://github.com/apache/spark/pull/28648#pullrequestreview-421509433",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "85c41f4c-fa66-4ad3-bddc-b3067206f312",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Just curious. Does abstract class like `AbstractJavaRDDLike` work here? I think JavaRDD, JavaPairRDD all inherit `AbstractJavaRDDLike`.",
        "createdAt" : "2020-05-29T15:33:18Z",
        "updatedAt" : "2020-05-29T15:33:18Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "ca0b1a12-57c1-43b2-81c5-9da22492d858",
        "parentId" : "85c41f4c-fa66-4ad3-bddc-b3067206f312",
        "authorId" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "body" : "Yeah, was wondering this also?",
        "createdAt" : "2020-05-29T18:39:42Z",
        "updatedAt" : "2020-05-29T18:39:45Z",
        "lastEditedBy" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "tags" : [
        ]
      },
      {
        "id" : "b5ead7d2-9567-447a-8051-ffbf6c36488c",
        "parentId" : "85c41f4c-fa66-4ad3-bddc-b3067206f312",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Nope, it seems not working:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/.../spark/python/pyspark/context.py\", line 885, in union\r\n    return RDD(self._jsc.union(jrdds), self, rdds[0]._jrdd_deserializer)\r\n  File \"/.../spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1305, in __call__\r\n  File \"/.../spark/python/pyspark/sql/utils.py\", line 98, in deco\r\n    return f(*a, **kw)\r\n  File \"/.../spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\", line 332, in get_return_value\r\npy4j.protocol.Py4JError: An error occurred while calling o15.union. Trace:\r\npy4j.Py4JException: Method union([class [Lorg.apache.spark.api.java.AbstractJavaRDDLike;]) does not exist\r\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)\r\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326)\r\n\tat py4j.Gateway.invoke(Gateway.java:274)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n```",
        "createdAt" : "2020-05-31T09:36:12Z",
        "updatedAt" : "2020-05-31T09:36:13Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "0055b73f-815f-4baf-b4f0-f51bbdf6f965",
        "parentId" : "85c41f4c-fa66-4ad3-bddc-b3067206f312",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "hmm, I see. It's interesting. Generally I think it should work, but `AbstractJavaRDDLike` doesn't implement some `JavaRDD` methods, e.g. `union`. For now this fix is okay.",
        "createdAt" : "2020-05-31T16:32:32Z",
        "updatedAt" : "2020-05-31T16:32:33Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "a7b2af405e6050372930c41181ccb974210f262f",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +880,884 @@            cls_name = rdds[0]._jrdd.getClass().getCanonicalName()\n            raise TypeError(\"Unsupported Java RDD class %s\" % cls_name)\n        jrdds = gw.new_array(cls, len(rdds))\n        for i in range(0, len(rdds)):\n            jrdds[i] = rdds[i]._jrdd"
  },
  {
    "id" : "ff1f28e9-0a7f-4e1a-a5fa-300e42bc2ed0",
    "prId" : 28603,
    "prUrl" : "https://github.com/apache/spark/pull/28603#pullrequestreview-417065536",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "baca081c-8822-4094-b9f3-d32b8d6cd0fe",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Can you check https://github.com/apache/spark/pull/19498? The best way might be just add `lambda x: x` and make it `JavaRDD`.",
        "createdAt" : "2020-05-22T02:02:42Z",
        "updatedAt" : "2020-05-22T20:07:32Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "0b5c94af-00f5-4690-9437-80c3cb5f3aad",
        "parentId" : "baca081c-8822-4094-b9f3-d32b8d6cd0fe",
        "authorId" : "fb1da7a4-0c05-42a2-9913-c8bd60c58a4f",
        "body" : "sure will take a look thanks",
        "createdAt" : "2020-05-22T13:11:01Z",
        "updatedAt" : "2020-05-22T20:07:32Z",
        "lastEditedBy" : "fb1da7a4-0c05-42a2-9913-c8bd60c58a4f",
        "tags" : [
        ]
      },
      {
        "id" : "11dd17cd-63b8-447a-a0cb-2cfbb6983fe0",
        "parentId" : "baca081c-8822-4094-b9f3-d32b8d6cd0fe",
        "authorId" : "fb1da7a4-0c05-42a2-9913-c8bd60c58a4f",
        "body" : "@HyukjinKwon Addressed plz take a look again thanks",
        "createdAt" : "2020-05-22T17:11:18Z",
        "updatedAt" : "2020-05-22T20:07:32Z",
        "lastEditedBy" : "fb1da7a4-0c05-42a2-9913-c8bd60c58a4f",
        "tags" : [
        ]
      }
    ],
    "commit" : "c65be0d6ac8e78f56ca8cd063d9db1012b65f0ab",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +869,873 @@        cls = SparkContext._jvm.org.apache.spark.api.java.JavaRDD\n        is_jrdd = is_instance_of(gw, rdds[0]._jrdd, cls)\n        jrdds = gw.new_array(cls, len(rdds))\n        for i in range(0, len(rdds)):\n            if is_jrdd:"
  },
  {
    "id" : "7849d37c-f44f-4c50-a61c-acdf22e12546",
    "prId" : 24898,
    "prUrl" : "https://github.com/apache/spark/pull/24898#pullrequestreview-312700010",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e6833edc-a9bd-4653-9ef6-319869d9ff16",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@squito, if I should write a warning, I think it will be like this. I don't feel strongly about this so I am not sure.\r\n\r\nFYI, in case Python, with default setting, the warning will be issued once. So if we're worried about being too noisy, it's fine.",
        "createdAt" : "2019-10-23T01:09:23Z",
        "updatedAt" : "2019-11-07T10:32:20Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "ae11e5ff-7014-4300-916e-102ee8fff769",
        "parentId" : "e6833edc-a9bd-4653-9ef6-319869d9ff16",
        "authorId" : "e913f9af-103d-40d4-976e-9aa8e12e7211",
        "body" : "Unless you feel strongly about it, I'd rather have a hard failure if you try to call this, when you are not in pinned mode.\r\n\r\nDid you intentionally want a warning even if you are in pinned mode, just to include the info about inherited threads?  I think warning is too strong in that case, how about info?  Also the docstring should be updated to include the caveat on inherited threads",
        "createdAt" : "2019-10-24T16:04:09Z",
        "updatedAt" : "2019-11-07T10:32:20Z",
        "lastEditedBy" : "e913f9af-103d-40d4-976e-9aa8e12e7211",
        "tags" : [
        ]
      },
      {
        "id" : "f1988782-2eca-43eb-9e3c-44765f30f7fe",
        "parentId" : "e6833edc-a9bd-4653-9ef6-319869d9ff16",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "What I am worries are .. \r\n\r\nFirstly, people use it although it's buggy because it kind of works okay in single thread without a pin-thread mode. Seems like there is still possibility that another thread is launched and local properties are reset though.\r\n\r\nSecondly, even with pin-thread mode, it does not work properly about inherited threads, yes, as you said.\r\n\r\nAbout warning vs info, PySpark currently does not have a proper logging system .. so we should rely on manual printing out or `warning` module (which can be integrated logging system later if we happen to add it to PySpark).\r\nIf we use manual printing way, it's tricky for users to control it. In case of warning, they can control, for instance, if they want to print out the warning only for the initial call or every call.",
        "createdAt" : "2019-10-25T00:14:00Z",
        "updatedAt" : "2019-11-07T10:32:20Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "27835f62-4444-4b9b-87b8-4ec1bedfa6f5",
        "parentId" : "e6833edc-a9bd-4653-9ef6-319869d9ff16",
        "authorId" : "e913f9af-103d-40d4-976e-9aa8e12e7211",
        "body" : "ok, I see your point.  I don't feel great about any of our options here, but perhaps you are right.  I do wish there was a way we could warn more loudly when using this in non-pinned mode.\r\n\r\nI'm just brainstorming here -- another thing we could do is just have the spark UI display some big warning for all of these fields whenever you're using pyspark in non-pinned mode (if we can reliabaly detect this).  Eg. job group would just say \"Unavailable with pyspark in non-pinned mode -- see docs (link)\".\r\n\r\n(Even if we want to do that, doesn't need to be done here.)",
        "createdAt" : "2019-10-25T14:49:45Z",
        "updatedAt" : "2019-11-07T10:32:20Z",
        "lastEditedBy" : "e913f9af-103d-40d4-976e-9aa8e12e7211",
        "tags" : [
        ]
      },
      {
        "id" : "a34b7fc2-2686-4ee6-82c8-3579685dd94b",
        "parentId" : "e6833edc-a9bd-4653-9ef6-319869d9ff16",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I don't know yet because this job group things (with non-pinned mode) work kind of fine in a single thread (without multiple threads). We might have to detect if the current thread is child thread (by checking if current thread is the main thread) .. but I think it will miss some corner cases ..",
        "createdAt" : "2019-10-29T03:41:13Z",
        "updatedAt" : "2019-11-07T10:32:20Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "40a21376-f3ff-4821-b791-17834a083d5a",
        "parentId" : "e6833edc-a9bd-4653-9ef6-319869d9ff16",
        "authorId" : "e913f9af-103d-40d4-976e-9aa8e12e7211",
        "body" : "I still think the docstrings should mention PYSPARK_PIN_THREAD.",
        "createdAt" : "2019-11-06T19:52:07Z",
        "updatedAt" : "2019-11-07T10:32:20Z",
        "lastEditedBy" : "e913f9af-103d-40d4-976e-9aa8e12e7211",
        "tags" : [
        ]
      }
    ],
    "commit" : "9e2d83277635b02cc03f35502294892dc7d8a3d2",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +1020,1024 @@            properties from the parent thread to the child thread when you create another thread.\n        \"\"\"\n        warnings.warn(\n            \"Currently, setting a group ID (set to local properties) with a thread does \"\n            \"not properly work. \""
  }
]