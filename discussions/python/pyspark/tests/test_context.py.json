[
  {
    "id" : "4ea77df2-ee32-4700-8cfa-1d241b3b10d4",
    "prId" : 28986,
    "prUrl" : "https://github.com/apache/spark/pull/28986#pullrequestreview-442352128",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ad0d4df1-a96d-4aa5-a6cd-2ba9b8678556",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I would put a JIRA ID and a short comment like as we lately documented it at http://spark.apache.org/contributing.html\r\n\r\n```python\r\ndef test_case(self):\r\n    # SPARK-12345: a short description of the test\r\n    ...\r\n```",
        "createdAt" : "2020-07-03T11:34:59Z",
        "updatedAt" : "2020-07-07T18:01:23Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "0faac2a4495bebc67819122d7a2d23a750c71ddd",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +270,274 @@    def test_disallow_to_create_spark_context_in_executors(self):\n        # SPARK-32160: SparkContext should not be created in executors.\n        with SparkContext(\"local-cluster[3, 1, 1024]\") as sc:\n            with self.assertRaises(Exception) as context:\n                sc.range(2).foreach(lambda _: SparkContext())"
  }
]