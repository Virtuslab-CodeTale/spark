[
  {
    "id" : "b60ee894-eb4e-4d67-a10c-a048903cc0ad",
    "prId" : 30288,
    "prUrl" : "https://github.com/apache/spark/pull/30288#pullrequestreview-525871701",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b654687b-1a4e-4051-955f-2a6bd7005ea1",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "This seems fine but if you're just looking for longer delays, randint(5,10) or something?",
        "createdAt" : "2020-11-08T18:54:14Z",
        "updatedAt" : "2020-11-08T18:54:14Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "064742a4-1a74-44bd-a235-13714d49ad05",
        "parentId" : "b654687b-1a4e-4051-955f-2a6bd7005ea1",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Maybe i had to add a comment from the original PR .. it has actually a reason for that https://github.com/apache/spark/pull/30277#discussion_r519038459",
        "createdAt" : "2020-11-09T01:16:06Z",
        "updatedAt" : "2020-11-09T01:16:07Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "68259004644a9ef2e926c32f6a8ecb2b27efa0a0",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +128,132 @@        def context_barrier(x):\n            tc = BarrierTaskContext.get()\n            time.sleep(random.randint(1, 5) * 2)\n            tc.barrier()\n            return time.time()"
  },
  {
    "id" : "adfe35d0-5419-4160-963c-bc1e90fdc4b5",
    "prId" : 30277,
    "prUrl" : "https://github.com/apache/spark/pull/30277#pullrequestreview-525535975",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bd7723dc-2537-45c4-9449-3064838fd59d",
        "parentId" : null,
        "authorId" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "body" : "Isn't this basically increasing the minimum sleep time? I would think that tend to make it more flaky..",
        "createdAt" : "2020-11-06T16:56:23Z",
        "updatedAt" : "2020-11-06T16:57:18Z",
        "lastEditedBy" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "tags" : [
        ]
      },
      {
        "id" : "ae2f39f1-d517-48d8-b19e-8feaafff288e",
        "parentId" : "bd7723dc-2537-45c4-9449-3064838fd59d",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "The same question. And, is it different from `random.randint(2, 10)`?",
        "createdAt" : "2020-11-06T22:06:59Z",
        "updatedAt" : "2020-11-06T22:06:59Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "14ddfdd0-60d3-4a68-9d52-f537e78fef7d",
        "parentId" : "bd7723dc-2537-45c4-9449-3064838fd59d",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Previously it failed even when the barrior worked some times. All the tasks properly waited for each other and finished properly but it looks like it could still take around a sec when the tests run in parallel.\r\n\r\nSo I increased the minimum sleep secs and the gap between each task. If barriers don't work, it will take more than 2 secs because the minimum and the gap is 2 seconds. If it works, the gap between each task will take less than 2 seconds.",
        "createdAt" : "2020-11-06T22:34:07Z",
        "updatedAt" : "2020-11-06T22:35:04Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "40b1e40c06ec14df04afdfb9c8c59afe35d0e633",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +125,129 @@        def context_barrier(x):\n            tc = BarrierTaskContext.get()\n            time.sleep(random.randint(1, 5) * 2)\n            tc.barrier()\n            return time.time()"
  },
  {
    "id" : "35b77ea0-5493-45ce-8278-2d3eaf61c8e9",
    "prId" : 26239,
    "prUrl" : "https://github.com/apache/spark/pull/26239#pullrequestreview-306962869",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7fe0b1c5-8807-4167-a726-116ef7581757",
        "parentId" : null,
        "authorId" : "28d581bb-a398-46f0-861f-a016fdb56186",
        "body" : "I have passed tests in python3.6. However, it failed in python2.7. It seems that the python workers aren't reused even the config is set in python2.7.\r\nI tested here with:\r\n```python\r\ndef f(iterator):\r\n  yield os.getpid()\r\n\r\nnormal_result = rdd.mapPartitions(f).collect()\r\nverify(normal_result)   # it passed\r\n\r\nnormal_result = rdd.mapPartitions(f).collect()\r\nnormal_result = rdd.mapPartitions(f).collect()\r\nnormal_result = rdd.mapPartitions(f).collect()\r\nverify(normal_result)  # it failed with pids have changed\r\n``` \r\n\r\nHi @srowen, would you know some background about this? Thanks a lot.",
        "createdAt" : "2019-10-25T01:49:59Z",
        "updatedAt" : "2019-10-31T03:16:28Z",
        "lastEditedBy" : "28d581bb-a398-46f0-861f-a016fdb56186",
        "tags" : [
        ]
      },
      {
        "id" : "e3645dcc-9a6d-45ae-ad86-48e755078867",
        "parentId" : "7fe0b1c5-8807-4167-a726-116ef7581757",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "If you're target 3.0 for this change, that's fine. We don't support 2.x in Spark 3.0. I have no idea otherwise.",
        "createdAt" : "2019-10-25T02:49:14Z",
        "updatedAt" : "2019-10-31T03:16:28Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "5e8749f5-ccf8-4ad2-9fbf-c057e8b74491",
        "parentId" : "7fe0b1c5-8807-4167-a726-116ef7581757",
        "authorId" : "28d581bb-a398-46f0-861f-a016fdb56186",
        "body" : "OK, thanks a lot.",
        "createdAt" : "2019-10-25T03:08:49Z",
        "updatedAt" : "2019-10-31T03:16:28Z",
        "lastEditedBy" : "28d581bb-a398-46f0-861f-a016fdb56186",
        "tags" : [
        ]
      }
    ],
    "commit" : "a3d14c83de89775e5bba7f5a94cc2101dacbe9bb",
    "line" : 89,
    "diffHunk" : "@@ -1,1 +253,257 @@            self.assertTrue(pid in worker_pids)\n        # barrier stage after normal stage\n        barrier_result = rdd.barrier().mapPartitions(context).collect()\n        tps, bps, pids = zip(*barrier_result)\n        self.assertTrue(tps == (0, 1))"
  }
]