[
  {
    "id" : "b85aad48-ec5e-48f5-85a3-de834273f0dd",
    "prId" : 27641,
    "prUrl" : "https://github.com/apache/spark/pull/27641#pullrequestreview-362078601",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "eb084750-7470-4e85-8020-c51471e61fe6",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "This is fine. You could also just not mention version here, to avoid maintaining this reference.",
        "createdAt" : "2020-02-20T14:47:16Z",
        "updatedAt" : "2020-02-20T14:47:51Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "37ee1971-5481-49f1-bf02-6f98b539843d",
        "parentId" : "eb084750-7470-4e85-8020-c51471e61fe6",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Thanks. I'll remove that during merging.",
        "createdAt" : "2020-02-20T17:07:34Z",
        "updatedAt" : "2020-02-20T17:07:34Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "43da2e265754efd524a0cbfe107ac0daf7ec393a",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +30,34 @@## Python Requirements\n\nAt its core PySpark depends on Py4J (currently version 0.10.9), but some additional sub-packages have their own extra requirements for some features (including numpy, pandas, and pyarrow)."
  }
]