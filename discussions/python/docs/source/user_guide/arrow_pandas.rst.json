[
  {
    "id" : "b527fc3f-8672-4479-80f9-6fd4de28680a",
    "prId" : 31738,
    "prUrl" : "https://github.com/apache/spark/pull/31738#pullrequestreview-623135464",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "92c6d33e-8500-46cc-b09c-31d2cfa46e01",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@lidavidm it would be great if we link related issues (so we can remove these notes later when they are solved).",
        "createdAt" : "2021-03-05T04:54:17Z",
        "updatedAt" : "2021-03-29T11:41:29Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "e5374471-c199-4332-be06-71b2f9f62d56",
        "parentId" : "92c6d33e-8500-46cc-b09c-31d2cfa46e01",
        "authorId" : "d1541f17-4fdb-40b8-a545-d0d79a158f71",
        "body" : "Unfortunately there's no real umbrella issue; it's an artifact of Pandas' implementation that's slowly being corrected as people find more cases.",
        "createdAt" : "2021-03-05T14:59:22Z",
        "updatedAt" : "2021-03-29T11:41:29Z",
        "lastEditedBy" : "d1541f17-4fdb-40b8-a545-d0d79a158f71",
        "tags" : [
        ]
      },
      {
        "id" : "0e94f74c-55fd-4999-b950-5b852f83d87a",
        "parentId" : "92c6d33e-8500-46cc-b09c-31d2cfa46e01",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Hm, can we maybe file an issue in pandas side and link it here?",
        "createdAt" : "2021-03-07T04:57:58Z",
        "updatedAt" : "2021-03-29T11:41:29Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "0065c27f-9dda-400d-af91-ed1b3d339220",
        "parentId" : "92c6d33e-8500-46cc-b09c-31d2cfa46e01",
        "authorId" : "d1541f17-4fdb-40b8-a545-d0d79a158f71",
        "body" : "We could link to https://github.com/pandas-dev/pandas/issues/33001 perhaps? It's not a blanket umbrella issue but it does cover a few hundred test cases and explains the issues facing a universal fix on their side.",
        "createdAt" : "2021-03-19T15:03:47Z",
        "updatedAt" : "2021-03-29T11:41:29Z",
        "lastEditedBy" : "d1541f17-4fdb-40b8-a545-d0d79a158f71",
        "tags" : [
        ]
      },
      {
        "id" : "c7cd5a6a-cd9d-41e7-9c5d-22121d38a22b",
        "parentId" : "92c6d33e-8500-46cc-b09c-31d2cfa46e01",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I doubt if that covers the current case here .. our issue happens specifically when the pandas DataFrame is from PyArrow, right?\r\n\r\nI wonder if we can just show a reproducible example and file an issue in pandas dev side to track .. otherwise, this warning will likely stay forever ..\r\n\r\nI am okay with as-is too .. assuming that we're sure on tracking the related issue.",
        "createdAt" : "2021-03-29T08:15:40Z",
        "updatedAt" : "2021-03-29T11:41:29Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "c8b1a36e-19fa-45d8-99d7-53c5667c3e7d",
        "parentId" : "92c6d33e-8500-46cc-b09c-31d2cfa46e01",
        "authorId" : "d1541f17-4fdb-40b8-a545-d0d79a158f71",
        "body" : "No, it's actually anytime that the underlying array is read-only. PyArrow is a common source of this but not the only source.",
        "createdAt" : "2021-03-29T11:41:21Z",
        "updatedAt" : "2021-03-29T11:41:29Z",
        "lastEditedBy" : "d1541f17-4fdb-40b8-a545-d0d79a158f71",
        "tags" : [
        ]
      }
    ],
    "commit" : "b0115e576e8fc1133acee9286e067a42703b792e",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +419,423 @@Newer versions of Pandas may fix these errors by improving support for such cases.\nYou can work around this error by copying the column(s) beforehand.\nAdditionally, this conversion may be slower because it is single-threaded."
  },
  {
    "id" : "5930b49d-cf57-4763-a574-86a9b207f79a",
    "prId" : 31738,
    "prUrl" : "https://github.com/apache/spark/pull/31738#pullrequestreview-606486024",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f82f849c-d8b5-4dd5-a173-e67b752edf69",
        "parentId" : null,
        "authorId" : "7e665d8b-d739-4edf-88c8-7379ff8585c2",
        "body" : "Could we explicitly say which version pandas will trigger the bug ?\r\n\r\nCurrently my test show that pandas version > 1.0.5 will trigger the bug.",
        "createdAt" : "2021-03-08T09:43:49Z",
        "updatedAt" : "2021-03-29T11:41:29Z",
        "lastEditedBy" : "7e665d8b-d739-4edf-88c8-7379ff8585c2",
        "tags" : [
        ]
      },
      {
        "id" : "54da4227-7709-49be-b024-88049dd7e2de",
        "parentId" : "f82f849c-d8b5-4dd5-a173-e67b752edf69",
        "authorId" : "d1541f17-4fdb-40b8-a545-d0d79a158f71",
        "body" : "I think I haven't fully explained the nature of this - it's not any single issue in Pandas, nor is it specific to any particular version. Instead, it's just that depending on how each Pandas operation was implemented underneath, it may or may not have been _declared_ to accept an immutable backing array, independently of whether that operation _could be implemented_ on an immutable array. So whether you see this will depend on what exactly you do with the dataframe, and there's no one version range we can list or one issue we can link to. And indeed, you could see this error see this _without_ this Arrow option enabled; it's just much less likely, since there will be few cases that Arrow can perform a zero-copy conversion in that case.",
        "createdAt" : "2021-03-08T16:41:29Z",
        "updatedAt" : "2021-03-29T11:41:29Z",
        "lastEditedBy" : "d1541f17-4fdb-40b8-a545-d0d79a158f71",
        "tags" : [
        ]
      }
    ],
    "commit" : "b0115e576e8fc1133acee9286e067a42703b792e",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +419,423 @@Newer versions of Pandas may fix these errors by improving support for such cases.\nYou can work around this error by copying the column(s) beforehand.\nAdditionally, this conversion may be slower because it is single-threaded."
  },
  {
    "id" : "382d604d-ac47-4d87-b9e0-ab201bd3fc12",
    "prId" : 31738,
    "prUrl" : "https://github.com/apache/spark/pull/31738#pullrequestreview-616495938",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d3101532-895e-40d2-82c0-53bfe07f9a93",
        "parentId" : null,
        "authorId" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "body" : "Would it be good to say a workaround is to make a copy of the column(s) used in the operation? I suppose they could just disable the setting is most cases though.",
        "createdAt" : "2021-03-18T21:05:32Z",
        "updatedAt" : "2021-03-29T11:41:29Z",
        "lastEditedBy" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "tags" : [
        ]
      },
      {
        "id" : "7d4340bd-f5de-49d7-8f66-f910a9dfc135",
        "parentId" : "d3101532-895e-40d2-82c0-53bfe07f9a93",
        "authorId" : "d1541f17-4fdb-40b8-a545-d0d79a158f71",
        "body" : "Probably, but still worth a brief mention.",
        "createdAt" : "2021-03-19T15:00:04Z",
        "updatedAt" : "2021-03-29T11:41:29Z",
        "lastEditedBy" : "d1541f17-4fdb-40b8-a545-d0d79a158f71",
        "tags" : [
        ]
      }
    ],
    "commit" : "b0115e576e8fc1133acee9286e067a42703b792e",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +416,420 @@Since Spark 3.2, the Spark configuration ``spark.sql.execution.arrow.pyspark.selfDestruct.enabled`` can be used to enable PyArrow's ``self_destruct`` feature, which can save memory when creating a Pandas DataFrame via ``toPandas`` by freeing Arrow-allocated memory while building the Pandas DataFrame.\nThis option is experimental, and some operations may fail on the resulting Pandas DataFrame due to immutable backing arrays.\nTypically, you would see the error ``ValueError: buffer source array is read-only``.\nNewer versions of Pandas may fix these errors by improving support for such cases.\nYou can work around this error by copying the column(s) beforehand."
  },
  {
    "id" : "07cf2b7b-28e6-443f-bc5f-8a33757e1dda",
    "prId" : 30393,
    "prUrl" : "https://github.com/apache/spark/pull/30393#pullrequestreview-533144657",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f6aef9a6-50bc-4cd7-baf2-b5420825ec45",
        "parentId" : null,
        "authorId" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "body" : "I should probably mention MapType only for pyarrow 2.0.0..",
        "createdAt" : "2020-11-18T06:42:36Z",
        "updatedAt" : "2020-11-18T06:48:12Z",
        "lastEditedBy" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "tags" : [
        ]
      },
      {
        "id" : "cc904b21-3f91-44be-b6a8-534b790ce291",
        "parentId" : "f6aef9a6-50bc-4cd7-baf2-b5420825ec45",
        "authorId" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "body" : "done",
        "createdAt" : "2020-11-18T06:49:07Z",
        "updatedAt" : "2020-11-18T06:49:07Z",
        "lastEditedBy" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "tags" : [
        ]
      }
    ],
    "commit" : "3f2ef9867d280b232010cae6b4bee76e8ca1e25d",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +342,346 @@.. currentmodule:: pyspark.sql.types\n\nCurrently, all Spark SQL data types are supported by Arrow-based conversion except\n:class:`ArrayType` of :class:`TimestampType`, and nested :class:`StructType`.\n:class: `MapType` is only supported when using PyArrow 2.0.0 and above."
  },
  {
    "id" : "a7beda3d-45b7-45b5-82a9-c2eb6bfd1019",
    "prId" : 29686,
    "prUrl" : "https://github.com/apache/spark/pull/29686#pullrequestreview-485283080",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5e9c105f-921f-4baa-b6d4-30152638ff05",
        "parentId" : null,
        "authorId" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "body" : "I changed the wording slightly to just state these are the minimum versions. Github checks are currently testing with Pandas 1.1.2 and PyArrow 1.0.1, but the version is not fixed, so they will continue to install the latest. We might also want to think about bumping the minimum version of Pandas to 1.0.0 or higher.",
        "createdAt" : "2020-09-09T18:21:10Z",
        "updatedAt" : "2020-09-09T18:21:10Z",
        "lastEditedBy" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "tags" : [
        ]
      }
    ],
    "commit" : "1cf7db18ce4f0ee94410298ad6baf460bd912dc5",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +387,391 @@~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nFor usage with pyspark.sql, the minimum supported versions of Pandas is 0.23.2 and PyArrow is 1.0.0.\nHigher versions may be used, however, compatibility and data correctness can not be guaranteed and should\nbe verified by the user."
  }
]