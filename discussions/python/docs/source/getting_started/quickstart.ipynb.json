[
  {
    "id" : "d37cf7a1-34de-440e-ae8b-319a1e31508f",
    "prId" : 29491,
    "prUrl" : "https://github.com/apache/spark/pull/29491#pullrequestreview-472669846",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f49ecd85-3135-4177-9d07-0cd003183e20",
        "parentId" : null,
        "authorId" : "fd6ebc48-7da4-490b-8d41-2e8530d92720",
        "body" : "General comment on this:\r\n\r\n> typically by passing a list of ... dictionaries\r\n\r\nInferring the schema from a list of dictionaries is deprecated, even though I think that would be natural for Python users to think of and understand.\r\n\r\ne.g.\r\n\r\n```python\r\n>>> spark.createDataFrame([{'a': 5}])\r\n.../python/pyspark/sql/session.py:378: UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead\r\n  warnings.warn(\"inferring schema from dict is deprecated,\"\r\nDataFrame[a: bigint]\r\n```",
        "createdAt" : "2020-08-20T15:39:48Z",
        "updatedAt" : "2020-08-25T07:51:55Z",
        "lastEditedBy" : "fd6ebc48-7da4-490b-8d41-2e8530d92720",
        "tags" : [
        ]
      },
      {
        "id" : "a3b6e439-7e82-4ce4-a21d-f2c457a8274c",
        "parentId" : "f49ecd85-3135-4177-9d07-0cd003183e20",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Yeah, I faced this one as well. Actually I already dig the history and it seems like deprecated when `Row` APIs came out. I think we should undeprecate it back.\r\n\r\n@nchammas, are you interested in submitting a PR to remove these warnings? Manual tests should be good enough.",
        "createdAt" : "2020-08-21T07:06:36Z",
        "updatedAt" : "2020-08-25T07:51:55Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "cb2609bb-fbf7-4c13-9f43-2975d7172129",
        "parentId" : "f49ecd85-3135-4177-9d07-0cd003183e20",
        "authorId" : "fd6ebc48-7da4-490b-8d41-2e8530d92720",
        "body" : "~Sure, will do.~\r\n\r\nDone: https://github.com/apache/spark/pull/29510",
        "createdAt" : "2020-08-21T17:34:36Z",
        "updatedAt" : "2020-08-25T07:51:55Z",
        "lastEditedBy" : "fd6ebc48-7da4-490b-8d41-2e8530d92720",
        "tags" : [
        ]
      }
    ],
    "commit" : "0e114b6ac47cde46467d18dce2ca8e1ac0650a35",
    "line" : 34,
    "diffHunk" : "@@ -1,1 +32,36 @@    \"## DataFrame Creation\\n\",\n    \"\\n\",\n    \"A PySpark DataFrame can be created via `pyspark.sql.SparkSession.createDataFrame` typically by passing a list of lists, tuples, dictionaries and `pyspark.sql.Row`s, a [pandas DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) and an RDD consisting of such a list.\\n\",\n    \"`pyspark.sql.SparkSession.createDataFrame` takes the `schema` argument to specify the schema of the DataFrame. When it is omitted, PySpark infers the corresponding schema by taking a sample from the data.\\n\",\n    \"\\n\","
  },
  {
    "id" : "6e3cf6cd-ebb7-46c4-a338-88c5b9f27fc4",
    "prId" : 29491,
    "prUrl" : "https://github.com/apache/spark/pull/29491#pullrequestreview-472258548",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a28b6106-2097-4914-bdd3-dfceb6c3d5f5",
        "parentId" : null,
        "authorId" : "fd6ebc48-7da4-490b-8d41-2e8530d92720",
        "body" : "Ew! ðŸ˜„\r\n\r\nWouldn't it be better / more useful to show people how to use a plain Python UDF without Pandas?",
        "createdAt" : "2020-08-20T15:51:27Z",
        "updatedAt" : "2020-08-25T07:51:55Z",
        "lastEditedBy" : "fd6ebc48-7da4-490b-8d41-2e8530d92720",
        "tags" : [
        ]
      },
      {
        "id" : "5b370121-8a9e-45c1-b02b-7b52a31f5b32",
        "parentId" : "a28b6106-2097-4914-bdd3-dfceb6c3d5f5",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Let's avoid promoting plain Python UDFs for now .. since pandas UDFs can do all what Python UDFs do. There was even a discussion about deprecating the plain Python UDFs somewhere before.",
        "createdAt" : "2020-08-21T07:15:58Z",
        "updatedAt" : "2020-08-25T07:51:55Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "0e114b6ac47cde46467d18dce2ca8e1ac0650a35",
    "line" : 834,
    "diffHunk" : "@@ -1,1 +832,836 @@   \"metadata\": {},\n   \"source\": [\n    \"You can also apply a Python native function against each group by using pandas APIs.\"\n   ]\n  },"
  },
  {
    "id" : "49c817e2-a0ea-460f-b653-94c72a27de54",
    "prId" : 29491,
    "prUrl" : "https://github.com/apache/spark/pull/29491#pullrequestreview-471657947",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "723b27f7-9ef6-402e-b9cf-2dc9d9db3980",
        "parentId" : null,
        "authorId" : "fd6ebc48-7da4-490b-8d41-2e8530d92720",
        "body" : "Do you plan to expand this section?",
        "createdAt" : "2020-08-20T15:53:05Z",
        "updatedAt" : "2020-08-25T07:51:55Z",
        "lastEditedBy" : "fd6ebc48-7da4-490b-8d41-2e8530d92720",
        "tags" : [
        ]
      }
    ],
    "commit" : "0e114b6ac47cde46467d18dce2ca8e1ac0650a35",
    "line" : 1039,
    "diffHunk" : "@@ -1,1 +1037,1041 @@   \"metadata\": {},\n   \"source\": [\n    \"## Working with SQL\\n\",\n    \"\\n\",\n    \"DataFrame and Spark SQL share the same execution engine so they can be interchangeably used seamlessly. For example, you can register the DataFrame as a table and run a SQL easily as below:\""
  },
  {
    "id" : "a8b8fca6-64cf-402f-adee-175cdeefa7ca",
    "prId" : 29491,
    "prUrl" : "https://github.com/apache/spark/pull/29491#pullrequestreview-472314435",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2943f46d-2c21-4d07-bd5d-ebc170aa90f7",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I put a couple of more items in \"Working with SQL\" here https://hyukjin-spark.readthedocs.io/en/stable/getting_started/quickstart.html#Working-with-SQL",
        "createdAt" : "2020-08-21T08:47:10Z",
        "updatedAt" : "2020-08-25T07:51:55Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "0e114b6ac47cde46467d18dce2ca8e1ac0650a35",
    "line" : 1039,
    "diffHunk" : "@@ -1,1 +1037,1041 @@   \"metadata\": {},\n   \"source\": [\n    \"## Working with SQL\\n\",\n    \"\\n\",\n    \"DataFrame and Spark SQL share the same execution engine so they can be interchangeably used seamlessly. For example, you can register the DataFrame as a table and run a SQL easily as below:\""
  },
  {
    "id" : "96be30b9-be0b-4e2c-b15d-6aa767cdbbba",
    "prId" : 29491,
    "prUrl" : "https://github.com/apache/spark/pull/29491#pullrequestreview-474211665",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a50e5637-7a9b-4972-b331-23096ff34d0a",
        "parentId" : null,
        "authorId" : "e29a7794-ac2e-4e6f-a690-737e83e1bace",
        "body" : "Where did column `d` and `e` go? \r\n\r\nWe need this:\r\n- https://github.com/apache/spark/pull/28554\r\n- https://github.com/apache/spark/pull/28754",
        "createdAt" : "2020-08-22T10:01:05Z",
        "updatedAt" : "2020-08-25T07:51:55Z",
        "lastEditedBy" : "e29a7794-ac2e-4e6f-a690-737e83e1bace",
        "tags" : [
        ]
      },
      {
        "id" : "a6af04e9-2155-4f41-abcf-bb9e57a2cbd4",
        "parentId" : "a50e5637-7a9b-4972-b331-23096ff34d0a",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I believe this is being discussed in the dev mailing list. Let's discuss there.",
        "createdAt" : "2020-08-25T07:37:54Z",
        "updatedAt" : "2020-08-25T07:51:55Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "0e114b6ac47cde46467d18dce2ca8e1ac0650a35",
    "line" : 373,
    "diffHunk" : "@@ -1,1 +371,375 @@     \"name\": \"stdout\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"+-------+---+---+-------+\\n\",\n      \"|summary|  a|  b|      c|\\n\","
  },
  {
    "id" : "4700d0dd-9606-4667-81e7-df27057e661c",
    "prId" : 29491,
    "prUrl" : "https://github.com/apache/spark/pull/29491#pullrequestreview-474216048",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "712ea5bd-c593-4e04-b01d-6125a94e16e7",
        "parentId" : null,
        "authorId" : "e29a7794-ac2e-4e6f-a690-737e83e1bace",
        "body" : "The `tail()` does not exists:\r\n![image](https://user-images.githubusercontent.com/1134248/90953863-7b56a480-e46f-11ea-84cc-abc5abc1fee6.png)\r\n",
        "createdAt" : "2020-08-22T10:04:04Z",
        "updatedAt" : "2020-08-25T07:51:55Z",
        "lastEditedBy" : "e29a7794-ac2e-4e6f-a690-737e83e1bace",
        "tags" : [
        ]
      },
      {
        "id" : "35b84e61-3eae-4a7f-9627-914d5e3f2538",
        "parentId" : "712ea5bd-c593-4e04-b01d-6125a94e16e7",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Maybe your PySpark was lower than 3.0 (?). Seems working fine in Binder and Python 3.0:\r\n![Screen Shot 2020-08-25 at 4 42 02 PM](https://user-images.githubusercontent.com/6477701/91146620-18c10b00-e6f2-11ea-90d2-7ecaa8546140.png)\r\nThis API was added from Spark 3.0.\r\n",
        "createdAt" : "2020-08-25T07:43:36Z",
        "updatedAt" : "2020-08-25T07:51:55Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "0e114b6ac47cde46467d18dce2ca8e1ac0650a35",
    "line" : 424,
    "diffHunk" : "@@ -1,1 +422,426 @@   \"metadata\": {},\n   \"source\": [\n    \"In order to avoid throwing an out-of-memory exception, use `DataFrame.take()` or `DataFrame.tail()`.\"\n   ]\n  },"
  }
]