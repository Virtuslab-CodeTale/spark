[
  {
    "id" : "7bdb2fc6-7b36-4119-ac5e-e465b14a1b7a",
    "prId" : 30436,
    "prUrl" : "https://github.com/apache/spark/pull/30436#pullrequestreview-535040050",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e54bc1f9-2cff-402b-9a15-e765b696dc0c",
        "parentId" : null,
        "authorId" : "43998e22-6c2f-401d-9914-8cecf6fad929",
        "body" : "Awesome!",
        "createdAt" : "2020-11-20T03:04:55Z",
        "updatedAt" : "2020-11-20T03:05:31Z",
        "lastEditedBy" : "43998e22-6c2f-401d-9914-8cecf6fad929",
        "tags" : [
        ]
      }
    ],
    "commit" : "4826a68e73449cbeb4460121ee3496dcf68b2757",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +73,77 @@- ``without``: Spark pre-built with user-provided Apache Hadoop\n- ``2.7``: Spark pre-built for Apache Hadoop 2.7\n- ``3.2``: Spark pre-built for Apache Hadoop 3.2 and later (default)\n\nNote that this installation way of PySpark with/without a specific Hadoop version is experimental. It can change or be removed between minor releases."
  }
]