[
  {
    "id" : "c619dce9-7572-4981-853d-f81fe8abff52",
    "prId" : 33630,
    "prUrl" : "https://github.com/apache/spark/pull/33630#pullrequestreview-721909143",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4c223112-8a80-4b63-b731-628c87c5e961",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "`MutableAggregationBuffer` itself seems to be used in our API. Is `jsonValue` incompatibility okay?\r\n\r\n```scala\r\n  /**\r\n   * Initializes the given aggregation buffer, i.e. the zero value of the aggregation buffer.\r\n   *\r\n   * The contract should be that applying the merge function on two initial buffers should just\r\n   * return the initial buffer itself, i.e.\r\n   * `merge(initialBuffer, initialBuffer)` should equal `initialBuffer`.\r\n   *\r\n   * @since 1.5.0\r\n   */\r\n  def initialize(buffer: MutableAggregationBuffer): Unit\r\n```\r\n\r\ncc @HyukjinKwon , @cloud-fan ",
        "createdAt" : "2021-08-04T06:15:31Z",
        "updatedAt" : "2021-08-04T06:16:09Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "13bb8eeb-29d7-42cd-aaaa-a8beda1e8a4d",
        "parentId" : "4c223112-8a80-4b63-b731-628c87c5e961",
        "authorId" : "3a227965-84e0-47cf-9974-11293764f028",
        "body" : "`MutableAggregationBuffer.jsonValue` is marked as `private[sql]` so I thought it's OK. But if I'm missing something, please let me know.\r\nMaybe, users need to re-compile their applications due to the binary compatibility of `MutableAggregationBuffer` will break?",
        "createdAt" : "2021-08-04T06:34:54Z",
        "updatedAt" : "2021-08-04T06:34:55Z",
        "lastEditedBy" : "3a227965-84e0-47cf-9974-11293764f028",
        "tags" : [
        ]
      }
    ],
    "commit" : "02782040e91b9ba38d72f43d016727b94efd405d",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +40,44 @@    ProblemFilters.exclude[IncompatibleMethTypeProblem](\"org.apache.spark.ml.param.FloatParam.jValueDecode\"),\n    ProblemFilters.exclude[IncompatibleMethTypeProblem](\"org.apache.spark.mllib.tree.model.TreeEnsembleModel#SaveLoadV1_0.readMetadata\"),\n    ProblemFilters.exclude[IncompatibleResultTypeProblem](\"org.apache.spark.sql.expressions.MutableAggregationBuffer.jsonValue\")\n  )\n"
  },
  {
    "id" : "19b28cc9-6320-402e-b0f4-efcae4950add",
    "prId" : 33199,
    "prUrl" : "https://github.com/apache/spark/pull/33199#pullrequestreview-698552027",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3866bce8-0cff-41f0-8dc6-15b43bc240ad",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "cc @AngersZhuuuu and @srowen for SPARK-34848\r\n- https://github.com/apache/spark/pull/31948",
        "createdAt" : "2021-07-03T01:00:03Z",
        "updatedAt" : "2021-07-03T01:02:42Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "78c6a4572bfccb8a29b2cc239d5f3d2c08ff3dc3",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +56,60 @@\n    // [SPARK-34848][CORE] Add duration to TaskMetricDistributions\n    ProblemFilters.exclude[DirectMissingMethodProblem](\"org.apache.spark.status.api.v1.TaskMetricDistributions.this\"),\n\n    // [SPARK-34488][CORE] Support task Metrics Distributions and executor Metrics Distributions"
  },
  {
    "id" : "3ff7fc91-02ea-4b9d-a5ba-47541cd03708",
    "prId" : 33199,
    "prUrl" : "https://github.com/apache/spark/pull/33199#pullrequestreview-698552138",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "314e4980-0d0c-426c-a2b3-337e59757779",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "cc @AngersZhuuuu and @srowen for SPARK-34488\r\n- https://github.com/apache/spark/pull/31611",
        "createdAt" : "2021-07-03T01:01:22Z",
        "updatedAt" : "2021-07-03T01:03:08Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "78c6a4572bfccb8a29b2cc239d5f3d2c08ff3dc3",
    "line" : 32,
    "diffHunk" : "@@ -1,1 +60,64 @@    // [SPARK-34488][CORE] Support task Metrics Distributions and executor Metrics Distributions\n    // in the REST API call for a specified stage\n    ProblemFilters.exclude[MissingMethodProblem](\"org.apache.spark.status.api.v1.StageData.this\"),\n\n    // [SPARK-35896] Include more granular metrics for stateful operators in StreamingQueryProgress"
  },
  {
    "id" : "e50732dd-7aa1-49e0-b327-1c2b0da53e3a",
    "prId" : 33199,
    "prUrl" : "https://github.com/apache/spark/pull/33199#pullrequestreview-698552202",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "549812cf-d3f7-46ba-85a6-8f25ce07e831",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "cc @vkorukanti and @HeartSaVioR for SPARK-35896\r\n- https://github.com/apache/spark/pull/33091",
        "createdAt" : "2021-07-03T01:02:14Z",
        "updatedAt" : "2021-07-03T01:03:26Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "78c6a4572bfccb8a29b2cc239d5f3d2c08ff3dc3",
    "line" : 35,
    "diffHunk" : "@@ -1,1 +63,67 @@\n    // [SPARK-35896] Include more granular metrics for stateful operators in StreamingQueryProgress\n    ProblemFilters.exclude[DirectMissingMethodProblem](\"org.apache.spark.sql.streaming.StateOperatorProgress.this\"),\n\n    (problem: Problem) => problem match {"
  },
  {
    "id" : "b708c6ba-de4e-4721-b5a2-082ef7943c81",
    "prId" : 33196,
    "prUrl" : "https://github.com/apache/spark/pull/33196#pullrequestreview-698476325",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "efcf79a0-de79-4660-a26f-5aecc3fb03a6",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "We need not address there here, but I'm wondering if we should carry forward all the previous excludes? In master (currently 3.3.0) we're only concerned with excluding changes since 3.2.x. If, somehow, a previous exclusion matched a new, legitimate breaking change of the same type, we'd be masking it, no? or maybe i'm not thinking about it correctly.",
        "createdAt" : "2021-07-02T20:15:56Z",
        "updatedAt" : "2021-07-02T20:15:57Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      }
    ],
    "commit" : "780fd6345d9d6f444534ed47aa4273fa9b810499",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +36,40 @@\n  // Exclude rules for 3.3.x\n  lazy val v33excludes = v32excludes ++ Seq(\n  )\n"
  },
  {
    "id" : "9371f595-6bb2-418a-9a19-2895ff90c437",
    "prId" : 31958,
    "prUrl" : "https://github.com/apache/spark/pull/31958#pullrequestreview-625151975",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "aac1bb92-f75e-4af9-80d6-9b9a181e05a9",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "This is weird, where do we change `org.apache.spark.sql.vectorized.ColumnVector` in this PR?",
        "createdAt" : "2021-03-30T12:43:30Z",
        "updatedAt" : "2021-03-31T20:09:47Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "db7f4d48-7c03-4b8d-86a8-cb5b2f76f74f",
        "parentId" : "aac1bb92-f75e-4af9-80d6-9b9a181e05a9",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "@cloud-fan - yeah it's weird. We don't change `ColumnVector` class at all. Do you have any idea for how to debug on this? I am still checking why, thanks.",
        "createdAt" : "2021-03-31T07:00:40Z",
        "updatedAt" : "2021-03-31T20:09:47Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      },
      {
        "id" : "72f84896-1c2e-482f-a2c0-37482dcd3e17",
        "parentId" : "aac1bb92-f75e-4af9-80d6-9b9a181e05a9",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "maybe it's some bugs in Mima, not a bit deal as we know this PR doesn't break binary compatibility.",
        "createdAt" : "2021-03-31T07:27:24Z",
        "updatedAt" : "2021-03-31T20:09:47Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "82023ef3-7d57-41c9-a6a8-c2e096a525bd",
        "parentId" : "aac1bb92-f75e-4af9-80d6-9b9a181e05a9",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "@cloud-fan - spent some time checking, but still not sure where the issue is, so I agree with you that might be some bug in Mima.",
        "createdAt" : "2021-03-31T10:52:03Z",
        "updatedAt" : "2021-03-31T20:09:47Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      }
    ],
    "commit" : "44feaccadc8fa81ad9f885686685431f3976f537",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +43,47 @@    ProblemFilters.exclude[DirectMissingMethodProblem](\"org.apache.spark.sql.streaming.SourceProgress.this\"),\n\n    // [SPARK-34862][SQL] Support nested column in ORC vectorized reader\n    ProblemFilters.exclude[DirectAbstractMethodProblem](\"org.apache.spark.sql.vectorized.ColumnVector.getBoolean\"),\n    ProblemFilters.exclude[DirectAbstractMethodProblem](\"org.apache.spark.sql.vectorized.ColumnVector.getByte\"),"
  },
  {
    "id" : "712cde81-7ef0-4946-938f-3d579ff194a9",
    "prId" : 31186,
    "prUrl" : "https://github.com/apache/spark/pull/31186#pullrequestreview-568814639",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9f4dbf7b-d226-45f7-8ef2-d818c0c800d0",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Though this class is now package private, Mima exclusion is still needed, unfortunately.",
        "createdAt" : "2021-01-15T04:49:43Z",
        "updatedAt" : "2021-01-15T04:49:44Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "41c22773f7b4ffe7805bd443f35940013b976146",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +108,112 @@    ProblemFilters.exclude[DirectMissingMethodProblem](\"org.apache.spark.sql.SparkSession.this\"),\n\n    // [SPARK-33790][CORE] Reduce the rpc call of getFileStatus in SingleFileEventLogFileReader\n    ProblemFilters.exclude[DirectMissingMethodProblem](\"org.apache.spark.deploy.history.SingleFileEventLogFileReader.this\")\n  )"
  }
]