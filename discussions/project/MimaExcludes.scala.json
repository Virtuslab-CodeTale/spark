[
  {
    "id" : "c619dce9-7572-4981-853d-f81fe8abff52",
    "prId" : 33630,
    "prUrl" : "https://github.com/apache/spark/pull/33630#pullrequestreview-721909143",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4c223112-8a80-4b63-b731-628c87c5e961",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "`MutableAggregationBuffer` itself seems to be used in our API. Is `jsonValue` incompatibility okay?\r\n\r\n```scala\r\n  /**\r\n   * Initializes the given aggregation buffer, i.e. the zero value of the aggregation buffer.\r\n   *\r\n   * The contract should be that applying the merge function on two initial buffers should just\r\n   * return the initial buffer itself, i.e.\r\n   * `merge(initialBuffer, initialBuffer)` should equal `initialBuffer`.\r\n   *\r\n   * @since 1.5.0\r\n   */\r\n  def initialize(buffer: MutableAggregationBuffer): Unit\r\n```\r\n\r\ncc @HyukjinKwon , @cloud-fan ",
        "createdAt" : "2021-08-04T06:15:31Z",
        "updatedAt" : "2021-08-04T06:16:09Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "13bb8eeb-29d7-42cd-aaaa-a8beda1e8a4d",
        "parentId" : "4c223112-8a80-4b63-b731-628c87c5e961",
        "authorId" : "3a227965-84e0-47cf-9974-11293764f028",
        "body" : "`MutableAggregationBuffer.jsonValue` is marked as `private[sql]` so I thought it's OK. But if I'm missing something, please let me know.\r\nMaybe, users need to re-compile their applications due to the binary compatibility of `MutableAggregationBuffer` will break?",
        "createdAt" : "2021-08-04T06:34:54Z",
        "updatedAt" : "2021-08-04T06:34:55Z",
        "lastEditedBy" : "3a227965-84e0-47cf-9974-11293764f028",
        "tags" : [
        ]
      }
    ],
    "commit" : "02782040e91b9ba38d72f43d016727b94efd405d",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +40,44 @@    ProblemFilters.exclude[IncompatibleMethTypeProblem](\"org.apache.spark.ml.param.FloatParam.jValueDecode\"),\n    ProblemFilters.exclude[IncompatibleMethTypeProblem](\"org.apache.spark.mllib.tree.model.TreeEnsembleModel#SaveLoadV1_0.readMetadata\"),\n    ProblemFilters.exclude[IncompatibleResultTypeProblem](\"org.apache.spark.sql.expressions.MutableAggregationBuffer.jsonValue\")\n  )\n"
  },
  {
    "id" : "19b28cc9-6320-402e-b0f4-efcae4950add",
    "prId" : 33199,
    "prUrl" : "https://github.com/apache/spark/pull/33199#pullrequestreview-698552027",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3866bce8-0cff-41f0-8dc6-15b43bc240ad",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "cc @AngersZhuuuu and @srowen for SPARK-34848\r\n- https://github.com/apache/spark/pull/31948",
        "createdAt" : "2021-07-03T01:00:03Z",
        "updatedAt" : "2021-07-03T01:02:42Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "78c6a4572bfccb8a29b2cc239d5f3d2c08ff3dc3",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +56,60 @@\n    // [SPARK-34848][CORE] Add duration to TaskMetricDistributions\n    ProblemFilters.exclude[DirectMissingMethodProblem](\"org.apache.spark.status.api.v1.TaskMetricDistributions.this\"),\n\n    // [SPARK-34488][CORE] Support task Metrics Distributions and executor Metrics Distributions"
  },
  {
    "id" : "3ff7fc91-02ea-4b9d-a5ba-47541cd03708",
    "prId" : 33199,
    "prUrl" : "https://github.com/apache/spark/pull/33199#pullrequestreview-698552138",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "314e4980-0d0c-426c-a2b3-337e59757779",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "cc @AngersZhuuuu and @srowen for SPARK-34488\r\n- https://github.com/apache/spark/pull/31611",
        "createdAt" : "2021-07-03T01:01:22Z",
        "updatedAt" : "2021-07-03T01:03:08Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "78c6a4572bfccb8a29b2cc239d5f3d2c08ff3dc3",
    "line" : 32,
    "diffHunk" : "@@ -1,1 +60,64 @@    // [SPARK-34488][CORE] Support task Metrics Distributions and executor Metrics Distributions\n    // in the REST API call for a specified stage\n    ProblemFilters.exclude[MissingMethodProblem](\"org.apache.spark.status.api.v1.StageData.this\"),\n\n    // [SPARK-35896] Include more granular metrics for stateful operators in StreamingQueryProgress"
  },
  {
    "id" : "e50732dd-7aa1-49e0-b327-1c2b0da53e3a",
    "prId" : 33199,
    "prUrl" : "https://github.com/apache/spark/pull/33199#pullrequestreview-698552202",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "549812cf-d3f7-46ba-85a6-8f25ce07e831",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "cc @vkorukanti and @HeartSaVioR for SPARK-35896\r\n- https://github.com/apache/spark/pull/33091",
        "createdAt" : "2021-07-03T01:02:14Z",
        "updatedAt" : "2021-07-03T01:03:26Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "78c6a4572bfccb8a29b2cc239d5f3d2c08ff3dc3",
    "line" : 35,
    "diffHunk" : "@@ -1,1 +63,67 @@\n    // [SPARK-35896] Include more granular metrics for stateful operators in StreamingQueryProgress\n    ProblemFilters.exclude[DirectMissingMethodProblem](\"org.apache.spark.sql.streaming.StateOperatorProgress.this\"),\n\n    (problem: Problem) => problem match {"
  },
  {
    "id" : "b708c6ba-de4e-4721-b5a2-082ef7943c81",
    "prId" : 33196,
    "prUrl" : "https://github.com/apache/spark/pull/33196#pullrequestreview-698476325",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "efcf79a0-de79-4660-a26f-5aecc3fb03a6",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "We need not address there here, but I'm wondering if we should carry forward all the previous excludes? In master (currently 3.3.0) we're only concerned with excluding changes since 3.2.x. If, somehow, a previous exclusion matched a new, legitimate breaking change of the same type, we'd be masking it, no? or maybe i'm not thinking about it correctly.",
        "createdAt" : "2021-07-02T20:15:56Z",
        "updatedAt" : "2021-07-02T20:15:57Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      }
    ],
    "commit" : "780fd6345d9d6f444534ed47aa4273fa9b810499",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +36,40 @@\n  // Exclude rules for 3.3.x\n  lazy val v33excludes = v32excludes ++ Seq(\n  )\n"
  },
  {
    "id" : "9371f595-6bb2-418a-9a19-2895ff90c437",
    "prId" : 31958,
    "prUrl" : "https://github.com/apache/spark/pull/31958#pullrequestreview-625151975",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "aac1bb92-f75e-4af9-80d6-9b9a181e05a9",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "This is weird, where do we change `org.apache.spark.sql.vectorized.ColumnVector` in this PR?",
        "createdAt" : "2021-03-30T12:43:30Z",
        "updatedAt" : "2021-03-31T20:09:47Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "db7f4d48-7c03-4b8d-86a8-cb5b2f76f74f",
        "parentId" : "aac1bb92-f75e-4af9-80d6-9b9a181e05a9",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "@cloud-fan - yeah it's weird. We don't change `ColumnVector` class at all. Do you have any idea for how to debug on this? I am still checking why, thanks.",
        "createdAt" : "2021-03-31T07:00:40Z",
        "updatedAt" : "2021-03-31T20:09:47Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      },
      {
        "id" : "72f84896-1c2e-482f-a2c0-37482dcd3e17",
        "parentId" : "aac1bb92-f75e-4af9-80d6-9b9a181e05a9",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "maybe it's some bugs in Mima, not a bit deal as we know this PR doesn't break binary compatibility.",
        "createdAt" : "2021-03-31T07:27:24Z",
        "updatedAt" : "2021-03-31T20:09:47Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "82023ef3-7d57-41c9-a6a8-c2e096a525bd",
        "parentId" : "aac1bb92-f75e-4af9-80d6-9b9a181e05a9",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "@cloud-fan - spent some time checking, but still not sure where the issue is, so I agree with you that might be some bug in Mima.",
        "createdAt" : "2021-03-31T10:52:03Z",
        "updatedAt" : "2021-03-31T20:09:47Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      }
    ],
    "commit" : "44feaccadc8fa81ad9f885686685431f3976f537",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +43,47 @@    ProblemFilters.exclude[DirectMissingMethodProblem](\"org.apache.spark.sql.streaming.SourceProgress.this\"),\n\n    // [SPARK-34862][SQL] Support nested column in ORC vectorized reader\n    ProblemFilters.exclude[DirectAbstractMethodProblem](\"org.apache.spark.sql.vectorized.ColumnVector.getBoolean\"),\n    ProblemFilters.exclude[DirectAbstractMethodProblem](\"org.apache.spark.sql.vectorized.ColumnVector.getByte\"),"
  },
  {
    "id" : "712cde81-7ef0-4946-938f-3d579ff194a9",
    "prId" : 31186,
    "prUrl" : "https://github.com/apache/spark/pull/31186#pullrequestreview-568814639",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9f4dbf7b-d226-45f7-8ef2-d818c0c800d0",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Though this class is now package private, Mima exclusion is still needed, unfortunately.",
        "createdAt" : "2021-01-15T04:49:43Z",
        "updatedAt" : "2021-01-15T04:49:44Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "41c22773f7b4ffe7805bd443f35940013b976146",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +108,112 @@    ProblemFilters.exclude[DirectMissingMethodProblem](\"org.apache.spark.sql.SparkSession.this\"),\n\n    // [SPARK-33790][CORE] Reduce the rpc call of getFileStatus in SingleFileEventLogFileReader\n    ProblemFilters.exclude[DirectMissingMethodProblem](\"org.apache.spark.deploy.history.SingleFileEventLogFileReader.this\")\n  )"
  },
  {
    "id" : "f28dc341-df4a-4b4d-a657-248c48d0b051",
    "prId" : 30780,
    "prUrl" : "https://github.com/apache/spark/pull/30780#pullrequestreview-554249713",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "280a6e27-4f93-447e-bb61-34f36c4ab653",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Hi, All. Is this inevitable incompatibility?",
        "createdAt" : "2020-12-17T03:10:17Z",
        "updatedAt" : "2020-12-17T03:10:17Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "09671b7a-37ac-44a7-b9b2-508c84a05f96",
        "parentId" : "280a6e27-4f93-447e-bb61-34f36c4ab653",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "I made a PR to recover this. Let's see.\r\n- https://github.com/apache/spark/pull/30814",
        "createdAt" : "2020-12-17T03:13:52Z",
        "updatedAt" : "2020-12-17T03:13:52Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "4d1d5e4108091678a0e5444a03484fd014a3fcca",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +113,117 @@\n    // [SPARK-33790][CORE] Reduce the rpc call of getFileStatus in SingleFileEventLogFileReader\n    ProblemFilters.exclude[DirectMissingMethodProblem](\"org.apache.spark.deploy.history.SingleFileEventLogFileReader.this\")\n  )\n"
  },
  {
    "id" : "1c58a51f-c738-4903-8787-f5371e1f79a9",
    "prId" : 28710,
    "prUrl" : "https://github.com/apache/spark/pull/28710#pullrequestreview-430434420",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "30a36bb2-b41e-4458-8180-c732d6b70499",
        "parentId" : null,
        "authorId" : "685f805b-e4fa-4f21-b066-58afcecf9ce6",
        "body" : "Are there some MiMa execlusions can be removed after the lastest change?",
        "createdAt" : "2020-06-15T07:07:55Z",
        "updatedAt" : "2020-06-18T17:11:25Z",
        "lastEditedBy" : "685f805b-e4fa-4f21-b066-58afcecf9ce6",
        "tags" : [
        ]
      },
      {
        "id" : "1a773152-3952-4bb4-b317-5b83c7aa148a",
        "parentId" : "30a36bb2-b41e-4458-8180-c732d6b70499",
        "authorId" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "body" : "MiMa exclusions have already been updated",
        "createdAt" : "2020-06-15T08:38:48Z",
        "updatedAt" : "2020-06-18T17:11:25Z",
        "lastEditedBy" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "tags" : [
        ]
      }
    ],
    "commit" : "ead6da2e0e3d1a269e6ef76b23f8e845690f42a3",
    "line" : 42,
    "diffHunk" : "@@ -1,1 +74,78 @@    ProblemFilters.exclude[InheritedNewAbstractMethodProblem](\"org.apache.spark.ml.classification.BinaryLogisticRegressionSummary.weightCol\")\n  )\n\n  // Exclude rules for 3.0.x\n  lazy val v30excludes = v24excludes ++ Seq("
  },
  {
    "id" : "ef550780-124c-4b21-82ac-cdb195ec7790",
    "prId" : 28710,
    "prUrl" : "https://github.com/apache/spark/pull/28710#pullrequestreview-433485810",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b6327c58-536c-4bb9-9e97-b487637baa0b",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Not sure if it is for all cases. But this looks like just because `weightCol` is inherited from `ClassificationSummary` instead of `LogisticRegressionSummary` now.\r\n\r\n",
        "createdAt" : "2020-06-18T15:54:03Z",
        "updatedAt" : "2020-06-18T17:11:25Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "58003a13-21df-4963-8ac3-2536eee49bd0",
        "parentId" : "b6327c58-536c-4bb9-9e97-b487637baa0b",
        "authorId" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "body" : "I added weightCol in ```LogisticRegressionSummary``` in another PR (I just realized the MiMaExclude for that PR is not needed any more since I moved weightCol to ```ClassificationSummary```)\r\n\r\n3.0: no weightCol \r\n\r\nafter my changes: \r\n```ClassificationSummary``` has weightCol, ```LogisticRegressionSummary```, ```LogisticRegressionTrainingSummary```, ```BinaryLogisticRegressionSummary```, ```BinaryLogisticRegressionTrainingSummary``` all have inherited weightCol, so there are four ```InheritedNewAbstractMethodProblem``` for weightCol",
        "createdAt" : "2020-06-18T17:10:59Z",
        "updatedAt" : "2020-06-18T17:11:25Z",
        "lastEditedBy" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "tags" : [
        ]
      }
    ],
    "commit" : "ead6da2e0e3d1a269e6ef76b23f8e845690f42a3",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +55,59 @@    ProblemFilters.exclude[InheritedNewAbstractMethodProblem](\"org.apache.spark.ml.classification.LogisticRegressionTrainingSummary.org$apache$spark$ml$classification$ClassificationSummary$_setter_$org$apache$spark$ml$classification$ClassificationSummary$$multiclassMetrics_=\"),\n    ProblemFilters.exclude[InheritedNewAbstractMethodProblem](\"org.apache.spark.ml.classification.LogisticRegressionTrainingSummary.org$apache$spark$ml$classification$ClassificationSummary$$multiclassMetrics\"),\n    ProblemFilters.exclude[InheritedNewAbstractMethodProblem](\"org.apache.spark.ml.classification.LogisticRegressionTrainingSummary.weightCol\"),\n    ProblemFilters.exclude[InheritedNewAbstractMethodProblem](\"org.apache.spark.ml.classification.BinaryLogisticRegressionTrainingSummary.org$apache$spark$ml$classification$BinaryClassificationSummary$_setter_$org$apache$spark$ml$classification$BinaryClassificationSummary$$sparkSession_=\"),\n    ProblemFilters.exclude[InheritedNewAbstractMethodProblem](\"org.apache.spark.ml.classification.BinaryLogisticRegressionTrainingSummary.org$apache$spark$ml$classification$BinaryClassificationSummary$_setter_$org$apache$spark$ml$classification$BinaryClassificationSummary$$binaryMetrics_=\"),"
  },
  {
    "id" : "8e207bd8-1a1a-4ebb-8a51-0e3f804a8243",
    "prId" : 28657,
    "prUrl" : "https://github.com/apache/spark/pull/28657#pullrequestreview-420923463",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5c5cd33c-429a-42e5-ba81-ce0f125accfc",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Looks fine, that's a sealed trait anyway",
        "createdAt" : "2020-05-29T12:54:23Z",
        "updatedAt" : "2020-05-29T12:54:25Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      }
    ],
    "commit" : "df38f4e55693a08df4ff045e3579926b3d222266",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +50,54 @@    //[SPARK-31840] Add instance weight support in LogisticRegressionSummary\n    // weightCol in org.apache.spark.ml.classification.LogisticRegressionSummary is present only in current version\n    ProblemFilters.exclude[ReversedMissingMethodProblem](\"org.apache.spark.ml.classification.LogisticRegressionSummary.weightCol\")\n  )\n"
  },
  {
    "id" : "42f7b269-2ae7-4f8c-9036-d3127c871d57",
    "prId" : 27978,
    "prUrl" : "https://github.com/apache/spark/pull/27978#pullrequestreview-379655934",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "22c6ba93-fb10-40c2-bf10-9ebd76bc26a2",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Weird: this class is still there, right? is this still definitely a MiMa error? I'm trying to figure out if we're missing something here that causes an incompatible change.\r\n\r\nThis would only go into 3.1 right?\r\nThe refactoring looks OK; hard to evaluate it all.",
        "createdAt" : "2020-03-22T15:45:37Z",
        "updatedAt" : "2020-04-29T16:19:43Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "0c87d6a3-bf87-45fb-bc4e-8c6918352a4c",
        "parentId" : "22c6ba93-fb10-40c2-bf10-9ebd76bc26a2",
        "authorId" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "body" : "This is for 3.1 only.\r\n\r\nThe first two MiMa errors are because of type hierarchy change. \r\n```\r\n[error]  * the type hierarchy of class org.apache.spark.ml.feature.ChiSqSelectorModel is different in current version. Missing types {org.apache.spark.ml.feature.ChiSqSelectorParams}\r\n```\r\nThe 3rd MiMa error is due to return type change.\r\n```\r\n[error]  * method fit(org.apache.spark.sql.Dataset)org.apache.spark.ml.feature.ChiSqSelectorModel in class org.apache.spark.ml.feature.ChiSqSelector has a different result type in current version, where it is org.apache.spark.ml.feature.SelectorModel rather than org.apache.spark.ml.feature.ChiSqSelectorModel\r\n```\r\n",
        "createdAt" : "2020-03-22T16:48:32Z",
        "updatedAt" : "2020-04-29T16:19:43Z",
        "lastEditedBy" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "tags" : [
        ]
      },
      {
        "id" : "51f9830d-856b-4c98-897c-3692f38d5c75",
        "parentId" : "22c6ba93-fb10-40c2-bf10-9ebd76bc26a2",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "The first two seem OK; that won't cause an actual source or binary incompatibility (right?) just because there is an additional class in the hierarchy.\r\n\r\nThe third one doesn't sound quite right. The type should be ChiSqSelectorModel right? do you have to override it in the subclass like you did with methods in ChiSqSelectorModel? the superclass returns this.type which I thought was 'inherited' and would become ChiSqSelectorModel.\r\n\r\nAm I right about that at all, and is there a workaround, or is MiMa incorrect somehow and the type is concretely ChiSqSelectorModel? or is this actually how lots of classes work?",
        "createdAt" : "2020-03-23T14:25:16Z",
        "updatedAt" : "2020-04-29T16:19:43Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "0f7f93e8-3015-431c-981c-36dfa6398576",
        "parentId" : "22c6ba93-fb10-40c2-bf10-9ebd76bc26a2",
        "authorId" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "body" : "Seems I have to override ```fit``` in subclasses. ",
        "createdAt" : "2020-03-23T17:25:22Z",
        "updatedAt" : "2020-04-29T16:19:43Z",
        "lastEditedBy" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "tags" : [
        ]
      }
    ],
    "commit" : "363d382c82cc28037e38c71d7d804cc3c325be98",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +45,49 @@    // after: class ChiSqSelector extends PSelector\n    // false positive, no binary incompatibility\n    ProblemFilters.exclude[MissingTypesProblem](\"org.apache.spark.ml.feature.ChiSqSelectorModel\"),\n    ProblemFilters.exclude[MissingTypesProblem](\"org.apache.spark.ml.feature.ChiSqSelector\")\n  )"
  },
  {
    "id" : "b6af9611-1643-495f-8a6a-95e4a72a057c",
    "prId" : 27599,
    "prUrl" : "https://github.com/apache/spark/pull/27599#pullrequestreview-383528703",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "eebc4f16-496e-47b4-85c4-f61317fdbdd3",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "If we add back some removed APIs, shouldn't we remove some items from Mima instead of adding more?",
        "createdAt" : "2020-03-30T04:50:10Z",
        "updatedAt" : "2020-03-30T04:50:10Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "070e372d5e27a74958514f274eff973591351d96",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +502,506 @@\n    // [SPARK-30846][CORE] Add AccumulatorV2 API in JavaSparkContext\n    ProblemFilters.exclude[IncompatibleMethTypeProblem](\"org.apache.spark.api.java.JavaSparkContext.doubleAccumulator\")\n  )\n"
  },
  {
    "id" : "86d30745-d587-4e77-b93c-019300c9e42e",
    "prId" : 27168,
    "prUrl" : "https://github.com/apache/spark/pull/27168#pullrequestreview-341306786",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "73b7f971-467f-4655-8994-52d3a2f19025",
        "parentId" : null,
        "authorId" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "body" : "Changing  hierarchy of ```AFTSurvivalRegression/AFTSurvivalRegressionModel```from extending ```Estimator/Model``` to ```Regressor/RegressionModel```  caused the following MiMa errors:\r\n```\r\n[error]  * method setFeaturesCol(java.lang.String)org.apache.spark.ml.regression.AFTSurvivalRegressionModel in class org.apache.spark.ml.regression.AFTSurvivalRegressionModel has a different result type in current version, where it is org.apache.spark.ml.PredictionModel rather than org.apache.spark.ml.regression.AFTSurvivalRegressionModel\r\n[error]    filter with: ProblemFilters.exclude[IncompatibleResultTypeProblem](\"org.apache.spark.ml.regression.AFTSurvivalRegressionModel.setFeaturesCol\")\r\n[error]  * method setPredictionCol(java.lang.String)org.apache.spark.ml.regression.AFTSurvivalRegressionModel in class org.apache.spark.ml.regression.AFTSurvivalRegressionModel has a different result type in current version, where it is org.apache.spark.ml.PredictionModel rather than org.apache.spark.ml.regression.AFTSurvivalRegressionModel\r\n[error]    filter with: ProblemFilters.exclude[IncompatibleResultTypeProblem](\"org.apache.spark.ml.regression.AFTSurvivalRegressionModel.setPredictionCol\")\r\n[error]  * method fit(org.apache.spark.sql.Dataset)org.apache.spark.ml.regression.AFTSurvivalRegressionModel in class org.apache.spark.ml.regression.AFTSurvivalRegression has a different result type in current version, where it is org.apache.spark.ml.Model rather than org.apache.spark.ml.regression.AFTSurvivalRegressionModel\r\n[error]    filter with: ProblemFilters.exclude[IncompatibleResultTypeProblem](\"org.apache.spark.ml.regression.AFTSurvivalRegression.fit\")\r\n[error]  * method setFeaturesCol(java.lang.String)org.apache.spark.ml.regression.AFTSurvivalRegression in class org.apache.spark.ml.regression.AFTSurvivalRegression has a different result type in current version, where it is org.apache.spark.ml.Predictor rather than org.apache.spark.ml.regression.AFTSurvivalRegression\r\n[error]    filter with: ProblemFilters.exclude[IncompatibleResultTypeProblem](\"org.apache.spark.ml.regression.AFTSurvivalRegression.setFeaturesCol\")\r\n[error]  * method setLabelCol(java.lang.String)org.apache.spark.ml.regression.AFTSurvivalRegression in class org.apache.spark.ml.regression.AFTSurvivalRegression has a different result type in current version, where it is org.apache.spark.ml.Predictor rather than org.apache.spark.ml.regression.AFTSurvivalRegression\r\n[error]    filter with: ProblemFilters.exclude[IncompatibleResultTypeProblem](\"org.apache.spark.ml.regression.AFTSurvivalRegression.setLabelCol\")\r\n[error]  * method setPredictionCol(java.lang.String)org.apache.spark.ml.regression.AFTSurvivalRegression in class org.apache.spark.ml.regression.AFTSurvivalRegression has a different result type in current version, where it is org.apache.spark.ml.Predictor rather than org.apache.spark.ml.regression.AFTSurvivalRegression\r\n[error]    filter with: ProblemFilters.exclude[IncompatibleResultTypeProblem](\"org.apache.spark.ml.regression.AFTSurvivalRegression.setPredictionCol\")\r\n```\r\nThere is not any API change, though. \r\n\r\nChanging the hierarchy of extending ```Predictor/PredictionModel``` to ```Regressor/RegressionModel``` doesn't cause MiMa problem. \r\n\r\n",
        "createdAt" : "2020-01-10T16:49:07Z",
        "updatedAt" : "2020-01-11T20:28:12Z",
        "lastEditedBy" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "tags" : [
        ]
      },
      {
        "id" : "d4eb576f-96a4-4c5f-a0f3-7d7408665939",
        "parentId" : "73b7f971-467f-4655-8994-52d3a2f19025",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Yeah same story as last time eh, the type should be identical but it's not super obvious to Mima that this.type is the same as type T = [the model type]. ",
        "createdAt" : "2020-01-10T17:17:50Z",
        "updatedAt" : "2020-01-11T20:28:12Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      }
    ],
    "commit" : "c22d2849f293e6471dd81a6ff9994933fbbf6a53",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +479,483 @@    ProblemFilters.exclude[IncompatibleResultTypeProblem](\"org.apache.spark.ml.regression.AFTSurvivalRegression.setLabelCol\"),\n    ProblemFilters.exclude[IncompatibleResultTypeProblem](\"org.apache.spark.ml.regression.AFTSurvivalRegression.setPredictionCol\")\n  )\n\n  // Exclude rules for 2.4.x"
  },
  {
    "id" : "19bebbed-dc6f-4bd3-a146-dfa87148872c",
    "prId" : 27094,
    "prUrl" : "https://github.com/apache/spark/pull/27094#pullrequestreview-338367141",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f3ae98c6-d8c1-456a-9a46-30e11a828ff0",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Sorry for the dumb questions, but, hm, this removes several key setters. Is the API that different, or, how do you set the feature col, etc now?",
        "createdAt" : "2020-01-04T16:15:24Z",
        "updatedAt" : "2020-01-04T16:15:24Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "c5fe4bf7-76e8-46e8-a8ee-5fa727214cfb",
        "parentId" : "f3ae98c6-d8c1-456a-9a46-30e11a828ff0",
        "authorId" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "body" : "I was confused as well when I first saw the Mima errors lol\r\n\r\nHere are the Mima errors:\r\n```\r\n[error]  * method fit(org.apache.spark.sql.Dataset)org.apache.spark.ml.regression.IsotonicRegressionModel in class org.apache.spark.ml.regression.IsotonicRegression has a different result type in current version, where it is org.apache.spark.ml.Model rather than org.apache.spark.ml.regression.IsotonicRegressionModel\r\n[error]    filter with: ProblemFilters.exclude[IncompatibleResultTypeProblem](\"org.apache.spark.ml.regression.IsotonicRegression.fit\")\r\n[error]  * method setFeaturesCol(java.lang.String)org.apache.spark.ml.regression.IsotonicRegression in class org.apache.spark.ml.regression.IsotonicRegression has a different result type in current version, where it is org.apache.spark.ml.Predictor rather than org.apache.spark.ml.regression.IsotonicRegression\r\n[error]    filter with: ProblemFilters.exclude[IncompatibleResultTypeProblem](\"org.apache.spark.ml.regression.IsotonicRegression.setFeaturesCol\")\r\n[error]  * method setLabelCol(java.lang.String)org.apache.spark.ml.regression.IsotonicRegression in class org.apache.spark.ml.regression.IsotonicRegression has a different result type in current version, where it is org.apache.spark.ml.Predictor rather than org.apache.spark.ml.regression.IsotonicRegression\r\n[error]    filter with: ProblemFilters.exclude[IncompatibleResultTypeProblem](\"org.apache.spark.ml.regression.IsotonicRegression.setLabelCol\")\r\n[error]  * method setPredictionCol(java.lang.String)org.apache.spark.ml.regression.IsotonicRegression in class org.apache.spark.ml.regression.IsotonicRegression has a different result type in current version, where it is org.apache.spark.ml.Predictor rather than org.apache.spark.ml.regression.IsotonicRegression\r\n[error]    filter with: ProblemFilters.exclude[IncompatibleResultTypeProblem](\"org.apache.spark.ml.regression.IsotonicRegression.setPredictionCol\")\r\n[error]  * method setFeaturesCol(java.lang.String)org.apache.spark.ml.regression.IsotonicRegressionModel in class org.apache.spark.ml.regression.IsotonicRegressionModel has a different result type in current version, where it is org.apache.spark.ml.PredictionModel rather than org.apache.spark.ml.regression.IsotonicRegressionModel\r\n[error]    filter with: ProblemFilters.exclude[IncompatibleResultTypeProblem](\"org.apache.spark.ml.regression.IsotonicRegressionModel.setFeaturesCol\")\r\n[error]  * method setPredictionCol(java.lang.String)org.apache.spark.ml.regression.IsotonicRegressionModel in class org.apache.spark.ml.regression.IsotonicRegressionModel has a different result type in current version, where it is org.apache.spark.ml.PredictionModel rather than org.apache.spark.ml.regression.IsotonicRegressionModel\r\n[error]    filter with: ProblemFilters.exclude[IncompatibleResultTypeProblem](\"org.apache.spark.ml.regression.IsotonicRegressionModel.setPredictionCol\")\r\n```\r\nThe APIs are still the same, but the return types are different. Before the change, setXXX is in ```IsotonicRegression``` and the return type is ```IsotonicRegression```\r\n\r\n```\r\n  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\r\n```\r\nAfter the change, setXXX is in the super class ```Predictor``` and the return type is ```Predictor```\r\n```\r\n  def setFeaturesCol(value: String): Learner = set(featuresCol, value).asInstanceOf[Learner]\r\n```\r\n",
        "createdAt" : "2020-01-04T17:41:47Z",
        "updatedAt" : "2020-01-04T17:41:47Z",
        "lastEditedBy" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "tags" : [
        ]
      },
      {
        "id" : "a3c3839a-0439-45e9-9fc1-eb8ba964d373",
        "parentId" : "f3ae98c6-d8c1-456a-9a46-30e11a828ff0",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Hm, weird, because Learner is the type IsotonicRegression here. So it shouldn't be a real change. I wonder if it's just a MiMa problem. \r\n\r\nSo as far as you know this isn't changing any APIs right?",
        "createdAt" : "2020-01-04T19:39:50Z",
        "updatedAt" : "2020-01-04T19:39:50Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      }
    ],
    "commit" : "b7ae191e4ce6874efc918affe6aaf5b4c092a620",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +470,474 @@    // [SPARK-30419][ML] Make IsotonicRegression extend Regressor\n    ProblemFilters.exclude[IncompatibleResultTypeProblem](\"org.apache.spark.ml.regression.IsotonicRegression.fit\"),\n    ProblemFilters.exclude[IncompatibleResultTypeProblem](\"org.apache.spark.ml.regression.IsotonicRegression.setFeaturesCol\"),\n    ProblemFilters.exclude[IncompatibleResultTypeProblem](\"org.apache.spark.ml.regression.IsotonicRegression.setLabelCol\"),\n    ProblemFilters.exclude[IncompatibleResultTypeProblem](\"org.apache.spark.ml.regression.IsotonicRegression.setPredictionCol\"),"
  },
  {
    "id" : "6331012d-3de3-4213-9503-032405a6c490",
    "prId" : 26838,
    "prUrl" : "https://github.com/apache/spark/pull/26838#pullrequestreview-330716355",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1d416daf-c865-4109-b47d-0921a3b4a7ed",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Just a question. Is this worth to break the API, @huaxingao ?",
        "createdAt" : "2019-12-11T17:31:42Z",
        "updatedAt" : "2019-12-31T23:22:55Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "40fc5da2c741af0a806fa9fecc9b52ecddc9e0bb",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +330,334 @@\n    // [SPARK-30144][ML] Make MultilayerPerceptronClassificationModel extend MultilayerPerceptronParams\n    ProblemFilters.exclude[IncompatibleResultTypeProblem](\"org.apache.spark.ml.classification.MultilayerPerceptronClassificationModel.layers\"),\n    ProblemFilters.exclude[DirectMissingMethodProblem](\"org.apache.spark.ml.classification.MultilayerPerceptronClassificationModel.this\"),\n"
  },
  {
    "id" : "da1a3882-c4d1-4bd0-963a-c52ac2fe9099",
    "prId" : 26413,
    "prUrl" : "https://github.com/apache/spark/pull/26413#pullrequestreview-314545025",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "caf3b8ad-28ec-4ec9-ac35-82aa3fe0b7a4",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Can we keep this constructor, if it's easy? It relates to the question of subclassing I guess.",
        "createdAt" : "2019-11-08T16:09:57Z",
        "updatedAt" : "2019-11-09T07:00:36Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "5100f702-964b-42d4-9168-e4324560aa95",
        "parentId" : "caf3b8ad-28ec-4ec9-ac35-82aa3fe0b7a4",
        "authorId" : "685f805b-e4fa-4f21-b066-58afcecf9ce6",
        "body" : "If we do not make a new subclass, then I guess we can not keep it.\r\nHowever, this constructor is not exposed to end users.",
        "createdAt" : "2019-11-09T07:11:54Z",
        "updatedAt" : "2019-11-09T07:11:55Z",
        "lastEditedBy" : "685f805b-e4fa-4f21-b066-58afcecf9ce6",
        "tags" : [
        ]
      }
    ],
    "commit" : "82961dae05500b865c5fe192c1d7ac1beec87861",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +120,124 @@\n    // [SPARK-16872][ML][PYSPARK] Impl Gaussian Naive Bayes Classifier\n    ProblemFilters.exclude[DirectMissingMethodProblem](\"org.apache.spark.ml.classification.NaiveBayesModel.this\"),\n\n    // [SPARK-25765][ML] Add training cost to BisectingKMeans summary"
  },
  {
    "id" : "047b86e5-683f-4c53-9316-ebad2f6109d2",
    "prId" : 26390,
    "prUrl" : "https://github.com/apache/spark/pull/26390#pullrequestreview-363803550",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a01e8462-8b73-40ed-b8fe-bf6f88b2d223",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "Why this removal is not reported in MIMA? This API was added in Spark 2.4",
        "createdAt" : "2020-02-24T18:33:34Z",
        "updatedAt" : "2020-02-24T18:33:34Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "efeee6a5-d5dc-4de3-8b76-3ea9ab5d1561",
        "parentId" : "a01e8462-8b73-40ed-b8fe-bf6f88b2d223",
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "cc @HyukjinKwon Do you have an idea?",
        "createdAt" : "2020-02-24T18:33:44Z",
        "updatedAt" : "2020-02-24T18:33:44Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "2cf51b56-c48c-47bb-84f9-734c9a419196",
        "parentId" : "a01e8462-8b73-40ed-b8fe-bf6f88b2d223",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "If you mean `org.apache.spark.ExecutorPluginContext`, that's not in Spark 2.4. Seems `org.apache.spark.ExecutorPlugin` is properly excluded here.",
        "createdAt" : "2020-02-25T00:32:29Z",
        "updatedAt" : "2020-02-25T00:32:30Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "18587f25-578a-4a55-bab9-808835cfd02b",
        "parentId" : "a01e8462-8b73-40ed-b8fe-bf6f88b2d223",
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "I see. Thanks!",
        "createdAt" : "2020-02-25T00:49:11Z",
        "updatedAt" : "2020-02-25T00:49:12Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad132575ab7120e78f68785a0d41a97a19dd48b1",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +38,42 @@  lazy val v30excludes = v24excludes ++ Seq(\n    // [SPARK-29399][core] Remove old ExecutorPlugin interface.\n    ProblemFilters.exclude[MissingClassProblem](\"org.apache.spark.ExecutorPlugin\"),\n\n    // [SPARK-][SQL][CORE][MLLIB] Remove more old deprecated items in Spark 3"
  }
]