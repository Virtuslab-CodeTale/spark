[
  {
    "id" : "4b3a35fe-594e-4bad-a999-b16727a7461e",
    "prId" : 33658,
    "prUrl" : "https://github.com/apache/spark/pull/33658#pullrequestreview-723851868",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1adfa9d2-fba1-4c8e-9693-531d320b0c48",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Well, this is a global setting instead of GHA-only. Anyway, no problem with this.",
        "createdAt" : "2021-08-05T22:36:10Z",
        "updatedAt" : "2021-08-05T22:37:05Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "d1e83ad8-3983-4b65-bb1d-38e20e5dc427",
        "parentId" : "1adfa9d2-fba1-4c8e-9693-531d320b0c48",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Yea, just set it back to previous level if GHA now allows.",
        "createdAt" : "2021-08-05T22:42:21Z",
        "updatedAt" : "2021-08-05T22:42:21Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "6e5a80e93333f820b7b58c7a88446d4c9722cecd",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1126,1130 @@    (Test / javaOptions) ++= {\n      val metaspaceSize = sys.env.get(\"METASPACE_SIZE\").getOrElse(\"1300m\")\n      s\"-Xmx4g -Xss4m -XX:MaxMetaspaceSize=$metaspaceSize -XX:+UseParallelGC -XX:-UseDynamicNumberOfGCThreads -XX:ReservedCodeCacheSize=128m\"\n        .split(\" \").toSeq\n    },"
  },
  {
    "id" : "ee6a62da-6afa-47c5-b553-359ce0534c7e",
    "prId" : 32253,
    "prUrl" : "https://github.com/apache/spark/pull/32253#pullrequestreview-640785436",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "232b0c9c-71f7-4f66-bfad-efdc0bc0fa14",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "I assume the same, that these flags are required to use the vectorized code, but, that's on someone who's experimenting with Java 16 at this point.",
        "createdAt" : "2021-04-20T23:32:17Z",
        "updatedAt" : "2021-04-26T17:40:42Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "7e79df41-391a-4026-a53e-c12e2faa5010",
        "parentId" : "232b0c9c-71f7-4f66-bfad-efdc0bc0fa14",
        "authorId" : "567dbcaa-2683-46db-af89-e44be673a7cc",
        "body" : "Yes, to use the code relying on the Vector API, one would need to pass `--add-modules=jdk.incubator.vector`. Otherwise, it will fall back to pure JDK8-compatible code. And the `--add-modules=jdk.incubator.foreign -Dforeign.restricted=warn` is for a native-wrapper BLAS implementation based on the [Foreign Linker API](https://openjdk.java.net/jeps/389). If that's not available, it automatically falls back to the `com.github.fommil` native wrappers.",
        "createdAt" : "2021-04-21T08:47:59Z",
        "updatedAt" : "2021-04-26T17:40:42Z",
        "lastEditedBy" : "567dbcaa-2683-46db-af89-e44be673a7cc",
        "tags" : [
        ]
      }
    ],
    "commit" : "17edfebb92586c77c8806f2a63daaa166f524657",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +295,299 @@      val versionParts = System.getProperty(\"java.version\").split(\"[+.\\\\-]+\", 3)\n      var major = versionParts(0).toInt\n      if (major >= 16) Seq(\"--add-modules=jdk.incubator.vector,jdk.incubator.foreign\", \"-Dforeign.restricted=warn\") else Seq.empty\n    },\n"
  },
  {
    "id" : "ea841a17-0674-40c2-af86-d75465d26578",
    "prId" : 31023,
    "prUrl" : "https://github.com/apache/spark/pull/31023#pullrequestreview-561800197",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ba106557-9119-4ac0-9a9a-7bf53ac17192",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "nit:\r\n\r\n```suggestion\r\n      val major = if (major == 1) versionParts(1).toInt else versionParts(0).toInt\r\n```",
        "createdAt" : "2021-01-05T09:02:54Z",
        "updatedAt" : "2021-01-05T09:02:54Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "a69b13c0-5334-4815-9cc4-160471007b38",
        "parentId" : "ba106557-9119-4ac0-9a9a-7bf53ac17192",
        "authorId" : "3a227965-84e0-47cf-9974-11293764f028",
        "body" : "It's impossible isn't it? `major` in the condition expression is undefined yet.",
        "createdAt" : "2021-01-05T13:48:17Z",
        "updatedAt" : "2021-01-05T13:48:17Z",
        "lastEditedBy" : "3a227965-84e0-47cf-9974-11293764f028",
        "tags" : [
        ]
      }
    ],
    "commit" : "b583d4afc8952897aa1c7b60a78f4de421c9d1aa",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +962,966 @@      val versionParts = System.getProperty(\"java.version\").split(\"[+.\\\\-]+\", 3)\n      var major = versionParts(0).toInt\n      if (major == 1) major = versionParts(1).toInt\n\n      Seq("
  },
  {
    "id" : "b754337b-ee29-4825-8eb0-96d79dc7ca4a",
    "prId" : 30810,
    "prUrl" : "https://github.com/apache/spark/pull/30810#pullrequestreview-631241230",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2a3389d9-050f-4e41-be82-8c4cfddee907",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Would a Spark cluster have to launch its JVMs with this flag to make it work? that's OK if so, we might just add a note in the docs about it.",
        "createdAt" : "2021-04-07T23:47:55Z",
        "updatedAt" : "2021-04-13T23:18:23Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "9cad6fac-fb70-4360-aebd-b395d3ca637e",
        "parentId" : "2a3389d9-050f-4e41-be82-8c4cfddee907",
        "authorId" : "567dbcaa-2683-46db-af89-e44be673a7cc",
        "body" : "Yes, they would have to. It's one of the failsafe to make sure users don't end up using the feature \"by accident\". That will be the case until the Vector API becomes stable (gets out of incubation) and doesn't need this flag anymore.",
        "createdAt" : "2021-04-08T10:40:48Z",
        "updatedAt" : "2021-04-13T23:18:23Z",
        "lastEditedBy" : "567dbcaa-2683-46db-af89-e44be673a7cc",
        "tags" : [
        ]
      }
    ],
    "commit" : "10595df8cd6ad01c6a9ae5cde49a9974ae58b667",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +295,299 @@      val versionParts = System.getProperty(\"java.version\").split(\"[+.\\\\-]+\", 3)\n      var major = versionParts(0).toInt\n      if (major >= 16) Seq(\"--add-modules=jdk.incubator.vector\") else Seq.empty\n    },\n"
  },
  {
    "id" : "8f19e5d1-ac62-459e-a95a-e3cdb7dc7a49",
    "prId" : 30810,
    "prUrl" : "https://github.com/apache/spark/pull/30810#pullrequestreview-634611638",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7b8fb97b-cbb1-44f5-bff2-565c7b47e6b0",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Apologies @luhenry there's a merge conflict around here. If you can resolve it I will merge.",
        "createdAt" : "2021-04-13T13:47:35Z",
        "updatedAt" : "2021-04-13T23:18:23Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      }
    ],
    "commit" : "10595df8cd6ad01c6a9ae5cde49a9974ae58b667",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +292,296 @@    publishLocal := Seq((MavenCompile / publishLocal), (SbtCompile / publishLocal)).dependOn.value,\n\n    javaOptions ++= {\n      val versionParts = System.getProperty(\"java.version\").split(\"[+.\\\\-]+\", 3)\n      var major = versionParts(0).toInt"
  },
  {
    "id" : "60095ff9-6e29-4c59-b4f7-71828f085354",
    "prId" : 30643,
    "prUrl" : "https://github.com/apache/spark/pull/30643#pullrequestreview-551025955",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fd621cb5-76d8-4005-8a3f-45bf69368189",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Would you mind to add some comment here?",
        "createdAt" : "2020-12-14T03:13:49Z",
        "updatedAt" : "2020-12-14T03:38:41Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "689ae3bb-87ac-4a82-bdb4-cfc97f873e29",
        "parentId" : "fd621cb5-76d8-4005-8a3f-45bf69368189",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "updated",
        "createdAt" : "2020-12-14T03:47:56Z",
        "updatedAt" : "2020-12-14T03:47:56Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "f0ac81782132276a389470a8cf627f4cc12a919a",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +671,675 @@      ExclusionRule(organization = \"com.sun.jersey\"),\n      ExclusionRule(\"javax.servlet\", \"javax.servlet-api\"),\n      ExclusionRule(\"javax.ws.rs\", \"jsr311-api\"))\n  )\n}"
  },
  {
    "id" : "1f8ea4fc-6bfe-4deb-90fc-6eaa89074e6f",
    "prId" : 30643,
    "prUrl" : "https://github.com/apache/spark/pull/30643#pullrequestreview-567874907",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "251b8e52-b2e2-444a-a629-49d89e26e301",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Hm, so in Yarn are we adding `javax.servlet-api` back with `jakarta.servlet-api`?",
        "createdAt" : "2021-01-13T04:42:42Z",
        "updatedAt" : "2021-01-13T04:42:42Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "1b3bf987-d6fe-4332-952a-e9f84da42517",
        "parentId" : "251b8e52-b2e2-444a-a629-49d89e26e301",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "THE YARN TESTS relies on this, so I\r\n added it back here\r\n",
        "createdAt" : "2021-01-13T05:13:30Z",
        "updatedAt" : "2021-01-13T05:13:30Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "6bd51d72-f1e2-491f-8782-46e98943cb84",
        "parentId" : "251b8e52-b2e2-444a-a629-49d89e26e301",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Is it needed only for tests? or during actual runtime with the regular release?",
        "createdAt" : "2021-01-13T10:23:11Z",
        "updatedAt" : "2021-01-13T10:23:11Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "0a714d25-4544-46a9-be0b-e0d3bda89880",
        "parentId" : "251b8e52-b2e2-444a-a629-49d89e26e301",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "It's for test only.\r\n\r\nFYI, https://github.com/apache/spark/blob/317048d708465723e7008ba4ce22c2406c1aa23a/resource-managers/yarn/pom.xml#L156-L158",
        "createdAt" : "2021-01-13T10:28:16Z",
        "updatedAt" : "2021-01-13T10:28:51Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "d7976a85-f08f-4cc2-94ce-4b8a3fa1909a",
        "parentId" : "251b8e52-b2e2-444a-a629-49d89e26e301",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "If this is for test-only, let's include this only for the test scope. It's going to introduce another problem with conflicting jars. Can we try to exclude and include this with test-only scope? If that doesn't work with SBT, that might be fine since we don't use SBT for official release process. We can add documentation for clarification.\r\n",
        "createdAt" : "2021-01-13T10:33:41Z",
        "updatedAt" : "2021-01-13T10:33:41Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "b0c2fcca-b464-4177-9685-e95440fbad6e",
        "parentId" : "251b8e52-b2e2-444a-a629-49d89e26e301",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "Do we have a JIRA ticket to track the new conflicting jars issue? I will try to fix it",
        "createdAt" : "2021-01-13T10:44:01Z",
        "updatedAt" : "2021-01-13T10:44:01Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "732ac9fd-b240-4a8d-abcc-e847d14c06e5",
        "parentId" : "251b8e52-b2e2-444a-a629-49d89e26e301",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "It would be able to do it at least in Maven. Nope, we don't have it. I was just post-reviewing it :-).",
        "createdAt" : "2021-01-13T10:49:57Z",
        "updatedAt" : "2021-01-13T10:49:58Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "53abf14d-8f4a-48a9-9561-00889a919608",
        "parentId" : "251b8e52-b2e2-444a-a629-49d89e26e301",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Just for clarification, we're all fine as long as ...:\r\n- `dev/deps/spark-deps-hadoop-3.2-hive-2.3` does not have the duplicated jars. This file contains the list of jars for the release.\r\n- The tests pass",
        "createdAt" : "2021-01-13T10:51:34Z",
        "updatedAt" : "2021-01-13T10:51:34Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "2788a24e-5a5b-4259-a915-b7a8792fafbc",
        "parentId" : "251b8e52-b2e2-444a-a629-49d89e26e301",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "got it",
        "createdAt" : "2021-01-13T13:06:27Z",
        "updatedAt" : "2021-01-13T13:06:28Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "b1109c1d-9e02-40c3-82db-48f6eb59c414",
        "parentId" : "251b8e52-b2e2-444a-a629-49d89e26e301",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Thank you @yaooqinn.",
        "createdAt" : "2021-01-13T13:19:10Z",
        "updatedAt" : "2021-01-13T13:19:11Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "0c1c280e-a356-4984-97f9-fe486e0e71da",
        "parentId" : "251b8e52-b2e2-444a-a629-49d89e26e301",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "my pleasure:)",
        "createdAt" : "2021-01-13T14:07:43Z",
        "updatedAt" : "2021-01-13T14:07:43Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "39edbb54-a025-41c0-9aaa-bd151ba3dc9d",
        "parentId" : "251b8e52-b2e2-444a-a629-49d89e26e301",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I filed a JIRA for you: SPARK-34111 :-).",
        "createdAt" : "2021-01-14T06:14:35Z",
        "updatedAt" : "2021-01-14T06:14:35Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "f0ac81782132276a389470a8cf627f4cc12a919a",
    "line" : 41,
    "diffHunk" : "@@ -1,1 +780,784 @@      ExclusionRule(organization = \"com.sun.jersey\"),\n      ExclusionRule(\"javax.servlet\", \"javax.servlet-api\"),\n      ExclusionRule(\"javax.ws.rs\", \"jsr311-api\"))\n  )\n}"
  },
  {
    "id" : "285092aa-9436-4075-b89c-cfe2225c9564",
    "prId" : 30008,
    "prUrl" : "https://github.com/apache/spark/pull/30008#pullrequestreview-506262847",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "098b9421-20c4-4d88-95ea-3251ca3ae8bd",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I think `hdp` was the short name of Hadoop instead of \"Hortonworks Data Platform\" if you meant this but I think it's okay to fix to avoid confusion.\r\n\r\nWhat about we name it `hadoopVersionValue` instead to make it extra explicit?",
        "createdAt" : "2020-10-12T02:54:02Z",
        "updatedAt" : "2020-10-12T02:55:01Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "1161bd073e9fd4012903fd8fe05d62b198c60824",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +757,761 @@    },\n    assemblyJarName in assembly := {\n      lazy val versionValue = hadoopVersion.value\n      if (moduleName.value.contains(\"streaming-kafka-0-10-assembly\")\n        || moduleName.value.contains(\"streaming-kinesis-asl-assembly\")) {"
  },
  {
    "id" : "e4be09d3-7198-4ee0-bb0a-ca2354defd13",
    "prId" : 29998,
    "prUrl" : "https://github.com/apache/spark/pull/29998#pullrequestreview-506434586",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "04144448-ae2d-49c1-872e-4a067ce0d56a",
        "parentId" : null,
        "authorId" : "7e665d8b-d739-4edf-88c8-7379ff8585c2",
        "body" : "why change this ?",
        "createdAt" : "2020-10-12T09:10:21Z",
        "updatedAt" : "2020-10-12T09:12:48Z",
        "lastEditedBy" : "7e665d8b-d739-4edf-88c8-7379ff8585c2",
        "tags" : [
        ]
      }
    ],
    "commit" : "67370185751f579b405d705cc31c3f1da88eab5c",
    "line" : 31,
    "diffHunk" : "@@ -1,1 +230,234 @@    publishLocal in MavenCompile := publishTask(publishLocalConfiguration in MavenCompile).value,\n    publishLocal in SbtCompile := publishTask(publishLocalConfiguration in SbtCompile).value,\n    publishLocal := Seq(publishLocal in MavenCompile, publishLocal in SbtCompile).dependOn.value,\n\n    javacOptions in (Compile, doc) ++= {"
  },
  {
    "id" : "af5447d8-64c9-4a6a-8110-45ead62f3597",
    "prId" : 29476,
    "prUrl" : "https://github.com/apache/spark/pull/29476#pullrequestreview-471720010",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "307f6efd-c78e-40c6-89bc-1ab03c066f44",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "in line 854, add\r\n```Scala\r\n      .map(_.filterNot(_.getCanonicalPath.contains(\"org/apache/spark/sql/v2/avro\")))\r\n```",
        "createdAt" : "2020-08-20T06:14:04Z",
        "updatedAt" : "2020-08-20T11:04:35Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "891427e6-2e0c-4f7e-9174-56819a0ddfb1",
        "parentId" : "307f6efd-c78e-40c6-89bc-1ab03c066f44",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "Nice catch, thanks!",
        "createdAt" : "2020-08-20T15:17:40Z",
        "updatedAt" : "2020-08-20T15:17:40Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "8996cbac8aa6c42b46269ebd3cd354e2caec9e95",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +866,870 @@  lazy val settings = scalaJavaUnidocSettings ++ Seq (\n    publish := {},\n\n    unidocProjectFilter in(ScalaUnidoc, unidoc) :=\n      inAnyProject -- inProjects(OldDeps.project, repl, examples, tools, kubernetes,"
  },
  {
    "id" : "2a58ca3a-8c4f-4eae-bf44-b3a88a283757",
    "prId" : 29476,
    "prUrl" : "https://github.com/apache/spark/pull/29476#pullrequestreview-471451017",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8c3a1980-bcbb-4f2d-b2f0-73f0afd81143",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "shall we just rename the package? all other file source v2 is in `org.apache.spark.sql.execution.datasources.v2`",
        "createdAt" : "2020-08-20T09:04:10Z",
        "updatedAt" : "2020-08-20T11:04:35Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "dd7c8368-dcd5-490d-8416-a55dac685929",
        "parentId" : "8c3a1980-bcbb-4f2d-b2f0-73f0afd81143",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "it's weird that avro doesn't follow this rule.",
        "createdAt" : "2020-08-20T09:04:44Z",
        "updatedAt" : "2020-08-20T11:04:35Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "dc594e2f-f602-4339-98f1-20dfef32a869",
        "parentId" : "8c3a1980-bcbb-4f2d-b2f0-73f0afd81143",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "Then we will have to move `org.apache.spark.sql.avro.AvroFileFormat` to `org.apache.spark.sql.execution.datasources.avro.AvroFileFormat` as well, to make it consistent with ParquetFileFormat.\r\n\r\nOriginally the package `org.apache.spark.sql.avro` is created for consistence with the kafka-sql module, since both of them are external modules.\r\n",
        "createdAt" : "2020-08-20T09:18:17Z",
        "updatedAt" : "2020-08-20T11:04:35Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "20b07332-cd7b-4ed0-8d58-ab1bd5b5e198",
        "parentId" : "8c3a1980-bcbb-4f2d-b2f0-73f0afd81143",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I think the problem here is we have public APIs in the avro module, while we assume it should all be private like other external modules.\r\n\r\nI think a safer way is to use whitelist and only generate doc for the classes we explicitly specified in avro module. Is it possible to do so?",
        "createdAt" : "2020-08-20T09:37:48Z",
        "updatedAt" : "2020-08-20T11:04:35Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "c92e91bf-5edb-4daa-bcb1-126eb4ec7aa0",
        "parentId" : "8c3a1980-bcbb-4f2d-b2f0-73f0afd81143",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "The `ignoreUndocumentedPackages` here is to ignore some packages. I think it will be complex if we introduce another step for adding some package as whitelist(allowlist)",
        "createdAt" : "2020-08-20T10:11:31Z",
        "updatedAt" : "2020-08-20T11:04:35Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "8996cbac8aa6c42b46269ebd3cd354e2caec9e95",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +853,857 @@      .map(_.filterNot(_.getCanonicalPath.contains(\"org/apache/spark/sql/catalog/v2/utils\")))\n      .map(_.filterNot(_.getCanonicalPath.contains(\"org/apache/hive\")))\n      .map(_.filterNot(_.getCanonicalPath.contains(\"org/apache/spark/sql/v2/avro\")))\n  }\n"
  },
  {
    "id" : "0b6b9b04-6700-4e1d-8665-210f1a85d5c3",
    "prId" : 29059,
    "prUrl" : "https://github.com/apache/spark/pull/29059#pullrequestreview-445821018",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "53668351-f3e7-4e52-8958-20aa9f35ad67",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "AFAIK this only works for SBT. Maybe we need to disable the test by default to avoid breaking the maven build.\r\n\r\ncc @gengliangwang ",
        "createdAt" : "2020-07-09T16:56:34Z",
        "updatedAt" : "2020-07-10T05:52:15Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "95d0b8f9-6695-485e-899d-d7b53fdf8023",
        "parentId" : "53668351-f3e7-4e52-8958-20aa9f35ad67",
        "authorId" : "2d6b46ba-4100-4c4d-9341-fff5e39647ec",
        "body" : "This is fine. The test is not failing in maven build. In the maven build, it will still pass but won't be able to catch the issue.",
        "createdAt" : "2020-07-09T16:57:52Z",
        "updatedAt" : "2020-07-10T05:52:15Z",
        "lastEditedBy" : "2d6b46ba-4100-4c4d-9341-fff5e39647ec",
        "tags" : [
        ]
      },
      {
        "id" : "38533c90-2c90-4e53-80d0-8e407242bdb7",
        "parentId" : "53668351-f3e7-4e52-8958-20aa9f35ad67",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "ah i see",
        "createdAt" : "2020-07-09T17:23:05Z",
        "updatedAt" : "2020-07-10T05:52:15Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "8f94ed688ac39a761e043192b07821b2ff15a48d",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +477,481 @@    \"org.apache.spark.ml.classification.LinearSVCSuite\",\n    \"org.apache.spark.sql.SQLQueryTestSuite\",\n    \"org.apache.spark.sql.hive.client.HadoopVersionInfoSuite\",\n    \"org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperationSuite\",\n    \"org.apache.spark.sql.hive.thriftserver.ThriftServerQueryTestSuite\","
  },
  {
    "id" : "80a2075b-7388-4468-a576-a3a88f657013",
    "prId" : 29057,
    "prUrl" : "https://github.com/apache/spark/pull/29057#pullrequestreview-445660027",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c69b9f3d-1d92-4c79-9c90-467a3cfb40b4",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "This is rather a followup of SPARK-24711.",
        "createdAt" : "2020-07-09T14:16:10Z",
        "updatedAt" : "2020-07-11T15:14:33Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "c3834b71918845a7fb8688bf9179c1e13798e17d",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +1016,1020 @@        Seq(\"--exclude-categories=\" + tags)\n      }.getOrElse(Nil): _*),\n    // Include tags defined in a system property\n    testOptions in Test += Tests.Argument(TestFrameworks.ScalaTest,\n      sys.props.get(\"test.include.tags\").map { tags =>"
  },
  {
    "id" : "64ae8192-f7d0-4894-947a-c2dc77d68964",
    "prId" : 29057,
    "prUrl" : "https://github.com/apache/spark/pull/29057#pullrequestreview-445660613",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "14ea1801-c7d9-4509-8e2e-bcbef58ab04a",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "This is required to run the Junit tests when you run the tests of the specified module. For example, `./build/sbt core/test`.",
        "createdAt" : "2020-07-09T14:16:45Z",
        "updatedAt" : "2020-07-11T15:14:33Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "c3834b71918845a7fb8688bf9179c1e13798e17d",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +1029,1033 @@    testOptions in Test += Tests.Argument(TestFrameworks.JUnit, \"-v\", \"-a\"),\n    // Required to detect Junit tests for each project, see also https://github.com/sbt/junit-interface/issues/35\n    crossPaths := false,\n    // Enable Junit testing.\n    libraryDependencies += \"com.novocode\" % \"junit-interface\" % \"0.11\" % \"test\","
  },
  {
    "id" : "c8b998c3-8e91-4706-96f0-3dd181464b08",
    "prId" : 28977,
    "prUrl" : "https://github.com/apache/spark/pull/28977#pullrequestreview-453897967",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2ca31a9a-40b3-4729-b5c1-633faf1461c7",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@xuanyuanking, shell we add a comment to distinguish `testsWhichShouldRunInTheirOwnDedicatedJvm` and  `org.apache.spark.sql.hive.execution`?\r\n\r\nI think the point is that it works differently from other opt-in test cases above defined at `testsWhichShouldRunInTheirOwnDedicatedJvm` since we're grouping all `org.apache.spark.sql.hive.execution.*` into a single group unlike other test cases above, which might be difficult to catch at a glance.",
        "createdAt" : "2020-07-22T11:43:34Z",
        "updatedAt" : "2020-07-23T06:46:52Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "41f29c72-d1bf-46a4-ae39-825f1c52289e",
        "parentId" : "2ca31a9a-40b3-4729-b5c1-633faf1461c7",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "Agree, let me add a comment to illustrate.",
        "createdAt" : "2020-07-22T13:58:47Z",
        "updatedAt" : "2020-07-23T06:46:52Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      },
      {
        "id" : "641e62b0-032d-4579-bfa7-57a864426c28",
        "parentId" : "2ca31a9a-40b3-4729-b5c1-633faf1461c7",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "Nit: add comment to mention that this is for reducing the testing time.",
        "createdAt" : "2020-07-23T07:40:50Z",
        "updatedAt" : "2020-07-23T07:40:51Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "6a68a7aedb58673ee7db9a37919f697eaa97053d",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +493,497 @@    // all suites of `org.apache.spark.sql.hive.execution.*` into a single group, instead of\n    // launching one JVM per suite.\n    case _ if name.contains(\"org.apache.spark.sql.hive.execution\") => HIVE_EXECUTION_TEST_GROUP\n    case _ => DEFAULT_TEST_GROUP\n  }"
  },
  {
    "id" : "a5c2e3ce-0e9b-403a-89da-d327a12d76d9",
    "prId" : 28977,
    "prUrl" : "https://github.com/apache/spark/pull/28977#pullrequestreview-453897589",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8eb8be20-f233-4893-b069-6deea8f4dca2",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@gengliangwang, do you know if the group names are displayed for anywhere else? or is it just purely grouping purpose internally?",
        "createdAt" : "2020-07-22T11:50:28Z",
        "updatedAt" : "2020-07-23T06:46:52Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "71512c1f-04e8-4a6d-8220-64028a998e13",
        "parentId" : "8eb8be20-f233-4893-b069-6deea8f4dca2",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "I haven't noticed that it is displayed somewhere.",
        "createdAt" : "2020-07-23T07:40:10Z",
        "updatedAt" : "2020-07-23T07:40:21Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "6a68a7aedb58673ee7db9a37919f697eaa97053d",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +486,490 @@\n  private val DEFAULT_TEST_GROUP = \"default_test_group\"\n  private val HIVE_EXECUTION_TEST_GROUP = \"hive_execution_test_group\"\n\n  private def testNameToTestGroup(name: String): String = name match {"
  }
]