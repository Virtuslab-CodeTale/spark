[
  {
    "id" : "40a7a7ec-bca1-482a-8fad-9fa3151afa4a",
    "prId" : 27374,
    "prUrl" : "https://github.com/apache/spark/pull/27374#pullrequestreview-349833017",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "66598b80-733d-458d-94ae-30ab79118df3",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Just curious, why do these have to be standardized now?\r\nOne thing I want to be careful about is not introducing bugs of course, and so having it pass existing tests as-is is a good indicator that it doesn't. This may be fine but are we changing inputs to match old outputs - did behavior change?",
        "createdAt" : "2020-01-28T14:47:32Z",
        "updatedAt" : "2020-01-30T03:59:49Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "6d655322-efff-43d1-9ec4-d84d59c4d451",
        "parentId" : "66598b80-733d-458d-94ae-30ab79118df3",
        "authorId" : "685f805b-e4fa-4f21-b066-58afcecf9ce6",
        "body" : "Two reasons:\r\n1, we do need to standardize the input at each iteration;\r\n2, if we keep this, then it is hard to use matrix operations in aggregator, since we  need: 1, element division, 2, broadcasting (block.matrix / stdvec), 3, special logic that if std==0 then output=0; the first 2 point may need to use `breeze` instead of BLAS, the last point may even make `breeze` not suitable.",
        "createdAt" : "2020-01-28T15:23:02Z",
        "updatedAt" : "2020-01-30T03:59:49Z",
        "lastEditedBy" : "685f805b-e4fa-4f21-b066-58afcecf9ce6",
        "tags" : [
        ]
      },
      {
        "id" : "4bfd2aa2-1bf5-4c40-9466-62358ef82a3c",
        "parentId" : "66598b80-733d-458d-94ae-30ab79118df3",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "This may be a dumb question, but you're not saying that LR input itself has to be standardized first, right? this is just required to meet preconditions of this aggregator? just trying to get a handle on why it wasn't required before. LR now standardizes internally?",
        "createdAt" : "2020-01-28T16:10:36Z",
        "updatedAt" : "2020-01-30T03:59:49Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "0d534567-bf5d-4aed-a950-f4331088f903",
        "parentId" : "66598b80-733d-458d-94ae-30ab79118df3",
        "authorId" : "685f805b-e4fa-4f21-b066-58afcecf9ce6",
        "body" : "Yes, both LinearSVC and LR always standardize the input vector in gradient computation in aggregator.\r\nIn existing single-instance `add(instance: Instance)` method, feature value `value` is always used along with std as ` value / localFeaturesStd(index)`\r\n",
        "createdAt" : "2020-01-29T02:40:48Z",
        "updatedAt" : "2020-01-30T03:59:49Z",
        "lastEditedBy" : "685f805b-e4fa-4f21-b066-58afcecf9ce6",
        "tags" : [
        ]
      }
    ],
    "commit" : "c49b379fe697ffb69bb8565eea6644528992f572",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +33,37 @@  override def beforeAll(): Unit = {\n    super.beforeAll()\n    instances = standardize(Array(\n      Instance(0.0, 0.1, Vectors.dense(1.0, 2.0)),\n      Instance(1.0, 0.5, Vectors.dense(1.5, 1.0)),"
  }
]