[
  {
    "id" : "38a93b49-b9b4-424a-8f7d-70ca895b82be",
    "prId" : 24939,
    "prUrl" : "https://github.com/apache/spark/pull/24939#pullrequestreview-283546951",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5014595f-0475-4b11-809c-3ca4a1a9883b",
        "parentId" : null,
        "authorId" : "99156419-7ce7-4671-b858-d0b5c4711f88",
        "body" : "`.alias(col)` - why is this needed? I think that's the alias by default",
        "createdAt" : "2019-09-02T04:05:17Z",
        "updatedAt" : "2020-01-14T22:43:12Z",
        "lastEditedBy" : "99156419-7ce7-4671-b858-d0b5c4711f88",
        "tags" : [
        ]
      },
      {
        "id" : "ccfaf03b-b04c-4fc9-b0fc-2d1b2dddd360",
        "parentId" : "5014595f-0475-4b11-809c-3ca4a1a9883b",
        "authorId" : "340aa00e-a2e7-422e-9a35-ef5a82672673",
        "body" : "Most of the time the name stays the same, but there are still some cases name might change. For example when casting is involved;\r\n\r\n```scala\r\nimport org.apache.spark.sql._\r\nimport org.apache.spark.sql.functions._\r\n\r\nval spark = SparkSession.builder()\r\n    .master(\"local\")\r\n    .getOrCreate()\r\nimport spark.implicits._\r\nSeq(1, 2, 3, 4, 5).toDF.select(expr(\"log(value)\")).show()\r\n\r\n+-------------------------------+\r\n|LOG(E(), CAST(value AS DOUBLE))|\r\n+-------------------------------+\r\n|                            0.0|\r\n|             0.6931471805599453|\r\n|             1.0986122886681098|\r\n|             1.3862943611198906|\r\n|             1.6094379124341003|\r\n+-------------------------------+\r\n```",
        "createdAt" : "2019-09-04T11:25:53Z",
        "updatedAt" : "2020-01-14T22:43:12Z",
        "lastEditedBy" : "340aa00e-a2e7-422e-9a35-ef5a82672673",
        "tags" : [
        ]
      }
    ],
    "commit" : "c3d97a351a04474a260ff6ae40909f366a8398cf",
    "line" : 44,
    "diffHunk" : "@@ -1,1 +228,232 @@    // Add evaluated expressions to the dataset\n    val selectedCols = resolvedFormula.evalExprs\n      .map(col => expr(col).alias(col)) ++ dataset.columns.map(col(_))\n    val datasetWithExprs = dataset.select(selectedCols: _*)\n"
  },
  {
    "id" : "d80cca49-b7e2-4470-b827-a201dd4e8778",
    "prId" : 24939,
    "prUrl" : "https://github.com/apache/spark/pull/24939#pullrequestreview-341507482",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d44e5dc7-067d-4566-ac5c-40c5bd8a5d39",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Looks correct. We better have a test for this. We can load a model from 2.4 in a test case. There are few examples in ml.",
        "createdAt" : "2020-01-10T02:05:55Z",
        "updatedAt" : "2020-01-14T22:43:12Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "1fe74061-67bb-4e70-98a9-25f707b5ad5b",
        "parentId" : "d44e5dc7-067d-4566-ac5c-40c5bd8a5d39",
        "authorId" : "340aa00e-a2e7-422e-9a35-ef5a82672673",
        "body" : "Thanks, added a testcase for loading a model from 2.4.4.",
        "createdAt" : "2020-01-11T06:18:50Z",
        "updatedAt" : "2020-01-14T22:43:12Z",
        "lastEditedBy" : "340aa00e-a2e7-422e-9a35-ef5a82672673",
        "tags" : [
        ]
      }
    ],
    "commit" : "c3d97a351a04474a260ff6ae40909f366a8398cf",
    "line" : 207,
    "diffHunk" : "@@ -1,1 +500,504 @@      val (major, minor) = VersionUtils.majorMinorVersion(metadata.sparkVersion)\n      val evalExprs = if (major.toInt < 3) Seq[String]() else data.getAs[Seq[String]](3)\n      val resolvedRFormula = ResolvedRFormula(label, terms, hasIntercept, evalExprs)\n\n      val pmPath = new Path(path, \"pipelineModel\").toString"
  }
]