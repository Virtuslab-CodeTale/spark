[
  {
    "id" : "518a398a-737c-46ab-b572-fc29af0ef180",
    "prId" : 27501,
    "prUrl" : "https://github.com/apache/spark/pull/27501#pullrequestreview-355556571",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "533e3698-010c-4463-8817-de8c4c2cca40",
        "parentId" : null,
        "authorId" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "body" : "I will not set the default value here. The default value will be set in each of the class that extends ```HasBlockSize```. ",
        "createdAt" : "2020-02-08T16:44:05Z",
        "updatedAt" : "2020-02-08T16:44:05Z",
        "lastEditedBy" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "tags" : [
        ]
      }
    ],
    "commit" : "ac6b55d9e344f420eacc3ca6dae7578f3b6301bb",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +108,112 @@      ParamDesc[Int](\"blockSize\", \"block size for stacking input data in matrices. Data is \" +\n        \"stacked within partitions. If block size is more than remaining data in a partition \" +\n        \"then it is adjusted to the size of this data.\",\n        isValid = \"ParamValidators.gt(0)\", isExpertParam = true)\n    )"
  }
]