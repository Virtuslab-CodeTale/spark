[
  {
    "id" : "07b95f89-06d1-482f-9e59-eb4f052c5e63",
    "prId" : 30889,
    "prUrl" : "https://github.com/apache/spark/pull/30889#pullrequestreview-614949788",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5cbd933e-52d4-441a-8e54-38ff7e2a255e",
        "parentId" : null,
        "authorId" : "29a3e834-d9e3-42dc-ab25-20ba332ef3f2",
        "body" : "I guess spark schema doesn't allow setting default values (and converted from case class) to enable something like below?\r\nspark.read.schema(nodeDataSchema).parquet(dataPath).as[NodeData]",
        "createdAt" : "2020-12-23T12:07:47Z",
        "updatedAt" : "2020-12-23T12:07:47Z",
        "lastEditedBy" : "29a3e834-d9e3-42dc-ab25-20ba332ef3f2",
        "tags" : [
        ]
      },
      {
        "id" : "cbf31ee0-d7fe-48a3-8716-c1ea8a708192",
        "parentId" : "5cbd933e-52d4-441a-8e54-38ff7e2a255e",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "CC @zhengruifeng on this question",
        "createdAt" : "2020-12-27T21:17:26Z",
        "updatedAt" : "2020-12-27T21:17:27Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "7e8464b8-dd0f-41b4-8d28-96ecdc78f00c",
        "parentId" : "5cbd933e-52d4-441a-8e54-38ff7e2a255e",
        "authorId" : "685f805b-e4fa-4f21-b066-58afcecf9ce6",
        "body" : "I think it is possible to use `spark.read.schema` here, but need to list all the fields manually (method `schema` needs a `schema: StructType` or `schemaString: String` as input).\r\nSo I think it maybe more complicated.",
        "createdAt" : "2020-12-29T08:18:22Z",
        "updatedAt" : "2020-12-29T08:18:22Z",
        "lastEditedBy" : "685f805b-e4fa-4f21-b066-58afcecf9ce6",
        "tags" : [
        ]
      },
      {
        "id" : "ce876df1-3afa-4ef5-a693-8e7724e654fc",
        "parentId" : "5cbd933e-52d4-441a-8e54-38ff7e2a255e",
        "authorId" : "29a3e834-d9e3-42dc-ab25-20ba332ef3f2",
        "body" : "you can create the StructType automatically from the case class like:\r\n`Encoders.product[NodeData].schema`\r\n...but I couldn't find a way to utilize default values, so the new column is all null.",
        "createdAt" : "2020-12-29T10:45:17Z",
        "updatedAt" : "2020-12-29T10:45:33Z",
        "lastEditedBy" : "29a3e834-d9e3-42dc-ab25-20ba332ef3f2",
        "tags" : [
        ]
      },
      {
        "id" : "3a460782-16da-4630-839e-ddccdc924578",
        "parentId" : "5cbd933e-52d4-441a-8e54-38ff7e2a255e",
        "authorId" : "8cbd88f4-1f55-4fdf-85cb-b6d7fb11284f",
        "body" : "@bmarcott @zhengruifeng Was this change applied for Spark 3.0.1 as well? I am facing the same error in Spark 3.0.1 libs. And I checked the above code inside the imported libraries and could see that the change has not been applied there.",
        "createdAt" : "2021-03-17T13:11:45Z",
        "updatedAt" : "2021-03-17T13:12:40Z",
        "lastEditedBy" : "8cbd88f4-1f55-4fdf-85cb-b6d7fb11284f",
        "tags" : [
        ]
      },
      {
        "id" : "fd6ea440-a92c-4de5-a587-ec507d5b001d",
        "parentId" : "5cbd933e-52d4-441a-8e54-38ff7e2a255e",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "It was reported vs 3.0.1, see JIRA. Looks like 3.0.2",
        "createdAt" : "2021-03-17T13:34:23Z",
        "updatedAt" : "2021-03-17T13:34:23Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "77d27154-8282-4548-95a6-93f12be0372c",
        "parentId" : "5cbd933e-52d4-441a-8e54-38ff7e2a255e",
        "authorId" : "685f805b-e4fa-4f21-b066-58afcecf9ce6",
        "body" : "https://github.com/apache/spark/pull/30889#issuecomment-749872161\r\n\r\nsee may previous comments, and this fix come in 3.0.2",
        "createdAt" : "2021-03-18T03:21:19Z",
        "updatedAt" : "2021-03-18T03:21:19Z",
        "lastEditedBy" : "685f805b-e4fa-4f21-b066-58afcecf9ce6",
        "tags" : [
        ]
      }
    ],
    "commit" : "027ec1cf9333fbc0c3435a662f8f3c418d3e7c7e",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +404,408 @@\n    val dataPath = new Path(path, \"data\").toString\n    var df = sparkSession.read.parquet(dataPath)\n    val (major, minor) = VersionUtils.majorMinorVersion(metadata.sparkVersion)\n    if (major.toInt < 3) {"
  },
  {
    "id" : "1a5ec39e-26d3-479e-b52a-39cf72df6bd1",
    "prId" : 25383,
    "prUrl" : "https://github.com/apache/spark/pull/25383#pullrequestreview-275360683",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4704970f-773d-4bcb-ab79-6d8d3c624526",
        "parentId" : null,
        "authorId" : "24e1dd39-ae3f-4bbb-a391-f60afb62d075",
        "body" : "I am a bit worried by the extra memory pressure caused by this, in case trees are big.... can't we rather assign an index to the `LeafNode` class?",
        "createdAt" : "2019-08-10T08:24:43Z",
        "updatedAt" : "2019-08-21T02:51:20Z",
        "lastEditedBy" : "24e1dd39-ae3f-4bbb-a391-f60afb62d075",
        "tags" : [
        ]
      },
      {
        "id" : "67bdd2ca-bb42-4029-b529-3fef31758409",
        "parentId" : "4704970f-773d-4bcb-ab79-6d8d3c624526",
        "authorId" : "685f805b-e4fa-4f21-b066-58afcecf9ce6",
        "body" : "I had impled another leaf-transformation in the .mllib side https://github.com/apache/spark/pull/11520, and it used the sorted `leafId` as the output.\r\nHowever, in the .ml side, the `LeafNode` class do not contain a Id, and is exposed to the end user. So I tend to leave current `LeafNode` class alone.\r\nAs to the extra memory pressure, I think its size O(#numLeaves * #numTrees) is much smaller than the model itself.\r\nWDYT @srowen ",
        "createdAt" : "2019-08-12T03:02:46Z",
        "updatedAt" : "2019-08-21T02:51:20Z",
        "lastEditedBy" : "685f805b-e4fa-4f21-b066-58afcecf9ce6",
        "tags" : [
        ]
      },
      {
        "id" : "b2ef8a04-a455-457c-af29-4f87132859c1",
        "parentId" : "4704970f-773d-4bcb-ab79-6d8d3c624526",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "If it's lazy, it won't affect usages that don't set the leaf col (right?). I think it's reasonable.",
        "createdAt" : "2019-08-14T14:28:12Z",
        "updatedAt" : "2019-08-21T02:51:20Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "eb5f71b2-56da-484d-9f0f-9c0b0513ff55",
        "parentId" : "4704970f-773d-4bcb-ab79-6d8d3c624526",
        "authorId" : "685f805b-e4fa-4f21-b066-58afcecf9ce6",
        "body" : "yes, this var is only generated when needed. and I make it transient, so it don't cause extra network/storage overhead.",
        "createdAt" : "2019-08-15T10:29:49Z",
        "updatedAt" : "2019-08-21T02:51:20Z",
        "lastEditedBy" : "685f805b-e4fa-4f21-b066-58afcecf9ce6",
        "tags" : [
        ]
      }
    ],
    "commit" : "4eb97be97cc4e509e74181dbab428548e6da28e3",
    "line" : 26,
    "diffHunk" : "@@ -1,1 +90,94 @@  }\n\n  @transient private lazy val leafIndices: Map[LeafNode, Int] = {\n    leafIterator(rootNode).zipWithIndex.toMap\n  }"
  }
]