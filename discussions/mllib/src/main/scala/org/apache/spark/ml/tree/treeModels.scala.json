[
  {
    "id" : "07b95f89-06d1-482f-9e59-eb4f052c5e63",
    "prId" : 30889,
    "prUrl" : "https://github.com/apache/spark/pull/30889#pullrequestreview-614949788",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5cbd933e-52d4-441a-8e54-38ff7e2a255e",
        "parentId" : null,
        "authorId" : "29a3e834-d9e3-42dc-ab25-20ba332ef3f2",
        "body" : "I guess spark schema doesn't allow setting default values (and converted from case class) to enable something like below?\r\nspark.read.schema(nodeDataSchema).parquet(dataPath).as[NodeData]",
        "createdAt" : "2020-12-23T12:07:47Z",
        "updatedAt" : "2020-12-23T12:07:47Z",
        "lastEditedBy" : "29a3e834-d9e3-42dc-ab25-20ba332ef3f2",
        "tags" : [
        ]
      },
      {
        "id" : "cbf31ee0-d7fe-48a3-8716-c1ea8a708192",
        "parentId" : "5cbd933e-52d4-441a-8e54-38ff7e2a255e",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "CC @zhengruifeng on this question",
        "createdAt" : "2020-12-27T21:17:26Z",
        "updatedAt" : "2020-12-27T21:17:27Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "7e8464b8-dd0f-41b4-8d28-96ecdc78f00c",
        "parentId" : "5cbd933e-52d4-441a-8e54-38ff7e2a255e",
        "authorId" : "685f805b-e4fa-4f21-b066-58afcecf9ce6",
        "body" : "I think it is possible to use `spark.read.schema` here, but need to list all the fields manually (method `schema` needs a `schema: StructType` or `schemaString: String` as input).\r\nSo I think it maybe more complicated.",
        "createdAt" : "2020-12-29T08:18:22Z",
        "updatedAt" : "2020-12-29T08:18:22Z",
        "lastEditedBy" : "685f805b-e4fa-4f21-b066-58afcecf9ce6",
        "tags" : [
        ]
      },
      {
        "id" : "ce876df1-3afa-4ef5-a693-8e7724e654fc",
        "parentId" : "5cbd933e-52d4-441a-8e54-38ff7e2a255e",
        "authorId" : "29a3e834-d9e3-42dc-ab25-20ba332ef3f2",
        "body" : "you can create the StructType automatically from the case class like:\r\n`Encoders.product[NodeData].schema`\r\n...but I couldn't find a way to utilize default values, so the new column is all null.",
        "createdAt" : "2020-12-29T10:45:17Z",
        "updatedAt" : "2020-12-29T10:45:33Z",
        "lastEditedBy" : "29a3e834-d9e3-42dc-ab25-20ba332ef3f2",
        "tags" : [
        ]
      },
      {
        "id" : "3a460782-16da-4630-839e-ddccdc924578",
        "parentId" : "5cbd933e-52d4-441a-8e54-38ff7e2a255e",
        "authorId" : "8cbd88f4-1f55-4fdf-85cb-b6d7fb11284f",
        "body" : "@bmarcott @zhengruifeng Was this change applied for Spark 3.0.1 as well? I am facing the same error in Spark 3.0.1 libs. And I checked the above code inside the imported libraries and could see that the change has not been applied there.",
        "createdAt" : "2021-03-17T13:11:45Z",
        "updatedAt" : "2021-03-17T13:12:40Z",
        "lastEditedBy" : "8cbd88f4-1f55-4fdf-85cb-b6d7fb11284f",
        "tags" : [
        ]
      },
      {
        "id" : "fd6ea440-a92c-4de5-a587-ec507d5b001d",
        "parentId" : "5cbd933e-52d4-441a-8e54-38ff7e2a255e",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "It was reported vs 3.0.1, see JIRA. Looks like 3.0.2",
        "createdAt" : "2021-03-17T13:34:23Z",
        "updatedAt" : "2021-03-17T13:34:23Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "77d27154-8282-4548-95a6-93f12be0372c",
        "parentId" : "5cbd933e-52d4-441a-8e54-38ff7e2a255e",
        "authorId" : "685f805b-e4fa-4f21-b066-58afcecf9ce6",
        "body" : "https://github.com/apache/spark/pull/30889#issuecomment-749872161\r\n\r\nsee may previous comments, and this fix come in 3.0.2",
        "createdAt" : "2021-03-18T03:21:19Z",
        "updatedAt" : "2021-03-18T03:21:19Z",
        "lastEditedBy" : "685f805b-e4fa-4f21-b066-58afcecf9ce6",
        "tags" : [
        ]
      }
    ],
    "commit" : "027ec1cf9333fbc0c3435a662f8f3c418d3e7c7e",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +404,408 @@\n    val dataPath = new Path(path, \"data\").toString\n    var df = sparkSession.read.parquet(dataPath)\n    val (major, minor) = VersionUtils.majorMinorVersion(metadata.sparkVersion)\n    if (major.toInt < 3) {"
  }
]