[
  {
    "id" : "bad1afb0-bf48-464d-89d4-4cc9cd167abc",
    "prId" : 27522,
    "prUrl" : "https://github.com/apache/spark/pull/27522#pullrequestreview-356073285",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c871a412-b88a-4935-bf04-e26b493a6eef",
        "parentId" : null,
        "authorId" : "7e665d8b-d739-4edf-88c8-7379ff8585c2",
        "body" : "Why not merge this method into `vectorToArrayUdf` ? like:\r\n`vectorToArrayUdf = udf { (vec: Any, type: String) => ... }`",
        "createdAt" : "2020-02-10T11:24:37Z",
        "updatedAt" : "2020-02-13T09:06:46Z",
        "lastEditedBy" : "7e665d8b-d739-4edf-88c8-7379ff8585c2",
        "tags" : [
        ]
      },
      {
        "id" : "eea7d97f-20b4-44e0-a9c7-774f4acfd34f",
        "parentId" : "c871a412-b88a-4935-bf04-e26b493a6eef",
        "authorId" : "7e665d8b-d739-4edf-88c8-7379ff8585c2",
        "body" : "@liangz1 It's my fault. We cannot combine the two UDF together. Their return type are `Array<Double>` and `Array<Float>`, they're incompatible. This leads to error in your tests.",
        "createdAt" : "2020-02-10T16:21:27Z",
        "updatedAt" : "2020-02-13T09:06:46Z",
        "lastEditedBy" : "7e665d8b-d739-4edf-88c8-7379ff8585c2",
        "tags" : [
        ]
      }
    ],
    "commit" : "848220d8a892e08e6bc99f5b6293b3c57dd892c4",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +46,50 @@        data\n      case v: Vector => v.toArray.map(_.toFloat)\n      case v: OldVector => v.toArray.map(_.toFloat)\n      case v => throw new IllegalArgumentException(\n        \"function vector_to_array requires a non-null input argument and input type must be \" +"
  },
  {
    "id" : "696ef0e5-b9a9-4928-8845-e7f604a89e4e",
    "prId" : 26910,
    "prUrl" : "https://github.com/apache/spark/pull/26910#pullrequestreview-334490523",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e3774575-5496-49ea-87bd-a957dd7da2ba",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Why not create a custom expression to do it instead of a UDF? We can avoid serde overhead with a custom expression.",
        "createdAt" : "2019-12-19T05:22:36Z",
        "updatedAt" : "2019-12-21T03:51:28Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "96db973b-73e5-464e-926d-8a1178bd1170",
        "parentId" : "e3774575-5496-49ea-87bd-a957dd7da2ba",
        "authorId" : "7e665d8b-d739-4edf-88c8-7379ff8585c2",
        "body" : "We have discussed this in this thread https://github.com/apache/spark/pull/26910#discussion_r359128973",
        "createdAt" : "2019-12-19T08:50:41Z",
        "updatedAt" : "2019-12-21T03:51:28Z",
        "lastEditedBy" : "7e665d8b-d739-4edf-88c8-7379ff8585c2",
        "tags" : [
        ]
      },
      {
        "id" : "7f4ce4e3-fbdc-4ffa-b038-0b085fab104e",
        "parentId" : "e3774575-5496-49ea-87bd-a957dd7da2ba",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "OK then it's fine",
        "createdAt" : "2019-12-19T09:07:19Z",
        "updatedAt" : "2019-12-21T03:51:28Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "d257dce703986e4ed325af0bae07c88a467a684e",
    "line" : 31,
    "diffHunk" : "@@ -1,1 +29,33 @@// scalastyle:on\n\n  private val vectorToArrayUdf = udf { vec: Any =>\n    vec match {\n      case v: Vector => v.toArray"
  }
]