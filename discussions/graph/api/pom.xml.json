[
  {
    "id" : "60ff287c-3717-40d9-8689-ee90a6afaede",
    "prId" : 24851,
    "prUrl" : "https://github.com/apache/spark/pull/24851#pullrequestreview-266543431",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5720e981-8da0-4842-816b-a36f5eaf9da8",
        "parentId" : null,
        "authorId" : "0c812942-02cb-4975-9748-394d1387affa",
        "body" : "Do the tests explicitly use classes from spark-core test jar? If not, we shouldn't need to declare it as a test dependency here. Same for spark-catalyst test jar.",
        "createdAt" : "2019-07-16T17:53:42Z",
        "updatedAt" : "2019-10-29T08:54:44Z",
        "lastEditedBy" : "0c812942-02cb-4975-9748-394d1387affa",
        "tags" : [
        ]
      },
      {
        "id" : "ea4ca5db-7bb3-4369-92ad-468986e26029",
        "parentId" : "5720e981-8da0-4842-816b-a36f5eaf9da8",
        "authorId" : "cf7f0cf3-88a3-4a5f-87f7-04ccf6a74f3b",
        "body" : "Our tests depend on `org.apache.spark.sql.QueryTest` which requires `SparkFunSuite` (spark-sql) and `PlanTest` (spark-catalyst).",
        "createdAt" : "2019-07-25T10:42:39Z",
        "updatedAt" : "2019-10-29T08:54:44Z",
        "lastEditedBy" : "cf7f0cf3-88a3-4a5f-87f7-04ccf6a74f3b",
        "tags" : [
        ]
      }
    ],
    "commit" : "44a955bdbe7f9ef88b69193635e1678668584ea3",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +52,56 @@    <dependency>\n      <groupId>org.apache.spark</groupId>\n      <artifactId>spark-core_${scala.binary.version}</artifactId>\n      <version>${project.version}</version>\n      <type>test-jar</type>"
  },
  {
    "id" : "9af6e25c-a111-41d5-b06c-28f360f90e8c",
    "prId" : 24490,
    "prUrl" : "https://github.com/apache/spark/pull/24490#pullrequestreview-245468072",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "266e2ac5-141b-4c48-a7f9-6a5f50108d73",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "I don't see a strong need for 3 (!) new modules here. Just put them into one? This doesn't cover that much ground.",
        "createdAt" : "2019-06-03T22:21:09Z",
        "updatedAt" : "2019-06-06T08:24:41Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "8e4a5d8e-0e47-4d3f-aea3-b03ec3f6b360",
        "parentId" : "266e2ac5-141b-4c48-a7f9-6a5f50108d73",
        "authorId" : "23eb82aa-4e11-441d-968f-aee66e7aac69",
        "body" : "The idea with three modules is to allow one for Cypher querying, one for Graph Algorithms, and one parent to unite the both. Is there a large cost associated with additional modules?",
        "createdAt" : "2019-06-04T07:05:30Z",
        "updatedAt" : "2019-06-06T08:24:41Z",
        "lastEditedBy" : "23eb82aa-4e11-441d-968f-aee66e7aac69",
        "tags" : [
        ]
      },
      {
        "id" : "95a82d9d-5b8a-4895-89b1-2a1d9b77d2e9",
        "parentId" : "266e2ac5-141b-4c48-a7f9-6a5f50108d73",
        "authorId" : "cf7f0cf3-88a3-4a5f-87f7-04ccf6a74f3b",
        "body" : "Just to add to this: We discussed the module architecture on the [SPIP design sketch document](https://docs.google.com/document/d/1Wxzghj0PvpOVu7XD1iA8uonRYhexwn18utdcTxtkxlI/edit#heading=h.om6xtdv3p3fn) and offline with @mengxr. We want to have the option for user applications to switch the engine implementation without changing the API. Furthermore, graph algorithms ported from GraphFrames/GraphX will live in an additional module that depends on `graph-api`.",
        "createdAt" : "2019-06-04T07:57:39Z",
        "updatedAt" : "2019-06-06T08:24:41Z",
        "lastEditedBy" : "cf7f0cf3-88a3-4a5f-87f7-04ccf6a74f3b",
        "tags" : [
        ]
      },
      {
        "id" : "171a42e5-8d94-4819-b7dc-3adf65d56396",
        "parentId" : "266e2ac5-141b-4c48-a7f9-6a5f50108d73",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "I don't feel strongly about it, but it seems like overkill for a small module that probably only has one implementation in the foreseeable future. I generally would have preferred (at least some of) this live outside Spark, but I agree to disagree",
        "createdAt" : "2019-06-04T14:03:59Z",
        "updatedAt" : "2019-06-06T08:24:41Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      }
    ],
    "commit" : "47bd5bc50989b8514a6a4dab29b47bec485f0d53",
    "line" : 1,
    "diffHunk" : "@@ -1,1 +-1,3 @@<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<!--\n  ~ Licensed to the Apache Software Foundation (ASF) under one or more"
  }
]