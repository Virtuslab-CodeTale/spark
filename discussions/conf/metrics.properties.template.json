[
  {
    "id" : "28327f4c-df23-4004-8c8c-2a558bc60463",
    "prId" : 25769,
    "prUrl" : "https://github.com/apache/spark/pull/25769#pullrequestreview-299024869",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8d4d305c-649b-47d8-b253-9404467e3425",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "@dongjoon-hyun Will this swamp the Prometheus server? A lot of metrics of numerous Spark applications will be sent to the central server, for example, `metrics_{$app_id}_driver_BlockManager_disk_diskSpaceUsed_MB_Value 0`\r\n\r\nHave you tried to run it ? How to deal with the high load issues? ",
        "createdAt" : "2019-10-05T23:45:52Z",
        "updatedAt" : "2019-10-05T23:45:53Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "b4e39afe-262e-42fb-b2b1-75cfae7430f7",
        "parentId" : "8d4d305c-649b-47d8-b253-9404467e3425",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Actually, I've used this in a long-running spark streaming app in the dedicated Spark standalone cluster.",
        "createdAt" : "2019-10-05T23:50:01Z",
        "updatedAt" : "2019-10-05T23:51:17Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "613f3bca-0b54-492b-9e37-e71ba1d0e10a",
        "parentId" : "8d4d305c-649b-47d8-b253-9404467e3425",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "And, I fully understand what you mean.",
        "createdAt" : "2019-10-05T23:51:06Z",
        "updatedAt" : "2019-10-05T23:51:06Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "4fd86cd4-f33b-4eaa-ba57-9cd74ae7ff7f",
        "parentId" : "8d4d305c-649b-47d8-b253-9404467e3425",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "@gatorsmile . Did you try to use `spark.metrics.namespace`?\r\n```\r\n$ bin/spark-shell --conf spark.metrics.namespace=ns\r\n... (running)\r\n```\r\n\r\n```\r\n$ curl -s http://localhost:4040/metrics/prometheus/ | grep driver_BlockManager_disk_diskSpaceUse\r\nmetrics_ns_driver_BlockManager_disk_diskSpaceUsed_MB_Value 0\r\n```",
        "createdAt" : "2019-10-08T20:16:01Z",
        "updatedAt" : "2019-10-08T20:16:02Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "f66c2e7b-35d6-4c0f-9390-dc05d1a30c28",
        "parentId" : "8d4d305c-649b-47d8-b253-9404467e3425",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "The key is generated by `MetricsSystem.buildRegistryName`, not by the individual `Sink`s.\r\n- http://spark.apache.org/docs/latest/monitoring.html#metrics\r\n> However, often times, users want to be able to track the metrics across apps for driver and executors, which is hard to do with application ID (i.e. spark.app.id) since it changes with every invocation of the app. For such use cases, a custom namespace can be specified for metrics reporting using spark.metrics.namespace configuration property. \r\n\r\nI'd like to improve this further according to the additional requirement. Please let me know.",
        "createdAt" : "2019-10-08T20:20:48Z",
        "updatedAt" : "2019-10-08T20:20:48Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "0fda09667478b5010e53d614be4655fd01b845a7",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +208,212 @@#*.sink.prometheusServlet.path=/metrics/prometheus\n#master.sink.prometheusServlet.path=/metrics/master/prometheus\n#applications.sink.prometheusServlet.path=/metrics/applications/prometheus"
  }
]