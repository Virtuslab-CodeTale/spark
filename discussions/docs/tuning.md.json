[
  {
    "id" : "dd5c4436-830b-4ffb-b5ef-f0e806afc7c5",
    "prId" : 29498,
    "prUrl" : "https://github.com/apache/spark/pull/29498#pullrequestreview-472172851",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "393008f2-4104-4565-ac8e-9cd35fa3f3a4",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "How about making a new subsection (e.g., `## XXXX`) for this? ",
        "createdAt" : "2020-08-21T00:20:43Z",
        "updatedAt" : "2020-08-21T06:58:07Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "7faa0818-a1a3-4916-88df-3a46dcb25d1d",
        "parentId" : "393008f2-4104-4565-ac8e-9cd35fa3f3a4",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "sure will do",
        "createdAt" : "2020-08-21T01:22:10Z",
        "updatedAt" : "2020-08-21T06:58:07Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "6b22ae03-8837-4c53-a9f9-98aa0bbfe449",
        "parentId" : "393008f2-4104-4565-ac8e-9cd35fa3f3a4",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Yep. +1 for @maropu 's suggestion. This content looks worthy for that.",
        "createdAt" : "2020-08-21T05:23:27Z",
        "updatedAt" : "2020-08-21T06:58:07Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "0efbe230-164f-48a5-9681-ff02ec1648df",
        "parentId" : "393008f2-4104-4565-ac8e-9cd35fa3f3a4",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "üëç ",
        "createdAt" : "2020-08-21T06:10:58Z",
        "updatedAt" : "2020-08-21T06:58:07Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      }
    ],
    "commit" : "c4622716b9c7b6e15f6d4f4724e57323d147dd97",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +267,271 @@## Parallel Listing on Input Paths\n\nSometimes you may also need to increase directory listing parallelism when job input has large number of directories,\notherwise the process could take a very long time, especially when against object store like S3.\nIf your job works on RDD with Hadoop input formats (e.g., via `SparkContext.sequenceFile`), the parallelism is"
  },
  {
    "id" : "6e6f3bcc-af22-449e-9462-a80baa3482cf",
    "prId" : 29498,
    "prUrl" : "https://github.com/apache/spark/pull/29498#pullrequestreview-472208167",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "348cdb21-45bc-4b42-b553-0d7bc3581003",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "What about remote HDFS? Since this looks like a general issue for remote storage access. Especially, in disaggregated clusters where remote storage (HDFS/S3) are used, can we generalize more like the following?\r\n```\r\n- especially when against object store like S3\r\n- especially when against remote HDFS or S3 or in the disaggregated clusters\r\n```",
        "createdAt" : "2020-08-21T05:43:47Z",
        "updatedAt" : "2020-08-21T06:58:07Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "a65af06b-1317-4ab8-a25d-cd85f0e021ab",
        "parentId" : "348cdb21-45bc-4b42-b553-0d7bc3581003",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "It depends on how \"remote\" the storage is. For HDFS, depending on the use case the compute and storage can still be deployed within the same region or even zone and therefore network/metadata cost is much cheaper than that from S3.\r\n\r\nTherefore, I think we can stick with the S3 case as it is more characteristic. Let me know if you think otherwise.",
        "createdAt" : "2020-08-21T06:31:34Z",
        "updatedAt" : "2020-08-21T06:58:07Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      }
    ],
    "commit" : "c4622716b9c7b6e15f6d4f4724e57323d147dd97",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +268,272 @@\nSometimes you may also need to increase directory listing parallelism when job input has large number of directories,\notherwise the process could take a very long time, especially when against object store like S3.\nIf your job works on RDD with Hadoop input formats (e.g., via `SparkContext.sequenceFile`), the parallelism is\ncontrolled via [`spark.hadoop.mapreduce.input.fileinputformat.list-status.num-threads`](https://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml) (currently default is 1)."
  }
]