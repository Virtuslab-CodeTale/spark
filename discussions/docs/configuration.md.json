[
  {
    "id" : "7b99ccd1-e42f-4d33-a35b-86beb5425c8f",
    "prId" : 33615,
    "prUrl" : "https://github.com/apache/spark/pull/33615#pullrequestreview-722033841",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0e6c952c-a215-4571-888b-382ce5506b1f",
        "parentId" : null,
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "`By default, push-based shuffle is disabled at the server side, which does not perform push-based shuffle irrespective of the client side configuration` ->\r\n\r\n`Currently, push-based shuffle is disabled by default at the server side.`\r\n\r\nI am not very sure whether we need the suffix - irrespective of client part.\r\nThoughts ?",
        "createdAt" : "2021-08-04T09:07:46Z",
        "updatedAt" : "2021-08-04T09:25:07Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      }
    ],
    "commit" : "12618284d95d7b1685c2c48c79bffcb618cf4523",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +3184,3188 @@  <td>\n    Class name of the implementation of <code>MergedShuffleFileManager</code> that manages push-based shuffle. This acts as a server side config to disable or enable push-based shuffle. By default, push-based shuffle is disabled at the server side. <p> To enable push-based shuffle on the server side, set this config to <code>org.apache.spark.network.shuffle.RemoteBlockPushResolver</code></p>\n  </td>\n  <td>3.2.0</td>\n</tr>"
  },
  {
    "id" : "3b41b2a0-faba-47f1-981c-e901aae0fd30",
    "prId" : 33615,
    "prUrl" : "https://github.com/apache/spark/pull/33615#pullrequestreview-727774199",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a1b33809-242c-469d-9dff-42b7853242eb",
        "parentId" : null,
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "nit: Remove the config key in the end of the line. The description is for that key.\r\n```\r\nTo enable push-based shuffle on the server side, set this config  to <code>org.apache.spark.network.shuffle.RemoteBlockPushResolver</code></p>\r\n```",
        "createdAt" : "2021-08-11T17:48:31Z",
        "updatedAt" : "2021-08-11T17:57:02Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      }
    ],
    "commit" : "12618284d95d7b1685c2c48c79bffcb618cf4523",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +3184,3188 @@  <td>\n    Class name of the implementation of <code>MergedShuffleFileManager</code> that manages push-based shuffle. This acts as a server side config to disable or enable push-based shuffle. By default, push-based shuffle is disabled at the server side. <p> To enable push-based shuffle on the server side, set this config to <code>org.apache.spark.network.shuffle.RemoteBlockPushResolver</code></p>\n  </td>\n  <td>3.2.0</td>\n</tr>"
  },
  {
    "id" : "915bc95d-d253-4d9f-bbeb-3c16101e9bec",
    "prId" : 33615,
    "prUrl" : "https://github.com/apache/spark/pull/33615#pullrequestreview-727774199",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "91045dc6-7c15-4b11-ba58-7cdf83572696",
        "parentId" : null,
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "nit: Add a newline after `<code>MB-sized chunks</code>`",
        "createdAt" : "2021-08-11T17:49:47Z",
        "updatedAt" : "2021-08-11T17:57:02Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      }
    ],
    "commit" : "12618284d95d7b1685c2c48c79bffcb618cf4523",
    "line" : 32,
    "diffHunk" : "@@ -1,1 +3192,3196 @@  <td>\n    <p> The minimum size of a chunk when dividing a merged shuffle file into multiple chunks during push-based shuffle. A merged shuffle file consists of multiple small shuffle blocks. Fetching the complete merged shuffle file in a single disk I/O increases the memory requirements for both the clients and the external shuffle services. Instead, the external shuffle service serves the merged file in <code>MB-sized chunks</code>.<br /> This configuration controls how big a chunk can get. A corresponding index file for each merged shuffle file will be generated indicating chunk boundaries. </p>\n    <p> Setting this too high would increase the memory requirements on both the clients and the external shuffle service. </p>\n    <p> Setting this too low would increase the overall number of RPC requests to external shuffle service unnecessarily.</p>\n  </td>"
  },
  {
    "id" : "2ab2f2dd-8ba9-4b00-8c14-b2d846c53556",
    "prId" : 31990,
    "prUrl" : "https://github.com/apache/spark/pull/31990#pullrequestreview-623125959",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4160de6e-177a-4af3-8eba-1ad94261f12b",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "what about `spark.files.io.connectionTimeout`?",
        "createdAt" : "2021-03-29T11:27:47Z",
        "updatedAt" : "2021-03-29T12:08:47Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "26f07e9ee3dcf4c1f50fc73e6575d2adb7b6a146",
    "line" : 39,
    "diffHunk" : "@@ -1,1 +2019,2023 @@</tr>\n<tr>\n  <td><code>spark.rpc.io.connectionTimeout</code></td>\n  <td>value of <code>spark.network.timeout</code></td>\n  <td>"
  },
  {
    "id" : "d44a2d5d-f4bd-43fc-bb69-39224043e653",
    "prId" : 31618,
    "prUrl" : "https://github.com/apache/spark/pull/31618#pullrequestreview-601207826",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ad10e182-df49-4ded-b2e9-2c6aeb9f7126",
        "parentId" : null,
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "sorry for coming in late, was out last week, we may want to reference what other codecs can be used here.   @dongjoon-hyun thoughts?",
        "createdAt" : "2021-03-01T17:18:01Z",
        "updatedAt" : "2021-03-01T17:18:01Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "f2665c6d-5457-472e-94d5-c73e9bd88df0",
        "parentId" : "ad10e182-df49-4ded-b2e9-2c6aeb9f7126",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Thank you for review, @tgravescs . Sure, I'll make a documentation follow-up.",
        "createdAt" : "2021-03-01T21:56:40Z",
        "updatedAt" : "2021-03-01T21:56:40Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "b84158af-373b-4d60-ba5f-a3c4e4f599da",
        "parentId" : "ad10e182-df49-4ded-b2e9-2c6aeb9f7126",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Here, I made a PR.\r\n- https://github.com/apache/spark/pull/31695",
        "createdAt" : "2021-03-01T22:02:52Z",
        "updatedAt" : "2021-03-01T22:02:52Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "0e88652b99b833551d3e940a24d9d2c217fe4f51",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +1043,1047 @@  <td>zstd</td>\n  <td>\n    The codec to compress logged events.\n  </td>\n  <td>3.0.0</td>"
  },
  {
    "id" : "c3556901-198a-4bf6-b626-f32e3d122d5e",
    "prId" : 31162,
    "prUrl" : "https://github.com/apache/spark/pull/31162#pullrequestreview-567439514",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "32226d14-cbf5-4bab-8bdf-83960f790f53",
        "parentId" : null,
        "authorId" : "363b0980-061e-4268-a6b1-8d344cf0887e",
        "body" : "Is this the correct release to put here?",
        "createdAt" : "2021-01-13T17:07:39Z",
        "updatedAt" : "2021-02-13T00:30:53Z",
        "lastEditedBy" : "363b0980-061e-4268-a6b1-8d344cf0887e",
        "tags" : [
        ]
      }
    ],
    "commit" : "43f30c68017b349b2b776314714c93d05cb12a01",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +2845,2849 @@    wait until it decides that it can't launch the SparkR daemon?\n  </td>\n  <td>3.2.0</td>\n</tr>\n<tr>"
  },
  {
    "id" : "2220adc3-8347-41a4-87bd-0bb06b85ce43",
    "prId" : 30710,
    "prUrl" : "https://github.com/apache/spark/pull/30710#pullrequestreview-564482933",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "aa4cade1-eba8-4dc4-b89e-aed3ae961717",
        "parentId" : null,
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "sorry I wasn't clear, I meant for you to keep the second sentence as well to explain its use.\r\n\r\nMaybe something like: \"\r\nMinimum amount of time a task runs before being considered for speculation.\r\nThis can be used to avoid launching speculative copies of tasks that are very short.\"",
        "createdAt" : "2021-01-08T17:20:08Z",
        "updatedAt" : "2021-01-09T06:53:31Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "c0461ef4-5ee7-401b-9628-f528ad008807",
        "parentId" : "aa4cade1-eba8-4dc4-b89e-aed3ae961717",
        "authorId" : "fb1da7a4-0c05-42a2-9913-c8bd60c58a4f",
        "body" : "ok",
        "createdAt" : "2021-01-08T18:22:32Z",
        "updatedAt" : "2021-01-09T06:53:31Z",
        "lastEditedBy" : "fb1da7a4-0c05-42a2-9913-c8bd60c58a4f",
        "tags" : [
        ]
      }
    ],
    "commit" : "86c8d8c51050b67ae8714f1dd1249f62d8b56bd0",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +2314,2318 @@  <td>100ms</td>\n  <td>\n    Minimum amount of time a task runs before being considered for speculation.\n    This can be used to avoid launching speculative copies of tasks that are very short.\n  </td>"
  },
  {
    "id" : "d4f956c4-4432-41c4-b4de-c0f5260708e9",
    "prId" : 29090,
    "prUrl" : "https://github.com/apache/spark/pull/29090#pullrequestreview-458404806",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "30292276-39ab-4ab0-bc6d-c28d0640c058",
        "parentId" : null,
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "it seems we are a bit inconsistent across the documentation as wel (pyspark.memory, memoryOverhead)l. other memory settings just say MiB unless otherwise specified but don't mention the suffix options. I wonder if we make them all consistent.  Note one of the yarn configs says: Use lower-case suffixes, e.g. k, m, g, t, and p, for kibi-, mebi-, gibi-, tebi-, and pebibytes, respectively. but again doesn't say m is the default.",
        "createdAt" : "2020-07-30T13:34:33Z",
        "updatedAt" : "2020-08-03T13:06:05Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "8dfe64373fdf5020f635c1ccac003c97d9a64df4",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +250,254 @@  <td>\n    Amount of memory to use per executor process, in the same format as JVM memory strings with\n    a size unit suffix (\"k\", \"m\", \"g\" or \"t\") (e.g. <code>512m</code>, <code>2g</code>) using\n    \"m\" as the default unit.\n  </td>"
  },
  {
    "id" : "c869612f-bf03-48c6-a9e1-4dc853b59c1f",
    "prId" : 28274,
    "prUrl" : "https://github.com/apache/spark/pull/28274#pullrequestreview-398152976",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "38378a66-f1ab-440d-a436-03b805659f43",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I intentionally removed the leading spaces here because they are considered actual white spaces. Newer liquid syntax supports to ignore these white spaces but I didn't use in case old Jykill is used.\r\n\r\nSeems like it could make the html format malformed in some cases given my rough testing. Let's remove these leading white spaces next time.",
        "createdAt" : "2020-04-22T11:20:22Z",
        "updatedAt" : "2020-04-22T11:20:22Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "0393b63d-e465-4ed9-8b58-68ed2a83bfd3",
        "parentId" : "38378a66-f1ab-440d-a436-03b805659f43",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "Might the problem be that `### Spark SQL` content was inside the for-loop before?",
        "createdAt" : "2020-04-22T11:25:36Z",
        "updatedAt" : "2020-04-22T11:25:53Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "b900280a-cf11-4723-86f9-f8808556e401",
        "parentId" : "38378a66-f1ab-440d-a436-03b805659f43",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "No, we already use the markdown and Liquid syntax together at https://github.com/apache/spark/blob/master/docs/sql-ref-functions-builtin.md",
        "createdAt" : "2020-04-22T12:46:58Z",
        "updatedAt" : "2020-04-22T12:46:58Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "81e043f592da25d03fe7674f00eeab9333a473a5",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +2634,2638 @@{% for static_file in site.static_files %}\n    {% if static_file.name == 'generated-runtime-sql-config-table.html' %}\n        {% include_relative generated-runtime-sql-config-table.html %}\n        {% break %}\n    {% endif %}"
  },
  {
    "id" : "8fce85cb-95ae-4d8e-a0c9-9480152516bd",
    "prId" : 28274,
    "prUrl" : "https://github.com/apache/spark/pull/28274#pullrequestreview-398095226",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "810a1b1c-f577-4105-8702-d619892c4bcb",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "You could mention `spark-default.conf` too, but it's okay.",
        "createdAt" : "2020-04-22T11:21:28Z",
        "updatedAt" : "2020-04-22T11:21:29Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "c53d21c5-128f-484e-9423-4283c29064c1",
        "parentId" : "810a1b1c-f577-4105-8702-d619892c4bcb",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "`by the config file` might have the same meaning but it could be better if we use `spark-default.conf`",
        "createdAt" : "2020-04-22T11:28:13Z",
        "updatedAt" : "2020-04-22T11:28:13Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "81e043f592da25d03fe7674f00eeab9333a473a5",
    "line" : 27,
    "diffHunk" : "@@ -1,1 +2643,2647 @@\nStatic SQL configurations are cross-session, immutable Spark SQL configurations. They can be set with final values by the config file\nand command-line options with `--conf/-c` prefixed, or by setting `SparkConf` that are used to create `SparkSession`.\nExternal users can query the static sql config values via `SparkSession.conf` or via set command, e.g. `SET spark.sql.extensions;`, but cannot set/unset them.\n"
  },
  {
    "id" : "77e6641a-74ba-4f85-880d-e6225ae07ccb",
    "prId" : 28224,
    "prUrl" : "https://github.com/apache/spark/pull/28224#pullrequestreview-396286022",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "288a64f8-9fa1-4906-8f6c-1c9e728f8647",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Looks the spaces are inserted together. I removed it just to be safe.",
        "createdAt" : "2020-04-20T09:26:53Z",
        "updatedAt" : "2020-04-21T00:28:49Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "f302c91fc270fb4a7607381c3ca1af7ccbecd7c1",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +2627,2631 @@### Spark SQL\n\n{% include_relative generated-sql-configuration-table.html %}\n        {% break %}\n    {% endif %}"
  },
  {
    "id" : "04bc4a69-4b49-4a77-9cf9-342bb5a38735",
    "prId" : 28132,
    "prUrl" : "https://github.com/apache/spark/pull/28132#pullrequestreview-387899281",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "42d1154c-3daa-486c-b348-3d47f0881ae8",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "`spark.shuffle.service.enabled=false` and `spark.dynamicAllocation.shuffleTracking.enabled=true` is experimental now? https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/ExecutorAllocationManager.scala#L208\r\n\r\nIf so, its better to leave some notes about that here?",
        "createdAt" : "2020-04-06T02:09:35Z",
        "updatedAt" : "2020-04-06T02:09:35Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "cc3faa78-611e-4feb-9c9f-1e97d269ebee",
        "parentId" : "42d1154c-3daa-486c-b348-3d47f0881ae8",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "You mean documenting that dynamic allocation without a shuffle service is experimental? I think it's fine as is. That's documented at `spark.dynamicAllocation.shuffleTracking.enabled` at https://github.com/apache/spark/pull/24817/files#diff-76e731333fb756df3bff5ddb3b731c46R2121 (right below), and it will show a warning as you pointed out.",
        "createdAt" : "2020-04-06T02:14:14Z",
        "updatedAt" : "2020-04-06T02:15:19Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "ff3bc758-c215-4d75-b307-5aeb22354e9e",
        "parentId" : "42d1154c-3daa-486c-b348-3d47f0881ae8",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "ok",
        "createdAt" : "2020-04-06T02:15:33Z",
        "updatedAt" : "2020-04-06T02:15:33Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "c0491b3f58b6b13cc289edc348b52a66a60daf6a",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +2458,2462 @@    <br><br>\n    This requires <code>spark.shuffle.service.enabled</code> or\n    <code>spark.dynamicAllocation.shuffleTracking.enabled</code> to be set.\n    The following configurations are also relevant:\n    <code>spark.dynamicAllocation.minExecutors</code>,"
  }
]