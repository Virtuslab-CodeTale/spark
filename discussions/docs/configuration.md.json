[
  {
    "id" : "7b99ccd1-e42f-4d33-a35b-86beb5425c8f",
    "prId" : 33615,
    "prUrl" : "https://github.com/apache/spark/pull/33615#pullrequestreview-722033841",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0e6c952c-a215-4571-888b-382ce5506b1f",
        "parentId" : null,
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "`By default, push-based shuffle is disabled at the server side, which does not perform push-based shuffle irrespective of the client side configuration` ->\r\n\r\n`Currently, push-based shuffle is disabled by default at the server side.`\r\n\r\nI am not very sure whether we need the suffix - irrespective of client part.\r\nThoughts ?",
        "createdAt" : "2021-08-04T09:07:46Z",
        "updatedAt" : "2021-08-04T09:25:07Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      }
    ],
    "commit" : "12618284d95d7b1685c2c48c79bffcb618cf4523",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +3184,3188 @@  <td>\n    Class name of the implementation of <code>MergedShuffleFileManager</code> that manages push-based shuffle. This acts as a server side config to disable or enable push-based shuffle. By default, push-based shuffle is disabled at the server side. <p> To enable push-based shuffle on the server side, set this config to <code>org.apache.spark.network.shuffle.RemoteBlockPushResolver</code></p>\n  </td>\n  <td>3.2.0</td>\n</tr>"
  },
  {
    "id" : "3b41b2a0-faba-47f1-981c-e901aae0fd30",
    "prId" : 33615,
    "prUrl" : "https://github.com/apache/spark/pull/33615#pullrequestreview-727774199",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a1b33809-242c-469d-9dff-42b7853242eb",
        "parentId" : null,
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "nit: Remove the config key in the end of the line. The description is for that key.\r\n```\r\nTo enable push-based shuffle on the server side, set this config  to <code>org.apache.spark.network.shuffle.RemoteBlockPushResolver</code></p>\r\n```",
        "createdAt" : "2021-08-11T17:48:31Z",
        "updatedAt" : "2021-08-11T17:57:02Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      }
    ],
    "commit" : "12618284d95d7b1685c2c48c79bffcb618cf4523",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +3184,3188 @@  <td>\n    Class name of the implementation of <code>MergedShuffleFileManager</code> that manages push-based shuffle. This acts as a server side config to disable or enable push-based shuffle. By default, push-based shuffle is disabled at the server side. <p> To enable push-based shuffle on the server side, set this config to <code>org.apache.spark.network.shuffle.RemoteBlockPushResolver</code></p>\n  </td>\n  <td>3.2.0</td>\n</tr>"
  },
  {
    "id" : "915bc95d-d253-4d9f-bbeb-3c16101e9bec",
    "prId" : 33615,
    "prUrl" : "https://github.com/apache/spark/pull/33615#pullrequestreview-727774199",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "91045dc6-7c15-4b11-ba58-7cdf83572696",
        "parentId" : null,
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "nit: Add a newline after `<code>MB-sized chunks</code>`",
        "createdAt" : "2021-08-11T17:49:47Z",
        "updatedAt" : "2021-08-11T17:57:02Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      }
    ],
    "commit" : "12618284d95d7b1685c2c48c79bffcb618cf4523",
    "line" : 32,
    "diffHunk" : "@@ -1,1 +3192,3196 @@  <td>\n    <p> The minimum size of a chunk when dividing a merged shuffle file into multiple chunks during push-based shuffle. A merged shuffle file consists of multiple small shuffle blocks. Fetching the complete merged shuffle file in a single disk I/O increases the memory requirements for both the clients and the external shuffle services. Instead, the external shuffle service serves the merged file in <code>MB-sized chunks</code>.<br /> This configuration controls how big a chunk can get. A corresponding index file for each merged shuffle file will be generated indicating chunk boundaries. </p>\n    <p> Setting this too high would increase the memory requirements on both the clients and the external shuffle service. </p>\n    <p> Setting this too low would increase the overall number of RPC requests to external shuffle service unnecessarily.</p>\n  </td>"
  },
  {
    "id" : "2ab2f2dd-8ba9-4b00-8c14-b2d846c53556",
    "prId" : 31990,
    "prUrl" : "https://github.com/apache/spark/pull/31990#pullrequestreview-623125959",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4160de6e-177a-4af3-8eba-1ad94261f12b",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "what about `spark.files.io.connectionTimeout`?",
        "createdAt" : "2021-03-29T11:27:47Z",
        "updatedAt" : "2021-03-29T12:08:47Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "26f07e9ee3dcf4c1f50fc73e6575d2adb7b6a146",
    "line" : 39,
    "diffHunk" : "@@ -1,1 +2019,2023 @@</tr>\n<tr>\n  <td><code>spark.rpc.io.connectionTimeout</code></td>\n  <td>value of <code>spark.network.timeout</code></td>\n  <td>"
  },
  {
    "id" : "d44a2d5d-f4bd-43fc-bb69-39224043e653",
    "prId" : 31618,
    "prUrl" : "https://github.com/apache/spark/pull/31618#pullrequestreview-601207826",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ad10e182-df49-4ded-b2e9-2c6aeb9f7126",
        "parentId" : null,
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "sorry for coming in late, was out last week, we may want to reference what other codecs can be used here.   @dongjoon-hyun thoughts?",
        "createdAt" : "2021-03-01T17:18:01Z",
        "updatedAt" : "2021-03-01T17:18:01Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "f2665c6d-5457-472e-94d5-c73e9bd88df0",
        "parentId" : "ad10e182-df49-4ded-b2e9-2c6aeb9f7126",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Thank you for review, @tgravescs . Sure, I'll make a documentation follow-up.",
        "createdAt" : "2021-03-01T21:56:40Z",
        "updatedAt" : "2021-03-01T21:56:40Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "b84158af-373b-4d60-ba5f-a3c4e4f599da",
        "parentId" : "ad10e182-df49-4ded-b2e9-2c6aeb9f7126",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Here, I made a PR.\r\n- https://github.com/apache/spark/pull/31695",
        "createdAt" : "2021-03-01T22:02:52Z",
        "updatedAt" : "2021-03-01T22:02:52Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "0e88652b99b833551d3e940a24d9d2c217fe4f51",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +1043,1047 @@  <td>zstd</td>\n  <td>\n    The codec to compress logged events.\n  </td>\n  <td>3.0.0</td>"
  },
  {
    "id" : "c3556901-198a-4bf6-b626-f32e3d122d5e",
    "prId" : 31162,
    "prUrl" : "https://github.com/apache/spark/pull/31162#pullrequestreview-567439514",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "32226d14-cbf5-4bab-8bdf-83960f790f53",
        "parentId" : null,
        "authorId" : "363b0980-061e-4268-a6b1-8d344cf0887e",
        "body" : "Is this the correct release to put here?",
        "createdAt" : "2021-01-13T17:07:39Z",
        "updatedAt" : "2021-02-13T00:30:53Z",
        "lastEditedBy" : "363b0980-061e-4268-a6b1-8d344cf0887e",
        "tags" : [
        ]
      }
    ],
    "commit" : "43f30c68017b349b2b776314714c93d05cb12a01",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +2845,2849 @@    wait until it decides that it can't launch the SparkR daemon?\n  </td>\n  <td>3.2.0</td>\n</tr>\n<tr>"
  }
]