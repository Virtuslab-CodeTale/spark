[
  {
    "id" : "e9071061-a932-459e-8e10-88afa65cc536",
    "prId" : 27989,
    "prUrl" : "https://github.com/apache/spark/pull/27989#pullrequestreview-379368252",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9b8c5fef-bfdc-4744-bf5f-8c7b7f3d5ff2",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-27294, commit ID: 2f558094257c38d26650049f2ac93be6d65d6d85#diff-7df71bd47f5a3428ebdb05ced3c31f49",
        "createdAt" : "2020-03-23T12:08:14Z",
        "updatedAt" : "2020-03-23T12:08:14Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "738b704b36156e5eb606838bbb9184021602f0ec",
    "line" : 87,
    "diffHunk" : "@@ -1,1 +952,956 @@      to the Kafka cluster. For further details please see Kafka documentation. Only used to obtain delegation token.\n    </td>\n    <td>3.0.0</td>\n  </tr>\n  <tr>"
  },
  {
    "id" : "b5478ebf-7186-4a97-aacc-2cbbd82d163f",
    "prId" : 27989,
    "prUrl" : "https://github.com/apache/spark/pull/27989#pullrequestreview-379368380",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "65cb81b8-0b8f-4ec2-b90c-f53a9db70ea8",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-27294, commit ID: 2f558094257c38d26650049f2ac93be6d65d6d85#diff-7df71bd47f5a3428ebdb05ced3c31f49",
        "createdAt" : "2020-03-23T12:08:26Z",
        "updatedAt" : "2020-03-23T12:08:27Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "738b704b36156e5eb606838bbb9184021602f0ec",
    "line" : 95,
    "diffHunk" : "@@ -1,1 +963,967 @@      Kafka's secure and unsecure listeners are bound to different ports. When both used the secure listener port has to be part of the regular expression.\n    </td>\n    <td>3.0.0</td>\n  </tr>\n  <tr>"
  },
  {
    "id" : "9aa6ed1f-6218-4a37-871d-bfd48ea3c2c9",
    "prId" : 27989,
    "prUrl" : "https://github.com/apache/spark/pull/27989#pullrequestreview-379368494",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5ba60fde-9913-451e-bc1e-7a83fa116094",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-27294, commit ID: 2f558094257c38d26650049f2ac93be6d65d6d85#diff-7df71bd47f5a3428ebdb05ced3c31f49",
        "createdAt" : "2020-03-23T12:08:36Z",
        "updatedAt" : "2020-03-23T12:08:36Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "738b704b36156e5eb606838bbb9184021602f0ec",
    "line" : 103,
    "diffHunk" : "@@ -1,1 +973,977 @@      and can be overridden by setting <code>kafka.security.protocol</code> on the source or sink.\n    </td>\n    <td>3.0.0</td>\n  </tr>\n  <tr>"
  },
  {
    "id" : "a6b965fa-21a6-4d07-aa92-2d107c5808fb",
    "prId" : 27989,
    "prUrl" : "https://github.com/apache/spark/pull/27989#pullrequestreview-379368617",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cf6b99bf-036f-444d-80b2-67a44cd3bd5d",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-27294, commit ID: 2f558094257c38d26650049f2ac93be6d65d6d85#diff-7df71bd47f5a3428ebdb05ced3c31f49",
        "createdAt" : "2020-03-23T12:08:47Z",
        "updatedAt" : "2020-03-23T12:08:47Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "738b704b36156e5eb606838bbb9184021602f0ec",
    "line" : 111,
    "diffHunk" : "@@ -1,1 +982,986 @@      For further details please see Kafka documentation. Only used to obtain delegation token.\n    </td>\n    <td>3.0.0</td>\n  </tr>\n  <tr>"
  },
  {
    "id" : "54fb2faf-a5c7-4bdb-8d85-1bbbb60a4da0",
    "prId" : 27989,
    "prUrl" : "https://github.com/apache/spark/pull/27989#pullrequestreview-379368730",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bfc61afc-dc30-4d47-a948-250d435b7eaa",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-27294, commit ID: 2f558094257c38d26650049f2ac93be6d65d6d85#diff-7df71bd47f5a3428ebdb05ced3c31f49",
        "createdAt" : "2020-03-23T12:08:57Z",
        "updatedAt" : "2020-03-23T12:08:57Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "738b704b36156e5eb606838bbb9184021602f0ec",
    "line" : 119,
    "diffHunk" : "@@ -1,1 +990,994 @@      The location of the trust store file. For further details please see Kafka documentation. Only used to obtain delegation token.\n    </td>\n    <td>3.0.0</td>\n  </tr>\n  <tr>"
  },
  {
    "id" : "e96b0f88-b781-437a-be49-7f4d18f5921d",
    "prId" : 27989,
    "prUrl" : "https://github.com/apache/spark/pull/27989#pullrequestreview-379368850",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0d04dc40-2c65-45bf-8391-849f2ecc92e0",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-27294, commit ID: 2f558094257c38d26650049f2ac93be6d65d6d85#diff-7df71bd47f5a3428ebdb05ced3c31f49",
        "createdAt" : "2020-03-23T12:09:08Z",
        "updatedAt" : "2020-03-23T12:09:08Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "738b704b36156e5eb606838bbb9184021602f0ec",
    "line" : 127,
    "diffHunk" : "@@ -1,1 +999,1003 @@      For further details please see Kafka documentation. Only used to obtain delegation token.\n    </td>\n    <td>3.0.0</td>\n  </tr>\n  <tr>"
  },
  {
    "id" : "a4e8468d-f850-43c3-bbc6-72c509d2482a",
    "prId" : 27989,
    "prUrl" : "https://github.com/apache/spark/pull/27989#pullrequestreview-379368952",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8919b37d-8ea3-44db-b774-6bbea00d361d",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-27294, commit ID: 2f558094257c38d26650049f2ac93be6d65d6d85#diff-7df71bd47f5a3428ebdb05ced3c31f49",
        "createdAt" : "2020-03-23T12:09:17Z",
        "updatedAt" : "2020-03-23T12:09:18Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "738b704b36156e5eb606838bbb9184021602f0ec",
    "line" : 135,
    "diffHunk" : "@@ -1,1 +1008,1012 @@      For further details please see Kafka documentation. Only used to obtain delegation token.\n    </td>\n    <td>3.0.0</td>\n  </tr>\n  <tr>"
  },
  {
    "id" : "b81f1456-aeff-4092-a946-0011fb630050",
    "prId" : 27989,
    "prUrl" : "https://github.com/apache/spark/pull/27989#pullrequestreview-379369039",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "97156564-57f1-41d0-a508-b47656df13ac",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-27294, commit ID: 2f558094257c38d26650049f2ac93be6d65d6d85#diff-7df71bd47f5a3428ebdb05ced3c31f49",
        "createdAt" : "2020-03-23T12:09:25Z",
        "updatedAt" : "2020-03-23T12:09:26Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "738b704b36156e5eb606838bbb9184021602f0ec",
    "line" : 143,
    "diffHunk" : "@@ -1,1 +1017,1021 @@      For further details please see Kafka documentation. Only used to obtain delegation token.\n    </td>\n    <td>3.0.0</td>\n  </tr>\n  <tr>"
  },
  {
    "id" : "8d4e63c7-fe77-4fac-b0ce-85adb4b8ae1e",
    "prId" : 27989,
    "prUrl" : "https://github.com/apache/spark/pull/27989#pullrequestreview-379369143",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ccb65bfc-4d66-45ca-9d12-26b70f6f3ad0",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-27294, commit ID: 2f558094257c38d26650049f2ac93be6d65d6d85#diff-7df71bd47f5a3428ebdb05ced3c31f49",
        "createdAt" : "2020-03-23T12:09:34Z",
        "updatedAt" : "2020-03-23T12:09:34Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "738b704b36156e5eb606838bbb9184021602f0ec",
    "line" : 151,
    "diffHunk" : "@@ -1,1 +1026,1030 @@      For further details please see Kafka documentation. Only used to obtain delegation token.\n    </td>\n    <td>3.0.0</td>\n  </tr>\n  <tr>"
  },
  {
    "id" : "8f9e8aa2-eb40-4a9d-b4e9-756c8bb5853d",
    "prId" : 27989,
    "prUrl" : "https://github.com/apache/spark/pull/27989#pullrequestreview-379369238",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "36b8657a-018a-4435-b835-639895a7c0fa",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-27294, commit ID: 2f558094257c38d26650049f2ac93be6d65d6d85#diff-7df71bd47f5a3428ebdb05ced3c31f49",
        "createdAt" : "2020-03-23T12:09:41Z",
        "updatedAt" : "2020-03-23T12:09:41Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "738b704b36156e5eb606838bbb9184021602f0ec",
    "line" : 159,
    "diffHunk" : "@@ -1,1 +1035,1039 @@      For further details please see Kafka documentation (<code>sasl.mechanism</code>). Only used to authenticate against Kafka broker with delegation token.\n    </td>\n    <td>3.0.0</td>\n  </tr>\n</table>"
  },
  {
    "id" : "8a90b838-42f3-4ccc-ba47-b9d843446a4b",
    "prId" : 27989,
    "prUrl" : "https://github.com/apache/spark/pull/27989#pullrequestreview-381119409",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e421408d-2ef2-4b1a-88c8-67b013c6cf43",
        "parentId" : null,
        "authorId" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "body" : "Hmm, here I can't see the `.version(...)` in the code area but when I've checked out the PR it's there. The PR is fine just wanted to mention this...",
        "createdAt" : "2020-03-25T12:12:59Z",
        "updatedAt" : "2020-03-25T12:30:20Z",
        "lastEditedBy" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "tags" : [
        ]
      },
      {
        "id" : "452b1042-36be-40bb-a2ba-adc00b3caefb",
        "parentId" : "e421408d-2ef2-4b1a-88c8-67b013c6cf43",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "That's because GitHub folded some code.",
        "createdAt" : "2020-03-25T12:56:22Z",
        "updatedAt" : "2020-03-25T12:56:22Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "738b704b36156e5eb606838bbb9184021602f0ec",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +537,541 @@  <td>The minimum amount of time a consumer may sit idle in the pool before it is eligible for eviction by the evictor.</td>\n  <td>5m (5 minutes)</td>\n  <td>3.0.0</td>\n</tr>\n<tr>"
  },
  {
    "id" : "12800aee-4550-4a95-afd8-3090f8d253a3",
    "prId" : 27989,
    "prUrl" : "https://github.com/apache/spark/pull/27989#pullrequestreview-381120351",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "54b4e83b-f2bc-47f9-a696-41cea27a2bf6",
        "parentId" : null,
        "authorId" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "body" : "Same here.",
        "createdAt" : "2020-03-25T12:13:59Z",
        "updatedAt" : "2020-03-25T12:30:20Z",
        "lastEditedBy" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "tags" : [
        ]
      },
      {
        "id" : "2f9660a5-cd3e-455e-b71d-6d0980a15dfb",
        "parentId" : "54b4e83b-f2bc-47f9-a696-41cea27a2bf6",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "The same reason.",
        "createdAt" : "2020-03-25T12:57:38Z",
        "updatedAt" : "2020-03-25T12:57:38Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "738b704b36156e5eb606838bbb9184021602f0ec",
    "line" : 41,
    "diffHunk" : "@@ -1,1 +578,582 @@<tr><th>Property Name</th><th>Default</th><th>Meaning</th><th>Since Version</th></tr>\n<tr>\n  <td>spark.kafka.consumer.fetchedData.cache.timeout</td>\n  <td>The minimum amount of time a fetched data may sit idle in the pool before it is eligible for eviction by the evictor.</td>\n  <td>5m (5 minutes)</td>"
  },
  {
    "id" : "84118534-284c-4760-9498-0c7db05d0574",
    "prId" : 27146,
    "prUrl" : "https://github.com/apache/spark/pull/27146#pullrequestreview-341620330",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7e42e793-cba4-421f-b2a0-3ada7c153988",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Just a question. Does `The following information` cover all configurations used in `paramsToSeq`? It seems that `setAuthenticationConfigIfNeeded` injects more config in addition to the following twos.",
        "createdAt" : "2020-01-10T08:29:16Z",
        "updatedAt" : "2020-01-13T02:16:09Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "15d781f9-0530-4a38-a0ca-8ed147839e64",
        "parentId" : "7e42e793-cba4-421f-b2a0-3ada7c153988",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Technically you're right, the cache key will contain the configuration including auth config Spark will inject. I've not mentioned it as we start to explain the internal which end users may be OK with only knowing abstracted info.\r\n\r\nAnd same configuration goes to same addition, except a case where delegation token is renewed. Not 100% sure about details, @gaborgsomogyi could you help me confirming this?",
        "createdAt" : "2020-01-10T08:39:43Z",
        "updatedAt" : "2020-01-13T02:16:09Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "1fe7d36e-ded6-4d03-9150-9c99a6a906a5",
        "parentId" : "7e42e793-cba4-421f-b2a0-3ada7c153988",
        "authorId" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "body" : "@dongjoon-hyun really good point! If delegation token used then time to time new producer must be created and the old must be evicted otherwise the query will fail. There are multiple ways to reach that (not yet analyzed how it's done in the latest change made by @HeartSaVioR but I'm on it):\r\n* Either the cache key contains authentication information (dynamic jaas config). This way the new producer creation and old eviction would be automatic. Without super deep consideration that's my suggested way.\r\n* Or the cache key NOT contains authentication information (dynamic jaas config). This way additional logic must be added to handle this scenario. At the first place I have the feeling it would just add complexity increase and would make this part of code brittle.\r\n\r\nAs I understand from @HeartSaVioR comment the first approach is implemented at the moment. If that so then I'm fine with that but I would mention 2 things here:\r\n* The key may contain authentication information\r\n* There could be situations where more than one producer is instantiated. This is important because producers are consuming significant amount of memory as @zsxwing pointed out.",
        "createdAt" : "2020-01-10T10:11:25Z",
        "updatedAt" : "2020-01-13T02:16:09Z",
        "lastEditedBy" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "tags" : [
        ]
      },
      {
        "id" : "15dd21a2-0089-4453-9bcd-f8ab5d109e73",
        "parentId" : "7e42e793-cba4-421f-b2a0-3ada7c153988",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "I think we take the first approach - I just followed the way we did it before.\r\n\r\nBack to the topic, would we like to add the details on the guide doc? I'll address it if we would like to let end users know about. Otherwise we could leave it as it is.",
        "createdAt" : "2020-01-10T12:10:40Z",
        "updatedAt" : "2020-01-13T02:16:09Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "acbfc78e-7cb0-4d83-97a3-c65309d74fe2",
        "parentId" : "7e42e793-cba4-421f-b2a0-3ada7c153988",
        "authorId" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "body" : "I agree the first bullet point is questionable since it's a deep detail but the second is an important memory sizing information. Let's hear what @dongjoon-hyun thinks.",
        "createdAt" : "2020-01-10T13:50:39Z",
        "updatedAt" : "2020-01-13T02:16:09Z",
        "lastEditedBy" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "tags" : [
        ]
      },
      {
        "id" : "f16a21ec-2a50-4361-ba4e-36a06c1f5b68",
        "parentId" : "7e42e793-cba4-421f-b2a0-3ada7c153988",
        "authorId" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "body" : "I think this would be more useful if you want to explain how a user can force separate producers to be used. Otherwise it's an internal detail that doesn't really affect users.",
        "createdAt" : "2020-01-10T18:03:05Z",
        "updatedAt" : "2020-01-13T02:16:09Z",
        "lastEditedBy" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "tags" : [
        ]
      },
      {
        "id" : "665d095f-09d1-43ba-b27b-06b6372d16e0",
        "parentId" : "7e42e793-cba4-421f-b2a0-3ada7c153988",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "+1 for @vanzin 's advice.",
        "createdAt" : "2020-01-10T19:05:21Z",
        "updatedAt" : "2020-01-13T02:16:09Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "cba12eb8-ed5a-4a0d-82c7-4f2e0940410e",
        "parentId" : "7e42e793-cba4-421f-b2a0-3ada7c153988",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Hmm... would we like to guide how to tweak it, or would like to just hide it? The reason I explain this is that end users may encounter an issue on producer pool and would like to debug a bit, but if it feels us to be too internal, sounds OK to hide it as well.",
        "createdAt" : "2020-01-11T05:07:50Z",
        "updatedAt" : "2020-01-13T02:16:09Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "62b2ed3e-6f80-4b62-a11f-826026c7c5a1",
        "parentId" : "7e42e793-cba4-421f-b2a0-3ada7c153988",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Just added some explanation since it makes sense to let end users know about it to debug.",
        "createdAt" : "2020-01-13T02:17:36Z",
        "updatedAt" : "2020-01-13T02:17:36Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "d3c3094973efff05bb7a3a7038c55903b8f8d476",
    "line" : 26,
    "diffHunk" : "@@ -1,1 +807,811 @@Given Kafka producer instance is designed to be thread-safe, Spark initializes a Kafka producer instance and co-use across tasks for same caching key.\n\nThe caching key is built up from the following information:\n\n* Kafka producer configuration"
  },
  {
    "id" : "1cc4caa5-9aca-498c-950a-b83a0d97b517",
    "prId" : 25911,
    "prUrl" : "https://github.com/apache/spark/pull/25911#pullrequestreview-292216990",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "27d08f7d-b609-4e5d-a18e-45bd8815583a",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Just fixed a nit which IDEA warns about.",
        "createdAt" : "2019-09-24T07:15:27Z",
        "updatedAt" : "2019-09-24T07:34:08Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "70454134af555d934cd78ee48acb54736cf24a77",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +368,372 @@  \"\"\" {\"topicA\":{\"0\": 1000, \"1\": 1000}, \"topicB\": {\"0\": 2000, \"1\": 2000}} \"\"\" or \"\"\" {\"topicA\":{\"-1\": 1000}, \"topicB\": {\"-1\": 2000}} \"\"\"\n  </td>\n  <td>none (the value of <code>startingOffsets</code> will apply)</td>\n  <td>streaming and batch</td>\n  <td>The start point of timestamp when a query is started, a json string specifying a starting timestamp for"
  },
  {
    "id" : "f7bf3d91-36e7-49b5-9b8e-8aa6d4661904",
    "prId" : 25911,
    "prUrl" : "https://github.com/apache/spark/pull/25911#pullrequestreview-292216990",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1eeec6ec-439e-4e67-82dd-377ee5b819af",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "This is actually same meaning as NOTE 2 but in case of misunderstanding I just mentioned this again here. Please let me know if it is just OK to be latest.",
        "createdAt" : "2019-09-24T07:17:56Z",
        "updatedAt" : "2019-09-24T07:34:08Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "70454134af555d934cd78ee48acb54736cf24a77",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +405,409 @@  \"\"\" {\"topicA\":{\"0\": 1000, \"1\": 1000}, \"topicB\": {\"0\": 2000, \"1\": 2000}} \"\"\" or \"\"\" {\"topicA\":{\"-1\": 1000}, \"topicB\": {\"-1\": 2000} \"\"\"\n  </td>\n  <td>the value of <code>endingOffsets</code> will apply</td>\n  <td>batch query</td>\n  <td>The end point when a batch query is ended, a json string specifying an ending timestamp for each TopicPartition."
  },
  {
    "id" : "cb9859bd-90ec-49cb-b32d-784dc62fd62f",
    "prId" : 25911,
    "prUrl" : "https://github.com/apache/spark/pull/25911#pullrequestreview-292216990",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "082971f3-b26e-4a4b-93f3-8e2ca3f46244",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Just fixed a nit.",
        "createdAt" : "2019-09-24T07:18:06Z",
        "updatedAt" : "2019-09-24T07:34:08Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "70454134af555d934cd78ee48acb54736cf24a77",
    "line" : 31,
    "diffHunk" : "@@ -1,1 +407,411 @@  <td>the value of <code>endingOffsets</code> will apply</td>\n  <td>batch query</td>\n  <td>The end point when a batch query is ended, a json string specifying an ending timestamp for each TopicPartition.\n  The returned offset for each partition is the earliest offset whose timestamp is greater than or equal to\n  the given timestamp in the corresponding partition. If the matched offset doesn't exist, the offset will"
  },
  {
    "id" : "837e6517-f4c5-439f-acee-7c0a7b3f44ad",
    "prId" : 25219,
    "prUrl" : "https://github.com/apache/spark/pull/25219#pullrequestreview-264552428",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "066550f6-7739-42c2-8d5e-994dddae63be",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "`</td>` is missed. I'll fix that during merging.",
        "createdAt" : "2019-07-21T20:06:53Z",
        "updatedAt" : "2019-07-21T20:06:53Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "7c0e448a356a379f362b2bdb7c8b22a5ffcb9a46",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +397,401 @@  By default, Spark has a 1-1 mapping of Kafka TopicPartitions to Spark partitions consuming from Kafka.\n  If you set the minPartitions option to a value greater than your Kafka TopicPartitions,\n  Spark will divvy up large Kafka partitions to smaller pieces.\n</tr>\n<tr>"
  },
  {
    "id" : "4e8079d0-1a24-45ae-b8ef-88f1bd508a9a",
    "prId" : 24613,
    "prUrl" : "https://github.com/apache/spark/pull/24613#pullrequestreview-238573713",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bbef6a38-f796-4d36-948e-b5200c732e69",
        "parentId" : null,
        "authorId" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "body" : "Why not `<td>true or false</td>`?",
        "createdAt" : "2019-05-16T13:59:11Z",
        "updatedAt" : "2019-05-16T14:30:48Z",
        "lastEditedBy" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "tags" : [
        ]
      },
      {
        "id" : "9142cffc-205a-4463-8e55-99fec71b0bd2",
        "parentId" : "bbef6a38-f796-4d36-948e-b5200c732e69",
        "authorId" : "63281e91-e961-4e6e-b0bb-6855587910d1",
        "body" : "I thought since this is passed as a string in the option call it is string can change.",
        "createdAt" : "2019-05-16T18:46:09Z",
        "updatedAt" : "2019-05-16T18:46:09Z",
        "lastEditedBy" : "63281e91-e961-4e6e-b0bb-6855587910d1",
        "tags" : [
        ]
      }
    ],
    "commit" : "cf5bd3edd9473131c1159b658679e46e0350745a",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +417,421 @@<tr>\n  <td>setCommitOffsetsOnCheckpoints</td>\n  <td>string</td>\n  <td>false</td>\n  <td>streaming</td>"
  },
  {
    "id" : "de779d32-c71c-4a77-9825-2b489fde9695",
    "prId" : 24613,
    "prUrl" : "https://github.com/apache/spark/pull/24613#pullrequestreview-238573794",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e55aa8c3-1b11-46e3-a0e2-43d04f5be36b",
        "parentId" : null,
        "authorId" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "body" : "Maybe `commitOffsetsOnCheckpoints` since this is not a function.",
        "createdAt" : "2019-05-16T14:00:06Z",
        "updatedAt" : "2019-05-16T14:30:48Z",
        "lastEditedBy" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "tags" : [
        ]
      },
      {
        "id" : "12db3dfb-b586-46fb-8899-dc0ce735fca8",
        "parentId" : "e55aa8c3-1b11-46e3-a0e2-43d04f5be36b",
        "authorId" : "63281e91-e961-4e6e-b0bb-6855587910d1",
        "body" : "ok just used the Flink one.",
        "createdAt" : "2019-05-16T18:46:18Z",
        "updatedAt" : "2019-05-21T10:55:15Z",
        "lastEditedBy" : "63281e91-e961-4e6e-b0bb-6855587910d1",
        "tags" : [
        ]
      }
    ],
    "commit" : "cf5bd3edd9473131c1159b658679e46e0350745a",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +416,420 @@</tr>\n<tr>\n  <td>setCommitOffsetsOnCheckpoints</td>\n  <td>string</td>\n  <td>false</td>"
  },
  {
    "id" : "3f3fdc39-57b2-4bad-9cbb-09a5af5b7a9d",
    "prId" : 24613,
    "prUrl" : "https://github.com/apache/spark/pull/24613#pullrequestreview-238573905",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a6e0c24e-58e0-4d35-a72f-e677524cf1c0",
        "parentId" : null,
        "authorId" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "body" : "wrt? :)",
        "createdAt" : "2019-05-16T14:02:43Z",
        "updatedAt" : "2019-05-16T14:30:48Z",
        "lastEditedBy" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "tags" : [
        ]
      },
      {
        "id" : "3b927c65-c749-43d5-808e-db989b90313a",
        "parentId" : "a6e0c24e-58e0-4d35-a72f-e677524cf1c0",
        "authorId" : "63281e91-e961-4e6e-b0bb-6855587910d1",
        "body" : "will fix :)",
        "createdAt" : "2019-05-16T18:46:29Z",
        "updatedAt" : "2019-05-16T18:46:29Z",
        "lastEditedBy" : "63281e91-e961-4e6e-b0bb-6855587910d1",
        "tags" : [
        ]
      }
    ],
    "commit" : "cf5bd3edd9473131c1159b658679e46e0350745a",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +422,426 @@  <td>Whetherer to commit back the offsets read by the driver consumer per batch.\n  Due to the fact that each batch will get its offsets committed when a new is constructed there might be a lag\n  wrt last batch. \n  </td>\n</tr>"
  },
  {
    "id" : "1ddf7e0e-12e6-4c52-8460-5572d1921933",
    "prId" : 24613,
    "prUrl" : "https://github.com/apache/spark/pull/24613#pullrequestreview-238410684",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "445186ea-169d-4787-82fb-78aa4ca485ce",
        "parentId" : null,
        "authorId" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "body" : "Right before this sentence I would mentioned similar things like Flink:\r\n```\r\nNote that the Flink Kafka Consumer does not rely on the committed offsets for fault tolerance guarantees. The committed offsets are only a means to expose the consumer’s progress for monitoring purposes.\r\n```\r\n",
        "createdAt" : "2019-05-16T14:09:17Z",
        "updatedAt" : "2019-05-16T14:30:48Z",
        "lastEditedBy" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "tags" : [
        ]
      }
    ],
    "commit" : "cf5bd3edd9473131c1159b658679e46e0350745a",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +421,425 @@  <td>streaming</td>\n  <td>Whetherer to commit back the offsets read by the driver consumer per batch.\n  Due to the fact that each batch will get its offsets committed when a new is constructed there might be a lag\n  wrt last batch. \n  </td>"
  },
  {
    "id" : "bd8d0aac-53ae-4fab-9052-d6b801a63f25",
    "prId" : 24613,
    "prUrl" : "https://github.com/apache/spark/pull/24613#pullrequestreview-238410684",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9d525d8a-c358-461d-8dfb-88557d5d86bb",
        "parentId" : null,
        "authorId" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "body" : "Later in the file this has to be re-phrased to make it true:\r\n```\r\nenable.auto.commit: Kafka source doesn’t commit any offset.\r\n```\r\n",
        "createdAt" : "2019-05-16T14:10:50Z",
        "updatedAt" : "2019-05-16T14:30:48Z",
        "lastEditedBy" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "tags" : [
        ]
      }
    ],
    "commit" : "cf5bd3edd9473131c1159b658679e46e0350745a",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +427,431 @@</table>\n\n## Writing Data to Kafka\n\nHere, we describe the support for writing Streaming Queries and Batch Queries to Apache Kafka. Take note that"
  },
  {
    "id" : "c3beb104-d60f-4849-bd00-a5fa01ad5ee6",
    "prId" : 24305,
    "prUrl" : "https://github.com/apache/spark/pull/24305#pullrequestreview-233463518",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3870237a-28e8-4ff6-ad8b-48e579323f6b",
        "parentId" : null,
        "authorId" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "body" : "Could you add a clarification somewhere that if a bootstrap server doesn't match any cluster definition, then Spark will not try to use authentication for it (instead of e.g. throwing an error)?",
        "createdAt" : "2019-05-02T20:15:53Z",
        "updatedAt" : "2019-05-06T10:37:16Z",
        "lastEditedBy" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "tags" : [
        ]
      },
      {
        "id" : "acd25763-722b-4875-a2e1-8859d03bb818",
        "parentId" : "3870237a-28e8-4ff6-ad8b-48e579323f6b",
        "authorId" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "body" : "Clarification added.",
        "createdAt" : "2019-05-03T13:17:49Z",
        "updatedAt" : "2019-05-06T10:37:16Z",
        "lastEditedBy" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "tags" : [
        ]
      }
    ],
    "commit" : "e5bcc181d5b748490efdbf83625a47fb36892b82",
    "line" : 43,
    "diffHunk" : "@@ -1,1 +716,720 @@#### Configuration\n\nDelegation tokens can be obtained from multiple clusters and <code>${cluster}</code> is an arbitrary unique identifier which helps to group different configurations.\n\n<table class=\"table\">"
  }
]