[
  {
    "id" : "6e35e257-98c7-4520-9901-a6618b35559f",
    "prId" : 31936,
    "prUrl" : "https://github.com/apache/spark/pull/31936#pullrequestreview-621133390",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c62d88e2-9202-4f19-bd42-bf4332f9ee1b",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Could you add some description about the limitation with old Hadoop versions (like 2.7.x)? Here or at Section `Running multiple versions of the Spark Shuffle Service`?",
        "createdAt" : "2021-03-23T05:08:08Z",
        "updatedAt" : "2021-03-29T17:15:56Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "6874701c-39ac-4fea-8b26-dee7e7df5cf3",
        "parentId" : "c62d88e2-9202-4f19-bd42-bf4332f9ee1b",
        "authorId" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "body" : "For this section, everything will work as expected on Hadoop 2.7.x. The \"Running multiple versions\" section won't work on 2.7, but I already called out the supported YARN versions there. Can you let me know if there's anything else you think I should call out?",
        "createdAt" : "2021-03-23T15:46:51Z",
        "updatedAt" : "2021-03-29T17:15:56Z",
        "lastEditedBy" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "tags" : [
        ]
      },
      {
        "id" : "ddabb930-61dc-4044-9706-9665e33ef3e1",
        "parentId" : "c62d88e2-9202-4f19-bd42-bf4332f9ee1b",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "I'm worrying about the situation some users try to use `Apache Spark distribution (with Hadoop 2.7)` at YARN 2.9+ cluster. Does it work?",
        "createdAt" : "2021-03-24T05:26:38Z",
        "updatedAt" : "2021-03-29T17:15:56Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "ec16436e-6bca-483f-bfb7-543715ecb1c1",
        "parentId" : "c62d88e2-9202-4f19-bd42-bf4332f9ee1b",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "it looks like the name referenced by the node manager works with the Hadoop 2.9+ custom class loader, but I assume with Hadoop 2.7 it requires the spark_shuffle name ?  hence the spark.shuffle.service.name won't work unless you have recompiled the code and manually changed it.\r\nPerhaps we just need to be more explicit in the config spark.shuffle.service.name that either references  the section running multiple versions of the Spark Shuffle Service or explicitly states supported in YARN 2.9+.     I assume this config with metrics doesn't matter as far as Hadoop version.\r\nAlso did we explicitly test with Hadoop 2.7 and the case @dongjoon-hyun brings up?",
        "createdAt" : "2021-03-24T13:29:09Z",
        "updatedAt" : "2021-03-29T17:15:56Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "0eb834b8-ed85-4e7d-b4dc-bac9987f850e",
        "parentId" : "c62d88e2-9202-4f19-bd42-bf4332f9ee1b",
        "authorId" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "body" : "> it looks like the name referenced by the node manager works with the Hadoop 2.9+ custom class loader, but I assume with Hadoop 2.7 it requires the spark_shuffle name ? hence the spark.shuffle.service.name won't work unless you have recompiled the code and manually changed it.\r\n\r\nNo, this is not correct. YARN ignores the hard-coded name on _all_ versions of YARN. Take a look at `AuxServices` on the 2.7.0 branch: \r\nhttps://github.com/apache/hadoop/blob/f95b390df2ca7d599f0ad82cf6e8d980469e7abb/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/AuxServices.java#L129-L136\r\n\r\n`spark.shuffle.service.name` works fine on Hadoop 2.7, it is only the isolated classloader that won't work on older versions.\r\n\r\n> I'm worrying about the situation some users try to use `Apache Spark distribution (with Hadoop 2.7)` at YARN 2.9+ cluster. Does it work?\r\n\r\nI don't quite understand the concern here. Does my explanation above address your question? We haven't changed any of the interfaces used to interact with YARN, there should be no binary compatibility issues or anything of that sort. I can test whichever combination of `Spark Version + Hadoop Version Distribution` running on top of `Hadoop Version YARN` you like, but I am failing to see where the concern is / what you'd like me to look for.",
        "createdAt" : "2021-03-24T17:05:03Z",
        "updatedAt" : "2021-03-29T17:15:56Z",
        "lastEditedBy" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "tags" : [
        ]
      },
      {
        "id" : "361500bf-40d5-4798-b763-d079e6a0fd1b",
        "parentId" : "c62d88e2-9202-4f19-bd42-bf4332f9ee1b",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "great, I'm glad it works with 2.7 as well, thanks for clarifying.  Yeah the concern was if it didn't work in 2.7 so I think you answered that.",
        "createdAt" : "2021-03-25T13:48:54Z",
        "updatedAt" : "2021-03-29T17:15:56Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "ef880526e05b6143c31d98829d22d25cea659401",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +779,783 @@  <td>\n    The namespace to use when emitting shuffle service metrics into Hadoop metrics2 system of the\n    NodeManager.\n  </td>\n</tr>"
  },
  {
    "id" : "c8be3966-4181-4aec-9744-195e052c8ed6",
    "prId" : 31936,
    "prUrl" : "https://github.com/apache/spark/pull/31936#pullrequestreview-620002732",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "78922857-358c-4498-9104-e693afd6f953",
        "parentId" : null,
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "I think we should be more explicit here and say requires Yarn 2.9+",
        "createdAt" : "2021-03-24T13:29:06Z",
        "updatedAt" : "2021-03-29T17:15:56Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "1a2deaac-ca00-4b95-9c77-c7bff2249d8f",
        "parentId" : "78922857-358c-4498-9104-e693afd6f953",
        "authorId" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "body" : "Makes sense, called this out as the first line of the section.",
        "createdAt" : "2021-03-24T17:13:04Z",
        "updatedAt" : "2021-03-29T17:15:56Z",
        "lastEditedBy" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "tags" : [
        ]
      }
    ],
    "commit" : "ef880526e05b6143c31d98829d22d25cea659401",
    "line" : 42,
    "diffHunk" : "@@ -1,1 +853,857 @@with a mixed workload of applications running multiple Spark versions, since a given version of\nthe shuffle service is not always compatible with other versions of Spark. YARN versions since 2.9.0\nsupport the ability to run shuffle services within an isolated classloader\n(see [YARN-4577](https://issues.apache.org/jira/browse/YARN-4577)), meaning multiple Spark versions\ncan coexist within a single NodeManager. The"
  },
  {
    "id" : "87bfe2bc-ce48-42e7-be9e-e82aee457993",
    "prId" : 30298,
    "prUrl" : "https://github.com/apache/spark/pull/30298#pullrequestreview-526363121",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "58044eaf-0839-4095-89e2-127e5b981d7e",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "we need to change `<` ->`&lt;` and `>` -> `&gt;`?",
        "createdAt" : "2020-11-09T14:13:24Z",
        "updatedAt" : "2020-11-09T14:13:24Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "285bf93e-9802-4f91-a250-cab99cab75df",
        "parentId" : "58044eaf-0839-4095-89e2-127e5b981d7e",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "yes. otherwiseï¼Œthey become tags inside codeblock",
        "createdAt" : "2020-11-09T15:22:06Z",
        "updatedAt" : "2020-11-09T15:22:07Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "078bae1a8bb879d7c37ecb44209eadef2642ff37",
    "line" : 57,
    "diffHunk" : "@@ -1,1 +624,628 @@For example, suppose you would like to point log url link to Job History Server directly instead of let NodeManager http server redirects it, you can configure `spark.history.custom.executor.log.url` as below:\n\n<code>&#123;&#123;HTTP_SCHEME&#125;&#125;&lt;JHS_HOST&gt;:&lt;JHS_PORT&gt;/jobhistory/logs/&#123;&#123;NM_HOST&#125;&#125;:&#123;&#123;NM_PORT&#125;&#125;/&#123;&#123;CONTAINER_ID&#125;&#125;/&#123;&#123;CONTAINER_ID&#125;&#125;/&#123;&#123;USER&#125;&#125;/&#123;&#123;FILE_NAME&#125;&#125;?start=-4096</code>\n\nNOTE: you need to replace `<JHS_POST>` and `<JHS_PORT>` with actual value."
  },
  {
    "id" : "e517d367-0c81-42d6-88f1-8a885cc4081a",
    "prId" : 30204,
    "prUrl" : "https://github.com/apache/spark/pull/30204#pullrequestreview-530301133",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cd2f4cdc-5ef7-43eb-a834-a45b9dc74e11",
        "parentId" : null,
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "I wish yarn supported allocation to multiple queues (so that DL workloads could use a queue with gpu's, and 'regular' queue for non-DL stages within the same application) - and not tie it at the application level.\r\n\r\nNode labels could be used for this - iirc we dont use that for resource profile, but only at executor/application level, right ?\r\nAre we planning to support specifying this per resource profile later ?",
        "createdAt" : "2020-11-13T16:10:39Z",
        "updatedAt" : "2020-11-13T19:50:27Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "49f398d4-7dd6-404c-9ab2-aed2639018ca",
        "parentId" : "cd2f4cdc-5ef7-43eb-a834-a45b9dc74e11",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "we currently do not. this falls under the allow other spark confs to be specified in the profile and was to be added after initial development.  this definitely makes sense though as I know of a couple companies that have things setup this way.  I'm honestly not sure if it can be done via yarn api right now though. I thought they added ability to request application master container go to separate queue from the other ones but not sure if you can ask for containers from different queue.  \r\nDo you have immediate use case for this? ",
        "createdAt" : "2020-11-13T17:25:53Z",
        "updatedAt" : "2020-11-13T19:50:27Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "f17f4e40-f1db-46da-b923-58e85368c85e",
        "parentId" : "cd2f4cdc-5ef7-43eb-a834-a45b9dc74e11",
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "No immediate use case; but I was looking at the possibilities for using resource profiles for composite jobs doing both data prep and DL in single app, and ability to leverage either a queue with GPU resources or specify node labels for a resource profile would help.\r\n\r\nIf it is already tracked in a separate jira, that is fine ! Will wait for that to be merged :-)",
        "createdAt" : "2020-11-13T18:11:54Z",
        "updatedAt" : "2020-11-13T19:50:27Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "b1d9f964-4dab-4d5e-9535-e41e8c0f75fa",
        "parentId" : "cd2f4cdc-5ef7-43eb-a834-a45b9dc74e11",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "https://issues.apache.org/jira/browse/SPARK-33447",
        "createdAt" : "2020-11-13T18:18:27Z",
        "updatedAt" : "2020-11-13T19:50:27Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "6f5321ffe8637ae32434ac2d5eaff476fc3e0d19",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +646,650 @@Stage level scheduling is supported on YARN when dynamic allocation is enabled. One thing to note that is YARN specific is that each ResourceProfile requires a different container priority on YARN. The mapping is simply the ResourceProfile id becomes the priority, on YARN lower numbers are higher priority. This means that profiles created earlier will have a higher priority in YARN. Normally this won't matter as Spark finishes one stage before starting another one, the only case this might have an affect is in a job server type scenario, so its something to keep in mind.\nNote there is a difference in the way custom resources are handled between the base default profile and custom ResourceProfiles. To allow for the user to request YARN containers with extra resources without Spark scheduling on them, the user can specify resources via the <code>spark.yarn.executor.resource.</code> config. Those configs are only used in the base default profile though and do not get propogated into any other custom ResourceProfiles. This is because there would be no way to remove them if you wanted a stage to not have them. This results in your default profile getting custom resources defined in <code>spark.yarn.executor.resource.</code> plus spark defined resources of GPU or FPGA. Spark converts GPU and FPGA resources into the YARN built in types <code>yarn.io/gpu</code>) and <code>yarn.io/fpga</code>, but does not know the mapping of any other resources. Any other Spark custom resources are not propogated to YARN for the default profile. So if you want Spark to schedule based off a custom resource and have it requested from YARN, you must specify it in both YARN (<code>spark.yarn.{driver/executor}.resource.</code>) and Spark (<code>spark.{driver/executor}.resource.</code>) configs. Leave the Spark config off if you only want YARN containers with the extra resources but Spark not to schedule using them. Now for custom ResourceProfiles, it doesn't currently have a way to only specify YARN resources without Spark scheduling off of them. This means for custom ResourceProfiles we propogate all the resources defined in the ResourceProfile to YARN. We still convert GPU and FPGA to the YARN build in types as well. This requires that the name of any custom resources you specify match what they are defined as in YARN.\n\n# Important notes\n"
  },
  {
    "id" : "163df7dc-7b16-4048-a48b-b24523525740",
    "prId" : 28788,
    "prUrl" : "https://github.com/apache/spark/pull/28788#pullrequestreview-429300993",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7ee8d31e-66d8-4b60-bf8f-3877985b270f",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "~`pre-built with` -> `pre-built one with`?~ nvm.",
        "createdAt" : "2020-06-11T21:28:17Z",
        "updatedAt" : "2020-06-18T06:03:21Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "c2241528374cb64638204b21fd20a8c15cf931da",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +83,87 @@Running Spark on YARN requires a binary distribution of Spark which is built with YARN support.\nBinary distributions can be downloaded from the [downloads page](https://spark.apache.org/downloads.html) of the project website.\nThere are two variants of Spark binary distributions you can download. One is pre-built with a certain\nversion of Apache Hadoop; this Spark distribution contains built-in Hadoop runtime, so we call it `with-hadoop` Spark\ndistribution. The other one is pre-built with user-provided Hadoop; since this Spark distribution"
  },
  {
    "id" : "8c49f3cc-3d95-4885-bdfd-1fcd543576b0",
    "prId" : 28788,
    "prUrl" : "https://github.com/apache/spark/pull/28788#pullrequestreview-429457467",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5bf01db1-0f98-4c63-b61e-6c810b8b9bb7",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "no really biggie but I personally avoid abbreviation in the public documentation.",
        "createdAt" : "2020-06-12T02:39:26Z",
        "updatedAt" : "2020-06-18T06:03:21Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "402bbd5e-6791-44c8-851c-a1465a18ba4e",
        "parentId" : "5bf01db1-0f98-4c63-b61e-6c810b8b9bb7",
        "authorId" : "677aa336-324b-4b93-8300-21f8bd378f26",
        "body" : "In the download link, we use terminology of `pre-built` and `pre-built with user provided hadoop`, and in some other places, we use `with-hadoop`, and `no-hadoop` distributions. That was why I use both of them here to make it clear.",
        "createdAt" : "2020-06-12T04:41:10Z",
        "updatedAt" : "2020-06-18T06:03:21Z",
        "lastEditedBy" : "677aa336-324b-4b93-8300-21f8bd378f26",
        "tags" : [
        ]
      },
      {
        "id" : "46ade60a-fa0e-4b6f-a771-e36b669b6743",
        "parentId" : "5bf01db1-0f98-4c63-b61e-6c810b8b9bb7",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Oops, I meant things like \"don't\"",
        "createdAt" : "2020-06-12T04:44:16Z",
        "updatedAt" : "2020-06-18T06:03:21Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "c2241528374cb64638204b21fd20a8c15cf931da",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +83,87 @@Running Spark on YARN requires a binary distribution of Spark which is built with YARN support.\nBinary distributions can be downloaded from the [downloads page](https://spark.apache.org/downloads.html) of the project website.\nThere are two variants of Spark binary distributions you can download. One is pre-built with a certain\nversion of Apache Hadoop; this Spark distribution contains built-in Hadoop runtime, so we call it `with-hadoop` Spark\ndistribution. The other one is pre-built with user-provided Hadoop; since this Spark distribution"
  }
]