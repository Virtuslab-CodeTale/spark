[
  {
    "id" : "4f280911-3ffe-404e-a6fe-72909a119431",
    "prId" : 33370,
    "prUrl" : "https://github.com/apache/spark/pull/33370#pullrequestreview-707923102",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ea0cf53e-a324-41d7-ab8d-7417914bb9de",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "nit: the indents look incorrect.",
        "createdAt" : "2021-07-16T00:51:59Z",
        "updatedAt" : "2021-07-16T01:43:29Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "e94cf0cc507fe31f26651587a257a36db16763fc",
    "line" : 35,
    "diffHunk" : "@@ -1,1 +286,290 @@    </td>\n    <td>read/write</td>\n </tr>  \n</table>\n"
  },
  {
    "id" : "84665106-f45f-481c-9206-88d4f44eb250",
    "prId" : 32745,
    "prUrl" : "https://github.com/apache/spark/pull/32745#pullrequestreview-675937697",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "52398c91-805e-498a-94ab-4a18634157d0",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Do we need `<b>` inside `<th>`?",
        "createdAt" : "2021-06-04T05:11:00Z",
        "updatedAt" : "2021-06-04T05:11:00Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "622a4628-9750-42e6-a300-8570098b9218",
        "parentId" : "52398c91-805e-498a-94ab-4a18634157d0",
        "authorId" : "2d8ced12-ada1-43e1-9240-bf1c11a01e4c",
        "body" : "Actually I just followed the rule in existing files, and seems like It's a little bit more thicker ðŸ˜…\r\n\r\n- Before <b>\r\n\r\n<img width=\"336\" alt=\"Screen Shot 2021-06-04 at 2 24 59 PM\" src=\"https://user-images.githubusercontent.com/44108233/120750365-35238b00-c541-11eb-8faa-90e94c6a0a9d.png\">\r\n\r\n- After <b>\r\n\r\n![Screen Shot 2021-06-04 at 2 25 34 PM](https://user-images.githubusercontent.com/44108233/120750375-394fa880-c541-11eb-8e66-b3b28a34cb6f.png)",
        "createdAt" : "2021-06-04T05:29:43Z",
        "updatedAt" : "2021-06-04T05:29:43Z",
        "lastEditedBy" : "2d8ced12-ada1-43e1-9240-bf1c11a01e4c",
        "tags" : [
        ]
      },
      {
        "id" : "707e6293-7eec-4651-8998-7772f63ffc11",
        "parentId" : "52398c91-805e-498a-94ab-4a18634157d0",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Thank you for checking! Looks better. ðŸ˜„ ",
        "createdAt" : "2021-06-04T05:50:59Z",
        "updatedAt" : "2021-06-04T05:50:59Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "73d26ac6684a4c6bdef1b8d3f585b8524db9f260",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +53,57 @@\n<table class=\"table\">\n  <tr><th><b>Property Name</b></th><th><b>Default</b></th><th><b>Meaning</b></th><th><b>Scope</b></th></tr>\n  <tr>\n    <td><code>url</code></td>"
  },
  {
    "id" : "424a2c56-9f28-4452-8de3-7105f3f70c0d",
    "prId" : 28209,
    "prUrl" : "https://github.com/apache/spark/pull/28209#pullrequestreview-392594628",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "159f4c4c-d3fb-4c74-8dbe-faeade467c5a",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Sorry for a bit of forth and back. I think `sql` was correct considering all instances look lower-cased: `scala`, `python` and `r`. I checked via `git grep -r \"data-lang\"`. I googled and seems they are lower-cased in general.\r\n\r\n",
        "createdAt" : "2020-04-14T02:00:34Z",
        "updatedAt" : "2020-04-14T02:00:35Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "4dc13756-e8a4-4178-bc30-331667391cd9",
        "parentId" : "159f4c4c-d3fb-4c74-8dbe-faeade467c5a",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Ah, I see. My bad. I just wanted to make the tab `Sql` label uppercase though.\r\n<img width=\"459\" alt=\"Screen Shot 2020-04-14 at 11 37 02\" src=\"https://user-images.githubusercontent.com/692303/79180252-6c6a4080-7e44-11ea-8895-be791c17d4dc.png\">\r\n",
        "createdAt" : "2020-04-14T02:32:55Z",
        "updatedAt" : "2020-04-14T02:38:57Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "3a35dabc-4f4d-4c7f-aae9-45aacb50d074",
        "parentId" : "159f4c4c-d3fb-4c74-8dbe-faeade467c5a",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Oh, does `sql` -> `SQL` make it uppercased? then +1",
        "createdAt" : "2020-04-14T04:07:31Z",
        "updatedAt" : "2020-04-14T04:07:31Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "9cbf5517-1be3-4e3e-8883-b57d1ecd7c2c",
        "parentId" : "159f4c4c-d3fb-4c74-8dbe-faeade467c5a",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Yea, it seems yes..., @huaxingao Could you check this again? ",
        "createdAt" : "2020-04-14T04:25:39Z",
        "updatedAt" : "2020-04-14T04:28:16Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "289b8f45d34d8dcef08033e2fae35492539784df",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +232,236 @@</div>\n\n<div data-lang=\"SQL\"  markdown=\"1\">\n\n{% highlight sql %}"
  },
  {
    "id" : "4d4c2141-1a77-40cf-b82d-540f7e640350",
    "prId" : 27637,
    "prUrl" : "https://github.com/apache/spark/pull/27637#pullrequestreview-366766115",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3f3594a1-74b6-479a-a37a-fbb528c29231",
        "parentId" : null,
        "authorId" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "body" : "One thing I dislike about this approach is that it forces you to change application code depending on how you want to authenticate. Have you thought about ways in which to avoid that? \r\n\r\ne.g. if the postgres jdbc url syntax allows for these options to be defined there, you could check whether there's a placeholder for the principal / keytab and replace with some globally-set Spark config.\r\n\r\nAnother question I have is whether postgres (at least, since that's what this change covers) can use auth data from the current logged in subject, in which case the above discussion could be solved by extending Spark's principal / keytab support to the executor processes too.",
        "createdAt" : "2020-02-26T22:20:47Z",
        "updatedAt" : "2020-03-09T09:04:40Z",
        "lastEditedBy" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "tags" : [
        ]
      },
      {
        "id" : "4d56dede-e9fb-4fb6-9305-5989ea995bbd",
        "parentId" : "3f3594a1-74b6-479a-a37a-fbb528c29231",
        "authorId" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "body" : "> e.g. if the postgres jdbc url syntax allows for these options to be defined there, you could check whether there's a placeholder for the principal / keytab and replace with some globally-set Spark config.\r\n\r\nI've considered global config but since Spark can use multiple databases at the same time like postgres on source side and oracle on sink side I thought it would make the implementation rigid.\r\n\r\nThe other option which is the jdbc url also considered but that way for each database type an url parser must be implemented which I thought is an overkill (for instance postgres uses `&` as separator and MSSQL uses `;`).\r\n",
        "createdAt" : "2020-02-27T12:44:49Z",
        "updatedAt" : "2020-03-09T09:04:40Z",
        "lastEditedBy" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "tags" : [
        ]
      },
      {
        "id" : "70823ce0-e248-4e09-8c1b-d9f2e5aebb20",
        "parentId" : "3f3594a1-74b6-479a-a37a-fbb528c29231",
        "authorId" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "body" : "> Another question I have is whether postgres (at least, since that's what this change covers) can use auth data from the current logged in subject, in which case the above discussion could be solved by extending Spark's principal / keytab support to the executor processes too.\r\n\r\nI've gone through the documentation + scanned the client code but not found such possibility.\r\n",
        "createdAt" : "2020-02-27T12:46:08Z",
        "updatedAt" : "2020-03-09T09:04:40Z",
        "lastEditedBy" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "tags" : [
        ]
      },
      {
        "id" : "58df3cc2-e171-44cb-8cd3-97c97f7083a5",
        "parentId" : "3f3594a1-74b6-479a-a37a-fbb528c29231",
        "authorId" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "body" : "> I've gone through the documentation + scanned the client code but not found such possibility.\r\n\r\nSigh. Single sign-on kinda loses its meaning if every library insists on managing its own login and requiring user creds... :-/ Will have to live with it, I guess.",
        "createdAt" : "2020-02-29T01:37:49Z",
        "updatedAt" : "2020-03-09T09:04:40Z",
        "lastEditedBy" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "tags" : [
        ]
      }
    ],
    "commit" : "301cc1a7426ac5917b2535374151921064eaa392",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +200,204 @@\n  <tr>\n    <td><code>keytab</code></td>\n    <td>\n     Location of the kerberos keytab file (which must be pre-uploaded to all nodes either by <code>--files</code> option of spark-submit or manually) for the JDBC client. When path information found then Spark considers the keytab distributed manually, otherwise <code>--files</code> assumed. If both <code>keytab</code> and <code>principal</code> are defined then Spark tries to do kerberos authentication."
  }
]