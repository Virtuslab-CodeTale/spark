[
  {
    "id" : "a42869dd-ca31-4c81-a6cf-e9e35c64549b",
    "prId" : 33691,
    "prUrl" : "https://github.com/apache/spark/pull/33691#pullrequestreview-727794453",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d27c5ab8-f4de-4013-b941-e30298588164",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Does it work? We check the type of gapDuration and expect str type here.\r\n\r\nhttps://github.com/apache/spark/blob/f7c85b855ba99757c750dd0a2f7aced788c89374/python/pyspark/sql/functions.py#L2336-L2368\r\n\r\nhttps://github.com/apache/spark/blob/bbf988bd73b00d18dd1d443f225b3915a2c4433f/python/pyspark/sql/functions.pyi#L139\r\n",
        "createdAt" : "2021-08-11T10:30:44Z",
        "updatedAt" : "2021-08-11T10:50:47Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "535bc6f9-7ed7-44b3-9ec8-8d3724edb710",
        "parentId" : "d27c5ab8-f4de-4013-b941-e30298588164",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Oh, I don't realize python `session_window` doesn't accept an expression...",
        "createdAt" : "2021-08-11T18:11:06Z",
        "updatedAt" : "2021-08-11T18:11:07Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "51e49ca63a812f2c220f32cdd284e6f46e863fe7",
    "line" : 70,
    "diffHunk" : "@@ -1,1 +1190,1194 @@events = ...  # streaming DataFrame of schema { timestamp: Timestamp, userId: String }\n\nsession_window = session_window(events.timestamp, \\\n    F.when(events.userId == \"user1\", \"5 seconds\") \\\n    .when(events.userId == \"user2\", \"20 seconds\").otherwise(\"5 minutes\"))"
  },
  {
    "id" : "383c6465-26e3-43a9-88c7-92bc1d97b6c1",
    "prId" : 33433,
    "prUrl" : "https://github.com/apache/spark/pull/33433#pullrequestreview-710615504",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c0de3fa9-4dd2-4254-a40a-cb9d242aba39",
        "parentId" : null,
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "Do we need to mention that the `session_window` function should be a grouping key and should not be the only grouping key?",
        "createdAt" : "2021-07-20T13:36:30Z",
        "updatedAt" : "2021-07-20T13:36:31Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      },
      {
        "id" : "470124e9-dcbd-46d6-a636-a489d60b5384",
        "parentId" : "c0de3fa9-4dd2-4254-a40a-cb9d242aba39",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Actually there's no such restriction, like we have no such restriction for `window` function. You can use `session_window` anywhere we expect expression. We just don't calculate any windowing aggregation if the function is not used in groping key, same as we do for `window` function.\r\n\r\nIt should not be the only grouping key <= this restriction is only applied to the streaming query. In batch query we allow global window.",
        "createdAt" : "2021-07-20T13:49:56Z",
        "updatedAt" : "2021-07-20T13:49:56Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "5b4d0765-724e-4ba9-9725-65f15e32770b",
        "parentId" : "c0de3fa9-4dd2-4254-a40a-cb9d242aba39",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "Ah yea make sense, I only considered the SS scenario. ",
        "createdAt" : "2021-07-20T14:00:05Z",
        "updatedAt" : "2021-07-20T14:00:05Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "5f625ebde16678fe3c90859436e8c05063427f50",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +1084,1088 @@received within gap duration after receiving the latest input.\n\nSession window uses `session_window` function. The usage of the function is similar to the `window` function.\n\n<div class=\"codetabs\">"
  },
  {
    "id" : "fe49fd64-da12-4932-ab13-30f69a41391b",
    "prId" : 31590,
    "prUrl" : "https://github.com/apache/spark/pull/31590#pullrequestreview-594056392",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d8f5ad38-4a4c-4029-845e-bf9c041c2c66",
        "parentId" : null,
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "Maybe we can remove the `R` tab.",
        "createdAt" : "2021-02-19T09:35:56Z",
        "updatedAt" : "2021-02-20T01:27:28Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      },
      {
        "id" : "70e84b28-fbbe-4f51-8bfa-5ec4dc23dd34",
        "parentId" : "d8f5ad38-4a4c-4029-845e-bf9c041c2c66",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "Never mind, I saw other places also list the `R` tab even though it's not available.",
        "createdAt" : "2021-02-19T09:40:23Z",
        "updatedAt" : "2021-02-20T01:27:28Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      },
      {
        "id" : "19c55b0a-705c-460e-8713-ba441f01e30b",
        "parentId" : "d8f5ad38-4a4c-4029-845e-bf9c041c2c66",
        "authorId" : "6e99a03b-1d60-4065-9e56-3151748ea94e",
        "body" : "Trying to follow the pattern in `Foreach` and `ForeachBatch` sections here.",
        "createdAt" : "2021-02-19T10:27:57Z",
        "updatedAt" : "2021-02-20T01:27:28Z",
        "lastEditedBy" : "6e99a03b-1d60-4065-9e56-3151748ea94e",
        "tags" : [
        ]
      }
    ],
    "commit" : "46f650ea4130b5065f66ed02c66b708c6d5fc5b9",
    "line" : 120,
    "diffHunk" : "@@ -1,1 +2475,2479 @@\n<div data-lang=\"r\"  markdown=\"1\">\nNot available in R.\n</div>\n</div>"
  },
  {
    "id" : "e0c74ce3-c349-47bc-9eae-6aedcc3805e4",
    "prId" : 31590,
    "prUrl" : "https://github.com/apache/spark/pull/31590#pullrequestreview-594046338",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5fe6436b-80af-4f1a-9d4b-9f2393cc227c",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "While the code example looks to describe the simple cases, I'd like to guide to the end users they need to check the documentation of API for details before using. A simple sentence is OK, just remind to check the documentation, because the auto-creation of table has lacks on v2 table.",
        "createdAt" : "2021-02-19T10:02:21Z",
        "updatedAt" : "2021-02-20T01:27:28Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "08da03aa-f454-4b62-a15d-964ee8c52cc8",
        "parentId" : "5fe6436b-80af-4f1a-9d4b-9f2393cc227c",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "```\r\nA simple sentence is OK, just remind to check the documentation, because the auto-creation of table has lacks on v2 table.\r\n```\r\nAgree, it's also a reminder for us to add the support for the v2 table in the next major version.",
        "createdAt" : "2021-02-19T10:15:33Z",
        "updatedAt" : "2021-02-20T01:27:28Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "46f650ea4130b5065f66ed02c66b708c6d5fc5b9",
    "line" : 122,
    "diffHunk" : "@@ -1,1 +2477,2481 @@Not available in R.\n</div>\n</div>\n\nFor more details, please check the docs for DataStreamReader ([Scala](api/scala/org/apache/spark/sql/streaming/DataStreamReader.html)/[Java](api/java/org/apache/spark/sql/streaming/DataStreamReader.html)/[Python](api/python/pyspark.sql.html#pyspark.sql.streaming.DataStreamReader) docs) and DataStreamWriter ([Scala](api/scala/org/apache/spark/sql/streaming/DataStreamWriter.html)/[Java](api/java/org/apache/spark/sql/streaming/DataStreamWriter.html)/[Python](api/python/pyspark.sql.html#pyspark.sql.streaming.DataStreamWriter) docs)."
  },
  {
    "id" : "5a3822aa-8d3b-4997-aab8-6f2209e64e2b",
    "prId" : 31572,
    "prUrl" : "https://github.com/apache/spark/pull/31572#pullrequestreview-591824438",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d25c2c48-9c9f-4f2c-89a6-36ed043a6eb0",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Not due to this PR, but I saw this section is \"Outer Joins with Watermarking\", seems to be \"inner\" and \"leftSemi\" should not be here.",
        "createdAt" : "2021-02-16T08:58:04Z",
        "updatedAt" : "2021-02-17T04:11:29Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "4aeba22d-5fe8-4f1a-ad77-83d43595a801",
        "parentId" : "d25c2c48-9c9f-4f2c-89a6-36ed043a6eb0",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "probably the purpose was to enumerate the all possible values for joinType. while I agree it's not matching with the section, I feel it's not a big deal. either is fine, including leaving it as it is (with adding left semi) and another minor PR to fix.",
        "createdAt" : "2021-02-16T21:13:49Z",
        "updatedAt" : "2021-02-17T04:11:29Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "35c1596c-00df-48c5-8bab-1806e86d3abc",
        "parentId" : "d25c2c48-9c9f-4f2c-89a6-36ed043a6eb0",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "I don't feel this is too big deal, leave it as it is for now.",
        "createdAt" : "2021-02-17T04:15:00Z",
        "updatedAt" : "2021-02-17T04:15:00Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      }
    ],
    "commit" : "0c9e5d21a2b14cb5a0dfe9814ab298f2f8d606ad",
    "line" : 25,
    "diffHunk" : "@@ -1,1 +1338,1342 @@    clickTime <= impressionTime + interval 1 hour\n    \"\"\"),\n  joinType = \"leftOuter\"      // can be \"inner\", \"leftOuter\", \"rightOuter\", \"fullOuter\", \"leftSemi\"\n )\n"
  },
  {
    "id" : "fcff8faa-4b80-4244-818f-791ba9e9420e",
    "prId" : 30789,
    "prUrl" : "https://github.com/apache/spark/pull/30789#pullrequestreview-553938919",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "98c955e4-9b5c-4c78-bd29-8e414be5d983",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "This is good info but can we get more specific about the tuning? if you see what then you change spark.locality.wait to what, up or down, etc? what is too big, too small, etc",
        "createdAt" : "2020-12-16T13:31:48Z",
        "updatedAt" : "2020-12-17T00:49:12Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "bb89f6db-f724-43bc-adff-05bfe41d94f0",
        "parentId" : "98c955e4-9b5c-4c78-bd29-8e414be5d983",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Good point. I added few specific info.",
        "createdAt" : "2020-12-16T17:51:09Z",
        "updatedAt" : "2020-12-17T00:49:12Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "d93f6dc5628b7514cf2527d49dbf99e3d51b9be3",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +1706,1710 @@\nBy changing the Spark configurations related to task scheduling, for example `spark.locality.wait`, users can configure Spark how long to wait to launch a data-local task.\nFor stateful operations in Structured Streaming, it can be used to let state store providers running on the same executors across batches.\n\nSpecifically for built-in HDFS state store provider, users can check the state store metrics such as `loadedMapCacheHitCount` and `loadedMapCacheMissCount`. Ideally,"
  },
  {
    "id" : "d9962e91-3e6e-4474-bc52-951b94bd6bab",
    "prId" : 30789,
    "prUrl" : "https://github.com/apache/spark/pull/30789#pullrequestreview-554168581",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d8db4be9-12e2-4ea1-8bee-1d815b1400ff",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Probably explicitly mentioning `microbatches`, or if we still concern about continuous processing, then `during the lifetime of the query`?",
        "createdAt" : "2020-12-16T23:03:53Z",
        "updatedAt" : "2020-12-17T00:49:12Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "9db18890-6ba8-4bab-8d3e-a425b761064f",
        "parentId" : "d8db4be9-12e2-4ea1-8bee-1d815b1400ff",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "`batches` would also just work - looks like in further sentences you seem to use `batches` simply.",
        "createdAt" : "2020-12-16T23:29:56Z",
        "updatedAt" : "2020-12-17T00:49:12Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "d93f6dc5628b7514cf2527d49dbf99e3d51b9be3",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +1693,1697 @@\nThe stateful operations store states for events in state stores of executors. State stores occupy resources such as memory and disk space to store the states.\nSo it is more efficient to keep a state store provider running in the same executor across different streaming batches.\nChanging the location of a state store provider requires the extra overhead of loading checkpointed states. The overhead of loading state from checkpoint depends\non the external storage and the size of the state, which tends to hurt the latency of micro-batch run. For some use cases such as processing very large state data,"
  },
  {
    "id" : "0b74ba33-5e9e-489f-bb39-700322e02ba3",
    "prId" : 30789,
    "prUrl" : "https://github.com/apache/spark/pull/30789#pullrequestreview-554155313",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b6db13b5-2308-427b-b681-b36b14b6d98c",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "I'd rearrange following sentences to explain happy case, and the bad case. Like...\r\n\r\n\r\n> The stateful operations in Structured Streaming queries rely on the preferred location feature of Spark's RDD to run the state store provider on the same executor. If in the next batch the corresponding state store provider is scheduled on this executor again, it could reuse the previous states and save the time of loading checkpointed states.\r\n> \r\n> However, generally the preferred location is not a hard requirement and it is still possible that Spark schedules tasks to the executors other than the preferred ones. In this case, Spark will load state store providers from checkpointed states on HDFS to new executors. The state store providers run in the previous batch will not be unloaded immediately.\r\n",
        "createdAt" : "2020-12-16T23:17:36Z",
        "updatedAt" : "2020-12-17T00:49:12Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "d93f6dc5628b7514cf2527d49dbf99e3d51b9be3",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +1698,1702 @@loading new state store providers from checkpointed states can be very time-consuming and inefficient.\n\nThe stateful operations in Structured Streaming queries rely on the preferred location feature of Spark's RDD to run the state store provider on the same executor.\nIf in the next batch the corresponding state store provider is scheduled on this executor again, it could reuse the previous states and save the time of loading checkpointed states.\n"
  },
  {
    "id" : "2d57f23b-b1c5-4090-80e7-c47480d040fd",
    "prId" : 30789,
    "prUrl" : "https://github.com/apache/spark/pull/30789#pullrequestreview-554202454",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2aed25cb-ba04-4d24-852c-c845d3bc9d35",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "OFF TOPIC (for #30770): probably we may need to allow having different timeout between state RDD and others.",
        "createdAt" : "2020-12-17T00:56:20Z",
        "updatedAt" : "2020-12-17T00:56:20Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "d93f6dc5628b7514cf2527d49dbf99e3d51b9be3",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +1705,1709 @@Spark runs a maintenance task which checks and unloads the state store providers that are inactive on the executors.\n\nBy changing the Spark configurations related to task scheduling, for example `spark.locality.wait`, users can configure Spark how long to wait to launch a data-local task.\nFor stateful operations in Structured Streaming, it can be used to let state store providers running on the same executors across batches.\n"
  },
  {
    "id" : "9fe4297a-49e7-41c5-8697-ebeac6b1b404",
    "prId" : 30074,
    "prUrl" : "https://github.com/apache/spark/pull/30074#pullrequestreview-510862733",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "449b3dc5-0f4a-4640-baa9-76fb4633063c",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Aggregations are not just disallowed after `flatMapGroupsWithState` in in update mode. It is disallowed in the query. \r\n\r\n`org.apache.spark.sql.catalyst.analysis.UnsupportedOperationsSuite`:\r\n\r\n```\r\n[info] - streaming plan - flatMapGroupsWithState - flatMapGroupsWithState(Update) on streaming relation with aggregation in Append mode: not supported \r\n[info] - streaming plan - flatMapGroupsWithState - flatMapGroupsWithState(Update) on streaming relation with aggregation in Update mode: not supported \r\n[info] - streaming plan - flatMapGroupsWithState - flatMapGroupsWithState(Update) on streaming relation with aggregation in Complete mode: not supported\r\n```\r\n",
        "createdAt" : "2020-10-17T01:27:35Z",
        "updatedAt" : "2020-10-17T01:29:10Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "ecf3d4c377c8e6d31d7c8082f3064a30cab40979",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +1780,1784 @@    <td style=\"vertical-align: middle;\">Update</td>\n    <td style=\"vertical-align: middle;\">\n      Aggregations not allowed in a query with <code>flatMapGroupsWithState</code>.\n    </td>\n  </tr>"
  },
  {
    "id" : "f26f73f0-b9b9-4252-b274-335061c0f3ad",
    "prId" : 30074,
    "prUrl" : "https://github.com/apache/spark/pull/30074#pullrequestreview-510863855",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bfb91260-6f43-41fb-8784-1bba10825d8a",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "`UnsupportedOperationChecker.scala`:\r\n\r\n```scala\r\nif (m.isMapGroupsWithState) {                       // check mapGroupsWithState\r\n  // allowed only in update query output mode and without aggregation\r\n  if (aggsInQuery.nonEmpty) {\r\n    throwError(\r\n      \"mapGroupsWithState is not supported with aggregation \" +\r\n      \"on a streaming DataFrame/Dataset\")\r\n  } else if (outputMode != InternalOutputModes.Update) {\r\n    ...\r\n  }\r\n} ..\r\n..\r\n```",
        "createdAt" : "2020-10-17T01:41:48Z",
        "updatedAt" : "2020-10-17T01:41:48Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "ecf3d4c377c8e6d31d7c8082f3064a30cab40979",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +1766,1770 @@    <td style=\"vertical-align: middle;\">\n      Aggregations not allowed in a query with <code>mapGroupsWithState</code>.\n    </td>\n  </tr>\n  <tr>"
  },
  {
    "id" : "9240da7c-e9f3-4b01-aee6-eddfa6f2b84f",
    "prId" : 28739,
    "prUrl" : "https://github.com/apache/spark/pull/28739#pullrequestreview-425440653",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "87d4c514-865a-4954-9a05-fe0aa4e05b86",
        "parentId" : null,
        "authorId" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "body" : "Moving line break is an addition but I wanted to highlight it. Presume that was the reason of the split.",
        "createdAt" : "2020-06-05T16:15:37Z",
        "updatedAt" : "2020-06-05T16:15:38Z",
        "lastEditedBy" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "tags" : [
        ]
      }
    ],
    "commit" : "ef6827916c045c55bdc6f43db4bf52e65ee4450e",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +545,549 @@        \"s3n://a/b/dataset.txt\"<br/>\n        \"s3a://a/b/c/dataset.txt\"\n        <br/>\n        <code>maxFileAge</code>: Maximum age of a file that can be found in this directory, before it is ignored. For the first batch all files will be considered valid. If <code>latestFirst</code> is set to `true` and <code>maxFilesPerTrigger</code> is set, then this parameter will be ignored, because old files that are valid, and should be processed, may be ignored. The max age is specified with respect to the timestamp of the latest file, and not the timestamp of the current system.(default: 1 week)\n        <br/>"
  },
  {
    "id" : "3204dfb9-161d-42ef-9c49-2295822517c8",
    "prId" : 28422,
    "prUrl" : "https://github.com/apache/spark/pull/28422#pullrequestreview-404195996",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "32aff9bb-7e57-48c2-8354-1da5df129b7f",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Out of curiosity, did we document what kind of value is expected for these options, e.g., `7d`?",
        "createdAt" : "2020-05-01T01:49:20Z",
        "updatedAt" : "2020-08-25T07:49:16Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "e31ed00f-bcf3-42c2-a534-5bb1f979d4b8",
        "parentId" : "32aff9bb-7e57-48c2-8354-1da5df129b7f",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Looks like the kinds of values weren't specified in many options, but implied by default values. This option doesn't have default value - maybe better to explicitly specify kind of value. Good point!",
        "createdAt" : "2020-05-01T14:41:14Z",
        "updatedAt" : "2020-08-25T07:49:16Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "b7d94f77860ca9343505f86e4688c30890fbb6e8",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +553,557 @@        NOTE 1: Please be careful to set the value if the query replays from the old input files.<br/>\n        NOTE 2: Please make sure the timestamp is in sync between nodes which run the query.<br/>\n        <br/>\n        <code>cleanSource</code>: option to clean up completed files after processing.<br/>\n        Available options are \"archive\", \"delete\", \"off\". If the option is not provided, the default value is \"off\".<br/>"
  },
  {
    "id" : "730ab323-fdd4-465b-bf11-e41d51cfcbf3",
    "prId" : 27740,
    "prUrl" : "https://github.com/apache/spark/pull/27740#pullrequestreview-366830848",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7fa6244b-62e8-4da3-9770-04b91ddf1a53",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "This seems already fixed at https://github.com/apache/spark/commit/9a2cec9b1ee3ba095461d9c07c9932107228d42e . Can you remove this in your PR?",
        "createdAt" : "2020-03-01T02:23:08Z",
        "updatedAt" : "2020-03-01T02:23:08Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "df6539b5c5b62346802f1e2e0dac3aff4dd583d4",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +2203,2207 @@{% highlight java %}\nstreamingDatasetOfString.writeStream().foreach(\n  new ForeachWriter<String>() {\n\n    @Override public boolean open(long partitionId, long version) {"
  }
]