[
  {
    "id" : "23cd095c-4cb3-40c1-9650-141dfd9abc13",
    "prId" : 33516,
    "prUrl" : "https://github.com/apache/spark/pull/33516#pullrequestreview-715064600",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e4eea353-d277-44c8-956c-635f019e8413",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Should we distinguish year-month and day-time interval types? ",
        "createdAt" : "2021-07-26T14:18:50Z",
        "updatedAt" : "2021-07-26T14:21:03Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "3c550c68-1623-436d-a27a-e97942c38e56",
        "parentId" : "e4eea353-d277-44c8-956c-635f019e8413",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "Let's keep it simple in this section",
        "createdAt" : "2021-07-26T16:41:56Z",
        "updatedAt" : "2021-07-26T16:41:56Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "f75359a8187d38668a9f2cd37e91d7d11ee8223c",
    "line" : 64,
    "diffHunk" : "@@ -1,1 +178,182 @@| Binary    | Binary                                                           |\n| Boolean   | Boolean                                                          |\n| Interval  | Interval                                                         |\n| Map       | Map**                                                            |\n| Array     | Array**                                                          |"
  },
  {
    "id" : "21c3d6f2-ba94-4d0c-a373-781535057568",
    "prId" : 33516,
    "prUrl" : "https://github.com/apache/spark/pull/33516#pullrequestreview-715853379",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "61aba214-ca0a-439c-b65b-9fd6782c1d11",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "In this section, it says `In future releases, the behaviour of type coercion might change along with the other two type conversion rules.`\r\n\r\nWe should update it.",
        "createdAt" : "2021-07-27T12:33:24Z",
        "updatedAt" : "2021-07-27T12:33:25Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "f75359a8187d38668a9f2cd37e91d7d11ee8223c",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +67,71 @@```\n\n### Cast\n\nWhen `spark.sql.ansi.enabled` is set to `true`, explicit casting by `CAST` syntax throws a runtime exception for illegal cast patterns defined in the standard, e.g. casts from a string to an integer."
  },
  {
    "id" : "231f221c-e757-4b3c-8372-37511647cdf4",
    "prId" : 32129,
    "prUrl" : "https://github.com/apache/spark/pull/32129#pullrequestreview-634124490",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6bd55cb8-5e50-4a29-a77d-e112d637cf45",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "nit: `in a GROUP BY clause` -> `by a GROUP BY clause`?",
        "createdAt" : "2021-04-12T23:00:32Z",
        "updatedAt" : "2021-04-12T23:00:32Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "a92670dc-2179-4a9a-99ce-1919c79e4078",
        "parentId" : "6bd55cb8-5e50-4a29-a77d-e112d637cf45",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "Both should work. The second sentence is from the ANSI SQL standard.",
        "createdAt" : "2021-04-13T02:41:55Z",
        "updatedAt" : "2021-04-13T02:41:55Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "62cee4f24ed49d9c8f967f82267dbd3f3180c32c",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +184,188 @@  - `array_col[index]`: This operator throws `ArrayIndexOutOfBoundsException` if using invalid indices.\n  - `map_col[key]`: This operator throws `NoSuchElementException` if key does not exist in map.\n  - `GROUP BY`: aliases in a select list can not be used in GROUP BY clauses. Each column referenced in a GROUP BY clause shall unambiguously reference a column of the table resulting from the FROM clause.\n\n### SQL Keywords"
  },
  {
    "id" : "eca331e2-2587-4b51-b211-2946a72c6e18",
    "prId" : 31734,
    "prUrl" : "https://github.com/apache/spark/pull/31734#pullrequestreview-603808380",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "660f9e58-7a08-4b3b-86b8-83195e8d487b",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "not related to this PR. Does section `### Type Conversion` cover the CAST behaviors?",
        "createdAt" : "2021-03-04T07:53:16Z",
        "updatedAt" : "2021-03-04T07:53:17Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "08ca0af6-235a-48d0-b698-2f25136940d2",
        "parentId" : "660f9e58-7a08-4b3b-86b8-83195e8d487b",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "Yes and no.\r\n```\r\nWhen `spark.sql.ansi.enabled` is set to `true`, explicit casting by `CAST` syntax throws a runtime exception for illegal cast patterns defined in the standard, e.g. casts from a string to an integer.\r\n```\r\ncovers this, but we had better write down all the cases that would throw exceptions.\r\n\r\nHow about a follow-up to add a subsection for all these cases under `### Type Conversion`?",
        "createdAt" : "2021-03-04T08:05:23Z",
        "updatedAt" : "2021-03-04T08:05:23Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "d2e8e187-e65e-4ec8-8dd8-e5fb139da360",
        "parentId" : "660f9e58-7a08-4b3b-86b8-83195e8d487b",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "SGTM",
        "createdAt" : "2021-03-04T08:09:38Z",
        "updatedAt" : "2021-03-04T08:09:38Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "78277190f07798956adce09c1257f9e351ea7308",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +166,170 @@  - `CAST(string_col AS TIMESTAMP)`: This operator should fail with an exception if the input string can't be parsed.\n  - `CAST(string_col AS DATE)`: This operator should fail with an exception if the input string can't be parsed.\n  - `CAST(string_col AS BOOLEAN)`: This operator should fail with an exception if the input string can't be parsed.\n\n### SQL Keywords"
  },
  {
    "id" : "0eb8d901-c47b-41c4-96a1-461e265ef9c2",
    "prId" : 31180,
    "prUrl" : "https://github.com/apache/spark/pull/31180#pullrequestreview-568162743",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5bc1a5c5-7482-4679-ba9f-5c72ebb72906",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "We could even make it lower-cased and say these are simple strings or catalog strings .. but I think it's fine either way.",
        "createdAt" : "2021-01-14T09:49:44Z",
        "updatedAt" : "2021-01-14T09:49:46Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "8da4bc2a-6213-4c76-a56b-a5c82b0e1611",
        "parentId" : "5bc1a5c5-7482-4679-ba9f-5c72ebb72906",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "Considering there are \"Y\"s and \"N\"s in the table, let's keep the current way.",
        "createdAt" : "2021-01-14T11:08:32Z",
        "updatedAt" : "2021-01-14T11:08:32Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "c8e57e5b-055b-4cf3-9bae-b0a016795979",
        "parentId" : "5bc1a5c5-7482-4679-ba9f-5c72ebb72906",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "üëå ",
        "createdAt" : "2021-01-14T11:52:49Z",
        "updatedAt" : "2021-01-14T11:52:50Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "9fe059ca5bec9a17c15b0f92a0db9ce94069421c",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +71,75 @@‚ÄúY‚Äù indicates that the combination is syntactically valid without restriction and ‚ÄúN‚Äù indicates that the combination is not valid.\n\n| Source\\Target | Numeric | String | Date | Timestamp | Interval | Boolean | Binary | Array | Map | Struct |\n|-----------|---------|--------|------|-----------|----------|---------|--------|-------|-----|--------|\n| Numeric   | Y       | Y      | N    | N         | N        | Y       | N      | N     | N   | N      |"
  },
  {
    "id" : "20effa6b-3de9-4776-9074-e96c226e90f6",
    "prId" : 29055,
    "prUrl" : "https://github.com/apache/spark/pull/29055#pullrequestreview-445978357",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3bf3473d-9147-4775-a221-fbde33772087",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Ah, I see. We cannot remove this line: `non-reserved|non-reserved|not a keyword`? I think users don't need to care about the words.",
        "createdAt" : "2020-07-09T14:06:33Z",
        "updatedAt" : "2020-07-10T05:54:31Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "055d40ac-d048-42b5-bcd7-f48d88d0811f",
        "parentId" : "3bf3473d-9147-4775-a221-fbde33772087",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "But is DIV a keyword?",
        "createdAt" : "2020-07-09T15:03:44Z",
        "updatedAt" : "2020-07-10T05:54:31Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "a0109c1b-ac08-4ad7-9672-452748e5fb84",
        "parentId" : "3bf3473d-9147-4775-a221-fbde33772087",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "`DIV` is a non-reserved keyword in Apache Spark. I guess we need to have this here.",
        "createdAt" : "2020-07-09T21:22:32Z",
        "updatedAt" : "2020-07-10T05:54:31Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "71e0c13277b11055b33ba5da33f1b911af505cf2",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +194,198 @@|DISTINCT|reserved|non-reserved|reserved|\n|DISTRIBUTE|non-reserved|non-reserved|non-reserved|\n|DIV|non-reserved|non-reserved|not a keyword|\n|DROP|non-reserved|non-reserved|reserved|\n|ELSE|reserved|non-reserved|reserved|"
  },
  {
    "id" : "648d3419-7ba9-4c81-9c4f-1a46cffe2e73",
    "prId" : 29055,
    "prUrl" : "https://github.com/apache/spark/pull/29055#pullrequestreview-446233898",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "468ea371-dd0d-4231-b55f-f9a1fdf67f67",
        "parentId" : null,
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "Is there any consensus to decide which version of ANSI SQL standard that we actually try to comply with? \r\n\r\nOr any protocol to define the compatibility between Spark versions and ANSI SQL standardsÔºü",
        "createdAt" : "2020-07-10T08:02:35Z",
        "updatedAt" : "2020-07-10T08:02:36Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "fc48e77a-5dea-4778-8daf-efa5d02fb057",
        "parentId" : "468ea371-dd0d-4231-b55f-f9a1fdf67f67",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I think the latest ANSI SQL standard is preferred.\r\n\r\nAlso to note that: ANSI compliant doesn't mean to follow everything in ANSI SQL. e.g. a keyword reserved in ANSI SQL can be non-reserved in Spark.",
        "createdAt" : "2020-07-10T08:20:58Z",
        "updatedAt" : "2020-07-10T08:20:58Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "1fee10c2-4074-4541-aad4-58f2bb27f6a1",
        "parentId" : "468ea371-dd0d-4231-b55f-f9a1fdf67f67",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "Looking at the history of this standard, the trend of it seems to add more and more keywords and change the non-reserved to reserved. I guess it's ok to follow the latest one here as literally we will add migration guide for every newly added keyword.",
        "createdAt" : "2020-07-10T08:52:39Z",
        "updatedAt" : "2020-07-10T08:52:39Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "71e0c13277b11055b33ba5da33f1b911af505cf2",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +128,132 @@Below is a list of all the keywords in Spark SQL.\n\n|Keyword|Spark SQL<br/>ANSI Mode|Spark SQL<br/>Default Mode|SQL-2016|\n|-------|----------------------|-------------------------|--------|\n|ADD|non-reserved|non-reserved|non-reserved|"
  },
  {
    "id" : "80eb8f65-b316-4297-a6a0-369fd7a4f57b",
    "prId" : 28451,
    "prUrl" : "https://github.com/apache/spark/pull/28451#pullrequestreview-405501292",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c1e6e0de-f8ab-469d-9c5a-1f5b1c5b0c8a",
        "parentId" : null,
        "authorId" : "a2fcc15e-a51a-42f0-87a6-137048a28e30",
        "body" : "@huaxingao I know that it's not related to the format change that you r doing in this PR. But shouldn't we have a SET statement here, so users can cut-paste the command in their shell to see the behavior ? Perhaps we discussed it in the pr that added this clause. Just a question :-)",
        "createdAt" : "2020-05-05T00:49:57Z",
        "updatedAt" : "2020-05-05T16:09:41Z",
        "lastEditedBy" : "a2fcc15e-a51a-42f0-87a6-137048a28e30",
        "tags" : [
        ]
      },
      {
        "id" : "d673acda-4a8b-465b-85ea-154b3646a3ff",
        "parentId" : "c1e6e0de-f8ab-469d-9c5a-1f5b1c5b0c8a",
        "authorId" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "body" : "I don't have a strong opinion on this. seems to me comment is OK too. ",
        "createdAt" : "2020-05-05T05:31:47Z",
        "updatedAt" : "2020-05-05T16:09:41Z",
        "lastEditedBy" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "tags" : [
        ]
      }
    ],
    "commit" : "bd82fdd6d132020a72d8f1ed3f4327e3b334555b",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +43,47 @@\n```sql\n-- `spark.sql.ansi.enabled=true`\nSELECT 2147483647 + 1;\njava.lang.ArithmeticException: integer overflow"
  },
  {
    "id" : "49bc2f99-a8b5-45b4-91e9-0684c63e9abb",
    "prId" : 28451,
    "prUrl" : "https://github.com/apache/spark/pull/28451#pullrequestreview-405910876",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dd549cd8-8090-476f-9008-07c1d0be4dba",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Seems OK, is there any behavior difference?\r\nI'm on the fence about whether it's worth changing across all files.",
        "createdAt" : "2020-05-05T13:24:20Z",
        "updatedAt" : "2020-05-05T16:09:41Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "62eab6e6-baeb-44b4-abad-62b7705bb5fd",
        "parentId" : "dd549cd8-8090-476f-9008-07c1d0be4dba",
        "authorId" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "body" : "Both of them highlight SQL keywords the same way. The only difference I noticed is that ```sql doesn't indent the code block: \r\n<img width=\"503\" alt=\"Screen Shot 2020-05-05 at 8 34 07 AM\" src=\"https://user-images.githubusercontent.com/13592258/81085006-76171f00-8eab-11ea-8d3d-acb82e26fbca.png\">\r\n\r\n\r\nbut {% hightlight sql %} indents\r\n<img width=\"566\" alt=\"Screen Shot 2020-05-05 at 8 35 18 AM\" src=\"https://user-images.githubusercontent.com/13592258/81085021-7ca59680-8eab-11ea-83fb-732fb92eae24.png\">\r\n\r\n\r\n",
        "createdAt" : "2020-05-05T15:38:09Z",
        "updatedAt" : "2020-05-05T16:09:41Z",
        "lastEditedBy" : "99765afe-8dbe-4d5f-9ad3-d5bc40f0462b",
        "tags" : [
        ]
      }
    ],
    "commit" : "bd82fdd6d132020a72d8f1ed3f4327e3b334555b",
    "line" : 27,
    "diffHunk" : "@@ -1,1 +65,69 @@In future releases, the behaviour of type coercion might change along with the other two type conversion rules.\n\n```sql\n-- Examples of explicit casting\n"
  },
  {
    "id" : "a84456a0-b45e-41e8-a019-82e25a42c80b",
    "prId" : 28096,
    "prUrl" : "https://github.com/apache/spark/pull/28096#pullrequestreview-386131764",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "426648dc-fefa-466e-955a-19bd68a8f7bf",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-30125, commit ID: d9b30694122f8716d3acb448638ef1e2b96ebc7a#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-04-02T06:15:03Z",
        "updatedAt" : "2020-04-02T06:15:03Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "88d955fcb061461743575833677fbe8388af845d",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +38,42 @@    2. Spark will forbid using the reserved keywords of ANSI SQL as identifiers in the SQL parser.\n  </td>\n  <td>3.0.0</td>\n</tr>\n<tr>"
  },
  {
    "id" : "2e89b2c1-6408-43cb-8b72-bf4e304c06c1",
    "prId" : 28096,
    "prUrl" : "https://github.com/apache/spark/pull/28096#pullrequestreview-386131887",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "635c437f-2f02-4929-a34f-38b80c6d4f1b",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-28730, commit ID: 895c90b582cc2b2667241f66d5b733852aeef9eb#diff-9a6b543db706f1a90f790783d6930a13",
        "createdAt" : "2020-04-02T06:15:24Z",
        "updatedAt" : "2020-04-02T06:15:24Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "88d955fcb061461743575833677fbe8388af845d",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +54,58 @@    e.g. converting double to int or decimal to double is not allowed.\n  </td>\n  <td>3.0.0</td>\n</tr>\n</table>"
  },
  {
    "id" : "faee4f46-2f97-4db8-8b44-5c801282ad4d",
    "prId" : 27897,
    "prUrl" : "https://github.com/apache/spark/pull/27897#pullrequestreview-374796881",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "071fb409-deb5-42f9-bf97-cf8c06f8f677",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Can you update `TableIdentifierParserSuite`, too?",
        "createdAt" : "2020-03-14T06:50:29Z",
        "updatedAt" : "2020-04-07T16:02:30Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "1dddcbbd-6ba8-4d9b-aa54-932ac06011f6",
        "parentId" : "071fb409-deb5-42f9-bf97-cf8c06f8f677",
        "authorId" : "c48b13f0-9ce5-4910-a8eb-aee1ac73ffcb",
        "body" : "Sure, updated in c5570d249426a821896ab85e37bf2e0c1d9a39ee.",
        "createdAt" : "2020-03-15T12:56:29Z",
        "updatedAt" : "2020-04-07T16:02:30Z",
        "lastEditedBy" : "c48b13f0-9ce5-4910-a8eb-aee1ac73ffcb",
        "tags" : [
        ]
      }
    ],
    "commit" : "f6132c0e40e8cd20ed9620f27ad5ba0983891a3f",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +412,416 @@  <tr><td>VALUES</td><td>non-reserved</td><td>non-reserved</td><td>reserved</td></tr>\n  <tr><td>VIEW</td><td>non-reserved</td><td>non-reserved</td><td>non-reserved</td></tr>\n  <tr><td>VIEWS</td><td>non-reserved</td><td>non-reserved</td><td>non-reserved</td></tr>\n  <tr><td>WHEN</td><td>reserved</td><td>non-reserved</td><td>reserved</td></tr>\n  <tr><td>WHERE</td><td>reserved</td><td>non-reserved</td><td>reserved</td></tr>"
  },
  {
    "id" : "57703f60-ea84-45b8-a1c3-2769e970313f",
    "prId" : 27819,
    "prUrl" : "https://github.com/apache/spark/pull/27819#pullrequestreview-370007995",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a9d1ce99-eaa7-41ef-b7c5-fcf4b508533a",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "https://github.com/apache/spark/pull/27489#discussion_r380022536",
        "createdAt" : "2020-03-06T00:31:50Z",
        "updatedAt" : "2020-03-06T00:31:50Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "931a392f3806b5fb8b10dc88eaf5866601b3762e",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +61,65 @@\nIn Spark SQL, arithmetic operations performed on numeric types (with the exception of decimal) are not checked for overflows by default.\nThis means that in case an operation causes overflows, the result is the same with the corresponding operation in a Java/Scala program (e.g., if the sum of 2 integers is higher than the maximum value representable, the result is a negative number).\nOn the other hand, Spark SQL returns null for decimal overflows.\nWhen `spark.sql.ansi.enabled` is set to `true` and an overflow occurs in numeric and interval arithmetic operations, it throws an arithmetic exception at runtime."
  },
  {
    "id" : "2a2e9805-0987-46a2-b2bd-719c18529cc5",
    "prId" : 27590,
    "prUrl" : "https://github.com/apache/spark/pull/27590#pullrequestreview-359359579",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fe085640-addd-4127-b1a2-8bb4110bd899",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "What does `Experimental` mean? That is the same with the `@Experimental` annotation? Anyway, this statement is just copied from one in the SQL configuration document (https://github.com/apache/spark/pull/27459). So, instead of adding the prefix `(Experimental)` manually, I personally think its better to add this prefix automatically via `sql/gen-sql-config-docs.py`. For example, how about adding an `experimental` method in `ConfigBuilder`;\r\n```\r\n  val ANSI_ENABLED = buildConf(\"spark.sql.ansi.enabled\")\r\n    .experimental()\r\n    .doc(\"When true, Spark tries to conform to the ANSI SQL specification: 1. Spark will \" +\r\n      \"throw a runtime exception if an overflow occurs in any operation on integral/decimal \" +\r\n      \"field. 2. Spark will forbid using the reserved keywords of ANSI SQL as identifiers in \" +\r\n      \"the SQL parser.\")\r\n    .booleanConf\r\n    .createWithDefault(false)\r\n```\r\nThen, the script adds `(Experimental)` in the head of its description.",
        "createdAt" : "2020-02-15T09:08:22Z",
        "updatedAt" : "2020-02-15T09:08:22Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "74f89375-23ea-42aa-8aa9-3176f15993b8",
        "parentId" : "fe085640-addd-4127-b1a2-8bb4110bd899",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "@maropu I was following this one https://spark.apache.org/docs/latest/configuration.html\r\nI think it's a good idea to add a new method in `ConfigBuilder`, but I prefer to keep it in this way for this PR, as the configuration table here is not generated from the code.",
        "createdAt" : "2020-02-15T10:26:58Z",
        "updatedAt" : "2020-02-15T10:26:59Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "e63c6b73-cd25-4885-b9d4-1bc5e119ced5",
        "parentId" : "fe085640-addd-4127-b1a2-8bb4110bd899",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Yea, I think the fix itself in this pr looks fine. cc: @dongjoon-hyun @cloud-fan @HyukjinKwon ",
        "createdAt" : "2020-02-15T10:44:33Z",
        "updatedAt" : "2020-02-15T10:44:34Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "a897625f-c2ef-4e4b-b835-39b9ed88fb07",
        "parentId" : "fe085640-addd-4127-b1a2-8bb4110bd899",
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "In general, this changes make sense to me. If we have more experimental public conf, we should consider doing it in a proper way. \r\n\r\nRegarding the ANSI mode, we need to consider the roadmap of Spark 3.x: which are still missing and what kind of behavior changes we plan to add.",
        "createdAt" : "2020-02-15T18:32:52Z",
        "updatedAt" : "2020-02-15T18:32:52Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      }
    ],
    "commit" : "e8837f9a277743c6e321e67e54bdf297a573062e",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +34,38 @@  <td>false</td>\n  <td>\n    (Experimental) When true, Spark tries to conform to the ANSI SQL specification:\n    1. Spark will throw a runtime exception if an overflow occurs in any operation on integral/decimal field.\n    2. Spark will forbid using the reserved keywords of ANSI SQL as identifiers in the SQL parser."
  },
  {
    "id" : "7c4985f7-2800-4acf-a9ae-5cf25c09eb82",
    "prId" : 27590,
    "prUrl" : "https://github.com/apache/spark/pull/27590#pullrequestreview-359538732",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4323ed17-8ead-44b8-9e7b-2f9fcb2f8351",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "`separate` -> `separated`?",
        "createdAt" : "2020-02-17T07:49:59Z",
        "updatedAt" : "2020-02-17T07:49:59Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "cf77f889-2824-49a4-9d96-619d8d229b10",
        "parentId" : "4323ed17-8ead-44b8-9e7b-2f9fcb2f8351",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "Separate can be an adjective",
        "createdAt" : "2020-02-17T07:51:13Z",
        "updatedAt" : "2020-02-17T07:51:14Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "e8837f9a277743c6e321e67e54bdf297a573062e",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +26,30 @@The casting behaviours are defined as store assignment rules in the standard.\n\nWhen `spark.sql.storeAssignmentPolicy` is set to `ANSI`, Spark SQL complies with the ANSI store assignment rules. This is a separate configuration because its default value is `ANSI`, while the configuration `spark.sql.ansi.enabled` is disabled by default.\n\n<table class=\"table\">"
  },
  {
    "id" : "27cb383c-fefb-4e57-800b-6d5b717e0cac",
    "prId" : 27489,
    "prUrl" : "https://github.com/apache/spark/pull/27489#pullrequestreview-355587124",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1d3bd096-cc73-4bee-9f32-11c0f351da95",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Just a question, shall we mention `CTAS(CREATE TABLE AS SELECT)` together?",
        "createdAt" : "2020-02-08T22:45:28Z",
        "updatedAt" : "2020-02-12T22:55:03Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "198af3cc-522f-44f3-a4c7-dcc059d1cb22",
        "parentId" : "1d3bd096-cc73-4bee-9f32-11c0f351da95",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Ah, good suggestion! I totally missed that syntax. I'll check.",
        "createdAt" : "2020-02-09T00:40:17Z",
        "updatedAt" : "2020-02-12T22:55:03Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "5ab2f042-a923-46a9-b9c6-f2f172c33908",
        "parentId" : "1d3bd096-cc73-4bee-9f32-11c0f351da95",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "I've checked CTAS (CTAS cannot exist with schema definition);\r\n```\r\nscala> sql(\"create table t1 (v string)\")\r\nscala> sql(\"create table t2 (v int) as select * from t1\")\r\norg.apache.spark.sql.catalyst.parser.ParseException:\r\nOperation not allowed: Schema may not be specified in a Create Table As Select (CTAS) statement(line 1, pos 0)\r\n```\r\nAny other concern?",
        "createdAt" : "2020-02-09T06:44:41Z",
        "updatedAt" : "2020-02-12T22:55:03Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "64b42b4cd919a15d7c3f85b3527a1a916e472558",
    "line" : 78,
    "diffHunk" : "@@ -1,1 +84,88 @@Spark SQL has three kinds of type conversions: explicit casting, type coercion, and store assignment casting.\nWhen `spark.sql.ansi.enabled` is set to `true`, explicit casting by `CAST` syntax throws a runtime exception for illegal cast patterns defined in the standard, e.g. casts from a string to an integer.\nOn the other hand, `INSERT INTO` syntax throws an analysis exception when the ANSI mode enabled via `spark.sql.storeAssignmentPolicy=ANSI`.\n\nCurrently, the ANSI mode affects explicit casting and assignment casting only."
  },
  {
    "id" : "9b1ced09-7655-4e8a-aa16-2e101bdce8b9",
    "prId" : 27489,
    "prUrl" : "https://github.com/apache/spark/pull/27489#pullrequestreview-370007511",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2993be6e-2aaf-4c64-91e5-eb1ebaad6fa9",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "`the result is the same that` -> `the result is the same with`",
        "createdAt" : "2020-02-17T07:41:56Z",
        "updatedAt" : "2020-02-17T07:49:10Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "c4cb2ad8-ea5b-41ad-af8e-6f5b3b75e50f",
        "parentId" : "2993be6e-2aaf-4c64-91e5-eb1ebaad6fa9",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Ah, yes. I'll fix later. (If other developers plan to open a PR for typo fixes, it would be helpful to include this fix as well.)",
        "createdAt" : "2020-02-17T08:13:53Z",
        "updatedAt" : "2020-02-17T08:13:53Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "24fd8019-53d7-4115-bb0a-e483e8150b78",
        "parentId" : "2993be6e-2aaf-4c64-91e5-eb1ebaad6fa9",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "https://github.com/apache/spark/pull/27819",
        "createdAt" : "2020-03-06T00:30:27Z",
        "updatedAt" : "2020-03-06T00:30:28Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "64b42b4cd919a15d7c3f85b3527a1a916e472558",
    "line" : 53,
    "diffHunk" : "@@ -1,1 +59,63 @@\nIn Spark SQL, arithmetic operations performed on numeric types (with the exception of decimal) are not checked for overflows by default.\nThis means that in case an operation causes overflows, the result is the same that the same operation returns in a Java/Scala program (e.g., if the sum of 2 integers is higher than the maximum value representable, the result is a negative number).\nOn the other hand, Spark SQL returns null for decimal overflows.\nWhen `spark.sql.ansi.enabled` is set to `true` and an overflow occurs in numeric and interval arithmetic operations, it throws an arithmetic exception at runtime."
  },
  {
    "id" : "38d0ec16-7bb4-451a-bfa1-cd1e0e5561dc",
    "prId" : 27489,
    "prUrl" : "https://github.com/apache/spark/pull/27489#pullrequestreview-359535019",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "da101da8-1d20-4c04-8042-6a0c59e3cfa0",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I'm wondering if we should remove the STRICT mode. It's not ANSI compliant and no other SQL system has this behavior.\r\n\r\ncc @rdblue @brkyvz @rxin ",
        "createdAt" : "2020-02-17T07:44:08Z",
        "updatedAt" : "2020-02-17T07:49:11Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "64b42b4cd919a15d7c3f85b3527a1a916e472558",
    "line" : 42,
    "diffHunk" : "@@ -1,1 +48,52 @@    e.g. converting string to int or double to boolean is allowed.\n    It is also the only behavior in Spark 2.x and it is compatible with Hive.\n    With strict policy, Spark doesn't allow any possible precision loss or data truncation in type coercion,\n    e.g. converting double to int or decimal to double is not allowed.\n  </td>"
  }
]