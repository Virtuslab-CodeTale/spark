[
  {
    "id" : "dea1efdd-529f-4997-b3be-29d1f7926ee8",
    "prId" : 31551,
    "prUrl" : "https://github.com/apache/spark/pull/31551#pullrequestreview-590828503",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "861248c1-67cb-4035-8b40-f5dfb0ce2852",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@Nozziel, I think we should better just mention both configurations `spark.driver.extraJavaOptions` and `spark.executor.extraJavaOptions` because there are many other ways to set the configurations, for example, Spark submit script, Spark configuration file, etc. In addition, Arrow exposes a set of developer APIs that are not specific to PySpark side.\r\n\r\nAlso, it would be great if we file a JIRA, and fix the PR title accordingly. See also http://spark.apache.org/contributing.html",
        "createdAt" : "2021-02-13T04:41:26Z",
        "updatedAt" : "2021-02-13T04:41:26Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "c6ef0fc2-0c69-4df6-8520-8c06da0e2b33",
        "parentId" : "861248c1-67cb-4035-8b40-f5dfb0ce2852",
        "authorId" : "b0812097-156e-4c61-ab51-d97edfabfaa8",
        "body" : "updated with jira link. \r\nThe pyspark example is intended for those that rarely ever change the config...\r\nThose that would not think of using 'spark.driver.extraJavaOptions' in the first place... \r\nAnd should be clear enough now for everybody to adapt it to their prefered way to set this config setting.\r\n\r\nAnd hopeful this underlying issue gets resolved soon making this all moot. But till then I hope to save a few people a bit of time.",
        "createdAt" : "2021-02-13T10:03:46Z",
        "updatedAt" : "2021-02-13T10:03:46Z",
        "lastEditedBy" : "b0812097-156e-4c61-ab51-d97edfabfaa8",
        "tags" : [
        ]
      },
      {
        "id" : "e80e8055-75e6-4733-82d1-24542b43213d",
        "parentId" : "861248c1-67cb-4035-8b40-f5dfb0ce2852",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I would prefer to just explain the configuration keys alone given that there are multiple ways of setting it. Also, I think we should set `spark.executor.extraJavaOptions`. I suspect setting `spark.driver.extraJavaOptions` will only work with toPandas and createDataFrame with Arrow optimization (?).",
        "createdAt" : "2021-02-16T01:24:23Z",
        "updatedAt" : "2021-02-16T01:24:23Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "c79468f977249ab9558139bafb31ab19eef55fe3",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +56,60 @@```python\n# PySpark\nSparkSession.builder.config('spark.driver.extraJavaOptions', '-Dio.netty.tryReflectionSetAccessible=true').getOrCreate()\n```\n"
  }
]