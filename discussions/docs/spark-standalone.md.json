[
  {
    "id" : "7d0c8c77-9918-48e3-8d94-5534b2795bb8",
    "prId" : 28958,
    "prUrl" : "https://github.com/apache/spark/pull/28958#pullrequestreview-440188191",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b67f5b16-7929-4855-9bfa-06981a482405",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Ur, this looks like not a typo, isn't it? So, is this about changing to CamelCase, `resourcesfile` -> `resourcesFile`, only? Is Apache Spark case sensitive?",
        "createdAt" : "2020-06-30T15:56:50Z",
        "updatedAt" : "2020-06-30T15:56:50Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "3f179008-ac59-4a88-9f2d-46618f99fa96",
        "parentId" : "b67f5b16-7929-4855-9bfa-06981a482405",
        "authorId" : "3a227965-84e0-47cf-9974-11293764f028",
        "body" : "I think it's not just about notation. Property names in Apache Spark should be case sensitive.",
        "createdAt" : "2020-06-30T16:10:52Z",
        "updatedAt" : "2020-06-30T16:10:52Z",
        "lastEditedBy" : "3a227965-84e0-47cf-9974-11293764f028",
        "tags" : [
        ]
      },
      {
        "id" : "f421debf-0df3-4d18-9db2-83f8903eb4fe",
        "parentId" : "b67f5b16-7929-4855-9bfa-06981a482405",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Got it. Thanks.",
        "createdAt" : "2020-06-30T16:27:43Z",
        "updatedAt" : "2020-06-30T16:27:43Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "d7d6d25177fefbdbb825c3dc9916b57faea6b395",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +360,364 @@The user must configure the Workers to have a set of resources available so that it can assign them out to Executors. The <code>spark.worker.resource.{resourceName}.amount</code> is used to control the amount of each resource the worker has allocated. The user must also specify either <code>spark.worker.resourcesFile</code> or <code>spark.worker.resource.{resourceName}.discoveryScript</code> to specify how the Worker discovers the resources its assigned. See the descriptions above for each of those to see which method works best for your setup.\n\nThe second part is running an application on Spark Standalone. The only special case from the standard Spark resource configs is when you are running the Driver in client mode. For a Driver in client mode, the user can specify the resources it uses via <code>spark.driver.resourcesFile</code> or <code>spark.driver.resource.{resourceName}.discoveryScript</code>. If the Driver is running on the same host as other Drivers, please make sure the resources file or discovery script only returns resources that do not conflict with other Drivers running on the same node.\n\nNote, the user does not need to specify a discovery script when submitting an application as the Worker will start each Executor with the resources it allocates to it."
  }
]