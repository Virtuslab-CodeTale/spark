[
  {
    "id" : "6a630fcc-ce5c-48cd-9ba3-5d19e3877e21",
    "prId" : 32546,
    "prUrl" : "https://github.com/apache/spark/pull/32546#pullrequestreview-664207370",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bd3d1b1c-0d33-46cb-be68-bf77a0d79a9d",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "If you're going to link generic options here, let's don't mention generic options in the API documentation in the codes as it duplicates.",
        "createdAt" : "2021-05-20T10:25:32Z",
        "updatedAt" : "2021-05-20T10:25:32Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "043d3087c838c2f6439d798bdc8e889d6a728ff7",
    "line" : 29,
    "diffHunk" : "@@ -1,1 +198,202 @@  </tr>\n</table>\nOther generic options can be found in <a href=\"https://spark.apache.org/docs/latest/sql-data-sources-generic-options.html\"> Generic File Source Options</a>."
  },
  {
    "id" : "87a7b940-a831-4496-a6a9-a9eba9b716e0",
    "prId" : 32546,
    "prUrl" : "https://github.com/apache/spark/pull/32546#pullrequestreview-665077251",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "99d97d42-357f-4141-905a-b53f53e6e386",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Although I know that this is inherited, `https://spark.apache.org/docs/latest/` looks fragile to me because it is going to be a broken link when we cut `branch-3.2` on July 1st. In `branch-3.2`, it should point `3.2` document only. Shall we use a relative link instead of `/latest/`?\r\n\r\nLike this PR, we don't know what refactoring happens in the future.",
        "createdAt" : "2021-05-21T03:45:43Z",
        "updatedAt" : "2021-05-21T03:47:06Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "9ca9dd44-31fe-4e02-92fc-cb66b6c6d1b3",
        "parentId" : "99d97d42-357f-4141-905a-b53f53e6e386",
        "authorId" : "2d8ced12-ada1-43e1-9240-bf1c11a01e4c",
        "body" : "Thanks, @dongjoon-hyun .\r\nI took a look for that but seems tricky to create a link for each release in Scaladoc ..\r\nI created a JIRA to track it separately here: SPARK-35481.\r\nI will take a separate look if that's fine to you too!",
        "createdAt" : "2021-05-21T04:23:25Z",
        "updatedAt" : "2021-05-21T04:23:25Z",
        "lastEditedBy" : "2d8ced12-ada1-43e1-9240-bf1c11a01e4c",
        "tags" : [
        ]
      }
    ],
    "commit" : "043d3087c838c2f6439d798bdc8e889d6a728ff7",
    "line" : 29,
    "diffHunk" : "@@ -1,1 +198,202 @@  </tr>\n</table>\nOther generic options can be found in <a href=\"https://spark.apache.org/docs/latest/sql-data-sources-generic-options.html\"> Generic File Source Options</a>."
  },
  {
    "id" : "12ceae62-ec85-4396-a4fc-62985c7d9da1",
    "prId" : 32546,
    "prUrl" : "https://github.com/apache/spark/pull/32546#pullrequestreview-667684686",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9ef7248e-eab4-4308-a8c2-d4185e7a044b",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@itholic it has the same issue. The default value isn't `None` but `false`.",
        "createdAt" : "2021-05-25T11:00:18Z",
        "updatedAt" : "2021-05-25T11:00:18Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "043d3087c838c2f6439d798bdc8e889d6a728ff7",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +187,191 @@  <tr>\n    <td><code>mergeSchema</code></td>\n    <td>None</td>\n    <td>sets whether we should merge schemas collected from all ORC part-files. This will override <code>spark.sql.orc.mergeSchema</code>. The default value is specified in <code>spark.sql.orc.mergeSchema</code>.</td>\n    <td>read</td>"
  },
  {
    "id" : "b8822178-12c0-44c6-8d27-e9894b6c7c5f",
    "prId" : 32546,
    "prUrl" : "https://github.com/apache/spark/pull/32546#pullrequestreview-667685599",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "66b76732-2f25-43a4-9c67-abc7be2dfb45",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "In this case when the default value doesn't exist, you can follow https://spark.apache.org/docs/latest/configuration.html#runtime-sql-configuration `(none)`.",
        "createdAt" : "2021-05-25T11:01:21Z",
        "updatedAt" : "2021-05-25T11:01:21Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "043d3087c838c2f6439d798bdc8e889d6a728ff7",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +193,197 @@  <tr>\n    <td><code>compression</code></td>\n    <td>None</td>\n    <td>compression codec to use when saving to file. This can be one of the known case-insensitive shorten names (none, snappy, zlib, lzo, and zstd). This will override <code>orc.compress</code> and <code>spark.sql.orc.compression.codec</code>. If None is set, it uses the value specified in <code>spark.sql.orc.compression.codec</code>.</td>\n    <td>write</td>"
  },
  {
    "id" : "d65c511a-5dd7-466e-9e3a-fa5313c75e70",
    "prId" : 32546,
    "prUrl" : "https://github.com/apache/spark/pull/32546#pullrequestreview-667692656",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "814c080a-657a-4ca0-aa9d-e00bfa50f1d0",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "also mention:\r\n\r\n```\r\n* `OPTIONS` clause at [CREATE TABLE USING DATA_SOURCE](sql-ref-syntax-ddl-create-table-datasource.html)\r\n```",
        "createdAt" : "2021-05-25T11:09:24Z",
        "updatedAt" : "2021-05-25T11:09:34Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "043d3087c838c2f6439d798bdc8e889d6a728ff7",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +181,185 @@  *  `DataFrameWriter`\n  *  `DataStreamReader`\n  *  `DataStreamWriter`\n\n<table class=\"table\">"
  },
  {
    "id" : "6b31d216-856a-41b0-b0d4-3806667ebe06",
    "prId" : 28064,
    "prUrl" : "https://github.com/apache/spark/pull/28064#pullrequestreview-383364895",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "19ed746f-fa17-45e5-a9c7-e700695345eb",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-20728, commit ID:  326f1d6728a7734c228d8bfaa69442a1c7b92e9b#diff-9a6b543db706f1a90f790783d6930a13\r\n\r\n",
        "createdAt" : "2020-03-29T01:42:26Z",
        "updatedAt" : "2020-03-29T01:42:26Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "6787d16f5e86e5a6dbcb2d9b25c41ec5dfbedb1d",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +37,41 @@      in Hive.\n    </td>\n    <td>2.3.0</td>\n  </tr>\n  <tr>"
  },
  {
    "id" : "272ab26d-233a-4c89-b2e5-4d900a39b260",
    "prId" : 28064,
    "prUrl" : "https://github.com/apache/spark/pull/28064#pullrequestreview-383364921",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3764fb8d-de4d-490c-bb22-902208d4dc00",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-16060, commit ID: 60f6b994505e3f82091a04eed2dc0a9e8bd523ce#diff-9a6b543db706f1a90f790783d6930a13\r\n",
        "createdAt" : "2020-03-29T01:42:56Z",
        "updatedAt" : "2020-03-29T01:42:57Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "6787d16f5e86e5a6dbcb2d9b25c41ec5dfbedb1d",
    "line" : 26,
    "diffHunk" : "@@ -1,1 +47,51 @@      For <code>hive</code> implementation, this is ignored.\n    </td>\n    <td>2.3.0</td>\n  </tr>\n</table>"
  },
  {
    "id" : "0959f0c8-0a0f-4fe1-89d2-9616041511e4",
    "prId" : 26146,
    "prUrl" : "https://github.com/apache/spark/pull/26146#pullrequestreview-304488052",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "10787473-8d13-4922-8730-96670727a5a1",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Can you fix `spark.sql.orc.impl` at `SQLConf.scala` too?",
        "createdAt" : "2019-10-21T11:16:18Z",
        "updatedAt" : "2019-10-22T02:19:09Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "c5ddd1eba1afdd33c050818e9a1ff1a69ed8f71b",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +32,36 @@    <td><code>spark.sql.orc.impl</code></td>\n    <td><code>native</code></td>\n    <td>The name of ORC implementation. It can be one of <code>native</code> and <code>hive</code>. <code>native</code> means the native ORC support. <code>hive</code> means the ORC library in Hive.</td>\n  </tr>\n  <tr>"
  }
]