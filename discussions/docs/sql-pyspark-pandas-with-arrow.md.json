[
  {
    "id" : "627ab128-d63b-4d7b-ae1c-8c1ab8eb5bb1",
    "prId" : 28160,
    "prUrl" : "https://github.com/apache/spark/pull/28160#pullrequestreview-390510252",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4adbbe43-08cc-4b1d-a064-7165b611fd40",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I piggy-backed some doc changes here while I am here.",
        "createdAt" : "2020-04-09T06:01:19Z",
        "updatedAt" : "2020-04-09T06:01:20Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "91239270c97579960b3deb3ade66872667899a19",
    "line" : 67,
    "diffHunk" : "@@ -1,1 +207,211 @@API behaves as a regular API under PySpark `DataFrame` instead of `Column`, and Python type hints in Pandas\nFunctions APIs are optional and do not affect how it works internally at this moment although they\nmight be required in the future.\n\nFrom Spark 3.0, grouped map pandas UDF is now categorized as a separate Pandas Function API,"
  },
  {
    "id" : "39a444e7-308a-44ff-882a-d04705ca86a5",
    "prId" : 27587,
    "prUrl" : "https://github.com/apache/spark/pull/27587#pullrequestreview-359339394",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dec622ac-2779-4bcf-a924-ed02ae5eab2b",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Bryan, we should maybe fix 0.15+ to just 0.15.1 at \"Ensure PyArrow Installed\".",
        "createdAt" : "2020-02-15T10:46:48Z",
        "updatedAt" : "2020-02-17T02:05:16Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "d473ea486c2bd2a3789dd701deebb345c0743082",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +343,347 @@### Recommended Pandas and PyArrow Versions\n\nFor usage with pyspark.sql, the supported versions of Pandas is 0.24.2 and PyArrow is 0.15.1. Higher\nversions may be used, however, compatibility and data correctness can not be guaranteed and should\nbe verified by the user."
  },
  {
    "id" : "fb7ac796-6a81-49fb-916d-33ccc0872a4d",
    "prId" : 27586,
    "prUrl" : "https://github.com/apache/spark/pull/27586#pullrequestreview-359338953",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "33283f88-37fc-49b5-a9d8-d04d107cce61",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "BTW, I forgot the exact versions of the `branch-2.4` PR builders in AmpLab Jenkins. These are the community testing version. Did I understand correctly?",
        "createdAt" : "2020-02-14T21:23:22Z",
        "updatedAt" : "2020-02-17T02:04:36Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "0faf312b-4655-4e76-b643-1f99c1cfed54",
        "parentId" : "33283f88-37fc-49b5-a9d8-d04d107cce61",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Yeah, i believe so. I double checked - https://github.com/apache/spark/pull/27529#issuecomment-586056059",
        "createdAt" : "2020-02-15T10:33:39Z",
        "updatedAt" : "2020-02-17T02:04:36Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "9459349432d42d0f8d892473af330a4a30a20fce",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +171,175 @@### Recommended Pandas and PyArrow Versions\n\nFor usage with pyspark.sql, the supported versions of Pandas is 0.19.2 and PyArrow is 0.8.0. Higher\nversions may be used, however, compatibility and data correctness can not be guaranteed and should\nbe verified by the user."
  },
  {
    "id" : "0bc7a113-305b-4b83-8dfd-2f538e7bb4f6",
    "prId" : 27466,
    "prUrl" : "https://github.com/apache/spark/pull/27466#pullrequestreview-353661389",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d55c7870-15bd-4d81-ab01-edf558af6344",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Please see also the PR description of https://github.com/apache/spark/pull/27165#issue-361321870",
        "createdAt" : "2020-02-05T11:58:13Z",
        "updatedAt" : "2020-02-11T08:35:36Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "626ff3cb245ccd305f9a32d9c99f774ef154ebe1",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +64,68 @@Spark will fall back to create the DataFrame without Arrow.\n\n## Pandas UDFs (a.k.a. Vectorized UDFs)\n\nPandas UDFs are user defined functions that are executed by Spark using"
  },
  {
    "id" : "bb1cca66-299a-4bf0-93fa-4c5b6ed7e62c",
    "prId" : 27466,
    "prUrl" : "https://github.com/apache/spark/pull/27466#pullrequestreview-353692581",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b0e51b48-4413-4641-ad0e-52d37bee96d8",
        "parentId" : null,
        "authorId" : "981b170c-729a-429c-b115-0350ea50b32b",
        "body" : "Could we maybe link to [PEP 484](https://www.python.org/dev/peps/pep-0484/) here? ",
        "createdAt" : "2020-02-05T12:55:10Z",
        "updatedAt" : "2020-02-11T08:35:36Z",
        "lastEditedBy" : "981b170c-729a-429c-b115-0350ea50b32b",
        "tags" : [
        ]
      }
    ],
    "commit" : "626ff3cb245ccd305f9a32d9c99f774ef154ebe1",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +71,75 @@configuration is required. A Pandas UDF behaves as a regular PySpark function API in general.\n\nBefore Spark 3.0, Pandas UDFs used to be defined with `PandasUDFType`. From Spark 3.0\nwith Python 3.6+, you can also use [Python type hints](https://www.python.org/dev/peps/pep-0484).\nUsing Python type hints are preferred and using `PandasUDFType` will be deprecated in"
  },
  {
    "id" : "7e1c0a56-12ed-4639-800b-387ac9c7a81e",
    "prId" : 27466,
    "prUrl" : "https://github.com/apache/spark/pull/27466#pullrequestreview-354169089",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "67a9f219-e330-4808-8fe4-7756f8afce95",
        "parentId" : null,
        "authorId" : "981b170c-729a-429c-b115-0350ea50b32b",
        "body" : "Nitpick. It is more `Iterator[Union[Tuple[pandas.Series, ...],  pandas.Series]]` -> `Iterator[pandas.Series]`, isn't it? But I guess that's too much...",
        "createdAt" : "2020-02-05T13:15:21Z",
        "updatedAt" : "2020-02-11T08:35:36Z",
        "lastEditedBy" : "981b170c-729a-429c-b115-0350ea50b32b",
        "tags" : [
        ]
      },
      {
        "id" : "52b061eb-ac32-4a7f-9c7b-e44f6799f300",
        "parentId" : "67a9f219-e330-4808-8fe4-7756f8afce95",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "True .. although I didn't add `Iterator[Union[Tuple[pandas.Series, ...], pandas.Series]]` type hint to support yet ...\r\n\r\nI think we can combine it later if we happen to add this type hint as well. Shouldn't be a big deal at this moment.",
        "createdAt" : "2020-02-06T02:33:28Z",
        "updatedAt" : "2020-02-11T08:35:36Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "626ff3cb245ccd305f9a32d9c99f774ef154ebe1",
    "line" : 75,
    "diffHunk" : "@@ -1,1 +115,119 @@### Iterator of Series to Iterator of Series\n\nThe type hint can be expressed as `Iterator[pandas.Series]` -> `Iterator[pandas.Series]`.\n\nBy using `pandas_udf` with the function having such type hints, it creates a Pandas UDF where the given"
  },
  {
    "id" : "eddc3a5f-260f-4f3c-8c51-7ccbce66ac98",
    "prId" : 27466,
    "prUrl" : "https://github.com/apache/spark/pull/27466#pullrequestreview-354226259",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f90c9ad7-5241-4f84-a91e-5abb9d4ab5cc",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Is it Python 3.6 or 3.5? In the pep 484 page, it claims Python 3.5.",
        "createdAt" : "2020-02-06T06:27:04Z",
        "updatedAt" : "2020-02-11T08:35:36Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "1394490f-a882-4dfb-8818-e190ae2c27c0",
        "parentId" : "f90c9ad7-5241-4f84-a91e-5abb9d4ab5cc",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I am pretty sure we can support Python 3.5 too although maybe it needs some trivial fixes, in particular, [here](https://github.com/apache/spark/blob/master/python/pyspark/sql/pandas/typehints.py#L120-L141) but I made [the explicit condition with Python 3.6+](https://github.com/apache/spark/blob/master/python/pyspark/sql/pandas/functions.py#L440-L441)\r\n\r\nI just excluded Python 3.5 because we deprecated Python versions lower than 3.6, and I just wanted to make it simple.",
        "createdAt" : "2020-02-06T06:37:14Z",
        "updatedAt" : "2020-02-11T08:35:36Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "626ff3cb245ccd305f9a32d9c99f774ef154ebe1",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +72,76 @@\nBefore Spark 3.0, Pandas UDFs used to be defined with `PandasUDFType`. From Spark 3.0\nwith Python 3.6+, you can also use [Python type hints](https://www.python.org/dev/peps/pep-0484).\nUsing Python type hints are preferred and using `PandasUDFType` will be deprecated in\nthe future release."
  },
  {
    "id" : "e3884cb9-f2ca-4300-a605-e7dcf400e0ed",
    "prId" : 27466,
    "prUrl" : "https://github.com/apache/spark/pull/27466#pullrequestreview-354901190",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d0d3038a-c703-49ca-994f-5aafd94046d7",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@cloud-fan what about now?\r\n\r\n![Screen Shot 2020-02-07 at 11 35 48 AM](https://user-images.githubusercontent.com/6477701/73996130-1e494480-499e-11ea-8353-2474ef88cf40.png)\r\n",
        "createdAt" : "2020-02-07T02:36:55Z",
        "updatedAt" : "2020-02-11T08:35:36Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "626ff3cb245ccd305f9a32d9c99f774ef154ebe1",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +64,68 @@Spark will fall back to create the DataFrame without Arrow.\n\n## Pandas UDFs (a.k.a. Vectorized UDFs)\n\nPandas UDFs are user defined functions that are executed by Spark using"
  },
  {
    "id" : "9cb6459e-c9b4-4918-9b36-7b0d779b33a7",
    "prId" : 27383,
    "prUrl" : "https://github.com/apache/spark/pull/27383#pullrequestreview-350435176",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "599ecbb6-9e8f-4dce-9d26-fd474b118b68",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Shall we not mention EOL versions (2.3.x)? We need to discourage it. Or, at least, we should not advertise it.",
        "createdAt" : "2020-01-29T20:00:27Z",
        "updatedAt" : "2020-01-29T20:00:28Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "6c056ab8-0ac8-4d11-8295-5efd76115e32",
        "parentId" : "599ecbb6-9e8f-4dce-9d26-fd474b118b68",
        "authorId" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "body" : "This is mostly for people that are stuck on an existing Spark version and upgrade pyarrow somehow. They would end up seeing some crazy errors that aren't obvious. Hopefully if they see this, they can fix it instead of making a JIRA, etc. I don't think this is advertising to use an old Spark version, but If you think it's better to remove, I can.",
        "createdAt" : "2020-01-29T21:25:57Z",
        "updatedAt" : "2020-01-29T21:25:57Z",
        "lastEditedBy" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "tags" : [
        ]
      }
    ],
    "commit" : "a0cb75d25b182b1949c6399506794497ecc9b0bd",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +167,171 @@[here](https://pandas.pydata.org/pandas-docs/stable/timeseries.html) for details.\n\n### Compatibiliy Setting for PyArrow >= 0.15.0 and Spark 2.3.x, 2.4.x\n\nSince Arrow 0.15.0, a change in the binary IPC format requires an environment variable to be"
  },
  {
    "id" : "9f065064-968e-41f4-b26b-7b42704041eb",
    "prId" : 27383,
    "prUrl" : "https://github.com/apache/spark/pull/27383#pullrequestreview-350431997",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6dbcde23-a765-46b5-8ee0-53e6c9ddabf9",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Is this the only setting we need?",
        "createdAt" : "2020-01-29T20:02:36Z",
        "updatedAt" : "2020-01-29T20:02:37Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "abef6ff2-cae3-44cf-9fcb-2401c905973d",
        "parentId" : "6dbcde23-a765-46b5-8ee0-53e6c9ddabf9",
        "authorId" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "body" : "Yes",
        "createdAt" : "2020-01-29T21:20:39Z",
        "updatedAt" : "2020-01-29T21:20:39Z",
        "lastEditedBy" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "tags" : [
        ]
      }
    ],
    "commit" : "a0cb75d25b182b1949c6399506794497ecc9b0bd",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +175,179 @@\n```\nARROW_PRE_0_15_IPC_FORMAT=1\n```\n"
  },
  {
    "id" : "e0738ba5-ea82-47b0-8150-db2b1ac60868",
    "prId" : 27383,
    "prUrl" : "https://github.com/apache/spark/pull/27383#pullrequestreview-350519247",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "77ef4fe7-3ded-4873-8e07-19610a86e499",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Yeah, I am not very fond of mentioning 2.3.x either but I guess it's fine.",
        "createdAt" : "2020-01-30T00:35:29Z",
        "updatedAt" : "2020-01-30T00:35:30Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "a0cb75d25b182b1949c6399506794497ecc9b0bd",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +179,183 @@\nThis will instruct PyArrow >= 0.15.0 to use the legacy IPC format with the older Arrow Java that\nis in Spark 2.3.x and 2.4.x. Not setting this environment variable will lead to a similar error as\ndescribed in [SPARK-29367](https://issues.apache.org/jira/browse/SPARK-29367) when running\n`pandas_udf`s or `toPandas()` with Arrow enabled. More information about the Arrow IPC change can"
  },
  {
    "id" : "f920d32f-19bf-46ad-b1f3-133c53da24eb",
    "prId" : 26045,
    "prUrl" : "https://github.com/apache/spark/pull/26045#pullrequestreview-299693327",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b0473b14-23e6-44bc-81d6-dff48eadb19b",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Oh, actually @BryanCutler, would you mind adding this (or link back) at https://github.com/apache/spark/blob/master/docs/sparkr.md#apache-arrow-in-sparkr ? No worry about testing it out, I will do it tonight.",
        "createdAt" : "2019-10-07T23:49:09Z",
        "updatedAt" : "2019-10-09T20:54:58Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "8a214173-be25-4eb9-8858-7e588512bb2c",
        "parentId" : "b0473b14-23e6-44bc-81d6-dff48eadb19b",
        "authorId" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "body" : "@HyukjinKwon so you don't need this note for R, Arrow was not used in 2.4.x?",
        "createdAt" : "2019-10-09T20:57:51Z",
        "updatedAt" : "2019-10-09T20:57:52Z",
        "lastEditedBy" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "tags" : [
        ]
      }
    ],
    "commit" : "82d5f5420e365b9f407dfc9f5cf2f8ac9406bc78",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +221,225 @@[here](https://pandas.pydata.org/pandas-docs/stable/timeseries.html) for details.\n\n### Compatibiliy Setting for PyArrow >= 0.15.0 and Spark 2.3.x, 2.4.x\n\nSince Arrow 0.15.0, a change in the binary IPC format requires an environment variable to be"
  },
  {
    "id" : "65b09143-803f-45e4-85ee-070f3c7f3806",
    "prId" : 26045,
    "prUrl" : "https://github.com/apache/spark/pull/26045#pullrequestreview-299673098",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "88a555b0-bea5-41a1-a9a8-89522a797f77",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "Can we just set it by default?",
        "createdAt" : "2019-10-08T00:00:56Z",
        "updatedAt" : "2019-10-09T20:54:58Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "328ad0fa-5cb3-4e11-b0da-bd7342577992",
        "parentId" : "88a555b0-bea5-41a1-a9a8-89522a797f77",
        "authorId" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "body" : "This is for already released Spark versions, where the user has upgraded pyarrow to 0.15.0 in their cluster",
        "createdAt" : "2019-10-09T00:50:25Z",
        "updatedAt" : "2019-10-09T20:54:59Z",
        "lastEditedBy" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "tags" : [
        ]
      },
      {
        "id" : "01015edb-b39b-4238-ab2a-37b04ce14944",
        "parentId" : "88a555b0-bea5-41a1-a9a8-89522a797f77",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "I think I'd just clarify this in the new docs, that it's only needed if you manually update pyarrow this way.",
        "createdAt" : "2019-10-09T01:04:12Z",
        "updatedAt" : "2019-10-09T20:54:59Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "0843229a-9753-42a0-9801-39ddd1b9da86",
        "parentId" : "88a555b0-bea5-41a1-a9a8-89522a797f77",
        "authorId" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "body" : "Yes I agree, I'll clarify this",
        "createdAt" : "2019-10-09T20:21:13Z",
        "updatedAt" : "2019-10-09T20:54:59Z",
        "lastEditedBy" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "tags" : [
        ]
      }
    ],
    "commit" : "82d5f5420e365b9f407dfc9f5cf2f8ac9406bc78",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +229,233 @@\n```\nARROW_PRE_0_15_IPC_FORMAT=1\n```\n"
  },
  {
    "id" : "df4e7eee-487a-4ea5-8138-fba071025a3c",
    "prId" : 24897,
    "prUrl" : "https://github.com/apache/spark/pull/24897#pullrequestreview-250843193",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d0024e22-cb19-4de9-9a1f-d1209706049b",
        "parentId" : null,
        "authorId" : "7e665d8b-d739-4edf-88c8-7379ff8585c2",
        "body" : "Add a note that the UDF can also return an iterator (instead of write a function with yield), and the input iterator is allowed to be prefetched.",
        "createdAt" : "2019-06-18T02:53:26Z",
        "updatedAt" : "2019-06-18T03:22:59Z",
        "lastEditedBy" : "7e665d8b-d739-4edf-88c8-7379ff8585c2",
        "tags" : [
        ]
      },
      {
        "id" : "fdbc06c8-967a-4ee5-942a-3fc52b4ddc93",
        "parentId" : "d0024e22-cb19-4de9-9a1f-d1209706049b",
        "authorId" : "0c812942-02cb-4975-9748-394d1387affa",
        "body" : "Updated.",
        "createdAt" : "2019-06-18T03:23:56Z",
        "updatedAt" : "2019-06-18T03:23:56Z",
        "lastEditedBy" : "0c812942-02cb-4975-9748-394d1387affa",
        "tags" : [
        ]
      }
    ],
    "commit" : "eacb5e49911f85f93934f9de5ae1895ab2444b7c",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +95,99 @@It is useful when the UDF execution requires initializing some states, e.g., loading an machine\nlearning model file to apply inference to every input batch.\n\nThe following example shows how to create scalar iterator Pandas UDFs:\n"
  }
]