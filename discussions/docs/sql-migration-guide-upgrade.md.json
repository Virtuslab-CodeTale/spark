[
  {
    "id" : "f54621c4-5b50-4c88-be5d-bae7dbb230b4",
    "prId" : 25431,
    "prUrl" : "https://github.com/apache/spark/pull/25431#pullrequestreview-274688191",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "aeac4bf7-1197-4578-8ac2-58b8f6f40b89",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "This seems fairly incomplete to a reader. What database? \r\nhive -> Hive. This also needs to clarify that the Hive client was updated",
        "createdAt" : "2019-08-13T14:29:06Z",
        "updatedAt" : "2019-08-13T14:29:06Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "5c6829de-f5c9-4e8a-8a19-5f9b781ca1ff",
        "parentId" : "aeac4bf7-1197-4578-8ac2-58b8f6f40b89",
        "authorId" : "f2e3e1f3-82a3-4b68-a0c2-2f7fdf58cf35",
        "body" : "Hive maintains the `SCHEMA_VERSION` in the `VERSION` table of the metastore db(this  database name will be configured by the user in the hive-site.xml), this SCHEMA_VERSION should be updated to at least 2.3.0.  \r\n\r\n I taught it's better capture in the migration guide as in my production environment thrift server failed to start after upgrade to spark 3.0. If the users are aware of this then I can cancel this ticket",
        "createdAt" : "2019-08-13T16:57:09Z",
        "updatedAt" : "2019-08-13T16:57:09Z",
        "lastEditedBy" : "f2e3e1f3-82a3-4b68-a0c2-2f7fdf58cf35",
        "tags" : [
        ]
      },
      {
        "id" : "35970f35-cb9d-4187-8107-957a2ba76cc3",
        "parentId" : "aeac4bf7-1197-4578-8ac2-58b8f6f40b89",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "@sandeep-katta How to reproduce this issue?",
        "createdAt" : "2019-08-14T00:14:06Z",
        "updatedAt" : "2019-08-14T00:14:06Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      },
      {
        "id" : "2a891efa-3bf7-4cff-a008-b1ac80ff853f",
        "parentId" : "aeac4bf7-1197-4578-8ac2-58b8f6f40b89",
        "authorId" : "f2e3e1f3-82a3-4b68-a0c2-2f7fdf58cf35",
        "body" : "configure the metastoredb in the hive-site.xml as below\r\n <property>\r\n    <name>javax.jdo.option.ConnectionURL</name>\r\n    <value>jdbc:mysql://vm1:3306/**sparksql**?characterEncoding=UTF-8</value>\r\n  </property>\r\n\r\nunder **sparksql** database there will be one table by name VERSION with the below values\r\n\r\n\r\nmysql> select * from VERSION;\r\n+--------+----------------+--------------------------------------+\r\n| VER_ID | SCHEMA_VERSION | VERSION_COMMENT                |\r\n+--------+----------------+--------------------------------------+\r\n|      1       |              **1.3.0**    |   Set by MetaStore root@127.0.0.1 |\r\n+--------+----------------+--------------------------------------+\r\n1 row in set (0.00 sec)\r\n\r\nif you upgrade the spark to 3.0 and restart thrift server it will not start. You need to update SCHEMA_VERSION to at least **2.3.0**",
        "createdAt" : "2019-08-14T04:49:13Z",
        "updatedAt" : "2019-08-14T04:49:13Z",
        "lastEditedBy" : "f2e3e1f3-82a3-4b68-a0c2-2f7fdf58cf35",
        "tags" : [
        ]
      },
      {
        "id" : "31945b1c-5f71-4e09-8b57-229285670202",
        "parentId" : "aeac4bf7-1197-4578-8ac2-58b8f6f40b89",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "Do you know why it's `1.3.0`?  Hive 1.3.0 has not been released yet.",
        "createdAt" : "2019-08-14T05:07:46Z",
        "updatedAt" : "2019-08-14T05:07:46Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      },
      {
        "id" : "76404bad-bf75-420a-9fea-e9972b9c2807",
        "parentId" : "aeac4bf7-1197-4578-8ac2-58b8f6f40b89",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "Anyway, could you try to start thriftserver with `spark.sql.hive.metastore.version=1.2`?\r\n```sh\r\nsbin/start-thriftserver.sh --conf spark.sql.hive.metastore.version=1.2 --conf spark.sql.hive.metastore.jars=maven\r\n```",
        "createdAt" : "2019-08-14T05:12:45Z",
        "updatedAt" : "2019-08-14T05:12:45Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      },
      {
        "id" : "80325e8b-f1f9-4767-b898-74286e1e7908",
        "parentId" : "aeac4bf7-1197-4578-8ac2-58b8f6f40b89",
        "authorId" : "f2e3e1f3-82a3-4b68-a0c2-2f7fdf58cf35",
        "body" : "Till 2.4 I think Spark uses some private fork of Hive which is 1.2.1, so we updated the SCHEMA_VERSION to 1.3.0 which > 1.2.1",
        "createdAt" : "2019-08-14T05:14:33Z",
        "updatedAt" : "2019-08-14T05:14:34Z",
        "lastEditedBy" : "f2e3e1f3-82a3-4b68-a0c2-2f7fdf58cf35",
        "tags" : [
        ]
      },
      {
        "id" : "862b5b80-5ccb-4108-9803-00c43c15fe7b",
        "parentId" : "aeac4bf7-1197-4578-8ac2-58b8f6f40b89",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "Could you try to start thriftserver with `spark.sql.hive.metastore.version=1.2`?\r\n```sh\r\nsbin/start-thriftserver.sh --conf spark.sql.hive.metastore.version=1.2 --conf spark.sql.hive.metastore.jars=maven\r\n```",
        "createdAt" : "2019-08-14T05:25:01Z",
        "updatedAt" : "2019-08-14T05:25:02Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      },
      {
        "id" : "124a0b3b-d1b8-4e5d-8825-e7fa24addba5",
        "parentId" : "aeac4bf7-1197-4578-8ac2-58b8f6f40b89",
        "authorId" : "f2e3e1f3-82a3-4b68-a0c2-2f7fdf58cf35",
        "body" : "I can start the thrift server even by disabling schema verification `hive.metastore.schema.verification`\r\n\r\nBut by doing this we may not be able to get the benefits of upgrading to latest Hive version.\r\n\r\nIf the users are aware of this then I can close this PR.",
        "createdAt" : "2019-08-14T05:35:12Z",
        "updatedAt" : "2019-08-14T05:35:12Z",
        "lastEditedBy" : "f2e3e1f3-82a3-4b68-a0c2-2f7fdf58cf35",
        "tags" : [
        ]
      },
      {
        "id" : "ea023c7c-81db-42f5-9fe9-b1766616ff4b",
        "parentId" : "aeac4bf7-1197-4578-8ac2-58b8f6f40b89",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "There are two things here:\r\n1. If you want to improve the performance of the Hive Metastore Server. The correct way is to upgrade your Hive Metastore Server to latest version. `SCHEMA_VERSION` should be updated by [Hive itself](https://github.com/apache/hive/blob/c57a59611fa168ee38c6ee0ee60b1d6c4994f9f8/metastore/scripts/upgrade/mysql/upgrade-1.2.0-to-1.3.0.mysql.sql).\r\n2. Upgrade built-in Hive to 2.3.x still can get benefits if you Hive Metastore Server is 1.2.x, such as [SPARK-12014](https://issues.apache.org/jira/browse/SPARK-12014), [SPARK-27500](https://issues.apache.org/jira/browse/SPARK-27500) and [SPARK-26321](https://issues.apache.org/jira/browse/SPARK-26321).",
        "createdAt" : "2019-08-14T06:20:49Z",
        "updatedAt" : "2019-08-14T06:20:49Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      },
      {
        "id" : "2c7264cb-b0ea-436f-9910-8c0a511ba616",
        "parentId" : "aeac4bf7-1197-4578-8ac2-58b8f6f40b89",
        "authorId" : "f2e3e1f3-82a3-4b68-a0c2-2f7fdf58cf35",
        "body" : "okay understood, this should be in the scope of Hive upgrade. Thank you @wangyum . I will close this PR as invalid ",
        "createdAt" : "2019-08-14T06:28:26Z",
        "updatedAt" : "2019-08-14T06:28:26Z",
        "lastEditedBy" : "f2e3e1f3-82a3-4b68-a0c2-2f7fdf58cf35",
        "tags" : [
        ]
      }
    ],
    "commit" : "bb9e8f92d0975ef5c57bbe35ac50776e27a964a8",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +24,28 @@\n## Upgrading From Spark SQL 2.4 to 3.0\n  - Since Spark 3.0, hive is upgraded to 2.3.x, so it is required to update the Hive\n  `SCHEMA_VERSION` in the database to at least 2.3.0. \n  "
  },
  {
    "id" : "6a0fb435-87f3-413b-ab69-ed1a5ac6f22b",
    "prId" : 25431,
    "prUrl" : "https://github.com/apache/spark/pull/25431#pullrequestreview-274395765",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "04daa3b3-6aaf-4aa8-abb4-f872292528e7",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Hive is only upgraded into 2.3.5 (and presumably it will be 2.3.6) only Hadoop 3 profile, which has not been officially released with.",
        "createdAt" : "2019-08-13T16:10:59Z",
        "updatedAt" : "2019-08-13T16:11:12Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "bb9e8f92d0975ef5c57bbe35ac50776e27a964a8",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +24,28 @@\n## Upgrading From Spark SQL 2.4 to 3.0\n  - Since Spark 3.0, hive is upgraded to 2.3.x, so it is required to update the Hive\n  `SCHEMA_VERSION` in the database to at least 2.3.0. \n  "
  },
  {
    "id" : "b7184ea5-ecda-45b6-882b-c9e4af5e0d03",
    "prId" : 25224,
    "prUrl" : "https://github.com/apache/spark/pull/25224#pullrequestreview-264598034",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ac714fdc-f5f6-43b9-b8f0-eb10eba4783c",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Just a typo fix from `resultes` -> `results`.",
        "createdAt" : "2019-07-22T04:43:37Z",
        "updatedAt" : "2019-07-22T04:43:37Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "be8ae4071e7057ea189679f1ae9f8459ba3ad88c",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +152,156 @@  - Since Spark 3.0, substitution order of nested WITH clauses is changed and an inner CTE definition takes precedence over an outer. In version 2.4 and earlier, `WITH t AS (SELECT 1), t2 AS (WITH t AS (SELECT 2) SELECT * FROM t) SELECT * FROM t2` returns `1` while in version 3.0 it returns `2`. The previous behaviour can be restored by setting `spark.sql.legacy.ctePrecedence.enabled` to `true`.\n\n  - Since Spark 3.0, the `add_months` function does not adjust the resulting date to a last day of month if the original date is a last day of months. For example, `select add_months(DATE'2019-02-28', 1)` results `2019-03-28`. In Spark version 2.4 and earlier, the resulting date is adjusted when the original date is a last day of months. For example, adding a month to `2019-02-28` results in `2019-03-31`.\n\n  - Since Spark 3.0, 0-argument Java UDF is executed in the executor side identically with other UDFs. In Spark version 2.4 and earlier, 0-argument Java UDF alone was executed in the driver side, and the result was propagated to executors, which might be more performant in some cases but caused inconsistency with a correctness issue in some cases."
  },
  {
    "id" : "91a4ff11-47f2-48c8-8512-9ce84a69557f",
    "prId" : 25199,
    "prUrl" : "https://github.com/apache/spark/pull/25199#pullrequestreview-264521968",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0388be55-1ae5-407f-8f23-a791712b3002",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "Nit: resultes -> results ",
        "createdAt" : "2019-07-21T06:35:49Z",
        "updatedAt" : "2019-07-21T06:35:49Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      }
    ],
    "commit" : "1f20426d9000c09c9d6329fc9b406edf0cc9d45b",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +152,156 @@  - Since Spark 3.0, substitution order of nested WITH clauses is changed and an inner CTE definition takes precedence over an outer. In version 2.4 and earlier, `WITH t AS (SELECT 1), t2 AS (WITH t AS (SELECT 2) SELECT * FROM t) SELECT * FROM t2` returns `1` while in version 3.0 it returns `2`. The previous behaviour can be restored by setting `spark.sql.legacy.ctePrecedence.enabled` to `true`.\n\n  - Since Spark 3.0, the `add_months` function does not adjust the resulting date to a last day of month if the original date is a last day of months. For example, `select add_months(DATE'2019-02-28', 1)` results `2019-03-28`. In Spark version 2.4 and earlier, the resulting date is adjusted when the original date is a last day of months. For example, adding a month to `2019-02-28` resultes in `2019-03-31`.\n\n## Upgrading from Spark SQL 2.4 to 2.4.1"
  },
  {
    "id" : "c37ceebf-934f-47fe-ae6d-5b9fe3b145c9",
    "prId" : 25153,
    "prUrl" : "https://github.com/apache/spark/pull/25153#pullrequestreview-264003519",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "20022f90-80e2-4467-8bdb-a0958cdee14e",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Hm .. shall we update this migration guide? Actually `select add_months(DATE'2019-01-31', 1)` returns `2019-02-28` in both the current master and the versions before Spark 2.4.x.\r\n\r\nI think we should explicitly mention `select add_months(DATE'2019-02-28', 1)` case only:\r\n\r\n```\r\nscala> sql(\"select add_months(DATE'2019-02-28', 1)\").show()\r\n+--------------------------------+\r\n|add_months(DATE '2019-02-28', 1)|\r\n+--------------------------------+\r\n|                      2019-03-31|\r\n+--------------------------------+\r\n```\r\n\r\n```\r\nscala> sql(\"select add_months(DATE'2019-02-28', 1)\").show()\r\n+--------------------------------+\r\n|add_months(DATE '2019-02-28', 1)|\r\n+--------------------------------+\r\n|                      2019-03-28|\r\n+--------------------------------+\r\n```",
        "createdAt" : "2019-07-19T03:45:20Z",
        "updatedAt" : "2019-07-19T03:45:21Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "24a2b2db-6915-4be4-b601-3f256354cbf8",
        "parentId" : "20022f90-80e2-4467-8bdb-a0958cdee14e",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Let me update it soon.",
        "createdAt" : "2019-07-19T03:52:40Z",
        "updatedAt" : "2019-07-19T03:52:40Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "f1a4241cf9cf175648f5b314b832124dfccdce01",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +152,156 @@  - Since Spark 3.0, substitution order of nested WITH clauses is changed and an inner CTE definition takes precedence over an outer. In version 2.4 and earlier, `WITH t AS (SELECT 1), t2 AS (WITH t AS (SELECT 2) SELECT * FROM t) SELECT * FROM t2` returns `1` while in version 3.0 it returns `2`. The previous behaviour can be restored by setting `spark.sql.legacy.ctePrecedence.enabled` to `true`.\n\n  - Since Spark 3.0, the `add_months` function adjusts the resulting date to a last day of month only if it is invalid. For example, `select add_months(DATE'2019-01-31', 1)` results `2019-02-28`. In Spark version 2.4 and earlier, the resulting date is adjusted when it is invalid, or the original date is a last day of months. For example, adding a month to `2019-02-28` resultes in `2019-03-31`.\n\n## Upgrading from Spark SQL 2.4 to 2.4.1"
  },
  {
    "id" : "8f9da415-d8c0-4bd7-965e-5c44c52f7422",
    "prId" : 24872,
    "prUrl" : "https://github.com/apache/spark/pull/24872#pullrequestreview-251140925",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ba81f498-cf01-4436-a066-9733183c9b02",
        "parentId" : null,
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "How about:\r\nSince Spark 3.0, when a string is cast to boolean/date/timestamp/numeric types, it is trimmed before it is parsed.",
        "createdAt" : "2019-06-18T14:33:43Z",
        "updatedAt" : "2019-06-18T14:33:43Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "e0ae14b0f827e55f23b94b5ecc60d59c7a0b8bb1",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +24,28 @@\n## Upgrading From Spark SQL 2.4 to 3.0\n  - Since Spark 3.0, trim the string when casting from string to boolean, date, timestamp or numeric types, whitespace is trimmed from the ends of the value first.\n\n  - Since Spark 3.0, PySpark requires a Pandas version of 0.23.2 or higher to use Pandas related functionality, such as `toPandas`, `createDataFrame` from Pandas DataFrame, etc."
  },
  {
    "id" : "31686276-44ad-46ff-a7d1-3bd33006d058",
    "prId" : 24867,
    "prUrl" : "https://github.com/apache/spark/pull/24867#pullrequestreview-250136116",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cd7099b9-13c8-4298-91c0-016f788b8c3f",
        "parentId" : null,
        "authorId" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "body" : "Added a note about the minimum pyarrow version. Further down here https://github.com/apache/spark/pull/24867/files#diff-3f19ec3d15dcd8cd42bb25dde1c5c1a9L58 we talk about safe casting, which I think is still relevant so I won't modify it, unless it seems confusing to talk about versions < 0.12.1?",
        "createdAt" : "2019-06-14T21:29:50Z",
        "updatedAt" : "2019-06-14T21:30:16Z",
        "lastEditedBy" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "tags" : [
        ]
      }
    ],
    "commit" : "13b6ed448d7323f509792184b78df4bdec04d917",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +27,31 @@\n  - Since Spark 3.0, PySpark requires a PyArrow version of 0.12.1 or higher to use PyArrow related functionality, such as `pandas_udf`, `toPandas` and `createDataFrame` with \"spark.sql.execution.arrow.enabled=true\", etc.\n\n  - In Spark version 2.4 and earlier, SQL queries such as `FROM <table>` or `FROM <table> UNION ALL FROM <table>` are supported by accident. In hive-style `FROM <table> SELECT <expr>`, the `SELECT` clause is not negligible. Neither Hive nor Presto support this syntax. Therefore we will treat these queries as invalid since Spark 3.0.\n"
  },
  {
    "id" : "e4d931af-ca24-4784-bc5e-8df9d51d8a87",
    "prId" : 24859,
    "prUrl" : "https://github.com/apache/spark/pull/24859#pullrequestreview-255616975",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "272a17a2-6ec7-4f4d-b4e3-f41ff8d63005",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Is it not \"has the lowest precedence among operators\"? Also, could you add a simple example query in this statement?",
        "createdAt" : "2019-06-28T07:16:59Z",
        "updatedAt" : "2019-06-28T07:16:59Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "ef1bafc11a80aa260185015a1496baf23cfd010a",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +24,28 @@\n## Upgrading From Spark SQL 2.4 to 3.0\n  - Since Spark 3.0, the string concatenation operator(`||`) has lower precedence than other operators.\n\n  - Since Spark 3.0, PySpark requires a Pandas version of 0.23.2 or higher to use Pandas related functionality, such as `toPandas`, `createDataFrame` from Pandas DataFrame, etc."
  },
  {
    "id" : "9127c381-2836-42d4-824b-7826c21deb0c",
    "prId" : 24788,
    "prUrl" : "https://github.com/apache/spark/pull/24788#pullrequestreview-248724393",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9b300956-d9d7-489e-932c-397793a65c41",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "I think you included a bunch of unrelated changes. Can you revert or open a new PR?",
        "createdAt" : "2019-06-12T12:05:22Z",
        "updatedAt" : "2019-06-12T12:06:12Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "2c3e350e-40b6-4356-a845-82cec215529b",
        "parentId" : "9b300956-d9d7-489e-932c-397793a65c41",
        "authorId" : "fa9d4e35-692a-43fd-9079-74047a9b3445",
        "body" : "yes .. looks like .. i better open a new PR",
        "createdAt" : "2019-06-12T12:27:26Z",
        "updatedAt" : "2019-06-12T12:27:26Z",
        "lastEditedBy" : "fa9d4e35-692a-43fd-9079-74047a9b3445",
        "tags" : [
        ]
      }
    ],
    "commit" : "9aa423f7e1e82cfc9b3046637ae35140ece6c960",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +24,28 @@\n## Upgrading From Spark SQL 2.4 to 3.0\n  - In Spark version 2.4 and earlier, SQL queries such as `FROM <table>` or `FROM <table> UNION ALL FROM <table>` are supported by accident. In hive-style `FROM <table> SELECT <expr>`, the `SELECT` clause is not negligible. Neither Hive nor Presto support this syntax. Therefore we will treat these queries as invalid since Spark 3.0.\n\n  - Since Spark 3.0, the Dataset and DataFrame API `unionAll` is not deprecated any more. It is an alias for `union`."
  },
  {
    "id" : "4dace6e6-e206-401c-9eaa-2baa5162c56a",
    "prId" : 24682,
    "prUrl" : "https://github.com/apache/spark/pull/24682#pullrequestreview-355817776",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8dee11b6-fdeb-4fd4-a55f-87b9ec48db94",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "Throwing a runtime exception by default looks risky to me. Could we change back the original behavior? We can add a conf if the end users can accept the runtime exception for nullability constraints. ",
        "createdAt" : "2020-01-02T23:42:09Z",
        "updatedAt" : "2020-01-02T23:42:09Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "b61eb6eb-f76a-4238-b525-9f26348796e0",
        "parentId" : "8dee11b6-fdeb-4fd4-a55f-87b9ec48db94",
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "cc @cloud-fan @gengliangwang @HyukjinKwon ",
        "createdAt" : "2020-01-02T23:42:22Z",
        "updatedAt" : "2020-01-02T23:42:22Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "b3918e16-8cc0-40d1-b6e3-212a14c23dac",
        "parentId" : "8dee11b6-fdeb-4fd4-a55f-87b9ec48db94",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "What was the original behaviour?",
        "createdAt" : "2020-01-03T02:01:43Z",
        "updatedAt" : "2020-01-03T02:01:43Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "3156d474-7f3a-4d95-8ddb-dfb0a4f8d6d9",
        "parentId" : "8dee11b6-fdeb-4fd4-a55f-87b9ec48db94",
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "Let us add a legacy conf and throw an exception by default. ",
        "createdAt" : "2020-02-10T02:39:33Z",
        "updatedAt" : "2020-02-10T02:39:34Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "ee573566-16b4-40c8-88df-d300496d0c20",
        "parentId" : "8dee11b6-fdeb-4fd4-a55f-87b9ec48db94",
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "cc @xuanyuanking could you submit a follow-up PR?",
        "createdAt" : "2020-02-10T02:40:32Z",
        "updatedAt" : "2020-02-10T02:40:33Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "55466c79-76d4-4ef9-960a-eff788b41d97",
        "parentId" : "8dee11b6-fdeb-4fd4-a55f-87b9ec48db94",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "Sure",
        "createdAt" : "2020-02-10T02:55:01Z",
        "updatedAt" : "2020-02-10T02:55:01Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      },
      {
        "id" : "b9a42dd8-8d9c-4872-9921-3a2d8ba70f2a",
        "parentId" : "8dee11b6-fdeb-4fd4-a55f-87b9ec48db94",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "What's the legacy behavior in Spark 2.4? @gengliangwang ",
        "createdAt" : "2020-02-10T04:25:42Z",
        "updatedAt" : "2020-02-10T04:25:43Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "869c4e3d-c470-4dd0-9b1f-0ff1abc8cc2d",
        "parentId" : "8dee11b6-fdeb-4fd4-a55f-87b9ec48db94",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "@cloud-fan before this PR, there should be an error in the `resolveNullableType` if user specifies a non-nullable Avro schema to write a nullable dataframe.\r\nAfter this PR, the behavior is allowed. There will be warning message and possible runtime error.",
        "createdAt" : "2020-02-10T06:31:51Z",
        "updatedAt" : "2020-02-10T06:34:27Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "b3f2a5bc-44eb-4ada-8729-72b7410bdf76",
        "parentId" : "8dee11b6-fdeb-4fd4-a55f-87b9ec48db94",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "i see, basically we move query compile-time error to runtime error.",
        "createdAt" : "2020-02-10T06:43:21Z",
        "updatedAt" : "2020-02-10T06:43:22Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "6cac8046-a56d-46bf-8bc1-785fe8fbd4da",
        "parentId" : "8dee11b6-fdeb-4fd4-a55f-87b9ec48db94",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "After further investigation, I think the 3.0 behavior here is good enough, cause this behavior only take effect while no records contain null. Also, while any records contain null, the new approach will give a better exception.\r\n\r\nLet's use the UT added in this PR to illustrate:\r\nFor the old behavior, we'll get exception `org.apache.avro.AvroRuntimeException: Not a union: \"int\"` which didn't express the problem of null data insertion.\r\nFor the current approach, the error message will be `NullPointerException: in test_schema in string null of string in field Name of test_schema`.\r\n\r\nSo if both legacy and non-legacy mode throw exception, and the legacy mode message is less clear than the new one, the legacy config might not necessary. WDYT?",
        "createdAt" : "2020-02-10T10:12:20Z",
        "updatedAt" : "2020-02-10T10:12:20Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "09197e66a53df33c5cc135012658f3f8ddaac4a2",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +135,139 @@  - Since Spark 3.0, when Avro files are written with user provided schema, the fields will be matched by field names between catalyst schema and avro schema instead of positions.\n\n  - Since Spark 3.0, when Avro files are written with user provided non-nullable schema, even the catalyst schema is nullable, Spark is still able to write the files. However, Spark will throw runtime NPE if any of the records contains null.\n\n## Upgrading from Spark SQL 2.4 to 2.4.1"
  },
  {
    "id" : "1fe6bd32-3636-41f6-82e4-6d4078d24980",
    "prId" : 24668,
    "prUrl" : "https://github.com/apache/spark/pull/24668#pullrequestreview-251994449",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b0893c6e-b4c0-495a-9c2b-38af12700806",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "What I am not sure is that .. at least .. I know one user group dependent on this behaviour and presumably (by  #15153), another user group will be affected - it's quite a breaking change ..\r\n\r\nDo you think it makes sense:\r\n\r\n```diff\r\n     .doc(\"Whether to ignore missing files. If true, the Spark jobs will continue to run when \" +\r\n       \"encountering missing files and the contents that have been read will still be returned.\")\r\n     .booleanConf\r\n-    .createWithDefault(false)\r\n+    .createOptional\r\n```\r\n\r\nand make it `true` by default here and `false` in existing code base?\r\n\r\nI was thinking about documenting:\r\n- if it's unset, it follows old behaviours, execution time it's false by default but true when listing up at driver side.\r\n- If we explicitly set true, ignore at both execution and driver\r\n- If we explicitly set false, fail fast at both sides.",
        "createdAt" : "2019-06-14T09:10:58Z",
        "updatedAt" : "2019-06-24T04:44:54Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "4055742e-a286-420a-b10e-b32de8ce2e33",
        "parentId" : "b0893c6e-b4c0-495a-9c2b-38af12700806",
        "authorId" : "a11722a8-ecfa-4827-88e3-4d708f023846",
        "body" : "Out of curiosity, can you give an example of a user workload which is dependent on ignoring files that go listing between the initial listing and a recursive listing / stat? I understand the case of wanting to ignore missing table _roots_ but can't come up with an intuitive example of when someone would want to purposely ignore the \"list inconsistency\" case.\r\n\r\nI think that only ignoring deletions at the root and _not_ ignoring deletions at lower levels of the listing represents a fair compromise: we may fail to reliably detect _all_ race conditions but the additional detection still provides some incremental value and I feel like it's unlikely to result in \"false positives\" which would break real workloads.\r\n\r\n@marmbrus @rxin @hvanhovell @brkyvz, do any of you have opinions on this PR's changes? The user-facing migration guide documentation is a bit convoluted here because it's trying to describe what I think is pretty narrow set of circumstances where this change might break workloads.\r\n\r\nIf we're not comfortable tying this to the `ignoreMissingFiles` configuration, maybe we could add a new configuration specifically for this behavior?",
        "createdAt" : "2019-06-19T02:06:31Z",
        "updatedAt" : "2019-06-24T04:44:54Z",
        "lastEditedBy" : "a11722a8-ecfa-4827-88e3-4d708f023846",
        "tags" : [
        ]
      },
      {
        "id" : "741e632c-a1ea-4a67-9a42-bd2bbcebd98e",
        "parentId" : "b0893c6e-b4c0-495a-9c2b-38af12700806",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "One usecase I faced at that time was that files were periodically removed by another script in structured streaming. They wanted to sync the timing somehow with the script and structured streaming. So, some files were missing comparing to the listed in the file index.\r\n\r\nI am good as is defaulting to `false` if we're all fine because logically it sounds we should better explicitly set `spark.sql.files.ignoreMissingFiles` for such cases .. but to me I feel like there might be some cases missing and better be stay safe and conservative.",
        "createdAt" : "2019-06-19T02:32:41Z",
        "updatedAt" : "2019-06-24T04:44:54Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "79b83b99-fc80-4532-a702-5ddbf3281c1c",
        "parentId" : "b0893c6e-b4c0-495a-9c2b-38af12700806",
        "authorId" : "1d40bb30-68bd-4124-82bc-33d93ee4bc65",
        "body" : "Given the we are coming up on a major release I'm inclined to fix this and have one simple flag that controls this behavior (and that defaults to `false`, as most people would expect to be told if there is an underlying inconsistency).\r\n\r\nI think it was a mistake to make the changes in #15153, rather than just telling the user to set this flag.",
        "createdAt" : "2019-06-19T22:01:38Z",
        "updatedAt" : "2019-06-24T04:44:54Z",
        "lastEditedBy" : "1d40bb30-68bd-4124-82bc-33d93ee4bc65",
        "tags" : [
        ]
      }
    ],
    "commit" : "d9c59037604763e7cd50a63c59b8f441f35331ee",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +146,150 @@  - Since Spark 3.0, a higher-order function `exists` follows the three-valued boolean logic, i.e., if the `predicate` returns any `null`s and no `true` is obtained, then `exists` will return `null` instead of `false`. For example, `exists(array(1, null, 3), x -> x % 2 == 0)` will be `null`. The previous behaviour can be restored by setting `spark.sql.legacy.arrayExistsFollowsThreeValuedLogic` to `false`.\n\n  - Since Spark 3.0, if files or subdirectories disappear during recursive directory listing (i.e. they appear in an intermediate listing but then cannot be read or listed during later phases of the recursive directory listing, due to either concurrent file deletions or object store consistency issues) then the listing will fail with an exception unless `spark.sql.files.ignoreMissingFiles` is `true` (default `false`). In previous versions, these missing files or subdirectories would be ignored. Note that this change of behavior only applies during initial table file listing (or during `REFRESH TABLE`), not during query execution: the net change is that `spark.sql.files.ignoreMissingFiles` is now obeyed during table file listing / query planning, not only at query execution time.\n\n## Upgrading from Spark SQL 2.4 to 2.4.1"
  },
  {
    "id" : "594ce8e1-5d9e-4ca7-80d7-4fea2acb857c",
    "prId" : 24567,
    "prUrl" : "https://github.com/apache/spark/pull/24567#pullrequestreview-337517544",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "36d38530-59e2-4974-8f2c-2967c286f87f",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "`spark.sql.legacy.typeCoercion.datetimeToString` -> `spark.sql.legacy.typeCoercion.datetimeToString.enabled`",
        "createdAt" : "2020-01-01T23:54:01Z",
        "updatedAt" : "2020-01-01T23:54:01Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "29f39406-9ade-4e55-b788-b32416291f8a",
        "parentId" : "36d38530-59e2-4974-8f2c-2967c286f87f",
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "cc @Ngone51 ",
        "createdAt" : "2020-01-01T23:54:10Z",
        "updatedAt" : "2020-01-01T23:54:10Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "f4182c7f-09f3-4651-ad8e-bfab9b3cc964",
        "parentId" : "36d38530-59e2-4974-8f2c-2967c286f87f",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "#27065",
        "createdAt" : "2020-01-02T02:15:14Z",
        "updatedAt" : "2020-01-02T02:15:14Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "c3e80f7ff98e49802a4c41d545d8988f798c52e9",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +129,133 @@  - Since Spark 3.0, if `hive.default.fileformat` is not found in `Spark SQL configuration` then it will fallback to hive-site.xml present in the `Hadoop configuration` of `SparkContext`.\n\n  - Since Spark 3.0, Spark will cast `String` to `Date/TimeStamp` in binary comparisons with dates/timestamps. The previous behaviour of casting `Date/Timestamp` to `String` can be restored by setting `spark.sql.legacy.typeCoercion.datetimeToString` to `true`.\n\n## Upgrading from Spark SQL 2.4 to 2.4.1"
  },
  {
    "id" : "79091e3d-ff83-4295-a56c-5c278c9144c5",
    "prId" : 24489,
    "prUrl" : "https://github.com/apache/spark/pull/24489#pullrequestreview-233710039",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c03e9789-0c89-438d-b1dd-66352e391b74",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I meant \"Spark SQL configuration\" without backquotes (not `` `Spark SQL configuration` `` ) but that;s fine.",
        "createdAt" : "2019-05-04T00:03:21Z",
        "updatedAt" : "2019-05-04T00:03:21Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "21a750c294e9119f33132d0d89131b84d6bfa302",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +127,131 @@  - Since Spark 3.0, parquet logical type `TIMESTAMP_MICROS` is used by default while saving `TIMESTAMP` columns. In Spark version 2.4 and earlier, `TIMESTAMP` columns are saved as `INT96` in parquet files. To set `INT96` to `spark.sql.parquet.outputTimestampType` restores the previous behavior.\n\n  - Since Spark 3.0, if `hive.default.fileformat` is not found in `Spark SQL configuration` then it will fallback to hive-site.xml present in the `Hadoop configuration` of `SparkContext`.\n\n## Upgrading from Spark SQL 2.4 to 2.4.1"
  }
]