[
  {
    "id" : "c2fb4c04-9bb8-4e09-89e9-c21ec81eefdb",
    "prId" : 33212,
    "prUrl" : "https://github.com/apache/spark/pull/33212#pullrequestreview-706902075",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9d4dab5a-5c78-4405-99dc-817df8e4a06a",
        "parentId" : null,
        "authorId" : "c12eb4aa-c39d-4fa9-8470-ed071c0be24a",
        "body" : "This change also affects parsing CSV string, I will add a unit test for CSV in a separate pr. cc @cloud-fan @HyukjinKwon ",
        "createdAt" : "2021-07-15T03:56:37Z",
        "updatedAt" : "2021-07-15T03:56:37Z",
        "lastEditedBy" : "c12eb4aa-c39d-4fa9-8470-ed071c0be24a",
        "tags" : [
        ]
      }
    ],
    "commit" : "9fb8fbd5598eb0d80c11bf7e129ad5c9b146f837",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +25,29 @@## Upgrading from Spark SQL 3.2 to 3.3\n\n  - In Spark 3.3, spark will fail when parsing a JSON/CSV string with `PERMISSIVE` mode and schema contains non-nullable fields. You can set mode to `FAILFAST/DROPMALFORMED` if you want to read JSON/CSV with a schema that contains nullable fields.\n\n## Upgrading from Spark SQL 3.1 to 3.2"
  },
  {
    "id" : "4d327633-1813-4a91-8f3c-d273ee97cb0c",
    "prId" : 33040,
    "prUrl" : "https://github.com/apache/spark/pull/33040#pullrequestreview-690828539",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6d4958b2-3897-4e1f-a5a2-65fcf1e64da4",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "union of top-level columns is also \"left dominant\", this makes sense.",
        "createdAt" : "2021-06-23T15:34:27Z",
        "updatedAt" : "2021-06-23T15:34:27Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "eb8ddfb432ee120ba617b71f77a0f3f71d56e01c",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +98,102 @@  - In Spark 3.2, the query executions triggered by `DataFrameWriter` are always named `command` when being sent to `QueryExecutionListener`. In Spark 3.1 and earlier, the name is one of `save`, `insertInto`, `saveAsTable`.\n  \n  - In Spark 3.2, `Dataset.unionByName` with `allowMissingColumns` set to true will add missing nested fields to the end of structs. In Spark 3.1, nested struct fields are sorted alphabetically.\n\n## Upgrading from Spark SQL 3.0 to 3.1"
  }
]