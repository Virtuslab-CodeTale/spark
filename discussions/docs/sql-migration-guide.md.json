[
  {
    "id" : "c2fb4c04-9bb8-4e09-89e9-c21ec81eefdb",
    "prId" : 33212,
    "prUrl" : "https://github.com/apache/spark/pull/33212#pullrequestreview-706902075",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9d4dab5a-5c78-4405-99dc-817df8e4a06a",
        "parentId" : null,
        "authorId" : "c12eb4aa-c39d-4fa9-8470-ed071c0be24a",
        "body" : "This change also affects parsing CSV string, I will add a unit test for CSV in a separate pr. cc @cloud-fan @HyukjinKwon ",
        "createdAt" : "2021-07-15T03:56:37Z",
        "updatedAt" : "2021-07-15T03:56:37Z",
        "lastEditedBy" : "c12eb4aa-c39d-4fa9-8470-ed071c0be24a",
        "tags" : [
        ]
      }
    ],
    "commit" : "9fb8fbd5598eb0d80c11bf7e129ad5c9b146f837",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +25,29 @@## Upgrading from Spark SQL 3.2 to 3.3\n\n  - In Spark 3.3, spark will fail when parsing a JSON/CSV string with `PERMISSIVE` mode and schema contains non-nullable fields. You can set mode to `FAILFAST/DROPMALFORMED` if you want to read JSON/CSV with a schema that contains nullable fields.\n\n## Upgrading from Spark SQL 3.1 to 3.2"
  },
  {
    "id" : "4d327633-1813-4a91-8f3c-d273ee97cb0c",
    "prId" : 33040,
    "prUrl" : "https://github.com/apache/spark/pull/33040#pullrequestreview-690828539",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6d4958b2-3897-4e1f-a5a2-65fcf1e64da4",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "union of top-level columns is also \"left dominant\", this makes sense.",
        "createdAt" : "2021-06-23T15:34:27Z",
        "updatedAt" : "2021-06-23T15:34:27Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "eb8ddfb432ee120ba617b71f77a0f3f71d56e01c",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +98,102 @@  - In Spark 3.2, the query executions triggered by `DataFrameWriter` are always named `command` when being sent to `QueryExecutionListener`. In Spark 3.1 and earlier, the name is one of `save`, `insertInto`, `saveAsTable`.\n  \n  - In Spark 3.2, `Dataset.unionByName` with `allowMissingColumns` set to true will add missing nested fields to the end of structs. In Spark 3.1, nested struct fields are sorted alphabetically.\n\n## Upgrading from Spark SQL 3.0 to 3.1"
  },
  {
    "id" : "1854e4af-094e-4da8-9bce-1af0175b5e58",
    "prId" : 32714,
    "prUrl" : "https://github.com/apache/spark/pull/32714#pullrequestreview-672997061",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e4a58352-8efe-4954-8bbe-44aa5fae1226",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "> if (c in ('now', 'today'), current_date(), cast(c as date))\r\n\r\nWhat does it mean?",
        "createdAt" : "2021-06-01T12:02:56Z",
        "updatedAt" : "2021-06-01T12:02:56Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "277df9d5-d6d8-4b80-9016-7921e98f87ff",
        "parentId" : "e4a58352-8efe-4954-8bbe-44aa5fae1226",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "@cloud-fan See the example:\r\n```scala\r\nscala> val df = Seq(\"now\", \"2021-01-19\", \"today\").toDF(\"c\")\r\ndf: org.apache.spark.sql.DataFrame = [c: string]\r\n\r\nscala> df.selectExpr(\"if (c in ('now', 'today'), current_date(), cast(c as date))\").show(false)\r\n+----------------------------------------------------------+\r\n|(IF((c IN (now, today)), current_date(), CAST(c AS DATE)))|\r\n+----------------------------------------------------------+\r\n|2021-06-01                                                |\r\n|2021-01-19                                                |\r\n|2021-06-01                                                |\r\n+----------------------------------------------------------+\r\n```",
        "createdAt" : "2021-06-01T12:15:35Z",
        "updatedAt" : "2021-06-01T12:15:35Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "c8423fa267836d3f2c693798ea3d8fdc972c3f8b",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +92,96 @@  - In Spark 3.2, `CREATE TABLE AS SELECT` with non-empty `LOCATION` will throw `AnalysisException`. To restore the behavior before Spark 3.2, you can set `spark.sql.legacy.allowNonEmptyLocationInCTAS` to `true`.\n\n  - In Spark 3.2, special datetime values such as `epoch`, `today`, `yesterday`, `tomorrow`, and `now` are supported in typed literals only, for instance, `select timestamp'now'`. In Spark 3.1 and 3.0, such special values are supported in any casts of strings to dates/timestamps. To keep these special values as dates/timestamps in Spark 3.1 and 3.0, you should replace them manually, e.g. `if (c in ('now', 'today'), current_date(), cast(c as date))`.\n\n## Upgrading from Spark SQL 3.0 to 3.1"
  },
  {
    "id" : "8e31f087-9457-4c95-b80a-afb8b53c3b1f",
    "prId" : 32165,
    "prUrl" : "https://github.com/apache/spark/pull/32165#pullrequestreview-636304348",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d0d308bd-00eb-4a3e-a576-7a00c939fe00",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Is there any valid use case around it? Can users access `c1, c2` somehow? e.g. `SELECT c1 FROM (SELECT TRANSFORM ...)`",
        "createdAt" : "2021-04-15T06:08:32Z",
        "updatedAt" : "2021-04-15T06:08:33Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "fc76d350-a2f9-474b-bf98-51710303dee0",
        "parentId" : "d0d308bd-00eb-4a3e-a576-7a00c939fe00",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> Is there any valid use case around it? Can users access `c1, c2` somehow? e.g. `SELECT c1 FROM (SELECT TRANSFORM ...)`\r\n\r\nSuch as \r\n```\r\nSELECT TRANSFORM(a as a1, sum(b) as sum_b)\r\nUSING 'cat'\r\nFROM tbl\r\nWHERE a1 > 3\r\nHAVING sum_b > 10\r\n```",
        "createdAt" : "2021-04-15T06:11:21Z",
        "updatedAt" : "2021-04-15T06:11:55Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "4605790b-f021-4ba7-9e04-41990829be82",
        "parentId" : "d0d308bd-00eb-4a3e-a576-7a00c939fe00",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "this looks super weird and not SQL-ish.",
        "createdAt" : "2021-04-15T06:16:42Z",
        "updatedAt" : "2021-04-15T06:16:42Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "8209d0f8fb69e092e79eeffc33a4ad85cc87c46e",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +79,83 @@\n  - In Spark 3.2, `TRANSFORM` operator can't support alias in inputs. In Spark 3.1 and earlier, we can write script transform like `SELECT TRANSFORM(a AS c1, b AS c2) USING 'cat' FROM TBL`.\n\n## Upgrading from Spark SQL 3.0 to 3.1\n"
  },
  {
    "id" : "aa5f43e3-eed6-4350-b6f6-d93f0655a9aa",
    "prId" : 32025,
    "prUrl" : "https://github.com/apache/spark/pull/32025#pullrequestreview-627902181",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9ad99314-b833-4dbf-899a-71f2bcb172df",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "`the reserved properties are only affect other DDLs except for this one` what does it mean? What's \"this one\"?",
        "createdAt" : "2021-04-05T14:51:02Z",
        "updatedAt" : "2021-04-05T15:05:01Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "e760d6c1-4064-439f-8a2a-780995b456c5",
        "parentId" : "9ad99314-b833-4dbf-899a-71f2bcb172df",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "It's a typo, let me fix it:)",
        "createdAt" : "2021-04-05T14:53:20Z",
        "updatedAt" : "2021-04-05T15:05:01Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "0cf2ce1efc5344deace91a29795fe3afb7658276",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +75,79 @@\n  - In Spark 3.2, `CREATE TABLE .. LIKE ..` command can not use reserved properties. You need their specific clauses to specify them, for example, `CREATE TABLE test1 LIKE test LOCATION 'some path'`. You can set `spark.sql.legacy.notReserveProperties` to `true` to ignore the `ParseException`, in this case, these properties will be silently removed, for example: `TBLPROPERTIES('owner'='yao')` will have no effect. In Spark version 3.1 and below, the reserved properties can be used in `CREATE TABLE .. LIKE ..` command but have no side effects, for example, `TBLPROPERTIES('location'='/tmp')` does not change the location of the table but only create a headless property just like `'a'='b'`.\n\n## Upgrading from Spark SQL 3.0 to 3.1\n"
  },
  {
    "id" : "18b9b1bd-73ad-4958-b22f-ea8ae326ba63",
    "prId" : 31780,
    "prUrl" : "https://github.com/apache/spark/pull/31780#pullrequestreview-606828501",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4ae89ba5-49ac-4c6f-b9a4-511c354a0b82",
        "parentId" : null,
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "I don't think `Hive external catalog` has any problem (`HiveExternalCatalog` class).",
        "createdAt" : "2021-03-08T22:00:27Z",
        "updatedAt" : "2021-03-08T22:00:30Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      },
      {
        "id" : "c563c8a1-1be8-4ddd-bb7e-5627b9a4f355",
        "parentId" : "4ae89ba5-49ac-4c6f-b9a4-511c354a0b82",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "+1 I'll close this.",
        "createdAt" : "2021-03-09T00:25:20Z",
        "updatedAt" : "2021-03-09T00:25:21Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "d7f9234f0450e8030db6310b0c5db6f6ec54d230",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +106,110 @@  - Since Spark 3.1, CHAR/CHARACTER and VARCHAR types are supported in the table schema. Table scan/insertion will respect the char/varchar semantic. If char/varchar is used in places other than table schema, an exception will be thrown (CAST is an exception that simply treats char/varchar as string like before). To restore the behavior before Spark 3.1, which treats them as STRING types and ignores a length parameter, e.g. `CHAR(4)`, you can set `spark.sql.legacy.charVarcharAsString` to `true`.\n\n  - In Spark 3.1, `AnalysisException` is replaced by its sub-classes that are thrown for tables from Hive an external catalog in the following situations:\n    * `ALTER TABLE .. ADD PARTITION` throws `PartitionsAlreadyExistException` if new partition exists already\n    * `ALTER TABLE .. DROP PARTITION` throws `NoSuchPartitionsException` for not existing partitions"
  },
  {
    "id" : "53de2a51-876f-4f2f-abe1-9d29bdadbb98",
    "prId" : 31608,
    "prUrl" : "https://github.com/apache/spark/pull/31608#pullrequestreview-595049957",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7f8e7e94-8eef-43fe-8266-620dea869284",
        "parentId" : null,
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "+1. Otherwise, users will be confused whether it is `SHOW CREATE TABLE AS SERDE table_identifier` or `SHOW CREATE TABLE table_identifier AS SERDE`.",
        "createdAt" : "2021-02-22T06:54:23Z",
        "updatedAt" : "2021-02-22T06:54:23Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      }
    ],
    "commit" : "7dcd44f0abb59a61fa22554cfa525d4a9e3c0047",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +90,94 @@  - In Spark 3.0, `SHOW TBLPROPERTIES` throws `AnalysisException` if the table does not exist. In Spark version 2.4 and below, this scenario caused `NoSuchTableException`.\n\n  - In Spark 3.0, `SHOW CREATE TABLE table_identifier` always returns Spark DDL, even when the given table is a Hive SerDe table. For generating Hive DDL, use `SHOW CREATE TABLE table_identifier AS SERDE` command instead.\n\n  - In Spark 3.0, column of CHAR type is not allowed in non-Hive-Serde tables, and CREATE/ALTER TABLE commands will fail if CHAR type is detected. Please use STRING type instead. In Spark version 2.4 and below, CHAR type is treated as STRING type and the length parameter is simply ignored."
  },
  {
    "id" : "b8a8d7e2-60aa-4a77-97a1-4ae6020065cc",
    "prId" : 31491,
    "prUrl" : "https://github.com/apache/spark/pull/31491#pullrequestreview-587760989",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "87f633c0-433a-4626-b2be-3ef080aa2f59",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "`dialects` is an internal word? If so, how about saying \"Since Spark 3.2, \\`java.sql.ROWID\\` is mapped to \\`StringType\\` when reading data from other databases via JDBC\"?",
        "createdAt" : "2021-02-10T13:03:40Z",
        "updatedAt" : "2021-02-17T01:59:57Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "6ed315b5-8167-4316-a31e-1390a362a2fd",
        "parentId" : "87f633c0-433a-4626-b2be-3ef080aa2f59",
        "authorId" : "3a227965-84e0-47cf-9974-11293764f028",
        "body" : "At least, I use `dialects` here as a general word, not represents specific implementations like `PostgresDialect`.\r\n`dialect` and `dialects` have been used from before in the migration guide.",
        "createdAt" : "2021-02-10T15:52:15Z",
        "updatedAt" : "2021-02-17T01:59:57Z",
        "lastEditedBy" : "3a227965-84e0-47cf-9974-11293764f028",
        "tags" : [
        ]
      }
    ],
    "commit" : "73f53d91e609a7450819ee3bb14b81eee34d42f9",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +25,29 @@## Upgrading from Spark SQL 3.1 to 3.2\n\n  - Since Spark 3.2, all the supported JDBC dialects use StringType for ROWID. In Spark 3.1 or earlier, Oracle dialect uses StringType and the other dialects use LongType.\n\n  - In Spark 3.2, PostgreSQL JDBC dialect uses StringType for MONEY and MONEY[] is not supported due to the JDBC driver for PostgreSQL can't handle those types properly. In Spark 3.1 or earlier, DoubleType and ArrayType of DoubleType are used respectively."
  },
  {
    "id" : "ee2d107a-6930-4d99-94b9-7acc68946794",
    "prId" : 31442,
    "prUrl" : "https://github.com/apache/spark/pull/31442#pullrequestreview-590197194",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dfdcb42f-82b9-4243-a55d-8c4ce7206626",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "I have the same comment here with https://github.com/apache/spark/pull/31491#discussion_r573709268",
        "createdAt" : "2021-02-10T13:45:02Z",
        "updatedAt" : "2021-02-10T13:45:02Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "f2d4c542-9e11-4f49-a4c2-70a258747500",
        "parentId" : "dfdcb42f-82b9-4243-a55d-8c4ce7206626",
        "authorId" : "3a227965-84e0-47cf-9974-11293764f028",
        "body" : "I've replied at https://github.com/apache/spark/pull/31491#discussion_r573846295 .",
        "createdAt" : "2021-02-10T15:53:09Z",
        "updatedAt" : "2021-02-10T15:53:09Z",
        "lastEditedBy" : "3a227965-84e0-47cf-9974-11293764f028",
        "tags" : [
        ]
      },
      {
        "id" : "058cd4f1-1682-493f-88ce-c758f64cad21",
        "parentId" : "dfdcb42f-82b9-4243-a55d-8c4ce7206626",
        "authorId" : "3a227965-84e0-47cf-9974-11293764f028",
        "body" : "@maropu Any more concern?",
        "createdAt" : "2021-02-15T06:55:18Z",
        "updatedAt" : "2021-02-15T06:55:18Z",
        "lastEditedBy" : "3a227965-84e0-47cf-9974-11293764f028",
        "tags" : [
        ]
      }
    ],
    "commit" : "b83821806c87ba6d710efe604a7c9d270af9617b",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +25,29 @@## Upgrading from Spark SQL 3.1 to 3.2\n\n  - In Spark 3.2, PostgreSQL JDBC dialect uses StringType for MONEY and MONEY[] is not supported due to the JDBC driver for PostgreSQL can't handle those types properly. In Spark 3.1 or earlier, DoubleType and ArrayType of DoubleType are used respectively.\n\n  - In Spark 3.2, `spark.sql.adaptive.enabled` is enabled by default. To restore the behavior before Spark 3.2, you can set `spark.sql.adaptive.enabled` to `false`."
  },
  {
    "id" : "96bb1825-0ab7-4dd6-8392-7e569bd6610f",
    "prId" : 31421,
    "prUrl" : "https://github.com/apache/spark/pull/31421#pullrequestreview-581054049",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "deb9b469-0be4-43b8-9f3c-527c8bdaeb2a",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Let's be more specific\r\n```\r\nIn Spark 3.2, `PARTITION(col=null)` is always parsed as a null literal in the partition spec. In Spark 3.1 or earlier,\r\nit is parsed as a string literal ..., if the partition column is string type. To restore the legacy behavior, ...\r\n```",
        "createdAt" : "2021-02-02T07:36:23Z",
        "updatedAt" : "2021-02-02T07:36:23Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "971ccbd5947924992a0c24acd40d40f1587d7741",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +42,46 @@  - In Spark 3.2, the auto-generated `Cast` (such as those added by type coercion rules) will be stripped when generating column alias names. E.g., `sql(\"SELECT floor(1)\").columns` will be `FLOOR(1)` instead of `FLOOR(CAST(1 AS DOUBLE))`.\n\n  - In Spark 3.2, a null partition value is parsed as it is. In Spark 3.1 or earlier, it is parsed as a string literal of its text representation, e.g., string \"null\". To restore the legacy behavior, you can set `spark.sql.legacy.parseNullPartitionSpecAsStringLiteral` as true.\n\n  - In Spark 3.2, table refreshing clears cached data of the table as well as of all its dependents such as views while keeping the dependents cached. The following commands perform table refreshing:"
  },
  {
    "id" : "4fcb2975-c7f1-49c3-a73e-71a5327bd664",
    "prId" : 31309,
    "prUrl" : "https://github.com/apache/spark/pull/31309#pullrequestreview-576351870",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "95fe9e25-5f87-41c3-8f42-66837e185c49",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "This doc change is for only `master`, right?",
        "createdAt" : "2021-01-24T19:56:06Z",
        "updatedAt" : "2021-01-24T19:56:07Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "0feecb07-18b1-4c96-9e5f-a3a597d8b5b8",
        "parentId" : "95fe9e25-5f87-41c3-8f42-66837e185c49",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "@dongjoon-hyun This one, yes but we made some fixes in 3.0/3.1 regarding to cache refreshing:\r\n- 3.0: #31006, #31060, #31236, #31116, #31305\r\n- 3.1: #31006, #31060, #31235, #31115, #31304\r\n\r\nShould I update the SQL migration guide about the changes, and say that \"the commands clear cached table data\"? cc @cloud-fan @HyukjinKwon ",
        "createdAt" : "2021-01-25T15:41:33Z",
        "updatedAt" : "2021-01-25T15:41:33Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "d5b243e9-3b56-4ebb-a43e-6c94065817ca",
        "parentId" : "95fe9e25-5f87-41c3-8f42-66837e185c49",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "In that case, for each branch, we may need a separate PR and independent update (if we want to update the doc).",
        "createdAt" : "2021-01-26T01:29:57Z",
        "updatedAt" : "2021-01-26T01:29:57Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "e9a1c318-5bcc-475f-a5a0-0e758a8bd60f",
        "parentId" : "95fe9e25-5f87-41c3-8f42-66837e185c49",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "@HyukjinKwon @cloud-fan Do we want to update the SQL migration guide?",
        "createdAt" : "2021-01-26T11:43:28Z",
        "updatedAt" : "2021-01-26T11:43:28Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "cf833bfc-2037-4e2c-a9e3-32ddff242c42",
        "parentId" : "95fe9e25-5f87-41c3-8f42-66837e185c49",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "\"the commands clear cached table data\" seems like a correctness bug fix that doesn't need a migration guide?",
        "createdAt" : "2021-01-26T12:45:08Z",
        "updatedAt" : "2021-01-26T12:45:08Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "bfe25b78-d654-42fc-ae04-bee563214809",
        "parentId" : "95fe9e25-5f87-41c3-8f42-66837e185c49",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "I would expect that currently users have to uncache and cache tables before any table modifications like:\r\n```\r\nUNCACHE TABLE tbl;\r\nALTER TABLE tbl ...;\r\nCACHE tbl;\r\n```\r\nAfter reading the SQL migration guide, they could remove the legacy code.",
        "createdAt" : "2021-01-26T13:36:31Z",
        "updatedAt" : "2021-01-26T13:36:31Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "d484830639850806488136023a5dc27fb5fd2950",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +48,52 @@    * `ALTER TABLE .. RECOVER PARTITIONS`\n    * `MSCK REPAIR TABLE`\n    * `LOAD DATA`\n    * `REFRESH TABLE`\n    * and the method `spark.catalog.refreshTable`"
  },
  {
    "id" : "513ec49a-5d53-489c-85bd-08937ae239c0",
    "prId" : 30202,
    "prUrl" : "https://github.com/apache/spark/pull/30202#pullrequestreview-521636412",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e44623b0-b0f8-4ed8-9341-880ac8f56e71",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I do think throwing `ArrayIndexOutOfBoundsException` is a bug. Do we still throw it somewhere in the script transformation code path?",
        "createdAt" : "2020-11-02T03:13:47Z",
        "updatedAt" : "2020-11-02T03:13:47Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "1cd95bb4-ba96-4b19-97bd-ed575569999c",
        "parentId" : "e44623b0-b0f8-4ed8-9341-880ac8f56e71",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I think it's arguable to call it a bug. That's why I suggested to add the migration guide here. The behaviour is just mismatch compared to Hive's behaviour. The previous behaviour was not padding null with the failfast.",
        "createdAt" : "2020-11-02T03:17:22Z",
        "updatedAt" : "2020-11-02T03:17:22Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "f513645c-386a-4f87-a387-0639b4ea7c19",
        "parentId" : "e44623b0-b0f8-4ed8-9341-880ac8f56e71",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "If we intend to fail, we should give a better error message with `AnalysisException`, not bug-like exceptions such as `ArrayIndexOutOfBoundsException`, NPE, etc.",
        "createdAt" : "2020-11-02T04:57:23Z",
        "updatedAt" : "2020-11-02T04:57:24Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "886ea545-a458-46c1-97af-2b4ec4c1e70f",
        "parentId" : "e44623b0-b0f8-4ed8-9341-880ac8f56e71",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Yeah that might be the best for completeness. I am just elaborating why I think it should be best to update the migration guide. We'll move to this new behaviour by default and probably remove it away in the future anyway.",
        "createdAt" : "2020-11-02T05:02:32Z",
        "updatedAt" : "2020-11-02T05:02:33Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "fc0e65eb-6aa2-4855-82eb-a6ea07e8c1a7",
        "parentId" : "e44623b0-b0f8-4ed8-9341-880ac8f56e71",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Yea I'm just saying that we usually don't add migration guide if we make something failed to work, as it's not a breaking change and users have nothing to do for migration.",
        "createdAt" : "2020-11-02T05:58:45Z",
        "updatedAt" : "2020-11-02T05:58:45Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "82c44840-281c-4436-b451-f7decf4f4082",
        "parentId" : "e44623b0-b0f8-4ed8-9341-880ac8f56e71",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "So, What should I do next? Personally, we need to let users know about this change",
        "createdAt" : "2020-11-02T12:18:37Z",
        "updatedAt" : "2020-11-02T12:18:37Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "dd75e32c-5d04-46e8-ab9a-00216597a506",
        "parentId" : "e44623b0-b0f8-4ed8-9341-880ac8f56e71",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Nit: NUll -> NULL\r\nYeah, why would you ever set this flag to the original behavior, to fail? this is just a bug fix. Nobody can reasonably be relying on this behavior.",
        "createdAt" : "2020-11-02T13:27:52Z",
        "updatedAt" : "2020-11-02T13:27:52Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "a591ad8f-1675-4566-baf4-f522aa82a6b0",
        "parentId" : "e44623b0-b0f8-4ed8-9341-880ac8f56e71",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "All  right, we  can just revert last pr.",
        "createdAt" : "2020-11-02T13:41:26Z",
        "updatedAt" : "2020-11-02T13:41:26Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "c3d8bfec453454c3f0b2e0c802dae54d39077c96",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +53,57 @@  - In Spark 3.1, the `schema_of_json` and `schema_of_csv` functions return the schema in the SQL format in which field names are quoted. In Spark 3.0, the function returns a catalog string without field quoting and in lower case. \n \n  - In Spark 3.1, when script transformation output's value size is less then schema size in default-serde mode(script transformation with row format of `ROW FORMAT DELIMITED`), Spark will pad NUll value to supplement data. In Spark 3.0 or earlier, Spark will do nothing and throw `ArrayIndexOutOfBoundsException`. To restore the behavior before Spark 3.1, you can set `spark.sql.legacy.transformationNotPadNullToSupplementData.enabled` to `true`.\n\n## Upgrading from Spark SQL 3.0 to 3.0.1"
  },
  {
    "id" : "97e0dcc0-2990-4268-9340-ec85301a90c2",
    "prId" : 30156,
    "prUrl" : "https://github.com/apache/spark/pull/30156#pullrequestreview-520409456",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c8a041b4-5ebe-4252-84cc-3bf18aae49d8",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Please follow other migration guide items: first explain what's the behavior change, then mention how to restore the legacy behavior with the legacy config.",
        "createdAt" : "2020-10-30T06:28:43Z",
        "updatedAt" : "2020-10-30T06:28:55Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "4dcff2cc-a4ab-41f4-8267-53932fe3c6ee",
        "parentId" : "c8a041b4-5ebe-4252-84cc-3bf18aae49d8",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "e.g.\r\n```\r\n  - In Spark 3.1, NULL elements of structures, arrays and maps are converted to \"null\" in casting them to strings. In Spark 3.0 or earlier, NULL elements are converted to empty strings. To restore the behavior before Spark 3.1, you can set `spark.sql.legacy.castComplexTypesToString.enabled` to `true`.\r\n```",
        "createdAt" : "2020-10-30T06:30:27Z",
        "updatedAt" : "2020-10-30T06:30:27Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "53a2ab9d-b703-40a9-ba3a-0927611b190c",
        "parentId" : "c8a041b4-5ebe-4252-84cc-3bf18aae49d8",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "Got it, with a follow up  pr  or revert current one?  @HyukjinKwon ",
        "createdAt" : "2020-10-30T06:38:43Z",
        "updatedAt" : "2020-10-30T06:38:43Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "4aebd0c7-1869-47e4-936f-a1602e099ad5",
        "parentId" : "c8a041b4-5ebe-4252-84cc-3bf18aae49d8",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Followup should be fine.",
        "createdAt" : "2020-10-30T06:46:48Z",
        "updatedAt" : "2020-10-30T06:46:48Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "0541ff50ba75278e984dd3414efed8dc5c339881",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +53,57 @@  - In Spark 3.1, the `schema_of_json` and `schema_of_csv` functions return the schema in the SQL format in which field names are quoted. In Spark 3.0, the function returns a catalog string without field quoting and in lower case. \n \n  - In Spark 3.1, when `spark.sql.legacy.transformationPadNullWhenValueLessThenSchema` is true, Spark will pad NULL value when script transformation's output value size less then schema size in default-serde mode(script transformation with row format of `ROW FORMAT DELIMITED`). If false, Spark will keep original behavior to throw `ArrayIndexOutOfBoundsException`.\n\n## Upgrading from Spark SQL 3.0 to 3.0.1"
  },
  {
    "id" : "4aaec37a-ee63-44dd-b004-03481d7a14fa",
    "prId" : 29635,
    "prUrl" : "https://github.com/apache/spark/pull/29635#pullrequestreview-513261828",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4da23724-92bc-4018-9bc7-990a17e02b38",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "> In Spark 3.1, incomplete interval literals, e.g. `INTERVAL '1'`, `INTERVAL '1 DAY 2'` will fail with IllegalArgumentException. In Spark 3.0, they result `NULL`s.\r\n\r\n=>\r\n\r\n> In Spark 3.1, IllegalArgumentException is returned for the incomplete interval literals, e.g. `INTERVAL '1'`, `INTERVAL '1 DAY 2'`, which are invalid. In Spark 3.0, these literals result in `NULL`s.",
        "createdAt" : "2020-10-18T23:04:20Z",
        "updatedAt" : "2020-10-18T23:04:20Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "42912692-8ccd-481e-a91f-f2b7e0b99bf9",
        "parentId" : "4da23724-92bc-4018-9bc7-990a17e02b38",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Could you fix it in followup, @yaooqinn ?",
        "createdAt" : "2020-10-21T01:53:43Z",
        "updatedAt" : "2020-10-21T01:53:43Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "7cc1f932-13b8-40fb-8bcc-24b2240479f8",
        "parentId" : "4da23724-92bc-4018-9bc7-990a17e02b38",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "OK, I will raise a PR today~",
        "createdAt" : "2020-10-21T02:26:35Z",
        "updatedAt" : "2020-10-21T02:26:35Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "daed4e68fa8b9ce44ef88078d1cb0405ec6d3fe5",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +41,45 @@  - In Spark 3.1, `path` option cannot coexist when the following methods are called with path parameter(s): `DataFrameReader.load()`, `DataFrameWriter.save()`, `DataStreamReader.load()`, or `DataStreamWriter.start()`. In addition, `paths` option cannot coexist for `DataFrameReader.load()`. For example, `spark.read.format(\"csv\").option(\"path\", \"/tmp\").load(\"/tmp2\")` or `spark.read.option(\"path\", \"/tmp\").csv(\"/tmp2\")` will throw `org.apache.spark.sql.AnalysisException`. In Spark version 3.0 and below, `path` option is overwritten if one path parameter is passed to above methods; `path` option is added to the overall paths if multiple path parameters are passed to `DataFrameReader.load()`. To restore the behavior before Spark 3.1, you can set `spark.sql.legacy.pathOptionBehavior.enabled` to `true`.\n\n  - In Spark 3.1, incomplete interval literals, e.g. `INTERVAL '1'`, `INTERVAL '1 DAY 2'` will fail with IllegalArgumentException. In Spark 3.0, they result `NULL`s.\n\n## Upgrading from Spark SQL 3.0 to 3.0.1"
  },
  {
    "id" : "d1191e94-8c57-4c57-9220-86f23ccc5bf5",
    "prId" : 29458,
    "prUrl" : "https://github.com/apache/spark/pull/29458#pullrequestreview-470100932",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0ab05d5f-3d7d-410a-8e6b-59737c8f6aa1",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "nit: we need to describe `spark.sql.ansi.enabled is false` two times? I think its okay just to describe it like this;\r\n```\r\nIn Spark 3.0 or earlier, the sum of...\r\n```\r\nor\r\n```\r\nIn Spark 3.0 or earlier, in the case, the sum of...\r\n```",
        "createdAt" : "2020-08-18T23:02:25Z",
        "updatedAt" : "2020-08-19T03:33:40Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "e2a917c5-010d-4791-a006-fbf28501c74e",
        "parentId" : "0ab05d5f-3d7d-410a-8e6b-59737c8f6aa1",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "@maropu Thanks",
        "createdAt" : "2020-08-19T03:34:01Z",
        "updatedAt" : "2020-08-19T03:34:01Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "877f47653cd1b8cc580a221870be78c5bd407bea",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +38,42 @@\n  - In Spark 3.1, when `spark.sql.ansi.enabled` is false, Spark always returns null if the sum of decimal type column overflows. In Spark 3.0 or earlier, in the case, the sum of decimal type column may return null or incorrect result, or even fails at runtime (depending on the actual query plan execution).\n\n## Upgrading from Spark SQL 3.0 to 3.0.1\n"
  },
  {
    "id" : "6b296bc8-21ab-475b-8c09-82ec070a3c98",
    "prId" : 29234,
    "prUrl" : "https://github.com/apache/spark/pull/29234#pullrequestreview-483733582",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bacebc52-934b-4cb3-84a3-3f05b7146718",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "@MaxGekk Also update this for the changes made in https://github.com/apache/spark/pull/29317?",
        "createdAt" : "2020-09-08T00:30:22Z",
        "updatedAt" : "2020-09-08T00:30:22Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      }
    ],
    "commit" : "918c77c996d632f76d9594724944550a32ce42a3",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +31,35 @@  - In Spark 3.1, `from_unixtime`, `unix_timestamp`,`to_unix_timestamp`, `to_timestamp` and `to_date` will fail if the specified datetime pattern is invalid. In Spark 3.0 or earlier, they result `NULL`.\n  \n  - In Spark 3.1, the Parquet, ORC, Avro and JSON datasources throw the exception `org.apache.spark.sql.AnalysisException: Found duplicate column(s) in the data schema` in read if they detect duplicate names in top-level columns as well in nested structures. The datasources take into account the SQL config `spark.sql.caseSensitive` while detecting column name duplicates.\n\n## Upgrading from Spark SQL 3.0 to 3.0.1"
  },
  {
    "id" : "c785e47a-a766-46f5-9cf3-38b6ae27b102",
    "prId" : 28375,
    "prUrl" : "https://github.com/apache/spark/pull/28375#pullrequestreview-401634969",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "42e1ede6-8c33-4d95-b5e8-26aba7380e00",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "maybe we can remove it. Exception class change doesn't worth a migration guide item.",
        "createdAt" : "2020-04-28T05:47:29Z",
        "updatedAt" : "2020-04-28T16:44:22Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "290fc7c6-e872-484b-8f4c-e24774f045ca",
        "parentId" : "42e1ede6-8c33-4d95-b5e8-26aba7380e00",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "This exception class change was brought up in this comment: https://github.com/apache/spark/pull/26921#discussion_r368060187. Can I still remove it?",
        "createdAt" : "2020-04-28T06:03:11Z",
        "updatedAt" : "2020-04-28T16:44:22Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      },
      {
        "id" : "c1d758d2-9e0d-45e3-892a-abaf9d2a36ba",
        "parentId" : "42e1ede6-8c33-4d95-b5e8-26aba7380e00",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I don't think error message/exception type change is a breaking change. @dongjoon-hyun what do you think?",
        "createdAt" : "2020-04-28T08:58:42Z",
        "updatedAt" : "2020-04-28T16:44:22Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "c4c02d78adc79f02033fadf86272040f05165ad4",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +60,64 @@  - In Spark 3.0, you can use `ADD FILE` to add file directories as well. Earlier you could add only single files using this command. To restore the behavior of earlier versions, set `spark.sql.legacy.addSingleFileInAddFile` to `true`.\n\n  - In Spark 3.0, `SHOW TBLPROPERTIES` throws `AnalysisException` if the table does not exist. In Spark version 2.4 and below, this scenario caused `NoSuchTableException`.\n\n  - In Spark 3.0, `SHOW CREATE TABLE` always returns Spark DDL, even when the given table is a Hive SerDe table. For generating Hive DDL, use `SHOW CREATE TABLE AS SERDE` command instead."
  },
  {
    "id" : "3cb99927-6ec9-4197-bad4-4ff7e9988061",
    "prId" : 28265,
    "prUrl" : "https://github.com/apache/spark/pull/28265#pullrequestreview-396075110",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c0adc807-09ef-4ba1-9443-cb2f4050b86d",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "This assumes https://github.com/apache/spark/pull/28262 will be merged to 2.4.6",
        "createdAt" : "2020-04-19T21:06:31Z",
        "updatedAt" : "2020-04-20T04:33:29Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "2fce1674-c03b-45ca-b7e5-55c393823b33",
        "parentId" : "c0adc807-09ef-4ba1-9443-cb2f4050b86d",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Yes. #28262 is merged a few minutes ago.",
        "createdAt" : "2020-04-19T23:26:29Z",
        "updatedAt" : "2020-04-20T04:33:29Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "a63ad80b75777e0d6aaf40f26825612b389518d8",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +219,223 @@## Upgrading from Spark SQL 2.4.5 to 2.4.6\n\n  - In Spark 2.4.6, the `RESET` command does not reset the static SQL configuration values to the default. It only clears the runtime SQL configuration values.\n\n## Upgrading from Spark SQL 2.4.4 to 2.4.5"
  },
  {
    "id" : "e7b54aa0-1632-4c55-9d06-5f48a5ff8660",
    "prId" : 28125,
    "prUrl" : "https://github.com/apache/spark/pull/28125#pullrequestreview-387865980",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ae6c485c-0899-4caf-ba73-2dce5700c6f4",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Please be careful during backporting. this section is only for `master` branch.",
        "createdAt" : "2020-04-05T22:01:30Z",
        "updatedAt" : "2020-04-07T21:50:05Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "ffe4584560b553202fef8f7a80b7e7419a073627",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +27,31 @@  - In Spark 3.1, grouping_id() returns long values. In Spark version 3.0 and earlier, this function returns int values. To restore the behavior before Spark 3.0, you can set `spark.sql.legacy.integerGroupingId` to `true`.\n\n  - In Spark 3.1, SQL UI data adopts the `formatted` mode for the query plan explain results. To restore the behavior before Spark 3.0, you can set `spark.sql.ui.explainMode` to `extended`.\n\n## Upgrading from Spark SQL 2.4 to 3.0"
  },
  {
    "id" : "bb13dc1b-cd22-4742-8c3a-aa888511221a",
    "prId" : 28125,
    "prUrl" : "https://github.com/apache/spark/pull/28125#pullrequestreview-389521741",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "df2191fe-d0f8-4e47-b06c-1b67392bca4a",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Shall we mention briefly the performance difference in Parquet/ORC datasources and the related conf in addition?\r\n\r\ncc @cloud-fan , @MaxGekk , @bersprockets ",
        "createdAt" : "2020-04-05T22:32:33Z",
        "updatedAt" : "2020-04-07T21:50:05Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "3820b9aa-1841-44da-897f-f76598ba1d23",
        "parentId" : "df2191fe-d0f8-4e47-b06c-1b67392bca4a",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "I am working on improving the performance #28067, #28119 . We can mention the performance difference but later when we will have final numbers, I think.",
        "createdAt" : "2020-04-06T03:37:47Z",
        "updatedAt" : "2020-04-07T21:50:05Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "93400a00-4726-4fce-9675-e44678a0fd52",
        "parentId" : "df2191fe-d0f8-4e47-b06c-1b67392bca4a",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "+1",
        "createdAt" : "2020-04-06T05:30:16Z",
        "updatedAt" : "2020-04-07T21:50:05Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "c54a8290-27c4-43fe-bf1c-a67a6137b58c",
        "parentId" : "df2191fe-d0f8-4e47-b06c-1b67392bca4a",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Even at the final stage, I don't think we can mention here quantitively (x times slower or x%s slower). The speed-up PRs are only mitigating the slowdown, isn't it?",
        "createdAt" : "2020-04-07T20:36:34Z",
        "updatedAt" : "2020-04-07T21:50:05Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "f2fbc062-d425-48a4-9114-d8402f0c8c0f",
        "parentId" : "df2191fe-d0f8-4e47-b06c-1b67392bca4a",
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "This doc majorly focuses on the behavior changes. Performance-related numbers are sensitive to the workloads and environments. Not sure how significant it is after these PRs. ",
        "createdAt" : "2020-04-07T22:01:15Z",
        "updatedAt" : "2020-04-07T22:01:16Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      }
    ],
    "commit" : "ffe4584560b553202fef8f7a80b7e7419a073627",
    "line" : 309,
    "diffHunk" : "@@ -1,1 +143,147 @@  - In Spark 3.0, Proleptic Gregorian calendar is used in parsing, formatting, and converting dates and timestamps as well as in extracting sub-components like years, days and so on. Spark 3.0 uses Java 8 API classes from the `java.time` packages that are based on [ISO chronology](https://docs.oracle.com/javase/8/docs/api/java/time/chrono/IsoChronology.html). In Spark version 2.4 and below, those operations are performed using the hybrid calendar ([Julian + Gregorian](https://docs.oracle.com/javase/7/docs/api/java/util/GregorianCalendar.html). The changes impact on the results for dates before October 15, 1582 (Gregorian) and affect on the following Spark 3.0 API:\n\n    * Parsing/formatting of timestamp/date strings. This effects on CSV/JSON datasources and on the `unix_timestamp`, `date_format`, `to_unix_timestamp`, `from_unixtime`, `to_date`, `to_timestamp` functions when patterns specified by users is used for parsing and formatting. In Spark 3.0, we define our own pattern strings in `sql-ref-datetime-pattern.md`, which is implemented via `java.time.format.DateTimeFormatter` under the hood. New implementation performs strict checking of its input. For example, the `2015-07-22 10:00:00` timestamp cannot be parse if pattern is `yyyy-MM-dd` because the parser does not consume whole input. Another example is the `31/01/2015 00:00` input cannot be parsed by the `dd/MM/yyyy hh:mm` pattern because `hh` supposes hours in the range `1-12`. In Spark version 2.4 and below, `java.text.SimpleDateFormat` is used for timestamp/date string conversions, and the supported patterns are described in [simpleDateFormat](https://docs.oracle.com/javase/7/docs/api/java/text/SimpleDateFormat.html). The old behavior can be restored by setting `spark.sql.legacy.timeParserPolicy` to `LEGACY`.\n\n    * The `weekofyear`, `weekday`, `dayofweek`, `date_trunc`, `from_utc_timestamp`, `to_utc_timestamp`, and `unix_timestamp` functions use java.time API for calculation week number of year, day number of week as well for conversion from/to TimestampType values in UTC time zone."
  },
  {
    "id" : "405023ef-438b-4e21-80e7-e5a8784da5d0",
    "prId" : 28097,
    "prUrl" : "https://github.com/apache/spark/pull/28097#pullrequestreview-386277493",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "115e1ae2-62f0-4185-8ce2-107cb4c64b04",
        "parentId" : null,
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "nit: unneeded change?",
        "createdAt" : "2020-04-02T06:22:41Z",
        "updatedAt" : "2020-04-02T22:39:21Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "6eab865d-ec4d-41a4-9a49-e4a6c23a8229",
        "parentId" : "115e1ae2-62f0-4185-8ce2-107cb4c64b04",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "It seems the other parts (e.g., `## Upgrading from Spark SQL 2.4.4 to 2.4.5`) have a single blank line in the head. So, I added a blank here, too.",
        "createdAt" : "2020-04-02T06:47:32Z",
        "updatedAt" : "2020-04-02T22:39:21Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "3b6a634f-47a5-4e16-90bd-554e14db1d9a",
        "parentId" : "115e1ae2-62f0-4185-8ce2-107cb4c64b04",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "A space between `##` and `-` is actually a required change for the correct MD syntax IIRC. Morden tools support no spaces between them, but technically it's required for strict MD syntax.",
        "createdAt" : "2020-04-02T09:25:28Z",
        "updatedAt" : "2020-04-02T22:39:21Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "561d1a24-76be-4ab6-8b17-5d9d25ece146",
        "parentId" : "115e1ae2-62f0-4185-8ce2-107cb4c64b04",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Ah, I see.",
        "createdAt" : "2020-04-02T09:59:42Z",
        "updatedAt" : "2020-04-02T22:39:21Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "e950c414a0119836ffffcdf4048b9ab3fc14435b",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +24,28 @@\n## Upgrading from Spark SQL 3.0 to 3.1\n\n  - Since Spark 3.1, grouping_id() returns long values. In Spark version 3.0 and earlier, this function returns int values. To restore the behavior before Spark 3.0, you can set `spark.sql.legacy.integerGroupingId` to `true`.\n"
  },
  {
    "id" : "7f5596a9-09a8-4577-a0f3-c8bea4ed0e56",
    "prId" : 27965,
    "prUrl" : "https://github.com/apache/spark/pull/27965#pullrequestreview-378672939",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a32b8bad-a21a-483a-8234-1295935c637b",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "The new approach looks good.",
        "createdAt" : "2020-03-20T17:20:02Z",
        "updatedAt" : "2020-03-23T11:05:16Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "bd6be1620d54e4e5ac65f6f9b930af9ccd604997",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +114,118 @@### UDFs and Built-in Functions\n\n  - Since Spark 3.0, the `date_add` and `date_sub` functions only accept int, smallint, tinyint as the 2nd argument, fractional and non-literal string are not valid anymore, e.g. `date_add(cast('1964-05-23' as date), 12.34)` will cause `AnalysisException`. Note that, string literals are still allowed, but Spark will throw Analysis Exception if the string content is not a valid integer. In Spark version 2.4 and earlier, if the 2nd argument is fractional or string value, it will be coerced to int value, and the result will be a date value of `1964-06-04`.\n\n  - Since Spark 3.0, the function `percentile_approx` and its alias `approx_percentile` only accept integral value with range in `[1, 2147483647]` as its 3rd argument `accuracy`, fractional and string types are disallowed, e.g. `percentile_approx(10.0, 0.2, 1.8D)` will cause `AnalysisException`. In Spark version 2.4 and earlier, if `accuracy` is fractional or string value, it will be coerced to an int value, `percentile_approx(10.0, 0.2, 1.8D)` is operated as `percentile_approx(10.0, 0.2, 1)` which results in `10.0`."
  },
  {
    "id" : "8e047990-7dff-4dcf-9477-db812b53068e",
    "prId" : 27909,
    "prUrl" : "https://github.com/apache/spark/pull/27909#pullrequestreview-374746798",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "17bed72f-54fa-4892-9f72-26672a7e9a21",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Since it seems this section describes parser behaviours, how about `SQL Query Statements` -> `SQL Parsing`?",
        "createdAt" : "2020-03-14T06:33:35Z",
        "updatedAt" : "2020-03-14T18:59:07Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "c505c743-43f0-4cd1-a689-0a7863c9976b",
        "parentId" : "17bed72f-54fa-4892-9f72-26672a7e9a21",
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "To end users, it is hard to tell the difference between parsing and engine. Let me combine it to Query Engine.",
        "createdAt" : "2020-03-14T18:57:18Z",
        "updatedAt" : "2020-03-14T18:59:07Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      }
    ],
    "commit" : "c9d39d321a1bf7db442946d2b96d4334b3600730",
    "line" : 181,
    "diffHunk" : "@@ -1,1 +234,238 @@\n### Query Engine\n\n  - In Spark version 2.4 and earlier, SQL queries such as `FROM <table>` or `FROM <table> UNION ALL FROM <table>` are supported by accident. In hive-style `FROM <table> SELECT <expr>`, the `SELECT` clause is not negligible. Neither Hive nor Presto support this syntax. Therefore we will treat these queries as invalid since Spark 3.0.\n"
  },
  {
    "id" : "bc789a4c-6281-4fbb-80f9-23e6703b7c80",
    "prId" : 27775,
    "prUrl" : "https://github.com/apache/spark/pull/27775#pullrequestreview-368460173",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0858f631-b78b-4d50-8b3a-9551bfd7c2a4",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "nit: like -> such as",
        "createdAt" : "2020-03-04T02:12:09Z",
        "updatedAt" : "2020-03-04T02:12:10Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "1317477cd2b96633b0343fac0bce68a127d45579",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +342,346 @@    - The decimal string representation can be different between Hive 1.2 and Hive 2.3 when using `TRANSFORM` operator in SQL for script transformation, which depends on hive's behavior. In Hive 1.2, the string representation omits trailing zeroes. But in Hive 2.3, it is always padded to 18 digits with trailing zeroes if necessary.\n\n  - Since Spark 3.0, group by alias fails if there are name conflicts like `SELECT col + 1 as col FROM t GROUP BY col`. In Spark version 2.4 and earlier, it works and the column will be resolved using child output. To restore the previous behaviour, set `spark.sql.legacy.allowAmbiguousGroupByAlias` to `true`.\n\n## Upgrading from Spark SQL 2.4.4 to 2.4.5"
  }
]