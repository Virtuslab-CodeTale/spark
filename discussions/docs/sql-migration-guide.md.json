[
  {
    "id" : "c2fb4c04-9bb8-4e09-89e9-c21ec81eefdb",
    "prId" : 33212,
    "prUrl" : "https://github.com/apache/spark/pull/33212#pullrequestreview-706902075",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9d4dab5a-5c78-4405-99dc-817df8e4a06a",
        "parentId" : null,
        "authorId" : "c12eb4aa-c39d-4fa9-8470-ed071c0be24a",
        "body" : "This change also affects parsing CSV string, I will add a unit test for CSV in a separate pr. cc @cloud-fan @HyukjinKwon ",
        "createdAt" : "2021-07-15T03:56:37Z",
        "updatedAt" : "2021-07-15T03:56:37Z",
        "lastEditedBy" : "c12eb4aa-c39d-4fa9-8470-ed071c0be24a",
        "tags" : [
        ]
      }
    ],
    "commit" : "9fb8fbd5598eb0d80c11bf7e129ad5c9b146f837",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +25,29 @@## Upgrading from Spark SQL 3.2 to 3.3\n\n  - In Spark 3.3, spark will fail when parsing a JSON/CSV string with `PERMISSIVE` mode and schema contains non-nullable fields. You can set mode to `FAILFAST/DROPMALFORMED` if you want to read JSON/CSV with a schema that contains nullable fields.\n\n## Upgrading from Spark SQL 3.1 to 3.2"
  },
  {
    "id" : "4d327633-1813-4a91-8f3c-d273ee97cb0c",
    "prId" : 33040,
    "prUrl" : "https://github.com/apache/spark/pull/33040#pullrequestreview-690828539",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6d4958b2-3897-4e1f-a5a2-65fcf1e64da4",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "union of top-level columns is also \"left dominant\", this makes sense.",
        "createdAt" : "2021-06-23T15:34:27Z",
        "updatedAt" : "2021-06-23T15:34:27Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "eb8ddfb432ee120ba617b71f77a0f3f71d56e01c",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +98,102 @@  - In Spark 3.2, the query executions triggered by `DataFrameWriter` are always named `command` when being sent to `QueryExecutionListener`. In Spark 3.1 and earlier, the name is one of `save`, `insertInto`, `saveAsTable`.\n  \n  - In Spark 3.2, `Dataset.unionByName` with `allowMissingColumns` set to true will add missing nested fields to the end of structs. In Spark 3.1, nested struct fields are sorted alphabetically.\n\n## Upgrading from Spark SQL 3.0 to 3.1"
  },
  {
    "id" : "1854e4af-094e-4da8-9bce-1af0175b5e58",
    "prId" : 32714,
    "prUrl" : "https://github.com/apache/spark/pull/32714#pullrequestreview-672997061",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e4a58352-8efe-4954-8bbe-44aa5fae1226",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "> if (c in ('now', 'today'), current_date(), cast(c as date))\r\n\r\nWhat does it mean?",
        "createdAt" : "2021-06-01T12:02:56Z",
        "updatedAt" : "2021-06-01T12:02:56Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "277df9d5-d6d8-4b80-9016-7921e98f87ff",
        "parentId" : "e4a58352-8efe-4954-8bbe-44aa5fae1226",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "@cloud-fan See the example:\r\n```scala\r\nscala> val df = Seq(\"now\", \"2021-01-19\", \"today\").toDF(\"c\")\r\ndf: org.apache.spark.sql.DataFrame = [c: string]\r\n\r\nscala> df.selectExpr(\"if (c in ('now', 'today'), current_date(), cast(c as date))\").show(false)\r\n+----------------------------------------------------------+\r\n|(IF((c IN (now, today)), current_date(), CAST(c AS DATE)))|\r\n+----------------------------------------------------------+\r\n|2021-06-01                                                |\r\n|2021-01-19                                                |\r\n|2021-06-01                                                |\r\n+----------------------------------------------------------+\r\n```",
        "createdAt" : "2021-06-01T12:15:35Z",
        "updatedAt" : "2021-06-01T12:15:35Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "c8423fa267836d3f2c693798ea3d8fdc972c3f8b",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +92,96 @@  - In Spark 3.2, `CREATE TABLE AS SELECT` with non-empty `LOCATION` will throw `AnalysisException`. To restore the behavior before Spark 3.2, you can set `spark.sql.legacy.allowNonEmptyLocationInCTAS` to `true`.\n\n  - In Spark 3.2, special datetime values such as `epoch`, `today`, `yesterday`, `tomorrow`, and `now` are supported in typed literals only, for instance, `select timestamp'now'`. In Spark 3.1 and 3.0, such special values are supported in any casts of strings to dates/timestamps. To keep these special values as dates/timestamps in Spark 3.1 and 3.0, you should replace them manually, e.g. `if (c in ('now', 'today'), current_date(), cast(c as date))`.\n\n## Upgrading from Spark SQL 3.0 to 3.1"
  },
  {
    "id" : "8e31f087-9457-4c95-b80a-afb8b53c3b1f",
    "prId" : 32165,
    "prUrl" : "https://github.com/apache/spark/pull/32165#pullrequestreview-636304348",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d0d308bd-00eb-4a3e-a576-7a00c939fe00",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Is there any valid use case around it? Can users access `c1, c2` somehow? e.g. `SELECT c1 FROM (SELECT TRANSFORM ...)`",
        "createdAt" : "2021-04-15T06:08:32Z",
        "updatedAt" : "2021-04-15T06:08:33Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "fc76d350-a2f9-474b-bf98-51710303dee0",
        "parentId" : "d0d308bd-00eb-4a3e-a576-7a00c939fe00",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> Is there any valid use case around it? Can users access `c1, c2` somehow? e.g. `SELECT c1 FROM (SELECT TRANSFORM ...)`\r\n\r\nSuch as \r\n```\r\nSELECT TRANSFORM(a as a1, sum(b) as sum_b)\r\nUSING 'cat'\r\nFROM tbl\r\nWHERE a1 > 3\r\nHAVING sum_b > 10\r\n```",
        "createdAt" : "2021-04-15T06:11:21Z",
        "updatedAt" : "2021-04-15T06:11:55Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "4605790b-f021-4ba7-9e04-41990829be82",
        "parentId" : "d0d308bd-00eb-4a3e-a576-7a00c939fe00",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "this looks super weird and not SQL-ish.",
        "createdAt" : "2021-04-15T06:16:42Z",
        "updatedAt" : "2021-04-15T06:16:42Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "8209d0f8fb69e092e79eeffc33a4ad85cc87c46e",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +79,83 @@\n  - In Spark 3.2, `TRANSFORM` operator can't support alias in inputs. In Spark 3.1 and earlier, we can write script transform like `SELECT TRANSFORM(a AS c1, b AS c2) USING 'cat' FROM TBL`.\n\n## Upgrading from Spark SQL 3.0 to 3.1\n"
  },
  {
    "id" : "aa5f43e3-eed6-4350-b6f6-d93f0655a9aa",
    "prId" : 32025,
    "prUrl" : "https://github.com/apache/spark/pull/32025#pullrequestreview-627902181",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9ad99314-b833-4dbf-899a-71f2bcb172df",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "`the reserved properties are only affect other DDLs except for this one` what does it mean? What's \"this one\"?",
        "createdAt" : "2021-04-05T14:51:02Z",
        "updatedAt" : "2021-04-05T15:05:01Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "e760d6c1-4064-439f-8a2a-780995b456c5",
        "parentId" : "9ad99314-b833-4dbf-899a-71f2bcb172df",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "It's a typo, let me fix it:)",
        "createdAt" : "2021-04-05T14:53:20Z",
        "updatedAt" : "2021-04-05T15:05:01Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "0cf2ce1efc5344deace91a29795fe3afb7658276",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +75,79 @@\n  - In Spark 3.2, `CREATE TABLE .. LIKE ..` command can not use reserved properties. You need their specific clauses to specify them, for example, `CREATE TABLE test1 LIKE test LOCATION 'some path'`. You can set `spark.sql.legacy.notReserveProperties` to `true` to ignore the `ParseException`, in this case, these properties will be silently removed, for example: `TBLPROPERTIES('owner'='yao')` will have no effect. In Spark version 3.1 and below, the reserved properties can be used in `CREATE TABLE .. LIKE ..` command but have no side effects, for example, `TBLPROPERTIES('location'='/tmp')` does not change the location of the table but only create a headless property just like `'a'='b'`.\n\n## Upgrading from Spark SQL 3.0 to 3.1\n"
  },
  {
    "id" : "18b9b1bd-73ad-4958-b22f-ea8ae326ba63",
    "prId" : 31780,
    "prUrl" : "https://github.com/apache/spark/pull/31780#pullrequestreview-606828501",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4ae89ba5-49ac-4c6f-b9a4-511c354a0b82",
        "parentId" : null,
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "I don't think `Hive external catalog` has any problem (`HiveExternalCatalog` class).",
        "createdAt" : "2021-03-08T22:00:27Z",
        "updatedAt" : "2021-03-08T22:00:30Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      },
      {
        "id" : "c563c8a1-1be8-4ddd-bb7e-5627b9a4f355",
        "parentId" : "4ae89ba5-49ac-4c6f-b9a4-511c354a0b82",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "+1 I'll close this.",
        "createdAt" : "2021-03-09T00:25:20Z",
        "updatedAt" : "2021-03-09T00:25:21Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "d7f9234f0450e8030db6310b0c5db6f6ec54d230",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +106,110 @@  - Since Spark 3.1, CHAR/CHARACTER and VARCHAR types are supported in the table schema. Table scan/insertion will respect the char/varchar semantic. If char/varchar is used in places other than table schema, an exception will be thrown (CAST is an exception that simply treats char/varchar as string like before). To restore the behavior before Spark 3.1, which treats them as STRING types and ignores a length parameter, e.g. `CHAR(4)`, you can set `spark.sql.legacy.charVarcharAsString` to `true`.\n\n  - In Spark 3.1, `AnalysisException` is replaced by its sub-classes that are thrown for tables from Hive an external catalog in the following situations:\n    * `ALTER TABLE .. ADD PARTITION` throws `PartitionsAlreadyExistException` if new partition exists already\n    * `ALTER TABLE .. DROP PARTITION` throws `NoSuchPartitionsException` for not existing partitions"
  },
  {
    "id" : "53de2a51-876f-4f2f-abe1-9d29bdadbb98",
    "prId" : 31608,
    "prUrl" : "https://github.com/apache/spark/pull/31608#pullrequestreview-595049957",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7f8e7e94-8eef-43fe-8266-620dea869284",
        "parentId" : null,
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "+1. Otherwise, users will be confused whether it is `SHOW CREATE TABLE AS SERDE table_identifier` or `SHOW CREATE TABLE table_identifier AS SERDE`.",
        "createdAt" : "2021-02-22T06:54:23Z",
        "updatedAt" : "2021-02-22T06:54:23Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      }
    ],
    "commit" : "7dcd44f0abb59a61fa22554cfa525d4a9e3c0047",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +90,94 @@  - In Spark 3.0, `SHOW TBLPROPERTIES` throws `AnalysisException` if the table does not exist. In Spark version 2.4 and below, this scenario caused `NoSuchTableException`.\n\n  - In Spark 3.0, `SHOW CREATE TABLE table_identifier` always returns Spark DDL, even when the given table is a Hive SerDe table. For generating Hive DDL, use `SHOW CREATE TABLE table_identifier AS SERDE` command instead.\n\n  - In Spark 3.0, column of CHAR type is not allowed in non-Hive-Serde tables, and CREATE/ALTER TABLE commands will fail if CHAR type is detected. Please use STRING type instead. In Spark version 2.4 and below, CHAR type is treated as STRING type and the length parameter is simply ignored."
  },
  {
    "id" : "b8a8d7e2-60aa-4a77-97a1-4ae6020065cc",
    "prId" : 31491,
    "prUrl" : "https://github.com/apache/spark/pull/31491#pullrequestreview-587760989",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "87f633c0-433a-4626-b2be-3ef080aa2f59",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "`dialects` is an internal word? If so, how about saying \"Since Spark 3.2, \\`java.sql.ROWID\\` is mapped to \\`StringType\\` when reading data from other databases via JDBC\"?",
        "createdAt" : "2021-02-10T13:03:40Z",
        "updatedAt" : "2021-02-17T01:59:57Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "6ed315b5-8167-4316-a31e-1390a362a2fd",
        "parentId" : "87f633c0-433a-4626-b2be-3ef080aa2f59",
        "authorId" : "3a227965-84e0-47cf-9974-11293764f028",
        "body" : "At least, I use `dialects` here as a general word, not represents specific implementations like `PostgresDialect`.\r\n`dialect` and `dialects` have been used from before in the migration guide.",
        "createdAt" : "2021-02-10T15:52:15Z",
        "updatedAt" : "2021-02-17T01:59:57Z",
        "lastEditedBy" : "3a227965-84e0-47cf-9974-11293764f028",
        "tags" : [
        ]
      }
    ],
    "commit" : "73f53d91e609a7450819ee3bb14b81eee34d42f9",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +25,29 @@## Upgrading from Spark SQL 3.1 to 3.2\n\n  - Since Spark 3.2, all the supported JDBC dialects use StringType for ROWID. In Spark 3.1 or earlier, Oracle dialect uses StringType and the other dialects use LongType.\n\n  - In Spark 3.2, PostgreSQL JDBC dialect uses StringType for MONEY and MONEY[] is not supported due to the JDBC driver for PostgreSQL can't handle those types properly. In Spark 3.1 or earlier, DoubleType and ArrayType of DoubleType are used respectively."
  },
  {
    "id" : "ee2d107a-6930-4d99-94b9-7acc68946794",
    "prId" : 31442,
    "prUrl" : "https://github.com/apache/spark/pull/31442#pullrequestreview-590197194",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dfdcb42f-82b9-4243-a55d-8c4ce7206626",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "I have the same comment here with https://github.com/apache/spark/pull/31491#discussion_r573709268",
        "createdAt" : "2021-02-10T13:45:02Z",
        "updatedAt" : "2021-02-10T13:45:02Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "f2d4c542-9e11-4f49-a4c2-70a258747500",
        "parentId" : "dfdcb42f-82b9-4243-a55d-8c4ce7206626",
        "authorId" : "3a227965-84e0-47cf-9974-11293764f028",
        "body" : "I've replied at https://github.com/apache/spark/pull/31491#discussion_r573846295 .",
        "createdAt" : "2021-02-10T15:53:09Z",
        "updatedAt" : "2021-02-10T15:53:09Z",
        "lastEditedBy" : "3a227965-84e0-47cf-9974-11293764f028",
        "tags" : [
        ]
      },
      {
        "id" : "058cd4f1-1682-493f-88ce-c758f64cad21",
        "parentId" : "dfdcb42f-82b9-4243-a55d-8c4ce7206626",
        "authorId" : "3a227965-84e0-47cf-9974-11293764f028",
        "body" : "@maropu Any more concern?",
        "createdAt" : "2021-02-15T06:55:18Z",
        "updatedAt" : "2021-02-15T06:55:18Z",
        "lastEditedBy" : "3a227965-84e0-47cf-9974-11293764f028",
        "tags" : [
        ]
      }
    ],
    "commit" : "b83821806c87ba6d710efe604a7c9d270af9617b",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +25,29 @@## Upgrading from Spark SQL 3.1 to 3.2\n\n  - In Spark 3.2, PostgreSQL JDBC dialect uses StringType for MONEY and MONEY[] is not supported due to the JDBC driver for PostgreSQL can't handle those types properly. In Spark 3.1 or earlier, DoubleType and ArrayType of DoubleType are used respectively.\n\n  - In Spark 3.2, `spark.sql.adaptive.enabled` is enabled by default. To restore the behavior before Spark 3.2, you can set `spark.sql.adaptive.enabled` to `false`."
  },
  {
    "id" : "96bb1825-0ab7-4dd6-8392-7e569bd6610f",
    "prId" : 31421,
    "prUrl" : "https://github.com/apache/spark/pull/31421#pullrequestreview-581054049",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "deb9b469-0be4-43b8-9f3c-527c8bdaeb2a",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Let's be more specific\r\n```\r\nIn Spark 3.2, `PARTITION(col=null)` is always parsed as a null literal in the partition spec. In Spark 3.1 or earlier,\r\nit is parsed as a string literal ..., if the partition column is string type. To restore the legacy behavior, ...\r\n```",
        "createdAt" : "2021-02-02T07:36:23Z",
        "updatedAt" : "2021-02-02T07:36:23Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "971ccbd5947924992a0c24acd40d40f1587d7741",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +42,46 @@  - In Spark 3.2, the auto-generated `Cast` (such as those added by type coercion rules) will be stripped when generating column alias names. E.g., `sql(\"SELECT floor(1)\").columns` will be `FLOOR(1)` instead of `FLOOR(CAST(1 AS DOUBLE))`.\n\n  - In Spark 3.2, a null partition value is parsed as it is. In Spark 3.1 or earlier, it is parsed as a string literal of its text representation, e.g., string \"null\". To restore the legacy behavior, you can set `spark.sql.legacy.parseNullPartitionSpecAsStringLiteral` as true.\n\n  - In Spark 3.2, table refreshing clears cached data of the table as well as of all its dependents such as views while keeping the dependents cached. The following commands perform table refreshing:"
  },
  {
    "id" : "4fcb2975-c7f1-49c3-a73e-71a5327bd664",
    "prId" : 31309,
    "prUrl" : "https://github.com/apache/spark/pull/31309#pullrequestreview-576351870",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "95fe9e25-5f87-41c3-8f42-66837e185c49",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "This doc change is for only `master`, right?",
        "createdAt" : "2021-01-24T19:56:06Z",
        "updatedAt" : "2021-01-24T19:56:07Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "0feecb07-18b1-4c96-9e5f-a3a597d8b5b8",
        "parentId" : "95fe9e25-5f87-41c3-8f42-66837e185c49",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "@dongjoon-hyun This one, yes but we made some fixes in 3.0/3.1 regarding to cache refreshing:\r\n- 3.0: #31006, #31060, #31236, #31116, #31305\r\n- 3.1: #31006, #31060, #31235, #31115, #31304\r\n\r\nShould I update the SQL migration guide about the changes, and say that \"the commands clear cached table data\"? cc @cloud-fan @HyukjinKwon ",
        "createdAt" : "2021-01-25T15:41:33Z",
        "updatedAt" : "2021-01-25T15:41:33Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "d5b243e9-3b56-4ebb-a43e-6c94065817ca",
        "parentId" : "95fe9e25-5f87-41c3-8f42-66837e185c49",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "In that case, for each branch, we may need a separate PR and independent update (if we want to update the doc).",
        "createdAt" : "2021-01-26T01:29:57Z",
        "updatedAt" : "2021-01-26T01:29:57Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "e9a1c318-5bcc-475f-a5a0-0e758a8bd60f",
        "parentId" : "95fe9e25-5f87-41c3-8f42-66837e185c49",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "@HyukjinKwon @cloud-fan Do we want to update the SQL migration guide?",
        "createdAt" : "2021-01-26T11:43:28Z",
        "updatedAt" : "2021-01-26T11:43:28Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "cf833bfc-2037-4e2c-a9e3-32ddff242c42",
        "parentId" : "95fe9e25-5f87-41c3-8f42-66837e185c49",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "\"the commands clear cached table data\" seems like a correctness bug fix that doesn't need a migration guide?",
        "createdAt" : "2021-01-26T12:45:08Z",
        "updatedAt" : "2021-01-26T12:45:08Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "bfe25b78-d654-42fc-ae04-bee563214809",
        "parentId" : "95fe9e25-5f87-41c3-8f42-66837e185c49",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "I would expect that currently users have to uncache and cache tables before any table modifications like:\r\n```\r\nUNCACHE TABLE tbl;\r\nALTER TABLE tbl ...;\r\nCACHE tbl;\r\n```\r\nAfter reading the SQL migration guide, they could remove the legacy code.",
        "createdAt" : "2021-01-26T13:36:31Z",
        "updatedAt" : "2021-01-26T13:36:31Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "d484830639850806488136023a5dc27fb5fd2950",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +48,52 @@    * `ALTER TABLE .. RECOVER PARTITIONS`\n    * `MSCK REPAIR TABLE`\n    * `LOAD DATA`\n    * `REFRESH TABLE`\n    * and the method `spark.catalog.refreshTable`"
  },
  {
    "id" : "513ec49a-5d53-489c-85bd-08937ae239c0",
    "prId" : 30202,
    "prUrl" : "https://github.com/apache/spark/pull/30202#pullrequestreview-521636412",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e44623b0-b0f8-4ed8-9341-880ac8f56e71",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I do think throwing `ArrayIndexOutOfBoundsException` is a bug. Do we still throw it somewhere in the script transformation code path?",
        "createdAt" : "2020-11-02T03:13:47Z",
        "updatedAt" : "2020-11-02T03:13:47Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "1cd95bb4-ba96-4b19-97bd-ed575569999c",
        "parentId" : "e44623b0-b0f8-4ed8-9341-880ac8f56e71",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I think it's arguable to call it a bug. That's why I suggested to add the migration guide here. The behaviour is just mismatch compared to Hive's behaviour. The previous behaviour was not padding null with the failfast.",
        "createdAt" : "2020-11-02T03:17:22Z",
        "updatedAt" : "2020-11-02T03:17:22Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "f513645c-386a-4f87-a387-0639b4ea7c19",
        "parentId" : "e44623b0-b0f8-4ed8-9341-880ac8f56e71",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "If we intend to fail, we should give a better error message with `AnalysisException`, not bug-like exceptions such as `ArrayIndexOutOfBoundsException`, NPE, etc.",
        "createdAt" : "2020-11-02T04:57:23Z",
        "updatedAt" : "2020-11-02T04:57:24Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "886ea545-a458-46c1-97af-2b4ec4c1e70f",
        "parentId" : "e44623b0-b0f8-4ed8-9341-880ac8f56e71",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Yeah that might be the best for completeness. I am just elaborating why I think it should be best to update the migration guide. We'll move to this new behaviour by default and probably remove it away in the future anyway.",
        "createdAt" : "2020-11-02T05:02:32Z",
        "updatedAt" : "2020-11-02T05:02:33Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "fc0e65eb-6aa2-4855-82eb-a6ea07e8c1a7",
        "parentId" : "e44623b0-b0f8-4ed8-9341-880ac8f56e71",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Yea I'm just saying that we usually don't add migration guide if we make something failed to work, as it's not a breaking change and users have nothing to do for migration.",
        "createdAt" : "2020-11-02T05:58:45Z",
        "updatedAt" : "2020-11-02T05:58:45Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "82c44840-281c-4436-b451-f7decf4f4082",
        "parentId" : "e44623b0-b0f8-4ed8-9341-880ac8f56e71",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "So, What should I do next? Personally, we need to let users know about this change",
        "createdAt" : "2020-11-02T12:18:37Z",
        "updatedAt" : "2020-11-02T12:18:37Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "dd75e32c-5d04-46e8-ab9a-00216597a506",
        "parentId" : "e44623b0-b0f8-4ed8-9341-880ac8f56e71",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Nit: NUll -> NULL\r\nYeah, why would you ever set this flag to the original behavior, to fail? this is just a bug fix. Nobody can reasonably be relying on this behavior.",
        "createdAt" : "2020-11-02T13:27:52Z",
        "updatedAt" : "2020-11-02T13:27:52Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "a591ad8f-1675-4566-baf4-f522aa82a6b0",
        "parentId" : "e44623b0-b0f8-4ed8-9341-880ac8f56e71",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "All  right, we  can just revert last pr.",
        "createdAt" : "2020-11-02T13:41:26Z",
        "updatedAt" : "2020-11-02T13:41:26Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "c3d8bfec453454c3f0b2e0c802dae54d39077c96",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +53,57 @@  - In Spark 3.1, the `schema_of_json` and `schema_of_csv` functions return the schema in the SQL format in which field names are quoted. In Spark 3.0, the function returns a catalog string without field quoting and in lower case. \n \n  - In Spark 3.1, when script transformation output's value size is less then schema size in default-serde mode(script transformation with row format of `ROW FORMAT DELIMITED`), Spark will pad NUll value to supplement data. In Spark 3.0 or earlier, Spark will do nothing and throw `ArrayIndexOutOfBoundsException`. To restore the behavior before Spark 3.1, you can set `spark.sql.legacy.transformationNotPadNullToSupplementData.enabled` to `true`.\n\n## Upgrading from Spark SQL 3.0 to 3.0.1"
  },
  {
    "id" : "97e0dcc0-2990-4268-9340-ec85301a90c2",
    "prId" : 30156,
    "prUrl" : "https://github.com/apache/spark/pull/30156#pullrequestreview-520409456",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c8a041b4-5ebe-4252-84cc-3bf18aae49d8",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Please follow other migration guide items: first explain what's the behavior change, then mention how to restore the legacy behavior with the legacy config.",
        "createdAt" : "2020-10-30T06:28:43Z",
        "updatedAt" : "2020-10-30T06:28:55Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "4dcff2cc-a4ab-41f4-8267-53932fe3c6ee",
        "parentId" : "c8a041b4-5ebe-4252-84cc-3bf18aae49d8",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "e.g.\r\n```\r\n  - In Spark 3.1, NULL elements of structures, arrays and maps are converted to \"null\" in casting them to strings. In Spark 3.0 or earlier, NULL elements are converted to empty strings. To restore the behavior before Spark 3.1, you can set `spark.sql.legacy.castComplexTypesToString.enabled` to `true`.\r\n```",
        "createdAt" : "2020-10-30T06:30:27Z",
        "updatedAt" : "2020-10-30T06:30:27Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "53a2ab9d-b703-40a9-ba3a-0927611b190c",
        "parentId" : "c8a041b4-5ebe-4252-84cc-3bf18aae49d8",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "Got it, with a follow up  pr  or revert current one?  @HyukjinKwon ",
        "createdAt" : "2020-10-30T06:38:43Z",
        "updatedAt" : "2020-10-30T06:38:43Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "4aebd0c7-1869-47e4-936f-a1602e099ad5",
        "parentId" : "c8a041b4-5ebe-4252-84cc-3bf18aae49d8",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Followup should be fine.",
        "createdAt" : "2020-10-30T06:46:48Z",
        "updatedAt" : "2020-10-30T06:46:48Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "0541ff50ba75278e984dd3414efed8dc5c339881",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +53,57 @@  - In Spark 3.1, the `schema_of_json` and `schema_of_csv` functions return the schema in the SQL format in which field names are quoted. In Spark 3.0, the function returns a catalog string without field quoting and in lower case. \n \n  - In Spark 3.1, when `spark.sql.legacy.transformationPadNullWhenValueLessThenSchema` is true, Spark will pad NULL value when script transformation's output value size less then schema size in default-serde mode(script transformation with row format of `ROW FORMAT DELIMITED`). If false, Spark will keep original behavior to throw `ArrayIndexOutOfBoundsException`.\n\n## Upgrading from Spark SQL 3.0 to 3.0.1"
  },
  {
    "id" : "4aaec37a-ee63-44dd-b004-03481d7a14fa",
    "prId" : 29635,
    "prUrl" : "https://github.com/apache/spark/pull/29635#pullrequestreview-513261828",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4da23724-92bc-4018-9bc7-990a17e02b38",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "> In Spark 3.1, incomplete interval literals, e.g. `INTERVAL '1'`, `INTERVAL '1 DAY 2'` will fail with IllegalArgumentException. In Spark 3.0, they result `NULL`s.\r\n\r\n=>\r\n\r\n> In Spark 3.1, IllegalArgumentException is returned for the incomplete interval literals, e.g. `INTERVAL '1'`, `INTERVAL '1 DAY 2'`, which are invalid. In Spark 3.0, these literals result in `NULL`s.",
        "createdAt" : "2020-10-18T23:04:20Z",
        "updatedAt" : "2020-10-18T23:04:20Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "42912692-8ccd-481e-a91f-f2b7e0b99bf9",
        "parentId" : "4da23724-92bc-4018-9bc7-990a17e02b38",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Could you fix it in followup, @yaooqinn ?",
        "createdAt" : "2020-10-21T01:53:43Z",
        "updatedAt" : "2020-10-21T01:53:43Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "7cc1f932-13b8-40fb-8bcc-24b2240479f8",
        "parentId" : "4da23724-92bc-4018-9bc7-990a17e02b38",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "OK, I will raise a PR today~",
        "createdAt" : "2020-10-21T02:26:35Z",
        "updatedAt" : "2020-10-21T02:26:35Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "daed4e68fa8b9ce44ef88078d1cb0405ec6d3fe5",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +41,45 @@  - In Spark 3.1, `path` option cannot coexist when the following methods are called with path parameter(s): `DataFrameReader.load()`, `DataFrameWriter.save()`, `DataStreamReader.load()`, or `DataStreamWriter.start()`. In addition, `paths` option cannot coexist for `DataFrameReader.load()`. For example, `spark.read.format(\"csv\").option(\"path\", \"/tmp\").load(\"/tmp2\")` or `spark.read.option(\"path\", \"/tmp\").csv(\"/tmp2\")` will throw `org.apache.spark.sql.AnalysisException`. In Spark version 3.0 and below, `path` option is overwritten if one path parameter is passed to above methods; `path` option is added to the overall paths if multiple path parameters are passed to `DataFrameReader.load()`. To restore the behavior before Spark 3.1, you can set `spark.sql.legacy.pathOptionBehavior.enabled` to `true`.\n\n  - In Spark 3.1, incomplete interval literals, e.g. `INTERVAL '1'`, `INTERVAL '1 DAY 2'` will fail with IllegalArgumentException. In Spark 3.0, they result `NULL`s.\n\n## Upgrading from Spark SQL 3.0 to 3.0.1"
  },
  {
    "id" : "d1191e94-8c57-4c57-9220-86f23ccc5bf5",
    "prId" : 29458,
    "prUrl" : "https://github.com/apache/spark/pull/29458#pullrequestreview-470100932",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0ab05d5f-3d7d-410a-8e6b-59737c8f6aa1",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "nit: we need to describe `spark.sql.ansi.enabled is false` two times? I think its okay just to describe it like this;\r\n```\r\nIn Spark 3.0 or earlier, the sum of...\r\n```\r\nor\r\n```\r\nIn Spark 3.0 or earlier, in the case, the sum of...\r\n```",
        "createdAt" : "2020-08-18T23:02:25Z",
        "updatedAt" : "2020-08-19T03:33:40Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "e2a917c5-010d-4791-a006-fbf28501c74e",
        "parentId" : "0ab05d5f-3d7d-410a-8e6b-59737c8f6aa1",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "@maropu Thanks",
        "createdAt" : "2020-08-19T03:34:01Z",
        "updatedAt" : "2020-08-19T03:34:01Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "877f47653cd1b8cc580a221870be78c5bd407bea",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +38,42 @@\n  - In Spark 3.1, when `spark.sql.ansi.enabled` is false, Spark always returns null if the sum of decimal type column overflows. In Spark 3.0 or earlier, in the case, the sum of decimal type column may return null or incorrect result, or even fails at runtime (depending on the actual query plan execution).\n\n## Upgrading from Spark SQL 3.0 to 3.0.1\n"
  },
  {
    "id" : "6b296bc8-21ab-475b-8c09-82ec070a3c98",
    "prId" : 29234,
    "prUrl" : "https://github.com/apache/spark/pull/29234#pullrequestreview-483733582",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bacebc52-934b-4cb3-84a3-3f05b7146718",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "@MaxGekk Also update this for the changes made in https://github.com/apache/spark/pull/29317?",
        "createdAt" : "2020-09-08T00:30:22Z",
        "updatedAt" : "2020-09-08T00:30:22Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      }
    ],
    "commit" : "918c77c996d632f76d9594724944550a32ce42a3",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +31,35 @@  - In Spark 3.1, `from_unixtime`, `unix_timestamp`,`to_unix_timestamp`, `to_timestamp` and `to_date` will fail if the specified datetime pattern is invalid. In Spark 3.0 or earlier, they result `NULL`.\n  \n  - In Spark 3.1, the Parquet, ORC, Avro and JSON datasources throw the exception `org.apache.spark.sql.AnalysisException: Found duplicate column(s) in the data schema` in read if they detect duplicate names in top-level columns as well in nested structures. The datasources take into account the SQL config `spark.sql.caseSensitive` while detecting column name duplicates.\n\n## Upgrading from Spark SQL 3.0 to 3.0.1"
  }
]