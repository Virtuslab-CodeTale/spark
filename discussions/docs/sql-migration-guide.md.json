[
  {
    "id" : "c2fb4c04-9bb8-4e09-89e9-c21ec81eefdb",
    "prId" : 33212,
    "prUrl" : "https://github.com/apache/spark/pull/33212#pullrequestreview-706902075",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9d4dab5a-5c78-4405-99dc-817df8e4a06a",
        "parentId" : null,
        "authorId" : "c12eb4aa-c39d-4fa9-8470-ed071c0be24a",
        "body" : "This change also affects parsing CSV string, I will add a unit test for CSV in a separate pr. cc @cloud-fan @HyukjinKwon ",
        "createdAt" : "2021-07-15T03:56:37Z",
        "updatedAt" : "2021-07-15T03:56:37Z",
        "lastEditedBy" : "c12eb4aa-c39d-4fa9-8470-ed071c0be24a",
        "tags" : [
        ]
      }
    ],
    "commit" : "9fb8fbd5598eb0d80c11bf7e129ad5c9b146f837",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +25,29 @@## Upgrading from Spark SQL 3.2 to 3.3\n\n  - In Spark 3.3, spark will fail when parsing a JSON/CSV string with `PERMISSIVE` mode and schema contains non-nullable fields. You can set mode to `FAILFAST/DROPMALFORMED` if you want to read JSON/CSV with a schema that contains nullable fields.\n\n## Upgrading from Spark SQL 3.1 to 3.2"
  }
]