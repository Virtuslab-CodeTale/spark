[
  {
    "id" : "c2fb4c04-9bb8-4e09-89e9-c21ec81eefdb",
    "prId" : 33212,
    "prUrl" : "https://github.com/apache/spark/pull/33212#pullrequestreview-706902075",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9d4dab5a-5c78-4405-99dc-817df8e4a06a",
        "parentId" : null,
        "authorId" : "c12eb4aa-c39d-4fa9-8470-ed071c0be24a",
        "body" : "This change also affects parsing CSV string, I will add a unit test for CSV in a separate pr. cc @cloud-fan @HyukjinKwon ",
        "createdAt" : "2021-07-15T03:56:37Z",
        "updatedAt" : "2021-07-15T03:56:37Z",
        "lastEditedBy" : "c12eb4aa-c39d-4fa9-8470-ed071c0be24a",
        "tags" : [
        ]
      }
    ],
    "commit" : "9fb8fbd5598eb0d80c11bf7e129ad5c9b146f837",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +25,29 @@## Upgrading from Spark SQL 3.2 to 3.3\n\n  - In Spark 3.3, spark will fail when parsing a JSON/CSV string with `PERMISSIVE` mode and schema contains non-nullable fields. You can set mode to `FAILFAST/DROPMALFORMED` if you want to read JSON/CSV with a schema that contains nullable fields.\n\n## Upgrading from Spark SQL 3.1 to 3.2"
  },
  {
    "id" : "4d327633-1813-4a91-8f3c-d273ee97cb0c",
    "prId" : 33040,
    "prUrl" : "https://github.com/apache/spark/pull/33040#pullrequestreview-690828539",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6d4958b2-3897-4e1f-a5a2-65fcf1e64da4",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "union of top-level columns is also \"left dominant\", this makes sense.",
        "createdAt" : "2021-06-23T15:34:27Z",
        "updatedAt" : "2021-06-23T15:34:27Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "eb8ddfb432ee120ba617b71f77a0f3f71d56e01c",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +98,102 @@  - In Spark 3.2, the query executions triggered by `DataFrameWriter` are always named `command` when being sent to `QueryExecutionListener`. In Spark 3.1 and earlier, the name is one of `save`, `insertInto`, `saveAsTable`.\n  \n  - In Spark 3.2, `Dataset.unionByName` with `allowMissingColumns` set to true will add missing nested fields to the end of structs. In Spark 3.1, nested struct fields are sorted alphabetically.\n\n## Upgrading from Spark SQL 3.0 to 3.1"
  },
  {
    "id" : "1854e4af-094e-4da8-9bce-1af0175b5e58",
    "prId" : 32714,
    "prUrl" : "https://github.com/apache/spark/pull/32714#pullrequestreview-672997061",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e4a58352-8efe-4954-8bbe-44aa5fae1226",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "> if (c in ('now', 'today'), current_date(), cast(c as date))\r\n\r\nWhat does it mean?",
        "createdAt" : "2021-06-01T12:02:56Z",
        "updatedAt" : "2021-06-01T12:02:56Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "277df9d5-d6d8-4b80-9016-7921e98f87ff",
        "parentId" : "e4a58352-8efe-4954-8bbe-44aa5fae1226",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "@cloud-fan See the example:\r\n```scala\r\nscala> val df = Seq(\"now\", \"2021-01-19\", \"today\").toDF(\"c\")\r\ndf: org.apache.spark.sql.DataFrame = [c: string]\r\n\r\nscala> df.selectExpr(\"if (c in ('now', 'today'), current_date(), cast(c as date))\").show(false)\r\n+----------------------------------------------------------+\r\n|(IF((c IN (now, today)), current_date(), CAST(c AS DATE)))|\r\n+----------------------------------------------------------+\r\n|2021-06-01                                                |\r\n|2021-01-19                                                |\r\n|2021-06-01                                                |\r\n+----------------------------------------------------------+\r\n```",
        "createdAt" : "2021-06-01T12:15:35Z",
        "updatedAt" : "2021-06-01T12:15:35Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "c8423fa267836d3f2c693798ea3d8fdc972c3f8b",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +92,96 @@  - In Spark 3.2, `CREATE TABLE AS SELECT` with non-empty `LOCATION` will throw `AnalysisException`. To restore the behavior before Spark 3.2, you can set `spark.sql.legacy.allowNonEmptyLocationInCTAS` to `true`.\n\n  - In Spark 3.2, special datetime values such as `epoch`, `today`, `yesterday`, `tomorrow`, and `now` are supported in typed literals only, for instance, `select timestamp'now'`. In Spark 3.1 and 3.0, such special values are supported in any casts of strings to dates/timestamps. To keep these special values as dates/timestamps in Spark 3.1 and 3.0, you should replace them manually, e.g. `if (c in ('now', 'today'), current_date(), cast(c as date))`.\n\n## Upgrading from Spark SQL 3.0 to 3.1"
  },
  {
    "id" : "8e31f087-9457-4c95-b80a-afb8b53c3b1f",
    "prId" : 32165,
    "prUrl" : "https://github.com/apache/spark/pull/32165#pullrequestreview-636304348",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d0d308bd-00eb-4a3e-a576-7a00c939fe00",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Is there any valid use case around it? Can users access `c1, c2` somehow? e.g. `SELECT c1 FROM (SELECT TRANSFORM ...)`",
        "createdAt" : "2021-04-15T06:08:32Z",
        "updatedAt" : "2021-04-15T06:08:33Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "fc76d350-a2f9-474b-bf98-51710303dee0",
        "parentId" : "d0d308bd-00eb-4a3e-a576-7a00c939fe00",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> Is there any valid use case around it? Can users access `c1, c2` somehow? e.g. `SELECT c1 FROM (SELECT TRANSFORM ...)`\r\n\r\nSuch as \r\n```\r\nSELECT TRANSFORM(a as a1, sum(b) as sum_b)\r\nUSING 'cat'\r\nFROM tbl\r\nWHERE a1 > 3\r\nHAVING sum_b > 10\r\n```",
        "createdAt" : "2021-04-15T06:11:21Z",
        "updatedAt" : "2021-04-15T06:11:55Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "4605790b-f021-4ba7-9e04-41990829be82",
        "parentId" : "d0d308bd-00eb-4a3e-a576-7a00c939fe00",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "this looks super weird and not SQL-ish.",
        "createdAt" : "2021-04-15T06:16:42Z",
        "updatedAt" : "2021-04-15T06:16:42Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "8209d0f8fb69e092e79eeffc33a4ad85cc87c46e",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +79,83 @@\n  - In Spark 3.2, `TRANSFORM` operator can't support alias in inputs. In Spark 3.1 and earlier, we can write script transform like `SELECT TRANSFORM(a AS c1, b AS c2) USING 'cat' FROM TBL`.\n\n## Upgrading from Spark SQL 3.0 to 3.1\n"
  },
  {
    "id" : "aa5f43e3-eed6-4350-b6f6-d93f0655a9aa",
    "prId" : 32025,
    "prUrl" : "https://github.com/apache/spark/pull/32025#pullrequestreview-627902181",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9ad99314-b833-4dbf-899a-71f2bcb172df",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "`the reserved properties are only affect other DDLs except for this one` what does it mean? What's \"this one\"?",
        "createdAt" : "2021-04-05T14:51:02Z",
        "updatedAt" : "2021-04-05T15:05:01Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "e760d6c1-4064-439f-8a2a-780995b456c5",
        "parentId" : "9ad99314-b833-4dbf-899a-71f2bcb172df",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "It's a typo, let me fix it:)",
        "createdAt" : "2021-04-05T14:53:20Z",
        "updatedAt" : "2021-04-05T15:05:01Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "0cf2ce1efc5344deace91a29795fe3afb7658276",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +75,79 @@\n  - In Spark 3.2, `CREATE TABLE .. LIKE ..` command can not use reserved properties. You need their specific clauses to specify them, for example, `CREATE TABLE test1 LIKE test LOCATION 'some path'`. You can set `spark.sql.legacy.notReserveProperties` to `true` to ignore the `ParseException`, in this case, these properties will be silently removed, for example: `TBLPROPERTIES('owner'='yao')` will have no effect. In Spark version 3.1 and below, the reserved properties can be used in `CREATE TABLE .. LIKE ..` command but have no side effects, for example, `TBLPROPERTIES('location'='/tmp')` does not change the location of the table but only create a headless property just like `'a'='b'`.\n\n## Upgrading from Spark SQL 3.0 to 3.1\n"
  },
  {
    "id" : "18b9b1bd-73ad-4958-b22f-ea8ae326ba63",
    "prId" : 31780,
    "prUrl" : "https://github.com/apache/spark/pull/31780#pullrequestreview-606828501",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4ae89ba5-49ac-4c6f-b9a4-511c354a0b82",
        "parentId" : null,
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "I don't think `Hive external catalog` has any problem (`HiveExternalCatalog` class).",
        "createdAt" : "2021-03-08T22:00:27Z",
        "updatedAt" : "2021-03-08T22:00:30Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      },
      {
        "id" : "c563c8a1-1be8-4ddd-bb7e-5627b9a4f355",
        "parentId" : "4ae89ba5-49ac-4c6f-b9a4-511c354a0b82",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "+1 I'll close this.",
        "createdAt" : "2021-03-09T00:25:20Z",
        "updatedAt" : "2021-03-09T00:25:21Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "d7f9234f0450e8030db6310b0c5db6f6ec54d230",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +106,110 @@  - Since Spark 3.1, CHAR/CHARACTER and VARCHAR types are supported in the table schema. Table scan/insertion will respect the char/varchar semantic. If char/varchar is used in places other than table schema, an exception will be thrown (CAST is an exception that simply treats char/varchar as string like before). To restore the behavior before Spark 3.1, which treats them as STRING types and ignores a length parameter, e.g. `CHAR(4)`, you can set `spark.sql.legacy.charVarcharAsString` to `true`.\n\n  - In Spark 3.1, `AnalysisException` is replaced by its sub-classes that are thrown for tables from Hive an external catalog in the following situations:\n    * `ALTER TABLE .. ADD PARTITION` throws `PartitionsAlreadyExistException` if new partition exists already\n    * `ALTER TABLE .. DROP PARTITION` throws `NoSuchPartitionsException` for not existing partitions"
  }
]