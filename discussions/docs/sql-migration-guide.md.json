[
  {
    "id" : "c2fb4c04-9bb8-4e09-89e9-c21ec81eefdb",
    "prId" : 33212,
    "prUrl" : "https://github.com/apache/spark/pull/33212#pullrequestreview-706902075",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9d4dab5a-5c78-4405-99dc-817df8e4a06a",
        "parentId" : null,
        "authorId" : "c12eb4aa-c39d-4fa9-8470-ed071c0be24a",
        "body" : "This change also affects parsing CSV string, I will add a unit test for CSV in a separate pr. cc @cloud-fan @HyukjinKwon ",
        "createdAt" : "2021-07-15T03:56:37Z",
        "updatedAt" : "2021-07-15T03:56:37Z",
        "lastEditedBy" : "c12eb4aa-c39d-4fa9-8470-ed071c0be24a",
        "tags" : [
        ]
      }
    ],
    "commit" : "9fb8fbd5598eb0d80c11bf7e129ad5c9b146f837",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +25,29 @@## Upgrading from Spark SQL 3.2 to 3.3\n\n  - In Spark 3.3, spark will fail when parsing a JSON/CSV string with `PERMISSIVE` mode and schema contains non-nullable fields. You can set mode to `FAILFAST/DROPMALFORMED` if you want to read JSON/CSV with a schema that contains nullable fields.\n\n## Upgrading from Spark SQL 3.1 to 3.2"
  },
  {
    "id" : "4d327633-1813-4a91-8f3c-d273ee97cb0c",
    "prId" : 33040,
    "prUrl" : "https://github.com/apache/spark/pull/33040#pullrequestreview-690828539",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6d4958b2-3897-4e1f-a5a2-65fcf1e64da4",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "union of top-level columns is also \"left dominant\", this makes sense.",
        "createdAt" : "2021-06-23T15:34:27Z",
        "updatedAt" : "2021-06-23T15:34:27Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "eb8ddfb432ee120ba617b71f77a0f3f71d56e01c",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +98,102 @@  - In Spark 3.2, the query executions triggered by `DataFrameWriter` are always named `command` when being sent to `QueryExecutionListener`. In Spark 3.1 and earlier, the name is one of `save`, `insertInto`, `saveAsTable`.\n  \n  - In Spark 3.2, `Dataset.unionByName` with `allowMissingColumns` set to true will add missing nested fields to the end of structs. In Spark 3.1, nested struct fields are sorted alphabetically.\n\n## Upgrading from Spark SQL 3.0 to 3.1"
  },
  {
    "id" : "1854e4af-094e-4da8-9bce-1af0175b5e58",
    "prId" : 32714,
    "prUrl" : "https://github.com/apache/spark/pull/32714#pullrequestreview-672997061",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e4a58352-8efe-4954-8bbe-44aa5fae1226",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "> if (c in ('now', 'today'), current_date(), cast(c as date))\r\n\r\nWhat does it mean?",
        "createdAt" : "2021-06-01T12:02:56Z",
        "updatedAt" : "2021-06-01T12:02:56Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "277df9d5-d6d8-4b80-9016-7921e98f87ff",
        "parentId" : "e4a58352-8efe-4954-8bbe-44aa5fae1226",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "@cloud-fan See the example:\r\n```scala\r\nscala> val df = Seq(\"now\", \"2021-01-19\", \"today\").toDF(\"c\")\r\ndf: org.apache.spark.sql.DataFrame = [c: string]\r\n\r\nscala> df.selectExpr(\"if (c in ('now', 'today'), current_date(), cast(c as date))\").show(false)\r\n+----------------------------------------------------------+\r\n|(IF((c IN (now, today)), current_date(), CAST(c AS DATE)))|\r\n+----------------------------------------------------------+\r\n|2021-06-01                                                |\r\n|2021-01-19                                                |\r\n|2021-06-01                                                |\r\n+----------------------------------------------------------+\r\n```",
        "createdAt" : "2021-06-01T12:15:35Z",
        "updatedAt" : "2021-06-01T12:15:35Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "c8423fa267836d3f2c693798ea3d8fdc972c3f8b",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +92,96 @@  - In Spark 3.2, `CREATE TABLE AS SELECT` with non-empty `LOCATION` will throw `AnalysisException`. To restore the behavior before Spark 3.2, you can set `spark.sql.legacy.allowNonEmptyLocationInCTAS` to `true`.\n\n  - In Spark 3.2, special datetime values such as `epoch`, `today`, `yesterday`, `tomorrow`, and `now` are supported in typed literals only, for instance, `select timestamp'now'`. In Spark 3.1 and 3.0, such special values are supported in any casts of strings to dates/timestamps. To keep these special values as dates/timestamps in Spark 3.1 and 3.0, you should replace them manually, e.g. `if (c in ('now', 'today'), current_date(), cast(c as date))`.\n\n## Upgrading from Spark SQL 3.0 to 3.1"
  }
]