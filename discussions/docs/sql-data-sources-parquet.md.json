[
  {
    "id" : "7ed0e2cd-9b50-4167-92d0-b8d86d55108a",
    "prId" : 32895,
    "prUrl" : "https://github.com/apache/spark/pull/32895#pullrequestreview-686492505",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1215fd1d-61fb-4669-9ff1-45c8fa288f99",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "To be complete, could you add more examples for the other languages like `python`, `SQL`, `r`?",
        "createdAt" : "2021-06-13T15:51:46Z",
        "updatedAt" : "2021-06-13T15:51:47Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "409acb0e-13a5-4b44-b7a3-6e114a2978d0",
        "parentId" : "1215fd1d-61fb-4669-9ff1-45c8fa288f99",
        "authorId" : "0447cdcd-ca55-4eb1-986e-528fbe1e853a",
        "body" : "Spark/PME* functionality has been mass tested with Scala, so I'm mostly comfortable with this example. Also, ORC column encryption has one [example](https://github.com/apache/spark/blob/master/docs/sql-data-sources-orc.md#columnar-encryption) too, I've followed the same approach. But if needed, I can add a Python sample, this is known to work with PME. In the future, as the community gathers experience with PME in e.g. Java or R applications, this doc could be augmented with additional examples.\r\n(* PME - \"parquet modular encryption\")",
        "createdAt" : "2021-06-14T12:23:51Z",
        "updatedAt" : "2021-06-14T12:44:16Z",
        "lastEditedBy" : "0447cdcd-ca55-4eb1-986e-528fbe1e853a",
        "tags" : [
        ]
      },
      {
        "id" : "842d3c10-e3ee-4bbf-9bc6-97812b329dd0",
        "parentId" : "1215fd1d-61fb-4669-9ff1-45c8fa288f99",
        "authorId" : "0447cdcd-ca55-4eb1-986e-528fbe1e853a",
        "body" : "I've removed the tabs, and stressed that this an illustration-only sample that can be run without a KMS server - just `spark-shell` is sufficient. This should make a scala example a sort of an intuitive choice in this place.",
        "createdAt" : "2021-06-17T12:49:50Z",
        "updatedAt" : "2021-06-17T12:49:50Z",
        "lastEditedBy" : "0447cdcd-ca55-4eb1-986e-528fbe1e853a",
        "tags" : [
        ]
      },
      {
        "id" : "e11cf6b1-8c47-41a7-8f55-576ae532db1a",
        "parentId" : "1215fd1d-61fb-4669-9ff1-45c8fa288f99",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "ORC column encryption has SQL example because it can work in all environments (Spark Shell/PySpark/SparkR/SQL Shell/STS). I'm curious if Apache Parquet doesn't work with SQL environment, for example Spark Thrift Server?",
        "createdAt" : "2021-06-17T15:03:57Z",
        "updatedAt" : "2021-06-17T15:03:57Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "b836b64f-0ee4-4a4e-9214-c999b7fb7de5",
        "parentId" : "1215fd1d-61fb-4669-9ff1-45c8fa288f99",
        "authorId" : "0447cdcd-ca55-4eb1-986e-528fbe1e853a",
        "body" : "Yep, I also wonder about this; but the bulk of my personal experience is with direct activation of this function - so I'm comfortable with adding these samples and parameters to the Spark documentation (since they are well tested etc). If other community members get to test and verify PME via SQL interface, I think it would be a good future addition to this section.",
        "createdAt" : "2021-06-17T15:53:10Z",
        "updatedAt" : "2021-06-17T15:53:10Z",
        "lastEditedBy" : "0447cdcd-ca55-4eb1-986e-528fbe1e853a",
        "tags" : [
        ]
      }
    ],
    "commit" : "c3d840e3a2db65c306064485c83efa04d294371f",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +261,265 @@\n<div data-lang=\"scala\"  markdown=\"1\">\n{% highlight scala %}\n\nsc.hadoopConfiguration.set(\"parquet.encryption.kms.client.class\" ,"
  },
  {
    "id" : "da726e3c-21f3-4b96-8156-dbd01f287fde",
    "prId" : 32895,
    "prUrl" : "https://github.com/apache/spark/pull/32895#pullrequestreview-682851119",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a157f83b-9be8-47f8-93b5-583ccf90f474",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "ditto. Please use the Hadoop KMS example.",
        "createdAt" : "2021-06-13T15:55:25Z",
        "updatedAt" : "2021-06-13T15:55:25Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "5d111051-84bf-4a36-9035-05ff3e66b7af",
        "parentId" : "a157f83b-9be8-47f8-93b5-583ccf90f474",
        "authorId" : "0447cdcd-ca55-4eb1-986e-528fbe1e853a",
        "body" : "As I mentioned above, we don't support any particular KMS. The advantage of the mock InMemoryKMS class is that it provides an easy to understand demo code, and can be tried without any KMS server. But I'd agree we must stress this is not a real KMS and should never be used in production.",
        "createdAt" : "2021-06-14T12:30:40Z",
        "updatedAt" : "2021-06-14T12:30:41Z",
        "lastEditedBy" : "0447cdcd-ca55-4eb1-986e-528fbe1e853a",
        "tags" : [
        ]
      }
    ],
    "commit" : "c3d840e3a2db65c306064485c83efa04d294371f",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +264,268 @@\nsc.hadoopConfiguration.set(\"parquet.encryption.kms.client.class\" ,\n                           \"org.apache.parquet.crypto.keytools.mocks.InMemoryKMS\")\n\n// Explicit master keys (base64 encoded) - required only for mock InMemoryKMS"
  },
  {
    "id" : "24c565ef-f57a-4cc6-8310-ea6d16c12762",
    "prId" : 32895,
    "prUrl" : "https://github.com/apache/spark/pull/32895#pullrequestreview-682851742",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "78372b40-714f-4e4b-af1a-ee4e401412da",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Ditto. We should remove this.",
        "createdAt" : "2021-06-13T15:55:33Z",
        "updatedAt" : "2021-06-13T15:55:33Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "b7abb882-1aed-40e8-a87e-e6d6bf833516",
        "parentId" : "78372b40-714f-4e4b-af1a-ee4e401412da",
        "authorId" : "0447cdcd-ca55-4eb1-986e-528fbe1e853a",
        "body" : "as above",
        "createdAt" : "2021-06-14T12:31:19Z",
        "updatedAt" : "2021-06-14T12:31:20Z",
        "lastEditedBy" : "0447cdcd-ca55-4eb1-986e-528fbe1e853a",
        "tags" : [
        ]
      }
    ],
    "commit" : "c3d840e3a2db65c306064485c83efa04d294371f",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +268,272 @@// Explicit master keys (base64 encoded) - required only for mock InMemoryKMS\nsc.hadoopConfiguration.set(\"parquet.encryption.key.list\" ,\n                   \"keyA:AAECAwQFBgcICQoLDA0ODw== ,  keyB:AAECAAECAAECAAECAAECAA==\")\n\n// Activate Parquet encryption, driven by Hadoop properties"
  },
  {
    "id" : "1ed924ef-fc93-4d1d-a5d1-7c9ab6655fef",
    "prId" : 32895,
    "prUrl" : "https://github.com/apache/spark/pull/32895#pullrequestreview-682858344",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5d8bd15e-aecb-4089-9c94-9d76ceacde8c",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Don't we need any options for reading encrypted file?",
        "createdAt" : "2021-06-13T21:40:11Z",
        "updatedAt" : "2021-06-13T21:40:11Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "3d24388d-1ce1-4e20-9d1b-e3ff15f37049",
        "parentId" : "5d8bd15e-aecb-4089-9c94-9d76ceacde8c",
        "authorId" : "0447cdcd-ca55-4eb1-986e-528fbe1e853a",
        "body" : "Nope. The options are passed in writing only; and stored in Parquet metadata, so the readers don't need them.",
        "createdAt" : "2021-06-14T12:38:18Z",
        "updatedAt" : "2021-06-14T12:38:18Z",
        "lastEditedBy" : "0447cdcd-ca55-4eb1-986e-528fbe1e853a",
        "tags" : [
        ]
      }
    ],
    "commit" : "c3d840e3a2db65c306064485c83efa04d294371f",
    "line" : 34,
    "diffHunk" : "@@ -1,1 +283,287 @@\n// Read encrypted dataframe files\nval df2 = spark.read.parquet(\"/path/to/table.parquet.encrypted\")\n\n{% endhighlight %}"
  },
  {
    "id" : "5b06ea05-41cb-4c82-90ff-1e0a7ed9bd9c",
    "prId" : 32895,
    "prUrl" : "https://github.com/apache/spark/pull/32895#pullrequestreview-709631097",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f2dc3602-f50f-4333-89a4-e5498c94d3ac",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : " This section looks proper to Apache Parquet website and I believe that a link to Apache Parquet website will be enough. Isn't it a little weird to have Apache Parquet's document about a class which should not be used in a real deployment. Please remove this section.",
        "createdAt" : "2021-07-14T14:27:50Z",
        "updatedAt" : "2021-07-14T14:27:51Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "593f2adf-d4a0-4d6b-8dff-f5031d5603de",
        "parentId" : "f2dc3602-f50f-4333-89a4-e5498c94d3ac",
        "authorId" : "0447cdcd-ca55-4eb1-986e-528fbe1e853a",
        "body" : "As I mentioned in the comment above, Parquet doesn't have a proper documentation website, unfortunately.",
        "createdAt" : "2021-07-14T15:02:40Z",
        "updatedAt" : "2021-07-14T15:02:40Z",
        "lastEditedBy" : "0447cdcd-ca55-4eb1-986e-528fbe1e853a",
        "tags" : [
        ]
      },
      {
        "id" : "f990b1fd-3e84-4992-bb85-7cb7279524fd",
        "parentId" : "f2dc3602-f50f-4333-89a4-e5498c94d3ac",
        "authorId" : "0447cdcd-ca55-4eb1-986e-528fbe1e853a",
        "body" : "To expand more on this point - removing this section would mean removal of the previous section too, since it also uses the \"Hello World\" example. Therefore, it removes the full content of this pull request, comprised of the two sections.\r\nI agree it would be reasonable to move some of the content to a Parquet documentation site, but such site doesn't exist.., AFAIK Parquet doesn't have API documentation (keeps only a page on its Hadoop parameters).\r\n\r\nI realize this section/PR seems to be somewhat unusual compared to other Spark/Parquet doc sections, but there is a simple reason. Encryption is somewhat unusual compared to other Parquet features. To be really useful, it requires more than just Hadoop parameters. It has an API, or more specifically, an interface for custom KMS Client classes tailored for user-specific KMS/IAM systems, deployed in their organizations. Providing any such classes in Parquet or Spark packages will be totally pointless, as detailed in other comments. \r\n\r\nTherefore, the approach taken here, is to provide a simple to understand \"Hello World\" KmsClient class, which is also easy to experiment with (since it runs alone and doesn't require a real KMS Server). Followed by an explanation about how to take the next step and develop a real-life KmsClient. This should provide sufficient documentation for new adopters of the Spark/ParquetEncryption capability, which is already used in numerous deployments.",
        "createdAt" : "2021-07-19T11:56:33Z",
        "updatedAt" : "2021-07-19T11:57:02Z",
        "lastEditedBy" : "0447cdcd-ca55-4eb1-986e-528fbe1e853a",
        "tags" : [
        ]
      },
      {
        "id" : "2308fbe4-fde2-4e40-8d24-bcb9d02a8eb6",
        "parentId" : "f2dc3602-f50f-4333-89a4-e5498c94d3ac",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "I think the question is, is this specific to Spark, or generally applicable to Parquet? if the latter, it is better in the Parquet docs. I understand it's not there, but a similar PR could go into Parquet docs. It has API docs, of course.",
        "createdAt" : "2021-07-19T14:07:48Z",
        "updatedAt" : "2021-07-19T14:09:49Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "e54826c5-12d1-4a59-b2b6-f54ba1392739",
        "parentId" : "f2dc3602-f50f-4333-89a4-e5498c94d3ac",
        "authorId" : "0447cdcd-ca55-4eb1-986e-528fbe1e853a",
        "body" : "Mm, a good point. Thinking of this, I'm not aware of any other analytic framework where Parquet encryption is (or can be) activated this way. While this approach is supposed to be general, it was designed and tested within Spark. In other frameworks, updating parquet to 1.12.0 is not sufficient, they need to call low-level Parquet APIs to leverage the encryption feature.\r\n\r\nStill, I agree it would be good to document Parquet APIs (general; not only encryption). However, there is a high chance such documentation simply doesn't exist.. At least I couldn't find anything, besides a page with the parquet-hadoop parameters..\r\n\r\nGiven these two points, I believe it is reasonable to add this section in the Spark documentation.",
        "createdAt" : "2021-07-19T14:39:42Z",
        "updatedAt" : "2021-07-19T14:39:42Z",
        "lastEditedBy" : "0447cdcd-ca55-4eb1-986e-528fbe1e853a",
        "tags" : [
        ]
      }
    ],
    "commit" : "c3d840e3a2db65c306064485c83efa04d294371f",
    "line" : 41,
    "diffHunk" : "@@ -1,1 +290,294 @@\n\n#### KMS Client\n\nThe InMemoryKMS class is provided only for illustration and simple demonstration of Parquet encryption functionality. **It should not be used in a real deployment**. The master encryption keys must be kept and managed in a production-grade KMS system, deployed in user's organization. Rollout of Spark with Parquet encryption requires implementation of a client class for the KMS server. Parquet provides a plug-in [interface](https://github.com/apache/parquet-mr/blob/apache-parquet-1.12.0/parquet-hadoop/src/main/java/org/apache/parquet/crypto/keytools/KmsClient.java) for development of such classes,"
  },
  {
    "id" : "460a057d-7dd7-4e94-8e61-27b1dce2a786",
    "prId" : 32161,
    "prUrl" : "https://github.com/apache/spark/pull/32161#pullrequestreview-667693142",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a6e3aead-4a8e-4349-890b-bcd402cf98b4",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "also mention:\r\n\r\n```\r\n* `OPTIONS` clause at [CREATE TABLE USING DATA_SOURCE](sql-ref-syntax-ddl-create-table-datasource.html)\r\n```",
        "createdAt" : "2021-05-25T11:10:03Z",
        "updatedAt" : "2021-05-25T11:10:03Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "d6417a8124eb61390089313d108eff18fd89e412",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +260,264 @@  *  `DataFrameWriter`\n  *  `DataStreamReader`\n  *  `DataStreamWriter`\n\n<table class=\"table\">"
  },
  {
    "id" : "5db604b8-ded7-4c7c-a0a4-dd4d83d772f0",
    "prId" : 31564,
    "prUrl" : "https://github.com/apache/spark/pull/31564#pullrequestreview-592867313",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c5f70419-308e-4099-a07c-a0fcd74eb181",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Do you plan to list other datasource options here? If not, I would write it as another section like Rebasing Datetime. ",
        "createdAt" : "2021-02-18T00:33:57Z",
        "updatedAt" : "2021-02-18T00:33:57Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "3b6f1c59-f184-40e7-9f54-546b3c1952eb",
        "parentId" : "c5f70419-308e-4099-a07c-a0fcd74eb181",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "Yes, other options should be here.",
        "createdAt" : "2021-02-18T05:41:13Z",
        "updatedAt" : "2021-02-18T05:41:13Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "773cb0b6351a4a87ae755305e0e889e110168eee",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +253,257 @@</div>\n\n## Data Source Option\n\nData source options of Parquet can be set via:"
  },
  {
    "id" : "267b3ee7-4d54-4c43-b741-e6ea4661651f",
    "prId" : 29613,
    "prUrl" : "https://github.com/apache/spark/pull/29613#pullrequestreview-480979803",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6fe1ecef-5d73-4fef-b5e5-81a92c4ccd46",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I think all sql confs are now documented at https://spark.apache.org/docs/latest/configuration.html#spark-sql",
        "createdAt" : "2020-09-02T01:16:12Z",
        "updatedAt" : "2020-09-02T01:16:13Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "586f2605-83b3-4fbd-8bcd-a996876012b5",
        "parentId" : "6fe1ecef-5d73-4fef-b5e5-81a92c4ccd46",
        "authorId" : "ae8d9025-e683-4bbf-8ff8-8fdb08fa16db",
        "body" : "@HyukjinKwon thanks! I didn't see that.\r\n\r\nThat said, anyone following the parquet guide doesn't see all the parquet options. Perhaps this should be changed this to link back to the general config docs and state that all parquet options can be found there?\r\n\r\nNot having all options on the guide page, or at least an indicator that there are other values, is a user experience issue.",
        "createdAt" : "2020-09-02T15:41:29Z",
        "updatedAt" : "2020-09-02T15:42:41Z",
        "lastEditedBy" : "ae8d9025-e683-4bbf-8ff8-8fdb08fa16db",
        "tags" : [
        ]
      },
      {
        "id" : "98ed2eb0-20a6-418c-aba3-021f34899f7b",
        "parentId" : "6fe1ecef-5d73-4fef-b5e5-81a92c4ccd46",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Probably we could add a link to the SQL configuration page to avoid duplication.",
        "createdAt" : "2020-09-02T16:09:51Z",
        "updatedAt" : "2020-09-02T16:09:51Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "d5bb07a76c76b6db8589527820af70ffe72125c1",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +280,284 @@</tr>\n<tr>\n  <td><code>spark.sql.parquet.outputTimestampType</code></td>\n  <td>INT96</td>\n  <td>"
  },
  {
    "id" : "715e49b2-6556-4eca-82be-9f0ebde5b687",
    "prId" : 28064,
    "prUrl" : "https://github.com/apache/spark/pull/28064#pullrequestreview-383364943",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "10a1f3e3-2233-4ff4-be22-198015039e98",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-2927, commit ID: de501e169f24e4573747aec85b7651c98633c028#diff-41ef65b9ef5b518f77e2a03559893f4d",
        "createdAt" : "2020-03-29T01:43:22Z",
        "updatedAt" : "2020-03-29T01:43:23Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "6787d16f5e86e5a6dbcb2d9b25c41ec5dfbedb1d",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +268,272 @@    flag tells Spark SQL to interpret binary data as a string to provide compatibility with these systems.\n  </td>\n  <td>1.1.1</td>\n</tr>\n<tr>"
  },
  {
    "id" : "63eff345-4a27-4abe-af8d-c5e043d17ec7",
    "prId" : 28064,
    "prUrl" : "https://github.com/apache/spark/pull/28064#pullrequestreview-383364957",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "71b2d275-0358-41bd-a2a3-e7c899183e3b",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-4987, commit ID: 67d52207b5cf2df37ca70daff2a160117510f55e#diff-41ef65b9ef5b518f77e2a03559893f4d",
        "createdAt" : "2020-03-29T01:43:43Z",
        "updatedAt" : "2020-03-29T01:43:44Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "6787d16f5e86e5a6dbcb2d9b25c41ec5dfbedb1d",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +277,281 @@    flag tells Spark SQL to interpret INT96 data as a timestamp to provide compatibility with these systems.\n  </td>\n  <td>1.3.0</td>\n</tr>\n<tr>"
  },
  {
    "id" : "d2bc4ef5-bbbc-4ecd-abc2-282f9e9c183a",
    "prId" : 28064,
    "prUrl" : "https://github.com/apache/spark/pull/28064#pullrequestreview-383364982",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c6d815ad-de35-4430-88a9-ae811d28214e",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-3131, commit ID: 3a9d874d7a46ab8b015631d91ba479d9a0ba827f#diff-41ef65b9ef5b518f77e2a03559893f4d",
        "createdAt" : "2020-03-29T01:44:13Z",
        "updatedAt" : "2020-03-29T01:44:14Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "6787d16f5e86e5a6dbcb2d9b25c41ec5dfbedb1d",
    "line" : 29,
    "diffHunk" : "@@ -1,1 +290,294 @@    <code>BrotliCodec</code> to be installed.\n  </td>\n  <td>1.1.1</td>\n</tr>\n<tr>"
  },
  {
    "id" : "be476b7b-d34a-422f-a542-b813d39fa480",
    "prId" : 28064,
    "prUrl" : "https://github.com/apache/spark/pull/28064#pullrequestreview-383364994",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "00f43ea3-2629-4dfe-975c-e1b6d4722183",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-4391, commit ID: 576688aa2a19bd4ba239a2b93af7947f983e5124#diff-41ef65b9ef5b518f77e2a03559893f4d",
        "createdAt" : "2020-03-29T01:44:32Z",
        "updatedAt" : "2020-03-29T01:44:33Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "6787d16f5e86e5a6dbcb2d9b25c41ec5dfbedb1d",
    "line" : 35,
    "diffHunk" : "@@ -1,1 +296,300 @@  <td>true</td>\n  <td>Enables Parquet filter push-down optimization when set to true.</td>\n  <td>1.2.0</td>\n</tr>\n<tr>"
  },
  {
    "id" : "7fc65cc8-692e-460c-8b53-95efe5e0ab20",
    "prId" : 28064,
    "prUrl" : "https://github.com/apache/spark/pull/28064#pullrequestreview-383365014",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3d287e18-5dc0-4f17-a76e-4f03d73aa2d8",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-2406, commit ID: cc4015d2fa3785b92e6ab079b3abcf17627f7c56#diff-ff50aea397a607b79df9bec6f2a841db",
        "createdAt" : "2020-03-29T01:45:00Z",
        "updatedAt" : "2020-03-29T01:45:00Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "6787d16f5e86e5a6dbcb2d9b25c41ec5dfbedb1d",
    "line" : 43,
    "diffHunk" : "@@ -1,1 +305,309 @@    support.\n  </td>\n  <td>1.1.1</td>\n</tr>\n<tr>"
  },
  {
    "id" : "cf3158c8-a5cc-420c-b367-4a78f6465b2a",
    "prId" : 28064,
    "prUrl" : "https://github.com/apache/spark/pull/28064#pullrequestreview-383365024",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8116063f-2a72-430d-8d10-91126f3242f4",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-8690, commit ID: 246265f2bb056d5e9011d3331b809471a24ff8d7#diff-41ef65b9ef5b518f77e2a03559893f4d",
        "createdAt" : "2020-03-29T01:45:19Z",
        "updatedAt" : "2020-03-29T01:45:19Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "6787d16f5e86e5a6dbcb2d9b25c41ec5dfbedb1d",
    "line" : 51,
    "diffHunk" : "@@ -1,1 +316,320 @@    </p>\n  </td>\n  <td>1.5.0</td>\n</tr>\n<tr>"
  },
  {
    "id" : "996a5d57-844f-4f92-a6f0-25bec1a59485",
    "prId" : 28064,
    "prUrl" : "https://github.com/apache/spark/pull/28064#pullrequestreview-383365036",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0f39c3ff-98d3-4557-a542-337bff4e16d5",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-10400, commit ID: 01cd688f5245cbb752863100b399b525b31c3510#diff-41ef65b9ef5b518f77e2a03559893f4d",
        "createdAt" : "2020-03-29T01:45:43Z",
        "updatedAt" : "2020-03-29T01:45:44Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "6787d16f5e86e5a6dbcb2d9b25c41ec5dfbedb1d",
    "line" : 59,
    "diffHunk" : "@@ -1,1 +328,332 @@    with systems that do not support this newer format, set to true.\n  </td>\n  <td>1.6.0</td>\n</tr>\n</table>"
  }
]