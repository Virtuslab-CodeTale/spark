[
  {
    "id" : "ef797560-648a-4d1d-8a96-073c2880063b",
    "prId" : 31611,
    "prUrl" : "https://github.com/apache/spark/pull/31611#pullrequestreview-606879779",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "07d4aee1-9c49-4836-bbe3-e1d0e7fb77ff",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Is the 1.0 quantile useful? there's already a max in the API or is that not true.\r\nI wonder if 0.9 or 0.95 is more useful\r\n\r\nMy only concern here is returning more data in the API but I suppose this is optional.",
        "createdAt" : "2021-03-08T13:07:20Z",
        "updatedAt" : "2021-03-08T13:07:20Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "71c8c9fd-d286-4cb2-a4cd-9138e5f35924",
        "parentId" : "07d4aee1-9c49-4836-bbe3-e1d0e7fb77ff",
        "authorId" : "050916e0-b79b-4b4c-a6a1-4df5cf6c0da3",
        "body" : "The default quantiles value is 0.0,0.25,0.5,0.75,1.0 so that it is consistent with Spark web UI.   The quantile 1.0 value is useful to show the maximal metric value so that users can easily identify the bottleneck by using the REST call.  To my understanding, some downstream product such as LinkedIn/Dr. Elephant (https://github.com/linkedin/dr-elephant) computes the ratio of quantile-1.0-value/quantile-0.5-value to decide how skewed/bottlenecked the load is among the tasks/executors of a stage.\r\nIf a user does not like the default value, he can always change it by explicitly specifying the quantile values he wants.",
        "createdAt" : "2021-03-09T00:59:03Z",
        "updatedAt" : "2021-03-09T01:19:58Z",
        "lastEditedBy" : "050916e0-b79b-4b4c-a6a1-4df5cf6c0da3",
        "tags" : [
        ]
      },
      {
        "id" : "7ba6f816-e2ca-4111-985f-e0ff40a1bac4",
        "parentId" : "07d4aee1-9c49-4836-bbe3-e1d0e7fb77ff",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> My only concern here is returning more data in the API but I suppose this is optional.\r\n\r\nYea, it's optional, but we can collect all data we need for each stage together instead of to call restful multiple times for different data.  and these data is also very useful for us to analysis spark jobs.\r\n",
        "createdAt" : "2021-03-09T02:27:19Z",
        "updatedAt" : "2021-03-09T02:27:19Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "22f85006551f34272c97c963a2879cfd47c0ad8e",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +482,486 @@        <br><code>?details=true</code> lists all attempts with the task data for the given stage.\n        <br><code>?withSummaries=true</code> lists task metrics distribution and executor metrics distribution of each attempt.\n        <br><code>?quantiles=0.1,0.25,0.5,0.75,1.0</code> summarize the metrics with the given quantiles. Query parameter quantiles takes effect only when <code>withSummaries=true</code>. Default value is <code>0.0,0.25,0.5,0.75,1.0</code>. \n      <br>Example:\n        <br><code>?details=true</code>"
  }
]