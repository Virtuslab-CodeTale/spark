[
  {
    "id" : "ef797560-648a-4d1d-8a96-073c2880063b",
    "prId" : 31611,
    "prUrl" : "https://github.com/apache/spark/pull/31611#pullrequestreview-606879779",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "07d4aee1-9c49-4836-bbe3-e1d0e7fb77ff",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Is the 1.0 quantile useful? there's already a max in the API or is that not true.\r\nI wonder if 0.9 or 0.95 is more useful\r\n\r\nMy only concern here is returning more data in the API but I suppose this is optional.",
        "createdAt" : "2021-03-08T13:07:20Z",
        "updatedAt" : "2021-03-08T13:07:20Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "71c8c9fd-d286-4cb2-a4cd-9138e5f35924",
        "parentId" : "07d4aee1-9c49-4836-bbe3-e1d0e7fb77ff",
        "authorId" : "050916e0-b79b-4b4c-a6a1-4df5cf6c0da3",
        "body" : "The default quantiles value is 0.0,0.25,0.5,0.75,1.0 so that it is consistent with Spark web UI.   The quantile 1.0 value is useful to show the maximal metric value so that users can easily identify the bottleneck by using the REST call.  To my understanding, some downstream product such as LinkedIn/Dr. Elephant (https://github.com/linkedin/dr-elephant) computes the ratio of quantile-1.0-value/quantile-0.5-value to decide how skewed/bottlenecked the load is among the tasks/executors of a stage.\r\nIf a user does not like the default value, he can always change it by explicitly specifying the quantile values he wants.",
        "createdAt" : "2021-03-09T00:59:03Z",
        "updatedAt" : "2021-03-09T01:19:58Z",
        "lastEditedBy" : "050916e0-b79b-4b4c-a6a1-4df5cf6c0da3",
        "tags" : [
        ]
      },
      {
        "id" : "7ba6f816-e2ca-4111-985f-e0ff40a1bac4",
        "parentId" : "07d4aee1-9c49-4836-bbe3-e1d0e7fb77ff",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> My only concern here is returning more data in the API but I suppose this is optional.\r\n\r\nYea, it's optional, but we can collect all data we need for each stage together instead of to call restful multiple times for different data.  and these data is also very useful for us to analysis spark jobs.\r\n",
        "createdAt" : "2021-03-09T02:27:19Z",
        "updatedAt" : "2021-03-09T02:27:19Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "22f85006551f34272c97c963a2879cfd47c0ad8e",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +482,486 @@        <br><code>?details=true</code> lists all attempts with the task data for the given stage.\n        <br><code>?withSummaries=true</code> lists task metrics distribution and executor metrics distribution of each attempt.\n        <br><code>?quantiles=0.1,0.25,0.5,0.75,1.0</code> summarize the metrics with the given quantiles. Query parameter quantiles takes effect only when <code>withSummaries=true</code>. Default value is <code>0.0,0.25,0.5,0.75,1.0</code>. \n      <br>Example:\n        <br><code>?details=true</code>"
  },
  {
    "id" : "1e9255a5-1e32-46ec-b53a-d2f4ab74b010",
    "prId" : 30713,
    "prUrl" : "https://github.com/apache/spark/pull/30713#pullrequestreview-549977929",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "18d9d424-3190-460a-9f78-83d4ca4e2411",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Why we need this example? You think it is difficult for users to know how to pass these parameter into Spark?",
        "createdAt" : "2020-12-11T04:19:22Z",
        "updatedAt" : "2020-12-11T04:19:23Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "6ba5bb11-484b-4d49-b603-fe2fdddac2fc",
        "parentId" : "18d9d424-3190-460a-9f78-83d4ca4e2411",
        "authorId" : "48cfdd1d-e6ce-4ee2-8cd7-283e4555a88c",
        "body" : "Yes, I loss two weeks and more than 80 hours of my life looking for a way to monitoring my scala-spark application, because there is not an easy and official example to do that. ",
        "createdAt" : "2020-12-11T10:30:05Z",
        "updatedAt" : "2020-12-11T10:34:06Z",
        "lastEditedBy" : "48cfdd1d-e6ce-4ee2-8cd7-283e4555a88c",
        "tags" : [
        ]
      }
    ],
    "commit" : "b723e4799af350edd59fcbc22a2e340125cf780a",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +1041,1045 @@Also it could be done under spark session like:\n```\nimport org.apache.spark.sql.SparkSession\n\nval spark = SparkSession.builder"
  },
  {
    "id" : "45d5acdb-82c6-4d1b-a0fd-c22038df5de7",
    "prId" : 30713,
    "prUrl" : "https://github.com/apache/spark/pull/30713#pullrequestreview-552289905",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8823390d-45a6-4da0-bc5a-559be4fbf339",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "isn't this a copy paste of the configurations above? I don't think this is very useful.",
        "createdAt" : "2020-12-15T10:05:49Z",
        "updatedAt" : "2020-12-15T10:05:49Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "b723e4799af350edd59fcbc22a2e340125cf780a",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +1051,1055 @@            .config(\"spark.metrics.conf.*.sink.graphite.unit\",\"seconds\")\n            .config(\"spark.metrics.conf.*.sink.graphite.prefix\",\"optional_prefix\")\n            .config(\"spark.metrics.conf.*.sink.graphite.regex\",\"optional_regex_to_send_matching_metrics\")\t\n            .appName(\"YOUR_APP_NAME\")\n            .getOrCreate()"
  },
  {
    "id" : "53781b85-632a-4dcc-96da-9ef510b8c3ab",
    "prId" : 29906,
    "prUrl" : "https://github.com/apache/spark/pull/29906#pullrequestreview-503924845",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5e873836-bebe-4b8b-94ed-8063782167ee",
        "parentId" : null,
        "authorId" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "body" : "Is it even worth mentioning in the documentation?",
        "createdAt" : "2020-10-02T23:07:00Z",
        "updatedAt" : "2020-10-28T13:50:33Z",
        "lastEditedBy" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "tags" : [
        ]
      },
      {
        "id" : "b6414f35-ced7-4ba7-b012-af27247f83c6",
        "parentId" : "5e873836-bebe-4b8b-94ed-8063782167ee",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "I think it is as until we come up with different way of deprecating only way to tell users",
        "createdAt" : "2020-10-07T14:13:00Z",
        "updatedAt" : "2020-10-28T13:50:33Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "b38dd66500cdd4a12f78cca1eeacf116f0ea5b4e",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1126,1130 @@  - stages.skippedStages.count\n  - stages.completedStages.count\n  - tasks.blackListedExecutors.count // deprecated use excludedExecutors instead\n  - tasks.excludedExecutors.count\n  - tasks.completedTasks.count"
  }
]