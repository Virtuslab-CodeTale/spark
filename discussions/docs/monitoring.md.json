[
  {
    "id" : "ef797560-648a-4d1d-8a96-073c2880063b",
    "prId" : 31611,
    "prUrl" : "https://github.com/apache/spark/pull/31611#pullrequestreview-606879779",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "07d4aee1-9c49-4836-bbe3-e1d0e7fb77ff",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Is the 1.0 quantile useful? there's already a max in the API or is that not true.\r\nI wonder if 0.9 or 0.95 is more useful\r\n\r\nMy only concern here is returning more data in the API but I suppose this is optional.",
        "createdAt" : "2021-03-08T13:07:20Z",
        "updatedAt" : "2021-03-08T13:07:20Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "71c8c9fd-d286-4cb2-a4cd-9138e5f35924",
        "parentId" : "07d4aee1-9c49-4836-bbe3-e1d0e7fb77ff",
        "authorId" : "050916e0-b79b-4b4c-a6a1-4df5cf6c0da3",
        "body" : "The default quantiles value is 0.0,0.25,0.5,0.75,1.0 so that it is consistent with Spark web UI.   The quantile 1.0 value is useful to show the maximal metric value so that users can easily identify the bottleneck by using the REST call.  To my understanding, some downstream product such as LinkedIn/Dr. Elephant (https://github.com/linkedin/dr-elephant) computes the ratio of quantile-1.0-value/quantile-0.5-value to decide how skewed/bottlenecked the load is among the tasks/executors of a stage.\r\nIf a user does not like the default value, he can always change it by explicitly specifying the quantile values he wants.",
        "createdAt" : "2021-03-09T00:59:03Z",
        "updatedAt" : "2021-03-09T01:19:58Z",
        "lastEditedBy" : "050916e0-b79b-4b4c-a6a1-4df5cf6c0da3",
        "tags" : [
        ]
      },
      {
        "id" : "7ba6f816-e2ca-4111-985f-e0ff40a1bac4",
        "parentId" : "07d4aee1-9c49-4836-bbe3-e1d0e7fb77ff",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> My only concern here is returning more data in the API but I suppose this is optional.\r\n\r\nYea, it's optional, but we can collect all data we need for each stage together instead of to call restful multiple times for different data.  and these data is also very useful for us to analysis spark jobs.\r\n",
        "createdAt" : "2021-03-09T02:27:19Z",
        "updatedAt" : "2021-03-09T02:27:19Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "22f85006551f34272c97c963a2879cfd47c0ad8e",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +482,486 @@        <br><code>?details=true</code> lists all attempts with the task data for the given stage.\n        <br><code>?withSummaries=true</code> lists task metrics distribution and executor metrics distribution of each attempt.\n        <br><code>?quantiles=0.1,0.25,0.5,0.75,1.0</code> summarize the metrics with the given quantiles. Query parameter quantiles takes effect only when <code>withSummaries=true</code>. Default value is <code>0.0,0.25,0.5,0.75,1.0</code>. \n      <br>Example:\n        <br><code>?details=true</code>"
  },
  {
    "id" : "1e9255a5-1e32-46ec-b53a-d2f4ab74b010",
    "prId" : 30713,
    "prUrl" : "https://github.com/apache/spark/pull/30713#pullrequestreview-549977929",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "18d9d424-3190-460a-9f78-83d4ca4e2411",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Why we need this example? You think it is difficult for users to know how to pass these parameter into Spark?",
        "createdAt" : "2020-12-11T04:19:22Z",
        "updatedAt" : "2020-12-11T04:19:23Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "6ba5bb11-484b-4d49-b603-fe2fdddac2fc",
        "parentId" : "18d9d424-3190-460a-9f78-83d4ca4e2411",
        "authorId" : "48cfdd1d-e6ce-4ee2-8cd7-283e4555a88c",
        "body" : "Yes, I loss two weeks and more than 80 hours of my life looking for a way to monitoring my scala-spark application, because there is not an easy and official example to do that. ",
        "createdAt" : "2020-12-11T10:30:05Z",
        "updatedAt" : "2020-12-11T10:34:06Z",
        "lastEditedBy" : "48cfdd1d-e6ce-4ee2-8cd7-283e4555a88c",
        "tags" : [
        ]
      }
    ],
    "commit" : "b723e4799af350edd59fcbc22a2e340125cf780a",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +1041,1045 @@Also it could be done under spark session like:\n```\nimport org.apache.spark.sql.SparkSession\n\nval spark = SparkSession.builder"
  },
  {
    "id" : "45d5acdb-82c6-4d1b-a0fd-c22038df5de7",
    "prId" : 30713,
    "prUrl" : "https://github.com/apache/spark/pull/30713#pullrequestreview-552289905",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8823390d-45a6-4da0-bc5a-559be4fbf339",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "isn't this a copy paste of the configurations above? I don't think this is very useful.",
        "createdAt" : "2020-12-15T10:05:49Z",
        "updatedAt" : "2020-12-15T10:05:49Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "b723e4799af350edd59fcbc22a2e340125cf780a",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +1051,1055 @@            .config(\"spark.metrics.conf.*.sink.graphite.unit\",\"seconds\")\n            .config(\"spark.metrics.conf.*.sink.graphite.prefix\",\"optional_prefix\")\n            .config(\"spark.metrics.conf.*.sink.graphite.regex\",\"optional_regex_to_send_matching_metrics\")\t\n            .appName(\"YOUR_APP_NAME\")\n            .getOrCreate()"
  },
  {
    "id" : "53781b85-632a-4dcc-96da-9ef510b8c3ab",
    "prId" : 29906,
    "prUrl" : "https://github.com/apache/spark/pull/29906#pullrequestreview-503924845",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5e873836-bebe-4b8b-94ed-8063782167ee",
        "parentId" : null,
        "authorId" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "body" : "Is it even worth mentioning in the documentation?",
        "createdAt" : "2020-10-02T23:07:00Z",
        "updatedAt" : "2020-10-28T13:50:33Z",
        "lastEditedBy" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "tags" : [
        ]
      },
      {
        "id" : "b6414f35-ced7-4ba7-b012-af27247f83c6",
        "parentId" : "5e873836-bebe-4b8b-94ed-8063782167ee",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "I think it is as until we come up with different way of deprecating only way to tell users",
        "createdAt" : "2020-10-07T14:13:00Z",
        "updatedAt" : "2020-10-28T13:50:33Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "b38dd66500cdd4a12f78cca1eeacf116f0ea5b4e",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1126,1130 @@  - stages.skippedStages.count\n  - stages.completedStages.count\n  - tasks.blackListedExecutors.count // deprecated use excludedExecutors instead\n  - tasks.excludedExecutors.count\n  - tasks.completedTasks.count"
  },
  {
    "id" : "96f2c29a-482d-4f6e-9c45-f4441197e979",
    "prId" : 28528,
    "prUrl" : "https://github.com/apache/spark/pull/28528#pullrequestreview-490339111",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "22ca5c2b-c8e9-461e-8006-6e61a7690976",
        "parentId" : null,
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "this looks the same as the ExecutorMetrics but I think they are actually different in that this doesn't give you the JVM metrics - correct?  Perhaps we need to update the one below.",
        "createdAt" : "2020-09-14T13:53:55Z",
        "updatedAt" : "2020-09-17T09:36:17Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "61be400e-2b43-4433-a1dd-5717ce097d69",
        "parentId" : "22ca5c2b-c8e9-461e-8006-6e61a7690976",
        "authorId" : "acf5aefc-4c46-451e-a28d-492ceaffd160",
        "body" : "I agree that the naming could be improved, in particular metrics under namespace=executor and namespace=ExecutorMetrics are similar in scope, however the implementation goes via quite different paths [[ExecutorSource]] vs. [[ExecutoMetricsSource]]. Merging the two could be the subject for future refactoring.\r\n\r\nJust to clarify: metrics in the namespace=ExecutorMetrics are already available in local mode. The goal of this PR is to make matrics in the namespace=\"executor\" also available in local mode.",
        "createdAt" : "2020-09-17T08:01:44Z",
        "updatedAt" : "2020-09-17T09:36:17Z",
        "lastEditedBy" : "acf5aefc-4c46-451e-a28d-492ceaffd160",
        "tags" : [
        ]
      }
    ],
    "commit" : "c8ce5a31cd64e23496984c9f06f41675384aeca1",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +1154,1158 @@  - jvmCpuTime\n\n- namespace=executor\n  - **note:** These metrics are available in the driver in local mode only.\n  - A full list of available metrics in this "
  },
  {
    "id" : "0ff578dc-46a8-48b7-b3d8-3aab18758680",
    "prId" : 27398,
    "prUrl" : "https://github.com/apache/spark/pull/27398#pullrequestreview-359379845",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5dd75dd6-e275-40f9-b25c-8ac503e8604d",
        "parentId" : null,
        "authorId" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "body" : "This paragraph not really answers to me why not always compact.",
        "createdAt" : "2020-02-03T09:04:07Z",
        "updatedAt" : "2020-02-17T22:39:47Z",
        "lastEditedBy" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "tags" : [
        ]
      },
      {
        "id" : "fde1900d-a59b-4718-95ca-ac09f571f40a",
        "parentId" : "5dd75dd6-e275-40f9-b25c-8ac503e8604d",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "We already described which events are the candidates in above, so it's saying \"Please note that Spark History Server may not compact the old event log files if figures out not a lot of space would be reduced during compaction because these event log files majorly fill with running jobs or SQL executions.\"\r\n\r\nDoes it answer your question?",
        "createdAt" : "2020-02-16T04:36:38Z",
        "updatedAt" : "2020-02-17T22:39:47Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "a95a4a4c10f3cd1a51cff93d9f19a2a5bbd0a2df",
    "line" : 37,
    "diffHunk" : "@@ -1,1 +129,133 @@the original log files, but it will not affect the operation of the History Server.\n\nPlease note that Spark History Server may not compact the old event log files if figures out not a lot of space\nwould be reduced during compaction. For streaming query we normally expect compaction\nwill run as each micro-batch will trigger one or more jobs which will be finished shortly, but compaction won't run"
  },
  {
    "id" : "f21bcf3d-f3aa-48e9-860b-6be397c4e404",
    "prId" : 25349,
    "prUrl" : "https://github.com/apache/spark/pull/25349#pullrequestreview-272937293",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "15350cb4-6c3d-443e-8537-ee665965de0e",
        "parentId" : null,
        "authorId" : "685f805b-e4fa-4f21-b066-58afcecf9ce6",
        "body" : "I want to add a `Web UI` link behind `Monitoring`, however it never work. \r\n\r\n<img width=\"245\" alt=\"图片\" src=\"https://user-images.githubusercontent.com/7322292/62681577-971a1480-b9ec-11e9-9e1c-608f487ec3e8.png\">\r\n\r\nDou you know how to add it? @srowen @gatorsmile ",
        "createdAt" : "2019-08-08T06:55:57Z",
        "updatedAt" : "2019-08-10T02:58:47Z",
        "lastEditedBy" : "685f805b-e4fa-4f21-b066-58afcecf9ce6",
        "tags" : [
        ]
      },
      {
        "id" : "bf56e38b-9391-4695-a3ad-dcb306ea7265",
        "parentId" : "15350cb4-6c3d-443e-8537-ee665965de0e",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "I think it's in `_layouts/global.html`, but I am not sure this needs a top level menu item.",
        "createdAt" : "2019-08-08T14:09:59Z",
        "updatedAt" : "2019-08-10T02:58:47Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "adbe66f3-2e28-49cf-9c33-8a8759a25c8f",
        "parentId" : "15350cb4-6c3d-443e-8537-ee665965de0e",
        "authorId" : "685f805b-e4fa-4f21-b066-58afcecf9ce6",
        "body" : "@gatorsmile What's your thought? Do we need a top level menu item?",
        "createdAt" : "2019-08-09T03:22:48Z",
        "updatedAt" : "2019-08-10T02:58:47Z",
        "lastEditedBy" : "685f805b-e4fa-4f21-b066-58afcecf9ce6",
        "tags" : [
        ]
      },
      {
        "id" : "9267506b-82c5-4ff7-8613-002bde7dd1ef",
        "parentId" : "15350cb4-6c3d-443e-8537-ee665965de0e",
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "Either is fine to me. I majorly care the contents after we have the basic UI descriptions. :-) ",
        "createdAt" : "2019-08-09T04:14:25Z",
        "updatedAt" : "2019-08-10T02:58:47Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      }
    ],
    "commit" : "6c17d742946be9f148af244542cc53bb39eeea60",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +24,28 @@# Web Interfaces\n\nEvery SparkContext launches a [Web UI](web-ui.html), by default on port 4040, that\ndisplays useful information about the application. This includes:\n"
  }
]