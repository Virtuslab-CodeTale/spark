[
  {
    "id" : "9c851137-56a5-41e5-8586-232ce2889b04",
    "prId" : 32730,
    "prUrl" : "https://github.com/apache/spark/pull/32730#pullrequestreview-675429736",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "69a331c5-e4d4-440c-907e-1585cc5a4b6d",
        "parentId" : null,
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "This has potential for interacting badly with correctness changes, right ?\r\nSee `DAGScheduler.submitMissingTasks` when stage is Indeterminate",
        "createdAt" : "2021-06-02T04:09:28Z",
        "updatedAt" : "2021-06-02T04:09:28Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "ca8eb4e2-d9fe-4ec5-a4e4-7a7bfa3baad4",
        "parentId" : "69a331c5-e4d4-440c-907e-1585cc5a4b6d",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Thank you for review. In that case, `mapId` is not the same, isn't it? We are reusing with `mapId` at [line 170](https://github.com/apache/spark/pull/32730/files#diff-a3b15298f97577c1fadcc2d76d015eebd6343e246c6717417d33f3c458847f46R170), @mridulm .\r\n```\r\nval index = mapStatusesDeleted.indexWhere(x => x != null && x.mapId == mapId)\r\n```",
        "createdAt" : "2021-06-02T05:48:40Z",
        "updatedAt" : "2021-06-02T05:51:10Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "46bd8d36-2a0b-4421-b7f0-77d9dc7cf739",
        "parentId" : "69a331c5-e4d4-440c-907e-1585cc5a4b6d",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "BTW, I agree with you that we don't have a test coverage for the indeterministic stage case. Let me try to add some.",
        "createdAt" : "2021-06-02T05:50:37Z",
        "updatedAt" : "2021-06-02T05:51:40Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "7ef50fd7-cc24-4c22-b63e-f64fbbc2cd67",
        "parentId" : "69a331c5-e4d4-440c-907e-1585cc5a4b6d",
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "You are right, the check on mapId and mapIndex ensure correctness. Thanks for clarifying !",
        "createdAt" : "2021-06-03T15:39:10Z",
        "updatedAt" : "2021-06-03T15:39:10Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      }
    ],
    "commit" : "a9bf099c653ccd485e3409dc20f9d067974deb53",
    "line" : 52,
    "diffHunk" : "@@ -1,1 +252,256 @@      if (mapStatuses(mapIndex) != null && f(mapStatuses(mapIndex).location)) {\n        _numAvailableMapOutputs -= 1\n        mapStatusesDeleted(mapIndex) = mapStatuses(mapIndex)\n        mapStatuses(mapIndex) = null\n        invalidateSerializedMapOutputStatusCache()"
  },
  {
    "id" : "91f5234f-a934-4c50-b38b-dc1a1aaed9b7",
    "prId" : 32033,
    "prUrl" : "https://github.com/apache/spark/pull/32033#pullrequestreview-626783590",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "94e3c9b4-8e2e-49a8-bb0e-3647a41c1b7f",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Expose this for test.",
        "createdAt" : "2021-04-02T04:39:36Z",
        "updatedAt" : "2021-04-03T07:53:33Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "7719c3afa3fcb7c205048b785f23947e1844d804",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +101,105 @@   * explicitly destroyed later on when the ShuffleMapStage is garbage-collected.\n   */\n  private[spark] var cachedSerializedBroadcast: Broadcast[Array[Byte]] = _\n\n  /**"
  },
  {
    "id" : "67e2b157-3c4a-404f-80fb-43fbb256538d",
    "prId" : 32033,
    "prUrl" : "https://github.com/apache/spark/pull/32033#pullrequestreview-626845118",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "49405ab4-a00d-4f58-b1d0-2ee3812e1824",
        "parentId" : null,
        "authorId" : "677aa336-324b-4b93-8300-21f8bd378f26",
        "body" : "The failure could be `DIRECT`, how can you ensure it's only catching exception from broadcast?",
        "createdAt" : "2021-04-02T07:55:41Z",
        "updatedAt" : "2021-04-03T07:53:33Z",
        "lastEditedBy" : "677aa336-324b-4b93-8300-21f8bd378f26",
        "tags" : [
        ]
      },
      {
        "id" : "10e10fa6-becd-4394-960a-443f118332ff",
        "parentId" : "49405ab4-a00d-4f58-b1d0-2ee3812e1824",
        "authorId" : "677aa336-324b-4b93-8300-21f8bd378f26",
        "body" : "Oh, never mind. I saw the code in the following.",
        "createdAt" : "2021-04-02T07:56:31Z",
        "updatedAt" : "2021-04-03T07:53:33Z",
        "lastEditedBy" : "677aa336-324b-4b93-8300-21f8bd378f26",
        "tags" : [
        ]
      }
    ],
    "commit" : "7719c3afa3fcb7c205048b785f23947e1844d804",
    "line" : 26,
    "diffHunk" : "@@ -1,1 +847,851 @@            fetchedStatuses = MapOutputTracker.deserializeMapStatuses(fetchedBytes, conf)\n          } catch {\n            case e: SparkException =>\n              throw new MetadataFetchFailedException(shuffleId, -1,\n                s\"Unable to deserialize broadcasted map statuses for shuffle $shuffleId: \" +"
  },
  {
    "id" : "31e3766f-a390-4995-8e33-e8f6cd2123ce",
    "prId" : 32033,
    "prUrl" : "https://github.com/apache/spark/pull/32033#pullrequestreview-627082927",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dfcb82d5-9a80-46e5-9af3-2debca167ee7",
        "parentId" : null,
        "authorId" : "677aa336-324b-4b93-8300-21f8bd378f26",
        "body" : "Maybe we should move line 964 to 967 out of the try block like in `DIRECT` case.",
        "createdAt" : "2021-04-02T07:58:43Z",
        "updatedAt" : "2021-04-03T07:53:33Z",
        "lastEditedBy" : "677aa336-324b-4b93-8300-21f8bd378f26",
        "tags" : [
        ]
      },
      {
        "id" : "89e925e2-59c0-4d8f-8449-81a6f764e9c5",
        "parentId" : "dfcb82d5-9a80-46e5-9af3-2debca167ee7",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "This is for the need of writing the test case. In the test case, if we call `getStatuses`, the mapoutput tracker worker will ask tracker master for new broadcasted value. So we cannot test the situation we need.",
        "createdAt" : "2021-04-02T16:28:37Z",
        "updatedAt" : "2021-04-03T07:53:33Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "7719c3afa3fcb7c205048b785f23947e1844d804",
    "line" : 50,
    "diffHunk" : "@@ -1,1 +966,970 @@            asInstanceOf[Broadcast[Array[Byte]]]\n          logInfo(\"Broadcast mapstatuses size = \" + bytes.length +\n            \", actual size = \" + bcast.value.length)\n          // Important - ignore the DIRECT tag ! Start from offset 1\n          deserializeObject(bcast.value, 1, bcast.value.length - 1).asInstanceOf[Array[MapStatus]]"
  },
  {
    "id" : "df8b90a5-6698-4c97-bedb-7a5797820051",
    "prId" : 32033,
    "prUrl" : "https://github.com/apache/spark/pull/32033#pullrequestreview-627732980",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a02f64a4-6ff5-45d5-8eac-b6801ced85d8",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "shall we throw MetadataFetchFailedException directly here?",
        "createdAt" : "2021-04-05T07:21:32Z",
        "updatedAt" : "2021-04-05T07:21:32Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "e5b920b4-2eaf-41cf-adc5-b91067cb907b",
        "parentId" : "a02f64a4-6ff5-45d5-8eac-b6801ced85d8",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "<del>Throw `MetadataFetchFailedException` here and catch it and rethrow?</del>",
        "createdAt" : "2021-04-05T07:46:25Z",
        "updatedAt" : "2021-04-05T07:53:06Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "4ca4c43b-788f-4697-af9d-44b071a6531f",
        "parentId" : "a02f64a4-6ff5-45d5-8eac-b6801ced85d8",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Oh, recall why I did this. To construct `MetadataFetchFailedException` needs `shuffleId`.\r\n\r\nI choose to not throw `MetadataFetchFailedException` as `deserializeMapStatuses ` doesn't have `shuffleId` and doesn't need it at all.",
        "createdAt" : "2021-04-05T07:50:20Z",
        "updatedAt" : "2021-04-05T07:50:20Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "41ce4a40-efc4-4e45-ac89-03370d3d479f",
        "parentId" : "a02f64a4-6ff5-45d5-8eac-b6801ced85d8",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "ah I see, thanks for the explanation!",
        "createdAt" : "2021-04-05T09:24:36Z",
        "updatedAt" : "2021-04-05T09:24:36Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "7719c3afa3fcb7c205048b785f23947e1844d804",
    "line" : 56,
    "diffHunk" : "@@ -1,1 +972,976 @@          case e: IOException =>\n            logWarning(\"Exception encountered during deserializing broadcasted map statuses: \", e)\n            throw new SparkException(\"Unable to deserialize broadcasted map statuses\", e)\n        }\n      case _ => throw new IllegalArgumentException(\"Unexpected byte tag = \" + bytes(0))"
  },
  {
    "id" : "626fb1b5-fb32-46e6-852d-c230dcd03aac",
    "prId" : 30763,
    "prUrl" : "https://github.com/apache/spark/pull/30763#pullrequestreview-576508076",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5b19a9d7-1f77-4877-933b-696ebfb89985",
        "parentId" : null,
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "The blocking param does not seem to be used here?",
        "createdAt" : "2020-12-18T19:03:34Z",
        "updatedAt" : "2021-03-08T08:42:24Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "a7737170-e274-4ffc-90e4-b75c41bfe48e",
        "parentId" : "5b19a9d7-1f77-4877-933b-696ebfb89985",
        "authorId" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "body" : "\r\nNo, it isn't used in this implementation. \r\n\r\nBut this  param is still needed in the interface  to override the abstract method coming from the `MapOutputTracker` class:\r\nhttps://github.com/apache/spark/blob/a09ced3589d90c533744a04f0c3b976d4bb42352/core/src/main/scala/org/apache/spark/MapOutputTracker.scala#L391 \r\n\r\nAnd in the abstract `MapOutputTracker` method is introduced to reuse this method in the context cleaning.\r\nhttps://github.com/attilapiros/spark/blob/a09ced3589d90c533744a04f0c3b976d4bb42352/core/src/main/scala/org/apache/spark/ContextCleaner.scala#L223\r\n\r\nWhere we could trigger a blocking or a non-blocking cleanup depending on the config: `spark.cleaner.referenceTracking.blocking.shuffle`.\r\n\r\nSo I think this is fine here.\r\n ",
        "createdAt" : "2021-01-26T16:07:52Z",
        "updatedAt" : "2021-03-08T08:42:24Z",
        "lastEditedBy" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "tags" : [
        ]
      }
    ],
    "commit" : "8e54b41702fd3f8f78d14a57eeb3cf9277b666cc",
    "line" : 142,
    "diffHunk" : "@@ -1,1 +884,888 @@\n  /** Unregister shuffle data. */\n  def unregisterShuffle(shuffleId: Int, blocking: Boolean): Boolean = {\n    mapStatuses.remove(shuffleId).isDefined\n  }"
  },
  {
    "id" : "65aa4854-7190-4fb0-befa-5b709f026a6d",
    "prId" : 30691,
    "prUrl" : "https://github.com/apache/spark/pull/30691#pullrequestreview-675632947",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "de50587e-dd4d-4b77-af6d-892a2146dc80",
        "parentId" : null,
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "Is this more logically part of SPARK-32923 and move it out of this PR ?",
        "createdAt" : "2021-06-02T03:42:49Z",
        "updatedAt" : "2021-06-02T03:42:50Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "d2294fb9-a391-4d03-b6ba-4388378cf23a",
        "parentId" : "de50587e-dd4d-4b77-af6d-892a2146dc80",
        "authorId" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "body" : "[SPARK-32923](https://issues.apache.org/jira/browse/SPARK-32923) would handle non deterministic stage retries right? Do you mean we should remove the `mapOutputTracker.unregisterMergeResult` call in `DAGScheduler`? This change is already added as part of [SPARK-32921](https://issues.apache.org/jira/browse/SPARK-32921)",
        "createdAt" : "2021-06-02T05:24:35Z",
        "updatedAt" : "2021-06-02T05:24:35Z",
        "lastEditedBy" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "tags" : [
        ]
      },
      {
        "id" : "e7d3a389-9d77-46ed-a4a2-83b3cea2e89a",
        "parentId" : "de50587e-dd4d-4b77-af6d-892a2146dc80",
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "This was actually for `unregisterAllMergeResult` below - ended up commenting at wrong line.\r\nBut this is required, let me resolve this comment thread.",
        "createdAt" : "2021-06-03T19:02:50Z",
        "updatedAt" : "2021-06-03T19:02:50Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      }
    ],
    "commit" : "35d06150f37e01256314af4956c6e8fdcf7244b4",
    "line" : 74,
    "diffHunk" : "@@ -1,1 +760,764 @@        if (mergeStatus != null &&\n          (mapIndex.isEmpty || mergeStatus.tracker.contains(mapIndex.get))) {\n          shuffleStatus.removeMergeResult(reduceId, bmAddress)\n          incrementEpoch()\n        }"
  },
  {
    "id" : "e83e5ee2-a1ad-4a4e-b478-a8e4fe253f11",
    "prId" : 30480,
    "prUrl" : "https://github.com/apache/spark/pull/30480#pullrequestreview-540811155",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4ea1dd31-761c-4f4a-b04f-a364e51a1f09",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Shall we ensure `mergeStatus.totalSize > 0` before updating here?",
        "createdAt" : "2020-11-30T11:38:50Z",
        "updatedAt" : "2021-04-19T23:44:44Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "d1422bdd5a7b17c69a43208c23becf76a4bad16c",
    "line" : 932,
    "diffHunk" : "@@ -1,1 +1415,1419 @@            // ShuffleBlockId with mapId being SHUFFLE_PUSH_MAP_ID to indicate this is\n            // a merged shuffle block.\n            splitsByAddress.getOrElseUpdate(mergeStatus.location, ListBuffer()) +=\n              ((ShuffleBlockId(shuffleId, SHUFFLE_PUSH_MAP_ID, partId), mergeStatus.totalSize, -1))\n            // For the \"holes\" in this pre-merged shuffle partition, i.e., unmerged mapper"
  },
  {
    "id" : "cfb3a0bb-c67a-487a-93c5-9df6435f8d11",
    "prId" : 30480,
    "prUrl" : "https://github.com/apache/spark/pull/30480#pullrequestreview-541380537",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "11890564-9359-4b3a-81bd-c04b8027a72a",
        "parentId" : null,
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "`invalidateSerializedMergeOutputStatusCache` should be done if `mergeStatuses(reduceId) != status`.\r\n",
        "createdAt" : "2020-11-30T23:39:41Z",
        "updatedAt" : "2021-04-19T23:44:44Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      }
    ],
    "commit" : "d1422bdd5a7b17c69a43208c23becf76a4bad16c",
    "line" : 101,
    "diffHunk" : "@@ -1,1 +192,196 @@      invalidateSerializedMergeOutputStatusCache()\n    }\n    mergeStatuses(reduceId) = status\n  }\n"
  },
  {
    "id" : "47d5f7e9-b731-40d9-b20f-ba9556d48b03",
    "prId" : 30480,
    "prUrl" : "https://github.com/apache/spark/pull/30480#pullrequestreview-541380537",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cd0d3f01-886a-4754-a27d-7274de7f9689",
        "parentId" : null,
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "Add `@return` with details for both methods.",
        "createdAt" : "2020-11-30T23:52:05Z",
        "updatedAt" : "2021-04-19T23:44:44Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      }
    ],
    "commit" : "d1422bdd5a7b17c69a43208c23becf76a4bad16c",
    "line" : 326,
    "diffHunk" : "@@ -1,1 +515,519 @@   *         and the second item is a sequence of (shuffle block ID, shuffle block size, map index)\n   *         tuples describing the shuffle blocks that are stored at that block manager.\n   */\n  def getMapSizesForMergeResult(\n      shuffleId: Int,"
  },
  {
    "id" : "3aad2046-657b-4047-b95b-b64d7a7f6491",
    "prId" : 30480,
    "prUrl" : "https://github.com/apache/spark/pull/30480#pullrequestreview-617044087",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "306f1290-8fa5-4231-8620-d14cd6b2b98c",
        "parentId" : null,
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "Some of the missing maps can be colocated on the same node - if blocks were not pushed due to map output for reducer being large. So this is a more conservative estimate for locality preference.",
        "createdAt" : "2021-03-08T16:17:36Z",
        "updatedAt" : "2021-04-19T23:44:44Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "8458db47-0257-4935-bceb-0440659c08ee",
        "parentId" : "306f1290-8fa5-4231-8620-d14cd6b2b98c",
        "authorId" : "15ef9b5b-1583-47d9-b883-14ebfd23a00a",
        "body" : "That is true.\r\nAlthough we can check among MapStatus to figure this part out as well, it does not change the locality preference significantly.\r\n`REDUCER_PREF_LOCS_FRACTION` is hard coded to 0.2, a merged shuffle partition in almost all cases would have more than 20% of the blocks merged.\r\nThe average merge ratio in terms of number of blocks on our cluster after 100% rollout is around 90%.",
        "createdAt" : "2021-03-21T17:36:06Z",
        "updatedAt" : "2021-04-19T23:44:44Z",
        "lastEditedBy" : "15ef9b5b-1583-47d9-b883-14ebfd23a00a",
        "tags" : [
        ]
      }
    ],
    "commit" : "d1422bdd5a7b17c69a43208c23becf76a4bad16c",
    "line" : 551,
    "diffHunk" : "@@ -1,1 +894,898 @@          if (status != null && status.getNumMissingMapOutputs(numMaps).toDouble / numMaps\n            <= (1 - REDUCER_PREF_LOCS_FRACTION)) {\n            Seq(status.location.host)\n          } else {\n            Nil"
  },
  {
    "id" : "bab2aa13-77b0-4f5b-a432-aef4164d91cc",
    "prId" : 30480,
    "prUrl" : "https://github.com/apache/spark/pull/30480#pullrequestreview-617082848",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e3df59f2-e2bb-46d0-a94b-a5ad4d665e3b",
        "parentId" : null,
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "Note: With data skew, it is possible that merged output is smaller in size than what is computed from `shuffleLocalityEnabled` case - particularly given these mappers could be running on the same host.\r\nPractically, this is unlikely.",
        "createdAt" : "2021-03-08T16:19:06Z",
        "updatedAt" : "2021-04-19T23:44:44Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "d9d44a82-52ca-4e8b-b183-a1114e9c18b1",
        "parentId" : "e3df59f2-e2bb-46d0-a94b-a5ad4d665e3b",
        "authorId" : "15ef9b5b-1583-47d9-b883-14ebfd23a00a",
        "body" : "This is true.\r\nThe consideration is to optimize both the local fetched block size and the reduction of shuffle fetch RPCs.\r\nAlthough only calculating based on number of blocks instead of the size of the blocks would lead to slight inaccuracy, it should be ignorable since the case where most of the mappers for the skewed partition run on the same host should very unlikely.\r\nThe current approach would lead to satisfactory enough calculation while preserving the simplicity in the implementation.",
        "createdAt" : "2021-03-22T00:24:39Z",
        "updatedAt" : "2021-04-19T23:44:44Z",
        "lastEditedBy" : "15ef9b5b-1583-47d9-b883-14ebfd23a00a",
        "tags" : [
        ]
      }
    ],
    "commit" : "d1422bdd5a7b17c69a43208c23becf76a4bad16c",
    "line" : 574,
    "diffHunk" : "@@ -1,1 +917,921 @@          Nil\n        }\n      }\n    } else {\n      Nil"
  },
  {
    "id" : "3e80b3d0-9d27-476a-bc79-96355fa16add",
    "prId" : 30480,
    "prUrl" : "https://github.com/apache/spark/pull/30480#pullrequestreview-541380537",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d5fd62be-89d2-432e-963f-c1a478a6a5d5",
        "parentId" : null,
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "Add a note about `chunkBitmap`",
        "createdAt" : "2021-03-08T16:31:10Z",
        "updatedAt" : "2021-04-19T23:44:44Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      }
    ],
    "commit" : "d1422bdd5a7b17c69a43208c23becf76a4bad16c",
    "line" : 344,
    "diffHunk" : "@@ -1,1 +533,537 @@   *         and the second item is a sequence of (shuffle block ID, shuffle block size, map index)\n   *         tuples describing the shuffle blocks that are stored at that block manager.\n   */\n  def getMapSizesForMergeResult(\n      shuffleId: Int,"
  },
  {
    "id" : "670f7501-40e9-4900-bcbb-86cb99aa8ee2",
    "prId" : 30480,
    "prUrl" : "https://github.com/apache/spark/pull/30480#pullrequestreview-634022310",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b1bb4d66-51e3-493f-9a60-25ce22021753",
        "parentId" : null,
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "If `fetchMergeResult == true`, is it right that there is an expectation that `(mapOutputStatuses == null) == (mergeResultStatuses == null)` ?\r\n\r\nIf yes, can we simplify this ?\r\na) Make this method simpler by using that condition.\r\nb) Do we have any usecase for `GetMergeResultStatuses` withough also fetching  `GetMapOutputStatuses` immediately before ? If not, combine both to avoid two rpc's when `fetchMergeResult == true` ?",
        "createdAt" : "2021-03-08T16:48:46Z",
        "updatedAt" : "2021-04-19T23:44:44Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "24d5332e-c63f-4f22-9534-78a498225ae4",
        "parentId" : "b1bb4d66-51e3-493f-9a60-25ce22021753",
        "authorId" : "15ef9b5b-1583-47d9-b883-14ebfd23a00a",
        "body" : "That's not always true.\r\nWe currently fetch map status and merge status using 2 separate RPCs.\r\nAlthough the fetching of these statuses is guarded by the lock, the initial check at line 1113 for these statuses being not null is out of the lock.\r\nSo, it would be possible that a task might see the map status being non-null while merge status being null.\r\n\r\nWe always need to fetch both map status and merge status together, either during the initial fetch or during fallback.\r\nCombine both RPCs into 1 would increase the code complexity.\r\nFor now, the RPC just returns the pre-serialized bytes for either MapStatus array or MergeStatus array.\r\nIf we want to combine both into a single RPC, we would need to define additional RPC messages so that we can encode the 2 byte arrays for serialized MapStatus array and MergeStatus array together.\r\nCombining both together does not seem to bring enough benefits.\r\nWe haven't observed any issue indicating Spark driver performance regression with doubling the number of RPCs for fetching shuffle statuses.\r\nThis would also help to keep code simpler.",
        "createdAt" : "2021-03-21T17:46:45Z",
        "updatedAt" : "2021-04-19T23:44:44Z",
        "lastEditedBy" : "15ef9b5b-1583-47d9-b883-14ebfd23a00a",
        "tags" : [
        ]
      },
      {
        "id" : "229886df-6624-4717-8cb0-06fbbae4030b",
        "parentId" : "b1bb4d66-51e3-493f-9a60-25ce22021753",
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "I would expect multiple rpc's to not be the preferred option given the impact on driver, but code simplicity needs to be balanced against that.\r\n+CC @JoshRosen, @Ngone51 who last made changes here. Any thoughts on this ?",
        "createdAt" : "2021-03-26T06:34:31Z",
        "updatedAt" : "2021-04-19T23:44:44Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "62c128d4-5045-46fb-88fd-3d2fe548c2c1",
        "parentId" : "b1bb4d66-51e3-493f-9a60-25ce22021753",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "I'd prefer to combine them. Actually, the first time when I reviewed this PR, I began to think about a unified way to provide a consistent API for both map status and merged status in `MapOutputTracker` & `ShuffleStatus`. Unfortunately, I didn't get a good idea.\r\n\r\nI think one RPC would ease the error handling for us. Not sure how much complexity you'd expect?\r\n\r\nAnd I'd suggest adding an additional new RPC for the combined case and leave the current one as it is, so that we don't affect the existing code path when push-based shuffle disabled.",
        "createdAt" : "2021-03-26T13:28:54Z",
        "updatedAt" : "2021-04-19T23:44:44Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "64f51bcc-2386-419c-9917-5f7b97e68a14",
        "parentId" : "b1bb4d66-51e3-493f-9a60-25ce22021753",
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "One option could be replace `GetMergeResultStatuses` with `GetMapOutputAndMergeResultStatuses`.\r\nThat keeps non push based shuffle codepaths unchanged, and when push based shuffle is enabled, a single rpc handles the response : the code change would mirror what has been done for `GetMergeResultStatuses` already.\r\n\r\nThoughts @Ngone51, @Victsm, @venkata91 ?",
        "createdAt" : "2021-03-27T09:03:58Z",
        "updatedAt" : "2021-04-19T23:44:44Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "3b67521b-faa6-400c-a117-d36d4d8d3ab7",
        "parentId" : "b1bb4d66-51e3-493f-9a60-25ce22021753",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "+1. This actually what I mean above.",
        "createdAt" : "2021-03-29T01:44:48Z",
        "updatedAt" : "2021-04-19T23:44:44Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "dae5fe61-5b3b-4c2b-99be-fd44c4a2930a",
        "parentId" : "b1bb4d66-51e3-493f-9a60-25ce22021753",
        "authorId" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "body" : "With the current RPC (`RpcCallContext`) mechanism with `MapOutputTracker`, we can only send one response as oppose to other RPC mechanisms with in Spark. If we have to combine getting both `MapStatuses` and `MergeStatuses` when push based shuffle is enabled, then we have couple of options:\r\n\r\n1. Encode both MapStatuses and MergeStatuses in the same `Array[Byte]` getting returned from `serializedOutputStatus` with some encoding scheme like `length` in bytes of mapStatuses as first part and then mapStatuses similarly for mergeStatuses and in the `deserializeOutputStatuses` we have to decode it accordingly for the output of `GetMapOutputAndMergeResultStatuses` RPC call. This is some what not a cleaner approach as the client keeps the semantics of encoding/decoding of the byte array instead of the RPC layer itself. Although this is already being done wrt whether the mapStatuses are a `DIRECT` fetch or `BROADCAST` fetch. \r\n2. If not, we might need to make changes to `RpcCallContext` in order to respond with 2 byte arrays. This seems to be lot of additional overhead just for this purpose.\r\n\r\nAny other suggestions? cc @Victsm @mridulm @Ngone51 ",
        "createdAt" : "2021-03-30T19:43:44Z",
        "updatedAt" : "2021-04-19T23:44:44Z",
        "lastEditedBy" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "tags" : [
        ]
      },
      {
        "id" : "694e7bda-ffdb-4ae7-acba-0e5a77849632",
        "parentId" : "b1bb4d66-51e3-493f-9a60-25ce22021753",
        "authorId" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "body" : "Gentle ping @Victsm @mridulm @Ngone51 \r\n",
        "createdAt" : "2021-04-05T17:48:04Z",
        "updatedAt" : "2021-04-19T23:44:44Z",
        "lastEditedBy" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "tags" : [
        ]
      },
      {
        "id" : "19b38151-62c6-4bc1-9460-c8dd8a2c3cf2",
        "parentId" : "b1bb4d66-51e3-493f-9a60-25ce22021753",
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "That is an implementation detail of what is response of `GetMapOutputAndMergeResultStatuses` right ?\r\nIt can simply be encoding of `Array[Array[Byte]]` (for example) - where result(0) is for `MapStatus` and result(1) is for `MergeStatus` - keeping everything else same ?",
        "createdAt" : "2021-04-05T18:11:40Z",
        "updatedAt" : "2021-04-19T23:44:44Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "721e637d-61d9-4e60-9e0b-7f583f488c65",
        "parentId" : "b1bb4d66-51e3-493f-9a60-25ce22021753",
        "authorId" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "body" : "Makes sense @mridulm Instead of `Array[Array[Byte]]` I used a tuple of `(Array[Byte], Array[Byte])`. cc @Victsm ",
        "createdAt" : "2021-04-12T19:39:33Z",
        "updatedAt" : "2021-04-19T23:44:44Z",
        "lastEditedBy" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "tags" : [
        ]
      },
      {
        "id" : "dc62cbc4-f9ee-43f3-8163-ee6a771024f0",
        "parentId" : "b1bb4d66-51e3-493f-9a60-25ce22021753",
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "I am fine with Tuple as well.\r\n+CC @Ngone51 in case you have any other thoughts.",
        "createdAt" : "2021-04-12T22:41:58Z",
        "updatedAt" : "2021-04-19T23:44:44Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      }
    ],
    "commit" : "d1422bdd5a7b17c69a43208c23becf76a4bad16c",
    "line" : 754,
    "diffHunk" : "@@ -1,1 +1209,1213 @@      } else {\n        (mapOutputStatuses, mergeOutputStatuses)\n      }\n    } else {\n      val statuses = mapStatuses.get(shuffleId).orNull"
  },
  {
    "id" : "29b537e1-e2a9-413e-993e-246ce4b64225",
    "prId" : 30480,
    "prUrl" : "https://github.com/apache/spark/pull/30480#pullrequestreview-633913941",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "088bf802-834d-46bc-9038-bef3b2983dc3",
        "parentId" : null,
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "Can we rephrase this as a `TODO` comment ? This will be something we should support in future.",
        "createdAt" : "2021-03-08T16:55:22Z",
        "updatedAt" : "2021-04-19T23:44:44Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "857fd351-425b-4c21-948b-02a3ec16fc95",
        "parentId" : "088bf802-834d-46bc-9038-bef3b2983dc3",
        "authorId" : "15ef9b5b-1583-47d9-b883-14ebfd23a00a",
        "body" : "Just to further clarify a bit.\r\nPush-based shuffle cannot support the current map ID based subdivision of a shuffle partition into multiple smaller parts, because the blocks from different mappers are merged out of order.\r\nWe could, however, support the chunk ID based subdivision of a merged shuffle partition.\r\nWhen blocks get merged, the current approach already divides the merged shuffle file into multiple fix-sized chunks.\r\nTo leverage this, it would also require merging potentially skewed shuffle partition as well.",
        "createdAt" : "2021-03-21T17:51:05Z",
        "updatedAt" : "2021-04-19T23:44:44Z",
        "lastEditedBy" : "15ef9b5b-1583-47d9-b883-14ebfd23a00a",
        "tags" : [
        ]
      },
      {
        "id" : "b445e268-5eff-4381-9558-2c4702c5fe26",
        "parentId" : "088bf802-834d-46bc-9038-bef3b2983dc3",
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "There are couple of things here:\r\n\r\na) Can we leverage existing skew algo ? My understanding is we can, though it might not be necessarily as optimal as current reads for push based shuffle.\r\nWhat I mean is, if reducer r1.1 is processing m1-m100 and r1.2 is processing m101-m200 for reducer partition 1, we can ensure that m1-m100 can be satisfied with bin packing to get better read than reading from 100 mappers/ESS - right ? It is not as optimal as reading m1-m200, but it will be better than alternative.\r\n\r\nb) Alternative ways to split mapper input for a reducer : which is what you described, and this can be an option as spark evolves.\r\n\r\nGiven both of these, we would want to make the comment a TODO with a jira for it - which can be addressed in some subsequent work.\r\n\r\nOr are there concerns with (a) or (b) that I am missing ?",
        "createdAt" : "2021-03-26T03:40:27Z",
        "updatedAt" : "2021-04-19T23:44:44Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "7617aeee-1dd0-4e1b-b123-2b98e4bce8e0",
        "parentId" : "088bf802-834d-46bc-9038-bef3b2983dc3",
        "authorId" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "body" : "Added a TODO comment along with the [JIRA](https://issues.apache.org/jira/browse/SPARK-35036). Please take a look @mridulm and @Victsm ",
        "createdAt" : "2021-04-12T19:56:34Z",
        "updatedAt" : "2021-04-19T23:44:44Z",
        "lastEditedBy" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "tags" : [
        ]
      }
    ],
    "commit" : "d1422bdd5a7b17c69a43208c23becf76a4bad16c",
    "line" : 917,
    "diffHunk" : "@@ -1,1 +1400,1404 @@    // subrange requests. However, when a reduce task needs to fetch blocks from a subrange of\n    // map outputs, it usually indicates skewed partitions which push-based shuffle delegates\n    // to AQE to handle.\n    // TODO: SPARK-35036: Instead of reading map blocks in case of AQE with Push based shuffle,\n    // TODO: improve push based shuffle to read partial merged blocks satisfying the start/end"
  },
  {
    "id" : "ed46ae42-8f00-4bb1-afb5-9ff9113e27b2",
    "prId" : 30480,
    "prUrl" : "https://github.com/apache/spark/pull/30480#pullrequestreview-635038497",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fbb7b17c-816d-43e1-94bc-76242458b2dc",
        "parentId" : null,
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "Both `GetMapStatusMessage` and `GetMergeStatusMessage` are doing pretty much same thing except for log message difference and `isMapOutput` flag difference.\r\nAdd a local method and delegate to it to minimize duplication.",
        "createdAt" : "2021-03-26T06:19:03Z",
        "updatedAt" : "2021-04-19T23:44:44Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "ada51f75-a97e-4944-87b4-c923687f5eb2",
        "parentId" : "fbb7b17c-816d-43e1-94bc-76242458b2dc",
        "authorId" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "body" : "As part of https://issues.apache.org/jira/browse/SPARK-34826  we would be adding one more message for getting shuffle mergers which is why made this change. ",
        "createdAt" : "2021-03-28T23:31:26Z",
        "updatedAt" : "2021-04-19T23:44:44Z",
        "lastEditedBy" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "tags" : [
        ]
      },
      {
        "id" : "47f30c46-6d89-4c18-bc58-3a79e3e52744",
        "parentId" : "fbb7b17c-816d-43e1-94bc-76242458b2dc",
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "That would be for a new message right ? Will it have changes to these existing messages ? If not, would something like this not work ?\r\n\r\n```\r\n\r\ndef handleStatusMessage(shuffleId: Int, context: RpcCallContext, messageType: String, mapOutput: Boolean): Unit = {\r\n  val hostPort = context.senderAddress.hostPort\r\n  val shuffleStatus = shuffleStatuses.get(shuffleId).head\r\n  logDebug(s\"Handling request to send $messageType output locations for shuffle $shuffleId to $hostPort\")\r\n  context.reply(\r\n    shuffleStatus.serializedOutputStatus(broadcastManager, isLocal, minSizeForBroadcast, conf, isMapOutput = mapOutput))\r\n}\r\n\r\ndata match {\r\n  case GetMapStatusMessage(shuffleId, context) => handleStatusMessage(shuffleId, context, \"map\", true)\r\n  case GetMergeStatusMessage(shuffleId, context) => handleStatusMessage(shuffleId, context, \"map\", false)\r\n}\r\n\r\n```",
        "createdAt" : "2021-03-29T01:54:05Z",
        "updatedAt" : "2021-04-19T23:44:44Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "7382790d-e559-48cc-90a3-d715e4e1efb6",
        "parentId" : "fbb7b17c-816d-43e1-94bc-76242458b2dc",
        "authorId" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "body" : "Addressed this comment.",
        "createdAt" : "2021-03-30T18:44:21Z",
        "updatedAt" : "2021-04-19T23:44:44Z",
        "lastEditedBy" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "tags" : [
        ]
      },
      {
        "id" : "b90286e2-de9d-4837-b353-78830d90b098",
        "parentId" : "fbb7b17c-816d-43e1-94bc-76242458b2dc",
        "authorId" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "body" : "Just realized I totally misunderstood the first comment above @mridulm ",
        "createdAt" : "2021-04-13T21:23:35Z",
        "updatedAt" : "2021-04-19T23:44:44Z",
        "lastEditedBy" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "tags" : [
        ]
      }
    ],
    "commit" : "d1422bdd5a7b17c69a43208c23becf76a4bad16c",
    "line" : 425,
    "diffHunk" : "@@ -1,1 +654,658 @@              case GetMapAndMergeOutputMessage(shuffleId, context) =>\n                handleStatusMessage(shuffleId, context, true)\n            }\n          } catch {\n            case NonFatal(e) => logError(e.getMessage, e)"
  },
  {
    "id" : "59b4217c-4bfc-4718-9b45-118128abe9ee",
    "prId" : 30480,
    "prUrl" : "https://github.com/apache/spark/pull/30480#pullrequestreview-622114344",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "974a3a44-5834-4590-859a-365c3e7d7cc8",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "\"shuffle partition\" is a bit confusing here. The original mapstatus also uses the word `partition`. Maybe, change all \"shuffle partition\" to \"shuffle reduce partition\"?",
        "createdAt" : "2021-03-26T13:40:01Z",
        "updatedAt" : "2021-04-19T23:44:44Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "d1422bdd5a7b17c69a43208c23becf76a4bad16c",
    "line" : 36,
    "diffHunk" : "@@ -1,1 +95,99 @@   * a shuffle partition, or null if not available. When push-based shuffle is enabled, this array\n   * provides a reducer oriented view of the shuffle status specifically for the results of\n   * merging shuffle partition blocks into per-partition merged shuffle files.\n   */\n  val mergeStatuses = if (numReducers > 0) {"
  },
  {
    "id" : "5788e151-35bd-434f-a886-ff3b9c41f105",
    "prId" : 30480,
    "prUrl" : "https://github.com/apache/spark/pull/30480#pullrequestreview-638415039",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f33ee4dc-dc29-41a1-9455-6a1c5561dbe2",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "I may miss some discussion after my last discussion, I think this breaches our decision made before:\r\n\r\nwe won't affect the existing code path in the case of map status only.\r\n\r\n\r\nI think you can return the mapstatus only at the sender side to keep the same behavior?",
        "createdAt" : "2021-04-16T06:14:52Z",
        "updatedAt" : "2021-04-19T23:44:44Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "75704b1f-228d-4209-86f3-9e3a83b6252b",
        "parentId" : "f33ee4dc-dc29-41a1-9455-6a1c5561dbe2",
        "authorId" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "body" : "do you mean separate out the handling of both `GetMapStatusMessage` and `GetMapAndMergeStatusMessage` to avoid returning `(mapStatuses, null)` in the case of `GetMapStatusMessage` and keep it the same way as it is before, just returning `mapStatuses`?",
        "createdAt" : "2021-04-19T01:42:14Z",
        "updatedAt" : "2021-04-19T23:44:44Z",
        "lastEditedBy" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "tags" : [
        ]
      },
      {
        "id" : "d56d941a-927e-4395-90f5-55f4a6788b61",
        "parentId" : "f33ee4dc-dc29-41a1-9455-6a1c5561dbe2",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Yes. (cc @mridulm) ",
        "createdAt" : "2021-04-19T02:02:45Z",
        "updatedAt" : "2021-04-19T23:44:44Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "402c478c-7f70-4773-b0db-3e5b0baf332c",
        "parentId" : "f33ee4dc-dc29-41a1-9455-6a1c5561dbe2",
        "authorId" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "body" : "@Ngone51 I updated the PR assuming that is what you meant with your above comment. Let me know if thats not the case.",
        "createdAt" : "2021-04-19T02:10:31Z",
        "updatedAt" : "2021-04-19T23:44:44Z",
        "lastEditedBy" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "tags" : [
        ]
      }
    ],
    "commit" : "d1422bdd5a7b17c69a43208c23becf76a4bad16c",
    "line" : 727,
    "diffHunk" : "@@ -1,1 +1187,1191 @@            logInfo(\"Doing the fetch; tracker endpoint = \" + trackerEndpoint)\n            val fetchedBytes =\n              askTracker[(Array[Byte], Array[Byte])](GetMapAndMergeResultStatuses(shuffleId))\n            try {\n              fetchedMapStatuses ="
  },
  {
    "id" : "411398ec-ff51-4302-bc2c-e067558c1751",
    "prId" : 30480,
    "prUrl" : "https://github.com/apache/spark/pull/30480#pullrequestreview-638415122",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "952ddce1-76ff-476c-bc3e-dd7a843680be",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "I remember Magnet declares that it's able to fall back to the original fetch (using mapstatus) when fetch failure happens. But, here, it looks like we only collect the merged status for those maps only without backup mapstatuses. (Because in my mind, I think we can collect both merged statues and original mapstatus together so that we can fall back if need). How do we plan to support the fallback?",
        "createdAt" : "2021-04-16T07:11:35Z",
        "updatedAt" : "2021-04-19T23:44:44Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "8ea88521-036c-40dd-a824-7e7b00e7a682",
        "parentId" : "952ddce1-76ff-476c-bc3e-dd7a843680be",
        "authorId" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "body" : "For fallback support we have added the `getMapSizesForMergeResult` methods. Once there is a fetch failure for a merged shuffle block/chunk then the iterator queries the MOT to get all the map statuses for the blocks that are part of that merged shuffle block/chunk using these new methods. For reference this is the PR:\r\nhttps://github.com/apache/spark/pull/32140/",
        "createdAt" : "2021-04-16T16:28:06Z",
        "updatedAt" : "2021-04-19T23:44:44Z",
        "lastEditedBy" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "tags" : [
        ]
      },
      {
        "id" : "2d37e5c7-0ca1-4b76-8beb-51176b8a98ee",
        "parentId" : "952ddce1-76ff-476c-bc3e-dd7a843680be",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "OK, I'll review it in detail when it gets in.",
        "createdAt" : "2021-04-19T02:10:51Z",
        "updatedAt" : "2021-04-19T23:44:44Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "d1422bdd5a7b17c69a43208c23becf76a4bad16c",
    "line" : 927,
    "diffHunk" : "@@ -1,1 +1410,1414 @@      mergeStatuses.get.zipWithIndex.slice(startPartition, endPartition).foreach {\n        case (mergeStatus, partId) =>\n          val remainingMapStatuses = if (mergeStatus != null && mergeStatus.totalSize > 0) {\n            // If MergeStatus is available for the given partition, add location of the\n            // pre-merged shuffle partition for this partition ID. Here we create a"
  },
  {
    "id" : "5dc39c7a-be93-4154-9090-10d8154615c0",
    "prId" : 30480,
    "prUrl" : "https://github.com/apache/spark/pull/30480#pullrequestreview-637879033",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e66ec236-5cd1-4ea4-b36f-0ea38d3761a7",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "comment \"test only\"?",
        "createdAt" : "2021-04-16T07:13:23Z",
        "updatedAt" : "2021-04-19T23:44:44Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "84f3e786-f6eb-4f29-9b9a-3af82314472b",
        "parentId" : "e66ec236-5cd1-4ea4-b36f-0ea38d3761a7",
        "authorId" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "body" : "This is not just for test. It is used by the iterator when fetchfailure happens for a merged block. This is the wip PR that depends on this change https://github.com/apache/spark/pull/32140/",
        "createdAt" : "2021-04-16T16:22:41Z",
        "updatedAt" : "2021-04-19T23:44:44Z",
        "lastEditedBy" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "tags" : [
        ]
      }
    ],
    "commit" : "d1422bdd5a7b17c69a43208c23becf76a4bad16c",
    "line" : 642,
    "diffHunk" : "@@ -1,1 +1120,1124 @@  }\n\n  override def getMapSizesForMergeResult(\n      shuffleId: Int,\n      partitionId: Int): Iterator[(BlockManagerId, Seq[(BlockId, Long, Int)])] = {"
  },
  {
    "id" : "cd6e96e8-c0f8-4050-bd72-8f518ed6ace0",
    "prId" : 30480,
    "prUrl" : "https://github.com/apache/spark/pull/30480#pullrequestreview-637880001",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "097cc1b6-6f96-42ef-bf53-d42a14220e1e",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "comment \"test only\"?\r\n\r\n",
        "createdAt" : "2021-04-16T07:13:31Z",
        "updatedAt" : "2021-04-19T23:44:44Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "8c5b43e5-7b3c-4423-9f7c-109a1e544916",
        "parentId" : "097cc1b6-6f96-42ef-bf53-d42a14220e1e",
        "authorId" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "body" : "Same as above. Used by the iterator during fallback",
        "createdAt" : "2021-04-16T16:23:17Z",
        "updatedAt" : "2021-04-19T23:44:44Z",
        "lastEditedBy" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "tags" : [
        ]
      }
    ],
    "commit" : "d1422bdd5a7b17c69a43208c23becf76a4bad16c",
    "line" : 666,
    "diffHunk" : "@@ -1,1 +1144,1148 @@  }\n\n  override def getMapSizesForMergeResult(\n      shuffleId: Int,\n      partitionId: Int,"
  },
  {
    "id" : "8e60a08e-7937-461f-9603-af28261eb9c7",
    "prId" : 30480,
    "prUrl" : "https://github.com/apache/spark/pull/30480#pullrequestreview-639381121",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3ad39beb-dbda-497d-b98c-8d786eb748dd",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Doesn't this path need to respect `shuffleLocalityEnabled` too?",
        "createdAt" : "2021-04-16T07:30:30Z",
        "updatedAt" : "2021-04-19T23:44:44Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "943f6455-9e4e-45c5-980e-75019f29a212",
        "parentId" : "3ad39beb-dbda-497d-b98c-8d786eb748dd",
        "authorId" : "15ef9b5b-1583-47d9-b883-14ebfd23a00a",
        "body" : "I agree that we should make it consistent, but there's also clear difference between locality calculation for push-based shuffle and the original shuffle.\r\nMy understanding of the reason for adding this flag is due to the potentially costly computation for shuffle locality in the original shuffle.\r\nFor push based shuffle, that cost is no longer a concern, and the reducer task can achieve much much better locality.\r\nAlways calculating shuffle locality is preferred.\r\n",
        "createdAt" : "2021-04-19T23:15:07Z",
        "updatedAt" : "2021-04-19T23:44:44Z",
        "lastEditedBy" : "15ef9b5b-1583-47d9-b883-14ebfd23a00a",
        "tags" : [
        ]
      }
    ],
    "commit" : "d1422bdd5a7b17c69a43208c23becf76a4bad16c",
    "line" : 545,
    "diffHunk" : "@@ -1,1 +888,892 @@      // Check if the map output is pre-merged and if the merge ratio is above the threshold.\n      // If so, the location of the merged block is the preferred location.\n      val preferredLoc = if (pushBasedShuffleEnabled) {\n        shuffleStatus.withMergeStatuses { statuses =>\n          val status = statuses(partitionId)"
  },
  {
    "id" : "5a62a47a-28bc-43da-abca-9905eb6ab07b",
    "prId" : 30004,
    "prUrl" : "https://github.com/apache/spark/pull/30004#pullrequestreview-605734036",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ce0b45ac-a33c-4b68-b2e0-e63096ae4ce0",
        "parentId" : null,
        "authorId" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "body" : "Please clear the `mapStatuses` in case of `MetadataFetchFailedException`!\r\n\r\nReasoning:\r\nThe `getStatuses` method before this PR was only used in `getMapSizesByExecutorId ` where the `MetadataFetchFailedException` (the case when missing output location was detected) handled by clearing of the `mapStatuses` cache as it is probably outdated.\r\n\r\n\r\n~~I am sure that clearing would not be missed if this cleaning would be done at the throwing of that exception.~~\r\n~~Could you please check whether it can be moved there?~~\r\n\r\n",
        "createdAt" : "2021-03-06T03:48:54Z",
        "updatedAt" : "2021-03-06T04:30:20Z",
        "lastEditedBy" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "tags" : [
        ]
      },
      {
        "id" : "ec90cc18-27c8-4cb1-aca0-2667ad8f1710",
        "parentId" : "ce0b45ac-a33c-4b68-b2e0-e63096ae4ce0",
        "authorId" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "body" : "Now I see why you cannot move the clearing there! \r\nStill the clearing itself is needed to be done.",
        "createdAt" : "2021-03-06T04:28:52Z",
        "updatedAt" : "2021-03-06T04:28:52Z",
        "lastEditedBy" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "tags" : [
        ]
      },
      {
        "id" : "5c20f53e-6f08-4301-b245-929de8fb4045",
        "parentId" : "ce0b45ac-a33c-4b68-b2e0-e63096ae4ce0",
        "authorId" : "c52c39ef-2b7f-4855-a2f1-2a2e50a719ae",
        "body" : "yes, we only need to clear `mapStatuses` in `MapOutputTrackerWorker` , will add that",
        "createdAt" : "2021-03-06T06:46:27Z",
        "updatedAt" : "2021-03-06T07:07:28Z",
        "lastEditedBy" : "c52c39ef-2b7f-4855-a2f1-2a2e50a719ae",
        "tags" : [
        ]
      }
    ],
    "commit" : "fe693eb2d7c22d2be1344214e3d3f073be643eb4",
    "line" : 41,
    "diffHunk" : "@@ -1,1 +851,855 @@  override def getAllMapOutputStatuses(shuffleId: Int): Array[MapStatus] = {\n    logDebug(s\"Fetching all output statuses for shuffle $shuffleId\")\n    val statuses = getStatuses(shuffleId, conf)\n    MapOutputTracker.checkMapStatuses(statuses, shuffleId)\n    statuses"
  },
  {
    "id" : "5b054599-caf5-42ce-8b41-e3efffd79851",
    "prId" : 30004,
    "prUrl" : "https://github.com/apache/spark/pull/30004#pullrequestreview-605748307",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "81f9a57f-3ba5-49ad-b95d-bf9a14dbd3a7",
        "parentId" : null,
        "authorId" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "body" : "I see your intention by calling this `clone` here but I do not think this is enough.\r\nAs the `MapStatus` is trait and not a case class in addition its implementations are mutable with a lot of `var` fields.\r\n\r\nThe `clone` on the `Array` is not a deep copy.",
        "createdAt" : "2021-03-06T04:11:31Z",
        "updatedAt" : "2021-03-06T04:11:31Z",
        "lastEditedBy" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "tags" : [
        ]
      },
      {
        "id" : "a6c70705-e20d-42cd-99c7-28daf408ae9b",
        "parentId" : "81f9a57f-3ba5-49ad-b95d-bf9a14dbd3a7",
        "authorId" : "c52c39ef-2b7f-4855-a2f1-2a2e50a719ae",
        "body" : "Yes, got your point. How about change this method to `getAllMapOutputStatusMetadata` to only return the metadada?\r\n\r\n\r\n",
        "createdAt" : "2021-03-06T06:35:37Z",
        "updatedAt" : "2021-03-06T07:07:28Z",
        "lastEditedBy" : "c52c39ef-2b7f-4855-a2f1-2a2e50a719ae",
        "tags" : [
        ]
      },
      {
        "id" : "78c72ac1-d996-4836-be73-3cbb9516006b",
        "parentId" : "81f9a57f-3ba5-49ad-b95d-bf9a14dbd3a7",
        "authorId" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "body" : "It could not be enough if the metadata can mutate. But as I see we could solve all the problems with immutable metadata easily. So to be on the safe side please document we require the metadata to be immutable and introduce an `updateMetadata(meta: Option[Serializable])` method in `MapStatus`. Then we will be safe and all the use cases are covered.\r\n\r\n(And you can use a case class for the Uber RSS's `MapTaskRssInfo`)",
        "createdAt" : "2021-03-06T10:55:52Z",
        "updatedAt" : "2021-03-06T10:55:52Z",
        "lastEditedBy" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "tags" : [
        ]
      }
    ],
    "commit" : "fe693eb2d7c22d2be1344214e3d3f073be643eb4",
    "line" : 26,
    "diffHunk" : "@@ -1,1 +790,794 @@        shuffleStatus.withMapStatuses { statuses =>\n          MapOutputTracker.checkMapStatuses(statuses, shuffleId)\n          statuses.clone\n        }\n      case None => Array.empty"
  },
  {
    "id" : "a24cf7b5-db44-4db0-839c-8897abf58fa0",
    "prId" : 30004,
    "prUrl" : "https://github.com/apache/spark/pull/30004#pullrequestreview-605734036",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e744fc00-24cc-458d-9016-2846d3e17976",
        "parentId" : null,
        "authorId" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "body" : "I think you can extract this `if` into a new method and reuse the method in `convertMapStatuses`.\r\n",
        "createdAt" : "2021-03-06T04:27:42Z",
        "updatedAt" : "2021-03-06T04:27:42Z",
        "lastEditedBy" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "tags" : [
        ]
      },
      {
        "id" : "09f869f3-e966-49ad-a114-ce11efd4b90d",
        "parentId" : "e744fc00-24cc-458d-9016-2846d3e17976",
        "authorId" : "c52c39ef-2b7f-4855-a2f1-2a2e50a719ae",
        "body" : "yes, good suggestion!",
        "createdAt" : "2021-03-06T06:48:54Z",
        "updatedAt" : "2021-03-06T07:07:28Z",
        "lastEditedBy" : "c52c39ef-2b7f-4855-a2f1-2a2e50a719ae",
        "tags" : [
        ]
      }
    ],
    "commit" : "fe693eb2d7c22d2be1344214e3d3f073be643eb4",
    "line" : 57,
    "diffHunk" : "@@ -1,1 +1044,1048 @@    assert (statuses != null)\n    for (status <- statuses) {\n      if (status == null) {\n        val errorMessage = s\"Missing an output location for shuffle $shuffleId\"\n        logError(errorMessage)"
  },
  {
    "id" : "211a89d8-f789-4f6d-a25b-e044562b8d21",
    "prId" : 29992,
    "prUrl" : "https://github.com/apache/spark/pull/29992#pullrequestreview-506063531",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "aa891d72-ca2b-4599-96cc-4225a8f13cc9",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Shall we remove `java.lang.`?",
        "createdAt" : "2020-10-10T04:01:40Z",
        "updatedAt" : "2020-10-10T04:01:41Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "1ca46a477020efa4e5bd11e4d55a82ef1e1b2254",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +137,141 @@      }\n    } catch {\n      case e: java.lang.NullPointerException =>\n        logWarning(s\"Unable to update map output for ${mapId}, status removed in-flight\")\n    }"
  },
  {
    "id" : "7cb5fd53-d951-4c9e-b2c9-6a626cb5d171",
    "prId" : 29992,
    "prUrl" : "https://github.com/apache/spark/pull/29992#pullrequestreview-507945293",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "58721be5-319e-44c2-beab-f3322c341ccf",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Quick question: can we avoid catching `NullPointerException`? It's a bit odd that we catch `NullPointerException`. We could just switch to if-else I guess.",
        "createdAt" : "2020-10-14T02:09:54Z",
        "updatedAt" : "2020-10-14T02:09:55Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "1ca46a477020efa4e5bd11e4d55a82ef1e1b2254",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +137,141 @@      }\n    } catch {\n      case e: java.lang.NullPointerException =>\n        logWarning(s\"Unable to update map output for ${mapId}, status removed in-flight\")\n    }"
  },
  {
    "id" : "5f62f02e-704c-4321-86be-67f1c1cff49e",
    "prId" : 28895,
    "prUrl" : "https://github.com/apache/spark/pull/28895#pullrequestreview-435716629",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "731e64de-9622-414f-b9a7-d2c3052c3349",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "can we keep it as overload as well?",
        "createdAt" : "2020-06-23T09:07:00Z",
        "updatedAt" : "2020-06-29T02:51:02Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "fc6b26b7-8d61-4908-ac19-5f8fd47b3e61",
        "parentId" : "731e64de-9622-414f-b9a7-d2c3052c3349",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "It would be only used by 3 tests yet. Do you think it's necessary? ",
        "createdAt" : "2020-06-23T12:10:19Z",
        "updatedAt" : "2020-06-29T02:51:02Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "4247fa323f264af65609f7285e82f86b8dd9be60",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +338,342 @@   *         Note that zero-sized blocks are excluded in the result.\n   */\n  def getMapSizesByExecutorId(\n      shuffleId: Int,\n      startMapIndex: Int,"
  },
  {
    "id" : "e935f20b-2d5c-4f7f-b79b-26dc705a5540",
    "prId" : 28708,
    "prUrl" : "https://github.com/apache/spark/pull/28708#pullrequestreview-430237875",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "46c5cb8b-cb1c-4f1b-b2a3-f8cf19bd7726",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Thank you for adding this API.",
        "createdAt" : "2020-06-14T17:37:52Z",
        "updatedAt" : "2020-07-17T23:45:20Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "8494bdd94285c7cc5a41e151da920710be7f4671",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +125,129 @@   * Update the map output location (e.g. during migration).\n   */\n  def updateMapOutput(mapId: Long, bmAddress: BlockManagerId): Unit = withWriteLock {\n    val mapStatusOpt = mapStatuses.find(_.mapId == mapId)\n    mapStatusOpt match {"
  },
  {
    "id" : "a625040d-566c-438a-b5ae-deff9ce09439",
    "prId" : 28708,
    "prUrl" : "https://github.com/apache/spark/pull/28708#pullrequestreview-430237945",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ec5d548e-4d54-4279-9c9b-1821f79853ce",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Thanks for this one, too.",
        "createdAt" : "2020-06-14T17:38:49Z",
        "updatedAt" : "2020-07-17T23:45:20Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "8494bdd94285c7cc5a41e151da920710be7f4671",
    "line" : 67,
    "diffHunk" : "@@ -1,1 +484,488 @@  }\n\n  def updateMapOutput(shuffleId: Int, mapId: Long, bmAddress: BlockManagerId): Unit = {\n    shuffleStatuses.get(shuffleId) match {\n      case Some(shuffleStatus) =>"
  },
  {
    "id" : "85842c1e-d675-48f6-8933-be6c2c37ef16",
    "prId" : 28708,
    "prUrl" : "https://github.com/apache/spark/pull/28708#pullrequestreview-430907642",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "818abca8-fb64-4e07-911b-328d47c324e3",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Shall we add an `assert(mapStatus.location != bmAddress)` here as a sanity check?",
        "createdAt" : "2020-06-15T08:06:13Z",
        "updatedAt" : "2020-07-17T23:45:20Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "0d5db554-2f24-4852-8f44-dd8e792cfebf",
        "parentId" : "818abca8-fb64-4e07-911b-328d47c324e3",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "no reason to, this is pretty close to a noop if we get a duplicated update message.",
        "createdAt" : "2020-06-15T18:52:47Z",
        "updatedAt" : "2020-07-17T23:45:20Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "8494bdd94285c7cc5a41e151da920710be7f4671",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +130,134 @@      case Some(mapStatus) =>\n        logInfo(s\"Updating map output for ${mapId} to ${bmAddress}\")\n        mapStatus.updateLocation(bmAddress)\n        invalidateSerializedMapOutputStatusCache()\n      case None =>"
  },
  {
    "id" : "0d959b93-a3e0-4f09-8b4d-2bed5a4664fb",
    "prId" : 28708,
    "prUrl" : "https://github.com/apache/spark/pull/28708#pullrequestreview-433770929",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f4e29790-a2b4-4d5e-9b62-6b7a74e30df4",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "I am curious on why sendTracker didn't throw an exception before and now it does ? Did the migration cause this ?",
        "createdAt" : "2020-06-19T01:18:49Z",
        "updatedAt" : "2020-07-17T23:45:20Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "019995e7-6ca2-4f60-9a14-abfbd01bb118",
        "parentId" : "f4e29790-a2b4-4d5e-9b62-6b7a74e30df4",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "So this is only because we call shutdown during the tests that this is needed. It doesn't throw exceptions normally.",
        "createdAt" : "2020-06-19T02:55:17Z",
        "updatedAt" : "2020-07-17T23:45:20Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "8494bdd94285c7cc5a41e151da920710be7f4671",
    "line" : 84,
    "diffHunk" : "@@ -1,1 +773,777 @@    mapOutputRequests.offer(PoisonPill)\n    threadpool.shutdown()\n    try {\n      sendTracker(StopMapOutputTracker)\n    } catch {"
  },
  {
    "id" : "094cbb5e-9a00-42e1-b976-58c5fa340e81",
    "prId" : 27604,
    "prUrl" : "https://github.com/apache/spark/pull/27604#pullrequestreview-362414708",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "22e1661d-1dbb-4944-9c18-c43dc035364a",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "I'd prefer to add another try catch when we call `broadcast.vaule` at:\r\n\r\nhttps://github.com/apache/spark/blob/a2aa966ef64bc06f65a646777568427d360605e9/core/src/main/scala/org/apache/spark/MapOutputTracker.scala#L969\r\n\r\nand throw `FetchFailedException` when we catch `SparkException` from broadcast.\r\n\r\n",
        "createdAt" : "2020-02-21T06:09:13Z",
        "updatedAt" : "2020-06-15T02:58:51Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "2e11d1bedf15b59c89b1f686ea716a575802f1e6",
    "line" : 42,
    "diffHunk" : "@@ -1,1 +827,831 @@      s\"partitions $startPartition-$endPartition\")\n    try {\n      val statuses = getStatuses(shuffleId, conf)\n      MapOutputTracker.convertMapStatuses(\n        shuffleId, startPartition, endPartition, statuses, startMapIndex, endMapIndex)"
  },
  {
    "id" : "fa406bb8-b67d-409f-b3dd-eee82386839e",
    "prId" : 27604,
    "prUrl" : "https://github.com/apache/spark/pull/27604#pullrequestreview-496100609",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "35eedc6e-8456-464b-b220-81f4afe312b6",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "Is it OK to clear out all the map status? Shouldn't we only drop the data of the current shuffle id?",
        "createdAt" : "2020-03-09T11:29:37Z",
        "updatedAt" : "2020-06-15T02:58:51Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "b14285b5-382f-46f1-91ab-d9d79fa82af9",
        "parentId" : "35eedc6e-8456-464b-b220-81f4afe312b6",
        "authorId" : "2e5e2a86-4faf-484a-9251-577e584e1564",
        "body" : "@cloud-fan Good question! yes, it's ok to clear all the map status, but I think maybe just drop the data of the current shuffle id is enough. But it seems that we currently bind an global epoch to the `MapOutputTracker`, if one stage `FetchFailed`, then the epoch will be updated, so that it will clear all the map statuses cache in the executor side.\r\nShould we change this behavior? if so may be we can put another PR for that.",
        "createdAt" : "2020-03-10T06:09:56Z",
        "updatedAt" : "2020-06-15T02:58:51Z",
        "lastEditedBy" : "2e5e2a86-4faf-484a-9251-577e584e1564",
        "tags" : [
        ]
      },
      {
        "id" : "81f76f35-263e-4735-9bf0-b86b2572cef8",
        "parentId" : "35eedc6e-8456-464b-b220-81f4afe312b6",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "I think there's a potential assuming that *shuffle data are aways randomly and evenly placed on nodes*. That means, any shuffle fetch failure can imply the potential fetch failure for other shuffles in future. So, currently, we aways clear `mapStatuses` when fetch failure happens.",
        "createdAt" : "2020-03-10T09:38:33Z",
        "updatedAt" : "2020-06-15T02:58:51Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "9dc9efd1-4c16-464b-a29b-f37da243ea55",
        "parentId" : "35eedc6e-8456-464b-b220-81f4afe312b6",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "But here is the broadcast being invalid issue. I don't think it usually happens for a lot of shuffles at the same time.",
        "createdAt" : "2020-09-25T04:20:31Z",
        "updatedAt" : "2020-09-25T04:20:31Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "2e11d1bedf15b59c89b1f686ea716a575802f1e6",
    "line" : 58,
    "diffHunk" : "@@ -1,1 +859,863 @@            case e: IOException if\n              Throwables.getCausalChain(e).asScala.exists(_.isInstanceOf[BlockNotFoundException]) =>\n              mapStatuses.clear()\n              throw new MetadataFetchFailedException(\n                shuffleId, -1, Throwables.getStackTraceAsString(e))"
  },
  {
    "id" : "01dc5baa-4f7b-4de2-a08b-3bf960cb3f02",
    "prId" : 26235,
    "prUrl" : "https://github.com/apache/spark/pull/26235#pullrequestreview-306208268",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fe64d159-2b75-4d52-be05-1359d0253099",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "So, this is the main change?",
        "createdAt" : "2019-10-23T21:36:03Z",
        "updatedAt" : "2019-10-23T22:49:58Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "e275f7f6-192f-4a8f-b18e-e9e31aef299e",
        "parentId" : "fe64d159-2b75-4d52-be05-1359d0253099",
        "authorId" : "677aa336-324b-4b93-8300-21f8bd378f26",
        "body" : "Yes. The codec is wrapped in buffered stream which avoids overhead excessive of JNI call while trying to decompress small amount of data each time when `objIn.readObject()` is called.",
        "createdAt" : "2019-10-23T21:39:32Z",
        "updatedAt" : "2019-10-23T22:49:58Z",
        "lastEditedBy" : "677aa336-324b-4b93-8300-21f8bd378f26",
        "tags" : [
        ]
      }
    ],
    "commit" : "e5acdbf46098d7e8d08f8db87f6646409dc4460a",
    "line" : 163,
    "diffHunk" : "@@ -1,1 +941,945 @@\n    def deserializeObject(arr: Array[Byte], off: Int, len: Int): AnyRef = {\n      val codec = CompressionCodec.createCodec(conf, \"zstd\")\n      // The ZStd codec is wrapped in a `BufferedInputStream` which avoids overhead excessive\n      // of JNI call while trying to decompress small amount of data for each element"
  },
  {
    "id" : "977b4278-e833-4a60-8e02-9d5a408b2094",
    "prId" : 26235,
    "prUrl" : "https://github.com/apache/spark/pull/26235#pullrequestreview-306233569",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4cf66365-0c47-4b5e-a166-f6dd25f691ef",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "It is now controlled by spark.io.compression.zstd.level. The default value is 1. Is it the same as before?",
        "createdAt" : "2019-10-23T22:09:12Z",
        "updatedAt" : "2019-10-23T22:49:58Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "1d0b6cb7-ac9f-40e8-878e-a65f04b912bb",
        "parentId" : "4cf66365-0c47-4b5e-a166-f6dd25f691ef",
        "authorId" : "677aa336-324b-4b93-8300-21f8bd378f26",
        "body" : "I think the default one for ZStd codec is 3 (before). Just tested, and I don't see much difference between 1 or 3.",
        "createdAt" : "2019-10-23T22:43:35Z",
        "updatedAt" : "2019-10-23T22:49:58Z",
        "lastEditedBy" : "677aa336-324b-4b93-8300-21f8bd378f26",
        "tags" : [
        ]
      }
    ],
    "commit" : "e5acdbf46098d7e8d08f8db87f6646409dc4460a",
    "line" : 101,
    "diffHunk" : "@@ -1,1 +904,908 @@    val out = new ApacheByteArrayOutputStream()\n    out.write(DIRECT)\n    val codec = CompressionCodec.createCodec(conf, \"zstd\")\n    val objOut = new ObjectOutputStream(codec.compressedOutputStream(out))\n    Utils.tryWithSafeFinally {"
  },
  {
    "id" : "e3dfd1fb-dea6-4511-a07d-07eebe5d6ab5",
    "prId" : 26235,
    "prUrl" : "https://github.com/apache/spark/pull/26235#pullrequestreview-306233837",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e6eb7474-56c9-4a31-9c48-bd47160e5818",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "since we are using CompressionCodec, instead of hard-coding zstd here, it is good to have a config for it now?",
        "createdAt" : "2019-10-23T22:11:17Z",
        "updatedAt" : "2019-10-23T22:49:58Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "4158220b-e291-4347-ade3-1fc929373e63",
        "parentId" : "e6eb7474-56c9-4a31-9c48-bd47160e5818",
        "authorId" : "677aa336-324b-4b93-8300-21f8bd378f26",
        "body" : "I don't like to have too many configurations, but we have the flexibility to add it later.",
        "createdAt" : "2019-10-23T22:44:23Z",
        "updatedAt" : "2019-10-23T22:49:58Z",
        "lastEditedBy" : "677aa336-324b-4b93-8300-21f8bd378f26",
        "tags" : [
        ]
      }
    ],
    "commit" : "e5acdbf46098d7e8d08f8db87f6646409dc4460a",
    "line" : 101,
    "diffHunk" : "@@ -1,1 +904,908 @@    val out = new ApacheByteArrayOutputStream()\n    out.write(DIRECT)\n    val codec = CompressionCodec.createCodec(conf, \"zstd\")\n    val objOut = new ObjectOutputStream(codec.compressedOutputStream(out))\n    Utils.tryWithSafeFinally {"
  },
  {
    "id" : "bba4250c-bb7c-48db-a5bc-92b4415639e7",
    "prId" : 26235,
    "prUrl" : "https://github.com/apache/spark/pull/26235#pullrequestreview-318331900",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "404d9193-05e4-46d0-9ea5-dc478cd3a5ca",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "All the other compressions have conf. Could we do it for this too? See the examples:\r\n\r\nhttps://github.com/apache/spark/blob/1b575ef5d1b8e3e672b2fca5c354d6678bd78bd1/core/src/main/scala/org/apache/spark/serializer/SerializerManager.scala#L67-L73",
        "createdAt" : "2019-11-07T07:39:37Z",
        "updatedAt" : "2019-11-07T07:39:37Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "7e17a340-46e2-4fec-941b-e0a5f24423ac",
        "parentId" : "404d9193-05e4-46d0-9ea5-dc478cd3a5ca",
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "cc @zsxwing @cloud-fan ",
        "createdAt" : "2019-11-07T07:39:50Z",
        "updatedAt" : "2019-11-07T07:39:50Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "93fd14a3-4816-4df0-94d9-c0071f7ee7c6",
        "parentId" : "404d9193-05e4-46d0-9ea5-dc478cd3a5ca",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "I brought the up here in the other pr, please see discussion there: https://github.com/apache/spark/pull/26085\r\n\r\nIf you think its needed now then we should file a jira for it.",
        "createdAt" : "2019-11-07T14:42:24Z",
        "updatedAt" : "2019-11-07T14:42:24Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "b5280eee-5ccf-4f77-90c8-c17ac556ec3e",
        "parentId" : "404d9193-05e4-46d0-9ea5-dc478cd3a5ca",
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "Created a JIRA https://issues.apache.org/jira/browse/SPARK-29939 @Ngone51 Could you submit a PR to fix it?",
        "createdAt" : "2019-11-18T07:16:48Z",
        "updatedAt" : "2019-11-18T07:16:48Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "6c15feb0-9c1c-40f0-8e38-0f095391c552",
        "parentId" : "404d9193-05e4-46d0-9ea5-dc478cd3a5ca",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Sure:)@gatorsmile",
        "createdAt" : "2019-11-18T13:34:21Z",
        "updatedAt" : "2019-11-18T13:34:21Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "e5acdbf46098d7e8d08f8db87f6646409dc4460a",
    "line" : 101,
    "diffHunk" : "@@ -1,1 +904,908 @@    val out = new ApacheByteArrayOutputStream()\n    out.write(DIRECT)\n    val codec = CompressionCodec.createCodec(conf, \"zstd\")\n    val objOut = new ObjectOutputStream(codec.compressedOutputStream(out))\n    Utils.tryWithSafeFinally {"
  },
  {
    "id" : "06993bf5-37db-48c0-81c0-8a979350ebc8",
    "prId" : 26085,
    "prUrl" : "https://github.com/apache/spark/pull/26085#pullrequestreview-300964537",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "09ec7100-3ec7-4a7c-bd2a-730d29638247",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Is the default compress level good for us?",
        "createdAt" : "2019-10-11T20:53:45Z",
        "updatedAt" : "2019-10-18T22:25:48Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "b18af0e2-479c-4667-9760-b46ce8331e60",
        "parentId" : "09ec7100-3ec7-4a7c-bd2a-730d29638247",
        "authorId" : "677aa336-324b-4b93-8300-21f8bd378f26",
        "body" : "Better than the gzip. We can tune it later.",
        "createdAt" : "2019-10-11T23:21:57Z",
        "updatedAt" : "2019-10-18T22:25:48Z",
        "lastEditedBy" : "677aa336-324b-4b93-8300-21f8bd378f26",
        "tags" : [
        ]
      }
    ],
    "commit" : "08c8fb2d79f499b074578991dc217a61d1633bb6",
    "line" : 53,
    "diffHunk" : "@@ -1,1 +911,915 @@\n    val arr: Array[Byte] = {\n      val zos = new ZstdOutputStream(compressedOut)\n      Utils.tryWithSafeFinally {\n        compressedOut.write(DIRECT)"
  },
  {
    "id" : "8d61d289-c641-4520-a9e4-2ded8b6a71e0",
    "prId" : 26085,
    "prUrl" : "https://github.com/apache/spark/pull/26085#pullrequestreview-304164961",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a7127062-0845-4178-b782-439d2374c3aa",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "This comment is out-of-dated now. Can be removed or updated, I think.",
        "createdAt" : "2019-10-16T23:05:35Z",
        "updatedAt" : "2019-10-18T22:25:48Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "438a42cb-a3ea-41a5-b63b-336f870fddb6",
        "parentId" : "a7127062-0845-4178-b782-439d2374c3aa",
        "authorId" : "677aa336-324b-4b93-8300-21f8bd378f26",
        "body" : "We are still calling toByteArray; thus, compressedOut can be reused.",
        "createdAt" : "2019-10-18T21:55:24Z",
        "updatedAt" : "2019-10-18T22:25:48Z",
        "lastEditedBy" : "677aa336-324b-4b93-8300-21f8bd378f26",
        "tags" : [
        ]
      },
      {
        "id" : "2a3ffa36-276c-4ddb-aecc-6d3edddd5248",
        "parentId" : "a7127062-0845-4178-b782-439d2374c3aa",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "this is super nit: toByteArray is called on  compressedOut, instead of out. this comment is a bit misleading.",
        "createdAt" : "2019-10-18T22:31:37Z",
        "updatedAt" : "2019-10-18T22:31:37Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "08c8fb2d79f499b074578991dc217a61d1633bb6",
    "line" : 68,
    "diffHunk" : "@@ -1,1 +926,930 @@      // Important arr(0) is the tag == DIRECT, ignore that while deserializing !\n      val bcast = broadcastManager.newBroadcast(arr, isLocal)\n      // toByteArray creates copy, so we can reuse out\n      out.reset()\n      val oos = new ObjectOutputStream(out)"
  },
  {
    "id" : "80ce22aa-1eb6-49ce-8e50-d5baaf865ba5",
    "prId" : 26085,
    "prUrl" : "https://github.com/apache/spark/pull/26085#pullrequestreview-340934919",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "991ac2ed-6bfb-46be-87ad-82f1793f4561",
        "parentId" : null,
        "authorId" : "8e87861a-4202-49a2-baf7-6d51f6aaa5a2",
        "body" : "Hi, @dbtsai , I am back-porting this into our internal repo. Looks like this compression is unnecessary since `arr` is already compressed by zstd. Compress again with already compressed byte[] is a waste of cpu time. WDYT?",
        "createdAt" : "2020-01-09T06:25:02Z",
        "updatedAt" : "2020-01-09T06:25:03Z",
        "lastEditedBy" : "8e87861a-4202-49a2-baf7-6d51f6aaa5a2",
        "tags" : [
        ]
      },
      {
        "id" : "4f14d0e4-d261-4e5f-8141-fef1e9c230be",
        "parentId" : "991ac2ed-6bfb-46be-87ad-82f1793f4561",
        "authorId" : "677aa336-324b-4b93-8300-21f8bd378f26",
        "body" : "The actually value of the data (which is already compressed) will not be in the serialized form of  `out.writeTo(zos)` as it's transient. Here, we are just serializing the reference to the actual data, and the actual data will be broadcast through `TorrentBroadcast`. See the next log, `\"Broadcast mapstatuses size = \" + outArr.length + \", actual size = \" + arr.length` for your real data. The broadcast one is very small.",
        "createdAt" : "2020-01-10T00:18:47Z",
        "updatedAt" : "2020-01-10T00:18:48Z",
        "lastEditedBy" : "677aa336-324b-4b93-8300-21f8bd378f26",
        "tags" : [
        ]
      },
      {
        "id" : "42616067-7d08-4894-bfd9-1dab22636346",
        "parentId" : "991ac2ed-6bfb-46be-87ad-82f1793f4561",
        "authorId" : "8e87861a-4202-49a2-baf7-6d51f6aaa5a2",
        "body" : "Thanks for your clarification. It's indeed not including the compressed data.",
        "createdAt" : "2020-01-10T03:11:13Z",
        "updatedAt" : "2020-01-10T03:11:13Z",
        "lastEditedBy" : "8e87861a-4202-49a2-baf7-6d51f6aaa5a2",
        "tags" : [
        ]
      }
    ],
    "commit" : "08c8fb2d79f499b074578991dc217a61d1633bb6",
    "line" : 83,
    "diffHunk" : "@@ -1,1 +936,940 @@      val outArr = {\n        compressedOut.reset()\n        val zos = new ZstdOutputStream(compressedOut)\n        Utils.tryWithSafeFinally {\n          compressedOut.write(BROADCAST)"
  },
  {
    "id" : "8f953860-9546-4a3d-b870-19dca8456e12",
    "prId" : 26017,
    "prUrl" : "https://github.com/apache/spark/pull/26017#pullrequestreview-297244059",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0f183155-8058-4ace-9b04-38279050dd0e",
        "parentId" : null,
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "This is the part where we expect the largest decrease in lock contention right?",
        "createdAt" : "2019-10-03T21:16:35Z",
        "updatedAt" : "2019-10-03T22:20:11Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "1bd28855-2924-4199-b689-20128cd42a5f",
        "parentId" : "0f183155-8058-4ace-9b04-38279050dd0e",
        "authorId" : "677aa336-324b-4b93-8300-21f8bd378f26",
        "body" : "The largest decrease is in serializing the large map status which I'll address in another PR. ",
        "createdAt" : "2019-10-04T02:59:06Z",
        "updatedAt" : "2019-10-04T02:59:06Z",
        "lastEditedBy" : "677aa336-324b-4b93-8300-21f8bd378f26",
        "tags" : [
        ]
      }
    ],
    "commit" : "f84efaf3deb2090da081eae00c8c07b4f08dc726",
    "line" : 118,
    "diffHunk" : "@@ -1,1 +196,200 @@    var result: Array[Byte] = null\n\n    withReadLock {\n      if (cachedSerializedMapStatus != null) {\n        result = cachedSerializedMapStatus"
  },
  {
    "id" : "62fec243-5ded-4f52-b976-01de62cd8147",
    "prId" : 26017,
    "prUrl" : "https://github.com/apache/spark/pull/26017#pullrequestreview-297244566",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e084e2ae-4c63-42e2-b0bc-26fd1e1ec858",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "We don't need `withWriteLock` here because `removeOutputsByFilter` is already wrapped `withWriteLock`.",
        "createdAt" : "2019-10-03T23:05:49Z",
        "updatedAt" : "2019-10-03T23:05:50Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "2d1a1375-67ed-444a-90aa-2f34e2b40ff6",
        "parentId" : "e084e2ae-4c63-42e2-b0bc-26fd1e1ec858",
        "authorId" : "677aa336-324b-4b93-8300-21f8bd378f26",
        "body" : "Since it's reentrant lock, I prefer to have it in all the methods in case someone adds something without guarding by lock.",
        "createdAt" : "2019-10-04T03:02:03Z",
        "updatedAt" : "2019-10-04T03:02:03Z",
        "lastEditedBy" : "677aa336-324b-4b93-8300-21f8bd378f26",
        "tags" : [
        ]
      }
    ],
    "commit" : "f84efaf3deb2090da081eae00c8c07b4f08dc726",
    "line" : 67,
    "diffHunk" : "@@ -1,1 +137,141 @@   * outputs which are served by an external shuffle server (if one exists).\n   */\n  def removeOutputsOnHost(host: String): Unit = withWriteLock {\n    removeOutputsByFilter(x => x.host == host)\n  }"
  },
  {
    "id" : "88cef9f2-4926-4cb3-b969-55e94d6533b3",
    "prId" : 25620,
    "prUrl" : "https://github.com/apache/spark/pull/25620#pullrequestreview-290029523",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b56b4d3e-add6-44ed-87ce-fe5c8f2238ad",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "This is a place that I think `mapIndex` makes sense.",
        "createdAt" : "2019-09-17T16:41:33Z",
        "updatedAt" : "2019-09-18T18:26:21Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "713f608c-0628-46a5-a66e-a7ff5f16cc5a",
        "parentId" : "b56b4d3e-add6-44ed-87ce-fe5c8f2238ad",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "Thanks for pointing this out.",
        "createdAt" : "2019-09-18T15:57:46Z",
        "updatedAt" : "2019-09-18T18:26:21Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "28c9f9c2da1215a836fef7925e95a40c7c6dd87e",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +89,93 @@   * will be replaced by the new location.\n   */\n  def addMapOutput(mapIndex: Int, status: MapStatus): Unit = synchronized {\n    if (mapStatuses(mapIndex) == null) {\n      _numAvailableOutputs += 1"
  },
  {
    "id" : "ed6458d0-6de1-4b40-9dbd-d644532c22b9",
    "prId" : 25620,
    "prUrl" : "https://github.com/apache/spark/pull/25620#pullrequestreview-289400086",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "53997cd2-6c61-430c-bc64-1d6e04fd4a47",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "ditto",
        "createdAt" : "2019-09-17T16:41:44Z",
        "updatedAt" : "2019-09-18T18:26:21Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "28c9f9c2da1215a836fef7925e95a40c7c6dd87e",
    "line" : 31,
    "diffHunk" : "@@ -1,1 +102,106 @@   * different block manager.\n   */\n  def removeMapOutput(mapIndex: Int, bmAddress: BlockManagerId): Unit = synchronized {\n    if (mapStatuses(mapIndex) != null && mapStatuses(mapIndex).location == bmAddress) {\n      _numAvailableOutputs -= 1"
  }
]