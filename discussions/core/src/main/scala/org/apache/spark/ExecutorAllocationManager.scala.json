[
  {
    "id" : "d7ebc8ef-081c-4f48-9085-64561cde6bc5",
    "prId" : 32526,
    "prUrl" : "https://github.com/apache/spark/pull/32526#pullrequestreview-663634581",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a24cda71-7570-42e5-8c19-9f5383c9e226",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Is the case of 0 different from >1 here? If there is >1 do you want to remove something anyway?",
        "createdAt" : "2021-05-15T17:41:14Z",
        "updatedAt" : "2021-05-15T17:41:14Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "62cf9e3a-7bda-403c-b8d0-dff3ee2b4435",
        "parentId" : "a24cda71-7570-42e5-8c19-9f5383c9e226",
        "authorId" : "607c2c93-3680-41e7-baaf-dc5887b76b9f",
        "body" : "I'm not sure. This behaviour was there before and it is not directly related to the bug.",
        "createdAt" : "2021-05-19T20:31:47Z",
        "updatedAt" : "2021-05-19T20:31:47Z",
        "lastEditedBy" : "607c2c93-3680-41e7-baaf-dc5887b76b9f",
        "tags" : [
        ]
      }
    ],
    "commit" : "887cbfb01dec1d417571b4f416b7ec5150e1e506",
    "line" : 53,
    "diffHunk" : "@@ -1,1 +867,871 @@          resourceProfileIdToStageAttempt(rpForStage.head) -= stageAttempt\n        } else {\n          logWarning(s\"Should have exactly one resource profile for stage $stageAttempt,\" +\n              s\" but have $rpForStage\")\n        }"
  },
  {
    "id" : "1c8f1b6b-f5f3-409a-a5da-6c5f319845c3",
    "prId" : 32526,
    "prUrl" : "https://github.com/apache/spark/pull/32526#pullrequestreview-666843236",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5e698a9c-ffae-4d7b-9e72-a3e9e6bb0571",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Last item - is the point here that there's no point is storing a 0 in the map, as that's the default, and it makes the attempt turn up in the map keys, when you need it not to?",
        "createdAt" : "2021-05-20T21:12:37Z",
        "updatedAt" : "2021-05-20T21:12:38Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "6a91ce09-55cb-48c5-b2ed-e1aea7fc74dc",
        "parentId" : "5e698a9c-ffae-4d7b-9e72-a3e9e6bb0571",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "I would like to clarify this, is this causing a leak?  ie are you getting some event late or out of order such that this added an entry in stageAttemptToNumRunningTask but then its never removed?    I think this change is actually ok but would like to know if its needed. I would also like to know when it happened to see if perhaps there is a race we can handle elsewhere in the stage attempt being added. ",
        "createdAt" : "2021-05-24T14:00:52Z",
        "updatedAt" : "2021-05-24T14:00:52Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "b2d2c401-569e-4741-a4d7-6be4972a7d1c",
        "parentId" : "5e698a9c-ffae-4d7b-9e72-a3e9e6bb0571",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "guess there is more in the jira:\r\n> If spark-dynamic-executor-allocation thread calls schedule() after a SparkListenerTaskEnd event for the last task in a stage\r\nbut before SparkListenerStageCompleted event for the stage, then stageAttemptToNumRunningTask will not be cleaned up properly.\r\n\r\nI'm not following this though because onTaskEnd both removes the stage from stageAttemptToNumRunningTask as well as removes the stageAttempt from resourceProfileIdToStageAttempt. so when this is called it shouldn't be in  resourceProfileIdToStageAttempt.  Is there a case we are hitting that it is in there, like multiple stages using that resource profile?",
        "createdAt" : "2021-05-24T14:07:02Z",
        "updatedAt" : "2021-05-24T14:07:02Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "772db2bf-61c5-4c6e-a503-d2f7a0410e8d",
        "parentId" : "5e698a9c-ffae-4d7b-9e72-a3e9e6bb0571",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "ok understanding the other issue, makes this one clear now, this change is fine. \r\nthis introduces the possibility to add it back to the map with value 0 and it never be removed, returning 0 here is fine because it is really needs to be added back it will be added in the task start handling",
        "createdAt" : "2021-05-24T14:55:43Z",
        "updatedAt" : "2021-05-24T14:55:44Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "887cbfb01dec1d417571b4f416b7ec5150e1e506",
    "line" : 64,
    "diffHunk" : "@@ -1,1 +931,935 @@      // attempts is a Set, change to Seq so we keep all values\n      attempts.map { attempt =>\n        stageAttemptToNumRunningTask.getOrElse(attempt, 0)\n      }.sum\n    }"
  },
  {
    "id" : "2c2958b9-4dfb-473f-8e5f-f35ecadd463a",
    "prId" : 32526,
    "prUrl" : "https://github.com/apache/spark/pull/32526#pullrequestreview-666802100",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "66a9bb10-36da-4224-9c61-020bda5b5c85",
        "parentId" : null,
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "from jira:\r\n> If a SparkListenerTaskEnd event for the last task in a stage was processed before SparkListenerStageCompleted for that stage,\r\nthen resourceProfileIdToStageAttempt will not be cleaned up properly.\r\n\r\nI don't follow this, onTaskEnd is where resourceProfileIdToStageAttempt is cleaned up and removed, so how does  SparkListenerStageCompleted being called afterward affect it?  Is it being added back somehow that I\"m not seeing?  If you can give the sequence of events that would be very helpful.",
        "createdAt" : "2021-05-24T14:15:27Z",
        "updatedAt" : "2021-05-24T14:15:28Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "67eade0e-c754-43f1-a087-a72ede17f552",
        "parentId" : "66a9bb10-36da-4224-9c61-020bda5b5c85",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "oh wait, I think I see it.   stageAttemptToNumTasks only gets cleaned up in the onStageCompleted method.",
        "createdAt" : "2021-05-24T14:18:58Z",
        "updatedAt" : "2021-05-24T14:18:58Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "887cbfb01dec1d417571b4f416b7ec5150e1e506",
    "line" : 26,
    "diffHunk" : "@@ -1,1 +783,787 @@          if (stageAttemptToNumRunningTask(stageAttempt) == 0) {\n            stageAttemptToNumRunningTask -= stageAttempt\n            removeStageFromResourceProfileIfUnused(stageAttempt)\n          }\n        }"
  },
  {
    "id" : "4e698130-c99c-471d-bd77-03b462a9015d",
    "prId" : 32306,
    "prUrl" : "https://github.com/apache/spark/pull/32306#pullrequestreview-642930840",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f5bf136a-9449-4cce-85b8-cc9a895fe4d5",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "previously `pending` is `pendingTasksPerResourceProfile(rp) + pendingSpeculativeTasksPerResourceProfile(rp)`, now we use `pendingTasksPerResourceProfile() + pendingSpeculative`, and `pendingSpeculative` simply calls `pendingSpeculativeTasksPerResourceProfile(...)`\r\n\r\nseems like a straightforward code clean up.",
        "createdAt" : "2021-04-23T04:49:26Z",
        "updatedAt" : "2021-04-23T04:49:26Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "d3ca2c70-cd9f-409f-8ece-b35a2193f30c",
        "parentId" : "f5bf136a-9449-4cce-85b8-cc9a895fe4d5",
        "authorId" : "31afe32d-3af0-4fcf-93e2-115f5d7bab18",
        "body" : "previously  the `pendingSpeculativeTasksPerResourceProfile(rp)` will be called not only  by `totalPendingTasksPerResourceProfile(rpId)` but also  by  ` val pendingSpeculative = listener.pendingSpeculativeTasksPerResourceProfile(rpId)`, this PR  we only call the `pendingSpeculativeTasksPerResourceProfile(rp) ` once  to get the  pendingSpeculativeTasks , and  use  it for   `numRunningOrPendingTasks` and  `pendingSpeculative`.",
        "createdAt" : "2021-04-23T05:14:01Z",
        "updatedAt" : "2021-04-23T05:17:01Z",
        "lastEditedBy" : "31afe32d-3af0-4fcf-93e2-115f5d7bab18",
        "tags" : [
        ]
      }
    ],
    "commit" : "2499f818f0acf16db90b6445d9a28fc9e233b385",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +295,299 @@    val unschedulableTaskSets = listener.pendingUnschedulableTaskSetsPerResourceProfile(rpId)\n    val running = listener.totalRunningTasksPerResourceProfile(rpId)\n    val numRunningOrPendingTasks = pendingTask + pendingSpeculative + running\n    val rp = resourceProfileManager.resourceProfileFromId(rpId)\n    val tasksPerExecutor = rp.maxTasksPerExecutor(conf)"
  },
  {
    "id" : "98aae3a8-f05a-428f-b969-8de3a6865d90",
    "prId" : 31025,
    "prUrl" : "https://github.com/apache/spark/pull/31025#pullrequestreview-561524577",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b4f93e21-d7f6-4703-8379-5c5187c34363",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Since this is non-trivial, could you add some comments describing when this happens?",
        "createdAt" : "2021-01-05T04:48:23Z",
        "updatedAt" : "2021-01-05T05:21:58Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "2d0a3e4c-4f33-4233-93c2-3ecd27e65b4b",
        "parentId" : "b4f93e21-d7f6-4703-8379-5c5187c34363",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Or, we can add some meaningful log message with `else` statement. It will be helpful to understand the job.",
        "createdAt" : "2021-01-05T04:48:53Z",
        "updatedAt" : "2021-01-05T05:21:58Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "6011ca87-3977-4128-9084-f9c9a9c4f5a0",
        "parentId" : "b4f93e21-d7f6-4703-8379-5c5187c34363",
        "authorId" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "body" : "comment added.",
        "createdAt" : "2021-01-05T05:22:15Z",
        "updatedAt" : "2021-01-05T05:22:16Z",
        "lastEditedBy" : "0223d15f-93d2-46b2-bee2-7f40fcd0335a",
        "tags" : [
        ]
      },
      {
        "id" : "28ebe6db-58e9-4523-b939-d5ad3e65e493",
        "parentId" : "b4f93e21-d7f6-4703-8379-5c5187c34363",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Thanks!",
        "createdAt" : "2021-01-05T05:34:46Z",
        "updatedAt" : "2021-01-05T05:34:46Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "74be2eade9824137269996e54b170f7f103767d4",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +803,807 @@          if (stageAttemptToNumSpeculativeTasks.contains(stageAttempt)) {\n            stageAttemptToNumSpeculativeTasks(stageAttempt) -= 1\n          }\n        }\n"
  },
  {
    "id" : "d9547d91-7ddc-4307-9146-fcc6f6a1f826",
    "prId" : 30795,
    "prUrl" : "https://github.com/apache/spark/pull/30795#pullrequestreview-557482652",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e4e34873-591c-42a3-8572-02ca9a4b8fc9",
        "parentId" : null,
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "so I don't agree with this, at least not how its defined.  The user defined the maximum number of executors to use, this is getting more than that. I realize that some are excluded, but this also comes down to a resource utilization question as well. If I am in multi-tenant environment, I want to make sure 1 job doesn't take over the entire cluster. max is one way to do this.  I think we would either need to redefine this, which isn't great for backwards compatibility and could result in unexpected behavior or we add another config that is around the excluded nodes. this would either just be an allow to go over or a allow to go over by X. The downside to this is default would be 0 or false so you would have to configure if you do set max and want to use this feature. But I don't see a lot of jobs setting max unless they are trying to be nice in multi-tenant so it seems ok as long as its in release notes, etc.\r\n\r\n  you will notice the other logic for unschedulableTaskSets does not increase this, just increases the number we ask for.",
        "createdAt" : "2020-12-18T15:19:48Z",
        "updatedAt" : "2020-12-18T16:06:17Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "c5317faa-ec25-41ce-adec-3733a7f46982",
        "parentId" : "e4e34873-591c-42a3-8572-02ca9a4b8fc9",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Make sense to me. Add an extra conf would be a good choice.\r\n\r\nAlthough, I'm rethinking this change. It only takes effect when users set the max explicitly and the cluster reaches the max.( By default, max is Int.MaxValue. So we won't reach the max normally.) However, we still want to replace those excluded executors even if the cluster doesn't reach the max. For example, max/2 may be enough for task scheduling. And TaskScheduler also thinks there're max/2 executors without realizing X executors actually excluded.\r\n\r\nSo I think what we actually need here is to forcibly replace excluded executors when dynamic allocation & exclusion (but not kill) are both enabled. And it should not be related to the max value.\r\n\r\n\r\n",
        "createdAt" : "2020-12-23T03:01:29Z",
        "updatedAt" : "2020-12-23T03:01:29Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "75cdd1e334c15c8af89c9b0d283bedf12d0f33ee",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +505,509 @@    // launch new executors to replace the excluded executors.\n    val exclude = executorMonitor.excludedExecutorCount\n    val maxOverheadExecutors = maxNumExecutors + exclude\n    // Do not request more executors if it would put our target over the upper bound\n    // this is doing a max check per ResourceProfile"
  },
  {
    "id" : "06b065e2-0b50-4419-8f59-9eeda6cd8651",
    "prId" : 29367,
    "prUrl" : "https://github.com/apache/spark/pull/29367#pullrequestreview-463829692",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d2c4034a-ee7b-427d-b0c5-462005c0d89d",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "Please change the warning message to also say something about storage migration. ",
        "createdAt" : "2020-08-06T05:35:08Z",
        "updatedAt" : "2020-08-12T19:09:04Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "99d24048-32e7-407c-b1d8-c3e8cb5620f7",
        "parentId" : "d2c4034a-ee7b-427d-b0c5-462005c0d89d",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "What do you want mentioned about storage migration?",
        "createdAt" : "2020-08-06T18:58:41Z",
        "updatedAt" : "2020-08-12T19:09:04Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "d282722b-0be3-46b4-b5f5-e1f996f6db2a",
        "parentId" : "d2c4034a-ee7b-427d-b0c5-462005c0d89d",
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "Ideally I would have a different log message: \r\n\r\n```\r\n     if (conf.get(config.DYN_ALLOCATION_SHUFFLE_TRACKING_ENABLED)) {\r\n        logWarning(\"Dynamic allocation without a shuffle service is an experimental feature.\")\r\n     } else if (conf.get(WORKER_DECOMMISSION_ENABLED) &&\r\n            conf.get(config.STORAGE_DECOMMISSION_SHUFFLE_BLOCKS_ENABLED)) {\r\n        logWarning(\"Decommissioning with shuffle block migration is an experimental feature.\")\r\n     }\r\n```\r\n",
        "createdAt" : "2020-08-06T21:39:09Z",
        "updatedAt" : "2020-08-12T19:09:04Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "e55c245b-20e1-4f62-9d4e-3a6602a0793e",
        "parentId" : "d2c4034a-ee7b-427d-b0c5-462005c0d89d",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "That doesnâ€™t make sense. Dynamic allocation without a shuffle service is the expiremental feature in either case.",
        "createdAt" : "2020-08-08T18:25:14Z",
        "updatedAt" : "2020-08-12T19:09:04Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "0ec3bbd6-2f79-40b9-bc55-1d32d760c1df",
        "parentId" : "d2c4034a-ee7b-427d-b0c5-462005c0d89d",
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "Okay. I understand the intention now. Nevermind :-) ",
        "createdAt" : "2020-08-08T22:30:21Z",
        "updatedAt" : "2020-08-12T19:09:05Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      }
    ],
    "commit" : "e970cb10147fb64533f5088edc3a448b5ef198cf",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +214,218 @@          (decommissionEnabled &&\n            conf.get(config.STORAGE_DECOMMISSION_SHUFFLE_BLOCKS_ENABLED))) {\n        logWarning(\"Dynamic allocation without a shuffle service is an experimental feature.\")\n      } else if (!testing) {\n        throw new SparkException(\"Dynamic allocation of executors requires the external \" +"
  },
  {
    "id" : "9d2216ab-358d-4343-817c-b7e802e75c35",
    "prId" : 29367,
    "prUrl" : "https://github.com/apache/spark/pull/29367#pullrequestreview-462895113",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "18b4ca15-c8cc-48b3-a073-e5d4da7c8635",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "Question: The default implementation of the ExecutorAllocationClient is to call killExecutors. So what does it mean for the WORKER_DECOM_ENABLED = true and the default implementation being called (ie the decommissionExecutors is not overridden) ? Perhaps it means that decommission is enabled by a config but the cluster manager does not support decommission and is thus ignored ? \r\n\r\nIn such an event should we be at parity with the else codepath -- ie `force` should be set to false in ExecutorAllocationClient ?",
        "createdAt" : "2020-08-06T05:36:56Z",
        "updatedAt" : "2020-08-12T19:09:04Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "4b2cc146-a971-4f29-ac7f-3862bb497496",
        "parentId" : "18b4ca15-c8cc-48b3-a073-e5d4da7c8635",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "So if someones enabled worker decommissioning and there cluster manager doesn't support it, we delegate to kill. Force is set to false already as a default in the call to `killExecutors`.",
        "createdAt" : "2020-08-06T19:01:50Z",
        "updatedAt" : "2020-08-12T19:09:04Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "cfb3f764-c14a-4895-a23c-a5c247dc5351",
        "parentId" : "18b4ca15-c8cc-48b3-a073-e5d4da7c8635",
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "Great, thanks for double checking that !",
        "createdAt" : "2020-08-06T21:39:39Z",
        "updatedAt" : "2020-08-12T19:09:04Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      }
    ],
    "commit" : "e970cb10147fb64533f5088edc3a448b5ef198cf",
    "line" : 51,
    "diffHunk" : "@@ -1,1 +579,583 @@        val executorIdsWithoutHostLoss = executorIdsToBeRemoved.toSeq.map(\n          id => (id, ExecutorDecommissionInfo(\"spark scale down\", false))).toArray\n        client.decommissionExecutors(executorIdsWithoutHostLoss, adjustTargetNumExecutors = false)\n      } else {\n        client.killExecutors(executorIdsToBeRemoved.toSeq, adjustTargetNumExecutors = false,"
  },
  {
    "id" : "17027ef5-b59c-42f0-88d0-0896da685c46",
    "prId" : 29367,
    "prUrl" : "https://github.com/apache/spark/pull/29367#pullrequestreview-464610852",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "25105b01-e221-4e85-bf8b-b6354b7797bc",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "Actually there is a key big difference between `killExecutors` and `decommissionExecutors`: The former does the actual killing and the latter does 'graceful death': Graceful death waits for all the migrations etc to happen and then the executor exits. I think this may lead to an overcommit: The old executor is still being decommissioned and migrating things, and a new one could be spun up. The two executors are alive at the same time (perhaps on the same node). Would this cause issues ?\r\n\r\nThis problem does not exist for `killExecutors` so much: Kill executor will kill the executor soon enough and thus the overlap window is smaller. ",
        "createdAt" : "2020-08-08T23:29:25Z",
        "updatedAt" : "2020-08-12T19:09:05Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "d1fae541-f9b7-421c-9c46-1b33da6dfcde",
        "parentId" : "25105b01-e221-4e85-bf8b-b6354b7797bc",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "We're talking in standalone mode? Multiple executors on the same worker is a deprecated feature so I don't think we have to worry about supporting new functionality with it.",
        "createdAt" : "2020-08-10T18:23:27Z",
        "updatedAt" : "2020-08-12T19:09:05Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "61eddf97-c1b8-4a72-8377-6083121cb970",
        "parentId" : "25105b01-e221-4e85-bf8b-b6354b7797bc",
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "Okay. It is a little bit unsettling to mer personally, that we don't have a timeout on the time that an executor would take to \"migrate and finally die\", but I guess we can live with that shortcoming for a while. It just reduces the effective cluster capacity for an indeterminate amount of time, which is what makes me slightly queasy about it.",
        "createdAt" : "2020-08-10T19:41:03Z",
        "updatedAt" : "2020-08-12T19:09:05Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "6cb63c0a-59b1-4b42-a2ec-6e64cb863a16",
        "parentId" : "25105b01-e221-4e85-bf8b-b6354b7797bc",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "So given this is in the Spark planned scale down path that _seems_ to be ok (the goal is to eventually reduce the capacity).",
        "createdAt" : "2020-08-10T21:47:21Z",
        "updatedAt" : "2020-08-12T19:09:05Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "e970cb10147fb64533f5088edc3a448b5ef198cf",
    "line" : 53,
    "diffHunk" : "@@ -1,1 +581,585 @@        client.decommissionExecutors(executorIdsWithoutHostLoss, adjustTargetNumExecutors = false)\n      } else {\n        client.killExecutors(executorIdsToBeRemoved.toSeq, adjustTargetNumExecutors = false,\n          countFailures = false, force = false)\n      }"
  },
  {
    "id" : "1a333294-f37d-45d8-a509-5a7503f76e34",
    "prId" : 28287,
    "prUrl" : "https://github.com/apache/spark/pull/28287#pullrequestreview-442592044",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ebea198e-c006-4681-bddf-52fde5c46a14",
        "parentId" : null,
        "authorId" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "body" : "I think this should be:\n```\nmath.max(totalNeed, executorMonitor.executorCountWithResourceProfile(rpId) + maxNeededForUnschedules)\n```\nIf we were already going to request `maxNeededForUnschedulables` _more_ executors than we currently have as part of `totalNeed`, that already satisfies the requirements. ",
        "createdAt" : "2020-06-29T17:25:36Z",
        "updatedAt" : "2020-07-22T17:58:20Z",
        "lastEditedBy" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "tags" : [
        ]
      },
      {
        "id" : "1849d8b6-a474-4735-bde8-05d8437bbd5a",
        "parentId" : "ebea198e-c006-4681-bddf-52fde5c46a14",
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "Can you update with details about how this was resolved ? The original looked fine to me, but I want to make sure I am not missing something.",
        "createdAt" : "2020-07-04T06:42:29Z",
        "updatedAt" : "2020-07-22T17:58:20Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "e0538fd4-6e15-4603-85e1-982f0a474b5e",
        "parentId" : "ebea198e-c006-4681-bddf-52fde5c46a14",
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "Scratch that - what I had seen was probably a modified version, not original ...",
        "createdAt" : "2020-07-04T06:53:32Z",
        "updatedAt" : "2020-07-22T17:58:20Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      }
    ],
    "commit" : "d6f1e73ae49d9641a8efcb1f5e9016bd378a70c6",
    "line" : 31,
    "diffHunk" : "@@ -1,1 +309,313 @@      math.max(maxNeededWithSpeculationLocalityOffset,\n        executorMonitor.executorCountWithResourceProfile(rpId) + maxNeededForUnschedulables)\n    } else {\n      maxNeededWithSpeculationLocalityOffset\n    }"
  },
  {
    "id" : "e0a1730f-7728-4d09-81f6-4d2defed9410",
    "prId" : 28084,
    "prUrl" : "https://github.com/apache/spark/pull/28084#pullrequestreview-384882196",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5c824679-c7d6-497f-b0d8-1b37e4e31656",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "A minor fix.",
        "createdAt" : "2020-03-31T16:02:44Z",
        "updatedAt" : "2020-03-31T16:02:51Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "7dcd604572b192ede1b6b05562dcae22efe8ff1c",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +438,442 @@            logDebug(s\"Lowering target number of executors to\" +\n              s\" ${numExecutorsTargetPerResourceProfileId(rpId)} (previously \" +\n              s\"${targetNum.oldNumExecutorsTarget} for resource profile id: ${rpId}) \" +\n              \"because not all requested executors \" +\n              \"are actually needed\")"
  },
  {
    "id" : "ea4b820a-8872-4518-a701-18df42cfc633",
    "prId" : 27313,
    "prUrl" : "https://github.com/apache/spark/pull/27313#pullrequestreview-346791579",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e21f6cc3-cbd4-4870-87ed-b61c74414e1f",
        "parentId" : null,
        "authorId" : "99e31844-a5b3-48a2-af71-412edb607a13",
        "body" : "nit: `s` is not necessary at line 473",
        "createdAt" : "2020-01-22T18:04:16Z",
        "updatedAt" : "2020-02-10T21:40:38Z",
        "lastEditedBy" : "99e31844-a5b3-48a2-af71-412edb607a13",
        "tags" : [
        ]
      }
    ],
    "commit" : "15f4c96b06e887913dc3c8ede8d385bfae514db9",
    "line" : 354,
    "diffHunk" : "@@ -1,1 +479,483 @@    // Do not request more executors if it would put our target over the upper bound\n    // this is doing a max check per ResourceProfile\n    if (oldNumExecutorsTarget >= maxNumExecutors) {\n      logDebug(\"Not adding executors because our current target total \" +\n        s\"is already ${oldNumExecutorsTarget} (limit $maxNumExecutors)\")"
  },
  {
    "id" : "4c9a8af3-0e89-44e0-a7f8-3d5ed652fd9c",
    "prId" : 27313,
    "prUrl" : "https://github.com/apache/spark/pull/27313#pullrequestreview-353509183",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f29895d7-e8c2-49fe-8973-a231ce0c0212",
        "parentId" : null,
        "authorId" : "99e31844-a5b3-48a2-af71-412edb607a13",
        "body" : "nit: Do we need to update this line?",
        "createdAt" : "2020-01-22T18:09:42Z",
        "updatedAt" : "2020-02-10T21:40:38Z",
        "lastEditedBy" : "99e31844-a5b3-48a2-af71-412edb607a13",
        "tags" : [
        ]
      },
      {
        "id" : "adf03fb5-3b3e-4cea-9be2-59199c554df5",
        "parentId" : "f29895d7-e8c2-49fe-8973-a231ce0c0212",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "No but the comment was wrong there is no stageAttemptToNumRunningTasks datastructure and didn't want it confusing for people reviewing the change, I can remove if you like?",
        "createdAt" : "2020-01-22T22:00:49Z",
        "updatedAt" : "2020-02-10T21:40:38Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "ecbbb024-cd56-49a5-bd61-d47b40f03b8c",
        "parentId" : "f29895d7-e8c2-49fe-8973-a231ce0c0212",
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "Probably some refactoring which changed the name and not comment.",
        "createdAt" : "2020-02-05T08:21:08Z",
        "updatedAt" : "2020-02-10T21:40:38Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      }
    ],
    "commit" : "15f4c96b06e887913dc3c8ede8d385bfae514db9",
    "line" : 542,
    "diffHunk" : "@@ -1,1 +673,677 @@      val stageAttempt = StageAttempt(stageId, stageAttemptId)\n      allocationManager.synchronized {\n        // do NOT remove stageAttempt from stageAttemptToNumRunningTask\n        // because the attempt may still have running tasks,\n        // even after another attempt for the stage is submitted."
  },
  {
    "id" : "e43a1974-23be-4f6e-9a29-de8e902e7d61",
    "prId" : 27313,
    "prUrl" : "https://github.com/apache/spark/pull/27313#pullrequestreview-346935403",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d3882da0-58c0-4512-b9fa-aac4c3b3b543",
        "parentId" : null,
        "authorId" : "99e31844-a5b3-48a2-af71-412edb607a13",
        "body" : "Is it better to refactor code to use a common function from line 778 to line 814? The following stuff can be reused.\r\n\r\n```\r\n      val pending = attempts.map { attempt =>\r\n        val numTotalTasks = stageAttemptToNumTasks.getOrElse(attempt, 0)\r\n        val numRunning = stageAttemptToTaskIndices.get(attempt).map(_.size).getOrElse(0)\r\n        numTotalTasks - numRunning\r\n      }.sum\r\n```",
        "createdAt" : "2020-01-22T18:13:16Z",
        "updatedAt" : "2020-02-10T21:40:38Z",
        "lastEditedBy" : "99e31844-a5b3-48a2-af71-412edb607a13",
        "tags" : [
        ]
      },
      {
        "id" : "35345662-036d-48d2-9554-0614ebe4c45b",
        "parentId" : "d3882da0-58c0-4512-b9fa-aac4c3b3b543",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "yes good idea, I'll make a utility function",
        "createdAt" : "2020-01-22T22:03:26Z",
        "updatedAt" : "2020-02-10T21:40:38Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "15f4c96b06e887913dc3c8ede8d385bfae514db9",
    "line" : 596,
    "diffHunk" : "@@ -1,1 +786,790 @@     * Note: This is not thread-safe without the caller owning the `allocationManager` lock.\n     */\n    def pendingTasksPerResourceProfile(rpId: Int): Int = {\n      val attempts = resourceProfileIdToStageAttempt.getOrElse(rpId, Set.empty).toSeq\n      attempts.map(attempt => getPendingTaskSum(attempt)).sum"
  },
  {
    "id" : "041ba66e-9d93-4133-ba1b-aeba833eb91c",
    "prId" : 27313,
    "prUrl" : "https://github.com/apache/spark/pull/27313#pullrequestreview-353509183",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c7685e16-25f5-4149-a1e5-e58c4f66c2c0",
        "parentId" : null,
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "nit: This is per resource profile (and not global) right ? Can we add that into the language ? The 'there are not enough tasks that could use the executor' does imply it - but  would be nice if we made it explicit.",
        "createdAt" : "2020-02-05T07:28:25Z",
        "updatedAt" : "2020-02-10T21:40:38Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      }
    ],
    "commit" : "15f4c96b06e887913dc3c8ede8d385bfae514db9",
    "line" : 31,
    "diffHunk" : "@@ -1,1 +63,67 @@ * for that ResourceProfile has been idle for K seconds and the number of executors is more\n * then what is needed for that ResourceProfile, meaning there are not enough tasks that could use\n * the executor, then it is removed. Note that an executor caching any data\n * blocks will be removed if it has been idle for more than L seconds.\n *"
  },
  {
    "id" : "a3d79784-d3d7-4218-a491-b0f58e5868a4",
    "prId" : 27223,
    "prUrl" : "https://github.com/apache/spark/pull/27223#pullrequestreview-347627795",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d080aced-c731-424a-a535-fcd4114b6b6d",
        "parentId" : null,
        "authorId" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "body" : "Why do we want to include `TaskKilled` here? It's not quite straight forward to me",
        "createdAt" : "2020-01-16T23:21:38Z",
        "updatedAt" : "2020-01-27T18:47:04Z",
        "lastEditedBy" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "tags" : [
        ]
      },
      {
        "id" : "2015c09b-92db-4bca-bbaf-3e31f21f1841",
        "parentId" : "d080aced-c731-424a-a535-fcd4114b6b6d",
        "authorId" : "0387304e-5de3-452a-8609-ecd82ff5bfbe",
        "body" : "Inside the the brackets, there are two things:\r\n```\r\nif (totalPendingTasks() == 0) {\r\n    allocationManager.onSchedulerBacklogged()\r\n}\r\n```\r\nThis one is straightforward. If a task is intentionally killed, then we don't expect this task to be resubmitted again, and we don't need to mark the scheduler as backlogged.\r\n```\r\nif (!taskEnd.taskInfo.speculative) {\r\n    stageAttemptToTaskIndices.get(stageAttempt).foreach {_.remove(taskIndex)}\r\n}\r\n```\r\nIf a non-speculative task is intentionally killed, it means the speculative task has succeeded, and no further task of this task index will be resubmitted. In this case, the task index is completed and we shouldn't remove it from `stageAttemptToTaskIndices`. Otherwise, we will have a pending non-speculative task for the task index.\r\n\r\n",
        "createdAt" : "2020-01-18T04:10:40Z",
        "updatedAt" : "2020-01-27T18:47:04Z",
        "lastEditedBy" : "0387304e-5de3-452a-8609-ecd82ff5bfbe",
        "tags" : [
        ]
      },
      {
        "id" : "9609945b-f09c-4f56-9be1-ada7bdb143cf",
        "parentId" : "d080aced-c731-424a-a535-fcd4114b6b6d",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "I think it would be good to add a comment under the taskkilled explaining this",
        "createdAt" : "2020-01-23T22:40:50Z",
        "updatedAt" : "2020-01-27T18:47:04Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "3ea1175a4d7ccf71095a48945fa8946558b83c21",
    "line" : 69,
    "diffHunk" : "@@ -1,1 +623,627 @@\n        taskEnd.reason match {\n          case Success | _: TaskKilled =>\n          case _ =>\n            if (totalPendingTasks() == 0) {"
  },
  {
    "id" : "6c48bb70-9f1c-42c7-a712-1fda3269a9e5",
    "prId" : 27223,
    "prUrl" : "https://github.com/apache/spark/pull/27223#pullrequestreview-344923379",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bbb4def6-e182-4ebe-9bee-691741c41711",
        "parentId" : null,
        "authorId" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "body" : "thanks for fixing this, we made a mistake previously.",
        "createdAt" : "2020-01-16T23:22:35Z",
        "updatedAt" : "2020-01-27T18:47:04Z",
        "lastEditedBy" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "tags" : [
        ]
      },
      {
        "id" : "a755b3a6-52ec-4292-be4b-38f0b64c6067",
        "parentId" : "bbb4def6-e182-4ebe-9bee-691741c41711",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "I think `stageAttemptToSpeculativeTaskIndices` also includes successful finished speculative tasks? Remove a successful speculative task will count it into pending speculative tasks again.",
        "createdAt" : "2020-01-17T03:08:39Z",
        "updatedAt" : "2020-01-27T18:47:04Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "610ce72b-cf91-4c73-9dfc-24bdf72df79a",
        "parentId" : "bbb4def6-e182-4ebe-9bee-691741c41711",
        "authorId" : "0387304e-5de3-452a-8609-ecd82ff5bfbe",
        "body" : "Note we already remove pending task `stageAttemptToNumSpeculativeTasks(stageAttempt) -= 1`. @Ngone51 ",
        "createdAt" : "2020-01-18T03:54:25Z",
        "updatedAt" : "2020-01-27T18:47:04Z",
        "lastEditedBy" : "0387304e-5de3-452a-8609-ecd82ff5bfbe",
        "tags" : [
        ]
      }
    ],
    "commit" : "3ea1175a4d7ccf71095a48945fa8946558b83c21",
    "line" : 63,
    "diffHunk" : "@@ -1,1 +617,621 @@        }\n\n        if (taskEnd.taskInfo.speculative) {\n          stageAttemptToSpeculativeTaskIndices.get(stageAttempt).foreach {_.remove{taskIndex}}\n          stageAttemptToNumSpeculativeTasks(stageAttempt) -= 1"
  },
  {
    "id" : "46763235-6654-4457-9dac-f9d94a731aec",
    "prId" : 27223,
    "prUrl" : "https://github.com/apache/spark/pull/27223#pullrequestreview-348725284",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2bf85cdb-dcd3-4011-800d-248f35e04e5b",
        "parentId" : null,
        "authorId" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "body" : "This could lead to wast of executors ?",
        "createdAt" : "2020-01-16T23:31:38Z",
        "updatedAt" : "2020-01-27T18:47:04Z",
        "lastEditedBy" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "tags" : [
        ]
      },
      {
        "id" : "75714705-5843-4e44-83e5-9cb19021573f",
        "parentId" : "2bf85cdb-dcd3-4011-800d-248f35e04e5b",
        "authorId" : "0387304e-5de3-452a-8609-ecd82ff5bfbe",
        "body" : "As specified in the comments, this is to satisfy the locality requirements of speculative tasks. Let's say we have 1 normal task and 1 speculative task (setting is 4 tasks/executor), in this case we should allocate 2 executors instead of 1.",
        "createdAt" : "2020-01-18T03:50:32Z",
        "updatedAt" : "2020-01-27T18:47:04Z",
        "lastEditedBy" : "0387304e-5de3-452a-8609-ecd82ff5bfbe",
        "tags" : [
        ]
      },
      {
        "id" : "756d808f-e27e-429d-a8f8-99f6cb36e14f",
        "parentId" : "2bf85cdb-dcd3-4011-800d-248f35e04e5b",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "But this isn't necessarily going to work, it doesn't guarantee that you get an executor on a different host.  I guess it gives it a chance at least. I think the comment needs to be more clear and state that the scheduler logic will only start a speculative task if its on a different host and that this may not work. I now see why that code was added below in the addExecutors, so thanks for pointing it out.\r\nThis also doesn't make sense if we already have more then 1 executor. The numRunningOrPendingTasks already includes speculative tasks, so lets say we have 1000 total tasks and putting 1 task per executor. We are already asking for 1000 executors, so we don't need to add 1 more here that won't be used. Really I think it should only be needed if maxNeeded == 1 and we have speculative tasks. Otherwise we have at least 2 executors so they should be able to run.\r\n\r\n I think we should file another followup jira to perhaps handle speculative tasks differently in cluster manager side",
        "createdAt" : "2020-01-23T21:42:23Z",
        "updatedAt" : "2020-01-27T18:47:04Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "e3cad486-0f46-451d-8d01-9e9bc7c6c225",
        "parentId" : "2bf85cdb-dcd3-4011-800d-248f35e04e5b",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "I typoed my example should be 1000 tasks with 2 tasks per executor, would results in 500 executors, we don't need to add one more",
        "createdAt" : "2020-01-23T23:04:30Z",
        "updatedAt" : "2020-01-27T18:47:04Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "524a07dd-26d5-4a02-972f-4ac59517f2c2",
        "parentId" : "2bf85cdb-dcd3-4011-800d-248f35e04e5b",
        "authorId" : "0387304e-5de3-452a-8609-ecd82ff5bfbe",
        "body" : "Let's use your 1000 tasks (with 2 tasks/executor) example. The dynamic allocation scheduler currently doesn't track the underlying distribution of tasks on different executors. So it's totally possible that we have 999 tasks running, then on one executor we will have a single task running. And we have a pending speculative task for the same task index as this single task, then the speculative task can't be launched in this case. Doing a \"+1\" is a very simple way to address this situation.",
        "createdAt" : "2020-01-24T08:56:02Z",
        "updatedAt" : "2020-01-27T18:47:04Z",
        "lastEditedBy" : "0387304e-5de3-452a-8609-ecd82ff5bfbe",
        "tags" : [
        ]
      },
      {
        "id" : "ba2443c7-d033-4f1f-8267-0277bc3888f4",
        "parentId" : "2bf85cdb-dcd3-4011-800d-248f35e04e5b",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "right but you can't guarantee that the +1 will get your speculative task or that the new executor won't end up on the same host. In our example case of all 999 already running it would give it a better chance if the 1 that finished happened to be on the same executor but what are the odds only 1 finished and it finished exactly on the same host?  Generally the reason you speculate is that you are assuming something on that host is wrong or slow so you want to run it somewhere else.  If that is the case that executor is likely to stay loaded up while other ones finish. If you have 500 executors already and your speculation configs are sane, the odds of another task finishing on a different executor are pretty good. If you don't have all your regular tasks finished, they could easily be put on the +1 host as well before your speculative has a chance to be scheduled on it and you could end up with only the 1 your counter part is on.  Since we don't have them integrated well its all just a guess/heuristic.  \r\n\r\nI'm ok with leaving this logic to +1 like you have it (with expanded comment) since that is what it was but at the same time it can be a huge waste or resources. If every application has speculative on and you have 1000 apps running, you could easily be wasting 1000's of containers when you already have enough resources to run within.\r\nWe could definitely come up with better heuristics that don't waste so much but can be done separately.\r\nAlso can you add in a debug log statement here when we do the +1",
        "createdAt" : "2020-01-24T15:25:22Z",
        "updatedAt" : "2020-01-27T18:47:04Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "2bccf3f7-45ba-42ad-8f24-5b69388812fc",
        "parentId" : "2bf85cdb-dcd3-4011-800d-248f35e04e5b",
        "authorId" : "0387304e-5de3-452a-8609-ecd82ff5bfbe",
        "body" : "Thanks! You have made excellent points: the +1 gives it a better chance but not a guarantee, and the slow hosts are more likely to be loaded up. I'm OK with doing a +1 only when maxNeeded = 1, will address it.",
        "createdAt" : "2020-01-26T05:08:36Z",
        "updatedAt" : "2020-01-27T18:47:04Z",
        "lastEditedBy" : "0387304e-5de3-452a-8609-ecd82ff5bfbe",
        "tags" : [
        ]
      },
      {
        "id" : "67d6fd04-f86b-4c37-a315-18b5bb78a60d",
        "parentId" : "2bf85cdb-dcd3-4011-800d-248f35e04e5b",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "another option would be to do the logic for speculative separately instead of lumping it with the normal tasks.  This might be an in between the 2 options. You could for instance calculate how many you need for regular tasks, then calculate separately how many executors for speculative, if  speculative doesn't add another executor on top of the normal tasks then add one.  That would help with us double counting.\r\nI'm fine with any of these as we can follow up to make it smarter in the future. ",
        "createdAt" : "2020-01-27T14:58:15Z",
        "updatedAt" : "2020-01-27T18:47:04Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "3ea1175a4d7ccf71095a48945fa8946558b83c21",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +270,274 @@      // If we have pending speculative tasks and only need a single executor, allocate one more\n      // to satisfy the locality requirements of speculation\n      maxNeeded + 1\n    } else {\n      maxNeeded"
  },
  {
    "id" : "72250c3d-f967-4385-9e7f-897198830ff1",
    "prId" : 27223,
    "prUrl" : "https://github.com/apache/spark/pull/27223#pullrequestreview-350438941",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "aae15cc4-f8ca-44c5-aedf-b3bffaa9b2d9",
        "parentId" : null,
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "one question I actually have here is what happens on a speculative task failure.  The scheduler will try to rerun but we may not be accounting for it if we have removed, I need to look at the scheduler logic a bit more.",
        "createdAt" : "2020-01-23T22:35:39Z",
        "updatedAt" : "2020-01-27T18:47:04Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "c8d88a2c-8bca-4643-91d8-103f9336240f",
        "parentId" : "aae15cc4-f8ca-44c5-aedf-b3bffaa9b2d9",
        "authorId" : "0387304e-5de3-452a-8609-ecd82ff5bfbe",
        "body" : "When a speculative task fails while the original task is running, it will not be resubmitted (because we have one copy running, see https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala#L289-L293), but a new speculative task will be launched later in a future speculation check cycle. The code works correctly in this case, as we update both `stageAttemptToSpeculativeTaskIndices` and `stageAttemptToNumSpeculativeTasks`, so this failed speculative task will just behave as it never existed. I can add this to my test case.\r\n\r\n@tgravescs, @jiangxb1987: Note that the current design (with or without this PR) will launch more-than-needed executors when **the original task fails when the speculative task is running**. In this case, the original task will not be resubmitted (same as above because we have one copy running). Speculation check will later launch one more speculative task on the already running speculative task. In this case, **we have 2 speculative tasks for the same task index**. In this case, we will still launch more than needed executors. This is a limitation of the current design unfortunately, as the existing data structures assume we have at most one speculative task running for a task index.",
        "createdAt" : "2020-01-24T09:24:07Z",
        "updatedAt" : "2020-01-27T18:47:04Z",
        "lastEditedBy" : "0387304e-5de3-452a-8609-ecd82ff5bfbe",
        "tags" : [
        ]
      },
      {
        "id" : "b1c5b6ea-d960-4231-b7cf-0966ee7ba4c2",
        "parentId" : "aae15cc4-f8ca-44c5-aedf-b3bffaa9b2d9",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "thanks for the explanation, its been a while since I looked at the speculative handling in the scheduler.   I think the failed task case with speculative is ok. I think the problem is the scheduler isn't  tracking those task as speculative or not after the initial launch.  I would rather error on the side of asking for to many and the failure case should be less frequent.  that is a lot more complex case along with making the dynamic allocation manager smarter about it.  If you can file that other jira I mentioned above to follow up, if you could put those details in there as well that would be great.",
        "createdAt" : "2020-01-24T14:54:02Z",
        "updatedAt" : "2020-01-27T18:47:04Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "1c4f719e-e25a-422e-b5f6-09ec6f42234c",
        "parentId" : "aae15cc4-f8ca-44c5-aedf-b3bffaa9b2d9",
        "authorId" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "body" : "sorry I don't follow the logic. According to `TSM.dequeueTask()`:\r\n>     // Tries to schedule a regular task first; if it returns None, then schedules\r\n>     // a speculative task\r\nIt would first try to schedule a regular task (which would return None in this case), then it would try to schedule a speculative task. Since we have only the original task running, it shall schedule one speculative task in this case. Am I missing something? I also searched the test case but didn't locate the place describing the behavior.",
        "createdAt" : "2020-01-29T00:39:56Z",
        "updatedAt" : "2020-01-29T00:39:57Z",
        "lastEditedBy" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "tags" : [
        ]
      },
      {
        "id" : "8ee97b69-c58d-4f18-82b0-e5ca4e7cf4b7",
        "parentId" : "aae15cc4-f8ca-44c5-aedf-b3bffaa9b2d9",
        "authorId" : "0387304e-5de3-452a-8609-ecd82ff5bfbe",
        "body" : "> sorry I don't follow the logic. According to `TSM.dequeueTask()`:\r\n> \r\n> > ```\r\n> > // Tries to schedule a regular task first; if it returns None, then schedules\r\n> > // a speculative task\r\n> > ```\r\n> \r\n> It would first try to schedule a regular task (which would return None in this case), then it would try to schedule a speculative task. Since we have only the original task running, it shall schedule one speculative task in this case. Am I missing something? I also searched the test case but didn't locate the place describing the behavior.\r\n\r\n@jiangxb1987 Let's make sure we are on the same page. I assume you are talking about the case on \"speculation task failure\". As I have explained above, when a speculative task fails, `handleFailedTask` will be invoked, and we will have `addPendingTask(index)` at L894. This will add a pending task for the failed task index (with `speculatable` default to false).\r\n\r\nThen from `TSM.dequeueTask()`, we have a call chain of `dequeueTaskHelper` -> `dequeueTaskFromList`, see https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala#L289-L293: as we already have one copy running (the original task), the enqueued pending task will not be resubmitted here. Instead, in a future speculation cycle, we will see the original task running too slow and will try to speculate again on the original task from there.\r\n\r\nFrom my understanding, \r\n> // Tries to schedule a regular task first; if it returns None, then schedules\r\n> // a speculative task\r\n\r\nThis is how we are prioritizing speculative vs. non-speculative tasks on receiving an executor. This is orthogonal to what we are discussing here.",
        "createdAt" : "2020-01-29T04:47:56Z",
        "updatedAt" : "2020-01-29T04:49:37Z",
        "lastEditedBy" : "0387304e-5de3-452a-8609-ecd82ff5bfbe",
        "tags" : [
        ]
      },
      {
        "id" : "b7295f48-a523-4fa3-b7e6-8e2b9cc65d1e",
        "parentId" : "aae15cc4-f8ca-44c5-aedf-b3bffaa9b2d9",
        "authorId" : "0387304e-5de3-452a-8609-ecd82ff5bfbe",
        "body" : "I guess you might be questioning on the code itself. As I have explained above, a failed speculative task will not be resubmitted, but instead a new speculative task will be launched in a future speculation cycle.",
        "createdAt" : "2020-01-29T05:08:32Z",
        "updatedAt" : "2020-01-29T05:08:32Z",
        "lastEditedBy" : "0387304e-5de3-452a-8609-ecd82ff5bfbe",
        "tags" : [
        ]
      },
      {
        "id" : "95e5b111-197a-47ba-b25e-69fc7657f8fb",
        "parentId" : "aae15cc4-f8ca-44c5-aedf-b3bffaa9b2d9",
        "authorId" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "body" : "Yes, there seems to be a bug at L894, a failed speculative task should be add to the pendingSpeculatableTasks. But as you explained, your fix is correct here.",
        "createdAt" : "2020-01-29T21:32:22Z",
        "updatedAt" : "2020-01-29T21:32:23Z",
        "lastEditedBy" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "3ea1175a4d7ccf71095a48945fa8946558b83c21",
    "line" : 65,
    "diffHunk" : "@@ -1,1 +619,623 @@        if (taskEnd.taskInfo.speculative) {\n          stageAttemptToSpeculativeTaskIndices.get(stageAttempt).foreach {_.remove{taskIndex}}\n          stageAttemptToNumSpeculativeTasks(stageAttempt) -= 1\n        }\n"
  },
  {
    "id" : "543b2da2-564c-4518-8bb6-a451314dc3f3",
    "prId" : 27223,
    "prUrl" : "https://github.com/apache/spark/pull/27223#pullrequestreview-352028745",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4104c0fa-9a19-4be1-82b5-4794cd7ff8dd",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Previously, we always call `onSchedulerBacklogged` when `totalPendingTasks() == 0` for both speculative and non-speculative tasks. But according to this PR, we should not do this for speculative tasks now. Right? @linzebing ",
        "createdAt" : "2020-02-03T03:45:38Z",
        "updatedAt" : "2020-02-03T03:45:39Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "ca44fae8-5bcd-4f65-bd48-039d7f67b5c0",
        "parentId" : "4104c0fa-9a19-4be1-82b5-4794cd7ff8dd",
        "authorId" : "0387304e-5de3-452a-8609-ecd82ff5bfbe",
        "body" : "As explained in the comment, if a task is intentionally killed, we should not mark the scheduler as backlogged.\r\n\r\nIf a speculative task fails, while it will not be directly resubmitted, a new speculative task will be launched in next speculation cycle. So it's OK for us to mark the scheduler as backlogged in this case.",
        "createdAt" : "2020-02-03T05:11:34Z",
        "updatedAt" : "2020-02-03T05:11:34Z",
        "lastEditedBy" : "0387304e-5de3-452a-8609-ecd82ff5bfbe",
        "tags" : [
        ]
      },
      {
        "id" : "aa8cc782-4281-4e30-a40a-300a773d1335",
        "parentId" : "4104c0fa-9a19-4be1-82b5-4794cd7ff8dd",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "> If a speculative task fails, while it will not be directly resubmitted, a new speculative task will be launched in next speculation cycle. So it's OK for us to mark the scheduler as backlogged in this case.\r\n\r\nHmm...but the new speculative task may not be launched if the normal task finish. And even if it launched, `ExecutorAllocationManager` could still handle it by receiving `SparkListenerSpeculativeTaskSubmitted`. Though, calling `onSchedulerBacklogged` may could reserve executor resource more early to reduce the delay. Fine!",
        "createdAt" : "2020-02-03T05:27:33Z",
        "updatedAt" : "2020-02-03T05:27:33Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "acbf8c37-a405-4cba-a946-a85048f109f7",
        "parentId" : "4104c0fa-9a19-4be1-82b5-4794cd7ff8dd",
        "authorId" : "0387304e-5de3-452a-8609-ecd82ff5bfbe",
        "body" : "The speculation interval is default to 100ms, so a new speculative task will launched almost instantly.",
        "createdAt" : "2020-02-03T05:31:19Z",
        "updatedAt" : "2020-02-03T05:31:20Z",
        "lastEditedBy" : "0387304e-5de3-452a-8609-ecd82ff5bfbe",
        "tags" : [
        ]
      },
      {
        "id" : "87e966bb-1841-42a9-83b4-77916ce26857",
        "parentId" : "4104c0fa-9a19-4be1-82b5-4794cd7ff8dd",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "make sense.",
        "createdAt" : "2020-02-03T05:38:41Z",
        "updatedAt" : "2020-02-03T05:38:41Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "3ea1175a4d7ccf71095a48945fa8946558b83c21",
    "line" : 76,
    "diffHunk" : "@@ -1,1 +630,634 @@              // mark the scheduler as backlogged again if it's not already marked as such\n              // (SPARK-8366)\n              allocationManager.onSchedulerBacklogged()\n            }\n            if (!taskEnd.taskInfo.speculative) {"
  },
  {
    "id" : "1fe4dcd8-d02c-4dd0-a49d-69e3a724a62f",
    "prId" : 26058,
    "prUrl" : "https://github.com/apache/spark/pull/26058#pullrequestreview-301026326",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ff068ffe-887d-4ed8-a571-7c08fae5813e",
        "parentId" : null,
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "System.nanoTime() is not monotonic across CPU cores. Are you sure that this value of `clock.nanoTime()` is not compared to `System.nanoTime()` from another thread? For instance, from `onSchedulerBacklogged()`.",
        "createdAt" : "2019-10-08T18:57:50Z",
        "updatedAt" : "2019-10-14T21:01:39Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "908e4853-2a1f-498c-9a1f-e2cda97c5843",
        "parentId" : "ff068ffe-887d-4ed8-a571-7c08fae5813e",
        "authorId" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "body" : "Are you sure about that?\r\n\r\nThis is what the javadocs say:\r\n\r\n    The same origin is used by all invocations of this method in an instance of a Java virtual\r\n    machine; other virtual machine instances are likely to use a different origin. \r\n\r\n(Comment about \"other virtual machines\" aside.)\r\n\r\nThat seems to be corroborated by other sources, too (e.g. https://stackoverflow.com/questions/510462/is-system-nanotime-completely-useless). Seems in a long gone past there might have been issues, but not today.",
        "createdAt" : "2019-10-08T19:14:32Z",
        "updatedAt" : "2019-10-14T21:01:39Z",
        "lastEditedBy" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "tags" : [
        ]
      },
      {
        "id" : "362c79d9-8667-415c-98c0-5fd5906eda95",
        "parentId" : "ff068ffe-887d-4ed8-a571-7c08fae5813e",
        "authorId" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "body" : "Also, just wanted to point out that it's ok if `nanoTime` is not completely monotonic, as long as it's better than `getTimeMillis()`. After all, this code was working with a non-monotonic clock before.\r\n\r\nThe only problem is if different threads can have wildly different views of what `nanoTime` returns, so that comparing the values makes no sense. I don't believe that to be the case.",
        "createdAt" : "2019-10-08T20:01:57Z",
        "updatedAt" : "2019-10-14T21:01:39Z",
        "lastEditedBy" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "tags" : [
        ]
      },
      {
        "id" : "f1d60a33-e6ea-402c-9917-04e1f2c283f7",
        "parentId" : "ff068ffe-887d-4ed8-a571-7c08fae5813e",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "`System.nanoTime()` can use `clock_gettime(CLOCK_MONOTONIC, ...)` on Linux: http://hg.openjdk.java.net/jdk8/jdk8/hotspot/file/tip/src/os/linux/vm/os_linux.cpp#l1453\r\n`CLOCK_MONOTONIC` is based on the Time Stamp Counter (https://en.wikipedia.org/wiki/Time_Stamp_Counter). Its synchronization across CPU cores depends on CPU architecture, for example: _\"AMD processors up to the K8 core always incremented the time-stamp counter every clock cycle.[6] Thus, power management features were able to change the number of increments per second, and the values could get out of sync between different cores or processors in the same system.\"_",
        "createdAt" : "2019-10-08T20:04:23Z",
        "updatedAt" : "2019-10-14T21:01:39Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "24daa430-9041-4ce1-81b9-3b9033f159c4",
        "parentId" : "ff068ffe-887d-4ed8-a571-7c08fae5813e",
        "authorId" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "body" : "That is interesting, but according to this:\r\n\r\nhttp://btorpey.github.io/blog/2014/02/18/clock-sources-in-linux/\r\n\r\n`CLOCK_MONOTONIC` uses RDTSC (or RDTSCP when available), which solves that problem. The preferred one (RDTSCP) seems to be available in all modern CPUs. So, `CLOCK_MONOTONIC` actually fulfills the requirements of the `nanoTime()` API (that the source of time is consistent).\r\n\r\n(The AMD K8 mentioned in your comment, which came out more than 10 years ago, is where RDTSCP was added.)",
        "createdAt" : "2019-10-08T20:21:51Z",
        "updatedAt" : "2019-10-14T21:01:39Z",
        "lastEditedBy" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "tags" : [
        ]
      },
      {
        "id" : "863f8d20-2060-4c7d-a363-28f10d734e52",
        "parentId" : "ff068ffe-887d-4ed8-a571-7c08fae5813e",
        "authorId" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "body" : "Probably worth to link directly to the following also (which is linked from the article above)\r\n\r\nhttps://stackoverflow.com/questions/10921210/cpu-tsc-fetch-operation-especially-in-multicore-multi-processor-environment",
        "createdAt" : "2019-10-08T20:32:18Z",
        "updatedAt" : "2019-10-14T21:01:39Z",
        "lastEditedBy" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "tags" : [
        ]
      },
      {
        "id" : "64f2dd98-a7eb-41c7-a5ff-0ccae09efd59",
        "parentId" : "ff068ffe-887d-4ed8-a571-7c08fae5813e",
        "authorId" : "e913f9af-103d-40d4-976e-9aa8e12e7211",
        "body" : "this is a good discussion -- its really easy to get confused about the details here, especially as some googling can take you to old info.  Worth including a bit of this on the doc for `SystemClock.nanoTime()`?  Maybe at least just noting that this is dependable across cores as long as its Java7+.",
        "createdAt" : "2019-10-11T18:13:52Z",
        "updatedAt" : "2019-10-14T21:01:39Z",
        "lastEditedBy" : "e913f9af-103d-40d4-976e-9aa8e12e7211",
        "tags" : [
        ]
      },
      {
        "id" : "a404f429-1232-4c72-ab3d-06e9db7e3817",
        "parentId" : "ff068ffe-887d-4ed8-a571-7c08fae5813e",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "+1 for @squito 's comment. Also, could you summarize the above into the PR description a little bit please, @vanzin ?",
        "createdAt" : "2019-10-12T23:39:14Z",
        "updatedAt" : "2019-10-14T21:01:39Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "da6f156777a8e1572651b925c2884bb3688fd66d",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +289,293 @@\n    // Update executor target number only after initializing flag is unset\n    updateAndSyncNumExecutorsTarget(clock.nanoTime())\n    if (executorIdsToBeRemoved.nonEmpty) {\n      removeExecutors(executorIdsToBeRemoved)"
  },
  {
    "id" : "9ff036af-3e27-44d8-9e9d-7aca8f4e4776",
    "prId" : 24497,
    "prUrl" : "https://github.com/apache/spark/pull/24497#pullrequestreview-252912331",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dd081352-fbe6-47c3-bd92-7df1b7b847d4",
        "parentId" : null,
        "authorId" : "e913f9af-103d-40d4-976e-9aa8e12e7211",
        "body" : "I think its worth adding a comment here along the lines of \"do NOT remove stageAttempt from stageAttemptToNumRunningTasks, because the attempt may still have running tasks, even after another attempt for the stage is submitted.\"",
        "createdAt" : "2019-06-21T16:15:05Z",
        "updatedAt" : "2019-06-24T03:38:58Z",
        "lastEditedBy" : "e913f9af-103d-40d4-976e-9aa8e12e7211",
        "tags" : [
        ]
      }
    ],
    "commit" : "b91965098c3c2b71d5adda9d501ee46cf14831ed",
    "line" : 85,
    "diffHunk" : "@@ -1,1 +560,564 @@        // because the attempt may still have running tasks,\n        // even after another attempt for the stage is submitted.\n        stageAttemptToNumTasks -= stageAttempt\n        stageAttemptToNumSpeculativeTasks -= stageAttempt\n        stageAttemptToTaskIndices -= stageAttempt"
  },
  {
    "id" : "fa71e7f4-f329-45f6-b900-b13173a09ed6",
    "prId" : 24497,
    "prUrl" : "https://github.com/apache/spark/pull/24497#pullrequestreview-253227049",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "aeaccaf4-6dce-4e35-bb60-09fc7ccb6e20",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "I think we should not take tasks from previous stage attempts, which are already zombie,  into account, right ?",
        "createdAt" : "2019-06-24T06:22:05Z",
        "updatedAt" : "2019-06-24T06:22:05Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "9b9f87e8-d619-4dea-989a-52f287d50206",
        "parentId" : "aeaccaf4-6dce-4e35-bb60-09fc7ccb6e20",
        "authorId" : "8de20372-3c43-4f6b-838b-45ccb852019b",
        "body" : "In ```onStageCompleted``` processing, ```stageAttemptToNumTasks```, ```stageAttemptToTaskIndices``` removes the zombie stage, so pendingTasks does not calculate the zombie stage's task.",
        "createdAt" : "2019-06-24T06:46:09Z",
        "updatedAt" : "2019-06-24T06:46:09Z",
        "lastEditedBy" : "8de20372-3c43-4f6b-838b-45ccb852019b",
        "tags" : [
        ]
      },
      {
        "id" : "6cb086e1-a5e8-4f2a-9ae1-d2a6d5bdf1b0",
        "parentId" : "aeaccaf4-6dce-4e35-bb60-09fc7ccb6e20",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "I see.",
        "createdAt" : "2019-06-24T06:57:19Z",
        "updatedAt" : "2019-06-24T06:57:19Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "b91965098c3c2b71d5adda9d501ee46cf14831ed",
    "line" : 187,
    "diffHunk" : "@@ -1,1 +647,651 @@    def pendingTasks(): Int = {\n      stageAttemptToNumTasks.map { case (stageAttempt, numTasks) =>\n        numTasks - stageAttemptToTaskIndices.get(stageAttempt).map(_.size).getOrElse(0)\n      }.sum\n    }"
  },
  {
    "id" : "9dc0b3c8-bc95-43dd-b238-a3c6176c6a49",
    "prId" : 24497,
    "prUrl" : "https://github.com/apache/spark/pull/24497#pullrequestreview-253216461",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "88a7f0f6-ed6d-4951-800f-c58abc78b6b4",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "ditto.",
        "createdAt" : "2019-06-24T06:22:19Z",
        "updatedAt" : "2019-06-24T06:22:19Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "b91965098c3c2b71d5adda9d501ee46cf14831ed",
    "line" : 195,
    "diffHunk" : "@@ -1,1 +653,657 @@    def pendingSpeculativeTasks(): Int = {\n      stageAttemptToNumSpeculativeTasks.map { case (stageAttempt, numTasks) =>\n        numTasks - stageAttemptToSpeculativeTaskIndices.get(stageAttempt).map(_.size).getOrElse(0)\n      }.sum\n    }"
  }
]