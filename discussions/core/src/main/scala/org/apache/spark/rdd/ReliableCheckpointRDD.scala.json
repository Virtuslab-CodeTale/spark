[
  {
    "id" : "d722f9f9-52f1-442e-8035-242cd9faff17",
    "prId" : 31517,
    "prUrl" : "https://github.com/apache/spark/pull/31517#pullrequestreview-716421452",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e5b5cc74-560f-4762-877c-aa2a0a0e1c32",
        "parentId" : null,
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "nit: why can't we just use `builder.build[Partition, Seq[String]](getPartitionBlockLocations)`?",
        "createdAt" : "2021-07-26T17:58:10Z",
        "updatedAt" : "2021-07-26T18:12:19Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "127cf5ac-f3ae-4477-880e-d21419f900af",
        "parentId" : "e5b5cc74-560f-4762-877c-aa2a0a0e1c32",
        "authorId" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "body" : "1. Try to use `builder.build[Partition, Seq[String]](getPartitionBlockLocations)`\r\nThe compile errors as follows:\r\n```\r\n[ERROR] /spark-mine/core/src/main/scala/org/apache/spark/rdd/ReliableCheckpointRDD.scala:92: missing argument list for method getPartitionBlockLocations in class ReliableCheckpointRDD\r\nUnapplied methods are only converted to functions when a function type is expected.\r\nYou can make this conversion explicit by writing `getPartitionBlockLocations _` or `getPartitionBlockLocations(_)` instead of `getPartitionBlockLocations`.\r\n```\r\n\r\n2. Then try to use  `getPartitionBlockLocations(_)` and `getPartitionBlockLocations _`\r\n\r\nThe compile errors as follows:\r\n```\r\n[ERROR] /spark-mine/core/src/main/scala/org/apache/spark/rdd/ReliableCheckpointRDD.scala:92: overloaded method value build with alternatives:\r\n  (x$1: com.github.benmanes.caffeine.cache.CacheLoader[_ >: org.apache.spark.Partition, Seq[String]])com.github.benmanes.caffeine.cache.LoadingCache[org.apache.spark.Partition,Seq[String]] <and>\r\n  ()com.github.benmanes.caffeine.cache.Cache[org.apache.spark.Partition,Seq[String]]\r\n cannot be applied to (org.apache.spark.Partition => Seq[String])\r\n```",
        "createdAt" : "2021-07-27T03:16:08Z",
        "updatedAt" : "2021-07-27T03:16:09Z",
        "lastEditedBy" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "tags" : [
        ]
      },
      {
        "id" : "5b3fda61-e4da-4a49-8d12-8d82a260b2f1",
        "parentId" : "e5b5cc74-560f-4762-877c-aa2a0a0e1c32",
        "authorId" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "body" : "Then try to use `(partition: Partition) => getPartitionBlockLocations(partition)`, the error message same as `getPartitionBlockLocations(_) `",
        "createdAt" : "2021-07-27T03:18:44Z",
        "updatedAt" : "2021-07-27T03:18:44Z",
        "lastEditedBy" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "tags" : [
        ]
      },
      {
        "id" : "b55690fc-c1a6-4895-9e6f-a8cd05265621",
        "parentId" : "e5b5cc74-560f-4762-877c-aa2a0a0e1c32",
        "authorId" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "body" : "7b360a7 revert this change",
        "createdAt" : "2021-07-27T03:30:00Z",
        "updatedAt" : "2021-07-27T03:30:00Z",
        "lastEditedBy" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "tags" : [
        ]
      },
      {
        "id" : "8b7d8861-a513-4358-84f2-a803dd775863",
        "parentId" : "e5b5cc74-560f-4762-877c-aa2a0a0e1c32",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Ah thanks for trying it out - seems there are some issue in Java/Scala interop there.",
        "createdAt" : "2021-07-27T22:10:47Z",
        "updatedAt" : "2021-07-27T22:10:48Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      }
    ],
    "commit" : "81f863ff67d0236f050f4a24e9470c2b1bb7aaff",
    "line" : 33,
    "diffHunk" : "@@ -1,1 +96,100 @@      }\n    }\n    builder.build[Partition, Seq[String]](loader)\n  }\n"
  },
  {
    "id" : "b7c7b323-c5ae-4b11-bb70-5f6413b6b395",
    "prId" : 28289,
    "prUrl" : "https://github.com/apache/spark/pull/28289#pullrequestreview-398043296",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b56cd8c7-273d-46c9-8dfc-9d69b3b0145c",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Ya. It looks simpler.",
        "createdAt" : "2020-04-22T07:46:15Z",
        "updatedAt" : "2020-04-22T07:46:16Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "41657892-5af1-4bb5-9b7c-3e382c4e369f",
        "parentId" : "b56cd8c7-273d-46c9-8dfc-9d69b3b0145c",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "BTW, do we need `-attempt-`?",
        "createdAt" : "2020-04-22T07:53:29Z",
        "updatedAt" : "2020-04-22T07:53:30Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "355e2d61-f000-49f3-a716-fe68ebea8ac6",
        "parentId" : "b56cd8c7-273d-46c9-8dfc-9d69b3b0145c",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "```\r\nprivate[spark] class TaskDescription(\r\n    val taskId: Long,\r\n    val attemptNumber: Int,\r\n...\r\n```\r\nI believe `taskAttempId` is some kind of historic name in `TaskContext`.",
        "createdAt" : "2020-04-22T10:12:25Z",
        "updatedAt" : "2020-04-22T10:12:26Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "59e99f6d7aad7c464c68fbb6e568f9347f6774ac",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +200,204 @@    val finalOutputName = ReliableCheckpointRDD.checkpointFileName(ctx.partitionId())\n    val finalOutputPath = new Path(outputDir, finalOutputName)\n    val tempOutputPath = new Path(outputDir, s\".$finalOutputName-attempt-${ctx.taskAttemptId()}\")\n\n    val bufferSize = env.conf.get(BUFFER_SIZE)"
  }
]