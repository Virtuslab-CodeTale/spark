[
  {
    "id" : "d722f9f9-52f1-442e-8035-242cd9faff17",
    "prId" : 31517,
    "prUrl" : "https://github.com/apache/spark/pull/31517#pullrequestreview-716421452",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e5b5cc74-560f-4762-877c-aa2a0a0e1c32",
        "parentId" : null,
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "nit: why can't we just use `builder.build[Partition, Seq[String]](getPartitionBlockLocations)`?",
        "createdAt" : "2021-07-26T17:58:10Z",
        "updatedAt" : "2021-07-26T18:12:19Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "127cf5ac-f3ae-4477-880e-d21419f900af",
        "parentId" : "e5b5cc74-560f-4762-877c-aa2a0a0e1c32",
        "authorId" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "body" : "1. Try to use `builder.build[Partition, Seq[String]](getPartitionBlockLocations)`\r\nThe compile errors as follows:\r\n```\r\n[ERROR] /spark-mine/core/src/main/scala/org/apache/spark/rdd/ReliableCheckpointRDD.scala:92: missing argument list for method getPartitionBlockLocations in class ReliableCheckpointRDD\r\nUnapplied methods are only converted to functions when a function type is expected.\r\nYou can make this conversion explicit by writing `getPartitionBlockLocations _` or `getPartitionBlockLocations(_)` instead of `getPartitionBlockLocations`.\r\n```\r\n\r\n2. Then try to use  `getPartitionBlockLocations(_)` and `getPartitionBlockLocations _`\r\n\r\nThe compile errors as follows:\r\n```\r\n[ERROR] /spark-mine/core/src/main/scala/org/apache/spark/rdd/ReliableCheckpointRDD.scala:92: overloaded method value build with alternatives:\r\n  (x$1: com.github.benmanes.caffeine.cache.CacheLoader[_ >: org.apache.spark.Partition, Seq[String]])com.github.benmanes.caffeine.cache.LoadingCache[org.apache.spark.Partition,Seq[String]] <and>\r\n  ()com.github.benmanes.caffeine.cache.Cache[org.apache.spark.Partition,Seq[String]]\r\n cannot be applied to (org.apache.spark.Partition => Seq[String])\r\n```",
        "createdAt" : "2021-07-27T03:16:08Z",
        "updatedAt" : "2021-07-27T03:16:09Z",
        "lastEditedBy" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "tags" : [
        ]
      },
      {
        "id" : "5b3fda61-e4da-4a49-8d12-8d82a260b2f1",
        "parentId" : "e5b5cc74-560f-4762-877c-aa2a0a0e1c32",
        "authorId" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "body" : "Then try to use `(partition: Partition) => getPartitionBlockLocations(partition)`, the error message same as `getPartitionBlockLocations(_) `",
        "createdAt" : "2021-07-27T03:18:44Z",
        "updatedAt" : "2021-07-27T03:18:44Z",
        "lastEditedBy" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "tags" : [
        ]
      },
      {
        "id" : "b55690fc-c1a6-4895-9e6f-a8cd05265621",
        "parentId" : "e5b5cc74-560f-4762-877c-aa2a0a0e1c32",
        "authorId" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "body" : "7b360a7 revert this change",
        "createdAt" : "2021-07-27T03:30:00Z",
        "updatedAt" : "2021-07-27T03:30:00Z",
        "lastEditedBy" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "tags" : [
        ]
      },
      {
        "id" : "8b7d8861-a513-4358-84f2-a803dd775863",
        "parentId" : "e5b5cc74-560f-4762-877c-aa2a0a0e1c32",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Ah thanks for trying it out - seems there are some issue in Java/Scala interop there.",
        "createdAt" : "2021-07-27T22:10:47Z",
        "updatedAt" : "2021-07-27T22:10:48Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      }
    ],
    "commit" : "81f863ff67d0236f050f4a24e9470c2b1bb7aaff",
    "line" : 33,
    "diffHunk" : "@@ -1,1 +96,100 @@      }\n    }\n    builder.build[Partition, Seq[String]](loader)\n  }\n"
  },
  {
    "id" : "b7c7b323-c5ae-4b11-bb70-5f6413b6b395",
    "prId" : 28289,
    "prUrl" : "https://github.com/apache/spark/pull/28289#pullrequestreview-398043296",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b56cd8c7-273d-46c9-8dfc-9d69b3b0145c",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Ya. It looks simpler.",
        "createdAt" : "2020-04-22T07:46:15Z",
        "updatedAt" : "2020-04-22T07:46:16Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "41657892-5af1-4bb5-9b7c-3e382c4e369f",
        "parentId" : "b56cd8c7-273d-46c9-8dfc-9d69b3b0145c",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "BTW, do we need `-attempt-`?",
        "createdAt" : "2020-04-22T07:53:29Z",
        "updatedAt" : "2020-04-22T07:53:30Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "355e2d61-f000-49f3-a716-fe68ebea8ac6",
        "parentId" : "b56cd8c7-273d-46c9-8dfc-9d69b3b0145c",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "```\r\nprivate[spark] class TaskDescription(\r\n    val taskId: Long,\r\n    val attemptNumber: Int,\r\n...\r\n```\r\nI believe `taskAttempId` is some kind of historic name in `TaskContext`.",
        "createdAt" : "2020-04-22T10:12:25Z",
        "updatedAt" : "2020-04-22T10:12:26Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "59e99f6d7aad7c464c68fbb6e568f9347f6774ac",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +200,204 @@    val finalOutputName = ReliableCheckpointRDD.checkpointFileName(ctx.partitionId())\n    val finalOutputPath = new Path(outputDir, finalOutputName)\n    val tempOutputPath = new Path(outputDir, s\".$finalOutputName-attempt-${ctx.taskAttemptId()}\")\n\n    val bufferSize = env.conf.get(BUFFER_SIZE)"
  },
  {
    "id" : "53079c8a-5706-46d1-9bf1-f620143eb870",
    "prId" : 28255,
    "prUrl" : "https://github.com/apache/spark/pull/28255#pullrequestreview-395961691",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2f48cb6d-cb8f-46de-b9cc-7d2c333c132e",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Just a question. This is `Info` because the job is not failed?",
        "createdAt" : "2020-04-18T22:56:53Z",
        "updatedAt" : "2020-04-19T05:16:37Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "332ba07a-bc0b-4055-b9bd-11a96ebb78e5",
        "parentId" : "2f48cb6d-cb8f-46de-b9cc-7d2c333c132e",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "I think either logWarning or logInfo should be fine. After adding stage attempt, even we fail to delete previous temporary output path, we won't see file already existing exception.",
        "createdAt" : "2020-04-18T23:57:15Z",
        "updatedAt" : "2020-04-19T05:16:37Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "f402180974408bf3a22e80081eeed57793ad4830",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +224,228 @@      val deleted = fs.delete(tempOutputPath, false)\n      if (!deleted) {\n        logInfo(s\"Failed to delete tempOutputPath $tempOutputPath.\")\n      }\n    }, finallyBlock = {"
  },
  {
    "id" : "4e76ab09-140a-4545-9068-29044ed686b8",
    "prId" : 28255,
    "prUrl" : "https://github.com/apache/spark/pull/28255#pullrequestreview-397833855",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5d4da601-85a6-4329-9dce-2dd1f46fe367",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "`stageAttemptNumber` isn't unique among stages and `attemptNumber` isn't unique among tasks within the same stage. So it seems that this could still lead to the file name conflict. e.g. task 0.0 from stage 0.0 could conflict with task 1.0 from stage 1.0  (different stage) and task 1.0 from stage 1.0 could conflict with task 2.0 from stage 1.0 (same stage).\r\n\r\nI think the unique file format should be `...-stageId-stageAttemptId-taskId-taskAttemptId-...`.",
        "createdAt" : "2020-04-20T02:48:48Z",
        "updatedAt" : "2020-04-20T02:48:53Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "4b2e28c9-651c-48db-97b0-bae3983d166f",
        "parentId" : "5d4da601-85a6-4329-9dce-2dd1f46fe367",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "A RDD can across stages?\r\n",
        "createdAt" : "2020-04-20T02:52:26Z",
        "updatedAt" : "2020-04-20T02:52:26Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "d1315088-8055-4e8f-97ba-f872cba84c01",
        "parentId" : "5d4da601-85a6-4329-9dce-2dd1f46fe367",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "No, but all attempts Id starts from 0.",
        "createdAt" : "2020-04-20T02:55:51Z",
        "updatedAt" : "2020-04-20T02:55:51Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "239d7f1f-5466-4637-8503-a1877dedaebd",
        "parentId" : "5d4da601-85a6-4329-9dce-2dd1f46fe367",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Oh, I see we suffix the checkpoint path with rdd id...",
        "createdAt" : "2020-04-20T02:58:29Z",
        "updatedAt" : "2020-04-20T02:58:29Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "55f57c4b-fcfc-43d5-bcc3-330ec2273d20",
        "parentId" : "5d4da601-85a6-4329-9dce-2dd1f46fe367",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "If we just want a unique file name, can we use the task id? It's unique within the Spark application.",
        "createdAt" : "2020-04-22T02:18:53Z",
        "updatedAt" : "2020-04-22T02:18:53Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "6ad36b1c-6af0-4279-99d2-cdd9edac6978",
        "parentId" : "5d4da601-85a6-4329-9dce-2dd1f46fe367",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Do you mean `taskAttemptId`?",
        "createdAt" : "2020-04-22T02:46:27Z",
        "updatedAt" : "2020-04-22T02:46:28Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "a779a180-340f-4a39-b7e4-ebaa41d526ea",
        "parentId" : "5d4da601-85a6-4329-9dce-2dd1f46fe367",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "Yes, `taskAttemptId`, we also use it in the shuffle map file for making the file name unique. https://github.com/apache/spark/pull/24892#issuecomment-526215837",
        "createdAt" : "2020-04-22T03:12:09Z",
        "updatedAt" : "2020-04-22T03:12:10Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      },
      {
        "id" : "67f46c2c-0179-49e6-b9ae-88a2d22f8986",
        "parentId" : "5d4da601-85a6-4329-9dce-2dd1f46fe367",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "OK. Let me create a follow-up for it. Thanks.",
        "createdAt" : "2020-04-22T04:24:15Z",
        "updatedAt" : "2020-04-22T04:24:15Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "f402180974408bf3a22e80081eeed57793ad4830",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +201,205 @@    val finalOutputPath = new Path(outputDir, finalOutputName)\n    val tempOutputPath = new Path(outputDir,\n      s\".$finalOutputName-attempt-${ctx.stageAttemptNumber()}-${ctx.attemptNumber()}\")\n\n    val bufferSize = env.conf.get(BUFFER_SIZE)"
  },
  {
    "id" : "89ddf983-b156-4b28-bffd-f422eb289175",
    "prId" : 25856,
    "prUrl" : "https://github.com/apache/spark/pull/25856#pullrequestreview-294477672",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8c28e885-82af-4a6f-a592-90545b9aabfb",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "How big can this get -- would it be significant?",
        "createdAt" : "2019-09-27T17:49:51Z",
        "updatedAt" : "2019-10-14T23:01:51Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "68770807-9864-4717-a02f-c7ec1908c5bc",
        "parentId" : "8c28e885-82af-4a6f-a592-90545b9aabfb",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "I have given an estimate in previous comment. For one partition, we will cache the number of string (hostnames) as same as the number of relicas on DFS. I think it is not significant.",
        "createdAt" : "2019-09-27T17:56:34Z",
        "updatedAt" : "2019-10-14T23:01:51Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "4c9118b3d1dcf0c589ccf2326e7169d20497c721",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +86,90 @@\n  // Cache of preferred locations of checkpointed files.\n  @transient private[spark] lazy val cachedPreferredLocations = CacheBuilder.newBuilder()\n    .expireAfterWrite(\n      SparkEnv.get.conf.get(CACHE_CHECKPOINT_PREFERRED_LOCS_EXPIRE_TIME).get,"
  }
]