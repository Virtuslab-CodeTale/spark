[
  {
    "id" : "8a247806-967a-4a39-bfe3-2eb301fe7ebf",
    "prId" : 28972,
    "prUrl" : "https://github.com/apache/spark/pull/28972#pullrequestreview-443872044",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "948d78a9-e06e-4259-a50e-761b0e3fc7dc",
        "parentId" : null,
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "should document how this interacts with. spark.memory.offHeap.enabled and I think we want to enforce that here. Or at least warn them. Right now you have this working on yarn even if its disabled, where as before it would return 0 for this size if it was disabled.  \r\n\r\n\r\n",
        "createdAt" : "2020-07-06T15:53:07Z",
        "updatedAt" : "2020-07-23T21:54:32Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "2caf1d00-55f0-4edd-847b-f5de691a83c5",
        "parentId" : "948d78a9-e06e-4259-a50e-761b0e3fc7dc",
        "authorId" : "cf8c9534-0cf3-4aad-8ead-54c363cfa86e",
        "body" : "I have this line of code to only set offHeapMemory when `spark.memory.offHeap.enabled` is `true`:\r\n```\r\n    if (conf.get(MEMORY_OFFHEAP_ENABLED)) {\r\n      // Explicitly add suffix b as default unit of offHeapMemory is Mib\r\n      ereqs.offHeapMemory(conf.get(MEMORY_OFFHEAP_SIZE).toString + \"b\")\r\n    }\r\n```\r\nIs this enough? if its disabled, the method won't be called, the default value should be 0.",
        "createdAt" : "2020-07-06T16:23:34Z",
        "updatedAt" : "2020-07-23T21:54:32Z",
        "lastEditedBy" : "cf8c9534-0cf3-4aad-8ead-54c363cfa86e",
        "tags" : [
        ]
      },
      {
        "id" : "ddc9377f-782e-4a12-9d20-7a04d1b18527",
        "parentId" : "948d78a9-e06e-4259-a50e-761b0e3fc7dc",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "that is only in the getDefaultExecutorResources function, which creates the default profile from the standard set of configs.  If the user creates a new ResourceProfile and called this function directly to set off heap memory, there is nothing checking the config MEMORY_OFFHEAP_ENABLED and on the yarn side your change doesn't check ti either, it just picks up whatever value is set here.",
        "createdAt" : "2020-07-06T17:08:22Z",
        "updatedAt" : "2020-07-23T21:54:32Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "cea9e3db-f5ea-4ac5-886d-ed985e21cf46",
        "parentId" : "948d78a9-e06e-4259-a50e-761b0e3fc7dc",
        "authorId" : "cf8c9534-0cf3-4aad-8ead-54c363cfa86e",
        "body" : "When it's disabled, I should always return 0, and log warning when user try to set this value. Is my understanding right? BTW, do you know how to access sparkConf here?",
        "createdAt" : "2020-07-07T01:36:58Z",
        "updatedAt" : "2020-07-23T21:54:32Z",
        "lastEditedBy" : "cf8c9534-0cf3-4aad-8ead-54c363cfa86e",
        "tags" : [
        ]
      },
      {
        "id" : "9ea0df0d-695f-44cb-89ee-4d531aa2cd21",
        "parentId" : "948d78a9-e06e-4259-a50e-761b0e3fc7dc",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "Ah good point, the only way to access SparkConf is for user to pass it in, which we don't want to do here.\r\nSo I think we should treat this like the config does now. First document this function that says you can set this but it requires MEMORY_OFFHEAP_ENABLED for it to be used, then allow user to set it even if MEMORY_OFFHEAP_ENABLED is false. Then in Yarn allocator before we set offHeapMem = execReq.amount we check MEMORY_OFFHEAP_ENABLED similar to what executorOffHeapMemorySizeAsMb does now.\r\n\r\n\r\n",
        "createdAt" : "2020-07-07T13:08:44Z",
        "updatedAt" : "2020-07-23T21:54:32Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "e25dafbac32534eaa998b00da98d4e90114a2c64",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +56,60 @@\n  /**\n   * Specify off heap memory. The value specified will be converted to MiB.\n   * This value only take effect when MEMORY_OFFHEAP_ENABLED is true.\n   *"
  },
  {
    "id" : "b4a3dba2-b809-411d-93b9-111e86aa9a49",
    "prId" : 28085,
    "prUrl" : "https://github.com/apache/spark/pull/28085#pullrequestreview-393021182",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "574aaea6-f93a-49f7-9afa-e903b12325df",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "If you target to support Java API from this class, it might be just best to let `requests` return a Java map directly. In Scala side, it's pretty easily able to use via using `asScala`. I think this is the usual approach in the case base if I am not mistaken.",
        "createdAt" : "2020-04-13T06:11:47Z",
        "updatedAt" : "2020-04-22T14:03:45Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "422ec083-c4a7-4d0b-b826-d5471162c24d",
        "parentId" : "574aaea6-f93a-49f7-9afa-e903b12325df",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "that is less convenient api for scala, which I think it used more. We talked about this when we added resources into TaskContextImpl class for the original resource scheduling and went with this approach so am keeping it common with that.  I would prefer to keep consistent. ",
        "createdAt" : "2020-04-13T16:37:00Z",
        "updatedAt" : "2020-04-22T14:03:45Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "d4f729de-b18f-417c-b792-c10e67f6c8d7",
        "parentId" : "574aaea6-f93a-49f7-9afa-e903b12325df",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Hm .. I am still hesitant about having different methods for Java and Scala in particular given that It is legitimate for Scala side to use Java side (the opposite is not).\r\n\r\nMy impression of Spark APIs are in general same or similar API usage across languages. There are many examples that uses Java classes, for example, in Structured Streaming as an example. e.g) `StateOperatorProgress`, `StreamingQueryProgress`, etc.\r\n\r\nSo, we should probably keep it consistent with other instances, instead of `TaskContextImpl`. Seems we marked `resourceJMap` as `Evolving` so looks not too late to change .. though ..",
        "createdAt" : "2020-04-14T01:06:05Z",
        "updatedAt" : "2020-04-22T14:03:45Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "88677b46-4888-487f-a117-fab0fbe7c470",
        "parentId" : "574aaea6-f93a-49f7-9afa-e903b12325df",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "But .. should be okay to keep it as is for now .. we can discuss and do this in a separate JIRA too.",
        "createdAt" : "2020-04-14T01:07:19Z",
        "updatedAt" : "2020-04-22T14:03:45Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "99502f60-5696-4b21-8f63-9f80e8b88323",
        "parentId" : "574aaea6-f93a-49f7-9afa-e903b12325df",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "I guess I disagree with you and others already agreed on the previous PR for TaskContextImpl.  why not just have the function and its friendly to both java and scala api's. We have lots of java specific versions of apis. Go look at the DataSet api docs and look for \"Java-specific\". I agree that there is no set rule for this though, but I would also rather make it more friendly for the common case which I think is the scala api.  One point here is that I don't think Spark has many api that return things, instead its the input parameters.  there is collect() and collectAsList() but I think when I was looking for TaskContextImpl I didn't find any other usages\r\n\r\n @mengxr  @jiangxb1987  as they were in on original api.",
        "createdAt" : "2020-04-14T13:57:41Z",
        "updatedAt" : "2020-04-22T14:03:45Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "9da2f257-436c-43fc-acde-204b69fd5ddf",
        "parentId" : "574aaea6-f93a-49f7-9afa-e903b12325df",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I was not so against it because I took it as just one or several exceptions. If this is going to be a pattern, I think it is a problem. Why not have one function legitimate for both Scala and Java that gives you a consistent API usage. Other \"java specific\" have the same names and most of then are just overloading. See `functions.scala` as an example.\r\n\r\nI would like to run this separately though rather then setting up the policy here which might be slightly off topic.",
        "createdAt" : "2020-04-14T14:49:22Z",
        "updatedAt" : "2020-04-22T14:03:45Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "491daf1b-c5d4-4035-9d8a-c6b489460c9e",
        "parentId" : "574aaea6-f93a-49f7-9afa-e903b12325df",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "None of the java specific functions in functions.scala return a different type, they take different types. That is the difference. You can't overload a function based on return type in scala.",
        "createdAt" : "2020-04-14T15:00:04Z",
        "updatedAt" : "2020-04-22T14:03:45Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "c38a1f26-171b-445b-97b5-d93bbdc8105b",
        "parentId" : "574aaea6-f93a-49f7-9afa-e903b12325df",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "My point is the most of java specifics are like that. I think I pointed out some examples that return Java instances in structured streaming. I have an idea to kind of compromising. I will send an email to the dev list tomorrow. We could discuss further there. For now, I am okay with keeping this line in this PR.",
        "createdAt" : "2020-04-14T15:05:54Z",
        "updatedAt" : "2020-04-22T14:03:45Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "354fb0c09ff9ed6be985f0ca7a8b6fae835303a2",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +40,44 @@  def requests: Map[String, ExecutorResourceRequest] = _executorResources.asScala.toMap\n\n  def requestsJMap: JMap[String, ExecutorResourceRequest] = requests.asJava\n\n  /**"
  },
  {
    "id" : "f03d7210-44b9-4981-850e-78a4dad584b0",
    "prId" : 26682,
    "prUrl" : "https://github.com/apache/spark/pull/26682#pullrequestreview-342748217",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7c679584-7953-45a6-ad75-0b6bd878cd41",
        "parentId" : null,
        "authorId" : "99e31844-a5b3-48a2-af71-412edb607a13",
        "body" : "Is this change required due to this PR? Or, should we apply this change without this PR?",
        "createdAt" : "2020-01-13T14:51:25Z",
        "updatedAt" : "2020-01-17T01:51:16Z",
        "lastEditedBy" : "99e31844-a5b3-48a2-af71-412edb607a13",
        "tags" : [
        ]
      },
      {
        "id" : "653c5aff-6c05-4dc6-97d3-851c4c3af5b2",
        "parentId" : "7c679584-7953-45a6-ad75-0b6bd878cd41",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "it can apply without the executor side changes, but all of these classes are private right now so I just included it here.\r\nI can split it off if you would like",
        "createdAt" : "2020-01-13T15:02:07Z",
        "updatedAt" : "2020-01-17T01:51:16Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "8222c2c7-9529-4825-b8f5-811afbea5d84",
        "parentId" : "7c679584-7953-45a6-ad75-0b6bd878cd41",
        "authorId" : "99e31844-a5b3-48a2-af71-412edb607a13",
        "body" : "I am curious whether this concurrency issue also exists in the master and past branches.",
        "createdAt" : "2020-01-14T18:13:48Z",
        "updatedAt" : "2020-01-17T01:51:16Z",
        "lastEditedBy" : "99e31844-a5b3-48a2-af71-412edb607a13",
        "tags" : [
        ]
      },
      {
        "id" : "f31fe108-8cee-4df8-a556-d283a5f690aa",
        "parentId" : "7c679584-7953-45a6-ad75-0b6bd878cd41",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "same answer as below for ResourceProfile",
        "createdAt" : "2020-01-14T18:33:08Z",
        "updatedAt" : "2020-01-17T01:51:16Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "75091102d6bd64a61fb903c8df3edf9ca047e879",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +35,39 @@private[spark] class ExecutorResourceRequests() extends Serializable {\n\n  private val _executorResources = new ConcurrentHashMap[String, ExecutorResourceRequest]()\n\n  def requests: Map[String, ExecutorResourceRequest] = _executorResources.asScala.toMap"
  },
  {
    "id" : "a06acc27-d257-418c-b1ef-9c1f0b5c30b1",
    "prId" : 26682,
    "prUrl" : "https://github.com/apache/spark/pull/26682#pullrequestreview-348999269",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e4959970-bb47-43c4-9afe-64fef034b1f6",
        "parentId" : null,
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "This change to ConcurrentHashMap - is it a correctness fix ? Or does use of ExecutorResourceRequests become multi-threaded now and was not earlier ?",
        "createdAt" : "2020-01-26T07:57:11Z",
        "updatedAt" : "2020-01-26T08:51:12Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "09e831b3-bbd1-4f45-b7a5-87c38239324f",
        "parentId" : "e4959970-bb47-43c4-9afe-64fef034b1f6",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "it's for correctness.  to allow users to manipulate the ExecutorResourceRequests multiple times.  The original intent was all of these classes would be immutable, but that made things less user friendly so in the original PR this class got created and users are allowed to modify multiple times and they could do it from multiple threads.  It's something I missed in the original pr rework.",
        "createdAt" : "2020-01-27T15:46:34Z",
        "updatedAt" : "2020-01-27T15:46:34Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "f89bef70-5a65-4c3a-97b7-b2590a982e55",
        "parentId" : "e4959970-bb47-43c4-9afe-64fef034b1f6",
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "Sounds good, thx for clarifying Tom !",
        "createdAt" : "2020-01-27T21:49:02Z",
        "updatedAt" : "2020-01-27T21:49:02Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      }
    ],
    "commit" : "75091102d6bd64a61fb903c8df3edf9ca047e879",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +35,39 @@private[spark] class ExecutorResourceRequests() extends Serializable {\n\n  private val _executorResources = new ConcurrentHashMap[String, ExecutorResourceRequest]()\n\n  def requests: Map[String, ExecutorResourceRequest] = _executorResources.asScala.toMap"
  }
]