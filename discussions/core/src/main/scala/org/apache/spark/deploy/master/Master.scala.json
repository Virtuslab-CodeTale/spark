[
  {
    "id" : "a5b60c87-a18d-45e2-9a74-0219d63af849",
    "prId" : 31348,
    "prUrl" : "https://github.com/apache/spark/pull/31348#pullrequestreview-584114799",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5bdeacdd-32a3-451d-8ef7-56a051f7bacb",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "This change moves the handling of `ExecutorStateChanged` from `receive` to `receiveAndReply` and `context.reply(true)` is the only difference.\r\n",
        "createdAt" : "2021-02-05T08:27:35Z",
        "updatedAt" : "2021-02-19T04:46:17Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "7e55532799f98b55ec6bae47fd2bd830d5be4b78",
    "line" : 60,
    "diffHunk" : "@@ -1,1 +504,508 @@      }\n\n    case ExecutorStateChanged(appId, execId, state, message, exitStatus) =>\n      val execOption = idToApp.get(appId).flatMap(app => app.executors.get(execId))\n      execOption match {"
  },
  {
    "id" : "70196b48-4899-4e48-aa01-2772d729a9f5",
    "prId" : 30131,
    "prUrl" : "https://github.com/apache/spark/pull/30131#pullrequestreview-516489592",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d8cd7b10-6da5-4a2a-ae7b-c57f038eaa57",
        "parentId" : null,
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "Clean Timeout worker with a delay is ok. ",
        "createdAt" : "2020-10-26T06:16:44Z",
        "updatedAt" : "2020-11-26T00:49:06Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "5643ec267a339877c1004aae9e1ed8ae1f8f6cca",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +154,158 @@    }\n    checkForWorkerTimeOutTask = forwardMessageThread.scheduleWithFixedDelay(\n      () => Utils.tryLogNonFatalError { self.send(CheckForWorkerTimeOut) },\n      0, workerTimeoutMs, TimeUnit.MILLISECONDS)\n"
  },
  {
    "id" : "79d8521c-d12e-430d-8408-4503e852c841",
    "prId" : 30131,
    "prUrl" : "https://github.com/apache/spark/pull/30131#pullrequestreview-538681948",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d679df58-a23f-45c8-9afa-b56f1f00b2cf",
        "parentId" : null,
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "+CC @Ngone51 Any thoughts on how this will impact standalone ? For changes to both `Master` and `Worker` classes.\r\nTypically I would not prefer a behavior change for a cleanup PR.",
        "createdAt" : "2020-11-25T16:45:12Z",
        "updatedAt" : "2020-11-26T00:49:06Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      }
    ],
    "commit" : "5643ec267a339877c1004aae9e1ed8ae1f8f6cca",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +154,158 @@    }\n    checkForWorkerTimeOutTask = forwardMessageThread.scheduleWithFixedDelay(\n      () => Utils.tryLogNonFatalError { self.send(CheckForWorkerTimeOut) },\n      0, workerTimeoutMs, TimeUnit.MILLISECONDS)\n"
  },
  {
    "id" : "91965cc6-6bb9-4924-8eea-a34ca8330d78",
    "prId" : 29722,
    "prUrl" : "https://github.com/apache/spark/pull/29722#pullrequestreview-489303229",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "85db06ae-cd6c-4329-a187-0bcfdf05903e",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "shall we skip it for `if (state == RecoveryState.STANDBY)` ?",
        "createdAt" : "2020-09-16T06:09:37Z",
        "updatedAt" : "2020-09-16T06:57:25Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "9db698d0-7d71-4ff8-95bc-107bce98fdb8",
        "parentId" : "85db06ae-cd6c-4329-a187-0bcfdf05903e",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "or it will never happen because the request comes from master web UI?",
        "createdAt" : "2020-09-16T06:10:09Z",
        "updatedAt" : "2020-09-16T06:57:25Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "e94b43f9-9e37-43ed-a464-f7271c091f79",
        "parentId" : "85db06ae-cd6c-4329-a187-0bcfdf05903e",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "It has been checked by the caller when handling `DecommissionWorkersOnHosts`.",
        "createdAt" : "2020-09-16T06:11:35Z",
        "updatedAt" : "2020-09-16T06:57:25Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "de826e69-6f01-4d09-8e4e-c85585a791a4",
        "parentId" : "85db06ae-cd6c-4329-a187-0bcfdf05903e",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "ok, make sense.",
        "createdAt" : "2020-09-16T06:14:04Z",
        "updatedAt" : "2020-09-16T06:57:25Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "cef50efe-3ba1-4edb-926e-d5b84ffaad77",
        "parentId" : "85db06ae-cd6c-4329-a187-0bcfdf05903e",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "ah i see",
        "createdAt" : "2020-09-16T06:14:11Z",
        "updatedAt" : "2020-09-16T06:57:25Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "290d2c08a1adfce3c0c4afe1cb8c9e214b25ea3d",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +258,262 @@      // so it should not be the STANDBY\n      assert(state != RecoveryState.STANDBY)\n      ids.foreach ( id =>\n        // We use foreach since get gives us an option and we can skip the failures.\n        idToWorker.get(id).foreach { w =>"
  },
  {
    "id" : "49f722c2-0959-4700-944c-e5e92f96f75f",
    "prId" : 29579,
    "prUrl" : "https://github.com/apache/spark/pull/29579#pullrequestreview-481677592",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ba68ae0e-d74e-4bbb-911d-5864f6e62c2c",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "nit: can you reword the comment to be more accurate now :-) ",
        "createdAt" : "2020-08-31T06:21:18Z",
        "updatedAt" : "2020-09-07T13:30:39Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "bd571560-bc87-4b48-a90a-5c036bb240ce",
        "parentId" : "ba68ae0e-d74e-4bbb-911d-5864f6e62c2c",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Updated a little bit.",
        "createdAt" : "2020-09-03T09:48:24Z",
        "updatedAt" : "2020-09-07T13:30:39Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "d2468407f3d02e5a191ad373b5f2201a930cbb09",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +910,914 @@          exec.id, ExecutorState.DECOMMISSIONED,\n          Some(\"worker decommissioned\"), None,\n          // worker host is being set here to let the driver know that the host (aka. worker)\n          // is also being decommissioned. So the driver can unregister all the shuffle map\n          // statues located at this host when it receives the executor lost event."
  },
  {
    "id" : "4a5c43ed-f8dd-4b44-8f3e-a120472ade30",
    "prId" : 28742,
    "prUrl" : "https://github.com/apache/spark/pull/28742#pullrequestreview-426291962",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "363e9910-e770-4192-b0e8-4fb5c6215f61",
        "parentId" : null,
        "authorId" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "body" : "Why do you need to verify `waitingApps.length == 1` ?",
        "createdAt" : "2020-06-08T06:41:47Z",
        "updatedAt" : "2020-06-08T06:41:47Z",
        "lastEditedBy" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "tags" : [
        ]
      },
      {
        "id" : "0bbb8914-9020-4aad-8608-b0fb70e62aec",
        "parentId" : "363e9910-e770-4192-b0e8-4fb5c6215f61",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "hmm..I was also thinking about this question when I was fixing. I think we can extend it to multiple apps. Let me try it.",
        "createdAt" : "2020-06-08T07:06:06Z",
        "updatedAt" : "2020-06-08T07:06:07Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "940bee1e-0bc4-45eb-aa45-1a50154d6cbd",
        "parentId" : "363e9910-e770-4192-b0e8-4fb5c6215f61",
        "authorId" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "body" : "I'm not sure how useful is this warning message, this just reflects the current status within one round of scheduling, we can tolerant the behavior to not launch a new executor on currently available workers.",
        "createdAt" : "2020-06-08T07:11:10Z",
        "updatedAt" : "2020-06-08T07:11:10Z",
        "lastEditedBy" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "tags" : [
        ]
      },
      {
        "id" : "1de0a809-2bcd-4ed7-9def-72e798db6634",
        "parentId" : "363e9910-e770-4192-b0e8-4fb5c6215f61",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "It's from https://github.com/apache/spark/pull/25047#discussion_r311779752 originally. The warning is mainly used to detect the possible hang of an application.",
        "createdAt" : "2020-06-08T07:15:09Z",
        "updatedAt" : "2020-06-08T07:15:09Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "3b2e3a5a-84ee-41ab-b668-b8d2e22df1c6",
        "parentId" : "363e9910-e770-4192-b0e8-4fb5c6215f61",
        "authorId" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "body" : "I don't like this check, but that seems to be the most simple solution for now. Let's keep it then.",
        "createdAt" : "2020-06-08T07:21:27Z",
        "updatedAt" : "2020-06-08T07:21:27Z",
        "lastEditedBy" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "tags" : [
        ]
      },
      {
        "id" : "245bd195-7a74-464f-b918-120bc43e2b6d",
        "parentId" : "363e9910-e770-4192-b0e8-4fb5c6215f61",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "I think the idea of ==1 is that there is only 1 active application, if more then 1, other applications could be using resources so unless you did a lot more checking. We could definitely make it a more comprehensive check in the future.  I believe this was supposed to be a simple sanity check for people playing around that may have started the cluster with not enough resources.  The standalone mode does not tell you there aren't enough and will just hang forever.",
        "createdAt" : "2020-06-08T14:21:12Z",
        "updatedAt" : "2020-06-08T14:21:12Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "55d47307df8f5eb7f303dd3ce4b09fbe45cf6e84",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +716,720 @@          .filter(canLaunchExecutor(_, app.desc))\n          .sortBy(_.coresFree).reverse\n        val appMayHang = waitingApps.length == 1 &&\n          waitingApps.head.executors.isEmpty && usableWorkers.isEmpty\n        if (appMayHang) {"
  },
  {
    "id" : "51c0f19e-6857-42cd-83f0-7bb4d3542271",
    "prId" : 27697,
    "prUrl" : "https://github.com/apache/spark/pull/27697#pullrequestreview-366735887",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5a79b31d-e738-404b-bbb1-1e9b366c44e0",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Could you make a test case for this?",
        "createdAt" : "2020-02-28T23:05:49Z",
        "updatedAt" : "2020-02-28T23:05:49Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "c8fe334d005046a23029b352c057d1b0c00c5129",
    "line" : 31,
    "diffHunk" : "@@ -1,1 +801,805 @@    val allFreeCores = shuffledAliveWorkers.map(_.coresFree).sum\n    val forDriversFreeCores = math.max(allFreeCores - coresReservedForApps, 0)\n    if (forDriversFreeCores > 0) {\n      for (driver <- waitingDrivers.toList) { // iterate over a copy of waitingDrivers\n        // We assign workers to each waiting driver in a round-robin fashion. For each driver, we"
  },
  {
    "id" : "c7f311f1-5dc2-40bf-bd9c-8e85cd6ca9a8",
    "prId" : 26440,
    "prUrl" : "https://github.com/apache/spark/pull/26440#pullrequestreview-315652175",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a7186920-1ec1-4c94-b45c-45e5ec57c2a8",
        "parentId" : null,
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "might want to have a getOrElse or make sure idToWorker contains id in case something else timed out the worker between when message sent and when processed.",
        "createdAt" : "2019-11-08T21:41:57Z",
        "updatedAt" : "2020-02-13T23:10:10Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "39eb0c96-4b3d-4976-834f-f26554a5be03",
        "parentId" : "a7186920-1ec1-4c94-b45c-45e5ec57c2a8",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "So the get returns an option and we use `foreach` instead of `getOrElse`, but I'll the comment to make that clearer.",
        "createdAt" : "2019-11-12T16:08:01Z",
        "updatedAt" : "2020-02-13T23:10:10Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "af550303e0b929dc9f7436bcfb36438ff36b8208",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +250,254 @@      } else {\n        // We use foreach since get gives us an option and we can skip the failures.\n        idToWorker.get(id).foreach(decommissionWorker)\n      }\n"
  },
  {
    "id" : "4d4e496e-1c21-4b94-8080-932208dbdbdb",
    "prId" : 25047,
    "prUrl" : "https://github.com/apache/spark/pull/25047#pullrequestreview-267496866",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5bda1f20-7414-445c-bab4-dae8b5a6d97f",
        "parentId" : null,
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "the logic here to keep adding cores to an executor is a bit odd to me, at first I didn't think it was handling the resources properly, but I guess its doing the same thing as memory right now, the first executor launch would require all memory and resources, but I guess in default case it only actually allocates 1 core at a time.  Is that correct?  I'm not seeing how the whole increase the number of cores worked though, I'll have to look in more detail",
        "createdAt" : "2019-07-26T15:02:46Z",
        "updatedAt" : "2019-08-09T02:27:38Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "6cdd4872-a9b7-4511-b40c-401f73d4d224",
        "parentId" : "5bda1f20-7414-445c-bab4-dae8b5a6d97f",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "`scheduleExecutorsOnWorkers()` currently has two modes: 1) one executor per worker 2) multiple executors per worker.  If we don't configured `coresPerExecutor`, then, we use mode 1. In mode 1, only one executor can be launched per worker and that executor grabs all cores on the worker. But, for memory and resources, it still respects to memoryPerExecutor and resourcesPerExecutor rather than requires all.",
        "createdAt" : "2019-07-28T09:00:16Z",
        "updatedAt" : "2019-08-09T02:27:38Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "15a9897131f06ec115b13d97e86c497ef36dace8",
    "line" : 150,
    "diffHunk" : "@@ -1,1 +708,712 @@          logWarning(s\"App ${app.id} requires more resource than any of Workers could have.\")\n        }\n        val assignedCores = scheduleExecutorsOnWorkers(app, usableWorkers, spreadOutApps)\n\n        // Now that we've decided how many cores to allocate on each worker, let's allocate them"
  },
  {
    "id" : "33119f66-288b-41dd-922b-8b141e3daa9f",
    "prId" : 25047,
    "prUrl" : "https://github.com/apache/spark/pull/25047#pullrequestreview-272739676",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d7e8b7b4-f24c-4655-9192-e1ed980fc649",
        "parentId" : null,
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "We have an issue here if no Workers have the resources required by the executor/task requirements, then it doesn't warn/error and it doesn't retry.  Basically I started a Worker without GPUs and then said I need gpus for my executor task and it end up hanging.  I suppose one could argue this is ok as someone could start another Worker that has the resources, but I think we at least need to Warn about it",
        "createdAt" : "2019-08-07T21:54:26Z",
        "updatedAt" : "2019-08-09T02:27:38Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "d7aeec46-44ec-4137-8afe-670ad9eee4fd",
        "parentId" : "d7e8b7b4-f24c-4655-9192-e1ed980fc649",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Actually, it does retry when other executors or drivers finish. But, we can warn if executor or driver requires more resources than any of workers could have. BTW, I'm thinking do we have the same issue for memory and cores ? For example, a Worker has 10 cores at most while an executor ask for 20 cores ?",
        "createdAt" : "2019-08-08T15:17:16Z",
        "updatedAt" : "2019-08-09T02:27:38Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "a1419981-b0ef-4f66-b142-c51db4935460",
        "parentId" : "d7e8b7b4-f24c-4655-9192-e1ed980fc649",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "Right if something changes (other app, other workers, etc) it retries, but if I'm the only app on the cluster its not clear why the app isn't launching.  The one thing I don't want is it to be to noisy though either. I thought about that before making the comment, because like you said if its just out of resources because other apps are running we don't really want to print anything.  I think for now we should just limit it to resources and perhaps just say no Workers are configured with the resources you requested.  If we can do that without much performance impact lets do it. If not maybe we just file a separate jira for it and look at it there",
        "createdAt" : "2019-08-08T15:50:13Z",
        "updatedAt" : "2019-08-09T02:27:38Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "642acec3-7d0d-48b5-b5fc-e24d691bb900",
        "parentId" : "d7e8b7b4-f24c-4655-9192-e1ed980fc649",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "How about this way?\r\n\r\n```\r\nfor (app <- waitingApps) {\r\n   ...\r\n   val usableWorkers = workers.toArray.filter(_.state == WorkerState.ALIVE)\r\n     .filter(canLaunchExecutor(_, app.desc))\r\n     .sortBy(_.coresFree).reverse\r\n   if (waitingApps.size == 1 && usableWorkers.isEmpty) {\r\n    logWarn(\"The app requires more resources(mem, core, accelerator) than any of Workers could have.\") \r\n  }\r\n  ...\r\n}\r\n```\r\n\r\nTelling \"the Workers are not configured with the resources(I mean accelerator) as an app requested\" may require more changes. For example, you may need to traversal workers again to judge whether it's due to resources(I mean accelerator) or memory or cores. Or, you need to refactor `canLaunchExecutor` to tell more details.",
        "createdAt" : "2019-08-08T16:24:04Z",
        "updatedAt" : "2019-08-09T02:27:38Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "d7b7c12e-f819-4a0b-b424-523ade8163ef",
        "parentId" : "d7e8b7b4-f24c-4655-9192-e1ed980fc649",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "that sounds good for now.  Lets also leave it called resource since that is what its called everywhere right now.  just leave off the (mem, core, accelerator) part. ",
        "createdAt" : "2019-08-08T17:54:52Z",
        "updatedAt" : "2019-08-09T02:27:38Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "15a9897131f06ec115b13d97e86c497ef36dace8",
    "line" : 146,
    "diffHunk" : "@@ -1,1 +704,708 @@        val usableWorkers = workers.toArray.filter(_.state == WorkerState.ALIVE)\n          .filter(canLaunchExecutor(_, app.desc))\n          .sortBy(_.coresFree).reverse\n        if (waitingApps.length == 1 && usableWorkers.isEmpty) {\n          logWarning(s\"App ${app.id} requires more resource than any of Workers could have.\")"
  },
  {
    "id" : "46336f0f-b037-44af-8600-a8b61caf1ab2",
    "prId" : 25047,
    "prUrl" : "https://github.com/apache/spark/pull/25047#pullrequestreview-272252741",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ac54110b-b9cf-43f5-b7d6-11185585d5a5",
        "parentId" : null,
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "I'm assuming driver could also not launch if worker doesn't have any GPUs and it requests them, may want to warn here as well",
        "createdAt" : "2019-08-07T22:01:07Z",
        "updatedAt" : "2019-08-09T02:27:38Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "15a9897131f06ec115b13d97e86c497ef36dace8",
    "line" : 212,
    "diffHunk" : "@@ -1,1 +800,804 @@        if (canLaunchDriver(worker, driver.desc)) {\n          val allocated = worker.acquireResources(driver.desc.resourceReqs)\n          driver.withResources(allocated)\n          launchDriver(worker, driver)\n          waitingDrivers -= driver"
  },
  {
    "id" : "dde1c9c8-5a6d-4e75-846d-ea8f641d8165",
    "prId" : 24848,
    "prUrl" : "https://github.com/apache/spark/pull/24848#pullrequestreview-252205559",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9a8c93fa-9725-450f-beb3-bd34a37a9668",
        "parentId" : null,
        "authorId" : "b4e64da3-d07f-4391-82fc-7bfc071fcb0b",
        "body" : "Shall we also check the `state` to guarantee only the `WAITING` state and `coresGranted > 0` can be shifted to `RUNNING` ?",
        "createdAt" : "2019-06-17T06:20:29Z",
        "updatedAt" : "2019-06-17T06:20:29Z",
        "lastEditedBy" : "b4e64da3-d07f-4391-82fc-7bfc071fcb0b",
        "tags" : [
        ]
      },
      {
        "id" : "9da3932c-5e34-40b9-8da4-c1c3c9ebb5db",
        "parentId" : "9a8c93fa-9725-450f-beb3-bd34a37a9668",
        "authorId" : "8d712730-9885-49fa-aeff-705d5a9e1bd9",
        "body" : "Correct me if I'm wrong, but isn't the app state should be RUNNING as long as it gained an executor? not matter whatever state before.",
        "createdAt" : "2019-06-17T08:08:56Z",
        "updatedAt" : "2019-06-17T08:08:56Z",
        "lastEditedBy" : "8d712730-9885-49fa-aeff-705d5a9e1bd9",
        "tags" : [
        ]
      },
      {
        "id" : "e21844d3-5a9e-46a3-b82c-e664be3309fd",
        "parentId" : "9a8c93fa-9725-450f-beb3-bd34a37a9668",
        "authorId" : "b4e64da3-d07f-4391-82fc-7bfc071fcb0b",
        "body" : "My only concern is that there might be some corner cases to transfer wrong state to `RUNNING` (though I cannot imagine). ",
        "createdAt" : "2019-06-17T08:49:29Z",
        "updatedAt" : "2019-06-17T08:49:30Z",
        "lastEditedBy" : "b4e64da3-d07f-4391-82fc-7bfc071fcb0b",
        "tags" : [
        ]
      },
      {
        "id" : "46e75721-74cd-4147-95d4-37f42ff7f8ab",
        "parentId" : "9a8c93fa-9725-450f-beb3-bd34a37a9668",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "> Shall we also check the state to guarantee only the WAITING state and coresGranted > 0 can be shifted to RUNNING ?\r\n\r\nNot necessary. When we run into `completeRecovery()`, an app can only be either `UNKNOWN` or `WAITING`. And, all `UNKNOWN` apps have been filtered out when we reach here. So, they are definitely in `WAITING` state at this time. See: \r\n\r\nhttps://github.com/apache/spark/blob/d9e4cf67c06b2d6daa4cd24b056e33dfb5eb35f5/core/src/main/scala/org/apache/spark/deploy/master/Master.scala#L560-L563\r\n\r\n\r\n",
        "createdAt" : "2019-06-17T09:16:02Z",
        "updatedAt" : "2019-06-17T09:16:03Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "d453b18f-6d8a-41d0-a43b-40b8e9eeec51",
        "parentId" : "9a8c93fa-9725-450f-beb3-bd34a37a9668",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "I'm thinking that there may be a corner case where `_.coresGranted > 0` can not work. For example, in dynamic allocation mode, app A is running on HA Standalone. And master just happens to change during the time app A has released all of its executors, which result in  `coresGranted=0`. Then, app A can't recover to RUNNING but remain in WAITING at this time. @zuotingbing  @jerryshao ",
        "createdAt" : "2019-06-17T09:31:08Z",
        "updatedAt" : "2019-06-17T09:31:08Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "649b4070-4d39-48c3-94a1-50630baeec09",
        "parentId" : "9a8c93fa-9725-450f-beb3-bd34a37a9668",
        "authorId" : "b4e64da3-d07f-4391-82fc-7bfc071fcb0b",
        "body" : "So this brings out another question, can we say that this application is `WAITING` or `RUNNING` if by judging the executors? Obviously when dynamic allocation is enabled, no executor doesn't mean the application is waiting.",
        "createdAt" : "2019-06-17T09:44:27Z",
        "updatedAt" : "2019-06-17T09:44:30Z",
        "lastEditedBy" : "b4e64da3-d07f-4391-82fc-7bfc071fcb0b",
        "tags" : [
        ]
      },
      {
        "id" : "d8865ce0-9363-48b6-8d24-837ab0bd7c65",
        "parentId" : "9a8c93fa-9725-450f-beb3-bd34a37a9668",
        "authorId" : "b4e64da3-d07f-4391-82fc-7bfc071fcb0b",
        "body" : "So this brings out another question, can we say that this application is `WAITING` or `RUNNING` by judging the executors? Obviously when dynamic allocation is enabled, no executor doesn't mean the application is waiting.",
        "createdAt" : "2019-06-17T09:44:53Z",
        "updatedAt" : "2019-06-17T09:45:21Z",
        "lastEditedBy" : "b4e64da3-d07f-4391-82fc-7bfc071fcb0b",
        "tags" : [
        ]
      },
      {
        "id" : "ea043afc-8497-49fb-bc68-9b7fdb04ddf6",
        "parentId" : "9a8c93fa-9725-450f-beb3-bd34a37a9668",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Maybe, the better way to mark an app as `WAITING` is the time the bind driver for that app waiting for an executor to launch on. Once the driver set up, the app is `RUNNING`.",
        "createdAt" : "2019-06-17T11:09:55Z",
        "updatedAt" : "2019-06-17T11:09:56Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "c31d1605-5f59-4c12-b1ae-276508577857",
        "parentId" : "9a8c93fa-9725-450f-beb3-bd34a37a9668",
        "authorId" : "8d712730-9885-49fa-aeff-705d5a9e1bd9",
        "body" : "> app A has released all of its executors, which result in `coresGranted=0`. Then, app A can't recover to RUNNING but remain in WAITING at this time. \r\n\r\nWhy app A release all of its executors? Does app A finished? Or shall we change the state to Running when app A grant executors again?\r\n\r\n",
        "createdAt" : "2019-06-18T01:58:35Z",
        "updatedAt" : "2019-06-18T01:58:56Z",
        "lastEditedBy" : "8d712730-9885-49fa-aeff-705d5a9e1bd9",
        "tags" : [
        ]
      },
      {
        "id" : "3ee5295c-24c4-42d5-bac9-05376233d241",
        "parentId" : "9a8c93fa-9725-450f-beb3-bd34a37a9668",
        "authorId" : "b4e64da3-d07f-4391-82fc-7bfc071fcb0b",
        "body" : "In a Dynamic Executor Allocation enabled scenario, executor number can be 0 in idle state (when min executor number is set to 0). So in this scenario, we can hardly say that application is waiting (and it is not finished).",
        "createdAt" : "2019-06-18T05:58:27Z",
        "updatedAt" : "2019-06-18T05:58:27Z",
        "lastEditedBy" : "b4e64da3-d07f-4391-82fc-7bfc071fcb0b",
        "tags" : [
        ]
      },
      {
        "id" : "c495bb85-4a43-4054-a5c7-4ad319d3e748",
        "parentId" : "9a8c93fa-9725-450f-beb3-bd34a37a9668",
        "authorId" : "8d712730-9885-49fa-aeff-705d5a9e1bd9",
        "body" : "I search for \"ApplicationState.RUNNING\" in the whole project, only be set in function `org.apache.spark.deploy.master.Master#allocateWorkerResourceToExecutors`, which means if app gained 0 executors it should be the init state(ApplicationState.WAITING)",
        "createdAt" : "2019-06-18T06:18:56Z",
        "updatedAt" : "2019-06-18T06:18:56Z",
        "lastEditedBy" : "8d712730-9885-49fa-aeff-705d5a9e1bd9",
        "tags" : [
        ]
      },
      {
        "id" : "28aa350d-f5ef-4d67-b596-c8d4c97efe81",
        "parentId" : "9a8c93fa-9725-450f-beb3-bd34a37a9668",
        "authorId" : "8d712730-9885-49fa-aeff-705d5a9e1bd9",
        "body" : ">For example, in dynamic allocation mode, app A is running on HA Standalone. And master just happens to change during the time app A has released all of its executors, which result in `coresGranted=0`. Then, app A can't recover to RUNNING but remain in WAITING at this time. \r\n\r\nAfter debug, i found when the app gain executors again, the app can update to RUNNING from WAITING. So it may be OK in master changed scenario that if app gained 0 executors we set it to WAITING state as usual as an app init do.\r\n\r\n",
        "createdAt" : "2019-06-18T08:26:12Z",
        "updatedAt" : "2019-06-18T08:26:54Z",
        "lastEditedBy" : "8d712730-9885-49fa-aeff-705d5a9e1bd9",
        "tags" : [
        ]
      },
      {
        "id" : "8dc05b9c-6abc-4408-908a-9a8bf5794db0",
        "parentId" : "9a8c93fa-9725-450f-beb3-bd34a37a9668",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Hmmm...with the current code logic, I'd say yes that an app could always update to RUNNING from WAITING whenever it gains an executor. Then, the WAITING status, here, exactly means that **the app is waiting for at least one executor**. As a result, an app in Standalone with dynamic allocation could change its status from RUNNING/WAITING to WAITING/RUNNING back and forth. But, usually, it doesn't make sense. \r\n\r\nThe argument here, I think, is that how do you define the WAITING status for an app in Standalone.  If you say, an app should be in WAITING if it does not own  any executors at the moment. Then, current code logic and PR change would be fine. But, if you say, an app should be in WAITING if it hasn't launched its driver. Then, current code logic may be wrong.",
        "createdAt" : "2019-06-18T15:39:06Z",
        "updatedAt" : "2019-06-18T15:39:07Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "710ffd1e-b180-4fc7-997b-6e50a85be445",
        "parentId" : "9a8c93fa-9725-450f-beb3-bd34a37a9668",
        "authorId" : "8d712730-9885-49fa-aeff-705d5a9e1bd9",
        "body" : "> But, if you say, an app should be in WAITING if it hasn't launched its driver\r\n\r\nHmmm... maybe you make it extremely complicated, there is no synchronous causal link between the Driver state and the App state, when driver is active in client mode, app can be WAITING or RUNNING.\r\n",
        "createdAt" : "2019-06-19T03:27:01Z",
        "updatedAt" : "2019-06-19T03:55:29Z",
        "lastEditedBy" : "8d712730-9885-49fa-aeff-705d5a9e1bd9",
        "tags" : [
        ]
      },
      {
        "id" : "d4e053d6-d328-4254-a220-a1e1e4d21326",
        "parentId" : "9a8c93fa-9725-450f-beb3-bd34a37a9668",
        "authorId" : "8d712730-9885-49fa-aeff-705d5a9e1bd9",
        "body" : "> As a result, an app in Standalone with dynamic allocation could change its status from RUNNING/WAITING to WAITING/RUNNING back and forth\r\n\r\nRUNNING to WAITING only exists when Master has changed. Let us go to the beginning. Why we set app to WAITING, then change WAITING to RUNNING during the recovery time? **Shall we keep the state before and just filter out the UNKNOWN app that didn't respond during the recovery time?**",
        "createdAt" : "2019-06-19T06:52:15Z",
        "updatedAt" : "2019-06-19T06:52:15Z",
        "lastEditedBy" : "8d712730-9885-49fa-aeff-705d5a9e1bd9",
        "tags" : [
        ]
      },
      {
        "id" : "8e13fb04-b6c5-4dc5-8776-8505de2df7ee",
        "parentId" : "9a8c93fa-9725-450f-beb3-bd34a37a9668",
        "authorId" : "b4e64da3-d07f-4391-82fc-7bfc071fcb0b",
        "body" : "From the code, it looks like only when executor is registered, then the state will change to `RUNNING` in master UI (the code there is quite old). But I think it is not valid to use registered executor numbers to judge the state of application:\r\n\r\n1. When dynamic allocation is enabled, executors can be 0, we cannot say this application is waiting because it will ramp up soon.\r\n2. Some driver only tasks can be run without executors, we cannot say this application is waiting.\r\n\r\nSo as a thorough fix, I would suggest to revisit all the changes related to application state, to see whether it is meaningful as for now, rather than just a point fix here.",
        "createdAt" : "2019-06-19T07:45:11Z",
        "updatedAt" : "2019-06-19T07:45:12Z",
        "lastEditedBy" : "b4e64da3-d07f-4391-82fc-7bfc071fcb0b",
        "tags" : [
        ]
      },
      {
        "id" : "4ca24528-84b0-4414-8ea7-d87d02705eb3",
        "parentId" : "9a8c93fa-9725-450f-beb3-bd34a37a9668",
        "authorId" : "8d712730-9885-49fa-aeff-705d5a9e1bd9",
        "body" : "Yes the point is when and why app is set to WAITING state(besides the app init).  \r\n\r\nsee [MasterChangeAcknowledged](https://github.com/apache/spark/blob/d9e4cf67c06b2d6daa4cd24b056e33dfb5eb35f5/core/src/main/scala/org/apache/spark/deploy/master/Master.scala#L352-L361)\r\n\r\n```\r\n    case MasterChangeAcknowledged(appId) =>\r\n      idToApp.get(appId) match {\r\n        case Some(app) =>\r\n          logInfo(\"Application has been re-registered: \" + appId)\r\n          app.state = ApplicationState.WAITING\r\n        case None =>\r\n          logWarning(\"Master change ack from unknown app: \" + appId)\r\n      }\r\n\r\n      if (canCompleteRecovery) { completeRecovery() }\r\n```\r\n\r\nplanB: Maybe when master changed ,we just keep the state before(do not need to set app to WAITING) and filter out the UNKNOWN app that didn't respond . If so i will update code for planB",
        "createdAt" : "2019-06-19T09:21:23Z",
        "updatedAt" : "2019-06-19T09:45:44Z",
        "lastEditedBy" : "8d712730-9885-49fa-aeff-705d5a9e1bd9",
        "tags" : [
        ]
      },
      {
        "id" : "5260dad5-2d6c-48a9-861b-08014afb8deb",
        "parentId" : "9a8c93fa-9725-450f-beb3-bd34a37a9668",
        "authorId" : "8d712730-9885-49fa-aeff-705d5a9e1bd9",
        "body" : "Further learning the logic of recover app when master has changed, i found that in function RegisterApplication , an app is created and then stored in persistenceEngine:  `persistenceEngine.addApplication(app)` as WAITING state. When master has changed, recover app from the stored data and register application again with WAITING state. The current fix maybe right(after recover,the app is on WAITING with init state, then change to RUNNING as long as it gained an executor), otherwise the stored app data in persistenceEngine is wrong.\r\n",
        "createdAt" : "2019-06-20T10:18:07Z",
        "updatedAt" : "2019-06-20T10:18:07Z",
        "lastEditedBy" : "8d712730-9885-49fa-aeff-705d5a9e1bd9",
        "tags" : [
        ]
      }
    ],
    "commit" : "fd5b685b3731e05a76402a50f9e4d592182c9a6c",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +562,566 @@\n    // Update the state of recovered apps to RUNNING\n    apps.filter(_.coresGranted > 0).foreach(_.state = ApplicationState.RUNNING)\n\n    // Reschedule drivers which were not claimed by any workers"
  }
]