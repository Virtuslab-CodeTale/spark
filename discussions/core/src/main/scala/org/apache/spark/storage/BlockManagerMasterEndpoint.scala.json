[
  {
    "id" : "2183d6d9-1793-46c9-ac00-fb7e83abe255",
    "prId" : 33020,
    "prUrl" : "https://github.com/apache/spark/pull/33020#pullrequestreview-690541966",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7ccaac81-41b7-4fb8-b6e9-ce479404b367",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Seems like we never clear the key after this change. Could you add some comments (maybe here) to explain?",
        "createdAt" : "2021-06-23T04:12:46Z",
        "updatedAt" : "2021-06-23T04:12:46Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "5f3c65fa-9403-4b69-926e-78bf8103fca5",
        "parentId" : "7ccaac81-41b7-4fb8-b6e9-ce479404b367",
        "authorId" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "body" : "done",
        "createdAt" : "2021-06-23T11:44:45Z",
        "updatedAt" : "2021-06-23T11:44:45Z",
        "lastEditedBy" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "tags" : [
        ]
      }
    ],
    "commit" : "9d8d35712228e886ea27d72da4139f5c18285f0b",
    "line" : 31,
    "diffHunk" : "@@ -1,1 +570,574 @@          // BlockStatusPerBlockId releases the backing HashMap.\n          val externalShuffleServiceBlocks = blockStatusByShuffleService\n            .getOrElseUpdate(externalShuffleServiceIdOnHost(id), new BlockStatusPerBlockId)\n          Some(externalShuffleServiceBlocks)\n        } else {"
  },
  {
    "id" : "59916ba8-6057-4ab8-a4cf-f1a4c30a0c72",
    "prId" : 32790,
    "prUrl" : "https://github.com/apache/spark/pull/32790#pullrequestreview-676765544",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b23e83c7-6d8f-4b67-99b2-eb5d2b017cdb",
        "parentId" : null,
        "authorId" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "body" : "After this PR `blockStatusByShuffleService.get(bmId)` can be `None` and even when it has some value for the key then even `m.get(blockId)` can be `null` as `m` is a Java `HashMap`. ",
        "createdAt" : "2021-06-05T09:22:42Z",
        "updatedAt" : "2021-06-05T09:22:55Z",
        "lastEditedBy" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "tags" : [
        ]
      }
    ],
    "commit" : "6de20f10ca60c778db25ba3d5975f85e5c23aedb",
    "line" : 29,
    "diffHunk" : "@@ -1,1 +672,676 @@    val status = locations.headOption.flatMap { bmId =>\n      if (externalShuffleServiceRddFetchEnabled && bmId.port == externalShuffleServicePort) {\n        blockStatusByShuffleService.get(bmId).flatMap(m => Option(m.get(blockId)))\n      } else {\n        aliveBlockManagerInfo(bmId).flatMap(_.getStatus(blockId))"
  },
  {
    "id" : "e0aa7dc4-a407-4e7e-b030-6716e8d90266",
    "prId" : 32790,
    "prUrl" : "https://github.com/apache/spark/pull/32790#pullrequestreview-676899781",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c6babb4d-0755-4b8d-aef7-ba45dc2f8817",
        "parentId" : null,
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "nit: can we add a comment about why we remove it here? (e.g the memory leak) because just reading the code through it might not be super obvious.",
        "createdAt" : "2021-06-06T20:18:53Z",
        "updatedAt" : "2021-06-06T20:21:26Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "6de20f10ca60c778db25ba3d5975f85e5c23aedb",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +281,285 @@            // when all blocks are removed from the block statuses then for this BM Id the whole\n            // blockStatusByShuffleService entry can be removed to avoid leaking memory\n            if (blockStatusForId.isEmpty) {\n              blockStatusByShuffleService.remove(bmIdForShuffleService)\n            }"
  },
  {
    "id" : "f349b634-2505-4f4d-a598-68a1645f8bf7",
    "prId" : 32114,
    "prUrl" : "https://github.com/apache/spark/pull/32114#pullrequestreview-669787347",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "66148e7f-3625-4910-8fb6-881a67591e24",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Oh..Just some succinct description should be fine. This's too detailed for the comment. I think you can remove the detailed description below.",
        "createdAt" : "2021-05-27T06:31:23Z",
        "updatedAt" : "2021-05-27T06:31:24Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "2c7a4395c3dc75ff803b37a29541292104c53cb7",
    "line" : 60,
    "diffHunk" : "@@ -1,1 +353,357 @@    // We are delaying the removal of BlockManagerInfo to avoid a BlockManager reregistration\n    // while a executor is shutting. This unwanted reregistration causes inconsistent bookkeeping\n    // of executors in Spark.\n    // Delaying this removal until blockManagerInfoCleaner decides to remove it ensures\n    // BlockManagerMasterHeartbeatEndpoint does not ask the BlockManager on a recently removed"
  },
  {
    "id" : "41de3732-d325-404f-a52b-6dffbf78df3a",
    "prId" : 30164,
    "prUrl" : "https://github.com/apache/spark/pull/30164#pullrequestreview-530769366",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5d544e59-0f14-40bf-a64f-2d9c9899d85d",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "hmm..why we need to set a threshold of the `shuffleMergerLocations`? I think the number of external shuffle service should be static within a cluster(no matter what the resource allocation mode of the application is, dynamic or static) and wouldn't increase unlimitedly.\r\n\r\nBesides, the removed oldest merger may store more merged shuffle data than others...would be better if we could remove depends on merged shuffle data size...but this should be another topic though..",
        "createdAt" : "2020-11-05T05:39:13Z",
        "updatedAt" : "2020-11-20T00:32:25Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "50b6dfc3-a506-4872-af7b-4d81116acd94",
        "parentId" : "5d544e59-0f14-40bf-a64f-2d9c9899d85d",
        "authorId" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "body" : "This would be useful in a cloud based deployment where the nodes keeps coming up and going down and so the number of External shuffle services for a cluster is not static. \r\n\r\n> Besides, the removed oldest merger may store more merged shuffle data than others...would be better if we could remove depends on merged shuffle data size...but this should be another topic though..\r\n\r\nAgree, the first part of that is to have a pluggable API (https://issues.apache.org/jira/browse/SPARK-33329) that way we can come up with specific implementation based on cloud/on-prem/ as well as based on scheduler backends. This is the basic implementation but eventually we need load balancing between shuffle mergers",
        "createdAt" : "2020-11-05T19:03:24Z",
        "updatedAt" : "2020-11-20T00:32:25Z",
        "lastEditedBy" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "tags" : [
        ]
      },
      {
        "id" : "e87a454c-c79f-4c85-8649-e1746dca1f45",
        "parentId" : "5d544e59-0f14-40bf-a64f-2d9c9899d85d",
        "authorId" : "15ef9b5b-1583-47d9-b883-14ebfd23a00a",
        "body" : "Just to clarify, removing a merger does not remove the merged shuffle data on that location. It only prevents using that location for future shuffles. Reducers will still be able to fetch merged shuffle data from the removed merger locations.",
        "createdAt" : "2020-11-15T06:00:37Z",
        "updatedAt" : "2020-11-20T00:32:25Z",
        "lastEditedBy" : "15ef9b5b-1583-47d9-b883-14ebfd23a00a",
        "tags" : [
        ]
      }
    ],
    "commit" : "5ce29340c8aa4a6aadfc8d00cb5053a9be9aa839",
    "line" : 46,
    "diffHunk" : "@@ -1,1 +382,386 @@        blockManagerId.host, externalShuffleServicePort)\n      if (shuffleMergerLocations.size >= maxRetainedMergerLocations) {\n        shuffleMergerLocations -= shuffleMergerLocations.head._1\n      }\n      shuffleMergerLocations(shuffleServerId.host) = shuffleServerId"
  },
  {
    "id" : "00f35ac3-4547-4ed1-aa3c-e67ca35ab472",
    "prId" : 30164,
    "prUrl" : "https://github.com/apache/spark/pull/30164#pullrequestreview-524564885",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "75c5ffd0-03df-4bb9-bf1a-aae821219144",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "I wonder why you need a separate `shuffleMergerLocations` since it doesn't have a big difference comparing to the `BlockManagerId`. I guess this is because of the dynamic allocation case where a node may have no running executors but external shuffle service only...right?",
        "createdAt" : "2020-11-05T05:42:13Z",
        "updatedAt" : "2020-11-20T00:32:25Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "365530f3-cf5e-4d0e-8b73-f3952a11c010",
        "parentId" : "75c5ffd0-03df-4bb9-bf1a-aae821219144",
        "authorId" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "body" : "Yes, thats right. this is to keep track of locations where an executor ran before that we we know the application has registered with that particular host's External shuffle service which is needed to push the shuffle data.",
        "createdAt" : "2020-11-05T18:50:02Z",
        "updatedAt" : "2020-11-20T00:32:25Z",
        "lastEditedBy" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "tags" : [
        ]
      }
    ],
    "commit" : "5ce29340c8aa4a6aadfc8d00cb5053a9be9aa839",
    "line" : 48,
    "diffHunk" : "@@ -1,1 +384,388 @@        shuffleMergerLocations -= shuffleMergerLocations.head._1\n      }\n      shuffleMergerLocations(shuffleServerId.host) = shuffleServerId\n    }\n  }"
  },
  {
    "id" : "763c6e0b-81fa-488d-bc4e-d27053683caa",
    "prId" : 30164,
    "prUrl" : "https://github.com/apache/spark/pull/30164#pullrequestreview-529606629",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8673911c-d446-46c3-abf6-d60b65cd8edd",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Maybe we could remove the corresponding merger when there's fetch failure happens on it?",
        "createdAt" : "2020-11-10T06:55:09Z",
        "updatedAt" : "2020-11-20T00:32:25Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "475862e1-bd37-4fd8-ba3b-1af400f946a9",
        "parentId" : "8673911c-d446-46c3-abf6-d60b65cd8edd",
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "@Ngone51 That is a good idea, is there any concern with adding this @venkata91 ?\r\nBut I would like to add that it would help in only a subset of cases.\r\n\r\nWhat I mean is, when fetch of a merged block fails, executors will fallback to fetching the constituent blocks [1]. A fetch failure would be reported to driver only when both of these fetches fail - the merged block and the mapper output shuffle block fetch. The fallback mechanism would mean that if the individual blocks for a merged block were not computed on the lost host for the parent stage, we wont see the fetch failure.\r\n\r\nWith the recent changes to prefer hosts with active executors, we do improve the chances of detecting lost merger candidates due to lost hosts though.\r\n\r\n[1] Ignoring cases like large blocks which are not candidates for merge, etc.",
        "createdAt" : "2020-11-10T08:12:49Z",
        "updatedAt" : "2020-11-20T00:32:25Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "1e795d00-d96b-4c06-83bb-d8d76a5b93d6",
        "parentId" : "8673911c-d446-46c3-abf6-d60b65cd8edd",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "> With the recent changes to prefer hosts with active executors, we do improve the chances of detecting lost merger candidates due to lost hosts though.\r\n\r\nYeah, it shall be fine mostly. But please also note that executors and external shuffle service/merger are two separate processes. So the active executor doesn't necessarily mean the active merger.\r\n\r\nI agree it may not help a lot so I'm fine to not add it now.",
        "createdAt" : "2020-11-12T09:14:53Z",
        "updatedAt" : "2020-11-20T00:32:25Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "b666ce38-3162-4547-a642-c3e0464a9f4a",
        "parentId" : "8673911c-d446-46c3-abf6-d60b65cd8edd",
        "authorId" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "body" : "Yes, makes sense. Still I went ahead and made that change. One question I had is what if its a transient failure due to heavy load or something of that sorts? Do we still want to remove the host from the list in those cases?",
        "createdAt" : "2020-11-13T00:16:25Z",
        "updatedAt" : "2020-11-20T00:32:25Z",
        "lastEditedBy" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "tags" : [
        ]
      }
    ],
    "commit" : "5ce29340c8aa4a6aadfc8d00cb5053a9be9aa839",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +78,82 @@  // registered an executor in the past. Older hosts are removed when the\n  // maxRetainedMergerLocations size is reached in favor of newer locations.\n  private val shuffleMergerLocations = new mutable.LinkedHashMap[String, BlockManagerId]()\n\n  // Maximum number of merger locations to cache"
  },
  {
    "id" : "e0164d0c-7026-4c8f-8ae9-8e310f324b14",
    "prId" : 30164,
    "prUrl" : "https://github.com/apache/spark/pull/30164#pullrequestreview-528519193",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bbdeebed-82b0-4f71-a1e2-059682149e3e",
        "parentId" : null,
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "This would be an Iterable right ? Convert it to Set with `toSet` ? Else the cost remains O(N^2)",
        "createdAt" : "2020-11-10T19:46:26Z",
        "updatedAt" : "2020-11-20T00:32:25Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "6b27d122-c236-4318-a9f2-96ad2a8d733a",
        "parentId" : "bbdeebed-82b0-4f71-a1e2-059682149e3e",
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "Scratch that, did not see the .toSet while creating `blockManagersWithExecutors` !",
        "createdAt" : "2020-11-10T19:48:09Z",
        "updatedAt" : "2020-11-20T00:32:25Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "992f6215-26bd-4e64-a324-85bb8a316806",
        "parentId" : "bbdeebed-82b0-4f71-a1e2-059682149e3e",
        "authorId" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "body" : "I think it would be a `Set` as `filteredMergersWithExecutors` is already a `Set`. Also double checked the type and it is `Set[String` for `filteredMergersWithExecutorsHosts` ",
        "createdAt" : "2020-11-10T22:02:36Z",
        "updatedAt" : "2020-11-20T00:32:25Z",
        "lastEditedBy" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "tags" : [
        ]
      },
      {
        "id" : "d4455bfd-b47a-4c74-8b82-15b874f4dad7",
        "parentId" : "bbdeebed-82b0-4f71-a1e2-059682149e3e",
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "Yeah, I missed that - hence my second followup comment :-)",
        "createdAt" : "2020-11-11T20:42:11Z",
        "updatedAt" : "2020-11-20T00:32:25Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      }
    ],
    "commit" : "5ce29340c8aa4a6aadfc8d00cb5053a9be9aa839",
    "line" : 82,
    "diffHunk" : "@@ -1,1 +701,705 @@    } else {\n      // Delta mergers added from inactive mergers list to the active mergers list\n      val filteredMergersWithExecutorsHosts = filteredMergersWithExecutors.map(_.host)\n      val filteredMergersWithoutExecutors = shuffleMergerLocations.values\n        .filterNot(x => hostsToFilter.contains(x.host))"
  },
  {
    "id" : "b911fb50-7301-43d2-9a3f-fc78602f8f9f",
    "prId" : 29992,
    "prUrl" : "https://github.com/apache/spark/pull/29992#pullrequestreview-506063716",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5875c92a-8a64-4dde-ad55-48c361dafb19",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Shall we import?",
        "createdAt" : "2020-10-10T04:03:37Z",
        "updatedAt" : "2020-10-10T04:03:37Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "d40e6697-47d0-45e8-abb4-7e74d1a693ea",
        "parentId" : "5875c92a-8a64-4dde-ad55-48c361dafb19",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Also, it would be great if we have a warning log here.",
        "createdAt" : "2020-10-10T04:04:19Z",
        "updatedAt" : "2020-10-10T04:04:19Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "1ca46a477020efa4e5bd11e4d55a82ef1e1b2254",
    "line" : 26,
    "diffHunk" : "@@ -1,1 +394,398 @@    } catch {\n      // If the block manager has already exited, nothing to replicate.\n      case e: java.util.NoSuchElementException =>\n        Seq.empty[ReplicateBlock]\n    }"
  },
  {
    "id" : "bc9559ad-b0b3-4177-9009-a5b0a65e4119",
    "prId" : 28924,
    "prUrl" : "https://github.com/apache/spark/pull/28924#pullrequestreview-441604096",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8084a094-f0a0-490a-bbc7-0bc491e6c688",
        "parentId" : null,
        "authorId" : "ad4cb313-47b2-4cfa-9c1f-0a3596794a67",
        "body" : "[nit] since this method deals with handling block removal failures explicitly, can we rename it to reflect the same. maybe something like: handleBlockRemovalFailure? ",
        "createdAt" : "2020-07-01T08:06:57Z",
        "updatedAt" : "2020-07-10T07:38:55Z",
        "lastEditedBy" : "ad4cb313-47b2-4cfa-9c1f-0a3596794a67",
        "tags" : [
        ]
      },
      {
        "id" : "2c63a0bc-2af1-4041-869a-f521a468a96a",
        "parentId" : "8084a094-f0a0-490a-bbc7-0bc491e6c688",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Good idea!",
        "createdAt" : "2020-07-02T11:51:52Z",
        "updatedAt" : "2020-07-10T07:38:55Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "319d9257b200bb087afe15aace8b66d96925ce93",
    "line" : 53,
    "diffHunk" : "@@ -1,1 +191,195 @@      blockId: String,\n      bmId: BlockManagerId,\n      defaultValue: T): PartialFunction[Throwable, T] = {\n    case e: IOException =>\n      logWarning(s\"Error trying to remove $blockType $blockId\" +"
  },
  {
    "id" : "3a3e75f3-8445-4e63-95a4-452789234db0",
    "prId" : 28708,
    "prUrl" : "https://github.com/apache/spark/pull/28708#pullrequestreview-433770929",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d952a200-d55f-4ea9-9342-d13add0cec71",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "I see that `getMigrationBlocks` returns the sub blocks in the order of `[index, data]`, but I wonder if the sub blocks are indeed migrated strictly in order ? Is it ever possible to have a data block updated before an index block ? It seems that we would want both, before we update the mapOutputTracker -- but I am not 100% clear on how that is ensured.",
        "createdAt" : "2020-06-19T02:43:08Z",
        "updatedAt" : "2020-07-17T23:45:20Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "b67fd747-d5a8-463f-b0e2-fb6efdfe29ce",
        "parentId" : "d952a200-d55f-4ea9-9342-d13add0cec71",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Yeah sub blocks are migrated in order (we don't parallel map them).",
        "createdAt" : "2020-06-19T03:18:52Z",
        "updatedAt" : "2020-07-17T23:45:20Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "8494bdd94285c7cc5a41e151da920710be7f4671",
    "line" : 38,
    "diffHunk" : "@@ -1,1 +547,551 @@      blockId match {\n        case ShuffleIndexBlockId(shuffleId, mapId, _) =>\n          // Don't update the map output on just the index block\n          logDebug(s\"Received shuffle index block update for ${shuffleId} ${mapId}, ignoring.\")\n          return true"
  },
  {
    "id" : "70080bbe-e5bf-4256-85e3-e61dc5edbfad",
    "prId" : 28708,
    "prUrl" : "https://github.com/apache/spark/pull/28708#pullrequestreview-436926949",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d18f82f0-33b2-4b7a-b42c-51f46e5dea85",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "I wanted to confirm my understanding that this PR does not do anything about shuffle files written by executors that have since finished ? That would be the External Shuffle Service's responsibility (if enabled), Right ? And that piece of the puzzle isn't implemented yet ?\r\n\r\nMy question stems from how we handle this scenario: (Consider the standalone scheduler mode here)\r\n- Executor wrote some shuffle data and is enabled with external shuffle service \r\n- Executor finished normally and nothing is running on the worker. \r\n- DecommissionExecutor is not even called because the executor does not exist\r\n- Thus the DecommissionBlockManager isn't called and thus no migrations take place.\r\n\r\nTherefore fetch failures would happen for other tasks that try to read this executor's shuffle data (via the External Shuffle Service). I believe fixing this is not in the scope of this PR.\r\n\r\nCan you please check this understanding. Thanks.\r\n",
        "createdAt" : "2020-06-22T05:04:45Z",
        "updatedAt" : "2020-07-17T23:45:20Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "5fa2abf6-ac42-4f76-84c8-a90fd4e7850b",
        "parentId" : "d18f82f0-33b2-4b7a-b42c-51f46e5dea85",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Right we don't do anything in that situation.",
        "createdAt" : "2020-06-24T19:04:26Z",
        "updatedAt" : "2020-07-17T23:45:20Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "8494bdd94285c7cc5a41e151da920710be7f4671",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +163,167 @@      context.reply(true)\n\n    case DecommissionBlockManagers(executorIds) =>\n      val bmIds = executorIds.flatMap(blockManagerIdByExecutor.get)\n      decommissionBlockManagers(bmIds)"
  },
  {
    "id" : "3462a6e1-b0da-4497-bbc2-97f2eb7d1be4",
    "prId" : 28370,
    "prUrl" : "https://github.com/apache/spark/pull/28370#pullrequestreview-405386112",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "00d6cc7e-d50f-4e4a-92e7-e4c557e85cad",
        "parentId" : null,
        "authorId" : "31c8cfa2-9d5f-412b-80d0-c7d204dd8cd1",
        "body" : "Any idea of how expensive this is gonna be? \r\nIf there are large number during production, may be they can be stored separately? May be another PR for this",
        "createdAt" : "2020-05-02T13:44:47Z",
        "updatedAt" : "2020-05-14T19:07:46Z",
        "lastEditedBy" : "31c8cfa2-9d5f-412b-80d0-c7d204dd8cd1",
        "tags" : [
        ]
      },
      {
        "id" : "1c177ca0-e049-401a-91c1-8c2a72167d64",
        "parentId" : "00d6cc7e-d50f-4e4a-92e7-e4c557e85cad",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "I think storing a separate list of RDD blocks would be more expensive, we also only this call occasionally compared to the other operations in the block manager so I think it's ok if filtering the keyset is expensive (although it probably is not  very expensive).",
        "createdAt" : "2020-05-04T22:15:26Z",
        "updatedAt" : "2020-05-14T19:07:46Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "c34305662cd91d05b5160c7de72e35e4108b6755",
    "line" : 59,
    "diffHunk" : "@@ -1,1 +334,338 @@    val info = blockManagerInfo(blockManagerId)\n\n    val rddBlocks = info.blocks.keySet().asScala.filter(_.isRDD)\n    rddBlocks.map { blockId =>\n      val currentBlockLocations = blockLocations.get(blockId)"
  },
  {
    "id" : "7f4342a0-c083-45f8-9bab-b66ed9b16c42",
    "prId" : 28370,
    "prUrl" : "https://github.com/apache/spark/pull/28370#pullrequestreview-405385233",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8a36ca1c-dcaa-45e1-a4de-fadb6d20d1db",
        "parentId" : null,
        "authorId" : "31c8cfa2-9d5f-412b-80d0-c7d204dd8cd1",
        "body" : "Can we make this an interface/trait that is implemented by some entity, which holistically decides the location for replication?\r\nI suggest this model, which will help to plugin code which YARN based cluster can extend based on node decommissioning(this PR deals with executor decommissioning) or some strategy to equally distribute the other block managers or external storage etc. ",
        "createdAt" : "2020-05-02T14:21:01Z",
        "updatedAt" : "2020-05-14T19:07:46Z",
        "lastEditedBy" : "31c8cfa2-9d5f-412b-80d0-c7d204dd8cd1",
        "tags" : [
        ]
      },
      {
        "id" : "0ee0e55e-f67f-4b28-8ee4-4943da641876",
        "parentId" : "8a36ca1c-dcaa-45e1-a4de-fadb6d20d1db",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Currently the logic exists inside of blockReplicationPolicy so that would be the place to explore that.",
        "createdAt" : "2020-05-04T17:21:50Z",
        "updatedAt" : "2020-05-14T19:07:46Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "57fdf9b4-7f57-479a-9a50-d7ebae1c499a",
        "parentId" : "8a36ca1c-dcaa-45e1-a4de-fadb6d20d1db",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "It seems to be plugable, so if you wanted to specify your own policy you could specify `spark.storage.replication.policy`",
        "createdAt" : "2020-05-04T22:13:45Z",
        "updatedAt" : "2020-05-14T19:07:46Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "c34305662cd91d05b5160c7de72e35e4108b6755",
    "line" : 64,
    "diffHunk" : "@@ -1,1 +339,343 @@      val maxReplicas = currentBlockLocations.size + 1\n      val remainingLocations = currentBlockLocations.toSeq.filter(bm => bm != blockManagerId)\n      val replicateMsg = ReplicateBlock(blockId, remainingLocations, maxReplicas)\n      replicateMsg\n    }.toSeq"
  },
  {
    "id" : "89106d99-1a38-4112-a0ee-7202cbf8e547",
    "prId" : 28370,
    "prUrl" : "https://github.com/apache/spark/pull/28370#pullrequestreview-413816377",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "221bc857-5a83-4d3c-b031-dd6596b171bb",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "IIUC, there's no need to do replication if `remainingLocations` is equal to `currentBlockLocations` after filtering the `blockManagerId`, which means we've already successfully done decommission for the block in previous decommission round.",
        "createdAt" : "2020-05-18T02:59:07Z",
        "updatedAt" : "2020-05-18T02:59:28Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "3cebb00d-10b0-4609-8b95-f40b1ea885f3",
        "parentId" : "221bc857-5a83-4d3c-b031-dd6596b171bb",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "So we currently remove the block on successful decommissioning. But it's possible it somehow is sufficiently replicated we don't need to do anything so I've added this to https://issues.apache.org/jira/browse/SPARK-31555 for tracking.\r\n",
        "createdAt" : "2020-05-18T17:55:59Z",
        "updatedAt" : "2020-05-18T17:55:59Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "c34305662cd91d05b5160c7de72e35e4108b6755",
    "line" : 64,
    "diffHunk" : "@@ -1,1 +339,343 @@      val maxReplicas = currentBlockLocations.size + 1\n      val remainingLocations = currentBlockLocations.toSeq.filter(bm => bm != blockManagerId)\n      val replicateMsg = ReplicateBlock(blockId, remainingLocations, maxReplicas)\n      replicateMsg\n    }.toSeq"
  },
  {
    "id" : "0a34ada0-4323-4718-87ff-b63565c7ff09",
    "prId" : 28370,
    "prUrl" : "https://github.com/apache/spark/pull/28370#pullrequestreview-413819644",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9b0010a9-dc74-4340-a2be-54727ec584c4",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Could you please add some comments to explain why we need \"+1\"?",
        "createdAt" : "2020-05-18T03:35:17Z",
        "updatedAt" : "2020-05-18T03:35:17Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "8e587f0c-64ac-45e9-a1aa-633d33b074b0",
        "parentId" : "9b0010a9-dc74-4340-a2be-54727ec584c4",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "I think the fact that weâ€™re decommissioning here makes this self evident",
        "createdAt" : "2020-05-18T03:48:57Z",
        "updatedAt" : "2020-05-18T03:48:58Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "8c3bf1fa-ccb5-42b3-8249-3d3c93c93bbc",
        "parentId" : "9b0010a9-dc74-4340-a2be-54727ec584c4",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "The method itself does not declare that it's used for decommissioning.",
        "createdAt" : "2020-05-18T04:21:56Z",
        "updatedAt" : "2020-05-18T04:21:56Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "ba7e37a3-0325-4717-b252-fde710294e0e",
        "parentId" : "9b0010a9-dc74-4340-a2be-54727ec584c4",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Reasonable then to add a comment",
        "createdAt" : "2020-05-18T05:11:09Z",
        "updatedAt" : "2020-05-18T05:11:09Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "ef78ae78-7926-4313-807f-a6846ef7fa42",
        "parentId" : "9b0010a9-dc74-4340-a2be-54727ec584c4",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Added to https://issues.apache.org/jira/browse/SPARK-31555",
        "createdAt" : "2020-05-18T18:00:48Z",
        "updatedAt" : "2020-05-18T18:00:48Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "c34305662cd91d05b5160c7de72e35e4108b6755",
    "line" : 62,
    "diffHunk" : "@@ -1,1 +337,341 @@    rddBlocks.map { blockId =>\n      val currentBlockLocations = blockLocations.get(blockId)\n      val maxReplicas = currentBlockLocations.size + 1\n      val remainingLocations = currentBlockLocations.toSeq.filter(bm => bm != blockManagerId)\n      val replicateMsg = ReplicateBlock(blockId, remainingLocations, maxReplicas)"
  },
  {
    "id" : "3974849b-5150-4c89-a647-1574e9c511a2",
    "prId" : 27418,
    "prUrl" : "https://github.com/apache/spark/pull/27418#pullrequestreview-351808010",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2097d007-9dd1-420a-bb84-f5d4254c125f",
        "parentId" : null,
        "authorId" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "body" : "nit: maybe `result` -> `isSuccess` ?",
        "createdAt" : "2020-01-31T22:12:27Z",
        "updatedAt" : "2020-02-01T00:09:18Z",
        "lastEditedBy" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "fecaf20a4b8c88596f6fd60e5033b9b4e328b623",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +99,103 @@    case _updateBlockInfo @\n        UpdateBlockInfo(blockManagerId, blockId, storageLevel, deserializedSize, size) =>\n      val result = updateBlockInfo(blockManagerId, blockId, storageLevel, deserializedSize, size)\n      context.reply(result)\n      // SPARK-30594: we should not post `SparkListenerBlockUpdated` when updateBlockInfo"
  },
  {
    "id" : "8d908a4d-532a-47d3-82be-114d7e26a0c6",
    "prId" : 27306,
    "prUrl" : "https://github.com/apache/spark/pull/27306#pullrequestreview-346969368",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7ebf6cce-65f4-4e25-83ba-e20f23370961",
        "parentId" : null,
        "authorId" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "body" : "nit: Just say we shouldn't post the event when the result is false because the block info would be updated again later.",
        "createdAt" : "2020-01-22T23:22:18Z",
        "updatedAt" : "2020-02-03T05:09:39Z",
        "lastEditedBy" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "5f123173bc03cb057c97fb98f5ea5879452e00ba",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +101,105 @@      val isSuccess = updateBlockInfo(blockManagerId, blockId, storageLevel, deserializedSize, size)\n      context.reply(isSuccess)\n      // SPARK-30594: we should not post `SparkListenerBlockUpdated` when updateBlockInfo\n      // returns false since the block info would be updated again later.\n      if (isSuccess) {"
  },
  {
    "id" : "19472215-fdf5-438a-a1ca-5e2221dce946",
    "prId" : 25299,
    "prUrl" : "https://github.com/apache/spark/pull/25299#pullrequestreview-282613954",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ae62475f-c4e4-4202-a9a2-78bf85c8dbc7",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "shall we update it in `removeBlockManager`?",
        "createdAt" : "2019-08-30T11:52:16Z",
        "updatedAt" : "2019-11-26T12:18:14Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "c7e639c8-c59b-4319-9f80-79c25a50324b",
        "parentId" : "ae62475f-c4e4-4202-a9a2-78bf85c8dbc7",
        "authorId" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "body" : "No as local directories are outlive the executors. ",
        "createdAt" : "2019-09-02T14:09:27Z",
        "updatedAt" : "2019-11-26T12:18:14Z",
        "lastEditedBy" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "tags" : [
        ]
      }
    ],
    "commit" : "da93837805efc76c83511eed6a71d49851b34945",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +53,57 @@\n  // Mapping from executor id to the block manager's local disk directories.\n  private val executorIdToLocalDirs =\n    CacheBuilder\n      .newBuilder()"
  }
]