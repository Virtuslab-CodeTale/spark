[
  {
    "id" : "880a33d8-12b9-4e35-b63f-f8f94124b554",
    "prId" : 31898,
    "prUrl" : "https://github.com/apache/spark/pull/31898#pullrequestreview-617184332",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ca6094fe-f0ae-4ea4-8daa-d00cd98b5c15",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I think we still have enough time to take a look and make AQE supported with IO encryption. Does it require a lot of change to make it supported?",
        "createdAt" : "2021-03-21T00:40:16Z",
        "updatedAt" : "2021-03-22T14:31:44Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "ef133b70-ceb9-487e-92f7-321088b6ea99",
        "parentId" : "ca6094fe-f0ae-4ea4-8daa-d00cd98b5c15",
        "authorId" : "b0e245a1-4f84-42f1-9f20-4c5f111c43a7",
        "body" : "Yes, you are right. This PR makes a simple read control, which may sacrifice shuffle read performance in AQE. It would be better to allow batch fetching for contiguous shuffle blocks while IO encryption enabled.\r\n",
        "createdAt" : "2021-03-21T15:38:25Z",
        "updatedAt" : "2021-03-22T14:31:44Z",
        "lastEditedBy" : "b0e245a1-4f84-42f1-9f20-4c5f111c43a7",
        "tags" : [
        ]
      },
      {
        "id" : "a84883ac-66cc-41a1-a369-53b0ca852ab2",
        "parentId" : "ca6094fe-f0ae-4ea4-8daa-d00cd98b5c15",
        "authorId" : "b0e245a1-4f84-42f1-9f20-4c5f111c43a7",
        "body" : "BTW, it seems to me that when io encryption enabled, the demarcation point (end of blocks) between consecutive blocks cannot be resolved correctly during deserialization (shuffle read).\r\n",
        "createdAt" : "2021-03-21T15:39:17Z",
        "updatedAt" : "2021-03-22T14:31:44Z",
        "lastEditedBy" : "b0e245a1-4f84-42f1-9f20-4c5f111c43a7",
        "tags" : [
        ]
      },
      {
        "id" : "e1a68ccc-6183-4936-ae5c-de67f6d4e1de",
        "parentId" : "ca6094fe-f0ae-4ea4-8daa-d00cd98b5c15",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "We could store the size info of each consecutive block in `ShuffleBlockBatchId` to resolve it?",
        "createdAt" : "2021-03-22T03:43:55Z",
        "updatedAt" : "2021-03-22T14:31:44Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "7aae271e-ef66-4a14-980b-46e67f375e6b",
        "parentId" : "ca6094fe-f0ae-4ea4-8daa-d00cd98b5c15",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "That sounds like a good idea but we need to think about how to support it in the current shuffle protocol.",
        "createdAt" : "2021-03-22T05:59:16Z",
        "updatedAt" : "2021-03-22T14:31:44Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "c47b4fad-8a7d-42e1-a4f2-70b7038a35d2",
        "parentId" : "ca6094fe-f0ae-4ea4-8daa-d00cd98b5c15",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "Maybe we can have this bandage fix first. The encryption shuffle mode breaks the assumption of batch serialized and deserialize, similar to the issue with codec concatenation.",
        "createdAt" : "2021-03-22T06:33:41Z",
        "updatedAt" : "2021-03-22T14:31:44Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "d824a9b36d41154d15c54925be440ba92759f599",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +53,57 @@    val useOldFetchProtocol = conf.get(config.SHUFFLE_USE_OLD_FETCH_PROTOCOL)\n    // SPARK-34790: Fetching continuous blocks in batch is incompatible with io encryption.\n    val ioEncryption = conf.get(config.IO_ENCRYPTION_ENABLED)\n\n    val doBatchFetch = shouldBatchFetch && serializerRelocatable &&"
  },
  {
    "id" : "33328c24-c9e3-4b4b-9b77-b8c821ea5b8c",
    "prId" : 31898,
    "prUrl" : "https://github.com/apache/spark/pull/31898#pullrequestreview-617034876",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "90d5a99f-b35c-4451-9e1d-e0428340f7f7",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "@hezuojiao . Do you want to ban IO encryption when we don't use adaptive SQL query execution,  `spark.sql.adaptive.enabled=false`?",
        "createdAt" : "2021-03-21T00:57:29Z",
        "updatedAt" : "2021-03-22T14:31:44Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "01f2f641-3c08-41b0-9549-8646fd4dfc28",
        "parentId" : "90d5a99f-b35c-4451-9e1d-e0428340f7f7",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "IIUC, the test coverage of this code path works correctly when `spark.sql.adaptive.enabled=false`. If not, please file another JIRA. It's independent from AQE.",
        "createdAt" : "2021-03-21T00:58:36Z",
        "updatedAt" : "2021-03-22T14:31:44Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "bc90fce5-5b4b-4c5b-864a-6ab538825ae8",
        "parentId" : "90d5a99f-b35c-4451-9e1d-e0428340f7f7",
        "authorId" : "b0e245a1-4f84-42f1-9f20-4c5f111c43a7",
        "body" : "No. In my understanding, continuous reading in batch only occurs when adaptive query execution is turned on. Furthermore, it is actually only related to partition coalescing of AQE. So it will not affect the case when the adaptive query execution is closed.",
        "createdAt" : "2021-03-21T15:56:11Z",
        "updatedAt" : "2021-03-22T14:31:44Z",
        "lastEditedBy" : "b0e245a1-4f84-42f1-9f20-4c5f111c43a7",
        "tags" : [
        ]
      }
    ],
    "commit" : "d824a9b36d41154d15c54925be440ba92759f599",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +56,60 @@\n    val doBatchFetch = shouldBatchFetch && serializerRelocatable &&\n      (!compressed || codecConcatenation) && !useOldFetchProtocol && !ioEncryption\n    if (shouldBatchFetch && !doBatchFetch) {\n      logDebug(\"The feature tag of continuous shuffle block fetching is set to true, but \" +"
  },
  {
    "id" : "b2b4a593-498d-44bb-9820-5a2b6a6dba6b",
    "prId" : 26095,
    "prUrl" : "https://github.com/apache/spark/pull/26095#pullrequestreview-301101374",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "71036f81-deaa-41e4-81ac-0947805e2026",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "This is the shuffle read side and we need to know the value of `SHUFFLE_USE_OLD_FETCH_PROTOCOL`. I think the bug is in the shuffle write side which is fixed in this PR. Do we really need to change the shuffle read side?",
        "createdAt" : "2019-10-14T04:20:17Z",
        "updatedAt" : "2019-10-14T13:01:08Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "e5aa034f-9b98-45e4-9954-79474cf0a7f1",
        "parentId" : "71036f81-deaa-41e4-81ac-0947805e2026",
        "authorId" : "f2e3e1f3-82a3-4b68-a0c2-2f7fdf58cf35",
        "body" : "This is redundant code, since ShuffleWrite writes the mapId based on the `spark.shuffle.useOldFetchProtocol` flag, `MapStatus.mapTaskId` always gives the mapId which is set by the ShuffleWriter",
        "createdAt" : "2019-10-14T04:32:30Z",
        "updatedAt" : "2019-10-14T13:01:08Z",
        "lastEditedBy" : "f2e3e1f3-82a3-4b68-a0c2-2f7fdf58cf35",
        "tags" : [
        ]
      }
    ],
    "commit" : "1e22dc31a2e21af7aeae7896af8194f6f68ff70b",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +48,52 @@      blockManager.blockStoreClient,\n      blockManager,\n      mapOutputTracker.getMapSizesByExecutorId(handle.shuffleId, startPartition, endPartition),\n      serializerManager.wrapStream,\n      // Note: we use getSizeAsMb when no suffix is provided for backwards compatibility"
  },
  {
    "id" : "7972c0e9-9eae-4e63-bb59-883eced4e4b6",
    "prId" : 26040,
    "prUrl" : "https://github.com/apache/spark/pull/26040#pullrequestreview-302983118",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f1f19da2-572e-4c24-a9b9-56144d58b25d",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "AQE is off by default, and the continuous fetch is on by default. This means we always do log. I think we should log only if the compressor/serializer is not suitable.\r\n",
        "createdAt" : "2019-10-16T13:53:38Z",
        "updatedAt" : "2019-10-17T03:26:34Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "e65b6b09-66e7-44eb-a16c-79fef9eaf97a",
        "parentId" : "f1f19da2-572e-4c24-a9b9-56144d58b25d",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "e.g. the code can be\r\n```\r\nval shouldBatchFetch = fetchMultiPartitions && context.getLocalProperties...\r\n...\r\nval doBatchFetch = ...\r\nif (shouldBatchFetch && !doBatchFetch) ...\r\n```",
        "createdAt" : "2019-10-16T14:00:31Z",
        "updatedAt" : "2019-10-17T03:26:34Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "b268be7e-05fb-4719-b0b9-a4daffdfa400",
        "parentId" : "f1f19da2-572e-4c24-a9b9-56144d58b25d",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "Make sense, done in afd74e5.",
        "createdAt" : "2019-10-17T03:15:45Z",
        "updatedAt" : "2019-10-17T03:26:34Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "8f855a7bbe7be4095e758d4bff1b0fccf71dbc79",
    "line" : 41,
    "diffHunk" : "@@ -1,1 +55,59 @@      (!compressed || codecConcatenation)\n    if (shouldBatchFetch && !doBatchFetch) {\n      logDebug(\"The feature tag of continuous shuffle block fetching is set to true, but \" +\n        \"we can not enable the feature because other conditions are not satisfied. \" +\n        s\"Shuffle compress: $compressed, serializer relocatable: $serializerRelocatable, \" +"
  },
  {
    "id" : "f8c1483f-8e3e-44c3-95c8-1a0fd3abc66c",
    "prId" : 26040,
    "prUrl" : "https://github.com/apache/spark/pull/26040#pullrequestreview-303104213",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3b123d99-1857-4bec-a11a-76943e2cebaf",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "shall we use \"false\" as the default value?",
        "createdAt" : "2019-10-17T04:31:48Z",
        "updatedAt" : "2019-10-17T04:31:48Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "5065b404-7cd2-4238-90cc-a9056549da2f",
        "parentId" : "3b123d99-1857-4bec-a11a-76943e2cebaf",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "Thanks, done in the follow-up PR #26147.",
        "createdAt" : "2019-10-17T09:02:50Z",
        "updatedAt" : "2019-10-17T09:02:51Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "8f855a7bbe7be4095e758d4bff1b0fccf71dbc79",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +37,41 @@    blockManager: BlockManager = SparkEnv.get.blockManager,\n    mapOutputTracker: MapOutputTracker = SparkEnv.get.mapOutputTracker,\n    shouldBatchFetch: Boolean)\n  extends ShuffleReader[K, C] with Logging {\n"
  },
  {
    "id" : "8c0e2d96-cf75-4f13-b5aa-32caf7ff0ee1",
    "prId" : 25295,
    "prUrl" : "https://github.com/apache/spark/pull/25295#pullrequestreview-301703852",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d650e66a-32bc-445b-aa30-d689147c67b5",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "nit:\r\n```\r\ncase Some(..) =>\r\ncase None =>\r\n```",
        "createdAt" : "2019-10-15T07:35:42Z",
        "updatedAt" : "2019-10-15T07:35:42Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "9c1dc5538afce26c4e693e353d8d4ef4231bb78c",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +55,59 @@        startPartition,\n        endPartition)\n      case (_) => throw new IllegalArgumentException(\n        \"mapId should be both set or unset\")\n    }"
  }
]