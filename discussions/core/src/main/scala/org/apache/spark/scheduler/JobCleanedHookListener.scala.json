[
  {
    "id" : "7ad56185-a36a-40c7-8972-bad2d578a3aa",
    "prId" : 28280,
    "prUrl" : "https://github.com/apache/spark/pull/28280#pullrequestreview-398823693",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "25e2a933-df2f-4e8c-8cc8-e0368ed4d0a4",
        "parentId" : null,
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "`synchronized {`",
        "createdAt" : "2020-04-23T07:21:54Z",
        "updatedAt" : "2020-07-14T12:59:36Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "71df478d9630dc569dd442277a70d9fd9c2f6d01",
    "line" : 32,
    "diffHunk" : "@@ -1,1 +30,34 @@  private val jobCleanedHooks = new HashMap[Int, Int => Unit]()\n\n  def addCleanedHook(jobId: Int, fun: Int => Unit): Unit = synchronized{\n    jobCleanedHooks.put(jobId, fun)\n  }"
  },
  {
    "id" : "a5ec70cf-3bcb-443b-9503-449c12bdad8c",
    "prId" : 28280,
    "prUrl" : "https://github.com/apache/spark/pull/28280#pullrequestreview-398823693",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "840283b2-4796-4289-9d28-b6a3b36b5a95",
        "parentId" : null,
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "`foreach { function`",
        "createdAt" : "2020-04-23T07:22:45Z",
        "updatedAt" : "2020-07-14T12:59:36Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "71df478d9630dc569dd442277a70d9fd9c2f6d01",
    "line" : 42,
    "diffHunk" : "@@ -1,1 +40,44 @@  override def onJobCleaned(jobCleaned: SparkListenerJobCleaned): Unit = {\n    jobCleanedHooks.get(jobCleaned.jobId)\n      .foreach{function =>\n        function(jobCleaned.jobId)\n        deleteCleanedHook(jobCleaned.jobId)"
  },
  {
    "id" : "2c6c185f-7de9-4d85-bc19-a50012e36d2c",
    "prId" : 28280,
    "prUrl" : "https://github.com/apache/spark/pull/28280#pullrequestreview-403242071",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "23032a4c-ca49-4166-bb69-be9f16f80214",
        "parentId" : null,
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "Maybe we need to add UT for the new class in SparkListenerSuite or somewhere else.",
        "createdAt" : "2020-04-23T07:35:11Z",
        "updatedAt" : "2020-07-14T12:59:36Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      },
      {
        "id" : "da4c05a4-e7af-462c-849d-7824a12028d9",
        "parentId" : "23032a4c-ca49-4166-bb69-be9f16f80214",
        "authorId" : "0f4ef4e8-09be-436f-b743-bf8fbc343490",
        "body" : "Yes, I am doing it.",
        "createdAt" : "2020-04-30T06:44:25Z",
        "updatedAt" : "2020-07-14T12:59:36Z",
        "lastEditedBy" : "0f4ef4e8-09be-436f-b743-bf8fbc343490",
        "tags" : [
        ]
      }
    ],
    "commit" : "71df478d9630dc569dd442277a70d9fd9c2f6d01",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +26,30 @@ * jobs and run cleaned hook after a job is cleaned.\n */\nclass JobCleanedHookListener extends SparkListener with Logging {\n  // use private val and synchronized to keep thread safe\n  private val jobCleanedHooks = new HashMap[Int, Int => Unit]()"
  }
]