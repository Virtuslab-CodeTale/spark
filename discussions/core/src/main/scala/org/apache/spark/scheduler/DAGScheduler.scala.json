[
  {
    "id" : "9bbdde18-c31a-4014-8e9d-18bfefa73b69",
    "prId" : 32356,
    "prUrl" : "https://github.com/apache/spark/pull/32356#pullrequestreview-645315412",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cfb75a82-d5a9-4a32-8a7e-4daac8936d9d",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Note that we don't append `\\n` for  this \"Most recent failure reason\" because `message` already contains it:\r\n\r\n```scala\r\nval message = s\"Stage failed because barrier task $task finished unsuccessfully.\\n\" +\r\n  failure.toErrorString\r\n```",
        "createdAt" : "2021-04-27T03:28:27Z",
        "updatedAt" : "2021-04-27T03:28:27Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "0896255f43c622e096cab6b0eee5bc4f715abc40",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +1952,1956 @@            } else {\n              s\"$failedStage (${failedStage.name}) has failed the maximum allowable number of \" +\n                s\"times: $maxConsecutiveStageAttempts. Most recent failure reason: $message\"\n            }\n            abortStage(failedStage, abortMessage, None)"
  },
  {
    "id" : "07661673-37f1-4ff2-91ac-a56f4cdcf4e9",
    "prId" : 31715,
    "prUrl" : "https://github.com/apache/spark/pull/31715#pullrequestreview-603642771",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e7039fba-bacb-4502-b438-a94232d3b931",
        "parentId" : null,
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "shouldn't we check this config to not take effect, if `spark.shuffle.manager` is `sort` (the default one)?",
        "createdAt" : "2021-03-04T02:20:21Z",
        "updatedAt" : "2021-03-04T02:20:21Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      }
    ],
    "commit" : "1418e8f9507375f4fec025d8df357a86cb99ee45",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +2036,2040 @@    // from a Standalone cluster, where the shuffle service lives in the Worker.)\n    val fileLost = (workerHost.isDefined || !env.blockManager.externalShuffleServiceEnabled) &&\n      env.blockManager.markFileLostOnExecutorLost\n    removeExecutorAndUnregisterOutputs(\n      execId = execId,"
  },
  {
    "id" : "e7433168-ebba-4150-b585-ee67e40f84cc",
    "prId" : 30716,
    "prUrl" : "https://github.com/apache/spark/pull/30716#pullrequestreview-568153899",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d5388bbe-2c21-4ef0-b8d4-54d3325e1af1",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "In the corner case, e.g., the rerun stage may only need to rerun one task and the task finished before this FetchFailure...which means the rerun stage attempt could be removed from `taskSetManagerForAttempt`...then, I don't think we should return `true` here..",
        "createdAt" : "2020-12-30T07:43:59Z",
        "updatedAt" : "2020-12-30T09:36:27Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "e7cfa4e7-40c1-4506-8045-2a28614b1adc",
        "parentId" : "d5388bbe-2c21-4ef0-b8d4-54d3325e1af1",
        "authorId" : "31afe32d-3af0-4fcf-93e2-115f5d7bab18",
        "body" : "Compared with before, I think this is acceptable, do you have any idea?",
        "createdAt" : "2021-01-14T11:39:31Z",
        "updatedAt" : "2021-01-14T11:39:31Z",
        "lastEditedBy" : "31afe32d-3af0-4fcf-93e2-115f5d7bab18",
        "tags" : [
        ]
      }
    ],
    "commit" : "8dbfdcbd2146daea48de3199b620594be404c363",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +266,270 @@          latestInfo.stageId, latestInfo.attemptNumber()) match {\n          case Some(tsm: TaskSetManager) => !tsm.hasPartitionId(fetchFailed.mapId.toInt)\n          case _ => true\n        }\n      }"
  },
  {
    "id" : "18ebf8fa-3038-4858-8625-9f5ed814756d",
    "prId" : 30716,
    "prUrl" : "https://github.com/apache/spark/pull/30716#pullrequestreview-559939984",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8229e7b0-aee0-4772-b8fc-b40a2f693ded",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "I think you actually want `fetchFailed.mapIndex` rather than `fetchFailed.mapId`?",
        "createdAt" : "2020-12-30T08:25:15Z",
        "updatedAt" : "2020-12-30T09:36:27Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "8dbfdcbd2146daea48de3199b620594be404c363",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +265,269 @@        taskScheduler.taskSetManagerForAttempt(\n          latestInfo.stageId, latestInfo.attemptNumber()) match {\n          case Some(tsm: TaskSetManager) => !tsm.hasPartitionId(fetchFailed.mapId.toInt)\n          case _ => true\n        }"
  },
  {
    "id" : "1b5cfa39-a85c-4582-8b0a-b58a45901d6e",
    "prId" : 30691,
    "prUrl" : "https://github.com/apache/spark/pull/30691#pullrequestreview-652435188",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d3849148-7486-4cb5-be37-cd87e1291990",
        "parentId" : null,
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "review note: no changes here. Method extracted from `handleTaskCompletion`",
        "createdAt" : "2021-05-05T15:43:23Z",
        "updatedAt" : "2021-05-11T05:07:03Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      }
    ],
    "commit" : "35d06150f37e01256314af4956c6e8fdcf7244b4",
    "line" : 292,
    "diffHunk" : "@@ -1,1 +2092,2096 @@  }\n\n  private def processShuffleMapStageCompletion(shuffleStage: ShuffleMapStage): Unit = {\n    markStageAsFinished(shuffleStage)\n    logInfo(\"looking for newly runnable stages\")"
  },
  {
    "id" : "502ba9b0-269d-4809-9749-c878d3396a05",
    "prId" : 30691,
    "prUrl" : "https://github.com/apache/spark/pull/30691#pullrequestreview-670858886",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2b1b1b68-d309-4213-a799-83ed005a07d0",
        "parentId" : null,
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "What happens if the stage was cancelled during `shuffleMergeFinalizeWaitSec` ?",
        "createdAt" : "2021-05-11T03:00:51Z",
        "updatedAt" : "2021-05-11T05:07:04Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "3b05c777-a9b3-4bd0-bc49-a8bd2fa58456",
        "parentId" : "2b1b1b68-d309-4213-a799-83ed005a07d0",
        "authorId" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "body" : "Addressed this comment after discussing with @mridulm offline. Mridul, I tried adding test for the cancellation but the `DAGSchedulerEventProcessLoopTester` is not async as it simply forwards the event for immediate processing. Let me know if you have other ideas to test this particular situation.",
        "createdAt" : "2021-05-25T18:57:02Z",
        "updatedAt" : "2021-05-25T18:57:02Z",
        "lastEditedBy" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "tags" : [
        ]
      },
      {
        "id" : "4d272a64-d565-4b97-bb3c-92ed50df097e",
        "parentId" : "2b1b1b68-d309-4213-a799-83ed005a07d0",
        "authorId" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "body" : "Added additional tests to handle the cases of stage cancellation, barrier stage, late arrival of merge results etc.",
        "createdAt" : "2021-05-28T01:19:20Z",
        "updatedAt" : "2021-05-28T01:19:20Z",
        "lastEditedBy" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "tags" : [
        ]
      }
    ],
    "commit" : "35d06150f37e01256314af4956c6e8fdcf7244b4",
    "line" : 246,
    "diffHunk" : "@@ -1,1 +2046,2050 @@   */\n  private[scheduler] def finalizeShuffleMerge(stage: ShuffleMapStage): Unit = {\n    logInfo(\"%s (%s) finalizing the shuffle merge\".format(stage, stage.name))\n    externalShuffleClient.foreach { shuffleClient =>\n      val shuffleId = stage.shuffleDep.shuffleId"
  },
  {
    "id" : "2ab4da93-7359-4e85-826d-07e59c9ae01b",
    "prId" : 30691,
    "prUrl" : "https://github.com/apache/spark/pull/30691#pullrequestreview-669853548",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dc440513-9907-416d-a4da-a6db4ab199fc",
        "parentId" : null,
        "authorId" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "body" : "@mridulm mentioned that one of the benefits of this way is that we can still accept and register merge statuses which are received after the stage gets finalized.\r\nBut here, we are ignoring these  statuses if shuffle merge is finalized. Why?\r\n In case of deterministic stage retries, accepting merge statuses after finalize is still okay, isn't it.",
        "createdAt" : "2021-05-25T19:31:50Z",
        "updatedAt" : "2021-05-25T19:53:54Z",
        "lastEditedBy" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "tags" : [
        ]
      },
      {
        "id" : "1cd447b9-044b-4e93-827c-0cbc3aa26355",
        "parentId" : "dc440513-9907-416d-a4da-a6db4ab199fc",
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "Let us do it as a follow up work, given the stage would have completed by the time that comes through.\r\nCan you add a TODO for this @venkata91 ?\r\n\r\nNote - the check `runningStages.contains(stage)` is not sufficient - as I detail below for `handleShuffleMergeFinalized`: the same condition applies here too.",
        "createdAt" : "2021-05-27T03:16:44Z",
        "updatedAt" : "2021-05-27T03:39:30Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "1d0298f4-cddc-435d-828f-9dd9cb0abcc2",
        "parentId" : "dc440513-9907-416d-a4da-a6db4ab199fc",
        "authorId" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "body" : "In that case, currently there is no need to even post these messages to `eventProcessLoop` after `ShuffleMergeFinalized` message is posted. \r\nSo should `RegisterMergeStatuses`  be posted only when `!timedOut.get()`?  Anyways, these messages are just getting ignored so why to even deserialize them.\r\n                   ",
        "createdAt" : "2021-05-27T07:52:45Z",
        "updatedAt" : "2021-05-27T07:52:45Z",
        "lastEditedBy" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "tags" : [
        ]
      }
    ],
    "commit" : "35d06150f37e01256314af4956c6e8fdcf7244b4",
    "line" : 329,
    "diffHunk" : "@@ -1,1 +2129,2133 @@    // TODO: SPARK-35549: Currently merge statuses results which come after shuffle merge\n    // TODO: is finalized is not registered.\n    if (runningStages.contains(stage) && !stage.shuffleDep.shuffleMergeFinalized) {\n      mapOutputTracker.registerMergeResults(stage.shuffleDep.shuffleId, mergeStatuses)\n    }"
  },
  {
    "id" : "6db4e3c7-0acc-419e-9b7d-5cbbc9f84273",
    "prId" : 30691,
    "prUrl" : "https://github.com/apache/spark/pull/30691#pullrequestreview-670858610",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "59d95cbd-8820-46a9-b422-c90d98c1fc81",
        "parentId" : null,
        "authorId" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "body" : "If the stage is not part of runningStages does it really mean that it was cancelled? \r\nIt seems like runningStages contain active stages. If a stage is completed successfully, it will not be part of runningStages. Why do we unregister all the mergeResults here ? \r\nAlso please add a comment that why we need to unregister all the merge results here",
        "createdAt" : "2021-05-25T19:53:36Z",
        "updatedAt" : "2021-05-25T19:53:54Z",
        "lastEditedBy" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "tags" : [
        ]
      },
      {
        "id" : "d8c973f3-9452-480b-90e7-164043ae034f",
        "parentId" : "59d95cbd-8820-46a9-b422-c90d98c1fc81",
        "authorId" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "body" : "Stage will be active until the shuffle merge is finalized and then only we are processing the map stage completion, isn't it? So if the stage is not part of running stages and we still reach the handling shuffle merge finalize, then we need to unregister the merge results, isn't it?\r\n\r\nCan you think of a scenario where stage is not part of running stages and still shuffle merge is finalized? - Ideally this should not happen.",
        "createdAt" : "2021-05-26T18:35:39Z",
        "updatedAt" : "2021-05-26T18:35:39Z",
        "lastEditedBy" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "tags" : [
        ]
      },
      {
        "id" : "98be8232-f36d-40ca-92a8-36037f9fddd3",
        "parentId" : "59d95cbd-8820-46a9-b422-c90d98c1fc81",
        "authorId" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "body" : "> Stage will be active until the shuffle merge is finalized and then only we are processing the map stage completion, isn't it? So if the stage is not part of running stages and we still reach the handling shuffle merge finalize, then we need to unregister the merge results, isn't it?\r\n\r\nAre you just making an assumption here, that if the stage is not running and then this finalized message is processed that means the stage is cancelled?  Is this a valid assumption?\r\nIf yes, then can you add a comment here. \r\n\r\n> Can you think of a scenario where stage is not part of running stages and still shuffle merge is finalized? - Ideally this should not happen.\r\n\r\nThis is what is throwing me off. If this is not ideally going to happen then why are we unregistering the results here. Again, if the assumption is that this happens when the stage was cancelled then document it. Also, if handling stage cancellation wrt merge finalization is not handled in this PR then why have this unregistration of merge results here?",
        "createdAt" : "2021-05-26T19:49:05Z",
        "updatedAt" : "2021-05-26T19:49:05Z",
        "lastEditedBy" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "tags" : [
        ]
      },
      {
        "id" : "93515e51-3bfc-4ab7-bdbd-541f64ffee83",
        "parentId" : "59d95cbd-8820-46a9-b422-c90d98c1fc81",
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "In scheduler, if a stage is in`runningStages` is the typical way in which we check if a stage is completed or not.\r\nOutside of scheduler, stage.latestInfo.failureReason is used to identify stage failure.\r\n\r\nHaving said that, latestInfo is overwritten when a new attempt starts.\r\n\r\nFor this specific case we had discussed (stage resubmission after a cancellation after initiating merge finalization), given a new stage attempt would result in the stage continuing to be in `runningStages` (but for newer attempt), the way to check would be:\r\n\r\na) Fetch `stage.latestInfo.attemptNumber()` in start of `scheduleShuffleMergeFinalize` (within dag scheduler event loop) - and pass this to finalizeShuffleMerge -> RegisterMergeStatuses and ShuffleMergeFinalized.\r\n\r\nIn handleShuffleMergeFinalized:\r\nb) If stage is not in runningStages (like currently being done), then not running - or\r\nc) If stage.latestInfo.attemptNumber() != ShuffleMergeFinalized.attemptNumber then attempt has changed, and finalization message.",
        "createdAt" : "2021-05-27T03:37:04Z",
        "updatedAt" : "2021-05-27T03:39:30Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "44ae1b3a-50ea-4b70-b9ef-a16abd7ba664",
        "parentId" : "59d95cbd-8820-46a9-b422-c90d98c1fc81",
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "On further thought ... my comment about attemptNumber is not valid.\r\nA new job will result in a new stage (since `stage` would have been removed from `shuffleIdToMapStage` and `runningStages` due to cancellation).\r\nA new attempt is scheduled typically when there are fetch failures in current job, and reexecution of parent is followed by re-execution of the stage - and so new attempt.\r\n\r\nI think the code looks fine as is.\r\n\r\n+CC @otterc ... thoughts ?",
        "createdAt" : "2021-05-27T04:20:58Z",
        "updatedAt" : "2021-05-27T04:21:59Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "c8e1b254-24b5-4c67-8430-7191ce4775ba",
        "parentId" : "59d95cbd-8820-46a9-b422-c90d98c1fc81",
        "authorId" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "body" : "@mridulm For the case that you pointed out\r\n> stage resubmission after a cancellation after initiating merge finalization\r\n\r\nThe problem that we were registering the mergeResults without a check and that would interfere with resubmission is solved here by not registering merge results if the stage is not running. Also, it makes sense to finalize the stage when it is running.\r\n\r\nHowever, shouldn't unregistering of merge results happen when there is a fetch failure or a stage is cancelled?\r\n\r\nBoth of the above are not part of this PR. So, I just want to understand in the context of this PR, why are we unregistering the merge results in the else part here and what does that help with?\r\n ```\r\n else {      mapOutputTracker.unregisterAllMergeResult(stage.shuffleDep.shuffleId)\r\n ```\r\n Also if there is a good reason to do it, then we should document it here as a comment.",
        "createdAt" : "2021-05-27T07:36:04Z",
        "updatedAt" : "2021-05-27T07:36:05Z",
        "lastEditedBy" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "tags" : [
        ]
      },
      {
        "id" : "a938300a-4c8c-4967-92df-419f33ab8843",
        "parentId" : "59d95cbd-8820-46a9-b422-c90d98c1fc81",
        "authorId" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "body" : "Discussed offline with @mridulm and currently there are few corner cases which needs to be carefully thought through before having this behavior. Created a TODO and a corresponding follow up JIRA - https://issues.apache.org/jira/browse/SPARK-35549",
        "createdAt" : "2021-05-28T01:18:30Z",
        "updatedAt" : "2021-05-28T01:18:30Z",
        "lastEditedBy" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "tags" : [
        ]
      }
    ],
    "commit" : "35d06150f37e01256314af4956c6e8fdcf7244b4",
    "line" : 342,
    "diffHunk" : "@@ -1,1 +2142,2146 @@      // Unregister all merge results if the stage is currently not\n      // active (i.e. the stage is cancelled)\n      mapOutputTracker.unregisterAllMergeResult(stage.shuffleDep.shuffleId)\n    }\n  }"
  },
  {
    "id" : "f8e7c7a5-5f50-4d68-b00a-b1944dbbac77",
    "prId" : 30691,
    "prUrl" : "https://github.com/apache/spark/pull/30691#pullrequestreview-673695267",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b2a7b269-91b6-4b3d-8e16-f90c0b121bf9",
        "parentId" : null,
        "authorId" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "body" : "@otterc one question which I couldn't understand from the internal fix is, should we pass `mapId` or `mapIndex` to the `unregisterMergeResult` here?",
        "createdAt" : "2021-06-01T17:31:15Z",
        "updatedAt" : "2021-06-01T17:31:15Z",
        "lastEditedBy" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "tags" : [
        ]
      },
      {
        "id" : "cd633651-c87a-41e1-9303-55b840b834d3",
        "parentId" : "b2a7b269-91b6-4b3d-8e16-f90c0b121bf9",
        "authorId" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "body" : "We need to pass `mapIndex`. The intention here is to unregister the mergeStatus if the block corresponding to the <shuffleId, mapIndex, reduceId> got merged for the reduce partition <shuffleId, reduceId>.  mergeStatus tracks mapIndex. \r\nAlso we can add a comment here that why are we doing this:\r\nWhen there is fetch failure for a block <shuffleId, mapIndex, reduceId> and if that block was merged to partition <shuffleId, reduceId>, it indicates that the iterator also failed to fetch the merged block for <shuffleId, reduceId>. So, we unregister the mergeResult for partition<shuffleId, reduceId> as there is no guarantee that the merged block for <shuffleId, reduceId> from the same blockManager will be successful the next time this stage is resubmitted.\r\n\r\nOn a side note, this was one of the reasons I thought this change should go with the fetch side change because it makes the explanation easier.",
        "createdAt" : "2021-06-01T19:40:08Z",
        "updatedAt" : "2021-06-01T21:44:37Z",
        "lastEditedBy" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "tags" : [
        ]
      },
      {
        "id" : "46a2b445-2411-4807-a081-dbd5d5e61c6b",
        "parentId" : "b2a7b269-91b6-4b3d-8e16-f90c0b121bf9",
        "authorId" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "body" : "@venkata91 I corrected the above response. Passing mapIndex is correct, because that is what we send in the ShuffleBlockPush message. ",
        "createdAt" : "2021-06-01T21:47:18Z",
        "updatedAt" : "2021-06-01T21:47:18Z",
        "lastEditedBy" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "tags" : [
        ]
      },
      {
        "id" : "ff439e51-11fd-4709-be9d-3518a7c438a3",
        "parentId" : "b2a7b269-91b6-4b3d-8e16-f90c0b121bf9",
        "authorId" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "body" : "Thanks. Added a comment to keep it clear as well.",
        "createdAt" : "2021-06-02T00:43:51Z",
        "updatedAt" : "2021-06-02T00:43:51Z",
        "lastEditedBy" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "tags" : [
        ]
      }
    ],
    "commit" : "35d06150f37e01256314af4956c6e8fdcf7244b4",
    "line" : 195,
    "diffHunk" : "@@ -1,1 +1763,1767 @@              // mapIndex is part of the merge result of <shuffleId, reduceId>\n              mapOutputTracker.\n                unregisterMergeResult(shuffleId, reduceId, bmAddress, Option(mapIndex))\n            }\n          }"
  },
  {
    "id" : "4fa3f504-bb0d-4422-8078-6733d9137b3b",
    "prId" : 30691,
    "prUrl" : "https://github.com/apache/spark/pull/30691#pullrequestreview-678961411",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3e365131-3d97-486a-9d8a-fe26ff92f8bb",
        "parentId" : null,
        "authorId" : "15ef9b5b-1583-47d9-b883-14ebfd23a00a",
        "body" : "So we decide to use a fixed number of threads instead of exposing a config?",
        "createdAt" : "2021-06-08T18:29:57Z",
        "updatedAt" : "2021-06-08T18:29:57Z",
        "lastEditedBy" : "15ef9b5b-1583-47d9-b883-14ebfd23a00a",
        "tags" : [
        ]
      },
      {
        "id" : "f0517b53-1eac-478a-a04d-91e5b0b11e58",
        "parentId" : "3e365131-3d97-486a-9d8a-fe26ff92f8bb",
        "authorId" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "body" : "Yes, I don't think we need a config as of now. In future, if needed we can always add a config.",
        "createdAt" : "2021-06-08T20:16:53Z",
        "updatedAt" : "2021-06-08T20:16:53Z",
        "lastEditedBy" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "tags" : [
        ]
      }
    ],
    "commit" : "35d06150f37e01256314af4956c6e8fdcf7244b4",
    "line" : 46,
    "diffHunk" : "@@ -1,1 +275,279 @@\n  private val shuffleMergeFinalizeScheduler =\n    ThreadUtils.newDaemonThreadPoolScheduledExecutor(\"shuffle-merge-finalizer\", 8)\n\n  /**"
  },
  {
    "id" : "46712c36-b58e-4e1d-8d55-99adeda82e17",
    "prId" : 30691,
    "prUrl" : "https://github.com/apache/spark/pull/30691#pullrequestreview-678893443",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a5ffbdc2-d19f-48c1-967d-12345ba9b5b7",
        "parentId" : null,
        "authorId" : "15ef9b5b-1583-47d9-b883-14ebfd23a00a",
        "body" : "Is it safe to put the second check into this assert?\r\nIt could be for submitting a retry of an already merge finalized map stage.",
        "createdAt" : "2021-06-08T18:49:28Z",
        "updatedAt" : "2021-06-08T18:49:28Z",
        "lastEditedBy" : "15ef9b5b-1583-47d9-b883-14ebfd23a00a",
        "tags" : [
        ]
      },
      {
        "id" : "f20a2ec1-9d96-48e2-a6e7-99d047850b51",
        "parentId" : "a5ffbdc2-d19f-48c1-967d-12345ba9b5b7",
        "authorId" : "15ef9b5b-1583-47d9-b883-14ebfd23a00a",
        "body" : "I see that this case is now handled outside of this method.\r\nYou can ignore this comment.",
        "createdAt" : "2021-06-08T18:52:30Z",
        "updatedAt" : "2021-06-08T18:52:30Z",
        "lastEditedBy" : "15ef9b5b-1583-47d9-b883-14ebfd23a00a",
        "tags" : [
        ]
      }
    ],
    "commit" : "35d06150f37e01256314af4956c6e8fdcf7244b4",
    "line" : 82,
    "diffHunk" : "@@ -1,1 +1297,1301 @@   */\n  private def prepareShuffleServicesForShuffleMapStage(stage: ShuffleMapStage): Unit = {\n    assert(stage.shuffleDep.shuffleMergeEnabled && !stage.shuffleDep.shuffleMergeFinalized)\n    if (stage.shuffleDep.getMergerLocs.isEmpty) {\n      val mergerLocs = sc.schedulerBackend.getShufflePushMergerLocations("
  },
  {
    "id" : "fa7449bd-e197-4af8-8164-1304f35d69df",
    "prId" : 30691,
    "prUrl" : "https://github.com/apache/spark/pull/30691#pullrequestreview-679095366",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a0607dc0-ce5c-4083-b891-ca3856c69bd3",
        "parentId" : null,
        "authorId" : "15ef9b5b-1583-47d9-b883-14ebfd23a00a",
        "body" : "If we make this change, is SPARK-32923 (for properly handling indeterminate stage retries) still needed as part of SPARK-30602?\r\nThis will always recompute all partitions.\r\nShould we also reset the other metadata here, such as resetting `sms.shuffleDep.shuffleMergeEnabled`?\r\nThis way it would make sure that the later invocation to `prepareShuffleServicesForShuffleMapStage` would not be interfered from the previous attempt of this indeterminate stage.",
        "createdAt" : "2021-06-08T18:50:34Z",
        "updatedAt" : "2021-06-08T18:50:34Z",
        "lastEditedBy" : "15ef9b5b-1583-47d9-b883-14ebfd23a00a",
        "tags" : [
        ]
      },
      {
        "id" : "17366aa5-fb75-4c56-84c9-5ddcf96d22cf",
        "parentId" : "a0607dc0-ce5c-4083-b891-ca3856c69bd3",
        "authorId" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "body" : "Made a comment here to handle clean up of shuffle merge metadata as part of handling SPARK-32923 (non-deterministic stage retries) and in SPARK-35547 (handling barrier execution mode)",
        "createdAt" : "2021-06-08T23:47:28Z",
        "updatedAt" : "2021-06-08T23:47:28Z",
        "lastEditedBy" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "tags" : [
        ]
      }
    ],
    "commit" : "35d06150f37e01256314af4956c6e8fdcf7244b4",
    "line" : 107,
    "diffHunk" : "@@ -1,1 +1326,1330 @@        // TODO: SPARK-32923: Clean all push-based shuffle metadata like merge enabled and\n        // TODO: finalized as we are clearing all the merge results.\n        mapOutputTracker.unregisterAllMapAndMergeOutput(sms.shuffleDep.shuffleId)\n      case _ =>\n    }"
  },
  {
    "id" : "353d556b-b473-4c6b-9d05-37993d9ca130",
    "prId" : 30691,
    "prUrl" : "https://github.com/apache/spark/pull/30691#pullrequestreview-679096983",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "55689607-5c08-4f84-9eb5-bb3299469c29",
        "parentId" : null,
        "authorId" : "15ef9b5b-1583-47d9-b883-14ebfd23a00a",
        "body" : "Similar to the handling of indeterminate stage retry, should we also reset other metadata here?",
        "createdAt" : "2021-06-08T19:10:46Z",
        "updatedAt" : "2021-06-08T19:10:46Z",
        "lastEditedBy" : "15ef9b5b-1583-47d9-b883-14ebfd23a00a",
        "tags" : [
        ]
      },
      {
        "id" : "2f117bbd-7908-426a-98bd-023ed0d6086f",
        "parentId" : "55689607-5c08-4f84-9eb5-bb3299469c29",
        "authorId" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "body" : "Same as above, added a comment here to handle it as part of handling barrier execution mode handling.",
        "createdAt" : "2021-06-08T23:51:38Z",
        "updatedAt" : "2021-06-08T23:51:39Z",
        "lastEditedBy" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "tags" : [
        ]
      }
    ],
    "commit" : "35d06150f37e01256314af4956c6e8fdcf7244b4",
    "line" : 187,
    "diffHunk" : "@@ -1,1 +1755,1759 @@            // TODO: SPARK-35547: Clean all push-based shuffle metadata like merge enabled and\n            // TODO: finalized as we are clearing all the merge results.\n            mapOutputTracker.unregisterAllMapAndMergeOutput(shuffleId)\n          } else if (mapIndex != -1) {\n            // Mark the map whose fetch failed as broken in the map stage"
  },
  {
    "id" : "c902f171-824f-476a-827c-de088ee9d588",
    "prId" : 30691,
    "prUrl" : "https://github.com/apache/spark/pull/30691#pullrequestreview-678944960",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d12ccf81-5867-4959-bce3-5cc3d1161467",
        "parentId" : null,
        "authorId" : "15ef9b5b-1583-47d9-b883-14ebfd23a00a",
        "body" : "By removing the `timedOut` field, we will always register MergeStatus even after the timeout.\r\nThe original code was also doing the same, but it was easier to fix this there.\r\nSo SPARK-35549 will address this?",
        "createdAt" : "2021-06-08T19:49:08Z",
        "updatedAt" : "2021-06-08T19:49:08Z",
        "lastEditedBy" : "15ef9b5b-1583-47d9-b883-14ebfd23a00a",
        "tags" : [
        ]
      },
      {
        "id" : "13cb14fd-0188-413d-9696-35cd68aa386f",
        "parentId" : "d12ccf81-5867-4959-bce3-5cc3d1161467",
        "authorId" : "15ef9b5b-1583-47d9-b883-14ebfd23a00a",
        "body" : "Ignore this comment.\r\nI see that you already handle this during `handleRegisterMergeStatuses` by checking if the shuffle is already merge finalized.",
        "createdAt" : "2021-06-08T19:56:15Z",
        "updatedAt" : "2021-06-08T19:56:15Z",
        "lastEditedBy" : "15ef9b5b-1583-47d9-b883-14ebfd23a00a",
        "tags" : [
        ]
      }
    ],
    "commit" : "35d06150f37e01256314af4956c6e8fdcf7244b4",
    "line" : 251,
    "diffHunk" : "@@ -1,1 +2051,2055 @@      val numMergers = stage.shuffleDep.getMergerLocs.length\n      val results = (0 until numMergers).map(_ => SettableFuture.create[Boolean]())\n\n      stage.shuffleDep.getMergerLocs.zipWithIndex.foreach {\n        case (shuffleServiceLoc, index) =>"
  },
  {
    "id" : "1810bc64-f85a-40b5-97aa-8c45adf42f80",
    "prId" : 30691,
    "prUrl" : "https://github.com/apache/spark/pull/30691#pullrequestreview-678955416",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "63b83d26-f9b0-4a44-8a86-7dbddc358d2f",
        "parentId" : null,
        "authorId" : "15ef9b5b-1583-47d9-b883-14ebfd23a00a",
        "body" : "Could you help me to understand why we need SPARK-35549?\r\nWhy do we want to register late MergeStatus?",
        "createdAt" : "2021-06-08T20:00:17Z",
        "updatedAt" : "2021-06-08T20:00:17Z",
        "lastEditedBy" : "15ef9b5b-1583-47d9-b883-14ebfd23a00a",
        "tags" : [
        ]
      },
      {
        "id" : "6593970f-32c9-4b57-9a05-cfb7748a669a",
        "parentId" : "63b83d26-f9b0-4a44-8a86-7dbddc358d2f",
        "authorId" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "body" : "Yes, that is correct. That would help in getting better overall merge ratio eventually right?",
        "createdAt" : "2021-06-08T20:09:14Z",
        "updatedAt" : "2021-06-08T20:09:15Z",
        "lastEditedBy" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "tags" : [
        ]
      }
    ],
    "commit" : "35d06150f37e01256314af4956c6e8fdcf7244b4",
    "line" : 327,
    "diffHunk" : "@@ -1,1 +2127,2131 @@      mergeStatuses: Seq[(Int, MergeStatus)]): Unit = {\n    // Register merge statuses if the stage is still running and shuffle merge is not finalized yet.\n    // TODO: SPARK-35549: Currently merge statuses results which come after shuffle merge\n    // TODO: is finalized is not registered.\n    if (runningStages.contains(stage) && !stage.shuffleDep.shuffleMergeFinalized) {"
  }
]