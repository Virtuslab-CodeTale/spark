[
  {
    "id" : "2e7e7d1f-da77-4380-97b1-aecd1c76edcb",
    "prId" : 33064,
    "prUrl" : "https://github.com/apache/spark/pull/33064#pullrequestreview-692411886",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ece72f02-4946-4e2a-aef4-bfb1c12a360e",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Two more spaces.",
        "createdAt" : "2021-06-25T03:01:43Z",
        "updatedAt" : "2021-06-25T03:01:43Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "4116751df2f413e8da4130e704e996811b91d7e0",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +498,502 @@    // later Hadoop releases\n    if (hadoopConf.get(\"fs.s3a.endpoint\", \"\").isEmpty &&\n      hadoopConf.get(\"fs.s3a.endpoint.region\") == null) {\n      // set to US central endpoint which can also connect to buckets\n      // in other regions at the expense of a HEAD request during fs creation"
  },
  {
    "id" : "76b93bd9-5039-4e4d-9e64-c6c5422060b1",
    "prId" : 33064,
    "prUrl" : "https://github.com/apache/spark/pull/33064#pullrequestreview-692828961",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "096fd808-b90b-48d8-8330-a23cad965a3c",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "We can use `AWS_REGION` as a workaround, too.",
        "createdAt" : "2021-06-25T12:21:59Z",
        "updatedAt" : "2021-06-25T12:21:59Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "066ea420-cbf1-483c-b9cc-cc054b286fe1",
        "parentId" : "096fd808-b90b-48d8-8330-a23cad965a3c",
        "authorId" : "224991ce-ad69-410b-8143-bf394b6b5c59",
        "body" : "that's automatically picked up in the default chain, but since you have to set it everywhere it's not something you can rely on. ",
        "createdAt" : "2021-06-25T13:39:01Z",
        "updatedAt" : "2021-06-25T13:39:02Z",
        "lastEditedBy" : "224991ce-ad69-410b-8143-bf394b6b5c59",
        "tags" : [
        ]
      }
    ],
    "commit" : "4116751df2f413e8da4130e704e996811b91d7e0",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +493,497 @@    // In Hadoop 3.3.1, AWS region handling with the default \"\" endpoint only works\n    // in EC2 deployments or when the AWS CLI is installed.\n    // The workaround is to set the name of the S3 endpoint explicitly,\n    // if not already set. See HADOOP-17771.\n    // This change is harmless on older versions and compatible with"
  },
  {
    "id" : "00881480-6341-4d56-80a2-9d23fd64028b",
    "prId" : 31460,
    "prUrl" : "https://github.com/apache/spark/pull/31460#pullrequestreview-583140545",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9209ad35-cf37-4851-b6bc-503afc564d4a",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Hi, @MaxGekk . According to your email, was this the only property affected?",
        "createdAt" : "2021-02-03T17:58:54Z",
        "updatedAt" : "2021-02-04T02:17:18Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "c9a8b89f-43b7-4661-a73e-fe06cbcb76e0",
        "parentId" : "9209ad35-cf37-4851-b6bc-503afc564d4a",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "@dongjoon-hyun Yes.",
        "createdAt" : "2021-02-04T08:30:55Z",
        "updatedAt" : "2021-02-04T08:30:55Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      }
    ],
    "commit" : "58a532e4e0ebaeb6f351fa347d1ce4da90d2b1e5",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +455,459 @@      appendSparkHiveConfigs(conf, hadoopConf)\n      val bufferSize = conf.get(BUFFER_SIZE).toString\n      hadoopConf.set(\"io.file.buffer.size\", bufferSize)\n    }\n  }"
  },
  {
    "id" : "acf43e9a-1d7e-4c07-afcb-d88f25b6d9cf",
    "prId" : 29895,
    "prUrl" : "https://github.com/apache/spark/pull/29895#pullrequestreview-498107972",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d8c69e8d-6dc7-42e0-8ae2-c623e09adb3e",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "What about we just use `hadoopConf.setIfUnset`?",
        "createdAt" : "2020-09-29T00:24:30Z",
        "updatedAt" : "2020-09-29T00:24:30Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "2d6d0f4f-8ff9-4be4-b8c1-82d7662a53e9",
        "parentId" : "d8c69e8d-6dc7-42e0-8ae2-c623e09adb3e",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Thanks for review, but we cannot use `org.apache.hadoop.conf.Configuration.setIfUnset` because Apache Hadoop already loads the default value of `mapreduce.fileoutputcommitter.algorithm.version`, @HyukjinKwon .",
        "createdAt" : "2020-09-29T05:04:28Z",
        "updatedAt" : "2020-09-29T05:09:27Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "75ee9752-9057-4114-9fb6-b8fb33071b34",
        "parentId" : "d8c69e8d-6dc7-42e0-8ae2-c623e09adb3e",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Gotya",
        "createdAt" : "2020-09-29T05:34:44Z",
        "updatedAt" : "2020-09-29T05:34:44Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "1d4d3ea719a7bb21e3757ad6e163145140cbef61",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +464,468 @@    }\n    if (conf.getOption(\"spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version\").isEmpty) {\n      hadoopConf.set(\"mapreduce.fileoutputcommitter.algorithm.version\", \"1\")\n    }\n  }"
  },
  {
    "id" : "30ad63bf-19e1-4034-bc80-e09604e861ae",
    "prId" : 27222,
    "prUrl" : "https://github.com/apache/spark/pull/27222#pullrequestreview-357165820",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bbc58194-8a48-4dd6-9996-efa3eda95517",
        "parentId" : null,
        "authorId" : "39fe625f-8c54-48bc-ac6f-7279921adf02",
        "body" : "is \"HADOOP_USER_NAME\" constant defined?",
        "createdAt" : "2020-01-27T22:08:07Z",
        "updatedAt" : "2020-01-27T22:08:07Z",
        "lastEditedBy" : "39fe625f-8c54-48bc-ac6f-7279921adf02",
        "tags" : [
        ]
      },
      {
        "id" : "5fad6fef-035f-472c-9784-7dad7c91e5c7",
        "parentId" : "bbc58194-8a48-4dd6-9996-efa3eda95517",
        "authorId" : "d8f7c73a-630d-4cfa-a1c5-997b340a2cd1",
        "body" : "Yes, it's specified by spark.executorEnv.HADOOP_USER_NAME and all executors will use it",
        "createdAt" : "2020-02-12T03:17:56Z",
        "updatedAt" : "2020-02-12T03:20:32Z",
        "lastEditedBy" : "d8f7c73a-630d-4cfa-a1c5-997b340a2cd1",
        "tags" : [
        ]
      }
    ],
    "commit" : "7c8d8e307750d1cf47b635302af845356cbf4d54",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +65,69 @@\n  def createSparkUser(): UserGroupInformation = {\n    val user = Option(System.getenv(\"HADOOP_USER_NAME\")).getOrElse(Utils.getCurrentUserName())\n    logDebug(\"creating UGI for user: \" + user)\n    val ugi = UserGroupInformation.createRemoteUser(user)"
  }
]