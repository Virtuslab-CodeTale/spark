[
  {
    "id" : "54e432e7-c670-4b76-b72a-7cf5e4d31745",
    "prId" : 26682,
    "prUrl" : "https://github.com/apache/spark/pull/26682#pullrequestreview-352602337",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "caaadd6c-f271-4ade-942b-10d439826ef5",
        "parentId" : null,
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "I know this is not part of this PR, but shouldn't discoveryScript not be Option[String] instead of String with default \"\" ?",
        "createdAt" : "2020-01-26T08:03:33Z",
        "updatedAt" : "2020-01-26T08:51:12Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "1e422010-9142-4bb5-9f19-53ca3fecd70d",
        "parentId" : "caaadd6c-f271-4ade-942b-10d439826ef5",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "I specifically made it \"\" instead of Option for the Java API compatibility rather than creating separate api's.  Did you have other suggestions?",
        "createdAt" : "2020-01-27T15:48:00Z",
        "updatedAt" : "2020-01-27T15:48:00Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "ebe3afa5-6e42-4b55-8155-b9ea32aaa73c",
        "parentId" : "caaadd6c-f271-4ade-942b-10d439826ef5",
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "That is a good point. What about using Optional instead then ?",
        "createdAt" : "2020-01-27T21:46:06Z",
        "updatedAt" : "2020-01-27T21:46:07Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "23a3d5d8-0e8d-4bf8-90b6-9a4cb40172db",
        "parentId" : "caaadd6c-f271-4ade-942b-10d439826ef5",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "yes should be able to. I wasn't sure if the Scala side would get annoyed with having to use java object as parameter, or more so that spark had any api's that did that. Maybe what I'll do is change ExecutorResourceRequest which I don't expect the user to wield directly as much to Optional and then provide 2 different resource() api's here,  one that take java Optional and one that takes the scala Option and convert to Optional before creating the ExecutorResourceRequest, just as a convenience thing for scala users.  thoughts?  ",
        "createdAt" : "2020-01-27T22:22:09Z",
        "updatedAt" : "2020-01-27T22:22:10Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "935ba9b2-5075-42fb-8fa7-1b353afa9b5f",
        "parentId" : "caaadd6c-f271-4ade-942b-10d439826ef5",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "actually I looked at Optional and one issue is its not serializable. so that would make things a bit of a hassle.",
        "createdAt" : "2020-02-03T22:15:46Z",
        "updatedAt" : "2020-02-03T22:15:46Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "75091102d6bd64a61fb903c8df3edf9ca047e879",
    "line" : 33,
    "diffHunk" : "@@ -1,1 +54,58 @@    val resourceName: String,\n    val amount: Long,\n    val discoveryScript: String = \"\",\n    val vendor: String = \"\") extends Serializable {\n"
  },
  {
    "id" : "3a785ca7-fd47-42e4-8ba1-223757fa4919",
    "prId" : 26284,
    "prUrl" : "https://github.com/apache/spark/pull/26284#pullrequestreview-317886763",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a9be10e5-dfb4-4515-a5db-1cbc3a33f8ea",
        "parentId" : null,
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Maybe just add a comment here that this is just for the Spark internal resources, and GPUs & FPGAs and everything inside inside of `resource.` is allowed?",
        "createdAt" : "2019-11-15T17:27:44Z",
        "updatedAt" : "2019-11-15T22:25:46Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "f6fffcee-2f5b-4f20-be76-ced87849f547",
        "parentId" : "a9be10e5-dfb4-4515-a5db-1cbc3a33f8ea",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "sure, will update",
        "createdAt" : "2019-11-15T21:46:46Z",
        "updatedAt" : "2019-11-15T22:25:47Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "246de3c66c8c5a656b766ccd2f9bb3df12c9da0d",
    "line" : 68,
    "diffHunk" : "@@ -1,1 +66,70 @@  // A list of allowed Spark internal resources. Custom resources (spark.executor.resource.*)\n  // like GPUs/FPGAs are also allowed, see the check below.\n  private val allowedExecutorResources = mutable.HashSet[String](\n    ResourceProfile.MEMORY,\n    ResourceProfile.OVERHEAD_MEM,"
  },
  {
    "id" : "ef80b526-ba53-4e23-ad4b-d50e535aaca5",
    "prId" : 26284,
    "prUrl" : "https://github.com/apache/spark/pull/26284#pullrequestreview-317885687",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ed01d042-6b99-4abe-a821-99dfa98a52ec",
        "parentId" : null,
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "From looking at the design I thin the discoveryScript seems to need to be resident on the executors already, maybe call that out since another option would have been to broadcast the discovery script.",
        "createdAt" : "2019-11-15T17:30:18Z",
        "updatedAt" : "2019-11-15T22:25:46Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "09978025-b6ab-4662-8038-9d23c43a1e2d",
        "parentId" : "ed01d042-6b99-4abe-a821-99dfa98a52ec",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "it depends on the cluster manager. yarn doesn't need it resident as it will ship it and it will be there before startup, standalone mode does.  You can't broadcast it as it needs to be there at process startup unless I'm missing what you are saying",
        "createdAt" : "2019-11-15T21:44:19Z",
        "updatedAt" : "2019-11-15T22:25:47Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "246de3c66c8c5a656b766ccd2f9bb3df12c9da0d",
    "line" : 51,
    "diffHunk" : "@@ -1,1 +49,53 @@ * @param resourceName Name of the resource\n * @param amount Amount requesting\n * @param discoveryScript Optional script used to discover the resources. This is required on some\n *                        cluster managers that don't tell Spark the addresses of the resources\n *                        allocated. The script runs on Executors startup to discover the addresses"
  }
]