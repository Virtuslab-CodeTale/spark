[
  {
    "id" : "40f63446-5f6a-4ef4-9fd1-88b1b951d580",
    "prId" : 31761,
    "prUrl" : "https://github.com/apache/spark/pull/31761#pullrequestreview-605807459",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c88ce54d-b01a-4107-bc90-578f26ed63e0",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Since this excludes all file systems on that host, shall we revise the description in line 107?",
        "createdAt" : "2021-03-06T15:48:26Z",
        "updatedAt" : "2021-03-22T18:58:50Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "e91f5a2b-2f92-4519-b061-4278faeae37d",
        "parentId" : "c88ce54d-b01a-4107-bc90-578f26ed63e0",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "`The hosts on which the file systems to be excluded from token renewal`?",
        "createdAt" : "2021-03-06T18:03:48Z",
        "updatedAt" : "2021-03-22T18:58:50Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "69c375dc-9838-4d8f-8446-5d61c2354e95",
        "parentId" : "c88ce54d-b01a-4107-bc90-578f26ed63e0",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Ya. Maybe. \r\nBTW, we are accept `hadoopFileSystems` via 'spark.kerberos.renewal.exclude.hadoopFileSystem' and convert it to hosts?\r\nCould you describe some examples where we cannot accept `hosts` directly?",
        "createdAt" : "2021-03-07T01:05:09Z",
        "updatedAt" : "2021-03-22T18:58:50Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "3a91f17a-cb9d-4071-bb41-9f5f2e82e32c",
        "parentId" : "c88ce54d-b01a-4107-bc90-578f26ed63e0",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "I'm also wondering why unlike the MR config we don't use hosts directly but rather URLs.",
        "createdAt" : "2021-03-07T01:26:37Z",
        "updatedAt" : "2021-03-22T18:58:50Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "3ca7ef09-df28-426e-96b1-85446fcc3c93",
        "parentId" : "c88ce54d-b01a-4107-bc90-578f26ed63e0",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "No, we can accept hosts directly. It doesn't matter here. Either URL or host can achieve the same result we want.\r\n\r\nIt takes file system URLs because I let it to be consistent with other configs like `KERBEROS_FILESYSTEMS_TO_ACCESS`.\r\n\r\n",
        "createdAt" : "2021-03-07T04:09:36Z",
        "updatedAt" : "2021-03-22T18:58:50Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "cf8da757045d23f5657ec1fb68e20f4ca0247772",
    "line" : 25,
    "diffHunk" : "@@ -1,1 +109,113 @@\n    filesystems.foreach { fs =>\n      if (fsToExclude.contains(fs.getUri.getHost)) {\n        // YARN RM skips renewing token with empty renewer\n        logInfo(s\"getting token for: $fs with empty renewer to skip renewal\")"
  },
  {
    "id" : "cd4a1877-3a28-42e2-856a-64f0db3c81c0",
    "prId" : 31761,
    "prUrl" : "https://github.com/apache/spark/pull/31761#pullrequestreview-606050250",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "22c97c8c-24f7-4ebf-8f8c-2e692973f29a",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Just curious, suppose we have a fs which we exclude the renewal, then my understanding on behavior is it fetches the delegation token but won't renew. Do I understand correctly? Just curious about how these credentials will be handled from here, like whether they're not `AbstractDelegationTokenIdentifier`, or `token.renew` fails but Try will swallow.\r\n",
        "createdAt" : "2021-03-07T22:45:17Z",
        "updatedAt" : "2021-03-22T18:58:50Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "c1170126-28eb-40ae-bc4e-3c488f2f4c6f",
        "parentId" : "22c97c8c-24f7-4ebf-8f8c-2e692973f29a",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "The customer uses spark-submit to submit Spark job to YARN, I think the customer already obtains delegation token for the remote HDFS. YARN will try to renew the delegation token when we submit job to it, but the YARN cluster is not able to renew it and fails.",
        "createdAt" : "2021-03-08T08:19:55Z",
        "updatedAt" : "2021-03-22T18:58:50Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "cf8da757045d23f5657ec1fb68e20f4ca0247772",
    "line" : 43,
    "diffHunk" : "@@ -1,1 +134,138 @@    fetchDelegationTokens(renewer, filesystems, creds, Set.empty)\n\n    val renewIntervals = creds.getAllTokens.asScala.filter {\n      _.decodeIdentifier().isInstanceOf[AbstractDelegationTokenIdentifier]\n    }.flatMap { token =>"
  },
  {
    "id" : "d791500a-21dd-4285-9586-ea9e53d738fc",
    "prId" : 31761,
    "prUrl" : "https://github.com/apache/spark/pull/31761#pullrequestreview-608582347",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "35036145-2abe-46a0-aea9-eeea19318249",
        "parentId" : null,
        "authorId" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "body" : "This is not required here but lately I've seen that `$fs` produces a really cryptic out (which was 0 help in debugging) and maybe we can print out the `Uri` or something more meaningful. But again, this is a nice to have separately.",
        "createdAt" : "2021-03-10T10:59:06Z",
        "updatedAt" : "2021-03-22T18:58:50Z",
        "lastEditedBy" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "tags" : [
        ]
      }
    ],
    "commit" : "cf8da757045d23f5657ec1fb68e20f4ca0247772",
    "line" : 30,
    "diffHunk" : "@@ -1,1 +114,118 @@        fs.addDelegationTokens(\"\", creds)\n      } else {\n        logInfo(s\"getting token for: $fs with renewer $renewer\")\n        fs.addDelegationTokens(renewer, creds)\n      }"
  },
  {
    "id" : "aaa021bc-c83c-47eb-95dc-31a386328a94",
    "prId" : 30366,
    "prUrl" : "https://github.com/apache/spark/pull/30366#pullrequestreview-534963746",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f90754b0-402f-4a71-b2e1-ebcf4e34d780",
        "parentId" : null,
        "authorId" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "body" : "Since others asked for tests maybe we can make this package private and add positive and negative cases (hitting both side of the if condition). I personally don't insist to this.",
        "createdAt" : "2020-11-19T10:51:50Z",
        "updatedAt" : "2020-11-30T02:08:35Z",
        "lastEditedBy" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "tags" : [
        ]
      },
      {
        "id" : "10377299-1cb9-435e-b2da-0106fec3ddec",
        "parentId" : "f90754b0-402f-4a71-b2e1-ebcf4e34d780",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "The reason I was negative on adding test is, as we know how thing was broken, we basically expect the test to simulate it and verify it's no longer failing. e.g. testing `obtainDelegationTokens()`. `obtainDelegationTokens()` calls various private methods which call Hadoop API, hence requiring mocks for everything. This is actually an existing debt, and non-trivial to pay off, compared to what I did for the fix.\r\n\r\nI'm totally OK to add test verifying this method, as it only requires me to mock AbstractDelegationTokenIdentifier. I'm not sure this fits the needs of adding test, though.",
        "createdAt" : "2020-11-19T11:55:44Z",
        "updatedAt" : "2020-11-30T02:08:35Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "43932e46-89a1-43fb-8860-a4f5d11a5dd3",
        "parentId" : "f90754b0-402f-4a71-b2e1-ebcf4e34d780",
        "authorId" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "body" : "That said I don't insist on the test because I agree with you. Just wanted to come up with some coverage. Creating a normal test with security manager + KDC + providers + hadoop sever would require a brain surgery complicated code (I've tried it before in the Kafka area but it was not a success story).\r\n\r\n@dongjoon-hyun WDYT?\r\n",
        "createdAt" : "2020-11-19T13:27:06Z",
        "updatedAt" : "2020-11-30T02:08:35Z",
        "lastEditedBy" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "tags" : [
        ]
      },
      {
        "id" : "8f04f207-36f5-4965-a6ff-a4d5a39f11bc",
        "parentId" : "f90754b0-402f-4a71-b2e1-ebcf4e34d780",
        "authorId" : "224991ce-ad69-410b-8143-bf394b6b5c59",
        "body" : "You don't need a security manager for fetching DTs of filesystems or renewing. Example: the S3A ones can be issued if you have AWS creds. It's just that all code written today assumes that filesystems & services only issue DTs if user is kerberized or was launched with an existing set of creds. Example: how to collect a DT to talk to the landsat bucket for the next 12 hours\r\n\r\n```\r\nbin/hdfs fetchdt -D fs.s3a.delegation.token.binding=org.apache.hadoop.fs.s3a.auth.delegation.SessionTokenBinding --webservice s3a://landsat-pds/ secrets.bin\r\n```\r\n\r\nRenewal is a different bit of work tho'. It would get complicated fast",
        "createdAt" : "2020-11-19T15:11:32Z",
        "updatedAt" : "2020-11-30T02:08:35Z",
        "lastEditedBy" : "224991ce-ad69-410b-8143-bf394b6b5c59",
        "tags" : [
        ]
      },
      {
        "id" : "fc373114-ec81-4978-a954-06a54f38b335",
        "parentId" : "f90754b0-402f-4a71-b2e1-ebcf4e34d780",
        "authorId" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "body" : "What I've learned from past things is depending on external executables is not always a good decision. This tool must be available on all OS and must behave the exact same way otherwise the test will be pain in the ass.",
        "createdAt" : "2020-11-19T15:21:21Z",
        "updatedAt" : "2020-11-30T02:08:35Z",
        "lastEditedBy" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "tags" : [
        ]
      },
      {
        "id" : "9ea31a36-033e-4f5d-b054-8e9904498f8a",
        "parentId" : "f90754b0-402f-4a71-b2e1-ebcf4e34d780",
        "authorId" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "body" : "Maybe this is the case here but I'm always a little afraid when this pops up.",
        "createdAt" : "2020-11-19T15:23:10Z",
        "updatedAt" : "2020-11-30T02:08:35Z",
        "lastEditedBy" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "tags" : [
        ]
      },
      {
        "id" : "768391d3-ca7d-4108-8ab6-a510a84db479",
        "parentId" : "f90754b0-402f-4a71-b2e1-ebcf4e34d780",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "This discussion shows how hard to deal with test obtainDelegationTokens(). I hope any hero jumps in and deals with this.\r\n\r\nAnother approach would be changing all private methods to protected methods and fake via subclassing (say, not calling Hadoop API whenever possible), but we lose the ability to test the original methods we override, as well as we modify the code just because we need to add tests. A bit odd rationalization.",
        "createdAt" : "2020-11-19T22:49:49Z",
        "updatedAt" : "2020-11-30T02:08:35Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "0012b582-83bb-419e-a780-bae159088efb",
        "parentId" : "f90754b0-402f-4a71-b2e1-ebcf4e34d780",
        "authorId" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "body" : "There is a pretty good reason behind why only sub-parts of token providers are tested, please see Kafka for example...\r\nI still say test getIssueDate.",
        "createdAt" : "2020-11-19T23:05:43Z",
        "updatedAt" : "2020-11-30T02:08:35Z",
        "lastEditedBy" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "tags" : [
        ]
      },
      {
        "id" : "9b8a56e8-1203-44b9-a739-f763a4f33bdf",
        "parentId" : "f90754b0-402f-4a71-b2e1-ebcf4e34d780",
        "authorId" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "body" : "If you still need the pain maybe you can instantiate the provider only and try to mock HadoopFSDelegationTokenProvider.hadoopFSsToAccess with powermock and generate a faulty token. That could lead far.",
        "createdAt" : "2020-11-19T23:07:51Z",
        "updatedAt" : "2020-11-30T02:08:35Z",
        "lastEditedBy" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "tags" : [
        ]
      },
      {
        "id" : "e10ab002-9d8b-4e98-b044-8df4f7bb2891",
        "parentId" : "f90754b0-402f-4a71-b2e1-ebcf4e34d780",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "```\r\n  private def getIssueDate(kind: String, identifier: AbstractDelegationTokenIdentifier): Long = {\r\n    val issueDate = identifier.getIssueDate\r\n    if (issueDate > 0L) {\r\n      issueDate\r\n    } else {\r\n      val now = System.currentTimeMillis()\r\n      logWarning(s\"Token $kind has not set up issue date properly. (provided: $issueDate) \" +\r\n        s\"Using current timestamp ($now) as issue date instead. Consult token implementor to fix \" +\r\n        \"the behavior.\")\r\n      now\r\n    }\r\n  }\r\n```\r\n\r\nThis doesn't seem to be something we would really want to have tests. Unless we separate out the calculation logic and try to craft tests, I don't see any option to have tests for this class.",
        "createdAt" : "2020-11-20T00:02:21Z",
        "updatedAt" : "2020-11-30T02:08:35Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "b749574fb2b3d11037c4ff48a80910d63aa01455",
    "line" : 25,
    "diffHunk" : "@@ -1,1 +137,141 @@  }\n\n  private def getIssueDate(kind: String, identifier: AbstractDelegationTokenIdentifier): Long = {\n    val now = System.currentTimeMillis()\n    val issueDate = identifier.getIssueDate"
  }
]