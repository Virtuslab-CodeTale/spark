[
  {
    "id" : "666f1bb2-ef80-4f0d-a875-ae12829676cf",
    "prId" : 32530,
    "prUrl" : "https://github.com/apache/spark/pull/32530#pullrequestreview-659313592",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5f8e2bc3-ca1c-48c9-bcdd-365984ac665b",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "do we need to do the rename when `dynamicPartitionOverwrite == false`?",
        "createdAt" : "2021-05-13T13:35:32Z",
        "updatedAt" : "2021-05-13T13:35:32Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "08f10df3-883b-411e-9927-52bc97f71fef",
        "parentId" : "5f8e2bc3-ca1c-48c9-bcdd-365984ac665b",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I'm asking because this is different from https://github.com/apache/spark/pull/32207/files#diff-714f288ff8f97acca2fc3449e005d4abcab4a5d95883498652f507fa42a92a6aR191",
        "createdAt" : "2021-05-13T13:36:53Z",
        "updatedAt" : "2021-05-13T13:36:53Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "0fbd1216-6890-4edc-97c2-37b02225fbc1",
        "parentId" : "5f8e2bc3-ca1c-48c9-bcdd-365984ac665b",
        "authorId" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "body" : "I believe we do actually need the rename here. My PR was based on a very limited understanding of what was going on here and made the assumption that, with HDFS semantics, the code works properly. Later investigations (as in the comments) showed this wasn't the case and that this rename step is actually necessary.",
        "createdAt" : "2021-05-13T15:11:32Z",
        "updatedAt" : "2021-05-13T15:11:32Z",
        "lastEditedBy" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "tags" : [
        ]
      },
      {
        "id" : "7ff2e1c6-5475-4025-b41f-1168ec79a93a",
        "parentId" : "5f8e2bc3-ca1c-48c9-bcdd-365984ac665b",
        "authorId" : "d0706b4b-56e1-4a0a-a5a1-1f93c9c8ebd0",
        "body" : "Yes, [staging files for custom partition are under `.spark-staging...`](https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/internal/io/HadoopMapReduceCommitProtocol.scala#L149),  even when `dynamicPartitionOverwrite == false` ",
        "createdAt" : "2021-05-13T21:22:31Z",
        "updatedAt" : "2021-05-13T21:22:31Z",
        "lastEditedBy" : "d0706b4b-56e1-4a0a-a5a1-1f93c9c8ebd0",
        "tags" : [
        ]
      }
    ],
    "commit" : "68fd13176673f2b4fbcec54661fd7dcf8e900e39",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +197,201 @@      absParentPaths.foreach(fs.mkdirs)\n      for ((src, dst) <- filesToMove) {\n        if (!fs.rename(new Path(src), new Path(dst))) {\n          throw new IOException(s\"Failed to rename $src to $dst when committing files staged for \" +\n            s\"absolute locations\")"
  },
  {
    "id" : "4a31427e-b2f1-449a-b463-6febb8b8f81e",
    "prId" : 32530,
    "prUrl" : "https://github.com/apache/spark/pull/32530#pullrequestreview-662703141",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f98e862a-c6ca-42e5-aea7-ae7f00cc3966",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "If `dynamicPartitionOverwrite == false`, we don't delete `absParentPaths`. Why do we always make dirs here?",
        "createdAt" : "2021-05-17T15:56:02Z",
        "updatedAt" : "2021-05-17T15:56:03Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "0001fc10-60dd-4639-b481-8487913c55ac",
        "parentId" : "f98e862a-c6ca-42e5-aea7-ae7f00cc3966",
        "authorId" : "d0706b4b-56e1-4a0a-a5a1-1f93c9c8ebd0",
        "body" : "It's in case that `absParentPaths` has never been created before the job.",
        "createdAt" : "2021-05-19T01:38:17Z",
        "updatedAt" : "2021-05-19T01:38:17Z",
        "lastEditedBy" : "d0706b4b-56e1-4a0a-a5a1-1f93c9c8ebd0",
        "tags" : [
        ]
      }
    ],
    "commit" : "68fd13176673f2b4fbcec54661fd7dcf8e900e39",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +195,199 @@      }\n      logDebug(s\"Create absolute parent directories: $absParentPaths\")\n      absParentPaths.foreach(fs.mkdirs)\n      for ((src, dst) <- filesToMove) {\n        if (!fs.rename(new Path(src), new Path(dst))) {"
  },
  {
    "id" : "fb83a371-c1b4-4036-afad-ca07d4493df5",
    "prId" : 29000,
    "prUrl" : "https://github.com/apache/spark/pull/29000#pullrequestreview-460637765",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ff261b18-2d41-4f14-970d-444f4a6e96db",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Could we make sure that we actually only support `dynamicPartitionOverwrite` with `FileOutputCommitter`?",
        "createdAt" : "2020-08-01T14:06:14Z",
        "updatedAt" : "2020-11-24T16:38:32Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "12392a76-59ae-49cc-8509-46f0591cd64f",
        "parentId" : "ff261b18-2d41-4f14-970d-444f4a6e96db",
        "authorId" : "19bcd642-216d-45ca-9e34-13e334fdc8fc",
        "body" : "Hmm, AFAIK yes, dynamicPartitionOverwrite only works for FileOutputCommitter, correct me if wrong :)",
        "createdAt" : "2020-08-02T16:13:26Z",
        "updatedAt" : "2020-11-24T16:38:32Z",
        "lastEditedBy" : "19bcd642-216d-45ca-9e34-13e334fdc8fc",
        "tags" : [
        ]
      },
      {
        "id" : "25daabc3-9019-42a8-98c4-e796aef9c436",
        "parentId" : "ff261b18-2d41-4f14-970d-444f4a6e96db",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "cc: @cloud-fan @turboFei ?",
        "createdAt" : "2020-08-04T09:14:00Z",
        "updatedAt" : "2020-11-24T16:38:32Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "85aa12a618ceadfe510a4f9fc3718a746a1bc357",
    "line" : 56,
    "diffHunk" : "@@ -1,1 +124,128 @@      // For FileOutputCommitter it has its own staging path called \"work path\".\n      case f: FileOutputCommitter =>\n        if (dynamicPartitionOverwrite) {\n          assert(dir.isDefined,\n            \"The dataset to be written must be partitioned when dynamicPartitionOverwrite is true.\")"
  },
  {
    "id" : "80110691-b9aa-4d2c-ba84-ba8ccf32a4e8",
    "prId" : 29000,
    "prUrl" : "https://github.com/apache/spark/pull/29000#pullrequestreview-477218500",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3b8ad848-6a14-4f4e-87a3-4a4c1530a89d",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "What about the case of `dynamicPartitionOverwrite=true` but `dir.isEmpty`?  IIUC, the workPath will be `/path/to/outputPath/.spark-staging-{jobId}/_temporary/{appAttemptId}/_temporary/{taskAttemptId}/` in this case. And it will be committed to `/path/to/outputPath/.spark-staging-{jobId}/` then. But it seems we don't move them to the `/path/to/outputPath/` in the end.",
        "createdAt" : "2020-08-27T16:14:08Z",
        "updatedAt" : "2020-11-24T16:38:32Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "edd93213-6cf3-45a3-a4d0-2a7fc1b1f1cd",
        "parentId" : "3b8ad848-6a14-4f4e-87a3-4a4c1530a89d",
        "authorId" : "19bcd642-216d-45ca-9e34-13e334fdc8fc",
        "body" : "IFAIK，`assert(dir.isDefined, ...)` already avoid this case, `dir.isDefined` means` !dir.isEmpty`, correct me if wrong :)",
        "createdAt" : "2020-08-28T02:31:09Z",
        "updatedAt" : "2020-11-24T16:38:32Z",
        "lastEditedBy" : "19bcd642-216d-45ca-9e34-13e334fdc8fc",
        "tags" : [
        ]
      },
      {
        "id" : "2fd4d6a0-82bd-45f3-bb8b-f6e45ada2bc5",
        "parentId" : "3b8ad848-6a14-4f4e-87a3-4a4c1530a89d",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "oh, I see. My mistake.",
        "createdAt" : "2020-08-28T03:33:16Z",
        "updatedAt" : "2020-11-24T16:38:32Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "85aa12a618ceadfe510a4f9fc3718a746a1bc357",
    "line" : 61,
    "diffHunk" : "@@ -1,1 +129,133 @@          partitionPaths += dir.get\n        }\n        new Path(Option(f.getWorkPath).map(_.toString).getOrElse(path))\n      case _ => new Path(path)\n    }"
  },
  {
    "id" : "4a590d4d-1569-4469-a54e-08b561396ba9",
    "prId" : 29000,
    "prUrl" : "https://github.com/apache/spark/pull/29000#pullrequestreview-681881561",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e4872de2-4351-4288-983c-68f728e40897",
        "parentId" : null,
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "so this isn't the normal behavior of the algorithm version 2, right?  Normally it writes the task files directly to the final output location.  The whole point of algorithm 2 is to prevent all of the extra moves on the driver at the end of the job. For large jobs this time can be huge.   I'm not sure the benefit here of algorithm 2 because that is all happening distributed on each task?",
        "createdAt" : "2021-06-11T14:16:30Z",
        "updatedAt" : "2021-06-11T14:16:30Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "85aa12a618ceadfe510a4f9fc3718a746a1bc357",
    "line" : 25,
    "diffHunk" : "@@ -1,1 +56,60 @@ *                                  then move them to\n *                                  /path/to/outputPath/.spark-staging-{jobId}/a=1/b=1.\n *                                  2. When [[FileOutputCommitter]] algorithm version set to 2,\n *                                  committing tasks directly move task attempt output files to\n *                                  /path/to/outputPath/.spark-staging-{jobId}/a=1/b=1."
  },
  {
    "id" : "c8c50d62-5963-4e55-8ebe-57f54fde0104",
    "prId" : 26971,
    "prUrl" : "https://github.com/apache/spark/pull/26971#pullrequestreview-388577549",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "02e42874-6ce4-4572-b935-418f6dd12d7c",
        "parentId" : null,
        "authorId" : "eb4e5e35-a753-4069-bd95-fda614c83ab3",
        "body" : "do i understand it correctly that part here is a directory (e.g. x=1/y=2), not a file? so a directory full of files is being moved.\r\nif so couldn't multiple tasks write to the same partition? and then wouldnt these moves conflict with each other?",
        "createdAt" : "2020-04-06T20:06:46Z",
        "updatedAt" : "2020-04-06T20:06:47Z",
        "lastEditedBy" : "eb4e5e35-a753-4069-bd95-fda614c83ab3",
        "tags" : [
        ]
      }
    ],
    "commit" : "2c873dbae136904f086a23290c29a5559ce8d14f",
    "line" : 70,
    "diffHunk" : "@@ -1,1 +204,208 @@                fs.mkdirs(finalPartPath.getParent)\n              }\n              fs.rename(new Path(s\"$stagingDir/$successAttemptID\", part), finalPartPath)\n            })\n            allPartitionPath"
  },
  {
    "id" : "dc04d8c2-c646-460c-81a5-b17377d89dea",
    "prId" : 26339,
    "prUrl" : "https://github.com/apache/spark/pull/26339#pullrequestreview-392594467",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9bea6a47-9f71-461d-a8ef-7a4228af399a",
        "parentId" : null,
        "authorId" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "body" : "so where does SparkHadoopMapRedUtil.commitTask moves the data from/to and how does dynamicPartitionOverwrite deal with that output? Can you please give an example? It seems there are 2 renames happening one due to SparkHadoopMapRedUtil.commitTask and then with the code block inside dynamicPartitionOverwrite.",
        "createdAt" : "2020-04-07T18:29:16Z",
        "updatedAt" : "2020-06-18T07:04:58Z",
        "lastEditedBy" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "tags" : [
        ]
      },
      {
        "id" : "fd0b75c0-2702-4b8c-bf4e-dd34cdf88478",
        "parentId" : "9bea6a47-9f71-461d-a8ef-7a4228af399a",
        "authorId" : "334a081c-cba1-403e-ac80-30e6124077d1",
        "body" : "IIUC, `SparkHadoopMapRedUtil.commitTask` will output to `dynamicStagingTaskFiles` where each task attempt gets its own staging directory to avoid conflicts. Then, we rename the staging files to their final destination if not exist. \r\n\r\nOne minor issue is the following log in `SparkHadoopMapRedUtil.commitTask`. When we say the task attempt has committed, it's actually not. @turboFei \r\n\r\n```        \r\nlogInfo(s\"$mrTaskAttemptID: Committed\")\r\n```",
        "createdAt" : "2020-04-14T04:03:54Z",
        "updatedAt" : "2020-06-18T07:04:58Z",
        "lastEditedBy" : "334a081c-cba1-403e-ac80-30e6124077d1",
        "tags" : [
        ]
      },
      {
        "id" : "7ae41b7d-d064-4884-b6aa-1b2cf4d0f563",
        "parentId" : "9bea6a47-9f71-461d-a8ef-7a4228af399a",
        "authorId" : "b6393788-12cc-4066-9294-857f4baec092",
        "body" : "yeah, this log is a little confused.",
        "createdAt" : "2020-04-14T04:25:00Z",
        "updatedAt" : "2020-06-18T07:04:58Z",
        "lastEditedBy" : "b6393788-12cc-4066-9294-857f4baec092",
        "tags" : [
        ]
      }
    ],
    "commit" : "717d9a56faf128e4864844cdf6341cb7b8731307",
    "line" : 82,
    "diffHunk" : "@@ -1,1 +281,285 @@    SparkHadoopMapRedUtil.commitTask(\n      committer, taskContext, attemptId.getJobID.getId, attemptId.getTaskID.getId)\n    if (dynamicPartitionOverwrite) {\n      val fs = stagingDir.getFileSystem(taskContext.getConfiguration)\n      dynamicStagingTaskFiles.foreach { stagingTaskFile =>"
  },
  {
    "id" : "51ac1199-1a8a-4cdf-9fbe-e79ffc4be439",
    "prId" : 26339,
    "prUrl" : "https://github.com/apache/spark/pull/26339#pullrequestreview-392555662",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e3549b57-5bb3-4544-82c3-edb820bd60f2",
        "parentId" : null,
        "authorId" : "334a081c-cba1-403e-ac80-30e6124077d1",
        "body" : "I thinks this can be an immutable `Set`",
        "createdAt" : "2020-04-14T01:54:41Z",
        "updatedAt" : "2020-06-18T07:04:58Z",
        "lastEditedBy" : "334a081c-cba1-403e-ac80-30e6124077d1",
        "tags" : [
        ]
      },
      {
        "id" : "824d54d2-543d-49cf-b5d1-d9dc69b4c9fb",
        "parentId" : "e3549b57-5bb3-4544-82c3-edb820bd60f2",
        "authorId" : "b6393788-12cc-4066-9294-857f4baec092",
        "body" : "Just test, it should be mutable, otherwise there is no `+=` method for immutable.Set.\r\n",
        "createdAt" : "2020-04-14T02:08:37Z",
        "updatedAt" : "2020-06-18T07:04:58Z",
        "lastEditedBy" : "b6393788-12cc-4066-9294-857f4baec092",
        "tags" : [
        ]
      }
    ],
    "commit" : "717d9a56faf128e4864844cdf6341cb7b8731307",
    "line" : 26,
    "diffHunk" : "@@ -1,1 +95,99 @@   * Tracks the staging task files with dynamicPartitionOverwrite=true.\n   */\n  @transient private[spark] var dynamicStagingTaskFiles: mutable.Set[Path] = null\n\n  /**"
  },
  {
    "id" : "5d4961ce-2d42-4fca-a117-be87bdd8396e",
    "prId" : 26339,
    "prUrl" : "https://github.com/apache/spark/pull/26339#pullrequestreview-392593417",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "569980d7-c946-4970-9856-1bfcf63e5126",
        "parentId" : null,
        "authorId" : "334a081c-cba1-403e-ac80-30e6124077d1",
        "body" : "what happens if the `finalFile` exists ?",
        "createdAt" : "2020-04-14T02:01:41Z",
        "updatedAt" : "2020-06-18T07:04:58Z",
        "lastEditedBy" : "334a081c-cba1-403e-ac80-30e6124077d1",
        "tags" : [
        ]
      },
      {
        "id" : "2a7d1fcc-defe-4062-a85d-8f6cd8506c00",
        "parentId" : "569980d7-c946-4970-9856-1bfcf63e5126",
        "authorId" : "b6393788-12cc-4066-9294-857f4baec092",
        "body" : "I think the finalFile may exist when spark.hadoop.outputCommitCoordination.enabled is false.\r\nIt is true by defaults.",
        "createdAt" : "2020-04-14T02:06:28Z",
        "updatedAt" : "2020-06-18T07:04:58Z",
        "lastEditedBy" : "b6393788-12cc-4066-9294-857f4baec092",
        "tags" : [
        ]
      },
      {
        "id" : "46b71f81-5d9f-4381-ba23-cca6649aca0e",
        "parentId" : "569980d7-c946-4970-9856-1bfcf63e5126",
        "authorId" : "334a081c-cba1-403e-ac80-30e6124077d1",
        "body" : "It was a dumb question by me. It's by design to skip `rename` when `finalFile` exists to avoid conflicts. As to when the `finalFile` will exist, are you saying there will be no conflict with task speculation if output commit coordination is enabled ? \r\n\r\nAnother question is if it's possible the existing `finalFile` is corrupted due to another task crashing during commit.\r\n\r\n",
        "createdAt" : "2020-04-14T02:49:52Z",
        "updatedAt" : "2020-06-18T07:04:58Z",
        "lastEditedBy" : "334a081c-cba1-403e-ac80-30e6124077d1",
        "tags" : [
        ]
      },
      {
        "id" : "d124608d-28a5-4b24-b3ef-5ca04c5cafa3",
        "parentId" : "569980d7-c946-4970-9856-1bfcf63e5126",
        "authorId" : "b6393788-12cc-4066-9294-857f4baec092",
        "body" : "Thanks, I just check the code.\r\nIt seems that the  `SparkHadoopMapRedUtil.commitTask` has no meaning for dynamic partition overwrite, because dynamicPartitionOverwrite operation has a specific staging dir (`.spark-staging-jobId`) instead of using the working temporary of FileOutputCommitter(`_temporary/0`).\r\n\r\nSo, the conflict issue is not related with the enable of outputCommitCoordination.\r\n\r\n\r\n> Another question is if it's possible the existing finalFile is corrupted due to another task crashing during commit.\r\n\r\nIt is possible, because there may be several dynamicStagingTaskFiles，if the task aborted when renaming these dynamicStagingTaskFiles to finalFiles, partial outputs are moved to the partition paths.\r\n",
        "createdAt" : "2020-04-14T03:02:44Z",
        "updatedAt" : "2020-06-18T07:04:58Z",
        "lastEditedBy" : "b6393788-12cc-4066-9294-857f4baec092",
        "tags" : [
        ]
      },
      {
        "id" : "f442e779-a933-4e08-9426-78e2493af346",
        "parentId" : "569980d7-c946-4970-9856-1bfcf63e5126",
        "authorId" : "334a081c-cba1-403e-ac80-30e6124077d1",
        "body" : "I think `outputCommitCoordination` still avoids two attempts committing at the same time through driver. \r\n\r\n> It is possible, because there may be several dynamicStagingTaskFiles，if the task aborted when commitTask, partial outputs are moved to the partition paths.\r\n\r\nI mean what will happen if `rename` fails. \r\n ",
        "createdAt" : "2020-04-14T03:53:32Z",
        "updatedAt" : "2020-06-18T07:04:58Z",
        "lastEditedBy" : "334a081c-cba1-403e-ac80-30e6124077d1",
        "tags" : [
        ]
      },
      {
        "id" : "4d9808da-5573-45e4-9534-acd73d29adfe",
        "parentId" : "569980d7-c946-4970-9856-1bfcf63e5126",
        "authorId" : "b6393788-12cc-4066-9294-857f4baec092",
        "body" : "If rename failed, partial result would be loss.",
        "createdAt" : "2020-04-14T03:56:30Z",
        "updatedAt" : "2020-06-18T07:04:58Z",
        "lastEditedBy" : "b6393788-12cc-4066-9294-857f4baec092",
        "tags" : [
        ]
      },
      {
        "id" : "af10dd68-fd3b-47aa-86fa-84663ee7c7f4",
        "parentId" : "569980d7-c946-4970-9856-1bfcf63e5126",
        "authorId" : "334a081c-cba1-403e-ac80-30e6124077d1",
        "body" : "Do we tests for the two failure scenarios ?\r\n\r\n1. task aborted between renaming staging files\r\n2. `rename` fails and an `IOException` is thrown",
        "createdAt" : "2020-04-14T04:07:53Z",
        "updatedAt" : "2020-06-18T07:04:58Z",
        "lastEditedBy" : "334a081c-cba1-403e-ac80-30e6124077d1",
        "tags" : [
        ]
      },
      {
        "id" : "9b32ef9b-f20f-4b61-abaa-7e6f24160b07",
        "parentId" : "569980d7-c946-4970-9856-1bfcf63e5126",
        "authorId" : "b6393788-12cc-4066-9294-857f4baec092",
        "body" : "Will try to add relative tests",
        "createdAt" : "2020-04-14T04:21:03Z",
        "updatedAt" : "2020-06-18T07:04:58Z",
        "lastEditedBy" : "b6393788-12cc-4066-9294-857f4baec092",
        "tags" : [
        ]
      }
    ],
    "commit" : "717d9a56faf128e4864844cdf6341cb7b8731307",
    "line" : 88,
    "diffHunk" : "@@ -1,1 +287,291 @@        val partitionPath = getDynamicPartitionPath(fs, stagingTaskFile, taskContext)\n        val finalFile = new Path(partitionPath, fileName)\n        if (!fs.exists(finalFile) && !fs.rename(stagingTaskFile, finalFile)) {\n          if (fs.exists(finalFile)) {\n            logWarning("
  },
  {
    "id" : "d67ab051-3317-4c0b-b02c-91628a60c186",
    "prId" : 26339,
    "prUrl" : "https://github.com/apache/spark/pull/26339#pullrequestreview-400441194",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0d8d8572-68cc-4d06-aed0-98ee56ac833d",
        "parentId" : null,
        "authorId" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "body" : "This will fail wrt HDFS, without having the parent directory fs.rename won't work. So the partitionPath directory has to be created before renaming it to final location. Please refer SPARK-23815",
        "createdAt" : "2020-04-25T23:42:16Z",
        "updatedAt" : "2020-06-18T07:04:58Z",
        "lastEditedBy" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "tags" : [
        ]
      }
    ],
    "commit" : "717d9a56faf128e4864844cdf6341cb7b8731307",
    "line" : 88,
    "diffHunk" : "@@ -1,1 +287,291 @@        val partitionPath = getDynamicPartitionPath(fs, stagingTaskFile, taskContext)\n        val finalFile = new Path(partitionPath, fileName)\n        if (!fs.exists(finalFile) && !fs.rename(stagingTaskFile, finalFile)) {\n          if (fs.exists(finalFile)) {\n            logWarning("
  },
  {
    "id" : "96f168a9-351c-4e9d-b83b-f40cce4916ba",
    "prId" : 26339,
    "prUrl" : "https://github.com/apache/spark/pull/26339#pullrequestreview-400470194",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f96bceab-1e60-4a25-8410-c095b68d9879",
        "parentId" : null,
        "authorId" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "body" : "if rename failed shouldn't we wrap and propagate that exception? Like permission denied etc.",
        "createdAt" : "2020-04-26T05:50:55Z",
        "updatedAt" : "2020-06-18T07:04:58Z",
        "lastEditedBy" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "tags" : [
        ]
      },
      {
        "id" : "6853528a-5e83-452b-af81-5a49744be533",
        "parentId" : "f96bceab-1e60-4a25-8410-c095b68d9879",
        "authorId" : "b6393788-12cc-4066-9294-857f4baec092",
        "body" : "I think it is not necessary, it would be wrapped as a `SparkException`.",
        "createdAt" : "2020-04-26T07:34:44Z",
        "updatedAt" : "2020-06-18T07:04:58Z",
        "lastEditedBy" : "b6393788-12cc-4066-9294-857f4baec092",
        "tags" : [
        ]
      }
    ],
    "commit" : "717d9a56faf128e4864844cdf6341cb7b8731307",
    "line" : 96,
    "diffHunk" : "@@ -1,1 +295,299 @@              \"\"\".stripMargin)\n          } else {\n            throw new IOException(s\"Failed to rename $stagingTaskFile to $finalFile\")\n          }\n        }"
  },
  {
    "id" : "3162fd2d-b09d-4b4c-9bd3-a40bf970ff03",
    "prId" : 26339,
    "prUrl" : "https://github.com/apache/spark/pull/26339#pullrequestreview-401501667",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "073800f4-37bb-4d62-8f1b-4c9a028a431d",
        "parentId" : null,
        "authorId" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "body" : "@turboFei It seems even with outputCoordinatorEnabled, there are cases with speculation where the other task also tries to commit and fails with IOException because the finalFile already exists. I think this needs to be fixed. committer.needsTaskCommit is turning out to be false in those cases I think. I don't think throwing IOException is fine here.",
        "createdAt" : "2020-04-28T01:24:49Z",
        "updatedAt" : "2020-06-18T07:04:58Z",
        "lastEditedBy" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "tags" : [
        ]
      },
      {
        "id" : "78363d55-b8a6-428d-843d-c5b212edfa1c",
        "parentId" : "073800f4-37bb-4d62-8f1b-4c9a028a431d",
        "authorId" : "b6393788-12cc-4066-9294-857f4baec092",
        "body" : "thanks",
        "createdAt" : "2020-04-28T01:40:33Z",
        "updatedAt" : "2020-06-18T07:04:58Z",
        "lastEditedBy" : "b6393788-12cc-4066-9294-857f4baec092",
        "tags" : [
        ]
      },
      {
        "id" : "ee973d50-627b-4e49-816f-e9ac24b6cf9c",
        "parentId" : "073800f4-37bb-4d62-8f1b-4c9a028a431d",
        "authorId" : "b6393788-12cc-4066-9294-857f4baec092",
        "body" : "I have double checked whether this finalFile exists",
        "createdAt" : "2020-04-28T05:11:29Z",
        "updatedAt" : "2020-06-18T07:04:58Z",
        "lastEditedBy" : "b6393788-12cc-4066-9294-857f4baec092",
        "tags" : [
        ]
      }
    ],
    "commit" : "717d9a56faf128e4864844cdf6341cb7b8731307",
    "line" : 96,
    "diffHunk" : "@@ -1,1 +295,299 @@              \"\"\".stripMargin)\n          } else {\n            throw new IOException(s\"Failed to rename $stagingTaskFile to $finalFile\")\n          }\n        }"
  },
  {
    "id" : "ad9b502c-7b3f-4606-8c0e-1627fa56c315",
    "prId" : 26086,
    "prUrl" : "https://github.com/apache/spark/pull/26086#pullrequestreview-308902032",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d6151e29-f949-4d43-8e97-3162c3462f52",
        "parentId" : null,
        "authorId" : "b6393788-12cc-4066-9294-857f4baec092",
        "body" : "@viirya I only fix this issue for dynamic partition overwrite.\r\n\r\n",
        "createdAt" : "2019-10-29T23:58:56Z",
        "updatedAt" : "2019-10-29T23:59:00Z",
        "lastEditedBy" : "b6393788-12cc-4066-9294-857f4baec092",
        "tags" : [
        ]
      }
    ],
    "commit" : "cfa0c017e9234f1c87d0cad59f884edfbe7d7726",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +145,149 @@    val attemptId = taskContext.getTaskAttemptID.getId\n    if (dynamicPartitionOverwrite) {\n      f\"part-$split%05d-$attemptId%05d-$jobId$ext\"\n    } else {\n      f\"part-$split%05d-$jobId$ext\""
  },
  {
    "id" : "5fa5a9e9-aa00-443f-b54e-416f33697fd3",
    "prId" : 25863,
    "prUrl" : "https://github.com/apache/spark/pull/25863#pullrequestreview-292916187",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "560f3400-a180-42bc-9619-88632e0c65f1",
        "parentId" : null,
        "authorId" : "8e87861a-4202-49a2-baf7-6d51f6aaa5a2",
        "body" : "I believe a warning log is needed here to notify user that we set committer algorithm version to 2.",
        "createdAt" : "2019-09-25T08:12:21Z",
        "updatedAt" : "2019-12-09T16:11:48Z",
        "lastEditedBy" : "8e87861a-4202-49a2-baf7-6d51f6aaa5a2",
        "tags" : [
        ]
      },
      {
        "id" : "fcdadd70-a91e-4f87-836c-b9eeb0bda159",
        "parentId" : "560f3400-a180-42bc-9619-88632e0c65f1",
        "authorId" : "b6393788-12cc-4066-9294-857f4baec092",
        "body" : "thanks",
        "createdAt" : "2019-09-25T08:43:30Z",
        "updatedAt" : "2019-12-09T16:11:48Z",
        "lastEditedBy" : "b6393788-12cc-4066-9294-857f4baec092",
        "tags" : [
        ]
      }
    ],
    "commit" : "f45ca9bf6c51d3efb064a87de56b985e17b60788",
    "line" : 123,
    "diffHunk" : "@@ -1,1 +156,160 @@        \" for that the task output would be committed to staging output path firstly,\" +\n        \" which is equivalent to algorithm 1.\")\n      context.getConfiguration.setInt(FileOutputCommitter.FILEOUTPUTCOMMITTER_ALGORITHM_VERSION, 2)\n    }\n"
  },
  {
    "id" : "293c5b51-8537-492a-822b-b78ea7167d8e",
    "prId" : 25863,
    "prUrl" : "https://github.com/apache/spark/pull/25863#pullrequestreview-294975951",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e5cf9260-e49f-4bd4-ad23-f18e2795271a",
        "parentId" : null,
        "authorId" : "b6393788-12cc-4066-9294-857f4baec092",
        "body" : "How about logDebug? It produces a lot of logs in Jenkins.\r\n@advancedxy ",
        "createdAt" : "2019-09-26T09:31:46Z",
        "updatedAt" : "2019-12-09T16:11:48Z",
        "lastEditedBy" : "b6393788-12cc-4066-9294-857f4baec092",
        "tags" : [
        ]
      },
      {
        "id" : "27418d9f-1a2f-415a-8390-fe5d2fa5aa2b",
        "parentId" : "e5cf9260-e49f-4bd4-ad23-f18e2795271a",
        "authorId" : "b6393788-12cc-4066-9294-857f4baec092",
        "body" : "about 15843 lines",
        "createdAt" : "2019-09-26T10:42:50Z",
        "updatedAt" : "2019-12-09T16:11:48Z",
        "lastEditedBy" : "b6393788-12cc-4066-9294-857f4baec092",
        "tags" : [
        ]
      },
      {
        "id" : "adeeef97-bc26-4b92-ab8f-ef89b60c5e6d",
        "parentId" : "e5cf9260-e49f-4bd4-ad23-f18e2795271a",
        "authorId" : "8e87861a-4202-49a2-baf7-6d51f6aaa5a2",
        "body" : "15843 is a lot, however, it would be not that much inside one spark application.\r\nOne way to solve this, is to use an object level counter to only log the first warning log(or logs).\r\nBut I am not sure if that's worth it. Also, the head of logs may get rotated and discarded...\r\n\r\nOr use logDebug is fine, but normally user won't set log level to DEBUG.\r\n\r\nI am not sure which one is better.  It's up to you then.",
        "createdAt" : "2019-09-29T03:06:12Z",
        "updatedAt" : "2019-12-09T16:11:48Z",
        "lastEditedBy" : "8e87861a-4202-49a2-baf7-6d51f6aaa5a2",
        "tags" : [
        ]
      },
      {
        "id" : "e08d98aa-a64d-4ff5-9169-0212373d212c",
        "parentId" : "e5cf9260-e49f-4bd4-ad23-f18e2795271a",
        "authorId" : "224991ce-ad69-410b-8143-bf394b6b5c59",
        "body" : "not sure what that means. V2 task commit is non-atomic so isn't the same as v1. if task attempt 1 failed, task attempt 2 will call mergepaths into the same dir, so the set of files to commit in job commit may contain the output of both.",
        "createdAt" : "2019-09-30T13:33:01Z",
        "updatedAt" : "2019-12-09T16:11:48Z",
        "lastEditedBy" : "224991ce-ad69-410b-8143-bf394b6b5c59",
        "tags" : [
        ]
      }
    ],
    "commit" : "f45ca9bf6c51d3efb064a87de56b985e17b60788",
    "line" : 122,
    "diffHunk" : "@@ -1,1 +155,159 @@      logDebug(\"Set file output committer algorithm version to 2 implicitly,\" +\n        \" for that the task output would be committed to staging output path firstly,\" +\n        \" which is equivalent to algorithm 1.\")\n      context.getConfiguration.setInt(FileOutputCommitter.FILEOUTPUTCOMMITTER_ALGORITHM_VERSION, 2)\n    }"
  },
  {
    "id" : "ebd8ec2d-7142-4b3d-ac2a-2bb834ae9ffa",
    "prId" : 25795,
    "prUrl" : "https://github.com/apache/spark/pull/25795#pullrequestreview-288351903",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2bdd41c0-ff8e-4cea-a23b-e2071d0e0045",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Thank you for making a PR. Can we have a test case for this, @turboFei ?\r\ncc @wangyum ",
        "createdAt" : "2019-09-15T07:05:49Z",
        "updatedAt" : "2019-09-17T17:28:11Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "72fa3d89-e39a-4678-b1da-57056d43d080",
        "parentId" : "2bdd41c0-ff8e-4cea-a23b-e2071d0e0045",
        "authorId" : "b6393788-12cc-4066-9294-857f4baec092",
        "body" : "done.",
        "createdAt" : "2019-09-15T08:21:33Z",
        "updatedAt" : "2019-09-17T17:28:11Z",
        "lastEditedBy" : "b6393788-12cc-4066-9294-857f4baec092",
        "tags" : [
        ]
      },
      {
        "id" : "a323e526-c94a-4df2-a90b-0eb733f6ce62",
        "parentId" : "2bdd41c0-ff8e-4cea-a23b-e2071d0e0045",
        "authorId" : "813f0961-9a16-4e42-a167-961d914c472c",
        "body" : "What if we set `spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version` to 2? ",
        "createdAt" : "2019-09-15T08:37:58Z",
        "updatedAt" : "2019-09-17T17:28:11Z",
        "lastEditedBy" : "813f0961-9a16-4e42-a167-961d914c472c",
        "tags" : [
        ]
      },
      {
        "id" : "04030102-7de4-487f-a0f8-f4cf97f38896",
        "parentId" : "2bdd41c0-ff8e-4cea-a23b-e2071d0e0045",
        "authorId" : "b6393788-12cc-4066-9294-857f4baec092",
        "body" : "It works well  when we set spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version to 2.\r\n\r\nBut for the version 2, is there a probability that partial result was produced  when we kill an application, which is committing tasks?\r\n\r\n",
        "createdAt" : "2019-09-15T08:48:59Z",
        "updatedAt" : "2019-09-17T17:28:11Z",
        "lastEditedBy" : "b6393788-12cc-4066-9294-857f4baec092",
        "tags" : [
        ]
      }
    ],
    "commit" : "f0b6c844b02ec8349137fa3f2a6773b2e94e941b",
    "line" : 65,
    "diffHunk" : "@@ -1,1 +199,203 @@    if (!dynamicPartitionOverwrite) {\n      committer.commitJob(jobContext)\n    }\n\n    if (hasValidPath) {"
  },
  {
    "id" : "f267a3d4-0cff-4dda-87f8-cfbe38dbe26e",
    "prId" : 25795,
    "prUrl" : "https://github.com/apache/spark/pull/25795#pullrequestreview-289077194",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "41038b54-fddd-429d-81d3-0fc118a4306c",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "We need to add comments to explain it. It looks to me that the hadoop output committer doesn't support concurrent writing to the same directory by design, so there is nothing we can do at Spark side.\r\n\r\nThe fix here is to avoid using the hadoop output committer when `dynamicPartitionOverwrite=true`. I'm fine with this fix.\r\n\r\nBTW, when writing partitioned table with `dynamicPartitionOverwrite=false`, can we support it as well?",
        "createdAt" : "2019-09-17T06:26:38Z",
        "updatedAt" : "2019-09-17T17:28:11Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "21e2ad5b-bdab-45cc-ba13-37223dd54fc7",
        "parentId" : "41038b54-fddd-429d-81d3-0fc118a4306c",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "also cc @advancedxy",
        "createdAt" : "2019-09-17T06:27:16Z",
        "updatedAt" : "2019-09-17T17:28:11Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "ef4c2485-e56a-4f76-89f8-7e12e4beef90",
        "parentId" : "41038b54-fddd-429d-81d3-0fc118a4306c",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "and for non-partitioned table, can we clean up the staging dir when the job is killed?",
        "createdAt" : "2019-09-17T06:28:30Z",
        "updatedAt" : "2019-09-17T17:28:11Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "9b96f681-88cc-44d6-9133-5ddcd1112534",
        "parentId" : "41038b54-fddd-429d-81d3-0fc118a4306c",
        "authorId" : "b6393788-12cc-4066-9294-857f4baec092",
        "body" : "@advancedxy  has discussed with me offline about writing partitioned table with dynamicPartitionOverwrite=false .\r\nHe proposed a suggestion that, we can add JobAttemptPath(_temporary/0) existence check when dynamicPartitionOverwrite=false.",
        "createdAt" : "2019-09-17T07:14:30Z",
        "updatedAt" : "2019-09-17T17:28:11Z",
        "lastEditedBy" : "b6393788-12cc-4066-9294-857f4baec092",
        "tags" : [
        ]
      },
      {
        "id" : "f8e64019-5cde-4c78-b7a1-31367996f983",
        "parentId" : "41038b54-fddd-429d-81d3-0fc118a4306c",
        "authorId" : "b6393788-12cc-4066-9294-857f4baec092",
        "body" : "For a non-partitioned table, dynamicPartitionOverwrite is false,  and the staging dir is under JobAttemptPath(_temporary/0), I think the staging dir will be cleaned up by FileOutputCommitter.abortJob().",
        "createdAt" : "2019-09-17T07:28:00Z",
        "updatedAt" : "2019-09-17T17:28:11Z",
        "lastEditedBy" : "b6393788-12cc-4066-9294-857f4baec092",
        "tags" : [
        ]
      },
      {
        "id" : "17b94aad-d6fa-44c0-8c58-f109d9686379",
        "parentId" : "41038b54-fddd-429d-81d3-0fc118a4306c",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "> I think the staging dir will be cleaned up by FileOutputCommitter.abortJob().\r\n\r\nWhy it can't be cleaned when `dynamicPartitionOverwrite=true`?",
        "createdAt" : "2019-09-17T07:29:48Z",
        "updatedAt" : "2019-09-17T17:28:11Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "7c4e46b8-e8df-403c-a222-e93ab3b7797c",
        "parentId" : "41038b54-fddd-429d-81d3-0fc118a4306c",
        "authorId" : "b6393788-12cc-4066-9294-857f4baec092",
        "body" : "When a job is killed, its staging dir can be cleaned up by `abortJob` method.\r\nBut when an application is killed, its job's staging dir would not be cleaned up gracefully.",
        "createdAt" : "2019-09-17T07:31:43Z",
        "updatedAt" : "2019-09-17T17:28:11Z",
        "lastEditedBy" : "b6393788-12cc-4066-9294-857f4baec092",
        "tags" : [
        ]
      },
      {
        "id" : "78b00d4f-9022-428b-a649-bd3445ed33a2",
        "parentId" : "41038b54-fddd-429d-81d3-0fc118a4306c",
        "authorId" : "b6393788-12cc-4066-9294-857f4baec092",
        "body" : "> > I think the staging dir will be cleaned up by FileOutputCommitter.abortJob().\r\n> \r\n> Why it can't be cleaned when `dynamicPartitionOverwrite=true`?\r\n\r\nFor the case in PR description,  It is happened when appA(static partition overwrite) is killed and its staging dir is not cleaned up gracefully, then appB commits parts result of appA. ",
        "createdAt" : "2019-09-17T07:37:49Z",
        "updatedAt" : "2019-09-17T17:28:11Z",
        "lastEditedBy" : "b6393788-12cc-4066-9294-857f4baec092",
        "tags" : [
        ]
      },
      {
        "id" : "697947b0-6906-4deb-9772-853c492c8504",
        "parentId" : "41038b54-fddd-429d-81d3-0fc118a4306c",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "OK, so we can't rely on the job cleanup. And ideally we should use different staging dir for each job.\r\n\r\nThat said, seems we can't fix the problem for non-partitioned table if we continue to use the hadoop output committer.",
        "createdAt" : "2019-09-17T07:41:41Z",
        "updatedAt" : "2019-09-17T17:28:11Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "3ef51d41-b1fa-4873-818d-ba8c3bc9bd61",
        "parentId" : "41038b54-fddd-429d-81d3-0fc118a4306c",
        "authorId" : "b6393788-12cc-4066-9294-857f4baec092",
        "body" : "Yes,  a solution proposed by advancedxy is adding job attempt path existence check  for non-partitioned table and static partition overwrite operation.\r\n\r\nAnd the implementation of `InsertIntoHiveTable` uses different staging dir for each job.\r\n",
        "createdAt" : "2019-09-17T07:51:21Z",
        "updatedAt" : "2019-09-17T17:28:11Z",
        "lastEditedBy" : "b6393788-12cc-4066-9294-857f4baec092",
        "tags" : [
        ]
      }
    ],
    "commit" : "f0b6c844b02ec8349137fa3f2a6773b2e94e941b",
    "line" : 55,
    "diffHunk" : "@@ -1,1 +190,194 @@    // For dynamic partition overwrite, it has specific job attempt path, so we don't need\n    // committer.setupJob here. Same with the commitJob and abortJob operations below.\n    if (!dynamicPartitionOverwrite) {\n      checkStaticInsertConflict(jobContext)\n      committer.setupJob(jobContext)"
  },
  {
    "id" : "6f7cd695-c33c-4c4b-a8c9-69a535a01335",
    "prId" : 25739,
    "prUrl" : "https://github.com/apache/spark/pull/25739#pullrequestreview-288986959",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6f39a339-6ebe-411b-ab90-592cc1ad3830",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "when `dynamicPartitionOverwrite=true`, we already write files to the staging dir, see `newTaskTempFile`.\r\n\r\nIn fact, I don't see how the committer is related to the staging dir. If you look at `commitTask` and `commitJob`, we kind of manually commit the files in the staging dir, by moving it to the table dir.",
        "createdAt" : "2019-09-16T15:29:30Z",
        "updatedAt" : "2019-09-16T15:29:58Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "2c716cfc-a40e-4d79-add1-98cc542fff85",
        "parentId" : "6f39a339-6ebe-411b-ab90-592cc1ad3830",
        "authorId" : "8e87861a-4202-49a2-baf7-6d51f6aaa5a2",
        "body" : "> In fact, I don't see how the committer is related to the staging dir. If you look at commitTask and commitJob, we kind of manually commit the files in the staging dir, by moving it to the table dir.\r\n\r\nYes, we manually commit files in the staging dir. The problem is in the `HadoopMapReduceCommitProtocol`'s commitJob calls, it first calls `committer.commitJob(jobContext)`, which relates to the output path passes to the JobContext.\r\nhttps://github.com/apache/spark/blob/1de7d307fe7f6bde8753e8b348d38575e2516e4a/core/src/main/scala/org/apache/spark/internal/io/HadoopMapReduceCommitProtocol.scala#L190-L198\r\n\r\nThe OutputCommitter cannot work correctly if multiple OutputCommitter working on the same output path( concurrent writes to different partition to the same table, as the output would be the same: the table output location). After changing the output path to the staging dir, concurrent jobs can have different output dirs.",
        "createdAt" : "2019-09-17T02:12:07Z",
        "updatedAt" : "2019-09-17T02:12:08Z",
        "lastEditedBy" : "8e87861a-4202-49a2-baf7-6d51f6aaa5a2",
        "tags" : [
        ]
      }
    ],
    "commit" : "1de7d307fe7f6bde8753e8b348d38575e2516e4a",
    "line" : 32,
    "diffHunk" : "@@ -1,1 +111,115 @@\n  protected def setupCommitter(context: TaskAttemptContext): OutputCommitter = {\n    // set output path to stagingDir to avoid potential collision of multiple concurrent write tasks\n    if (dynamicPartitionOverwrite) {\n      val newOutputPath = getOutputPath(context)"
  },
  {
    "id" : "1d16f81e-3088-46c3-9f6c-7dba0f1bb218",
    "prId" : 25739,
    "prUrl" : "https://github.com/apache/spark/pull/25739#pullrequestreview-290684837",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3ea72b1f-d054-471d-b60e-66be113ff851",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "`The output will be [[path]]`  what does path mean here?",
        "createdAt" : "2019-09-19T15:02:23Z",
        "updatedAt" : "2019-09-19T15:02:41Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "fd5f5c50-9bd1-4c71-9c0c-04d16c2bb54c",
        "parentId" : "3ea72b1f-d054-471d-b60e-66be113ff851",
        "authorId" : "8e87861a-4202-49a2-baf7-6d51f6aaa5a2",
        "body" : "the path is defined in the class parameter, and the comment for that is:\r\n\r\n```\r\n * @param jobId the job's or stage's id\r\n * @param path the job's output path, or null if committer acts as a noop\r\n * @param dynamicPartitionOverwrite If true, Spark will overwrite partition directories at runtime\r\n *                                  dynamically, i.e., we first write files under a staging\r\n *                                  directory with partition path, e.g.\r\n *                                  /path/to/staging/a=1/b=1/xxx.parquet. When committing the job,\r\n *                                  we first clean up the corresponding partition directories at\r\n *                                  destination path, e.g. /path/to/destination/a=1/b=1, and move\r\n *                                  files from staging directory to the corresponding partition\r\n *                                  directories under destination path.\r\n\r\n```",
        "createdAt" : "2019-09-19T16:15:26Z",
        "updatedAt" : "2019-09-19T16:15:26Z",
        "lastEditedBy" : "8e87861a-4202-49a2-baf7-6d51f6aaa5a2",
        "tags" : [
        ]
      }
    ],
    "commit" : "1de7d307fe7f6bde8753e8b348d38575e2516e4a",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +93,97 @@\n  /**\n   * Get the desired output path for the job. The output will be [[path]] when\n   * dynamicPartitionOverwrite is disabled, otherwise, it will be [[stagingDir]]. We choose\n   * [[stagingDir]] over [[path]] to avoid potential collision of concurrent write jobs as the same"
  },
  {
    "id" : "2a3bc552-c7e8-4e22-82bd-3574a84f8f97",
    "prId" : 24970,
    "prUrl" : "https://github.com/apache/spark/pull/24970#pullrequestreview-275201697",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "51eb6c60-1610-44aa-a536-2391d23e9ad9",
        "parentId" : null,
        "authorId" : "e913f9af-103d-40d4-976e-9aa8e12e7211",
        "body" : "you are right about this -- exceptions from here would mess up `SparkHadoopWriter` and `FileFormatWriter`.  But isn't the right thing to do for those to catch the errors and use `originalException.addSuppressed` as is done in `WriteToDataSourceV2Exec`?\r\nhttps://github.com/apache/spark/blob/48d04f74ca895497b9d8bab18c7708f76f55c520/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/WriteToDataSourceV2Exec.scala#L388-L393\r\n",
        "createdAt" : "2019-08-12T19:51:22Z",
        "updatedAt" : "2019-08-12T20:07:45Z",
        "lastEditedBy" : "e913f9af-103d-40d4-976e-9aa8e12e7211",
        "tags" : [
        ]
      },
      {
        "id" : "410a2fe9-a7bd-4cca-b50a-776cc47d27ef",
        "parentId" : "51eb6c60-1610-44aa-a536-2391d23e9ad9",
        "authorId" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "body" : ">  isn't the right thing to do for those to catch the errors and use originalException.addSuppressed\r\n\r\nThat sounds better, but at the same time it's a change in all of the call sites, not here. We should have a bug open to do that (at which point we should remove these try..catch blocks).",
        "createdAt" : "2019-08-14T23:28:05Z",
        "updatedAt" : "2019-08-14T23:28:06Z",
        "lastEditedBy" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "tags" : [
        ]
      }
    ],
    "commit" : "ee247d9c473f665791da1bbc84e1f61b35ff8dc1",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +210,214 @@   * Abort the job; log and ignore any IO exception thrown.\n   * This is invariably invoked in an exception handler; raising\n   * an exception here will lose the root cause of the failure.\n   *\n   * @param jobContext job context"
  }
]