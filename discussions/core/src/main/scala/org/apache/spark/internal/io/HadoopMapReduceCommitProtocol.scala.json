[
  {
    "id" : "666f1bb2-ef80-4f0d-a875-ae12829676cf",
    "prId" : 32530,
    "prUrl" : "https://github.com/apache/spark/pull/32530#pullrequestreview-659313592",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5f8e2bc3-ca1c-48c9-bcdd-365984ac665b",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "do we need to do the rename when `dynamicPartitionOverwrite == false`?",
        "createdAt" : "2021-05-13T13:35:32Z",
        "updatedAt" : "2021-05-13T13:35:32Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "08f10df3-883b-411e-9927-52bc97f71fef",
        "parentId" : "5f8e2bc3-ca1c-48c9-bcdd-365984ac665b",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I'm asking because this is different from https://github.com/apache/spark/pull/32207/files#diff-714f288ff8f97acca2fc3449e005d4abcab4a5d95883498652f507fa42a92a6aR191",
        "createdAt" : "2021-05-13T13:36:53Z",
        "updatedAt" : "2021-05-13T13:36:53Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "0fbd1216-6890-4edc-97c2-37b02225fbc1",
        "parentId" : "5f8e2bc3-ca1c-48c9-bcdd-365984ac665b",
        "authorId" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "body" : "I believe we do actually need the rename here. My PR was based on a very limited understanding of what was going on here and made the assumption that, with HDFS semantics, the code works properly. Later investigations (as in the comments) showed this wasn't the case and that this rename step is actually necessary.",
        "createdAt" : "2021-05-13T15:11:32Z",
        "updatedAt" : "2021-05-13T15:11:32Z",
        "lastEditedBy" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "tags" : [
        ]
      },
      {
        "id" : "7ff2e1c6-5475-4025-b41f-1168ec79a93a",
        "parentId" : "5f8e2bc3-ca1c-48c9-bcdd-365984ac665b",
        "authorId" : "d0706b4b-56e1-4a0a-a5a1-1f93c9c8ebd0",
        "body" : "Yes, [staging files for custom partition are under `.spark-staging...`](https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/internal/io/HadoopMapReduceCommitProtocol.scala#L149),  even when `dynamicPartitionOverwrite == false` ",
        "createdAt" : "2021-05-13T21:22:31Z",
        "updatedAt" : "2021-05-13T21:22:31Z",
        "lastEditedBy" : "d0706b4b-56e1-4a0a-a5a1-1f93c9c8ebd0",
        "tags" : [
        ]
      }
    ],
    "commit" : "68fd13176673f2b4fbcec54661fd7dcf8e900e39",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +197,201 @@      absParentPaths.foreach(fs.mkdirs)\n      for ((src, dst) <- filesToMove) {\n        if (!fs.rename(new Path(src), new Path(dst))) {\n          throw new IOException(s\"Failed to rename $src to $dst when committing files staged for \" +\n            s\"absolute locations\")"
  },
  {
    "id" : "4a31427e-b2f1-449a-b463-6febb8b8f81e",
    "prId" : 32530,
    "prUrl" : "https://github.com/apache/spark/pull/32530#pullrequestreview-662703141",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f98e862a-c6ca-42e5-aea7-ae7f00cc3966",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "If `dynamicPartitionOverwrite == false`, we don't delete `absParentPaths`. Why do we always make dirs here?",
        "createdAt" : "2021-05-17T15:56:02Z",
        "updatedAt" : "2021-05-17T15:56:03Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "0001fc10-60dd-4639-b481-8487913c55ac",
        "parentId" : "f98e862a-c6ca-42e5-aea7-ae7f00cc3966",
        "authorId" : "d0706b4b-56e1-4a0a-a5a1-1f93c9c8ebd0",
        "body" : "It's in case that `absParentPaths` has never been created before the job.",
        "createdAt" : "2021-05-19T01:38:17Z",
        "updatedAt" : "2021-05-19T01:38:17Z",
        "lastEditedBy" : "d0706b4b-56e1-4a0a-a5a1-1f93c9c8ebd0",
        "tags" : [
        ]
      }
    ],
    "commit" : "68fd13176673f2b4fbcec54661fd7dcf8e900e39",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +195,199 @@      }\n      logDebug(s\"Create absolute parent directories: $absParentPaths\")\n      absParentPaths.foreach(fs.mkdirs)\n      for ((src, dst) <- filesToMove) {\n        if (!fs.rename(new Path(src), new Path(dst))) {"
  },
  {
    "id" : "fb83a371-c1b4-4036-afad-ca07d4493df5",
    "prId" : 29000,
    "prUrl" : "https://github.com/apache/spark/pull/29000#pullrequestreview-460637765",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ff261b18-2d41-4f14-970d-444f4a6e96db",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Could we make sure that we actually only support `dynamicPartitionOverwrite` with `FileOutputCommitter`?",
        "createdAt" : "2020-08-01T14:06:14Z",
        "updatedAt" : "2020-11-24T16:38:32Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "12392a76-59ae-49cc-8509-46f0591cd64f",
        "parentId" : "ff261b18-2d41-4f14-970d-444f4a6e96db",
        "authorId" : "19bcd642-216d-45ca-9e34-13e334fdc8fc",
        "body" : "Hmm, AFAIK yes, dynamicPartitionOverwrite only works for FileOutputCommitter, correct me if wrong :)",
        "createdAt" : "2020-08-02T16:13:26Z",
        "updatedAt" : "2020-11-24T16:38:32Z",
        "lastEditedBy" : "19bcd642-216d-45ca-9e34-13e334fdc8fc",
        "tags" : [
        ]
      },
      {
        "id" : "25daabc3-9019-42a8-98c4-e796aef9c436",
        "parentId" : "ff261b18-2d41-4f14-970d-444f4a6e96db",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "cc: @cloud-fan @turboFei ?",
        "createdAt" : "2020-08-04T09:14:00Z",
        "updatedAt" : "2020-11-24T16:38:32Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "85aa12a618ceadfe510a4f9fc3718a746a1bc357",
    "line" : 56,
    "diffHunk" : "@@ -1,1 +124,128 @@      // For FileOutputCommitter it has its own staging path called \"work path\".\n      case f: FileOutputCommitter =>\n        if (dynamicPartitionOverwrite) {\n          assert(dir.isDefined,\n            \"The dataset to be written must be partitioned when dynamicPartitionOverwrite is true.\")"
  },
  {
    "id" : "80110691-b9aa-4d2c-ba84-ba8ccf32a4e8",
    "prId" : 29000,
    "prUrl" : "https://github.com/apache/spark/pull/29000#pullrequestreview-477218500",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3b8ad848-6a14-4f4e-87a3-4a4c1530a89d",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "What about the case of `dynamicPartitionOverwrite=true` but `dir.isEmpty`?  IIUC, the workPath will be `/path/to/outputPath/.spark-staging-{jobId}/_temporary/{appAttemptId}/_temporary/{taskAttemptId}/` in this case. And it will be committed to `/path/to/outputPath/.spark-staging-{jobId}/` then. But it seems we don't move them to the `/path/to/outputPath/` in the end.",
        "createdAt" : "2020-08-27T16:14:08Z",
        "updatedAt" : "2020-11-24T16:38:32Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "edd93213-6cf3-45a3-a4d0-2a7fc1b1f1cd",
        "parentId" : "3b8ad848-6a14-4f4e-87a3-4a4c1530a89d",
        "authorId" : "19bcd642-216d-45ca-9e34-13e334fdc8fc",
        "body" : "IFAIK，`assert(dir.isDefined, ...)` already avoid this case, `dir.isDefined` means` !dir.isEmpty`, correct me if wrong :)",
        "createdAt" : "2020-08-28T02:31:09Z",
        "updatedAt" : "2020-11-24T16:38:32Z",
        "lastEditedBy" : "19bcd642-216d-45ca-9e34-13e334fdc8fc",
        "tags" : [
        ]
      },
      {
        "id" : "2fd4d6a0-82bd-45f3-bb8b-f6e45ada2bc5",
        "parentId" : "3b8ad848-6a14-4f4e-87a3-4a4c1530a89d",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "oh, I see. My mistake.",
        "createdAt" : "2020-08-28T03:33:16Z",
        "updatedAt" : "2020-11-24T16:38:32Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "85aa12a618ceadfe510a4f9fc3718a746a1bc357",
    "line" : 61,
    "diffHunk" : "@@ -1,1 +129,133 @@          partitionPaths += dir.get\n        }\n        new Path(Option(f.getWorkPath).map(_.toString).getOrElse(path))\n      case _ => new Path(path)\n    }"
  },
  {
    "id" : "4a590d4d-1569-4469-a54e-08b561396ba9",
    "prId" : 29000,
    "prUrl" : "https://github.com/apache/spark/pull/29000#pullrequestreview-681881561",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e4872de2-4351-4288-983c-68f728e40897",
        "parentId" : null,
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "so this isn't the normal behavior of the algorithm version 2, right?  Normally it writes the task files directly to the final output location.  The whole point of algorithm 2 is to prevent all of the extra moves on the driver at the end of the job. For large jobs this time can be huge.   I'm not sure the benefit here of algorithm 2 because that is all happening distributed on each task?",
        "createdAt" : "2021-06-11T14:16:30Z",
        "updatedAt" : "2021-06-11T14:16:30Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "85aa12a618ceadfe510a4f9fc3718a746a1bc357",
    "line" : 25,
    "diffHunk" : "@@ -1,1 +56,60 @@ *                                  then move them to\n *                                  /path/to/outputPath/.spark-staging-{jobId}/a=1/b=1.\n *                                  2. When [[FileOutputCommitter]] algorithm version set to 2,\n *                                  committing tasks directly move task attempt output files to\n *                                  /path/to/outputPath/.spark-staging-{jobId}/a=1/b=1."
  },
  {
    "id" : "c8c50d62-5963-4e55-8ebe-57f54fde0104",
    "prId" : 26971,
    "prUrl" : "https://github.com/apache/spark/pull/26971#pullrequestreview-388577549",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "02e42874-6ce4-4572-b935-418f6dd12d7c",
        "parentId" : null,
        "authorId" : "eb4e5e35-a753-4069-bd95-fda614c83ab3",
        "body" : "do i understand it correctly that part here is a directory (e.g. x=1/y=2), not a file? so a directory full of files is being moved.\r\nif so couldn't multiple tasks write to the same partition? and then wouldnt these moves conflict with each other?",
        "createdAt" : "2020-04-06T20:06:46Z",
        "updatedAt" : "2020-04-06T20:06:47Z",
        "lastEditedBy" : "eb4e5e35-a753-4069-bd95-fda614c83ab3",
        "tags" : [
        ]
      }
    ],
    "commit" : "2c873dbae136904f086a23290c29a5559ce8d14f",
    "line" : 70,
    "diffHunk" : "@@ -1,1 +204,208 @@                fs.mkdirs(finalPartPath.getParent)\n              }\n              fs.rename(new Path(s\"$stagingDir/$successAttemptID\", part), finalPartPath)\n            })\n            allPartitionPath"
  },
  {
    "id" : "dc04d8c2-c646-460c-81a5-b17377d89dea",
    "prId" : 26339,
    "prUrl" : "https://github.com/apache/spark/pull/26339#pullrequestreview-392594467",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9bea6a47-9f71-461d-a8ef-7a4228af399a",
        "parentId" : null,
        "authorId" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "body" : "so where does SparkHadoopMapRedUtil.commitTask moves the data from/to and how does dynamicPartitionOverwrite deal with that output? Can you please give an example? It seems there are 2 renames happening one due to SparkHadoopMapRedUtil.commitTask and then with the code block inside dynamicPartitionOverwrite.",
        "createdAt" : "2020-04-07T18:29:16Z",
        "updatedAt" : "2020-06-18T07:04:58Z",
        "lastEditedBy" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "tags" : [
        ]
      },
      {
        "id" : "fd0b75c0-2702-4b8c-bf4e-dd34cdf88478",
        "parentId" : "9bea6a47-9f71-461d-a8ef-7a4228af399a",
        "authorId" : "334a081c-cba1-403e-ac80-30e6124077d1",
        "body" : "IIUC, `SparkHadoopMapRedUtil.commitTask` will output to `dynamicStagingTaskFiles` where each task attempt gets its own staging directory to avoid conflicts. Then, we rename the staging files to their final destination if not exist. \r\n\r\nOne minor issue is the following log in `SparkHadoopMapRedUtil.commitTask`. When we say the task attempt has committed, it's actually not. @turboFei \r\n\r\n```        \r\nlogInfo(s\"$mrTaskAttemptID: Committed\")\r\n```",
        "createdAt" : "2020-04-14T04:03:54Z",
        "updatedAt" : "2020-06-18T07:04:58Z",
        "lastEditedBy" : "334a081c-cba1-403e-ac80-30e6124077d1",
        "tags" : [
        ]
      },
      {
        "id" : "7ae41b7d-d064-4884-b6aa-1b2cf4d0f563",
        "parentId" : "9bea6a47-9f71-461d-a8ef-7a4228af399a",
        "authorId" : "b6393788-12cc-4066-9294-857f4baec092",
        "body" : "yeah, this log is a little confused.",
        "createdAt" : "2020-04-14T04:25:00Z",
        "updatedAt" : "2020-06-18T07:04:58Z",
        "lastEditedBy" : "b6393788-12cc-4066-9294-857f4baec092",
        "tags" : [
        ]
      }
    ],
    "commit" : "717d9a56faf128e4864844cdf6341cb7b8731307",
    "line" : 82,
    "diffHunk" : "@@ -1,1 +281,285 @@    SparkHadoopMapRedUtil.commitTask(\n      committer, taskContext, attemptId.getJobID.getId, attemptId.getTaskID.getId)\n    if (dynamicPartitionOverwrite) {\n      val fs = stagingDir.getFileSystem(taskContext.getConfiguration)\n      dynamicStagingTaskFiles.foreach { stagingTaskFile =>"
  },
  {
    "id" : "51ac1199-1a8a-4cdf-9fbe-e79ffc4be439",
    "prId" : 26339,
    "prUrl" : "https://github.com/apache/spark/pull/26339#pullrequestreview-392555662",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e3549b57-5bb3-4544-82c3-edb820bd60f2",
        "parentId" : null,
        "authorId" : "334a081c-cba1-403e-ac80-30e6124077d1",
        "body" : "I thinks this can be an immutable `Set`",
        "createdAt" : "2020-04-14T01:54:41Z",
        "updatedAt" : "2020-06-18T07:04:58Z",
        "lastEditedBy" : "334a081c-cba1-403e-ac80-30e6124077d1",
        "tags" : [
        ]
      },
      {
        "id" : "824d54d2-543d-49cf-b5d1-d9dc69b4c9fb",
        "parentId" : "e3549b57-5bb3-4544-82c3-edb820bd60f2",
        "authorId" : "b6393788-12cc-4066-9294-857f4baec092",
        "body" : "Just test, it should be mutable, otherwise there is no `+=` method for immutable.Set.\r\n",
        "createdAt" : "2020-04-14T02:08:37Z",
        "updatedAt" : "2020-06-18T07:04:58Z",
        "lastEditedBy" : "b6393788-12cc-4066-9294-857f4baec092",
        "tags" : [
        ]
      }
    ],
    "commit" : "717d9a56faf128e4864844cdf6341cb7b8731307",
    "line" : 26,
    "diffHunk" : "@@ -1,1 +95,99 @@   * Tracks the staging task files with dynamicPartitionOverwrite=true.\n   */\n  @transient private[spark] var dynamicStagingTaskFiles: mutable.Set[Path] = null\n\n  /**"
  },
  {
    "id" : "5d4961ce-2d42-4fca-a117-be87bdd8396e",
    "prId" : 26339,
    "prUrl" : "https://github.com/apache/spark/pull/26339#pullrequestreview-392593417",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "569980d7-c946-4970-9856-1bfcf63e5126",
        "parentId" : null,
        "authorId" : "334a081c-cba1-403e-ac80-30e6124077d1",
        "body" : "what happens if the `finalFile` exists ?",
        "createdAt" : "2020-04-14T02:01:41Z",
        "updatedAt" : "2020-06-18T07:04:58Z",
        "lastEditedBy" : "334a081c-cba1-403e-ac80-30e6124077d1",
        "tags" : [
        ]
      },
      {
        "id" : "2a7d1fcc-defe-4062-a85d-8f6cd8506c00",
        "parentId" : "569980d7-c946-4970-9856-1bfcf63e5126",
        "authorId" : "b6393788-12cc-4066-9294-857f4baec092",
        "body" : "I think the finalFile may exist when spark.hadoop.outputCommitCoordination.enabled is false.\r\nIt is true by defaults.",
        "createdAt" : "2020-04-14T02:06:28Z",
        "updatedAt" : "2020-06-18T07:04:58Z",
        "lastEditedBy" : "b6393788-12cc-4066-9294-857f4baec092",
        "tags" : [
        ]
      },
      {
        "id" : "46b71f81-5d9f-4381-ba23-cca6649aca0e",
        "parentId" : "569980d7-c946-4970-9856-1bfcf63e5126",
        "authorId" : "334a081c-cba1-403e-ac80-30e6124077d1",
        "body" : "It was a dumb question by me. It's by design to skip `rename` when `finalFile` exists to avoid conflicts. As to when the `finalFile` will exist, are you saying there will be no conflict with task speculation if output commit coordination is enabled ? \r\n\r\nAnother question is if it's possible the existing `finalFile` is corrupted due to another task crashing during commit.\r\n\r\n",
        "createdAt" : "2020-04-14T02:49:52Z",
        "updatedAt" : "2020-06-18T07:04:58Z",
        "lastEditedBy" : "334a081c-cba1-403e-ac80-30e6124077d1",
        "tags" : [
        ]
      },
      {
        "id" : "d124608d-28a5-4b24-b3ef-5ca04c5cafa3",
        "parentId" : "569980d7-c946-4970-9856-1bfcf63e5126",
        "authorId" : "b6393788-12cc-4066-9294-857f4baec092",
        "body" : "Thanks, I just check the code.\r\nIt seems that the  `SparkHadoopMapRedUtil.commitTask` has no meaning for dynamic partition overwrite, because dynamicPartitionOverwrite operation has a specific staging dir (`.spark-staging-jobId`) instead of using the working temporary of FileOutputCommitter(`_temporary/0`).\r\n\r\nSo, the conflict issue is not related with the enable of outputCommitCoordination.\r\n\r\n\r\n> Another question is if it's possible the existing finalFile is corrupted due to another task crashing during commit.\r\n\r\nIt is possible, because there may be several dynamicStagingTaskFiles，if the task aborted when renaming these dynamicStagingTaskFiles to finalFiles, partial outputs are moved to the partition paths.\r\n",
        "createdAt" : "2020-04-14T03:02:44Z",
        "updatedAt" : "2020-06-18T07:04:58Z",
        "lastEditedBy" : "b6393788-12cc-4066-9294-857f4baec092",
        "tags" : [
        ]
      },
      {
        "id" : "f442e779-a933-4e08-9426-78e2493af346",
        "parentId" : "569980d7-c946-4970-9856-1bfcf63e5126",
        "authorId" : "334a081c-cba1-403e-ac80-30e6124077d1",
        "body" : "I think `outputCommitCoordination` still avoids two attempts committing at the same time through driver. \r\n\r\n> It is possible, because there may be several dynamicStagingTaskFiles，if the task aborted when commitTask, partial outputs are moved to the partition paths.\r\n\r\nI mean what will happen if `rename` fails. \r\n ",
        "createdAt" : "2020-04-14T03:53:32Z",
        "updatedAt" : "2020-06-18T07:04:58Z",
        "lastEditedBy" : "334a081c-cba1-403e-ac80-30e6124077d1",
        "tags" : [
        ]
      },
      {
        "id" : "4d9808da-5573-45e4-9534-acd73d29adfe",
        "parentId" : "569980d7-c946-4970-9856-1bfcf63e5126",
        "authorId" : "b6393788-12cc-4066-9294-857f4baec092",
        "body" : "If rename failed, partial result would be loss.",
        "createdAt" : "2020-04-14T03:56:30Z",
        "updatedAt" : "2020-06-18T07:04:58Z",
        "lastEditedBy" : "b6393788-12cc-4066-9294-857f4baec092",
        "tags" : [
        ]
      },
      {
        "id" : "af10dd68-fd3b-47aa-86fa-84663ee7c7f4",
        "parentId" : "569980d7-c946-4970-9856-1bfcf63e5126",
        "authorId" : "334a081c-cba1-403e-ac80-30e6124077d1",
        "body" : "Do we tests for the two failure scenarios ?\r\n\r\n1. task aborted between renaming staging files\r\n2. `rename` fails and an `IOException` is thrown",
        "createdAt" : "2020-04-14T04:07:53Z",
        "updatedAt" : "2020-06-18T07:04:58Z",
        "lastEditedBy" : "334a081c-cba1-403e-ac80-30e6124077d1",
        "tags" : [
        ]
      },
      {
        "id" : "9b32ef9b-f20f-4b61-abaa-7e6f24160b07",
        "parentId" : "569980d7-c946-4970-9856-1bfcf63e5126",
        "authorId" : "b6393788-12cc-4066-9294-857f4baec092",
        "body" : "Will try to add relative tests",
        "createdAt" : "2020-04-14T04:21:03Z",
        "updatedAt" : "2020-06-18T07:04:58Z",
        "lastEditedBy" : "b6393788-12cc-4066-9294-857f4baec092",
        "tags" : [
        ]
      }
    ],
    "commit" : "717d9a56faf128e4864844cdf6341cb7b8731307",
    "line" : 88,
    "diffHunk" : "@@ -1,1 +287,291 @@        val partitionPath = getDynamicPartitionPath(fs, stagingTaskFile, taskContext)\n        val finalFile = new Path(partitionPath, fileName)\n        if (!fs.exists(finalFile) && !fs.rename(stagingTaskFile, finalFile)) {\n          if (fs.exists(finalFile)) {\n            logWarning("
  },
  {
    "id" : "d67ab051-3317-4c0b-b02c-91628a60c186",
    "prId" : 26339,
    "prUrl" : "https://github.com/apache/spark/pull/26339#pullrequestreview-400441194",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0d8d8572-68cc-4d06-aed0-98ee56ac833d",
        "parentId" : null,
        "authorId" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "body" : "This will fail wrt HDFS, without having the parent directory fs.rename won't work. So the partitionPath directory has to be created before renaming it to final location. Please refer SPARK-23815",
        "createdAt" : "2020-04-25T23:42:16Z",
        "updatedAt" : "2020-06-18T07:04:58Z",
        "lastEditedBy" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "tags" : [
        ]
      }
    ],
    "commit" : "717d9a56faf128e4864844cdf6341cb7b8731307",
    "line" : 88,
    "diffHunk" : "@@ -1,1 +287,291 @@        val partitionPath = getDynamicPartitionPath(fs, stagingTaskFile, taskContext)\n        val finalFile = new Path(partitionPath, fileName)\n        if (!fs.exists(finalFile) && !fs.rename(stagingTaskFile, finalFile)) {\n          if (fs.exists(finalFile)) {\n            logWarning("
  },
  {
    "id" : "96f168a9-351c-4e9d-b83b-f40cce4916ba",
    "prId" : 26339,
    "prUrl" : "https://github.com/apache/spark/pull/26339#pullrequestreview-400470194",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f96bceab-1e60-4a25-8410-c095b68d9879",
        "parentId" : null,
        "authorId" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "body" : "if rename failed shouldn't we wrap and propagate that exception? Like permission denied etc.",
        "createdAt" : "2020-04-26T05:50:55Z",
        "updatedAt" : "2020-06-18T07:04:58Z",
        "lastEditedBy" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "tags" : [
        ]
      },
      {
        "id" : "6853528a-5e83-452b-af81-5a49744be533",
        "parentId" : "f96bceab-1e60-4a25-8410-c095b68d9879",
        "authorId" : "b6393788-12cc-4066-9294-857f4baec092",
        "body" : "I think it is not necessary, it would be wrapped as a `SparkException`.",
        "createdAt" : "2020-04-26T07:34:44Z",
        "updatedAt" : "2020-06-18T07:04:58Z",
        "lastEditedBy" : "b6393788-12cc-4066-9294-857f4baec092",
        "tags" : [
        ]
      }
    ],
    "commit" : "717d9a56faf128e4864844cdf6341cb7b8731307",
    "line" : 96,
    "diffHunk" : "@@ -1,1 +295,299 @@              \"\"\".stripMargin)\n          } else {\n            throw new IOException(s\"Failed to rename $stagingTaskFile to $finalFile\")\n          }\n        }"
  },
  {
    "id" : "3162fd2d-b09d-4b4c-9bd3-a40bf970ff03",
    "prId" : 26339,
    "prUrl" : "https://github.com/apache/spark/pull/26339#pullrequestreview-401501667",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "073800f4-37bb-4d62-8f1b-4c9a028a431d",
        "parentId" : null,
        "authorId" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "body" : "@turboFei It seems even with outputCoordinatorEnabled, there are cases with speculation where the other task also tries to commit and fails with IOException because the finalFile already exists. I think this needs to be fixed. committer.needsTaskCommit is turning out to be false in those cases I think. I don't think throwing IOException is fine here.",
        "createdAt" : "2020-04-28T01:24:49Z",
        "updatedAt" : "2020-06-18T07:04:58Z",
        "lastEditedBy" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "tags" : [
        ]
      },
      {
        "id" : "78363d55-b8a6-428d-843d-c5b212edfa1c",
        "parentId" : "073800f4-37bb-4d62-8f1b-4c9a028a431d",
        "authorId" : "b6393788-12cc-4066-9294-857f4baec092",
        "body" : "thanks",
        "createdAt" : "2020-04-28T01:40:33Z",
        "updatedAt" : "2020-06-18T07:04:58Z",
        "lastEditedBy" : "b6393788-12cc-4066-9294-857f4baec092",
        "tags" : [
        ]
      },
      {
        "id" : "ee973d50-627b-4e49-816f-e9ac24b6cf9c",
        "parentId" : "073800f4-37bb-4d62-8f1b-4c9a028a431d",
        "authorId" : "b6393788-12cc-4066-9294-857f4baec092",
        "body" : "I have double checked whether this finalFile exists",
        "createdAt" : "2020-04-28T05:11:29Z",
        "updatedAt" : "2020-06-18T07:04:58Z",
        "lastEditedBy" : "b6393788-12cc-4066-9294-857f4baec092",
        "tags" : [
        ]
      }
    ],
    "commit" : "717d9a56faf128e4864844cdf6341cb7b8731307",
    "line" : 96,
    "diffHunk" : "@@ -1,1 +295,299 @@              \"\"\".stripMargin)\n          } else {\n            throw new IOException(s\"Failed to rename $stagingTaskFile to $finalFile\")\n          }\n        }"
  },
  {
    "id" : "ad9b502c-7b3f-4606-8c0e-1627fa56c315",
    "prId" : 26086,
    "prUrl" : "https://github.com/apache/spark/pull/26086#pullrequestreview-308902032",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d6151e29-f949-4d43-8e97-3162c3462f52",
        "parentId" : null,
        "authorId" : "b6393788-12cc-4066-9294-857f4baec092",
        "body" : "@viirya I only fix this issue for dynamic partition overwrite.\r\n\r\n",
        "createdAt" : "2019-10-29T23:58:56Z",
        "updatedAt" : "2019-10-29T23:59:00Z",
        "lastEditedBy" : "b6393788-12cc-4066-9294-857f4baec092",
        "tags" : [
        ]
      }
    ],
    "commit" : "cfa0c017e9234f1c87d0cad59f884edfbe7d7726",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +145,149 @@    val attemptId = taskContext.getTaskAttemptID.getId\n    if (dynamicPartitionOverwrite) {\n      f\"part-$split%05d-$attemptId%05d-$jobId$ext\"\n    } else {\n      f\"part-$split%05d-$jobId$ext\""
  }
]