[
  {
    "id" : "77ae7ec0-1c53-4df0-889d-78c1f8cd2cd8",
    "prId" : 33457,
    "prUrl" : "https://github.com/apache/spark/pull/33457#pullrequestreview-716870447",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1a99987b-05fa-4b25-b3f5-d97b84f9882b",
        "parentId" : null,
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "btw, why do we need to attach all the handlers again? And there are so many \"attachAllHandler\" in the code changes.",
        "createdAt" : "2021-07-27T12:34:15Z",
        "updatedAt" : "2021-07-27T12:34:54Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "774d52b2-af84-493b-9541-a0019bc3581c",
        "parentId" : "1a99987b-05fa-4b25-b3f5-d97b84f9882b",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> btw, why do we need to attach all the handlers again? And there are so many \"attachAllHandler\" in the code changes.\r\n\r\nThat me clarify the process.\r\nBefore this change. We add handlers to SparkUI but not start it when init SparkUI since we not start jetty server.\r\nThen it call `bind()`, in this method we start jetty server and attach all handler to server.\r\nthis cause a problem that when we call `bind()`, we expose all servlet API to user. But application not fully started yet.\r\nNow in this pr, I split the behavior of start jetty server and attach handlers to server.\r\nWe need to bind address(start jetty server) first before start AM since we need driver url address to bind with spark proxy server.\r\n\r\nThen after application fully started, we attach all handlers to server(means expose UI url to user).\r\n\r\n\r\nFor this comment https://github.com/apache/spark/pull/33457#issuecomment-886624148\r\nI add a initHandler to handle all request between  starting jetty server and  fully start application to show the hint message.\r\n",
        "createdAt" : "2021-07-27T12:46:16Z",
        "updatedAt" : "2021-07-27T12:46:17Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "cf960bda-fdb6-47e2-96d8-059d398f6f20",
        "parentId" : "1a99987b-05fa-4b25-b3f5-d97b84f9882b",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "> I split the behavior of start jetty server and attach handlers to server\r\n\r\nI add some logging and find that `attachHandler` is still called multiple times before `attachAllHandler`",
        "createdAt" : "2021-07-27T13:37:09Z",
        "updatedAt" : "2021-07-27T13:37:09Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "2fd16ec9-92d2-4e6d-a34c-d2891bbac962",
        "parentId" : "1a99987b-05fa-4b25-b3f5-d97b84f9882b",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> > I split the behavior of start jetty server and attach handlers to server\r\n> \r\n> I add some logging and find that `attachHandler` is still called multiple times before `attachAllHandler`\r\n\r\nYes, but when we call `attachHandler()` the `serverInfo` is none, so the handler is not attached to the server.\r\n",
        "createdAt" : "2021-07-27T13:56:18Z",
        "updatedAt" : "2021-07-27T13:56:18Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "05716be7-44bf-4df0-a12c-10bb18c1aa06",
        "parentId" : "1a99987b-05fa-4b25-b3f5-d97b84f9882b",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "Is there a way to avoid calling `attachAllHandler` in so many places?",
        "createdAt" : "2021-07-27T15:35:59Z",
        "updatedAt" : "2021-07-27T15:35:59Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "26148544-974d-41f5-8b5a-7dc7cfa3af33",
        "parentId" : "1a99987b-05fa-4b25-b3f5-d97b84f9882b",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> Is there a way to avoid calling `attachAllHandler` in so many places?\r\n\r\nAdd a new method call it `bindAndAttachAllHandler()` then replace other place to call this one? only call `bind()` and `attachAllHandler` in  SparkContext?",
        "createdAt" : "2021-07-27T16:04:16Z",
        "updatedAt" : "2021-07-27T16:04:17Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "c6243036-a033-4bde-a2c3-9a1811c897a0",
        "parentId" : "1a99987b-05fa-4b25-b3f5-d97b84f9882b",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "Will that work for history server and master/worker UI? If yes let's try to avoid changing them.",
        "createdAt" : "2021-07-27T16:18:17Z",
        "updatedAt" : "2021-07-27T16:18:17Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "18c38b9d-cd03-4b70-a850-ed37f9f8faaf",
        "parentId" : "1a99987b-05fa-4b25-b3f5-d97b84f9882b",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> Will that work for history server and master/worker UI? If yes let's try to avoid changing them.\r\n\r\nHow about current?",
        "createdAt" : "2021-07-27T16:45:01Z",
        "updatedAt" : "2021-07-27T16:45:01Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "cd8483bf-0017-4096-b3b5-11def3522e75",
        "parentId" : "1a99987b-05fa-4b25-b3f5-d97b84f9882b",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "I mean, will history server and master/worker UI show the same page(spark is starting up...) on starting up?",
        "createdAt" : "2021-07-28T10:53:07Z",
        "updatedAt" : "2021-07-28T10:53:27Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "9ace0a0c-5ade-484a-bc50-6f4d2b75fc85",
        "parentId" : "1a99987b-05fa-4b25-b3f5-d97b84f9882b",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> I mean, will history server and master/worker UI show the same page(spark is starting up...) on starting up?\r\n\r\nWith current code, won't impact history server/master/worker web UI. and   history server/master/worker web UI doesn't have such problem.",
        "createdAt" : "2021-07-28T11:09:38Z",
        "updatedAt" : "2021-07-28T11:09:38Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "0c9c3ce97871ae1d081c3e3cdc32034b1a1e66fd",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +639,643 @@\n    // After application started, attach handlers to started server and start handler.\n    _ui.foreach(_.attachAllHandler())\n    // Attach the driver metrics servlet handler to the web ui after the metrics system is started.\n    _env.metricsSystem.getServletHandlers.foreach(handler => ui.foreach(_.attachHandler(handler)))"
  },
  {
    "id" : "d6c99155-4ed9-46d7-8370-b357a5812735",
    "prId" : 32648,
    "prUrl" : "https://github.com/apache/spark/pull/32648#pullrequestreview-666442028",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8131f32c-6de4-402c-9ac0-174b1604cd79",
        "parentId" : null,
        "authorId" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "body" : "```\r\n[ERROR] [Error] /spark/core/src/main/scala/org/apache/spark/SparkContext.scala:355: weaker access privileges in overriding\r\nprivate[package lang] def childValue(x$1: java.util.Properties): java.util.Properties (defined in class ThreadLocal)\r\n  override should at least be private[lang]\r\n```",
        "createdAt" : "2021-05-24T06:46:31Z",
        "updatedAt" : "2021-05-24T06:47:20Z",
        "lastEditedBy" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "tags" : [
        ]
      }
    ],
    "commit" : "2a867a6277c1c6c4f1a6497973588304f907bede",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +353,357 @@  // Thread Local variable that can be used by users to pass information down the stack\n  protected[spark] val localProperties = new InheritableThreadLocal[Properties] {\n    override def childValue(parent: Properties): Properties = {\n      // Note: make a clone such that changes in the parent properties aren't reflected in\n      // the those of the children threads, which has confusing semantics (SPARK-10563)."
  },
  {
    "id" : "cfcc8465-afde-4c8d-b5de-cd24d7b93459",
    "prId" : 32518,
    "prUrl" : "https://github.com/apache/spark/pull/32518#pullrequestreview-657993847",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2f5bcd94-36a6-4ba7-a534-b491778fca6d",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@dongjoon-hyun, should we maybe do this in `SparkSubmitArguments` with `sparkProperties` like `ignoreNonSparkProperties` handling? I think that;s as early as possible",
        "createdAt" : "2021-05-12T06:29:13Z",
        "updatedAt" : "2021-05-12T06:29:13Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "c8e3baa8-42ba-4640-a1b1-f0fe955f641e",
        "parentId" : "2f5bcd94-36a6-4ba7-a534-b491778fca6d",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Yes, it's possible to handle there for SparkSubmit command parameters, however Hadoop configurations also can be handed over to `SparkSession` inside Spark Apps after Spark submits.",
        "createdAt" : "2021-05-12T14:56:23Z",
        "updatedAt" : "2021-05-12T15:07:18Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "afeb68fadb57b7d02258cbd3a2cc540e73f803ff",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +398,402 @@    }\n    // This should be set as early as possible.\n    SparkContext.fillMissingMagicCommitterConfsIfNeeded(_conf)\n\n    _driverLogger = DriverLogger(_conf)"
  },
  {
    "id" : "81ae343f-05e5-4ca5-bd99-c8ddbd466646",
    "prId" : 31953,
    "prUrl" : "https://github.com/apache/spark/pull/31953#pullrequestreview-619747303",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "599c394f-0296-41af-81ba-a17faf09cc4c",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "You can probably simplify by creating two Options for the two properties as vals first",
        "createdAt" : "2021-03-24T13:55:18Z",
        "updatedAt" : "2021-03-24T13:55:18Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      }
    ],
    "commit" : "8bafffbdafccad64b961bdc85337594588aaff84",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +2187,2191 @@   */\n  private[spark] def getCallSite(): CallSite = {\n    if (getLocalProperty(CallSite.SHORT_FORM) == null\n      || getLocalProperty(CallSite.LONG_FORM) == null) {\n      val callSite = Utils.getCallSite()"
  },
  {
    "id" : "874796f6-b076-45dc-b377-fd76521c367a",
    "prId" : 31953,
    "prUrl" : "https://github.com/apache/spark/pull/31953#pullrequestreview-622939903",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "076d2ca9-a076-4380-9691-9e424ae33307",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@lxian, it would be great if we can elabourate how it causes the lock as you did in JIRA. In addition, to clarify, this is not an issue in Spark 3+ as we dropped Scala 2.11 at SPARK-26132.\r\n\r\nCan you create a PR for bracnh-2.4 alone? cc @viirya and @dongjoon-hyun FYI",
        "createdAt" : "2021-03-25T03:11:46Z",
        "updatedAt" : "2021-03-25T03:11:46Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "34858135-4a86-4d7c-ac60-26ec532f81e6",
        "parentId" : "076d2ca9-a076-4380-9691-9e424ae33307",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Is this easy to add test? Otherwise we can also show up how to reproduce it manually in the description.",
        "createdAt" : "2021-03-25T06:59:43Z",
        "updatedAt" : "2021-03-25T06:59:43Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "9b2668b5-b687-4c68-a9b7-e6e900081462",
        "parentId" : "076d2ca9-a076-4380-9691-9e424ae33307",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "@lxian Can you create a PR for branch-2.4? If you are busy, would you mind I create a PR for branch-2.4? Thanks.",
        "createdAt" : "2021-03-29T03:55:51Z",
        "updatedAt" : "2021-03-29T03:55:51Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "11ee410e-1b40-4bbd-ad68-0afcd1a55947",
        "parentId" : "076d2ca9-a076-4380-9691-9e424ae33307",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@viirya I believe it's fine for you to just go ahead IMO .. the author became inactive 4 days and this is the blocker of 2.4 (I guess?).",
        "createdAt" : "2021-03-29T04:07:42Z",
        "updatedAt" : "2021-03-29T04:07:42Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "894cb5f1-88f7-4967-9841-e3fafab9b2b8",
        "parentId" : "076d2ca9-a076-4380-9691-9e424ae33307",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "This is the last issue for 2.4, I think. Okay, let me create a PR first. I can close it if the author opens his after.",
        "createdAt" : "2021-03-29T04:16:22Z",
        "updatedAt" : "2021-03-29T04:16:23Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "5300db07-bb52-4a4c-83d8-3265ac060112",
        "parentId" : "076d2ca9-a076-4380-9691-9e424ae33307",
        "authorId" : "9708b891-7e0d-4e53-ab50-2cc008f04131",
        "body" : "@viirya Thank you for creating the PR. you can close mine if needed. ",
        "createdAt" : "2021-03-29T07:19:03Z",
        "updatedAt" : "2021-03-29T07:19:03Z",
        "lastEditedBy" : "9708b891-7e0d-4e53-ab50-2cc008f04131",
        "tags" : [
        ]
      },
      {
        "id" : "bb712e7a-fff3-4b2b-b3b3-60f60a128bc2",
        "parentId" : "076d2ca9-a076-4380-9691-9e424ae33307",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Thank you @lxian. Your authorship is still kept in the commit.",
        "createdAt" : "2021-03-29T07:40:33Z",
        "updatedAt" : "2021-03-29T07:40:33Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "8bafffbdafccad64b961bdc85337594588aaff84",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +2187,2191 @@   */\n  private[spark] def getCallSite(): CallSite = {\n    if (getLocalProperty(CallSite.SHORT_FORM) == null\n      || getLocalProperty(CallSite.LONG_FORM) == null) {\n      val callSite = Utils.getCallSite()"
  },
  {
    "id" : "1e938f96-f40a-4f7c-9c93-01903a0d2df7",
    "prId" : 31718,
    "prUrl" : "https://github.com/apache/spark/pull/31718#pullrequestreview-635372845",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b56631da-8d5b-469f-8318-e9a9721882e8",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@sarutak, I think we can just repalce `Path` here too (looks like you're fixing this problem across the codebase, right?)",
        "createdAt" : "2021-04-14T01:47:42Z",
        "updatedAt" : "2021-04-14T01:47:42Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "6380025c-1dcc-4563-a9fd-b50c683c50ad",
        "parentId" : "b56631da-8d5b-469f-8318-e9a9721882e8",
        "authorId" : "3a227965-84e0-47cf-9974-11293764f028",
        "body" : "Yeah, the behavior of `Path` seems to be a little bit confusable and there may be lots of places where `Path` does not behave  as we expect.\r\nO.K, I'll looking into it.",
        "createdAt" : "2021-04-14T01:55:58Z",
        "updatedAt" : "2021-04-14T01:55:58Z",
        "lastEditedBy" : "3a227965-84e0-47cf-9974-11293764f028",
        "tags" : [
        ]
      },
      {
        "id" : "207ab294-3d60-46d1-9318-cd246bc7c86d",
        "parentId" : "b56631da-8d5b-469f-8318-e9a9721882e8",
        "authorId" : "3a227965-84e0-47cf-9974-11293764f028",
        "body" : "@HyukjinKwon BTW, Just replacing `Path` is my first idea of this PR.\r\nhttps://github.com/apache/spark/pull/31718/commits/3c40f988de81600a75190751bf9987dfc93d09e9#diff-1e596e9e9bd50e3075847a66f8fecee741289b2b0478f9099c3bf2b680ebaa35R1586\r\n\r\nBut I changed to reflect [your comment](https://github.com/apache/spark/pull/31718#issuecomment-792225217).\r\nAnyway, I'll open a followup PR.",
        "createdAt" : "2021-04-14T07:32:20Z",
        "updatedAt" : "2021-04-14T07:32:21Z",
        "lastEditedBy" : "3a227965-84e0-47cf-9974-11293764f028",
        "tags" : [
        ]
      },
      {
        "id" : "802e7e53-ccc3-40fc-bdb7-a01d95b31e83",
        "parentId" : "b56631da-8d5b-469f-8318-e9a9721882e8",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Yeah, I know :). Sorry for a bit of forth and back. If we change it all in the code base, I think it makes more sense to land a correct fix rather then staying safer and conservative with a bandaid fix",
        "createdAt" : "2021-04-14T08:53:07Z",
        "updatedAt" : "2021-04-14T08:53:07Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "9dc15b92d3ea94e6bb5f00ff02da3655fa9ae381",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +1591,1595 @@      }\n    } else {\n      Utils.resolveURI(path)\n    }\n    val schemeCorrectedURI = uri.getScheme match {"
  },
  {
    "id" : "f9417e83-d1dd-4288-bd8e-b5535db310ff",
    "prId" : 31521,
    "prUrl" : "https://github.com/apache/spark/pull/31521#pullrequestreview-585570977",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a683af74-2c14-47ab-9c87-38ad4d33e2f8",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Do we expect users would set `spark.job.interruptOnCance` (as I notice it's private)? I think it may be better to prevent users from setting this kind of private property instead of doing the valid check.  And we could recommend users to use `setJobGroup` instead. cc @tgravescs @mridulm any thoughts about this?\r\n",
        "createdAt" : "2021-02-08T12:33:22Z",
        "updatedAt" : "2021-02-08T14:29:58Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "639aad76-d2a1-440b-8515-892fe64817db",
        "parentId" : "a683af74-2c14-47ab-9c87-38ad4d33e2f8",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "I perfer to the suggestion, forbid is better. If so we might need an another internal method to set local property for some test case.",
        "createdAt" : "2021-02-08T14:29:38Z",
        "updatedAt" : "2021-02-08T14:29:38Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      },
      {
        "id" : "de6782c3-c45a-4857-8350-4041b6b13ac0",
        "parentId" : "a683af74-2c14-47ab-9c87-38ad4d33e2f8",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Yeah, we need the internal method if we want to forbid. May let's see others' feedback first.",
        "createdAt" : "2021-02-08T14:32:54Z",
        "updatedAt" : "2021-02-08T14:32:54Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "d938f3c4-65c6-4695-be36-f9452ad3731e",
        "parentId" : "a683af74-2c14-47ab-9c87-38ad4d33e2f8",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "we should just change this to use the ConfigBuilder. mark it as internal and the toBoolean function already does this same check to make sure valid.  That doesn't prevent them from doing it though so if we wanted to add other checking for all internal that might be a bit more work and complex as I'm sure we use some for testing.",
        "createdAt" : "2021-02-08T14:42:58Z",
        "updatedAt" : "2021-02-08T14:42:58Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "ad62af8f-20b2-4f2b-9347-56925e36bf0a",
        "parentId" : "a683af74-2c14-47ab-9c87-38ad4d33e2f8",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "oh sorry I guess we can't do that because local property, I would like to more about context ",
        "createdAt" : "2021-02-08T14:44:32Z",
        "updatedAt" : "2021-02-08T14:46:11Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "17669d900568635be2151870101f0d6b33885230",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +720,724 @@      localProperties.get.remove(key)\n    } else {\n      checkLocalProperty(key, value)\n      localProperties.get.setProperty(key, value)\n    }"
  },
  {
    "id" : "be4db0f2-b21e-439c-bce3-abff8427ad7f",
    "prId" : 31227,
    "prUrl" : "https://github.com/apache/spark/pull/31227#pullrequestreview-570476370",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "34ca2a53-360d-496f-bc31-285565e3c3a5",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "We can define it in sql/core module, as it's only used there and the STS module. Probably in `SQLExecution` object.",
        "createdAt" : "2021-01-18T13:11:13Z",
        "updatedAt" : "2021-01-18T13:11:13Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "1a751ba9933b42750b33efd4c65dbee065425781",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +2769,2773 @@   * Statement id is only used for thrift server\n   */\n  private[spark] val SPARK_STATEMENT_ID = \"spark.statement.id\"\n\n  /**"
  },
  {
    "id" : "4c5bfcf1-692c-4624-bb36-e754c84417b4",
    "prId" : 30581,
    "prUrl" : "https://github.com/apache/spark/pull/30581#pullrequestreview-543951766",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "16cf9175-0b06-464f-a167-1d8a6895b887",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "This change is actually not related but I found it during debug. This isn't a bug but improvement:\r\n\r\nBy using `file:/` instead of `spark:/`  on the driver side, we can directly copy the file in driver side instead of fetching through the Netty file server layer. This is matched with `spark.files`.\r\n",
        "createdAt" : "2020-12-03T13:29:04Z",
        "updatedAt" : "2020-12-04T05:52:39Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "9f048008d061bbed27e8df6831b3b5386088669f",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +1642,1646 @@      // If the scheme is file, use URI to simply copy instead of downloading.\n      val uriToUse = if (!isLocal && scheme == \"file\") uri else new URI(key)\n      val uriToDownload = UriBuilder.fromUri(uriToUse).fragment(null).build()\n      val source = Utils.fetchFile(uriToDownload.toString, Utils.createTempDir(), conf,\n        env.securityManager, hadoopConfiguration, timestamp, useCache = false, shouldUntar = false)"
  },
  {
    "id" : "38dd6dca-0612-4c9e-8c99-36fb3a7fcf17",
    "prId" : 30522,
    "prUrl" : "https://github.com/apache/spark/pull/30522#pullrequestreview-540407565",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2b59ff8e-4928-4122-bae2-3720ffbbd6dc",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Spark config is supposed to be immutable. I don't think we should allow this in Spark context. If you should set a static SQL config, you should either stop and start the context again or set it initially when you create a Spark context.",
        "createdAt" : "2020-11-28T00:09:45Z",
        "updatedAt" : "2020-11-28T00:10:03Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "daa32519-aa49-4476-abc5-5c6fa02d8e23",
        "parentId" : "2b59ff8e-4928-4122-bae2-3720ffbbd6dc",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Agree with @HyukjinKwon. Static SQL configs are static and immutable. Forcefully modifying the configs could cause unexpected behavior.",
        "createdAt" : "2020-11-28T17:58:12Z",
        "updatedAt" : "2020-11-28T17:58:13Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "d1f3578a8846afd1ef56973bed44fb07472d9a58",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +712,716 @@\n  /** Set spark conf */\n  def setSparkConf(sparkConf: SparkConf): Unit = {\n    _conf = sparkConf\n  }"
  },
  {
    "id" : "43baa75c-5e87-41e1-a76a-07ecda777ad8",
    "prId" : 30486,
    "prUrl" : "https://github.com/apache/spark/pull/30486#pullrequestreview-537540290",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "90a5ad5e-1794-4cd3-bfd3-0a082b89eb16",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Here we cannot rely on `new Path(path).toUri`. it makes the fragment (`#`) in URI as the part of path. `Utils.resolveURI` is used for `spark.yarn.dist.archives` as well.",
        "createdAt" : "2020-11-24T14:23:02Z",
        "updatedAt" : "2020-11-30T05:09:49Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "e35e94cad2888c98e03feb391a8dfcca8afa365c",
    "line" : 112,
    "diffHunk" : "@@ -1,1 +1586,1590 @@      new Path(path).toUri\n    } else {\n      Utils.resolveURI(path)\n    }\n    val schemeCorrectedURI = uri.getScheme match {"
  },
  {
    "id" : "3c7193d3-758e-45d9-a643-d5bbd92961e4",
    "prId" : 30486,
    "prUrl" : "https://github.com/apache/spark/pull/30486#pullrequestreview-537541608",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6a2fc4fb-17d0-4ad8-92d5-8c30f79ab5d3",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Archive is not supposed to be a directory.",
        "createdAt" : "2020-11-24T14:24:21Z",
        "updatedAt" : "2020-11-30T05:09:49Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "e35e94cad2888c98e03feb391a8dfcca8afa365c",
    "line" : 122,
    "diffHunk" : "@@ -1,1 +1599,1603 @@    val hadoopPath = new Path(schemeCorrectedURI)\n    val scheme = schemeCorrectedURI.getScheme\n    if (!Array(\"http\", \"https\", \"ftp\").contains(scheme) && !isArchive) {\n      val fs = hadoopPath.getFileSystem(hadoopConfiguration)\n      val isDir = fs.getFileStatus(hadoopPath).isDirectory"
  },
  {
    "id" : "dd327f32-84c9-4103-a796-245bd834260a",
    "prId" : 30486,
    "prUrl" : "https://github.com/apache/spark/pull/30486#pullrequestreview-537542208",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0fcafae6-421e-4825-83ec-63ee134a5e1b",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "For the same reason of keeping the fragment, it uses URI when it's archive.",
        "createdAt" : "2020-11-24T14:24:56Z",
        "updatedAt" : "2020-11-30T05:09:49Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "e35e94cad2888c98e03feb391a8dfcca8afa365c",
    "line" : 133,
    "diffHunk" : "@@ -1,1 +1620,1624 @@      schemeCorrectedURI.toString\n    } else if (isArchive) {\n      uri.toString\n    } else {\n      path"
  },
  {
    "id" : "67cfd990-4c3b-4ff9-9f5b-c47d31b1861c",
    "prId" : 30486,
    "prUrl" : "https://github.com/apache/spark/pull/30486#pullrequestreview-538054514",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0b2b17b3-511d-4d04-bc3d-60e6c8896894",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Is this limitation applied to the other ways like `spark.files` too?",
        "createdAt" : "2020-11-24T17:13:54Z",
        "updatedAt" : "2020-11-30T05:09:49Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "b864c5e6-0e21-4bcc-ae90-c6f1fc8fec50",
        "parentId" : "0b2b17b3-511d-4d04-bc3d-60e6c8896894",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Yeah, it does. It was actually copied from `addFile`.",
        "createdAt" : "2020-11-24T23:44:44Z",
        "updatedAt" : "2020-11-30T05:09:49Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "e35e94cad2888c98e03feb391a8dfcca8afa365c",
    "line" : 71,
    "diffHunk" : "@@ -1,1 +1537,1541 @@   * Add an archive to be downloaded and unpacked with this Spark job on every node.\n   *\n   * If an archive is added during execution, it will not be available until the next TaskSet\n   * starts.\n   *"
  },
  {
    "id" : "3a4a035a-3659-4a9b-962a-066d1a3661e2",
    "prId" : 30486,
    "prUrl" : "https://github.com/apache/spark/pull/30486#pullrequestreview-538373049",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "03861327-dd5c-4a35-b641-ae4e383e6b1a",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "> --archives ARCHIVES         Comma-separated list of archives to be extracted into the working directory of each executor.\r\n\r\nDo we need to download archives at driver as it only declares they are extracted to executors? \r\n\r\n",
        "createdAt" : "2020-11-25T09:17:46Z",
        "updatedAt" : "2020-11-30T05:09:49Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "06013097-b659-4d19-bee5-d20a574fed8e",
        "parentId" : "03861327-dd5c-4a35-b641-ae4e383e6b1a",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I think it's fine to unpack/download on the driver side as well to mimic the previous Yarn's behaviour. At least seems that's what Yarn does in Yarn cluster mode.",
        "createdAt" : "2020-11-25T10:27:52Z",
        "updatedAt" : "2020-11-30T05:09:49Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "037f06d8-7334-476d-83f8-35972f93103a",
        "parentId" : "03861327-dd5c-4a35-b641-ae4e383e6b1a",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "In fact, `--files` describes same:\r\n\r\n```\r\n        |  --files FILES               Comma-separated list of files to be placed in the working\r\n        |                              directory of each executor. File paths of these files\r\n        |                              in executors can be accessed via SparkFiles.get(fileName).\r\n```\r\n\r\nI think we should fix the docs to say it's also available on the driver side :-). but I would like to run it separately.",
        "createdAt" : "2020-11-25T10:50:51Z",
        "updatedAt" : "2020-11-30T05:09:49Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "e35e94cad2888c98e03feb391a8dfcca8afa365c",
    "line" : 159,
    "diffHunk" : "@@ -1,1 +1640,1644 @@      logInfo(s\"Added archive $path at $key with timestamp $timestamp\")\n      val uriToDownload = UriBuilder.fromUri(new URI(key)).fragment(null).build()\n      val source = Utils.fetchFile(uriToDownload.toString, Utils.createTempDir(), conf,\n        env.securityManager, hadoopConfiguration, timestamp, useCache = false, shouldUntar = false)\n      val dest = new File("
  },
  {
    "id" : "9ca6a050-9052-4c05-b533-15f7d1574c66",
    "prId" : 29966,
    "prUrl" : "https://github.com/apache/spark/pull/29966#pullrequestreview-542007755",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2e116da3-5685-43cd-ade4-7940ab5b62c5",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "What if two added jars have the same dependency with different versions? e.g.,\r\n\r\n```\r\nsc.addJar(\"ivy://lib1:1.0?transitive=true\") // --> it depends on `libX v1.0`\r\nsc.addJar(\"ivy://lib2:1.0?transitive=true\") // --> it depends on `libX v2.0`\r\n```",
        "createdAt" : "2020-12-01T13:12:21Z",
        "updatedAt" : "2020-12-24T01:53:11Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "42c7c3ff-fae4-47fe-a62f-08235006b28c",
        "parentId" : "2e116da3-5685-43cd-ade4-7940ab5b62c5",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "Done",
        "createdAt" : "2020-12-01T15:20:26Z",
        "updatedAt" : "2020-12-24T01:53:11Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "4c44daecc7c77527e150d30051170f7ee8667f70",
    "line" : 73,
    "diffHunk" : "@@ -1,1 +1994,1998 @@            // Since `new Path(path).toUri` will lose query information,\n            // so here we use `URI.create(path)`\n            DependencyUtils.resolveMavenDependencies(URI.create(path))\n              .flatMap(jar => addLocalJarFile(new File(jar)))\n          case _ => checkRemoteJarFile(path)"
  },
  {
    "id" : "6992dc05-9296-476a-9e4a-c48deaf41b8b",
    "prId" : 29966,
    "prUrl" : "https://github.com/apache/spark/pull/29966#pullrequestreview-542500940",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bd502adb-334d-4fdd-b272-ccb921f74b93",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Could you add tests to check if this warning message is shown only once by using `LogAppender`?",
        "createdAt" : "2020-12-02T00:54:17Z",
        "updatedAt" : "2020-12-24T01:53:11Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "7a01f469-df50-4998-a8a0-712b02dfc471",
        "parentId" : "bd502adb-334d-4fdd-b272-ccb921f74b93",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> Could you add tests to check if this warning message is shown only once by using `LogAppender`?\r\n\r\nSure",
        "createdAt" : "2020-12-02T05:03:13Z",
        "updatedAt" : "2020-12-24T01:53:11Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "4c44daecc7c77527e150d30051170f7ee8667f70",
    "line" : 93,
    "diffHunk" : "@@ -1,1 +2008,2012 @@          postEnvironmentUpdate()\n        }\n        if (existed.nonEmpty) {\n          val jarMessage = if (scheme != \"ivy\") \"JAR\" else \"dependency jars of Ivy URI\"\n          logInfo(s\"The $jarMessage $path at ${existed.mkString(\",\")} has been added already.\" +"
  },
  {
    "id" : "fb1e81b4-acc6-46b8-ab5c-c1d6009667e3",
    "prId" : 29966,
    "prUrl" : "https://github.com/apache/spark/pull/29966#pullrequestreview-545794248",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dc017e8f-cc7f-4a66-8ef2-80bbdb009a8c",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@AngersZhuuuu, out of curiosity, is the Ivy URI the standard form documented somewhere? or something specific to Spark that you came up with?",
        "createdAt" : "2020-12-07T02:19:37Z",
        "updatedAt" : "2020-12-24T01:53:11Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "9e1a6e7a-294f-4955-a7ee-6d62be764b70",
        "parentId" : "dc017e8f-cc7f-4a66-8ef2-80bbdb009a8c",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> @AngersZhuuuu, out of curiosity, is the Ivy URI the standard form documented somewhere? or something specific to Spark that you came up with?\r\n\r\nFrom hive https://issues.apache.org/jira/browse/HIVE-9664, since it download jar use `ivy` then use schema as `ivy`? I think this useful for a lot of companies that have standard package management, so I implemented it in Spark\r\n",
        "createdAt" : "2020-12-07T02:36:16Z",
        "updatedAt" : "2020-12-24T01:53:11Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "4c44daecc7c77527e150d30051170f7ee8667f70",
    "line" : 70,
    "diffHunk" : "@@ -1,1 +1991,1995 @@          // A JAR file which exists locally on every worker node\n          case \"local\" => Seq(\"file:\" + uri.getPath)\n          case \"ivy\" =>\n            // Since `new Path(path).toUri` will lose query information,\n            // so here we use `URI.create(path)`"
  },
  {
    "id" : "87d578a4-052a-4c10-952c-8309e19ee2c1",
    "prId" : 29909,
    "prUrl" : "https://github.com/apache/spark/pull/29909#pullrequestreview-499972884",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e5bc323e-38c2-46c1-bfa3-f56cab4128e1",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "@AngersZhuuuu . This is `CORE` instead of `SQL`.\r\n- `core/src/main/scala/org/apache/spark/SparkContext.scala`",
        "createdAt" : "2020-10-01T02:23:43Z",
        "updatedAt" : "2020-10-01T02:23:44Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "03dd2157-eaab-47ab-b505-7955d1a7d071",
        "parentId" : "e5bc323e-38c2-46c1-bfa3-f56cab4128e1",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> @AngersZhuuuu . This is `CORE` instead of `SQL`.\r\n> \r\n> * `core/src/main/scala/org/apache/spark/SparkContext.scala`\r\n\r\nSorry, since always work at `SQL`  module",
        "createdAt" : "2020-10-01T02:36:37Z",
        "updatedAt" : "2020-10-01T02:36:37Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "27ebe4858cb46e998a6dc3ecd8f7bd110d05bfe4",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1900,1904 @@      logWarning(\"null or empty path specified as parameter to addJar\")\n    } else {\n      val key = if (path.contains(\"\\\\\") && Utils.isWindows) {\n        // For local paths with backslashes on Windows, URI throws an exception\n        addLocalJarFile(new File(path))"
  },
  {
    "id" : "b3f2afaf-0d6f-401e-8ebf-8fdb918b7659",
    "prId" : 29796,
    "prUrl" : "https://github.com/apache/spark/pull/29796#pullrequestreview-491134080",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dde8575e-1288-4450-9e90-3cb11d4b899a",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@williamhyun, looks `fs.getFileStatus(hadoopPath).isDirectory` isn't the exact synonym of `fs.isDirectory`, e.g.)\r\n\r\n```java\r\n  public boolean isDirectory(Path f) throws IOException {\r\n    try {\r\n      return getFileStatus(f).isDirectory();\r\n    } catch (FileNotFoundException e) {\r\n      return false;               // f does not exist\r\n    }\r\n  }\r\n```\r\n\r\nI think you can create utility methods under `Utils.scala` to reuse.",
        "createdAt" : "2020-09-18T03:04:59Z",
        "updatedAt" : "2020-09-18T05:27:44Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "2873ba11-d060-443c-ab69-6152d257c4d8",
        "parentId" : "dde8575e-1288-4450-9e90-3cb11d4b899a",
        "authorId" : "95160cff-696a-48c9-a7e7-7d1a98cbe371",
        "body" : "I missed that. Sorry. In that case, the utility methods will be identical with the deprecated Hadoop 3.2 `isFile/isDirectory` themselves. I'll close my PR. Thank you for your feedback.",
        "createdAt" : "2020-09-18T03:40:30Z",
        "updatedAt" : "2020-09-18T05:27:44Z",
        "lastEditedBy" : "95160cff-696a-48c9-a7e7-7d1a98cbe371",
        "tags" : [
        ]
      },
      {
        "id" : "7064d066-8920-4b37-9ff4-1dcdf07e67c6",
        "parentId" : "dde8575e-1288-4450-9e90-3cb11d4b899a",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@williamhyun, I think they were deprecated to avoid calling redundant `getFileStatus` at [HADOOP-13321](https://issues.apache.org/jira/browse/HADOOP-13321):\r\n\r\n> These methods have a habit of fostering inefficient call patterns in applications, resulting in multiple redundant getFileStatus calls.\r\n\r\n You can have one util that takes both `FileStatus` or `path` for example.\r\n\r\nIf you'd like to take a more conservative approach, you can keep only some valid calls (with `fs.exists()` as an example).",
        "createdAt" : "2020-09-18T03:54:16Z",
        "updatedAt" : "2020-09-18T05:27:44Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "71a046515efd306f36e4894069abe234271b99d2",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1882,1886 @@            throw new FileNotFoundException(s\"Jar ${path} not found\")\n          }\n          if (fs.getFileStatus(hadoopPath).isDirectory) {\n            throw new IllegalArgumentException(\n              s\"Directory ${path} is not allowed for addJar\")"
  },
  {
    "id" : "c0ea54fb-c0bc-4885-9899-71c40495f02d",
    "prId" : 29796,
    "prUrl" : "https://github.com/apache/spark/pull/29796#pullrequestreview-491134329",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "77546cdb-709a-470b-bc1a-239b2fa5feee",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "For example, `fs.getFileStatus(hadoopPath).isDirectory` here looks valid.",
        "createdAt" : "2020-09-18T03:55:05Z",
        "updatedAt" : "2020-09-18T05:27:44Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "71a046515efd306f36e4894069abe234271b99d2",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1882,1886 @@            throw new FileNotFoundException(s\"Jar ${path} not found\")\n          }\n          if (fs.getFileStatus(hadoopPath).isDirectory) {\n            throw new IllegalArgumentException(\n              s\"Directory ${path} is not allowed for addJar\")"
  },
  {
    "id" : "34c96c60-43cc-469a-9604-b79b784f9b62",
    "prId" : 29796,
    "prUrl" : "https://github.com/apache/spark/pull/29796#pullrequestreview-491158645",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d5118f09-653c-4283-987f-769078fabd9f",
        "parentId" : null,
        "authorId" : "95160cff-696a-48c9-a7e7-7d1a98cbe371",
        "body" : "This is safe because of `if (!fs.exists(hadoopPath)) {` at line 1881.",
        "createdAt" : "2020-09-18T05:23:12Z",
        "updatedAt" : "2020-09-18T05:27:44Z",
        "lastEditedBy" : "95160cff-696a-48c9-a7e7-7d1a98cbe371",
        "tags" : [
        ]
      }
    ],
    "commit" : "71a046515efd306f36e4894069abe234271b99d2",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1882,1886 @@            throw new FileNotFoundException(s\"Jar ${path} not found\")\n          }\n          if (fs.getFileStatus(hadoopPath).isDirectory) {\n            throw new IllegalArgumentException(\n              s\"Directory ${path} is not allowed for addJar\")"
  },
  {
    "id" : "ef27b18a-4d6d-412a-a6bf-8d2420375fd6",
    "prId" : 28986,
    "prUrl" : "https://github.com/apache/spark/pull/28986#pullrequestreview-442081563",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e73008f2-3f8c-4b79-a3f4-54678806929a",
        "parentId" : null,
        "authorId" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "body" : "Have you tested this under local mode?",
        "createdAt" : "2020-07-02T23:57:43Z",
        "updatedAt" : "2020-07-07T18:01:23Z",
        "lastEditedBy" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "tags" : [
        ]
      },
      {
        "id" : "b69d60e6-40e6-4b11-bfff-4084de879891",
        "parentId" : "e73008f2-3f8c-4b79-a3f4-54678806929a",
        "authorId" : "43998e22-6c2f-401d-9914-8cecf6fad929",
        "body" : "Under local mode:\r\n\r\n```\r\nscala> sc.range(0, 1).foreach { _ => new SparkContext(new SparkConf().setAppName(\"test\").setMaster(\"local\")) }\r\njava.lang.IllegalStateException: SparkContext should only be created and accessed on the driver.\r\n...\r\n```\r\n\r\nbefore this patch:\r\n\r\n```\r\nscala> sc.range(0, 1).foreach { _ => new SparkContext(new SparkConf().setAppName(\"test\").setMaster(\"local\")) }\r\norg.apache.spark.SparkException: Only one SparkContext should be running in this JVM (see SPARK-2243).The currently running SparkContext was created at:\r\norg.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:921)\r\n...\r\n```\r\n\r\nAlthough the exception is different, it fails anyway.\r\n\r\nI think the new error message is more reasonable.",
        "createdAt" : "2020-07-03T00:17:11Z",
        "updatedAt" : "2020-07-07T18:01:23Z",
        "lastEditedBy" : "43998e22-6c2f-401d-9914-8cecf6fad929",
        "tags" : [
        ]
      }
    ],
    "commit" : "0faac2a4495bebc67819122d7a2d23a750c71ddd",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +2564,2568 @@   */\n  private def assertOnDriver(): Unit = {\n    if (TaskContext.get != null) {\n      // we're accessing it during task execution, fail.\n      throw new IllegalStateException("
  },
  {
    "id" : "62b5c8a0-d6ee-418d-9b21-f230b247516e",
    "prId" : 28986,
    "prUrl" : "https://github.com/apache/spark/pull/28986#pullrequestreview-458081180",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "28c47ee7-319d-4445-a53f-980459e47ca2",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "@ueshin could you submit a follow-up PR to add a conf? In Spark 3.0, turn it off by default; in Spark 3.1 turn it on by default? Also add it to the migration guide?",
        "createdAt" : "2020-07-27T16:23:37Z",
        "updatedAt" : "2020-07-27T16:23:37Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "77672472-6b45-4997-9aa0-2e4cd4ef4ba4",
        "parentId" : "28c47ee7-319d-4445-a53f-980459e47ca2",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Does this actually affect any legitimate use case that would otherwise work? this should be more of a fail-fast for things that will already fail",
        "createdAt" : "2020-07-27T16:31:57Z",
        "updatedAt" : "2020-07-27T16:31:57Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "849fe617-b52e-42a0-9e33-a18bcffba45a",
        "parentId" : "28c47ee7-319d-4445-a53f-980459e47ca2",
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "This is a user error. However, the users may use a Spark library in executors but the library calls SparkContext.getOrCreate. We should still let their workloads work if it worked in the previous releases. ",
        "createdAt" : "2020-07-27T17:19:48Z",
        "updatedAt" : "2020-07-27T17:19:48Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "24865dc4-a3af-4b5c-b4a8-1a1068a6ae93",
        "parentId" : "28c47ee7-319d-4445-a53f-980459e47ca2",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "I see, but would that ever succeed, using the SparkContext that was created? I'm trying to work out what library would do this and not fail. Is it, perhaps, some boilerplate initialization code that gets executed on driver and executor, and it makes a SparkContext that is never actually used on the executor, but now it fails fast?\r\n\r\nI get it, if that's the use case. A release note and/or hidden config to disable it might be an OK workaround.\r\n\r\nAlternatively if arguing that this is not-uncommon, maybe we just don't do this at all, and revert it",
        "createdAt" : "2020-07-27T17:25:02Z",
        "updatedAt" : "2020-07-27T17:25:03Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "07b1c5a4-0956-4d1c-a77c-cf96bbe738f0",
        "parentId" : "28c47ee7-319d-4445-a53f-980459e47ca2",
        "authorId" : "125776fc-3ad3-48e4-8f31-9dd074ca3131",
        "body" : "BTW, had been discussing this offline with @ueshin and @HyukjinKwon - one use case that previously (and admittedly unfortunately) relied on the ability to create `SparkContext`s in the executors is MLflow's `mlflow.pyfunc.spark_udf` API ([see docs & usage example](https://www.mlflow.org/docs/latest/python_api/mlflow.pyfunc.html#mlflow.pyfunc.spark_udf)), which provides a unified interface for scoring models trained in arbitrary ML frameworks (tensorflow, scikit-learn, pyspark.ml) on Spark DataFrames as a pandas UDF. Most ML frameworks (e.g. sklearn) are single-node frameworks that can operate on pandas dataframes, so applying them via pandas UDF works well. For a pyspark.ml model to be applied via pandas UDF, we need to convert the input pandas series -> spark dataframe on the executors, which requires a SparkContext on the executors.\r\n\r\nI'll dig to see if there's a way to keep the MLflow use case working with this change (e.g. pyspark.ml models may ultimately perform inference via spark UDF, so maybe we can somehow extract the underlying UDF from the model and return that from `mlflow.pyfunc.spark_udf`?), but otherwise agree that we may want to revert this change.\r\n\r\n\r\n\r\n\r\n\r\n",
        "createdAt" : "2020-07-27T21:02:58Z",
        "updatedAt" : "2020-07-27T21:02:58Z",
        "lastEditedBy" : "125776fc-3ad3-48e4-8f31-9dd074ca3131",
        "tags" : [
        ]
      },
      {
        "id" : "3a15f683-5fe9-4c8c-aaf8-d458ae222377",
        "parentId" : "28c47ee7-319d-4445-a53f-980459e47ca2",
        "authorId" : "125776fc-3ad3-48e4-8f31-9dd074ca3131",
        "body" : "I also sympathize with the notion that it's an antipattern to create a `SparkContext` on the executors, but not sure there's always a workaround for doing so",
        "createdAt" : "2020-07-27T21:10:44Z",
        "updatedAt" : "2020-07-27T21:10:44Z",
        "lastEditedBy" : "125776fc-3ad3-48e4-8f31-9dd074ca3131",
        "tags" : [
        ]
      },
      {
        "id" : "c54ecd7d-84b8-4b5c-b498-ba314280b481",
        "parentId" : "28c47ee7-319d-4445-a53f-980459e47ca2",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Yep if that's the logic - that one might previously have harmlessly created a SparkContext that does not work and now it fails explicitly - then I'd say just revert it. It's just trying to fail-fast, but maybe that's not a good idea. If it affects mlflow, I'd guess it affects N other applications.",
        "createdAt" : "2020-07-27T21:17:16Z",
        "updatedAt" : "2020-07-27T21:17:16Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "fb4075d9-8a31-4a58-a137-44500bf72072",
        "parentId" : "28c47ee7-319d-4445-a53f-980459e47ca2",
        "authorId" : "43998e22-6c2f-401d-9914-8cecf6fad929",
        "body" : "I agree with having a config, even if the default is disable for a while, like during 3.x series.\r\nI hope the config will be enabled by default eventually since I believe `SparkContext` is supposed to not be created in executors.",
        "createdAt" : "2020-07-27T21:33:21Z",
        "updatedAt" : "2020-07-27T21:33:21Z",
        "lastEditedBy" : "43998e22-6c2f-401d-9914-8cecf6fad929",
        "tags" : [
        ]
      },
      {
        "id" : "644c4c43-6ac0-49f1-904e-49fdc620ea03",
        "parentId" : "28c47ee7-319d-4445-a53f-980459e47ca2",
        "authorId" : "2d6b46ba-4100-4c4d-9341-fff5e39647ec",
        "body" : "This is pretty similar to the old `spark.driver.allowMultipleContexts`. It looks like working if you are lucky, but there are some surprising behaviors which are pretty hard to debug. For example, `SparkEnv.get` will be set to a new one when someone creates a SparkContext in an executor. (Search `SparkEnv.get` and you will find lots of codes running in executors call it).\r\n\r\nI'm pretty surprised that nobody reports this issue in MLFlow. For myself, I used SparkContext in executors to run stress tests for codes running in driver, but I did hit multiple weird issues. I managed to workaround them by some hacks, such as setting Spark.env back. But it requires deep Spark knowledge. I believe many users won't be able to figure out the root cause without an explicit error added by this PR.\r\n\r\nIMO, it's better to stop people from doing this, but we should also add a flag to give the users an option to turn it off, so that they have some time to migrate their workloads away from the fragile pattern. We can eventually remove the flag like what we did for `spark.driver.allowMultipleContexts`.",
        "createdAt" : "2020-07-27T21:57:55Z",
        "updatedAt" : "2020-07-27T21:57:55Z",
        "lastEditedBy" : "2d6b46ba-4100-4c4d-9341-fff5e39647ec",
        "tags" : [
        ]
      },
      {
        "id" : "c2909a3f-2820-43ed-998e-6f67a5d1aaf3",
        "parentId" : "28c47ee7-319d-4445-a53f-980459e47ca2",
        "authorId" : "2d6b46ba-4100-4c4d-9341-fff5e39647ec",
        "body" : "@smurching how does mlflow create SparkContext? IIUC, it doesn't create SparkContext in the executor JVM, so it should not be impacted by this change.",
        "createdAt" : "2020-07-27T22:14:23Z",
        "updatedAt" : "2020-07-27T22:14:23Z",
        "lastEditedBy" : "2d6b46ba-4100-4c4d-9341-fff5e39647ec",
        "tags" : [
        ]
      },
      {
        "id" : "266cddd6-e629-4b29-b636-125fc761f171",
        "parentId" : "28c47ee7-319d-4445-a53f-980459e47ca2",
        "authorId" : "2d6b46ba-4100-4c4d-9341-fff5e39647ec",
        "body" : "NVM. I thought the check was only added in JVM side. If we remove the pyspark side change, it should not impact MLflow. That's actually a case working pretty well...",
        "createdAt" : "2020-07-27T22:21:52Z",
        "updatedAt" : "2020-07-27T22:22:08Z",
        "lastEditedBy" : "2d6b46ba-4100-4c4d-9341-fff5e39647ec",
        "tags" : [
        ]
      },
      {
        "id" : "59c88177-2fe6-45aa-84cf-bbeae7e4fae3",
        "parentId" : "28c47ee7-319d-4445-a53f-980459e47ca2",
        "authorId" : "2d6b46ba-4100-4c4d-9341-fff5e39647ec",
        "body" : "So I'm +1 on blocking creating SparkContext in executor JVM because it messes up the singleton `SparkEnv` (there may be others). But blocking it in Python process is probably not worth since it's working pretty well (unless in future we introduce some singleton objects in executor python processes).",
        "createdAt" : "2020-07-27T22:27:23Z",
        "updatedAt" : "2020-07-27T22:28:19Z",
        "lastEditedBy" : "2d6b46ba-4100-4c4d-9341-fff5e39647ec",
        "tags" : [
        ]
      },
      {
        "id" : "d0b6ff15-4a76-4afc-b37f-9d587e634b15",
        "parentId" : "28c47ee7-319d-4445-a53f-980459e47ca2",
        "authorId" : "125776fc-3ad3-48e4-8f31-9dd074ca3131",
        "body" : "@zsxwing got it makes sense! Yeah, MLflow's spark UDF works pretty reliably AFAIK, at least for this use case :P. When scoring a pyspark.ml model in a pandas UDF, we construct a SparkContext via the standard SparkSession.builder.getOrCreate API ([see MLflow code](https://github.com/mlflow/mlflow/blob/86ea84f9e6d58b702f362ff512f61cff8ada021f/mlflow/spark.py#L520-L521))",
        "createdAt" : "2020-07-27T22:50:03Z",
        "updatedAt" : "2020-07-27T22:50:03Z",
        "lastEditedBy" : "125776fc-3ad3-48e4-8f31-9dd074ca3131",
        "tags" : [
        ]
      },
      {
        "id" : "952df6ee-820d-44f9-a548-5c9e6635caa0",
        "parentId" : "28c47ee7-319d-4445-a53f-980459e47ca2",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "+1 for adding a config, disabling it in 3.0 and enabling it at 3.1.",
        "createdAt" : "2020-07-28T00:59:49Z",
        "updatedAt" : "2020-07-28T00:59:49Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "7907ed1d-5e6e-4aaf-82ef-73903be3886a",
        "parentId" : "28c47ee7-319d-4445-a53f-980459e47ca2",
        "authorId" : "43998e22-6c2f-401d-9914-8cecf6fad929",
        "body" : "I submitted a PR to add configs #29278.\r\nCould you take a look at it when you are available?",
        "createdAt" : "2020-07-29T02:35:27Z",
        "updatedAt" : "2020-07-29T02:35:27Z",
        "lastEditedBy" : "43998e22-6c2f-401d-9914-8cecf6fad929",
        "tags" : [
        ]
      },
      {
        "id" : "c083316b-c50b-46d3-9860-e094f3c1b34b",
        "parentId" : "28c47ee7-319d-4445-a53f-980459e47ca2",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I think it's a bit odds to have two separate configuration for Scala and Python. I don't think we will happen to allow this in PySpark specifically in the future even if it worked. I think we will discourage this behaviour anyway whether it is in Scala or Python, and deprecate/remove both configurations together eventually. In which case would we disable it in Scala side but enable it in Python side?",
        "createdAt" : "2020-07-30T01:02:27Z",
        "updatedAt" : "2020-07-30T01:04:44Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "527c8113-92ec-4fbb-9497-be8cc810be03",
        "parentId" : "28c47ee7-319d-4445-a53f-980459e47ca2",
        "authorId" : "2d6b46ba-4100-4c4d-9341-fff5e39647ec",
        "body" : "> In which case would we disable it in Scala side but enable it in Python side?\r\n\r\nLike how MLflow uses it today. It's pretty hard to make it work correctly in Scala side. But it's working in Python today.",
        "createdAt" : "2020-07-30T01:23:15Z",
        "updatedAt" : "2020-07-30T01:23:15Z",
        "lastEditedBy" : "2d6b46ba-4100-4c4d-9341-fff5e39647ec",
        "tags" : [
        ]
      },
      {
        "id" : "8c1dab04-e533-449f-8e52-9160b8e25296",
        "parentId" : "28c47ee7-319d-4445-a53f-980459e47ca2",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I see but I guess we should discourage/deprecate/remove the behaviour in the end - am I correct? It's a bit weird to have two configuration to control the same behaviour (to end users). And I think it's a bit unlikely when it should be disabled in Scala but enabled in Python specifically. We could just enable it in both sides.",
        "createdAt" : "2020-07-30T01:43:38Z",
        "updatedAt" : "2020-07-30T01:43:38Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "d20f845d-9f5e-422a-837b-1b4bec612ceb",
        "parentId" : "28c47ee7-319d-4445-a53f-980459e47ca2",
        "authorId" : "43998e22-6c2f-401d-9914-8cecf6fad929",
        "body" : "I pushed a commit to use a single config.",
        "createdAt" : "2020-07-30T05:31:00Z",
        "updatedAt" : "2020-07-30T05:31:00Z",
        "lastEditedBy" : "43998e22-6c2f-401d-9914-8cecf6fad929",
        "tags" : [
        ]
      }
    ],
    "commit" : "0faac2a4495bebc67819122d7a2d23a750c71ddd",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +2564,2568 @@   */\n  private def assertOnDriver(): Unit = {\n    if (TaskContext.get != null) {\n      // we're accessing it during task execution, fail.\n      throw new IllegalStateException("
  },
  {
    "id" : "1fa0cd8c-15e9-4682-8121-7770546cc3e1",
    "prId" : 27787,
    "prUrl" : "https://github.com/apache/spark/pull/27787#pullrequestreview-368616606",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b6571620-e689-47b7-8812-84e8458d52ad",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "`sc._resources` is for `driver` rather than `executor`? \r\n\r\nI know that `driver` and `executor` are in the same JVM in this case, but we also have separate resource configurations for `executor.` ",
        "createdAt" : "2020-03-04T08:26:39Z",
        "updatedAt" : "2020-03-04T08:28:58Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "e8df053f-ea26-4b28-8ef9-35e7b8516e83",
        "parentId" : "b6571620-e689-47b7-8812-84e8458d52ad",
        "authorId" : "3a227965-84e0-47cf-9974-11293764f028",
        "body" : "> sc._resources is for driver rather than executor?\r\n\r\nYes. But in local mode, `driver` is behave as `executor` and `spark.executor.resource.*` doesn't affect.",
        "createdAt" : "2020-03-04T08:41:12Z",
        "updatedAt" : "2020-03-04T08:41:22Z",
        "lastEditedBy" : "3a227965-84e0-47cf-9974-11293764f028",
        "tags" : [
        ]
      },
      {
        "id" : "220168c9-19a7-4944-a3e6-00476c516d40",
        "parentId" : "b6571620-e689-47b7-8812-84e8458d52ad",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "I see. But we also need to pass `_resources` into `LocalEndpoint` and then to `Executor`, not only UI?",
        "createdAt" : "2020-03-04T09:08:50Z",
        "updatedAt" : "2020-03-04T09:08:50Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "18827b32-aa90-45c8-8575-cab04322340e",
        "parentId" : "b6571620-e689-47b7-8812-84e8458d52ad",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Maybe we should do `parseOrFindResources` for `executor` in `LocalSchedulerBackend` and then we could get the real resources which configured from `executor`.",
        "createdAt" : "2020-03-04T09:10:51Z",
        "updatedAt" : "2020-03-04T09:10:51Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "bf3b294a-2e40-4133-a1c2-63bc965e0744",
        "parentId" : "b6571620-e689-47b7-8812-84e8458d52ad",
        "authorId" : "3a227965-84e0-47cf-9974-11293764f028",
        "body" : "Ah, right. I'll also fix it. Thanks!",
        "createdAt" : "2020-03-04T09:24:15Z",
        "updatedAt" : "2020-03-04T09:24:16Z",
        "lastEditedBy" : "3a227965-84e0-47cf-9974-11293764f028",
        "tags" : [
        ]
      }
    ],
    "commit" : "78e172e5eaa8085e620236dc90d0ee8a1fb11da4",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +2789,2793 @@        checkResourcesPerTask(1)\n        val scheduler = new TaskSchedulerImpl(sc, MAX_LOCAL_TASK_FAILURES, isLocal = true)\n        val backend = new LocalSchedulerBackend(sc.getConf, scheduler, 1, sc._resources)\n        scheduler.initialize(backend)\n        (backend, scheduler)"
  },
  {
    "id" : "0d54fb7c-92cc-49a2-a3bb-7f1e1c7f4825",
    "prId" : 27773,
    "prUrl" : "https://github.com/apache/spark/pull/27773#pullrequestreview-371295778",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9e4aa384-e30b-436f-b9b1-98bf9a04ecea",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Give a default value(e.g. default ResourceProfile) for `rp`? So caller/developer who isn't aware of `ResourceProfile` doesn't need to care about it.",
        "createdAt" : "2020-03-09T07:29:36Z",
        "updatedAt" : "2020-03-19T20:03:38Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "2fe02280-73b5-4476-a020-efad5db19101",
        "parentId" : "9e4aa384-e30b-436f-b9b1-98bf9a04ecea",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "its a private function  and I think caller should be aware of it, or at least thought about it making sure they aren't using another profile. The caller can simply get the default profile to pass in.",
        "createdAt" : "2020-03-09T15:41:34Z",
        "updatedAt" : "2020-03-19T20:03:38Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "2a4a73385ee948d81e51091990a37ebbc8010f61",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +1626,1630 @@   * @return The max number of tasks that can be concurrent launched currently.\n   */\n  private[spark] def maxNumConcurrentTasks(rp: ResourceProfile): Int = {\n    schedulerBackend.maxNumConcurrentTasks(rp)\n  }"
  },
  {
    "id" : "39579b45-23f0-4f9d-ab36-f193dc422e94",
    "prId" : 27686,
    "prUrl" : "https://github.com/apache/spark/pull/27686#pullrequestreview-364210624",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6fb44db5-2a37-4d17-93e1-910724cbb2cc",
        "parentId" : null,
        "authorId" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "body" : "I guess we should also update the comment above?",
        "createdAt" : "2020-02-24T23:05:26Z",
        "updatedAt" : "2020-02-24T23:05:55Z",
        "lastEditedBy" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "tags" : [
        ]
      },
      {
        "id" : "511eeea4-4f29-42d2-8181-195bf983212c",
        "parentId" : "6fb44db5-2a37-4d17-93e1-910724cbb2cc",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Shall we give one or two examples for the \"some scheduling\" here?",
        "createdAt" : "2020-02-25T01:42:18Z",
        "updatedAt" : "2020-02-25T01:42:22Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "44fda727-f7dc-44ec-964b-88cedfd19487",
        "parentId" : "6fb44db5-2a37-4d17-93e1-910724cbb2cc",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "I don't see the point of giving  the examples, this is a temporary error that will go away when when the rest of the feature is put in.  The end user just needs to know cores has to be limiting factor. All they need to know is spark won't work right if you don't.",
        "createdAt" : "2020-02-25T15:00:39Z",
        "updatedAt" : "2020-02-25T15:00:40Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "ca97b348-5aab-4cb4-9e9f-3f3cd422cf1e",
        "parentId" : "6fb44db5-2a37-4d17-93e1-910724cbb2cc",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "The comment above already says \"dynamic allocation and scheduler \".  I think that covers most of the cases.",
        "createdAt" : "2020-02-25T15:02:18Z",
        "updatedAt" : "2020-02-25T15:02:19Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "88789ef6e8cae0147acf2c1a4ad369736fc0ba47",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +2799,2803 @@        throw new IllegalArgumentException(\"The number of slots on an executor has to be \" +\n          \"limited by the number of cores, otherwise you waste resources and \" +\n          \"some scheduling doesn't work properly. Your configuration has \" +\n          s\"core/task cpu slots = ${cpuSlots} and \" +\n          s\"${limitingResource} = \" +"
  },
  {
    "id" : "48afa50e-8b51-473f-b55d-e084655c0c48",
    "prId" : 27138,
    "prUrl" : "https://github.com/apache/spark/pull/27138#pullrequestreview-340153178",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "36c537e1-248d-487a-9223-c6a9af5f6f82",
        "parentId" : null,
        "authorId" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "body" : "only throw this exception when dynamic allocation is enabled?",
        "createdAt" : "2020-01-08T20:53:29Z",
        "updatedAt" : "2020-01-08T21:10:46Z",
        "lastEditedBy" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "tags" : [
        ]
      },
      {
        "id" : "7f49a790-bbae-4926-8ef8-34ef707d3b1f",
        "parentId" : "36c537e1-248d-487a-9223-c6a9af5f6f82",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "I think its safer to always require it just in case there are other places in the code that use cores and task cpus to determine slots.  I know in doing the stage level sched work there were a bunch of places that did this but I would have to go back thru to see if they were only during dynamic allocation. \r\nActually one example of this is #27126\r\n",
        "createdAt" : "2020-01-08T21:13:17Z",
        "updatedAt" : "2020-01-08T21:13:18Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "f45d8c4e-f9fc-4143-a971-de638bfab14d",
        "parentId" : "36c537e1-248d-487a-9223-c6a9af5f6f82",
        "authorId" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "body" : "sounds good",
        "createdAt" : "2020-01-08T21:22:26Z",
        "updatedAt" : "2020-01-08T21:22:26Z",
        "lastEditedBy" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "b0ddd1c7f807339bb3b959a4b503ce5877d5d292",
    "line" : 39,
    "diffHunk" : "@@ -1,1 +2827,2831 @@        if (resourceNumSlots < numSlots) {\n          if (shouldCheckExecCores) {\n            throw new IllegalArgumentException(\"The number of slots on an executor has to be \" +\n              \"limited by the number of cores, otherwise you waste resources and \" +\n              \"dynamic allocation doesn't work properly. Your configuration has \" +"
  },
  {
    "id" : "18733152-96a8-4151-b5bc-8ea2e4604ab7",
    "prId" : 26773,
    "prUrl" : "https://github.com/apache/spark/pull/26773#pullrequestreview-330252020",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "03becff3-7393-4423-8814-ef6a61ef543c",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "nit: `hadoopPath.toUri`",
        "createdAt" : "2019-12-10T16:09:15Z",
        "updatedAt" : "2019-12-12T05:45:59Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "25fa99e9-08b2-4f32-b8e6-027fccd8d92d",
        "parentId" : "03becff3-7393-4423-8814-ef6a61ef543c",
        "authorId" : "20b33833-5417-4c8d-b239-5c3a1f547ffe",
        "body" : "hadooppath define inside `checkRemoteJarFile` so i am unable to used outside",
        "createdAt" : "2019-12-11T01:30:33Z",
        "updatedAt" : "2019-12-12T05:45:59Z",
        "lastEditedBy" : "20b33833-5417-4c8d-b239-5c3a1f547ffe",
        "tags" : [
        ]
      }
    ],
    "commit" : "df3c1e3f0095c13e254ac555f94ca08be30e7d4b",
    "line" : 59,
    "diffHunk" : "@@ -1,1 +1882,1886 @@        addLocalJarFile(new File(path))\n      } else {\n        val uri = new Path(path).toUri\n        // SPARK-17650: Make sure this is a valid URL before adding it to the list of dependencies\n        Utils.validateURL(uri)"
  },
  {
    "id" : "8db54d10-8415-4131-bd83-15144a6d598f",
    "prId" : 26773,
    "prUrl" : "https://github.com/apache/spark/pull/26773#pullrequestreview-330255278",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cac27f53-acd1-40a2-a9b2-c19a6ef4fed0",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "this change looks a little random to me: what's wrong with `getRawPath`?",
        "createdAt" : "2019-12-10T16:09:39Z",
        "updatedAt" : "2019-12-12T05:45:59Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "1927d217-6ef8-43a4-a860-56b299ba4eca",
        "parentId" : "cac27f53-acd1-40a2-a9b2-c19a6ef4fed0",
        "authorId" : "20b33833-5417-4c8d-b239-5c3a1f547ffe",
        "body" : "this also will give URI path which will cause further error",
        "createdAt" : "2019-12-11T01:22:43Z",
        "updatedAt" : "2019-12-12T05:45:59Z",
        "lastEditedBy" : "20b33833-5417-4c8d-b239-5c3a1f547ffe",
        "tags" : [
        ]
      },
      {
        "id" : "32f098f5-df21-47d2-9e7d-c4556eda827c",
        "parentId" : "cac27f53-acd1-40a2-a9b2-c19a6ef4fed0",
        "authorId" : "20b33833-5417-4c8d-b239-5c3a1f547ffe",
        "body" : "i run in my local some failure suite file like CachedTableSuite and HiveMetastoreCatalogSuite all test case passed",
        "createdAt" : "2019-12-11T01:43:03Z",
        "updatedAt" : "2019-12-12T05:45:59Z",
        "lastEditedBy" : "20b33833-5417-4c8d-b239-5c3a1f547ffe",
        "tags" : [
        ]
      }
    ],
    "commit" : "df3c1e3f0095c13e254ac555f94ca08be30e7d4b",
    "line" : 67,
    "diffHunk" : "@@ -1,1 +1889,1893 @@          case null =>\n            // SPARK-22585 path without schema is not url encoded\n            addLocalJarFile(new File(uri.getPath))\n          // A JAR file which exists only on the driver node\n          case \"file\" => addLocalJarFile(new File(uri.getPath))"
  },
  {
    "id" : "3b30e1f0-3061-49d6-bc9b-3a455943c5c7",
    "prId" : 26723,
    "prUrl" : "https://github.com/apache/spark/pull/26723#pullrequestreview-324909173",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ee09c08c-597e-4671-aebe-f917abb0816a",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "I'm seeing the needs of this patch, as Utils.resolveURI just uses `file` scheme if there's no scheme, though we may want to handle the case where the scheme is specified in the value of EVENT_LOG_DIR.\r\n\r\nThis would handle the scheme-less case as well as relative case:\r\n\r\n```\r\nval fs = new Path(unresolvedDir).getFileSystem(_hadoopConfiguration)\r\nval qualifiedPath = fs.makeQualified(new Path(unresolvedDir))\r\nSome(qualifiedPath.toUri)\r\n```\r\n\r\nBtw, can we add some tests to check the possible cases for _eventLogDir? You can find some existing FileSystem implementations like FakeFileSystem in Spark code to add tests without HDFS.",
        "createdAt" : "2019-11-30T15:14:26Z",
        "updatedAt" : "2019-11-30T21:23:05Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "4eb2db3e-d460-4ac7-aaf6-a4c197d0d660",
        "parentId" : "ee09c08c-597e-4671-aebe-f917abb0816a",
        "authorId" : "c4496f50-d6b8-41cb-bce8-beb0fad088a1",
        "body" : "agree with you, I have added changes by your suggestions. ",
        "createdAt" : "2019-11-30T21:24:35Z",
        "updatedAt" : "2019-11-30T21:24:35Z",
        "lastEditedBy" : "c4496f50-d6b8-41cb-bce8-beb0fad088a1",
        "tags" : [
        ]
      }
    ],
    "commit" : "622f90ddb4758633032492620bdfe57687352785",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +507,511 @@        val defaultFS = if (defaultFSProperty == null) \"\" else defaultFSProperty\n\n        val unresolvedDir = s\"$defaultFS${conf.get(EVENT_LOG_DIR).stripSuffix(\"/\")}\"\n\n        val fs = new Path(unresolvedDir).getFileSystem(_hadoopConfiguration)"
  },
  {
    "id" : "893ffe33-af67-4d49-af6f-c2f004b8af61",
    "prId" : 26631,
    "prUrl" : "https://github.com/apache/spark/pull/26631#pullrequestreview-321936955",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e3c432a3-90f2-47bd-a251-84d6b646a514",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Hi, @nishkamravi2 .\r\nLike the other review comments, I'm also wondering why you need this.\r\nI'd like to use the existing official API instead of making another public API.\r\n```scala\r\nscala> org.apache.spark.sql.SparkSession.getActiveSession.get.sparkContext\r\nres7: org.apache.spark.SparkContext = org.apache.spark.SparkContext@298154d4\r\n```\r\n\r\nWe recommend users to use `SparkSession` instead of `SparkContext` if possible. And, we provide the API you need like the above.",
        "createdAt" : "2019-11-24T01:30:41Z",
        "updatedAt" : "2019-11-24T01:30:41Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "9cb554af51426050323070245da429ce705a12bd",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +2591,2595 @@\n  /** Return the current active [[SparkContext]] if any. */\n  def getActive: Option[SparkContext] = {\n    SPARK_CONTEXT_CONSTRUCTOR_LOCK.synchronized {\n      Option(activeContext.get())"
  },
  {
    "id" : "04271baf-eca0-4469-9bc7-7dbc486fc1c5",
    "prId" : 26357,
    "prUrl" : "https://github.com/apache/spark/pull/26357#pullrequestreview-310892788",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6f80d0a0-ff3d-49b3-be31-bdb9ace319b8",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I am not sure about this one. `setLogLevel` is basically just for quick debugging purpose in a shell in general. If we want some more complicated log settings, it should go for log4j. ",
        "createdAt" : "2019-11-04T05:57:29Z",
        "updatedAt" : "2019-11-04T05:57:30Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "c1912ce9-1c6c-4482-b91c-912897dc665c",
        "parentId" : "6f80d0a0-ff3d-49b3-be31-bdb9ace319b8",
        "authorId" : "cb93ab63-5789-483a-864c-9467d1506bf5",
        "body" : "Yes, I just added the method to support thriftserver dynamic log level. It also looks great to support shell debugging.",
        "createdAt" : "2019-11-04T06:32:13Z",
        "updatedAt" : "2019-11-04T06:32:14Z",
        "lastEditedBy" : "cb93ab63-5789-483a-864c-9467d1506bf5",
        "tags" : [
        ]
      }
    ],
    "commit" : "a26dd2b38a4ca3a6c27732efa591a9e06fc4dc34",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +381,385 @@  }\n\n  def setLogLevel(logLevel: String, className: String): Unit = {\n    // let's allow lowercase or mixed case too\n    val upperCased = logLevel.toUpperCase(Locale.ROOT)"
  },
  {
    "id" : "e7b924fb-fe19-44f4-b3ed-3e7ebbd90ee5",
    "prId" : 26170,
    "prUrl" : "https://github.com/apache/spark/pull/26170#pullrequestreview-307848802",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8ae1bb75-1445-4d11-8386-71e4604b82a8",
        "parentId" : null,
        "authorId" : "acf5aefc-4c46-451e-a28d-492ceaffd160",
        "body" : "There are obvious advantages to initialize the driver plugin at this early stage, however this is not an ideal point for registering metrics (for those plugins that want to do so), as the metrics source should ideally be registered with _env.metricsSystem which is only started at later point, after the task scheduler has been started. As it is now, driver plugin metrics do not get the application id, so they are difficult to consume. \r\nHow about, for example, splitting the driver plugin code, so that the metrics registration part (if any is needed) can take advantage of registering with env.metricsSystem.registerSource(...)?",
        "createdAt" : "2019-10-24T20:40:40Z",
        "updatedAt" : "2019-10-29T16:43:10Z",
        "lastEditedBy" : "acf5aefc-4c46-451e-a28d-492ceaffd160",
        "tags" : [
        ]
      },
      {
        "id" : "67789a97-acc5-42e7-8f93-74c67cd5f725",
        "parentId" : "8ae1bb75-1445-4d11-8386-71e4604b82a8",
        "authorId" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "body" : "I can take another look, but I'm 99% sure that when I tried `registerSource` after the metrics system had already been initialized, the metrics were not being picked up by `ConsoleSink`.\r\n\r\nMaybe it's a bug in that particular sink, but I didn't investigate that far.",
        "createdAt" : "2019-10-24T20:43:59Z",
        "updatedAt" : "2019-10-29T16:43:10Z",
        "lastEditedBy" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "tags" : [
        ]
      },
      {
        "id" : "ee71a83f-a8ae-4eb5-b3ae-55c9f1e5cea5",
        "parentId" : "8ae1bb75-1445-4d11-8386-71e4604b82a8",
        "authorId" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "body" : "Also, if you really want to, you can register metrics later. Just install a listener and wait for the \"application start\" event. (You just need at least a dummy metric registered here, or your source won't be initialized. But that can be fixed easily.)",
        "createdAt" : "2019-10-24T20:45:41Z",
        "updatedAt" : "2019-10-29T16:43:10Z",
        "lastEditedBy" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "tags" : [
        ]
      },
      {
        "id" : "7d26ef6d-8bf1-43ca-bcaf-94c4c8e89a08",
        "parentId" : "8ae1bb75-1445-4d11-8386-71e4604b82a8",
        "authorId" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "body" : "I'll just move the conversation back here because github doesn't have top-level threads and that's annoying.\r\n\r\n> Just to clarify, I was thinking about registering the plugin source in the driver somewhere \"near\" to what was done in #23838\r\n\r\nThat has the problem I ran into - that's done after `metricsSystem.start()` is called, but in my case metrics added after that were not showing up in `ConsoleSink`.\r\n\r\nAnd the thing is, I can't just move the metrics registration to that spot. I'd have to move all plugin initialization to that spot, otherwise there's no place where the plugin can initialize the metrics before the registration. And I can't move the plugin initialization to that spot, because then I can't send the plugin data to executors via config. I could use RPCs for that but it would slow down executor startup unnecessarily.\r\n\r\nSo, ignoring the `ConsoleSink` issue, I still think my suggestion is better: if you need to initialize your metrics later, then use a listener that is installed from your plugin's init method.\r\n\r\nI can change the code to allow for this late initialization, so you don't have to add a dummy metric in the plugin's init.",
        "createdAt" : "2019-10-24T21:13:35Z",
        "updatedAt" : "2019-10-29T16:43:10Z",
        "lastEditedBy" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "tags" : [
        ]
      },
      {
        "id" : "5056527e-eb13-4109-9c81-2f5fff95bc1d",
        "parentId" : "8ae1bb75-1445-4d11-8386-71e4604b82a8",
        "authorId" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "body" : "Ok, here's what happens. If you add a metric to the registry after you register it as a source, those metrics don't show up. At least not in `ConsoleSink`. So it's ok to register the source at the point you mention as long as all metrics have been registered before then.\r\n\r\nI'll add a new method to the driver plugins for explicitly registering metrics; that way initialization can happen early, and metrics initialization later. It diverges a bit from the executor API, but I don't see a much better alternative. To be able to register plugin RPC endpoints before executors are up, initialization needs to happen early.\r\n\r\n(That also avoids a warning from the metrics system in the output when you register a source before an app ID is known.)",
        "createdAt" : "2019-10-24T21:50:19Z",
        "updatedAt" : "2019-10-29T16:43:10Z",
        "lastEditedBy" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "tags" : [
        ]
      },
      {
        "id" : "9243a126-868b-4d70-aec8-d0afc253f600",
        "parentId" : "8ae1bb75-1445-4d11-8386-71e4604b82a8",
        "authorId" : "acf5aefc-4c46-451e-a28d-492ceaffd160",
        "body" : "I have just tested it and it works for me. Thanks for the work and explanation.",
        "createdAt" : "2019-10-28T13:34:46Z",
        "updatedAt" : "2019-10-29T16:43:10Z",
        "lastEditedBy" : "acf5aefc-4c46-451e-a28d-492ceaffd160",
        "tags" : [
        ]
      }
    ],
    "commit" : "37ad680ec33ec6afac9d031897a549321e782d9c",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +543,547 @@\n    // Initialize any plugins before the task scheduler is initialized.\n    _plugins = PluginContainer(this)\n\n    // Create and start the scheduler"
  },
  {
    "id" : "c6cf6b18-0c8a-4120-a9b6-cf8bafba1a63",
    "prId" : 26078,
    "prUrl" : "https://github.com/apache/spark/pull/26078#pullrequestreview-310621321",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4fcff2fe-c746-44c2-9679-61dce46a6066",
        "parentId" : null,
        "authorId" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "body" : "IIUC, either `numParts` or `amount` should be 1 here? Shall we introduce a var or function to `ResourceRequirement` to represent `amount / numParts` to simplify the code?",
        "createdAt" : "2019-10-29T21:42:47Z",
        "updatedAt" : "2019-11-01T21:58:00Z",
        "lastEditedBy" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "tags" : [
        ]
      },
      {
        "id" : "bcacb1ea-88cf-49e2-bf18-a564fa2c8bf1",
        "parentId" : "4fcff2fe-c746-44c2-9679-61dce46a6066",
        "authorId" : "0f2e4443-2e01-4589-9e1c-17979892188b",
        "body" : "I think that you could go either way. If you feel strongly that this is confusing, I can try to change the case class. I like the formula here because it shows you explicitly how slots are computed.",
        "createdAt" : "2019-11-01T18:25:25Z",
        "updatedAt" : "2019-11-01T21:58:00Z",
        "lastEditedBy" : "0f2e4443-2e01-4589-9e1c-17979892188b",
        "tags" : [
        ]
      }
    ],
    "commit" : "d579c5d50ee793a09eebbb94f7998012ec81aa1e",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +2794,2798 @@        // executor resources. If the amount per task was > 1.0, the task wants\n        // multiple executor resources.\n        val resourceNumSlots = Math.floor(execAmount * taskReq.numParts / taskReq.amount).toInt\n        if (resourceNumSlots < numSlots) {\n          numSlots = resourceNumSlots"
  },
  {
    "id" : "5ac8cc02-559f-4fc5-97f6-f674e2c39542",
    "prId" : 26078,
    "prUrl" : "https://github.com/apache/spark/pull/26078#pullrequestreview-310619825",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5bfce453-614c-4498-a054-c1073f54ebd7",
        "parentId" : null,
        "authorId" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "body" : "In this case the `taskReq.amount` should always be `1` ?",
        "createdAt" : "2019-10-29T21:45:34Z",
        "updatedAt" : "2019-11-01T21:58:00Z",
        "lastEditedBy" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "tags" : [
        ]
      },
      {
        "id" : "2a542b79-fd5f-4356-ba96-595376b6ef49",
        "parentId" : "5bfce453-614c-4498-a054-c1073f54ebd7",
        "authorId" : "0f2e4443-2e01-4589-9e1c-17979892188b",
        "body" : "Yes that is correct. Though I am not sure this hurts. I like the idea of seeing it in text if it is misconfigured, but let me know if you want that changed.",
        "createdAt" : "2019-11-01T18:22:42Z",
        "updatedAt" : "2019-11-01T21:58:00Z",
        "lastEditedBy" : "0f2e4443-2e01-4589-9e1c-17979892188b",
        "tags" : [
        ]
      }
    ],
    "commit" : "d579c5d50ee793a09eebbb94f7998012ec81aa1e",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +2806,2810 @@        if ((numSlots * taskReq.amount / taskReq.numParts) < execAmount) {\n          val taskReqStr = if (taskReq.numParts > 1) {\n            s\"${taskReq.amount}/${taskReq.numParts}\"\n          } else {\n            s\"${taskReq.amount}\""
  },
  {
    "id" : "05c8b423-9198-4fdc-b832-c0852b927fd4",
    "prId" : 25668,
    "prUrl" : "https://github.com/apache/spark/pull/25668#pullrequestreview-284455436",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "77709419-5311-4abf-9127-d5e6f33c1058",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "I'm not sure this is correct. See the next 4 lines.",
        "createdAt" : "2019-09-04T13:58:41Z",
        "updatedAt" : "2019-09-04T13:58:42Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "4564e0ba-acb2-4277-b63d-02c69de444dc",
        "parentId" : "77709419-5311-4abf-9127-d5e6f33c1058",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "really I think this should be turned into conf.get(DRIVER_CORES) which has the right default.\r\n\r\nDRIVER_CORES = ConfigBuilder(\"spark.driver.cores\")\r\n    .doc(\"Number of cores to use for the driver process, only in cluster mode.\")\r\n    .intConf\r\n    .createWithDefault(1)\r\n",
        "createdAt" : "2019-09-05T16:46:52Z",
        "updatedAt" : "2019-09-05T16:46:52Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "f598dcfb-3ed4-40f2-b632-1e48971cae1a",
        "parentId" : "77709419-5311-4abf-9127-d5e6f33c1058",
        "authorId" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "body" : "Hmm.\r\n\r\nThis is used eventually in the call to `NettyUtils.defaultNumThreads`. Changing this from 0 to 1 means that yarn-cluster apps will run with 1 thread for RPC handling instead of the default (which is `Math.min(availableCores, 8)`).\r\n\r\nSo I'm -1 on this change. Unless the problem above is handled in a different way.",
        "createdAt" : "2019-09-05T17:52:50Z",
        "updatedAt" : "2019-09-05T17:52:50Z",
        "lastEditedBy" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "tags" : [
        ]
      },
      {
        "id" : "74ac44c0-cebc-4abf-ad05-b324dfa61b8f",
        "parentId" : "77709419-5311-4abf-9127-d5e6f33c1058",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "ah yes you are right, I missed that, agree",
        "createdAt" : "2019-09-05T18:26:29Z",
        "updatedAt" : "2019-09-05T18:26:29Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "0f7d6f60e68f89558b7f405677f4a1026e036d82",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +2715,2719 @@      case \"yarn\" =>\n        if (conf != null && conf.get(SUBMIT_DEPLOY_MODE) == \"cluster\") {\n          conf.getInt(DRIVER_CORES.key, 1)\n        } else {\n          0"
  },
  {
    "id" : "44348ae1-b66c-4eda-b3cc-80492318eb14",
    "prId" : 24909,
    "prUrl" : "https://github.com/apache/spark/pull/24909#pullrequestreview-261333477",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0bfd7991-9eb0-49cd-a102-e6b3ff12af34",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Hm, why null here? if you can't check existence, I'd think you either want to continue anyway with a warning and return path, or throw an exception (or just let the exception throw)?",
        "createdAt" : "2019-07-12T13:24:46Z",
        "updatedAt" : "2019-07-15T06:17:05Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "4ab2ab88-7a43-4799-a557-c8a4ba079584",
        "parentId" : "0bfd7991-9eb0-49cd-a102-e6b3ff12af34",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "I am confused too, since for local file , it will just continue with a warn, then return a null, the the wrong path won't be add to jar path collection. Seems reasonable too. \r\nIt's reasonable to do like this for dfs file the same.",
        "createdAt" : "2019-07-12T14:07:09Z",
        "updatedAt" : "2019-07-15T06:17:05Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "645783be-3182-4c36-9abf-e441528e6d0c",
        "parentId" : "0bfd7991-9eb0-49cd-a102-e6b3ff12af34",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Oh I see, the existing code does that too.",
        "createdAt" : "2019-07-12T15:39:20Z",
        "updatedAt" : "2019-07-15T06:17:05Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      }
    ],
    "commit" : "780a2b58326e13994444b0e322e82d4eba3a9952",
    "line" : 30,
    "diffHunk" : "@@ -1,1 +1810,1814 @@          case NonFatal(e) =>\n            logError(s\"Failed to add $path to Spark environment\", e)\n            null\n        }\n      } else {"
  },
  {
    "id" : "be3ff0c5-3b0a-442c-89a1-927820b2bb24",
    "prId" : 24856,
    "prUrl" : "https://github.com/apache/spark/pull/24856#pullrequestreview-251170874",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4cfd3fd5-bc27-4cd2-8b45-3b01a492e42a",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "I'd prefer to log warn the limited `numSlots` right above `taskResourceRequirements.foreach` if `limitingResourceName` changed. And in `message`, appending the specific amount of the wasted resource to show user the config result directly.",
        "createdAt" : "2019-06-14T15:17:36Z",
        "updatedAt" : "2019-06-18T18:50:56Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "d119b4b6-c14d-4822-8a97-27211bf060cd",
        "parentId" : "4cfd3fd5-bc27-4cd2-8b45-3b01a492e42a",
        "authorId" : "0c812942-02cb-4975-9748-394d1387affa",
        "body" : "I think it is unnecessary because in most use cases we will only see one CPU and one additional resource.",
        "createdAt" : "2019-06-18T15:15:23Z",
        "updatedAt" : "2019-06-18T18:50:56Z",
        "lastEditedBy" : "0c812942-02cb-4975-9748-394d1387affa",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3915856344f7b260397b5582d6738accf319619",
    "line" : 131,
    "diffHunk" : "@@ -1,1 +2729,2733 @@            s\"(exec = ${execAmount}, task = ${taskReq.amount}) will result in wasted \" +\n            s\"resources due to resource ${limitingResourceName} limiting the number of \" +\n            s\"runnable tasks per executor to: ${numSlots}. Please adjust your configuration.\"\n          if (Utils.isTesting) {\n            throw new SparkException(message)"
  },
  {
    "id" : "02796f36-a8cd-4ed3-86c7-abcfdfebe70c",
    "prId" : 24615,
    "prUrl" : "https://github.com/apache/spark/pull/24615#pullrequestreview-241255113",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a2d3ecc1-ff34-493f-8cdb-80444ae5fbd1",
        "parentId" : null,
        "authorId" : "0c812942-02cb-4975-9748-394d1387affa",
        "body" : "It is possible that some resources have addresses passed in while some other resources require discovery script. So it is not one or the other.",
        "createdAt" : "2019-05-21T18:51:17Z",
        "updatedAt" : "2019-05-23T14:59:29Z",
        "lastEditedBy" : "0c812942-02cb-4975-9748-394d1387affa",
        "tags" : [
        ]
      },
      {
        "id" : "1dd14481-e278-41ec-84e3-0828b5d6dd75",
        "parentId" : "a2d3ecc1-ff34-493f-8cdb-80444ae5fbd1",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "in what case are you thinking?   The address config is really an internal config meant for standalone mode.  \r\nThe only case I can think of is if a user requests a resource the standalone mode worker handles but then asks for another resource that it doesn't and thus needs discoveryScript.   But that means that they are on their own for handling isolation and assignment. I kind of question whether we want to allow this but I also haven't seen the standalone implemnetation",
        "createdAt" : "2019-05-21T19:16:13Z",
        "updatedAt" : "2019-05-23T14:59:29Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "4c812a21-ab97-4403-89a6-900dc6e7d12e",
        "parentId" : "a2d3ecc1-ff34-493f-8cdb-80444ae5fbd1",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "also note this matches the way the executor does it as well. For the executor, either the worker in standalone mode passes in a resources file with what it has allocated per executor or it doesn't pass a file and we use the discovery scripts.  Yarn and k8s I don't expect to have issues with this as they will always use discovery Script.  If we want to be able to support both ways we should update both places.   I don't run standalone mode much so if you have a usecase this makes sense we can certainly change it.",
        "createdAt" : "2019-05-21T19:33:02Z",
        "updatedAt" : "2019-05-23T14:59:29Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "c8e42936-223d-4c2d-acb7-ee8869b455d5",
        "parentId" : "a2d3ecc1-ff34-493f-8cdb-80444ae5fbd1",
        "authorId" : "0c812942-02cb-4975-9748-394d1387affa",
        "body" : "It is fine to have that assumption. But we should check and throw when `resourcesWithAddrsInConfs.nonEmpty` but there are remaining resources that require discovery. In the current implementation, it is silently ignored.",
        "createdAt" : "2019-05-22T15:35:16Z",
        "updatedAt" : "2019-05-23T14:59:29Z",
        "lastEditedBy" : "0c812942-02cb-4975-9748-394d1387affa",
        "tags" : [
        ]
      },
      {
        "id" : "f7471c13-d2a8-490d-a38a-d641f0842a1a",
        "parentId" : "a2d3ecc1-ff34-493f-8cdb-80444ae5fbd1",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "we can support both ways, lets do this with the abstraction layer changes though",
        "createdAt" : "2019-05-23T14:48:46Z",
        "updatedAt" : "2019-05-23T14:59:29Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "768392ca83c29a2ef6b3ecb2d175cccd20ffa0ab",
    "line" : 40,
    "diffHunk" : "@@ -1,1 +383,387 @@      SparkConf.getConfigsWithSuffix(allDriverResourceConfs, SPARK_RESOURCE_ADDRESSES_SUFFIX)\n\n    _resources = if (resourcesWithAddrsInConfs.nonEmpty) {\n      resourcesWithAddrsInConfs.map { case (rName, addrString) =>\n        val addrsArray = addrString.split(\",\").map(_.trim())"
  }
]