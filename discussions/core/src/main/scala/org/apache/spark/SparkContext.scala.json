[
  {
    "id" : "77ae7ec0-1c53-4df0-889d-78c1f8cd2cd8",
    "prId" : 33457,
    "prUrl" : "https://github.com/apache/spark/pull/33457#pullrequestreview-716870447",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1a99987b-05fa-4b25-b3f5-d97b84f9882b",
        "parentId" : null,
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "btw, why do we need to attach all the handlers again? And there are so many \"attachAllHandler\" in the code changes.",
        "createdAt" : "2021-07-27T12:34:15Z",
        "updatedAt" : "2021-07-27T12:34:54Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "774d52b2-af84-493b-9541-a0019bc3581c",
        "parentId" : "1a99987b-05fa-4b25-b3f5-d97b84f9882b",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> btw, why do we need to attach all the handlers again? And there are so many \"attachAllHandler\" in the code changes.\r\n\r\nThat me clarify the process.\r\nBefore this change. We add handlers to SparkUI but not start it when init SparkUI since we not start jetty server.\r\nThen it call `bind()`, in this method we start jetty server and attach all handler to server.\r\nthis cause a problem that when we call `bind()`, we expose all servlet API to user. But application not fully started yet.\r\nNow in this pr, I split the behavior of start jetty server and attach handlers to server.\r\nWe need to bind address(start jetty server) first before start AM since we need driver url address to bind with spark proxy server.\r\n\r\nThen after application fully started, we attach all handlers to server(means expose UI url to user).\r\n\r\n\r\nFor this comment https://github.com/apache/spark/pull/33457#issuecomment-886624148\r\nI add a initHandler to handle all request between  starting jetty server and  fully start application to show the hint message.\r\n",
        "createdAt" : "2021-07-27T12:46:16Z",
        "updatedAt" : "2021-07-27T12:46:17Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "cf960bda-fdb6-47e2-96d8-059d398f6f20",
        "parentId" : "1a99987b-05fa-4b25-b3f5-d97b84f9882b",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "> I split the behavior of start jetty server and attach handlers to server\r\n\r\nI add some logging and find that `attachHandler` is still called multiple times before `attachAllHandler`",
        "createdAt" : "2021-07-27T13:37:09Z",
        "updatedAt" : "2021-07-27T13:37:09Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "2fd16ec9-92d2-4e6d-a34c-d2891bbac962",
        "parentId" : "1a99987b-05fa-4b25-b3f5-d97b84f9882b",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> > I split the behavior of start jetty server and attach handlers to server\r\n> \r\n> I add some logging and find that `attachHandler` is still called multiple times before `attachAllHandler`\r\n\r\nYes, but when we call `attachHandler()` the `serverInfo` is none, so the handler is not attached to the server.\r\n",
        "createdAt" : "2021-07-27T13:56:18Z",
        "updatedAt" : "2021-07-27T13:56:18Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "05716be7-44bf-4df0-a12c-10bb18c1aa06",
        "parentId" : "1a99987b-05fa-4b25-b3f5-d97b84f9882b",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "Is there a way to avoid calling `attachAllHandler` in so many places?",
        "createdAt" : "2021-07-27T15:35:59Z",
        "updatedAt" : "2021-07-27T15:35:59Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "26148544-974d-41f5-8b5a-7dc7cfa3af33",
        "parentId" : "1a99987b-05fa-4b25-b3f5-d97b84f9882b",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> Is there a way to avoid calling `attachAllHandler` in so many places?\r\n\r\nAdd a new method call it `bindAndAttachAllHandler()` then replace other place to call this one? only call `bind()` and `attachAllHandler` in  SparkContext?",
        "createdAt" : "2021-07-27T16:04:16Z",
        "updatedAt" : "2021-07-27T16:04:17Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "c6243036-a033-4bde-a2c3-9a1811c897a0",
        "parentId" : "1a99987b-05fa-4b25-b3f5-d97b84f9882b",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "Will that work for history server and master/worker UI? If yes let's try to avoid changing them.",
        "createdAt" : "2021-07-27T16:18:17Z",
        "updatedAt" : "2021-07-27T16:18:17Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "18c38b9d-cd03-4b70-a850-ed37f9f8faaf",
        "parentId" : "1a99987b-05fa-4b25-b3f5-d97b84f9882b",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> Will that work for history server and master/worker UI? If yes let's try to avoid changing them.\r\n\r\nHow about current?",
        "createdAt" : "2021-07-27T16:45:01Z",
        "updatedAt" : "2021-07-27T16:45:01Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "cd8483bf-0017-4096-b3b5-11def3522e75",
        "parentId" : "1a99987b-05fa-4b25-b3f5-d97b84f9882b",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "I mean, will history server and master/worker UI show the same page(spark is starting up...) on starting up?",
        "createdAt" : "2021-07-28T10:53:07Z",
        "updatedAt" : "2021-07-28T10:53:27Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "9ace0a0c-5ade-484a-bc50-6f4d2b75fc85",
        "parentId" : "1a99987b-05fa-4b25-b3f5-d97b84f9882b",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> I mean, will history server and master/worker UI show the same page(spark is starting up...) on starting up?\r\n\r\nWith current code, won't impact history server/master/worker web UI. and   history server/master/worker web UI doesn't have such problem.",
        "createdAt" : "2021-07-28T11:09:38Z",
        "updatedAt" : "2021-07-28T11:09:38Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "0c9c3ce97871ae1d081c3e3cdc32034b1a1e66fd",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +639,643 @@\n    // After application started, attach handlers to started server and start handler.\n    _ui.foreach(_.attachAllHandler())\n    // Attach the driver metrics servlet handler to the web ui after the metrics system is started.\n    _env.metricsSystem.getServletHandlers.foreach(handler => ui.foreach(_.attachHandler(handler)))"
  },
  {
    "id" : "d6c99155-4ed9-46d7-8370-b357a5812735",
    "prId" : 32648,
    "prUrl" : "https://github.com/apache/spark/pull/32648#pullrequestreview-666442028",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8131f32c-6de4-402c-9ac0-174b1604cd79",
        "parentId" : null,
        "authorId" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "body" : "```\r\n[ERROR] [Error] /spark/core/src/main/scala/org/apache/spark/SparkContext.scala:355: weaker access privileges in overriding\r\nprivate[package lang] def childValue(x$1: java.util.Properties): java.util.Properties (defined in class ThreadLocal)\r\n  override should at least be private[lang]\r\n```",
        "createdAt" : "2021-05-24T06:46:31Z",
        "updatedAt" : "2021-05-24T06:47:20Z",
        "lastEditedBy" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "tags" : [
        ]
      }
    ],
    "commit" : "2a867a6277c1c6c4f1a6497973588304f907bede",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +353,357 @@  // Thread Local variable that can be used by users to pass information down the stack\n  protected[spark] val localProperties = new InheritableThreadLocal[Properties] {\n    override def childValue(parent: Properties): Properties = {\n      // Note: make a clone such that changes in the parent properties aren't reflected in\n      // the those of the children threads, which has confusing semantics (SPARK-10563)."
  },
  {
    "id" : "cfcc8465-afde-4c8d-b5de-cd24d7b93459",
    "prId" : 32518,
    "prUrl" : "https://github.com/apache/spark/pull/32518#pullrequestreview-657993847",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2f5bcd94-36a6-4ba7-a534-b491778fca6d",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@dongjoon-hyun, should we maybe do this in `SparkSubmitArguments` with `sparkProperties` like `ignoreNonSparkProperties` handling? I think that;s as early as possible",
        "createdAt" : "2021-05-12T06:29:13Z",
        "updatedAt" : "2021-05-12T06:29:13Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "c8e3baa8-42ba-4640-a1b1-f0fe955f641e",
        "parentId" : "2f5bcd94-36a6-4ba7-a534-b491778fca6d",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Yes, it's possible to handle there for SparkSubmit command parameters, however Hadoop configurations also can be handed over to `SparkSession` inside Spark Apps after Spark submits.",
        "createdAt" : "2021-05-12T14:56:23Z",
        "updatedAt" : "2021-05-12T15:07:18Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "afeb68fadb57b7d02258cbd3a2cc540e73f803ff",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +398,402 @@    }\n    // This should be set as early as possible.\n    SparkContext.fillMissingMagicCommitterConfsIfNeeded(_conf)\n\n    _driverLogger = DriverLogger(_conf)"
  },
  {
    "id" : "81ae343f-05e5-4ca5-bd99-c8ddbd466646",
    "prId" : 31953,
    "prUrl" : "https://github.com/apache/spark/pull/31953#pullrequestreview-619747303",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "599c394f-0296-41af-81ba-a17faf09cc4c",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "You can probably simplify by creating two Options for the two properties as vals first",
        "createdAt" : "2021-03-24T13:55:18Z",
        "updatedAt" : "2021-03-24T13:55:18Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      }
    ],
    "commit" : "8bafffbdafccad64b961bdc85337594588aaff84",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +2187,2191 @@   */\n  private[spark] def getCallSite(): CallSite = {\n    if (getLocalProperty(CallSite.SHORT_FORM) == null\n      || getLocalProperty(CallSite.LONG_FORM) == null) {\n      val callSite = Utils.getCallSite()"
  },
  {
    "id" : "874796f6-b076-45dc-b377-fd76521c367a",
    "prId" : 31953,
    "prUrl" : "https://github.com/apache/spark/pull/31953#pullrequestreview-622939903",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "076d2ca9-a076-4380-9691-9e424ae33307",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@lxian, it would be great if we can elabourate how it causes the lock as you did in JIRA. In addition, to clarify, this is not an issue in Spark 3+ as we dropped Scala 2.11 at SPARK-26132.\r\n\r\nCan you create a PR for bracnh-2.4 alone? cc @viirya and @dongjoon-hyun FYI",
        "createdAt" : "2021-03-25T03:11:46Z",
        "updatedAt" : "2021-03-25T03:11:46Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "34858135-4a86-4d7c-ac60-26ec532f81e6",
        "parentId" : "076d2ca9-a076-4380-9691-9e424ae33307",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Is this easy to add test? Otherwise we can also show up how to reproduce it manually in the description.",
        "createdAt" : "2021-03-25T06:59:43Z",
        "updatedAt" : "2021-03-25T06:59:43Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "9b2668b5-b687-4c68-a9b7-e6e900081462",
        "parentId" : "076d2ca9-a076-4380-9691-9e424ae33307",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "@lxian Can you create a PR for branch-2.4? If you are busy, would you mind I create a PR for branch-2.4? Thanks.",
        "createdAt" : "2021-03-29T03:55:51Z",
        "updatedAt" : "2021-03-29T03:55:51Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "11ee410e-1b40-4bbd-ad68-0afcd1a55947",
        "parentId" : "076d2ca9-a076-4380-9691-9e424ae33307",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@viirya I believe it's fine for you to just go ahead IMO .. the author became inactive 4 days and this is the blocker of 2.4 (I guess?).",
        "createdAt" : "2021-03-29T04:07:42Z",
        "updatedAt" : "2021-03-29T04:07:42Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "894cb5f1-88f7-4967-9841-e3fafab9b2b8",
        "parentId" : "076d2ca9-a076-4380-9691-9e424ae33307",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "This is the last issue for 2.4, I think. Okay, let me create a PR first. I can close it if the author opens his after.",
        "createdAt" : "2021-03-29T04:16:22Z",
        "updatedAt" : "2021-03-29T04:16:23Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "5300db07-bb52-4a4c-83d8-3265ac060112",
        "parentId" : "076d2ca9-a076-4380-9691-9e424ae33307",
        "authorId" : "9708b891-7e0d-4e53-ab50-2cc008f04131",
        "body" : "@viirya Thank you for creating the PR. you can close mine if needed. ",
        "createdAt" : "2021-03-29T07:19:03Z",
        "updatedAt" : "2021-03-29T07:19:03Z",
        "lastEditedBy" : "9708b891-7e0d-4e53-ab50-2cc008f04131",
        "tags" : [
        ]
      },
      {
        "id" : "bb712e7a-fff3-4b2b-b3b3-60f60a128bc2",
        "parentId" : "076d2ca9-a076-4380-9691-9e424ae33307",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Thank you @lxian. Your authorship is still kept in the commit.",
        "createdAt" : "2021-03-29T07:40:33Z",
        "updatedAt" : "2021-03-29T07:40:33Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "8bafffbdafccad64b961bdc85337594588aaff84",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +2187,2191 @@   */\n  private[spark] def getCallSite(): CallSite = {\n    if (getLocalProperty(CallSite.SHORT_FORM) == null\n      || getLocalProperty(CallSite.LONG_FORM) == null) {\n      val callSite = Utils.getCallSite()"
  },
  {
    "id" : "1e938f96-f40a-4f7c-9c93-01903a0d2df7",
    "prId" : 31718,
    "prUrl" : "https://github.com/apache/spark/pull/31718#pullrequestreview-635372845",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b56631da-8d5b-469f-8318-e9a9721882e8",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@sarutak, I think we can just repalce `Path` here too (looks like you're fixing this problem across the codebase, right?)",
        "createdAt" : "2021-04-14T01:47:42Z",
        "updatedAt" : "2021-04-14T01:47:42Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "6380025c-1dcc-4563-a9fd-b50c683c50ad",
        "parentId" : "b56631da-8d5b-469f-8318-e9a9721882e8",
        "authorId" : "3a227965-84e0-47cf-9974-11293764f028",
        "body" : "Yeah, the behavior of `Path` seems to be a little bit confusable and there may be lots of places where `Path` does not behave  as we expect.\r\nO.K, I'll looking into it.",
        "createdAt" : "2021-04-14T01:55:58Z",
        "updatedAt" : "2021-04-14T01:55:58Z",
        "lastEditedBy" : "3a227965-84e0-47cf-9974-11293764f028",
        "tags" : [
        ]
      },
      {
        "id" : "207ab294-3d60-46d1-9318-cd246bc7c86d",
        "parentId" : "b56631da-8d5b-469f-8318-e9a9721882e8",
        "authorId" : "3a227965-84e0-47cf-9974-11293764f028",
        "body" : "@HyukjinKwon BTW, Just replacing `Path` is my first idea of this PR.\r\nhttps://github.com/apache/spark/pull/31718/commits/3c40f988de81600a75190751bf9987dfc93d09e9#diff-1e596e9e9bd50e3075847a66f8fecee741289b2b0478f9099c3bf2b680ebaa35R1586\r\n\r\nBut I changed to reflect [your comment](https://github.com/apache/spark/pull/31718#issuecomment-792225217).\r\nAnyway, I'll open a followup PR.",
        "createdAt" : "2021-04-14T07:32:20Z",
        "updatedAt" : "2021-04-14T07:32:21Z",
        "lastEditedBy" : "3a227965-84e0-47cf-9974-11293764f028",
        "tags" : [
        ]
      },
      {
        "id" : "802e7e53-ccc3-40fc-bdb7-a01d95b31e83",
        "parentId" : "b56631da-8d5b-469f-8318-e9a9721882e8",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Yeah, I know :). Sorry for a bit of forth and back. If we change it all in the code base, I think it makes more sense to land a correct fix rather then staying safer and conservative with a bandaid fix",
        "createdAt" : "2021-04-14T08:53:07Z",
        "updatedAt" : "2021-04-14T08:53:07Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "9dc15b92d3ea94e6bb5f00ff02da3655fa9ae381",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +1591,1595 @@      }\n    } else {\n      Utils.resolveURI(path)\n    }\n    val schemeCorrectedURI = uri.getScheme match {"
  },
  {
    "id" : "f9417e83-d1dd-4288-bd8e-b5535db310ff",
    "prId" : 31521,
    "prUrl" : "https://github.com/apache/spark/pull/31521#pullrequestreview-585570977",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a683af74-2c14-47ab-9c87-38ad4d33e2f8",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Do we expect users would set `spark.job.interruptOnCance` (as I notice it's private)? I think it may be better to prevent users from setting this kind of private property instead of doing the valid check.  And we could recommend users to use `setJobGroup` instead. cc @tgravescs @mridulm any thoughts about this?\r\n",
        "createdAt" : "2021-02-08T12:33:22Z",
        "updatedAt" : "2021-02-08T14:29:58Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "639aad76-d2a1-440b-8515-892fe64817db",
        "parentId" : "a683af74-2c14-47ab-9c87-38ad4d33e2f8",
        "authorId" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "body" : "I perfer to the suggestion, forbid is better. If so we might need an another internal method to set local property for some test case.",
        "createdAt" : "2021-02-08T14:29:38Z",
        "updatedAt" : "2021-02-08T14:29:38Z",
        "lastEditedBy" : "baca2fab-b749-483f-8c77-c4db14eca9d9",
        "tags" : [
        ]
      },
      {
        "id" : "de6782c3-c45a-4857-8350-4041b6b13ac0",
        "parentId" : "a683af74-2c14-47ab-9c87-38ad4d33e2f8",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Yeah, we need the internal method if we want to forbid. May let's see others' feedback first.",
        "createdAt" : "2021-02-08T14:32:54Z",
        "updatedAt" : "2021-02-08T14:32:54Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "d938f3c4-65c6-4695-be36-f9452ad3731e",
        "parentId" : "a683af74-2c14-47ab-9c87-38ad4d33e2f8",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "we should just change this to use the ConfigBuilder. mark it as internal and the toBoolean function already does this same check to make sure valid.  That doesn't prevent them from doing it though so if we wanted to add other checking for all internal that might be a bit more work and complex as I'm sure we use some for testing.",
        "createdAt" : "2021-02-08T14:42:58Z",
        "updatedAt" : "2021-02-08T14:42:58Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "ad62af8f-20b2-4f2b-9347-56925e36bf0a",
        "parentId" : "a683af74-2c14-47ab-9c87-38ad4d33e2f8",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "oh sorry I guess we can't do that because local property, I would like to more about context ",
        "createdAt" : "2021-02-08T14:44:32Z",
        "updatedAt" : "2021-02-08T14:46:11Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "17669d900568635be2151870101f0d6b33885230",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +720,724 @@      localProperties.get.remove(key)\n    } else {\n      checkLocalProperty(key, value)\n      localProperties.get.setProperty(key, value)\n    }"
  },
  {
    "id" : "be4db0f2-b21e-439c-bce3-abff8427ad7f",
    "prId" : 31227,
    "prUrl" : "https://github.com/apache/spark/pull/31227#pullrequestreview-570476370",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "34ca2a53-360d-496f-bc31-285565e3c3a5",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "We can define it in sql/core module, as it's only used there and the STS module. Probably in `SQLExecution` object.",
        "createdAt" : "2021-01-18T13:11:13Z",
        "updatedAt" : "2021-01-18T13:11:13Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "1a751ba9933b42750b33efd4c65dbee065425781",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +2769,2773 @@   * Statement id is only used for thrift server\n   */\n  private[spark] val SPARK_STATEMENT_ID = \"spark.statement.id\"\n\n  /**"
  },
  {
    "id" : "4c5bfcf1-692c-4624-bb36-e754c84417b4",
    "prId" : 30581,
    "prUrl" : "https://github.com/apache/spark/pull/30581#pullrequestreview-543951766",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "16cf9175-0b06-464f-a167-1d8a6895b887",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "This change is actually not related but I found it during debug. This isn't a bug but improvement:\r\n\r\nBy using `file:/` instead of `spark:/`  on the driver side, we can directly copy the file in driver side instead of fetching through the Netty file server layer. This is matched with `spark.files`.\r\n",
        "createdAt" : "2020-12-03T13:29:04Z",
        "updatedAt" : "2020-12-04T05:52:39Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "9f048008d061bbed27e8df6831b3b5386088669f",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +1642,1646 @@      // If the scheme is file, use URI to simply copy instead of downloading.\n      val uriToUse = if (!isLocal && scheme == \"file\") uri else new URI(key)\n      val uriToDownload = UriBuilder.fromUri(uriToUse).fragment(null).build()\n      val source = Utils.fetchFile(uriToDownload.toString, Utils.createTempDir(), conf,\n        env.securityManager, hadoopConfiguration, timestamp, useCache = false, shouldUntar = false)"
  },
  {
    "id" : "38dd6dca-0612-4c9e-8c99-36fb3a7fcf17",
    "prId" : 30522,
    "prUrl" : "https://github.com/apache/spark/pull/30522#pullrequestreview-540407565",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2b59ff8e-4928-4122-bae2-3720ffbbd6dc",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Spark config is supposed to be immutable. I don't think we should allow this in Spark context. If you should set a static SQL config, you should either stop and start the context again or set it initially when you create a Spark context.",
        "createdAt" : "2020-11-28T00:09:45Z",
        "updatedAt" : "2020-11-28T00:10:03Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "daa32519-aa49-4476-abc5-5c6fa02d8e23",
        "parentId" : "2b59ff8e-4928-4122-bae2-3720ffbbd6dc",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Agree with @HyukjinKwon. Static SQL configs are static and immutable. Forcefully modifying the configs could cause unexpected behavior.",
        "createdAt" : "2020-11-28T17:58:12Z",
        "updatedAt" : "2020-11-28T17:58:13Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "d1f3578a8846afd1ef56973bed44fb07472d9a58",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +712,716 @@\n  /** Set spark conf */\n  def setSparkConf(sparkConf: SparkConf): Unit = {\n    _conf = sparkConf\n  }"
  },
  {
    "id" : "43baa75c-5e87-41e1-a76a-07ecda777ad8",
    "prId" : 30486,
    "prUrl" : "https://github.com/apache/spark/pull/30486#pullrequestreview-537540290",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "90a5ad5e-1794-4cd3-bfd3-0a082b89eb16",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Here we cannot rely on `new Path(path).toUri`. it makes the fragment (`#`) in URI as the part of path. `Utils.resolveURI` is used for `spark.yarn.dist.archives` as well.",
        "createdAt" : "2020-11-24T14:23:02Z",
        "updatedAt" : "2020-11-30T05:09:49Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "e35e94cad2888c98e03feb391a8dfcca8afa365c",
    "line" : 112,
    "diffHunk" : "@@ -1,1 +1586,1590 @@      new Path(path).toUri\n    } else {\n      Utils.resolveURI(path)\n    }\n    val schemeCorrectedURI = uri.getScheme match {"
  },
  {
    "id" : "3c7193d3-758e-45d9-a643-d5bbd92961e4",
    "prId" : 30486,
    "prUrl" : "https://github.com/apache/spark/pull/30486#pullrequestreview-537541608",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6a2fc4fb-17d0-4ad8-92d5-8c30f79ab5d3",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Archive is not supposed to be a directory.",
        "createdAt" : "2020-11-24T14:24:21Z",
        "updatedAt" : "2020-11-30T05:09:49Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "e35e94cad2888c98e03feb391a8dfcca8afa365c",
    "line" : 122,
    "diffHunk" : "@@ -1,1 +1599,1603 @@    val hadoopPath = new Path(schemeCorrectedURI)\n    val scheme = schemeCorrectedURI.getScheme\n    if (!Array(\"http\", \"https\", \"ftp\").contains(scheme) && !isArchive) {\n      val fs = hadoopPath.getFileSystem(hadoopConfiguration)\n      val isDir = fs.getFileStatus(hadoopPath).isDirectory"
  },
  {
    "id" : "dd327f32-84c9-4103-a796-245bd834260a",
    "prId" : 30486,
    "prUrl" : "https://github.com/apache/spark/pull/30486#pullrequestreview-537542208",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0fcafae6-421e-4825-83ec-63ee134a5e1b",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "For the same reason of keeping the fragment, it uses URI when it's archive.",
        "createdAt" : "2020-11-24T14:24:56Z",
        "updatedAt" : "2020-11-30T05:09:49Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "e35e94cad2888c98e03feb391a8dfcca8afa365c",
    "line" : 133,
    "diffHunk" : "@@ -1,1 +1620,1624 @@      schemeCorrectedURI.toString\n    } else if (isArchive) {\n      uri.toString\n    } else {\n      path"
  },
  {
    "id" : "67cfd990-4c3b-4ff9-9f5b-c47d31b1861c",
    "prId" : 30486,
    "prUrl" : "https://github.com/apache/spark/pull/30486#pullrequestreview-538054514",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0b2b17b3-511d-4d04-bc3d-60e6c8896894",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Is this limitation applied to the other ways like `spark.files` too?",
        "createdAt" : "2020-11-24T17:13:54Z",
        "updatedAt" : "2020-11-30T05:09:49Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "b864c5e6-0e21-4bcc-ae90-c6f1fc8fec50",
        "parentId" : "0b2b17b3-511d-4d04-bc3d-60e6c8896894",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Yeah, it does. It was actually copied from `addFile`.",
        "createdAt" : "2020-11-24T23:44:44Z",
        "updatedAt" : "2020-11-30T05:09:49Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "e35e94cad2888c98e03feb391a8dfcca8afa365c",
    "line" : 71,
    "diffHunk" : "@@ -1,1 +1537,1541 @@   * Add an archive to be downloaded and unpacked with this Spark job on every node.\n   *\n   * If an archive is added during execution, it will not be available until the next TaskSet\n   * starts.\n   *"
  },
  {
    "id" : "3a4a035a-3659-4a9b-962a-066d1a3661e2",
    "prId" : 30486,
    "prUrl" : "https://github.com/apache/spark/pull/30486#pullrequestreview-538373049",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "03861327-dd5c-4a35-b641-ae4e383e6b1a",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "> --archives ARCHIVES         Comma-separated list of archives to be extracted into the working directory of each executor.\r\n\r\nDo we need to download archives at driver as it only declares they are extracted to executors? \r\n\r\n",
        "createdAt" : "2020-11-25T09:17:46Z",
        "updatedAt" : "2020-11-30T05:09:49Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "06013097-b659-4d19-bee5-d20a574fed8e",
        "parentId" : "03861327-dd5c-4a35-b641-ae4e383e6b1a",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I think it's fine to unpack/download on the driver side as well to mimic the previous Yarn's behaviour. At least seems that's what Yarn does in Yarn cluster mode.",
        "createdAt" : "2020-11-25T10:27:52Z",
        "updatedAt" : "2020-11-30T05:09:49Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "037f06d8-7334-476d-83f8-35972f93103a",
        "parentId" : "03861327-dd5c-4a35-b641-ae4e383e6b1a",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "In fact, `--files` describes same:\r\n\r\n```\r\n        |  --files FILES               Comma-separated list of files to be placed in the working\r\n        |                              directory of each executor. File paths of these files\r\n        |                              in executors can be accessed via SparkFiles.get(fileName).\r\n```\r\n\r\nI think we should fix the docs to say it's also available on the driver side :-). but I would like to run it separately.",
        "createdAt" : "2020-11-25T10:50:51Z",
        "updatedAt" : "2020-11-30T05:09:49Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "e35e94cad2888c98e03feb391a8dfcca8afa365c",
    "line" : 159,
    "diffHunk" : "@@ -1,1 +1640,1644 @@      logInfo(s\"Added archive $path at $key with timestamp $timestamp\")\n      val uriToDownload = UriBuilder.fromUri(new URI(key)).fragment(null).build()\n      val source = Utils.fetchFile(uriToDownload.toString, Utils.createTempDir(), conf,\n        env.securityManager, hadoopConfiguration, timestamp, useCache = false, shouldUntar = false)\n      val dest = new File("
  },
  {
    "id" : "9ca6a050-9052-4c05-b533-15f7d1574c66",
    "prId" : 29966,
    "prUrl" : "https://github.com/apache/spark/pull/29966#pullrequestreview-542007755",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2e116da3-5685-43cd-ade4-7940ab5b62c5",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "What if two added jars have the same dependency with different versions? e.g.,\r\n\r\n```\r\nsc.addJar(\"ivy://lib1:1.0?transitive=true\") // --> it depends on `libX v1.0`\r\nsc.addJar(\"ivy://lib2:1.0?transitive=true\") // --> it depends on `libX v2.0`\r\n```",
        "createdAt" : "2020-12-01T13:12:21Z",
        "updatedAt" : "2020-12-24T01:53:11Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "42c7c3ff-fae4-47fe-a62f-08235006b28c",
        "parentId" : "2e116da3-5685-43cd-ade4-7940ab5b62c5",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "Done",
        "createdAt" : "2020-12-01T15:20:26Z",
        "updatedAt" : "2020-12-24T01:53:11Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "4c44daecc7c77527e150d30051170f7ee8667f70",
    "line" : 73,
    "diffHunk" : "@@ -1,1 +1994,1998 @@            // Since `new Path(path).toUri` will lose query information,\n            // so here we use `URI.create(path)`\n            DependencyUtils.resolveMavenDependencies(URI.create(path))\n              .flatMap(jar => addLocalJarFile(new File(jar)))\n          case _ => checkRemoteJarFile(path)"
  },
  {
    "id" : "6992dc05-9296-476a-9e4a-c48deaf41b8b",
    "prId" : 29966,
    "prUrl" : "https://github.com/apache/spark/pull/29966#pullrequestreview-542500940",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bd502adb-334d-4fdd-b272-ccb921f74b93",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Could you add tests to check if this warning message is shown only once by using `LogAppender`?",
        "createdAt" : "2020-12-02T00:54:17Z",
        "updatedAt" : "2020-12-24T01:53:11Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "7a01f469-df50-4998-a8a0-712b02dfc471",
        "parentId" : "bd502adb-334d-4fdd-b272-ccb921f74b93",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> Could you add tests to check if this warning message is shown only once by using `LogAppender`?\r\n\r\nSure",
        "createdAt" : "2020-12-02T05:03:13Z",
        "updatedAt" : "2020-12-24T01:53:11Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "4c44daecc7c77527e150d30051170f7ee8667f70",
    "line" : 93,
    "diffHunk" : "@@ -1,1 +2008,2012 @@          postEnvironmentUpdate()\n        }\n        if (existed.nonEmpty) {\n          val jarMessage = if (scheme != \"ivy\") \"JAR\" else \"dependency jars of Ivy URI\"\n          logInfo(s\"The $jarMessage $path at ${existed.mkString(\",\")} has been added already.\" +"
  },
  {
    "id" : "fb1e81b4-acc6-46b8-ab5c-c1d6009667e3",
    "prId" : 29966,
    "prUrl" : "https://github.com/apache/spark/pull/29966#pullrequestreview-545794248",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dc017e8f-cc7f-4a66-8ef2-80bbdb009a8c",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@AngersZhuuuu, out of curiosity, is the Ivy URI the standard form documented somewhere? or something specific to Spark that you came up with?",
        "createdAt" : "2020-12-07T02:19:37Z",
        "updatedAt" : "2020-12-24T01:53:11Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "9e1a6e7a-294f-4955-a7ee-6d62be764b70",
        "parentId" : "dc017e8f-cc7f-4a66-8ef2-80bbdb009a8c",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> @AngersZhuuuu, out of curiosity, is the Ivy URI the standard form documented somewhere? or something specific to Spark that you came up with?\r\n\r\nFrom hive https://issues.apache.org/jira/browse/HIVE-9664, since it download jar use `ivy` then use schema as `ivy`? I think this useful for a lot of companies that have standard package management, so I implemented it in Spark\r\n",
        "createdAt" : "2020-12-07T02:36:16Z",
        "updatedAt" : "2020-12-24T01:53:11Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "4c44daecc7c77527e150d30051170f7ee8667f70",
    "line" : 70,
    "diffHunk" : "@@ -1,1 +1991,1995 @@          // A JAR file which exists locally on every worker node\n          case \"local\" => Seq(\"file:\" + uri.getPath)\n          case \"ivy\" =>\n            // Since `new Path(path).toUri` will lose query information,\n            // so here we use `URI.create(path)`"
  },
  {
    "id" : "87d578a4-052a-4c10-952c-8309e19ee2c1",
    "prId" : 29909,
    "prUrl" : "https://github.com/apache/spark/pull/29909#pullrequestreview-499972884",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e5bc323e-38c2-46c1-bfa3-f56cab4128e1",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "@AngersZhuuuu . This is `CORE` instead of `SQL`.\r\n- `core/src/main/scala/org/apache/spark/SparkContext.scala`",
        "createdAt" : "2020-10-01T02:23:43Z",
        "updatedAt" : "2020-10-01T02:23:44Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "03dd2157-eaab-47ab-b505-7955d1a7d071",
        "parentId" : "e5bc323e-38c2-46c1-bfa3-f56cab4128e1",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> @AngersZhuuuu . This is `CORE` instead of `SQL`.\r\n> \r\n> * `core/src/main/scala/org/apache/spark/SparkContext.scala`\r\n\r\nSorry, since always work at `SQL`  module",
        "createdAt" : "2020-10-01T02:36:37Z",
        "updatedAt" : "2020-10-01T02:36:37Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "27ebe4858cb46e998a6dc3ecd8f7bd110d05bfe4",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1900,1904 @@      logWarning(\"null or empty path specified as parameter to addJar\")\n    } else {\n      val key = if (path.contains(\"\\\\\") && Utils.isWindows) {\n        // For local paths with backslashes on Windows, URI throws an exception\n        addLocalJarFile(new File(path))"
  }
]