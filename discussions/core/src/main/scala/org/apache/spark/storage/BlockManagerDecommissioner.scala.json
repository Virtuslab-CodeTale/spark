[
  {
    "id" : "eddc59b1-ab17-4164-a883-9830839f69f9",
    "prId" : 31102,
    "prUrl" : "https://github.com/apache/spark/pull/31102#pullrequestreview-566858258",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f6b18b23-3695-40ad-a654-733adff798c9",
        "parentId" : null,
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Can you explain why you no longer check if the thread is interrupted.",
        "createdAt" : "2021-01-11T21:26:30Z",
        "updatedAt" : "2021-03-23T02:08:10Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "21e43069-a7d6-48f6-902f-7472efb89311",
        "parentId" : "f6b18b23-3695-40ad-a654-733adff798c9",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "When we call `stopMigratingShuffleBlocks` explicitly, it always sets to `keepRunning` to `false` first before shutting down the thread pool(which would interrupt the threads). That means, `keepRunning=false` always comes first before `Thread.isInterrupted = true`. So checking `keepRunning` only should be enough.\r\n\r\n\r\nFor `InterruptedException` comes from the inner block of `while`, it should be caught by `catch` block and set to `keepRunning` to `false`. And checking `keepRunning` only also works in this case. Although, IIUC, I think we currently don't have a way to interrupt the thread from the inner block.\r\n\r\n",
        "createdAt" : "2021-01-12T14:51:51Z",
        "updatedAt" : "2021-03-23T02:08:10Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "a7f69ae0-78e2-42be-9cbc-13e37005bc19",
        "parentId" : "f6b18b23-3695-40ad-a654-733adff798c9",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "What about in the case where the JVM is shutting down?",
        "createdAt" : "2021-01-12T17:46:01Z",
        "updatedAt" : "2021-03-23T02:08:10Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "bd59ca3e-7a0c-4374-9eaf-3e95337b2ee4",
        "parentId" : "f6b18b23-3695-40ad-a654-733adff798c9",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "JVM shutting down would trigger the shutdown hook to call `exeutor.stop()`, which calls `BlockManagerDecommissioner.stop()` explicitly. So it's still valid.",
        "createdAt" : "2021-01-13T02:54:51Z",
        "updatedAt" : "2021-03-23T02:08:10Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "88a205ec8af63f744fdbee8b4e61cb718d3ec207",
    "line" : 61,
    "diffHunk" : "@@ -1,1 +97,101 @@      logInfo(s\"Starting shuffle block migration thread for $peer\")\n      // Once a block fails to transfer to an executor stop trying to transfer more blocks\n      while (keepRunning) {\n        try {\n          val (shuffleBlockInfo, retryCount) = nextShuffleBlockToMigrate()"
  },
  {
    "id" : "7a287c15-a869-4aa0-8678-1e36b10c56b9",
    "prId" : 31102,
    "prUrl" : "https://github.com/apache/spark/pull/31102#pullrequestreview-566522443",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9cc56842-1016-43f9-8f94-fc0b4f8c0f61",
        "parentId" : null,
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "I don't understand what this refactoring gives us.",
        "createdAt" : "2021-01-11T22:15:58Z",
        "updatedAt" : "2021-03-23T02:08:10Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "cd58accb-2075-4d43-96a0-521f9eb0acc0",
        "parentId" : "9cc56842-1016-43f9-8f94-fc0b4f8c0f61",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "IIUC,  it's decorated with `lazy` because we won't initiate it when rdd migration disabled. However, it isn't implemented correctly as we always access it in `stop()`, which breaks our expectation (I think). This refactor fixes the problem.",
        "createdAt" : "2021-01-12T15:06:03Z",
        "updatedAt" : "2021-03-23T02:08:10Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "1e8e23aa-73c2-4cb6-b589-d2f09e2ccb49",
        "parentId" : "9cc56842-1016-43f9-8f94-fc0b4f8c0f61",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Makes sense.",
        "createdAt" : "2021-01-12T17:48:18Z",
        "updatedAt" : "2021-03-23T02:08:10Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "88a205ec8af63f744fdbee8b4e61cb718d3ec207",
    "line" : 188,
    "diffHunk" : "@@ -1,1 +185,189 @@    mutable.HashMap[BlockManagerId, ShuffleMigrationRunnable]()\n\n  private val rddBlockMigrationExecutor =\n    if (conf.get(config.STORAGE_DECOMMISSION_RDD_BLOCKS_ENABLED)) {\n      Some(ThreadUtils.newDaemonSingleThreadExecutor(\"block-manager-decommission-rdd\"))"
  },
  {
    "id" : "95f6ff46-3a9b-48f2-83e9-af1121b0b74a",
    "prId" : 31102,
    "prUrl" : "https://github.com/apache/spark/pull/31102#pullrequestreview-566366984",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7c03e538-0f2e-44db-840b-81524be9dc04",
        "parentId" : null,
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Similar concerns above around not checking thread interruptions.",
        "createdAt" : "2021-01-11T22:16:33Z",
        "updatedAt" : "2021-03-23T02:08:10Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "8750d559-8580-4bf9-b395-3201c592f0ff",
        "parentId" : "7c03e538-0f2e-44db-840b-81524be9dc04",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "The reason is similarly to https://github.com/apache/spark/pull/31102/files#r555829938. We alwaysys set `stopped` to false before shutting down the thread poll, which interrupts this thread.",
        "createdAt" : "2021-01-12T15:09:10Z",
        "updatedAt" : "2021-03-23T02:08:10Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "88a205ec8af63f744fdbee8b4e61cb718d3ec207",
    "line" : 206,
    "diffHunk" : "@@ -1,1 +195,199 @@    override def run(): Unit = {\n      logInfo(\"Attempting to migrate all RDD blocks\")\n      while (!stopped && !stoppedRDD) {\n        // Validate if we have peers to migrate to. Otherwise, give up migration.\n        if (bm.getPeers(false).isEmpty) {"
  },
  {
    "id" : "6d23ae93-5371-46b9-b43c-ced9194e0439",
    "prId" : 31102,
    "prUrl" : "https://github.com/apache/spark/pull/31102#pullrequestreview-566368049",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dee1c830-17bb-4cb8-8945-d880ce8c256d",
        "parentId" : null,
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Why switching from lazy val to option?",
        "createdAt" : "2021-01-11T22:17:20Z",
        "updatedAt" : "2021-03-23T02:08:10Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "62192455-81a6-4f11-b506-a9df2cc2f677",
        "parentId" : "dee1c830-17bb-4cb8-8945-d880ce8c256d",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Same reason of https://github.com/apache/spark/pull/31102/files#r555840826: to aoivd initiating the thread pool when it's unncessary.",
        "createdAt" : "2021-01-12T15:10:14Z",
        "updatedAt" : "2021-03-23T02:08:10Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "88a205ec8af63f744fdbee8b4e61cb718d3ec207",
    "line" : 252,
    "diffHunk" : "@@ -1,1 +222,226 @@  }\n\n  private val shuffleBlockMigrationRefreshExecutor =\n    if (conf.get(config.STORAGE_DECOMMISSION_SHUFFLE_BLOCKS_ENABLED)) {\n      Some(ThreadUtils.newDaemonSingleThreadExecutor(\"block-manager-decommission-shuffle\"))"
  },
  {
    "id" : "1d77175b-e99b-4e94-8d61-61e235b6867e",
    "prId" : 31102,
    "prUrl" : "https://github.com/apache/spark/pull/31102#pullrequestreview-565751430",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "77f0c16e-ba34-4d5e-86c0-e1b97152e895",
        "parentId" : null,
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Same concern for thread interruption",
        "createdAt" : "2021-01-11T22:17:40Z",
        "updatedAt" : "2021-03-23T02:08:10Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "88a205ec8af63f744fdbee8b4e61cb718d3ec207",
    "line" : 264,
    "diffHunk" : "@@ -1,1 +232,236 @@    override def run(): Unit = {\n      logInfo(\"Attempting to migrate all shuffle blocks\")\n      while (!stopped && !stoppedShuffle) {\n        try {\n          val startTime = System.nanoTime()"
  },
  {
    "id" : "e1dd36b3-20ad-45ad-a99a-1e255837a275",
    "prId" : 31102,
    "prUrl" : "https://github.com/apache/spark/pull/31102#pullrequestreview-566523089",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f1d64e57-7799-4bde-9281-dbb74c13e637",
        "parentId" : null,
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "...?",
        "createdAt" : "2021-01-11T22:18:18Z",
        "updatedAt" : "2021-03-23T02:08:10Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "207a5914-ec33-4386-aad5-a3fc120ef372",
        "parentId" : "f1d64e57-7799-4bde-9281-dbb74c13e637",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Ah, this intends to keep the same code pattern inside `ShuffleMigrationRunnable`.  In `ShuffleMigrationRunnable`, we has:\r\n\r\n```scala\r\ncase _: InterruptedException if !keepRunning =>\r\n   logInfo(\"Stop shuffle block migration\")\r\n```\r\n\r\nThis help reduces the misleading error message to users when the block migration is stopped intentionally.",
        "createdAt" : "2021-01-12T15:15:25Z",
        "updatedAt" : "2021-03-23T02:08:10Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "0cbd004f-1675-4382-bf87-ca4027a15581",
        "parentId" : "f1d64e57-7799-4bde-9281-dbb74c13e637",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Ok so the point of checking the keepRunning flag is to only log when it isn't a Spark commanded shutdown, gotcha.",
        "createdAt" : "2021-01-12T17:49:06Z",
        "updatedAt" : "2021-03-23T02:08:10Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "88a205ec8af63f744fdbee8b4e61cb718d3ec207",
    "line" : 279,
    "diffHunk" : "@@ -1,1 +241,245 @@          Thread.sleep(sleepInterval)\n        } catch {\n          case _: InterruptedException if stopped =>\n            logInfo(\"Stop refreshing migratable shuffle blocks.\")\n          case NonFatal(e) =>"
  },
  {
    "id" : "8a57e780-b60e-466a-9f73-5b61ba657895",
    "prId" : 31102,
    "prUrl" : "https://github.com/apache/spark/pull/31102#pullrequestreview-569306069",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a38e5d2b-7efd-4158-9c3d-e3e279e42f34",
        "parentId" : null,
        "authorId" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "body" : "There are a few more places where offload is used in the `BlockManagerSuite`:\r\n- https://github.com/apache/spark/blob/d562aaf4ff350b66aef354ca4493c60e75d2b04b/core/src/test/scala/org/apache/spark/storage/BlockManagerSuite.scala#L1895\r\n- https://github.com/apache/spark/blob/d562aaf4ff350b66aef354ca4493c60e75d2b04b/core/src/test/scala/org/apache/spark/storage/BlockManagerSuite.scala#L1913\r\n- https://github.com/apache/spark/blob/d562aaf4ff350b66aef354ca4493c60e75d2b04b/core/src/test/scala/org/apache/spark/storage/BlockManagerSuite.scala#L1929\r\n\r\n- https://github.com/apache/spark/blob/d562aaf4ff350b66aef354ca4493c60e75d2b04b/core/src/test/scala/org/apache/spark/storage/BlockManagerSuite.scala#L1931",
        "createdAt" : "2021-01-15T14:28:38Z",
        "updatedAt" : "2021-03-23T02:08:10Z",
        "lastEditedBy" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "tags" : [
        ]
      }
    ],
    "commit" : "88a205ec8af63f744fdbee8b4e61cb718d3ec207",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +34,38 @@/**\n * Class to handle block manager decommissioning retries.\n * It creates a Thread to retry migrating all RDD cache and Shuffle blocks\n */\nprivate[storage] class BlockManagerDecommissioner("
  },
  {
    "id" : "8a1568ec-aa80-464e-917a-fbc27cc9b035",
    "prId" : 30492,
    "prUrl" : "https://github.com/apache/spark/pull/30492#pullrequestreview-539660655",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5fd2cf2a-2869-42f1-978d-322c7a3b6ddf",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "When is the best time to move block to fallback storage during decommission? For current change, it just chooses fallback storage once encountering failure at first try.\r\n\r\nShould we let retrying do the work until reaching `maxReplicationFailuresForDecommission` and then move to fallback storage? ",
        "createdAt" : "2020-11-26T07:30:12Z",
        "updatedAt" : "2020-11-30T19:56:40Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "f9b5b754-2074-4bd1-b248-e951addbd480",
        "parentId" : "5fd2cf2a-2869-42f1-978d-322c7a3b6ddf",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Oh I see. You only put fallback storage into peers when there is no other peer executors.",
        "createdAt" : "2020-11-26T17:33:26Z",
        "updatedAt" : "2020-11-30T19:56:40Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "930b39a0-9522-4303-9432-aece5faf2660",
        "parentId" : "5fd2cf2a-2869-42f1-978d-322c7a3b6ddf",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Yes. It does.",
        "createdAt" : "2020-11-27T01:24:31Z",
        "updatedAt" : "2020-11-30T19:56:40Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "9c9317378eb61b40f766e41dd168d2189f6008c1",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +117,121 @@                        logWarning(s\"Skipping block ${shuffleBlockInfo}, block deleted.\")\n                      } else if (fallbackStorage.isDefined) {\n                        fallbackStorage.foreach(_.copy(shuffleBlockInfo, bm))\n                      } else {\n                        throw e"
  },
  {
    "id" : "ec58d54c-b3b3-4597-b893-a2ed5380c823",
    "prId" : 30116,
    "prUrl" : "https://github.com/apache/spark/pull/30116#pullrequestreview-513333657",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b521b427-2517-4255-91f1-db2086def7d8",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "This is the fix.",
        "createdAt" : "2020-10-21T06:11:03Z",
        "updatedAt" : "2020-10-21T18:21:59Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "fa10f3ac3c88b860a5190dab2c8552f6c314e543",
    "line" : 31,
    "diffHunk" : "@@ -1,1 +270,274 @@    }\n    // If we found any new shuffles to migrate or otherwise have not migrated everything.\n    newShufflesToMigrate.nonEmpty || migratingShuffles.size > numMigratedShuffles.get()\n  }\n"
  },
  {
    "id" : "3473a4e5-5f85-44b5-933e-024c3c6a17ad",
    "prId" : 30046,
    "prUrl" : "https://github.com/apache/spark/pull/30046#pullrequestreview-509645381",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0ae5e07a-f93e-47ba-8c8d-44211eea3196",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Could you describe when this happens logically?",
        "createdAt" : "2020-10-15T05:02:41Z",
        "updatedAt" : "2020-10-16T02:47:00Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "92208c78-166d-4942-91cd-99b745baca18",
        "parentId" : "0ae5e07a-f93e-47ba-8c8d-44211eea3196",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "sure :)",
        "createdAt" : "2020-10-15T18:11:13Z",
        "updatedAt" : "2020-10-16T02:47:00Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "b50eea895a084c04784399faaf74f2b822405e84",
    "line" : 47,
    "diffHunk" : "@@ -1,1 +105,109 @@                } catch {\n                  case e: IOException =>\n                    // If a block got deleted before netty opened the file handle, then trying to\n                    // load the blocks now will fail. This is most likely to occur if we start\n                    // migrating blocks and then the shuffle TTL cleaner kicks in. However this"
  },
  {
    "id" : "f20c3a86-d934-48cf-9f94-77cff642beaa",
    "prId" : 29211,
    "prUrl" : "https://github.com/apache/spark/pull/29211#pullrequestreview-454565060",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6ca0ae05-f05a-48ad-8c32-e732330a7a1b",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "Thanks for changing this to 'stopping'. \"will not refresh migrations\" was not clear. :-)",
        "createdAt" : "2020-07-24T00:05:08Z",
        "updatedAt" : "2020-08-05T18:39:14Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      }
    ],
    "commit" : "e81c3fc2d0b8c28cd0aedf08a7ea1e86a153ae49",
    "line" : 83,
    "diffHunk" : "@@ -1,1 +170,174 @@        } catch {\n          case e: InterruptedException =>\n            logInfo(\"Interrupted during RDD migration, stopping\")\n            stoppedRDD = true\n          case NonFatal(e) =>"
  },
  {
    "id" : "526aea4d-3bb6-4a64-8718-80ce01d7f75e",
    "prId" : 29211,
    "prUrl" : "https://github.com/apache/spark/pull/29211#pullrequestreview-455125334",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cf4d0818-c2c9-4a4b-8e3c-749ee7f89aea",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "Help me understand the difference b/w `stopped` and `stoppedRDD || stoppedShuffle` ? \r\n\r\nCan we replace `stopped` here with `stoppedShuffle` ? Why is `stoppedShuffle` not being checked in this loop ?\r\n",
        "createdAt" : "2020-07-24T17:53:33Z",
        "updatedAt" : "2020-08-05T18:39:14Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "8ca6af48-0ff8-4078-9948-5fa62281265f",
        "parentId" : "cf4d0818-c2c9-4a4b-8e3c-749ee7f89aea",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "This is the loop for migrating RDDs, it doesn't matter if the shuffles have encountered an error or were not enabled.",
        "createdAt" : "2020-07-24T18:43:17Z",
        "updatedAt" : "2020-08-05T18:39:14Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "61bc2635-e016-4e27-904d-417e3da57350",
        "parentId" : "cf4d0818-c2c9-4a4b-8e3c-749ee7f89aea",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "stopped == we've stopped all migrations (e.g. Spark is shutting down)\r\nstoppedRDD == we've stopped RDD migrations\r\nstoppedShuffle == we've stopped Shuffle migrations",
        "createdAt" : "2020-07-24T18:44:14Z",
        "updatedAt" : "2020-08-05T18:39:14Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "739f0af7-a106-4411-8e34-8671a34e1988",
        "parentId" : "cf4d0818-c2c9-4a4b-8e3c-749ee7f89aea",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "No, we cannot replace stopped with stoppedShuffle.",
        "createdAt" : "2020-07-24T18:45:43Z",
        "updatedAt" : "2020-08-05T18:39:14Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "00a7b1e6-a999-4124-aac3-fcab0df8d8fb",
        "parentId" : "cf4d0818-c2c9-4a4b-8e3c-749ee7f89aea",
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "Oh okay, stupid me: This is in the run loop of RDD migration :-P. Obviously you cannot use stoppedShuffle here.",
        "createdAt" : "2020-07-24T19:12:22Z",
        "updatedAt" : "2020-08-05T18:39:14Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      }
    ],
    "commit" : "e81c3fc2d0b8c28cd0aedf08a7ea1e86a153ae49",
    "line" : 61,
    "diffHunk" : "@@ -1,1 +151,155 @@    override def run(): Unit = {\n      assert(conf.get(config.STORAGE_DECOMMISSION_RDD_BLOCKS_ENABLED))\n      while (!stopped && !stoppedRDD && !Thread.interrupted()) {\n        logInfo(\"Iterating on migrating from the block manager.\")\n        // Validate we have peers to migrate to."
  },
  {
    "id" : "911801ce-7c29-4747-8a88-11321bdbe91e",
    "prId" : 29211,
    "prUrl" : "https://github.com/apache/spark/pull/29211#pullrequestreview-459580765",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b048709a-091d-4834-8c09-27e9b2ef99c3",
        "parentId" : null,
        "authorId" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "body" : "This must be `Math.max`.\r\n\r\nAs I understand in `CoarseGrainedExecutorBackend` the `lastTaskRunningTime` increases with about 1000ms in every iteration, let's assume exactly 1000ms. So if the RDD migration finished at 500ms (let's count in the example from 0 here) but shuffle files to be migrated are still left and they will be finished only in the next round (let's assume in 1500ms) then we we never shutdown the executor: as in the current round `blocksMigrated` is false and in all the following ones `migrationTime` will be less than `lastTaskRunningTime`, so this condition will be never satisfied:\r\n\r\nhttps://github.com/apache/spark/blob/484f8e216d5727e516488aedbdb41b1f63569701/core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala#L305\r\n\r\n",
        "createdAt" : "2020-07-24T20:04:37Z",
        "updatedAt" : "2020-08-05T18:39:14Z",
        "lastEditedBy" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "tags" : [
        ]
      },
      {
        "id" : "10eea8e0-e152-47f2-885f-c3348a58198a",
        "parentId" : "b048709a-091d-4834-8c09-27e9b2ef99c3",
        "authorId" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "body" : "I was thinking a bit more on this part. \r\n\r\nThe intention of `lastRDDMigrationTime`, `lastShuffleMigrationTime` and `lastTaskRunningTime` is to notify the driver **only once** about the exit of this executor. \r\n\r\nMy first thought was to improve this by expressing this intention directly with a simple flag in the `shutdownThread`(like `exitTriggered` which can be used as a stopping condition instead of the `while (true)` ) and remove all the three time variables and change `lastMigrationInfo` to return just a boolean and rename it for its new role (for example to `isFinished` or just simply to `finished`). \r\n\r\nThen I went a bit further and checked the `exitExecutor` method and I think even the flag is not necessarily needed:\r\nhttps://github.com/apache/spark/blob/484f8e216d5727e516488aedbdb41b1f63569701/core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala#L267\r\n\r\nStill I like to have a stopping condition in the while loop so please consider to use the flag instead of the times. \r\n      ",
        "createdAt" : "2020-07-25T12:43:56Z",
        "updatedAt" : "2020-08-05T18:39:14Z",
        "lastEditedBy" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "tags" : [
        ]
      },
      {
        "id" : "fb7132a1-1710-4690-8d33-2a0bc9e27c1e",
        "parentId" : "b048709a-091d-4834-8c09-27e9b2ef99c3",
        "authorId" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "body" : "@holdenk @agrawaldevesh what do you think?",
        "createdAt" : "2020-07-25T13:16:12Z",
        "updatedAt" : "2020-08-05T18:39:14Z",
        "lastEditedBy" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "tags" : [
        ]
      },
      {
        "id" : "bc9f5a44-8f9c-44c5-8b76-b4a350066b89",
        "parentId" : "b048709a-091d-4834-8c09-27e9b2ef99c3",
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "I agree with you @attilapiros. On both the counts:\r\n- Converting this into a level trigger vs an edge trigger: ie the flag goes on and remains on when it is time to exit.\r\n- Having consistency b/w the termination condition of the thread above and the computation of the boolean variable in this method. Having a helper method would help with this and also improve longer term maintainability.\r\n\r\nIMHO, this is great feedback and will improve this PR. Thank you.",
        "createdAt" : "2020-07-25T17:17:10Z",
        "updatedAt" : "2020-08-05T18:39:14Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "c6dfdba9-481e-4e7f-95b3-0f02b938b7e8",
        "parentId" : "b048709a-091d-4834-8c09-27e9b2ef99c3",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "So it only increases if there is a task running. We don’t want it to shutdown or consider the blocks migrated if there is a task migrated.\r\nUnfortunately this flag can come and go depending on the tasks running on the executor which is why it’s structured the way it is.",
        "createdAt" : "2020-07-25T17:21:02Z",
        "updatedAt" : "2020-08-05T18:39:14Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "f4273913-0c7e-43cc-9c58-2a09c1748c7f",
        "parentId" : "b048709a-091d-4834-8c09-27e9b2ef99c3",
        "authorId" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "body" : "Now I am starting to get this part. Can we try to simplify this? :)\r\n\r\nOn a decommissioned executor there will be no new tasks started, right? (Theoretically one task could be started as the scheduler does not processed the `DecommissionExecutor` message but let's take this corner case out as we can have this handled with some sleep).\r\n\r\nSo when `executor.numRunningTasks` will be 0 it stays 0. As I remember caching is part of task running so no new cached blocks will be created when numRunningTasks=0.\r\n\r\nSo would it work if we first wait to reach numRunningTasks==0 in a sleep check loop and then we would check for the migration finished flag without using the time part of `lastMigrationInfo`?\r\n\r\n",
        "createdAt" : "2020-07-31T13:58:01Z",
        "updatedAt" : "2020-08-05T18:39:14Z",
        "lastEditedBy" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "tags" : [
        ]
      },
      {
        "id" : "a81c02d4-48ca-4b58-a21c-4d6d6f534839",
        "parentId" : "b048709a-091d-4834-8c09-27e9b2ef99c3",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "So we don’t reject tasks sent to us and an executor can start decommissioning without the driver knowing yet so it is possible (although unlikely) to get a new task after we’ve started decommissioning.",
        "createdAt" : "2020-07-31T17:59:19Z",
        "updatedAt" : "2020-08-05T18:39:14Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "e4b70835-e417-44a5-a36b-11096d9be5f3",
        "parentId" : "b048709a-091d-4834-8c09-27e9b2ef99c3",
        "authorId" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "body" : "Yes, I know this. That is what I meant by\r\n\r\n>  (Theoretically one task could be started as the scheduler does not processed the DecommissionExecutor message but let's take this corner case out as we can have this handled with some sleep).\r\n\r\nSo I think with some waiting (which I would avoid if I could) the probability of this unlikely case can be decreased more.\r\n\r\nBut let's assume even that very unlikely case happens then we are losing one cached RDD from the current stage which will be re-submitted. The advantage on the other hand a more simple code which can be fixed/improved by more developers.\r\n\r\n**Other idea**: leave this logic as it is but refactor the `shutdownThread` into a separate class to make it unittestable (all its dependencies would be mockable, even the time its uses would be a Clock and a ManualClock in the test).\r\n\r\nWhat do you think?",
        "createdAt" : "2020-07-31T18:23:52Z",
        "updatedAt" : "2020-08-05T18:39:14Z",
        "lastEditedBy" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "tags" : [
        ]
      },
      {
        "id" : "0ef3f25b-52ee-4cca-84d8-1b002392c416",
        "parentId" : "b048709a-091d-4834-8c09-27e9b2ef99c3",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Oh sorry I missed that part. So currently the block manager doesn't know if there are any running tasks, and I'm not sure it's better to expose that into the block manager.\r\n\r\nI'd rather not unit test the shutdownThread, realistically I think it's better suited to the integration style testing that we do. I don't mind moving it into a separate class/file though.\r\n\r\nIt seems like your advocating for either a solution with more edge cases or less information hiding which is why I'm hesitating to pursue either of these paths. The time stamp is certainly a bit complicated, but I'm not convinced it's complicated enough to throw it out and throw away good data for the user. What about if we documented how the timestamps are used a bit more would that be enough?",
        "createdAt" : "2020-07-31T18:39:51Z",
        "updatedAt" : "2020-08-05T18:39:14Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "774c9c93-3a30-4232-aa40-12deb18dea8c",
        "parentId" : "b048709a-091d-4834-8c09-27e9b2ef99c3",
        "authorId" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "body" : "Just a little misunderstanding remained:\r\n> So currently the block manager doesn't know if there are any running tasks, and I'm not sure it's better to expose that into the block manager.\r\n\r\nI do not think that is needed: I would implement the simplified solution within the shutdown thread.\r\n\r\nI advise some improved robustness by a more testable or easier provable code (of which easier to argue about) to make it more suppartable/extendable. \r\n\r\nI am sorry but regarding this code part your reviewer(s) (at least me myself) failed at first :( that is an indication we should do something. Yes we can try to improve the documentation first. \r\n\r\n@agrawaldevesh: what is your opinion/idea?\r\n\r\n",
        "createdAt" : "2020-07-31T19:08:55Z",
        "updatedAt" : "2020-08-05T18:39:14Z",
        "lastEditedBy" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "tags" : [
        ]
      },
      {
        "id" : "e5fb3fab-5d10-4bd6-9a14-d911b6470bd8",
        "parentId" : "b048709a-091d-4834-8c09-27e9b2ef99c3",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Ok I'm not sure how we'd implement this in the shutdown thread, maybe that's why I'm concerned about the proposal. In your mind, information from the block manager decommissioner would we expose to allow this logic to move to the shutdown thread?",
        "createdAt" : "2020-07-31T19:14:36Z",
        "updatedAt" : "2020-08-05T18:39:14Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "8a879de6-ecca-433f-adbd-07041739f53e",
        "parentId" : "b048709a-091d-4834-8c09-27e9b2ef99c3",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Also by \"failed at first\" what are you referring to? That the decommission call would block on the RPC thread or some other bug?",
        "createdAt" : "2020-07-31T19:17:16Z",
        "updatedAt" : "2020-08-05T18:39:14Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "c2053516-c22d-4759-8bff-2aefc00d9c1c",
        "parentId" : "b048709a-091d-4834-8c09-27e9b2ef99c3",
        "authorId" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "body" : "On Monday I can open a PR about what I had in my mind (that is my last day before a two-three weeks long computerless period). \r\n\r\nRegarding failed at first: I am referring to my mistake. To the fact I misunderstand your intentions regarding the code. So I was completely wrong here:\r\nhttps://github.com/apache/spark/pull/29211#discussion_r460263043\r\n\r\n",
        "createdAt" : "2020-07-31T19:26:21Z",
        "updatedAt" : "2020-08-05T18:39:14Z",
        "lastEditedBy" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "tags" : [
        ]
      },
      {
        "id" : "16267b72-35b2-41a9-9d28-357c884b2180",
        "parentId" : "b048709a-091d-4834-8c09-27e9b2ef99c3",
        "authorId" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "body" : "PR against your branch",
        "createdAt" : "2020-07-31T19:32:24Z",
        "updatedAt" : "2020-08-05T18:39:14Z",
        "lastEditedBy" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "tags" : [
        ]
      },
      {
        "id" : "f477bfff-7f71-4113-b6e6-a3f4950961fc",
        "parentId" : "b048709a-091d-4834-8c09-27e9b2ef99c3",
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "> @agrawaldevesh: what is your opinion/idea?\r\n\r\nI confess that even I am finding the logic in the lastMigrationInfo a bit hard to follow. And as far as I can see, this logic is the key to getting the shutdown thread to exit cleanly. If the information returned by lastMigrationInfo is wrong then the shutdown thread may exit prematurely or never. Both of which should be avoided, particularly because the shutdown thread has no timeout on the amount of time it can hang around. \r\n\r\nThis complexity is intrinsic. The logic is indeed complex but I think what we can add some more documentation here, perhaps explaining the intent of the code using a couple of examples ? Even better would be to please add a unit test for this. I think it should be easy to unit test just the lastMigrationInfo. This is just testing that the function is behaving as expected by stubbing out other parts. This test would also help with providing the necessary guardrails as we change this function in the future. I expect this function to change as we production harden the cache/shuffle block migration in the near term.",
        "createdAt" : "2020-07-31T21:44:54Z",
        "updatedAt" : "2020-08-05T18:39:14Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "9a3850e4-3ebe-4ea2-adde-850b71bc93d5",
        "parentId" : "b048709a-091d-4834-8c09-27e9b2ef99c3",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Yeah I’m down to unit test this function, makes sense to me and it’s fairly standalone.\r\n\r\nI’ll work on the documentation over the weekend.",
        "createdAt" : "2020-07-31T21:54:18Z",
        "updatedAt" : "2020-08-05T18:39:14Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "4e756369-167f-4bec-b1d0-ee3a16509fbb",
        "parentId" : "b048709a-091d-4834-8c09-27e9b2ef99c3",
        "authorId" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "body" : "https://github.com/holdenk/spark/pull/7 :)",
        "createdAt" : "2020-08-01T12:17:19Z",
        "updatedAt" : "2020-08-05T18:39:14Z",
        "lastEditedBy" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "tags" : [
        ]
      },
      {
        "id" : "ba71c8e5-c85e-4f36-90e0-5cf66ef69862",
        "parentId" : "b048709a-091d-4834-8c09-27e9b2ef99c3",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "So the approach in that PR re-opens the race condition were preventing here. I would really rather not do that.\r\n\r\nI’d also like us to make progress here though, so we can temporarily accept the race condition and file a JIRA and revisit it later as a stand-alone item if y’all are not comfortable with any of the ways to avoid the race.",
        "createdAt" : "2020-08-01T14:14:01Z",
        "updatedAt" : "2020-08-05T18:39:14Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "e81c3fc2d0b8c28cd0aedf08a7ea1e86a153ae49",
    "line" : 196,
    "diffHunk" : "@@ -1,1 +383,387 @@      // Chose the min of the active times. See the function description for more information.\n      val lastMigrationTime = if (!stoppedRDD && !stoppedShuffle) {\n        Math.min(lastRDDMigrationTime, lastShuffleMigrationTime)\n      } else if (!stoppedShuffle) {\n        lastShuffleMigrationTime"
  },
  {
    "id" : "624d4e4c-f56f-442e-b4f1-203ee212654f",
    "prId" : 29211,
    "prUrl" : "https://github.com/apache/spark/pull/29211#pullrequestreview-460494405",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "833f8366-2b76-4f8c-87f6-6f78f191b4cd",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "Thanks for the doc !",
        "createdAt" : "2020-08-04T06:18:02Z",
        "updatedAt" : "2020-08-05T18:39:14Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      }
    ],
    "commit" : "e81c3fc2d0b8c28cd0aedf08a7ea1e86a153ae49",
    "line" : 181,
    "diffHunk" : "@@ -1,1 +368,372 @@  }\n\n  /*\n   *  Returns the last migration time and a boolean for if all blocks have been migrated.\n   *  The last migration time is calculated to be the minimum of the last migration of any"
  },
  {
    "id" : "773ca4a7-47ff-4c6e-a387-3b81cf21655e",
    "prId" : 28708,
    "prUrl" : "https://github.com/apache/spark/pull/28708#pullrequestreview-430105527",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "874776b2-f413-4199-8fdb-032966f2701b",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "This code tries to copy the shuffled blocks to exactly one other live peer (ie offloads to another live peer), by virtue of the fact that `shufflesToMigrate` is polled. This should be documented :-) Its quite subtle as it currently reads.\r\n\r\nSecond, can we dedup this logic with the `replicate` codepath used for replicating `cached` blocks. The only semantic difference is wrt to you want to copy the block to one peer vs N peers (for replicate). The rest of the machinery should be shareable. ",
        "createdAt" : "2020-06-12T04:12:52Z",
        "updatedAt" : "2020-07-17T23:45:20Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "dc549117-956d-434e-b7af-fc0eddfec966",
        "parentId" : "874776b2-f413-4199-8fdb-032966f2701b",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "So it tries to migrate to whichever live peer is available first, and if that fails re-inserts the shuffle file into the queue.\r\n\r\nThe cache block one is different in a few ways: 1) we have a policy for determining what the replication target host is based on the current replication info in the block, 2) we only have to migrate one block at a time (here we have two separate blocks that need to migrate).\r\n\r\nI think the producer/consumer model is optimal for the shuffle blocks where we just want to migrate them quickly, but not a good fit for the cache blocks where we have a policy to choose a target block manager for a given block.",
        "createdAt" : "2020-06-12T12:24:54Z",
        "updatedAt" : "2020-07-17T23:45:20Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "c6165d43-5b4b-4a17-961e-a509110fe178",
        "parentId" : "874776b2-f413-4199-8fdb-032966f2701b",
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "Thank you !. This is a great explanation and perhaps should go into the code as code comments as well :-). ",
        "createdAt" : "2020-06-12T16:04:53Z",
        "updatedAt" : "2020-07-17T23:45:20Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "ccc85ce5-aced-4d72-81a0-3d4a77b6384b",
        "parentId" : "874776b2-f413-4199-8fdb-032966f2701b",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Cool, I'll add this in the code up above the loop where we launch the two different migrations :)",
        "createdAt" : "2020-06-13T02:12:58Z",
        "updatedAt" : "2020-07-17T23:45:20Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "8494bdd94285c7cc5a41e151da920710be7f4671",
    "line" : 210,
    "diffHunk" : "@@ -1,1 +208,212 @@    migrationPeers ++= newPeers.map { peer =>\n      logDebug(s\"Starting thread to migrate shuffle blocks to ${peer}\")\n      val runnable = new ShuffleMigrationRunnable(peer)\n      shuffleMigrationPool.submit(runnable)\n      (peer, runnable)"
  },
  {
    "id" : "bede07b1-86e8-49cd-b17b-9483d04f8ce3",
    "prId" : 28708,
    "prUrl" : "https://github.com/apache/spark/pull/28708#pullrequestreview-430905401",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "71c68323-4a8f-455f-b22d-9778a8f3c4e4",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Could you use more specific `Exception` type?",
        "createdAt" : "2020-06-14T17:53:55Z",
        "updatedAt" : "2020-07-17T23:45:20Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "264a1c4a-0f1a-4448-aa91-273ac2330439",
        "parentId" : "71c68323-4a8f-455f-b22d-9778a8f3c4e4",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "I kind of want to catch everything here though and just log it while re-inserting the shuffle into the queue if applicable, so I thin narrowing the scope here isn't the right thing to do.",
        "createdAt" : "2020-06-15T18:49:25Z",
        "updatedAt" : "2020-07-17T23:45:20Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "8494bdd94285c7cc5a41e151da920710be7f4671",
    "line" : 103,
    "diffHunk" : "@@ -1,1 +101,105 @@        // if we encounter errors migrating to an executor we want to stop.\n      } catch {\n        case e: Exception =>\n          migrating match {\n            case Some((shuffleMap, retryCount)) =>"
  },
  {
    "id" : "61d51511-2d99-4668-a161-76798f502574",
    "prId" : 28708,
    "prUrl" : "https://github.com/apache/spark/pull/28708#pullrequestreview-451119629",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5dcd0e5f-0acb-4363-b7c6-6644b998be28",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "Should we inline SLEEP_TIME_SECS ? \r\n\r\nAnd why sleep 1 second ? Why not like 100ms ? \r\n\r\nI also wonder if it would complicate the code a lot to switch this to a blocked poll, and use a poison pill when running is made false ? We wouldn't need the \"polling\" then.\r\n",
        "createdAt" : "2020-06-19T02:00:58Z",
        "updatedAt" : "2020-07-17T23:45:20Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "65216606-e8d0-4ae5-b16d-7481cf1232b8",
        "parentId" : "5dcd0e5f-0acb-4363-b7c6-6644b998be28",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Let me think about that, I'm not sure it's worth the complexity increase.",
        "createdAt" : "2020-06-19T03:12:07Z",
        "updatedAt" : "2020-07-17T23:45:20Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "46496c32-e73e-4c7b-b913-28c1bc6a279e",
        "parentId" : "5dcd0e5f-0acb-4363-b7c6-6644b998be28",
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "I agree. I think one second polling is fine but its a magic number that raises questions nevertheless ;-) ",
        "createdAt" : "2020-06-19T03:51:50Z",
        "updatedAt" : "2020-07-17T23:45:20Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "6df8629d-9bbc-490e-8f23-782b704ef001",
        "parentId" : "5dcd0e5f-0acb-4363-b7c6-6644b998be28",
        "authorId" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "body" : "I think this is fine as I have seen this pattern used in the code somewhere else too, ie.:\r\nhttps://github.com/apache/spark/blob/026b0b926dfd40038f2cee932f38b917eb25b77e/core/src/main/scala/org/apache/spark/storage/BlockManager.scala#L520\r\n\r\nI believe this is for emphasizing here is something to be set only once with a correct default (by code as opposed to a config param). So reading the code it raises attention that here an important choice was made. ",
        "createdAt" : "2020-07-19T10:41:50Z",
        "updatedAt" : "2020-07-19T11:05:14Z",
        "lastEditedBy" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "tags" : [
        ]
      }
    ],
    "commit" : "8494bdd94285c7cc5a41e151da920710be7f4671",
    "line" : 74,
    "diffHunk" : "@@ -1,1 +72,76 @@              // Nothing to do right now, but maybe a transfer will fail or a new block\n              // will finish being committed.\n              val SLEEP_TIME_SECS = 1\n              Thread.sleep(SLEEP_TIME_SECS * 1000L)\n            case Some((shuffleBlockInfo, retryCount)) =>"
  },
  {
    "id" : "0feee04d-22cc-4891-b8e4-7cbe008c57df",
    "prId" : 28708,
    "prUrl" : "https://github.com/apache/spark/pull/28708#pullrequestreview-433770929",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "31f4e930-ed9f-4fb6-aa2c-5ccd3fa9ae9a",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "The default setting of STORAGE_DECOMMISSION_REPLICATION_REATTEMPT_INTERVAL is 30 seconds. I think that is too high considering that ideally should work on the migration asap as the new blocks are written ? \r\n\r\nShould we reduce that ? In addition, `decommissionRddCacheBlocks` method will synchronously block until all RDD blocks have been migrated (I believe). That will further lead to more time loss.\r\n\r\nWhen a node is being decom'd, it does not have a lot of time. For example spot kills are like given 2 minutes to clean up. So I think we should make sure that the decommissioner is going as fast as needed. I am wondering if there should be two runnables here: one for shuffle blocks and one for persisted blocks ? ",
        "createdAt" : "2020-06-19T02:35:03Z",
        "updatedAt" : "2020-07-17T23:45:20Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "cb9c6445-93c6-44d2-80d9-44099f00c1ae",
        "parentId" : "31f4e930-ed9f-4fb6-aa2c-5ccd3fa9ae9a",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Yeah we can split this into two threads to simplify matters.",
        "createdAt" : "2020-06-19T03:14:54Z",
        "updatedAt" : "2020-07-17T23:45:20Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "8494bdd94285c7cc5a41e151da920710be7f4671",
    "line" : 143,
    "diffHunk" : "@@ -1,1 +141,145 @@          logInfo(\"Attempt to replicate all cached blocks done\")\n          logInfo(s\"Waiting for ${sleepInterval} before refreshing migrations.\")\n          Thread.sleep(sleepInterval)\n        } catch {\n          case e: InterruptedException =>"
  },
  {
    "id" : "7a54e8b4-1c91-436b-baad-75b7b3252e5f",
    "prId" : 28708,
    "prUrl" : "https://github.com/apache/spark/pull/28708#pullrequestreview-433770929",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "31e46e99-626d-4f3a-a5ea-1b8cdd6b20c3",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "new line before this comment block ?",
        "createdAt" : "2020-06-19T02:35:30Z",
        "updatedAt" : "2020-07-17T23:45:20Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "8b5d706d-5451-40a0-a336-287d7c57a253",
        "parentId" : "31e46e99-626d-4f3a-a5ea-1b8cdd6b20c3",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "sure",
        "createdAt" : "2020-06-19T03:15:09Z",
        "updatedAt" : "2020-07-17T23:45:20Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "8494bdd94285c7cc5a41e151da920710be7f4671",
    "line" : 188,
    "diffHunk" : "@@ -1,1 +186,190 @@    conf.get(config.STORAGE_DECOMMISSION_SHUFFLE_MAX_THREADS))\n\n  /**\n   * Tries to offload all shuffle blocks that are registered with the shuffle service locally.\n   * Note: this does not delete the shuffle files in-case there is an in-progress fetch"
  },
  {
    "id" : "57076613-1e19-4294-a259-f05f34980cd0",
    "prId" : 28708,
    "prUrl" : "https://github.com/apache/spark/pull/28708#pullrequestreview-448737816",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "02db571c-4433-42c2-81c9-ea6d06b6c3f6",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Could you add an explanation for the entry type, especially for the `Int`?",
        "createdAt" : "2020-07-15T08:28:11Z",
        "updatedAt" : "2020-07-17T23:45:20Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "8494bdd94285c7cc5a41e151da920710be7f4671",
    "line" : 120,
    "diffHunk" : "@@ -1,1 +118,122 @@  // Shuffles which are queued for migration & number of retries so far.\n  private[storage] val shufflesToMigrate =\n    new java.util.concurrent.ConcurrentLinkedQueue[(ShuffleBlockInfo, Int)]()\n\n  // Set if we encounter an error attempting to migrate and stop."
  }
]