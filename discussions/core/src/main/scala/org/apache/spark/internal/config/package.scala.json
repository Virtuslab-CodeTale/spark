[
  {
    "id" : "b6b791a9-9e9d-4bea-b6f6-af685ee12d62",
    "prId" : 33615,
    "prUrl" : "https://github.com/apache/spark/pull/33615#pullrequestreview-730204271",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f7ffe8f4-88e6-4452-9b66-99ad36e9ce8f",
        "parentId" : null,
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "It seems that we have exposed this in 3.1.0(3.1.1) already: https://github.com/apache/spark/blob/branch-3.1/core/src/main/scala/org/apache/spark/internal/config/package.scala\r\n\r\nWhy do we change the version here? The feature is not ready in 3.1?",
        "createdAt" : "2021-08-15T09:47:39Z",
        "updatedAt" : "2021-08-15T09:48:08Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "88507602-f0cf-4353-be51-e89c3ef92e46",
        "parentId" : "f7ffe8f4-88e6-4452-9b66-99ad36e9ce8f",
        "authorId" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "body" : "Yes, the feature was not ready in 3.1. It will be ready only in 3.2 which is why I changed it to 3.2 to keep it consistent.",
        "createdAt" : "2021-08-15T16:51:14Z",
        "updatedAt" : "2021-08-15T16:51:14Z",
        "lastEditedBy" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "tags" : [
        ]
      }
    ],
    "commit" : "12618284d95d7b1685c2c48c79bffcb618cf4523",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +2103,2107 @@        \"org.apache.spark.network.shuffle.MergedShuffleFileManager implementation for push-based \" +\n        \"shuffle to be enabled\")\n      .version(\"3.2.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "d220dc8b-fb03-4fdb-b003-8403f3829793",
    "prId" : 33078,
    "prUrl" : "https://github.com/apache/spark/pull/33078#pullrequestreview-699399725",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "54282deb-c470-4784-8b56-145ce80bda31",
        "parentId" : null,
        "authorId" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "body" : "nit: This feels like it should be `spark.app.attemptNumber` than `spark.app.attempt.id`. Also currently this is specific to push based shuffle right? Should we add that in the documentation?",
        "createdAt" : "2021-07-05T02:00:10Z",
        "updatedAt" : "2021-07-05T02:10:19Z",
        "lastEditedBy" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "tags" : [
        ]
      },
      {
        "id" : "687245d6-c8e9-4429-adc9-0955dd150e7a",
        "parentId" : "54282deb-c470-4784-8b56-145ce80bda31",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "`attemptNumber` sounds better to me.\r\n\r\nIf this conf can't be set by users, I think it's ok to keep it internal.",
        "createdAt" : "2021-07-05T05:37:23Z",
        "updatedAt" : "2021-07-05T05:37:23Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "8a1f417b-4ce6-4f70-945d-b37805cad71a",
        "parentId" : "54282deb-c470-4784-8b56-145ce80bda31",
        "authorId" : "e942fa3e-a5b9-4a9f-b9a5-a1afd3c29b2c",
        "body" : "AppAttemptId is currently used in lots of places to indicate the attempt id from Yarn, should we keep it as \"spark.app.attempt.id\" to align with all other places? Or we just want to change the configuration string to \"spark.app.attemptNumber\"? ",
        "createdAt" : "2021-07-05T05:54:51Z",
        "updatedAt" : "2021-07-05T05:54:51Z",
        "lastEditedBy" : "e942fa3e-a5b9-4a9f-b9a5-a1afd3c29b2c",
        "tags" : [
        ]
      },
      {
        "id" : "570171b6-51eb-481a-bb6e-3d364925f97e",
        "parentId" : "54282deb-c470-4784-8b56-145ce80bda31",
        "authorId" : "e942fa3e-a5b9-4a9f-b9a5-a1afd3c29b2c",
        "body" : "And which version should we mark it there? I added \"1.3.0\" since the support for multiple attempts in Yarn was added since 1.3.0. But if we change to \"spark.app.attemptNumber\", we should start from \"3.2.0\". Thoughts?",
        "createdAt" : "2021-07-05T05:56:32Z",
        "updatedAt" : "2021-07-05T05:56:32Z",
        "lastEditedBy" : "e942fa3e-a5b9-4a9f-b9a5-a1afd3c29b2c",
        "tags" : [
        ]
      },
      {
        "id" : "be958a99-e0af-4a11-a965-11d09502cdbf",
        "parentId" : "54282deb-c470-4784-8b56-145ce80bda31",
        "authorId" : "e942fa3e-a5b9-4a9f-b9a5-a1afd3c29b2c",
        "body" : "Should we use \"String\" or \"Integer\" here for this configuration? In [SparkContext where it gets the Application AttemptID](https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/SparkContext.scala#L340), it is a String format. But when parsing into Configuration and also resolving this in RemoteBlockPushResolver, we are using Integer to determine whether it is a new attempt.",
        "createdAt" : "2021-07-05T06:13:17Z",
        "updatedAt" : "2021-07-05T06:13:17Z",
        "lastEditedBy" : "e942fa3e-a5b9-4a9f-b9a5-a1afd3c29b2c",
        "tags" : [
        ]
      },
      {
        "id" : "b905ccf9-7e33-4b4f-8b47-306c4a2b0d63",
        "parentId" : "54282deb-c470-4784-8b56-145ce80bda31",
        "authorId" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "body" : "I see it makes sense. IMO, since it is already called `attemptId` it is fine to keep it probably like that. May be just adding slightly more info on the doc should be fine with me. Initially, I was under the impression we are generating this attemptId which is not the case.",
        "createdAt" : "2021-07-05T20:05:00Z",
        "updatedAt" : "2021-07-05T20:05:00Z",
        "lastEditedBy" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "tags" : [
        ]
      }
    ],
    "commit" : "53109918cbdbdba2fe79f38a991c171efec7e85f",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +2247,2251 @@\n  private[spark] val APP_ATTEMPT_ID =\n    ConfigBuilder(\"spark.app.attempt.id\")\n      .internal()\n      .doc(\"The application attempt Id assigned from Hadoop YARN. \" +"
  },
  {
    "id" : "a07b11d7-89d4-495d-832c-302b29348636",
    "prId" : 32287,
    "prUrl" : "https://github.com/apache/spark/pull/32287#pullrequestreview-655481906",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "09e9b13b-314f-402a-ab20-4f35b75b425b",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Just a question. Is there a reason of `10` instead of `3`?",
        "createdAt" : "2021-05-08T22:29:08Z",
        "updatedAt" : "2021-05-10T10:25:15Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "63892ca8-c862-42d6-8c6d-2f0e9c489469",
        "parentId" : "09e9b13b-314f-402a-ab20-4f35b75b425b",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Given the discussion (https://github.com/apache/spark/pull/32287#discussion_r625287419) there, Netty OOM could be raised more frequently in certain cases, e.g.,\r\n\r\n> For case b), the OOM threshold might be 20 requests. In this case, there're still 80 deferred requests, which would hit the OOM soon as you mentioned. That being said, I think the current fix would work around the issue in the end. Note that the application would fail before the fix.\r\n\r\nThus, I'd like to give more chances for the block in case we fall into the case like b).",
        "createdAt" : "2021-05-10T10:18:54Z",
        "updatedAt" : "2021-05-10T10:25:15Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "4af6ee73221a55e3cd7239f151ec33d5e802cbe4",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +1201,1205 @@      .internal()\n      .intConf\n      .createWithDefault(10)\n\n  private[spark] val REDUCER_MAX_BLOCKS_IN_FLIGHT_PER_ADDRESS ="
  },
  {
    "id" : "c4300ac3-1ee2-4706-9529-df5804b8d89d",
    "prId" : 31761,
    "prUrl" : "https://github.com/apache/spark/pull/31761#pullrequestreview-611033475",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e569a052-57ce-455e-851d-c2c07de35c3c",
        "parentId" : null,
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Is this only restricted to HDFS URLs? can \"hosts\" either be nameservice name or namenode name? some examples might help.",
        "createdAt" : "2021-03-07T01:36:53Z",
        "updatedAt" : "2021-03-22T18:58:50Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "4a07beb4-0a19-4462-89ec-9a244ef7b125",
        "parentId" : "e569a052-57ce-455e-851d-c2c07de35c3c",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Because the configuration value will be parsed as file system URLs to get file system. A simple host name doesn't work here.\r\n\r\ne.g.\r\n\r\n```scala\r\nscala> new Path(\"hdfs.namenode.net\").getFileSystem(new Configuration()).getUri.getHost\r\nres9: String = null\r\n```",
        "createdAt" : "2021-03-07T04:15:35Z",
        "updatedAt" : "2021-03-22T18:58:50Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "72b014c9-f5c5-48d8-b72b-2d3d3d48ae9a",
        "parentId" : "e569a052-57ce-455e-851d-c2c07de35c3c",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "It makes sense for `spark.kerberos.access.hadoopFileSystems` to be URLs since Spark needs to instantiate `FileSystem`s from them. But for this case I'm not sure if it's necessary: we can just parse the config into a set of host names and check whether the file systems above contain them:\r\n```scala\r\nval hostsToExclude = sparkConf.get(KERBEROS_FILESYSTEM_RENEWAL_EXCLUDE).toSet\r\nfilesystems.filter(fs => !hostsToExclude.contains(fs.getUri.getHost).foreach { fs =>\r\n ...\r\n}\r\n```\r\n\r\nI'm fine either way though since it also makes sense to keep it consistent with `spark.kerberos.access.hadoopFileSystems`. BTW I think we'll need to update security.md for the new config.",
        "createdAt" : "2021-03-07T04:50:39Z",
        "updatedAt" : "2021-03-22T18:58:50Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "96954ac1-4c6d-4481-bcf5-a2c524aecc31",
        "parentId" : "e569a052-57ce-455e-851d-c2c07de35c3c",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Thanks. Let me update security.md.",
        "createdAt" : "2021-03-07T04:56:45Z",
        "updatedAt" : "2021-03-22T18:58:50Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "232245ab-3ae5-4035-80f5-7d4abd22d76e",
        "parentId" : "e569a052-57ce-455e-851d-c2c07de35c3c",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "so just to be clear, these fileystems are ones the user specified in spark.kerberos.access.hadoopFileSystems so it wants to get initial tokens for them but then later we don't want to renew, correct?  We should perhaps update doc to mention that.  \r\nIt would be nice to know if this works with other cluster managers like k8s - @ifilonenko @mccheah  maybe?",
        "createdAt" : "2021-03-08T14:34:36Z",
        "updatedAt" : "2021-03-22T18:58:50Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "b8929d1f-2fbe-4461-9ec8-3b212eba9048",
        "parentId" : "e569a052-57ce-455e-851d-c2c07de35c3c",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "I guess it could be the staging or default Filesystem as well.",
        "createdAt" : "2021-03-08T14:36:09Z",
        "updatedAt" : "2021-03-22T18:58:50Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "aa66fdcd-4c24-4fe4-a03f-81299e364c24",
        "parentId" : "e569a052-57ce-455e-851d-c2c07de35c3c",
        "authorId" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "body" : "@tgravescs with the actual code Spark doesn't obtain the initial tokens when an FS is listed in \"spark.kerberos.renewal.exclude.hadoopFileSystems\". Please see my comment below...",
        "createdAt" : "2021-03-08T15:18:02Z",
        "updatedAt" : "2021-03-22T18:58:50Z",
        "lastEditedBy" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "tags" : [
        ]
      },
      {
        "id" : "a1ffedf5-ff22-4265-9c48-f14bda29a247",
        "parentId" : "e569a052-57ce-455e-851d-c2c07de35c3c",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Hm? @gaborgsomogyi I think this config doesn't affect Spark obtain the initial tokens?",
        "createdAt" : "2021-03-08T18:07:45Z",
        "updatedAt" : "2021-03-22T18:58:50Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "0906aa91-df70-494f-a93e-3d0fc5eafa2a",
        "parentId" : "e569a052-57ce-455e-851d-c2c07de35c3c",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "Right, the point I was getting at is that we only renew the ones in spark.kerberos.access.hadoopFileSystems + defaultFs + staging fs.  So I assume its basically defaultFs then? I was kind of wanting to make sure it wasn't something silly that they specified in both.  It seems odd for the defaultFs since this is the Hadoop filesystem which supports renewal.  I can obviously see this if its a separate name node but it would require you to specify it in the config for us to try to renew. So I'm assuming this is perhaps an odd config that you can't renew on the current default fs.  Just seems a bit odd case so wanted to make sure I wasn't missing something.\r\n",
        "createdAt" : "2021-03-08T18:39:17Z",
        "updatedAt" : "2021-03-22T18:58:50Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "ebe12a35-1f2a-4144-9dfe-0e1ebe0f43d4",
        "parentId" : "e569a052-57ce-455e-851d-c2c07de35c3c",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "Note I'm not against it just making sure the problem is understood and we document it appropriately. I guess it could be stage FS as well, which perhaps might make more sense if on a separate namenoxde",
        "createdAt" : "2021-03-08T18:47:34Z",
        "updatedAt" : "2021-03-22T18:58:50Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "eae852e1-361e-4319-8c76-502840e32af4",
        "parentId" : "e569a052-57ce-455e-851d-c2c07de35c3c",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "It's hard to imagine that this applies to defaultFS/stagingDir since it means the YARN cluster (or other types of clusters) is not configured to be in the same Kerberos realm as these which could cause other more serious issues. But, just as `spark.kerberos.access.hadoopFileSystems`, I guess nothing stops users from putting in the host for defaultFS/stagingDir in the config as well, and Spark will just do what it's told to do.",
        "createdAt" : "2021-03-09T01:55:47Z",
        "updatedAt" : "2021-03-22T18:58:50Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "d3ef676c-fb84-4830-9c24-0fbc7e1769b3",
        "parentId" : "e569a052-57ce-455e-851d-c2c07de35c3c",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "From the user perspective, I think it is still possible they want to avoid renewal for defaultFs. If the defaultFs is just a remote HDFS that whose tokens cannot be renew from YARN?",
        "createdAt" : "2021-03-10T01:12:36Z",
        "updatedAt" : "2021-03-22T18:58:50Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "9b236bff-77f4-4661-91d2-9783e997d31b",
        "parentId" : "e569a052-57ce-455e-851d-c2c07de35c3c",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "yes its true like I mentioned above. I'm not against it just want to understand and make sure this config makes sense. For instance instead of having this config we could have one that says don't include defaultFs or stageFs or both in renewal.   I think those are really the only 2 filesystems that make sense to put into this config. Anything else doesn't get renewed unless its in spark.kerberos.access.hadoopFileSystems and in that case just don't include it in spark.kerberos.access.hadoopFileSystems.  The nice things about this approach is its generic if things change in the future. The downside is I have to know namenode path for the defaultFs and staging fs to add to this config. ",
        "createdAt" : "2021-03-10T01:33:48Z",
        "updatedAt" : "2021-03-22T18:58:50Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "6bf04d7b-eafb-4f47-be94-7714c517fb9c",
        "parentId" : "e569a052-57ce-455e-851d-c2c07de35c3c",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "I'm not sure I understand correctly so just consider it as ignorant question.\r\n\r\n> Anything else doesn't get renewed unless its in spark.kerberos.access.hadoopFileSystems and in that case just don't include it in spark.kerberos.access.hadoopFileSystems.\r\n\r\nIs it possible if someone wants to get initial delegation token for them but doesn't renew them, and does this change make difference from here?",
        "createdAt" : "2021-03-10T02:42:55Z",
        "updatedAt" : "2021-03-22T18:58:50Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "5c79e985-b189-4962-963c-c2cfbe537e12",
        "parentId" : "e569a052-57ce-455e-851d-c2c07de35c3c",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "If I understand correctly, it is the same code path for obtaining delegation token and renewing. Actually the so called renew is that Spark periodically obtain delegation token.\r\n\r\nSo I guess it doesn't make sense for a case to get delegation token but not renew it. At least in Spark I don't see such case is possible. I may wrong here.\r\n",
        "createdAt" : "2021-03-10T05:26:32Z",
        "updatedAt" : "2021-03-22T18:58:50Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "46d35e8c-aecf-4bd1-b428-8669ae3caf97",
        "parentId" : "e569a052-57ce-455e-851d-c2c07de35c3c",
        "authorId" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "body" : "@HeartSaVioR Spark actually behaves this way (w/o this change): if one DT is created then it's renewed all the time except in the following cases:\r\n* `keytab` used but no `principal` provided\r\n* `ccache` used but it has no kerberos credentials\r\n\r\nPlease see the code here: https://github.com/apache/spark/blob/a916690dd9aac40df38922dbea233785354a2f2a/core/src/main/scala/org/apache/spark/deploy/security/HadoopDelegationTokenManager.scala#L83-L88\r\n",
        "createdAt" : "2021-03-10T10:51:35Z",
        "updatedAt" : "2021-03-22T18:58:50Z",
        "lastEditedBy" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "tags" : [
        ]
      },
      {
        "id" : "ac8b2856-cf46-4ddb-b3db-d3f4b2ee4f3a",
        "parentId" : "e569a052-57ce-455e-851d-c2c07de35c3c",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Ah OK I think I missed some part of @tgravescs 's comment. My bad.\r\n\r\nHis point wasn't that the new config is not necessary at all. His point was that the problem will occur from only defaultFs and/or stageFs, so the new config could be simplified instead of being general but a bit verbose.",
        "createdAt" : "2021-03-12T03:41:34Z",
        "updatedAt" : "2021-03-22T18:58:50Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "34b076b6-69a3-42a7-9378-81675181d0d9",
        "parentId" : "e569a052-57ce-455e-851d-c2c07de35c3c",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "> so just to be clear, these fileystems are ones the user specified in spark.kerberos.access.hadoopFileSystems so it wants to get initial tokens for them but then later we don't want to renew, correct? We should perhaps update doc to mention that.\r\n> It would be nice to know if this works with other cluster managers like k8s - @ifilonenko @mccheah maybe?\r\n\r\nHmm, for the customer case, the requirement is to prevent YARN from renewing the delegation token obtained. It doesn't matter if the file system is defaultFs/stageFs or the ones specified in `spark.kerberos.access.hadoopFileSystems`. That said, Spark still can obtain the token for the ones in `spark.kerberos.access.hadoopFileSystems`, but we don't want YARN to renew it.",
        "createdAt" : "2021-03-12T04:10:02Z",
        "updatedAt" : "2021-03-22T18:58:50Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "5cbfe720-ffd7-410f-b31e-d538419c7f2e",
        "parentId" : "e569a052-57ce-455e-851d-c2c07de35c3c",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "I was simply trying to clarify the exact case being hit here and make sure there wasn't an alternate solution and make sure the docs are clear.   The specific case being hit could affect the solution.\r\n\r\nI think there are comments on this review that are very confusing, thus why I wanted to clarification. Some indicate that Spark doesn't get initial tokens, others saying in this case the tokens were already acquired, etc.\r\n\r\nIn the end my comment ends up being I think we should update the security.md doc to mention renewal in the kerberos section for Hadoop filesystems to help explain to users.\r\n",
        "createdAt" : "2021-03-12T15:28:23Z",
        "updatedAt" : "2021-03-22T18:58:50Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "c1978145-73a2-416e-bdac-331948d75ddf",
        "parentId" : "e569a052-57ce-455e-851d-c2c07de35c3c",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Okay, thanks @tgravescs. I think in short this config doesn't affect Spark token handling, but only prevent YARN from renewing tokens, no matter the tokens are obtained in advance or by Spark. Let me update security.md and try to explain it.",
        "createdAt" : "2021-03-12T18:03:08Z",
        "updatedAt" : "2021-03-22T18:58:50Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "cf8da757045d23f5657ec1fb68e20f4ca0247772",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +694,698 @@  private[spark] val YARN_KERBEROS_FILESYSTEM_RENEWAL_EXCLUDE =\n    ConfigBuilder(\"spark.yarn.kerberos.renewal.excludeHadoopFileSystems\")\n      .doc(\"The list of Hadoop filesystem URLs whose hosts will be excluded from \" +\n        \"delegation token renewal at resource scheduler. Currently this is known to \" +\n        \"work under YARN, so YARN Resource Manager won't renew tokens for the application. \" +"
  },
  {
    "id" : "fb366351-fa9c-4baa-8012-7c1d8f85231e",
    "prId" : 31715,
    "prUrl" : "https://github.com/apache/spark/pull/31715#pullrequestreview-605557087",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "896d1f09-aeff-4099-88e4-32dc5cd16f02",
        "parentId" : null,
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "I am having similar concern with @viirya and @attilapiros . I think we should not make it as a user-facing config. If we would like introducing a config for this anyway, it'd better to start with `internal()` config, but not user-facing.\r\n\r\nThough `spark.shuffle.manager` can be arbitrary class, but in practice, they are just a handful of implementation in one company's environment, and ideally the infra developers should control which implementation to use, instead of user to control this. Similarly for whether to mark shuffle file lost should be controlled by developers team but not users.\r\n\r\nJust share some context, in FB, we (spark developer) control these kinds of behavior transparently and make this invisible to spark user. This also helps us to migrate to newer implementation easier without worrying about users setting wrong config. The mixed case (query uses customized shuffle service and default shuffle service) can happen quite a bit in production, as we have rate limit for traffic on customized service, and need fallback.",
        "createdAt" : "2021-03-04T02:11:59Z",
        "updatedAt" : "2021-03-04T02:12:04Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      },
      {
        "id" : "0e5e2c14-c5dc-4de2-8279-aa66831e3187",
        "parentId" : "896d1f09-aeff-4099-88e4-32dc5cd16f02",
        "authorId" : "c52c39ef-2b7f-4855-a2f1-2a2e50a719ae",
        "body" : "Hi @c21 , thanks for the comment, and known you from FB! We previously discussed with people from FB about Cosco, but not for this specific issue. Just curious, do you guys modify your internal Spark distribution to make executor lost event not trigger driver to delete map output?",
        "createdAt" : "2021-03-04T06:31:01Z",
        "updatedAt" : "2021-03-04T06:31:18Z",
        "lastEditedBy" : "c52c39ef-2b7f-4855-a2f1-2a2e50a719ae",
        "tags" : [
        ]
      },
      {
        "id" : "0a3d9375-faf2-4840-a66d-d8d2c28e2765",
        "parentId" : "896d1f09-aeff-4099-88e4-32dc5cd16f02",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "> do you guys modify your internal Spark distribution to make executor lost event not trigger driver to delete map output?\r\n\r\nYeah I think so, I can do a double check with cosco folks as well tomorrow. btw nice to know you as well!",
        "createdAt" : "2021-03-04T06:43:13Z",
        "updatedAt" : "2021-03-04T06:43:13Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      },
      {
        "id" : "ba9bfe2c-5545-4858-8431-c2497b152e9a",
        "parentId" : "896d1f09-aeff-4099-88e4-32dc5cd16f02",
        "authorId" : "c52c39ef-2b7f-4855-a2f1-2a2e50a719ae",
        "body" : "Cool, that will be interested to know, thanks!",
        "createdAt" : "2021-03-04T18:34:05Z",
        "updatedAt" : "2021-03-04T18:34:05Z",
        "lastEditedBy" : "c52c39ef-2b7f-4855-a2f1-2a2e50a719ae",
        "tags" : [
        ]
      },
      {
        "id" : "5c1d5b8e-463b-4230-905d-87265eb5e1fc",
        "parentId" : "896d1f09-aeff-4099-88e4-32dc5cd16f02",
        "authorId" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "body" : "@c21 May I ask what is the status of open sourcing Cosco? \r\nI just think it might help us to make better decisions about the Shuffle plugin API of Spark if we could look into that one too.\r\n",
        "createdAt" : "2021-03-05T17:01:31Z",
        "updatedAt" : "2021-03-05T17:01:31Z",
        "lastEditedBy" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "tags" : [
        ]
      },
      {
        "id" : "4e4f863e-1ac0-4860-bccf-7af919f0faf1",
        "parentId" : "896d1f09-aeff-4099-88e4-32dc5cd16f02",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "@attilapiros - unfortunately there's no specific ETA now, but the team is working on it.",
        "createdAt" : "2021-03-05T19:59:44Z",
        "updatedAt" : "2021-03-05T19:59:44Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      }
    ],
    "commit" : "1418e8f9507375f4fec025d8df357a86cb99ee45",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +2120,2124 @@\n  private[spark] val MARK_FILE_LOST_ON_EXECUTOR_LOST =\n    ConfigBuilder(\"spark.shuffle.markFileLostOnExecutorLost\")\n      .doc(\"Mark shuffle files as lost when an executor is lost. If you set \" +\n       \"spark.shuffle.manager with a customized class to use a different shuffle \" +"
  },
  {
    "id" : "fb375ded-d1df-40b0-8d4b-d7c7ace3a5f1",
    "prId" : 31249,
    "prUrl" : "https://github.com/apache/spark/pull/31249#pullrequestreview-575027717",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a337d1ab-f5df-41ff-8a05-cf84049e89b6",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "BTW, can we add docs since it's an exposed configuration?",
        "createdAt" : "2021-01-25T02:14:15Z",
        "updatedAt" : "2021-02-09T18:06:52Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "73bfd55dc3d9b3968658dd874de94c6ce5577252",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +813,817 @@    ConfigBuilder(\"spark.excludeOnFailure.killExcludedExecutors.decommission\")\n      .doc(\"Attempt decommission of excluded nodes instead of going directly to kill\")\n      .version(\"3.2.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "fca362b3-6035-4714-afbb-6cb8703f0a95",
    "prId" : 31215,
    "prUrl" : "https://github.com/apache/spark/pull/31215#pullrequestreview-570043082",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "33480e56-b31c-41f1-9917-b4eb40534926",
        "parentId" : null,
        "authorId" : "6a8d0007-eb0f-4236-a65a-2174a14ac689",
        "body" : "looks fine",
        "createdAt" : "2021-01-17T10:25:45Z",
        "updatedAt" : "2021-01-18T00:47:59Z",
        "lastEditedBy" : "6a8d0007-eb0f-4236-a65a-2174a14ac689",
        "tags" : [
        ]
      }
    ],
    "commit" : "9b7b19d677fd9c5d6416f5600866d47186883dcb",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +482,486 @@      .createOptional\n\n  private[spark] val STORAGE_DECOMMISSION_FALLBACK_STORAGE_CLEANUP =\n    ConfigBuilder(\"spark.storage.decommission.fallbackStorage.cleanUp\")\n      .doc(\"If true, Spark cleans up its fallback storage data during shutting down.\")"
  },
  {
    "id" : "77d791b9-a1ca-4824-b317-27dc26388acd",
    "prId" : 31215,
    "prUrl" : "https://github.com/apache/spark/pull/31215#pullrequestreview-570116057",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "79d37af8-0896-4f10-9700-6e0913a6ad24",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "BTW, should we plan to document this fallback storage somewhere like https://spark.apache.org/docs/latest/configuration.html#kubernetes or https://spark.apache.org/docs/latest/job-scheduling.html#graceful-decommission-of-executors? Should better be done separately though.",
        "createdAt" : "2021-01-18T00:44:54Z",
        "updatedAt" : "2021-01-18T00:47:59Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "324cbafc-8bd7-4d65-bfee-727c31226799",
        "parentId" : "79d37af8-0896-4f10-9700-6e0913a6ad24",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Sure, I'll do that separately. Thanks.",
        "createdAt" : "2021-01-18T00:49:06Z",
        "updatedAt" : "2021-01-18T00:49:06Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "9b7b19d677fd9c5d6416f5600866d47186883dcb",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +483,487 @@\n  private[spark] val STORAGE_DECOMMISSION_FALLBACK_STORAGE_CLEANUP =\n    ConfigBuilder(\"spark.storage.decommission.fallbackStorage.cleanUp\")\n      .doc(\"If true, Spark cleans up its fallback storage data during shutting down.\")\n      .version(\"3.2.0\")"
  },
  {
    "id" : "3e70f09a-5975-4fe9-bd0c-f6ef44bbb1f2",
    "prId" : 31151,
    "prUrl" : "https://github.com/apache/spark/pull/31151#pullrequestreview-566209146",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3e4986ad-b9e5-450b-83aa-4565094a19f2",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "cc @dongjoon-hyun ",
        "createdAt" : "2021-01-12T12:12:36Z",
        "updatedAt" : "2021-01-12T12:12:37Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "6087e59706181d8e1fbd2d88a33d9ae6a7ac2c2c",
    "line" : 57,
    "diffHunk" : "@@ -1,1 +473,477 @@\n  private[spark] val STORAGE_DECOMMISSION_FALLBACK_STORAGE_PATH =\n    ConfigBuilder(\"spark.decommission.storage.fallbackStoragePath\")\n      .doc(\"The location for fallback storage during block manager decommissioning. \" +\n        \"For example, `s3a://spark-storage/`. In case of empty, fallback storage is disabled. \" +"
  },
  {
    "id" : "8263f619-26fc-4655-9100-44f396879eb3",
    "prId" : 30876,
    "prUrl" : "https://github.com/apache/spark/pull/30876#pullrequestreview-558207527",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f8e4ee4c-8b44-4c0b-b1aa-f09a83b000e3",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Should we maybe just enable this by default when we're in Kubernates? I am okay with enabling it by default too if other people are fine. cc @tgravescs and @Ngone51 too FYI",
        "createdAt" : "2020-12-22T01:29:08Z",
        "updatedAt" : "2020-12-23T03:13:54Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "ce37bddf-1562-4b52-a529-d377c4c45bbc",
        "parentId" : "f8e4ee4c-8b44-4c0b-b1aa-f09a83b000e3",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "For the other resource managers, this will be helpful because this is a kind of self-healing code. And, this code has been here for a long time.",
        "createdAt" : "2020-12-22T02:26:38Z",
        "updatedAt" : "2020-12-23T03:13:54Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "4922ebd8-a993-4171-ac4e-da920968d5d1",
        "parentId" : "f8e4ee4c-8b44-4c0b-b1aa-f09a83b000e3",
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "I am not sure how widely this is used, particularly as it is not enabled by default.\r\nEspecially in context of dynamic resource allocation, it can become very chatty when executor's start getting dropped.\r\n\r\nGiven this, I am not very keen on enabling it atleast for yarn. Thoughts @tgravescs ?",
        "createdAt" : "2020-12-22T16:45:03Z",
        "updatedAt" : "2020-12-23T03:13:54Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "376027b7-8a40-4ba4-9578-66e9113e5824",
        "parentId" : "f8e4ee4c-8b44-4c0b-b1aa-f09a83b000e3",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Hi, @mridulm . Actually, we are using it now and it's a good time to test it by default, isn't it?\r\n> I am not sure how widely this is used, particularly as it is not enabled by default.\r\n\r\nFor the following, Apache Spark usually drop only empty executors. If you are saying a `storage timeout` configuration, I believe that what we need is to improve `storage timeout` configuration behavior after this enabling. I guess `storage timeout` had better not cause any `chatty` situation, of course.\r\n> Especially in context of dynamic resource allocation, it can become very chatty ",
        "createdAt" : "2020-12-22T18:02:10Z",
        "updatedAt" : "2020-12-23T03:13:54Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "56cff0fe-544d-4792-9772-73c7f33210fc",
        "parentId" : "f8e4ee4c-8b44-4c0b-b1aa-f09a83b000e3",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Thanks for the ping. I think I'm OK with the change. And shall we document the behaviour change in `core-migration-guide.md`?",
        "createdAt" : "2020-12-23T02:23:39Z",
        "updatedAt" : "2020-12-23T03:13:54Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "991cf7f9-dd8d-413b-ba83-a4e4d9fde6da",
        "parentId" : "f8e4ee4c-8b44-4c0b-b1aa-f09a83b000e3",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Sure, I'll update the PR, @Ngone51 .",
        "createdAt" : "2020-12-23T03:05:26Z",
        "updatedAt" : "2020-12-23T03:13:54Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "5b1aacaa-55ca-44d1-b915-a0846a7574f2",
        "parentId" : "f8e4ee4c-8b44-4c0b-b1aa-f09a83b000e3",
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "In the past, I found this to be noisy for the cases where replication was enabled - but this was a while back, and I would like to understand better what the 'cost' of enabling this for nontrivial usecases is for master : disabled by default means only developers who specifically test for it pay the price; not everyone.\r\nIt is quite common for an application to have references to a persisted RDD even after its use - with the loss of the RDD blocks having little to no functional impact.\r\nThis is similar to loss of blocks for an unreplicated persisted RDD - we do not proactively recompute the lost blocks; but do so on demand.\r\n\r\nIf the idea is we enable this for master, and evaluate the impact over the next 6 months and revisit at the end, I am fine with that: but an evaluation would need to be done before this goes out - else anyone using replicated storage will also get hit with the impact of proactive replication as well, and will need to disable this for their applications.",
        "createdAt" : "2020-12-23T20:02:53Z",
        "updatedAt" : "2020-12-23T20:02:53Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "b7df3051-6346-4caf-991c-e727a6ce11b5",
        "parentId" : "f8e4ee4c-8b44-4c0b-b1aa-f09a83b000e3",
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "> but an evaluation would need to be done before this goes out \r\n\r\nor perhaps identify the subset of conditions where it makes sense to enable it by default.",
        "createdAt" : "2020-12-23T20:11:17Z",
        "updatedAt" : "2020-12-23T20:11:18Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      }
    ],
    "commit" : "28fb534974203a382d472f77747d628ce046b83d",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +385,389 @@      .version(\"2.2.0\")\n      .booleanConf\n      .createWithDefault(true)\n\n  private[spark] val STORAGE_MEMORY_MAP_THRESHOLD ="
  },
  {
    "id" : "444aad32-8165-4c8e-8899-3060129088fe",
    "prId" : 30710,
    "prUrl" : "https://github.com/apache/spark/pull/30710#pullrequestreview-564425282",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d58fa2f0-88c1-461f-9f95-af73a8ff4e34",
        "parentId" : null,
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "please add the .doc section as well.",
        "createdAt" : "2021-01-08T15:32:20Z",
        "updatedAt" : "2021-01-09T06:53:31Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "99db62a4-f941-48f4-ab06-0ea81177471d",
        "parentId" : "d58fa2f0-88c1-461f-9f95-af73a8ff4e34",
        "authorId" : "fb1da7a4-0c05-42a2-9913-c8bd60c58a4f",
        "body" : "done",
        "createdAt" : "2021-01-08T16:57:29Z",
        "updatedAt" : "2021-01-09T06:53:31Z",
        "lastEditedBy" : "fb1da7a4-0c05-42a2-9913-c8bd60c58a4f",
        "tags" : [
        ]
      }
    ],
    "commit" : "86c8d8c51050b67ae8714f1dd1249f62d8b56bd0",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +1894,1898 @@      .doc(\"Minimum amount of time a task runs before being considered for speculation. \" +\n        \"This can be used to avoid launching speculative copies of tasks that are very short.\")\n      .version(\"3.2.0\")\n      .timeConf(TimeUnit.MILLISECONDS)\n      .createWithDefault(100)"
  },
  {
    "id" : "d64c0c7b-ae4d-4b68-b9f5-bc02ee0c37be",
    "prId" : 30710,
    "prUrl" : "https://github.com/apache/spark/pull/30710#pullrequestreview-673426557",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0a93fe18-67ef-43e7-94d5-2fdbf3b1923b",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "According to the [config naming policy](https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala#L21), `spark.speculation.shortTaskThreshold` may be better?\r\n\r\ncc @Ngone51 ",
        "createdAt" : "2021-06-01T16:38:47Z",
        "updatedAt" : "2021-06-01T16:38:47Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "9f1469b0-371f-46b9-ac31-2e404659d4c1",
        "parentId" : "0a93fe18-67ef-43e7-94d5-2fdbf3b1923b",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "I'm fine with renaming, though I don't care for shortTaskThreshold.  why not just minThreshold or minTaskThreshold",
        "createdAt" : "2021-06-01T16:47:47Z",
        "updatedAt" : "2021-06-01T16:47:48Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "8e67ec9d-c81d-4f37-987f-399624f9229b",
        "parentId" : "0a93fe18-67ef-43e7-94d5-2fdbf3b1923b",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "`minTaskRuntime`? I admit that I'm not good at naming ...",
        "createdAt" : "2021-06-01T17:10:48Z",
        "updatedAt" : "2021-06-01T17:10:48Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "2f3c21d2-cc73-4635-b458-f79a40f0a8da",
        "parentId" : "0a93fe18-67ef-43e7-94d5-2fdbf3b1923b",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "that works for me",
        "createdAt" : "2021-06-01T17:54:07Z",
        "updatedAt" : "2021-06-01T17:54:08Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "86c8d8c51050b67ae8714f1dd1249f62d8b56bd0",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1891,1895 @@\n  private[spark] val SPECULATION_MIN_THRESHOLD =\n    ConfigBuilder(\"spark.speculation.min.threshold\")\n      .doc(\"Minimum amount of time a task runs before being considered for speculation. \" +\n        \"This can be used to avoid launching speculative copies of tasks that are very short.\")"
  },
  {
    "id" : "17ea46f8-ee88-47b0-9280-d0b0bdb46d9c",
    "prId" : 30691,
    "prUrl" : "https://github.com/apache/spark/pull/30691#pullrequestreview-652586113",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b0bfc379-e366-46ec-b280-61955d161183",
        "parentId" : null,
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "Taken together, this accounts for driver waiting for upto 20s per stage ... do we have recommendations on how users can tune this ?",
        "createdAt" : "2021-05-05T15:19:59Z",
        "updatedAt" : "2021-05-11T05:07:03Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "4018682d-b30b-40b5-948e-5f653dd0101a",
        "parentId" : "b0bfc379-e366-46ec-b280-61955d161183",
        "authorId" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "body" : "I think currently this is hard to tune but once we have the changes in for `SPARK-33701` which does adaptive merge finalization, mostly this should be taken care of.",
        "createdAt" : "2021-05-05T17:46:41Z",
        "updatedAt" : "2021-05-11T05:07:03Z",
        "lastEditedBy" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "tags" : [
        ]
      }
    ],
    "commit" : "35d06150f37e01256314af4956c6e8fdcf7244b4",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +2105,2109 @@      .checkValue(_ >= 0L, \"Timeout must be >= 0.\")\n      .createWithDefaultString(\"10s\")\n\n  private[spark] val SHUFFLE_MERGER_MAX_RETAINED_LOCATIONS =\n    ConfigBuilder(\"spark.shuffle.push.maxRetainedMergerLocations\")"
  },
  {
    "id" : "b1b83291-9132-475b-bf29-7f0446b0fb57",
    "prId" : 30681,
    "prUrl" : "https://github.com/apache/spark/pull/30681#pullrequestreview-587200923",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ef8d0388-cf14-4613-8d7b-4f572b3adc00",
        "parentId" : null,
        "authorId" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "body" : "Do we have another configuration in `spark.file.` namespace? If not, we usually do not make a new namespace.",
        "createdAt" : "2021-02-10T02:24:53Z",
        "updatedAt" : "2021-02-10T02:24:53Z",
        "lastEditedBy" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "tags" : [
        ]
      }
    ],
    "commit" : "0824bfc6597c98c5057aeef16ea6c57637a0f654",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +819,823 @@\n  private[spark] val FILE_INSERT_STATUS_LOG_FLAG =\n    ConfigBuilder(\"spark.file.insert.status.log.flag\")\n      .internal()\n      .doc(\"When hive sql executes to generate HDFS data, \" +"
  },
  {
    "id" : "93b2c6e1-f75b-4a29-832d-466b74147b50",
    "prId" : 30528,
    "prUrl" : "https://github.com/apache/spark/pull/30528#pullrequestreview-540426955",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b07a6878-3fef-432d-af55-4d3ecf5e3337",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Although this is `internal`, shall we add Spark version, @zsxwing ? Since SPARK-33587 is filed as `Improvement`, `3.1.0`?\r\n```scala\r\n.version(\"3.1.0\")\r\n```",
        "createdAt" : "2020-11-29T00:42:47Z",
        "updatedAt" : "2020-11-29T00:59:59Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "bd3110ea-44b5-4e2a-94fd-c752a511e7ae",
        "parentId" : "b07a6878-3fef-432d-af55-4d3ecf5e3337",
        "authorId" : "2d6b46ba-4100-4c4d-9341-fff5e39647ec",
        "body" : "done",
        "createdAt" : "2020-11-29T01:00:02Z",
        "updatedAt" : "2020-11-29T01:00:02Z",
        "lastEditedBy" : "2d6b46ba-4100-4c4d-9341-fff5e39647ec",
        "tags" : [
        ]
      },
      {
        "id" : "b40b971a-2e86-4191-bdf5-472d9dc5345b",
        "parentId" : "b07a6878-3fef-432d-af55-4d3ecf5e3337",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Thanks!",
        "createdAt" : "2020-11-29T01:01:28Z",
        "updatedAt" : "2020-11-29T01:01:29Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "312f0422f7c6379747762c0b2eaf523e76c96a9b",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +1951,1955 @@      .doc(\"The max depth of the exception chain in a failed task Spark will search for a fatal \" +\n        \"error to check whether it should kill an executor. 0 means not checking any fatal \" +\n        \"error, 1 means checking only the exception but not the cause, and so on.\")\n      .internal()\n      .version(\"3.1.0\")"
  },
  {
    "id" : "c0670577-c42c-41a0-b471-96de469d5ee6",
    "prId" : 30492,
    "prUrl" : "https://github.com/apache/spark/pull/30492#pullrequestreview-541311974",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4a31f533-d495-46d7-9237-2bb864ca60e4",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Because we only register fallback storage when SparkContext initialization, I think it doesn't make sense to change this config after creating SparkContext, so this sounds like a static SQL config?",
        "createdAt" : "2020-11-30T20:38:44Z",
        "updatedAt" : "2020-11-30T20:38:44Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "007e2475-54b1-4a16-b571-cf01b05133cc",
        "parentId" : "4a31f533-d495-46d7-9237-2bb864ca60e4",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "This is `CORE` module which can work without SQL module. So, we cannot put it to Static SQL conf, @viirya .",
        "createdAt" : "2020-11-30T20:40:50Z",
        "updatedAt" : "2020-11-30T20:40:51Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "253eb43e-d2ac-45ed-9a06-9f27465905d3",
        "parentId" : "4a31f533-d495-46d7-9237-2bb864ca60e4",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Oh I see. Seems we don't have static config for core.",
        "createdAt" : "2020-11-30T20:46:20Z",
        "updatedAt" : "2020-11-30T20:46:20Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "8b226202-db7b-41c9-a659-1c9c7b6c772d",
        "parentId" : "4a31f533-d495-46d7-9237-2bb864ca60e4",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Could you add a simple note to the doc? E.g., \"Fallback storage is registered during SparkContext initialization, so this must be enabled before creating SparkContext.\"",
        "createdAt" : "2020-11-30T20:50:39Z",
        "updatedAt" : "2020-11-30T20:50:39Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "c67c03a6-274f-420d-ac50-9041661c9834",
        "parentId" : "4a31f533-d495-46d7-9237-2bb864ca60e4",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Ur, but it will be the same for all decommission conf like `spark.storage.decommission.enabled` and the other general conf like `spark.driver.host` and `spark.driver.port`.",
        "createdAt" : "2020-11-30T21:28:42Z",
        "updatedAt" : "2020-11-30T21:28:42Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "3d3d09b0-a337-4c5c-9d69-d4d956994b1f",
        "parentId" : "4a31f533-d495-46d7-9237-2bb864ca60e4",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Oh okay.",
        "createdAt" : "2020-11-30T21:32:47Z",
        "updatedAt" : "2020-11-30T21:32:47Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "9c9317378eb61b40f766e41dd168d2189f6008c1",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +472,476 @@      .createWithDefaultString(\"30s\")\n\n  private[spark] val STORAGE_DECOMMISSION_FALLBACK_STORAGE_PATH =\n    ConfigBuilder(\"spark.storage.decommission.fallbackStorage.path\")\n      .doc(\"The location for fallback storage during block manager decommissioning. \" +"
  },
  {
    "id" : "527d5fc9-1012-4004-a336-add2eaa83606",
    "prId" : 30486,
    "prUrl" : "https://github.com/apache/spark/pull/30486#pullrequestreview-538054997",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bd2d0f9f-e245-4388-a973-682d84a6ed03",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Ur, if this is still experimental, we should not deprecate the old ways.\r\n> This configuration is experimental.",
        "createdAt" : "2020-11-24T17:18:10Z",
        "updatedAt" : "2020-11-30T05:09:49Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "54642c07-08f1-4527-ba6b-5b7ce0043a73",
        "parentId" : "bd2d0f9f-e245-4388-a973-682d84a6ed03",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Makes sense. I will instead say the behaviour will be deprecated. We can remove that back once this configuration becomes stable.",
        "createdAt" : "2020-11-24T23:46:03Z",
        "updatedAt" : "2020-11-30T05:09:49Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "e35e94cad2888c98e03feb391a8dfcca8afa365c",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +1809,1813 @@      \"executor. .jar, .tar.gz, .tgz and .zip are supported. You can specify the directory \" +\n      \"name to unpack via adding '#' after the file name to unpack, for example, \" +\n      \"'file.zip#directory'. This configuration is experimental.\")\n    .stringConf\n    .toSequence"
  },
  {
    "id" : "c53095ef-63e0-41b9-b28f-5b0b9d2f08fc",
    "prId" : 30312,
    "prUrl" : "https://github.com/apache/spark/pull/30312#pullrequestreview-545165072",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6588efd1-2017-4831-b0f6-47e5246a7de3",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "This should be `2m` according to the comment?",
        "createdAt" : "2020-12-03T03:51:33Z",
        "updatedAt" : "2020-12-19T04:21:15Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "d8f9a6e8-5b35-48b7-8b3f-86e47a612f2e",
        "parentId" : "6588efd1-2017-4831-b0f6-47e5246a7de3",
        "authorId" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "body" : "The comment actually means that it should be higher than `2m`. If it is 2m than each block will be loaded in memory which increases memory overhead. I will make the comment more clear.",
        "createdAt" : "2020-12-03T17:31:01Z",
        "updatedAt" : "2020-12-19T04:21:15Z",
        "lastEditedBy" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "tags" : [
        ]
      },
      {
        "id" : "4879484e-6e31-421c-a414-78e78768fbdd",
        "parentId" : "6588efd1-2017-4831-b0f6-47e5246a7de3",
        "authorId" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "body" : "done",
        "createdAt" : "2020-12-04T17:40:03Z",
        "updatedAt" : "2020-12-19T04:21:15Z",
        "lastEditedBy" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "tags" : [
        ]
      }
    ],
    "commit" : "21ea881f398a35038e2359ef0b4ed5cd15aab736",
    "line" : 32,
    "diffHunk" : "@@ -1,1 +2052,2056 @@      // batch of block will be loaded in memory with memory mapping, which has higher overhead\n      // with small MB sized chunk of data.\n      .createWithDefaultString(\"3m\")\n}"
  },
  {
    "id" : "8c3068cb-0f21-4cd3-8ce7-735da242f1d4",
    "prId" : 30311,
    "prUrl" : "https://github.com/apache/spark/pull/30311#pullrequestreview-528732216",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c4ad6aa4-eba5-43d2-aa4d-32099340dfc7",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "This change means that the non-cluster mode refers to this value?",
        "createdAt" : "2020-11-11T13:26:23Z",
        "updatedAt" : "2020-11-11T13:26:23Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "a1a7f4d9-4539-4504-8875-fb2422577fbd",
        "parentId" : "c4ad6aa4-eba5-43d2-aa4d-32099340dfc7",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "yes, the 'client' deploy mode respects this too",
        "createdAt" : "2020-11-12T03:53:16Z",
        "updatedAt" : "2020-11-12T03:53:16Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "3aa18e885b60c232e2eba18ab92629edd4efb1e5",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +303,307 @@\n  private[spark] val EXECUTOR_MEMORY_OVERHEAD = ConfigBuilder(\"spark.executor.memoryOverhead\")\n    .doc(\"The amount of non-heap memory to be allocated per executor, in MiB unless otherwise\" +\n      \" specified.\")\n    .version(\"2.3.0\")"
  },
  {
    "id" : "932f2c56-7844-44fa-960f-7a05e6371a05",
    "prId" : 30164,
    "prUrl" : "https://github.com/apache/spark/pull/30164#pullrequestreview-526871460",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c994d568-746e-4f32-8067-b4b2b95f7b24",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Please add the version (3.1.0) info for these newly added confs.",
        "createdAt" : "2020-11-10T05:50:37Z",
        "updatedAt" : "2020-11-20T00:32:25Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "5ce29340c8aa4a6aadfc8d00cb5053a9be9aa839",
    "line" : 50,
    "diffHunk" : "@@ -1,1 +1992,1996 @@      .version(\"3.1.0\")\n      .doubleConf\n      .createWithDefault(5)\n}"
  },
  {
    "id" : "9ebd5055-0a17-403c-9dcc-3b0c6e47e05a",
    "prId" : 30164,
    "prUrl" : "https://github.com/apache/spark/pull/30164#pullrequestreview-528895493",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c031950d-71a5-42ab-9bdc-8fe094a19e45",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Please use `${SHUFFLE_MERGER_LOCATIONS_MIN_THRESHOLD_RATIO.key}` instead of hard-coded.",
        "createdAt" : "2020-11-12T09:24:59Z",
        "updatedAt" : "2020-11-20T00:32:25Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "5ce29340c8aa4a6aadfc8d00cb5053a9be9aa839",
    "line" : 45,
    "diffHunk" : "@@ -1,1 +1987,1991 @@        s\"${SHUFFLE_MERGER_LOCATIONS_MIN_THRESHOLD_RATIO.key} ratio number of mergers needed to \" +\n        \"enable push based shuffle for a stage. For eg: with 1000 partitions for the child \" +\n        \"stage with spark.shuffle.push.mergersMinStaticThreshold as 5 and \" +\n        s\"${SHUFFLE_MERGER_LOCATIONS_MIN_THRESHOLD_RATIO.key} set to 0.05, we would need \" +\n        \"at least 50 mergers to enable push based shuffle for that stage.\")"
  },
  {
    "id" : "711c980f-ea2d-431d-8212-097abcfcedbf",
    "prId" : 29906,
    "prUrl" : "https://github.com/apache/spark/pull/29906#pullrequestreview-501482167",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d6d7dd2b-bc8b-4762-9e58-1e51c9c1ed69",
        "parentId" : null,
        "authorId" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "body" : "Given that this is already a legacy config marked as deprecated, it doesn't seem worth adding a new version. What do you think?",
        "createdAt" : "2020-10-02T22:40:43Z",
        "updatedAt" : "2020-10-28T13:50:33Z",
        "lastEditedBy" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "tags" : [
        ]
      }
    ],
    "commit" : "b38dd66500cdd4a12f78cca1eeacf116f0ea5b4e",
    "line" : 97,
    "diffHunk" : "@@ -1,1 +791,795 @@      .version(\"3.1.0\")\n      .withAlternative(\"spark.scheduler.executorTaskBlacklistTime\")\n      .timeConf(TimeUnit.MILLISECONDS)\n      .createOptional\n"
  },
  {
    "id" : "40585586-90aa-41ad-9035-8460d8d6b6d9",
    "prId" : 29466,
    "prUrl" : "https://github.com/apache/spark/pull/29466#pullrequestreview-469688304",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fc2aff03-2a17-4abf-b4e0-7cfd838cc397",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "nit: some 2 spaces here ... before from",
        "createdAt" : "2020-08-18T15:46:47Z",
        "updatedAt" : "2020-08-18T16:39:40Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "3efc2477-64b8-4d89-bc43-f10880b67582",
        "parentId" : "fc2aff03-2a17-4abf-b4e0-7cfd838cc397",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "it's only one space?",
        "createdAt" : "2020-08-18T16:39:58Z",
        "updatedAt" : "2020-08-18T16:39:58Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "423d9c5d-a437-4fba-b646-8813f32fd0c5",
        "parentId" : "fc2aff03-2a17-4abf-b4e0-7cfd838cc397",
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "Yeah. Sorry. Fonts are misleading :-) ",
        "createdAt" : "2020-08-18T17:47:11Z",
        "updatedAt" : "2020-08-18T17:47:11Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      }
    ],
    "commit" : "c4c0ef4458072cfbe4dfa3b23c1f32b340e36f9a",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +1872,1876 @@        s\"gracefully. Spark will try to migrate all the RDD blocks (controlled by \" +\n        s\"${STORAGE_DECOMMISSION_RDD_BLOCKS_ENABLED.key}) and shuffle blocks (controlled by \" +\n        s\"${STORAGE_DECOMMISSION_SHUFFLE_BLOCKS_ENABLED.key}) from the decommissioning \" +\n        s\"executor to a remote executor when ${STORAGE_DECOMMISSION_ENABLED.key} is enabled. \" +\n        s\"With decommission enabled, Spark will also decommission an executor instead of \" +"
  },
  {
    "id" : "c1bd9a67-4bac-4d10-acde-8c517535b892",
    "prId" : 29278,
    "prUrl" : "https://github.com/apache/spark/pull/29278#pullrequestreview-457153997",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "254d873a-2d14-4edf-b406-9613cc7ab4c1",
        "parentId" : null,
        "authorId" : "43998e22-6c2f-401d-9914-8cecf6fad929",
        "body" : "I'll submit another PR with the opposite default value against `branch-3.0` when this is good to go.",
        "createdAt" : "2020-07-29T01:54:36Z",
        "updatedAt" : "2020-07-31T01:53:24Z",
        "lastEditedBy" : "43998e22-6c2f-401d-9914-8cecf6fad929",
        "tags" : [
        ]
      },
      {
        "id" : "00e7ec53-fc48-430c-a78e-b54ebaa86f3b",
        "parentId" : "254d873a-2d14-4edf-b406-9613cc7ab4c1",
        "authorId" : "43998e22-6c2f-401d-9914-8cecf6fad929",
        "body" : "Also I'll add a migration guide later.",
        "createdAt" : "2020-07-29T02:33:52Z",
        "updatedAt" : "2020-07-31T01:53:24Z",
        "lastEditedBy" : "43998e22-6c2f-401d-9914-8cecf6fad929",
        "tags" : [
        ]
      }
    ],
    "commit" : "0ea7ea956b3867b67c869fddcc294cb268157a92",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +1913,1917 @@    ConfigBuilder(\"spark.driver.allowSparkContextInExecutors\")\n      .doc(\"If set to true, SparkContext can be created in executors.\")\n      .version(\"3.0.1\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "c9e2fcf9-75be-4526-b41e-b3aad93ffb17",
    "prId" : 28864,
    "prUrl" : "https://github.com/apache/spark/pull/28864#pullrequestreview-435987239",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5f4c79fc-1637-4eb0-9d03-62740ab7a74c",
        "parentId" : null,
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "need to update the configuration.md doc for this name change.",
        "createdAt" : "2020-06-23T17:11:25Z",
        "updatedAt" : "2020-07-13T17:01:35Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "9c43172daf2b9ad38fce32f5349c87944fa62caa",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +461,465 @@\n  private[spark] val STORAGE_BLOCKMANAGER_HEARTBEAT_TIMEOUT =\n    ConfigBuilder(\"spark.storage.blockManagerHeartbeatTimeoutMs\")\n      .version(\"0.7.0\")\n      .withAlternative(\"spark.storage.blockManagerSlaveTimeoutMs\")"
  },
  {
    "id" : "716b5cf8-eaee-457f-98a8-3b90498aa4d4",
    "prId" : 28619,
    "prUrl" : "https://github.com/apache/spark/pull/28619#pullrequestreview-439401780",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "edfb4fd5-7d64-42a3-a822-14617e4c83b0",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "So the timeout is decided by the cloud vendors? What does this config specify?",
        "createdAt" : "2020-06-17T08:34:46Z",
        "updatedAt" : "2020-06-17T08:34:46Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "e3abec3d-2a7b-4594-8592-695690fffe7a",
        "parentId" : "edfb4fd5-7d64-42a3-a822-14617e4c83b0",
        "authorId" : "ad4cb313-47b2-4cfa-9c1f-0a3596794a67",
        "body" : "@cloud-fan This config can be set by users based on their setups. If they are using AWS spot nodes, timeout can be set to somewhere around 120 seconds, if they are using fix duration 6hrs spot blocks (say they decommission executors at 5:45), timeout can be set to 15 mins and so on.\r\n\r\nIf user doesn't set this timeout, things will remain as they were and tasks running on decommission executors  won't get any special treatment with respect to speculation.",
        "createdAt" : "2020-06-18T05:17:43Z",
        "updatedAt" : "2020-06-18T05:17:44Z",
        "lastEditedBy" : "ad4cb313-47b2-4cfa-9c1f-0a3596794a67",
        "tags" : [
        ]
      },
      {
        "id" : "f68ca0aa-77bc-4f69-bb40-7534007c9730",
        "parentId" : "edfb4fd5-7d64-42a3-a822-14617e4c83b0",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "is it possible that Spark can get this timeout value from the cluster manager? So that users don't need to set it manually. cc @holdenk ",
        "createdAt" : "2020-06-18T06:52:00Z",
        "updatedAt" : "2020-06-18T06:52:01Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "d70714bc-2cf4-469d-b8cf-bf146abf2f3c",
        "parentId" : "edfb4fd5-7d64-42a3-a822-14617e4c83b0",
        "authorId" : "ad4cb313-47b2-4cfa-9c1f-0a3596794a67",
        "body" : "@cloud-fan As per my understanding, Worker Decommissioning is getting triggered currently using SIGPWR signal (and not via some message coming from YARN/Kubernetes Cluster manager). So getting this timeout from Spark Cluster Manager might not be possible. We might be able to do this once Spark's Worker Decommissioning logic starts triggering via communication from YARN etc in future. cc  @holdenk ",
        "createdAt" : "2020-06-22T05:21:35Z",
        "updatedAt" : "2020-06-22T05:21:35Z",
        "lastEditedBy" : "ad4cb313-47b2-4cfa-9c1f-0a3596794a67",
        "tags" : [
        ]
      },
      {
        "id" : "4ad52605-dec1-43b3-918d-fb450e03c703",
        "parentId" : "edfb4fd5-7d64-42a3-a822-14617e4c83b0",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "I believe there are some situations where we can know the length of time from the cluster manager or from Spark it's self, but not all. I think having a configurable default for folks who know their cloud provider environment makes sense",
        "createdAt" : "2020-06-29T18:35:26Z",
        "updatedAt" : "2020-06-29T18:35:27Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "d87b311be85819ae884e2a24d94926fdd51165de",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +1848,1852 @@        \"This config is useful for cloud environments where we know in advance when \" +\n        \"an executor is going to go down after decommissioning signal i.e. around 2 mins \" +\n        \"in aws spot nodes, 1/2 hrs in spot block nodes etc. This config is currently \" +\n        \"used to decide what tasks running on decommission executors to speculate.\")\n      .version(\"3.1.0\")"
  },
  {
    "id" : "aa058fc6-a2be-41f1-9530-5734e06c1291",
    "prId" : 28426,
    "prUrl" : "https://github.com/apache/spark/pull/28426#pullrequestreview-403951301",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b932e42c-0162-45a3-871b-d70e03d6dfc6",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "I agree this name-spacing into `.shuffleTracking`.",
        "createdAt" : "2020-04-30T23:49:34Z",
        "updatedAt" : "2020-04-30T23:51:51Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "ec985fdc7a6fdd335a7a0cffc394467e7d7016ea",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +529,533 @@\n  private[spark] val DYN_ALLOCATION_SHUFFLE_TRACKING_TIMEOUT =\n    ConfigBuilder(\"spark.dynamicAllocation.shuffleTracking.timeout\")\n      .version(\"3.0.0\")\n      .timeConf(TimeUnit.MILLISECONDS)"
  },
  {
    "id" : "284017ce-04b4-430b-af88-364db0400dea",
    "prId" : 28370,
    "prUrl" : "https://github.com/apache/spark/pull/28370#pullrequestreview-413231624",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b3147723-10cb-41d0-b56e-3757e02959c2",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "I think we can just use `spark.storage.maxReplicationFailures` directly. Less configurations contribute to better UX.",
        "createdAt" : "2020-05-15T13:59:58Z",
        "updatedAt" : "2020-05-15T13:59:58Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "e3ffbdcd-d5a4-4d98-b9b4-91a246ab1bec",
        "parentId" : "b3147723-10cb-41d0-b56e-3757e02959c2",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "So I'm not sure that's a great idea. Looking at `maxReplicationFailures` the default is set to one, which certainly makes sense in the situation where we don't expect the host to be exiting. But this situation is different, we know the current block is going to disappear soon so it makes sense to more aggressively try and copy the block.",
        "createdAt" : "2020-05-15T17:25:00Z",
        "updatedAt" : "2020-05-15T17:25:01Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "1f4e99e2-a654-4ef7-9468-5a2ee2259b91",
        "parentId" : "b3147723-10cb-41d0-b56e-3757e02959c2",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "I see, thanks for your explanation.\r\n",
        "createdAt" : "2020-05-18T02:00:19Z",
        "updatedAt" : "2020-05-18T02:00:19Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "c34305662cd91d05b5160c7de72e35e4108b6755",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +422,426 @@\n  private[spark] val STORAGE_DECOMMISSION_MAX_REPLICATION_FAILURE_PER_BLOCK =\n    ConfigBuilder(\"spark.storage.decommission.maxReplicationFailuresPerBlock\")\n      .internal()\n      .doc(\"Maximum number of failures which can be handled for the replication of \" +"
  },
  {
    "id" : "2c0ddbf1-51cd-45a1-9d6f-24fee9123389",
    "prId" : 28053,
    "prUrl" : "https://github.com/apache/spark/pull/28053#pullrequestreview-385289302",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "687f5b27-835d-4482-8da9-118db47f6f74",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Hi, @tgravescs .\r\nCould you add `.doc` and `.version` like the other confs?",
        "createdAt" : "2020-03-27T18:29:37Z",
        "updatedAt" : "2020-04-01T19:28:46Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "f2373bfb-4698-469d-a984-822785609f8a",
        "parentId" : "687f5b27-835d-4482-8da9-118db47f6f74",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "yes, I will add",
        "createdAt" : "2020-03-27T18:34:35Z",
        "updatedAt" : "2020-04-01T19:28:46Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "f18f0fc1-770a-4ce4-8c3f-12189776944c",
        "parentId" : "687f5b27-835d-4482-8da9-118db47f6f74",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "When I look through confs which introduced around the feature, I feel that they're kind of messy. For example, we already have:\r\n\r\n- spark.resources.discoveryPlugin\r\n- spark.driver.resourcesFile\r\n- spark.driver.resource.GPU.amount\r\n\r\nSince they're all from the same feature, so I'm thinking that if we can make them under the same namespace, e.g. `spark.resource`.\r\n\r\nWDYT?",
        "createdAt" : "2020-03-30T07:18:14Z",
        "updatedAt" : "2020-04-01T19:28:46Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "2fc62b03-53a6-4e33-8afe-2fb85f427447",
        "parentId" : "687f5b27-835d-4482-8da9-118db47f6f74",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "our normal format for differentiating driver and executor configs is spark.driver. and spark.executor., so I am not in favor or changing that to be spark.resource.driver spark.resource.executor as I think that is not consistent.\r\nI'm fine with changing to spark.driver.resource. and spark.executor.resource. or if not specific spark.resource.\r\nI'm not sure what you would call spark.driver.resourcesFile that makes sense though, unless you call it. spark.driver.resource.resourcesFile which seems like just extra but is consistent among the feature.\r\n\r\nSimilar here, spark.scheduler is a prefix that tells you what component is affects, if we want to call this spark.scheduler.resource.resourceProfileMergeConflicts or something like that I'd be ok with that.",
        "createdAt" : "2020-03-30T14:31:34Z",
        "updatedAt" : "2020-04-01T19:28:46Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "0ea4b552-c9cf-4e02-9256-cb4623bd184a",
        "parentId" : "687f5b27-835d-4482-8da9-118db47f6f74",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Yeah, I also know the problem here. And I just wonder if they would not be such friendly for user. We shall doc them in a well organized way at least.",
        "createdAt" : "2020-04-01T06:12:11Z",
        "updatedAt" : "2020-04-01T19:28:46Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "452378be77b5371feb7d974776ad411b9e2bc232",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +1823,1827 @@        \"ResourceProfiles are found in RDDs going into the same stage.\")\n      .version(\"3.1.0\")\n      .booleanConf\n      .createWithDefault(false)\n}"
  },
  {
    "id" : "cdc634c8-f34f-4b7f-a23d-e65a4e5a4298",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375695813",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bbf6c0c1-6518-4b7a-acd9-ed6c9947fdbe",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-27651, commit ID: fd2bf55abaab08798a428d4e47d4050ba2b82a95#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-17T02:16:38Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +1368,1372 @@        s\"persisted RDD blocks or shuffle blocks \" +\n        s\"(when `${SHUFFLE_HOST_LOCAL_DISK_READING_ENABLED.key}` is set) from the same host.\")\n      .version(\"3.0.0\")\n      .intConf\n      .createWithDefault(1000)"
  },
  {
    "id" : "4f058eee-33c8-4ab6-a04b-b2fa53049312",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375695927",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "eb5ba01f-3c63-4c63-91ea-44fab90b9199",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-3151, commit ID: b8ffb51055108fd606b86f034747006962cd2df3#diff-abd96f2ae793cd6ea6aab5b96a3c1d7a",
        "createdAt" : "2020-03-17T02:17:07Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +1376,1380 @@      .internal()\n      .doc(\"For testing only, controls the size of chunks when memory mapping a file\")\n      .version(\"2.3.0\")\n      .bytesConf(ByteUnit.BYTE)\n      .createWithDefault(ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH)"
  },
  {
    "id" : "694a6c3c-9318-4f8a-b0fd-685029d55061",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375696057",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "eb4fd8a9-b1d1-45ad-a3fe-3dfa63e91ad2",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-24817, commit ID: 388f5a0635a2812cd71b08352e3ddc20293ec189#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-17T02:17:34Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +1386,1390 @@        \"configured time, throw a SparkException to fail all the tasks. The default value is set \" +\n        \"to 31536000(3600 * 24 * 365) so the barrier() call shall wait for one year.\")\n      .version(\"2.4.0\")\n      .timeConf(TimeUnit.SECONDS)\n      .checkValue(v => v > 0, \"The value should be a positive time value.\")"
  },
  {
    "id" : "a638bd97-6265-456f-b4b0-b7a66cc14edd",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375696229",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ac37d8b0-bd7a-496a-9583-e19f10db81c2",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-22148, commit ID: 52e9711d01694158ecb3691f2ec25c0ebe4b0207#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-17T02:18:10Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +1395,1399 @@      .doc(\"The timeout in seconds to wait to acquire a new executor and schedule a task \" +\n        \"before aborting a TaskSet which is unschedulable because of being completely blacklisted.\")\n      .version(\"2.4.1\")\n      .timeConf(TimeUnit.SECONDS)\n      .checkValue(v => v >= 0, \"The value should be a non negative time value.\")"
  },
  {
    "id" : "02d25d2d-6d90-460c-8522-29334b94926a",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375696345",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8284c5f5-1fb6-40be-9a06-f27e2bba4280",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-24819, commit ID: bfb74394a5513134ea1da9fcf4a1783b77dd64e4#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-17T02:18:36Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 36,
    "diffHunk" : "@@ -1,1 +1410,1414 @@        \"config only applies to jobs that contain one or more barrier stages, we won't perform \" +\n        \"the check on non-barrier jobs.\")\n      .version(\"2.4.0\")\n      .timeConf(TimeUnit.SECONDS)\n      .createWithDefaultString(\"15s\")"
  },
  {
    "id" : "835a69da-05b2-4613-ad57-254a4a00d4cb",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375696766",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "804256dd-9439-49cd-b350-433b325cbb96",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-24819, commit ID: bfb74394a5513134ea1da9fcf4a1783b77dd64e4#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-17T02:20:14Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 44,
    "diffHunk" : "@@ -1,1 +1424,1428 @@        \"applies to jobs that contain one or more barrier stages, we won't perform the check on \" +\n        \"non-barrier jobs.\")\n      .version(\"2.4.0\")\n      .intConf\n      .checkValue(v => v > 0, \"The max failures should be a positive value.\")"
  },
  {
    "id" : "df93989e-d432-4c2e-b4d6-20a106939458",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375697007",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "aad0f87e-fe34-42a6-987f-60468cc364e5",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-7076 and SPARK-7077 and SPARK-7080, commit ID: f49284b5bf3a69ed91a5e3e6e0ed3be93a6ab9e4#diff-5a0de266c82b95adb47d9bca714e1f1b",
        "createdAt" : "2020-03-17T02:21:11Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 52,
    "diffHunk" : "@@ -1,1 +1432,1436 @@    ConfigBuilder(\"spark.unsafe.exceptionOnMemoryLeak\")\n      .internal()\n      .version(\"1.4.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "03bf1e46-6194-408b-a1ed-ec72257f3417",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375697119",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b256a2eb-2d9e-4b46-9cf7-e2e44a43d1b1",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-21113, commit ID: 1e978b17d63d7ba20368057aa4e65f5ef6e87369#diff-93a086317cea72a113cf81056882c206",
        "createdAt" : "2020-03-17T02:21:42Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 59,
    "diffHunk" : "@@ -1,1 +1439,1443 @@    ConfigBuilder(\"spark.unsafe.sorter.spill.read.ahead.enabled\")\n      .internal()\n      .version(\"2.3.0\")\n      .booleanConf\n      .createWithDefault(true)"
  },
  {
    "id" : "5af497f5-63c1-4d6d-8a6e-b216215bcbdd",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375697264",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c5871bfc-8a94-4329-bb1e-75e8b0c4b320",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-16862, commit ID: c1937dd19a23bd096a4707656c7ba19fb5c16966#diff-93a086317cea72a113cf81056882c206",
        "createdAt" : "2020-03-17T02:22:12Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 66,
    "diffHunk" : "@@ -1,1 +1446,1450 @@    ConfigBuilder(\"spark.unsafe.sorter.spill.reader.buffer.size\")\n      .internal()\n      .version(\"2.1.0\")\n      .bytesConf(ByteUnit.BYTE)\n      .checkValue(v => 1024 * 1024 <= v && v <= MAX_BUFFER_SIZE_BYTES,"
  },
  {
    "id" : "f336b446-e31c-4cee-8bde-65db19d1b909",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375697388",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ee7b6b5d-68b7-409c-bdbe-a73fdd34de5c",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-29397, commit ID: d51d228048d519a9a666f48dc532625de13e7587#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-17T02:22:39Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 74,
    "diffHunk" : "@@ -1,1 +1459,1463 @@      .doc(\"Comma-separated list of class names implementing \" +\n        \"org.apache.spark.api.plugin.SparkPlugin to load into the application.\")\n      .version(\"3.0.0\")\n      .stringConf\n      .toSequence"
  },
  {
    "id" : "de36b013-f481-49d7-8f61-9222777ef9ac",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375697576",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "afaf92d1-925b-461f-a96e-d083a8787ba7",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-8414, commit ID: 72da2a21f0940b97757ace5975535e559d627688#diff-75141521b1d55bc32d72b70032ad96c0",
        "createdAt" : "2020-03-17T02:23:15Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 81,
    "diffHunk" : "@@ -1,1 +1466,1470 @@  private[spark] val CLEANER_PERIODIC_GC_INTERVAL =\n    ConfigBuilder(\"spark.cleaner.periodicGC.interval\")\n      .version(\"1.6.0\")\n      .timeConf(TimeUnit.SECONDS)\n      .createWithDefaultString(\"30min\")"
  },
  {
    "id" : "0c75aaf2-0d8a-4055-8afd-0e25c1363284",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375697706",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "907e1c4f-2fb0-49f1-84c6-155dd27c89f4",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-1103, commit ID: 11eabbe125b2ee572fad359c33c93f5e6fdf0b2d#diff-364713d7776956cb8b0a771e9b62f82d",
        "createdAt" : "2020-03-17T02:23:46Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 87,
    "diffHunk" : "@@ -1,1 +1472,1476 @@  private[spark] val CLEANER_REFERENCE_TRACKING =\n    ConfigBuilder(\"spark.cleaner.referenceTracking\")\n      .version(\"1.0.0\")\n      .booleanConf\n      .createWithDefault(true)"
  },
  {
    "id" : "af61bdf8-789a-4170-8899-fb0d3546607c",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375697739",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "11b06446-11b0-4dfe-86ac-bc668d62adb2",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-1103, commit ID: 11eabbe125b2ee572fad359c33c93f5e6fdf0b2d#diff-364713d7776956cb8b0a771e9b62f82d",
        "createdAt" : "2020-03-17T02:23:53Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 93,
    "diffHunk" : "@@ -1,1 +1478,1482 @@  private[spark] val CLEANER_REFERENCE_TRACKING_BLOCKING =\n    ConfigBuilder(\"spark.cleaner.referenceTracking.blocking\")\n      .version(\"1.0.0\")\n      .booleanConf\n      .createWithDefault(true)"
  },
  {
    "id" : "74597e72-6089-473e-a4de-6de683b621bf",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375697847",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d351e309-0320-4773-b8cd-914e880e830a",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-3139, commit ID: 5cf1e440137006eedd6846ac8fa57ccf9fd1958d#diff-75141521b1d55bc32d72b70032ad96c0",
        "createdAt" : "2020-03-17T02:24:18Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 99,
    "diffHunk" : "@@ -1,1 +1484,1488 @@  private[spark] val CLEANER_REFERENCE_TRACKING_BLOCKING_SHUFFLE =\n    ConfigBuilder(\"spark.cleaner.referenceTracking.blocking.shuffle\")\n      .version(\"1.1.1\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "13ce14fb-7b3f-4f4c-8131-8fa28f5e2122",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375697956",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "aad82810-71f9-493b-a8fc-290c19fb167d",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-2033, commit ID: 25998e4d73bcc95ac85d9af71adfdc726ec89568#diff-440e866c5df0b8386aff57f9f8bd8db1",
        "createdAt" : "2020-03-17T02:24:41Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 105,
    "diffHunk" : "@@ -1,1 +1490,1494 @@  private[spark] val CLEANER_REFERENCE_TRACKING_CLEAN_CHECKPOINTS =\n    ConfigBuilder(\"spark.cleaner.referenceTracking.cleanCheckpoints\")\n      .version(\"1.4.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "4e5ea975-5cb2-4d91-ab57-4c7e9d088343",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375698090",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d7141bbe-5b1d-43bf-9612-e458a283ab1e",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-1940, commit ID: 4823bf470ec1b47a6f404834d4453e61d3dcbec9#diff-2b4575e096e4db7165e087f9429f2a02",
        "createdAt" : "2020-03-17T02:25:06Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 112,
    "diffHunk" : "@@ -1,1 +1496,1500 @@  private[spark] val EXECUTOR_LOGS_ROLLING_STRATEGY =\n    ConfigBuilder(\"spark.executor.logs.rolling.strategy\")\n      .version(\"1.1.0\")\n      .stringConf\n      .createWithDefault(\"\")"
  },
  {
    "id" : "e73fe209-2eda-4ac0-9dfa-eb2393b4b0eb",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375698131",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "96f8d1a7-85ae-4076-bce8-1c8d305d21ce",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-1940, commit ID: 4823bf470ec1b47a6f404834d4453e61d3dcbec9#diff-2b4575e096e4db7165e087f9429f2a02",
        "createdAt" : "2020-03-17T02:25:13Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 119,
    "diffHunk" : "@@ -1,1 +1502,1506 @@  private[spark] val EXECUTOR_LOGS_ROLLING_TIME_INTERVAL =\n    ConfigBuilder(\"spark.executor.logs.rolling.time.interval\")\n      .version(\"1.1.0\")\n      .stringConf\n      .createWithDefault(\"daily\")"
  },
  {
    "id" : "31ba394e-6674-4325-aa8f-027731012062",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375698232",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e11b7716-7739-4f94-a66e-1d2bb34c1d9c",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-1940, commit ID: 4823bf470ec1b47a6f404834d4453e61d3dcbec9#diff-2b4575e096e4db7165e087f9429f2a02",
        "createdAt" : "2020-03-17T02:25:35Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 132,
    "diffHunk" : "@@ -1,1 +1514,1518 @@  private[spark] val EXECUTOR_LOGS_ROLLING_MAX_RETAINED_FILES =\n    ConfigBuilder(\"spark.executor.logs.rolling.maxRetainedFiles\")\n      .version(\"1.1.0\")\n      .intConf\n      .createWithDefault(-1)"
  },
  {
    "id" : "ea0b7878-f870-4603-800c-9c17520d86d8",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375698342",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "62cd9166-49c7-4ef1-b3b4-2c5785aecc7a",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-5932, commit ID: 2d222fb39dd978e5a33cde6ceb59307cbdf7b171#diff-529fc5c06b9731c1fbda6f3db60b16aa",
        "createdAt" : "2020-03-17T02:25:59Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 125,
    "diffHunk" : "@@ -1,1 +1508,1512 @@  private[spark] val EXECUTOR_LOGS_ROLLING_MAX_SIZE =\n    ConfigBuilder(\"spark.executor.logs.rolling.maxSize\")\n      .version(\"1.4.0\")\n      .stringConf\n      .createWithDefault((1024 * 1024).toString)"
  },
  {
    "id" : "78287d2f-bf9d-4369-9ced-db4905b03b44",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375698515",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2e98ba1d-f0c1-4915-8677-99c6633278fe",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-17711, commit ID: 26e978a93f029e1a1b5c7524d0b52c8141b70997#diff-2b4575e096e4db7165e087f9429f2a02",
        "createdAt" : "2020-03-17T02:26:31Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 138,
    "diffHunk" : "@@ -1,1 +1520,1524 @@  private[spark] val EXECUTOR_LOGS_ROLLING_ENABLE_COMPRESSION =\n    ConfigBuilder(\"spark.executor.logs.rolling.enableCompression\")\n      .version(\"2.0.2\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "9e69ab13-0149-4e32-934f-5c7f4c966e71",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375698648",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "52540b77-ed3b-4416-9880-82737d12a0ee",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-5388, commit ID: 6ec0cdc14390d4dc45acf31040f21e1efc476fc0#diff-29dffdccd5a7f4c8b496c293e87c8668",
        "createdAt" : "2020-03-17T02:26:58Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 143,
    "diffHunk" : "@@ -1,1 +1525,1529 @@\n  private[spark] val MASTER_REST_SERVER_ENABLED = ConfigBuilder(\"spark.master.rest.enabled\")\n    .version(\"1.3.0\")\n    .booleanConf\n    .createWithDefault(false)"
  },
  {
    "id" : "8aa88fd2-b71f-4738-bdec-010f97e96342",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375698681",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fd1d2726-8e25-4397-bbcd-f57d5fdf3a76",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-5388, commit ID: 6ec0cdc14390d4dc45acf31040f21e1efc476fc0#diff-29dffdccd5a7f4c8b496c293e87c8668",
        "createdAt" : "2020-03-17T02:27:06Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 148,
    "diffHunk" : "@@ -1,1 +1530,1534 @@\n  private[spark] val MASTER_REST_SERVER_PORT = ConfigBuilder(\"spark.master.rest.port\")\n    .version(\"1.3.0\")\n    .intConf\n    .createWithDefault(6066)"
  },
  {
    "id" : "d7994479-ad0e-4d88-a78f-6f6f56bc559f",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375698789",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bc6e27a0-2544-49af-ada1-0e9c7b362546",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-2857, commit ID: 12f99cf5f88faf94d9dbfe85cb72d0010a3a25ac#diff-366c88f47e9b5cfa4d4305febeb8b026",
        "createdAt" : "2020-03-17T02:27:29Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 153,
    "diffHunk" : "@@ -1,1 +1535,1539 @@\n  private[spark] val MASTER_UI_PORT = ConfigBuilder(\"spark.master.ui.port\")\n    .version(\"1.1.0\")\n    .intConf\n    .createWithDefault(8080)"
  },
  {
    "id" : "0d8b34e9-eb11-4b0a-85bf-c91d848fe68c",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375698938",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c24441d6-8130-492e-91be-b3c15dc81273",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-5932, commit ID: 2d222fb39dd978e5a33cde6ceb59307cbdf7b171#diff-529fc5c06b9731c1fbda6f3db60b16aa",
        "createdAt" : "2020-03-17T02:28:03Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 161,
    "diffHunk" : "@@ -1,1 +1544,1548 @@        \"Snappy compression codec is used. Lowering this block size \" +\n        \"will also lower shuffle memory usage when Snappy is used\")\n      .version(\"1.4.0\")\n      .bytesConf(ByteUnit.BYTE)\n      .createWithDefaultString(\"32k\")"
  },
  {
    "id" : "20c3fce1-bfad-4132-b62d-90c05b353daf",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375698980",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8e950054-f369-4b25-92d4-37f2f89f5380",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-5932, commit ID: 2d222fb39dd978e5a33cde6ceb59307cbdf7b171#diff-529fc5c06b9731c1fbda6f3db60b16aa",
        "createdAt" : "2020-03-17T02:28:13Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 169,
    "diffHunk" : "@@ -1,1 +1553,1557 @@        \"codec is used. Lowering this block size will also lower shuffle memory \" +\n        \"usage when LZ4 is used.\")\n      .version(\"1.4.0\")\n      .bytesConf(ByteUnit.BYTE)\n      .createWithDefaultString(\"32k\")"
  },
  {
    "id" : "9ae1ddfe-c0c0-4f7c-86cb-907a81c8b1ba",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375704027",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1e7ae406-ca00-41df-94fd-b57656adeea7",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "No JIRA ID, commit ID: 46eecd110a4017ea0c86cbb1010d0ccd6a5eb2ef#diff-df9e6118c481ceb27faa399114fac0a1",
        "createdAt" : "2020-03-17T02:46:07Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 177,
    "diffHunk" : "@@ -1,1 +1563,1567 @@        \"lz4, lzf, snappy, and zstd. You can also use fully qualified class names to specify \" +\n        \"the codec\")\n      .version(\"0.8.0\")\n      .stringConf\n      .createWithDefaultString(\"lz4\")"
  },
  {
    "id" : "5bab4389-5223-4dbd-bd03-a6f6ef73e46b",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375704203",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "878f51bd-c205-44f0-a7fc-8453570a7442",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-19112, commit ID: 444bce1c98c45147fe63e2132e9743a0c5e49598#diff-df9e6118c481ceb27faa399114fac0a1",
        "createdAt" : "2020-03-17T02:46:43Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 185,
    "diffHunk" : "@@ -1,1 +1573,1577 @@        \"memory usage when Zstd is used, but it might increase the compression \" +\n        \"cost because of excessive JNI call overhead\")\n      .version(\"2.3.0\")\n      .bytesConf(ByteUnit.BYTE)\n      .createWithDefaultString(\"32k\")"
  },
  {
    "id" : "ef0d2ed9-3eaf-41f8-98fa-b177e7008d50",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375704250",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bc649018-79c0-4565-938c-80c005b54082",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-19112, commit ID: 444bce1c98c45147fe63e2132e9743a0c5e49598#diff-df9e6118c481ceb27faa399114fac0a1",
        "createdAt" : "2020-03-17T02:46:53Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 193,
    "diffHunk" : "@@ -1,1 +1581,1585 @@      .doc(\"Compression level for Zstd compression codec. Increasing the compression \" +\n        \"level will result in better compression at the expense of more CPU and memory\")\n      .version(\"2.3.0\")\n      .intConf\n      .createWithDefault(1)"
  },
  {
    "id" : "04dc2fcc-542c-4f59-8630-750e879ba515",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375704384",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8ee64282-0ece-47d7-b299-2caf2a254766",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-28366, commit ID: 26d03b62e20d053943d03b5c5573dd349e49654c#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-17T02:47:25Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 201,
    "diffHunk" : "@@ -1,1 +1590,1594 @@      .doc(\"If the size in bytes of a file loaded by Spark exceeds this threshold, \" +\n        \"a warning is logged with the possible reasons.\")\n      .version(\"3.0.0\")\n      .bytesConf(ByteUnit.BYTE)\n      .createWithDefault(1024 * 1024 * 1024)"
  },
  {
    "id" : "fc2b0c5c-3355-4639-845c-f36047e89004",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375704527",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d438dd3a-07bf-4bdd-8d5d-131df46aea8c",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-28118, commit ID: 47f54b1ec717d0d744bf3ad46bb1ed3542b667c8#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-17T02:47:57Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 209,
    "diffHunk" : "@@ -1,1 +1599,1603 @@        \"lz4, lzf, snappy, and zstd. You can also use fully qualified class names to specify \" +\n        \"the codec. If this is not given, spark.io.compression.codec will be used.\")\n      .version(\"3.0.0\")\n      .fallbackConf(IO_COMPRESSION_CODEC)\n"
  },
  {
    "id" : "a3ec60dc-3b4f-4b05-a93c-90c712e546c6",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375704656",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0a584faf-ef3d-4384-b882-302966b57367",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "No JIRA ID, commit ID: 4b1646a25f7581cecae108553da13833e842e68a#diff-eaf125f56ce786d64dcef99cf446a751",
        "createdAt" : "2020-03-17T02:48:21Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 214,
    "diffHunk" : "@@ -1,1 +1604,1608 @@  private[spark] val BUFFER_SIZE =\n    ConfigBuilder(\"spark.buffer.size\")\n      .version(\"0.5.0\")\n      .intConf\n      .checkValue(_ >= 0, \"The buffer size must not be negative\")"
  },
  {
    "id" : "103eb4aa-98d6-4ec7-8794-f5e6b039de5c",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375704793",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "921961fb-b4dc-4791-968a-c19c82ef2816",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "No JIRA ID, commit ID: 46eecd110a4017ea0c86cbb1010d0ccd6a5eb2ef#diff-264da78fe625d594eae59d1adabc8ae9",
        "createdAt" : "2020-03-17T02:48:52Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 220,
    "diffHunk" : "@@ -1,1 +1610,1614 @@\n  private[spark] val LOCALITY_WAIT_PROCESS = ConfigBuilder(\"spark.locality.wait.process\")\n    .version(\"0.8.0\")\n    .fallbackConf(LOCALITY_WAIT)\n"
  },
  {
    "id" : "9b83c561-92a0-47c4-9c3d-0d44d7b46862",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375704924",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0fb3bd5c-e11a-47f5-811f-7f63d1e6ac21",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "No JIRA ID, commit ID: 46eecd110a4017ea0c86cbb1010d0ccd6a5eb2ef#diff-264da78fe625d594eae59d1adabc8ae9",
        "createdAt" : "2020-03-17T02:49:16Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 224,
    "diffHunk" : "@@ -1,1 +1614,1618 @@\n  private[spark] val LOCALITY_WAIT_NODE = ConfigBuilder(\"spark.locality.wait.node\")\n    .version(\"0.8.0\")\n    .fallbackConf(LOCALITY_WAIT)\n"
  },
  {
    "id" : "54f45a6a-0903-491f-b0d9-4fa9cc21c803",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375704959",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9e817a68-42c1-48b4-a072-403e3a94c4b0",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "No JIRA ID, commit ID: 46eecd110a4017ea0c86cbb1010d0ccd6a5eb2ef#diff-264da78fe625d594eae59d1adabc8ae9",
        "createdAt" : "2020-03-17T02:49:25Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 228,
    "diffHunk" : "@@ -1,1 +1618,1622 @@\n  private[spark] val LOCALITY_WAIT_RACK = ConfigBuilder(\"spark.locality.wait.rack\")\n    .version(\"0.8.0\")\n    .fallbackConf(LOCALITY_WAIT)\n"
  },
  {
    "id" : "a7dfe4ea-a542-4335-883e-124ccca8ebfb",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375705102",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a077d078-e59d-448a-8991-ab4411d13bb1",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-5932, commit ID: 2d222fb39dd978e5a33cde6ceb59307cbdf7b171#diff-529fc5c06b9731c1fbda6f3db60b16aa",
        "createdAt" : "2020-03-17T02:50:00Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 237,
    "diffHunk" : "@@ -1,1 +1626,1630 @@      \"buffer to receive it, this represents a fixed memory overhead per reduce task, \" +\n      \"so keep it small unless you have a large amount of memory\")\n    .version(\"1.4.0\")\n    .bytesConf(ByteUnit.MiB)\n    .createWithDefaultString(\"48m\")"
  },
  {
    "id" : "3f92962f-190c-41cc-a416-13676fa12c66",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375705196",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9241e54b-188d-43fd-8e6d-5207d17cc434",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-6166, commit ID: 894921d813a259f2f266fde7d86d2ecb5a0af24b#diff-eb30a71e0d04150b8e0b64929852e38b",
        "createdAt" : "2020-03-17T02:50:21Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 245,
    "diffHunk" : "@@ -1,1 +1636,1640 @@      \"causing the workers to fail under load. By allowing it to limit the number of \" +\n      \"fetch requests, this scenario can be mitigated\")\n    .version(\"2.0.0\")\n    .intConf\n    .createWithDefault(Int.MaxValue)"
  },
  {
    "id" : "2d1145b0-3539-41e0-9e81-d5e18d549c54",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375705285",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "35babc6b-0366-4f40-aa89-2273215e5498",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "No JIRA ID, commit ID: efc5423210d1aadeaea78273a4a8f10425753079#diff-76170a9c8f67b542bc58240a0a12fe08",
        "createdAt" : "2020-03-17T02:50:43Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 252,
    "diffHunk" : "@@ -1,1 +1643,1647 @@    .doc(\"Whether to compress broadcast variables before sending them. \" +\n      \"Generally a good idea. Compression will use spark.io.compression.codec\")\n    .version(\"0.6.0\")\n    .booleanConf.createWithDefault(true)\n"
  },
  {
    "id" : "389c374e-f52f-4178-8ed5-139491ac2af6",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375705385",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6afd22bf-4ff4-44bf-bc67-8f824741fd55",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "No JIRA ID, commit ID: b8ab7862b8bd168bca60bd930cd97c1099fbc8a8#diff-271d7958e14cdaa46cf3737cfcf51341",
        "createdAt" : "2020-03-17T02:51:04Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 260,
    "diffHunk" : "@@ -1,1 +1651,1655 @@      \"parallelism during broadcast (makes it slower); however, \" +\n      \"if it is too small, BlockManager might take a performance hit\")\n    .version(\"0.5.0\")\n    .bytesConf(ByteUnit.KiB)\n    .createWithDefaultString(\"4m\")"
  },
  {
    "id" : "9a5c1c80-7661-4655-80ce-75eb73984642",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375705541",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e90cd498-39ee-455c-8424-d8571328f536",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-18188, commit ID: 06a56df226aa0c03c21f23258630d8a96385c696#diff-4f43d14923008c6650a8eb7b40c07f74",
        "createdAt" : "2020-03-17T02:51:39Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 269,
    "diffHunk" : "@@ -1,1 +1661,1665 @@      \"more data. It's possible to disable it if the network has other \" +\n      \"mechanisms to guarantee data won't be corrupted during broadcast\")\n    .version(\"2.1.1\")\n    .booleanConf\n    .createWithDefault(true)"
  },
  {
    "id" : "945d4887-af51-4a38-a837-29295885730b",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375705669",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e8f6b54a-d608-4ee7-9cba-19f72c46f112",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-28355, commit ID: 79e204770300dab4a669b9f8e2421ef905236e7b#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-17T02:52:05Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 277,
    "diffHunk" : "@@ -1,1 +1669,1673 @@      .doc(\"The threshold at which user-defined functions (UDFs) and Python RDD commands \" +\n        \"are compressed by broadcast in bytes unless otherwise specified\")\n      .version(\"3.0.0\")\n      .bytesConf(ByteUnit.BYTE)\n      .checkValue(v => v >= 0, \"The threshold should be non-negative.\")"
  },
  {
    "id" : "3790f0b6-c732-4000-ab87-393eff735f09",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375705789",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8f17f6fe-eb9e-4e4b-9ae7-6cc22cdaf0f5",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "No JIRA ID, commit ID: efc5423210d1aadeaea78273a4a8f10425753079#diff-76170a9c8f67b542bc58240a0a12fe08",
        "createdAt" : "2020-03-17T02:52:33Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 286,
    "diffHunk" : "@@ -1,1 +1680,1684 @@      \"space at the cost of some extra CPU time. \" +\n      \"Compression will use spark.io.compression.codec\")\n    .version(\"0.6.0\")\n    .booleanConf\n    .createWithDefault(false)"
  },
  {
    "id" : "8c741fc1-c20c-46ae-9ec3-7063a15b1c03",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375705895",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7e1880fc-8edd-4fc6-ba51-8dfb3520e2e4",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-9926, commit ID: 80a4bfa4d1c86398b90b26c34d8dcbc2355f5a6a#diff-eaababfc87ea4949f97860e8b89b7586",
        "createdAt" : "2020-03-17T02:52:58Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 292,
    "diffHunk" : "@@ -1,1 +1686,1690 @@  private[spark] val RDD_PARALLEL_LISTING_THRESHOLD =\n    ConfigBuilder(\"spark.rdd.parallelListingThreshold\")\n      .version(\"2.0.0\")\n      .intConf\n      .createWithDefault(10)"
  },
  {
    "id" : "356ef1af-0ab4-49ca-80fe-6663f2a1c1b9",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375706017",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a29cbc9f-9b07-4ec0-9110-1fd80ef31552",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-16984, commit ID: 806d8a8e980d8ba2f4261bceb393c40bafaa2f73#diff-1d55e54678eff2076263f2fe36150c17",
        "createdAt" : "2020-03-17T02:53:23Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 298,
    "diffHunk" : "@@ -1,1 +1692,1696 @@  private[spark] val RDD_LIMIT_SCALE_UP_FACTOR =\n    ConfigBuilder(\"spark.rdd.limit.scaleUpFactor\")\n      .version(\"2.1.0\")\n      .intConf\n      .createWithDefault(4)"
  },
  {
    "id" : "4ac97a98-4b5f-49ff-88fc-e3564ab01789",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375706134",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9e954da0-56cc-4606-835b-7d4345834e20",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "No JIRA ID, commit ID: fd1d255821bde844af28e897fabd59a715659038#diff-b920b65c23bf3a1b3326325b0d6a81b2",
        "createdAt" : "2020-03-17T02:53:51Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 303,
    "diffHunk" : "@@ -1,1 +1697,1701 @@\n  private[spark] val SERIALIZER = ConfigBuilder(\"spark.serializer\")\n    .version(\"0.5.0\")\n    .stringConf\n    .createWithDefault(\"org.apache.spark.serializer.JavaSerializer\")"
  },
  {
    "id" : "6c980365-9db1-465f-9705-635e4127aa89",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375706290",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6436e655-d0d6-4de0-92f3-2a9aab07ab82",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-942, commit ID: 40566e10aae4b21ffc71ea72702b8df118ac5c8e#diff-6a59dfc43d1b31dc1c3072ceafa829f5",
        "createdAt" : "2020-03-17T02:54:23Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 309,
    "diffHunk" : "@@ -1,1 +1703,1707 @@  private[spark] val SERIALIZER_OBJECT_STREAM_RESET =\n    ConfigBuilder(\"spark.serializer.objectStreamReset\")\n      .version(\"1.0.0\")\n      .intConf\n      .createWithDefault(100)"
  },
  {
    "id" : "362b5287-5b91-445a-90e0-a603ac8fc841",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375706418",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "77b8ca5d-d2a9-4d1d-ae91-6f0dc4b16483",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-5307, commit ID: 636408311deeebd77fb83d2249e0afad1a1ba149#diff-6a59dfc43d1b31dc1c3072ceafa829f5",
        "createdAt" : "2020-03-17T02:54:49Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 314,
    "diffHunk" : "@@ -1,1 +1708,1712 @@\n  private[spark] val SERIALIZER_EXTRA_DEBUG_INFO = ConfigBuilder(\"spark.serializer.extraDebugInfo\")\n    .version(\"1.3.0\")\n    .booleanConf\n    .createWithDefault(true)"
  },
  {
    "id" : "6bd7e756-d65f-4deb-9141-c6551d29b1e6",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375706538",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "41f4576c-11ad-414c-a05e-e8488b62d48c",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "No JIRA ID, commit ID: f1d206c6b4c0a5b2517b05af05fdda6049e2f7c2#diff-364713d7776956cb8b0a771e9b62f82d",
        "createdAt" : "2020-03-17T02:55:18Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 319,
    "diffHunk" : "@@ -1,1 +1713,1717 @@\n  private[spark] val JARS = ConfigBuilder(\"spark.jars\")\n    .version(\"0.9.0\")\n    .stringConf\n    .toSequence"
  },
  {
    "id" : "1ae8c0a9-d6de-4b52-9795-e84d808a88fa",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375706672",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1bc6c932-8184-410c-80e0-1e86ec0a98cc",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "No JIRA ID, commit ID: 29ee101c73bf066bf7f4f8141c475b8d1bd3cf1c#diff-364713d7776956cb8b0a771e9b62f82d",
        "createdAt" : "2020-03-17T02:55:47Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 325,
    "diffHunk" : "@@ -1,1 +1719,1723 @@\n  private[spark] val FILES = ConfigBuilder(\"spark.files\")\n    .version(\"1.0.0\")\n    .stringConf\n    .toSequence"
  },
  {
    "id" : "420d64bd-d37c-4b0d-914c-0d5ecc8b9446",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375706773",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7a1d8c01-6dca-4aaf-b4dd-b70cb236a1e9",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-6797, commit ID: 7f487c8bde14dbdd244a3493ad11a129ef2bb327#diff-4d2ab44195558d5a9d5f15b8803ef39d",
        "createdAt" : "2020-03-17T02:56:10Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 331,
    "diffHunk" : "@@ -1,1 +1725,1729 @@\n  private[spark] val SUBMIT_DEPLOY_MODE = ConfigBuilder(\"spark.submit.deployMode\")\n    .version(\"1.5.0\")\n    .stringConf\n    .createWithDefault(\"client\")"
  },
  {
    "id" : "fae3bbcd-183a-4a80-bdbd-a86bbe962029",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375706882",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1a05c276-4c59-41f7-ac2e-fd2f3991281d",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-1549, commit ID: d7ddb26e1fa02e773999cc4a97c48d2cd1723956#diff-4d2ab44195558d5a9d5f15b8803ef39d",
        "createdAt" : "2020-03-17T02:56:33Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 336,
    "diffHunk" : "@@ -1,1 +1730,1734 @@\n  private[spark] val SUBMIT_PYTHON_FILES = ConfigBuilder(\"spark.submit.pyFiles\")\n    .version(\"1.0.1\")\n    .stringConf\n    .toSequence"
  },
  {
    "id" : "dc961e83-0eb2-4dde-a338-b99035a01bf3",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375706976",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0144a675-9b34-49c8-aeee-1d8b101204a3",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "No JIRA ID, commit ID: 976fe60f7609d7b905a34f18743efabd966407f0#diff-9bc0105ee454005379abed710cd20ced",
        "createdAt" : "2020-03-17T02:56:55Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 343,
    "diffHunk" : "@@ -1,1 +1737,1741 @@  private[spark] val SCHEDULER_ALLOCATION_FILE =\n    ConfigBuilder(\"spark.scheduler.allocation.file\")\n      .version(\"0.8.1\")\n      .stringConf\n      .createOptional"
  },
  {
    "id" : "83afeeaa-6ce9-47ca-bc26-8ded52f2944f",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375707082",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "57f37310-1045-4e7e-93b3-ec1d354b49cc",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-2635, commit ID: 3311da2f9efc5ff2c7d01273ac08f719b067d11d#diff-7d99a7c7a051e5e851aaaefb275a44a1",
        "createdAt" : "2020-03-17T02:57:20Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 349,
    "diffHunk" : "@@ -1,1 +1743,1747 @@  private[spark] val SCHEDULER_MIN_REGISTERED_RESOURCES_RATIO =\n    ConfigBuilder(\"spark.scheduler.minRegisteredResourcesRatio\")\n      .version(\"1.1.1\")\n      .doubleConf\n      .createOptional"
  },
  {
    "id" : "ab655174-a017-4e3d-993e-765b1c577333",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375707172",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "71f8a1a4-b19b-4f7e-9ac6-2bd2b774f42a",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-2635, commit ID: 3311da2f9efc5ff2c7d01273ac08f719b067d11d#diff-7d99a7c7a051e5e851aaaefb275a44a1",
        "createdAt" : "2020-03-17T02:57:39Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 355,
    "diffHunk" : "@@ -1,1 +1749,1753 @@  private[spark] val SCHEDULER_MAX_REGISTERED_RESOURCE_WAITING_TIME =\n    ConfigBuilder(\"spark.scheduler.maxRegisteredResourcesWaitingTime\")\n      .version(\"1.1.1\")\n      .timeConf(TimeUnit.MILLISECONDS)\n      .createWithDefaultString(\"30s\")"
  },
  {
    "id" : "e7b414e2-7d55-42e4-9ab3-ed644731df85",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375707298",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2d0256ea-a603-47e5-92db-a72dc4f4424b",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "No JIRA ID, commit ID: 98fb69822cf780160bca51abeaab7c82e49fab54#diff-cb7a25b3c9a7341c6d99bcb8e9780c92",
        "createdAt" : "2020-03-17T02:58:04Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 361,
    "diffHunk" : "@@ -1,1 +1755,1759 @@  private[spark] val SCHEDULER_MODE =\n    ConfigBuilder(\"spark.scheduler.mode\")\n      .version(\"0.8.0\")\n      .stringConf\n      .createWithDefault(SchedulingMode.FIFO.toString)"
  },
  {
    "id" : "7054272e-0f03-4ab6-9928-2a8d21a56d7a",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375707404",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "617a7897-fc1b-4348-abe6-9ec6a7d6ca47",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "No JIRA ID, commit ID: d0c9d41a061969d409715b86a91937d8de4c29f7#diff-7d99a7c7a051e5e851aaaefb275a44a1",
        "createdAt" : "2020-03-17T02:58:26Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 367,
    "diffHunk" : "@@ -1,1 +1761,1765 @@  private[spark] val SCHEDULER_REVIVE_INTERVAL =\n    ConfigBuilder(\"spark.scheduler.revive.interval\")\n      .version(\"0.8.1\")\n      .timeConf(TimeUnit.MILLISECONDS)\n      .createOptional"
  },
  {
    "id" : "ca147ed0-5512-470b-bd4d-6f8ae23393ec",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375707603",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "78b1b93f-9dae-406a-a99c-74332d78e07c",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "No JIRA ID, commit ID: e72afdb817bcc8388aeb8b8d31628fd5fd67acf1#diff-4e188f32951dc989d97fa7577858bc7c",
        "createdAt" : "2020-03-17T02:59:06Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 373,
    "diffHunk" : "@@ -1,1 +1767,1771 @@  private[spark] val SPECULATION_ENABLED =\n    ConfigBuilder(\"spark.speculation\")\n      .version(\"0.6.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "17ff290a-6893-45ce-8871-95073c1ea208",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375707647",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "000aa1f2-80f7-498c-a606-835d9c29907f",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "No JIRA ID, commit ID: e72afdb817bcc8388aeb8b8d31628fd5fd67acf1#diff-4e188f32951dc989d97fa7577858bc7c",
        "createdAt" : "2020-03-17T02:59:18Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 379,
    "diffHunk" : "@@ -1,1 +1773,1777 @@  private[spark] val SPECULATION_INTERVAL =\n    ConfigBuilder(\"spark.speculation.interval\")\n      .version(\"0.6.0\")\n      .timeConf(TimeUnit.MILLISECONDS)\n      .createWithDefault(100)"
  },
  {
    "id" : "5d22f030-ec18-4553-8fa2-a200107276fb",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375707766",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d6814db9-a3da-4f90-b626-0fe332c48574",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "No JIRA ID, commit ID: e72afdb817bcc8388aeb8b8d31628fd5fd67acf1#diff-fff59f72dfe6ca4ccb607ad12535da07",
        "createdAt" : "2020-03-17T02:59:45Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 385,
    "diffHunk" : "@@ -1,1 +1779,1783 @@  private[spark] val SPECULATION_MULTIPLIER =\n    ConfigBuilder(\"spark.speculation.multiplier\")\n      .version(\"0.6.0\")\n      .doubleConf\n      .createWithDefault(1.5)"
  },
  {
    "id" : "494b66ec-11d2-40f5-ae7a-1ef6ec6e036f",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375707818",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ee4ae6d0-40af-492f-815b-9f82598c7eda",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "No JIRA ID, commit ID: e72afdb817bcc8388aeb8b8d31628fd5fd67acf1#diff-fff59f72dfe6ca4ccb607ad12535da07",
        "createdAt" : "2020-03-17T02:59:58Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 391,
    "diffHunk" : "@@ -1,1 +1785,1789 @@  private[spark] val SPECULATION_QUANTILE =\n    ConfigBuilder(\"spark.speculation.quantile\")\n      .version(\"0.6.0\")\n      .doubleConf\n      .createWithDefault(0.75)"
  },
  {
    "id" : "a7a58b04-5c8d-4b74-99ad-658c27df96d5",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375707937",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3bbde446-d9f9-41b1-b18e-94a318a8b4a9",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-29976, commit ID: ad238a2238a9d0da89be4424574436cbfaee579d#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-17T03:00:23Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 399,
    "diffHunk" : "@@ -1,1 +1799,1803 @@        \"even though the threshold hasn't been reached. The number of slots is computed based \" +\n        \"on the conf values of spark.executor.cores and spark.task.cpus minimum 1.\")\n      .version(\"3.0.0\")\n      .timeConf(TimeUnit.MILLISECONDS)\n      .createOptional"
  },
  {
    "id" : "e64fcfcd-e517-4770-a7a9-3a9e42ebbc66",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375708145",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1f4f9b07-fb32-40d6-b447-9d3355f10e9a",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-13063, commit ID: bc36df127d3b9f56b4edaeb5eca7697d4aef761a#diff-14b8ed2ef4e3da985300b8d796a38fa9",
        "createdAt" : "2020-03-17T03:00:41Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 405,
    "diffHunk" : "@@ -1,1 +1805,1809 @@  private[spark] val STAGING_DIR = ConfigBuilder(\"spark.yarn.stagingDir\")\n    .doc(\"Staging directory used while submitting applications.\")\n    .version(\"2.0.0\")\n    .stringConf\n    .createOptional"
  },
  {
    "id" : "66699443-a718-4bcf-8ae1-9272675d411d",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375708626",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "41c76f30-d707-4c71-bd00-74441ad7bb9e",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-9411, commit ID: 1b0099fc62d02ff6216a76fbfe17a4ec5b2f3536#diff-1b22e54318c04824a6d53ed3f4d1bb35",
        "createdAt" : "2020-03-17T03:01:04Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 411,
    "diffHunk" : "@@ -1,1 +1811,1815 @@  private[spark] val BUFFER_PAGESIZE = ConfigBuilder(\"spark.buffer.pageSize\")\n    .doc(\"The amount of memory used per page in bytes\")\n    .version(\"1.5.0\")\n    .bytesConf(ByteUnit.BYTE)\n    .createOptional"
  },
  {
    "id" : "585dfc0c-db23-48ea-954f-0163f9c1a6d8",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374715708",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d8dab408-d683-4742-be31-22ffb323f443",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-5847, commit ID: 70f846a313061e4db6174e0dc6c12c8c806ccf78#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:26:15Z",
        "updatedAt" : "2020-03-14T11:26:15Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +777,781 @@  // This property sets the root namespace for metrics reporting\n  private[spark] val METRICS_NAMESPACE = ConfigBuilder(\"spark.metrics.namespace\")\n    .version(\"2.1.0\")\n    .stringConf\n    .createOptional"
  },
  {
    "id" : "37b4ff4a-a3bd-4ec7-a3fe-d2346ae30ff2",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374715724",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "884cb813-fa15-4205-84bd-862f50f5bc11",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "No JIRA ID, commit ID: 46eecd110a4017ea0c86cbb1010d0ccd6a5eb2ef#diff-7ea2624e832b166ca27cd4baca8691d9",
        "createdAt" : "2020-03-14T11:26:33Z",
        "updatedAt" : "2020-03-14T11:26:33Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +782,786 @@\n  private[spark] val METRICS_CONF = ConfigBuilder(\"spark.metrics.conf\")\n    .version(\"0.8.0\")\n    .stringConf\n    .createOptional"
  },
  {
    "id" : "146faa5f-953a-44c2-83c3-1b3d4e5e6a99",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374715743",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5f4cfded-0e7a-4be5-98de-ccfb94282c9d",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-27189, commit ID: 729f43f499f3dd2718c0b28d73f2ca29cc811eac#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:26:57Z",
        "updatedAt" : "2020-03-14T11:26:58Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +789,793 @@    ConfigBuilder(\"spark.metrics.executorMetricsSource.enabled\")\n      .doc(\"Whether to register the ExecutorMetrics source with the metrics system.\")\n      .version(\"3.0.0\")\n      .booleanConf\n      .createWithDefault(true)"
  },
  {
    "id" : "164220cb-e027-4b8c-81a3-849e42745c06",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374715779",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5dd47939-3ace-464c-859c-17c168727bf4",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-30060, commit ID: 60f20e5ea2000ab8f4a593b5e4217fd5637c5e22#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:27:34Z",
        "updatedAt" : "2020-03-14T11:27:34Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +796,800 @@    ConfigBuilder(\"spark.metrics.staticSources.enabled\")\n      .doc(\"Whether to register static sources with the metrics system.\")\n      .version(\"3.0.0\")\n      .booleanConf\n      .createWithDefault(true)"
  },
  {
    "id" : "0b3b93ed-9897-4002-b5ec-db184f8feb56",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374715801",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c31b403b-9eae-48f9-9a2c-ed5b2996370d",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-13081, commit ID: 7a9e25c38380e6c62080d62ad38a4830e44fe753#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:27:59Z",
        "updatedAt" : "2020-03-14T11:28:00Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +801,805 @@\n  private[spark] val PYSPARK_DRIVER_PYTHON = ConfigBuilder(\"spark.pyspark.driver.python\")\n    .version(\"2.1.0\")\n    .stringConf\n    .createOptional"
  },
  {
    "id" : "b128a816-cac6-4475-8d90-1a2b81035a35",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374715809",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0f912b5a-0c42-41c0-aa6f-13a753a7ee7c",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-13081, commit ID: 7a9e25c38380e6c62080d62ad38a4830e44fe753#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:28:07Z",
        "updatedAt" : "2020-03-14T11:28:07Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 33,
    "diffHunk" : "@@ -1,1 +806,810 @@\n  private[spark] val PYSPARK_PYTHON = ConfigBuilder(\"spark.pyspark.python\")\n    .version(\"2.1.0\")\n    .stringConf\n    .createOptional"
  },
  {
    "id" : "1cb79d38-68cc-4ff5-ad21-564d619f3c81",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374715823",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a3094837-7194-4853-856a-0a3a077a3e46",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-17243, commit ID: 021aa28f439443cda1bc7c5e3eee7c85b40c1a2d#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:28:27Z",
        "updatedAt" : "2020-03-14T11:28:27Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 41,
    "diffHunk" : "@@ -1,1 +813,817 @@  private[spark] val HISTORY_UI_MAX_APPS =\n    ConfigBuilder(\"spark.history.ui.maxApplications\")\n      .version(\"2.0.1\")\n      .intConf\n      .createWithDefault(Integer.MAX_VALUE)"
  },
  {
    "id" : "b12c833d-356a-4680-97af-a340d0e57550",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374715856",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "61eb2a66-d3f1-4d55-983b-ec82ab5bcb81",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-5682, commit ID: 4b4e329e49f8af28fa6301bd06c48d7097eaf9e6#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:29:07Z",
        "updatedAt" : "2020-03-14T11:29:07Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 46,
    "diffHunk" : "@@ -1,1 +818,822 @@\n  private[spark] val IO_ENCRYPTION_ENABLED = ConfigBuilder(\"spark.io.encryption.enabled\")\n    .version(\"2.1.0\")\n    .booleanConf\n    .createWithDefault(false)"
  },
  {
    "id" : "2908d230-d0e2-4798-9f36-a17c76c039e1",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374715864",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "82e49647-a5ed-4678-b1a6-44155ae304a1",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-5682, commit ID: 4b4e329e49f8af28fa6301bd06c48d7097eaf9e6#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:29:18Z",
        "updatedAt" : "2020-03-14T11:29:18Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 52,
    "diffHunk" : "@@ -1,1 +824,828 @@  private[spark] val IO_ENCRYPTION_KEYGEN_ALGORITHM =\n    ConfigBuilder(\"spark.io.encryption.keygen.algorithm\")\n      .version(\"2.1.0\")\n      .stringConf\n      .createWithDefault(\"HmacSHA1\")"
  },
  {
    "id" : "03932a8e-f0fa-48cc-9afd-e4407f1a673d",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374715870",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ccbb2bad-a29c-43d9-9193-c8bb3ee502c3",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-5682, commit ID: 4b4e329e49f8af28fa6301bd06c48d7097eaf9e6#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:29:25Z",
        "updatedAt" : "2020-03-14T11:29:25Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 57,
    "diffHunk" : "@@ -1,1 +829,833 @@\n  private[spark] val IO_ENCRYPTION_KEY_SIZE_BITS = ConfigBuilder(\"spark.io.encryption.keySizeBits\")\n    .version(\"2.1.0\")\n    .intConf\n    .checkValues(Set(128, 192, 256))"
  },
  {
    "id" : "f3efc81e-acfe-4ce5-bad6-9ac34109b4f0",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374715879",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e2669009-4c52-43c6-bd82-556ceca97731",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-5682, commit ID: 4b4e329e49f8af28fa6301bd06c48d7097eaf9e6#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:29:39Z",
        "updatedAt" : "2020-03-14T11:29:40Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 65,
    "diffHunk" : "@@ -1,1 +837,841 @@    ConfigBuilder(\"spark.io.crypto.cipher.transformation\")\n      .internal()\n      .version(\"2.1.0\")\n      .stringConf\n      .createWithDefaultString(\"AES/CTR/NoPadding\")"
  },
  {
    "id" : "8ee1fd9b-aadc-4606-967a-6c23ba43016e",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374715900",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6978a11c-09a0-49ef-a348-9f9fb0cff9cd",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "No JIRA ID, commit ID: 02a6761589c35f15f1a6e3b63a7964ba057d3ba6#diff-eaf125f56ce786d64dcef99cf446a751",
        "createdAt" : "2020-03-14T11:30:03Z",
        "updatedAt" : "2020-03-14T11:30:03Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 71,
    "diffHunk" : "@@ -1,1 +843,847 @@  private[spark] val DRIVER_HOST_ADDRESS = ConfigBuilder(\"spark.driver.host\")\n    .doc(\"Address of driver endpoints.\")\n    .version(\"0.7.0\")\n    .stringConf\n    .createWithDefault(Utils.localCanonicalHostName())"
  },
  {
    "id" : "f0c0b196-15c6-4d48-ab36-f3874d6cd779",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374715906",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2669f4bb-0fe9-4f66-ae5f-213df0499a19",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "No JIRA ID, commit ID: 02a6761589c35f15f1a6e3b63a7964ba057d3ba6#diff-eaf125f56ce786d64dcef99cf446a751",
        "createdAt" : "2020-03-14T11:30:10Z",
        "updatedAt" : "2020-03-14T11:30:10Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 77,
    "diffHunk" : "@@ -1,1 +849,853 @@  private[spark] val DRIVER_PORT = ConfigBuilder(\"spark.driver.port\")\n    .doc(\"Port of driver endpoints.\")\n    .version(\"0.7.0\")\n    .intConf\n    .createWithDefault(0)"
  },
  {
    "id" : "92694d4c-f434-4b3d-8ae2-da27af4a6bfb",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374715930",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6c2891ad-f9e3-48b4-8179-2515a9aa3d57",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-5388, commit ID: 6ec0cdc14390d4dc45acf31040f21e1efc476fc0#diff-4d2ab44195558d5a9d5f15b8803ef39d",
        "createdAt" : "2020-03-14T11:30:33Z",
        "updatedAt" : "2020-03-14T11:30:33Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 84,
    "diffHunk" : "@@ -1,1 +856,860 @@    .doc(\"If true, restarts the driver automatically if it fails with a non-zero exit status. \" +\n      \"Only has effect in Spark standalone mode or Mesos cluster deploy mode.\")\n    .version(\"1.3.0\")\n    .booleanConf\n    .createWithDefault(false)"
  },
  {
    "id" : "9fe399b5-d3f3-4ddb-a921-b9fc8f012519",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374715945",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9c7fe142-405b-4884-a66a-6bb4af905fa9",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-4563, commit ID: 2cd1bfa4f0c6625b0ab1dbeba2b9586b9a6a9f42#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:30:54Z",
        "updatedAt" : "2020-03-14T11:30:55Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 90,
    "diffHunk" : "@@ -1,1 +862,866 @@  private[spark] val DRIVER_BIND_ADDRESS = ConfigBuilder(\"spark.driver.bindAddress\")\n    .doc(\"Address where to bind network listen sockets on the driver.\")\n    .version(\"2.1.0\")\n    .fallbackConf(DRIVER_HOST_ADDRESS)\n"
  },
  {
    "id" : "745808dc-9dbe-4c74-997b-ddcbbd1beda0",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374715959",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7d97b642-4880-467f-b5f6-481e19c648c7",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-4563, commit ID: 2cd1bfa4f0c6625b0ab1dbeba2b9586b9a6a9f42#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:31:11Z",
        "updatedAt" : "2020-03-14T11:31:12Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 101,
    "diffHunk" : "@@ -1,1 +873,877 @@  private[spark] val DRIVER_BLOCK_MANAGER_PORT = ConfigBuilder(\"spark.driver.blockManager.port\")\n    .doc(\"Port to use for the block manager on the driver.\")\n    .version(\"2.1.0\")\n    .fallbackConf(BLOCK_MANAGER_PORT)\n"
  },
  {
    "id" : "2bea776d-4979-436a-864f-f1cc05ca91d1",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374715965",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "46bf56ab-8f0d-4b60-9e33-59e7a39c8c36",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-2157, commit ID: 31090e43ca91f687b0bc6e25c824dc25bd7027cd#diff-2b643ea78c1add0381754b1f47eec132",
        "createdAt" : "2020-03-14T11:31:29Z",
        "updatedAt" : "2020-03-14T11:31:29Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 95,
    "diffHunk" : "@@ -1,1 +867,871 @@  private[spark] val BLOCK_MANAGER_PORT = ConfigBuilder(\"spark.blockManager.port\")\n    .doc(\"Port to use for the block manager when a more specific setting is not provided.\")\n    .version(\"1.1.0\")\n    .intConf\n    .createWithDefault(0)"
  },
  {
    "id" : "6ce3ab93-0d68-4c9f-8d5d-61b8c9f9b3fa",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374715985",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f298cd61-7c08-4b37-8994-d798e5e34d2e",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-17850, commit ID: 47776e7c0c68590fe446cef910900b1aaead06f9#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:31:54Z",
        "updatedAt" : "2020-03-14T11:31:54Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 108,
    "diffHunk" : "@@ -1,1 +880,884 @@      \"encountering corrupted or non-existing files and contents that have been read will still \" +\n      \"be returned.\")\n    .version(\"2.1.0\")\n    .booleanConf\n    .createWithDefault(false)"
  },
  {
    "id" : "46f7f5b7-97a9-4741-bf4a-d4e25c34df92",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716003",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3aea0e36-7df1-4d5c-a3a7-994b3117546e",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-22676, commit ID: ed4101d29f50d54fd7846421e4c00e9ecd3599d0#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:32:15Z",
        "updatedAt" : "2020-03-14T11:32:16Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 115,
    "diffHunk" : "@@ -1,1 +887,891 @@    .doc(\"Whether to ignore missing files. If true, the Spark jobs will continue to run when \" +\n      \"encountering missing files and the contents that have been read will still be returned.\")\n    .version(\"2.4.0\")\n    .booleanConf\n    .createWithDefault(false)"
  },
  {
    "id" : "5e490c3b-147f-4b4e-9559-819314025623",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716027",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "adc80a40-bf3a-41fd-9fc2-7617ee2a33c3",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-16759, commit ID: 3af894511be6fcc17731e28b284dba432fe911f5#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:32:40Z",
        "updatedAt" : "2020-03-14T11:32:40Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 120,
    "diffHunk" : "@@ -1,1 +892,896 @@\n  private[spark] val APP_CALLER_CONTEXT = ConfigBuilder(\"spark.log.callerContext\")\n    .version(\"2.2.0\")\n    .stringConf\n    .createOptional"
  },
  {
    "id" : "40c5850b-95f1-4a04-95bf-3188898b8974",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716056",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3ffa1290-c9b0-4869-9b0c-e8bf38dd6549",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-16575, commit ID: c8879bf1ee2af9ccd5d5656571d931d2fc1da024#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:33:17Z",
        "updatedAt" : "2020-03-14T11:33:17Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 126,
    "diffHunk" : "@@ -1,1 +898,902 @@  private[spark] val FILES_MAX_PARTITION_BYTES = ConfigBuilder(\"spark.files.maxPartitionBytes\")\n    .doc(\"The maximum number of bytes to pack into a single partition when reading files.\")\n    .version(\"2.1.0\")\n    .bytesConf(ByteUnit.BYTE)\n    .createWithDefault(128 * 1024 * 1024)"
  },
  {
    "id" : "5d128898-b6fd-4ea2-91e9-f8935a2bf49f",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716061",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4dab63b7-f2e4-4c05-a78f-a0a66d4a2f8b",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-16575, commit ID: c8879bf1ee2af9ccd5d5656571d931d2fc1da024#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:33:26Z",
        "updatedAt" : "2020-03-14T11:33:26Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 134,
    "diffHunk" : "@@ -1,1 +907,911 @@      \" over estimate, then the partitions with small files will be faster than partitions with\" +\n      \" bigger files.\")\n    .version(\"2.1.0\")\n    .bytesConf(ByteUnit.BYTE)\n    .createWithDefault(4 * 1024 * 1024)"
  },
  {
    "id" : "a7acfdb5-a035-48fb-bf54-b35484efb64d",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716083",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ce333127-ce71-49ed-b1ad-b976ed8897ea",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-22233, commit ID: 0fa10666cf75e3c4929940af49c8a6f6ea874759#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:33:55Z",
        "updatedAt" : "2020-03-14T11:33:55Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 142,
    "diffHunk" : "@@ -1,1 +915,919 @@      .internal()\n      .doc(\"When true, HadoopRDD/NewHadoopRDD will not create partitions for empty input splits.\")\n      .version(\"2.3.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "3ed37f64-59e3-480c-a691-42c6a3486312",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716102",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "feb3e93a-9bbd-4c64-b5ed-a7a425fc7486",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-18535 and SPARK-19720, commit ID: 444cca14d7ac8c5ab5d7e9d080b11f4d6babe3bf#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:34:24Z",
        "updatedAt" : "2020-03-14T11:34:25Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 150,
    "diffHunk" : "@@ -1,1 +925,929 @@        \"a property key or value, the value is redacted from the environment UI and various logs \" +\n        \"like YARN and event logs.\")\n      .version(\"2.1.2\")\n      .regexConf\n      .createWithDefault(\"(?i)secret|password|token\".r)"
  },
  {
    "id" : "5e3e6a7c-dd3e-4d25-9845-332d6e4f08fe",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716117",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1acda4ee-0b64-478a-9386-f35dfb7e7f9e",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-20070, commit ID: 91fa80fe8a2480d64c430bd10f97b3d44c007bcc#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:34:55Z",
        "updatedAt" : "2020-03-14T11:34:55Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 158,
    "diffHunk" : "@@ -1,1 +934,938 @@        \"information. When this regex matches a string part, that string part is replaced by a \" +\n        \"dummy value. This is currently used to redact the output of SQL explain commands.\")\n      .version(\"2.2.0\")\n      .regexConf\n      .createOptional"
  },
  {
    "id" : "9172f60a-681c-478c-be1f-07f3ca77fb14",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716130",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "21682125-3a2e-4031-87ae-3f391b346c9f",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-1189, commit ID: 7edbea41b43e0dc11a2de156be220db8b7952d01#diff-afd88f677ec5ff8b5e96a5cbbe00cd98",
        "createdAt" : "2020-03-14T11:35:18Z",
        "updatedAt" : "2020-03-14T11:35:18Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 164,
    "diffHunk" : "@@ -1,1 +940,944 @@  private[spark] val AUTH_SECRET =\n    ConfigBuilder(\"spark.authenticate.secret\")\n      .version(\"1.0.0\")\n      .stringConf\n      .createOptional"
  },
  {
    "id" : "68d8659a-51cc-4e28-82be-0ec3dba98acc",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716134",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a4dd42b8-2bd0-4882-bccf-82de209e14a5",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-1189, commit ID: 7edbea41b43e0dc11a2de156be220db8b7952d01#diff-afd88f677ec5ff8b5e96a5cbbe00cd98",
        "createdAt" : "2020-03-14T11:35:26Z",
        "updatedAt" : "2020-03-14T11:35:26Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 176,
    "diffHunk" : "@@ -1,1 +952,956 @@  private[spark] val NETWORK_AUTH_ENABLED =\n    ConfigBuilder(\"spark.authenticate\")\n      .version(\"1.0.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "22f7d562-eddc-4eec-8bc9-75f38c3b2168",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716151",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2b27b7ef-5566-48f4-b4e6-1b06fc7baa42",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-11073, commit ID: f8d93edec82eedab59d50aec06ca2de7e4cf14f6#diff-afd88f677ec5ff8b5e96a5cbbe00cd98",
        "createdAt" : "2020-03-14T11:35:46Z",
        "updatedAt" : "2020-03-14T11:35:47Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 170,
    "diffHunk" : "@@ -1,1 +946,950 @@  private[spark] val AUTH_SECRET_BIT_LENGTH =\n    ConfigBuilder(\"spark.authenticate.secretBitLength\")\n      .version(\"1.6.0\")\n      .intConf\n      .createWithDefault(256)"
  },
  {
    "id" : "0a42a670-33ea-4d31-b13f-e89594db7951",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716174",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "32da1c5e-af1b-45ee-86c8-0e9d8128ce01",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-6229, commit ID: 38d4e9e446b425ca6a8fe8d8080f387b08683842#diff-afd88f677ec5ff8b5e96a5cbbe00cd98",
        "createdAt" : "2020-03-14T11:36:10Z",
        "updatedAt" : "2020-03-14T11:36:11Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 182,
    "diffHunk" : "@@ -1,1 +958,962 @@  private[spark] val SASL_ENCRYPTION_ENABLED =\n    ConfigBuilder(\"spark.authenticate.enableSaslEncryption\")\n      .version(\"1.4.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "a591ac68-8d65-4801-8f38-637a76402439",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716216",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "393cf433-2e6f-4e2f-b75f-e7eb0ef92d14",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-26239, commit ID: 57d6fbfa8c803ce1791e7be36aba0219a1fcaa63#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:36:55Z",
        "updatedAt" : "2020-03-14T11:36:55Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 190,
    "diffHunk" : "@@ -1,1 +968,972 @@        \"either entity (see below). File-based secret keys are only allowed when using \" +\n        \"Kubernetes.\")\n      .version(\"3.0.0\")\n      .stringConf\n      .createOptional"
  },
  {
    "id" : "9976f185-bc57-4190-9a33-0c1c29770e16",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716223",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c3b96c3f-138c-4e2a-9187-0147a1f7e9c1",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-26239, commit ID: 57d6fbfa8c803ce1791e7be36aba0219a1fcaa63#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:37:08Z",
        "updatedAt" : "2020-03-14T11:37:08Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 198,
    "diffHunk" : "@@ -1,1 +981,985 @@        \"used for both the driver and the executors when running in cluster mode. File-based \" +\n        \"secret keys are only allowed when using Kubernetes.\")\n      .version(\"3.0.0\")\n      .fallbackConf(AUTH_SECRET_FILE)\n"
  },
  {
    "id" : "83a71322-ccb7-43a1-a0c3-0b52db36e5e5",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716234",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6e37e5eb-8b97-43a8-b7ac-bce5cb8118f1",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-26239, commit ID: 57d6fbfa8c803ce1791e7be36aba0219a1fcaa63#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:37:19Z",
        "updatedAt" : "2020-03-14T11:37:19Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 206,
    "diffHunk" : "@@ -1,1 +993,997 @@        \"used for both the driver and the executors when running in cluster mode. File-based \" +\n        \"secret keys are only allowed when using Kubernetes.\")\n      .version(\"3.0.0\")\n      .fallbackConf(AUTH_SECRET_FILE)\n"
  },
  {
    "id" : "f4819b1d-0dcb-4f61-8869-3c9966361c9c",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716257",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fa3dcff5-3400-44f1-8331-bf7ff5e35f8c",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-21527, commit ID: 574ef6c987c636210828e96d2f797d8f10aff05e#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:37:48Z",
        "updatedAt" : "2020-03-14T11:37:48Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 213,
    "diffHunk" : "@@ -1,1 +1000,1004 @@      .internal()\n      .doc(\"The chunk size in bytes during writing out the bytes of ChunkedByteBuffer.\")\n      .version(\"2.3.0\")\n      .bytesConf(ByteUnit.BYTE)\n      .checkValue(_ <= ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH,"
  },
  {
    "id" : "53c3689f-a9e5-45a2-8e99-5bf16493703e",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716274",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9747384a-a8b2-457d-8396-88d3d3c1a514",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-19525, commit ID: 1405862382185e04b09f84af18f82f2f0295a755#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:38:17Z",
        "updatedAt" : "2020-03-14T11:38:17Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 221,
    "diffHunk" : "@@ -1,1 +1011,1015 @@      .doc(\"Whether to compress RDD checkpoints. Generally a good idea. Compression will use \" +\n        \"spark.io.compression.codec.\")\n      .version(\"2.2.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "0e95178e-8b72-46ab-9ba8-f092ea1da042",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716303",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "993c9079-c732-4ba9-bb81-913934d84f24",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-29182, commit ID: 4ecbdbb6a7bd3908da32c82832e886b4f9f9e596#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:38:46Z",
        "updatedAt" : "2020-03-14T11:38:46Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 229,
    "diffHunk" : "@@ -1,1 +1022,1026 @@        \"time. The drawback is that the cached locations can be possibly outdated and \" +\n        \"lose data locality. If this config is not specified, it will not cache.\")\n      .version(\"3.0.0\")\n      .timeConf(TimeUnit.MINUTES)\n      .checkValue(_ > 0, \"The expire time for caching preferred locations cannot be non-positive.\")"
  },
  {
    "id" : "af4f30c2-5cd7-4ff9-a3d0-10d4ddb3226a",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716323",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ca7887ab-04bb-427f-b198-5f032c4d78a9",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-20801, commit ID: 81f63c8923416014d5c6bc227dd3c4e2a62bac8e#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:39:13Z",
        "updatedAt" : "2020-03-14T11:39:14Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 237,
    "diffHunk" : "@@ -1,1 +1032,1036 @@        \"HighlyCompressedMapStatus is accurately recorded. This helps to prevent OOM \" +\n        \"by avoiding underestimating shuffle block size when fetch shuffle blocks.\")\n      .version(\"2.2.1\")\n      .bytesConf(ByteUnit.BYTE)\n      .createWithDefault(100 * 1024 * 1024)"
  },
  {
    "id" : "029713d3-b989-4114-9ec2-57c04d5eb30a",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716347",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8b793871-9039-45b9-a389-273be5ca4cf6",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-20640, commit ID: d107b3b910d8f434fb15b663a9db4c2dfe0a9f43#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:39:52Z",
        "updatedAt" : "2020-03-14T11:39:52Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 244,
    "diffHunk" : "@@ -1,1 +1039,1043 @@    ConfigBuilder(\"spark.shuffle.registration.timeout\")\n      .doc(\"Timeout in milliseconds for registration to the external shuffle service.\")\n      .version(\"2.3.0\")\n      .timeConf(TimeUnit.MILLISECONDS)\n      .createWithDefault(5000)"
  },
  {
    "id" : "86304ba6-4f0b-40c3-8df4-8dd1333267f2",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716363",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "002de867-39d1-48f3-a431-a85b598b7f30",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-20640, commit ID: d107b3b910d8f434fb15b663a9db4c2dfe0a9f43#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:40:10Z",
        "updatedAt" : "2020-03-14T11:40:10Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 252,
    "diffHunk" : "@@ -1,1 +1047,1051 @@      .doc(\"When we fail to register to the external shuffle service, we will \" +\n        \"retry for maxAttempts times.\")\n      .version(\"2.3.0\")\n      .intConf\n      .createWithDefault(3)"
  },
  {
    "id" : "95b71307-4eb3-4b0b-a40c-d39236ae7023",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716388",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2ae841e1-3766-41cc-91fe-a3496a088cfe",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-21243, commit ID: 88dccda393bc79dc6032f71b6acf8eb2b4b152be#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:40:38Z",
        "updatedAt" : "2020-03-14T11:40:38Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 260,
    "diffHunk" : "@@ -1,1 +1058,1062 @@        \"Node Manager. This is especially useful to reduce the load on the Node Manager when \" +\n        \"external shuffle is enabled. You can mitigate the issue by setting it to a lower value.\")\n      .version(\"2.2.1\")\n      .intConf\n      .checkValue(_ > 0, \"The max no. of blocks in flight cannot be non-positive.\")"
  },
  {
    "id" : "8c9d157a-5086-4135-b55f-c2e402c8582b",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716409",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fa88b17a-4516-431c-b0ad-60800a6ab93c",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-26700, commit ID: d8613571bc1847775dd5c1945757279234cb388c#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:41:05Z",
        "updatedAt" : "2020-03-14T11:41:05Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 268,
    "diffHunk" : "@@ -1,1 +1070,1074 @@        \"For users who enabled external shuffle service, this feature can only work when \" +\n        \"external shuffle service is at least 2.3.0.\")\n      .version(\"3.0.0\")\n      .bytesConf(ByteUnit.BYTE)\n      // fetch-to-mem is guaranteed to fail if the message is bigger than 2 GB, so we might"
  },
  {
    "id" : "82c5babd-7fef-4568-8c92-ed014cf756a0",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716433",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "55d25c43-b3ac-4d70-b80b-5aa6df197a14",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-20923, commit ID: 5b5a69bea9de806e2c39b04b248ee82a7b664d7b#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:41:29Z",
        "updatedAt" : "2020-03-14T11:41:29Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 276,
    "diffHunk" : "@@ -1,1 +1086,1090 @@        \"tracking the block statuses can use a lot of memory and its not used anywhere within \" +\n        \"spark.\")\n      .version(\"2.3.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "c319b2b6-6882-4c78-9494-98e333f70b10",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716451",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2680e8bd-6dc0-4394-ad8d-bcc318721597",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-28209, commit ID: abef84a868e9e15f346eea315bbab0ec8ac8e389#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:41:49Z",
        "updatedAt" : "2020-03-14T11:41:50Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 283,
    "diffHunk" : "@@ -1,1 +1093,1097 @@    ConfigBuilder(\"spark.shuffle.sort.io.plugin.class\")\n      .doc(\"Name of the class to use for shuffle IO.\")\n      .version(\"3.0.0\")\n      .stringConf\n      .createWithDefault(classOf[LocalDiskShuffleDataIO].getName)"
  },
  {
    "id" : "743d0691-73a8-4187-aba4-2f46fd5bf1c0",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716468",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8fdff1a0-833c-49a6-9a21-b03fca27cd59",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-7081, commit ID: c53ebea9db418099df50f9adc1a18cee7849cd97#diff-ecdafc46b901740134261d2cab24ccd9",
        "createdAt" : "2020-03-14T11:42:17Z",
        "updatedAt" : "2020-03-14T11:42:17Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 291,
    "diffHunk" : "@@ -1,1 +1102,1106 @@        \"otherwise specified. These buffers reduce the number of disk seeks and system calls \" +\n        \"made in creating intermediate shuffle files.\")\n      .version(\"1.4.0\")\n      .bytesConf(ByteUnit.KiB)\n      .checkValue(v => v > 0 && v <= ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH / 1024,"
  },
  {
    "id" : "cad009c8-867f-4cc9-8707-8dcca57d72fd",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716491",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "19cce0fc-14c5-4340-bb6e-1d2d1cf93ce8",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-20950, commit ID: 565e7a8d4ae7879ee704fb94ae9b3da31e202d7e#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:42:46Z",
        "updatedAt" : "2020-03-14T11:42:47Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 299,
    "diffHunk" : "@@ -1,1 +1113,1117 @@      .doc(\"The file system for this buffer size after each partition \" +\n        \"is written in unsafe shuffle writer. In KiB unless otherwise specified.\")\n      .version(\"2.3.0\")\n      .bytesConf(ByteUnit.KiB)\n      .checkValue(v => v > 0 && v <= ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH / 1024,"
  },
  {
    "id" : "49b679e3-5d21-4563-8320-707575354e3f",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716498",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "450b9bf1-a701-4580-9bf8-48c3f3adcc84",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-20950, commit ID: 565e7a8d4ae7879ee704fb94ae9b3da31e202d7e#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:42:55Z",
        "updatedAt" : "2020-03-14T11:42:55Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 307,
    "diffHunk" : "@@ -1,1 +1123,1127 @@    ConfigBuilder(\"spark.shuffle.spill.diskWriteBufferSize\")\n      .doc(\"The buffer size, in bytes, to use when writing the sorted records to an on-disk file.\")\n      .version(\"2.3.0\")\n      .bytesConf(ByteUnit.BYTE)\n      .checkValue(v => v > 12 && v <= ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH,"
  },
  {
    "id" : "8a4e081f-b750-46ba-8d19-d84125a1de2d",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716534",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4c438b78-b5ea-4de1-908b-346d796f8ab6",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-21923, commit ID: a11db942aaf4c470a85f8a1b180f034f7a584254#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:43:38Z",
        "updatedAt" : "2020-03-14T11:43:38Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 315,
    "diffHunk" : "@@ -1,1 +1135,1139 @@      .doc(\"The memory check period is used to determine how often we should check whether \"\n        + \"there is a need to request more memory when we try to unroll the given block in memory.\")\n      .version(\"2.3.0\")\n      .longConf\n      .createWithDefault(16)"
  },
  {
    "id" : "f0ceb6ab-82c3-4206-a07a-2bef11e5dcce",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716537",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8a69d21f-565e-45fc-8805-81ca73e74509",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-21923, commit ID: a11db942aaf4c470a85f8a1b180f034f7a584254#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:43:45Z",
        "updatedAt" : "2020-03-14T11:43:45Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 323,
    "diffHunk" : "@@ -1,1 +1143,1147 @@      .internal()\n      .doc(\"Memory to request as a multiple of the size that used to unroll the block.\")\n      .version(\"2.3.0\")\n      .doubleConf\n      .createWithDefault(1.5)"
  },
  {
    "id" : "2be0a405-4517-44e7-bc89-c6bb16997040",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716562",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2d46dca5-1bb0-44bd-86e1-13518633879b",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-21917, commit ID: 8319432af60b8e1dc00f08d794f7d80591e24d0c#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:44:10Z",
        "updatedAt" : "2020-03-14T11:44:11Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 331,
    "diffHunk" : "@@ -1,1 +1154,1158 @@        \"https and ftp, or jars required to be in the local YARN client's classpath. Wildcard \" +\n        \"'*' is denoted to download resources for all the schemes.\")\n      .version(\"2.3.0\")\n      .stringConf\n      .toSequence"
  },
  {
    "id" : "5f67b2d0-6ef5-4990-a9af-928be1167259",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716576",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3396c1bf-57f6-49ee-b8db-b0c1842e65b0",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-5411, commit ID: 47e4d579eb4a9aab8e0dd9c1400394d80c8d0388#diff-364713d7776956cb8b0a771e9b62f82d",
        "createdAt" : "2020-03-14T11:44:31Z",
        "updatedAt" : "2020-03-14T11:44:31Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 338,
    "diffHunk" : "@@ -1,1 +1161,1165 @@  private[spark] val EXTRA_LISTENERS = ConfigBuilder(\"spark.extraListeners\")\n    .doc(\"Class names of listeners to add to SparkContext during initialization.\")\n    .version(\"1.3.0\")\n    .stringConf\n    .toSequence"
  },
  {
    "id" : "21182768-c933-4de3-a19a-5397af301382",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716594",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c368da59-527b-44b9-aa1d-f04b858b6b63",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-10708, commit ID: f6d06adf05afa9c5386dc2396c94e7a98730289f#diff-3eedc75de4787b842477138d8cc7f150",
        "createdAt" : "2020-03-14T11:44:56Z",
        "updatedAt" : "2020-03-14T11:44:56Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 346,
    "diffHunk" : "@@ -1,1 +1173,1177 @@        \"until we reach some limitations, like the max page size limitation for the pointer \" +\n        \"array in the sorter.\")\n      .version(\"1.6.0\")\n      .intConf\n      .createWithDefault(Integer.MAX_VALUE)"
  },
  {
    "id" : "0df89255-d434-465d-9558-76196f1f80a3",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716625",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7a7f2d1f-08b3-47a8-81e6-7e88989086ea",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-22537, commit ID: efd0036ec88bdc385f5a9ea568d2e2bbfcda2912#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:45:28Z",
        "updatedAt" : "2020-03-14T11:45:29Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 354,
    "diffHunk" : "@@ -1,1 +1183,1187 @@        \"or equal to this threshold. Note that the actual parallelism is calculated by number of \" +\n        \"mappers * shuffle partitions / this threshold + 1, so this threshold should be positive.\")\n      .version(\"2.3.0\")\n      .intConf\n      .checkValue(v => v > 0, \"The threshold should be positive.\")"
  },
  {
    "id" : "6eb83ad9-a848-4d03-a238-bd1839a8da2f",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716649",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1ad96bc7-6685-494d-b925-3f3d9efb952f",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-3466, commit ID: 6181577e9935f46b646ba3925b873d031aa3d6ba#diff-d239aee594001f8391676e1047a0381e",
        "createdAt" : "2020-03-14T11:45:51Z",
        "updatedAt" : "2020-03-14T11:45:51Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 361,
    "diffHunk" : "@@ -1,1 +1190,1194 @@  private[spark] val MAX_RESULT_SIZE = ConfigBuilder(\"spark.driver.maxResultSize\")\n    .doc(\"Size limit for results.\")\n    .version(\"1.2.0\")\n    .bytesConf(ByteUnit.BYTE)\n    .createWithDefaultString(\"1g\")"
  },
  {
    "id" : "4e25f360-2945-44eb-a810-1a37c8964971",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716672",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a2fa81db-111c-4f9e-aaa8-d943c9383996",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-23361, commit ID: 5fa438471110afbf4e2174df449ac79e292501f8#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:46:16Z",
        "updatedAt" : "2020-03-14T11:46:16Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 368,
    "diffHunk" : "@@ -1,1 +1197,1201 @@    ConfigBuilder(\"spark.security.credentials.renewalRatio\")\n      .doc(\"Ratio of the credential's expiration time when Spark should fetch new credentials.\")\n      .version(\"2.4.0\")\n      .doubleConf\n      .createWithDefault(0.75d)"
  },
  {
    "id" : "61354868-1dba-4104-a86b-895683261a41",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716684",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0bd6af7a-b3a8-4105-acdb-0cf3e57991fc",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-23361, commit ID: 5fa438471110afbf4e2174df449ac79e292501f8#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:46:24Z",
        "updatedAt" : "2020-03-14T11:46:25Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 375,
    "diffHunk" : "@@ -1,1 +1204,1208 @@    ConfigBuilder(\"spark.security.credentials.retryWait\")\n      .doc(\"How long to wait before retrying to fetch new credentials after a failure.\")\n      .version(\"2.4.0\")\n      .timeConf(TimeUnit.SECONDS)\n      .createWithDefaultString(\"1h\")"
  },
  {
    "id" : "cbd95e7a-b61f-4c75-aeef-1756843889d3",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716707",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "11534a9c-4dca-4d31-b42d-bdf76781a476",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-15958, commit ID: bf665a958631125a1670504ef5966ef1a0e14798#diff-a1d00506391c1c4b2209f9bbff590c5b",
        "createdAt" : "2020-03-14T11:46:48Z",
        "updatedAt" : "2020-03-14T11:46:48Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 382,
    "diffHunk" : "@@ -1,1 +1211,1215 @@    ConfigBuilder(\"spark.shuffle.sort.initialBufferSize\")\n      .internal()\n      .version(\"2.1.0\")\n      .bytesConf(ByteUnit.BYTE)\n      .checkValue(v => v > 0 && v <= Int.MaxValue,"
  },
  {
    "id" : "b700fbe0-f8b7-4290-a14d-35d2b5642fdc",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716726",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3bce4f05-f4ad-4034-a775-ae22088d0906",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "No JIRA ID, commit ID: efc5423210d1aadeaea78273a4a8f10425753079#diff-76170a9c8f67b542bc58240a0a12fe08",
        "createdAt" : "2020-03-14T11:47:07Z",
        "updatedAt" : "2020-03-14T11:47:08Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 390,
    "diffHunk" : "@@ -1,1 +1221,1225 @@      .doc(\"Whether to compress shuffle output. Compression will use \" +\n        \"spark.io.compression.codec.\")\n      .version(\"0.6.0\")\n      .booleanConf\n      .createWithDefault(true)"
  },
  {
    "id" : "4bf580bd-dfe1-44a2-a5de-b4d9df48e3de",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716736",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d6de9488-b7fd-4c4c-8df4-1774b8546444",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "No JIRA ID, commit ID: c3816de5040e3c48e58ed4762d2f4eb606812938#diff-2b643ea78c1add0381754b1f47eec132",
        "createdAt" : "2020-03-14T11:47:26Z",
        "updatedAt" : "2020-03-14T11:47:27Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 398,
    "diffHunk" : "@@ -1,1 +1229,1233 @@      .doc(\"Whether to compress data spilled during shuffles. Compression will use \" +\n        \"spark.io.compression.codec.\")\n      .version(\"0.9.0\")\n      .booleanConf\n      .createWithDefault(true)"
  },
  {
    "id" : "527105b2-7b61-4300-bd44-e7ac370af0b4",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716763",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "53c65062-3f91-4788-ad65-2566c9dff0e1",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-29939, commit ID: 456cfe6e4693efd26d64f089d53c4e01bf8150a2#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:47:55Z",
        "updatedAt" : "2020-03-14T11:47:55Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 406,
    "diffHunk" : "@@ -1,1 +1239,1243 @@        \"By default, Spark provides four codecs: lz4, lzf, snappy, and zstd. You can also \" +\n        \"use fully qualified class names to specify the codec.\")\n      .version(\"3.0.0\")\n      .stringConf\n      .createWithDefault(\"zstd\")"
  },
  {
    "id" : "b3e71da3-0627-4a7e-b3ab-600e3eca3058",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716788",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b58ac048-5846-447a-95ee-38bd6d70df3b",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-4480, commit ID: 16bf5f3d17624db2a96c921fe8a1e153cdafb06c#diff-31417c461d8901d8e08167b0cbc344c1",
        "createdAt" : "2020-03-14T11:48:22Z",
        "updatedAt" : "2020-03-14T11:48:22Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 414,
    "diffHunk" : "@@ -1,1 +1248,1252 @@      .doc(\"Initial threshold for the size of a collection before we start tracking its \" +\n        \"memory usage.\")\n      .version(\"1.1.1\")\n      .bytesConf(ByteUnit.BYTE)\n      .createWithDefault(5 * 1024 * 1024)"
  },
  {
    "id" : "0e8ce018-452a-408b-abdd-39590e18e5bd",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716806",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "03135690-ca7e-4804-8a4c-c1752438718e",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "No JIRA ID, commit ID: c3816de5040e3c48e58ed4762d2f4eb606812938#diff-a470b9812a5ac8c37d732da7d9fbe39a",
        "createdAt" : "2020-03-14T11:48:46Z",
        "updatedAt" : "2020-03-14T11:48:46Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 422,
    "diffHunk" : "@@ -1,1 +1256,1260 @@      .internal()\n      .doc(\"Size of object batches when reading/writing from serializers.\")\n      .version(\"0.9.0\")\n      .longConf\n      .createWithDefault(10000)"
  },
  {
    "id" : "b485cd62-28f7-4ffb-bffa-da53d0d1d1de",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716828",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7dc98ccf-822d-4c58-a691-52b13ebc3521",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-2787, commit ID: 0f2274f8ed6131ad17326e3fff7f7e093863b72d#diff-31417c461d8901d8e08167b0cbc344c1",
        "createdAt" : "2020-03-14T11:49:12Z",
        "updatedAt" : "2020-03-14T11:49:12Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 430,
    "diffHunk" : "@@ -1,1 +1264,1268 @@      .doc(\"In the sort-based shuffle manager, avoid merge-sorting data if there is no \" +\n        \"map-side aggregation and there are at most this many reduce partitions\")\n      .version(\"1.1.1\")\n      .intConf\n      .createWithDefault(200)"
  },
  {
    "id" : "af7f065a-33d9-45de-9294-b583966d0338",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716853",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cca9bfcf-e12e-440c-8832-c58b8813f919",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-2044, commit ID: 508fd371d6dbb826fd8a00787d347235b549e189#diff-60df49b5d3c59f2c4540fa16a90033a1",
        "createdAt" : "2020-03-14T11:49:35Z",
        "updatedAt" : "2020-03-14T11:49:35Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 436,
    "diffHunk" : "@@ -1,1 +1270,1274 @@  private[spark] val SHUFFLE_MANAGER =\n    ConfigBuilder(\"spark.shuffle.manager\")\n      .version(\"1.1.0\")\n      .stringConf\n      .createWithDefault(\"sort\")"
  },
  {
    "id" : "af0e3e28-08ea-4847-a287-53be965d5514",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716873",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "93292844-c2c1-4658-bbe1-9c77e0d9d11d",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-2774, commit ID: 96a7c888d806adfdb2c722025a1079ed7eaa2052#diff-6a9ff7fb74fd490a50462d45db2d5e11",
        "createdAt" : "2020-03-14T11:49:56Z",
        "updatedAt" : "2020-03-14T11:49:56Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 443,
    "diffHunk" : "@@ -1,1 +1277,1281 @@    ConfigBuilder(\"spark.shuffle.reduceLocality.enabled\")\n      .doc(\"Whether to compute locality preferences for reduce tasks\")\n      .version(\"1.5.0\")\n      .booleanConf\n      .createWithDefault(true)"
  },
  {
    "id" : "ea9615ea-081a-4edc-a147-df276d34bcd8",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716904",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "994545fc-f355-4a07-b456-9eee4ad15805",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-1239, commit ID: d98dd72e7baeb59eacec4fefd66397513a607b2f#diff-609c3f8c26150ca96a94cd27146a809b",
        "createdAt" : "2020-03-14T11:50:27Z",
        "updatedAt" : "2020-03-14T11:50:27Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 450,
    "diffHunk" : "@@ -1,1 +1284,1288 @@    ConfigBuilder(\"spark.shuffle.mapOutput.minSizeForBroadcast\")\n      .doc(\"The size at which we use Broadcast to send the map output statuses to the executors.\")\n      .version(\"2.0.0\")\n      .bytesConf(ByteUnit.BYTE)\n      .createWithDefaultString(\"512k\")"
  },
  {
    "id" : "90dd58c1-58ce-4752-b7f9-5150d3e86337",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716913",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0f87ff7f-4436-45e3-bc2e-0fa5396f55ae",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-1239, commit ID: d98dd72e7baeb59eacec4fefd66397513a607b2f#diff-609c3f8c26150ca96a94cd27146a809b",
        "createdAt" : "2020-03-14T11:50:37Z",
        "updatedAt" : "2020-03-14T11:50:37Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 456,
    "diffHunk" : "@@ -1,1 +1290,1294 @@  private[spark] val SHUFFLE_MAPOUTPUT_DISPATCHER_NUM_THREADS =\n    ConfigBuilder(\"spark.shuffle.mapOutput.dispatcher.numThreads\")\n      .version(\"2.0.0\")\n      .intConf\n      .createWithDefault(8)"
  },
  {
    "id" : "b783c74a-5dc1-4863-b2b8-66db19d11c03",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716928",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d35fccc0-8c97-400e-afcc-6e503672a89e",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-4105, commit ID: cf33a86285629abe72c1acf235b8bfa6057220a8#diff-eb30a71e0d04150b8e0b64929852e38b",
        "createdAt" : "2020-03-14T11:50:58Z",
        "updatedAt" : "2020-03-14T11:50:58Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 463,
    "diffHunk" : "@@ -1,1 +1297,1301 @@    ConfigBuilder(\"spark.shuffle.detectCorrupt\")\n      .doc(\"Whether to detect any corruption in fetched blocks.\")\n      .version(\"2.2.0\")\n      .booleanConf\n      .createWithDefault(true)"
  },
  {
    "id" : "22b99ac5-52b2-4891-a712-892814f7efa9",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716944",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8e6f2e90-e41a-4607-9273-28b6188c9951",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-26089, commit ID: 688b0c01fac0db80f6473181673a89f1ce1be65b#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:51:20Z",
        "updatedAt" : "2020-03-14T11:51:20Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 471,
    "diffHunk" : "@@ -1,1 +1307,1311 @@        \"the task to be retried once and if it fails again with same exception, then \" +\n        \"FetchFailedException will be thrown to retry previous stage\")\n      .version(\"3.0.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "2f30c732-d91b-401c-8150-4a1e248133b3",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716963",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7b5cdf45-626a-440e-be99-29e551095310",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "No JIRA ID, commit ID: 31da065b1d08c1fad5283e4bcf8e0ed01818c03e#diff-ad46ed23fcc3fa87f30d05204917b917",
        "createdAt" : "2020-03-14T11:51:42Z",
        "updatedAt" : "2020-03-14T11:51:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 478,
    "diffHunk" : "@@ -1,1 +1314,1318 @@    ConfigBuilder(\"spark.shuffle.sync\")\n      .doc(\"Whether to force outstanding writes to disk.\")\n      .version(\"0.8.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "f5efe1d0-7089-4e06-afa9-2098e70f45cb",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716984",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9f65b6a7-43e5-484c-969e-cfea6cb9aa55",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-7081, commit ID: c53ebea9db418099df50f9adc1a18cee7849cd97#diff-642ce9f439435408382c3ac3b5c5e0a0",
        "createdAt" : "2020-03-14T11:52:01Z",
        "updatedAt" : "2020-03-14T11:52:01Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 485,
    "diffHunk" : "@@ -1,1 +1321,1325 @@    ConfigBuilder(\"spark.shuffle.unsafe.fastMergeEnabled\")\n      .doc(\"Whether to perform a fast spill merge.\")\n      .version(\"1.4.0\")\n      .booleanConf\n      .createWithDefault(true)"
  },
  {
    "id" : "674cc23d-79ed-4f5e-a85e-99d5f7bb7f14",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374717009",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2afd3fee-339c-4b8c-afe7-ada74d35860e",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-14724, commit ID: e2b5647ab92eb478b3f7b36a0ce6faf83e24c0e5#diff-3eedc75de4787b842477138d8cc7f150",
        "createdAt" : "2020-03-14T11:52:23Z",
        "updatedAt" : "2020-03-14T11:52:24Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 493,
    "diffHunk" : "@@ -1,1 +1329,1333 @@      .doc(\"Whether to use radix sort for sorting in-memory partition ids. Radix sort is much \" +\n        \"faster, but requires additional memory to be reserved memory as pointers are added.\")\n      .version(\"2.0.0\")\n      .booleanConf\n      .createWithDefault(true)"
  },
  {
    "id" : "43c15a85-0cad-4921-b6e5-135661389ec8",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374717034",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0d5ef0b6-4230-4ea9-beb7-329e6738bd30",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-24519, commit ID: 39dfaf2fd167cafc84ec9cc637c114ed54a331e3#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:52:49Z",
        "updatedAt" : "2020-03-14T11:52:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 501,
    "diffHunk" : "@@ -1,1 +1337,1341 @@      .internal()\n      .doc(\"Number of partitions to determine if MapStatus should use HighlyCompressedMapStatus\")\n      .version(\"2.4.0\")\n      .intConf\n      .checkValue(v => v > 0, \"The value should be a positive integer.\")"
  },
  {
    "id" : "1f9c7918-c7e9-4195-8918-93a484ad4926",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374717050",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2103c5f6-22a0-4795-8565-096e9a25f5ce",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-25341, commit ID: f725d472f51fb80c6ce1882ec283ff69bafb0de4#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:53:15Z",
        "updatedAt" : "2020-03-14T11:53:15Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 509,
    "diffHunk" : "@@ -1,1 +1347,1351 @@        \"It is only enabled while we need the compatibility in the scenario of new Spark \" +\n        \"version job fetching shuffle blocks from old version external shuffle service.\")\n      .version(\"3.0.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "b66fc63e-8331-4124-a783-5ac75bea3a49",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374717073",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "33d4a776-e3d9-45ac-abdf-aa40b5879d11",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-30812, commit ID: 68d7edf9497bea2f73707d32ab55dd8e53088e7c#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:53:42Z",
        "updatedAt" : "2020-03-14T11:53:43Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 517,
    "diffHunk" : "@@ -1,1 +1357,1361 @@        \"blocks requested from those block managers which are running on the same host are read \" +\n        \"from the disk directly instead of being fetched as remote blocks over the network.\")\n      .version(\"3.0.0\")\n      .booleanConf\n      .createWithDefault(true)"
  },
  {
    "id" : "b4dae02d-21b7-4171-8acb-3e1ee2773220",
    "prId" : 27864,
    "prUrl" : "https://github.com/apache/spark/pull/27864#pullrequestreview-388613385",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "934ca070-a96e-4f5f-9879-efeb371412e9",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Could you explain about the meaning of `decommission` here?\r\n\r\nLet say, we have Spark Thrift Server. In that case, dynamic allocation wants to decommission all workers. Then, what this PR can provide for zero-worker situation?",
        "createdAt" : "2020-04-07T18:48:30Z",
        "updatedAt" : "2020-04-23T14:23:15Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "18a36a95-65b2-46b1-a574-cb573976b43a",
        "parentId" : "934ca070-a96e-4f5f-9879-efeb371412e9",
        "authorId" : "ad4cb313-47b2-4cfa-9c1f-0a3596794a67",
        "body" : "@dongjoon-hyun Thanks for the detailed review.\r\n\r\nI couldn't understand your question with respect to dynamic allocation completely (as per my understanding dynamic allocation doesn't initiates decommissioning). This change works together with SPARK-20628 [PR](https://github.com/apache/spark/pull/26440). After SPARK-20628, Spark scheduler starts receiving decommissionExecutor message (because of AWS spot loss, Azure low-priority VMs etc). In such case, we should offload cache data from that executor to other active executors so that we don't eventually loose those cache blocks. This is done in a best effort way i.e. move cache blocks if space is available on active executors. If space is not available on other active executors, then keep the cache blocks.\r\n\r\nNext step after this change is to initiate decommissioning from DynamicAllocation. Let say minExecutor=2, maxExecutor=50. A spark application is running at max 50 executors and all of them have little bit of cache data (say 2GB out of total available 5GB capacity). So DynamicAllocation is not able to downscale any of the executors (although they are idle). In future, DynamicAllocation can leverage changes in this PR to do defragmentation of cache data to fewer set of executors so that some of the executors can be freed up.\r\n\r\nHope this answers your question.",
        "createdAt" : "2020-04-08T13:57:49Z",
        "updatedAt" : "2020-04-23T14:23:15Z",
        "lastEditedBy" : "ad4cb313-47b2-4cfa-9c1f-0a3596794a67",
        "tags" : [
        ]
      },
      {
        "id" : "0cf6535a-186d-4c4f-b384-f5bcfe9366bd",
        "parentId" : "934ca070-a96e-4f5f-9879-efeb371412e9",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Since this is focused on executor to executor migration supporting zero executors isn't going to be a viable option here. However since were using the same methods for putting blocks, assuming that we add support for storing these blocks in external storage in the future, this approach should be able to be generalized to use that same mechanism if configured.",
        "createdAt" : "2020-04-16T20:36:58Z",
        "updatedAt" : "2020-04-23T14:23:15Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "bb324f946019b8d700c517cc5eb2f7c11dc70cfc",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +416,420 @@  private[spark] val STORAGE_DECOMMISSION_ENABLED =\n    ConfigBuilder(\"spark.storage.decommission.enabled\")\n      .doc(\"Whether to decommission the block manager when decommissioning executor\")\n      .version(\"3.1.0\")\n      .booleanConf"
  },
  {
    "id" : "80dc927d-7c47-406d-90fc-e57782d66a6c",
    "prId" : 27852,
    "prUrl" : "https://github.com/apache/spark/pull/27852#pullrequestreview-370943941",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8f4a54f9-831c-49a2-83de-b2347d9480c9",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-5479, commit ID: 38112905bc3b33f2ae75274afba1c30e116f6e46#diff-4d2ab44195558d5a9d5f15b8803ef39d",
        "createdAt" : "2020-03-09T06:49:46Z",
        "updatedAt" : "2020-03-09T06:49:47Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "dc79da0cbcfb885820e57167b471ba9a8aea3036",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +465,469 @@    ConfigBuilder(\"spark.yarn.isPython\")\n      .internal()\n      .version(\"1.5.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "192c1139-8f95-4eec-8262-1ddd33a4e5fb",
    "prId" : 27852,
    "prUrl" : "https://github.com/apache/spark/pull/27852#pullrequestreview-370944120",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "14923e9f-0c44-446f-baee-e984dd7fab6b",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "No JIRA ID, commit ID: e5c4cd8a5e188592f8786a265c0cd073c69ac886#diff-391214d132a0fb4478f4f9c2313d8966",
        "createdAt" : "2020-03-09T06:50:19Z",
        "updatedAt" : "2020-03-09T06:50:20Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "dc79da0cbcfb885820e57167b471ba9a8aea3036",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +470,474 @@\n  private[spark] val CPUS_PER_TASK =\n    ConfigBuilder(\"spark.task.cpus\").version(\"0.5.0\").intConf.createWithDefault(1)\n\n  private[spark] val DYN_ALLOCATION_ENABLED ="
  },
  {
    "id" : "1ed421ce-72c3-41ee-be78-ec36a6b62d6d",
    "prId" : 27852,
    "prUrl" : "https://github.com/apache/spark/pull/27852#pullrequestreview-370944241",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "939002a6-7234-4dbe-a6ba-2d3a94847a08",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-3795, commit ID: 8d59b37b02eb36f37bcefafb952519d7dca744ad#diff-364713d7776956cb8b0a771e9b62f82d",
        "createdAt" : "2020-03-09T06:50:41Z",
        "updatedAt" : "2020-03-09T06:50:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "dc79da0cbcfb885820e57167b471ba9a8aea3036",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +474,478 @@  private[spark] val DYN_ALLOCATION_ENABLED =\n    ConfigBuilder(\"spark.dynamicAllocation.enabled\")\n      .version(\"1.2.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "207ef152-1963-447f-9491-a0bef119f6a1",
    "prId" : 27852,
    "prUrl" : "https://github.com/apache/spark/pull/27852#pullrequestreview-370944349",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "336ebfcf-43ad-40d7-b823-8d1edfaa9b90",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-3795, commit ID: 8d59b37b02eb36f37bcefafb952519d7dca744ad#diff-364713d7776956cb8b0a771e9b62f82d",
        "createdAt" : "2020-03-09T06:51:03Z",
        "updatedAt" : "2020-03-09T06:51:03Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "dc79da0cbcfb885820e57167b471ba9a8aea3036",
    "line" : 27,
    "diffHunk" : "@@ -1,1 +480,484 @@  private[spark] val DYN_ALLOCATION_TESTING =\n    ConfigBuilder(\"spark.dynamicAllocation.testing\")\n      .version(\"1.2.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "b42ac8bb-7453-4854-bb59-1b27d3964fc2",
    "prId" : 27852,
    "prUrl" : "https://github.com/apache/spark/pull/27852#pullrequestreview-370944400",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9d84f1ca-7a12-4173-9ed1-cd7d43e19d1c",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-3795, commit ID: 8d59b37b02eb36f37bcefafb952519d7dca744ad#diff-364713d7776956cb8b0a771e9b62f82d",
        "createdAt" : "2020-03-09T06:51:14Z",
        "updatedAt" : "2020-03-09T06:51:15Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "dc79da0cbcfb885820e57167b471ba9a8aea3036",
    "line" : 34,
    "diffHunk" : "@@ -1,1 +486,490 @@  private[spark] val DYN_ALLOCATION_MIN_EXECUTORS =\n    ConfigBuilder(\"spark.dynamicAllocation.minExecutors\")\n      .version(\"1.2.0\")\n      .intConf\n      .createWithDefault(0)"
  },
  {
    "id" : "bc409774-93d6-4fde-9294-f79c85d5bdba",
    "prId" : 27852,
    "prUrl" : "https://github.com/apache/spark/pull/27852#pullrequestreview-370944472",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "de5e8c4a-55e1-48ef-813b-cef5d31ae779",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-3795, commit ID: 8d59b37b02eb36f37bcefafb952519d7dca744ad#diff-364713d7776956cb8b0a771e9b62f82d",
        "createdAt" : "2020-03-09T06:51:30Z",
        "updatedAt" : "2020-03-09T06:51:31Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "dc79da0cbcfb885820e57167b471ba9a8aea3036",
    "line" : 46,
    "diffHunk" : "@@ -1,1 +497,501 @@  private[spark] val DYN_ALLOCATION_MAX_EXECUTORS =\n    ConfigBuilder(\"spark.dynamicAllocation.maxExecutors\")\n      .version(\"1.2.0\")\n      .intConf\n      .createWithDefault(Int.MaxValue)"
  },
  {
    "id" : "29e6800d-3f3f-487a-b215-6440a3045ebf",
    "prId" : 27852,
    "prUrl" : "https://github.com/apache/spark/pull/27852#pullrequestreview-370944591",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0caf8981-9fc0-4e61-8a70-a68a6e8365d3",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-4585, commit ID: b2047b55c5fc85de6b63276d8ab9610d2496e08b#diff-b096353602813e47074ace09a3890d56",
        "createdAt" : "2020-03-09T06:51:52Z",
        "updatedAt" : "2020-03-09T06:51:53Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "dc79da0cbcfb885820e57167b471ba9a8aea3036",
    "line" : 40,
    "diffHunk" : "@@ -1,1 +492,496 @@  private[spark] val DYN_ALLOCATION_INITIAL_EXECUTORS =\n    ConfigBuilder(\"spark.dynamicAllocation.initialExecutors\")\n      .version(\"1.3.0\")\n      .fallbackConf(DYN_ALLOCATION_MIN_EXECUTORS)\n"
  },
  {
    "id" : "0de59d37-bf8c-4b65-8423-8401d7252af7",
    "prId" : 27852,
    "prUrl" : "https://github.com/apache/spark/pull/27852#pullrequestreview-370944721",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "73de62b6-fe78-40e7-8975-66d4065bbd46",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-22683, commit ID: 55c4ca88a3b093ee197a8689631be8d1fac1f10f#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-09T06:52:18Z",
        "updatedAt" : "2020-03-09T06:52:18Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "dc79da0cbcfb885820e57167b471ba9a8aea3036",
    "line" : 53,
    "diffHunk" : "@@ -1,1 +503,507 @@  private[spark] val DYN_ALLOCATION_EXECUTOR_ALLOCATION_RATIO =\n    ConfigBuilder(\"spark.dynamicAllocation.executorAllocationRatio\")\n      .version(\"2.4.0\")\n      .doubleConf\n      .createWithDefault(1.0)"
  },
  {
    "id" : "b98a9636-968e-4c14-9885-5bd2ecc4a040",
    "prId" : 27852,
    "prUrl" : "https://github.com/apache/spark/pull/27852#pullrequestreview-370944828",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7f61548a-145e-4b3d-8133-720fa93412af",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-7955, commit ID: 6faaf15ba311bc3a79aae40a6c9c4befabb6889f#diff-b096353602813e47074ace09a3890d56",
        "createdAt" : "2020-03-09T06:52:37Z",
        "updatedAt" : "2020-03-09T06:52:38Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "dc79da0cbcfb885820e57167b471ba9a8aea3036",
    "line" : 59,
    "diffHunk" : "@@ -1,1 +509,513 @@  private[spark] val DYN_ALLOCATION_CACHED_EXECUTOR_IDLE_TIMEOUT =\n    ConfigBuilder(\"spark.dynamicAllocation.cachedExecutorIdleTimeout\")\n      .version(\"1.4.0\")\n      .timeConf(TimeUnit.SECONDS)\n      .checkValue(_ >= 0L, \"Timeout must be >= 0.\")"
  },
  {
    "id" : "2e924c74-67b9-43ff-9253-f4d7e2015b16",
    "prId" : 27852,
    "prUrl" : "https://github.com/apache/spark/pull/27852#pullrequestreview-370945006",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7f7c6967-23ce-4200-942e-e86779dbffed",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-3795, commit ID: 8d59b37b02eb36f37bcefafb952519d7dca744ad#diff-364713d7776956cb8b0a771e9b62f82d",
        "createdAt" : "2020-03-09T06:53:07Z",
        "updatedAt" : "2020-03-09T06:53:08Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "dc79da0cbcfb885820e57167b471ba9a8aea3036",
    "line" : 66,
    "diffHunk" : "@@ -1,1 +516,520 @@  private[spark] val DYN_ALLOCATION_EXECUTOR_IDLE_TIMEOUT =\n    ConfigBuilder(\"spark.dynamicAllocation.executorIdleTimeout\")\n      .version(\"1.2.0\")\n      .timeConf(TimeUnit.SECONDS)\n      .checkValue(_ >= 0L, \"Timeout must be >= 0.\")"
  },
  {
    "id" : "cd21eabe-6151-4a19-adfd-cfb62732d693",
    "prId" : 27852,
    "prUrl" : "https://github.com/apache/spark/pull/27852#pullrequestreview-370945124",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f59e203f-225c-4408-a854-9032dc70c8e6",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-27963, commit ID: 2ddeff97d7329942a98ef363991eeabc3fa71a76#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-09T06:53:31Z",
        "updatedAt" : "2020-03-09T06:53:31Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "dc79da0cbcfb885820e57167b471ba9a8aea3036",
    "line" : 73,
    "diffHunk" : "@@ -1,1 +523,527 @@  private[spark] val DYN_ALLOCATION_SHUFFLE_TRACKING =\n    ConfigBuilder(\"spark.dynamicAllocation.shuffleTracking.enabled\")\n      .version(\"3.0.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "bf5d9bcd-1ae1-4b15-8aac-b191eba7a684",
    "prId" : 27852,
    "prUrl" : "https://github.com/apache/spark/pull/27852#pullrequestreview-370945350",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a39b4c09-5d97-4aa0-82c4-cf374105f20d",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-27963, commit ID: 2ddeff97d7329942a98ef363991eeabc3fa71a76#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-09T06:54:15Z",
        "updatedAt" : "2020-03-09T06:54:16Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "dc79da0cbcfb885820e57167b471ba9a8aea3036",
    "line" : 79,
    "diffHunk" : "@@ -1,1 +529,533 @@  private[spark] val DYN_ALLOCATION_SHUFFLE_TIMEOUT =\n    ConfigBuilder(\"spark.dynamicAllocation.shuffleTimeout\")\n      .version(\"3.0.0\")\n      .timeConf(TimeUnit.MILLISECONDS)\n      .checkValue(_ >= 0L, \"Timeout must be >= 0.\")"
  },
  {
    "id" : "7a15c425-d795-48a8-bc67-23966e998871",
    "prId" : 27852,
    "prUrl" : "https://github.com/apache/spark/pull/27852#pullrequestreview-370945482",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "715ad0ad-9148-4b1e-ada5-b1b9070973ee",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-3795, commit ID: 8d59b37b02eb36f37bcefafb952519d7dca744ad#diff-364713d7776956cb8b0a771e9b62f82d",
        "createdAt" : "2020-03-09T06:54:46Z",
        "updatedAt" : "2020-03-09T06:54:46Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "dc79da0cbcfb885820e57167b471ba9a8aea3036",
    "line" : 86,
    "diffHunk" : "@@ -1,1 +536,540 @@  private[spark] val DYN_ALLOCATION_SCHEDULER_BACKLOG_TIMEOUT =\n    ConfigBuilder(\"spark.dynamicAllocation.schedulerBacklogTimeout\")\n      .version(\"1.2.0\")\n      .timeConf(TimeUnit.SECONDS).createWithDefault(1)\n"
  },
  {
    "id" : "1f9a3120-cffd-4439-b166-450462256774",
    "prId" : 27852,
    "prUrl" : "https://github.com/apache/spark/pull/27852#pullrequestreview-370945524",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "69292fe5-fc37-4054-a17e-ce4c1fda431d",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-3795, commit ID: 8d59b37b02eb36f37bcefafb952519d7dca744ad#diff-364713d7776956cb8b0a771e9b62f82d",
        "createdAt" : "2020-03-09T06:54:52Z",
        "updatedAt" : "2020-03-09T06:54:53Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "dc79da0cbcfb885820e57167b471ba9a8aea3036",
    "line" : 91,
    "diffHunk" : "@@ -1,1 +541,545 @@  private[spark] val DYN_ALLOCATION_SUSTAINED_SCHEDULER_BACKLOG_TIMEOUT =\n    ConfigBuilder(\"spark.dynamicAllocation.sustainedSchedulerBacklogTimeout\")\n      .version(\"1.2.0\")\n      .fallbackConf(DYN_ALLOCATION_SCHEDULER_BACKLOG_TIMEOUT)\n"
  },
  {
    "id" : "5404ebfa-c3dc-4a7b-ae18-96e317d366a3",
    "prId" : 27852,
    "prUrl" : "https://github.com/apache/spark/pull/27852#pullrequestreview-370945664",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c60c048f-00a0-42b3-a10d-e679f014cd32",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "No JIRA ID, commit ID: e5c4cd8a5e188592f8786a265c0cd073c69ac886#diff-391214d132a0fb4478f4f9c2313d8966",
        "createdAt" : "2020-03-09T06:55:19Z",
        "updatedAt" : "2020-03-09T06:55:19Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "dc79da0cbcfb885820e57167b471ba9a8aea3036",
    "line" : 95,
    "diffHunk" : "@@ -1,1 +545,549 @@\n  private[spark] val LOCALITY_WAIT = ConfigBuilder(\"spark.locality.wait\")\n    .version(\"0.5.0\")\n    .timeConf(TimeUnit.MILLISECONDS)\n    .createWithDefaultString(\"3s\")"
  },
  {
    "id" : "1690e445-fa95-443a-a5a2-65387e431bdd",
    "prId" : 27852,
    "prUrl" : "https://github.com/apache/spark/pull/27852#pullrequestreview-370945771",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7d1337d8-b3ef-4dbe-8aa9-40e2304415d0",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-3796, commit ID: f55218aeb1e9d638df6229b36a59a15ce5363482#diff-2b643ea78c1add0381754b1f47eec132",
        "createdAt" : "2020-03-09T06:55:40Z",
        "updatedAt" : "2020-03-09T06:55:41Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "dc79da0cbcfb885820e57167b471ba9a8aea3036",
    "line" : 102,
    "diffHunk" : "@@ -1,1 +551,555 @@  private[spark] val SHUFFLE_SERVICE_ENABLED =\n    ConfigBuilder(\"spark.shuffle.service.enabled\")\n      .version(\"1.2.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "adcabb12-b61b-40ca-8ae1-0b1cab8d8469",
    "prId" : 27852,
    "prUrl" : "https://github.com/apache/spark/pull/27852#pullrequestreview-370945933",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "63b1e749-7b25-41f6-9d08-9f0a0a7d8036",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-27677, commit ID: e9f3f62b2c0f521f3cc23fef381fc6754853ad4f#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-09T06:56:16Z",
        "updatedAt" : "2020-03-09T06:56:17Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "dc79da0cbcfb885820e57167b471ba9a8aea3036",
    "line" : 112,
    "diffHunk" : "@@ -1,1 +561,565 @@        \"persisted blocks are considered idle after \" +\n        \"'spark.dynamicAllocation.executorIdleTimeout' and will be released accordingly.\")\n      .version(\"3.0.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "e8021cc5-5e50-407f-b1ea-79e220112f73",
    "prId" : 27852,
    "prUrl" : "https://github.com/apache/spark/pull/27852#pullrequestreview-370946069",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "868b8170-7f52-4810-baa4-9dabe32f6aed",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-26288, commit ID: 8b0aa59218c209d39cbba5959302d8668b885cf6#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-09T06:56:43Z",
        "updatedAt" : "2020-03-09T06:56:43Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "dc79da0cbcfb885820e57167b471ba9a8aea3036",
    "line" : 120,
    "diffHunk" : "@@ -1,1 +569,573 @@      .doc(\"Whether to use db in ExternalShuffleService. Note that this only affects \" +\n        \"standalone mode.\")\n      .version(\"3.0.0\")\n      .booleanConf\n      .createWithDefault(true)"
  },
  {
    "id" : "0085b4fe-8265-4f7e-bcc5-e67a582f659f",
    "prId" : 27852,
    "prUrl" : "https://github.com/apache/spark/pull/27852#pullrequestreview-370946191",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3ef121f6-5d1e-4785-b224-069ca13d3967",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-3796, commit ID: f55218aeb1e9d638df6229b36a59a15ce5363482#diff-2b643ea78c1add0381754b1f47eec132",
        "createdAt" : "2020-03-09T06:57:05Z",
        "updatedAt" : "2020-03-09T06:57:06Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "dc79da0cbcfb885820e57167b471ba9a8aea3036",
    "line" : 126,
    "diffHunk" : "@@ -1,1 +574,578 @@\n  private[spark] val SHUFFLE_SERVICE_PORT =\n    ConfigBuilder(\"spark.shuffle.service.port\").version(\"1.2.0\").intConf.createWithDefault(7337)\n\n  private[spark] val KEYTAB = ConfigBuilder(\"spark.kerberos.keytab\")"
  },
  {
    "id" : "b133c6ad-0d71-4b3b-912b-7a662e934cfc",
    "prId" : 27852,
    "prUrl" : "https://github.com/apache/spark/pull/27852#pullrequestreview-370946285",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7629cd3c-db26-4e11-9dc8-59a1ee6bc0f1",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-25372, commit ID: 51540c2fa677658be954c820bc18ba748e4c8583#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-09T06:57:27Z",
        "updatedAt" : "2020-03-09T06:57:27Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "dc79da0cbcfb885820e57167b471ba9a8aea3036",
    "line" : 130,
    "diffHunk" : "@@ -1,1 +578,582 @@  private[spark] val KEYTAB = ConfigBuilder(\"spark.kerberos.keytab\")\n    .doc(\"Location of user's keytab.\")\n    .version(\"3.0.0\")\n    .stringConf.createOptional\n"
  },
  {
    "id" : "e4934e13-5ce9-4438-a83c-252a2a456071",
    "prId" : 27852,
    "prUrl" : "https://github.com/apache/spark/pull/27852#pullrequestreview-370946348",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "04f9aa0f-5cdc-4a57-8b19-ca9dafcaa575",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-25372, commit ID: 51540c2fa677658be954c820bc18ba748e4c8583#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-09T06:57:40Z",
        "updatedAt" : "2020-03-09T06:57:40Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "dc79da0cbcfb885820e57167b471ba9a8aea3036",
    "line" : 136,
    "diffHunk" : "@@ -1,1 +583,587 @@  private[spark] val PRINCIPAL = ConfigBuilder(\"spark.kerberos.principal\")\n    .doc(\"Name of the Kerberos principal.\")\n    .version(\"3.0.0\")\n    .stringConf\n    .createOptional"
  },
  {
    "id" : "e1464a66-f298-45f3-bbcc-4f350e9de7e8",
    "prId" : 27852,
    "prUrl" : "https://github.com/apache/spark/pull/27852#pullrequestreview-370946465",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "10d39eed-5c89-440d-a1e9-decd5f0ee25f",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-23781, commit ID: 68dde3481ea458b0b8deeec2f99233c2d4c1e056#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-09T06:58:03Z",
        "updatedAt" : "2020-03-09T06:58:03Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "dc79da0cbcfb885820e57167b471ba9a8aea3036",
    "line" : 141,
    "diffHunk" : "@@ -1,1 +588,592 @@\n  private[spark] val KERBEROS_RELOGIN_PERIOD = ConfigBuilder(\"spark.kerberos.relogin.period\")\n    .version(\"3.0.0\")\n    .timeConf(TimeUnit.SECONDS)\n    .createWithDefaultString(\"1m\")"
  },
  {
    "id" : "97687e04-a72f-477c-af8b-6eddfe1cc9f2",
    "prId" : 27852,
    "prUrl" : "https://github.com/apache/spark/pull/27852#pullrequestreview-370946591",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d0269d2d-29b1-4d9e-afa0-2e9bac948fd0",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-26595, commit ID: 2a67dbfbd341af166b1c85904875f26a6dea5ba8#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-09T06:58:28Z",
        "updatedAt" : "2020-03-09T06:58:29Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "dc79da0cbcfb885820e57167b471ba9a8aea3036",
    "line" : 149,
    "diffHunk" : "@@ -1,1 +598,602 @@        \"'keytab', the default, which requires a keytab to be provided, or 'ccache', which uses \" +\n        \"the local credentials cache.\")\n      .version(\"3.0.0\")\n      .stringConf\n      .checkValues(Set(\"keytab\", \"ccache\"))"
  },
  {
    "id" : "7ff25787-5ac3-428b-90d6-312f3bac212a",
    "prId" : 27852,
    "prUrl" : "https://github.com/apache/spark/pull/27852#pullrequestreview-370946731",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e98e0f3f-0116-4722-820c-be90cde5e6cf",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-26766, commit ID: d0443a74d185ec72b747fa39994fa9a40ce974cf#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-09T06:58:53Z",
        "updatedAt" : "2020-03-09T06:58:54Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "dc79da0cbcfb885820e57167b471ba9a8aea3036",
    "line" : 157,
    "diffHunk" : "@@ -1,1 +607,611 @@    .doc(\"Extra Hadoop filesystem URLs for which to request delegation tokens. The filesystem \" +\n      \"that hosts fs.defaultFS does not need to be listed here.\")\n    .version(\"3.0.0\")\n    .stringConf\n    .toSequence"
  },
  {
    "id" : "93495bdf-07b4-49bf-aee1-e7a95465d95a",
    "prId" : 27852,
    "prUrl" : "https://github.com/apache/spark/pull/27852#pullrequestreview-370946862",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e3a5cfc4-62ed-46ac-8744-f8e0d9b34823",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-1126, commit ID: 1617816090e7b20124a512a43860a21232ebf511#diff-4d2ab44195558d5a9d5f15b8803ef39d",
        "createdAt" : "2020-03-09T06:59:25Z",
        "updatedAt" : "2020-03-09T06:59:25Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "dc79da0cbcfb885820e57167b471ba9a8aea3036",
    "line" : 163,
    "diffHunk" : "@@ -1,1 +613,617 @@\n  private[spark] val EXECUTOR_INSTANCES = ConfigBuilder(\"spark.executor.instances\")\n    .version(\"1.0.0\")\n    .intConf\n    .createOptional"
  },
  {
    "id" : "e3d0ae1b-424b-4292-bec7-88b3dffc901a",
    "prId" : 27852,
    "prUrl" : "https://github.com/apache/spark/pull/27852#pullrequestreview-370946963",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "65d03707-e7db-4116-b46e-1875e6d53218",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-21714, commit ID: d10c9dc3f631a26dbbbd8f5c601ca2001a5d7c80#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-09T06:59:47Z",
        "updatedAt" : "2020-03-09T06:59:47Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "dc79da0cbcfb885820e57167b471ba9a8aea3036",
    "line" : 169,
    "diffHunk" : "@@ -1,1 +619,623 @@  private[spark] val PY_FILES = ConfigBuilder(\"spark.yarn.dist.pyFiles\")\n    .internal()\n    .version(\"2.2.1\")\n    .stringConf\n    .toSequence"
  },
  {
    "id" : "1bf4e71b-71a5-489c-81ca-0bb3e991d1ef",
    "prId" : 27852,
    "prUrl" : "https://github.com/apache/spark/pull/27852#pullrequestreview-370947093",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d949a68d-f137-44dd-acbc-1d5f36f8a49a",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-13830, commit ID: 2ef4c5963bff3574fe17e669d703b25ddd064e5d#diff-5a0de266c82b95adb47d9bca714e1f1b",
        "createdAt" : "2020-03-09T07:00:08Z",
        "updatedAt" : "2020-03-09T07:00:08Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "dc79da0cbcfb885820e57167b471ba9a8aea3036",
    "line" : 176,
    "diffHunk" : "@@ -1,1 +626,630 @@  private[spark] val TASK_MAX_DIRECT_RESULT_SIZE =\n    ConfigBuilder(\"spark.task.maxDirectResultSize\")\n      .version(\"2.0.0\")\n      .bytesConf(ByteUnit.BYTE)\n      .createWithDefault(1L << 20)"
  },
  {
    "id" : "7699d31a-141a-4eaf-8006-3d36a878a05f",
    "prId" : 27852,
    "prUrl" : "https://github.com/apache/spark/pull/27852#pullrequestreview-370947221",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0e66c4c1-16af-4b51-b042-9563110e6be2",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "No JIRA ID, commit ID: 46eecd110a4017ea0c86cbb1010d0ccd6a5eb2ef#diff-264da78fe625d594eae59d1adabc8ae9",
        "createdAt" : "2020-03-09T07:00:33Z",
        "updatedAt" : "2020-03-09T07:00:33Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "dc79da0cbcfb885820e57167b471ba9a8aea3036",
    "line" : 182,
    "diffHunk" : "@@ -1,1 +632,636 @@  private[spark] val TASK_MAX_FAILURES =\n    ConfigBuilder(\"spark.task.maxFailures\")\n      .version(\"0.8.0\")\n      .intConf\n      .createWithDefault(4)"
  },
  {
    "id" : "f12b4366-3ba3-4a66-906d-3f13c3b243d4",
    "prId" : 27852,
    "prUrl" : "https://github.com/apache/spark/pull/27852#pullrequestreview-370947340",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b2465d02-526c-4622-9736-b3b2c2e4d89f",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-18761, commit ID: 678d91c1d2283d9965a39656af9d383bad093ba8#diff-5a0de266c82b95adb47d9bca714e1f1b",
        "createdAt" : "2020-03-09T07:00:57Z",
        "updatedAt" : "2020-03-09T07:00:57Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "dc79da0cbcfb885820e57167b471ba9a8aea3036",
    "line" : 188,
    "diffHunk" : "@@ -1,1 +638,642 @@  private[spark] val TASK_REAPER_ENABLED =\n    ConfigBuilder(\"spark.task.reaper.enabled\")\n      .version(\"2.0.3\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "371c4aa2-dc56-4759-b2c2-b4d90c7f1bc3",
    "prId" : 27852,
    "prUrl" : "https://github.com/apache/spark/pull/27852#pullrequestreview-370947395",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5cd66443-1e30-4871-9b60-c753f7ce4e8c",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-18761, commit ID: 678d91c1d2283d9965a39656af9d383bad093ba8#diff-5a0de266c82b95adb47d9bca714e1f1b",
        "createdAt" : "2020-03-09T07:01:08Z",
        "updatedAt" : "2020-03-09T07:01:09Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "dc79da0cbcfb885820e57167b471ba9a8aea3036",
    "line" : 194,
    "diffHunk" : "@@ -1,1 +644,648 @@  private[spark] val TASK_REAPER_KILL_TIMEOUT =\n    ConfigBuilder(\"spark.task.reaper.killTimeout\")\n      .version(\"2.0.3\")\n      .timeConf(TimeUnit.MILLISECONDS)\n      .createWithDefault(-1)"
  },
  {
    "id" : "98dea6be-5b6e-46f1-b3e3-b049a0d11684",
    "prId" : 27852,
    "prUrl" : "https://github.com/apache/spark/pull/27852#pullrequestreview-370947425",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ce52374b-3f16-4e51-b59e-b71f10fe2c00",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-18761, commit ID: 678d91c1d2283d9965a39656af9d383bad093ba8#diff-5a0de266c82b95adb47d9bca714e1f1b",
        "createdAt" : "2020-03-09T07:01:15Z",
        "updatedAt" : "2020-03-09T07:01:15Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "dc79da0cbcfb885820e57167b471ba9a8aea3036",
    "line" : 200,
    "diffHunk" : "@@ -1,1 +650,654 @@  private[spark] val TASK_REAPER_POLLING_INTERVAL =\n    ConfigBuilder(\"spark.task.reaper.pollingInterval\")\n      .version(\"2.0.3\")\n      .timeConf(TimeUnit.MILLISECONDS)\n      .createWithDefaultString(\"10s\")"
  },
  {
    "id" : "aebb8907-a50a-4c53-9db9-b09a70c5642e",
    "prId" : 27852,
    "prUrl" : "https://github.com/apache/spark/pull/27852#pullrequestreview-370947465",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8de94ee0-65d2-49ce-a516-180550721ec1",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-18761, commit ID: 678d91c1d2283d9965a39656af9d383bad093ba8#diff-5a0de266c82b95adb47d9bca714e1f1b",
        "createdAt" : "2020-03-09T07:01:23Z",
        "updatedAt" : "2020-03-09T07:01:23Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "dc79da0cbcfb885820e57167b471ba9a8aea3036",
    "line" : 206,
    "diffHunk" : "@@ -1,1 +656,660 @@  private[spark] val TASK_REAPER_THREAD_DUMP =\n    ConfigBuilder(\"spark.task.reaper.threadDump\")\n      .version(\"2.0.3\")\n      .booleanConf\n      .createWithDefault(true)"
  },
  {
    "id" : "07a2c24b-4fda-472d-b5a9-0d544512febb",
    "prId" : 27852,
    "prUrl" : "https://github.com/apache/spark/pull/27852#pullrequestreview-370947599",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a8b31614-22e8-460c-88ce-57b1cc361b8d",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-17675, commit ID: 9ce7d3e542e786c62f047c13f3001e178f76e06a#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-09T07:01:50Z",
        "updatedAt" : "2020-03-09T07:01:51Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "dc79da0cbcfb885820e57167b471ba9a8aea3036",
    "line" : 213,
    "diffHunk" : "@@ -1,1 +663,667 @@  private[spark] val BLACKLIST_ENABLED =\n    ConfigBuilder(\"spark.blacklist.enabled\")\n      .version(\"2.1.0\")\n      .booleanConf\n      .createOptional"
  },
  {
    "id" : "2b44a56b-2407-4b9e-a111-8640be43ca29",
    "prId" : 27852,
    "prUrl" : "https://github.com/apache/spark/pull/27852#pullrequestreview-370947646",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fb9752b4-377e-4ab8-8c51-354834bbf283",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-17675, commit ID: 9ce7d3e542e786c62f047c13f3001e178f76e06a#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-09T07:02:03Z",
        "updatedAt" : "2020-03-09T07:02:03Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "dc79da0cbcfb885820e57167b471ba9a8aea3036",
    "line" : 219,
    "diffHunk" : "@@ -1,1 +669,673 @@  private[spark] val MAX_TASK_ATTEMPTS_PER_EXECUTOR =\n    ConfigBuilder(\"spark.blacklist.task.maxTaskAttemptsPerExecutor\")\n      .version(\"2.1.0\")\n      .intConf\n      .createWithDefault(1)"
  },
  {
    "id" : "f59dd312-7e45-42e2-9086-145485615bd9",
    "prId" : 27852,
    "prUrl" : "https://github.com/apache/spark/pull/27852#pullrequestreview-370947871",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "247dba3d-e604-41e0-8e3a-8fd68a3ac5fb",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-17675, commit ID: 9ce7d3e542e786c62f047c13f3001e178f76e06a#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-09T07:02:42Z",
        "updatedAt" : "2020-03-09T07:02:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "dc79da0cbcfb885820e57167b471ba9a8aea3036",
    "line" : 225,
    "diffHunk" : "@@ -1,1 +675,679 @@  private[spark] val MAX_TASK_ATTEMPTS_PER_NODE =\n    ConfigBuilder(\"spark.blacklist.task.maxTaskAttemptsPerNode\")\n      .version(\"2.1.0\")\n      .intConf\n      .createWithDefault(2)"
  },
  {
    "id" : "8a995dbd-dab2-4ea3-8a6f-b0f14b6f835c",
    "prId" : 27852,
    "prUrl" : "https://github.com/apache/spark/pull/27852#pullrequestreview-370948055",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "32875719-f6fc-4b0d-b253-2aff73819f01",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-17675, commit ID: 9ce7d3e542e786c62f047c13f3001e178f76e06a#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-09T07:03:19Z",
        "updatedAt" : "2020-03-09T07:03:19Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "dc79da0cbcfb885820e57167b471ba9a8aea3036",
    "line" : 237,
    "diffHunk" : "@@ -1,1 +687,691 @@  private[spark] val MAX_FAILURES_PER_EXEC_STAGE =\n    ConfigBuilder(\"spark.blacklist.stage.maxFailedTasksPerExecutor\")\n      .version(\"2.1.0\")\n      .intConf\n      .createWithDefault(2)"
  },
  {
    "id" : "e76e8265-2700-4e4c-a7ca-787c29306514",
    "prId" : 27852,
    "prUrl" : "https://github.com/apache/spark/pull/27852#pullrequestreview-370948158",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "636f1951-e1a0-4aea-90b3-c2742cc7c437",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-17675, commit ID: 9ce7d3e542e786c62f047c13f3001e178f76e06a#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-09T07:03:39Z",
        "updatedAt" : "2020-03-09T07:03:39Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "dc79da0cbcfb885820e57167b471ba9a8aea3036",
    "line" : 249,
    "diffHunk" : "@@ -1,1 +699,703 @@  private[spark] val MAX_FAILED_EXEC_PER_NODE_STAGE =\n    ConfigBuilder(\"spark.blacklist.stage.maxFailedExecutorsPerNode\")\n      .version(\"2.1.0\")\n      .intConf\n      .createWithDefault(2)"
  },
  {
    "id" : "b7be5eec-eb15-4845-9db5-5f1c9fe74aec",
    "prId" : 27852,
    "prUrl" : "https://github.com/apache/spark/pull/27852#pullrequestreview-370948271",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "001e1e03-6f80-4134-bb8d-783ad96f1c43",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-17675, commit ID: 9ce7d3e542e786c62f047c13f3001e178f76e06a#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-09T07:03:59Z",
        "updatedAt" : "2020-03-09T07:04:00Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "dc79da0cbcfb885820e57167b471ba9a8aea3036",
    "line" : 255,
    "diffHunk" : "@@ -1,1 +705,709 @@  private[spark] val BLACKLIST_TIMEOUT_CONF =\n    ConfigBuilder(\"spark.blacklist.timeout\")\n      .version(\"2.1.0\")\n      .timeConf(TimeUnit.MILLISECONDS)\n      .createOptional"
  },
  {
    "id" : "1cf57079-f559-4a76-a1cd-159e3a3cf24d",
    "prId" : 27852,
    "prUrl" : "https://github.com/apache/spark/pull/27852#pullrequestreview-370948480",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "37646c3f-65ed-4067-b384-223749fc0fc2",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-8425, commit ID: 93cdb8a7d0f124b4db069fd8242207c82e263c52#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-09T07:04:41Z",
        "updatedAt" : "2020-03-09T07:04:41Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "dc79da0cbcfb885820e57167b471ba9a8aea3036",
    "line" : 231,
    "diffHunk" : "@@ -1,1 +681,685 @@  private[spark] val MAX_FAILURES_PER_EXEC =\n    ConfigBuilder(\"spark.blacklist.application.maxFailedTasksPerExecutor\")\n      .version(\"2.2.0\")\n      .intConf\n      .createWithDefault(2)"
  },
  {
    "id" : "05b58def-4200-454a-a7db-f2ddcd54073d",
    "prId" : 27852,
    "prUrl" : "https://github.com/apache/spark/pull/27852#pullrequestreview-370948632",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d530461c-fc7c-42d1-89a5-6b292523ba89",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-8425, commit ID: 93cdb8a7d0f124b4db069fd8242207c82e263c52#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-09T07:05:08Z",
        "updatedAt" : "2020-03-09T07:05:09Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "dc79da0cbcfb885820e57167b471ba9a8aea3036",
    "line" : 243,
    "diffHunk" : "@@ -1,1 +693,697 @@  private[spark] val MAX_FAILED_EXEC_PER_NODE =\n    ConfigBuilder(\"spark.blacklist.application.maxFailedExecutorsPerNode\")\n      .version(\"2.2.0\")\n      .intConf\n      .createWithDefault(2)"
  },
  {
    "id" : "ca78b322-492d-4c6a-9a7a-590efb51c732",
    "prId" : 27852,
    "prUrl" : "https://github.com/apache/spark/pull/27852#pullrequestreview-370948773",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "db9ca63e-79c4-4407-8cc3-98dafa747549",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-16554, commit ID: 6287c94f08200d548df5cc0a401b73b84f9968c4#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-09T07:05:35Z",
        "updatedAt" : "2020-03-09T07:05:35Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "dc79da0cbcfb885820e57167b471ba9a8aea3036",
    "line" : 261,
    "diffHunk" : "@@ -1,1 +711,715 @@  private[spark] val BLACKLIST_KILL_ENABLED =\n    ConfigBuilder(\"spark.blacklist.killBlacklistedExecutors\")\n      .version(\"2.2.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "1312ddd9-f0c1-4900-b028-65ab958265a4",
    "prId" : 27852,
    "prUrl" : "https://github.com/apache/spark/pull/27852#pullrequestreview-370948889",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9fc21636-79d5-48ad-97f4-2c0dc394aa69",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "No JIRA ID, commit ID: ab747d39ddc7c8a314ed2fb26548fc5652af0d74#diff-bad3987c83bd22d46416d3dd9d208e76",
        "createdAt" : "2020-03-09T07:06:01Z",
        "updatedAt" : "2020-03-09T07:06:02Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "dc79da0cbcfb885820e57167b471ba9a8aea3036",
    "line" : 268,
    "diffHunk" : "@@ -1,1 +718,722 @@    ConfigBuilder(\"spark.scheduler.executorTaskBlacklistTime\")\n      .internal()\n      .version(\"1.0.0\")\n      .timeConf(TimeUnit.MILLISECONDS)\n      .createOptional"
  },
  {
    "id" : "ee3d793c-8bd3-40b7-b304-552df71200c0",
    "prId" : 27852,
    "prUrl" : "https://github.com/apache/spark/pull/27852#pullrequestreview-370949055",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "187b2a1c-a745-41b0-9c42-419542ba1594",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-13669 and SPARK-20898, commit ID: 9e50a1d37a4cf0c34e20a7c1a910ceaff41535a2#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-09T07:06:29Z",
        "updatedAt" : "2020-03-09T07:06:30Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "dc79da0cbcfb885820e57167b471ba9a8aea3036",
    "line" : 274,
    "diffHunk" : "@@ -1,1 +724,728 @@  private[spark] val BLACKLIST_FETCH_FAILURE_ENABLED =\n    ConfigBuilder(\"spark.blacklist.application.fetchFailure.enabled\")\n      .version(\"2.3.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "816cfeba-5bef-4505-bb3e-5a6c3819d225",
    "prId" : 27852,
    "prUrl" : "https://github.com/apache/spark/pull/27852#pullrequestreview-370949177",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6774ecaa-f67d-45c5-bfaf-1f124073428e",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-19753, commit ID: dccc0aa3cf957c8eceac598ac81ac82f03b52105#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-09T07:06:55Z",
        "updatedAt" : "2020-03-09T07:06:55Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "dc79da0cbcfb885820e57167b471ba9a8aea3036",
    "line" : 282,
    "diffHunk" : "@@ -1,1 +734,738 @@        \" a FetchFailure. This is set default to false, which means, we only un-register the \" +\n        \" outputs related to the exact executor(instead of the host) on a FetchFailure.\")\n      .version(\"2.3.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "ceb533c8-7bf4-4093-be99-95cbfcf069e3",
    "prId" : 27852,
    "prUrl" : "https://github.com/apache/spark/pull/27852#pullrequestreview-370949324",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8431db10-1f51-4526-a347-23aa61034666",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-20887, commit ID: 629f38e171409da614fd635bd8dd951b7fde17a4#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-09T07:07:20Z",
        "updatedAt" : "2020-03-09T07:07:20Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "dc79da0cbcfb885820e57167b471ba9a8aea3036",
    "line" : 290,
    "diffHunk" : "@@ -1,1 +744,748 @@        \".eventqueue.queueName.capacity` first. If it's not configured, Spark will \" +\n        \"use the default capacity specified by this config.\")\n      .version(\"2.3.0\")\n      .intConf\n      .checkValue(_ > 0, \"The capacity of listener bus event queue must be positive\")"
  },
  {
    "id" : "729de323-abe9-4487-b034-9f829a1bdfd9",
    "prId" : 27852,
    "prUrl" : "https://github.com/apache/spark/pull/27852#pullrequestreview-370949456",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b78ff7bf-fd9b-4895-b206-b3a77e31a12f",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-20863, commit ID: 2a23cdd078a7409d0bb92cf27718995766c41b1d#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-09T07:07:42Z",
        "updatedAt" : "2020-03-09T07:07:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "dc79da0cbcfb885820e57167b471ba9a8aea3036",
    "line" : 298,
    "diffHunk" : "@@ -1,1 +752,756 @@    ConfigBuilder(\"spark.scheduler.listenerbus.metrics.maxListenerClassesTimed\")\n      .internal()\n      .version(\"2.3.0\")\n      .intConf\n      .createWithDefault(128)"
  },
  {
    "id" : "acfe773b-f4b1-449c-9a47-53de840636dc",
    "prId" : 27852,
    "prUrl" : "https://github.com/apache/spark/pull/27852#pullrequestreview-370949611",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b8d32d2d-6bba-47f0-95ca-b38537001270",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-30812, commit ID: 68d7edf9497bea2f73707d32ab55dd8e53088e7c#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-09T07:08:10Z",
        "updatedAt" : "2020-03-09T07:08:10Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "dc79da0cbcfb885820e57167b471ba9a8aea3036",
    "line" : 306,
    "diffHunk" : "@@ -1,1 +762,766 @@        \"discover the event types that cause performance bottlenecks. The time threshold is \" +\n        \"controlled by spark.scheduler.listenerbus.logSlowEvent.threshold.\")\n      .version(\"3.0.0\")\n      .booleanConf\n      .createWithDefault(true)"
  },
  {
    "id" : "b4af3e49-1b56-4cfd-8947-ed6e0785dca7",
    "prId" : 27852,
    "prUrl" : "https://github.com/apache/spark/pull/27852#pullrequestreview-370949719",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ab826c98-8229-4d00-bc48-9cf816d56c3a",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-29001, commit ID: 0346afa8fc348aa1b3f5110df747a64e3b2da388#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-09T07:08:33Z",
        "updatedAt" : "2020-03-09T07:08:34Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "dc79da0cbcfb885820e57167b471ba9a8aea3036",
    "line" : 314,
    "diffHunk" : "@@ -1,1 +771,775 @@      .doc(\"The time threshold of whether a event is considered to be taking too much time to \" +\n        s\"process. Log the event if ${LISTENER_BUS_LOG_SLOW_EVENT_ENABLED.key} is true.\")\n      .version(\"3.0.0\")\n      .timeConf(TimeUnit.NANOSECONDS)\n      .createWithDefaultString(\"1s\")"
  },
  {
    "id" : "503e7fd8-d071-4208-9657-e3370c6f8ba9",
    "prId" : 27847,
    "prUrl" : "https://github.com/apache/spark/pull/27847#pullrequestreview-370738972",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a5f1838b-e091-4385-996b-86eddeb655de",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-30689, commit ID: 742e35f1d48c2523dda2ce21d73b7ab5ade20582#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-07T11:24:15Z",
        "updatedAt" : "2020-03-07T11:24:16Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "98c3def8275c6c2c0eb86ace24c0408613877c1a",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +46,50 @@        \"returns the resource information for that resource. It tries the discovery \" +\n        \"script last if none of the plugins return information for that resource.\")\n      .version(\"3.0.0\")\n      .stringConf\n      .toSequence"
  },
  {
    "id" : "7c52246a-43b4-494b-9df2-38b4e147f414",
    "prId" : 27847,
    "prUrl" : "https://github.com/apache/spark/pull/27847#pullrequestreview-370738987",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "191ff924-3f11-4f0d-884f-3560d8e0ca00",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-27835, commit ID: 6748b486a9afe8370786efb64a8c9f3470c62dcf#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-07T11:24:51Z",
        "updatedAt" : "2020-03-07T11:24:51Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "98c3def8275c6c2c0eb86ace24c0408613877c1a",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +57,61 @@        \"The file should be formatted as a JSON array of ResourceAllocation objects. \" +\n        \"Only used internally in standalone mode.\")\n      .version(\"3.0.0\")\n      .stringConf\n      .createOptional"
  },
  {
    "id" : "e6f7242c-62a0-4701-933e-157c0f9b043a",
    "prId" : 27847,
    "prUrl" : "https://github.com/apache/spark/pull/27847#pullrequestreview-370739009",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "34c0d081-85e1-4980-b723-2e41211d2f94",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "No JIRA ID, commit ID: 29ee101c73bf066bf7f4f8141c475b8d1bd3cf1c#diff-4d2ab44195558d5a9d5f15b8803ef39d",
        "createdAt" : "2020-03-07T11:25:20Z",
        "updatedAt" : "2020-03-07T11:25:21Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "98c3def8275c6c2c0eb86ace24c0408613877c1a",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +63,67 @@  private[spark] val DRIVER_CLASS_PATH =\n    ConfigBuilder(SparkLauncher.DRIVER_EXTRA_CLASSPATH)\n      .version(\"1.0.0\")\n      .stringConf\n      .createOptional"
  },
  {
    "id" : "a52a6454-9cd4-4735-94ea-e6f571ed9dfc",
    "prId" : 27847,
    "prUrl" : "https://github.com/apache/spark/pull/27847#pullrequestreview-370739027",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3921c574-98b7-49f6-8b8e-6482d10a6026",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "No JIRA ID, commit ID: 29ee101c73bf066bf7f4f8141c475b8d1bd3cf1c#diff-4d2ab44195558d5a9d5f15b8803ef39d",
        "createdAt" : "2020-03-07T11:25:43Z",
        "updatedAt" : "2020-03-07T11:25:44Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "98c3def8275c6c2c0eb86ace24c0408613877c1a",
    "line" : 26,
    "diffHunk" : "@@ -1,1 +70,74 @@    ConfigBuilder(SparkLauncher.DRIVER_EXTRA_JAVA_OPTIONS)\n      .withPrepended(SparkLauncher.DRIVER_DEFAULT_JAVA_OPTIONS)\n      .version(\"1.0.0\")\n      .stringConf\n      .createOptional"
  },
  {
    "id" : "9414c54b-50b3-4644-a6c2-8fba30bca5f9",
    "prId" : 27847,
    "prUrl" : "https://github.com/apache/spark/pull/27847#pullrequestreview-370739039",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d67e4556-b534-40f9-b7c0-6fb87b41ac79",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "No JIRA ID, commit ID: 29ee101c73bf066bf7f4f8141c475b8d1bd3cf1c#diff-4d2ab44195558d5a9d5f15b8803ef39d",
        "createdAt" : "2020-03-07T11:25:54Z",
        "updatedAt" : "2020-03-07T11:25:54Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "98c3def8275c6c2c0eb86ace24c0408613877c1a",
    "line" : 33,
    "diffHunk" : "@@ -1,1 +76,80 @@  private[spark] val DRIVER_LIBRARY_PATH =\n    ConfigBuilder(SparkLauncher.DRIVER_EXTRA_LIBRARY_PATH)\n      .version(\"1.0.0\")\n      .stringConf\n      .createOptional"
  },
  {
    "id" : "068696de-d841-4d79-8c3c-cd34f995a8e3",
    "prId" : 27847,
    "prUrl" : "https://github.com/apache/spark/pull/27847#pullrequestreview-370739049",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "91873d2b-492f-4191-9830-a4e24c0f679f",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-2996, commit ID: 6a1e0f967286945db13d94aeb6ed19f0a347c236#diff-4d2ab44195558d5a9d5f15b8803ef39d",
        "createdAt" : "2020-03-07T11:26:17Z",
        "updatedAt" : "2020-03-07T11:26:17Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "98c3def8275c6c2c0eb86ace24c0408613877c1a",
    "line" : 40,
    "diffHunk" : "@@ -1,1 +82,86 @@  private[spark] val DRIVER_USER_CLASS_PATH_FIRST =\n    ConfigBuilder(\"spark.driver.userClassPathFirst\")\n      .version(\"1.3.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "d04503d9-680f-4fea-b4e1-6df63b3d067f",
    "prId" : 27847,
    "prUrl" : "https://github.com/apache/spark/pull/27847#pullrequestreview-370739077",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "519f16e9-2cb7-4ae2-880f-6f6358aaa170",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-1507, commit ID: 2be82b1e66cd188456bbf1e5abb13af04d1629d5#diff-4d2ab44195558d5a9d5f15b8803ef39d",
        "createdAt" : "2020-03-07T11:26:39Z",
        "updatedAt" : "2020-03-07T11:26:39Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "98c3def8275c6c2c0eb86ace24c0408613877c1a",
    "line" : 46,
    "diffHunk" : "@@ -1,1 +88,92 @@  private[spark] val DRIVER_CORES = ConfigBuilder(\"spark.driver.cores\")\n    .doc(\"Number of cores to use for the driver process, only in cluster mode.\")\n    .version(\"1.3.0\")\n    .intConf\n    .createWithDefault(1)"
  },
  {
    "id" : "dd9facc8-f167-4036-851b-c97a1ba60365",
    "prId" : 27847,
    "prUrl" : "https://github.com/apache/spark/pull/27847#pullrequestreview-370739097",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "861f46a9-8d03-4947-960d-d7a96617bf7d",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-3243, commit ID: c1ffa3e4cdfbd1f84b5c8d8de5d0fb958a19e211#diff-4d2ab44195558d5a9d5f15b8803ef39d",
        "createdAt" : "2020-03-07T11:27:01Z",
        "updatedAt" : "2020-03-07T11:27:02Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "98c3def8275c6c2c0eb86ace24c0408613877c1a",
    "line" : 52,
    "diffHunk" : "@@ -1,1 +94,98 @@  private[spark] val DRIVER_MEMORY = ConfigBuilder(SparkLauncher.DRIVER_MEMORY)\n    .doc(\"Amount of memory to use for the driver process, in MiB unless otherwise specified.\")\n    .version(\"1.1.1\")\n    .bytesConf(ByteUnit.MiB)\n    .createWithDefaultString(\"1g\")"
  },
  {
    "id" : "fe6058e1-983f-4f33-b5dc-fd8c5baca7d2",
    "prId" : 27847,
    "prUrl" : "https://github.com/apache/spark/pull/27847#pullrequestreview-370739129",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2a5d7568-6f8c-41db-a709-73749b9050ab",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-22646, commit ID: 3f4060c340d6bac412e8819c4388ccba226efcf3#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-07T11:27:22Z",
        "updatedAt" : "2020-03-07T11:27:22Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "98c3def8275c6c2c0eb86ace24c0408613877c1a",
    "line" : 59,
    "diffHunk" : "@@ -1,1 +101,105 @@    .doc(\"The amount of non-heap memory to be allocated per driver in cluster mode, \" +\n      \"in MiB unless otherwise specified.\")\n    .version(\"2.3.0\")\n    .bytesConf(ByteUnit.MiB)\n    .createOptional"
  },
  {
    "id" : "d4eff228-7f5a-4e34-b7de-6f5567fe4061",
    "prId" : 27847,
    "prUrl" : "https://github.com/apache/spark/pull/27847#pullrequestreview-370739145",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ccbe8075-a0e2-44c8-9e9a-b552dcaaf360",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-25118, commit ID: 5f11e8c4cb9a5db037ac239b8fcc97f3a746e772#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-07T11:27:46Z",
        "updatedAt" : "2020-03-07T11:27:47Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "98c3def8275c6c2c0eb86ace24c0408613877c1a",
    "line" : 65,
    "diffHunk" : "@@ -1,1 +106,110 @@\n  private[spark] val DRIVER_LOG_DFS_DIR =\n    ConfigBuilder(\"spark.driver.log.dfsDir\").version(\"3.0.0\").stringConf.createOptional\n\n  private[spark] val DRIVER_LOG_LAYOUT ="
  },
  {
    "id" : "1a15cc15-48f1-44c7-866e-8d9b64b9b22b",
    "prId" : 27847,
    "prUrl" : "https://github.com/apache/spark/pull/27847#pullrequestreview-370739179",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "05a735f3-2b4d-46ff-ba5d-250195ade0d7",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-25118, commit ID: 5f11e8c4cb9a5db037ac239b8fcc97f3a746e772#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-07T11:28:15Z",
        "updatedAt" : "2020-03-07T11:28:16Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "98c3def8275c6c2c0eb86ace24c0408613877c1a",
    "line" : 69,
    "diffHunk" : "@@ -1,1 +110,114 @@  private[spark] val DRIVER_LOG_LAYOUT =\n    ConfigBuilder(\"spark.driver.log.layout\")\n      .version(\"3.0.0\")\n      .stringConf\n      .createOptional"
  },
  {
    "id" : "61f4247e-e091-44d1-bbb3-903a1bbcc3d0",
    "prId" : 27847,
    "prUrl" : "https://github.com/apache/spark/pull/27847#pullrequestreview-370739193",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1663cd2c-690e-4255-800b-dcc1a7480a68",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-25118, commit ID: 5f11e8c4cb9a5db037ac239b8fcc97f3a746e772#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-07T11:28:42Z",
        "updatedAt" : "2020-03-07T11:30:02Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "98c3def8275c6c2c0eb86ace24c0408613877c1a",
    "line" : 75,
    "diffHunk" : "@@ -1,1 +116,120 @@  private[spark] val DRIVER_LOG_PERSISTTODFS =\n    ConfigBuilder(\"spark.driver.log.persistToDfs.enabled\")\n      .version(\"3.0.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "c71023d3-2803-4703-b581-e16a9258dff1",
    "prId" : 27847,
    "prUrl" : "https://github.com/apache/spark/pull/27847#pullrequestreview-370739263",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3440069e-a9d6-425d-9fd3-8f70065d139e",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-29105, commit ID: 276aaaae8d404975f8701089e9f4dfecd16e0d9f#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-07T11:30:21Z",
        "updatedAt" : "2020-03-07T11:30:22Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "98c3def8275c6c2c0eb86ace24c0408613877c1a",
    "line" : 81,
    "diffHunk" : "@@ -1,1 +122,126 @@  private[spark] val DRIVER_LOG_ALLOW_EC =\n    ConfigBuilder(\"spark.driver.log.allowErasureCoding\")\n      .version(\"3.0.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "e6a3ae48-698f-4df4-8730-3f4cd9dac0c4",
    "prId" : 27847,
    "prUrl" : "https://github.com/apache/spark/pull/27847#pullrequestreview-370739286",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "96a81c68-55a5-41d2-aec9-ef9601e71ad4",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-1132, commit ID: 79d07d66040f206708e14de393ab0b80020ed96a#diff-364713d7776956cb8b0a771e9b62f82d",
        "createdAt" : "2020-03-07T11:30:44Z",
        "updatedAt" : "2020-03-07T11:30:45Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "98c3def8275c6c2c0eb86ace24c0408613877c1a",
    "line" : 86,
    "diffHunk" : "@@ -1,1 +127,131 @@\n  private[spark] val EVENT_LOG_ENABLED = ConfigBuilder(\"spark.eventLog.enabled\")\n    .version(\"1.0.0\")\n    .booleanConf\n    .createWithDefault(false)"
  },
  {
    "id" : "f00f2cf2-7c46-4291-a020-bb977039c731",
    "prId" : 27847,
    "prUrl" : "https://github.com/apache/spark/pull/27847#pullrequestreview-370739299",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d5591a0c-8e2f-43b6-82e7-cd63d4cc6583",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-1132, commit ID: 79d07d66040f206708e14de393ab0b80020ed96a#diff-364713d7776956cb8b0a771e9b62f82d",
        "createdAt" : "2020-03-07T11:31:03Z",
        "updatedAt" : "2020-03-07T11:31:03Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "98c3def8275c6c2c0eb86ace24c0408613877c1a",
    "line" : 91,
    "diffHunk" : "@@ -1,1 +132,136 @@\n  private[spark] val EVENT_LOG_DIR = ConfigBuilder(\"spark.eventLog.dir\")\n    .version(\"1.0.0\")\n    .stringConf\n    .createWithDefault(EventLoggingListener.DEFAULT_LOG_DIR)"
  },
  {
    "id" : "e9d78d0d-66f6-40fe-b1d1-e0fcdc100bb6",
    "prId" : 27847,
    "prUrl" : "https://github.com/apache/spark/pull/27847#pullrequestreview-370739301",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b8b5f1fc-4a2d-4414-b638-49b2b73cc965",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-1132, commit ID: 79d07d66040f206708e14de393ab0b80020ed96a#diff-364713d7776956cb8b0a771e9b62f82d",
        "createdAt" : "2020-03-07T11:31:10Z",
        "updatedAt" : "2020-03-07T11:31:10Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "98c3def8275c6c2c0eb86ace24c0408613877c1a",
    "line" : 97,
    "diffHunk" : "@@ -1,1 +138,142 @@  private[spark] val EVENT_LOG_COMPRESS =\n    ConfigBuilder(\"spark.eventLog.compress\")\n      .version(\"1.0.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "106cd577-f5ed-41ce-a046-36018225ee21",
    "prId" : 27847,
    "prUrl" : "https://github.com/apache/spark/pull/27847#pullrequestreview-370739319",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f0b3ed5d-dac7-4bdf-96e0-50ffe8e273b2",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-22050, commit ID: 1437e344ec0c29a44a19f4513986f5f184c44695#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-07T11:31:35Z",
        "updatedAt" : "2020-03-07T11:31:36Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "98c3def8275c6c2c0eb86ace24c0408613877c1a",
    "line" : 103,
    "diffHunk" : "@@ -1,1 +144,148 @@  private[spark] val EVENT_LOG_BLOCK_UPDATES =\n    ConfigBuilder(\"spark.eventLog.logBlockUpdates.enabled\")\n      .version(\"2.3.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "4816639b-ca38-4d92-8118-b1b788d9fce8",
    "prId" : 27847,
    "prUrl" : "https://github.com/apache/spark/pull/27847#pullrequestreview-370739337",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ad368a82-386b-462e-8c20-b79bb6472716",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-25855, commit ID: 35506dced739ef16136e9f3d5d48c638899d3cec#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-07T11:31:57Z",
        "updatedAt" : "2020-03-07T11:31:57Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "98c3def8275c6c2c0eb86ace24c0408613877c1a",
    "line" : 109,
    "diffHunk" : "@@ -1,1 +150,154 @@  private[spark] val EVENT_LOG_ALLOW_EC =\n    ConfigBuilder(\"spark.eventLog.erasureCoding.enabled\")\n      .version(\"3.0.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "8270decc-4687-49bf-9d0f-77c41d97d0fa",
    "prId" : 27847,
    "prUrl" : "https://github.com/apache/spark/pull/27847#pullrequestreview-370739355",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0d708b69-692a-48b7-b04c-c59a99e72e9f",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "No JIRA ID, commit ID: d4c8af87994acf3707027e6fab25363f51fd4615#diff-e4a5a68c15eed95d038acfed84b0b66a",
        "createdAt" : "2020-03-07T11:32:24Z",
        "updatedAt" : "2020-03-07T11:32:25Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "98c3def8275c6c2c0eb86ace24c0408613877c1a",
    "line" : 116,
    "diffHunk" : "@@ -1,1 +157,161 @@    ConfigBuilder(\"spark.eventLog.testing\")\n      .internal()\n      .version(\"1.0.1\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "187ab60b-71c9-43b8-bc94-73dde6dae875",
    "prId" : 27847,
    "prUrl" : "https://github.com/apache/spark/pull/27847#pullrequestreview-370739376",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2044471b-3bd2-45e6-acb7-e3b6e91bc890",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-1132, commit ID: 79d07d66040f206708e14de393ab0b80020ed96a#diff-364713d7776956cb8b0a771e9b62f82d",
        "createdAt" : "2020-03-07T11:32:47Z",
        "updatedAt" : "2020-03-07T11:32:47Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "98c3def8275c6c2c0eb86ace24c0408613877c1a",
    "line" : 122,
    "diffHunk" : "@@ -1,1 +163,167 @@  private[spark] val EVENT_LOG_OUTPUT_BUFFER_SIZE = ConfigBuilder(\"spark.eventLog.buffer.kb\")\n    .doc(\"Buffer size to use when writing to output streams, in KiB unless otherwise specified.\")\n    .version(\"1.0.0\")\n    .bytesConf(ByteUnit.KiB)\n    .createWithDefaultString(\"100k\")"
  },
  {
    "id" : "3a77f1c4-a538-4e2e-9fb7-e4b7abb58ff6",
    "prId" : 27847,
    "prUrl" : "https://github.com/apache/spark/pull/27847#pullrequestreview-370739392",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d6736fc3-ac7e-4b45-94da-87f174d079e6",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-30812, commit ID: 68d7edf9497bea2f73707d32ab55dd8e53088e7c#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-07T11:33:13Z",
        "updatedAt" : "2020-03-07T11:33:13Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "98c3def8275c6c2c0eb86ace24c0408613877c1a",
    "line" : 130,
    "diffHunk" : "@@ -1,1 +171,175 @@      .doc(\"Whether to write per-stage peaks of executor metrics (for each executor) \" +\n        \"to the event log.\")\n      .version(\"3.0.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "032aa554-2dbb-4442-b526-95be0dc989d7",
    "prId" : 27847,
    "prUrl" : "https://github.com/apache/spark/pull/27847#pullrequestreview-370739410",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c706cf38-77a8-4712-8e66-caf59bf4f821",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-25865, commit ID: e5c502c596563dce8eb58f86e42c1aea2c51ed17#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-07T11:33:41Z",
        "updatedAt" : "2020-03-07T11:33:41Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "98c3def8275c6c2c0eb86ace24c0408613877c1a",
    "line" : 138,
    "diffHunk" : "@@ -1,1 +180,184 @@        \" the return of GarbageCollectorMXBean.getName. The built-in young generation garbage \" +\n        s\"collectors are ${GarbageCollectionMetrics.YOUNG_GENERATION_BUILTIN_GARBAGE_COLLECTORS}\")\n      .version(\"3.0.0\")\n      .stringConf\n      .toSequence"
  },
  {
    "id" : "0d4921c2-8105-49f8-93ca-f8e875ec0bc8",
    "prId" : 27847,
    "prUrl" : "https://github.com/apache/spark/pull/27847#pullrequestreview-370739423",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8bdc4e15-733a-4898-aab9-a97723070920",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-25865, commit ID: e5c502c596563dce8eb58f86e42c1aea2c51ed17#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-07T11:33:49Z",
        "updatedAt" : "2020-03-07T11:33:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "98c3def8275c6c2c0eb86ace24c0408613877c1a",
    "line" : 146,
    "diffHunk" : "@@ -1,1 +190,194 @@        \"the return of GarbageCollectorMXBean.getName. The built-in old generation garbage \" +\n        s\"collectors are ${GarbageCollectionMetrics.OLD_GENERATION_BUILTIN_GARBAGE_COLLECTORS}\")\n      .version(\"3.0.0\")\n      .stringConf\n      .toSequence"
  },
  {
    "id" : "8f0fc0d4-58ad-483d-84f1-6cd5c212bddd",
    "prId" : 27847,
    "prUrl" : "https://github.com/apache/spark/pull/27847#pullrequestreview-370739462",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "038cad30-908c-47b9-97ed-d8c0b7165466",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-1132, commit ID: 79d07d66040f206708e14de393ab0b80020ed96a#diff-364713d7776956cb8b0a771e9b62f82d",
        "createdAt" : "2020-03-07T11:34:12Z",
        "updatedAt" : "2020-03-07T11:34:13Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "98c3def8275c6c2c0eb86ace24c0408613877c1a",
    "line" : 154,
    "diffHunk" : "@@ -1,1 +197,201 @@  private[spark] val EVENT_LOG_OVERWRITE =\n    ConfigBuilder(\"spark.eventLog.overwrite\")\n      .version(\"1.0.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "3a85f339-3c42-4986-8b14-47be4411178f",
    "prId" : 27847,
    "prUrl" : "https://github.com/apache/spark/pull/27847#pullrequestreview-370739470",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7f16d67b-3a39-487c-94f5-b16c75f8933c",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-23820, commit ID: 71f70130f1b2b4ec70595627f0a02a88e2c0e27d#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-07T11:34:33Z",
        "updatedAt" : "2020-03-07T11:34:33Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "98c3def8275c6c2c0eb86ace24c0408613877c1a",
    "line" : 161,
    "diffHunk" : "@@ -1,1 +203,207 @@  private[spark] val EVENT_LOG_CALLSITE_LONG_FORM =\n    ConfigBuilder(\"spark.eventLog.longForm.enabled\")\n      .version(\"2.4.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "35ac491d-a410-4b99-9749-9f5203e578c6",
    "prId" : 27847,
    "prUrl" : "https://github.com/apache/spark/pull/27847#pullrequestreview-370739496",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1dfa4907-8caf-488d-b8c3-8a772f915cff",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-28869, commit ID: 100fc58da54e026cda87832a10e2d06eaeccdf87#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-07T11:35:10Z",
        "updatedAt" : "2020-03-07T11:35:11Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "98c3def8275c6c2c0eb86ace24c0408613877c1a",
    "line" : 169,
    "diffHunk" : "@@ -1,1 +211,215 @@      .doc(\"Whether rolling over event log files is enabled. If set to true, it cuts down \" +\n        \"each event log file to the configured size.\")\n      .version(\"3.0.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "2dbf859a-c336-411b-8ac6-c83b9a04e7e7",
    "prId" : 27847,
    "prUrl" : "https://github.com/apache/spark/pull/27847#pullrequestreview-370739502",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e8acd36e-649e-4265-b5d3-f094fdc4a3fd",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-28869, commit ID: 100fc58da54e026cda87832a10e2d06eaeccdf87#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-07T11:35:18Z",
        "updatedAt" : "2020-03-07T11:35:18Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "98c3def8275c6c2c0eb86ace24c0408613877c1a",
    "line" : 177,
    "diffHunk" : "@@ -1,1 +219,223 @@      .doc(s\"When ${EVENT_LOG_ENABLE_ROLLING.key}=true, specifies the max size of event log file\" +\n        \" to be rolled over.\")\n      .version(\"3.0.0\")\n      .bytesConf(ByteUnit.BYTE)\n      .checkValue(_ >= ByteUnit.MiB.toBytes(10), \"Max file size of event log should be \" +"
  },
  {
    "id" : "da3f4b47-2eb9-4ec0-b07e-68116ce37a22",
    "prId" : 27847,
    "prUrl" : "https://github.com/apache/spark/pull/27847#pullrequestreview-370739520",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8a16c5da-c4c2-4c8d-b08e-1ed5a400e856",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-3377, commit ID: 79e45c9323455a51f25ed9acd0edd8682b4bbb88#diff-364713d7776956cb8b0a771e9b62f82d",
        "createdAt" : "2020-03-07T11:35:42Z",
        "updatedAt" : "2020-03-07T11:35:43Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "98c3def8275c6c2c0eb86ace24c0408613877c1a",
    "line" : 185,
    "diffHunk" : "@@ -1,1 +226,230 @@\n  private[spark] val EXECUTOR_ID =\n    ConfigBuilder(\"spark.executor.id\").version(\"1.2.0\").stringConf.createOptional\n\n  private[spark] val EXECUTOR_CLASS_PATH ="
  },
  {
    "id" : "3917b8e9-dbf8-44b2-91f6-b310d26a830e",
    "prId" : 27847,
    "prUrl" : "https://github.com/apache/spark/pull/27847#pullrequestreview-370739533",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "68c73f35-ff31-4084-9f7a-a549cfd5b4b8",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "No JIRA ID, commit ID: 29ee101c73bf066bf7f4f8141c475b8d1bd3cf1c#diff-4d2ab44195558d5a9d5f15b8803ef39d",
        "createdAt" : "2020-03-07T11:36:09Z",
        "updatedAt" : "2020-03-07T11:36:10Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "98c3def8275c6c2c0eb86ace24c0408613877c1a",
    "line" : 190,
    "diffHunk" : "@@ -1,1 +230,234 @@  private[spark] val EXECUTOR_CLASS_PATH =\n    ConfigBuilder(SparkLauncher.EXECUTOR_EXTRA_CLASSPATH)\n      .version(\"1.0.0\")\n      .stringConf\n      .createOptional"
  },
  {
    "id" : "d819a8c7-3f36-4beb-a61b-43d21bbcb6e9",
    "prId" : 27847,
    "prUrl" : "https://github.com/apache/spark/pull/27847#pullrequestreview-370739559",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ccb7b40d-38bd-41ad-8513-f1d0c0e5286a",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-25449, commit ID: 9362c5cc273fdd09f9b3b512e2f6b64bcefc25ab#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-07T11:36:33Z",
        "updatedAt" : "2020-03-07T11:36:33Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "98c3def8275c6c2c0eb86ace24c0408613877c1a",
    "line" : 197,
    "diffHunk" : "@@ -1,1 +237,241 @@    ConfigBuilder(\"spark.executor.heartbeat.dropZeroAccumulatorUpdates\")\n      .internal()\n      .version(\"3.0.0\")\n      .booleanConf\n      .createWithDefault(true)"
  },
  {
    "id" : "d6588e82-0d43-4d22-a050-cbdc7df7ec6f",
    "prId" : 27847,
    "prUrl" : "https://github.com/apache/spark/pull/27847#pullrequestreview-370739577",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f2b48f8c-eae6-4e32-8fc9-4517b0f5ec3a",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-2099, commit ID: 8d338f64c4eda45d22ae33f61ef7928011cc2846#diff-5a0de266c82b95adb47d9bca714e1f1b",
        "createdAt" : "2020-03-07T11:36:56Z",
        "updatedAt" : "2020-03-07T11:36:57Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "98c3def8275c6c2c0eb86ace24c0408613877c1a",
    "line" : 203,
    "diffHunk" : "@@ -1,1 +243,247 @@  private[spark] val EXECUTOR_HEARTBEAT_INTERVAL =\n    ConfigBuilder(\"spark.executor.heartbeatInterval\")\n      .version(\"1.1.0\")\n      .timeConf(TimeUnit.MILLISECONDS)\n      .createWithDefaultString(\"10s\")"
  },
  {
    "id" : "70439ece-cc8d-4ffb-b06e-9a909f18f762",
    "prId" : 27847,
    "prUrl" : "https://github.com/apache/spark/pull/27847#pullrequestreview-370739590",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5a2cd18b-a92d-4906-b5ab-04b37fe04d04",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-13522, commit ID: 86bf93e65481b8fe5d7532ca6d4cd29cafc9e9dd#diff-5a0de266c82b95adb47d9bca714e1f1b",
        "createdAt" : "2020-03-07T11:37:20Z",
        "updatedAt" : "2020-03-07T11:37:21Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "98c3def8275c6c2c0eb86ace24c0408613877c1a",
    "line" : 211,
    "diffHunk" : "@@ -1,1 +250,254 @@    ConfigBuilder(\"spark.executor.heartbeat.maxFailures\")\n      .internal()\n      .version(\"1.6.2\")\n      .intConf\n      .createWithDefault(60)"
  },
  {
    "id" : "d79790a6-7c40-49ce-96f0-569f4ad76ded",
    "prId" : 27847,
    "prUrl" : "https://github.com/apache/spark/pull/27847#pullrequestreview-370739617",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "86f1352e-c08e-4a5a-8592-c459d01817c1",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-27324, commit ID: 387ce89a0631f1a4c6668b90ff2a7bbcf11919cd#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-07T11:38:05Z",
        "updatedAt" : "2020-03-07T11:38:05Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "98c3def8275c6c2c0eb86ace24c0408613877c1a",
    "line" : 219,
    "diffHunk" : "@@ -1,1 +258,262 @@      .doc(\"Whether to collect process tree metrics (from the /proc filesystem) when collecting \" +\n        \"executor metrics.\")\n      .version(\"3.0.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "ae30eea1-1767-4b5f-a054-6346b6dc03b5",
    "prId" : 27847,
    "prUrl" : "https://github.com/apache/spark/pull/27847#pullrequestreview-370739635",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "06eb4b04-4265-4cd4-a063-9acc570ed908",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-26329, commit ID: 80ab19b9fd268adfc419457f12b99a5da7b6d1c7#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-07T11:38:30Z",
        "updatedAt" : "2020-03-07T11:38:31Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "98c3def8275c6c2c0eb86ace24c0408613877c1a",
    "line" : 227,
    "diffHunk" : "@@ -1,1 +267,271 @@        \"If 0, the polling is done on executor heartbeats. \" +\n        \"If positive, the polling is done at this interval.\")\n      .version(\"3.0.0\")\n      .timeConf(TimeUnit.MILLISECONDS)\n      .createWithDefaultString(\"0\")"
  },
  {
    "id" : "61e39154-93aa-458d-858b-e6dbbfded67b",
    "prId" : 27847,
    "prUrl" : "https://github.com/apache/spark/pull/27847#pullrequestreview-370739652",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f6488bd2-a0a9-422b-a139-456bfa1741bd",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "No JIRA ID, commit ID: 29ee101c73bf066bf7f4f8141c475b8d1bd3cf1c#diff-4d2ab44195558d5a9d5f15b8803ef39d",
        "createdAt" : "2020-03-07T11:39:09Z",
        "updatedAt" : "2020-03-07T11:39:09Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "98c3def8275c6c2c0eb86ace24c0408613877c1a",
    "line" : 234,
    "diffHunk" : "@@ -1,1 +274,278 @@    ConfigBuilder(SparkLauncher.EXECUTOR_EXTRA_JAVA_OPTIONS)\n      .withPrepended(SparkLauncher.EXECUTOR_DEFAULT_JAVA_OPTIONS)\n      .version(\"1.0.0\")\n      .stringConf\n      .createOptional"
  },
  {
    "id" : "833b154d-0dca-4a33-9e45-f4f0e7c9fdb5",
    "prId" : 27847,
    "prUrl" : "https://github.com/apache/spark/pull/27847#pullrequestreview-370739653",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "497ae077-eac3-4863-862d-20282b0fe065",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "No JIRA ID, commit ID: 29ee101c73bf066bf7f4f8141c475b8d1bd3cf1c#diff-4d2ab44195558d5a9d5f15b8803ef39d",
        "createdAt" : "2020-03-07T11:39:16Z",
        "updatedAt" : "2020-03-07T11:39:16Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "98c3def8275c6c2c0eb86ace24c0408613877c1a",
    "line" : 241,
    "diffHunk" : "@@ -1,1 +280,284 @@  private[spark] val EXECUTOR_LIBRARY_PATH =\n    ConfigBuilder(SparkLauncher.EXECUTOR_EXTRA_LIBRARY_PATH)\n      .version(\"1.0.0\")\n      .stringConf\n      .createOptional"
  },
  {
    "id" : "4477f64f-9ba0-4b9c-ad52-a51bbeb59a6b",
    "prId" : 27847,
    "prUrl" : "https://github.com/apache/spark/pull/27847#pullrequestreview-370739705",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5404d796-3c1c-4648-bc76-29c5375123a1",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-2996, commit ID: 6a1e0f967286945db13d94aeb6ed19f0a347c236#diff-529fc5c06b9731c1fbda6f3db60b16aa",
        "createdAt" : "2020-03-07T11:39:58Z",
        "updatedAt" : "2020-03-07T11:39:58Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "98c3def8275c6c2c0eb86ace24c0408613877c1a",
    "line" : 248,
    "diffHunk" : "@@ -1,1 +286,290 @@  private[spark] val EXECUTOR_USER_CLASS_PATH_FIRST =\n    ConfigBuilder(\"spark.executor.userClassPathFirst\")\n      .version(\"1.3.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "0856a01a-6ccb-4022-8890-a39ffc0a4718",
    "prId" : 27847,
    "prUrl" : "https://github.com/apache/spark/pull/27847#pullrequestreview-370739724",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "33ee1a78-e367-4b5a-8993-b73100366622",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-1126, commit ID: 1617816090e7b20124a512a43860a21232ebf511#diff-4d2ab44195558d5a9d5f15b8803ef39d",
        "createdAt" : "2020-03-07T11:40:24Z",
        "updatedAt" : "2020-03-07T11:40:24Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "98c3def8275c6c2c0eb86ace24c0408613877c1a",
    "line" : 253,
    "diffHunk" : "@@ -1,1 +291,295 @@\n  private[spark] val EXECUTOR_CORES = ConfigBuilder(SparkLauncher.EXECUTOR_CORES)\n    .version(\"1.0.0\")\n    .intConf\n    .createWithDefault(1)"
  },
  {
    "id" : "466a9ceb-d915-4a35-867e-7b1bac33cf93",
    "prId" : 27847,
    "prUrl" : "https://github.com/apache/spark/pull/27847#pullrequestreview-370739753",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2040327f-dae3-41c1-8240-c3f932a0dfb4",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "No JIRA ID, commit ID: 696eec32c982ca516c506de33f383a173bcbd131#diff-4f50ad37deb6742ad45472636c9a870b",
        "createdAt" : "2020-03-07T11:40:53Z",
        "updatedAt" : "2020-03-07T11:40:53Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "98c3def8275c6c2c0eb86ace24c0408613877c1a",
    "line" : 259,
    "diffHunk" : "@@ -1,1 +297,301 @@  private[spark] val EXECUTOR_MEMORY = ConfigBuilder(SparkLauncher.EXECUTOR_MEMORY)\n    .doc(\"Amount of memory to use per executor process, in MiB unless otherwise specified.\")\n    .version(\"0.7.0\")\n    .bytesConf(ByteUnit.MiB)\n    .createWithDefaultString(\"1g\")"
  },
  {
    "id" : "d4276077-8991-4982-b0f6-0d22f8f52118",
    "prId" : 27847,
    "prUrl" : "https://github.com/apache/spark/pull/27847#pullrequestreview-370740619",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d638a071-2a04-48ef-82b8-26b84d75a51e",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-22646, commit ID: 3f4060c340d6bac412e8819c4388ccba226efcf3#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-07T12:02:53Z",
        "updatedAt" : "2020-03-07T12:02:53Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "98c3def8275c6c2c0eb86ace24c0408613877c1a",
    "line" : 266,
    "diffHunk" : "@@ -1,1 +304,308 @@    .doc(\"The amount of non-heap memory to be allocated per executor in cluster mode, \" +\n      \"in MiB unless otherwise specified.\")\n    .version(\"2.3.0\")\n    .bytesConf(ByteUnit.MiB)\n    .createOptional"
  },
  {
    "id" : "7c9b367c-73dd-4dac-a678-21e2060ca573",
    "prId" : 27847,
    "prUrl" : "https://github.com/apache/spark/pull/27847#pullrequestreview-370740636",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8b3e41aa-738c-4d13-91e1-dc547b28cebd",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "No JIRA ID, commit ID: 0a472840030e4e7e84fe748f7bfa49f1ece599c5#diff-b6cc54c092b861f645c3cd69ea0f91e2",
        "createdAt" : "2020-03-07T12:03:17Z",
        "updatedAt" : "2020-03-07T12:03:17Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "98c3def8275c6c2c0eb86ace24c0408613877c1a",
    "line" : 274,
    "diffHunk" : "@@ -1,1 +314,318 @@      \"`spark.deploy.defaultCores` on Spark's standalone cluster manager, or infinite \" +\n      \"(all available cores) on Mesos.\")\n    .version(\"0.6.0\")\n    .intConf\n    .createOptional"
  },
  {
    "id" : "24ca3ff5-1f8d-4e01-9d44-c0617a686582",
    "prId" : 27847,
    "prUrl" : "https://github.com/apache/spark/pull/27847#pullrequestreview-370740658",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f63ce544-e963-4d8e-8ae5-9c908eef3dce",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-12251, commit ID: 9870e5c7af87190167ca3845ede918671b9420ca#diff-529fc5c06b9731c1fbda6f3db60b16aa",
        "createdAt" : "2020-03-07T12:03:42Z",
        "updatedAt" : "2020-03-07T12:03:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "98c3def8275c6c2c0eb86ace24c0408613877c1a",
    "line" : 281,
    "diffHunk" : "@@ -1,1 +321,325 @@    .doc(\"If true, Spark will attempt to use off-heap memory for certain operations. \" +\n      \"If off-heap memory use is enabled, then spark.memory.offHeap.size must be positive.\")\n    .version(\"1.6.0\")\n    .withAlternative(\"spark.unsafe.offHeap\")\n    .booleanConf"
  },
  {
    "id" : "4f100d82-21e0-4502-8a6e-fc140fca5656",
    "prId" : 27847,
    "prUrl" : "https://github.com/apache/spark/pull/27847#pullrequestreview-370740669",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a27c6187-80ab-4209-9f5c-11d818ae714a",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-12251, commit ID: 9870e5c7af87190167ca3845ede918671b9420ca#diff-529fc5c06b9731c1fbda6f3db60b16aa",
        "createdAt" : "2020-03-07T12:04:01Z",
        "updatedAt" : "2020-03-07T12:04:02Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "98c3def8275c6c2c0eb86ace24c0408613877c1a",
    "line" : 289,
    "diffHunk" : "@@ -1,1 +332,336 @@      \"consumption must fit within some hard limit then be sure to shrink your JVM heap size \" +\n      \"accordingly. This must be set to a positive value when spark.memory.offHeap.enabled=true.\")\n    .version(\"1.6.0\")\n    .bytesConf(ByteUnit.BYTE)\n    .checkValue(_ >= 0, \"The off-heap memory size must not be negative\")"
  },
  {
    "id" : "9decaf3d-a6b9-4364-9c80-89c53c350785",
    "prId" : 27847,
    "prUrl" : "https://github.com/apache/spark/pull/27847#pullrequestreview-370740687",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8f91d8fd-ba7b-4ad7-9c5f-b9dc4047c9d1",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-10983, commit ID: b3ffac5178795f2d8e7908b3e77e8e89f50b5f6f#diff-529fc5c06b9731c1fbda6f3db60b16aa",
        "createdAt" : "2020-03-07T12:04:31Z",
        "updatedAt" : "2020-03-07T12:04:31Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "98c3def8275c6c2c0eb86ace24c0408613877c1a",
    "line" : 297,
    "diffHunk" : "@@ -1,1 +342,346 @@      \"less working memory may be available to execution and tasks may spill to disk more \" +\n      \"often. Leaving this at the default value is recommended. \")\n    .version(\"1.6.0\")\n    .doubleConf\n    .checkValue(v => v >= 0.0 && v < 1.0, \"Storage fraction must be in [0,1)\")"
  },
  {
    "id" : "fe5da79e-0bc7-42a2-99f4-f5317abb6515",
    "prId" : 27847,
    "prUrl" : "https://github.com/apache/spark/pull/27847#pullrequestreview-370740693",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8941ba0f-a3ef-4810-a019-e6417c6a818d",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-10983, commit ID: b3ffac5178795f2d8e7908b3e77e8e89f50b5f6f#diff-529fc5c06b9731c1fbda6f3db60b16aa",
        "createdAt" : "2020-03-07T12:04:41Z",
        "updatedAt" : "2020-03-07T12:04:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "98c3def8275c6c2c0eb86ace24c0408613877c1a",
    "line" : 305,
    "diffHunk" : "@@ -1,1 +353,357 @@      \"user data structures, and imprecise size estimation in the case of sparse, \" +\n      \"unusually large records. Leaving this at the default value is recommended.  \")\n    .version(\"1.6.0\")\n    .doubleConf\n    .createWithDefault(0.6)"
  },
  {
    "id" : "16922df9-156e-4812-87fe-fd9480120e35",
    "prId" : 27847,
    "prUrl" : "https://github.com/apache/spark/pull/27847#pullrequestreview-370740709",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "751d5203-fa91-4fe9-8d3b-d7ab8212daf0",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-1777, commit ID: ecf30ee7e78ea59c462c54db0fde5328f997466c#diff-2b643ea78c1add0381754b1f47eec132",
        "createdAt" : "2020-03-07T12:05:10Z",
        "updatedAt" : "2020-03-07T12:05:10Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "98c3def8275c6c2c0eb86ace24c0408613877c1a",
    "line" : 310,
    "diffHunk" : "@@ -1,1 +358,362 @@\n  private[spark] val STORAGE_SAFETY_FRACTION = ConfigBuilder(\"spark.storage.safetyFraction\")\n    .version(\"1.1.0\")\n    .doubleConf\n    .createWithDefault(0.9)"
  },
  {
    "id" : "2229b286-0887-4888-99ce-6e00207e2266",
    "prId" : 27847,
    "prUrl" : "https://github.com/apache/spark/pull/27847#pullrequestreview-370740722",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "295fe2be-554a-4b78-be7a-61a6dd32baf5",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-1777, commit ID: ecf30ee7e78ea59c462c54db0fde5328f997466c#diff-692a329b5a7fb4134c55d559457b94e4",
        "createdAt" : "2020-03-07T12:05:32Z",
        "updatedAt" : "2020-03-07T12:05:32Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "98c3def8275c6c2c0eb86ace24c0408613877c1a",
    "line" : 317,
    "diffHunk" : "@@ -1,1 +365,369 @@    ConfigBuilder(\"spark.storage.unrollMemoryThreshold\")\n      .doc(\"Initial memory to request before unrolling any block\")\n      .version(\"1.1.0\")\n      .longConf\n      .createWithDefault(1024 * 1024)"
  },
  {
    "id" : "caaeef7d-2b73-4392-bd04-1544dabcacc5",
    "prId" : 27847,
    "prUrl" : "https://github.com/apache/spark/pull/27847#pullrequestreview-370740742",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1bff860d-0807-4984-94bb-6e7b8bf33a53",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-15355, commit ID: fa7c582e9442b985a0493fb1dd15b3fb9b6031b4#diff-186864190089a718680accb51de5f0d4",
        "createdAt" : "2020-03-07T12:05:57Z",
        "updatedAt" : "2020-03-07T12:05:57Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "98c3def8275c6c2c0eb86ace24c0408613877c1a",
    "line" : 325,
    "diffHunk" : "@@ -1,1 +375,379 @@        \"if there are any existing available replicas. This tries to \" +\n        \"get the replication level of the block to the initial number\")\n      .version(\"2.2.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "7f0a1b41-e545-4799-bbdd-3eb5208986d1",
    "prId" : 27847,
    "prUrl" : "https://github.com/apache/spark/pull/27847#pullrequestreview-370740759",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b8c179f9-ea65-49f8-8674-62db0c44f4b4",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-1145, commit ID: 76339495153dd895667ad609815c887b2c8960ea#diff-abd96f2ae793cd6ea6aab5b96a3c1d7a",
        "createdAt" : "2020-03-07T12:06:23Z",
        "updatedAt" : "2020-03-07T12:06:23Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "98c3def8275c6c2c0eb86ace24c0408613877c1a",
    "line" : 333,
    "diffHunk" : "@@ -1,1 +386,390 @@        \"In general, memory mapping has high overhead for blocks close to or below \" +\n        \"the page size of the operating system.\")\n      .version(\"0.9.2\")\n      .bytesConf(ByteUnit.BYTE)\n      .createWithDefaultString(\"2m\")"
  },
  {
    "id" : "ebafc817-39a3-4536-bb50-416365e045b6",
    "prId" : 27847,
    "prUrl" : "https://github.com/apache/spark/pull/27847#pullrequestreview-370740775",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0ab805d2-782b-45b3-ac6f-b3b477129de3",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-15353, commit ID: a26afd52198523dbd51dc94053424494638c7de5#diff-2b643ea78c1add0381754b1f47eec132",
        "createdAt" : "2020-03-07T12:06:45Z",
        "updatedAt" : "2020-03-07T12:06:45Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "98c3def8275c6c2c0eb86ace24c0408613877c1a",
    "line" : 339,
    "diffHunk" : "@@ -1,1 +392,396 @@  private[spark] val STORAGE_REPLICATION_POLICY =\n    ConfigBuilder(\"spark.storage.replication.policy\")\n      .version(\"2.1.0\")\n      .stringConf\n      .createWithDefaultString(classOf[RandomBlockReplicationPolicy].getName)"
  },
  {
    "id" : "24c6e9d2-6899-4eab-9241-bfb22e072b66",
    "prId" : 27847,
    "prUrl" : "https://github.com/apache/spark/pull/27847#pullrequestreview-370740791",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c5765c36-7e4a-4b14-9636-1c12d2acf91e",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-15353, commit ID: a26afd52198523dbd51dc94053424494638c7de5#diff-186864190089a718680accb51de5f0d4",
        "createdAt" : "2020-03-07T12:07:04Z",
        "updatedAt" : "2020-03-07T12:07:04Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "98c3def8275c6c2c0eb86ace24c0408613877c1a",
    "line" : 345,
    "diffHunk" : "@@ -1,1 +398,402 @@  private[spark] val STORAGE_REPLICATION_TOPOLOGY_MAPPER =\n    ConfigBuilder(\"spark.storage.replication.topologyMapper\")\n      .version(\"2.1.0\")\n      .stringConf\n      .createWithDefaultString(classOf[DefaultTopologyMapper].getName)"
  },
  {
    "id" : "7227be96-6071-40cd-85aa-f2d9cdc6d795",
    "prId" : 27847,
    "prUrl" : "https://github.com/apache/spark/pull/27847#pullrequestreview-370740804",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2c9fce95-9acb-43d4-8aef-1f90d762a8e0",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-3495 and SPARK-3496, commit ID: be0cc9952d6c8b4cfe9ff10a761e0677cba64489#diff-2b643ea78c1add0381754b1f47eec132",
        "createdAt" : "2020-03-07T12:07:30Z",
        "updatedAt" : "2020-03-07T12:07:30Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "98c3def8275c6c2c0eb86ace24c0408613877c1a",
    "line" : 351,
    "diffHunk" : "@@ -1,1 +403,407 @@\n  private[spark] val STORAGE_CACHED_PEERS_TTL = ConfigBuilder(\"spark.storage.cachedPeersTtl\")\n    .version(\"1.1.1\")\n    .intConf\n    .createWithDefault(60 * 1000)"
  },
  {
    "id" : "629ed170-ebe3-4ece-97a5-9abc33f6648f",
    "prId" : 27847,
    "prUrl" : "https://github.com/apache/spark/pull/27847#pullrequestreview-370740815",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "882d663a-05b9-4a57-81de-f3671dfc3d32",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-3495 and SPARK-3496, commit ID: be0cc9952d6c8b4cfe9ff10a761e0677cba64489#diff-2b643ea78c1add0381754b1f47eec132",
        "createdAt" : "2020-03-07T12:07:51Z",
        "updatedAt" : "2020-03-07T12:07:51Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "98c3def8275c6c2c0eb86ace24c0408613877c1a",
    "line" : 358,
    "diffHunk" : "@@ -1,1 +409,413 @@  private[spark] val STORAGE_MAX_REPLICATION_FAILURE =\n    ConfigBuilder(\"spark.storage.maxReplicationFailures\")\n      .version(\"1.1.1\")\n      .intConf\n      .createWithDefault(1)"
  },
  {
    "id" : "8a05382a-653f-487f-b570-7b1e1277885b",
    "prId" : 27847,
    "prUrl" : "https://github.com/apache/spark/pull/27847#pullrequestreview-370740831",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b1f20d98-18ac-45f3-9858-2351d2a7d1eb",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-15353, commit ID: a26afd52198523dbd51dc94053424494638c7de5#diff-e550ce522c12a31d805a7d0f41e802af",
        "createdAt" : "2020-03-07T12:08:19Z",
        "updatedAt" : "2020-03-07T12:08:20Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "98c3def8275c6c2c0eb86ace24c0408613877c1a",
    "line" : 364,
    "diffHunk" : "@@ -1,1 +414,418 @@\n  private[spark] val STORAGE_REPLICATION_TOPOLOGY_FILE =\n    ConfigBuilder(\"spark.storage.replication.topologyFile\")\n      .version(\"2.1.0\")\n      .stringConf"
  },
  {
    "id" : "65b934c2-e0b5-4f78-a880-910bc626dcf4",
    "prId" : 27847,
    "prUrl" : "https://github.com/apache/spark/pull/27847#pullrequestreview-370740846",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e4b59dbd-4e50-4621-93d1-d4691edcf28b",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-13566, commit ID: ab006523b840b1d2dbf3f5ff0a238558e7665a1e#diff-5a0de266c82b95adb47d9bca714e1f1b",
        "createdAt" : "2020-03-07T12:08:43Z",
        "updatedAt" : "2020-03-07T12:08:44Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "98c3def8275c6c2c0eb86ace24c0408613877c1a",
    "line" : 371,
    "diffHunk" : "@@ -1,1 +421,425 @@  private[spark] val STORAGE_EXCEPTION_PIN_LEAK =\n    ConfigBuilder(\"spark.storage.exceptionOnPinLeak\")\n      .version(\"1.6.2\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "e376f955-2b71-4d7d-bdd5-e389b7df9ce2",
    "prId" : 27847,
    "prUrl" : "https://github.com/apache/spark/pull/27847#pullrequestreview-370740869",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5c8b3e3c-655c-4b3c-aa86-f8443a5d0111",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "No JIRA ID, commit ID: 9085ebf3750c7d9bb7c6b5f6b4bdc5b807af93c2#diff-76170a9c8f67b542bc58240a0a12fe08",
        "createdAt" : "2020-03-07T12:09:18Z",
        "updatedAt" : "2020-03-07T12:09:18Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "98c3def8275c6c2c0eb86ace24c0408613877c1a",
    "line" : 377,
    "diffHunk" : "@@ -1,1 +427,431 @@  private[spark] val STORAGE_BLOCKMANAGER_TIMEOUTINTERVAL =\n    ConfigBuilder(\"spark.storage.blockManagerTimeoutIntervalMs\")\n      .version(\"0.7.3\")\n      .timeConf(TimeUnit.MILLISECONDS)\n      .createWithDefaultString(\"60s\")"
  },
  {
    "id" : "70dc4fee-f7eb-4389-9902-d7069ab4d109",
    "prId" : 27847,
    "prUrl" : "https://github.com/apache/spark/pull/27847#pullrequestreview-370740878",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a542a08a-a817-4a9c-98aa-81df9a71f7a0",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "No JIRA ID, commit ID: 97434f49b8c029e9b78c91ec5f58557cd1b5c943#diff-2ce6374aac24d70c69182b067216e684",
        "createdAt" : "2020-03-07T12:09:43Z",
        "updatedAt" : "2020-03-07T12:09:43Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "98c3def8275c6c2c0eb86ace24c0408613877c1a",
    "line" : 383,
    "diffHunk" : "@@ -1,1 +433,437 @@  private[spark] val STORAGE_BLOCKMANAGER_SLAVE_TIMEOUT =\n    ConfigBuilder(\"spark.storage.blockManagerSlaveTimeoutMs\")\n      .version(\"0.7.0\")\n      .timeConf(TimeUnit.MILLISECONDS)\n      .createWithDefaultString(Network.NETWORK_TIMEOUT.defaultValueString)"
  },
  {
    "id" : "6f0e5f12-61bb-4948-a685-cfad630bf70a",
    "prId" : 27847,
    "prUrl" : "https://github.com/apache/spark/pull/27847#pullrequestreview-370740892",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "799ace3f-7f51-4d7b-8dee-a0d9cf940564",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-24340, commit ID: 8ef167a5f9ba8a79bb7ca98a9844fe9cfcfea060#diff-916ca56b663f178f302c265b7ef38499",
        "createdAt" : "2020-03-07T12:10:11Z",
        "updatedAt" : "2020-03-07T12:10:11Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "98c3def8275c6c2c0eb86ace24c0408613877c1a",
    "line" : 391,
    "diffHunk" : "@@ -1,1 +441,445 @@      .doc(\"Whether or not cleanup the files not served by the external shuffle service \" +\n        \"on executor exits.\")\n      .version(\"2.4.0\")\n      .booleanConf\n      .createWithDefault(true)"
  },
  {
    "id" : "d350b989-fecf-499b-85c9-31d4f3634655",
    "prId" : 27847,
    "prUrl" : "https://github.com/apache/spark/pull/27847#pullrequestreview-370740912",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "12eac891-7a6a-48e6-89a9-62ee084c7de6",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "No JIRA ID, commit ID: 815d6bd69a0c1ba0e94fc0785f5c3619b37f19c5#diff-e8b73c5b81c403a5e5d581f97624c510",
        "createdAt" : "2020-03-07T12:10:34Z",
        "updatedAt" : "2020-03-07T12:10:34Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "98c3def8275c6c2c0eb86ace24c0408613877c1a",
    "line" : 399,
    "diffHunk" : "@@ -1,1 +449,453 @@      .doc(\"Number of subdirectories inside each path listed in spark.local.dir for \" +\n        \"hashing Block files into.\")\n      .version(\"0.6.0\")\n      .intConf\n      .checkValue(_ > 0, \"The number of subdirectories must be positive.\")"
  },
  {
    "id" : "f6375ffd-f158-42f4-be01-be1f63901f23",
    "prId" : 27847,
    "prUrl" : "https://github.com/apache/spark/pull/27847#pullrequestreview-370740937",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e6284954-acc7-492d-958c-89c1abec3793",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-13328, commit ID: ff776b2fc1cd4c571fd542dbf807e6fa3373cb34#diff-2b643ea78c1add0381754b1f47eec132",
        "createdAt" : "2020-03-07T12:10:59Z",
        "updatedAt" : "2020-03-07T12:10:59Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "98c3def8275c6c2c0eb86ace24c0408613877c1a",
    "line" : 407,
    "diffHunk" : "@@ -1,1 +458,462 @@      .doc(\"Max number of failures before this block manager refreshes \" +\n        \"the block locations from the driver.\")\n      .version(\"2.0.0\")\n      .intConf\n      .createWithDefault(5)"
  },
  {
    "id" : "48081cc5-d1bc-4961-b284-1a85be346668",
    "prId" : 27636,
    "prUrl" : "https://github.com/apache/spark/pull/27636#pullrequestreview-372000820",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "713fe4f6-b3d4-4a01-83c7-4dcadc5602e4",
        "parentId" : null,
        "authorId" : "0bad56d1-0d7e-4998-a237-abaf0d7617fe",
        "body" : "Is this config used in the PR? What's its intended relationship with `ABOUT_TO_BE_LOST_NODE_INTERVAL`?",
        "createdAt" : "2020-02-24T00:49:45Z",
        "updatedAt" : "2020-06-11T10:43:18Z",
        "lastEditedBy" : "0bad56d1-0d7e-4998-a237-abaf0d7617fe",
        "tags" : [
        ]
      },
      {
        "id" : "b533559a-b239-4a0c-919c-13cac15f13f5",
        "parentId" : "713fe4f6-b3d4-4a01-83c7-4dcadc5602e4",
        "authorId" : "11f6c849-b5b9-4c65-990c-525cf572b913",
        "body" : "I have added the information about this conf in the design doc. ",
        "createdAt" : "2020-02-27T03:28:06Z",
        "updatedAt" : "2020-06-11T10:43:18Z",
        "lastEditedBy" : "11f6c849-b5b9-4c65-990c-525cf572b913",
        "tags" : [
        ]
      },
      {
        "id" : "e72f956b-7d9d-462d-8ee9-3efd5f5e1119",
        "parentId" : "713fe4f6-b3d4-4a01-83c7-4dcadc5602e4",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "haven't looked at design doc, but same question - I only see this used in test?  If that is the case I think we atleast need a comment about it.  I think I have same concern here as above, I don't really know what this config means as well. what is the min time to termination?",
        "createdAt" : "2020-03-09T13:44:04Z",
        "updatedAt" : "2020-06-11T10:43:18Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "c5c74d31-d87b-4ad6-bd02-de763d17e87e",
        "parentId" : "713fe4f6-b3d4-4a01-83c7-4dcadc5602e4",
        "authorId" : "11f6c849-b5b9-4c65-990c-525cf572b913",
        "body" : "This is used in the DecommissionTracker.scala \r\n\r\n```\r\nprivate val minDecommissionTime =\r\n    conf.get(config.GRACEFUL_DECOMMISSION_MIN_TERMINATION_TIME_IN_SEC)\r\n```",
        "createdAt" : "2020-03-10T12:10:44Z",
        "updatedAt" : "2020-06-11T10:43:18Z",
        "lastEditedBy" : "11f6c849-b5b9-4c65-990c-525cf572b913",
        "tags" : [
        ]
      },
      {
        "id" : "322d7a3e-db2d-4877-8f7a-bfaf83791e12",
        "parentId" : "713fe4f6-b3d4-4a01-83c7-4dcadc5602e4",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "ah ok, the file in the diffs must have been minimized at the time",
        "createdAt" : "2020-03-10T14:25:34Z",
        "updatedAt" : "2020-06-11T10:43:18Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "c476e5271ca8b8fd3c401980a080334b8d7b9a36",
    "line" : 46,
    "diffHunk" : "@@ -1,1 +1916,1920 @@      .createWithDefault(90) // Pulled out of thin air.\n\n  private[spark] val GRACEFUL_DECOMMISSION_MIN_TERMINATION_TIME_IN_SEC =\n    ConfigBuilder(\"spark.graceful.decommission.min.termination.time\")\n      .doc(\"Minimum time to termination below which node decommissioning is performed \" +"
  },
  {
    "id" : "84f451be-7d27-4b06-9f94-c6b78f4bfad1",
    "prId" : 27636,
    "prUrl" : "https://github.com/apache/spark/pull/27636#pullrequestreview-371205729",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c1ccff2a-9d11-4b3f-a1e1-a09c5f290b1d",
        "parentId" : null,
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Would it make sense to handle this with `spark.worker.decommission.enabled` or not? I'm not sure just for discussion.",
        "createdAt" : "2020-02-28T20:08:32Z",
        "updatedAt" : "2020-06-11T10:43:18Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "7e46aefc-c4db-4226-881a-948e94a1a3fb",
        "parentId" : "c1ccff2a-9d11-4b3f-a1e1-a09c5f290b1d",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "is this specific to yarn or been tied in with previous work for all cluster managers?\r\n",
        "createdAt" : "2020-03-09T13:27:41Z",
        "updatedAt" : "2020-06-11T10:43:18Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "12d58481-6f36-489f-b774-5ef952af5b8e",
        "parentId" : "c1ccff2a-9d11-4b3f-a1e1-a09c5f290b1d",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "also all the configs need to be documented in configuration.md",
        "createdAt" : "2020-03-09T15:09:50Z",
        "updatedAt" : "2020-06-11T10:43:18Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "c476e5271ca8b8fd3c401980a080334b8d7b9a36",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1875,1879 @@      .createWithDefault(false)\n\n  private[spark] val GRACEFUL_DECOMMISSION_ENABLE =\n    ConfigBuilder(\"spark.graceful.decommission.enable\")\n      .doc(\"Whether to enable the node graceful decommissioning handling\")"
  },
  {
    "id" : "5bc939a7-c6ea-41c6-832a-494f9c70a650",
    "prId" : 27636,
    "prUrl" : "https://github.com/apache/spark/pull/27636#pullrequestreview-371188148",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "06d55e9f-311e-4935-8ee9-49d13c20cb00",
        "parentId" : null,
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "From the documentation, I have don't know how I would configure this value. Can we provide guidance? Also, why would we or why would not want to skip fetch failed? In my mind, if a node is decommissioning and the fetch fails that's expected but retrying the same fetch is very unlikely to succeed. Or am I misunderstanding the purpose of this?",
        "createdAt" : "2020-02-28T20:10:37Z",
        "updatedAt" : "2020-06-11T10:43:18Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "34449438-f465-4652-9295-18acac89c378",
        "parentId" : "06d55e9f-311e-4935-8ee9-49d13c20cb00",
        "authorId" : "11f6c849-b5b9-4c65-990c-525cf572b913",
        "body" : "I have tried to explain this here \r\nhttps://github.com/apache/spark/pull/27636#issuecomment-593338415",
        "createdAt" : "2020-03-02T10:45:42Z",
        "updatedAt" : "2020-06-11T10:43:18Z",
        "lastEditedBy" : "11f6c849-b5b9-4c65-990c-525cf572b913",
        "tags" : [
        ]
      },
      {
        "id" : "c50060fc-dbd4-493c-80d7-4f8e51934e3b",
        "parentId" : "06d55e9f-311e-4935-8ee9-49d13c20cb00",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "I assume this is a total number of fetch failures, or is it per stage? I'm sure I'll see it more in the code later, but if I don't understand from reading config text then the user won't either.\r\nWithout having read the rest of the code, this seems like kind of a weird setting to me. If we are handling decomissioned nodes then why do we need this.   Is it in case the entire cluster is being decommissioned?  \r\n\r\nIf we keep this we may want to combined the names here, there was thread on dev about making to many .component. type names.\r\n",
        "createdAt" : "2020-03-09T13:37:18Z",
        "updatedAt" : "2020-06-11T10:43:18Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "c476e5271ca8b8fd3c401980a080334b8d7b9a36",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +1882,1886 @@      .createWithDefault(false)\n\n  private[spark] val GRACEFUL_DECOMMISSION_FETCHFAILED_IGNORE_THRESHOLD =\n    ConfigBuilder(\"spark.graceful.decommission.fetchfailed.ignore.threshold\")\n      .doc(\"Threshold of number of times fetchfailed ignored due to node\" +"
  },
  {
    "id" : "818ddb7a-9f91-4a16-891c-0d024aa66c1a",
    "prId" : 27636,
    "prUrl" : "https://github.com/apache/spark/pull/27636#pullrequestreview-384637784",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a9f14c33-40c6-4502-b5ef-f8bd06d58c79",
        "parentId" : null,
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "I'm not sure I get the spot loss interaction here, the AM/RM won't know I think and we'd see more of like the K8s one happen.",
        "createdAt" : "2020-02-28T20:12:36Z",
        "updatedAt" : "2020-06-11T10:43:18Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "be5735fd-f0f1-4945-9604-2dcf0640eee5",
        "parentId" : "a9f14c33-40c6-4502-b5ef-f8bd06d58c79",
        "authorId" : "11f6c849-b5b9-4c65-990c-525cf572b913",
        "body" : "We can get the decommission timeout for hadoop-3.1 and later version of hadoop, so we can use that value to decide when the node is decommissioned.\r\nWhereas for lower version of hadoop(hadoop-2.8) there is no decommissionTimeout  for decommissioning nodes in those scenario we already knew from our experience that in AWS  spotloss nodes will stay for 2 min and GCP preemptible VM will stay for 30 sec after receiving the node decommissioning from hadoop end. \r\n\r\nThis config is added here to make decommissioning of nodes to work with multiple version of hadoop, \r\n\r\n Please find the logic used in YarnAllocator.scala to decide the timeout of the node\r\n\r\n```\r\nif (x.getNodeState.toString.equals(NodeState.DECOMMISSIONING.toString)) {\r\n          // In hadoop 2.7 there is no support getDecommissioningTimeout whereas\r\n          // In hadoop 3.1 and later version of hadoop there is support\r\n          // of getDecommissioningTimeout So the method call made using reflection\r\n          // to update the value nodeTerminationTime and for lower version of hadoop2.7\r\n          // use the config spark.graceful.decommission.node.timeout which is specific to cloud\r\n          var nodeTerminationTime = clock.getTimeMillis() + nodeLossInterval * 1000\r\n          try {\r\n              val decommiossioningTimeout = x.getClass.getMethod(\r\n                \"getDecommissioningTimeout\").invoke(x).asInstanceOf[Integer]\r\n              if (decommiossioningTimeout != null) {\r\n                nodeTerminationTime = clock.getTimeMillis() + decommiossioningTimeout * 1000\r\n              }\r\n          } catch {\r\n            case e: NoSuchMethodException => logDebug(e.toString)\r\n          }\r\n```",
        "createdAt" : "2020-03-02T11:07:26Z",
        "updatedAt" : "2020-06-11T10:43:18Z",
        "lastEditedBy" : "11f6c849-b5b9-4c65-990c-525cf572b913",
        "tags" : [
        ]
      },
      {
        "id" : "f0beb15e-bc61-48d4-9f13-7cad6130f5d4",
        "parentId" : "a9f14c33-40c6-4502-b5ef-f8bd06d58c79",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "so what happens with this in hadoop 3.1 and greater? is it ignored if a timeout is specified from yarn?",
        "createdAt" : "2020-03-09T13:46:57Z",
        "updatedAt" : "2020-06-11T10:43:18Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "b431acf0-e612-4593-974a-6072ceb9bc74",
        "parentId" : "a9f14c33-40c6-4502-b5ef-f8bd06d58c79",
        "authorId" : "11f6c849-b5b9-4c65-990c-525cf572b913",
        "body" : "For hadoop3.1 and later version of hadoop there is an interface to get the value of decommissioning timeout method name -  getDecommissioningTimeout. So we have used that here in the code \r\n\r\n```\r\nval decommiossioningTimeout = x.getClass.getMethod(\r\n                \"getDecommissioningTimeout\").invoke(x).asInstanceOf[Integer]\r\n              if (decommiossioningTimeout != null) {\r\n                nodeTerminationTime = clock.getTimeMillis() + decommiossioningTimeout * 1000\r\n              }\r\n```\r\n\r\nSince we are getting the value of decommiossioningTimeout  from the RM in that scenario we will be using that value otherwise we use the value specified in the config.\r\n\r\nAnd also if someone has backported the hadoop3.1 change to lower version of hadoop2.8 etc , For them also they can use decommiossioningTimeout instead of using config GRACEFUL_DECOMMISSION_NODE_TIMEOUT",
        "createdAt" : "2020-03-10T12:21:14Z",
        "updatedAt" : "2020-06-11T10:43:18Z",
        "lastEditedBy" : "11f6c849-b5b9-4c65-990c-525cf572b913",
        "tags" : [
        ]
      },
      {
        "id" : "9c913d62-caad-4235-9c93-6f0a9bb9847d",
        "parentId" : "a9f14c33-40c6-4502-b5ef-f8bd06d58c79",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "ok so we need to update description to say when it applies",
        "createdAt" : "2020-03-10T14:21:03Z",
        "updatedAt" : "2020-06-11T10:43:18Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "ea9dfeef-453e-4b6e-8779-0e3db7e5fc64",
        "parentId" : "a9f14c33-40c6-4502-b5ef-f8bd06d58c79",
        "authorId" : "11f6c849-b5b9-4c65-990c-525cf572b913",
        "body" : "Updated the description ",
        "createdAt" : "2020-03-31T11:24:37Z",
        "updatedAt" : "2020-06-11T10:43:18Z",
        "lastEditedBy" : "11f6c849-b5b9-4c65-990c-525cf572b913",
        "tags" : [
        ]
      }
    ],
    "commit" : "c476e5271ca8b8fd3c401980a080334b8d7b9a36",
    "line" : 59,
    "diffHunk" : "@@ -1,1 +1929,1933 @@\n  private[spark] val GRACEFUL_DECOMMISSION_NODE_TIMEOUT =\n    ConfigBuilder(\"spark.graceful.decommission.node.timeout\")\n      .doc(\"Interval in seconds after which the node is decommissioned in case aws spotloss\" +\n        \"the time is approximately 110s and in case of GCP preemptible VMs this is around 30s\" +"
  },
  {
    "id" : "1c0cac34-6530-4342-9a2b-26fc7f83435b",
    "prId" : 27636,
    "prUrl" : "https://github.com/apache/spark/pull/27636#pullrequestreview-367104747",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bf8f27d3-0fed-407b-8f24-9b012aba2cf3",
        "parentId" : null,
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Why would we exit at a different time? I know we probably want to keep the blocks as long as possible, but if any process is running the node can't gracefully shutdown so its seems strange to have these occur separately.",
        "createdAt" : "2020-02-28T20:18:55Z",
        "updatedAt" : "2020-06-11T10:43:18Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "b4aaf729-f89b-46aa-a50a-7d6fdcccee20",
        "parentId" : "bf8f27d3-0fed-407b-8f24-9b012aba2cf3",
        "authorId" : "11f6c849-b5b9-4c65-990c-525cf572b913",
        "body" : "This for minimising the recompute of the shuffle data and maximising the use of already generated shuffle data. Also tried to explain this in this comment https://github.com/apache/spark/pull/27636#issuecomment-593338415",
        "createdAt" : "2020-03-02T10:48:07Z",
        "updatedAt" : "2020-06-11T10:43:18Z",
        "lastEditedBy" : "11f6c849-b5b9-4c65-990c-525cf572b913",
        "tags" : [
        ]
      }
    ],
    "commit" : "c476e5271ca8b8fd3c401980a080334b8d7b9a36",
    "line" : 32,
    "diffHunk" : "@@ -1,1 +1902,1906 @@      .createWithDefault(50) // Pulled out of thin air.\n\n  private[spark] val GRACEFUL_DECOMMISSION_SHUFFLEDATA_LEASETIME_PCT =\n    ConfigBuilder(\"spark.graceful.decommission.shuffedata.leasetimePct\")\n      .doc(\"Percentage of time to expiry after which shuffle data \" +"
  },
  {
    "id" : "2aff57c1-adc2-49e1-97d0-7f383f7442d2",
    "prId" : 27636,
    "prUrl" : "https://github.com/apache/spark/pull/27636#pullrequestreview-371189862",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3923d7f8-4359-4d9f-aa75-d1d281a5393e",
        "parentId" : null,
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "I don't know what this mean. percentage of what time to expiry?  We get a notification that node being decommissioned and I assume this is how long to let executor run before killing it, but it doesn't say what this is a percentage of",
        "createdAt" : "2020-03-09T13:39:32Z",
        "updatedAt" : "2020-06-11T10:43:18Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "c476e5271ca8b8fd3c401980a080334b8d7b9a36",
    "line" : 25,
    "diffHunk" : "@@ -1,1 +1895,1899 @@  private[spark] val GRACEFUL_DECOMMISSION_EXECUTOR_LEASETIME_PCT =\n    ConfigBuilder(\"spark.graceful.decommission.executor.leasetimePct\")\n      .doc(\"Percentage of time to expiry after which executors are killed \" +\n        \"(if enabled) on the node. Value ranges between (0-100)\")\n      .version(\"3.1.0\")"
  },
  {
    "id" : "1af4c2bb-58ae-4f56-8639-5699a723f01f",
    "prId" : 27636,
    "prUrl" : "https://github.com/apache/spark/pull/27636#pullrequestreview-371192232",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7282a17a-6757-48ea-8db8-4f5f3a1528f9",
        "parentId" : null,
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "similar here we should describe what this is a percentage of.\r\nI assume this has to be >= so that executors don't write more shuffle data. \r\nIt might be better just to fail if this is incorrectly set assuming we can check it early enough. I guess I'll see more code later",
        "createdAt" : "2020-03-09T13:42:39Z",
        "updatedAt" : "2020-06-11T10:43:18Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "c476e5271ca8b8fd3c401980a080334b8d7b9a36",
    "line" : 34,
    "diffHunk" : "@@ -1,1 +1904,1908 @@  private[spark] val GRACEFUL_DECOMMISSION_SHUFFLEDATA_LEASETIME_PCT =\n    ConfigBuilder(\"spark.graceful.decommission.shuffedata.leasetimePct\")\n      .doc(\"Percentage of time to expiry after which shuffle data \" +\n        \"cleaned up (if enabled) on the node. Value ranges between (0-100)\" +\n        \"This value is always greater than or equal to executor\" +"
  },
  {
    "id" : "58f5370a-7b2b-45a6-aabe-2ec03b7690d1",
    "prId" : 27463,
    "prUrl" : "https://github.com/apache/spark/pull/27463#pullrequestreview-353687557",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "190a231d-ce3d-43d1-9106-e0c69c904e0c",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "shall we move it to `SparkConf.deprecatedConfigs`?",
        "createdAt" : "2020-02-05T04:56:05Z",
        "updatedAt" : "2020-02-06T05:54:45Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "78864c47-9a83-4b85-86b3-9a00033fa140",
        "parentId" : "190a231d-ce3d-43d1-9106-e0c69c904e0c",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "Thansk, done in 2d2637b.",
        "createdAt" : "2020-02-05T12:46:23Z",
        "updatedAt" : "2020-02-06T05:54:45Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "8f7540e4cb1336f99889ddf2d449f3ee59f9b491",
    "line" : 3,
    "diffHunk" : "@@ -1,1 +895,899 @@      .createWithDefault(Int.MaxValue)\n\n  private[spark] val MAX_REMOTE_BLOCK_SIZE_FETCH_TO_MEM =\n    ConfigBuilder(\"spark.network.maxRemoteBlockSizeFetchToMem\")\n      .doc(\"Remote block will be fetched to disk when size of the block is above this threshold \" +"
  },
  {
    "id" : "4d355261-9313-4f5d-b2ee-d825cf560089",
    "prId" : 27329,
    "prUrl" : "https://github.com/apache/spark/pull/27329#pullrequestreview-351202662",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0afeef98-d844-47ad-96d4-695f6323e8ff",
        "parentId" : null,
        "authorId" : "30443c52-abda-4db3-a11c-8edcec1c7159",
        "body" : "shouldn't we add .doc for this metric and also spark.eventLog.logStageExecutorMetrics.enabled in addition to what is added to configuration.md?",
        "createdAt" : "2020-01-30T21:40:16Z",
        "updatedAt" : "2020-01-31T19:14:28Z",
        "lastEditedBy" : "30443c52-abda-4db3-a11c-8edcec1c7159",
        "tags" : [
        ]
      },
      {
        "id" : "f2134b66-e2e2-424b-848a-70e817f0ff35",
        "parentId" : "0afeef98-d844-47ad-96d4-695f6323e8ff",
        "authorId" : "119f7711-a251-4127-b455-51a922a097f1",
        "body" : "Added.",
        "createdAt" : "2020-01-30T23:03:22Z",
        "updatedAt" : "2020-01-31T19:14:28Z",
        "lastEditedBy" : "119f7711-a251-4127-b455-51a922a097f1",
        "tags" : [
        ]
      }
    ],
    "commit" : "5e134dd1a81e2b9ec20c3f3bfe2bb76172a4a639",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +236,240 @@        \"executor metrics.\")\n      .booleanConf\n      .createWithDefault(false)\n\n  private[spark] val EXECUTOR_METRICS_POLLING_INTERVAL ="
  },
  {
    "id" : "cc0badde-d9b2-496b-b8cf-0cd2814ef510",
    "prId" : 27207,
    "prUrl" : "https://github.com/apache/spark/pull/27207#pullrequestreview-353462977",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "09476bcf-fa10-41d4-9424-4a8c26cf0ab3",
        "parentId" : null,
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "I think you said you hadn't updated all tests, but it would be nice to make sure we have one that tests the old logic as well",
        "createdAt" : "2020-02-04T16:41:59Z",
        "updatedAt" : "2020-04-03T05:29:21Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "253c9fd4-364d-4f6b-9eb3-59967feb93db",
        "parentId" : "09476bcf-fa10-41d4-9424-4a8c26cf0ab3",
        "authorId" : "29a3e834-d9e3-42dc-ab25-20ba332ef3f2",
        "body" : "will follow up on that",
        "createdAt" : "2020-02-05T04:45:47Z",
        "updatedAt" : "2020-04-03T05:29:21Z",
        "lastEditedBy" : "29a3e834-d9e3-42dc-ab25-20ba332ef3f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "24c8ad9ae23d360c7bb5bc813d04e6f5f6715d93",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +544,548 @@      .fallbackConf(DYN_ALLOCATION_SCHEDULER_BACKLOG_TIMEOUT)\n\n  private[spark] val LEGACY_LOCALITY_WAIT_RESET =\n    ConfigBuilder(\"spark.locality.wait.legacyResetOnTaskLaunch\")\n    .doc(\"Whether to use the legacy behavior of locality wait, which resets the delay timer \" +"
  },
  {
    "id" : "f3d371a8-cf90-40e4-be88-967e00c9fbe4",
    "prId" : 27207,
    "prUrl" : "https://github.com/apache/spark/pull/27207#pullrequestreview-386969288",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "229a7633-73b0-4d37-96b3-ffa8e8f14174",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "let's make it an internal conf and add the version info.",
        "createdAt" : "2020-03-30T14:01:12Z",
        "updatedAt" : "2020-04-03T05:29:22Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "e6edfbdc-262d-49dc-94b1-b678a3e43c20",
        "parentId" : "229a7633-73b0-4d37-96b3-ffa8e8f14174",
        "authorId" : "29a3e834-d9e3-42dc-ab25-20ba332ef3f2",
        "body" : "what all does adding internal do? I see internal ones are not exposed when SQLConf.getAllDefinedConfs is called.\r\nWhich version should I add?",
        "createdAt" : "2020-03-31T02:30:07Z",
        "updatedAt" : "2020-04-03T05:29:22Z",
        "lastEditedBy" : "29a3e834-d9e3-42dc-ab25-20ba332ef3f2",
        "tags" : [
        ]
      },
      {
        "id" : "8946dd72-008a-415f-80c7-2fb661e6efcd",
        "parentId" : "229a7633-73b0-4d37-96b3-ffa8e8f14174",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "internal means we can remove it later. Ideally all legacy configs should be internal.\r\n\r\nwe can set version as `3.0.0` and change it if we can't merge it before the release eventually.",
        "createdAt" : "2020-03-31T08:02:25Z",
        "updatedAt" : "2020-04-03T05:29:22Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "717b91f2-02f3-4f4b-9db6-6cd4dd5ed1e0",
        "parentId" : "229a7633-73b0-4d37-96b3-ffa8e8f14174",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "yes please add the .doc and .version - it would be nice to get into 3.0.0.\r\nI don't know that I agree with the internal tag. Maybe its being used differently then I thought. Internal to me means it's not meant for end user, this one could be if users find issues with the new algorithm. I'm definitely fine with not documenting it and it seems it has that side affect so maybe not a big deal.",
        "createdAt" : "2020-04-02T13:26:47Z",
        "updatedAt" : "2020-04-03T05:29:22Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "303e554f-5024-49c1-9090-c66626957235",
        "parentId" : "229a7633-73b0-4d37-96b3-ffa8e8f14174",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "all the configs are end-user facing (they can set and it will take effect). Internal config is for special use cases only (most of the users should not set it).",
        "createdAt" : "2020-04-03T06:38:35Z",
        "updatedAt" : "2020-04-03T06:38:35Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "24c8ad9ae23d360c7bb5bc813d04e6f5f6715d93",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +545,549 @@\n  private[spark] val LEGACY_LOCALITY_WAIT_RESET =\n    ConfigBuilder(\"spark.locality.wait.legacyResetOnTaskLaunch\")\n    .doc(\"Whether to use the legacy behavior of locality wait, which resets the delay timer \" +\n      \"anytime a task is scheduled. See Delay Scheduling section of TaskSchedulerImpl's class \" +"
  },
  {
    "id" : "5d4a0e61-4d57-411f-8728-9fedbe9aa09b",
    "prId" : 27207,
    "prUrl" : "https://github.com/apache/spark/pull/27207#pullrequestreview-398792782",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e570e123-d327-42ef-83c2-2e6e07a9dbdb",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "I think this was not merged into 3.0 branch, right?",
        "createdAt" : "2020-04-23T04:41:36Z",
        "updatedAt" : "2020-04-23T04:41:37Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "ba85d936-b87b-4991-bfb7-3aa5fc63d83a",
        "parentId" : "e570e123-d327-42ef-83c2-2e6e07a9dbdb",
        "authorId" : "29a3e834-d9e3-42dc-ab25-20ba332ef3f2",
        "body" : "afaik that is correct we didn't backport it.\r\nhere is PR fixing these: https://github.com/apache/spark/pull/28307",
        "createdAt" : "2020-04-23T06:28:34Z",
        "updatedAt" : "2020-04-23T06:28:34Z",
        "lastEditedBy" : "29a3e834-d9e3-42dc-ab25-20ba332ef3f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "24c8ad9ae23d360c7bb5bc813d04e6f5f6715d93",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +550,554 @@      \"documentation for more details.\")\n    .internal()\n    .version(\"3.0.0\")\n    .booleanConf\n    .createWithDefault(false)"
  },
  {
    "id" : "578ea9a9-4631-4f56-8dbb-a4072a8ff366",
    "prId" : 27085,
    "prUrl" : "https://github.com/apache/spark/pull/27085#pullrequestreview-339003541",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "26b0f3ff-9523-48e3-a123-e51e55214b27",
        "parentId" : null,
        "authorId" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "body" : "Couldn't it be 0, potentially? So you only keep one last compacted log file, instead of one compacted + one normal.",
        "createdAt" : "2020-01-03T19:22:11Z",
        "updatedAt" : "2020-01-09T00:50:36Z",
        "lastEditedBy" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "tags" : [
        ]
      },
      {
        "id" : "4e52220c-1cd4-4782-8037-9cf00c46c67a",
        "parentId" : "26b0f3ff-9523-48e3-a123-e51e55214b27",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "If I'm not missing, it can't be, as setting this 0 means we allow compacting the event log file which running app is writing now. It also doesn't make sense for finished app as well, since all jobs should have been finished when app is finished - so compaction would lose all job-related events.",
        "createdAt" : "2020-01-06T02:52:06Z",
        "updatedAt" : "2020-01-09T00:50:37Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "518c036a-e8d0-45ea-a840-1e513c84bce1",
        "parentId" : "26b0f3ff-9523-48e3-a123-e51e55214b27",
        "authorId" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "body" : "Hmm, I see. Merging the last two logs might still result in some savings, but probably not worth it if it means having extra logic to say \"don't filter out things that started in the last log file\".\r\n\r\nOn a side note, compaction currently overrides all the \"retained blah\" configurations; so at the end you may end up, for example, with very few jobs in the Spark UI, whereas the retention settings say you should keep the last 1000 jobs or so.",
        "createdAt" : "2020-01-06T18:48:06Z",
        "updatedAt" : "2020-01-09T00:50:37Z",
        "lastEditedBy" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "tags" : [
        ]
      },
      {
        "id" : "90922d89-4bd7-43e4-832d-6a8dc2b96bbd",
        "parentId" : "26b0f3ff-9523-48e3-a123-e51e55214b27",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "> On a side note, compaction currently overrides all the \"retained blah\" configurations\r\n\r\nYeah, that's actually the point I have been thinking about, as end users may see only few of jobs if they set this to 1 and the log files are just compacted.\r\n\r\nMaybe we could improve event filter to configure max retained jobs on compact file so that some of finished jobs can be still tracked (`live jobs + finished jobs <= max retained jobs`). It cannot be strictly max, as live jobs could have been more than the configured value, so may need to have better name.\r\n\r\nLooks like it could be further TODO worth filing JIRA issue.",
        "createdAt" : "2020-01-07T02:33:11Z",
        "updatedAt" : "2020-01-09T00:50:37Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "163bda018d054a0d41e983e6c448ce3b3e746d35",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +205,209 @@        \"the overall size of event log files.\")\n      .intConf\n      .checkValue(_ > 0, \"Max event log files to retain should be higher than 0.\")\n      .createWithDefault(Integer.MAX_VALUE)\n"
  },
  {
    "id" : "f421e0ab-6437-489e-baf1-97374d4a1508",
    "prId" : 27085,
    "prUrl" : "https://github.com/apache/spark/pull/27085#pullrequestreview-351341355",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "567ca6e8-7c46-4420-98e6-f4f625b105d1",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I think we should have added some docs here too.",
        "createdAt" : "2020-01-31T03:39:50Z",
        "updatedAt" : "2020-01-31T03:39:50Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "3cf83bab-5564-4abc-b3fa-1d77ee164e37",
        "parentId" : "567ca6e8-7c46-4420-98e6-f4f625b105d1",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "It was to ease of test for compaction but after reviewing it's no longer need to be a configuration.\r\nI'll just remove this and convert it to be constant unless we figure out needs to adjust manually.",
        "createdAt" : "2020-01-31T07:35:29Z",
        "updatedAt" : "2020-01-31T07:35:29Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "183ef3f7-9b7a-456b-a016-88cf1eec9172",
        "parentId" : "567ca6e8-7c46-4420-98e6-f4f625b105d1",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Just realized there's test to modify the value to ensure compaction doesn't skip with low value. I'll retain the configuration and add some doc.",
        "createdAt" : "2020-01-31T08:02:40Z",
        "updatedAt" : "2020-01-31T08:02:40Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "163bda018d054a0d41e983e6c448ce3b3e746d35",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +210,214 @@  private[spark] val EVENT_LOG_COMPACTION_SCORE_THRESHOLD =\n    ConfigBuilder(\"spark.eventLog.rolling.compaction.score.threshold\")\n      .internal()\n      .doubleConf\n      .createWithDefault(0.7d)"
  },
  {
    "id" : "3d0dec25-488e-446c-9b71-66c2809b1dd2",
    "prId" : 27085,
    "prUrl" : "https://github.com/apache/spark/pull/27085#pullrequestreview-351330567",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f5b49bc3-01de-4632-9396-a62f7bd62dc1",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Why didn't we make it optional, or defaults to -1 to express \"all event log files will be retained\"?",
        "createdAt" : "2020-01-31T03:56:14Z",
        "updatedAt" : "2020-01-31T03:56:14Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "6a27cb72-53ed-4496-8cab-54b1f4c830f0",
        "parentId" : "f5b49bc3-01de-4632-9396-a62f7bd62dc1",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "I think this default value \"naturally\" fits the semantic of the configuration even we don't indicate Integer.MAX_VALUE as special value. No need to care the configuration specially. This approach seems to be already used for couple of configurations.",
        "createdAt" : "2020-01-31T07:30:56Z",
        "updatedAt" : "2020-01-31T07:30:57Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "163bda018d054a0d41e983e6c448ce3b3e746d35",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +206,210 @@      .intConf\n      .checkValue(_ > 0, \"Max event log files to retain should be higher than 0.\")\n      .createWithDefault(Integer.MAX_VALUE)\n\n  private[spark] val EVENT_LOG_COMPACTION_SCORE_THRESHOLD ="
  }
]