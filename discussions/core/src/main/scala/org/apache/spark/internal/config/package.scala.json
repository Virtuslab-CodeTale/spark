[
  {
    "id" : "b6b791a9-9e9d-4bea-b6f6-af685ee12d62",
    "prId" : 33615,
    "prUrl" : "https://github.com/apache/spark/pull/33615#pullrequestreview-730204271",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f7ffe8f4-88e6-4452-9b66-99ad36e9ce8f",
        "parentId" : null,
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "It seems that we have exposed this in 3.1.0(3.1.1) already: https://github.com/apache/spark/blob/branch-3.1/core/src/main/scala/org/apache/spark/internal/config/package.scala\r\n\r\nWhy do we change the version here? The feature is not ready in 3.1?",
        "createdAt" : "2021-08-15T09:47:39Z",
        "updatedAt" : "2021-08-15T09:48:08Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "88507602-f0cf-4353-be51-e89c3ef92e46",
        "parentId" : "f7ffe8f4-88e6-4452-9b66-99ad36e9ce8f",
        "authorId" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "body" : "Yes, the feature was not ready in 3.1. It will be ready only in 3.2 which is why I changed it to 3.2 to keep it consistent.",
        "createdAt" : "2021-08-15T16:51:14Z",
        "updatedAt" : "2021-08-15T16:51:14Z",
        "lastEditedBy" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "tags" : [
        ]
      }
    ],
    "commit" : "12618284d95d7b1685c2c48c79bffcb618cf4523",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +2103,2107 @@        \"org.apache.spark.network.shuffle.MergedShuffleFileManager implementation for push-based \" +\n        \"shuffle to be enabled\")\n      .version(\"3.2.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "d220dc8b-fb03-4fdb-b003-8403f3829793",
    "prId" : 33078,
    "prUrl" : "https://github.com/apache/spark/pull/33078#pullrequestreview-699399725",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "54282deb-c470-4784-8b56-145ce80bda31",
        "parentId" : null,
        "authorId" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "body" : "nit: This feels like it should be `spark.app.attemptNumber` than `spark.app.attempt.id`. Also currently this is specific to push based shuffle right? Should we add that in the documentation?",
        "createdAt" : "2021-07-05T02:00:10Z",
        "updatedAt" : "2021-07-05T02:10:19Z",
        "lastEditedBy" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "tags" : [
        ]
      },
      {
        "id" : "687245d6-c8e9-4429-adc9-0955dd150e7a",
        "parentId" : "54282deb-c470-4784-8b56-145ce80bda31",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "`attemptNumber` sounds better to me.\r\n\r\nIf this conf can't be set by users, I think it's ok to keep it internal.",
        "createdAt" : "2021-07-05T05:37:23Z",
        "updatedAt" : "2021-07-05T05:37:23Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "8a1f417b-4ce6-4f70-945d-b37805cad71a",
        "parentId" : "54282deb-c470-4784-8b56-145ce80bda31",
        "authorId" : "e942fa3e-a5b9-4a9f-b9a5-a1afd3c29b2c",
        "body" : "AppAttemptId is currently used in lots of places to indicate the attempt id from Yarn, should we keep it as \"spark.app.attempt.id\" to align with all other places? Or we just want to change the configuration string to \"spark.app.attemptNumber\"? ",
        "createdAt" : "2021-07-05T05:54:51Z",
        "updatedAt" : "2021-07-05T05:54:51Z",
        "lastEditedBy" : "e942fa3e-a5b9-4a9f-b9a5-a1afd3c29b2c",
        "tags" : [
        ]
      },
      {
        "id" : "570171b6-51eb-481a-bb6e-3d364925f97e",
        "parentId" : "54282deb-c470-4784-8b56-145ce80bda31",
        "authorId" : "e942fa3e-a5b9-4a9f-b9a5-a1afd3c29b2c",
        "body" : "And which version should we mark it there? I added \"1.3.0\" since the support for multiple attempts in Yarn was added since 1.3.0. But if we change to \"spark.app.attemptNumber\", we should start from \"3.2.0\". Thoughts?",
        "createdAt" : "2021-07-05T05:56:32Z",
        "updatedAt" : "2021-07-05T05:56:32Z",
        "lastEditedBy" : "e942fa3e-a5b9-4a9f-b9a5-a1afd3c29b2c",
        "tags" : [
        ]
      },
      {
        "id" : "be958a99-e0af-4a11-a965-11d09502cdbf",
        "parentId" : "54282deb-c470-4784-8b56-145ce80bda31",
        "authorId" : "e942fa3e-a5b9-4a9f-b9a5-a1afd3c29b2c",
        "body" : "Should we use \"String\" or \"Integer\" here for this configuration? In [SparkContext where it gets the Application AttemptID](https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/SparkContext.scala#L340), it is a String format. But when parsing into Configuration and also resolving this in RemoteBlockPushResolver, we are using Integer to determine whether it is a new attempt.",
        "createdAt" : "2021-07-05T06:13:17Z",
        "updatedAt" : "2021-07-05T06:13:17Z",
        "lastEditedBy" : "e942fa3e-a5b9-4a9f-b9a5-a1afd3c29b2c",
        "tags" : [
        ]
      },
      {
        "id" : "b905ccf9-7e33-4b4f-8b47-306c4a2b0d63",
        "parentId" : "54282deb-c470-4784-8b56-145ce80bda31",
        "authorId" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "body" : "I see it makes sense. IMO, since it is already called `attemptId` it is fine to keep it probably like that. May be just adding slightly more info on the doc should be fine with me. Initially, I was under the impression we are generating this attemptId which is not the case.",
        "createdAt" : "2021-07-05T20:05:00Z",
        "updatedAt" : "2021-07-05T20:05:00Z",
        "lastEditedBy" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "tags" : [
        ]
      }
    ],
    "commit" : "53109918cbdbdba2fe79f38a991c171efec7e85f",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +2247,2251 @@\n  private[spark] val APP_ATTEMPT_ID =\n    ConfigBuilder(\"spark.app.attempt.id\")\n      .internal()\n      .doc(\"The application attempt Id assigned from Hadoop YARN. \" +"
  },
  {
    "id" : "a07b11d7-89d4-495d-832c-302b29348636",
    "prId" : 32287,
    "prUrl" : "https://github.com/apache/spark/pull/32287#pullrequestreview-655481906",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "09e9b13b-314f-402a-ab20-4f35b75b425b",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Just a question. Is there a reason of `10` instead of `3`?",
        "createdAt" : "2021-05-08T22:29:08Z",
        "updatedAt" : "2021-05-10T10:25:15Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "63892ca8-c862-42d6-8c6d-2f0e9c489469",
        "parentId" : "09e9b13b-314f-402a-ab20-4f35b75b425b",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Given the discussion (https://github.com/apache/spark/pull/32287#discussion_r625287419) there, Netty OOM could be raised more frequently in certain cases, e.g.,\r\n\r\n> For case b), the OOM threshold might be 20 requests. In this case, there're still 80 deferred requests, which would hit the OOM soon as you mentioned. That being said, I think the current fix would work around the issue in the end. Note that the application would fail before the fix.\r\n\r\nThus, I'd like to give more chances for the block in case we fall into the case like b).",
        "createdAt" : "2021-05-10T10:18:54Z",
        "updatedAt" : "2021-05-10T10:25:15Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "4af6ee73221a55e3cd7239f151ec33d5e802cbe4",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +1201,1205 @@      .internal()\n      .intConf\n      .createWithDefault(10)\n\n  private[spark] val REDUCER_MAX_BLOCKS_IN_FLIGHT_PER_ADDRESS ="
  },
  {
    "id" : "c4300ac3-1ee2-4706-9529-df5804b8d89d",
    "prId" : 31761,
    "prUrl" : "https://github.com/apache/spark/pull/31761#pullrequestreview-611033475",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e569a052-57ce-455e-851d-c2c07de35c3c",
        "parentId" : null,
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Is this only restricted to HDFS URLs? can \"hosts\" either be nameservice name or namenode name? some examples might help.",
        "createdAt" : "2021-03-07T01:36:53Z",
        "updatedAt" : "2021-03-22T18:58:50Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "4a07beb4-0a19-4462-89ec-9a244ef7b125",
        "parentId" : "e569a052-57ce-455e-851d-c2c07de35c3c",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Because the configuration value will be parsed as file system URLs to get file system. A simple host name doesn't work here.\r\n\r\ne.g.\r\n\r\n```scala\r\nscala> new Path(\"hdfs.namenode.net\").getFileSystem(new Configuration()).getUri.getHost\r\nres9: String = null\r\n```",
        "createdAt" : "2021-03-07T04:15:35Z",
        "updatedAt" : "2021-03-22T18:58:50Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "72b014c9-f5c5-48d8-b72b-2d3d3d48ae9a",
        "parentId" : "e569a052-57ce-455e-851d-c2c07de35c3c",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "It makes sense for `spark.kerberos.access.hadoopFileSystems` to be URLs since Spark needs to instantiate `FileSystem`s from them. But for this case I'm not sure if it's necessary: we can just parse the config into a set of host names and check whether the file systems above contain them:\r\n```scala\r\nval hostsToExclude = sparkConf.get(KERBEROS_FILESYSTEM_RENEWAL_EXCLUDE).toSet\r\nfilesystems.filter(fs => !hostsToExclude.contains(fs.getUri.getHost).foreach { fs =>\r\n ...\r\n}\r\n```\r\n\r\nI'm fine either way though since it also makes sense to keep it consistent with `spark.kerberos.access.hadoopFileSystems`. BTW I think we'll need to update security.md for the new config.",
        "createdAt" : "2021-03-07T04:50:39Z",
        "updatedAt" : "2021-03-22T18:58:50Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "96954ac1-4c6d-4481-bcf5-a2c524aecc31",
        "parentId" : "e569a052-57ce-455e-851d-c2c07de35c3c",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Thanks. Let me update security.md.",
        "createdAt" : "2021-03-07T04:56:45Z",
        "updatedAt" : "2021-03-22T18:58:50Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "232245ab-3ae5-4035-80f5-7d4abd22d76e",
        "parentId" : "e569a052-57ce-455e-851d-c2c07de35c3c",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "so just to be clear, these fileystems are ones the user specified in spark.kerberos.access.hadoopFileSystems so it wants to get initial tokens for them but then later we don't want to renew, correct?  We should perhaps update doc to mention that.  \r\nIt would be nice to know if this works with other cluster managers like k8s - @ifilonenko @mccheah  maybe?",
        "createdAt" : "2021-03-08T14:34:36Z",
        "updatedAt" : "2021-03-22T18:58:50Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "b8929d1f-2fbe-4461-9ec8-3b212eba9048",
        "parentId" : "e569a052-57ce-455e-851d-c2c07de35c3c",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "I guess it could be the staging or default Filesystem as well.",
        "createdAt" : "2021-03-08T14:36:09Z",
        "updatedAt" : "2021-03-22T18:58:50Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "aa66fdcd-4c24-4fe4-a03f-81299e364c24",
        "parentId" : "e569a052-57ce-455e-851d-c2c07de35c3c",
        "authorId" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "body" : "@tgravescs with the actual code Spark doesn't obtain the initial tokens when an FS is listed in \"spark.kerberos.renewal.exclude.hadoopFileSystems\". Please see my comment below...",
        "createdAt" : "2021-03-08T15:18:02Z",
        "updatedAt" : "2021-03-22T18:58:50Z",
        "lastEditedBy" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "tags" : [
        ]
      },
      {
        "id" : "a1ffedf5-ff22-4265-9c48-f14bda29a247",
        "parentId" : "e569a052-57ce-455e-851d-c2c07de35c3c",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Hm? @gaborgsomogyi I think this config doesn't affect Spark obtain the initial tokens?",
        "createdAt" : "2021-03-08T18:07:45Z",
        "updatedAt" : "2021-03-22T18:58:50Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "0906aa91-df70-494f-a93e-3d0fc5eafa2a",
        "parentId" : "e569a052-57ce-455e-851d-c2c07de35c3c",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "Right, the point I was getting at is that we only renew the ones in spark.kerberos.access.hadoopFileSystems + defaultFs + staging fs.  So I assume its basically defaultFs then? I was kind of wanting to make sure it wasn't something silly that they specified in both.  It seems odd for the defaultFs since this is the Hadoop filesystem which supports renewal.  I can obviously see this if its a separate name node but it would require you to specify it in the config for us to try to renew. So I'm assuming this is perhaps an odd config that you can't renew on the current default fs.  Just seems a bit odd case so wanted to make sure I wasn't missing something.\r\n",
        "createdAt" : "2021-03-08T18:39:17Z",
        "updatedAt" : "2021-03-22T18:58:50Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "ebe12a35-1f2a-4144-9dfe-0e1ebe0f43d4",
        "parentId" : "e569a052-57ce-455e-851d-c2c07de35c3c",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "Note I'm not against it just making sure the problem is understood and we document it appropriately. I guess it could be stage FS as well, which perhaps might make more sense if on a separate namenoxde",
        "createdAt" : "2021-03-08T18:47:34Z",
        "updatedAt" : "2021-03-22T18:58:50Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "eae852e1-361e-4319-8c76-502840e32af4",
        "parentId" : "e569a052-57ce-455e-851d-c2c07de35c3c",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "It's hard to imagine that this applies to defaultFS/stagingDir since it means the YARN cluster (or other types of clusters) is not configured to be in the same Kerberos realm as these which could cause other more serious issues. But, just as `spark.kerberos.access.hadoopFileSystems`, I guess nothing stops users from putting in the host for defaultFS/stagingDir in the config as well, and Spark will just do what it's told to do.",
        "createdAt" : "2021-03-09T01:55:47Z",
        "updatedAt" : "2021-03-22T18:58:50Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "d3ef676c-fb84-4830-9c24-0fbc7e1769b3",
        "parentId" : "e569a052-57ce-455e-851d-c2c07de35c3c",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "From the user perspective, I think it is still possible they want to avoid renewal for defaultFs. If the defaultFs is just a remote HDFS that whose tokens cannot be renew from YARN?",
        "createdAt" : "2021-03-10T01:12:36Z",
        "updatedAt" : "2021-03-22T18:58:50Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "9b236bff-77f4-4661-91d2-9783e997d31b",
        "parentId" : "e569a052-57ce-455e-851d-c2c07de35c3c",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "yes its true like I mentioned above. I'm not against it just want to understand and make sure this config makes sense. For instance instead of having this config we could have one that says don't include defaultFs or stageFs or both in renewal.   I think those are really the only 2 filesystems that make sense to put into this config. Anything else doesn't get renewed unless its in spark.kerberos.access.hadoopFileSystems and in that case just don't include it in spark.kerberos.access.hadoopFileSystems.  The nice things about this approach is its generic if things change in the future. The downside is I have to know namenode path for the defaultFs and staging fs to add to this config. ",
        "createdAt" : "2021-03-10T01:33:48Z",
        "updatedAt" : "2021-03-22T18:58:50Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "6bf04d7b-eafb-4f47-be94-7714c517fb9c",
        "parentId" : "e569a052-57ce-455e-851d-c2c07de35c3c",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "I'm not sure I understand correctly so just consider it as ignorant question.\r\n\r\n> Anything else doesn't get renewed unless its in spark.kerberos.access.hadoopFileSystems and in that case just don't include it in spark.kerberos.access.hadoopFileSystems.\r\n\r\nIs it possible if someone wants to get initial delegation token for them but doesn't renew them, and does this change make difference from here?",
        "createdAt" : "2021-03-10T02:42:55Z",
        "updatedAt" : "2021-03-22T18:58:50Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "5c79e985-b189-4962-963c-c2cfbe537e12",
        "parentId" : "e569a052-57ce-455e-851d-c2c07de35c3c",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "If I understand correctly, it is the same code path for obtaining delegation token and renewing. Actually the so called renew is that Spark periodically obtain delegation token.\r\n\r\nSo I guess it doesn't make sense for a case to get delegation token but not renew it. At least in Spark I don't see such case is possible. I may wrong here.\r\n",
        "createdAt" : "2021-03-10T05:26:32Z",
        "updatedAt" : "2021-03-22T18:58:50Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "46d35e8c-aecf-4bd1-b428-8669ae3caf97",
        "parentId" : "e569a052-57ce-455e-851d-c2c07de35c3c",
        "authorId" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "body" : "@HeartSaVioR Spark actually behaves this way (w/o this change): if one DT is created then it's renewed all the time except in the following cases:\r\n* `keytab` used but no `principal` provided\r\n* `ccache` used but it has no kerberos credentials\r\n\r\nPlease see the code here: https://github.com/apache/spark/blob/a916690dd9aac40df38922dbea233785354a2f2a/core/src/main/scala/org/apache/spark/deploy/security/HadoopDelegationTokenManager.scala#L83-L88\r\n",
        "createdAt" : "2021-03-10T10:51:35Z",
        "updatedAt" : "2021-03-22T18:58:50Z",
        "lastEditedBy" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "tags" : [
        ]
      },
      {
        "id" : "ac8b2856-cf46-4ddb-b3db-d3f4b2ee4f3a",
        "parentId" : "e569a052-57ce-455e-851d-c2c07de35c3c",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Ah OK I think I missed some part of @tgravescs 's comment. My bad.\r\n\r\nHis point wasn't that the new config is not necessary at all. His point was that the problem will occur from only defaultFs and/or stageFs, so the new config could be simplified instead of being general but a bit verbose.",
        "createdAt" : "2021-03-12T03:41:34Z",
        "updatedAt" : "2021-03-22T18:58:50Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "34b076b6-69a3-42a7-9378-81675181d0d9",
        "parentId" : "e569a052-57ce-455e-851d-c2c07de35c3c",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "> so just to be clear, these fileystems are ones the user specified in spark.kerberos.access.hadoopFileSystems so it wants to get initial tokens for them but then later we don't want to renew, correct? We should perhaps update doc to mention that.\r\n> It would be nice to know if this works with other cluster managers like k8s - @ifilonenko @mccheah maybe?\r\n\r\nHmm, for the customer case, the requirement is to prevent YARN from renewing the delegation token obtained. It doesn't matter if the file system is defaultFs/stageFs or the ones specified in `spark.kerberos.access.hadoopFileSystems`. That said, Spark still can obtain the token for the ones in `spark.kerberos.access.hadoopFileSystems`, but we don't want YARN to renew it.",
        "createdAt" : "2021-03-12T04:10:02Z",
        "updatedAt" : "2021-03-22T18:58:50Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "5cbfe720-ffd7-410f-b31e-d538419c7f2e",
        "parentId" : "e569a052-57ce-455e-851d-c2c07de35c3c",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "I was simply trying to clarify the exact case being hit here and make sure there wasn't an alternate solution and make sure the docs are clear.   The specific case being hit could affect the solution.\r\n\r\nI think there are comments on this review that are very confusing, thus why I wanted to clarification. Some indicate that Spark doesn't get initial tokens, others saying in this case the tokens were already acquired, etc.\r\n\r\nIn the end my comment ends up being I think we should update the security.md doc to mention renewal in the kerberos section for Hadoop filesystems to help explain to users.\r\n",
        "createdAt" : "2021-03-12T15:28:23Z",
        "updatedAt" : "2021-03-22T18:58:50Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "c1978145-73a2-416e-bdac-331948d75ddf",
        "parentId" : "e569a052-57ce-455e-851d-c2c07de35c3c",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Okay, thanks @tgravescs. I think in short this config doesn't affect Spark token handling, but only prevent YARN from renewing tokens, no matter the tokens are obtained in advance or by Spark. Let me update security.md and try to explain it.",
        "createdAt" : "2021-03-12T18:03:08Z",
        "updatedAt" : "2021-03-22T18:58:50Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "cf8da757045d23f5657ec1fb68e20f4ca0247772",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +694,698 @@  private[spark] val YARN_KERBEROS_FILESYSTEM_RENEWAL_EXCLUDE =\n    ConfigBuilder(\"spark.yarn.kerberos.renewal.excludeHadoopFileSystems\")\n      .doc(\"The list of Hadoop filesystem URLs whose hosts will be excluded from \" +\n        \"delegation token renewal at resource scheduler. Currently this is known to \" +\n        \"work under YARN, so YARN Resource Manager won't renew tokens for the application. \" +"
  },
  {
    "id" : "fb366351-fa9c-4baa-8012-7c1d8f85231e",
    "prId" : 31715,
    "prUrl" : "https://github.com/apache/spark/pull/31715#pullrequestreview-605557087",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "896d1f09-aeff-4099-88e4-32dc5cd16f02",
        "parentId" : null,
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "I am having similar concern with @viirya and @attilapiros . I think we should not make it as a user-facing config. If we would like introducing a config for this anyway, it'd better to start with `internal()` config, but not user-facing.\r\n\r\nThough `spark.shuffle.manager` can be arbitrary class, but in practice, they are just a handful of implementation in one company's environment, and ideally the infra developers should control which implementation to use, instead of user to control this. Similarly for whether to mark shuffle file lost should be controlled by developers team but not users.\r\n\r\nJust share some context, in FB, we (spark developer) control these kinds of behavior transparently and make this invisible to spark user. This also helps us to migrate to newer implementation easier without worrying about users setting wrong config. The mixed case (query uses customized shuffle service and default shuffle service) can happen quite a bit in production, as we have rate limit for traffic on customized service, and need fallback.",
        "createdAt" : "2021-03-04T02:11:59Z",
        "updatedAt" : "2021-03-04T02:12:04Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      },
      {
        "id" : "0e5e2c14-c5dc-4de2-8279-aa66831e3187",
        "parentId" : "896d1f09-aeff-4099-88e4-32dc5cd16f02",
        "authorId" : "c52c39ef-2b7f-4855-a2f1-2a2e50a719ae",
        "body" : "Hi @c21 , thanks for the comment, and known you from FB! We previously discussed with people from FB about Cosco, but not for this specific issue. Just curious, do you guys modify your internal Spark distribution to make executor lost event not trigger driver to delete map output?",
        "createdAt" : "2021-03-04T06:31:01Z",
        "updatedAt" : "2021-03-04T06:31:18Z",
        "lastEditedBy" : "c52c39ef-2b7f-4855-a2f1-2a2e50a719ae",
        "tags" : [
        ]
      },
      {
        "id" : "0a3d9375-faf2-4840-a66d-d8d2c28e2765",
        "parentId" : "896d1f09-aeff-4099-88e4-32dc5cd16f02",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "> do you guys modify your internal Spark distribution to make executor lost event not trigger driver to delete map output?\r\n\r\nYeah I think so, I can do a double check with cosco folks as well tomorrow. btw nice to know you as well!",
        "createdAt" : "2021-03-04T06:43:13Z",
        "updatedAt" : "2021-03-04T06:43:13Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      },
      {
        "id" : "ba9bfe2c-5545-4858-8431-c2497b152e9a",
        "parentId" : "896d1f09-aeff-4099-88e4-32dc5cd16f02",
        "authorId" : "c52c39ef-2b7f-4855-a2f1-2a2e50a719ae",
        "body" : "Cool, that will be interested to know, thanks!",
        "createdAt" : "2021-03-04T18:34:05Z",
        "updatedAt" : "2021-03-04T18:34:05Z",
        "lastEditedBy" : "c52c39ef-2b7f-4855-a2f1-2a2e50a719ae",
        "tags" : [
        ]
      },
      {
        "id" : "5c1d5b8e-463b-4230-905d-87265eb5e1fc",
        "parentId" : "896d1f09-aeff-4099-88e4-32dc5cd16f02",
        "authorId" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "body" : "@c21 May I ask what is the status of open sourcing Cosco? \r\nI just think it might help us to make better decisions about the Shuffle plugin API of Spark if we could look into that one too.\r\n",
        "createdAt" : "2021-03-05T17:01:31Z",
        "updatedAt" : "2021-03-05T17:01:31Z",
        "lastEditedBy" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "tags" : [
        ]
      },
      {
        "id" : "4e4f863e-1ac0-4860-bccf-7af919f0faf1",
        "parentId" : "896d1f09-aeff-4099-88e4-32dc5cd16f02",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "@attilapiros - unfortunately there's no specific ETA now, but the team is working on it.",
        "createdAt" : "2021-03-05T19:59:44Z",
        "updatedAt" : "2021-03-05T19:59:44Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      }
    ],
    "commit" : "1418e8f9507375f4fec025d8df357a86cb99ee45",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +2120,2124 @@\n  private[spark] val MARK_FILE_LOST_ON_EXECUTOR_LOST =\n    ConfigBuilder(\"spark.shuffle.markFileLostOnExecutorLost\")\n      .doc(\"Mark shuffle files as lost when an executor is lost. If you set \" +\n       \"spark.shuffle.manager with a customized class to use a different shuffle \" +"
  },
  {
    "id" : "fb375ded-d1df-40b0-8d4b-d7c7ace3a5f1",
    "prId" : 31249,
    "prUrl" : "https://github.com/apache/spark/pull/31249#pullrequestreview-575027717",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a337d1ab-f5df-41ff-8a05-cf84049e89b6",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "BTW, can we add docs since it's an exposed configuration?",
        "createdAt" : "2021-01-25T02:14:15Z",
        "updatedAt" : "2021-02-09T18:06:52Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "73bfd55dc3d9b3968658dd874de94c6ce5577252",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +813,817 @@    ConfigBuilder(\"spark.excludeOnFailure.killExcludedExecutors.decommission\")\n      .doc(\"Attempt decommission of excluded nodes instead of going directly to kill\")\n      .version(\"3.2.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "fca362b3-6035-4714-afbb-6cb8703f0a95",
    "prId" : 31215,
    "prUrl" : "https://github.com/apache/spark/pull/31215#pullrequestreview-570043082",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "33480e56-b31c-41f1-9917-b4eb40534926",
        "parentId" : null,
        "authorId" : "6a8d0007-eb0f-4236-a65a-2174a14ac689",
        "body" : "looks fine",
        "createdAt" : "2021-01-17T10:25:45Z",
        "updatedAt" : "2021-01-18T00:47:59Z",
        "lastEditedBy" : "6a8d0007-eb0f-4236-a65a-2174a14ac689",
        "tags" : [
        ]
      }
    ],
    "commit" : "9b7b19d677fd9c5d6416f5600866d47186883dcb",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +482,486 @@      .createOptional\n\n  private[spark] val STORAGE_DECOMMISSION_FALLBACK_STORAGE_CLEANUP =\n    ConfigBuilder(\"spark.storage.decommission.fallbackStorage.cleanUp\")\n      .doc(\"If true, Spark cleans up its fallback storage data during shutting down.\")"
  },
  {
    "id" : "77d791b9-a1ca-4824-b317-27dc26388acd",
    "prId" : 31215,
    "prUrl" : "https://github.com/apache/spark/pull/31215#pullrequestreview-570116057",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "79d37af8-0896-4f10-9700-6e0913a6ad24",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "BTW, should we plan to document this fallback storage somewhere like https://spark.apache.org/docs/latest/configuration.html#kubernetes or https://spark.apache.org/docs/latest/job-scheduling.html#graceful-decommission-of-executors? Should better be done separately though.",
        "createdAt" : "2021-01-18T00:44:54Z",
        "updatedAt" : "2021-01-18T00:47:59Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "324cbafc-8bd7-4d65-bfee-727c31226799",
        "parentId" : "79d37af8-0896-4f10-9700-6e0913a6ad24",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Sure, I'll do that separately. Thanks.",
        "createdAt" : "2021-01-18T00:49:06Z",
        "updatedAt" : "2021-01-18T00:49:06Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "9b7b19d677fd9c5d6416f5600866d47186883dcb",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +483,487 @@\n  private[spark] val STORAGE_DECOMMISSION_FALLBACK_STORAGE_CLEANUP =\n    ConfigBuilder(\"spark.storage.decommission.fallbackStorage.cleanUp\")\n      .doc(\"If true, Spark cleans up its fallback storage data during shutting down.\")\n      .version(\"3.2.0\")"
  },
  {
    "id" : "3e70f09a-5975-4fe9-bd0c-f6ef44bbb1f2",
    "prId" : 31151,
    "prUrl" : "https://github.com/apache/spark/pull/31151#pullrequestreview-566209146",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3e4986ad-b9e5-450b-83aa-4565094a19f2",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "cc @dongjoon-hyun ",
        "createdAt" : "2021-01-12T12:12:36Z",
        "updatedAt" : "2021-01-12T12:12:37Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "6087e59706181d8e1fbd2d88a33d9ae6a7ac2c2c",
    "line" : 57,
    "diffHunk" : "@@ -1,1 +473,477 @@\n  private[spark] val STORAGE_DECOMMISSION_FALLBACK_STORAGE_PATH =\n    ConfigBuilder(\"spark.decommission.storage.fallbackStoragePath\")\n      .doc(\"The location for fallback storage during block manager decommissioning. \" +\n        \"For example, `s3a://spark-storage/`. In case of empty, fallback storage is disabled. \" +"
  },
  {
    "id" : "8263f619-26fc-4655-9100-44f396879eb3",
    "prId" : 30876,
    "prUrl" : "https://github.com/apache/spark/pull/30876#pullrequestreview-558207527",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f8e4ee4c-8b44-4c0b-b1aa-f09a83b000e3",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Should we maybe just enable this by default when we're in Kubernates? I am okay with enabling it by default too if other people are fine. cc @tgravescs and @Ngone51 too FYI",
        "createdAt" : "2020-12-22T01:29:08Z",
        "updatedAt" : "2020-12-23T03:13:54Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "ce37bddf-1562-4b52-a529-d377c4c45bbc",
        "parentId" : "f8e4ee4c-8b44-4c0b-b1aa-f09a83b000e3",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "For the other resource managers, this will be helpful because this is a kind of self-healing code. And, this code has been here for a long time.",
        "createdAt" : "2020-12-22T02:26:38Z",
        "updatedAt" : "2020-12-23T03:13:54Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "4922ebd8-a993-4171-ac4e-da920968d5d1",
        "parentId" : "f8e4ee4c-8b44-4c0b-b1aa-f09a83b000e3",
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "I am not sure how widely this is used, particularly as it is not enabled by default.\r\nEspecially in context of dynamic resource allocation, it can become very chatty when executor's start getting dropped.\r\n\r\nGiven this, I am not very keen on enabling it atleast for yarn. Thoughts @tgravescs ?",
        "createdAt" : "2020-12-22T16:45:03Z",
        "updatedAt" : "2020-12-23T03:13:54Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "376027b7-8a40-4ba4-9578-66e9113e5824",
        "parentId" : "f8e4ee4c-8b44-4c0b-b1aa-f09a83b000e3",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Hi, @mridulm . Actually, we are using it now and it's a good time to test it by default, isn't it?\r\n> I am not sure how widely this is used, particularly as it is not enabled by default.\r\n\r\nFor the following, Apache Spark usually drop only empty executors. If you are saying a `storage timeout` configuration, I believe that what we need is to improve `storage timeout` configuration behavior after this enabling. I guess `storage timeout` had better not cause any `chatty` situation, of course.\r\n> Especially in context of dynamic resource allocation, it can become very chatty ",
        "createdAt" : "2020-12-22T18:02:10Z",
        "updatedAt" : "2020-12-23T03:13:54Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "56cff0fe-544d-4792-9772-73c7f33210fc",
        "parentId" : "f8e4ee4c-8b44-4c0b-b1aa-f09a83b000e3",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Thanks for the ping. I think I'm OK with the change. And shall we document the behaviour change in `core-migration-guide.md`?",
        "createdAt" : "2020-12-23T02:23:39Z",
        "updatedAt" : "2020-12-23T03:13:54Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "991cf7f9-dd8d-413b-ba83-a4e4d9fde6da",
        "parentId" : "f8e4ee4c-8b44-4c0b-b1aa-f09a83b000e3",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Sure, I'll update the PR, @Ngone51 .",
        "createdAt" : "2020-12-23T03:05:26Z",
        "updatedAt" : "2020-12-23T03:13:54Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "5b1aacaa-55ca-44d1-b915-a0846a7574f2",
        "parentId" : "f8e4ee4c-8b44-4c0b-b1aa-f09a83b000e3",
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "In the past, I found this to be noisy for the cases where replication was enabled - but this was a while back, and I would like to understand better what the 'cost' of enabling this for nontrivial usecases is for master : disabled by default means only developers who specifically test for it pay the price; not everyone.\r\nIt is quite common for an application to have references to a persisted RDD even after its use - with the loss of the RDD blocks having little to no functional impact.\r\nThis is similar to loss of blocks for an unreplicated persisted RDD - we do not proactively recompute the lost blocks; but do so on demand.\r\n\r\nIf the idea is we enable this for master, and evaluate the impact over the next 6 months and revisit at the end, I am fine with that: but an evaluation would need to be done before this goes out - else anyone using replicated storage will also get hit with the impact of proactive replication as well, and will need to disable this for their applications.",
        "createdAt" : "2020-12-23T20:02:53Z",
        "updatedAt" : "2020-12-23T20:02:53Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "b7df3051-6346-4caf-991c-e727a6ce11b5",
        "parentId" : "f8e4ee4c-8b44-4c0b-b1aa-f09a83b000e3",
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "> but an evaluation would need to be done before this goes out \r\n\r\nor perhaps identify the subset of conditions where it makes sense to enable it by default.",
        "createdAt" : "2020-12-23T20:11:17Z",
        "updatedAt" : "2020-12-23T20:11:18Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      }
    ],
    "commit" : "28fb534974203a382d472f77747d628ce046b83d",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +385,389 @@      .version(\"2.2.0\")\n      .booleanConf\n      .createWithDefault(true)\n\n  private[spark] val STORAGE_MEMORY_MAP_THRESHOLD ="
  },
  {
    "id" : "444aad32-8165-4c8e-8899-3060129088fe",
    "prId" : 30710,
    "prUrl" : "https://github.com/apache/spark/pull/30710#pullrequestreview-564425282",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d58fa2f0-88c1-461f-9f95-af73a8ff4e34",
        "parentId" : null,
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "please add the .doc section as well.",
        "createdAt" : "2021-01-08T15:32:20Z",
        "updatedAt" : "2021-01-09T06:53:31Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "99db62a4-f941-48f4-ab06-0ea81177471d",
        "parentId" : "d58fa2f0-88c1-461f-9f95-af73a8ff4e34",
        "authorId" : "fb1da7a4-0c05-42a2-9913-c8bd60c58a4f",
        "body" : "done",
        "createdAt" : "2021-01-08T16:57:29Z",
        "updatedAt" : "2021-01-09T06:53:31Z",
        "lastEditedBy" : "fb1da7a4-0c05-42a2-9913-c8bd60c58a4f",
        "tags" : [
        ]
      }
    ],
    "commit" : "86c8d8c51050b67ae8714f1dd1249f62d8b56bd0",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +1894,1898 @@      .doc(\"Minimum amount of time a task runs before being considered for speculation. \" +\n        \"This can be used to avoid launching speculative copies of tasks that are very short.\")\n      .version(\"3.2.0\")\n      .timeConf(TimeUnit.MILLISECONDS)\n      .createWithDefault(100)"
  },
  {
    "id" : "d64c0c7b-ae4d-4b68-b9f5-bc02ee0c37be",
    "prId" : 30710,
    "prUrl" : "https://github.com/apache/spark/pull/30710#pullrequestreview-673426557",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0a93fe18-67ef-43e7-94d5-2fdbf3b1923b",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "According to the [config naming policy](https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala#L21), `spark.speculation.shortTaskThreshold` may be better?\r\n\r\ncc @Ngone51 ",
        "createdAt" : "2021-06-01T16:38:47Z",
        "updatedAt" : "2021-06-01T16:38:47Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "9f1469b0-371f-46b9-ac31-2e404659d4c1",
        "parentId" : "0a93fe18-67ef-43e7-94d5-2fdbf3b1923b",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "I'm fine with renaming, though I don't care for shortTaskThreshold.  why not just minThreshold or minTaskThreshold",
        "createdAt" : "2021-06-01T16:47:47Z",
        "updatedAt" : "2021-06-01T16:47:48Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "8e67ec9d-c81d-4f37-987f-399624f9229b",
        "parentId" : "0a93fe18-67ef-43e7-94d5-2fdbf3b1923b",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "`minTaskRuntime`? I admit that I'm not good at naming ...",
        "createdAt" : "2021-06-01T17:10:48Z",
        "updatedAt" : "2021-06-01T17:10:48Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "2f3c21d2-cc73-4635-b458-f79a40f0a8da",
        "parentId" : "0a93fe18-67ef-43e7-94d5-2fdbf3b1923b",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "that works for me",
        "createdAt" : "2021-06-01T17:54:07Z",
        "updatedAt" : "2021-06-01T17:54:08Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "86c8d8c51050b67ae8714f1dd1249f62d8b56bd0",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1891,1895 @@\n  private[spark] val SPECULATION_MIN_THRESHOLD =\n    ConfigBuilder(\"spark.speculation.min.threshold\")\n      .doc(\"Minimum amount of time a task runs before being considered for speculation. \" +\n        \"This can be used to avoid launching speculative copies of tasks that are very short.\")"
  },
  {
    "id" : "17ea46f8-ee88-47b0-9280-d0b0bdb46d9c",
    "prId" : 30691,
    "prUrl" : "https://github.com/apache/spark/pull/30691#pullrequestreview-652586113",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b0bfc379-e366-46ec-b280-61955d161183",
        "parentId" : null,
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "Taken together, this accounts for driver waiting for upto 20s per stage ... do we have recommendations on how users can tune this ?",
        "createdAt" : "2021-05-05T15:19:59Z",
        "updatedAt" : "2021-05-11T05:07:03Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "4018682d-b30b-40b5-948e-5f653dd0101a",
        "parentId" : "b0bfc379-e366-46ec-b280-61955d161183",
        "authorId" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "body" : "I think currently this is hard to tune but once we have the changes in for `SPARK-33701` which does adaptive merge finalization, mostly this should be taken care of.",
        "createdAt" : "2021-05-05T17:46:41Z",
        "updatedAt" : "2021-05-11T05:07:03Z",
        "lastEditedBy" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "tags" : [
        ]
      }
    ],
    "commit" : "35d06150f37e01256314af4956c6e8fdcf7244b4",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +2105,2109 @@      .checkValue(_ >= 0L, \"Timeout must be >= 0.\")\n      .createWithDefaultString(\"10s\")\n\n  private[spark] val SHUFFLE_MERGER_MAX_RETAINED_LOCATIONS =\n    ConfigBuilder(\"spark.shuffle.push.maxRetainedMergerLocations\")"
  },
  {
    "id" : "b1b83291-9132-475b-bf29-7f0446b0fb57",
    "prId" : 30681,
    "prUrl" : "https://github.com/apache/spark/pull/30681#pullrequestreview-587200923",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ef8d0388-cf14-4613-8d7b-4f572b3adc00",
        "parentId" : null,
        "authorId" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "body" : "Do we have another configuration in `spark.file.` namespace? If not, we usually do not make a new namespace.",
        "createdAt" : "2021-02-10T02:24:53Z",
        "updatedAt" : "2021-02-10T02:24:53Z",
        "lastEditedBy" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "tags" : [
        ]
      }
    ],
    "commit" : "0824bfc6597c98c5057aeef16ea6c57637a0f654",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +819,823 @@\n  private[spark] val FILE_INSERT_STATUS_LOG_FLAG =\n    ConfigBuilder(\"spark.file.insert.status.log.flag\")\n      .internal()\n      .doc(\"When hive sql executes to generate HDFS data, \" +"
  }
]