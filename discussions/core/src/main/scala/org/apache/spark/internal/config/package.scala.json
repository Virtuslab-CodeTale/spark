[
  {
    "id" : "b6b791a9-9e9d-4bea-b6f6-af685ee12d62",
    "prId" : 33615,
    "prUrl" : "https://github.com/apache/spark/pull/33615#pullrequestreview-730204271",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f7ffe8f4-88e6-4452-9b66-99ad36e9ce8f",
        "parentId" : null,
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "It seems that we have exposed this in 3.1.0(3.1.1) already: https://github.com/apache/spark/blob/branch-3.1/core/src/main/scala/org/apache/spark/internal/config/package.scala\r\n\r\nWhy do we change the version here? The feature is not ready in 3.1?",
        "createdAt" : "2021-08-15T09:47:39Z",
        "updatedAt" : "2021-08-15T09:48:08Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "88507602-f0cf-4353-be51-e89c3ef92e46",
        "parentId" : "f7ffe8f4-88e6-4452-9b66-99ad36e9ce8f",
        "authorId" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "body" : "Yes, the feature was not ready in 3.1. It will be ready only in 3.2 which is why I changed it to 3.2 to keep it consistent.",
        "createdAt" : "2021-08-15T16:51:14Z",
        "updatedAt" : "2021-08-15T16:51:14Z",
        "lastEditedBy" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "tags" : [
        ]
      }
    ],
    "commit" : "12618284d95d7b1685c2c48c79bffcb618cf4523",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +2103,2107 @@        \"org.apache.spark.network.shuffle.MergedShuffleFileManager implementation for push-based \" +\n        \"shuffle to be enabled\")\n      .version(\"3.2.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "d220dc8b-fb03-4fdb-b003-8403f3829793",
    "prId" : 33078,
    "prUrl" : "https://github.com/apache/spark/pull/33078#pullrequestreview-699399725",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "54282deb-c470-4784-8b56-145ce80bda31",
        "parentId" : null,
        "authorId" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "body" : "nit: This feels like it should be `spark.app.attemptNumber` than `spark.app.attempt.id`. Also currently this is specific to push based shuffle right? Should we add that in the documentation?",
        "createdAt" : "2021-07-05T02:00:10Z",
        "updatedAt" : "2021-07-05T02:10:19Z",
        "lastEditedBy" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "tags" : [
        ]
      },
      {
        "id" : "687245d6-c8e9-4429-adc9-0955dd150e7a",
        "parentId" : "54282deb-c470-4784-8b56-145ce80bda31",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "`attemptNumber` sounds better to me.\r\n\r\nIf this conf can't be set by users, I think it's ok to keep it internal.",
        "createdAt" : "2021-07-05T05:37:23Z",
        "updatedAt" : "2021-07-05T05:37:23Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "8a1f417b-4ce6-4f70-945d-b37805cad71a",
        "parentId" : "54282deb-c470-4784-8b56-145ce80bda31",
        "authorId" : "e942fa3e-a5b9-4a9f-b9a5-a1afd3c29b2c",
        "body" : "AppAttemptId is currently used in lots of places to indicate the attempt id from Yarn, should we keep it as \"spark.app.attempt.id\" to align with all other places? Or we just want to change the configuration string to \"spark.app.attemptNumber\"? ",
        "createdAt" : "2021-07-05T05:54:51Z",
        "updatedAt" : "2021-07-05T05:54:51Z",
        "lastEditedBy" : "e942fa3e-a5b9-4a9f-b9a5-a1afd3c29b2c",
        "tags" : [
        ]
      },
      {
        "id" : "570171b6-51eb-481a-bb6e-3d364925f97e",
        "parentId" : "54282deb-c470-4784-8b56-145ce80bda31",
        "authorId" : "e942fa3e-a5b9-4a9f-b9a5-a1afd3c29b2c",
        "body" : "And which version should we mark it there? I added \"1.3.0\" since the support for multiple attempts in Yarn was added since 1.3.0. But if we change to \"spark.app.attemptNumber\", we should start from \"3.2.0\". Thoughts?",
        "createdAt" : "2021-07-05T05:56:32Z",
        "updatedAt" : "2021-07-05T05:56:32Z",
        "lastEditedBy" : "e942fa3e-a5b9-4a9f-b9a5-a1afd3c29b2c",
        "tags" : [
        ]
      },
      {
        "id" : "be958a99-e0af-4a11-a965-11d09502cdbf",
        "parentId" : "54282deb-c470-4784-8b56-145ce80bda31",
        "authorId" : "e942fa3e-a5b9-4a9f-b9a5-a1afd3c29b2c",
        "body" : "Should we use \"String\" or \"Integer\" here for this configuration? In [SparkContext where it gets the Application AttemptID](https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/SparkContext.scala#L340), it is a String format. But when parsing into Configuration and also resolving this in RemoteBlockPushResolver, we are using Integer to determine whether it is a new attempt.",
        "createdAt" : "2021-07-05T06:13:17Z",
        "updatedAt" : "2021-07-05T06:13:17Z",
        "lastEditedBy" : "e942fa3e-a5b9-4a9f-b9a5-a1afd3c29b2c",
        "tags" : [
        ]
      },
      {
        "id" : "b905ccf9-7e33-4b4f-8b47-306c4a2b0d63",
        "parentId" : "54282deb-c470-4784-8b56-145ce80bda31",
        "authorId" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "body" : "I see it makes sense. IMO, since it is already called `attemptId` it is fine to keep it probably like that. May be just adding slightly more info on the doc should be fine with me. Initially, I was under the impression we are generating this attemptId which is not the case.",
        "createdAt" : "2021-07-05T20:05:00Z",
        "updatedAt" : "2021-07-05T20:05:00Z",
        "lastEditedBy" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "tags" : [
        ]
      }
    ],
    "commit" : "53109918cbdbdba2fe79f38a991c171efec7e85f",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +2247,2251 @@\n  private[spark] val APP_ATTEMPT_ID =\n    ConfigBuilder(\"spark.app.attempt.id\")\n      .internal()\n      .doc(\"The application attempt Id assigned from Hadoop YARN. \" +"
  },
  {
    "id" : "a07b11d7-89d4-495d-832c-302b29348636",
    "prId" : 32287,
    "prUrl" : "https://github.com/apache/spark/pull/32287#pullrequestreview-655481906",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "09e9b13b-314f-402a-ab20-4f35b75b425b",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Just a question. Is there a reason of `10` instead of `3`?",
        "createdAt" : "2021-05-08T22:29:08Z",
        "updatedAt" : "2021-05-10T10:25:15Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "63892ca8-c862-42d6-8c6d-2f0e9c489469",
        "parentId" : "09e9b13b-314f-402a-ab20-4f35b75b425b",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Given the discussion (https://github.com/apache/spark/pull/32287#discussion_r625287419) there, Netty OOM could be raised more frequently in certain cases, e.g.,\r\n\r\n> For case b), the OOM threshold might be 20 requests. In this case, there're still 80 deferred requests, which would hit the OOM soon as you mentioned. That being said, I think the current fix would work around the issue in the end. Note that the application would fail before the fix.\r\n\r\nThus, I'd like to give more chances for the block in case we fall into the case like b).",
        "createdAt" : "2021-05-10T10:18:54Z",
        "updatedAt" : "2021-05-10T10:25:15Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "4af6ee73221a55e3cd7239f151ec33d5e802cbe4",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +1201,1205 @@      .internal()\n      .intConf\n      .createWithDefault(10)\n\n  private[spark] val REDUCER_MAX_BLOCKS_IN_FLIGHT_PER_ADDRESS ="
  },
  {
    "id" : "c4300ac3-1ee2-4706-9529-df5804b8d89d",
    "prId" : 31761,
    "prUrl" : "https://github.com/apache/spark/pull/31761#pullrequestreview-611033475",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e569a052-57ce-455e-851d-c2c07de35c3c",
        "parentId" : null,
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Is this only restricted to HDFS URLs? can \"hosts\" either be nameservice name or namenode name? some examples might help.",
        "createdAt" : "2021-03-07T01:36:53Z",
        "updatedAt" : "2021-03-22T18:58:50Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "4a07beb4-0a19-4462-89ec-9a244ef7b125",
        "parentId" : "e569a052-57ce-455e-851d-c2c07de35c3c",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Because the configuration value will be parsed as file system URLs to get file system. A simple host name doesn't work here.\r\n\r\ne.g.\r\n\r\n```scala\r\nscala> new Path(\"hdfs.namenode.net\").getFileSystem(new Configuration()).getUri.getHost\r\nres9: String = null\r\n```",
        "createdAt" : "2021-03-07T04:15:35Z",
        "updatedAt" : "2021-03-22T18:58:50Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "72b014c9-f5c5-48d8-b72b-2d3d3d48ae9a",
        "parentId" : "e569a052-57ce-455e-851d-c2c07de35c3c",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "It makes sense for `spark.kerberos.access.hadoopFileSystems` to be URLs since Spark needs to instantiate `FileSystem`s from them. But for this case I'm not sure if it's necessary: we can just parse the config into a set of host names and check whether the file systems above contain them:\r\n```scala\r\nval hostsToExclude = sparkConf.get(KERBEROS_FILESYSTEM_RENEWAL_EXCLUDE).toSet\r\nfilesystems.filter(fs => !hostsToExclude.contains(fs.getUri.getHost).foreach { fs =>\r\n ...\r\n}\r\n```\r\n\r\nI'm fine either way though since it also makes sense to keep it consistent with `spark.kerberos.access.hadoopFileSystems`. BTW I think we'll need to update security.md for the new config.",
        "createdAt" : "2021-03-07T04:50:39Z",
        "updatedAt" : "2021-03-22T18:58:50Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "96954ac1-4c6d-4481-bcf5-a2c524aecc31",
        "parentId" : "e569a052-57ce-455e-851d-c2c07de35c3c",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Thanks. Let me update security.md.",
        "createdAt" : "2021-03-07T04:56:45Z",
        "updatedAt" : "2021-03-22T18:58:50Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "232245ab-3ae5-4035-80f5-7d4abd22d76e",
        "parentId" : "e569a052-57ce-455e-851d-c2c07de35c3c",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "so just to be clear, these fileystems are ones the user specified in spark.kerberos.access.hadoopFileSystems so it wants to get initial tokens for them but then later we don't want to renew, correct?  We should perhaps update doc to mention that.  \r\nIt would be nice to know if this works with other cluster managers like k8s - @ifilonenko @mccheah  maybe?",
        "createdAt" : "2021-03-08T14:34:36Z",
        "updatedAt" : "2021-03-22T18:58:50Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "b8929d1f-2fbe-4461-9ec8-3b212eba9048",
        "parentId" : "e569a052-57ce-455e-851d-c2c07de35c3c",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "I guess it could be the staging or default Filesystem as well.",
        "createdAt" : "2021-03-08T14:36:09Z",
        "updatedAt" : "2021-03-22T18:58:50Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "aa66fdcd-4c24-4fe4-a03f-81299e364c24",
        "parentId" : "e569a052-57ce-455e-851d-c2c07de35c3c",
        "authorId" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "body" : "@tgravescs with the actual code Spark doesn't obtain the initial tokens when an FS is listed in \"spark.kerberos.renewal.exclude.hadoopFileSystems\". Please see my comment below...",
        "createdAt" : "2021-03-08T15:18:02Z",
        "updatedAt" : "2021-03-22T18:58:50Z",
        "lastEditedBy" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "tags" : [
        ]
      },
      {
        "id" : "a1ffedf5-ff22-4265-9c48-f14bda29a247",
        "parentId" : "e569a052-57ce-455e-851d-c2c07de35c3c",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Hm? @gaborgsomogyi I think this config doesn't affect Spark obtain the initial tokens?",
        "createdAt" : "2021-03-08T18:07:45Z",
        "updatedAt" : "2021-03-22T18:58:50Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "0906aa91-df70-494f-a93e-3d0fc5eafa2a",
        "parentId" : "e569a052-57ce-455e-851d-c2c07de35c3c",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "Right, the point I was getting at is that we only renew the ones in spark.kerberos.access.hadoopFileSystems + defaultFs + staging fs.  So I assume its basically defaultFs then? I was kind of wanting to make sure it wasn't something silly that they specified in both.  It seems odd for the defaultFs since this is the Hadoop filesystem which supports renewal.  I can obviously see this if its a separate name node but it would require you to specify it in the config for us to try to renew. So I'm assuming this is perhaps an odd config that you can't renew on the current default fs.  Just seems a bit odd case so wanted to make sure I wasn't missing something.\r\n",
        "createdAt" : "2021-03-08T18:39:17Z",
        "updatedAt" : "2021-03-22T18:58:50Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "ebe12a35-1f2a-4144-9dfe-0e1ebe0f43d4",
        "parentId" : "e569a052-57ce-455e-851d-c2c07de35c3c",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "Note I'm not against it just making sure the problem is understood and we document it appropriately. I guess it could be stage FS as well, which perhaps might make more sense if on a separate namenoxde",
        "createdAt" : "2021-03-08T18:47:34Z",
        "updatedAt" : "2021-03-22T18:58:50Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "eae852e1-361e-4319-8c76-502840e32af4",
        "parentId" : "e569a052-57ce-455e-851d-c2c07de35c3c",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "It's hard to imagine that this applies to defaultFS/stagingDir since it means the YARN cluster (or other types of clusters) is not configured to be in the same Kerberos realm as these which could cause other more serious issues. But, just as `spark.kerberos.access.hadoopFileSystems`, I guess nothing stops users from putting in the host for defaultFS/stagingDir in the config as well, and Spark will just do what it's told to do.",
        "createdAt" : "2021-03-09T01:55:47Z",
        "updatedAt" : "2021-03-22T18:58:50Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "d3ef676c-fb84-4830-9c24-0fbc7e1769b3",
        "parentId" : "e569a052-57ce-455e-851d-c2c07de35c3c",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "From the user perspective, I think it is still possible they want to avoid renewal for defaultFs. If the defaultFs is just a remote HDFS that whose tokens cannot be renew from YARN?",
        "createdAt" : "2021-03-10T01:12:36Z",
        "updatedAt" : "2021-03-22T18:58:50Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "9b236bff-77f4-4661-91d2-9783e997d31b",
        "parentId" : "e569a052-57ce-455e-851d-c2c07de35c3c",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "yes its true like I mentioned above. I'm not against it just want to understand and make sure this config makes sense. For instance instead of having this config we could have one that says don't include defaultFs or stageFs or both in renewal.   I think those are really the only 2 filesystems that make sense to put into this config. Anything else doesn't get renewed unless its in spark.kerberos.access.hadoopFileSystems and in that case just don't include it in spark.kerberos.access.hadoopFileSystems.  The nice things about this approach is its generic if things change in the future. The downside is I have to know namenode path for the defaultFs and staging fs to add to this config. ",
        "createdAt" : "2021-03-10T01:33:48Z",
        "updatedAt" : "2021-03-22T18:58:50Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "6bf04d7b-eafb-4f47-be94-7714c517fb9c",
        "parentId" : "e569a052-57ce-455e-851d-c2c07de35c3c",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "I'm not sure I understand correctly so just consider it as ignorant question.\r\n\r\n> Anything else doesn't get renewed unless its in spark.kerberos.access.hadoopFileSystems and in that case just don't include it in spark.kerberos.access.hadoopFileSystems.\r\n\r\nIs it possible if someone wants to get initial delegation token for them but doesn't renew them, and does this change make difference from here?",
        "createdAt" : "2021-03-10T02:42:55Z",
        "updatedAt" : "2021-03-22T18:58:50Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "5c79e985-b189-4962-963c-c2cfbe537e12",
        "parentId" : "e569a052-57ce-455e-851d-c2c07de35c3c",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "If I understand correctly, it is the same code path for obtaining delegation token and renewing. Actually the so called renew is that Spark periodically obtain delegation token.\r\n\r\nSo I guess it doesn't make sense for a case to get delegation token but not renew it. At least in Spark I don't see such case is possible. I may wrong here.\r\n",
        "createdAt" : "2021-03-10T05:26:32Z",
        "updatedAt" : "2021-03-22T18:58:50Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "46d35e8c-aecf-4bd1-b428-8669ae3caf97",
        "parentId" : "e569a052-57ce-455e-851d-c2c07de35c3c",
        "authorId" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "body" : "@HeartSaVioR Spark actually behaves this way (w/o this change): if one DT is created then it's renewed all the time except in the following cases:\r\n* `keytab` used but no `principal` provided\r\n* `ccache` used but it has no kerberos credentials\r\n\r\nPlease see the code here: https://github.com/apache/spark/blob/a916690dd9aac40df38922dbea233785354a2f2a/core/src/main/scala/org/apache/spark/deploy/security/HadoopDelegationTokenManager.scala#L83-L88\r\n",
        "createdAt" : "2021-03-10T10:51:35Z",
        "updatedAt" : "2021-03-22T18:58:50Z",
        "lastEditedBy" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "tags" : [
        ]
      },
      {
        "id" : "ac8b2856-cf46-4ddb-b3db-d3f4b2ee4f3a",
        "parentId" : "e569a052-57ce-455e-851d-c2c07de35c3c",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Ah OK I think I missed some part of @tgravescs 's comment. My bad.\r\n\r\nHis point wasn't that the new config is not necessary at all. His point was that the problem will occur from only defaultFs and/or stageFs, so the new config could be simplified instead of being general but a bit verbose.",
        "createdAt" : "2021-03-12T03:41:34Z",
        "updatedAt" : "2021-03-22T18:58:50Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "34b076b6-69a3-42a7-9378-81675181d0d9",
        "parentId" : "e569a052-57ce-455e-851d-c2c07de35c3c",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "> so just to be clear, these fileystems are ones the user specified in spark.kerberos.access.hadoopFileSystems so it wants to get initial tokens for them but then later we don't want to renew, correct? We should perhaps update doc to mention that.\r\n> It would be nice to know if this works with other cluster managers like k8s - @ifilonenko @mccheah maybe?\r\n\r\nHmm, for the customer case, the requirement is to prevent YARN from renewing the delegation token obtained. It doesn't matter if the file system is defaultFs/stageFs or the ones specified in `spark.kerberos.access.hadoopFileSystems`. That said, Spark still can obtain the token for the ones in `spark.kerberos.access.hadoopFileSystems`, but we don't want YARN to renew it.",
        "createdAt" : "2021-03-12T04:10:02Z",
        "updatedAt" : "2021-03-22T18:58:50Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "5cbfe720-ffd7-410f-b31e-d538419c7f2e",
        "parentId" : "e569a052-57ce-455e-851d-c2c07de35c3c",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "I was simply trying to clarify the exact case being hit here and make sure there wasn't an alternate solution and make sure the docs are clear.   The specific case being hit could affect the solution.\r\n\r\nI think there are comments on this review that are very confusing, thus why I wanted to clarification. Some indicate that Spark doesn't get initial tokens, others saying in this case the tokens were already acquired, etc.\r\n\r\nIn the end my comment ends up being I think we should update the security.md doc to mention renewal in the kerberos section for Hadoop filesystems to help explain to users.\r\n",
        "createdAt" : "2021-03-12T15:28:23Z",
        "updatedAt" : "2021-03-22T18:58:50Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "c1978145-73a2-416e-bdac-331948d75ddf",
        "parentId" : "e569a052-57ce-455e-851d-c2c07de35c3c",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Okay, thanks @tgravescs. I think in short this config doesn't affect Spark token handling, but only prevent YARN from renewing tokens, no matter the tokens are obtained in advance or by Spark. Let me update security.md and try to explain it.",
        "createdAt" : "2021-03-12T18:03:08Z",
        "updatedAt" : "2021-03-22T18:58:50Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "cf8da757045d23f5657ec1fb68e20f4ca0247772",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +694,698 @@  private[spark] val YARN_KERBEROS_FILESYSTEM_RENEWAL_EXCLUDE =\n    ConfigBuilder(\"spark.yarn.kerberos.renewal.excludeHadoopFileSystems\")\n      .doc(\"The list of Hadoop filesystem URLs whose hosts will be excluded from \" +\n        \"delegation token renewal at resource scheduler. Currently this is known to \" +\n        \"work under YARN, so YARN Resource Manager won't renew tokens for the application. \" +"
  },
  {
    "id" : "fb366351-fa9c-4baa-8012-7c1d8f85231e",
    "prId" : 31715,
    "prUrl" : "https://github.com/apache/spark/pull/31715#pullrequestreview-605557087",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "896d1f09-aeff-4099-88e4-32dc5cd16f02",
        "parentId" : null,
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "I am having similar concern with @viirya and @attilapiros . I think we should not make it as a user-facing config. If we would like introducing a config for this anyway, it'd better to start with `internal()` config, but not user-facing.\r\n\r\nThough `spark.shuffle.manager` can be arbitrary class, but in practice, they are just a handful of implementation in one company's environment, and ideally the infra developers should control which implementation to use, instead of user to control this. Similarly for whether to mark shuffle file lost should be controlled by developers team but not users.\r\n\r\nJust share some context, in FB, we (spark developer) control these kinds of behavior transparently and make this invisible to spark user. This also helps us to migrate to newer implementation easier without worrying about users setting wrong config. The mixed case (query uses customized shuffle service and default shuffle service) can happen quite a bit in production, as we have rate limit for traffic on customized service, and need fallback.",
        "createdAt" : "2021-03-04T02:11:59Z",
        "updatedAt" : "2021-03-04T02:12:04Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      },
      {
        "id" : "0e5e2c14-c5dc-4de2-8279-aa66831e3187",
        "parentId" : "896d1f09-aeff-4099-88e4-32dc5cd16f02",
        "authorId" : "c52c39ef-2b7f-4855-a2f1-2a2e50a719ae",
        "body" : "Hi @c21 , thanks for the comment, and known you from FB! We previously discussed with people from FB about Cosco, but not for this specific issue. Just curious, do you guys modify your internal Spark distribution to make executor lost event not trigger driver to delete map output?",
        "createdAt" : "2021-03-04T06:31:01Z",
        "updatedAt" : "2021-03-04T06:31:18Z",
        "lastEditedBy" : "c52c39ef-2b7f-4855-a2f1-2a2e50a719ae",
        "tags" : [
        ]
      },
      {
        "id" : "0a3d9375-faf2-4840-a66d-d8d2c28e2765",
        "parentId" : "896d1f09-aeff-4099-88e4-32dc5cd16f02",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "> do you guys modify your internal Spark distribution to make executor lost event not trigger driver to delete map output?\r\n\r\nYeah I think so, I can do a double check with cosco folks as well tomorrow. btw nice to know you as well!",
        "createdAt" : "2021-03-04T06:43:13Z",
        "updatedAt" : "2021-03-04T06:43:13Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      },
      {
        "id" : "ba9bfe2c-5545-4858-8431-c2497b152e9a",
        "parentId" : "896d1f09-aeff-4099-88e4-32dc5cd16f02",
        "authorId" : "c52c39ef-2b7f-4855-a2f1-2a2e50a719ae",
        "body" : "Cool, that will be interested to know, thanks!",
        "createdAt" : "2021-03-04T18:34:05Z",
        "updatedAt" : "2021-03-04T18:34:05Z",
        "lastEditedBy" : "c52c39ef-2b7f-4855-a2f1-2a2e50a719ae",
        "tags" : [
        ]
      },
      {
        "id" : "5c1d5b8e-463b-4230-905d-87265eb5e1fc",
        "parentId" : "896d1f09-aeff-4099-88e4-32dc5cd16f02",
        "authorId" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "body" : "@c21 May I ask what is the status of open sourcing Cosco? \r\nI just think it might help us to make better decisions about the Shuffle plugin API of Spark if we could look into that one too.\r\n",
        "createdAt" : "2021-03-05T17:01:31Z",
        "updatedAt" : "2021-03-05T17:01:31Z",
        "lastEditedBy" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "tags" : [
        ]
      },
      {
        "id" : "4e4f863e-1ac0-4860-bccf-7af919f0faf1",
        "parentId" : "896d1f09-aeff-4099-88e4-32dc5cd16f02",
        "authorId" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "body" : "@attilapiros - unfortunately there's no specific ETA now, but the team is working on it.",
        "createdAt" : "2021-03-05T19:59:44Z",
        "updatedAt" : "2021-03-05T19:59:44Z",
        "lastEditedBy" : "8df147d9-bc9d-4fd9-bb98-5dcf9619220b",
        "tags" : [
        ]
      }
    ],
    "commit" : "1418e8f9507375f4fec025d8df357a86cb99ee45",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +2120,2124 @@\n  private[spark] val MARK_FILE_LOST_ON_EXECUTOR_LOST =\n    ConfigBuilder(\"spark.shuffle.markFileLostOnExecutorLost\")\n      .doc(\"Mark shuffle files as lost when an executor is lost. If you set \" +\n       \"spark.shuffle.manager with a customized class to use a different shuffle \" +"
  },
  {
    "id" : "fb375ded-d1df-40b0-8d4b-d7c7ace3a5f1",
    "prId" : 31249,
    "prUrl" : "https://github.com/apache/spark/pull/31249#pullrequestreview-575027717",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a337d1ab-f5df-41ff-8a05-cf84049e89b6",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "BTW, can we add docs since it's an exposed configuration?",
        "createdAt" : "2021-01-25T02:14:15Z",
        "updatedAt" : "2021-02-09T18:06:52Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "73bfd55dc3d9b3968658dd874de94c6ce5577252",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +813,817 @@    ConfigBuilder(\"spark.excludeOnFailure.killExcludedExecutors.decommission\")\n      .doc(\"Attempt decommission of excluded nodes instead of going directly to kill\")\n      .version(\"3.2.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "fca362b3-6035-4714-afbb-6cb8703f0a95",
    "prId" : 31215,
    "prUrl" : "https://github.com/apache/spark/pull/31215#pullrequestreview-570043082",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "33480e56-b31c-41f1-9917-b4eb40534926",
        "parentId" : null,
        "authorId" : "6a8d0007-eb0f-4236-a65a-2174a14ac689",
        "body" : "looks fine",
        "createdAt" : "2021-01-17T10:25:45Z",
        "updatedAt" : "2021-01-18T00:47:59Z",
        "lastEditedBy" : "6a8d0007-eb0f-4236-a65a-2174a14ac689",
        "tags" : [
        ]
      }
    ],
    "commit" : "9b7b19d677fd9c5d6416f5600866d47186883dcb",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +482,486 @@      .createOptional\n\n  private[spark] val STORAGE_DECOMMISSION_FALLBACK_STORAGE_CLEANUP =\n    ConfigBuilder(\"spark.storage.decommission.fallbackStorage.cleanUp\")\n      .doc(\"If true, Spark cleans up its fallback storage data during shutting down.\")"
  },
  {
    "id" : "77d791b9-a1ca-4824-b317-27dc26388acd",
    "prId" : 31215,
    "prUrl" : "https://github.com/apache/spark/pull/31215#pullrequestreview-570116057",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "79d37af8-0896-4f10-9700-6e0913a6ad24",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "BTW, should we plan to document this fallback storage somewhere like https://spark.apache.org/docs/latest/configuration.html#kubernetes or https://spark.apache.org/docs/latest/job-scheduling.html#graceful-decommission-of-executors? Should better be done separately though.",
        "createdAt" : "2021-01-18T00:44:54Z",
        "updatedAt" : "2021-01-18T00:47:59Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "324cbafc-8bd7-4d65-bfee-727c31226799",
        "parentId" : "79d37af8-0896-4f10-9700-6e0913a6ad24",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Sure, I'll do that separately. Thanks.",
        "createdAt" : "2021-01-18T00:49:06Z",
        "updatedAt" : "2021-01-18T00:49:06Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "9b7b19d677fd9c5d6416f5600866d47186883dcb",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +483,487 @@\n  private[spark] val STORAGE_DECOMMISSION_FALLBACK_STORAGE_CLEANUP =\n    ConfigBuilder(\"spark.storage.decommission.fallbackStorage.cleanUp\")\n      .doc(\"If true, Spark cleans up its fallback storage data during shutting down.\")\n      .version(\"3.2.0\")"
  },
  {
    "id" : "3e70f09a-5975-4fe9-bd0c-f6ef44bbb1f2",
    "prId" : 31151,
    "prUrl" : "https://github.com/apache/spark/pull/31151#pullrequestreview-566209146",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3e4986ad-b9e5-450b-83aa-4565094a19f2",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "cc @dongjoon-hyun ",
        "createdAt" : "2021-01-12T12:12:36Z",
        "updatedAt" : "2021-01-12T12:12:37Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "6087e59706181d8e1fbd2d88a33d9ae6a7ac2c2c",
    "line" : 57,
    "diffHunk" : "@@ -1,1 +473,477 @@\n  private[spark] val STORAGE_DECOMMISSION_FALLBACK_STORAGE_PATH =\n    ConfigBuilder(\"spark.decommission.storage.fallbackStoragePath\")\n      .doc(\"The location for fallback storage during block manager decommissioning. \" +\n        \"For example, `s3a://spark-storage/`. In case of empty, fallback storage is disabled. \" +"
  },
  {
    "id" : "8263f619-26fc-4655-9100-44f396879eb3",
    "prId" : 30876,
    "prUrl" : "https://github.com/apache/spark/pull/30876#pullrequestreview-558207527",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f8e4ee4c-8b44-4c0b-b1aa-f09a83b000e3",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Should we maybe just enable this by default when we're in Kubernates? I am okay with enabling it by default too if other people are fine. cc @tgravescs and @Ngone51 too FYI",
        "createdAt" : "2020-12-22T01:29:08Z",
        "updatedAt" : "2020-12-23T03:13:54Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "ce37bddf-1562-4b52-a529-d377c4c45bbc",
        "parentId" : "f8e4ee4c-8b44-4c0b-b1aa-f09a83b000e3",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "For the other resource managers, this will be helpful because this is a kind of self-healing code. And, this code has been here for a long time.",
        "createdAt" : "2020-12-22T02:26:38Z",
        "updatedAt" : "2020-12-23T03:13:54Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "4922ebd8-a993-4171-ac4e-da920968d5d1",
        "parentId" : "f8e4ee4c-8b44-4c0b-b1aa-f09a83b000e3",
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "I am not sure how widely this is used, particularly as it is not enabled by default.\r\nEspecially in context of dynamic resource allocation, it can become very chatty when executor's start getting dropped.\r\n\r\nGiven this, I am not very keen on enabling it atleast for yarn. Thoughts @tgravescs ?",
        "createdAt" : "2020-12-22T16:45:03Z",
        "updatedAt" : "2020-12-23T03:13:54Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "376027b7-8a40-4ba4-9578-66e9113e5824",
        "parentId" : "f8e4ee4c-8b44-4c0b-b1aa-f09a83b000e3",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Hi, @mridulm . Actually, we are using it now and it's a good time to test it by default, isn't it?\r\n> I am not sure how widely this is used, particularly as it is not enabled by default.\r\n\r\nFor the following, Apache Spark usually drop only empty executors. If you are saying a `storage timeout` configuration, I believe that what we need is to improve `storage timeout` configuration behavior after this enabling. I guess `storage timeout` had better not cause any `chatty` situation, of course.\r\n> Especially in context of dynamic resource allocation, it can become very chatty ",
        "createdAt" : "2020-12-22T18:02:10Z",
        "updatedAt" : "2020-12-23T03:13:54Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "56cff0fe-544d-4792-9772-73c7f33210fc",
        "parentId" : "f8e4ee4c-8b44-4c0b-b1aa-f09a83b000e3",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Thanks for the ping. I think I'm OK with the change. And shall we document the behaviour change in `core-migration-guide.md`?",
        "createdAt" : "2020-12-23T02:23:39Z",
        "updatedAt" : "2020-12-23T03:13:54Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "991cf7f9-dd8d-413b-ba83-a4e4d9fde6da",
        "parentId" : "f8e4ee4c-8b44-4c0b-b1aa-f09a83b000e3",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Sure, I'll update the PR, @Ngone51 .",
        "createdAt" : "2020-12-23T03:05:26Z",
        "updatedAt" : "2020-12-23T03:13:54Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "5b1aacaa-55ca-44d1-b915-a0846a7574f2",
        "parentId" : "f8e4ee4c-8b44-4c0b-b1aa-f09a83b000e3",
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "In the past, I found this to be noisy for the cases where replication was enabled - but this was a while back, and I would like to understand better what the 'cost' of enabling this for nontrivial usecases is for master : disabled by default means only developers who specifically test for it pay the price; not everyone.\r\nIt is quite common for an application to have references to a persisted RDD even after its use - with the loss of the RDD blocks having little to no functional impact.\r\nThis is similar to loss of blocks for an unreplicated persisted RDD - we do not proactively recompute the lost blocks; but do so on demand.\r\n\r\nIf the idea is we enable this for master, and evaluate the impact over the next 6 months and revisit at the end, I am fine with that: but an evaluation would need to be done before this goes out - else anyone using replicated storage will also get hit with the impact of proactive replication as well, and will need to disable this for their applications.",
        "createdAt" : "2020-12-23T20:02:53Z",
        "updatedAt" : "2020-12-23T20:02:53Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "b7df3051-6346-4caf-991c-e727a6ce11b5",
        "parentId" : "f8e4ee4c-8b44-4c0b-b1aa-f09a83b000e3",
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "> but an evaluation would need to be done before this goes out \r\n\r\nor perhaps identify the subset of conditions where it makes sense to enable it by default.",
        "createdAt" : "2020-12-23T20:11:17Z",
        "updatedAt" : "2020-12-23T20:11:18Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      }
    ],
    "commit" : "28fb534974203a382d472f77747d628ce046b83d",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +385,389 @@      .version(\"2.2.0\")\n      .booleanConf\n      .createWithDefault(true)\n\n  private[spark] val STORAGE_MEMORY_MAP_THRESHOLD ="
  },
  {
    "id" : "444aad32-8165-4c8e-8899-3060129088fe",
    "prId" : 30710,
    "prUrl" : "https://github.com/apache/spark/pull/30710#pullrequestreview-564425282",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d58fa2f0-88c1-461f-9f95-af73a8ff4e34",
        "parentId" : null,
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "please add the .doc section as well.",
        "createdAt" : "2021-01-08T15:32:20Z",
        "updatedAt" : "2021-01-09T06:53:31Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "99db62a4-f941-48f4-ab06-0ea81177471d",
        "parentId" : "d58fa2f0-88c1-461f-9f95-af73a8ff4e34",
        "authorId" : "fb1da7a4-0c05-42a2-9913-c8bd60c58a4f",
        "body" : "done",
        "createdAt" : "2021-01-08T16:57:29Z",
        "updatedAt" : "2021-01-09T06:53:31Z",
        "lastEditedBy" : "fb1da7a4-0c05-42a2-9913-c8bd60c58a4f",
        "tags" : [
        ]
      }
    ],
    "commit" : "86c8d8c51050b67ae8714f1dd1249f62d8b56bd0",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +1894,1898 @@      .doc(\"Minimum amount of time a task runs before being considered for speculation. \" +\n        \"This can be used to avoid launching speculative copies of tasks that are very short.\")\n      .version(\"3.2.0\")\n      .timeConf(TimeUnit.MILLISECONDS)\n      .createWithDefault(100)"
  },
  {
    "id" : "d64c0c7b-ae4d-4b68-b9f5-bc02ee0c37be",
    "prId" : 30710,
    "prUrl" : "https://github.com/apache/spark/pull/30710#pullrequestreview-673426557",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0a93fe18-67ef-43e7-94d5-2fdbf3b1923b",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "According to the [config naming policy](https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala#L21), `spark.speculation.shortTaskThreshold` may be better?\r\n\r\ncc @Ngone51 ",
        "createdAt" : "2021-06-01T16:38:47Z",
        "updatedAt" : "2021-06-01T16:38:47Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "9f1469b0-371f-46b9-ac31-2e404659d4c1",
        "parentId" : "0a93fe18-67ef-43e7-94d5-2fdbf3b1923b",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "I'm fine with renaming, though I don't care for shortTaskThreshold.  why not just minThreshold or minTaskThreshold",
        "createdAt" : "2021-06-01T16:47:47Z",
        "updatedAt" : "2021-06-01T16:47:48Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "8e67ec9d-c81d-4f37-987f-399624f9229b",
        "parentId" : "0a93fe18-67ef-43e7-94d5-2fdbf3b1923b",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "`minTaskRuntime`? I admit that I'm not good at naming ...",
        "createdAt" : "2021-06-01T17:10:48Z",
        "updatedAt" : "2021-06-01T17:10:48Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "2f3c21d2-cc73-4635-b458-f79a40f0a8da",
        "parentId" : "0a93fe18-67ef-43e7-94d5-2fdbf3b1923b",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "that works for me",
        "createdAt" : "2021-06-01T17:54:07Z",
        "updatedAt" : "2021-06-01T17:54:08Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "86c8d8c51050b67ae8714f1dd1249f62d8b56bd0",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1891,1895 @@\n  private[spark] val SPECULATION_MIN_THRESHOLD =\n    ConfigBuilder(\"spark.speculation.min.threshold\")\n      .doc(\"Minimum amount of time a task runs before being considered for speculation. \" +\n        \"This can be used to avoid launching speculative copies of tasks that are very short.\")"
  },
  {
    "id" : "17ea46f8-ee88-47b0-9280-d0b0bdb46d9c",
    "prId" : 30691,
    "prUrl" : "https://github.com/apache/spark/pull/30691#pullrequestreview-652586113",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b0bfc379-e366-46ec-b280-61955d161183",
        "parentId" : null,
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "Taken together, this accounts for driver waiting for upto 20s per stage ... do we have recommendations on how users can tune this ?",
        "createdAt" : "2021-05-05T15:19:59Z",
        "updatedAt" : "2021-05-11T05:07:03Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "4018682d-b30b-40b5-948e-5f653dd0101a",
        "parentId" : "b0bfc379-e366-46ec-b280-61955d161183",
        "authorId" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "body" : "I think currently this is hard to tune but once we have the changes in for `SPARK-33701` which does adaptive merge finalization, mostly this should be taken care of.",
        "createdAt" : "2021-05-05T17:46:41Z",
        "updatedAt" : "2021-05-11T05:07:03Z",
        "lastEditedBy" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "tags" : [
        ]
      }
    ],
    "commit" : "35d06150f37e01256314af4956c6e8fdcf7244b4",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +2105,2109 @@      .checkValue(_ >= 0L, \"Timeout must be >= 0.\")\n      .createWithDefaultString(\"10s\")\n\n  private[spark] val SHUFFLE_MERGER_MAX_RETAINED_LOCATIONS =\n    ConfigBuilder(\"spark.shuffle.push.maxRetainedMergerLocations\")"
  },
  {
    "id" : "b1b83291-9132-475b-bf29-7f0446b0fb57",
    "prId" : 30681,
    "prUrl" : "https://github.com/apache/spark/pull/30681#pullrequestreview-587200923",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ef8d0388-cf14-4613-8d7b-4f572b3adc00",
        "parentId" : null,
        "authorId" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "body" : "Do we have another configuration in `spark.file.` namespace? If not, we usually do not make a new namespace.",
        "createdAt" : "2021-02-10T02:24:53Z",
        "updatedAt" : "2021-02-10T02:24:53Z",
        "lastEditedBy" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "tags" : [
        ]
      }
    ],
    "commit" : "0824bfc6597c98c5057aeef16ea6c57637a0f654",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +819,823 @@\n  private[spark] val FILE_INSERT_STATUS_LOG_FLAG =\n    ConfigBuilder(\"spark.file.insert.status.log.flag\")\n      .internal()\n      .doc(\"When hive sql executes to generate HDFS data, \" +"
  },
  {
    "id" : "93b2c6e1-f75b-4a29-832d-466b74147b50",
    "prId" : 30528,
    "prUrl" : "https://github.com/apache/spark/pull/30528#pullrequestreview-540426955",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b07a6878-3fef-432d-af55-4d3ecf5e3337",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Although this is `internal`, shall we add Spark version, @zsxwing ? Since SPARK-33587 is filed as `Improvement`, `3.1.0`?\r\n```scala\r\n.version(\"3.1.0\")\r\n```",
        "createdAt" : "2020-11-29T00:42:47Z",
        "updatedAt" : "2020-11-29T00:59:59Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "bd3110ea-44b5-4e2a-94fd-c752a511e7ae",
        "parentId" : "b07a6878-3fef-432d-af55-4d3ecf5e3337",
        "authorId" : "2d6b46ba-4100-4c4d-9341-fff5e39647ec",
        "body" : "done",
        "createdAt" : "2020-11-29T01:00:02Z",
        "updatedAt" : "2020-11-29T01:00:02Z",
        "lastEditedBy" : "2d6b46ba-4100-4c4d-9341-fff5e39647ec",
        "tags" : [
        ]
      },
      {
        "id" : "b40b971a-2e86-4191-bdf5-472d9dc5345b",
        "parentId" : "b07a6878-3fef-432d-af55-4d3ecf5e3337",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Thanks!",
        "createdAt" : "2020-11-29T01:01:28Z",
        "updatedAt" : "2020-11-29T01:01:29Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "312f0422f7c6379747762c0b2eaf523e76c96a9b",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +1951,1955 @@      .doc(\"The max depth of the exception chain in a failed task Spark will search for a fatal \" +\n        \"error to check whether it should kill an executor. 0 means not checking any fatal \" +\n        \"error, 1 means checking only the exception but not the cause, and so on.\")\n      .internal()\n      .version(\"3.1.0\")"
  },
  {
    "id" : "c0670577-c42c-41a0-b471-96de469d5ee6",
    "prId" : 30492,
    "prUrl" : "https://github.com/apache/spark/pull/30492#pullrequestreview-541311974",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4a31f533-d495-46d7-9237-2bb864ca60e4",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Because we only register fallback storage when SparkContext initialization, I think it doesn't make sense to change this config after creating SparkContext, so this sounds like a static SQL config?",
        "createdAt" : "2020-11-30T20:38:44Z",
        "updatedAt" : "2020-11-30T20:38:44Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "007e2475-54b1-4a16-b571-cf01b05133cc",
        "parentId" : "4a31f533-d495-46d7-9237-2bb864ca60e4",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "This is `CORE` module which can work without SQL module. So, we cannot put it to Static SQL conf, @viirya .",
        "createdAt" : "2020-11-30T20:40:50Z",
        "updatedAt" : "2020-11-30T20:40:51Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "253eb43e-d2ac-45ed-9a06-9f27465905d3",
        "parentId" : "4a31f533-d495-46d7-9237-2bb864ca60e4",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Oh I see. Seems we don't have static config for core.",
        "createdAt" : "2020-11-30T20:46:20Z",
        "updatedAt" : "2020-11-30T20:46:20Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "8b226202-db7b-41c9-a659-1c9c7b6c772d",
        "parentId" : "4a31f533-d495-46d7-9237-2bb864ca60e4",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Could you add a simple note to the doc? E.g., \"Fallback storage is registered during SparkContext initialization, so this must be enabled before creating SparkContext.\"",
        "createdAt" : "2020-11-30T20:50:39Z",
        "updatedAt" : "2020-11-30T20:50:39Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "c67c03a6-274f-420d-ac50-9041661c9834",
        "parentId" : "4a31f533-d495-46d7-9237-2bb864ca60e4",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Ur, but it will be the same for all decommission conf like `spark.storage.decommission.enabled` and the other general conf like `spark.driver.host` and `spark.driver.port`.",
        "createdAt" : "2020-11-30T21:28:42Z",
        "updatedAt" : "2020-11-30T21:28:42Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "3d3d09b0-a337-4c5c-9d69-d4d956994b1f",
        "parentId" : "4a31f533-d495-46d7-9237-2bb864ca60e4",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Oh okay.",
        "createdAt" : "2020-11-30T21:32:47Z",
        "updatedAt" : "2020-11-30T21:32:47Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "9c9317378eb61b40f766e41dd168d2189f6008c1",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +472,476 @@      .createWithDefaultString(\"30s\")\n\n  private[spark] val STORAGE_DECOMMISSION_FALLBACK_STORAGE_PATH =\n    ConfigBuilder(\"spark.storage.decommission.fallbackStorage.path\")\n      .doc(\"The location for fallback storage during block manager decommissioning. \" +"
  },
  {
    "id" : "527d5fc9-1012-4004-a336-add2eaa83606",
    "prId" : 30486,
    "prUrl" : "https://github.com/apache/spark/pull/30486#pullrequestreview-538054997",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bd2d0f9f-e245-4388-a973-682d84a6ed03",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Ur, if this is still experimental, we should not deprecate the old ways.\r\n> This configuration is experimental.",
        "createdAt" : "2020-11-24T17:18:10Z",
        "updatedAt" : "2020-11-30T05:09:49Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "54642c07-08f1-4527-ba6b-5b7ce0043a73",
        "parentId" : "bd2d0f9f-e245-4388-a973-682d84a6ed03",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Makes sense. I will instead say the behaviour will be deprecated. We can remove that back once this configuration becomes stable.",
        "createdAt" : "2020-11-24T23:46:03Z",
        "updatedAt" : "2020-11-30T05:09:49Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "e35e94cad2888c98e03feb391a8dfcca8afa365c",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +1809,1813 @@      \"executor. .jar, .tar.gz, .tgz and .zip are supported. You can specify the directory \" +\n      \"name to unpack via adding '#' after the file name to unpack, for example, \" +\n      \"'file.zip#directory'. This configuration is experimental.\")\n    .stringConf\n    .toSequence"
  },
  {
    "id" : "c53095ef-63e0-41b9-b28f-5b0b9d2f08fc",
    "prId" : 30312,
    "prUrl" : "https://github.com/apache/spark/pull/30312#pullrequestreview-545165072",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6588efd1-2017-4831-b0f6-47e5246a7de3",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "This should be `2m` according to the comment?",
        "createdAt" : "2020-12-03T03:51:33Z",
        "updatedAt" : "2020-12-19T04:21:15Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "d8f9a6e8-5b35-48b7-8b3f-86e47a612f2e",
        "parentId" : "6588efd1-2017-4831-b0f6-47e5246a7de3",
        "authorId" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "body" : "The comment actually means that it should be higher than `2m`. If it is 2m than each block will be loaded in memory which increases memory overhead. I will make the comment more clear.",
        "createdAt" : "2020-12-03T17:31:01Z",
        "updatedAt" : "2020-12-19T04:21:15Z",
        "lastEditedBy" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "tags" : [
        ]
      },
      {
        "id" : "4879484e-6e31-421c-a414-78e78768fbdd",
        "parentId" : "6588efd1-2017-4831-b0f6-47e5246a7de3",
        "authorId" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "body" : "done",
        "createdAt" : "2020-12-04T17:40:03Z",
        "updatedAt" : "2020-12-19T04:21:15Z",
        "lastEditedBy" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "tags" : [
        ]
      }
    ],
    "commit" : "21ea881f398a35038e2359ef0b4ed5cd15aab736",
    "line" : 32,
    "diffHunk" : "@@ -1,1 +2052,2056 @@      // batch of block will be loaded in memory with memory mapping, which has higher overhead\n      // with small MB sized chunk of data.\n      .createWithDefaultString(\"3m\")\n}"
  },
  {
    "id" : "8c3068cb-0f21-4cd3-8ce7-735da242f1d4",
    "prId" : 30311,
    "prUrl" : "https://github.com/apache/spark/pull/30311#pullrequestreview-528732216",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c4ad6aa4-eba5-43d2-aa4d-32099340dfc7",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "This change means that the non-cluster mode refers to this value?",
        "createdAt" : "2020-11-11T13:26:23Z",
        "updatedAt" : "2020-11-11T13:26:23Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "a1a7f4d9-4539-4504-8875-fb2422577fbd",
        "parentId" : "c4ad6aa4-eba5-43d2-aa4d-32099340dfc7",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "yes, the 'client' deploy mode respects this too",
        "createdAt" : "2020-11-12T03:53:16Z",
        "updatedAt" : "2020-11-12T03:53:16Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "3aa18e885b60c232e2eba18ab92629edd4efb1e5",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +303,307 @@\n  private[spark] val EXECUTOR_MEMORY_OVERHEAD = ConfigBuilder(\"spark.executor.memoryOverhead\")\n    .doc(\"The amount of non-heap memory to be allocated per executor, in MiB unless otherwise\" +\n      \" specified.\")\n    .version(\"2.3.0\")"
  },
  {
    "id" : "932f2c56-7844-44fa-960f-7a05e6371a05",
    "prId" : 30164,
    "prUrl" : "https://github.com/apache/spark/pull/30164#pullrequestreview-526871460",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c994d568-746e-4f32-8067-b4b2b95f7b24",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Please add the version (3.1.0) info for these newly added confs.",
        "createdAt" : "2020-11-10T05:50:37Z",
        "updatedAt" : "2020-11-20T00:32:25Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "5ce29340c8aa4a6aadfc8d00cb5053a9be9aa839",
    "line" : 50,
    "diffHunk" : "@@ -1,1 +1992,1996 @@      .version(\"3.1.0\")\n      .doubleConf\n      .createWithDefault(5)\n}"
  },
  {
    "id" : "9ebd5055-0a17-403c-9dcc-3b0c6e47e05a",
    "prId" : 30164,
    "prUrl" : "https://github.com/apache/spark/pull/30164#pullrequestreview-528895493",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c031950d-71a5-42ab-9bdc-8fe094a19e45",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Please use `${SHUFFLE_MERGER_LOCATIONS_MIN_THRESHOLD_RATIO.key}` instead of hard-coded.",
        "createdAt" : "2020-11-12T09:24:59Z",
        "updatedAt" : "2020-11-20T00:32:25Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "5ce29340c8aa4a6aadfc8d00cb5053a9be9aa839",
    "line" : 45,
    "diffHunk" : "@@ -1,1 +1987,1991 @@        s\"${SHUFFLE_MERGER_LOCATIONS_MIN_THRESHOLD_RATIO.key} ratio number of mergers needed to \" +\n        \"enable push based shuffle for a stage. For eg: with 1000 partitions for the child \" +\n        \"stage with spark.shuffle.push.mergersMinStaticThreshold as 5 and \" +\n        s\"${SHUFFLE_MERGER_LOCATIONS_MIN_THRESHOLD_RATIO.key} set to 0.05, we would need \" +\n        \"at least 50 mergers to enable push based shuffle for that stage.\")"
  },
  {
    "id" : "711c980f-ea2d-431d-8212-097abcfcedbf",
    "prId" : 29906,
    "prUrl" : "https://github.com/apache/spark/pull/29906#pullrequestreview-501482167",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d6d7dd2b-bc8b-4762-9e58-1e51c9c1ed69",
        "parentId" : null,
        "authorId" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "body" : "Given that this is already a legacy config marked as deprecated, it doesn't seem worth adding a new version. What do you think?",
        "createdAt" : "2020-10-02T22:40:43Z",
        "updatedAt" : "2020-10-28T13:50:33Z",
        "lastEditedBy" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "tags" : [
        ]
      }
    ],
    "commit" : "b38dd66500cdd4a12f78cca1eeacf116f0ea5b4e",
    "line" : 97,
    "diffHunk" : "@@ -1,1 +791,795 @@      .version(\"3.1.0\")\n      .withAlternative(\"spark.scheduler.executorTaskBlacklistTime\")\n      .timeConf(TimeUnit.MILLISECONDS)\n      .createOptional\n"
  },
  {
    "id" : "40585586-90aa-41ad-9035-8460d8d6b6d9",
    "prId" : 29466,
    "prUrl" : "https://github.com/apache/spark/pull/29466#pullrequestreview-469688304",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fc2aff03-2a17-4abf-b4e0-7cfd838cc397",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "nit: some 2 spaces here ... before from",
        "createdAt" : "2020-08-18T15:46:47Z",
        "updatedAt" : "2020-08-18T16:39:40Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "3efc2477-64b8-4d89-bc43-f10880b67582",
        "parentId" : "fc2aff03-2a17-4abf-b4e0-7cfd838cc397",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "it's only one space?",
        "createdAt" : "2020-08-18T16:39:58Z",
        "updatedAt" : "2020-08-18T16:39:58Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "423d9c5d-a437-4fba-b646-8813f32fd0c5",
        "parentId" : "fc2aff03-2a17-4abf-b4e0-7cfd838cc397",
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "Yeah. Sorry. Fonts are misleading :-) ",
        "createdAt" : "2020-08-18T17:47:11Z",
        "updatedAt" : "2020-08-18T17:47:11Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      }
    ],
    "commit" : "c4c0ef4458072cfbe4dfa3b23c1f32b340e36f9a",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +1872,1876 @@        s\"gracefully. Spark will try to migrate all the RDD blocks (controlled by \" +\n        s\"${STORAGE_DECOMMISSION_RDD_BLOCKS_ENABLED.key}) and shuffle blocks (controlled by \" +\n        s\"${STORAGE_DECOMMISSION_SHUFFLE_BLOCKS_ENABLED.key}) from the decommissioning \" +\n        s\"executor to a remote executor when ${STORAGE_DECOMMISSION_ENABLED.key} is enabled. \" +\n        s\"With decommission enabled, Spark will also decommission an executor instead of \" +"
  },
  {
    "id" : "c1bd9a67-4bac-4d10-acde-8c517535b892",
    "prId" : 29278,
    "prUrl" : "https://github.com/apache/spark/pull/29278#pullrequestreview-457153997",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "254d873a-2d14-4edf-b406-9613cc7ab4c1",
        "parentId" : null,
        "authorId" : "43998e22-6c2f-401d-9914-8cecf6fad929",
        "body" : "I'll submit another PR with the opposite default value against `branch-3.0` when this is good to go.",
        "createdAt" : "2020-07-29T01:54:36Z",
        "updatedAt" : "2020-07-31T01:53:24Z",
        "lastEditedBy" : "43998e22-6c2f-401d-9914-8cecf6fad929",
        "tags" : [
        ]
      },
      {
        "id" : "00e7ec53-fc48-430c-a78e-b54ebaa86f3b",
        "parentId" : "254d873a-2d14-4edf-b406-9613cc7ab4c1",
        "authorId" : "43998e22-6c2f-401d-9914-8cecf6fad929",
        "body" : "Also I'll add a migration guide later.",
        "createdAt" : "2020-07-29T02:33:52Z",
        "updatedAt" : "2020-07-31T01:53:24Z",
        "lastEditedBy" : "43998e22-6c2f-401d-9914-8cecf6fad929",
        "tags" : [
        ]
      }
    ],
    "commit" : "0ea7ea956b3867b67c869fddcc294cb268157a92",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +1913,1917 @@    ConfigBuilder(\"spark.driver.allowSparkContextInExecutors\")\n      .doc(\"If set to true, SparkContext can be created in executors.\")\n      .version(\"3.0.1\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "c9e2fcf9-75be-4526-b41e-b3aad93ffb17",
    "prId" : 28864,
    "prUrl" : "https://github.com/apache/spark/pull/28864#pullrequestreview-435987239",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5f4c79fc-1637-4eb0-9d03-62740ab7a74c",
        "parentId" : null,
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "need to update the configuration.md doc for this name change.",
        "createdAt" : "2020-06-23T17:11:25Z",
        "updatedAt" : "2020-07-13T17:01:35Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "9c43172daf2b9ad38fce32f5349c87944fa62caa",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +461,465 @@\n  private[spark] val STORAGE_BLOCKMANAGER_HEARTBEAT_TIMEOUT =\n    ConfigBuilder(\"spark.storage.blockManagerHeartbeatTimeoutMs\")\n      .version(\"0.7.0\")\n      .withAlternative(\"spark.storage.blockManagerSlaveTimeoutMs\")"
  },
  {
    "id" : "716b5cf8-eaee-457f-98a8-3b90498aa4d4",
    "prId" : 28619,
    "prUrl" : "https://github.com/apache/spark/pull/28619#pullrequestreview-439401780",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "edfb4fd5-7d64-42a3-a822-14617e4c83b0",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "So the timeout is decided by the cloud vendors? What does this config specify?",
        "createdAt" : "2020-06-17T08:34:46Z",
        "updatedAt" : "2020-06-17T08:34:46Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "e3abec3d-2a7b-4594-8592-695690fffe7a",
        "parentId" : "edfb4fd5-7d64-42a3-a822-14617e4c83b0",
        "authorId" : "ad4cb313-47b2-4cfa-9c1f-0a3596794a67",
        "body" : "@cloud-fan This config can be set by users based on their setups. If they are using AWS spot nodes, timeout can be set to somewhere around 120 seconds, if they are using fix duration 6hrs spot blocks (say they decommission executors at 5:45), timeout can be set to 15 mins and so on.\r\n\r\nIf user doesn't set this timeout, things will remain as they were and tasks running on decommission executors  won't get any special treatment with respect to speculation.",
        "createdAt" : "2020-06-18T05:17:43Z",
        "updatedAt" : "2020-06-18T05:17:44Z",
        "lastEditedBy" : "ad4cb313-47b2-4cfa-9c1f-0a3596794a67",
        "tags" : [
        ]
      },
      {
        "id" : "f68ca0aa-77bc-4f69-bb40-7534007c9730",
        "parentId" : "edfb4fd5-7d64-42a3-a822-14617e4c83b0",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "is it possible that Spark can get this timeout value from the cluster manager? So that users don't need to set it manually. cc @holdenk ",
        "createdAt" : "2020-06-18T06:52:00Z",
        "updatedAt" : "2020-06-18T06:52:01Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "d70714bc-2cf4-469d-b8cf-bf146abf2f3c",
        "parentId" : "edfb4fd5-7d64-42a3-a822-14617e4c83b0",
        "authorId" : "ad4cb313-47b2-4cfa-9c1f-0a3596794a67",
        "body" : "@cloud-fan As per my understanding, Worker Decommissioning is getting triggered currently using SIGPWR signal (and not via some message coming from YARN/Kubernetes Cluster manager). So getting this timeout from Spark Cluster Manager might not be possible. We might be able to do this once Spark's Worker Decommissioning logic starts triggering via communication from YARN etc in future. cc  @holdenk ",
        "createdAt" : "2020-06-22T05:21:35Z",
        "updatedAt" : "2020-06-22T05:21:35Z",
        "lastEditedBy" : "ad4cb313-47b2-4cfa-9c1f-0a3596794a67",
        "tags" : [
        ]
      },
      {
        "id" : "4ad52605-dec1-43b3-918d-fb450e03c703",
        "parentId" : "edfb4fd5-7d64-42a3-a822-14617e4c83b0",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "I believe there are some situations where we can know the length of time from the cluster manager or from Spark it's self, but not all. I think having a configurable default for folks who know their cloud provider environment makes sense",
        "createdAt" : "2020-06-29T18:35:26Z",
        "updatedAt" : "2020-06-29T18:35:27Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "d87b311be85819ae884e2a24d94926fdd51165de",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +1848,1852 @@        \"This config is useful for cloud environments where we know in advance when \" +\n        \"an executor is going to go down after decommissioning signal i.e. around 2 mins \" +\n        \"in aws spot nodes, 1/2 hrs in spot block nodes etc. This config is currently \" +\n        \"used to decide what tasks running on decommission executors to speculate.\")\n      .version(\"3.1.0\")"
  },
  {
    "id" : "aa058fc6-a2be-41f1-9530-5734e06c1291",
    "prId" : 28426,
    "prUrl" : "https://github.com/apache/spark/pull/28426#pullrequestreview-403951301",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b932e42c-0162-45a3-871b-d70e03d6dfc6",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "I agree this name-spacing into `.shuffleTracking`.",
        "createdAt" : "2020-04-30T23:49:34Z",
        "updatedAt" : "2020-04-30T23:51:51Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "ec985fdc7a6fdd335a7a0cffc394467e7d7016ea",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +529,533 @@\n  private[spark] val DYN_ALLOCATION_SHUFFLE_TRACKING_TIMEOUT =\n    ConfigBuilder(\"spark.dynamicAllocation.shuffleTracking.timeout\")\n      .version(\"3.0.0\")\n      .timeConf(TimeUnit.MILLISECONDS)"
  },
  {
    "id" : "284017ce-04b4-430b-af88-364db0400dea",
    "prId" : 28370,
    "prUrl" : "https://github.com/apache/spark/pull/28370#pullrequestreview-413231624",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b3147723-10cb-41d0-b56e-3757e02959c2",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "I think we can just use `spark.storage.maxReplicationFailures` directly. Less configurations contribute to better UX.",
        "createdAt" : "2020-05-15T13:59:58Z",
        "updatedAt" : "2020-05-15T13:59:58Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "e3ffbdcd-d5a4-4d98-b9b4-91a246ab1bec",
        "parentId" : "b3147723-10cb-41d0-b56e-3757e02959c2",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "So I'm not sure that's a great idea. Looking at `maxReplicationFailures` the default is set to one, which certainly makes sense in the situation where we don't expect the host to be exiting. But this situation is different, we know the current block is going to disappear soon so it makes sense to more aggressively try and copy the block.",
        "createdAt" : "2020-05-15T17:25:00Z",
        "updatedAt" : "2020-05-15T17:25:01Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "1f4e99e2-a654-4ef7-9468-5a2ee2259b91",
        "parentId" : "b3147723-10cb-41d0-b56e-3757e02959c2",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "I see, thanks for your explanation.\r\n",
        "createdAt" : "2020-05-18T02:00:19Z",
        "updatedAt" : "2020-05-18T02:00:19Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "c34305662cd91d05b5160c7de72e35e4108b6755",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +422,426 @@\n  private[spark] val STORAGE_DECOMMISSION_MAX_REPLICATION_FAILURE_PER_BLOCK =\n    ConfigBuilder(\"spark.storage.decommission.maxReplicationFailuresPerBlock\")\n      .internal()\n      .doc(\"Maximum number of failures which can be handled for the replication of \" +"
  },
  {
    "id" : "2c0ddbf1-51cd-45a1-9d6f-24fee9123389",
    "prId" : 28053,
    "prUrl" : "https://github.com/apache/spark/pull/28053#pullrequestreview-385289302",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "687f5b27-835d-4482-8da9-118db47f6f74",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Hi, @tgravescs .\r\nCould you add `.doc` and `.version` like the other confs?",
        "createdAt" : "2020-03-27T18:29:37Z",
        "updatedAt" : "2020-04-01T19:28:46Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "f2373bfb-4698-469d-a984-822785609f8a",
        "parentId" : "687f5b27-835d-4482-8da9-118db47f6f74",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "yes, I will add",
        "createdAt" : "2020-03-27T18:34:35Z",
        "updatedAt" : "2020-04-01T19:28:46Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "f18f0fc1-770a-4ce4-8c3f-12189776944c",
        "parentId" : "687f5b27-835d-4482-8da9-118db47f6f74",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "When I look through confs which introduced around the feature, I feel that they're kind of messy. For example, we already have:\r\n\r\n- spark.resources.discoveryPlugin\r\n- spark.driver.resourcesFile\r\n- spark.driver.resource.GPU.amount\r\n\r\nSince they're all from the same feature, so I'm thinking that if we can make them under the same namespace, e.g. `spark.resource`.\r\n\r\nWDYT?",
        "createdAt" : "2020-03-30T07:18:14Z",
        "updatedAt" : "2020-04-01T19:28:46Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "2fc62b03-53a6-4e33-8afe-2fb85f427447",
        "parentId" : "687f5b27-835d-4482-8da9-118db47f6f74",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "our normal format for differentiating driver and executor configs is spark.driver. and spark.executor., so I am not in favor or changing that to be spark.resource.driver spark.resource.executor as I think that is not consistent.\r\nI'm fine with changing to spark.driver.resource. and spark.executor.resource. or if not specific spark.resource.\r\nI'm not sure what you would call spark.driver.resourcesFile that makes sense though, unless you call it. spark.driver.resource.resourcesFile which seems like just extra but is consistent among the feature.\r\n\r\nSimilar here, spark.scheduler is a prefix that tells you what component is affects, if we want to call this spark.scheduler.resource.resourceProfileMergeConflicts or something like that I'd be ok with that.",
        "createdAt" : "2020-03-30T14:31:34Z",
        "updatedAt" : "2020-04-01T19:28:46Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "0ea4b552-c9cf-4e02-9256-cb4623bd184a",
        "parentId" : "687f5b27-835d-4482-8da9-118db47f6f74",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Yeah, I also know the problem here. And I just wonder if they would not be such friendly for user. We shall doc them in a well organized way at least.",
        "createdAt" : "2020-04-01T06:12:11Z",
        "updatedAt" : "2020-04-01T19:28:46Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "452378be77b5371feb7d974776ad411b9e2bc232",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +1823,1827 @@        \"ResourceProfiles are found in RDDs going into the same stage.\")\n      .version(\"3.1.0\")\n      .booleanConf\n      .createWithDefault(false)\n}"
  },
  {
    "id" : "cdc634c8-f34f-4b7f-a23d-e65a4e5a4298",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375695813",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bbf6c0c1-6518-4b7a-acd9-ed6c9947fdbe",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-27651, commit ID: fd2bf55abaab08798a428d4e47d4050ba2b82a95#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-17T02:16:38Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +1368,1372 @@        s\"persisted RDD blocks or shuffle blocks \" +\n        s\"(when `${SHUFFLE_HOST_LOCAL_DISK_READING_ENABLED.key}` is set) from the same host.\")\n      .version(\"3.0.0\")\n      .intConf\n      .createWithDefault(1000)"
  },
  {
    "id" : "4f058eee-33c8-4ab6-a04b-b2fa53049312",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375695927",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "eb5ba01f-3c63-4c63-91ea-44fab90b9199",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-3151, commit ID: b8ffb51055108fd606b86f034747006962cd2df3#diff-abd96f2ae793cd6ea6aab5b96a3c1d7a",
        "createdAt" : "2020-03-17T02:17:07Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +1376,1380 @@      .internal()\n      .doc(\"For testing only, controls the size of chunks when memory mapping a file\")\n      .version(\"2.3.0\")\n      .bytesConf(ByteUnit.BYTE)\n      .createWithDefault(ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH)"
  },
  {
    "id" : "694a6c3c-9318-4f8a-b0fd-685029d55061",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375696057",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "eb4fd8a9-b1d1-45ad-a3fe-3dfa63e91ad2",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-24817, commit ID: 388f5a0635a2812cd71b08352e3ddc20293ec189#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-17T02:17:34Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +1386,1390 @@        \"configured time, throw a SparkException to fail all the tasks. The default value is set \" +\n        \"to 31536000(3600 * 24 * 365) so the barrier() call shall wait for one year.\")\n      .version(\"2.4.0\")\n      .timeConf(TimeUnit.SECONDS)\n      .checkValue(v => v > 0, \"The value should be a positive time value.\")"
  },
  {
    "id" : "a638bd97-6265-456f-b4b0-b7a66cc14edd",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375696229",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ac37d8b0-bd7a-496a-9583-e19f10db81c2",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-22148, commit ID: 52e9711d01694158ecb3691f2ec25c0ebe4b0207#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-17T02:18:10Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +1395,1399 @@      .doc(\"The timeout in seconds to wait to acquire a new executor and schedule a task \" +\n        \"before aborting a TaskSet which is unschedulable because of being completely blacklisted.\")\n      .version(\"2.4.1\")\n      .timeConf(TimeUnit.SECONDS)\n      .checkValue(v => v >= 0, \"The value should be a non negative time value.\")"
  },
  {
    "id" : "02d25d2d-6d90-460c-8522-29334b94926a",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375696345",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8284c5f5-1fb6-40be-9a06-f27e2bba4280",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-24819, commit ID: bfb74394a5513134ea1da9fcf4a1783b77dd64e4#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-17T02:18:36Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 36,
    "diffHunk" : "@@ -1,1 +1410,1414 @@        \"config only applies to jobs that contain one or more barrier stages, we won't perform \" +\n        \"the check on non-barrier jobs.\")\n      .version(\"2.4.0\")\n      .timeConf(TimeUnit.SECONDS)\n      .createWithDefaultString(\"15s\")"
  },
  {
    "id" : "835a69da-05b2-4613-ad57-254a4a00d4cb",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375696766",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "804256dd-9439-49cd-b350-433b325cbb96",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-24819, commit ID: bfb74394a5513134ea1da9fcf4a1783b77dd64e4#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-17T02:20:14Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 44,
    "diffHunk" : "@@ -1,1 +1424,1428 @@        \"applies to jobs that contain one or more barrier stages, we won't perform the check on \" +\n        \"non-barrier jobs.\")\n      .version(\"2.4.0\")\n      .intConf\n      .checkValue(v => v > 0, \"The max failures should be a positive value.\")"
  },
  {
    "id" : "df93989e-d432-4c2e-b4d6-20a106939458",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375697007",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "aad0f87e-fe34-42a6-987f-60468cc364e5",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-7076 and SPARK-7077 and SPARK-7080, commit ID: f49284b5bf3a69ed91a5e3e6e0ed3be93a6ab9e4#diff-5a0de266c82b95adb47d9bca714e1f1b",
        "createdAt" : "2020-03-17T02:21:11Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 52,
    "diffHunk" : "@@ -1,1 +1432,1436 @@    ConfigBuilder(\"spark.unsafe.exceptionOnMemoryLeak\")\n      .internal()\n      .version(\"1.4.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "03bf1e46-6194-408b-a1ed-ec72257f3417",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375697119",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b256a2eb-2d9e-4b46-9cf7-e2e44a43d1b1",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-21113, commit ID: 1e978b17d63d7ba20368057aa4e65f5ef6e87369#diff-93a086317cea72a113cf81056882c206",
        "createdAt" : "2020-03-17T02:21:42Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 59,
    "diffHunk" : "@@ -1,1 +1439,1443 @@    ConfigBuilder(\"spark.unsafe.sorter.spill.read.ahead.enabled\")\n      .internal()\n      .version(\"2.3.0\")\n      .booleanConf\n      .createWithDefault(true)"
  },
  {
    "id" : "5af497f5-63c1-4d6d-8a6e-b216215bcbdd",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375697264",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c5871bfc-8a94-4329-bb1e-75e8b0c4b320",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-16862, commit ID: c1937dd19a23bd096a4707656c7ba19fb5c16966#diff-93a086317cea72a113cf81056882c206",
        "createdAt" : "2020-03-17T02:22:12Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 66,
    "diffHunk" : "@@ -1,1 +1446,1450 @@    ConfigBuilder(\"spark.unsafe.sorter.spill.reader.buffer.size\")\n      .internal()\n      .version(\"2.1.0\")\n      .bytesConf(ByteUnit.BYTE)\n      .checkValue(v => 1024 * 1024 <= v && v <= MAX_BUFFER_SIZE_BYTES,"
  },
  {
    "id" : "f336b446-e31c-4cee-8bde-65db19d1b909",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375697388",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ee7b6b5d-68b7-409c-bdbe-a73fdd34de5c",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-29397, commit ID: d51d228048d519a9a666f48dc532625de13e7587#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-17T02:22:39Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 74,
    "diffHunk" : "@@ -1,1 +1459,1463 @@      .doc(\"Comma-separated list of class names implementing \" +\n        \"org.apache.spark.api.plugin.SparkPlugin to load into the application.\")\n      .version(\"3.0.0\")\n      .stringConf\n      .toSequence"
  },
  {
    "id" : "de36b013-f481-49d7-8f61-9222777ef9ac",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375697576",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "afaf92d1-925b-461f-a96e-d083a8787ba7",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-8414, commit ID: 72da2a21f0940b97757ace5975535e559d627688#diff-75141521b1d55bc32d72b70032ad96c0",
        "createdAt" : "2020-03-17T02:23:15Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 81,
    "diffHunk" : "@@ -1,1 +1466,1470 @@  private[spark] val CLEANER_PERIODIC_GC_INTERVAL =\n    ConfigBuilder(\"spark.cleaner.periodicGC.interval\")\n      .version(\"1.6.0\")\n      .timeConf(TimeUnit.SECONDS)\n      .createWithDefaultString(\"30min\")"
  },
  {
    "id" : "0c75aaf2-0d8a-4055-8afd-0e25c1363284",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375697706",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "907e1c4f-2fb0-49f1-84c6-155dd27c89f4",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-1103, commit ID: 11eabbe125b2ee572fad359c33c93f5e6fdf0b2d#diff-364713d7776956cb8b0a771e9b62f82d",
        "createdAt" : "2020-03-17T02:23:46Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 87,
    "diffHunk" : "@@ -1,1 +1472,1476 @@  private[spark] val CLEANER_REFERENCE_TRACKING =\n    ConfigBuilder(\"spark.cleaner.referenceTracking\")\n      .version(\"1.0.0\")\n      .booleanConf\n      .createWithDefault(true)"
  },
  {
    "id" : "af61bdf8-789a-4170-8899-fb0d3546607c",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375697739",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "11b06446-11b0-4dfe-86ac-bc668d62adb2",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-1103, commit ID: 11eabbe125b2ee572fad359c33c93f5e6fdf0b2d#diff-364713d7776956cb8b0a771e9b62f82d",
        "createdAt" : "2020-03-17T02:23:53Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 93,
    "diffHunk" : "@@ -1,1 +1478,1482 @@  private[spark] val CLEANER_REFERENCE_TRACKING_BLOCKING =\n    ConfigBuilder(\"spark.cleaner.referenceTracking.blocking\")\n      .version(\"1.0.0\")\n      .booleanConf\n      .createWithDefault(true)"
  },
  {
    "id" : "74597e72-6089-473e-a4de-6de683b621bf",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375697847",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d351e309-0320-4773-b8cd-914e880e830a",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-3139, commit ID: 5cf1e440137006eedd6846ac8fa57ccf9fd1958d#diff-75141521b1d55bc32d72b70032ad96c0",
        "createdAt" : "2020-03-17T02:24:18Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 99,
    "diffHunk" : "@@ -1,1 +1484,1488 @@  private[spark] val CLEANER_REFERENCE_TRACKING_BLOCKING_SHUFFLE =\n    ConfigBuilder(\"spark.cleaner.referenceTracking.blocking.shuffle\")\n      .version(\"1.1.1\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "13ce14fb-7b3f-4f4c-8131-8fa28f5e2122",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375697956",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "aad82810-71f9-493b-a8fc-290c19fb167d",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-2033, commit ID: 25998e4d73bcc95ac85d9af71adfdc726ec89568#diff-440e866c5df0b8386aff57f9f8bd8db1",
        "createdAt" : "2020-03-17T02:24:41Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 105,
    "diffHunk" : "@@ -1,1 +1490,1494 @@  private[spark] val CLEANER_REFERENCE_TRACKING_CLEAN_CHECKPOINTS =\n    ConfigBuilder(\"spark.cleaner.referenceTracking.cleanCheckpoints\")\n      .version(\"1.4.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "4e5ea975-5cb2-4d91-ab57-4c7e9d088343",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375698090",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d7141bbe-5b1d-43bf-9612-e458a283ab1e",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-1940, commit ID: 4823bf470ec1b47a6f404834d4453e61d3dcbec9#diff-2b4575e096e4db7165e087f9429f2a02",
        "createdAt" : "2020-03-17T02:25:06Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 112,
    "diffHunk" : "@@ -1,1 +1496,1500 @@  private[spark] val EXECUTOR_LOGS_ROLLING_STRATEGY =\n    ConfigBuilder(\"spark.executor.logs.rolling.strategy\")\n      .version(\"1.1.0\")\n      .stringConf\n      .createWithDefault(\"\")"
  },
  {
    "id" : "e73fe209-2eda-4ac0-9dfa-eb2393b4b0eb",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375698131",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "96f8d1a7-85ae-4076-bce8-1c8d305d21ce",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-1940, commit ID: 4823bf470ec1b47a6f404834d4453e61d3dcbec9#diff-2b4575e096e4db7165e087f9429f2a02",
        "createdAt" : "2020-03-17T02:25:13Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 119,
    "diffHunk" : "@@ -1,1 +1502,1506 @@  private[spark] val EXECUTOR_LOGS_ROLLING_TIME_INTERVAL =\n    ConfigBuilder(\"spark.executor.logs.rolling.time.interval\")\n      .version(\"1.1.0\")\n      .stringConf\n      .createWithDefault(\"daily\")"
  },
  {
    "id" : "31ba394e-6674-4325-aa8f-027731012062",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375698232",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e11b7716-7739-4f94-a66e-1d2bb34c1d9c",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-1940, commit ID: 4823bf470ec1b47a6f404834d4453e61d3dcbec9#diff-2b4575e096e4db7165e087f9429f2a02",
        "createdAt" : "2020-03-17T02:25:35Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 132,
    "diffHunk" : "@@ -1,1 +1514,1518 @@  private[spark] val EXECUTOR_LOGS_ROLLING_MAX_RETAINED_FILES =\n    ConfigBuilder(\"spark.executor.logs.rolling.maxRetainedFiles\")\n      .version(\"1.1.0\")\n      .intConf\n      .createWithDefault(-1)"
  },
  {
    "id" : "ea0b7878-f870-4603-800c-9c17520d86d8",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375698342",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "62cd9166-49c7-4ef1-b3b4-2c5785aecc7a",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-5932, commit ID: 2d222fb39dd978e5a33cde6ceb59307cbdf7b171#diff-529fc5c06b9731c1fbda6f3db60b16aa",
        "createdAt" : "2020-03-17T02:25:59Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 125,
    "diffHunk" : "@@ -1,1 +1508,1512 @@  private[spark] val EXECUTOR_LOGS_ROLLING_MAX_SIZE =\n    ConfigBuilder(\"spark.executor.logs.rolling.maxSize\")\n      .version(\"1.4.0\")\n      .stringConf\n      .createWithDefault((1024 * 1024).toString)"
  },
  {
    "id" : "78287d2f-bf9d-4369-9ced-db4905b03b44",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375698515",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2e98ba1d-f0c1-4915-8677-99c6633278fe",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-17711, commit ID: 26e978a93f029e1a1b5c7524d0b52c8141b70997#diff-2b4575e096e4db7165e087f9429f2a02",
        "createdAt" : "2020-03-17T02:26:31Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 138,
    "diffHunk" : "@@ -1,1 +1520,1524 @@  private[spark] val EXECUTOR_LOGS_ROLLING_ENABLE_COMPRESSION =\n    ConfigBuilder(\"spark.executor.logs.rolling.enableCompression\")\n      .version(\"2.0.2\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "9e69ab13-0149-4e32-934f-5c7f4c966e71",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375698648",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "52540b77-ed3b-4416-9880-82737d12a0ee",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-5388, commit ID: 6ec0cdc14390d4dc45acf31040f21e1efc476fc0#diff-29dffdccd5a7f4c8b496c293e87c8668",
        "createdAt" : "2020-03-17T02:26:58Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 143,
    "diffHunk" : "@@ -1,1 +1525,1529 @@\n  private[spark] val MASTER_REST_SERVER_ENABLED = ConfigBuilder(\"spark.master.rest.enabled\")\n    .version(\"1.3.0\")\n    .booleanConf\n    .createWithDefault(false)"
  },
  {
    "id" : "8aa88fd2-b71f-4738-bdec-010f97e96342",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375698681",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fd1d2726-8e25-4397-bbcd-f57d5fdf3a76",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-5388, commit ID: 6ec0cdc14390d4dc45acf31040f21e1efc476fc0#diff-29dffdccd5a7f4c8b496c293e87c8668",
        "createdAt" : "2020-03-17T02:27:06Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 148,
    "diffHunk" : "@@ -1,1 +1530,1534 @@\n  private[spark] val MASTER_REST_SERVER_PORT = ConfigBuilder(\"spark.master.rest.port\")\n    .version(\"1.3.0\")\n    .intConf\n    .createWithDefault(6066)"
  },
  {
    "id" : "d7994479-ad0e-4d88-a78f-6f6f56bc559f",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375698789",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bc6e27a0-2544-49af-ada1-0e9c7b362546",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-2857, commit ID: 12f99cf5f88faf94d9dbfe85cb72d0010a3a25ac#diff-366c88f47e9b5cfa4d4305febeb8b026",
        "createdAt" : "2020-03-17T02:27:29Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 153,
    "diffHunk" : "@@ -1,1 +1535,1539 @@\n  private[spark] val MASTER_UI_PORT = ConfigBuilder(\"spark.master.ui.port\")\n    .version(\"1.1.0\")\n    .intConf\n    .createWithDefault(8080)"
  },
  {
    "id" : "0d8b34e9-eb11-4b0a-85bf-c91d848fe68c",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375698938",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c24441d6-8130-492e-91be-b3c15dc81273",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-5932, commit ID: 2d222fb39dd978e5a33cde6ceb59307cbdf7b171#diff-529fc5c06b9731c1fbda6f3db60b16aa",
        "createdAt" : "2020-03-17T02:28:03Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 161,
    "diffHunk" : "@@ -1,1 +1544,1548 @@        \"Snappy compression codec is used. Lowering this block size \" +\n        \"will also lower shuffle memory usage when Snappy is used\")\n      .version(\"1.4.0\")\n      .bytesConf(ByteUnit.BYTE)\n      .createWithDefaultString(\"32k\")"
  },
  {
    "id" : "20c3fce1-bfad-4132-b62d-90c05b353daf",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375698980",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8e950054-f369-4b25-92d4-37f2f89f5380",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-5932, commit ID: 2d222fb39dd978e5a33cde6ceb59307cbdf7b171#diff-529fc5c06b9731c1fbda6f3db60b16aa",
        "createdAt" : "2020-03-17T02:28:13Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 169,
    "diffHunk" : "@@ -1,1 +1553,1557 @@        \"codec is used. Lowering this block size will also lower shuffle memory \" +\n        \"usage when LZ4 is used.\")\n      .version(\"1.4.0\")\n      .bytesConf(ByteUnit.BYTE)\n      .createWithDefaultString(\"32k\")"
  },
  {
    "id" : "9ae1ddfe-c0c0-4f7c-86cb-907a81c8b1ba",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375704027",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1e7ae406-ca00-41df-94fd-b57656adeea7",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "No JIRA ID, commit ID: 46eecd110a4017ea0c86cbb1010d0ccd6a5eb2ef#diff-df9e6118c481ceb27faa399114fac0a1",
        "createdAt" : "2020-03-17T02:46:07Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 177,
    "diffHunk" : "@@ -1,1 +1563,1567 @@        \"lz4, lzf, snappy, and zstd. You can also use fully qualified class names to specify \" +\n        \"the codec\")\n      .version(\"0.8.0\")\n      .stringConf\n      .createWithDefaultString(\"lz4\")"
  },
  {
    "id" : "5bab4389-5223-4dbd-bd03-a6f6ef73e46b",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375704203",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "878f51bd-c205-44f0-a7fc-8453570a7442",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-19112, commit ID: 444bce1c98c45147fe63e2132e9743a0c5e49598#diff-df9e6118c481ceb27faa399114fac0a1",
        "createdAt" : "2020-03-17T02:46:43Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 185,
    "diffHunk" : "@@ -1,1 +1573,1577 @@        \"memory usage when Zstd is used, but it might increase the compression \" +\n        \"cost because of excessive JNI call overhead\")\n      .version(\"2.3.0\")\n      .bytesConf(ByteUnit.BYTE)\n      .createWithDefaultString(\"32k\")"
  },
  {
    "id" : "ef0d2ed9-3eaf-41f8-98fa-b177e7008d50",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375704250",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bc649018-79c0-4565-938c-80c005b54082",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-19112, commit ID: 444bce1c98c45147fe63e2132e9743a0c5e49598#diff-df9e6118c481ceb27faa399114fac0a1",
        "createdAt" : "2020-03-17T02:46:53Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 193,
    "diffHunk" : "@@ -1,1 +1581,1585 @@      .doc(\"Compression level for Zstd compression codec. Increasing the compression \" +\n        \"level will result in better compression at the expense of more CPU and memory\")\n      .version(\"2.3.0\")\n      .intConf\n      .createWithDefault(1)"
  },
  {
    "id" : "04dc2fcc-542c-4f59-8630-750e879ba515",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375704384",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8ee64282-0ece-47d7-b299-2caf2a254766",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-28366, commit ID: 26d03b62e20d053943d03b5c5573dd349e49654c#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-17T02:47:25Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 201,
    "diffHunk" : "@@ -1,1 +1590,1594 @@      .doc(\"If the size in bytes of a file loaded by Spark exceeds this threshold, \" +\n        \"a warning is logged with the possible reasons.\")\n      .version(\"3.0.0\")\n      .bytesConf(ByteUnit.BYTE)\n      .createWithDefault(1024 * 1024 * 1024)"
  },
  {
    "id" : "fc2b0c5c-3355-4639-845c-f36047e89004",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375704527",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d438dd3a-07bf-4bdd-8d5d-131df46aea8c",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-28118, commit ID: 47f54b1ec717d0d744bf3ad46bb1ed3542b667c8#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-17T02:47:57Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 209,
    "diffHunk" : "@@ -1,1 +1599,1603 @@        \"lz4, lzf, snappy, and zstd. You can also use fully qualified class names to specify \" +\n        \"the codec. If this is not given, spark.io.compression.codec will be used.\")\n      .version(\"3.0.0\")\n      .fallbackConf(IO_COMPRESSION_CODEC)\n"
  },
  {
    "id" : "a3ec60dc-3b4f-4b05-a93c-90c712e546c6",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375704656",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0a584faf-ef3d-4384-b882-302966b57367",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "No JIRA ID, commit ID: 4b1646a25f7581cecae108553da13833e842e68a#diff-eaf125f56ce786d64dcef99cf446a751",
        "createdAt" : "2020-03-17T02:48:21Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 214,
    "diffHunk" : "@@ -1,1 +1604,1608 @@  private[spark] val BUFFER_SIZE =\n    ConfigBuilder(\"spark.buffer.size\")\n      .version(\"0.5.0\")\n      .intConf\n      .checkValue(_ >= 0, \"The buffer size must not be negative\")"
  },
  {
    "id" : "103eb4aa-98d6-4ec7-8794-f5e6b039de5c",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375704793",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "921961fb-b4dc-4791-968a-c19c82ef2816",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "No JIRA ID, commit ID: 46eecd110a4017ea0c86cbb1010d0ccd6a5eb2ef#diff-264da78fe625d594eae59d1adabc8ae9",
        "createdAt" : "2020-03-17T02:48:52Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 220,
    "diffHunk" : "@@ -1,1 +1610,1614 @@\n  private[spark] val LOCALITY_WAIT_PROCESS = ConfigBuilder(\"spark.locality.wait.process\")\n    .version(\"0.8.0\")\n    .fallbackConf(LOCALITY_WAIT)\n"
  },
  {
    "id" : "9b83c561-92a0-47c4-9c3d-0d44d7b46862",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375704924",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0fb3bd5c-e11a-47f5-811f-7f63d1e6ac21",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "No JIRA ID, commit ID: 46eecd110a4017ea0c86cbb1010d0ccd6a5eb2ef#diff-264da78fe625d594eae59d1adabc8ae9",
        "createdAt" : "2020-03-17T02:49:16Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 224,
    "diffHunk" : "@@ -1,1 +1614,1618 @@\n  private[spark] val LOCALITY_WAIT_NODE = ConfigBuilder(\"spark.locality.wait.node\")\n    .version(\"0.8.0\")\n    .fallbackConf(LOCALITY_WAIT)\n"
  },
  {
    "id" : "54f45a6a-0903-491f-b0d9-4fa9cc21c803",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375704959",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9e817a68-42c1-48b4-a072-403e3a94c4b0",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "No JIRA ID, commit ID: 46eecd110a4017ea0c86cbb1010d0ccd6a5eb2ef#diff-264da78fe625d594eae59d1adabc8ae9",
        "createdAt" : "2020-03-17T02:49:25Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 228,
    "diffHunk" : "@@ -1,1 +1618,1622 @@\n  private[spark] val LOCALITY_WAIT_RACK = ConfigBuilder(\"spark.locality.wait.rack\")\n    .version(\"0.8.0\")\n    .fallbackConf(LOCALITY_WAIT)\n"
  },
  {
    "id" : "a7dfe4ea-a542-4335-883e-124ccca8ebfb",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375705102",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a077d078-e59d-448a-8991-ab4411d13bb1",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-5932, commit ID: 2d222fb39dd978e5a33cde6ceb59307cbdf7b171#diff-529fc5c06b9731c1fbda6f3db60b16aa",
        "createdAt" : "2020-03-17T02:50:00Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 237,
    "diffHunk" : "@@ -1,1 +1626,1630 @@      \"buffer to receive it, this represents a fixed memory overhead per reduce task, \" +\n      \"so keep it small unless you have a large amount of memory\")\n    .version(\"1.4.0\")\n    .bytesConf(ByteUnit.MiB)\n    .createWithDefaultString(\"48m\")"
  },
  {
    "id" : "3f92962f-190c-41cc-a416-13676fa12c66",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375705196",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9241e54b-188d-43fd-8e6d-5207d17cc434",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-6166, commit ID: 894921d813a259f2f266fde7d86d2ecb5a0af24b#diff-eb30a71e0d04150b8e0b64929852e38b",
        "createdAt" : "2020-03-17T02:50:21Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 245,
    "diffHunk" : "@@ -1,1 +1636,1640 @@      \"causing the workers to fail under load. By allowing it to limit the number of \" +\n      \"fetch requests, this scenario can be mitigated\")\n    .version(\"2.0.0\")\n    .intConf\n    .createWithDefault(Int.MaxValue)"
  },
  {
    "id" : "2d1145b0-3539-41e0-9e81-d5e18d549c54",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375705285",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "35babc6b-0366-4f40-aa89-2273215e5498",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "No JIRA ID, commit ID: efc5423210d1aadeaea78273a4a8f10425753079#diff-76170a9c8f67b542bc58240a0a12fe08",
        "createdAt" : "2020-03-17T02:50:43Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 252,
    "diffHunk" : "@@ -1,1 +1643,1647 @@    .doc(\"Whether to compress broadcast variables before sending them. \" +\n      \"Generally a good idea. Compression will use spark.io.compression.codec\")\n    .version(\"0.6.0\")\n    .booleanConf.createWithDefault(true)\n"
  },
  {
    "id" : "389c374e-f52f-4178-8ed5-139491ac2af6",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375705385",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6afd22bf-4ff4-44bf-bc67-8f824741fd55",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "No JIRA ID, commit ID: b8ab7862b8bd168bca60bd930cd97c1099fbc8a8#diff-271d7958e14cdaa46cf3737cfcf51341",
        "createdAt" : "2020-03-17T02:51:04Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 260,
    "diffHunk" : "@@ -1,1 +1651,1655 @@      \"parallelism during broadcast (makes it slower); however, \" +\n      \"if it is too small, BlockManager might take a performance hit\")\n    .version(\"0.5.0\")\n    .bytesConf(ByteUnit.KiB)\n    .createWithDefaultString(\"4m\")"
  },
  {
    "id" : "9a5c1c80-7661-4655-80ce-75eb73984642",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375705541",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e90cd498-39ee-455c-8424-d8571328f536",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-18188, commit ID: 06a56df226aa0c03c21f23258630d8a96385c696#diff-4f43d14923008c6650a8eb7b40c07f74",
        "createdAt" : "2020-03-17T02:51:39Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 269,
    "diffHunk" : "@@ -1,1 +1661,1665 @@      \"more data. It's possible to disable it if the network has other \" +\n      \"mechanisms to guarantee data won't be corrupted during broadcast\")\n    .version(\"2.1.1\")\n    .booleanConf\n    .createWithDefault(true)"
  },
  {
    "id" : "945d4887-af51-4a38-a837-29295885730b",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375705669",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e8f6b54a-d608-4ee7-9cba-19f72c46f112",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-28355, commit ID: 79e204770300dab4a669b9f8e2421ef905236e7b#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-17T02:52:05Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 277,
    "diffHunk" : "@@ -1,1 +1669,1673 @@      .doc(\"The threshold at which user-defined functions (UDFs) and Python RDD commands \" +\n        \"are compressed by broadcast in bytes unless otherwise specified\")\n      .version(\"3.0.0\")\n      .bytesConf(ByteUnit.BYTE)\n      .checkValue(v => v >= 0, \"The threshold should be non-negative.\")"
  },
  {
    "id" : "3790f0b6-c732-4000-ab87-393eff735f09",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375705789",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8f17f6fe-eb9e-4e4b-9ae7-6cc22cdaf0f5",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "No JIRA ID, commit ID: efc5423210d1aadeaea78273a4a8f10425753079#diff-76170a9c8f67b542bc58240a0a12fe08",
        "createdAt" : "2020-03-17T02:52:33Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 286,
    "diffHunk" : "@@ -1,1 +1680,1684 @@      \"space at the cost of some extra CPU time. \" +\n      \"Compression will use spark.io.compression.codec\")\n    .version(\"0.6.0\")\n    .booleanConf\n    .createWithDefault(false)"
  },
  {
    "id" : "8c741fc1-c20c-46ae-9ec3-7063a15b1c03",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375705895",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7e1880fc-8edd-4fc6-ba51-8dfb3520e2e4",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-9926, commit ID: 80a4bfa4d1c86398b90b26c34d8dcbc2355f5a6a#diff-eaababfc87ea4949f97860e8b89b7586",
        "createdAt" : "2020-03-17T02:52:58Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 292,
    "diffHunk" : "@@ -1,1 +1686,1690 @@  private[spark] val RDD_PARALLEL_LISTING_THRESHOLD =\n    ConfigBuilder(\"spark.rdd.parallelListingThreshold\")\n      .version(\"2.0.0\")\n      .intConf\n      .createWithDefault(10)"
  },
  {
    "id" : "356ef1af-0ab4-49ca-80fe-6663f2a1c1b9",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375706017",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a29cbc9f-9b07-4ec0-9110-1fd80ef31552",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-16984, commit ID: 806d8a8e980d8ba2f4261bceb393c40bafaa2f73#diff-1d55e54678eff2076263f2fe36150c17",
        "createdAt" : "2020-03-17T02:53:23Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 298,
    "diffHunk" : "@@ -1,1 +1692,1696 @@  private[spark] val RDD_LIMIT_SCALE_UP_FACTOR =\n    ConfigBuilder(\"spark.rdd.limit.scaleUpFactor\")\n      .version(\"2.1.0\")\n      .intConf\n      .createWithDefault(4)"
  },
  {
    "id" : "4ac97a98-4b5f-49ff-88fc-e3564ab01789",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375706134",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9e954da0-56cc-4606-835b-7d4345834e20",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "No JIRA ID, commit ID: fd1d255821bde844af28e897fabd59a715659038#diff-b920b65c23bf3a1b3326325b0d6a81b2",
        "createdAt" : "2020-03-17T02:53:51Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 303,
    "diffHunk" : "@@ -1,1 +1697,1701 @@\n  private[spark] val SERIALIZER = ConfigBuilder(\"spark.serializer\")\n    .version(\"0.5.0\")\n    .stringConf\n    .createWithDefault(\"org.apache.spark.serializer.JavaSerializer\")"
  },
  {
    "id" : "6c980365-9db1-465f-9705-635e4127aa89",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375706290",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6436e655-d0d6-4de0-92f3-2a9aab07ab82",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-942, commit ID: 40566e10aae4b21ffc71ea72702b8df118ac5c8e#diff-6a59dfc43d1b31dc1c3072ceafa829f5",
        "createdAt" : "2020-03-17T02:54:23Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 309,
    "diffHunk" : "@@ -1,1 +1703,1707 @@  private[spark] val SERIALIZER_OBJECT_STREAM_RESET =\n    ConfigBuilder(\"spark.serializer.objectStreamReset\")\n      .version(\"1.0.0\")\n      .intConf\n      .createWithDefault(100)"
  },
  {
    "id" : "362b5287-5b91-445a-90e0-a603ac8fc841",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375706418",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "77b8ca5d-d2a9-4d1d-ae91-6f0dc4b16483",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-5307, commit ID: 636408311deeebd77fb83d2249e0afad1a1ba149#diff-6a59dfc43d1b31dc1c3072ceafa829f5",
        "createdAt" : "2020-03-17T02:54:49Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 314,
    "diffHunk" : "@@ -1,1 +1708,1712 @@\n  private[spark] val SERIALIZER_EXTRA_DEBUG_INFO = ConfigBuilder(\"spark.serializer.extraDebugInfo\")\n    .version(\"1.3.0\")\n    .booleanConf\n    .createWithDefault(true)"
  },
  {
    "id" : "6bd7e756-d65f-4deb-9141-c6551d29b1e6",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375706538",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "41f4576c-11ad-414c-a05e-e8488b62d48c",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "No JIRA ID, commit ID: f1d206c6b4c0a5b2517b05af05fdda6049e2f7c2#diff-364713d7776956cb8b0a771e9b62f82d",
        "createdAt" : "2020-03-17T02:55:18Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 319,
    "diffHunk" : "@@ -1,1 +1713,1717 @@\n  private[spark] val JARS = ConfigBuilder(\"spark.jars\")\n    .version(\"0.9.0\")\n    .stringConf\n    .toSequence"
  },
  {
    "id" : "1ae8c0a9-d6de-4b52-9795-e84d808a88fa",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375706672",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1bc6c932-8184-410c-80e0-1e86ec0a98cc",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "No JIRA ID, commit ID: 29ee101c73bf066bf7f4f8141c475b8d1bd3cf1c#diff-364713d7776956cb8b0a771e9b62f82d",
        "createdAt" : "2020-03-17T02:55:47Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 325,
    "diffHunk" : "@@ -1,1 +1719,1723 @@\n  private[spark] val FILES = ConfigBuilder(\"spark.files\")\n    .version(\"1.0.0\")\n    .stringConf\n    .toSequence"
  },
  {
    "id" : "420d64bd-d37c-4b0d-914c-0d5ecc8b9446",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375706773",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7a1d8c01-6dca-4aaf-b4dd-b70cb236a1e9",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-6797, commit ID: 7f487c8bde14dbdd244a3493ad11a129ef2bb327#diff-4d2ab44195558d5a9d5f15b8803ef39d",
        "createdAt" : "2020-03-17T02:56:10Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 331,
    "diffHunk" : "@@ -1,1 +1725,1729 @@\n  private[spark] val SUBMIT_DEPLOY_MODE = ConfigBuilder(\"spark.submit.deployMode\")\n    .version(\"1.5.0\")\n    .stringConf\n    .createWithDefault(\"client\")"
  },
  {
    "id" : "fae3bbcd-183a-4a80-bdbd-a86bbe962029",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375706882",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1a05c276-4c59-41f7-ac2e-fd2f3991281d",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-1549, commit ID: d7ddb26e1fa02e773999cc4a97c48d2cd1723956#diff-4d2ab44195558d5a9d5f15b8803ef39d",
        "createdAt" : "2020-03-17T02:56:33Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 336,
    "diffHunk" : "@@ -1,1 +1730,1734 @@\n  private[spark] val SUBMIT_PYTHON_FILES = ConfigBuilder(\"spark.submit.pyFiles\")\n    .version(\"1.0.1\")\n    .stringConf\n    .toSequence"
  },
  {
    "id" : "dc961e83-0eb2-4dde-a338-b99035a01bf3",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375706976",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0144a675-9b34-49c8-aeee-1d8b101204a3",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "No JIRA ID, commit ID: 976fe60f7609d7b905a34f18743efabd966407f0#diff-9bc0105ee454005379abed710cd20ced",
        "createdAt" : "2020-03-17T02:56:55Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 343,
    "diffHunk" : "@@ -1,1 +1737,1741 @@  private[spark] val SCHEDULER_ALLOCATION_FILE =\n    ConfigBuilder(\"spark.scheduler.allocation.file\")\n      .version(\"0.8.1\")\n      .stringConf\n      .createOptional"
  },
  {
    "id" : "83afeeaa-6ce9-47ca-bc26-8ded52f2944f",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375707082",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "57f37310-1045-4e7e-93b3-ec1d354b49cc",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-2635, commit ID: 3311da2f9efc5ff2c7d01273ac08f719b067d11d#diff-7d99a7c7a051e5e851aaaefb275a44a1",
        "createdAt" : "2020-03-17T02:57:20Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 349,
    "diffHunk" : "@@ -1,1 +1743,1747 @@  private[spark] val SCHEDULER_MIN_REGISTERED_RESOURCES_RATIO =\n    ConfigBuilder(\"spark.scheduler.minRegisteredResourcesRatio\")\n      .version(\"1.1.1\")\n      .doubleConf\n      .createOptional"
  },
  {
    "id" : "ab655174-a017-4e3d-993e-765b1c577333",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375707172",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "71f8a1a4-b19b-4f7e-9ac6-2bd2b774f42a",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-2635, commit ID: 3311da2f9efc5ff2c7d01273ac08f719b067d11d#diff-7d99a7c7a051e5e851aaaefb275a44a1",
        "createdAt" : "2020-03-17T02:57:39Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 355,
    "diffHunk" : "@@ -1,1 +1749,1753 @@  private[spark] val SCHEDULER_MAX_REGISTERED_RESOURCE_WAITING_TIME =\n    ConfigBuilder(\"spark.scheduler.maxRegisteredResourcesWaitingTime\")\n      .version(\"1.1.1\")\n      .timeConf(TimeUnit.MILLISECONDS)\n      .createWithDefaultString(\"30s\")"
  },
  {
    "id" : "e7b414e2-7d55-42e4-9ab3-ed644731df85",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375707298",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2d0256ea-a603-47e5-92db-a72dc4f4424b",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "No JIRA ID, commit ID: 98fb69822cf780160bca51abeaab7c82e49fab54#diff-cb7a25b3c9a7341c6d99bcb8e9780c92",
        "createdAt" : "2020-03-17T02:58:04Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 361,
    "diffHunk" : "@@ -1,1 +1755,1759 @@  private[spark] val SCHEDULER_MODE =\n    ConfigBuilder(\"spark.scheduler.mode\")\n      .version(\"0.8.0\")\n      .stringConf\n      .createWithDefault(SchedulingMode.FIFO.toString)"
  },
  {
    "id" : "7054272e-0f03-4ab6-9928-2a8d21a56d7a",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375707404",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "617a7897-fc1b-4348-abe6-9ec6a7d6ca47",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "No JIRA ID, commit ID: d0c9d41a061969d409715b86a91937d8de4c29f7#diff-7d99a7c7a051e5e851aaaefb275a44a1",
        "createdAt" : "2020-03-17T02:58:26Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 367,
    "diffHunk" : "@@ -1,1 +1761,1765 @@  private[spark] val SCHEDULER_REVIVE_INTERVAL =\n    ConfigBuilder(\"spark.scheduler.revive.interval\")\n      .version(\"0.8.1\")\n      .timeConf(TimeUnit.MILLISECONDS)\n      .createOptional"
  },
  {
    "id" : "ca147ed0-5512-470b-bd4d-6f8ae23393ec",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375707603",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "78b1b93f-9dae-406a-a99c-74332d78e07c",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "No JIRA ID, commit ID: e72afdb817bcc8388aeb8b8d31628fd5fd67acf1#diff-4e188f32951dc989d97fa7577858bc7c",
        "createdAt" : "2020-03-17T02:59:06Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 373,
    "diffHunk" : "@@ -1,1 +1767,1771 @@  private[spark] val SPECULATION_ENABLED =\n    ConfigBuilder(\"spark.speculation\")\n      .version(\"0.6.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "17ff290a-6893-45ce-8871-95073c1ea208",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375707647",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "000aa1f2-80f7-498c-a606-835d9c29907f",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "No JIRA ID, commit ID: e72afdb817bcc8388aeb8b8d31628fd5fd67acf1#diff-4e188f32951dc989d97fa7577858bc7c",
        "createdAt" : "2020-03-17T02:59:18Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 379,
    "diffHunk" : "@@ -1,1 +1773,1777 @@  private[spark] val SPECULATION_INTERVAL =\n    ConfigBuilder(\"spark.speculation.interval\")\n      .version(\"0.6.0\")\n      .timeConf(TimeUnit.MILLISECONDS)\n      .createWithDefault(100)"
  },
  {
    "id" : "5d22f030-ec18-4553-8fa2-a200107276fb",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375707766",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d6814db9-a3da-4f90-b626-0fe332c48574",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "No JIRA ID, commit ID: e72afdb817bcc8388aeb8b8d31628fd5fd67acf1#diff-fff59f72dfe6ca4ccb607ad12535da07",
        "createdAt" : "2020-03-17T02:59:45Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 385,
    "diffHunk" : "@@ -1,1 +1779,1783 @@  private[spark] val SPECULATION_MULTIPLIER =\n    ConfigBuilder(\"spark.speculation.multiplier\")\n      .version(\"0.6.0\")\n      .doubleConf\n      .createWithDefault(1.5)"
  },
  {
    "id" : "494b66ec-11d2-40f5-ae7a-1ef6ec6e036f",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375707818",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ee4ae6d0-40af-492f-815b-9f82598c7eda",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "No JIRA ID, commit ID: e72afdb817bcc8388aeb8b8d31628fd5fd67acf1#diff-fff59f72dfe6ca4ccb607ad12535da07",
        "createdAt" : "2020-03-17T02:59:58Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 391,
    "diffHunk" : "@@ -1,1 +1785,1789 @@  private[spark] val SPECULATION_QUANTILE =\n    ConfigBuilder(\"spark.speculation.quantile\")\n      .version(\"0.6.0\")\n      .doubleConf\n      .createWithDefault(0.75)"
  },
  {
    "id" : "a7a58b04-5c8d-4b74-99ad-658c27df96d5",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375707937",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3bbde446-d9f9-41b1-b18e-94a318a8b4a9",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-29976, commit ID: ad238a2238a9d0da89be4424574436cbfaee579d#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-17T03:00:23Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 399,
    "diffHunk" : "@@ -1,1 +1799,1803 @@        \"even though the threshold hasn't been reached. The number of slots is computed based \" +\n        \"on the conf values of spark.executor.cores and spark.task.cpus minimum 1.\")\n      .version(\"3.0.0\")\n      .timeConf(TimeUnit.MILLISECONDS)\n      .createOptional"
  },
  {
    "id" : "e64fcfcd-e517-4770-a7a9-3a9e42ebbc66",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375708145",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1f4f9b07-fb32-40d6-b447-9d3355f10e9a",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-13063, commit ID: bc36df127d3b9f56b4edaeb5eca7697d4aef761a#diff-14b8ed2ef4e3da985300b8d796a38fa9",
        "createdAt" : "2020-03-17T03:00:41Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 405,
    "diffHunk" : "@@ -1,1 +1805,1809 @@  private[spark] val STAGING_DIR = ConfigBuilder(\"spark.yarn.stagingDir\")\n    .doc(\"Staging directory used while submitting applications.\")\n    .version(\"2.0.0\")\n    .stringConf\n    .createOptional"
  },
  {
    "id" : "66699443-a718-4bcf-8ae1-9272675d411d",
    "prId" : 27931,
    "prUrl" : "https://github.com/apache/spark/pull/27931#pullrequestreview-375708626",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "41c76f30-d707-4c71-bd00-74441ad7bb9e",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-9411, commit ID: 1b0099fc62d02ff6216a76fbfe17a4ec5b2f3536#diff-1b22e54318c04824a6d53ed3f4d1bb35",
        "createdAt" : "2020-03-17T03:01:04Z",
        "updatedAt" : "2020-03-19T06:40:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "f3c9998f44611a6d7a45f11b1406755a7bcd0b34",
    "line" : 411,
    "diffHunk" : "@@ -1,1 +1811,1815 @@  private[spark] val BUFFER_PAGESIZE = ConfigBuilder(\"spark.buffer.pageSize\")\n    .doc(\"The amount of memory used per page in bytes\")\n    .version(\"1.5.0\")\n    .bytesConf(ByteUnit.BYTE)\n    .createOptional"
  },
  {
    "id" : "585dfc0c-db23-48ea-954f-0163f9c1a6d8",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374715708",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d8dab408-d683-4742-be31-22ffb323f443",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-5847, commit ID: 70f846a313061e4db6174e0dc6c12c8c806ccf78#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:26:15Z",
        "updatedAt" : "2020-03-14T11:26:15Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +777,781 @@  // This property sets the root namespace for metrics reporting\n  private[spark] val METRICS_NAMESPACE = ConfigBuilder(\"spark.metrics.namespace\")\n    .version(\"2.1.0\")\n    .stringConf\n    .createOptional"
  },
  {
    "id" : "37b4ff4a-a3bd-4ec7-a3fe-d2346ae30ff2",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374715724",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "884cb813-fa15-4205-84bd-862f50f5bc11",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "No JIRA ID, commit ID: 46eecd110a4017ea0c86cbb1010d0ccd6a5eb2ef#diff-7ea2624e832b166ca27cd4baca8691d9",
        "createdAt" : "2020-03-14T11:26:33Z",
        "updatedAt" : "2020-03-14T11:26:33Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +782,786 @@\n  private[spark] val METRICS_CONF = ConfigBuilder(\"spark.metrics.conf\")\n    .version(\"0.8.0\")\n    .stringConf\n    .createOptional"
  },
  {
    "id" : "146faa5f-953a-44c2-83c3-1b3d4e5e6a99",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374715743",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5f4cfded-0e7a-4be5-98de-ccfb94282c9d",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-27189, commit ID: 729f43f499f3dd2718c0b28d73f2ca29cc811eac#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:26:57Z",
        "updatedAt" : "2020-03-14T11:26:58Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +789,793 @@    ConfigBuilder(\"spark.metrics.executorMetricsSource.enabled\")\n      .doc(\"Whether to register the ExecutorMetrics source with the metrics system.\")\n      .version(\"3.0.0\")\n      .booleanConf\n      .createWithDefault(true)"
  },
  {
    "id" : "164220cb-e027-4b8c-81a3-849e42745c06",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374715779",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5dd47939-3ace-464c-859c-17c168727bf4",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-30060, commit ID: 60f20e5ea2000ab8f4a593b5e4217fd5637c5e22#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:27:34Z",
        "updatedAt" : "2020-03-14T11:27:34Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +796,800 @@    ConfigBuilder(\"spark.metrics.staticSources.enabled\")\n      .doc(\"Whether to register static sources with the metrics system.\")\n      .version(\"3.0.0\")\n      .booleanConf\n      .createWithDefault(true)"
  },
  {
    "id" : "0b3b93ed-9897-4002-b5ec-db184f8feb56",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374715801",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c31b403b-9eae-48f9-9a2c-ed5b2996370d",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-13081, commit ID: 7a9e25c38380e6c62080d62ad38a4830e44fe753#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:27:59Z",
        "updatedAt" : "2020-03-14T11:28:00Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +801,805 @@\n  private[spark] val PYSPARK_DRIVER_PYTHON = ConfigBuilder(\"spark.pyspark.driver.python\")\n    .version(\"2.1.0\")\n    .stringConf\n    .createOptional"
  },
  {
    "id" : "b128a816-cac6-4475-8d90-1a2b81035a35",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374715809",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0f912b5a-0c42-41c0-aa6f-13a753a7ee7c",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-13081, commit ID: 7a9e25c38380e6c62080d62ad38a4830e44fe753#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:28:07Z",
        "updatedAt" : "2020-03-14T11:28:07Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 33,
    "diffHunk" : "@@ -1,1 +806,810 @@\n  private[spark] val PYSPARK_PYTHON = ConfigBuilder(\"spark.pyspark.python\")\n    .version(\"2.1.0\")\n    .stringConf\n    .createOptional"
  },
  {
    "id" : "1cb79d38-68cc-4ff5-ad21-564d619f3c81",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374715823",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a3094837-7194-4853-856a-0a3a077a3e46",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-17243, commit ID: 021aa28f439443cda1bc7c5e3eee7c85b40c1a2d#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:28:27Z",
        "updatedAt" : "2020-03-14T11:28:27Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 41,
    "diffHunk" : "@@ -1,1 +813,817 @@  private[spark] val HISTORY_UI_MAX_APPS =\n    ConfigBuilder(\"spark.history.ui.maxApplications\")\n      .version(\"2.0.1\")\n      .intConf\n      .createWithDefault(Integer.MAX_VALUE)"
  },
  {
    "id" : "b12c833d-356a-4680-97af-a340d0e57550",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374715856",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "61eb2a66-d3f1-4d55-983b-ec82ab5bcb81",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-5682, commit ID: 4b4e329e49f8af28fa6301bd06c48d7097eaf9e6#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:29:07Z",
        "updatedAt" : "2020-03-14T11:29:07Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 46,
    "diffHunk" : "@@ -1,1 +818,822 @@\n  private[spark] val IO_ENCRYPTION_ENABLED = ConfigBuilder(\"spark.io.encryption.enabled\")\n    .version(\"2.1.0\")\n    .booleanConf\n    .createWithDefault(false)"
  },
  {
    "id" : "2908d230-d0e2-4798-9f36-a17c76c039e1",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374715864",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "82e49647-a5ed-4678-b1a6-44155ae304a1",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-5682, commit ID: 4b4e329e49f8af28fa6301bd06c48d7097eaf9e6#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:29:18Z",
        "updatedAt" : "2020-03-14T11:29:18Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 52,
    "diffHunk" : "@@ -1,1 +824,828 @@  private[spark] val IO_ENCRYPTION_KEYGEN_ALGORITHM =\n    ConfigBuilder(\"spark.io.encryption.keygen.algorithm\")\n      .version(\"2.1.0\")\n      .stringConf\n      .createWithDefault(\"HmacSHA1\")"
  },
  {
    "id" : "03932a8e-f0fa-48cc-9afd-e4407f1a673d",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374715870",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ccbb2bad-a29c-43d9-9193-c8bb3ee502c3",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-5682, commit ID: 4b4e329e49f8af28fa6301bd06c48d7097eaf9e6#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:29:25Z",
        "updatedAt" : "2020-03-14T11:29:25Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 57,
    "diffHunk" : "@@ -1,1 +829,833 @@\n  private[spark] val IO_ENCRYPTION_KEY_SIZE_BITS = ConfigBuilder(\"spark.io.encryption.keySizeBits\")\n    .version(\"2.1.0\")\n    .intConf\n    .checkValues(Set(128, 192, 256))"
  },
  {
    "id" : "f3efc81e-acfe-4ce5-bad6-9ac34109b4f0",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374715879",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e2669009-4c52-43c6-bd82-556ceca97731",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-5682, commit ID: 4b4e329e49f8af28fa6301bd06c48d7097eaf9e6#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:29:39Z",
        "updatedAt" : "2020-03-14T11:29:40Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 65,
    "diffHunk" : "@@ -1,1 +837,841 @@    ConfigBuilder(\"spark.io.crypto.cipher.transformation\")\n      .internal()\n      .version(\"2.1.0\")\n      .stringConf\n      .createWithDefaultString(\"AES/CTR/NoPadding\")"
  },
  {
    "id" : "8ee1fd9b-aadc-4606-967a-6c23ba43016e",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374715900",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6978a11c-09a0-49ef-a348-9f9fb0cff9cd",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "No JIRA ID, commit ID: 02a6761589c35f15f1a6e3b63a7964ba057d3ba6#diff-eaf125f56ce786d64dcef99cf446a751",
        "createdAt" : "2020-03-14T11:30:03Z",
        "updatedAt" : "2020-03-14T11:30:03Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 71,
    "diffHunk" : "@@ -1,1 +843,847 @@  private[spark] val DRIVER_HOST_ADDRESS = ConfigBuilder(\"spark.driver.host\")\n    .doc(\"Address of driver endpoints.\")\n    .version(\"0.7.0\")\n    .stringConf\n    .createWithDefault(Utils.localCanonicalHostName())"
  },
  {
    "id" : "f0c0b196-15c6-4d48-ab36-f3874d6cd779",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374715906",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2669f4bb-0fe9-4f66-ae5f-213df0499a19",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "No JIRA ID, commit ID: 02a6761589c35f15f1a6e3b63a7964ba057d3ba6#diff-eaf125f56ce786d64dcef99cf446a751",
        "createdAt" : "2020-03-14T11:30:10Z",
        "updatedAt" : "2020-03-14T11:30:10Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 77,
    "diffHunk" : "@@ -1,1 +849,853 @@  private[spark] val DRIVER_PORT = ConfigBuilder(\"spark.driver.port\")\n    .doc(\"Port of driver endpoints.\")\n    .version(\"0.7.0\")\n    .intConf\n    .createWithDefault(0)"
  },
  {
    "id" : "92694d4c-f434-4b3d-8ae2-da27af4a6bfb",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374715930",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6c2891ad-f9e3-48b4-8179-2515a9aa3d57",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-5388, commit ID: 6ec0cdc14390d4dc45acf31040f21e1efc476fc0#diff-4d2ab44195558d5a9d5f15b8803ef39d",
        "createdAt" : "2020-03-14T11:30:33Z",
        "updatedAt" : "2020-03-14T11:30:33Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 84,
    "diffHunk" : "@@ -1,1 +856,860 @@    .doc(\"If true, restarts the driver automatically if it fails with a non-zero exit status. \" +\n      \"Only has effect in Spark standalone mode or Mesos cluster deploy mode.\")\n    .version(\"1.3.0\")\n    .booleanConf\n    .createWithDefault(false)"
  },
  {
    "id" : "9fe399b5-d3f3-4ddb-a921-b9fc8f012519",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374715945",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9c7fe142-405b-4884-a66a-6bb4af905fa9",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-4563, commit ID: 2cd1bfa4f0c6625b0ab1dbeba2b9586b9a6a9f42#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:30:54Z",
        "updatedAt" : "2020-03-14T11:30:55Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 90,
    "diffHunk" : "@@ -1,1 +862,866 @@  private[spark] val DRIVER_BIND_ADDRESS = ConfigBuilder(\"spark.driver.bindAddress\")\n    .doc(\"Address where to bind network listen sockets on the driver.\")\n    .version(\"2.1.0\")\n    .fallbackConf(DRIVER_HOST_ADDRESS)\n"
  },
  {
    "id" : "745808dc-9dbe-4c74-997b-ddcbbd1beda0",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374715959",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7d97b642-4880-467f-b5f6-481e19c648c7",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-4563, commit ID: 2cd1bfa4f0c6625b0ab1dbeba2b9586b9a6a9f42#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:31:11Z",
        "updatedAt" : "2020-03-14T11:31:12Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 101,
    "diffHunk" : "@@ -1,1 +873,877 @@  private[spark] val DRIVER_BLOCK_MANAGER_PORT = ConfigBuilder(\"spark.driver.blockManager.port\")\n    .doc(\"Port to use for the block manager on the driver.\")\n    .version(\"2.1.0\")\n    .fallbackConf(BLOCK_MANAGER_PORT)\n"
  },
  {
    "id" : "2bea776d-4979-436a-864f-f1cc05ca91d1",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374715965",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "46bf56ab-8f0d-4b60-9e33-59e7a39c8c36",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-2157, commit ID: 31090e43ca91f687b0bc6e25c824dc25bd7027cd#diff-2b643ea78c1add0381754b1f47eec132",
        "createdAt" : "2020-03-14T11:31:29Z",
        "updatedAt" : "2020-03-14T11:31:29Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 95,
    "diffHunk" : "@@ -1,1 +867,871 @@  private[spark] val BLOCK_MANAGER_PORT = ConfigBuilder(\"spark.blockManager.port\")\n    .doc(\"Port to use for the block manager when a more specific setting is not provided.\")\n    .version(\"1.1.0\")\n    .intConf\n    .createWithDefault(0)"
  },
  {
    "id" : "6ce3ab93-0d68-4c9f-8d5d-61b8c9f9b3fa",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374715985",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f298cd61-7c08-4b37-8994-d798e5e34d2e",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-17850, commit ID: 47776e7c0c68590fe446cef910900b1aaead06f9#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:31:54Z",
        "updatedAt" : "2020-03-14T11:31:54Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 108,
    "diffHunk" : "@@ -1,1 +880,884 @@      \"encountering corrupted or non-existing files and contents that have been read will still \" +\n      \"be returned.\")\n    .version(\"2.1.0\")\n    .booleanConf\n    .createWithDefault(false)"
  },
  {
    "id" : "46f7f5b7-97a9-4741-bf4a-d4e25c34df92",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716003",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3aea0e36-7df1-4d5c-a3a7-994b3117546e",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-22676, commit ID: ed4101d29f50d54fd7846421e4c00e9ecd3599d0#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:32:15Z",
        "updatedAt" : "2020-03-14T11:32:16Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 115,
    "diffHunk" : "@@ -1,1 +887,891 @@    .doc(\"Whether to ignore missing files. If true, the Spark jobs will continue to run when \" +\n      \"encountering missing files and the contents that have been read will still be returned.\")\n    .version(\"2.4.0\")\n    .booleanConf\n    .createWithDefault(false)"
  },
  {
    "id" : "5e490c3b-147f-4b4e-9559-819314025623",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716027",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "adc80a40-bf3a-41fd-9fc2-7617ee2a33c3",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-16759, commit ID: 3af894511be6fcc17731e28b284dba432fe911f5#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:32:40Z",
        "updatedAt" : "2020-03-14T11:32:40Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 120,
    "diffHunk" : "@@ -1,1 +892,896 @@\n  private[spark] val APP_CALLER_CONTEXT = ConfigBuilder(\"spark.log.callerContext\")\n    .version(\"2.2.0\")\n    .stringConf\n    .createOptional"
  },
  {
    "id" : "40c5850b-95f1-4a04-95bf-3188898b8974",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716056",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3ffa1290-c9b0-4869-9b0c-e8bf38dd6549",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-16575, commit ID: c8879bf1ee2af9ccd5d5656571d931d2fc1da024#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:33:17Z",
        "updatedAt" : "2020-03-14T11:33:17Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 126,
    "diffHunk" : "@@ -1,1 +898,902 @@  private[spark] val FILES_MAX_PARTITION_BYTES = ConfigBuilder(\"spark.files.maxPartitionBytes\")\n    .doc(\"The maximum number of bytes to pack into a single partition when reading files.\")\n    .version(\"2.1.0\")\n    .bytesConf(ByteUnit.BYTE)\n    .createWithDefault(128 * 1024 * 1024)"
  },
  {
    "id" : "5d128898-b6fd-4ea2-91e9-f8935a2bf49f",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716061",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4dab63b7-f2e4-4c05-a78f-a0a66d4a2f8b",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-16575, commit ID: c8879bf1ee2af9ccd5d5656571d931d2fc1da024#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:33:26Z",
        "updatedAt" : "2020-03-14T11:33:26Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 134,
    "diffHunk" : "@@ -1,1 +907,911 @@      \" over estimate, then the partitions with small files will be faster than partitions with\" +\n      \" bigger files.\")\n    .version(\"2.1.0\")\n    .bytesConf(ByteUnit.BYTE)\n    .createWithDefault(4 * 1024 * 1024)"
  },
  {
    "id" : "a7acfdb5-a035-48fb-bf54-b35484efb64d",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716083",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ce333127-ce71-49ed-b1ad-b976ed8897ea",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-22233, commit ID: 0fa10666cf75e3c4929940af49c8a6f6ea874759#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:33:55Z",
        "updatedAt" : "2020-03-14T11:33:55Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 142,
    "diffHunk" : "@@ -1,1 +915,919 @@      .internal()\n      .doc(\"When true, HadoopRDD/NewHadoopRDD will not create partitions for empty input splits.\")\n      .version(\"2.3.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "3ed37f64-59e3-480c-a691-42c6a3486312",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716102",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "feb3e93a-9bbd-4c64-b5ed-a7a425fc7486",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-18535 and SPARK-19720, commit ID: 444cca14d7ac8c5ab5d7e9d080b11f4d6babe3bf#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:34:24Z",
        "updatedAt" : "2020-03-14T11:34:25Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 150,
    "diffHunk" : "@@ -1,1 +925,929 @@        \"a property key or value, the value is redacted from the environment UI and various logs \" +\n        \"like YARN and event logs.\")\n      .version(\"2.1.2\")\n      .regexConf\n      .createWithDefault(\"(?i)secret|password|token\".r)"
  },
  {
    "id" : "5e3e6a7c-dd3e-4d25-9845-332d6e4f08fe",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716117",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1acda4ee-0b64-478a-9386-f35dfb7e7f9e",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-20070, commit ID: 91fa80fe8a2480d64c430bd10f97b3d44c007bcc#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:34:55Z",
        "updatedAt" : "2020-03-14T11:34:55Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 158,
    "diffHunk" : "@@ -1,1 +934,938 @@        \"information. When this regex matches a string part, that string part is replaced by a \" +\n        \"dummy value. This is currently used to redact the output of SQL explain commands.\")\n      .version(\"2.2.0\")\n      .regexConf\n      .createOptional"
  },
  {
    "id" : "9172f60a-681c-478c-be1f-07f3ca77fb14",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716130",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "21682125-3a2e-4031-87ae-3f391b346c9f",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-1189, commit ID: 7edbea41b43e0dc11a2de156be220db8b7952d01#diff-afd88f677ec5ff8b5e96a5cbbe00cd98",
        "createdAt" : "2020-03-14T11:35:18Z",
        "updatedAt" : "2020-03-14T11:35:18Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 164,
    "diffHunk" : "@@ -1,1 +940,944 @@  private[spark] val AUTH_SECRET =\n    ConfigBuilder(\"spark.authenticate.secret\")\n      .version(\"1.0.0\")\n      .stringConf\n      .createOptional"
  },
  {
    "id" : "68d8659a-51cc-4e28-82be-0ec3dba98acc",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716134",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a4dd42b8-2bd0-4882-bccf-82de209e14a5",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-1189, commit ID: 7edbea41b43e0dc11a2de156be220db8b7952d01#diff-afd88f677ec5ff8b5e96a5cbbe00cd98",
        "createdAt" : "2020-03-14T11:35:26Z",
        "updatedAt" : "2020-03-14T11:35:26Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 176,
    "diffHunk" : "@@ -1,1 +952,956 @@  private[spark] val NETWORK_AUTH_ENABLED =\n    ConfigBuilder(\"spark.authenticate\")\n      .version(\"1.0.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "22f7d562-eddc-4eec-8bc9-75f38c3b2168",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716151",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2b27b7ef-5566-48f4-b4e6-1b06fc7baa42",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-11073, commit ID: f8d93edec82eedab59d50aec06ca2de7e4cf14f6#diff-afd88f677ec5ff8b5e96a5cbbe00cd98",
        "createdAt" : "2020-03-14T11:35:46Z",
        "updatedAt" : "2020-03-14T11:35:47Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 170,
    "diffHunk" : "@@ -1,1 +946,950 @@  private[spark] val AUTH_SECRET_BIT_LENGTH =\n    ConfigBuilder(\"spark.authenticate.secretBitLength\")\n      .version(\"1.6.0\")\n      .intConf\n      .createWithDefault(256)"
  },
  {
    "id" : "0a42a670-33ea-4d31-b13f-e89594db7951",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716174",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "32da1c5e-af1b-45ee-86c8-0e9d8128ce01",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-6229, commit ID: 38d4e9e446b425ca6a8fe8d8080f387b08683842#diff-afd88f677ec5ff8b5e96a5cbbe00cd98",
        "createdAt" : "2020-03-14T11:36:10Z",
        "updatedAt" : "2020-03-14T11:36:11Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 182,
    "diffHunk" : "@@ -1,1 +958,962 @@  private[spark] val SASL_ENCRYPTION_ENABLED =\n    ConfigBuilder(\"spark.authenticate.enableSaslEncryption\")\n      .version(\"1.4.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "a591ac68-8d65-4801-8f38-637a76402439",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716216",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "393cf433-2e6f-4e2f-b75f-e7eb0ef92d14",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-26239, commit ID: 57d6fbfa8c803ce1791e7be36aba0219a1fcaa63#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:36:55Z",
        "updatedAt" : "2020-03-14T11:36:55Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 190,
    "diffHunk" : "@@ -1,1 +968,972 @@        \"either entity (see below). File-based secret keys are only allowed when using \" +\n        \"Kubernetes.\")\n      .version(\"3.0.0\")\n      .stringConf\n      .createOptional"
  },
  {
    "id" : "9976f185-bc57-4190-9a33-0c1c29770e16",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716223",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c3b96c3f-138c-4e2a-9187-0147a1f7e9c1",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-26239, commit ID: 57d6fbfa8c803ce1791e7be36aba0219a1fcaa63#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:37:08Z",
        "updatedAt" : "2020-03-14T11:37:08Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 198,
    "diffHunk" : "@@ -1,1 +981,985 @@        \"used for both the driver and the executors when running in cluster mode. File-based \" +\n        \"secret keys are only allowed when using Kubernetes.\")\n      .version(\"3.0.0\")\n      .fallbackConf(AUTH_SECRET_FILE)\n"
  },
  {
    "id" : "83a71322-ccb7-43a1-a0c3-0b52db36e5e5",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716234",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6e37e5eb-8b97-43a8-b7ac-bce5cb8118f1",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-26239, commit ID: 57d6fbfa8c803ce1791e7be36aba0219a1fcaa63#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:37:19Z",
        "updatedAt" : "2020-03-14T11:37:19Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 206,
    "diffHunk" : "@@ -1,1 +993,997 @@        \"used for both the driver and the executors when running in cluster mode. File-based \" +\n        \"secret keys are only allowed when using Kubernetes.\")\n      .version(\"3.0.0\")\n      .fallbackConf(AUTH_SECRET_FILE)\n"
  },
  {
    "id" : "f4819b1d-0dcb-4f61-8869-3c9966361c9c",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716257",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fa3dcff5-3400-44f1-8331-bf7ff5e35f8c",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-21527, commit ID: 574ef6c987c636210828e96d2f797d8f10aff05e#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:37:48Z",
        "updatedAt" : "2020-03-14T11:37:48Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 213,
    "diffHunk" : "@@ -1,1 +1000,1004 @@      .internal()\n      .doc(\"The chunk size in bytes during writing out the bytes of ChunkedByteBuffer.\")\n      .version(\"2.3.0\")\n      .bytesConf(ByteUnit.BYTE)\n      .checkValue(_ <= ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH,"
  },
  {
    "id" : "53c3689f-a9e5-45a2-8e99-5bf16493703e",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716274",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9747384a-a8b2-457d-8396-88d3d3c1a514",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-19525, commit ID: 1405862382185e04b09f84af18f82f2f0295a755#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:38:17Z",
        "updatedAt" : "2020-03-14T11:38:17Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 221,
    "diffHunk" : "@@ -1,1 +1011,1015 @@      .doc(\"Whether to compress RDD checkpoints. Generally a good idea. Compression will use \" +\n        \"spark.io.compression.codec.\")\n      .version(\"2.2.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "0e95178e-8b72-46ab-9ba8-f092ea1da042",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716303",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "993c9079-c732-4ba9-bb81-913934d84f24",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-29182, commit ID: 4ecbdbb6a7bd3908da32c82832e886b4f9f9e596#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:38:46Z",
        "updatedAt" : "2020-03-14T11:38:46Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 229,
    "diffHunk" : "@@ -1,1 +1022,1026 @@        \"time. The drawback is that the cached locations can be possibly outdated and \" +\n        \"lose data locality. If this config is not specified, it will not cache.\")\n      .version(\"3.0.0\")\n      .timeConf(TimeUnit.MINUTES)\n      .checkValue(_ > 0, \"The expire time for caching preferred locations cannot be non-positive.\")"
  },
  {
    "id" : "af4f30c2-5cd7-4ff9-a3d0-10d4ddb3226a",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716323",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ca7887ab-04bb-427f-b198-5f032c4d78a9",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-20801, commit ID: 81f63c8923416014d5c6bc227dd3c4e2a62bac8e#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:39:13Z",
        "updatedAt" : "2020-03-14T11:39:14Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 237,
    "diffHunk" : "@@ -1,1 +1032,1036 @@        \"HighlyCompressedMapStatus is accurately recorded. This helps to prevent OOM \" +\n        \"by avoiding underestimating shuffle block size when fetch shuffle blocks.\")\n      .version(\"2.2.1\")\n      .bytesConf(ByteUnit.BYTE)\n      .createWithDefault(100 * 1024 * 1024)"
  },
  {
    "id" : "029713d3-b989-4114-9ec2-57c04d5eb30a",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716347",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8b793871-9039-45b9-a389-273be5ca4cf6",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-20640, commit ID: d107b3b910d8f434fb15b663a9db4c2dfe0a9f43#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:39:52Z",
        "updatedAt" : "2020-03-14T11:39:52Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 244,
    "diffHunk" : "@@ -1,1 +1039,1043 @@    ConfigBuilder(\"spark.shuffle.registration.timeout\")\n      .doc(\"Timeout in milliseconds for registration to the external shuffle service.\")\n      .version(\"2.3.0\")\n      .timeConf(TimeUnit.MILLISECONDS)\n      .createWithDefault(5000)"
  },
  {
    "id" : "86304ba6-4f0b-40c3-8df4-8dd1333267f2",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716363",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "002de867-39d1-48f3-a431-a85b598b7f30",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-20640, commit ID: d107b3b910d8f434fb15b663a9db4c2dfe0a9f43#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:40:10Z",
        "updatedAt" : "2020-03-14T11:40:10Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 252,
    "diffHunk" : "@@ -1,1 +1047,1051 @@      .doc(\"When we fail to register to the external shuffle service, we will \" +\n        \"retry for maxAttempts times.\")\n      .version(\"2.3.0\")\n      .intConf\n      .createWithDefault(3)"
  },
  {
    "id" : "95b71307-4eb3-4b0b-a40c-d39236ae7023",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716388",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2ae841e1-3766-41cc-91fe-a3496a088cfe",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-21243, commit ID: 88dccda393bc79dc6032f71b6acf8eb2b4b152be#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:40:38Z",
        "updatedAt" : "2020-03-14T11:40:38Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 260,
    "diffHunk" : "@@ -1,1 +1058,1062 @@        \"Node Manager. This is especially useful to reduce the load on the Node Manager when \" +\n        \"external shuffle is enabled. You can mitigate the issue by setting it to a lower value.\")\n      .version(\"2.2.1\")\n      .intConf\n      .checkValue(_ > 0, \"The max no. of blocks in flight cannot be non-positive.\")"
  },
  {
    "id" : "8c9d157a-5086-4135-b55f-c2e402c8582b",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716409",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fa88b17a-4516-431c-b0ad-60800a6ab93c",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-26700, commit ID: d8613571bc1847775dd5c1945757279234cb388c#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:41:05Z",
        "updatedAt" : "2020-03-14T11:41:05Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 268,
    "diffHunk" : "@@ -1,1 +1070,1074 @@        \"For users who enabled external shuffle service, this feature can only work when \" +\n        \"external shuffle service is at least 2.3.0.\")\n      .version(\"3.0.0\")\n      .bytesConf(ByteUnit.BYTE)\n      // fetch-to-mem is guaranteed to fail if the message is bigger than 2 GB, so we might"
  },
  {
    "id" : "82c5babd-7fef-4568-8c92-ed014cf756a0",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716433",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "55d25c43-b3ac-4d70-b80b-5aa6df197a14",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-20923, commit ID: 5b5a69bea9de806e2c39b04b248ee82a7b664d7b#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:41:29Z",
        "updatedAt" : "2020-03-14T11:41:29Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 276,
    "diffHunk" : "@@ -1,1 +1086,1090 @@        \"tracking the block statuses can use a lot of memory and its not used anywhere within \" +\n        \"spark.\")\n      .version(\"2.3.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "c319b2b6-6882-4c78-9494-98e333f70b10",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716451",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2680e8bd-6dc0-4394-ad8d-bcc318721597",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-28209, commit ID: abef84a868e9e15f346eea315bbab0ec8ac8e389#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:41:49Z",
        "updatedAt" : "2020-03-14T11:41:50Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 283,
    "diffHunk" : "@@ -1,1 +1093,1097 @@    ConfigBuilder(\"spark.shuffle.sort.io.plugin.class\")\n      .doc(\"Name of the class to use for shuffle IO.\")\n      .version(\"3.0.0\")\n      .stringConf\n      .createWithDefault(classOf[LocalDiskShuffleDataIO].getName)"
  },
  {
    "id" : "743d0691-73a8-4187-aba4-2f46fd5bf1c0",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716468",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8fdff1a0-833c-49a6-9a21-b03fca27cd59",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-7081, commit ID: c53ebea9db418099df50f9adc1a18cee7849cd97#diff-ecdafc46b901740134261d2cab24ccd9",
        "createdAt" : "2020-03-14T11:42:17Z",
        "updatedAt" : "2020-03-14T11:42:17Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 291,
    "diffHunk" : "@@ -1,1 +1102,1106 @@        \"otherwise specified. These buffers reduce the number of disk seeks and system calls \" +\n        \"made in creating intermediate shuffle files.\")\n      .version(\"1.4.0\")\n      .bytesConf(ByteUnit.KiB)\n      .checkValue(v => v > 0 && v <= ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH / 1024,"
  },
  {
    "id" : "cad009c8-867f-4cc9-8707-8dcca57d72fd",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716491",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "19cce0fc-14c5-4340-bb6e-1d2d1cf93ce8",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-20950, commit ID: 565e7a8d4ae7879ee704fb94ae9b3da31e202d7e#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:42:46Z",
        "updatedAt" : "2020-03-14T11:42:47Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 299,
    "diffHunk" : "@@ -1,1 +1113,1117 @@      .doc(\"The file system for this buffer size after each partition \" +\n        \"is written in unsafe shuffle writer. In KiB unless otherwise specified.\")\n      .version(\"2.3.0\")\n      .bytesConf(ByteUnit.KiB)\n      .checkValue(v => v > 0 && v <= ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH / 1024,"
  },
  {
    "id" : "49b679e3-5d21-4563-8320-707575354e3f",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716498",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "450b9bf1-a701-4580-9bf8-48c3f3adcc84",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-20950, commit ID: 565e7a8d4ae7879ee704fb94ae9b3da31e202d7e#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:42:55Z",
        "updatedAt" : "2020-03-14T11:42:55Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 307,
    "diffHunk" : "@@ -1,1 +1123,1127 @@    ConfigBuilder(\"spark.shuffle.spill.diskWriteBufferSize\")\n      .doc(\"The buffer size, in bytes, to use when writing the sorted records to an on-disk file.\")\n      .version(\"2.3.0\")\n      .bytesConf(ByteUnit.BYTE)\n      .checkValue(v => v > 12 && v <= ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH,"
  },
  {
    "id" : "8a4e081f-b750-46ba-8d19-d84125a1de2d",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716534",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4c438b78-b5ea-4de1-908b-346d796f8ab6",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-21923, commit ID: a11db942aaf4c470a85f8a1b180f034f7a584254#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:43:38Z",
        "updatedAt" : "2020-03-14T11:43:38Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 315,
    "diffHunk" : "@@ -1,1 +1135,1139 @@      .doc(\"The memory check period is used to determine how often we should check whether \"\n        + \"there is a need to request more memory when we try to unroll the given block in memory.\")\n      .version(\"2.3.0\")\n      .longConf\n      .createWithDefault(16)"
  },
  {
    "id" : "f0ceb6ab-82c3-4206-a07a-2bef11e5dcce",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716537",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8a69d21f-565e-45fc-8805-81ca73e74509",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-21923, commit ID: a11db942aaf4c470a85f8a1b180f034f7a584254#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:43:45Z",
        "updatedAt" : "2020-03-14T11:43:45Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 323,
    "diffHunk" : "@@ -1,1 +1143,1147 @@      .internal()\n      .doc(\"Memory to request as a multiple of the size that used to unroll the block.\")\n      .version(\"2.3.0\")\n      .doubleConf\n      .createWithDefault(1.5)"
  },
  {
    "id" : "2be0a405-4517-44e7-bc89-c6bb16997040",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716562",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2d46dca5-1bb0-44bd-86e1-13518633879b",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-21917, commit ID: 8319432af60b8e1dc00f08d794f7d80591e24d0c#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:44:10Z",
        "updatedAt" : "2020-03-14T11:44:11Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 331,
    "diffHunk" : "@@ -1,1 +1154,1158 @@        \"https and ftp, or jars required to be in the local YARN client's classpath. Wildcard \" +\n        \"'*' is denoted to download resources for all the schemes.\")\n      .version(\"2.3.0\")\n      .stringConf\n      .toSequence"
  },
  {
    "id" : "5f67b2d0-6ef5-4990-a9af-928be1167259",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716576",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3396c1bf-57f6-49ee-b8db-b0c1842e65b0",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-5411, commit ID: 47e4d579eb4a9aab8e0dd9c1400394d80c8d0388#diff-364713d7776956cb8b0a771e9b62f82d",
        "createdAt" : "2020-03-14T11:44:31Z",
        "updatedAt" : "2020-03-14T11:44:31Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 338,
    "diffHunk" : "@@ -1,1 +1161,1165 @@  private[spark] val EXTRA_LISTENERS = ConfigBuilder(\"spark.extraListeners\")\n    .doc(\"Class names of listeners to add to SparkContext during initialization.\")\n    .version(\"1.3.0\")\n    .stringConf\n    .toSequence"
  },
  {
    "id" : "21182768-c933-4de3-a19a-5397af301382",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716594",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c368da59-527b-44b9-aa1d-f04b858b6b63",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-10708, commit ID: f6d06adf05afa9c5386dc2396c94e7a98730289f#diff-3eedc75de4787b842477138d8cc7f150",
        "createdAt" : "2020-03-14T11:44:56Z",
        "updatedAt" : "2020-03-14T11:44:56Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 346,
    "diffHunk" : "@@ -1,1 +1173,1177 @@        \"until we reach some limitations, like the max page size limitation for the pointer \" +\n        \"array in the sorter.\")\n      .version(\"1.6.0\")\n      .intConf\n      .createWithDefault(Integer.MAX_VALUE)"
  },
  {
    "id" : "0df89255-d434-465d-9558-76196f1f80a3",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716625",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7a7f2d1f-08b3-47a8-81e6-7e88989086ea",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-22537, commit ID: efd0036ec88bdc385f5a9ea568d2e2bbfcda2912#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:45:28Z",
        "updatedAt" : "2020-03-14T11:45:29Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 354,
    "diffHunk" : "@@ -1,1 +1183,1187 @@        \"or equal to this threshold. Note that the actual parallelism is calculated by number of \" +\n        \"mappers * shuffle partitions / this threshold + 1, so this threshold should be positive.\")\n      .version(\"2.3.0\")\n      .intConf\n      .checkValue(v => v > 0, \"The threshold should be positive.\")"
  },
  {
    "id" : "6eb83ad9-a848-4d03-a238-bd1839a8da2f",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716649",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1ad96bc7-6685-494d-b925-3f3d9efb952f",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-3466, commit ID: 6181577e9935f46b646ba3925b873d031aa3d6ba#diff-d239aee594001f8391676e1047a0381e",
        "createdAt" : "2020-03-14T11:45:51Z",
        "updatedAt" : "2020-03-14T11:45:51Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 361,
    "diffHunk" : "@@ -1,1 +1190,1194 @@  private[spark] val MAX_RESULT_SIZE = ConfigBuilder(\"spark.driver.maxResultSize\")\n    .doc(\"Size limit for results.\")\n    .version(\"1.2.0\")\n    .bytesConf(ByteUnit.BYTE)\n    .createWithDefaultString(\"1g\")"
  },
  {
    "id" : "4e25f360-2945-44eb-a810-1a37c8964971",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716672",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a2fa81db-111c-4f9e-aaa8-d943c9383996",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-23361, commit ID: 5fa438471110afbf4e2174df449ac79e292501f8#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:46:16Z",
        "updatedAt" : "2020-03-14T11:46:16Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 368,
    "diffHunk" : "@@ -1,1 +1197,1201 @@    ConfigBuilder(\"spark.security.credentials.renewalRatio\")\n      .doc(\"Ratio of the credential's expiration time when Spark should fetch new credentials.\")\n      .version(\"2.4.0\")\n      .doubleConf\n      .createWithDefault(0.75d)"
  },
  {
    "id" : "61354868-1dba-4104-a86b-895683261a41",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716684",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0bd6af7a-b3a8-4105-acdb-0cf3e57991fc",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-23361, commit ID: 5fa438471110afbf4e2174df449ac79e292501f8#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:46:24Z",
        "updatedAt" : "2020-03-14T11:46:25Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 375,
    "diffHunk" : "@@ -1,1 +1204,1208 @@    ConfigBuilder(\"spark.security.credentials.retryWait\")\n      .doc(\"How long to wait before retrying to fetch new credentials after a failure.\")\n      .version(\"2.4.0\")\n      .timeConf(TimeUnit.SECONDS)\n      .createWithDefaultString(\"1h\")"
  },
  {
    "id" : "cbd95e7a-b61f-4c75-aeef-1756843889d3",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716707",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "11534a9c-4dca-4d31-b42d-bdf76781a476",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-15958, commit ID: bf665a958631125a1670504ef5966ef1a0e14798#diff-a1d00506391c1c4b2209f9bbff590c5b",
        "createdAt" : "2020-03-14T11:46:48Z",
        "updatedAt" : "2020-03-14T11:46:48Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 382,
    "diffHunk" : "@@ -1,1 +1211,1215 @@    ConfigBuilder(\"spark.shuffle.sort.initialBufferSize\")\n      .internal()\n      .version(\"2.1.0\")\n      .bytesConf(ByteUnit.BYTE)\n      .checkValue(v => v > 0 && v <= Int.MaxValue,"
  },
  {
    "id" : "b700fbe0-f8b7-4290-a14d-35d2b5642fdc",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716726",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3bce4f05-f4ad-4034-a775-ae22088d0906",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "No JIRA ID, commit ID: efc5423210d1aadeaea78273a4a8f10425753079#diff-76170a9c8f67b542bc58240a0a12fe08",
        "createdAt" : "2020-03-14T11:47:07Z",
        "updatedAt" : "2020-03-14T11:47:08Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 390,
    "diffHunk" : "@@ -1,1 +1221,1225 @@      .doc(\"Whether to compress shuffle output. Compression will use \" +\n        \"spark.io.compression.codec.\")\n      .version(\"0.6.0\")\n      .booleanConf\n      .createWithDefault(true)"
  },
  {
    "id" : "4bf580bd-dfe1-44a2-a5de-b4d9df48e3de",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716736",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d6de9488-b7fd-4c4c-8df4-1774b8546444",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "No JIRA ID, commit ID: c3816de5040e3c48e58ed4762d2f4eb606812938#diff-2b643ea78c1add0381754b1f47eec132",
        "createdAt" : "2020-03-14T11:47:26Z",
        "updatedAt" : "2020-03-14T11:47:27Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 398,
    "diffHunk" : "@@ -1,1 +1229,1233 @@      .doc(\"Whether to compress data spilled during shuffles. Compression will use \" +\n        \"spark.io.compression.codec.\")\n      .version(\"0.9.0\")\n      .booleanConf\n      .createWithDefault(true)"
  },
  {
    "id" : "527105b2-7b61-4300-bd44-e7ac370af0b4",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716763",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "53c65062-3f91-4788-ad65-2566c9dff0e1",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-29939, commit ID: 456cfe6e4693efd26d64f089d53c4e01bf8150a2#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:47:55Z",
        "updatedAt" : "2020-03-14T11:47:55Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 406,
    "diffHunk" : "@@ -1,1 +1239,1243 @@        \"By default, Spark provides four codecs: lz4, lzf, snappy, and zstd. You can also \" +\n        \"use fully qualified class names to specify the codec.\")\n      .version(\"3.0.0\")\n      .stringConf\n      .createWithDefault(\"zstd\")"
  },
  {
    "id" : "b3e71da3-0627-4a7e-b3ab-600e3eca3058",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716788",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b58ac048-5846-447a-95ee-38bd6d70df3b",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-4480, commit ID: 16bf5f3d17624db2a96c921fe8a1e153cdafb06c#diff-31417c461d8901d8e08167b0cbc344c1",
        "createdAt" : "2020-03-14T11:48:22Z",
        "updatedAt" : "2020-03-14T11:48:22Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 414,
    "diffHunk" : "@@ -1,1 +1248,1252 @@      .doc(\"Initial threshold for the size of a collection before we start tracking its \" +\n        \"memory usage.\")\n      .version(\"1.1.1\")\n      .bytesConf(ByteUnit.BYTE)\n      .createWithDefault(5 * 1024 * 1024)"
  },
  {
    "id" : "0e8ce018-452a-408b-abdd-39590e18e5bd",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716806",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "03135690-ca7e-4804-8a4c-c1752438718e",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "No JIRA ID, commit ID: c3816de5040e3c48e58ed4762d2f4eb606812938#diff-a470b9812a5ac8c37d732da7d9fbe39a",
        "createdAt" : "2020-03-14T11:48:46Z",
        "updatedAt" : "2020-03-14T11:48:46Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 422,
    "diffHunk" : "@@ -1,1 +1256,1260 @@      .internal()\n      .doc(\"Size of object batches when reading/writing from serializers.\")\n      .version(\"0.9.0\")\n      .longConf\n      .createWithDefault(10000)"
  },
  {
    "id" : "b485cd62-28f7-4ffb-bffa-da53d0d1d1de",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716828",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7dc98ccf-822d-4c58-a691-52b13ebc3521",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-2787, commit ID: 0f2274f8ed6131ad17326e3fff7f7e093863b72d#diff-31417c461d8901d8e08167b0cbc344c1",
        "createdAt" : "2020-03-14T11:49:12Z",
        "updatedAt" : "2020-03-14T11:49:12Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 430,
    "diffHunk" : "@@ -1,1 +1264,1268 @@      .doc(\"In the sort-based shuffle manager, avoid merge-sorting data if there is no \" +\n        \"map-side aggregation and there are at most this many reduce partitions\")\n      .version(\"1.1.1\")\n      .intConf\n      .createWithDefault(200)"
  },
  {
    "id" : "af7f065a-33d9-45de-9294-b583966d0338",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716853",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cca9bfcf-e12e-440c-8832-c58b8813f919",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-2044, commit ID: 508fd371d6dbb826fd8a00787d347235b549e189#diff-60df49b5d3c59f2c4540fa16a90033a1",
        "createdAt" : "2020-03-14T11:49:35Z",
        "updatedAt" : "2020-03-14T11:49:35Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 436,
    "diffHunk" : "@@ -1,1 +1270,1274 @@  private[spark] val SHUFFLE_MANAGER =\n    ConfigBuilder(\"spark.shuffle.manager\")\n      .version(\"1.1.0\")\n      .stringConf\n      .createWithDefault(\"sort\")"
  },
  {
    "id" : "af0e3e28-08ea-4847-a287-53be965d5514",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716873",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "93292844-c2c1-4658-bbe1-9c77e0d9d11d",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-2774, commit ID: 96a7c888d806adfdb2c722025a1079ed7eaa2052#diff-6a9ff7fb74fd490a50462d45db2d5e11",
        "createdAt" : "2020-03-14T11:49:56Z",
        "updatedAt" : "2020-03-14T11:49:56Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 443,
    "diffHunk" : "@@ -1,1 +1277,1281 @@    ConfigBuilder(\"spark.shuffle.reduceLocality.enabled\")\n      .doc(\"Whether to compute locality preferences for reduce tasks\")\n      .version(\"1.5.0\")\n      .booleanConf\n      .createWithDefault(true)"
  },
  {
    "id" : "ea9615ea-081a-4edc-a147-df276d34bcd8",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716904",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "994545fc-f355-4a07-b456-9eee4ad15805",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-1239, commit ID: d98dd72e7baeb59eacec4fefd66397513a607b2f#diff-609c3f8c26150ca96a94cd27146a809b",
        "createdAt" : "2020-03-14T11:50:27Z",
        "updatedAt" : "2020-03-14T11:50:27Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 450,
    "diffHunk" : "@@ -1,1 +1284,1288 @@    ConfigBuilder(\"spark.shuffle.mapOutput.minSizeForBroadcast\")\n      .doc(\"The size at which we use Broadcast to send the map output statuses to the executors.\")\n      .version(\"2.0.0\")\n      .bytesConf(ByteUnit.BYTE)\n      .createWithDefaultString(\"512k\")"
  },
  {
    "id" : "90dd58c1-58ce-4752-b7f9-5150d3e86337",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716913",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0f87ff7f-4436-45e3-bc2e-0fa5396f55ae",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-1239, commit ID: d98dd72e7baeb59eacec4fefd66397513a607b2f#diff-609c3f8c26150ca96a94cd27146a809b",
        "createdAt" : "2020-03-14T11:50:37Z",
        "updatedAt" : "2020-03-14T11:50:37Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 456,
    "diffHunk" : "@@ -1,1 +1290,1294 @@  private[spark] val SHUFFLE_MAPOUTPUT_DISPATCHER_NUM_THREADS =\n    ConfigBuilder(\"spark.shuffle.mapOutput.dispatcher.numThreads\")\n      .version(\"2.0.0\")\n      .intConf\n      .createWithDefault(8)"
  },
  {
    "id" : "b783c74a-5dc1-4863-b2b8-66db19d11c03",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716928",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d35fccc0-8c97-400e-afcc-6e503672a89e",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-4105, commit ID: cf33a86285629abe72c1acf235b8bfa6057220a8#diff-eb30a71e0d04150b8e0b64929852e38b",
        "createdAt" : "2020-03-14T11:50:58Z",
        "updatedAt" : "2020-03-14T11:50:58Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 463,
    "diffHunk" : "@@ -1,1 +1297,1301 @@    ConfigBuilder(\"spark.shuffle.detectCorrupt\")\n      .doc(\"Whether to detect any corruption in fetched blocks.\")\n      .version(\"2.2.0\")\n      .booleanConf\n      .createWithDefault(true)"
  },
  {
    "id" : "22b99ac5-52b2-4891-a712-892814f7efa9",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716944",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8e6f2e90-e41a-4607-9273-28b6188c9951",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-26089, commit ID: 688b0c01fac0db80f6473181673a89f1ce1be65b#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:51:20Z",
        "updatedAt" : "2020-03-14T11:51:20Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 471,
    "diffHunk" : "@@ -1,1 +1307,1311 @@        \"the task to be retried once and if it fails again with same exception, then \" +\n        \"FetchFailedException will be thrown to retry previous stage\")\n      .version(\"3.0.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "2f30c732-d91b-401c-8150-4a1e248133b3",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716963",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7b5cdf45-626a-440e-be99-29e551095310",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "No JIRA ID, commit ID: 31da065b1d08c1fad5283e4bcf8e0ed01818c03e#diff-ad46ed23fcc3fa87f30d05204917b917",
        "createdAt" : "2020-03-14T11:51:42Z",
        "updatedAt" : "2020-03-14T11:51:42Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 478,
    "diffHunk" : "@@ -1,1 +1314,1318 @@    ConfigBuilder(\"spark.shuffle.sync\")\n      .doc(\"Whether to force outstanding writes to disk.\")\n      .version(\"0.8.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "f5efe1d0-7089-4e06-afa9-2098e70f45cb",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374716984",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9f65b6a7-43e5-484c-969e-cfea6cb9aa55",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-7081, commit ID: c53ebea9db418099df50f9adc1a18cee7849cd97#diff-642ce9f439435408382c3ac3b5c5e0a0",
        "createdAt" : "2020-03-14T11:52:01Z",
        "updatedAt" : "2020-03-14T11:52:01Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 485,
    "diffHunk" : "@@ -1,1 +1321,1325 @@    ConfigBuilder(\"spark.shuffle.unsafe.fastMergeEnabled\")\n      .doc(\"Whether to perform a fast spill merge.\")\n      .version(\"1.4.0\")\n      .booleanConf\n      .createWithDefault(true)"
  },
  {
    "id" : "674cc23d-79ed-4f5e-a85e-99d5f7bb7f14",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374717009",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2afd3fee-339c-4b8c-afe7-ada74d35860e",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-14724, commit ID: e2b5647ab92eb478b3f7b36a0ce6faf83e24c0e5#diff-3eedc75de4787b842477138d8cc7f150",
        "createdAt" : "2020-03-14T11:52:23Z",
        "updatedAt" : "2020-03-14T11:52:24Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 493,
    "diffHunk" : "@@ -1,1 +1329,1333 @@      .doc(\"Whether to use radix sort for sorting in-memory partition ids. Radix sort is much \" +\n        \"faster, but requires additional memory to be reserved memory as pointers are added.\")\n      .version(\"2.0.0\")\n      .booleanConf\n      .createWithDefault(true)"
  },
  {
    "id" : "43c15a85-0cad-4921-b6e5-135661389ec8",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374717034",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0d5ef0b6-4230-4ea9-beb7-329e6738bd30",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-24519, commit ID: 39dfaf2fd167cafc84ec9cc637c114ed54a331e3#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:52:49Z",
        "updatedAt" : "2020-03-14T11:52:49Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 501,
    "diffHunk" : "@@ -1,1 +1337,1341 @@      .internal()\n      .doc(\"Number of partitions to determine if MapStatus should use HighlyCompressedMapStatus\")\n      .version(\"2.4.0\")\n      .intConf\n      .checkValue(v => v > 0, \"The value should be a positive integer.\")"
  },
  {
    "id" : "1f9c7918-c7e9-4195-8918-93a484ad4926",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374717050",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2103c5f6-22a0-4795-8565-096e9a25f5ce",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-25341, commit ID: f725d472f51fb80c6ce1882ec283ff69bafb0de4#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:53:15Z",
        "updatedAt" : "2020-03-14T11:53:15Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 509,
    "diffHunk" : "@@ -1,1 +1347,1351 @@        \"It is only enabled while we need the compatibility in the scenario of new Spark \" +\n        \"version job fetching shuffle blocks from old version external shuffle service.\")\n      .version(\"3.0.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "b66fc63e-8331-4124-a783-5ac75bea3a49",
    "prId" : 27913,
    "prUrl" : "https://github.com/apache/spark/pull/27913#pullrequestreview-374717073",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "33d4a776-e3d9-45ac-abdf-aa40b5879d11",
        "parentId" : null,
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "SPARK-30812, commit ID: 68d7edf9497bea2f73707d32ab55dd8e53088e7c#diff-6bdad48cfc34314e89599655442ff210",
        "createdAt" : "2020-03-14T11:53:42Z",
        "updatedAt" : "2020-03-14T11:53:43Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad58e0cc8934f19eefc441f7977b840dfe850c0d",
    "line" : 517,
    "diffHunk" : "@@ -1,1 +1357,1361 @@        \"blocks requested from those block managers which are running on the same host are read \" +\n        \"from the disk directly instead of being fetched as remote blocks over the network.\")\n      .version(\"3.0.0\")\n      .booleanConf\n      .createWithDefault(true)"
  }
]