[
  {
    "id" : "b6b791a9-9e9d-4bea-b6f6-af685ee12d62",
    "prId" : 33615,
    "prUrl" : "https://github.com/apache/spark/pull/33615#pullrequestreview-730204271",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f7ffe8f4-88e6-4452-9b66-99ad36e9ce8f",
        "parentId" : null,
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "It seems that we have exposed this in 3.1.0(3.1.1) already: https://github.com/apache/spark/blob/branch-3.1/core/src/main/scala/org/apache/spark/internal/config/package.scala\r\n\r\nWhy do we change the version here? The feature is not ready in 3.1?",
        "createdAt" : "2021-08-15T09:47:39Z",
        "updatedAt" : "2021-08-15T09:48:08Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "88507602-f0cf-4353-be51-e89c3ef92e46",
        "parentId" : "f7ffe8f4-88e6-4452-9b66-99ad36e9ce8f",
        "authorId" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "body" : "Yes, the feature was not ready in 3.1. It will be ready only in 3.2 which is why I changed it to 3.2 to keep it consistent.",
        "createdAt" : "2021-08-15T16:51:14Z",
        "updatedAt" : "2021-08-15T16:51:14Z",
        "lastEditedBy" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "tags" : [
        ]
      }
    ],
    "commit" : "12618284d95d7b1685c2c48c79bffcb618cf4523",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +2103,2107 @@        \"org.apache.spark.network.shuffle.MergedShuffleFileManager implementation for push-based \" +\n        \"shuffle to be enabled\")\n      .version(\"3.2.0\")\n      .booleanConf\n      .createWithDefault(false)"
  },
  {
    "id" : "d220dc8b-fb03-4fdb-b003-8403f3829793",
    "prId" : 33078,
    "prUrl" : "https://github.com/apache/spark/pull/33078#pullrequestreview-699399725",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "54282deb-c470-4784-8b56-145ce80bda31",
        "parentId" : null,
        "authorId" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "body" : "nit: This feels like it should be `spark.app.attemptNumber` than `spark.app.attempt.id`. Also currently this is specific to push based shuffle right? Should we add that in the documentation?",
        "createdAt" : "2021-07-05T02:00:10Z",
        "updatedAt" : "2021-07-05T02:10:19Z",
        "lastEditedBy" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "tags" : [
        ]
      },
      {
        "id" : "687245d6-c8e9-4429-adc9-0955dd150e7a",
        "parentId" : "54282deb-c470-4784-8b56-145ce80bda31",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "`attemptNumber` sounds better to me.\r\n\r\nIf this conf can't be set by users, I think it's ok to keep it internal.",
        "createdAt" : "2021-07-05T05:37:23Z",
        "updatedAt" : "2021-07-05T05:37:23Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "8a1f417b-4ce6-4f70-945d-b37805cad71a",
        "parentId" : "54282deb-c470-4784-8b56-145ce80bda31",
        "authorId" : "e942fa3e-a5b9-4a9f-b9a5-a1afd3c29b2c",
        "body" : "AppAttemptId is currently used in lots of places to indicate the attempt id from Yarn, should we keep it as \"spark.app.attempt.id\" to align with all other places? Or we just want to change the configuration string to \"spark.app.attemptNumber\"? ",
        "createdAt" : "2021-07-05T05:54:51Z",
        "updatedAt" : "2021-07-05T05:54:51Z",
        "lastEditedBy" : "e942fa3e-a5b9-4a9f-b9a5-a1afd3c29b2c",
        "tags" : [
        ]
      },
      {
        "id" : "570171b6-51eb-481a-bb6e-3d364925f97e",
        "parentId" : "54282deb-c470-4784-8b56-145ce80bda31",
        "authorId" : "e942fa3e-a5b9-4a9f-b9a5-a1afd3c29b2c",
        "body" : "And which version should we mark it there? I added \"1.3.0\" since the support for multiple attempts in Yarn was added since 1.3.0. But if we change to \"spark.app.attemptNumber\", we should start from \"3.2.0\". Thoughts?",
        "createdAt" : "2021-07-05T05:56:32Z",
        "updatedAt" : "2021-07-05T05:56:32Z",
        "lastEditedBy" : "e942fa3e-a5b9-4a9f-b9a5-a1afd3c29b2c",
        "tags" : [
        ]
      },
      {
        "id" : "be958a99-e0af-4a11-a965-11d09502cdbf",
        "parentId" : "54282deb-c470-4784-8b56-145ce80bda31",
        "authorId" : "e942fa3e-a5b9-4a9f-b9a5-a1afd3c29b2c",
        "body" : "Should we use \"String\" or \"Integer\" here for this configuration? In [SparkContext where it gets the Application AttemptID](https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/SparkContext.scala#L340), it is a String format. But when parsing into Configuration and also resolving this in RemoteBlockPushResolver, we are using Integer to determine whether it is a new attempt.",
        "createdAt" : "2021-07-05T06:13:17Z",
        "updatedAt" : "2021-07-05T06:13:17Z",
        "lastEditedBy" : "e942fa3e-a5b9-4a9f-b9a5-a1afd3c29b2c",
        "tags" : [
        ]
      },
      {
        "id" : "b905ccf9-7e33-4b4f-8b47-306c4a2b0d63",
        "parentId" : "54282deb-c470-4784-8b56-145ce80bda31",
        "authorId" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "body" : "I see it makes sense. IMO, since it is already called `attemptId` it is fine to keep it probably like that. May be just adding slightly more info on the doc should be fine with me. Initially, I was under the impression we are generating this attemptId which is not the case.",
        "createdAt" : "2021-07-05T20:05:00Z",
        "updatedAt" : "2021-07-05T20:05:00Z",
        "lastEditedBy" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "tags" : [
        ]
      }
    ],
    "commit" : "53109918cbdbdba2fe79f38a991c171efec7e85f",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +2247,2251 @@\n  private[spark] val APP_ATTEMPT_ID =\n    ConfigBuilder(\"spark.app.attempt.id\")\n      .internal()\n      .doc(\"The application attempt Id assigned from Hadoop YARN. \" +"
  },
  {
    "id" : "a07b11d7-89d4-495d-832c-302b29348636",
    "prId" : 32287,
    "prUrl" : "https://github.com/apache/spark/pull/32287#pullrequestreview-655481906",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "09e9b13b-314f-402a-ab20-4f35b75b425b",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Just a question. Is there a reason of `10` instead of `3`?",
        "createdAt" : "2021-05-08T22:29:08Z",
        "updatedAt" : "2021-05-10T10:25:15Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "63892ca8-c862-42d6-8c6d-2f0e9c489469",
        "parentId" : "09e9b13b-314f-402a-ab20-4f35b75b425b",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Given the discussion (https://github.com/apache/spark/pull/32287#discussion_r625287419) there, Netty OOM could be raised more frequently in certain cases, e.g.,\r\n\r\n> For case b), the OOM threshold might be 20 requests. In this case, there're still 80 deferred requests, which would hit the OOM soon as you mentioned. That being said, I think the current fix would work around the issue in the end. Note that the application would fail before the fix.\r\n\r\nThus, I'd like to give more chances for the block in case we fall into the case like b).",
        "createdAt" : "2021-05-10T10:18:54Z",
        "updatedAt" : "2021-05-10T10:25:15Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "4af6ee73221a55e3cd7239f151ec33d5e802cbe4",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +1201,1205 @@      .internal()\n      .intConf\n      .createWithDefault(10)\n\n  private[spark] val REDUCER_MAX_BLOCKS_IN_FLIGHT_PER_ADDRESS ="
  }
]