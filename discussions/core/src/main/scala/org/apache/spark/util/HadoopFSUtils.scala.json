[
  {
    "id" : "ff445a7a-190e-4ef7-b56b-5c398569c401",
    "prId" : 30019,
    "prUrl" : "https://github.com/apache/spark/pull/30019#pullrequestreview-508148639",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "806b5afa-4e80-4ecd-b09f-ae414826dbe7",
        "parentId" : null,
        "authorId" : "224991ce-ad69-410b-8143-bf394b6b5c59",
        "body" : "Might be good to pull this out into something reusable for any `RemoteIterator[T]` as it gets used in a number of API calls (all because of java's checked exceptions...)",
        "createdAt" : "2020-10-14T09:13:34Z",
        "updatedAt" : "2020-10-20T21:51:16Z",
        "lastEditedBy" : "224991ce-ad69-410b-8143-bf394b6b5c59",
        "tags" : [
        ]
      }
    ],
    "commit" : "3c0ad259bea09095da30863af9e614e23cfff0d6",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +212,216 @@      } else {\n        val remoteIter = fs.listLocatedStatus(path)\n        new Iterator[LocatedFileStatus]() {\n          def next(): LocatedFileStatus = remoteIter.next\n          def hasNext(): Boolean = remoteIter.hasNext"
  },
  {
    "id" : "5c62f5fa-eba6-441a-af93-5345fd95d970",
    "prId" : 30019,
    "prUrl" : "https://github.com/apache/spark/pull/30019#pullrequestreview-512996356",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ca6e7bd9-51f1-460c-8c7f-a78727ce24a1",
        "parentId" : null,
        "authorId" : "224991ce-ad69-410b-8143-bf394b6b5c59",
        "body" : "switch to listStatusIterator(path) and again, provide a remoteIterator. This will give you on paged downloads on hdfs, webhdfs, async page prefetch on latest S3A builds, and, at worst elsewhere, exactly the same performance a listStatus",
        "createdAt" : "2020-10-14T09:15:24Z",
        "updatedAt" : "2020-10-20T21:51:16Z",
        "lastEditedBy" : "224991ce-ad69-410b-8143-bf394b6b5c59",
        "tags" : [
        ]
      },
      {
        "id" : "c0454043-dfa8-4b0a-8b88-2e68cb9545d5",
        "parentId" : "ca6e7bd9-51f1-460c-8c7f-a78727ce24a1",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "sg - I'll switch to `listStatusIterator` and create a wrapper class for the returned `RemoteIterator` in both cases.",
        "createdAt" : "2020-10-20T18:00:40Z",
        "updatedAt" : "2020-10-20T21:51:16Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      }
    ],
    "commit" : "3c0ad259bea09095da30863af9e614e23cfff0d6",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +209,213 @@    val statuses: Array[FileStatus] = try {\n      if (ignoreLocality) {\n        fs.listStatus(path)\n      } else {\n        val remoteIter = fs.listLocatedStatus(path)"
  },
  {
    "id" : "4bb37389-669f-4389-831e-58cd9a5f460b",
    "prId" : 30019,
    "prUrl" : "https://github.com/apache/spark/pull/30019#pullrequestreview-512995402",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "042e21f8-4c5e-45ad-a840-fde2c5913beb",
        "parentId" : null,
        "authorId" : "224991ce-ad69-410b-8143-bf394b6b5c59",
        "body" : "the longer you can incrementally do per entry in the remote iterator, the more latencies talking to the object stores can be hidden. See HADOOP-17074 and HADOOP-17023 for details; one of the PRs shows some numbers there. \r\n\r\nIf the spark API could return an iterator/yield and the processing of it used that, a lot of that listing cost could be absorbed entirely.",
        "createdAt" : "2020-10-14T09:19:18Z",
        "updatedAt" : "2020-10-20T21:51:16Z",
        "lastEditedBy" : "224991ce-ad69-410b-8143-bf394b6b5c59",
        "tags" : [
        ]
      },
      {
        "id" : "69db667e-6f88-44c4-9bc7-993a0f255061",
        "parentId" : "042e21f8-4c5e-45ad-a840-fde2c5913beb",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Yes it would be lovely if we can get async listing here, but I think it requires a much bigger surgery - up to the top level currently Spark's RDD model requires all the input partitions to be ready before it can start processing (deeply embedded in its primitives such as map/reduce).\r\n\r\nWe can perhaps add the async logic here in this class but I think \"local\" processing we're doing here is far cheaper than the remote listing and perhaps can't gain much from the change.\r\n\r\nWe can wrap the iterator and make it looks like a lazy array until certain info is needed but again I think it won't go very far until we make extensive changes in upper stack like in `PartitioningAwareFileIndex` or `DataSourceScanExec`. Anyways I'll perhaps try this in a separate PR.",
        "createdAt" : "2020-10-20T17:59:32Z",
        "updatedAt" : "2020-10-20T21:51:16Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      }
    ],
    "commit" : "3c0ad259bea09095da30863af9e614e23cfff0d6",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +215,219 @@          def next(): LocatedFileStatus = remoteIter.next\n          def hasNext(): Boolean = remoteIter.hasNext\n        }.toArray\n      }\n    } catch {"
  },
  {
    "id" : "34b4fa85-b473-4195-8ae0-b6ca15a7740c",
    "prId" : 29959,
    "prUrl" : "https://github.com/apache/spark/pull/29959#pullrequestreview-567415290",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dab04dde-8e14-49f0-aec6-f90934458d3b",
        "parentId" : null,
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "@sunchao the `dirs` here may contain hidden directories. We still need to filter them before listing leaf files.",
        "createdAt" : "2021-01-13T10:54:34Z",
        "updatedAt" : "2021-01-13T10:54:34Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "d1e77cf4-dcbd-464e-b945-b8a10285b381",
        "parentId" : "dab04dde-8e14-49f0-aec6-f90934458d3b",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "@gengliangwang you're right. Thanks for catching this! and sorry for introducing this regression.",
        "createdAt" : "2021-01-13T16:42:16Z",
        "updatedAt" : "2021-01-13T16:42:16Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      }
    ],
    "commit" : "e9d399de621a9cbfc32438b3460f78bef0e73de9",
    "line" : 144,
    "diffHunk" : "@@ -1,1 +210,214 @@\n    val allLeafStatuses = {\n      val (dirs, topLevelFiles) = statuses.partition(_.isDirectory)\n      val nestedFiles: Seq[FileStatus] = contextOpt match {\n        case Some(context) if dirs.size > parallelismThreshold =>"
  },
  {
    "id" : "7f5012a0-7ce6-4ce5-91ed-ea060fff54f8",
    "prId" : 29471,
    "prUrl" : "https://github.com/apache/spark/pull/29471#pullrequestreview-472053664",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "de6bf39a-1a95-47a3-a2e0-ff3d3db6fe14",
        "parentId" : null,
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "I remember in the other PR it was mentioned this is now serializable on its own, is that true with the versions of Hadoop we use/can we avoid using this wrapper?",
        "createdAt" : "2020-08-20T21:09:00Z",
        "updatedAt" : "2020-09-14T18:54:43Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "4c9d1fbf-0811-44db-bc0b-ce1ff52af5ca",
        "parentId" : "de6bf39a-1a95-47a3-a2e0-ff3d3db6fe14",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Ah - let me quickly check on this",
        "createdAt" : "2020-08-20T23:27:11Z",
        "updatedAt" : "2020-09-14T18:54:43Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "a0853954-cdb8-44ec-bed2-9a8a7e0bff8a",
        "parentId" : "de6bf39a-1a95-47a3-a2e0-ff3d3db6fe14",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "So no - it still doesn't inherit from `Serializable`. No change has happened to the signature since 2009.",
        "createdAt" : "2020-08-20T23:36:54Z",
        "updatedAt" : "2020-09-14T18:54:43Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "46aee59c-f5aa-4f03-99ca-f23c1b153762",
        "parentId" : "de6bf39a-1a95-47a3-a2e0-ff3d3db6fe14",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Thanks for checking :)",
        "createdAt" : "2020-08-20T23:38:46Z",
        "updatedAt" : "2020-09-14T18:54:43Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "2d8e64d72a014ed5067be5772f8b2db9129ce2f2",
    "line" : 98,
    "diffHunk" : "@@ -1,1 +96,100 @@    HiveCatalogMetrics.incrementParallelListingJobCount(1)\n\n    val serializableConfiguration = new SerializableConfiguration(hadoopConf)\n    val serializedPaths = paths.map(_.toString)\n"
  },
  {
    "id" : "d89e865c-e116-4226-9752-1d5c028699b9",
    "prId" : 29471,
    "prUrl" : "https://github.com/apache/spark/pull/29471#pullrequestreview-488013881",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "abecf215-3a35-4640-b362-adad9e47341b",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Put `HiveCatalogMetrics` here looks strange. This is a \"Metrics for access to the hive external catalog\". Should we skip this or create another metrics?",
        "createdAt" : "2020-08-21T03:54:02Z",
        "updatedAt" : "2020-09-14T18:54:43Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "7b3d9859-174b-48a6-8df3-ec718e04beb5",
        "parentId" : "abecf215-3a35-4640-b362-adad9e47341b",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Good catch. It seems inappropriate here. Not sure what is the best way to plugin Hive/SQL metrics here but I'll think over it later.",
        "createdAt" : "2020-08-21T05:17:36Z",
        "updatedAt" : "2020-09-14T18:54:43Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "8b9125f2-10fa-4674-994c-e30719a02ed1",
        "parentId" : "abecf215-3a35-4640-b362-adad9e47341b",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "We could pass in a callback for listing metrics?",
        "createdAt" : "2020-08-24T21:39:00Z",
        "updatedAt" : "2020-09-14T18:54:43Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "420d9107-79f4-4276-8807-7a89b310a286",
        "parentId" : "abecf215-3a35-4640-b362-adad9e47341b",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Or call this `HiveCatalogMetrics.incrementParallelListingJobCount(1)` in SQL side before calling `parallelListLeafFiles`?",
        "createdAt" : "2020-08-30T07:22:43Z",
        "updatedAt" : "2020-09-14T18:54:43Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "760d345d-e748-45b1-9c10-d3d8388e3adf",
        "parentId" : "abecf215-3a35-4640-b362-adad9e47341b",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "So the problem with that is that if we have a root directory with many sub-directories in it, we may initially choose to do non-parallel listing and then as the sub directories build up switch to parallel listing.",
        "createdAt" : "2020-08-31T17:18:59Z",
        "updatedAt" : "2020-09-14T18:54:43Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "0d85cf41-4f8f-4c85-9009-3cff50710613",
        "parentId" : "abecf215-3a35-4640-b362-adad9e47341b",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Yeah move this to SQL will not work - I think we can perhaps add a callback later just like @holdenk suggested (if you don't mind leaving it here in this PR). \r\n\r\nOn the other hand, I think the `HiveCatalogMetrics` is already misleading - `InMemoryFileIndex` is used by non-Hive data sources like file-based ones and this particular metric itself has nothing to do with Hive catalog. Perhaps a better approach is to move the metrics to some place in core that are for storage-specific things. In future we can include more such as listing total time etc.",
        "createdAt" : "2020-08-31T17:52:27Z",
        "updatedAt" : "2020-09-14T18:54:43Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "46233260-95cd-4dfb-8e5d-16a3869d255f",
        "parentId" : "abecf215-3a35-4640-b362-adad9e47341b",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "A callback sounds good. We can do it later.",
        "createdAt" : "2020-08-31T17:54:07Z",
        "updatedAt" : "2020-09-14T18:54:43Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "66ead07a-c9dd-4ad2-87a3-0eec9be5fa72",
        "parentId" : "abecf215-3a35-4640-b362-adad9e47341b",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Sounds good to me, @sunchao want to file a JIRA for switching this to a callback?",
        "createdAt" : "2020-09-10T20:51:56Z",
        "updatedAt" : "2020-09-14T18:54:43Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "c792178c-c5da-433b-b1aa-067fb2933594",
        "parentId" : "abecf215-3a35-4640-b362-adad9e47341b",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Sure filed https://issues.apache.org/jira/browse/SPARK-32880 for the follow-ups.",
        "createdAt" : "2020-09-14T17:57:21Z",
        "updatedAt" : "2020-09-14T18:54:43Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      }
    ],
    "commit" : "2d8e64d72a014ed5067be5772f8b2db9129ce2f2",
    "line" : 96,
    "diffHunk" : "@@ -1,1 +94,98 @@    logInfo(s\"Listing leaf files and directories in parallel under ${paths.length} paths.\" +\n      s\" The first several paths are: ${paths.take(10).mkString(\", \")}.\")\n    HiveCatalogMetrics.incrementParallelListingJobCount(1)\n\n    val serializableConfiguration = new SerializableConfiguration(hadoopConf)"
  },
  {
    "id" : "55ceac38-81ef-41b8-ba4b-9570b73a8671",
    "prId" : 29471,
    "prUrl" : "https://github.com/apache/spark/pull/29471#pullrequestreview-472133075",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9657b86e-6a7a-403e-8101-fb6a2a68ef7a",
        "parentId" : null,
        "authorId" : "2d6b46ba-4100-4c4d-9341-fff5e39647ec",
        "body" : "nit: missing parameter docs. It's pretty important for an API that has 9 parameters. ",
        "createdAt" : "2020-08-21T04:13:19Z",
        "updatedAt" : "2020-09-14T18:54:43Z",
        "lastEditedBy" : "2d6b46ba-4100-4c4d-9341-fff5e39647ec",
        "tags" : [
        ]
      }
    ],
    "commit" : "2d8e64d72a014ed5067be5772f8b2db9129ce2f2",
    "line" : 42,
    "diffHunk" : "@@ -1,1 +40,44 @@   * on the number of paths to list.\n   *\n   * This may only be called on the driver.\n   *\n   * @param sc Spark context used to run parallel listing."
  },
  {
    "id" : "df2c9449-db0b-44af-88cc-845ee1f38e9f",
    "prId" : 29471,
    "prUrl" : "https://github.com/apache/spark/pull/29471#pullrequestreview-482775400",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "102a564f-dcc9-4aab-8a5e-78f2d545b836",
        "parentId" : null,
        "authorId" : "224991ce-ad69-410b-8143-bf394b6b5c59",
        "body" : "recent hadoop builds declare path Serializable,. FileStatus should be too, but there's always the risk some subclass isn't",
        "createdAt" : "2020-08-22T17:35:55Z",
        "updatedAt" : "2020-09-14T18:54:43Z",
        "lastEditedBy" : "224991ce-ad69-410b-8143-bf394b6b5c59",
        "tags" : [
        ]
      },
      {
        "id" : "3d0a8df8-d69a-40c3-a978-2cbb51a323d5",
        "parentId" : "102a564f-dcc9-4aab-8a5e-78f2d545b836",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Yeah I plan to remove this in a followup, and since `FileStatus` is serializable, any subclass of it should also be, isn't it?",
        "createdAt" : "2020-08-22T18:12:22Z",
        "updatedAt" : "2020-09-14T18:54:43Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "38ef64a1-3cf6-4d1b-b051-e68e15382ad6",
        "parentId" : "102a564f-dcc9-4aab-8a5e-78f2d545b836",
        "authorId" : "224991ce-ad69-410b-8143-bf394b6b5c59",
        "body" : "> since FileStatus is serializable, any subclass of it should also be, isn't it?\r\n\r\n\"should\"; there's no tests for it, and nothing in the java language which stops you adding references to non-serializable objects. Exception has this exact issue",
        "createdAt" : "2020-09-04T16:01:41Z",
        "updatedAt" : "2020-09-14T18:54:43Z",
        "lastEditedBy" : "224991ce-ad69-410b-8143-bf394b6b5c59",
        "tags" : [
        ]
      }
    ],
    "commit" : "2d8e64d72a014ed5067be5772f8b2db9129ce2f2",
    "line" : 152,
    "diffHunk" : "@@ -1,1 +150,154 @@\n              SerializableFileStatus(\n                status.getPath.toString,\n                status.getLen,\n                status.isDirectory,"
  },
  {
    "id" : "2f0e5ed6-daf8-4fad-8e1a-e9bb9f726732",
    "prId" : 29471,
    "prUrl" : "https://github.com/apache/spark/pull/29471#pullrequestreview-483011683",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3a723bf8-12d7-4a30-948e-10c0f1d45197",
        "parentId" : null,
        "authorId" : "224991ce-ad69-410b-8143-bf394b6b5c59",
        "body" : "I'd question that -and its much better to use the incremental listLocatedStatus for better performance on paged object store listings, especially if you can do useful stuff while it takes place",
        "createdAt" : "2020-08-22T17:38:19Z",
        "updatedAt" : "2020-09-14T18:54:43Z",
        "lastEditedBy" : "224991ce-ad69-410b-8143-bf394b6b5c59",
        "tags" : [
        ]
      },
      {
        "id" : "5c45a084-51cd-4eb8-b06d-fea558cf0b62",
        "parentId" : "3a723bf8-12d7-4a30-948e-10c0f1d45197",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "I think we should add `S3AFileSystem` (in a followup) and perhaps a few others here since they also support pagination. The fallback part of the comments confuses me since I don't really see a fallback here for `DistributedFileSystem` etc.",
        "createdAt" : "2020-08-22T18:12:19Z",
        "updatedAt" : "2020-09-14T18:54:43Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "9a47276d-9f6a-4bf4-b6b0-c205a5943b50",
        "parentId" : "3a723bf8-12d7-4a30-948e-10c0f1d45197",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Also, do you think we can add something to [CommonPathCapabilities](https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/CommonPathCapabilities.java) for this? so that we don't have to enumerate all these here.",
        "createdAt" : "2020-08-22T18:22:02Z",
        "updatedAt" : "2020-09-14T18:54:43Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "76324d32-a480-4506-8418-87ab02ffea6b",
        "parentId" : "3a723bf8-12d7-4a30-948e-10c0f1d45197",
        "authorId" : "224991ce-ad69-410b-8143-bf394b6b5c59",
        "body" : "I would just switch to the incremental one *everywhere*\r\n\r\nHDFS likes it because when you have many, many files in a path they can release the lock on the NN; for object stores they always have to page in (s3, azure, GCS)....which gives the implementors an opportunity to move from the default implementation to exposing the incremental one. Add this and we can just let the relevant teams know.\r\n\r\nw.r.t CommonPathCapabilities, I suppose we could add one which declares that the listing ops are paged. But do you really want code to try and be that clever? I'm trying to use that feature to mark up\r\n\r\n* where optional features will fail with UnimplementedException or similar without you having to try using them (append, truncate, xattrs)\r\n* where there are fundamental semantics its important for algorithms to worry about (hflush)\r\n* where your store wants to expose some state to callers (e.g. s3a exposing its HADOOP-13230 dir marker policy)\r\n\r\nIf you want a performance option, we could add one.\r\n\r\nBTW, PathCapabilities is in hadoop-3.2.x now, will be in next release.  I might do it for 3.1 too...it makes a good way to programmatically/CLI probe for s3a dir marker policy, see.\r\n\r\nAlso, cloudstore has some CLI commands for the list calls (and pathcapabilities) to help explore what's going on, including, on s3a, listing cost in #of HTTP requests. Worth playing with to see what is good/bad, though as Mukund has been doing lots of work on 3.3.x s3a list, it will look worse than you'd expect on a dir treewalk,\r\nhttps://github.com/steveloughran/cloudstore\r\n\r\n",
        "createdAt" : "2020-08-24T12:55:24Z",
        "updatedAt" : "2020-09-14T18:54:43Z",
        "lastEditedBy" : "224991ce-ad69-410b-8143-bf394b6b5c59",
        "tags" : [
        ]
      },
      {
        "id" : "de99182d-f887-41b1-86d7-36fa186e9c10",
        "parentId" : "3a723bf8-12d7-4a30-948e-10c0f1d45197",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "I think changing this in a follow up PR sounds fine to me. I'd like to us to use the faster method by default and fall back on exception, but that could be a follow on. Want to file a JIRA for broadening the scope of using the faster method?",
        "createdAt" : "2020-08-24T21:43:11Z",
        "updatedAt" : "2020-09-14T18:54:43Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "f76242da-ff03-4415-9315-ad67077f7f7e",
        "parentId" : "3a723bf8-12d7-4a30-948e-10c0f1d45197",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Thanks @steveloughran !\r\n\r\n> I would just switch to the incremental one everywhere\r\n\r\nYes agree. This will help to simplify the code here a lot, although I guess we need to make sure this is enforced on the Hadoop side and that any future derived `FileSystem` implementations won't inherit the default behavior.\r\n\r\nBack to the code, seems the main reason why it calls `listStatus` in the else case (e.g., not `DistributedFileSystem`) is because it want to save `getBlockLocations` cost by calling that later on filtered leaf files. We can potentially use `FilterFileSystem` here and pass the filter to it instead. I'll try to explore this in a follow-up PR.\r\n\r\n> Want to file a JIRA for broadening the scope of using the faster method?\r\n\r\nYes will do that. The PR will be about improving and cleanup this utility class.",
        "createdAt" : "2020-08-24T22:56:50Z",
        "updatedAt" : "2020-09-14T18:54:43Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "3a369218-54e2-4aff-aa19-6a86a8396b00",
        "parentId" : "3a723bf8-12d7-4a30-948e-10c0f1d45197",
        "authorId" : "224991ce-ad69-410b-8143-bf394b6b5c59",
        "body" : "w.r.t the faster (incremental) calls, yes, something to consider next. At the very least, you will be able to collect and report/aggregate stats. For example, here is me getting the stats on a large/deep list just by calling toString on the RemoteIterator after it's done its work\r\n\r\n```\r\nListing statistics:\r\n  counters=((object_list_request=1) (object_continue_list_request=40)); gauges=(); minimums=((object_list_request.min=1298) (object_continue_list_request.min=414)); maximums=((object_continue_list_request.max=746) (object_list_request.max=1298)); means=((object_continue_list_request.mean=(sum=18210, samples=39, mean=466.9231)) (object_list_request.mean=(sum=1298, samples=1, mean=1298.0000))); \r\n\r\n2020-08-27 13:25:21,122 [main] INFO  tools.MarkerTool (DurationInfo.java:close(98)) - marker scan s3a://landsat-pds/: duration 0:19.563s\r\nListed 40000 objects under s3a://landsat-pds/\r\n```\r\nNow, who wouldn't like to know things like that. And ideally, collect across threads and merge back in.\r\n\r\nAs an aside, looked at {{org.apache.hadoop.mapred.LocatedFileStatusFetcher}}. This does multithreaded status fetching and collects those stats. Although it's tagged Private, I've noticed Parquet uses it so my next PR will convert to public/evolving and document the fact. \r\n\r\nIf you could use that, we could look @ evolving it better, especially returning a RemoteIterator of results which we could incrementally fill in across threads rather than block for the final results. Anything which makes it possible for app code to process data (read footers, etc) while the listing goes on has significant benefit in the World of Object Stores",
        "createdAt" : "2020-08-27T14:40:49Z",
        "updatedAt" : "2020-09-14T18:54:43Z",
        "lastEditedBy" : "224991ce-ad69-410b-8143-bf394b6b5c59",
        "tags" : [
        ]
      },
      {
        "id" : "d17c97cf-4c97-439f-92dd-d148fb5bb0bc",
        "parentId" : "3a723bf8-12d7-4a30-948e-10c0f1d45197",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Yes, stats will be nice. Looking forward to that ðŸ‘ (is it tracked by [this PR](https://github.com/apache/hadoop/pull/2069)?)\r\n\r\nw.r.t `LocatedFileStatusFetcher`, yes it will be great if this is exposed as a public API. I think we should also consider moving this to another module such as hadoop-common, and perhaps adding an option to turn on/off locality (so that we don't have to get the block locations if they are not needed). Pairing it with batch listing can also potentially help (although this is only available in HDFS currently).\r\n\r\nInstead of multiple threads, Spark currently use a distributed job so I'm not sure whether there will be any regression by doing that, but we can explore this later.",
        "createdAt" : "2020-08-28T00:47:56Z",
        "updatedAt" : "2020-09-14T18:54:43Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      },
      {
        "id" : "d392ed44-8534-476b-9faa-a84bd4256b02",
        "parentId" : "3a723bf8-12d7-4a30-948e-10c0f1d45197",
        "authorId" : "224991ce-ad69-410b-8143-bf394b6b5c59",
        "body" : "> w.r.t LocatedFileStatusFetcher, yes it will be great if this is exposed as a public API. I think we should also consider moving this to another module such as hadoop-common, and perhaps adding an option to turn on/off locality (so that we don't have to get the block locations if they are not needed). Pairing it with batch listing can also potentially help (although this is only available in HDFS currently).\r\n\r\nI'd worry about the effect of moving the package as is. It'd have to be a copy and paste, or move and subclass",
        "createdAt" : "2020-09-01T12:49:51Z",
        "updatedAt" : "2020-09-14T18:54:43Z",
        "lastEditedBy" : "224991ce-ad69-410b-8143-bf394b6b5c59",
        "tags" : [
        ]
      },
      {
        "id" : "cc9bba4c-bc0a-4e50-a354-52bd4e2bed6f",
        "parentId" : "3a723bf8-12d7-4a30-948e-10c0f1d45197",
        "authorId" : "224991ce-ad69-410b-8143-bf394b6b5c59",
        "body" : "@sunchao -yes, that's the stats API. Now is the time to review it and tell me where it doesn't suit your (perceived future) needs. I've been playing with a Parquet branch which uses it, not in Spark itself. ",
        "createdAt" : "2020-09-04T16:03:40Z",
        "updatedAt" : "2020-09-14T18:54:43Z",
        "lastEditedBy" : "224991ce-ad69-410b-8143-bf394b6b5c59",
        "tags" : [
        ]
      },
      {
        "id" : "3db8513c-d00a-4782-b7e6-2c57a29a4b92",
        "parentId" : "3a723bf8-12d7-4a30-948e-10c0f1d45197",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "Thanks for the info @steveloughran . I'll check out the PR (it is a very big one though).",
        "createdAt" : "2020-09-04T23:57:56Z",
        "updatedAt" : "2020-09-14T18:54:43Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      }
    ],
    "commit" : "2d8e64d72a014ed5067be5772f8b2db9129ce2f2",
    "line" : 213,
    "diffHunk" : "@@ -1,1 +211,215 @@        // DistributedFileSystem overrides listLocatedStatus to make 1 single call to namenode\n        // to retrieve the file status with the file block location. The reason to still fallback\n        // to listStatus is because the default implementation would potentially throw a\n        // FileNotFoundException which is better handled by doing the lookups manually below.\n        case (_: DistributedFileSystem | _: ViewFileSystem) if !ignoreLocality =>"
  },
  {
    "id" : "f7593aa1-a38a-4042-8743-bec69357551a",
    "prId" : 29471,
    "prUrl" : "https://github.com/apache/spark/pull/29471#pullrequestreview-488019143",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "444443df-02e9-4ea9-9b88-532e5406265c",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "This seems a new parameter that did not exist before. Why do we need this? If people want parallelized listing, people can invoke `parallelListLeafFiles` above.",
        "createdAt" : "2020-09-14T02:35:33Z",
        "updatedAt" : "2020-09-14T18:54:43Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "c276b868-0767-48b7-897e-a64019d86299",
        "parentId" : "444443df-02e9-4ea9-9b88-532e5406265c",
        "authorId" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "body" : "This is because `listLeafFiles` also recursively call `parallelListLeafFiles` inside so we need a way to pass down the arguments. These used to be read from conf in `SparkSession` but now made explicit as parameters, as we no longer have a session object.",
        "createdAt" : "2020-09-14T18:04:47Z",
        "updatedAt" : "2020-09-14T18:54:43Z",
        "lastEditedBy" : "9ae00886-75a7-4f39-aed7-d47b26b67afb",
        "tags" : [
        ]
      }
    ],
    "commit" : "2d8e64d72a014ed5067be5772f8b2db9129ce2f2",
    "line" : 201,
    "diffHunk" : "@@ -1,1 +199,203 @@      isRootPath: Boolean,\n      filterFun: Option[String => Boolean],\n      parallelismThreshold: Int,\n      parallelismMax: Int): Seq[FileStatus] = {\n"
  },
  {
    "id" : "26c3c05b-afae-45bd-80cf-34df3f88d219",
    "prId" : 29179,
    "prUrl" : "https://github.com/apache/spark/pull/29179#pullrequestreview-454529662",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "966e70eb-ce91-49d8-89b3-69f8f40fb928",
        "parentId" : null,
        "authorId" : "224991ce-ad69-410b-8143-bf394b6b5c59",
        "body" : "this is already optimised for in S3A with WiP on overlapping page fetch with returning current results, ABFS could do the same. Looking at the main filesystems, it's only local FS and the azure stores which don't override it",
        "createdAt" : "2020-07-22T17:53:18Z",
        "updatedAt" : "2020-08-18T17:51:49Z",
        "lastEditedBy" : "224991ce-ad69-410b-8143-bf394b6b5c59",
        "tags" : [
        ]
      },
      {
        "id" : "c9a9081b-ebc4-4708-98fe-39afc7a65ad5",
        "parentId" : "966e70eb-ce91-49d8-89b3-69f8f40fb928",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Makes sense. So maybe exposing this as a util function + trait + sample allowing people to selictively override parts would be better then yeah?",
        "createdAt" : "2020-07-23T22:09:59Z",
        "updatedAt" : "2020-08-18T17:51:49Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "b416c0017f7f1c3c662433cc7d63922e9b2b2427",
    "line" : 141,
    "diffHunk" : "@@ -1,1 +139,143 @@        // FileNotFoundException which is better handled by doing the lookups manually below.\n        case (_: DistributedFileSystem | _: ViewFileSystem) if !ignoreLocality =>\n          val remoteIter = fs.listLocatedStatus(path)\n          new Iterator[LocatedFileStatus]() {\n            def next(): LocatedFileStatus = remoteIter.next"
  },
  {
    "id" : "61dff598-ab02-47e7-8597-c1a599fbcbb6",
    "prId" : 29179,
    "prUrl" : "https://github.com/apache/spark/pull/29179#pullrequestreview-453548861",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "90cccec8-f24e-4380-a3c4-b20120a139ac",
        "parentId" : null,
        "authorId" : "224991ce-ad69-410b-8143-bf394b6b5c59",
        "body" : "yeah. getFileStatus is the expensive one; list calls more efficient as you can get lower latency as well as >1 response back",
        "createdAt" : "2020-07-22T17:54:47Z",
        "updatedAt" : "2020-08-18T17:51:49Z",
        "lastEditedBy" : "224991ce-ad69-410b-8143-bf394b6b5c59",
        "tags" : [
        ]
      }
    ],
    "commit" : "b416c0017f7f1c3c662433cc7d63922e9b2b2427",
    "line" : 248,
    "diffHunk" : "@@ -1,1 +246,250 @@      // NOTE:\n      //\n      // - Although S3/S3A/S3N file system can be quite slow for remote file metadata\n      //   operations, calling `getFileBlockLocations` does no harm here since these file system\n      //   implementations don't actually issue RPC for this method."
  },
  {
    "id" : "770fbc87-81b9-43d8-be8e-4956fcc52a87",
    "prId" : 29179,
    "prUrl" : "https://github.com/apache/spark/pull/29179#pullrequestreview-454529133",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "66718ea2-c55a-4971-bafe-a13658347505",
        "parentId" : null,
        "authorId" : "224991ce-ad69-410b-8143-bf394b6b5c59",
        "body" : "referencing this in the code means it may not work on installations without hadoop-hdfs on the classpath, which may include HD/I as well as spark standalone",
        "createdAt" : "2020-07-22T18:04:44Z",
        "updatedAt" : "2020-08-18T17:51:49Z",
        "lastEditedBy" : "224991ce-ad69-410b-8143-bf394b6b5c59",
        "tags" : [
        ]
      },
      {
        "id" : "6bc6a3f3-ca02-43de-901c-6d96ae983abe",
        "parentId" : "66718ea2-c55a-4971-bafe-a13658347505",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "So I think we've already got this in the SQL side. All the Spark standalone deployments I know of ship with a hadoop in the class path, and the no-hadoop distro is really just for situations where the user has their own hadoop on the class path.",
        "createdAt" : "2020-07-23T22:08:54Z",
        "updatedAt" : "2020-08-18T17:51:49Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "b416c0017f7f1c3c662433cc7d63922e9b2b2427",
    "line" : 29,
    "diffHunk" : "@@ -1,1 +27,31 @@import org.apache.hadoop.fs._\nimport org.apache.hadoop.fs.viewfs.ViewFileSystem\nimport org.apache.hadoop.hdfs.DistributedFileSystem\nimport org.apache.hadoop.mapred.JobConf\nimport org.apache.hadoop.mapreduce.JobContext"
  },
  {
    "id" : "07a45753-4913-480c-8635-aa1dac18b262",
    "prId" : 29179,
    "prUrl" : "https://github.com/apache/spark/pull/29179#pullrequestreview-453548861",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ee95f60c-b5fa-45c6-ba83-47ecb6aace2e",
        "parentId" : null,
        "authorId" : "224991ce-ad69-410b-8143-bf394b6b5c59",
        "body" : "be interesting to include the stats on how long the listing took (overall, #of files) so you could easily get some aggregate values to look @ when tuning performance here/in the FS",
        "createdAt" : "2020-07-22T18:06:14Z",
        "updatedAt" : "2020-08-18T17:51:49Z",
        "lastEditedBy" : "224991ce-ad69-410b-8143-bf394b6b5c59",
        "tags" : [
        ]
      }
    ],
    "commit" : "b416c0017f7f1c3c662433cc7d63922e9b2b2427",
    "line" : 96,
    "diffHunk" : "@@ -1,1 +94,98 @@              parallelismThreshold = Int.MaxValue,\n              maxParallelism = 0)\n            (path, leafFiles)\n          }.iterator\n        }.collect() // TODO(holden): should we use local itr here?"
  }
]