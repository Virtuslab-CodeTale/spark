[
  {
    "id" : "57a6219f-1f34-4df8-b3a4-07fcba2bd0eb",
    "prId" : 32401,
    "prUrl" : "https://github.com/apache/spark/pull/32401#pullrequestreview-704888368",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8767d8a7-2561-4b71-8730-eb28f5202d41",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "I did a bit more refactor here. Please take another look, thanks @mridulm ",
        "createdAt" : "2021-07-13T08:22:13Z",
        "updatedAt" : "2021-07-13T08:22:13Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "4bdde58a3791691f275968b117eeff260ef3016f",
    "line" : 106,
    "diffHunk" : "@@ -1,1 +362,366 @@              // this case, the current task attempt could write the missing checksum\n              // file by itself.\n              writeMetadataFile(checksums, checksumTmpOpt.get, checksumFileOpt.get, false)\n            }\n          }"
  },
  {
    "id" : "7d194ac4-3a9d-43a5-8124-3ace013aa078",
    "prId" : 32401,
    "prUrl" : "https://github.com/apache/spark/pull/32401#pullrequestreview-707022071",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "953644a1-ee61-4095-8619-72807e9022a9",
        "parentId" : null,
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "nit: Use `tryWithResource` instead ?",
        "createdAt" : "2021-07-14T07:04:39Z",
        "updatedAt" : "2021-07-14T07:04:39Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "e57e95b2-8037-424b-a606-a48160ce85c8",
        "parentId" : "953644a1-ee61-4095-8619-72807e9022a9",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "It could have different behavior with `tryWithSafeFinally`? (Note `tryWithSafeFinally` comes from the original code.)",
        "createdAt" : "2021-07-15T03:10:19Z",
        "updatedAt" : "2021-07-15T03:11:46Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "2b75b67d-c067-4c88-9ccf-8a013c864f64",
        "parentId" : "953644a1-ee61-4095-8619-72807e9022a9",
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "`tryWithResource` is more idiomatic - since this is a resource we are acquiring and then closing after its use :-)",
        "createdAt" : "2021-07-15T07:35:54Z",
        "updatedAt" : "2021-07-15T07:35:54Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "f9903318-ab09-4eb3-b23d-a9f3d9115ad0",
        "parentId" : "953644a1-ee61-4095-8619-72807e9022a9",
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "Having said that, I am fine with leaving it as is ... just mentioned since I saw it !",
        "createdAt" : "2021-07-15T07:36:25Z",
        "updatedAt" : "2021-07-15T07:36:25Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      }
    ],
    "commit" : "4bdde58a3791691f275968b117eeff260ef3016f",
    "line" : 208,
    "diffHunk" : "@@ -1,1 +446,450 @@      out.close()\n    }\n\n    if (targetFile.exists()) {\n      targetFile.delete()"
  },
  {
    "id" : "4c09ce94-4c30-4de0-bce1-9feedac58af1",
    "prId" : 32007,
    "prUrl" : "https://github.com/apache/spark/pull/32007#pullrequestreview-628573126",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0c2c094a-fb56-49ea-8a78-f6b0cb995684",
        "parentId" : null,
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "This assumption about length is fine as we are directly reading content into a `byte[]` and the number of chunks is reasonably bounded << `Int.MaxValue`.",
        "createdAt" : "2021-04-06T04:57:47Z",
        "updatedAt" : "2021-04-17T07:21:32Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      }
    ],
    "commit" : "e630725ca5c161cea62a2afcc7668a67a3e6d72e",
    "line" : 59,
    "diffHunk" : "@@ -1,1 +383,387 @@    val dataFile = getMergedBlockDataFile(conf.getAppId, blockId.shuffleId, blockId.reduceId, dirs)\n    // Load all the indexes in order to identify all chunks in the specified merged shuffle file.\n    val size = indexFile.length.toInt\n    val offsets = Utils.tryWithResource {\n      new DataInputStream(Files.newInputStream(indexFile.toPath))"
  },
  {
    "id" : "c5926591-32ec-4044-a685-bc6697c5e011",
    "prId" : 32007,
    "prUrl" : "https://github.com/apache/spark/pull/32007#pullrequestreview-671794918",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2f207cad-beba-477a-9e4f-77b1fd719ba5",
        "parentId" : null,
        "authorId" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "body" : "Nit: Missing javadoc\r\n```\r\n  /**\r\n   * This is only used for reading local merged block data. In such cases, all chunks in the\r\n   * merged shuffle file need to be identified at once, so the ShuffleBlockFetcherIterator\r\n   * knows how to consume local merged shuffle file as multiple chunks.\r\n   */\r\n   ```",
        "createdAt" : "2021-05-30T06:16:54Z",
        "updatedAt" : "2021-05-30T07:00:59Z",
        "lastEditedBy" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "tags" : [
        ]
      }
    ],
    "commit" : "e630725ca5c161cea62a2afcc7668a67a3e6d72e",
    "line" : 52,
    "diffHunk" : "@@ -1,1 +376,380 @@   * knows how to consume local merged shuffle file as multiple chunks.\n   */\n  override def getMergedBlockData(\n      blockId: ShuffleBlockId,\n      dirs: Option[Array[String]]): Seq[ManagedBuffer] = {"
  },
  {
    "id" : "04aad0bd-d1d3-4ec8-bd1a-b22b36eb1e6f",
    "prId" : 31493,
    "prUrl" : "https://github.com/apache/spark/pull/31493#pullrequestreview-584693342",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dabf4c90-d08d-44ea-a341-ec8e65a48f72",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "This looks like a performance degradation a little due to the additional disk IO. Do we need this at every `putShuffleBlockAsStream`? Do you know how much does this affected, @holdenk ?\r\n\r\ncc @dbtsai ",
        "createdAt" : "2021-02-05T19:02:41Z",
        "updatedAt" : "2021-02-08T20:53:03Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "b1887409-99cc-4776-a225-893792fe6f9b",
        "parentId" : "dabf4c90-d08d-44ea-a341-ec8e65a48f72",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "That's a good question. So looking at benchmarks folks have done for file size takes a few microseconds per file (and speeds up on subsequent runs due to cache). It doesn't need to actually read the file to determine the size (if it did that would be much slower).",
        "createdAt" : "2021-02-05T20:44:42Z",
        "updatedAt" : "2021-02-08T20:53:03Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "69b25406b9d5ee1f0862f06614f579a4ed9a1e03",
    "line" : 40,
    "diffHunk" : "@@ -1,1 +185,189 @@    // Throw an exception if we have exceeded maximum shuffle files stored\n    remoteShuffleMaxDisk.foreach { maxBytes =>\n      val bytesUsed = getShuffleBytesStored()\n      if (maxBytes < bytesUsed) {\n        throw new SparkException(s\"Not storing remote shuffles $bytesUsed exceeds $maxBytes\")"
  },
  {
    "id" : "68c8a458-9f40-45d0-87f8-472be2f914a9",
    "prId" : 31493,
    "prUrl" : "https://github.com/apache/spark/pull/31493#pullrequestreview-585759001",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0f00c29a-d92b-4bcf-b497-4318378c0340",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Give a hint of the conf for users to increase the maxBytes?",
        "createdAt" : "2021-02-06T13:55:24Z",
        "updatedAt" : "2021-02-08T20:53:03Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "3302b575-38ec-4c44-a6e1-94d88c3a8798",
        "parentId" : "0f00c29a-d92b-4bcf-b497-4318378c0340",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "So we wouldn't want them to actually increase maxBytes, this is really an internal error that the user won't see because we'll just eat the error and not migrate to this exec.",
        "createdAt" : "2021-02-08T17:42:50Z",
        "updatedAt" : "2021-02-08T20:53:03Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "69b25406b9d5ee1f0862f06614f579a4ed9a1e03",
    "line" : 42,
    "diffHunk" : "@@ -1,1 +187,191 @@      val bytesUsed = getShuffleBytesStored()\n      if (maxBytes < bytesUsed) {\n        throw new SparkException(s\"Not storing remote shuffles $bytesUsed exceeds $maxBytes\")\n      }\n    }"
  },
  {
    "id" : "f2093b8c-d940-43bc-a8cc-c470112af90b",
    "prId" : 28708,
    "prUrl" : "https://github.com/apache/spark/pull/28708#pullrequestreview-423519621",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bdf76928-4586-41d7-9aab-9adb9aa5decb",
        "parentId" : null,
        "authorId" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "body" : "The reason of the failure might be interesting here:\r\n \r\n```suggestion\r\n        // the framework handles the connection itself, we just need to do local cleanup\r\n        logWarning(s\"Error while uploading $blockId\", cause)\r\n```",
        "createdAt" : "2020-06-03T12:52:05Z",
        "updatedAt" : "2020-07-17T23:45:20Z",
        "lastEditedBy" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "tags" : [
        ]
      }
    ],
    "commit" : "8494bdd94285c7cc5a41e151da920710be7f4671",
    "line" : 104,
    "diffHunk" : "@@ -1,1 +214,218 @@\n      override def onFailure(streamId: String, cause: Throwable): Unit = {\n        // the framework handles the connection itself, we just need to do local cleanup\n        logWarning(s\"Error while uploading $blockId\", cause)\n        channel.close()"
  },
  {
    "id" : "1fe5cb85-d933-4191-9a8c-ac8707a99def",
    "prId" : 28708,
    "prUrl" : "https://github.com/apache/spark/pull/28708#pullrequestreview-464464617",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "54915d84-86ce-40bb-b8bd-6c50f99a5559",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "I didn't follow why this is locked ?",
        "createdAt" : "2020-06-19T01:39:50Z",
        "updatedAt" : "2020-07-17T23:45:20Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "f08f9f1b-7c22-4ee7-90ff-21f597478f6e",
        "parentId" : "54915d84-86ce-40bb-b8bd-6c50f99a5559",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "The corresponding logic of finalizing is also synchronized. I think it's fairly remote but if (somehow) we've got some other process renaming the same file (say a speculative task), we could end up with junk data if we don't synchronize here.",
        "createdAt" : "2020-06-19T03:03:48Z",
        "updatedAt" : "2020-07-17T23:45:20Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "46a48ef8-3fe3-41ca-b822-8c606e47235a",
        "parentId" : "54915d84-86ce-40bb-b8bd-6c50f99a5559",
        "authorId" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "body" : "I think this lock might not be needed as in `writeIndexFileAndCommit` the lock is used to have the `checkIndexAndDataFile` and the file rename in one atomic operation. \r\n\r\nBut in a follow-up PR we can investigate this more.",
        "createdAt" : "2020-07-19T10:59:22Z",
        "updatedAt" : "2020-07-19T11:05:14Z",
        "lastEditedBy" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "tags" : [
        ]
      },
      {
        "id" : "c6018bd7-1005-46a7-9b7c-4d7b65bd9a61",
        "parentId" : "54915d84-86ce-40bb-b8bd-6c50f99a5559",
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "The lock used for synchronizing looks incorrect here - should be `IndexShuffleBlockResolver.this` ?\r\n(I missed reviewing this earlier, apologies)",
        "createdAt" : "2020-08-10T18:16:04Z",
        "updatedAt" : "2020-08-10T18:16:04Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      }
    ],
    "commit" : "8494bdd94285c7cc5a41e151da920710be7f4671",
    "line" : 92,
    "diffHunk" : "@@ -1,1 +202,206 @@        channel.close()\n        val diskSize = fileTmp.length()\n        this.synchronized {\n          if (file.exists()) {\n            file.delete()"
  },
  {
    "id" : "b2bf5259-c0fc-4858-97a1-9e3a6e955b85",
    "prId" : 28331,
    "prUrl" : "https://github.com/apache/spark/pull/28331#pullrequestreview-418577086",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "41ec2f60-3bd8-40f4-96df-1d96e31a4a2b",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "When the file exists, does it mean there is index/data file with same shuffle id and map id? When it could happen?",
        "createdAt" : "2020-05-03T05:14:28Z",
        "updatedAt" : "2020-06-02T18:20:15Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "1b5eeb78-6574-431a-9cab-b9ad82623eb3",
        "parentId" : "41ec2f60-3bd8-40f4-96df-1d96e31a4a2b",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "I suppose this should never happen, I'm not sure though let me do some thinking on that.",
        "createdAt" : "2020-05-03T05:32:29Z",
        "updatedAt" : "2020-06-02T18:20:15Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "b7a3cec6-414e-493c-9964-936a1654a12b",
        "parentId" : "41ec2f60-3bd8-40f4-96df-1d96e31a4a2b",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "So this mirrors the logic inside of writeIndexFileAndCommit, the matching check there was introduced in SPARK-17547\r\n which I believe is for the situation where an exception occurred during a previous write and the filesystem is in a dirty state. So I think we should keep it to be safe.",
        "createdAt" : "2020-05-26T18:43:09Z",
        "updatedAt" : "2020-06-02T18:20:16Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "a3aa8ebe11de6011fc77daa5b805af1193992d74",
    "line" : 97,
    "diffHunk" : "@@ -1,1 +210,214 @@          if (file.exists()) {\n            file.delete()\n          }\n          if (!fileTmp.renameTo(file)) {\n            throw new IOException(s\"fail to rename file ${fileTmp} to ${file}\")"
  }
]