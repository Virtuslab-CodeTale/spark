[
  {
    "id" : "57a6219f-1f34-4df8-b3a4-07fcba2bd0eb",
    "prId" : 32401,
    "prUrl" : "https://github.com/apache/spark/pull/32401#pullrequestreview-704888368",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8767d8a7-2561-4b71-8730-eb28f5202d41",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "I did a bit more refactor here. Please take another look, thanks @mridulm ",
        "createdAt" : "2021-07-13T08:22:13Z",
        "updatedAt" : "2021-07-13T08:22:13Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "4bdde58a3791691f275968b117eeff260ef3016f",
    "line" : 106,
    "diffHunk" : "@@ -1,1 +362,366 @@              // this case, the current task attempt could write the missing checksum\n              // file by itself.\n              writeMetadataFile(checksums, checksumTmpOpt.get, checksumFileOpt.get, false)\n            }\n          }"
  },
  {
    "id" : "7d194ac4-3a9d-43a5-8124-3ace013aa078",
    "prId" : 32401,
    "prUrl" : "https://github.com/apache/spark/pull/32401#pullrequestreview-707022071",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "953644a1-ee61-4095-8619-72807e9022a9",
        "parentId" : null,
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "nit: Use `tryWithResource` instead ?",
        "createdAt" : "2021-07-14T07:04:39Z",
        "updatedAt" : "2021-07-14T07:04:39Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "e57e95b2-8037-424b-a606-a48160ce85c8",
        "parentId" : "953644a1-ee61-4095-8619-72807e9022a9",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "It could have different behavior with `tryWithSafeFinally`? (Note `tryWithSafeFinally` comes from the original code.)",
        "createdAt" : "2021-07-15T03:10:19Z",
        "updatedAt" : "2021-07-15T03:11:46Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "2b75b67d-c067-4c88-9ccf-8a013c864f64",
        "parentId" : "953644a1-ee61-4095-8619-72807e9022a9",
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "`tryWithResource` is more idiomatic - since this is a resource we are acquiring and then closing after its use :-)",
        "createdAt" : "2021-07-15T07:35:54Z",
        "updatedAt" : "2021-07-15T07:35:54Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "f9903318-ab09-4eb3-b23d-a9f3d9115ad0",
        "parentId" : "953644a1-ee61-4095-8619-72807e9022a9",
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "Having said that, I am fine with leaving it as is ... just mentioned since I saw it !",
        "createdAt" : "2021-07-15T07:36:25Z",
        "updatedAt" : "2021-07-15T07:36:25Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      }
    ],
    "commit" : "4bdde58a3791691f275968b117eeff260ef3016f",
    "line" : 208,
    "diffHunk" : "@@ -1,1 +446,450 @@      out.close()\n    }\n\n    if (targetFile.exists()) {\n      targetFile.delete()"
  },
  {
    "id" : "4c09ce94-4c30-4de0-bce1-9feedac58af1",
    "prId" : 32007,
    "prUrl" : "https://github.com/apache/spark/pull/32007#pullrequestreview-628573126",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0c2c094a-fb56-49ea-8a78-f6b0cb995684",
        "parentId" : null,
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "This assumption about length is fine as we are directly reading content into a `byte[]` and the number of chunks is reasonably bounded << `Int.MaxValue`.",
        "createdAt" : "2021-04-06T04:57:47Z",
        "updatedAt" : "2021-04-17T07:21:32Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      }
    ],
    "commit" : "e630725ca5c161cea62a2afcc7668a67a3e6d72e",
    "line" : 59,
    "diffHunk" : "@@ -1,1 +383,387 @@    val dataFile = getMergedBlockDataFile(conf.getAppId, blockId.shuffleId, blockId.reduceId, dirs)\n    // Load all the indexes in order to identify all chunks in the specified merged shuffle file.\n    val size = indexFile.length.toInt\n    val offsets = Utils.tryWithResource {\n      new DataInputStream(Files.newInputStream(indexFile.toPath))"
  },
  {
    "id" : "c5926591-32ec-4044-a685-bc6697c5e011",
    "prId" : 32007,
    "prUrl" : "https://github.com/apache/spark/pull/32007#pullrequestreview-671794918",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2f207cad-beba-477a-9e4f-77b1fd719ba5",
        "parentId" : null,
        "authorId" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "body" : "Nit: Missing javadoc\r\n```\r\n  /**\r\n   * This is only used for reading local merged block data. In such cases, all chunks in the\r\n   * merged shuffle file need to be identified at once, so the ShuffleBlockFetcherIterator\r\n   * knows how to consume local merged shuffle file as multiple chunks.\r\n   */\r\n   ```",
        "createdAt" : "2021-05-30T06:16:54Z",
        "updatedAt" : "2021-05-30T07:00:59Z",
        "lastEditedBy" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "tags" : [
        ]
      }
    ],
    "commit" : "e630725ca5c161cea62a2afcc7668a67a3e6d72e",
    "line" : 52,
    "diffHunk" : "@@ -1,1 +376,380 @@   * knows how to consume local merged shuffle file as multiple chunks.\n   */\n  override def getMergedBlockData(\n      blockId: ShuffleBlockId,\n      dirs: Option[Array[String]]): Seq[ManagedBuffer] = {"
  },
  {
    "id" : "04aad0bd-d1d3-4ec8-bd1a-b22b36eb1e6f",
    "prId" : 31493,
    "prUrl" : "https://github.com/apache/spark/pull/31493#pullrequestreview-584693342",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dabf4c90-d08d-44ea-a341-ec8e65a48f72",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "This looks like a performance degradation a little due to the additional disk IO. Do we need this at every `putShuffleBlockAsStream`? Do you know how much does this affected, @holdenk ?\r\n\r\ncc @dbtsai ",
        "createdAt" : "2021-02-05T19:02:41Z",
        "updatedAt" : "2021-02-08T20:53:03Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "b1887409-99cc-4776-a225-893792fe6f9b",
        "parentId" : "dabf4c90-d08d-44ea-a341-ec8e65a48f72",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "That's a good question. So looking at benchmarks folks have done for file size takes a few microseconds per file (and speeds up on subsequent runs due to cache). It doesn't need to actually read the file to determine the size (if it did that would be much slower).",
        "createdAt" : "2021-02-05T20:44:42Z",
        "updatedAt" : "2021-02-08T20:53:03Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "69b25406b9d5ee1f0862f06614f579a4ed9a1e03",
    "line" : 40,
    "diffHunk" : "@@ -1,1 +185,189 @@    // Throw an exception if we have exceeded maximum shuffle files stored\n    remoteShuffleMaxDisk.foreach { maxBytes =>\n      val bytesUsed = getShuffleBytesStored()\n      if (maxBytes < bytesUsed) {\n        throw new SparkException(s\"Not storing remote shuffles $bytesUsed exceeds $maxBytes\")"
  },
  {
    "id" : "68c8a458-9f40-45d0-87f8-472be2f914a9",
    "prId" : 31493,
    "prUrl" : "https://github.com/apache/spark/pull/31493#pullrequestreview-585759001",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0f00c29a-d92b-4bcf-b497-4318378c0340",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Give a hint of the conf for users to increase the maxBytes?",
        "createdAt" : "2021-02-06T13:55:24Z",
        "updatedAt" : "2021-02-08T20:53:03Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "3302b575-38ec-4c44-a6e1-94d88c3a8798",
        "parentId" : "0f00c29a-d92b-4bcf-b497-4318378c0340",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "So we wouldn't want them to actually increase maxBytes, this is really an internal error that the user won't see because we'll just eat the error and not migrate to this exec.",
        "createdAt" : "2021-02-08T17:42:50Z",
        "updatedAt" : "2021-02-08T20:53:03Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "69b25406b9d5ee1f0862f06614f579a4ed9a1e03",
    "line" : 42,
    "diffHunk" : "@@ -1,1 +187,191 @@      val bytesUsed = getShuffleBytesStored()\n      if (maxBytes < bytesUsed) {\n        throw new SparkException(s\"Not storing remote shuffles $bytesUsed exceeds $maxBytes\")\n      }\n    }"
  }
]