[
  {
    "id" : "57a6219f-1f34-4df8-b3a4-07fcba2bd0eb",
    "prId" : 32401,
    "prUrl" : "https://github.com/apache/spark/pull/32401#pullrequestreview-704888368",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8767d8a7-2561-4b71-8730-eb28f5202d41",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "I did a bit more refactor here. Please take another look, thanks @mridulm ",
        "createdAt" : "2021-07-13T08:22:13Z",
        "updatedAt" : "2021-07-13T08:22:13Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "4bdde58a3791691f275968b117eeff260ef3016f",
    "line" : 106,
    "diffHunk" : "@@ -1,1 +362,366 @@              // this case, the current task attempt could write the missing checksum\n              // file by itself.\n              writeMetadataFile(checksums, checksumTmpOpt.get, checksumFileOpt.get, false)\n            }\n          }"
  },
  {
    "id" : "7d194ac4-3a9d-43a5-8124-3ace013aa078",
    "prId" : 32401,
    "prUrl" : "https://github.com/apache/spark/pull/32401#pullrequestreview-707022071",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "953644a1-ee61-4095-8619-72807e9022a9",
        "parentId" : null,
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "nit: Use `tryWithResource` instead ?",
        "createdAt" : "2021-07-14T07:04:39Z",
        "updatedAt" : "2021-07-14T07:04:39Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "e57e95b2-8037-424b-a606-a48160ce85c8",
        "parentId" : "953644a1-ee61-4095-8619-72807e9022a9",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "It could have different behavior with `tryWithSafeFinally`? (Note `tryWithSafeFinally` comes from the original code.)",
        "createdAt" : "2021-07-15T03:10:19Z",
        "updatedAt" : "2021-07-15T03:11:46Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "2b75b67d-c067-4c88-9ccf-8a013c864f64",
        "parentId" : "953644a1-ee61-4095-8619-72807e9022a9",
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "`tryWithResource` is more idiomatic - since this is a resource we are acquiring and then closing after its use :-)",
        "createdAt" : "2021-07-15T07:35:54Z",
        "updatedAt" : "2021-07-15T07:35:54Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "f9903318-ab09-4eb3-b23d-a9f3d9115ad0",
        "parentId" : "953644a1-ee61-4095-8619-72807e9022a9",
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "Having said that, I am fine with leaving it as is ... just mentioned since I saw it !",
        "createdAt" : "2021-07-15T07:36:25Z",
        "updatedAt" : "2021-07-15T07:36:25Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      }
    ],
    "commit" : "4bdde58a3791691f275968b117eeff260ef3016f",
    "line" : 208,
    "diffHunk" : "@@ -1,1 +446,450 @@      out.close()\n    }\n\n    if (targetFile.exists()) {\n      targetFile.delete()"
  },
  {
    "id" : "4c09ce94-4c30-4de0-bce1-9feedac58af1",
    "prId" : 32007,
    "prUrl" : "https://github.com/apache/spark/pull/32007#pullrequestreview-628573126",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0c2c094a-fb56-49ea-8a78-f6b0cb995684",
        "parentId" : null,
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "This assumption about length is fine as we are directly reading content into a `byte[]` and the number of chunks is reasonably bounded << `Int.MaxValue`.",
        "createdAt" : "2021-04-06T04:57:47Z",
        "updatedAt" : "2021-04-17T07:21:32Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      }
    ],
    "commit" : "e630725ca5c161cea62a2afcc7668a67a3e6d72e",
    "line" : 59,
    "diffHunk" : "@@ -1,1 +383,387 @@    val dataFile = getMergedBlockDataFile(conf.getAppId, blockId.shuffleId, blockId.reduceId, dirs)\n    // Load all the indexes in order to identify all chunks in the specified merged shuffle file.\n    val size = indexFile.length.toInt\n    val offsets = Utils.tryWithResource {\n      new DataInputStream(Files.newInputStream(indexFile.toPath))"
  },
  {
    "id" : "c5926591-32ec-4044-a685-bc6697c5e011",
    "prId" : 32007,
    "prUrl" : "https://github.com/apache/spark/pull/32007#pullrequestreview-671794918",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2f207cad-beba-477a-9e4f-77b1fd719ba5",
        "parentId" : null,
        "authorId" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "body" : "Nit: Missing javadoc\r\n```\r\n  /**\r\n   * This is only used for reading local merged block data. In such cases, all chunks in the\r\n   * merged shuffle file need to be identified at once, so the ShuffleBlockFetcherIterator\r\n   * knows how to consume local merged shuffle file as multiple chunks.\r\n   */\r\n   ```",
        "createdAt" : "2021-05-30T06:16:54Z",
        "updatedAt" : "2021-05-30T07:00:59Z",
        "lastEditedBy" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "tags" : [
        ]
      }
    ],
    "commit" : "e630725ca5c161cea62a2afcc7668a67a3e6d72e",
    "line" : 52,
    "diffHunk" : "@@ -1,1 +376,380 @@   * knows how to consume local merged shuffle file as multiple chunks.\n   */\n  override def getMergedBlockData(\n      blockId: ShuffleBlockId,\n      dirs: Option[Array[String]]): Seq[ManagedBuffer] = {"
  }
]