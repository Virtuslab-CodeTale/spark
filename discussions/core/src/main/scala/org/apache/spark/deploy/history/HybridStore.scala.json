[
  {
    "id" : "eb4190d0-4d55-46e6-afc4-ddf5b97e9e4f",
    "prId" : 29149,
    "prUrl" : "https://github.com/apache/spark/pull/29149#pullrequestreview-451020781",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "350b1f25-6050-44df-a0d4-b5227d1a64dc",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "This would be OK, given all entries are from inMemoryStore which are already materialized into memory.",
        "createdAt" : "2020-07-18T01:36:42Z",
        "updatedAt" : "2020-07-19T02:53:22Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "a12e1783aa238d2c08ebdf34708c1539017b477d",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +147,151 @@      try {\n        for (klass <- klassMap.keys().asScala) {\n          val values = Lists.newArrayList(\n              inMemoryStore.view(klass).closeableIterator())\n          levelDB.writeAll(values)"
  },
  {
    "id" : "7f0afb4b-6b0e-400d-be02-71e3efbba0d0",
    "prId" : 28412,
    "prUrl" : "https://github.com/apache/spark/pull/28412#pullrequestreview-427358275",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "28c7bd1e-bf2a-487f-9376-1d90aa90eddf",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "This line can be moved to the catch statement.",
        "createdAt" : "2020-06-09T06:18:19Z",
        "updatedAt" : "2020-07-06T21:32:29Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "8cf36175-d92a-402c-9748-778e8962e72f",
        "parentId" : "28c7bd1e-bf2a-487f-9376-1d90aa90eddf",
        "authorId" : "788d6907-ae49-42f5-95b7-ef69c1460937",
        "body" : "done.",
        "createdAt" : "2020-06-09T16:52:51Z",
        "updatedAt" : "2020-07-06T21:32:29Z",
        "lastEditedBy" : "788d6907-ae49-42f5-95b7-ef69c1460937",
        "tags" : [
        ]
      }
    ],
    "commit" : "b71a923e4554e814ee923b64d1b22ce42825378c",
    "line" : 157,
    "diffHunk" : "@@ -1,1 +155,159 @@      } catch {\n        case e: Exception =>\n          listener.onSwitchToLevelDBFail(e)\n      }\n    })"
  },
  {
    "id" : "3e1e5080-b8f5-4486-9227-fe1a587cdf23",
    "prId" : 28412,
    "prUrl" : "https://github.com/apache/spark/pull/28412#pullrequestreview-427697653",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e0ab7f28-006a-4fe4-8ed9-e00d1d5dde41",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Probably we may want to guarantee this to be executed once regardless of exception being thrown in join().",
        "createdAt" : "2020-06-09T06:46:05Z",
        "updatedAt" : "2020-07-06T21:32:29Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "4a75ef8e-27dd-45fd-b620-e5f439b76fd6",
        "parentId" : "e0ab7f28-006a-4fe4-8ed9-e00d1d5dde41",
        "authorId" : "788d6907-ae49-42f5-95b7-ef69c1460937",
        "body" : "In the current implementation, the background thread won't throw uncaught exceptions. So I think levelDB.close() is guaranteed to be executed. Here the try-catch block is trying to catch the IOException that might be thrown during levelDB.close().",
        "createdAt" : "2020-06-09T16:57:01Z",
        "updatedAt" : "2020-07-06T21:32:29Z",
        "lastEditedBy" : "788d6907-ae49-42f5-95b7-ef69c1460937",
        "tags" : [
        ]
      },
      {
        "id" : "b4bb22ad-454f-4683-9c78-71006f81513d",
        "parentId" : "e0ab7f28-006a-4fe4-8ed9-e00d1d5dde41",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Isn't join() able to throw InterruptedException? That's not a runtime exception but you're playing with Scala which ignores checked exception so you still need to be careful.\r\n\r\nAnd IMHO, generally at any case we must ensure both level DB store and in memory store are closed because that's a resource leak.",
        "createdAt" : "2020-06-10T00:06:32Z",
        "updatedAt" : "2020-07-06T21:32:29Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "451b8b72-9ad5-44be-8139-415599f99b30",
        "parentId" : "e0ab7f28-006a-4fe4-8ed9-e00d1d5dde41",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Friendly reminder about the comment here.",
        "createdAt" : "2020-06-10T03:56:47Z",
        "updatedAt" : "2020-07-06T21:32:29Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "73da85cc-961e-44d1-886e-d69480b0de07",
        "parentId" : "e0ab7f28-006a-4fe4-8ed9-e00d1d5dde41",
        "authorId" : "788d6907-ae49-42f5-95b7-ef69c1460937",
        "body" : "Sorry I missed this comment. Yeah, you are right, join() can throw InterruptedException. I will update the code.",
        "createdAt" : "2020-06-10T04:11:52Z",
        "updatedAt" : "2020-07-06T21:32:29Z",
        "lastEditedBy" : "788d6907-ae49-42f5-95b7-ef69c1460937",
        "tags" : [
        ]
      }
    ],
    "commit" : "b71a923e4554e814ee923b64d1b22ce42825378c",
    "line" : 108,
    "diffHunk" : "@@ -1,1 +106,110 @@    } finally {\n      inMemoryStore.close()\n      if (levelDB != null) {\n        levelDB.close()\n      }"
  },
  {
    "id" : "c73201fe-b527-430b-b53b-12fc16cb62ff",
    "prId" : 28412,
    "prUrl" : "https://github.com/apache/spark/pull/28412#pullrequestreview-428063479",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8f735942-165f-408a-8312-573440933277",
        "parentId" : null,
        "authorId" : "fb1da7a4-0c05-42a2-9913-c8bd60c58a4f",
        "body" : "Do we expect a write after a background thread has started? We might want to throw an IIlegalStateException",
        "createdAt" : "2020-06-09T16:24:08Z",
        "updatedAt" : "2020-07-06T21:32:29Z",
        "lastEditedBy" : "fb1da7a4-0c05-42a2-9913-c8bd60c58a4f",
        "tags" : [
        ]
      },
      {
        "id" : "f85dbfcd-219f-46fb-a5e0-546d6a3c8a14",
        "parentId" : "8f735942-165f-408a-8312-573440933277",
        "authorId" : "788d6907-ae49-42f5-95b7-ef69c1460937",
        "body" : "write operation is only allowed for CacheQuantile objects after the rebuildAppStore() is finished. Here if we want to throw IIlegalStateException, we need to have special logic to check if the value is of class CacheQuantile. I think we would prefer to avoid that to make the HybridStore as generic as possible.",
        "createdAt" : "2020-06-09T17:34:08Z",
        "updatedAt" : "2020-07-06T21:32:29Z",
        "lastEditedBy" : "788d6907-ae49-42f5-95b7-ef69c1460937",
        "tags" : [
        ]
      },
      {
        "id" : "fb68f360-cc76-4748-9eb6-e25c6af4f2e2",
        "parentId" : "8f735942-165f-408a-8312-573440933277",
        "authorId" : "fb1da7a4-0c05-42a2-9913-c8bd60c58a4f",
        "body" : "May be worth mentioning CacheQuantile in the comment as it can be recalculated",
        "createdAt" : "2020-06-09T23:40:48Z",
        "updatedAt" : "2020-07-06T21:32:29Z",
        "lastEditedBy" : "fb1da7a4-0c05-42a2-9913-c8bd60c58a4f",
        "tags" : [
        ]
      },
      {
        "id" : "71da53e9-b3f5-4f7c-ada3-c5720778a835",
        "parentId" : "8f735942-165f-408a-8312-573440933277",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "I don't think we should mention specific class - we added the assumption in class doc, with trying to generalize the case. If we feel the class doc isn't enough then we can comment the assumption here as well, but let's generalize.",
        "createdAt" : "2020-06-09T23:58:46Z",
        "updatedAt" : "2020-07-06T21:32:29Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "8ed33761-8391-4cbd-a461-91bde677b070",
        "parentId" : "8f735942-165f-408a-8312-573440933277",
        "authorId" : "fb1da7a4-0c05-42a2-9913-c8bd60c58a4f",
        "body" : "Ah ok i missed that thanks",
        "createdAt" : "2020-06-10T13:40:49Z",
        "updatedAt" : "2020-07-06T21:32:29Z",
        "lastEditedBy" : "fb1da7a4-0c05-42a2-9913-c8bd60c58a4f",
        "tags" : [
        ]
      }
    ],
    "commit" : "b71a923e4554e814ee923b64d1b22ce42825378c",
    "line" : 74,
    "diffHunk" : "@@ -1,1 +72,76 @@    if (backgroundThread == null) {\n      // New classes won't be dumped once the background thread is started\n      klassMap.putIfAbsent(value.getClass(), true)\n    }\n  }"
  },
  {
    "id" : "4b17a044-7909-4867-96bf-21dd9f1009c7",
    "prId" : 28412,
    "prUrl" : "https://github.com/apache/spark/pull/28412#pullrequestreview-431514534",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "38a9d663-8a6d-42a7-808b-aac7f621ff97",
        "parentId" : null,
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "do we need to reset this to true if the exception is thrown in close?",
        "createdAt" : "2020-06-15T14:40:58Z",
        "updatedAt" : "2020-07-06T21:32:29Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "88c329a1-f999-4887-b122-28df0ac045fb",
        "parentId" : "38a9d663-8a6d-42a7-808b-aac7f621ff97",
        "authorId" : "788d6907-ae49-42f5-95b7-ef69c1460937",
        "body" : "I think we don't need to do that since inMemoryStore.close() won't throw exceptions.",
        "createdAt" : "2020-06-15T22:53:34Z",
        "updatedAt" : "2020-07-06T21:32:29Z",
        "lastEditedBy" : "788d6907-ae49-42f5-95b7-ef69c1460937",
        "tags" : [
        ]
      },
      {
        "id" : "7d781102-ef20-41f6-99cd-29f0917468e7",
        "parentId" : "38a9d663-8a6d-42a7-808b-aac7f621ff97",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "it doesn't now, but if someone were to modify it in future it could.  ",
        "createdAt" : "2020-06-16T13:41:51Z",
        "updatedAt" : "2020-07-06T21:32:29Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "b71a923e4554e814ee923b64d1b22ce42825378c",
    "line" : 153,
    "diffHunk" : "@@ -1,1 +151,155 @@        }\n        listener.onSwitchToLevelDBSuccess()\n        shouldUseInMemoryStore.set(false)\n        inMemoryStore.close()\n      } catch {"
  },
  {
    "id" : "61c816bb-3909-430e-b807-d446f4b8588b",
    "prId" : 28412,
    "prUrl" : "https://github.com/apache/spark/pull/28412#pullrequestreview-446022065",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e93f80c8-a51e-4694-bd8d-81b633a1b397",
        "parentId" : null,
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "Drive by comment - given I added something similar to an in-house patch.\r\nAdd a `write(Iterator<E> values)` to kv store - this should make this switch order(s) faster.",
        "createdAt" : "2020-07-08T17:19:41Z",
        "updatedAt" : "2020-07-08T17:19:41Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "530a8535-e013-450a-9059-225f7b335b7c",
        "parentId" : "e93f80c8-a51e-4694-bd8d-81b633a1b397",
        "authorId" : "788d6907-ae49-42f5-95b7-ef69c1460937",
        "body" : "Thanks for the information, I will try adding a `write(Iterator<E> values)` in this pr.",
        "createdAt" : "2020-07-08T19:19:27Z",
        "updatedAt" : "2020-07-08T19:19:28Z",
        "lastEditedBy" : "788d6907-ae49-42f5-95b7-ef69c1460937",
        "tags" : [
        ]
      },
      {
        "id" : "968567a7-87b2-4de1-a410-cc425c74346c",
        "parentId" : "e93f80c8-a51e-4694-bd8d-81b633a1b397",
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "If this helps, this is what I had written up for level db - for memory store, the default list traversal + write is good enough :\r\n```\r\n  @Override\r\n  public void write(List<?> values) throws Exception {\r\n    Preconditions.checkArgument(values != null && !values.isEmpty(),\r\n      \"Non-empty values required.\");\r\n\r\n    // Group by class, in case there are values from different classes in the iterator\r\n    // Typical usecase is for this to be a single class.\r\n    for (Map.Entry<? extends Class<?>, ? extends List<?>> entry :\r\n            values.stream().collect(Collectors.groupingBy(Object::getClass)).entrySet()) {\r\n\r\n      final Iterator<?> valueIter = entry.getValue().iterator();\r\n      final Iterator<byte[]> serializedValueIter;\r\n\r\n      {\r\n        // deserialize outside synchronized block\r\n        List<byte[]> list = new ArrayList<>(entry.getValue().size());\r\n        for (Object value : entry.getValue()) {\r\n          list.add(serializer.serialize(value));\r\n        }\r\n        serializedValueIter = list.iterator();\r\n      }\r\n\r\n      final Class<?> valueClass = entry.getKey();\r\n      final LevelDBTypeInfo ti = getTypeInfo(valueClass);\r\n\r\n      // Batching updates per type\r\n      synchronized (ti) {\r\n        final LevelDBTypeInfo.Index naturalIndex = ti.naturalIndex();\r\n        final Collection<LevelDBTypeInfo.Index> indices = ti.indices();\r\n\r\n        try (WriteBatch batch = db().createWriteBatch()) {\r\n          while (valueIter.hasNext()) {\r\n            final Object value = valueIter.next();\r\n\r\n            assert serializedValueIter.hasNext();\r\n            final byte[] serializedObject = serializedValueIter.next();\r\n\r\n            Object existing;\r\n            try {\r\n              existing = get(naturalIndex.entityKey(null, value), valueClass);\r\n            } catch (NoSuchElementException e) {\r\n              existing = null;\r\n            }\r\n\r\n            PrefixCache cache = new PrefixCache(value);\r\n            byte[] naturalKey = naturalIndex.toKey(naturalIndex.getValue(value));\r\n            for (LevelDBTypeInfo.Index idx : indices) {\r\n              byte[] prefix = cache.getPrefix(idx);\r\n              idx.add(batch, value, existing, serializedObject, naturalKey, prefix);\r\n            }\r\n          }\r\n          assert !serializedValueIter.hasNext();\r\n          db().write(batch);\r\n        }\r\n      }\r\n    }\r\n  }\r\n\r\n```",
        "createdAt" : "2020-07-08T20:06:43Z",
        "updatedAt" : "2020-07-08T20:08:43Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "8e44929a-c6be-4395-9909-ba4daee39442",
        "parentId" : "e93f80c8-a51e-4694-bd8d-81b633a1b397",
        "authorId" : "788d6907-ae49-42f5-95b7-ef69c1460937",
        "body" : "Thanks, this is helpful!",
        "createdAt" : "2020-07-08T21:33:42Z",
        "updatedAt" : "2020-07-08T21:33:42Z",
        "lastEditedBy" : "788d6907-ae49-42f5-95b7-ef69c1460937",
        "tags" : [
        ]
      },
      {
        "id" : "4a8c0b1a-8b4e-48e4-a09f-b75f665975ff",
        "parentId" : "e93f80c8-a51e-4694-bd8d-81b633a1b397",
        "authorId" : "788d6907-ae49-42f5-95b7-ef69c1460937",
        "body" : "Hi @mridulm, I updated your code and used it on in-memory store - leveldb switching, but only saw little switching time improvement. I am not sure if somewhere wrong.\r\n| log size, jobs and tasks per job            | 2 jobs, 400 tasks per job | 10 jobs, 400 tasks per job | 50 jobs, 400 tasks per job | 100 jobs, 400 tasks per job | 200 jobs, 400 tasks per job | 500 jobs, 400 tasks per job | 1000 jobs, 400 tasks per job | 5 jobs, 100000 tasks per job |\r\n| ------------------------------------------- | ------------------------- | -------------------------- | -------------------------- | --------------------------- | --------------------------- | --------------------------- | ---------------------------- | ---------------------------- |\r\n| original switching time                     | 1s                        | 2s                         | 4s                         | 8s                          | 16s                         | 37s                         | 65s                          | 90s                          |\r\n| switching time with write(Iterator<T> iter) | 1s                        | 1s                         | 4s                         | 7s                          | 13s                         | 34s                         | 58s                          | 84s                          |\r\n\r\nThe code:\r\n```\r\n        for (klass <- klassMap.keys().asScala) {\r\n          val it = inMemoryStore.view(klass).closeableIterator()\r\n          levelDB.write(it)\r\n        }\r\n```\r\n\r\n```\r\n  public <T> void write(Iterator<T> iter) throws Exception {\r\n    Preconditions.checkArgument(iter != null, \"Non-empty values required.\");\r\n\r\n    List<T> values = new ArrayList<>();\r\n    iter.forEachRemaining(values::add);\r\n\r\n    // Group by class, in case there are values from different classes in the iterator\r\n    // Typical usecase is for this to be a single class.\r\n    for (Map.Entry<? extends Class<?>, ? extends List<?>> entry :\r\n            values.stream().collect(Collectors.groupingBy(Object::getClass)).entrySet()) {\r\n\r\n      final Iterator<?> valueIter = entry.getValue().iterator();\r\n      final Iterator<byte[]> serializedValueIter;\r\n\r\n      {\r\n        // deserialize outside synchronized block\r\n        List<byte[]> list = new ArrayList<>(entry.getValue().size());\r\n        for (Object value : entry.getValue()) {\r\n          list.add(serializer.serialize(value));\r\n        }\r\n        serializedValueIter = list.iterator();\r\n      }\r\n\r\n      final Class<?> valueClass = entry.getKey();\r\n      final LevelDBTypeInfo ti = getTypeInfo(valueClass);\r\n\r\n      // Batching updates per type\r\n      synchronized (ti) {\r\n        final LevelDBTypeInfo.Index naturalIndex = ti.naturalIndex();\r\n        final Collection<LevelDBTypeInfo.Index> indices = ti.indices();\r\n\r\n        try (WriteBatch batch = db().createWriteBatch()) {\r\n          while (valueIter.hasNext()) {\r\n            final Object value = valueIter.next();\r\n\r\n            assert serializedValueIter.hasNext();\r\n            final byte[] serializedObject = serializedValueIter.next();\r\n\r\n            Object existing;\r\n            try {\r\n              existing = get(naturalIndex.entityKey(null, value), valueClass);\r\n            } catch (NoSuchElementException e) {\r\n              existing = null;\r\n            }\r\n\r\n            PrefixCache cache = new PrefixCache(value);\r\n            byte[] naturalKey = naturalIndex.toKey(naturalIndex.getValue(value));\r\n            for (LevelDBTypeInfo.Index idx : indices) {\r\n              byte[] prefix = cache.getPrefix(idx);\r\n              idx.add(batch, value, existing, serializedObject, naturalKey, prefix);\r\n            }\r\n          }\r\n          assert !serializedValueIter.hasNext();\r\n          db().write(batch);\r\n        }\r\n      }\r\n    }\r\n  }\r\n```\r\n\r\nI think using multiple threads to write data to leveldb might shorten the switching time but it would introduce more overhead to SHS. ",
        "createdAt" : "2020-07-09T00:11:36Z",
        "updatedAt" : "2020-07-09T00:12:35Z",
        "lastEditedBy" : "788d6907-ae49-42f5-95b7-ef69c1460937",
        "tags" : [
        ]
      },
      {
        "id" : "bf7f8422-bb96-4773-8685-768cde7bbf1d",
        "parentId" : "e93f80c8-a51e-4694-bd8d-81b633a1b397",
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "It is a function of how loaded your disk is, iops it can sustain, txn's leveldb can do concurrently.\r\n",
        "createdAt" : "2020-07-09T08:35:20Z",
        "updatedAt" : "2020-07-09T08:35:20Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "f9e36057-ef12-4539-8308-4c19ee050643",
        "parentId" : "e93f80c8-a51e-4694-bd8d-81b633a1b397",
        "authorId" : "788d6907-ae49-42f5-95b7-ef69c1460937",
        "body" : "Make sense. I am testing it on the mac which has SSD and the disk is not busy. I think the improvement might be more obvious on HDD or busy disk.  @HeartSaVioR @tgravescs Do we need to add batch write support for leveldb on this pr?",
        "createdAt" : "2020-07-09T16:04:33Z",
        "updatedAt" : "2020-07-09T16:04:33Z",
        "lastEditedBy" : "788d6907-ae49-42f5-95b7-ef69c1460937",
        "tags" : [
        ]
      },
      {
        "id" : "b96a8734-3704-46c4-a96c-d7d481aa99a6",
        "parentId" : "e93f80c8-a51e-4694-bd8d-81b633a1b397",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "I don't think it's mandatory. You can file another issue as \"improvement\" for this, but IMHO working with this is completely optional for you. I think we have already asked so many things to do.",
        "createdAt" : "2020-07-09T22:48:11Z",
        "updatedAt" : "2020-07-09T22:48:11Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "384e6478-cb4c-474d-b47f-2d56b7ad164e",
        "parentId" : "e93f80c8-a51e-4694-bd8d-81b633a1b397",
        "authorId" : "788d6907-ae49-42f5-95b7-ef69c1460937",
        "body" : "Sounds good. We can improve that afterward.",
        "createdAt" : "2020-07-09T22:57:39Z",
        "updatedAt" : "2020-07-09T22:57:39Z",
        "lastEditedBy" : "788d6907-ae49-42f5-95b7-ef69c1460937",
        "tags" : [
        ]
      }
    ],
    "commit" : "b71a923e4554e814ee923b64d1b22ce42825378c",
    "line" : 150,
    "diffHunk" : "@@ -1,1 +148,152 @@          while (it.hasNext()) {\n            levelDB.write(it.next())\n          }\n        }\n        listener.onSwitchToLevelDBSuccess()"
  }
]