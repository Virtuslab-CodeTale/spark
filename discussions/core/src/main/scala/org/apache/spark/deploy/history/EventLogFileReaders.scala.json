[
  {
    "id" : "f818b855-3da6-4f56-a6d3-b80724c1c11f",
    "prId" : 26416,
    "prUrl" : "https://github.com/apache/spark/pull/26416#pullrequestreview-331623712",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9f14aa4d-c465-4904-ae59-265059cb29c4",
        "parentId" : null,
        "authorId" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "body" : "Probably explained in code later, but want to leave the comment here in case I forget... do you ever compact an in progress file? That sounds dangerous.",
        "createdAt" : "2019-12-13T00:36:21Z",
        "updatedAt" : "2019-12-26T02:35:39Z",
        "lastEditedBy" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "tags" : [
        ]
      },
      {
        "id" : "4b80dd01-f21c-44b9-9227-75c80fc4da93",
        "parentId" : "9f14aa4d-c465-4904-ae59-265059cb29c4",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "We prevent such situation via restriction on configuration; max file to retain cannot be set to 0 which means we will don't try to compact the last file which might be in progress file.\r\n\r\nActually allowing to set it to 0 is bad for completed log as well, as you may lose everything on compaction - most of the cases all jobs should be finished.",
        "createdAt" : "2019-12-13T01:31:39Z",
        "updatedAt" : "2019-12-26T02:35:39Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "e1a6e42b73f8d58dbc0a04882f2288d2fae0dad8",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +174,178 @@  override def fileSizeForLastIndex: Long = status.getLen\n\n  override def completed: Boolean = !rootPath.getName.stripSuffix(EventLogFileWriter.COMPACTED)\n    .endsWith(EventLogFileWriter.IN_PROGRESS)\n"
  },
  {
    "id" : "bd903327-2462-4acc-9a23-61a701dcd8ac",
    "prId" : 25670,
    "prUrl" : "https://github.com/apache/spark/pull/25670#pullrequestreview-293428013",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2b70d9c9-1e09-4fc0-b4bb-55dc15857943",
        "parentId" : null,
        "authorId" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "body" : "This is not thread-safe, pretty sure this can be an issue in the SHS.",
        "createdAt" : "2019-09-26T00:16:59Z",
        "updatedAt" : "2019-10-16T20:56:19Z",
        "lastEditedBy" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "tags" : [
        ]
      }
    ],
    "commit" : "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "line" : 135,
    "diffHunk" : "@@ -1,1 +133,137 @@    val in = new BufferedInputStream(fs.open(log))\n    try {\n      val codec = codecName(log).map { c =>\n        codecMap.computeIfAbsent(c, CompressionCodec.createCodec(new SparkConf, _))\n      }"
  }
]