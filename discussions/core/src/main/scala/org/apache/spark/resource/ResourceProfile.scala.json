[
  {
    "id" : "e99d115e-b22a-4138-9d4c-10919d5aa60f",
    "prId" : 30204,
    "prUrl" : "https://github.com/apache/spark/pull/30204#pullrequestreview-530292964",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6f7a1219-2234-4ae9-aa46-d96e2c9c8b58",
        "parentId" : null,
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "Instead of creating `customResources` above, will this not work ?\r\n```\r\nval customResourceNames = execReq.map(_.id.resourceName).toSet\r\nval customResources = ereqs.requests.filter(v => customResourceNames.contains(v._1))\r\n```",
        "createdAt" : "2020-11-12T03:46:18Z",
        "updatedAt" : "2020-11-13T19:50:27Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "08e83740-d183-4180-b45a-3c066719a342",
        "parentId" : "6f7a1219-2234-4ae9-aa46-d96e2c9c8b58",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "we could but its less efficient you have to iterate over execReq a second time and then over all the requests we just added, now that should be very fast, but just thought since already iterating over it just do it at same time.  If you prefer it this way I can change it?",
        "createdAt" : "2020-11-13T16:54:51Z",
        "updatedAt" : "2020-11-13T19:50:27Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "76f6ddf6-7e98-48e4-9d33-fee9ecb521f4",
        "parentId" : "6f7a1219-2234-4ae9-aa46-d96e2c9c8b58",
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "You are right, I am assuming `ereqs.requests` is not very large; and was looking to minimize code.\r\nForgot to add 'nit' as prefix to my comment :-)\r\n\r\nUp to you if it simplifies the logic !",
        "createdAt" : "2020-11-13T18:09:11Z",
        "updatedAt" : "2020-11-13T19:50:27Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      }
    ],
    "commit" : "6f5321ffe8637ae32434ac2d5eaff476fc3e0d19",
    "line" : 68,
    "diffHunk" : "@@ -1,1 +320,324 @@      ereqs.resource(req.id.resourceName, req.amount, req.discoveryScript.orElse(\"\"),\n        req.vendor.orElse(\"\"))\n    }\n    val customResourceNames = execReq.map(_.id.resourceName).toSet\n    val customResources = ereqs.requests.filter(v => customResourceNames.contains(v._1))"
  },
  {
    "id" : "95e4e752-ef34-4611-b3c9-b667eb47683a",
    "prId" : 30204,
    "prUrl" : "https://github.com/apache/spark/pull/30204#pullrequestreview-530246684",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c5bf17b7-e300-4373-b742-d7cf94814724",
        "parentId" : null,
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "Can we replace `DefaultProfileExecutorResources` with `ExecutorResourcesOrDefaults` ?\r\nI am not sure what value it was adding which use of `ExecutorResourcesOrDefaults` cant cover.",
        "createdAt" : "2020-11-12T03:52:13Z",
        "updatedAt" : "2020-11-13T19:50:27Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "d60665d0-dbdd-4998-97c7-0f524123bf05",
        "parentId" : "c5bf17b7-e300-4373-b742-d7cf94814724",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "So they were intentionally different.  the default version doesn't have total and couple of the values are optional. This is used just to store what was in the default configs, not really do much logic on it for things like am I in a pyspark app or does the cluster manager calculate overhead differently.\r\n Once we get ExecutorResourcesOrDefaults everything there is required to be filled in as its either what is specific in the profile or falls back to the default value, or logic based on cluster manager inputs. Since its all fields are always filled out and the cluster manager code doesn't need special code to handle Optional fields.\r\nBut having said all that, really the main difference is in the optional of memoryOverheadMiB. I could use ExecutorResourcesOrDefaults for everything as long as I had a way for that to indicate not set. I could use -1 as special value if we really want to but thought having 2 was clearer.   But again I'm fine to combine as its not a big deal, what do you think?",
        "createdAt" : "2020-11-13T17:09:25Z",
        "updatedAt" : "2020-11-13T19:50:27Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "6f5321ffe8637ae32434ac2d5eaff476fc3e0d19",
    "line" : 107,
    "diffHunk" : "@@ -1,1 +380,384 @@      customResources: Map[String, ExecutorResourceRequest])\n\n  private[spark] case class DefaultProfileExecutorResources(\n      cores: Int,\n      executorMemoryMiB: Long,"
  },
  {
    "id" : "0e2ee2f6-d172-45d6-895e-534e5a569114",
    "prId" : 28972,
    "prUrl" : "https://github.com/apache/spark/pull/28972#pullrequestreview-443884530",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "39f95526-c2ba-4e1b-88fd-614a0bdfe312",
        "parentId" : null,
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "allSupportedExecutorResources needs to be updated to include this and make sure we have a test for it.",
        "createdAt" : "2020-07-06T15:53:38Z",
        "updatedAt" : "2020-07-23T21:54:32Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "3258feed-eccb-496b-a337-4bd9916a297a",
        "parentId" : "39f95526-c2ba-4e1b-88fd-614a0bdfe312",
        "authorId" : "cf8c9534-0cf3-4aad-8ead-54c363cfa86e",
        "body" : "Any hints about how the test look like? I'm unfamiliar with where this field will be used.",
        "createdAt" : "2020-07-07T01:27:33Z",
        "updatedAt" : "2020-07-23T21:54:32Z",
        "lastEditedBy" : "cf8c9534-0cf3-4aad-8ead-54c363cfa86e",
        "tags" : [
        ]
      },
      {
        "id" : "06b012e7-91da-4751-98ff-8ae61f10d339",
        "parentId" : "39f95526-c2ba-4e1b-88fd-614a0bdfe312",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "Sure, in ResourceProfileSuite, lets create a test that has every type of resource in allSupportedExecutorResources (including off heap) and then say 2 custom resources. Then just call getCustomExecutorResources to ensure that you only get the 2 custom resources back.  We may also have it check the size of allSupportedExecutorResources == 5, that way if someone modifies allSupportedExecutorResources but doesn't update the test it will fail.\r\n\r\nCould you also add a big comment   after \"// Executor resources\" that say make sure to update allSupportedExecutorResources\r\n",
        "createdAt" : "2020-07-07T13:23:03Z",
        "updatedAt" : "2020-07-23T21:54:32Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "e25dafbac32534eaa998b00da98d4e90114a2c64",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +250,254 @@  val OVERHEAD_MEM = \"memoryOverhead\"\n  val PYSPARK_MEM = \"pyspark.memory\"\n\n  // all supported spark executor resources (minus the custom resources like GPUs/FPGAs)\n  val allSupportedExecutorResources = Seq(CORES, MEMORY, OVERHEAD_MEM, PYSPARK_MEM, OFFHEAP_MEM)"
  },
  {
    "id" : "1dda543a-bfc2-4886-b9f9-7630969daccc",
    "prId" : 27313,
    "prUrl" : "https://github.com/apache/spark/pull/27313#pullrequestreview-360033870",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "191d280f-d3d4-4267-a93b-c1989f842b88",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Hey @tgravescs, I was investigating sudden warning message popped up in my local suddenly, presumably, after this fix. My Spark shell shows this warnings consistently in my local:\r\n\r\n```\r\n$ ./bin/spark-shell\r\n```\r\n```\r\n...\r\n20/02/18 11:04:56 WARN ResourceProfile: Please ensure that the number of slots available on your executors is \r\nlimited by the number of cores to task cpus and not another custom resource. If cores is not the limiting resource \r\nthen dynamic allocation will not work properly!\r\n```\r\n\r\nDo you have any idea?",
        "createdAt" : "2020-02-18T02:08:57Z",
        "updatedAt" : "2020-02-18T02:08:58Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "15f4c96b06e887913dc3c8ede8d385bfae514db9",
    "line" : 143,
    "diffHunk" : "@@ -1,1 +186,190 @@    if(!shouldCheckExecCores) {\n      // if we can't rely on the executor cores config throw a warning for user\n      logWarning(\"Please ensure that the number of slots available on your \" +\n        \"executors is limited by the number of cores to task cpus and not another \" +\n        \"custom resource. If cores is not the limiting resource then dynamic \" +"
  },
  {
    "id" : "441e71f8-4b98-4c84-8269-b6f3a031bc4e",
    "prId" : 26682,
    "prUrl" : "https://github.com/apache/spark/pull/26682#pullrequestreview-341339700",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dd3ce2bb-addc-4d90-ba95-3c2fa58cec2d",
        "parentId" : null,
        "authorId" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "body" : "Couldn't you reset it using `.set(null)`?",
        "createdAt" : "2020-01-10T18:22:11Z",
        "updatedAt" : "2020-01-17T01:51:16Z",
        "lastEditedBy" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "tags" : [
        ]
      }
    ],
    "commit" : "75091102d6bd64a61fb903c8df3edf9ca047e879",
    "line" : 144,
    "diffHunk" : "@@ -1,1 +110,114 @@\n  // The default resource profile uses the application level configs.\n  // var so that it can be reset for testing purposes.\n  @GuardedBy(\"DEFAULT_PROFILE_LOCK\")\n  private var defaultProfile: Option[ResourceProfile] = None"
  },
  {
    "id" : "3307ddd0-c967-440a-a185-08bb9ea04ee4",
    "prId" : 26682,
    "prUrl" : "https://github.com/apache/spark/pull/26682#pullrequestreview-342748029",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "702aa67d-6cb1-4244-a455-ad1516316529",
        "parentId" : null,
        "authorId" : "99e31844-a5b3-48a2-af71-412edb607a13",
        "body" : "Is this change necessary only for this PR?",
        "createdAt" : "2020-01-13T16:42:16Z",
        "updatedAt" : "2020-01-17T01:51:16Z",
        "lastEditedBy" : "99e31844-a5b3-48a2-af71-412edb607a13",
        "tags" : [
        ]
      },
      {
        "id" : "41c2a7ce-d011-4a3c-bcb3-fea249a04403",
        "parentId" : "702aa67d-6cb1-4244-a455-ad1516316529",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "yes, either that or we have to split all the ResourceProfile changes out separately. which I can do, please just let me know.",
        "createdAt" : "2020-01-13T16:46:07Z",
        "updatedAt" : "2020-01-17T01:51:16Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "8736ff3d-ad0f-4744-81d3-ea4b3057e8b0",
        "parentId" : "702aa67d-6cb1-4244-a455-ad1516316529",
        "authorId" : "99e31844-a5b3-48a2-af71-412edb607a13",
        "body" : "I am curious whether this concurrency issue also exists in the master and past branches.",
        "createdAt" : "2020-01-14T18:13:20Z",
        "updatedAt" : "2020-01-17T01:51:16Z",
        "lastEditedBy" : "99e31844-a5b3-48a2-af71-412edb607a13",
        "tags" : [
        ]
      },
      {
        "id" : "f25125c7-44e6-41ba-bf71-0f417bf0d188",
        "parentId" : "702aa67d-6cb1-4244-a455-ad1516316529",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "This is a new feature we are trying to add to 3.0, so this only exists in master branch, but the classes are all private for now, until we get the rest of pull requests for the feature in. Once the feature is completely in spark we will open up the public api to users.\r\n\r\nYou can find all of the code for the complete feature here: https://github.com/apache/spark/pull/27053",
        "createdAt" : "2020-01-14T18:32:48Z",
        "updatedAt" : "2020-01-17T01:51:16Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "75091102d6bd64a61fb903c8df3edf9ca047e879",
    "line" : 150,
    "diffHunk" : "@@ -1,1 +116,120 @@  private[spark] def getNextProfileId: Int = nextProfileId.getAndIncrement()\n\n  private[spark] def getOrCreateDefaultProfile(conf: SparkConf): ResourceProfile = {\n    DEFAULT_PROFILE_LOCK.synchronized {\n      defaultProfile match {"
  },
  {
    "id" : "8ceacf48-1b7a-418e-8471-f544b584d8f7",
    "prId" : 26682,
    "prUrl" : "https://github.com/apache/spark/pull/26682#pullrequestreview-343958574",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "98e0a9d6-6c62-4a3c-bd76-fc54e31198ec",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "In `CoarseGrainedExecutorBackend`, if `UNKNOWN_RESOURCE_PROFILE_ID` is given, we use `DEFAULT_RESOURCE_PROFILE_ID`. What is the difference between them?",
        "createdAt" : "2020-01-16T01:59:34Z",
        "updatedAt" : "2020-01-17T01:51:16Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "3232b21d-b1cb-49bf-b802-9958e6ff270b",
        "parentId" : "98e0a9d6-6c62-4a3c-bd76-fc54e31198ec",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "so the DEFAULT profile is the one created from the application level configs. UNKNOWN means it wasn't specified.  Actually in the CoarseGrainedExecutorBackend its not used anymore. before the changes to send the ResourceProfile via the RetrieveSparkAppConfig it was being used to tell which configs to read from. Before that change we had separtate configs for the resource profiles. if it was unknown it would read the application level configs, and then set to the DEFAULT.\r\nNow we can simply have it set to the DEFAULT so I'll update that",
        "createdAt" : "2020-01-16T14:35:04Z",
        "updatedAt" : "2020-01-17T01:51:16Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "bd646ded-bd42-4e10-bc8e-7d799ad78e5d",
        "parentId" : "98e0a9d6-6c62-4a3c-bd76-fc54e31198ec",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "Note that UNKNOWN is used other places in the code to really mean we don't know what the profile is. For instance when we get messages out of order for executor tracking - like a task start before an executor added, we mark it as unknown until we get to the executor added message",
        "createdAt" : "2020-01-16T14:38:19Z",
        "updatedAt" : "2020-01-17T01:51:16Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "75091102d6bd64a61fb903c8df3edf9ca047e879",
    "line" : 118,
    "diffHunk" : "@@ -1,1 +104,108 @@\n  val UNKNOWN_RESOURCE_PROFILE_ID = -1\n  val DEFAULT_RESOURCE_PROFILE_ID = 0\n\n  private lazy val nextProfileId = new AtomicInteger(0)"
  },
  {
    "id" : "36c6c7fc-13c5-4b1d-bb9c-8f3b2ff05fc5",
    "prId" : 26682,
    "prUrl" : "https://github.com/apache/spark/pull/26682#pullrequestreview-348997729",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fa93fb55-e69b-4ab4-8932-90eb27af9408",
        "parentId" : null,
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "nit: setToDefaultProfile -> setToDefaultProfile()",
        "createdAt" : "2020-01-26T08:08:00Z",
        "updatedAt" : "2020-01-26T08:51:12Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "45b516e9-26fd-48e9-bcf3-f4127db50854",
        "parentId" : "fa93fb55-e69b-4ab4-8932-90eb27af9408",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "yep, I'll update in PR 27313 if that is ok",
        "createdAt" : "2020-01-27T16:49:43Z",
        "updatedAt" : "2020-01-27T16:49:43Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "99c3921a-668d-4bd0-8f2e-6a9d1f213b74",
        "parentId" : "fa93fb55-e69b-4ab4-8932-90eb27af9408",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "https://github.com/apache/spark/pull/27313/commits/ae4db1e330ee7c415c4863ecd6a4230568236b7e",
        "createdAt" : "2020-01-27T17:22:01Z",
        "updatedAt" : "2020-01-27T17:22:01Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "c32f849a-e084-4a4e-826f-32d5ee7ed739",
        "parentId" : "fa93fb55-e69b-4ab4-8932-90eb27af9408",
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "thx, sounds good",
        "createdAt" : "2020-01-27T21:46:23Z",
        "updatedAt" : "2020-01-27T21:46:24Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      }
    ],
    "commit" : "75091102d6bd64a61fb903c8df3edf9ca047e879",
    "line" : 158,
    "diffHunk" : "@@ -1,1 +124,128 @@          val executorResources = getDefaultExecutorResources(conf)\n          val defProf = new ResourceProfile(executorResources, taskResources)\n          defProf.setToDefaultProfile\n          defaultProfile = Some(defProf)\n          logInfo(\"Default ResourceProfile created, executor resources: \" +"
  },
  {
    "id" : "64f84085-f109-4694-88ad-19727691d403",
    "prId" : 26682,
    "prUrl" : "https://github.com/apache/spark/pull/26682#pullrequestreview-348999401",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0fdf3860-7d66-48dd-b4a0-591013785c07",
        "parentId" : null,
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "nit: clearDefaultProfile -> clearDefaultProfile() ?",
        "createdAt" : "2020-01-26T08:10:37Z",
        "updatedAt" : "2020-01-26T08:51:12Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "68206657-64ea-4530-82ca-96a2192ad1a6",
        "parentId" : "0fdf3860-7d66-48dd-b4a0-591013785c07",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "yep, I'll update in PR 27313 if that is ok",
        "createdAt" : "2020-01-27T16:49:53Z",
        "updatedAt" : "2020-01-27T16:49:53Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "7e0d6fd6-16c6-4025-881b-b985c8bf11ce",
        "parentId" : "0fdf3860-7d66-48dd-b4a0-591013785c07",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "https://github.com/apache/spark/pull/27313/commits/ae4db1e330ee7c415c4863ecd6a4230568236b7e",
        "createdAt" : "2020-01-27T17:22:05Z",
        "updatedAt" : "2020-01-27T17:22:06Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "6ef1882c-ff9a-4953-b3b7-c1d43cc4ebf4",
        "parentId" : "0fdf3860-7d66-48dd-b4a0-591013785c07",
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "Sounds good",
        "createdAt" : "2020-01-27T21:49:15Z",
        "updatedAt" : "2020-01-27T21:49:16Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      }
    ],
    "commit" : "75091102d6bd64a61fb903c8df3edf9ca047e879",
    "line" : 212,
    "diffHunk" : "@@ -1,1 +164,168 @@\n  // for testing only\n  private[spark] def clearDefaultProfile: Unit = {\n    DEFAULT_PROFILE_LOCK.synchronized {\n      defaultProfile = None"
  },
  {
    "id" : "bb5bfae6-b5d2-4f38-88f1-6799bdc99329",
    "prId" : 26284,
    "prUrl" : "https://github.com/apache/spark/pull/26284#pullrequestreview-313620584",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4f6a8934-4380-4348-9306-5a9a6475cce1",
        "parentId" : null,
        "authorId" : "e913f9af-103d-40d4-976e-9aa8e12e7211",
        "body" : "cpus and cores are really the same thing, right?  its unfortunate we already chose different names for task vs. executor in existing confs.",
        "createdAt" : "2019-11-07T18:12:10Z",
        "updatedAt" : "2019-11-15T22:25:46Z",
        "lastEditedBy" : "e913f9af-103d-40d4-976e-9aa8e12e7211",
        "tags" : [
        ]
      },
      {
        "id" : "52b8b029-fcf4-4edd-b1dc-da824e06cc7f",
        "parentId" : "4f6a8934-4380-4348-9306-5a9a6475cce1",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "yes they are, I kept the consistency with the spark configs, I was trying to make it so the parameters to ExecutorResourceRequest and TaskResourceRequest matched the regular spark configs names minus the spark.executor or spark.task prefix.\r\n\r\nFor instance overhead memory in this api is memoryOverhead, which is spark.executor.memoryOverhead with spark.executor removed. Resources like GPUs are resource.gpu (spark configs spark.executor.resource.gpu.*)\r\n\r\nIf you don't think that matters we can change them here to be the same, thoughts?",
        "createdAt" : "2019-11-07T18:54:34Z",
        "updatedAt" : "2019-11-15T22:25:46Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "e1811931-a471-4d12-a19c-0288bcdbba8b",
        "parentId" : "4f6a8934-4380-4348-9306-5a9a6475cce1",
        "authorId" : "e913f9af-103d-40d4-976e-9aa8e12e7211",
        "body" : "sorry that was more just a general complaint from me about what we've already done, not really related to your change.  I think keeping it the same as the confs makes sense.",
        "createdAt" : "2019-11-07T20:37:16Z",
        "updatedAt" : "2019-11-15T22:25:46Z",
        "lastEditedBy" : "e913f9af-103d-40d4-976e-9aa8e12e7211",
        "tags" : [
        ]
      }
    ],
    "commit" : "246de3c66c8c5a656b766ccd2f9bb3df12c9da0d",
    "line" : 87,
    "diffHunk" : "@@ -1,1 +85,89 @@\n  val CPUS = \"cpus\"\n  val CORES = \"cores\"\n  val MEMORY = \"memory\"\n  val OVERHEAD_MEM = \"memoryOverhead\""
  }
]