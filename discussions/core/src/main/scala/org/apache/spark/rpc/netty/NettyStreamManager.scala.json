[
  {
    "id" : "d497beda-84bc-4945-91d5-30a009ebf380",
    "prId" : 24366,
    "prUrl" : "https://github.com/apache/spark/pull/24366#pullrequestreview-226375100",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "93a7b901-1b22-4d7a-9a52-e26d48bf5446",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Why did we add this?",
        "createdAt" : "2019-04-13T13:27:34Z",
        "updatedAt" : "2019-04-13T13:27:34Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "14681c47-4b26-44d9-9bd4-0325be6eeca3",
        "parentId" : "93a7b901-1b22-4d7a-9a52-e26d48bf5446",
        "authorId" : "b1f470c7-3ad6-4660-a8d6-37529d55b30b",
        "body" : "We refer to `[[SparkContext.addFile]]` below in scala doc. This import fix the link, otherwise scala doc cannot find where `[[SparkContext.addFile]]` should be linked to.\r\n\r\nAs the other comment of yours, adding this doesn't affect production code logic though.",
        "createdAt" : "2019-04-13T14:21:36Z",
        "updatedAt" : "2019-04-13T14:21:36Z",
        "lastEditedBy" : "b1f470c7-3ad6-4660-a8d6-37529d55b30b",
        "tags" : [
        ]
      },
      {
        "id" : "ad6147bb-7096-46db-8119-e7fb37396c2a",
        "parentId" : "93a7b901-1b22-4d7a-9a52-e26d48bf5446",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Yeah, this is a big general problem with the scaladoc. It requires imports that aren't there. I don't mind importing this but it doesn't solve the overall issue. ",
        "createdAt" : "2019-04-13T23:26:45Z",
        "updatedAt" : "2019-04-13T23:27:20Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "95bb1331-2804-4f82-94a9-0e5a70b98aa4",
        "parentId" : "93a7b901-1b22-4d7a-9a52-e26d48bf5446",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I personally work around this by fully qualified path in the doc though. I don't think this is something worth fixing alone.",
        "createdAt" : "2019-04-14T01:37:16Z",
        "updatedAt" : "2019-04-14T01:37:16Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "2d19c12996effb6608610c464d7fcb6c5776d99e",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +20,24 @@import java.util.concurrent.ConcurrentHashMap\n\nimport org.apache.spark.SparkContext\nimport org.apache.spark.network.buffer.{FileSegmentManagedBuffer, ManagedBuffer}\nimport org.apache.spark.network.server.StreamManager"
  },
  {
    "id" : "a427eb54-d7a2-43b5-93c4-7f279d23935f",
    "prId" : 24366,
    "prUrl" : "https://github.com/apache/spark/pull/24366#pullrequestreview-226372933",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "146e8a93-d8bb-4015-80d8-8fbdf0870b82",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "While these changes are still correct behavior-wise, these are java methods that are defines as zero-arg methods, so including parens is a little more correct",
        "createdAt" : "2019-04-13T23:27:17Z",
        "updatedAt" : "2019-04-13T23:27:20Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      }
    ],
    "commit" : "2d19c12996effb6608610c464d7fcb6c5776d99e",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +60,64 @@    }\n\n    if (file != null && file.isFile) {\n      new FileSegmentManagedBuffer(rpcEnv.transportConf, file, 0, file.length())\n    } else {"
  }
]