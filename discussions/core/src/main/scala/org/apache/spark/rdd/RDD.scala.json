[
  {
    "id" : "a7c16241-674a-4978-bbee-32fff3887665",
    "prId" : 31707,
    "prUrl" : "https://github.com/apache/spark/pull/31707#pullrequestreview-603390646",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2ab32c8e-10c0-4ccd-a3e3-9c1009f6265e",
        "parentId" : null,
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Should expose this given the TODO?",
        "createdAt" : "2021-03-03T21:37:32Z",
        "updatedAt" : "2021-03-05T06:53:17Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "75492c39-32ff-4a20-80bc-142c7cce81f6",
        "parentId" : "2ab32c8e-10c0-4ccd-a3e3-9c1009f6265e",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "I'd prefer not to expose it here. At least I don't hear a requirement to expose this.",
        "createdAt" : "2021-03-03T21:46:51Z",
        "updatedAt" : "2021-03-05T06:53:17Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "76f2a145-b8b7-4ae0-a301-9822d223fff3",
        "parentId" : "2ab32c8e-10c0-4ccd-a3e3-9c1009f6265e",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Sounds reasonable. Can we add a JIRA to track this and put the JIRA next to the TODO so we eventually come back and make a decision?",
        "createdAt" : "2021-03-03T21:50:16Z",
        "updatedAt" : "2021-03-05T06:53:17Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "cccef147-35e4-42ec-93ed-d47cb5489400",
        "parentId" : "2ab32c8e-10c0-4ccd-a3e3-9c1009f6265e",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Yea, sure. Let me add a JIRA here.",
        "createdAt" : "2021-03-03T21:59:25Z",
        "updatedAt" : "2021-03-05T06:53:17Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "4670eb219a868c77ab41a5b17c4acc66e3e45495",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +2026,2030 @@  // TODO: this can be per-partition. e.g. UnionRDD can have different deterministic level for\n  // different partitions.\n  private[spark] final def outputDeterministicLevel: DeterministicLevel.Value = {\n    if (isReliablyCheckpointed) {\n      DeterministicLevel.DETERMINATE"
  },
  {
    "id" : "26ddea50-268b-4e16-8cdb-eb6a37156547",
    "prId" : 31707,
    "prUrl" : "https://github.com/apache/spark/pull/31707#pullrequestreview-604848782",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1ab2035b-3770-450f-aa74-7334bee9f7b9",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Thank you for making it IDed TODO.",
        "createdAt" : "2021-03-05T06:51:21Z",
        "updatedAt" : "2021-03-05T06:53:17Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "4670eb219a868c77ab41a5b17c4acc66e3e45495",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +2023,2027 @@   * provide custom logic of calculating output deterministic level.\n   */\n  // TODO(SPARK-34612): make it public so users can set deterministic level to their custom RDDs.\n  // TODO: this can be per-partition. e.g. UnionRDD can have different deterministic level for\n  // different partitions."
  },
  {
    "id" : "cd704bb6-e62b-46e2-9be7-8da17633933f",
    "prId" : 30392,
    "prUrl" : "https://github.com/apache/spark/pull/30392#pullrequestreview-532047524",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "800ce9be-b7df-4466-9d2e-be768e238642",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "I'm not sure if I miss anything, but this looks makes more sense based on the reason in the PR description. ",
        "createdAt" : "2020-11-17T06:13:23Z",
        "updatedAt" : "2020-11-17T06:13:23Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "f82e69dd-e4d0-418b-96f6-a4cb27332d4f",
        "parentId" : "800ce9be-b7df-4466-9d2e-be768e238642",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "See what the test result is.",
        "createdAt" : "2020-11-17T06:14:07Z",
        "updatedAt" : "2020-11-17T06:14:07Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "59179f47412e13411da2bc6010b9623f333498d4",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1522,1526 @@        Array.empty\n      } else {\n        mapRDDs.treeReduce { (queue1, queue2) =>\n          queue1 ++= queue2\n          queue1"
  },
  {
    "id" : "88dd0957-2e7e-41f1-90b0-5ce0cffa6cc1",
    "prId" : 30179,
    "prUrl" : "https://github.com/apache/spark/pull/30179#pullrequestreview-519758664",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d0d3a041-d760-4886-b42a-ccc1594bb711",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Since it was removed at `toString`, it's okay to remove it from `toDebugString`.\r\n- https://github.com/apache/spark/pull/10752/files#diff-e49db126594e91593208902e13961dc09524442090801f4d6cfce7c43da13c72R42-R48",
        "createdAt" : "2020-10-29T14:59:28Z",
        "updatedAt" : "2020-10-29T14:59:28Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "a9e5fc025187b7f6576bd73e03dfc1e0cfd87f1d",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +1921,1925 @@      val storageInfo = rdd.context.getRDDStorageInfo(_.id == rdd.id).map(info =>\n        \"    CachedPartitions: %d; MemorySize: %s; DiskSize: %s\".format(\n          info.numCachedPartitions, bytesToString(info.memSize), bytesToString(info.diskSize)))\n\n      s\"$rdd [$persistence]\" +: storageInfo"
  },
  {
    "id" : "74a1b192-ce2e-4fa0-8fc1-0d775b238b74",
    "prId" : 29028,
    "prUrl" : "https://github.com/apache/spark/pull/29028#pullrequestreview-446419938",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8aebcf36-09e0-4267-b98e-0aeb4c66740d",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Then, the executor could easily be under excessive memory pressure by `repartition(1)`? \r\n\r\nActually, I think it's probably a good idea to use `treeReduce` instead as mentioned in SPARK-32212.",
        "createdAt" : "2020-07-08T13:04:51Z",
        "updatedAt" : "2020-07-15T09:45:52Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "83887da9-2ea7-4864-8991-e107242c65eb",
        "parentId" : "8aebcf36-09e0-4267-b98e-0aeb4c66740d",
        "authorId" : "d10d1d81-50d2-468f-8056-2c338a32070d",
        "body" : "Thank you very much for code review.\r\n\r\nThe analysis of several implementations is as follows:\r\n_p = rdd.getNumPartitions_\r\n_reduce - PriorityQueue_(Original implementation) => At worst, _O(p * k)_ memory may be used in the driver, but intermediate result(local TopK) merging does not need to wait until all partitions are returned. This is executed in parallel.\r\n\r\n_repartition(1) - guava.QuickSelect_ => The merging takes place in the executor. The merging algorithm itself uses _O(k)_ extra memory, and the time complexity is _O(p * k)_. The intermediate result data is from ShuffleBlockFetcherIterator of spark shuffle, the iterator does not use too much memory.\r\n\r\n_treeReduce(depth=2) - guava.QuickSelect_ => At worst, _O(sqrt(p) * k)_ memory may be used in the driver. The memory usage in the executor is the same as _repartition(1)_. Compared with _repartition(1)_ it increases the parallelism of local TopK merging, which can improve the speed of merging under very large amount of data, but it uses more memory on the driver.\r\n\r\nThe number of intermediate results that need to be merged is _p * k_. in general, this number is not very large, and the non parallel _O(p * k)_ merging algorithm is acceptable.\r\nSo I think maybe _repartition(1)_  is a better choice.\r\n",
        "createdAt" : "2020-07-10T13:53:28Z",
        "updatedAt" : "2020-07-15T09:45:52Z",
        "lastEditedBy" : "d10d1d81-50d2-468f-8056-2c338a32070d",
        "tags" : [
        ]
      }
    ],
    "commit" : "afacdfa37505df46116d191e7ba7d4904707e218",
    "line" : 45,
    "diffHunk" : "@@ -1,1 +1538,1542 @@        mapPartitions { items =>\n          collectionUtils.takeOrdered(items, num)(ord)\n        }.repartition(1).mapPartitions { items =>\n          collectionUtils.takeOrdered(items, num)(ord)\n        }.collect()"
  },
  {
    "id" : "4c9f991d-29a3-437d-804e-d1a08df1573d",
    "prId" : 28697,
    "prUrl" : "https://github.com/apache/spark/pull/28697#pullrequestreview-422680268",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "00527862-025c-4714-9df5-5733cd65d0d0",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Thanks for addressing this issue. I was about to suggest to keep it same `Evolving` but I suspect there's a reason for it :-). LGTM",
        "createdAt" : "2020-06-02T00:57:00Z",
        "updatedAt" : "2020-06-02T00:57:27Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "db8e4c12-f0b5-476b-a0e6-fedac7db64d5",
        "parentId" : "00527862-025c-4714-9df5-5733cd65d0d0",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "this one I just missed originally",
        "createdAt" : "2020-06-02T13:34:09Z",
        "updatedAt" : "2020-06-02T13:34:10Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "7c2e2b7bfecb760ba9e7fc571c4deb2fa89e3daa",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1781,1785 @@   * calculate the RDD.\n   */\n  @Experimental\n  @Since(\"3.1.0\")\n  def withResources(rp: ResourceProfile): this.type = {"
  },
  {
    "id" : "f619518b-5dfd-4f57-9417-0ec22ecd779e",
    "prId" : 28488,
    "prUrl" : "https://github.com/apache/spark/pull/28488#pullrequestreview-411376650",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "583a8562-8246-4867-a790-d402435346b2",
        "parentId" : null,
        "authorId" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "body" : "Should there be a default value since there is already `def toLocalIterator` above?",
        "createdAt" : "2020-05-13T21:28:38Z",
        "updatedAt" : "2020-05-14T00:03:53Z",
        "lastEditedBy" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "tags" : [
        ]
      },
      {
        "id" : "e1341900-6725-4fab-bcf1-3b3721400e77",
        "parentId" : "583a8562-8246-4867-a790-d402435346b2",
        "authorId" : "f287448f-34cc-4b9a-886c-ee793c2946c3",
        "body" : "the problem is the existing method is without ().\r\nif i reused it and added default value, it'd bring source incompatibility",
        "createdAt" : "2020-05-13T23:55:28Z",
        "updatedAt" : "2020-05-14T00:03:53Z",
        "lastEditedBy" : "f287448f-34cc-4b9a-886c-ee793c2946c3",
        "tags" : [
        ]
      }
    ],
    "commit" : "6c3856d5d82de3f98cd0dd93e951b8355e93fe7f",
    "line" : 31,
    "diffHunk" : "@@ -1,1 +1047,1051 @@   * recomputing the input RDD should be cached first.\n   */\n  def toLocalIterator(prefetchPartitions: Boolean = false): Iterator[T] = withScope {\n    new PrefetchingIterator(this, prefetchPartitions).flatMap(data => data)\n  }"
  },
  {
    "id" : "854d28f2-7356-4e97-b34a-5589ef9b4d12",
    "prId" : 28488,
    "prUrl" : "https://github.com/apache/spark/pull/28488#pullrequestreview-411376901",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c45f9e70-b700-4ca8-a120-167e4c6cce8b",
        "parentId" : null,
        "authorId" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "body" : "why `flatMap` here? doesn't look like anything is being mapped",
        "createdAt" : "2020-05-13T21:29:42Z",
        "updatedAt" : "2020-05-14T00:03:53Z",
        "lastEditedBy" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "tags" : [
        ]
      },
      {
        "id" : "d15c6c91-d557-47e6-a726-01c477d80647",
        "parentId" : "c45f9e70-b700-4ca8-a120-167e4c6cce8b",
        "authorId" : "f287448f-34cc-4b9a-886c-ee793c2946c3",
        "body" : "we need to flatten Iterator[array[T][ to Iterator[T]",
        "createdAt" : "2020-05-13T23:56:12Z",
        "updatedAt" : "2020-05-14T00:03:53Z",
        "lastEditedBy" : "f287448f-34cc-4b9a-886c-ee793c2946c3",
        "tags" : [
        ]
      }
    ],
    "commit" : "6c3856d5d82de3f98cd0dd93e951b8355e93fe7f",
    "line" : 32,
    "diffHunk" : "@@ -1,1 +1048,1052 @@   */\n  def toLocalIterator(prefetchPartitions: Boolean = false): Iterator[T] = withScope {\n    new PrefetchingIterator(this, prefetchPartitions).flatMap(data => data)\n  }\n"
  },
  {
    "id" : "a4630f94-e982-4ede-844f-5eac47904697",
    "prId" : 28108,
    "prUrl" : "https://github.com/apache/spark/pull/28108#pullrequestreview-388753379",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6b43d371-7cd0-4d4a-a780-2c6bcc59d996",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Not related to this PR but shall we remove `@Experimental` and `@Since(\"3.0.0\")` at `getResourceProfile`, `withResources`, etc. (probably in a separate PR)? It's counterintuitive to mark it `private` but annotate it as APIs.",
        "createdAt" : "2020-04-03T09:13:09Z",
        "updatedAt" : "2020-04-03T09:13:09Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "4e21e260-1d4d-4ad5-9c94-28cd7ab45326",
        "parentId" : "6b43d371-7cd0-4d4a-a780-2c6bcc59d996",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "We should annotate it properly when we actually make it public. I think I pointed out somewhere but the counter argument was that it's going to be public in Spark 3.0. But seems making public is not going to happen in Spark 3.0 (?).",
        "createdAt" : "2020-04-03T09:14:56Z",
        "updatedAt" : "2020-04-03T09:14:57Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "4c9b7e1c-79b3-45d8-908f-2483af18651f",
        "parentId" : "6b43d371-7cd0-4d4a-a780-2c6bcc59d996",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "cc @tgravescs ",
        "createdAt" : "2020-04-03T12:27:55Z",
        "updatedAt" : "2020-04-03T12:27:55Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "0fa45572-0afb-4229-a84a-2934ea87ea69",
        "parentId" : "6b43d371-7cd0-4d4a-a780-2c6bcc59d996",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "correct, it won't be public in 3.0 as the feature didn't get completely in. I have a jira to make them public in 3.1 and will update the tags at that point. If you want to remove them for 3.0 that is fine.",
        "createdAt" : "2020-04-03T13:06:58Z",
        "updatedAt" : "2020-04-03T13:06:58Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "e76c3c8c-b5fd-4801-92a8-0c710bc0df9b",
        "parentId" : "6b43d371-7cd0-4d4a-a780-2c6bcc59d996",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Let's fix then. I don't think it's right to keep invalid things but just for the future or just in case. Things ideally happen in-place.",
        "createdAt" : "2020-04-03T22:57:32Z",
        "updatedAt" : "2020-04-03T23:02:02Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "f25c91ea-7dd5-461a-951a-eb9c0c38b8d0",
        "parentId" : "6b43d371-7cd0-4d4a-a780-2c6bcc59d996",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@tgravescs, mind fixing them please? are `withResources` and `getResourceProfile` all instances to fix?",
        "createdAt" : "2020-04-03T23:01:15Z",
        "updatedAt" : "2020-04-03T23:01:15Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "355255e2-9824-4f15-b267-fdee37dae874",
        "parentId" : "6b43d371-7cd0-4d4a-a780-2c6bcc59d996",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "sure, I'll put up pr after a bit.",
        "createdAt" : "2020-04-06T15:42:26Z",
        "updatedAt" : "2020-04-06T15:42:27Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "fb07e9ad-778d-4314-abf7-bcde0e8be928",
        "parentId" : "6b43d371-7cd0-4d4a-a780-2c6bcc59d996",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "Actually sorry these aren't in spark 3.0 branch, its only in master, so no reason to change as I will be doing a jira in the next week or so to make these public",
        "createdAt" : "2020-04-06T16:03:38Z",
        "updatedAt" : "2020-04-06T16:03:39Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "99411813-4a97-4d0f-bb58-a9631571cfdd",
        "parentId" : "6b43d371-7cd0-4d4a-a780-2c6bcc59d996",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I see. Thanks for double checking @tgravescs.",
        "createdAt" : "2020-04-07T03:04:08Z",
        "updatedAt" : "2020-04-07T03:04:08Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "1ed3b12c9d41b3af01ce376da2f7bacc8718cf32",
    "line" : 1,
    "diffHunk" : "@@ -1,1 +1745,1749 @@  // =======================================================================\n  // Other internal methods and fields\n  // =======================================================================\n\n  private var storageLevel: StorageLevel = StorageLevel.NONE"
  },
  {
    "id" : "da0468eb-2928-4836-b84e-18966867dcfa",
    "prId" : 28038,
    "prUrl" : "https://github.com/apache/spark/pull/28038#pullrequestreview-388616585",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "65b465e7-8e48-48b8-a680-79f10a442ca9",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Seems it's not thread safe to do so?",
        "createdAt" : "2020-03-27T02:01:38Z",
        "updatedAt" : "2020-04-06T22:07:58Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "a4b2cb0c-076b-447e-8c36-f9fd9430fbcf",
        "parentId" : "65b465e7-8e48-48b8-a680-79f10a442ca9",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "I think it's fine, multiple concurrent cleanups would just log an error.",
        "createdAt" : "2020-03-30T22:12:20Z",
        "updatedAt" : "2020-04-06T22:07:58Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "3775c424-7c72-4ed6-9607-a8f30c7397c0",
        "parentId" : "65b465e7-8e48-48b8-a680-79f10a442ca9",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "It will also cause driver to send remove message to executors, though no fatal error there.",
        "createdAt" : "2020-03-31T02:12:55Z",
        "updatedAt" : "2020-04-06T22:07:58Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "aa1ac694-a1b1-465b-821e-3412047ca85d",
        "parentId" : "65b465e7-8e48-48b8-a680-79f10a442ca9",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Yes. Talking with @dbtsai he wanted to add a lock on the blocks inside of `doCleanupShuffle`, but given that the only price is duplicated messages to the executors I'm not sure its worth the overhead of keeping track of that many locks.",
        "createdAt" : "2020-04-02T18:34:55Z",
        "updatedAt" : "2020-04-06T22:07:58Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "3d00d68e-8a5c-40a6-ac24-e7464ba195e8",
        "parentId" : "65b465e7-8e48-48b8-a680-79f10a442ca9",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "How about this:\r\n\r\nIn `doCleanupShuffle`, we could call `if (MapOutputTrackerMaster.containsShuffle) {...}` before the cleaning.",
        "createdAt" : "2020-04-03T02:17:32Z",
        "updatedAt" : "2020-04-06T22:07:58Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "0dc8af87-9880-48ee-86aa-9aec1d34845d",
        "parentId" : "65b465e7-8e48-48b8-a680-79f10a442ca9",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Yeah ok, get is fairly cheap on a `ConcurrentHashmap` I think that balances the tradeoffs well.",
        "createdAt" : "2020-04-06T21:05:35Z",
        "updatedAt" : "2020-04-06T22:07:58Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "29019c88e29b85a2d549dd889c018896fa3e10af",
    "line" : 69,
    "diffHunk" : "@@ -1,1 +1746,1750 @@        if (dep.isInstanceOf[ShuffleDependency[_, _, _]]) {\n          val shuffleId = dep.asInstanceOf[ShuffleDependency[_, _, _]].shuffleId\n          cleaner.doCleanupShuffle(shuffleId, blocking)\n        }\n        val rdd = dep.rdd"
  },
  {
    "id" : "88f50f11-2866-4cfd-8e06-c4e0a2cd2581",
    "prId" : 28038,
    "prUrl" : "https://github.com/apache/spark/pull/28038#pullrequestreview-388965687",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0b4b71be-2c1b-43e2-a47d-31a8666896af",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "hmm...why we only do clean for `None` level rdd?\r\n",
        "createdAt" : "2020-03-31T02:38:01Z",
        "updatedAt" : "2020-04-06T22:07:58Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "459e38c7-fab6-49db-9113-6634c2b9d1bd",
        "parentId" : "0b4b71be-2c1b-43e2-a47d-31a8666896af",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "If someone has persisted an RDD and not unpersisted it we assume they intend to reuse it, and cleaning the shuffle files would be wise. This is important with the ALS code path (the original reason for this feature) so there is a cut point in the graph and we don't go unexpectedly cleaning up users shuffle files.",
        "createdAt" : "2020-04-02T18:30:37Z",
        "updatedAt" : "2020-04-06T22:07:58Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "ad063e77-7f6c-430e-9603-13cfc87b47df",
        "parentId" : "0b4b71be-2c1b-43e2-a47d-31a8666896af",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "> cleaning the shuffle files would be wise. \r\n\r\nIf I understand your explanation correctly here, it shall be `not wise`, no?\r\n\r\nAnd if so, IIUC, RDD persist is not backed by shuffle files but an extra copy in memory or disk? ",
        "createdAt" : "2020-04-03T01:56:44Z",
        "updatedAt" : "2020-04-06T22:07:58Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "d94f263b-05c7-4be0-84b3-a9f157399915",
        "parentId" : "0b4b71be-2c1b-43e2-a47d-31a8666896af",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Correct, sorry should have written `not wise`. RDD persistence is flexible in what it might be backed by, but generally, it has a higher chance of being evicted than shuffle files.",
        "createdAt" : "2020-04-06T21:26:22Z",
        "updatedAt" : "2020-04-06T22:07:58Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "42bc1003-6682-4108-8721-68e77b8b7a3e",
        "parentId" : "0b4b71be-2c1b-43e2-a47d-31a8666896af",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "I see.",
        "createdAt" : "2020-04-07T09:59:28Z",
        "updatedAt" : "2020-04-07T09:59:28Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "29019c88e29b85a2d549dd889c018896fa3e10af",
    "line" : 73,
    "diffHunk" : "@@ -1,1 +1750,1754 @@        val rdd = dep.rdd\n        val rddDepsOpt = rdd.internalDependencies\n        if (rdd.getStorageLevel == StorageLevel.NONE) {\n          rddDepsOpt.foreach(deps => deps.foreach(cleanEagerly))\n        }"
  },
  {
    "id" : "c905f51f-44d8-478b-976b-a1a34b002acd",
    "prId" : 27773,
    "prUrl" : "https://github.com/apache/spark/pull/27773#pullrequestreview-396519520",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a3b2d6f2-300b-4c92-ac5c-f9c2d4975c38",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "3.0.0 => 3.1.0",
        "createdAt" : "2020-04-18T14:38:39Z",
        "updatedAt" : "2020-04-18T14:38:39Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "410093a8-1f05-425a-bb5a-77543f132d6f",
        "parentId" : "a3b2d6f2-300b-4c92-ac5c-f9c2d4975c38",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "these will be fixed with SPARK-29150",
        "createdAt" : "2020-04-20T14:41:28Z",
        "updatedAt" : "2020-04-20T14:41:28Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "2a4a73385ee948d81e51091990a37ebbc8010f61",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +1726,1730 @@  // PRIVATE for now, added for testing purposes, will be made public with SPARK-29150\n  @Experimental\n  @Since(\"3.0.0\")\n  private[spark] def withResources(rp: ResourceProfile): this.type = {\n    resourceProfile = Option(rp)"
  },
  {
    "id" : "f44e1348-02b8-42e0-b053-bcaa80c3e8e9",
    "prId" : 27773,
    "prUrl" : "https://github.com/apache/spark/pull/27773#pullrequestreview-396519619",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "140095d1-afd7-4d00-9257-7d4d4410fe33",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "3.0.0 => 3.1.0",
        "createdAt" : "2020-04-18T14:38:47Z",
        "updatedAt" : "2020-04-18T14:38:47Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "b3264fb5-7961-4dc8-ae88-8765822b7baa",
        "parentId" : "140095d1-afd7-4d00-9257-7d4d4410fe33",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "these will be fixed with SPARK-29150",
        "createdAt" : "2020-04-20T14:41:34Z",
        "updatedAt" : "2020-04-20T14:41:34Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "2a4a73385ee948d81e51091990a37ebbc8010f61",
    "line" : 34,
    "diffHunk" : "@@ -1,1 +1740,1744 @@  // PRIVATE for now, added for testing purposes, will be made public with SPARK-29150\n  @Experimental\n  @Since(\"3.0.0\")\n  private[spark] def getResourceProfile(): ResourceProfile = resourceProfile.getOrElse(null)\n"
  },
  {
    "id" : "92aa7a67-000a-46ae-ae5a-82a94998ff45",
    "prId" : 27399,
    "prUrl" : "https://github.com/apache/spark/pull/27399#pullrequestreview-351749743",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "66c95ca5-b654-4203-8f06-915f19343588",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Yeah anything Serializable is fine. Integer.valueOf(0) isn't ideal because it's going to be a shared object. Anything would work - String(\"\") for example. This seems OK to just slightly odd looking. It could also be a proper Lock, used as a lock, but whatever.",
        "createdAt" : "2020-01-30T13:17:28Z",
        "updatedAt" : "2020-01-30T13:17:28Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "59d6e66b-3022-4ba7-b4ff-fef58ff43a85",
        "parentId" : "66c95ca5-b654-4203-8f06-915f19343588",
        "authorId" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "body" : "@srowen Do you want me to modify this?",
        "createdAt" : "2020-01-31T20:00:54Z",
        "updatedAt" : "2020-01-31T20:00:54Z",
        "lastEditedBy" : "5c8bf89e-8bb3-4151-8b92-286da26c827e",
        "tags" : [
        ]
      },
      {
        "id" : "db4b0792-0a4d-4ed4-a882-aa82b6f64c03",
        "parentId" : "66c95ca5-b654-4203-8f06-915f19343588",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Seems OK to me as a minimal change that shouldn't matter for behavior.",
        "createdAt" : "2020-01-31T20:10:46Z",
        "updatedAt" : "2020-01-31T20:10:46Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      }
    ],
    "commit" : "47c8d5d50faea03a27eb1eb9d2e92e9f92e881bb",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +238,242 @@   * that only happens on the driver).\n   */\n  private val stateLock = new Serializable {}\n\n  // Our dependencies and partitions will be gotten by calling subclass's methods below, and will"
  },
  {
    "id" : "2765a5b0-a6c4-4b74-8303-7dc57ca91755",
    "prId" : 26440,
    "prUrl" : "https://github.com/apache/spark/pull/26440#pullrequestreview-337467269",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "998d8888-eb7f-43d2-9c8a-bf1ca6dde55d",
        "parentId" : null,
        "authorId" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "body" : "not related to the change?",
        "createdAt" : "2019-11-15T22:13:18Z",
        "updatedAt" : "2020-02-13T23:10:10Z",
        "lastEditedBy" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "tags" : [
        ]
      },
      {
        "id" : "56b68270-135e-4feb-820a-1a4e5645108f",
        "parentId" : "998d8888-eb7f-43d2-9c8a-bf1ca6dde55d",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "True, I can back it out if needed, but I was reading this code path and thought this was a useful comment to add while I was in here.",
        "createdAt" : "2020-01-01T01:12:56Z",
        "updatedAt" : "2020-02-13T23:10:10Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "af550303e0b929dc9f7436bcfb36438ff36b8208",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +362,366 @@      computeOrReadCheckpoint(partition, context)\n    }) match {\n      // Block hit.\n      case Left(blockResult) =>\n        if (readCachedBlock) {"
  }
]