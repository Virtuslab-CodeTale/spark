[
  {
    "id" : "a7c16241-674a-4978-bbee-32fff3887665",
    "prId" : 31707,
    "prUrl" : "https://github.com/apache/spark/pull/31707#pullrequestreview-603390646",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2ab32c8e-10c0-4ccd-a3e3-9c1009f6265e",
        "parentId" : null,
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Should expose this given the TODO?",
        "createdAt" : "2021-03-03T21:37:32Z",
        "updatedAt" : "2021-03-05T06:53:17Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "75492c39-32ff-4a20-80bc-142c7cce81f6",
        "parentId" : "2ab32c8e-10c0-4ccd-a3e3-9c1009f6265e",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "I'd prefer not to expose it here. At least I don't hear a requirement to expose this.",
        "createdAt" : "2021-03-03T21:46:51Z",
        "updatedAt" : "2021-03-05T06:53:17Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "76f2a145-b8b7-4ae0-a301-9822d223fff3",
        "parentId" : "2ab32c8e-10c0-4ccd-a3e3-9c1009f6265e",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Sounds reasonable. Can we add a JIRA to track this and put the JIRA next to the TODO so we eventually come back and make a decision?",
        "createdAt" : "2021-03-03T21:50:16Z",
        "updatedAt" : "2021-03-05T06:53:17Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "cccef147-35e4-42ec-93ed-d47cb5489400",
        "parentId" : "2ab32c8e-10c0-4ccd-a3e3-9c1009f6265e",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Yea, sure. Let me add a JIRA here.",
        "createdAt" : "2021-03-03T21:59:25Z",
        "updatedAt" : "2021-03-05T06:53:17Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "4670eb219a868c77ab41a5b17c4acc66e3e45495",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +2026,2030 @@  // TODO: this can be per-partition. e.g. UnionRDD can have different deterministic level for\n  // different partitions.\n  private[spark] final def outputDeterministicLevel: DeterministicLevel.Value = {\n    if (isReliablyCheckpointed) {\n      DeterministicLevel.DETERMINATE"
  },
  {
    "id" : "26ddea50-268b-4e16-8cdb-eb6a37156547",
    "prId" : 31707,
    "prUrl" : "https://github.com/apache/spark/pull/31707#pullrequestreview-604848782",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1ab2035b-3770-450f-aa74-7334bee9f7b9",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Thank you for making it IDed TODO.",
        "createdAt" : "2021-03-05T06:51:21Z",
        "updatedAt" : "2021-03-05T06:53:17Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "4670eb219a868c77ab41a5b17c4acc66e3e45495",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +2023,2027 @@   * provide custom logic of calculating output deterministic level.\n   */\n  // TODO(SPARK-34612): make it public so users can set deterministic level to their custom RDDs.\n  // TODO: this can be per-partition. e.g. UnionRDD can have different deterministic level for\n  // different partitions."
  },
  {
    "id" : "cd704bb6-e62b-46e2-9be7-8da17633933f",
    "prId" : 30392,
    "prUrl" : "https://github.com/apache/spark/pull/30392#pullrequestreview-532047524",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "800ce9be-b7df-4466-9d2e-be768e238642",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "I'm not sure if I miss anything, but this looks makes more sense based on the reason in the PR description. ",
        "createdAt" : "2020-11-17T06:13:23Z",
        "updatedAt" : "2020-11-17T06:13:23Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "f82e69dd-e4d0-418b-96f6-a4cb27332d4f",
        "parentId" : "800ce9be-b7df-4466-9d2e-be768e238642",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "See what the test result is.",
        "createdAt" : "2020-11-17T06:14:07Z",
        "updatedAt" : "2020-11-17T06:14:07Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "59179f47412e13411da2bc6010b9623f333498d4",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1522,1526 @@        Array.empty\n      } else {\n        mapRDDs.treeReduce { (queue1, queue2) =>\n          queue1 ++= queue2\n          queue1"
  },
  {
    "id" : "88dd0957-2e7e-41f1-90b0-5ce0cffa6cc1",
    "prId" : 30179,
    "prUrl" : "https://github.com/apache/spark/pull/30179#pullrequestreview-519758664",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d0d3a041-d760-4886-b42a-ccc1594bb711",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Since it was removed at `toString`, it's okay to remove it from `toDebugString`.\r\n- https://github.com/apache/spark/pull/10752/files#diff-e49db126594e91593208902e13961dc09524442090801f4d6cfce7c43da13c72R42-R48",
        "createdAt" : "2020-10-29T14:59:28Z",
        "updatedAt" : "2020-10-29T14:59:28Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "a9e5fc025187b7f6576bd73e03dfc1e0cfd87f1d",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +1921,1925 @@      val storageInfo = rdd.context.getRDDStorageInfo(_.id == rdd.id).map(info =>\n        \"    CachedPartitions: %d; MemorySize: %s; DiskSize: %s\".format(\n          info.numCachedPartitions, bytesToString(info.memSize), bytesToString(info.diskSize)))\n\n      s\"$rdd [$persistence]\" +: storageInfo"
  },
  {
    "id" : "74a1b192-ce2e-4fa0-8fc1-0d775b238b74",
    "prId" : 29028,
    "prUrl" : "https://github.com/apache/spark/pull/29028#pullrequestreview-446419938",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8aebcf36-09e0-4267-b98e-0aeb4c66740d",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Then, the executor could easily be under excessive memory pressure by `repartition(1)`? \r\n\r\nActually, I think it's probably a good idea to use `treeReduce` instead as mentioned in SPARK-32212.",
        "createdAt" : "2020-07-08T13:04:51Z",
        "updatedAt" : "2020-07-15T09:45:52Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "83887da9-2ea7-4864-8991-e107242c65eb",
        "parentId" : "8aebcf36-09e0-4267-b98e-0aeb4c66740d",
        "authorId" : "d10d1d81-50d2-468f-8056-2c338a32070d",
        "body" : "Thank you very much for code review.\r\n\r\nThe analysis of several implementations is as follows:\r\n_p = rdd.getNumPartitions_\r\n_reduce - PriorityQueue_(Original implementation) => At worst, _O(p * k)_ memory may be used in the driver, but intermediate result(local TopK) merging does not need to wait until all partitions are returned. This is executed in parallel.\r\n\r\n_repartition(1) - guava.QuickSelect_ => The merging takes place in the executor. The merging algorithm itself uses _O(k)_ extra memory, and the time complexity is _O(p * k)_. The intermediate result data is from ShuffleBlockFetcherIterator of spark shuffle, the iterator does not use too much memory.\r\n\r\n_treeReduce(depth=2) - guava.QuickSelect_ => At worst, _O(sqrt(p) * k)_ memory may be used in the driver. The memory usage in the executor is the same as _repartition(1)_. Compared with _repartition(1)_ it increases the parallelism of local TopK merging, which can improve the speed of merging under very large amount of data, but it uses more memory on the driver.\r\n\r\nThe number of intermediate results that need to be merged is _p * k_. in general, this number is not very large, and the non parallel _O(p * k)_ merging algorithm is acceptable.\r\nSo I think maybe _repartition(1)_  is a better choice.\r\n",
        "createdAt" : "2020-07-10T13:53:28Z",
        "updatedAt" : "2020-07-15T09:45:52Z",
        "lastEditedBy" : "d10d1d81-50d2-468f-8056-2c338a32070d",
        "tags" : [
        ]
      }
    ],
    "commit" : "afacdfa37505df46116d191e7ba7d4904707e218",
    "line" : 45,
    "diffHunk" : "@@ -1,1 +1538,1542 @@        mapPartitions { items =>\n          collectionUtils.takeOrdered(items, num)(ord)\n        }.repartition(1).mapPartitions { items =>\n          collectionUtils.takeOrdered(items, num)(ord)\n        }.collect()"
  },
  {
    "id" : "4c9f991d-29a3-437d-804e-d1a08df1573d",
    "prId" : 28697,
    "prUrl" : "https://github.com/apache/spark/pull/28697#pullrequestreview-422680268",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "00527862-025c-4714-9df5-5733cd65d0d0",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Thanks for addressing this issue. I was about to suggest to keep it same `Evolving` but I suspect there's a reason for it :-). LGTM",
        "createdAt" : "2020-06-02T00:57:00Z",
        "updatedAt" : "2020-06-02T00:57:27Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "db8e4c12-f0b5-476b-a0e6-fedac7db64d5",
        "parentId" : "00527862-025c-4714-9df5-5733cd65d0d0",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "this one I just missed originally",
        "createdAt" : "2020-06-02T13:34:09Z",
        "updatedAt" : "2020-06-02T13:34:10Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "7c2e2b7bfecb760ba9e7fc571c4deb2fa89e3daa",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1781,1785 @@   * calculate the RDD.\n   */\n  @Experimental\n  @Since(\"3.1.0\")\n  def withResources(rp: ResourceProfile): this.type = {"
  },
  {
    "id" : "f619518b-5dfd-4f57-9417-0ec22ecd779e",
    "prId" : 28488,
    "prUrl" : "https://github.com/apache/spark/pull/28488#pullrequestreview-411376650",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "583a8562-8246-4867-a790-d402435346b2",
        "parentId" : null,
        "authorId" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "body" : "Should there be a default value since there is already `def toLocalIterator` above?",
        "createdAt" : "2020-05-13T21:28:38Z",
        "updatedAt" : "2020-05-14T00:03:53Z",
        "lastEditedBy" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "tags" : [
        ]
      },
      {
        "id" : "e1341900-6725-4fab-bcf1-3b3721400e77",
        "parentId" : "583a8562-8246-4867-a790-d402435346b2",
        "authorId" : "f287448f-34cc-4b9a-886c-ee793c2946c3",
        "body" : "the problem is the existing method is without ().\r\nif i reused it and added default value, it'd bring source incompatibility",
        "createdAt" : "2020-05-13T23:55:28Z",
        "updatedAt" : "2020-05-14T00:03:53Z",
        "lastEditedBy" : "f287448f-34cc-4b9a-886c-ee793c2946c3",
        "tags" : [
        ]
      }
    ],
    "commit" : "6c3856d5d82de3f98cd0dd93e951b8355e93fe7f",
    "line" : 31,
    "diffHunk" : "@@ -1,1 +1047,1051 @@   * recomputing the input RDD should be cached first.\n   */\n  def toLocalIterator(prefetchPartitions: Boolean = false): Iterator[T] = withScope {\n    new PrefetchingIterator(this, prefetchPartitions).flatMap(data => data)\n  }"
  },
  {
    "id" : "854d28f2-7356-4e97-b34a-5589ef9b4d12",
    "prId" : 28488,
    "prUrl" : "https://github.com/apache/spark/pull/28488#pullrequestreview-411376901",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c45f9e70-b700-4ca8-a120-167e4c6cce8b",
        "parentId" : null,
        "authorId" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "body" : "why `flatMap` here? doesn't look like anything is being mapped",
        "createdAt" : "2020-05-13T21:29:42Z",
        "updatedAt" : "2020-05-14T00:03:53Z",
        "lastEditedBy" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "tags" : [
        ]
      },
      {
        "id" : "d15c6c91-d557-47e6-a726-01c477d80647",
        "parentId" : "c45f9e70-b700-4ca8-a120-167e4c6cce8b",
        "authorId" : "f287448f-34cc-4b9a-886c-ee793c2946c3",
        "body" : "we need to flatten Iterator[array[T][ to Iterator[T]",
        "createdAt" : "2020-05-13T23:56:12Z",
        "updatedAt" : "2020-05-14T00:03:53Z",
        "lastEditedBy" : "f287448f-34cc-4b9a-886c-ee793c2946c3",
        "tags" : [
        ]
      }
    ],
    "commit" : "6c3856d5d82de3f98cd0dd93e951b8355e93fe7f",
    "line" : 32,
    "diffHunk" : "@@ -1,1 +1048,1052 @@   */\n  def toLocalIterator(prefetchPartitions: Boolean = false): Iterator[T] = withScope {\n    new PrefetchingIterator(this, prefetchPartitions).flatMap(data => data)\n  }\n"
  }
]