[
  {
    "id" : "1189c1d8-8b0c-4761-b582-ba34ed647c1f",
    "prId" : 31611,
    "prUrl" : "https://github.com/apache/spark/pull/31611#pullrequestreview-618140420",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3e31ce27-f4db-4d8c-a021-d890a858a70d",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Do we need both params? doesn't this have summaries iff quantiles isn't empty?",
        "createdAt" : "2021-03-10T14:42:07Z",
        "updatedAt" : "2021-03-10T14:42:07Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "97414483-467f-4883-a589-d313cb75f992",
        "parentId" : "3e31ce27-f4db-4d8c-a021-d890a858a70d",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> Do we need both params? doesn't this have summaries iff quantiles isn't empty?\r\n\r\ntwo params is necessary, yea, we need a check if quantiles is empty, we apply a default value",
        "createdAt" : "2021-03-11T03:09:07Z",
        "updatedAt" : "2021-03-11T03:09:07Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "1bd1d2c7-8ed6-4c8a-a128-17169b19e036",
        "parentId" : "3e31ce27-f4db-4d8c-a021-d890a858a70d",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "I don't follow - when would say withSummaries be true and quantiles be empty for example? or when is it false while quantiles is non-empty?",
        "createdAt" : "2021-03-11T03:14:32Z",
        "updatedAt" : "2021-03-11T03:14:32Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "e4c53c47-82fa-4e80-a6b2-fae43dfed8f1",
        "parentId" : "3e31ce27-f4db-4d8c-a021-d890a858a70d",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> when would say withSummaries be true and quantiles be empty for example?\r\n```\r\n?withSummaries=true&quantiles=\r\n```\r\nMaybe this case,  I need to try and test, then update the code.\r\n\r\n> when is it false while quantiles is non-empty?\r\n\r\nWe just ignore this, since quantiles only effect when withSummaries=true",
        "createdAt" : "2021-03-11T03:27:09Z",
        "updatedAt" : "2021-03-11T03:27:09Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "4d01fca6-d702-4f1a-9007-c951ed7a7ccb",
        "parentId" : "3e31ce27-f4db-4d8c-a021-d890a858a70d",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "![image](https://user-images.githubusercontent.com/46485123/111254942-194ece00-8651-11eb-9428-1272bbb913af.png)\r\n\r\nWe  don't need to worry about `quantiles ` is empty since we will throw exception.  `BadParameterException `",
        "createdAt" : "2021-03-16T05:30:51Z",
        "updatedAt" : "2021-03-16T05:31:07Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "3bc5e2ff-48d9-4664-b8df-0b3e6a1b904b",
        "parentId" : "3e31ce27-f4db-4d8c-a021-d890a858a70d",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "OK, I'm still not sure why there are two parameters? sorry if I'm missing the obvious here.",
        "createdAt" : "2021-03-19T16:12:09Z",
        "updatedAt" : "2021-03-19T16:12:09Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "685a89c0-81ea-436b-a9f2-852dc9291b04",
        "parentId" : "3e31ce27-f4db-4d8c-a021-d890a858a70d",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> OK, I'm still not sure why there are two parameters? sorry if I'm missing the obvious here.\r\n\r\nYou mean `details` and `withSummaries`?  Since we add these parameter in restful path `api/v1/applications/[app-id]/stages`",
        "createdAt" : "2021-03-20T12:59:53Z",
        "updatedAt" : "2021-03-20T12:59:53Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "ef258410-a6ec-4c96-842f-85195c4210d6",
        "parentId" : "3e31ce27-f4db-4d8c-a021-d890a858a70d",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Right, but, why add two parameters to begin with? what are the possible values of withSummaries that aren't implied by the value of details?",
        "createdAt" : "2021-03-20T19:12:26Z",
        "updatedAt" : "2021-03-20T19:12:26Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "4891b117-4a2b-4335-a327-5f74f16d789b",
        "parentId" : "3e31ce27-f4db-4d8c-a021-d890a858a70d",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> Right, but, why add two parameters to begin with? what are the possible values of withSummaries that aren't implied by the value of details?\r\n\r\nHmm, one for metrics distribution summaries one for task details. They work in different ways and do not affect each other.\r\n\r\nAdd tow parameters have two concern.\r\n1. Some use may just want summaries metrics to get distribution and check data skew but don't want task details data.\r\n2. `details` parameter keeps consistence with origin.",
        "createdAt" : "2021-03-22T02:25:16Z",
        "updatedAt" : "2021-03-22T02:25:17Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "76045d70-a679-4a34-b49b-46b6f71285f9",
        "parentId" : "3e31ce27-f4db-4d8c-a021-d890a858a70d",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "OK I see it now, withSummaries causes more info to be returned",
        "createdAt" : "2021-03-22T14:26:21Z",
        "updatedAt" : "2021-03-22T14:26:21Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "5fb85e91-0460-4fbe-a337-4b965b06b7e6",
        "parentId" : "3e31ce27-f4db-4d8c-a021-d890a858a70d",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> OK I see it now, withSummaries causes more info to be returned\r\n\r\nYea, return more summaries metrics in distribution. It's very useful.",
        "createdAt" : "2021-03-23T02:15:54Z",
        "updatedAt" : "2021-03-23T02:15:54Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "22f85006551f34272c97c963a2879cfd47c0ad8e",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +117,121 @@    stageId: Int,\n    details: Boolean = false,\n    withSummaries: Boolean = false,\n    unsortedQuantiles: Array[Double] = Array.empty[Double]): Seq[v1.StageData] = {\n    store.view(classOf[StageDataWrapper]).index(\"stageId\").first(stageId).last(stageId)"
  },
  {
    "id" : "a123719d-667a-4561-8f24-1ea5819d7ca2",
    "prId" : 29082,
    "prUrl" : "https://github.com/apache/spark/pull/29082#pullrequestreview-456232829",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b840d8a6-f752-491c-a184-02ad12a8f624",
        "parentId" : null,
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "The method call taskList can be slow. It would be great if we avoid calling it twice for one stage page",
        "createdAt" : "2020-07-27T17:58:25Z",
        "updatedAt" : "2021-01-26T02:07:13Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "f7aa0629-ca3d-4864-b02f-7d0f2568326c",
        "parentId" : "b840d8a6-f752-491c-a184-02ad12a8f624",
        "authorId" : "cf8c9534-0cf3-4aad-8ead-54c363cfa86e",
        "body" : "Cached result in store",
        "createdAt" : "2020-07-28T01:00:30Z",
        "updatedAt" : "2021-01-26T02:07:13Z",
        "lastEditedBy" : "cf8c9534-0cf3-4aad-8ead-54c363cfa86e",
        "tags" : [
        ]
      }
    ],
    "commit" : "769ce48b4f1500e65c9548e2bdc93d5c8e01079f",
    "line" : 33,
    "diffHunk" : "@@ -1,1 +452,456 @@\n  def computeFailureSummary(stageId: Int, attemptId: Int): Seq[v1.FailureSummary] = {\n    val tasks = taskList(stageId, attemptId, Int.MaxValue)\n    tasks.filter(t => t.status.equalsIgnoreCase(\"failed\"))\n      .flatMap(t => t.failureReason)"
  },
  {
    "id" : "50438bcc-375a-4cb8-a08b-9701673ccaba",
    "prId" : 29082,
    "prUrl" : "https://github.com/apache/spark/pull/29082#pullrequestreview-532849992",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0c50e33f-6ce8-4839-a13b-72f040a1a782",
        "parentId" : null,
        "authorId" : "1a9f8688-cf0f-450f-98b0-de28f86ae603",
        "body" : "Drop use of tuple component accessors by using `values` on the grouped items?\r\n\r\n```\r\n.groupBy(e => (e.failureType, e.message))\r\n.values\r\n.map(t => new v1.FailureSummary(t.head, t.length))\r\n```",
        "createdAt" : "2020-11-17T22:02:50Z",
        "updatedAt" : "2021-01-26T02:07:13Z",
        "lastEditedBy" : "1a9f8688-cf0f-450f-98b0-de28f86ae603",
        "tags" : [
        ]
      }
    ],
    "commit" : "769ce48b4f1500e65c9548e2bdc93d5c8e01079f",
    "line" : 36,
    "diffHunk" : "@@ -1,1 +455,459 @@    tasks.filter(t => t.status.equalsIgnoreCase(\"failed\"))\n      .flatMap(t => t.failureReason)\n      .groupBy(e => (e.failureType, e.message))\n      .values\n      .map(t => new v1.FailureSummary(t.head, t.length))"
  },
  {
    "id" : "a9006a47-a5c0-4f20-bf13-174b2cdaaa37",
    "prId" : 28769,
    "prUrl" : "https://github.com/apache/spark/pull/28769#pullrequestreview-428373840",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "47daa0a7-2f74-4b25-9eb5-85b8fa263d8c",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "(OFF-TOPIC, as my comment is for further PR, the change itself looks OK.)\r\n\r\n@srowen \r\nThis change shows the drawback if we fix the leak properly. I'm trying to implement at least `Iterable` to the `KVStoreIterator` so that `iterator().asScala` would work, but still require much more lines than before.\r\n\r\nI also see what you meant by toSeq - some callers don't materialize full list and still want to have stream, but then we can't apply the resource cleanup there. I'd say we cannot give up resource cleanup even for that case - looks like there's no choice but just materialize if it is not known to be a huge stream, otherwise just pass KVStoreIterator as a return and let caller deal with cleanup.\r\n\r\nDoes it make sense?",
        "createdAt" : "2020-06-10T19:31:26Z",
        "updatedAt" : "2020-06-14T03:10:06Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "8af3e637-10d4-455a-8744-b24e214635fd",
        "parentId" : "47daa0a7-2f74-4b25-9eb5-85b8fa263d8c",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "An Iterator shouldn't be Iterable, really... hm, what about explicitly draining the iterator to a Seq in the methods that call asScala, so we can manage the iterator lifecycle? \r\n\r\nAssuming there isn't much data there, materializing it could be fine. Or expose some other explicit method that returns the results from KVStore as a Seq, rather than just an iterator, if necessary.\r\n\r\nI still don't quite see how the iterator doesn't get closed; surely the .asScala wrapper has to evaluate all elements in the underlying iterator to produce the final filtered/mapped Seq. Hm. Maybe just being more explicit is better",
        "createdAt" : "2020-06-10T19:43:57Z",
        "updatedAt" : "2020-06-14T03:10:06Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      }
    ],
    "commit" : "9bcc084160021dd9a7dc1d573b787075016e9f01",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +40,44 @@    try {\n      // The ApplicationInfo may not be available when Spark is starting up.\n      Utils.tryWithResource(\n        store.view(classOf[ApplicationInfoWrapper])\n          .max(1)"
  },
  {
    "id" : "51d81d63-9473-4574-a9bd-2823f4b9a94e",
    "prId" : 28444,
    "prUrl" : "https://github.com/apache/spark/pull/28444#pullrequestreview-410239557",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d49c7eb6-4e84-419c-8ad9-4e5d85ed0b52",
        "parentId" : null,
        "authorId" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "body" : "Instead of make this function blocking, how about capture the Exception outside and display the proper message?",
        "createdAt" : "2020-05-08T07:24:07Z",
        "updatedAt" : "2020-05-12T22:20:38Z",
        "lastEditedBy" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "tags" : [
        ]
      },
      {
        "id" : "dd048c7a-7a1d-423d-8cee-9ec2266904a0",
        "parentId" : "d49c7eb6-4e84-419c-8ad9-4e5d85ed0b52",
        "authorId" : "2a16373d-c60e-410a-bc37-ba7eae4ec2ad",
        "body" : "If blocking is supposed to be harmful, I'd prefer an optional result instead of throwing an exception here. In fact, due to the previous design and implementation, all the invokers of the method are expecting a valid `ApplicationInfo`, which means they are not capable of dealing with a `None` value or an exception now. If we change the returned type to `Option`, all the logic of the invokers (and invokers of these invokers maybe) should be somehow adjusted. There will be more design choices then.\r\n\r\nAdding the retry logic solves the problem from the root. As I said, however, I can definitely make another choice if we reach a consensus.",
        "createdAt" : "2020-05-08T15:11:41Z",
        "updatedAt" : "2020-05-12T22:20:38Z",
        "lastEditedBy" : "2a16373d-c60e-410a-bc37-ba7eae4ec2ad",
        "tags" : [
        ]
      },
      {
        "id" : "c90cf7e5-6fcb-4116-bbcd-5d271755f412",
        "parentId" : "d49c7eb6-4e84-419c-8ad9-4e5d85ed0b52",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I think capturing the exception outside is actually a good compromise considering the issue is rather a corner case. Let's scope narrow here since the issue is rather minor, and consider a better fix with a standard approach when any bigger issue is found in the design.",
        "createdAt" : "2020-05-10T05:41:15Z",
        "updatedAt" : "2020-05-12T22:20:38Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "5d164724-2252-4922-9dfa-99aa911e00d9",
        "parentId" : "d49c7eb6-4e84-419c-8ad9-4e5d85ed0b52",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Also sounds reasonable to me.",
        "createdAt" : "2020-05-11T02:42:31Z",
        "updatedAt" : "2020-05-12T22:20:38Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "43a2d01a-c3d1-456b-8e56-fc648ae26ce2",
        "parentId" : "d49c7eb6-4e84-419c-8ad9-4e5d85ed0b52",
        "authorId" : "2a16373d-c60e-410a-bc37-ba7eae4ec2ad",
        "body" : "Thanks for your feedback, @HyukjinKwon. However, I didn't get your point (sorry for that). Did you mean changing the implementation to capturing the exception outside, or let's just keep the current approach?",
        "createdAt" : "2020-05-12T03:08:45Z",
        "updatedAt" : "2020-05-12T22:20:38Z",
        "lastEditedBy" : "2a16373d-c60e-410a-bc37-ba7eae4ec2ad",
        "tags" : [
        ]
      },
      {
        "id" : "1f008c3e-bc50-4592-bd03-ce86ef9bfa6d",
        "parentId" : "d49c7eb6-4e84-419c-8ad9-4e5d85ed0b52",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Sorry I wasn't clear. I was thinking a way, for example, as below:\r\n\r\n```diff\r\n-    store.view(classOf[ApplicationInfoWrapper]).max(1).iterator().next().info\r\n+    try {\r\n+      store.view(classOf[ApplicationInfoWrapper]).max(1).iterator().next().info\r\n+    } catch {\r\n+      case _: NoSuchElementException =>\r\n+        throw new NoSuchElementException(\r\n+          \"Application information was not found. If your Spark application is \" +\r\n+            \"starting up, try again when your Spark application is ready.\")\r\n+    }\r\n```",
        "createdAt" : "2020-05-12T04:15:57Z",
        "updatedAt" : "2020-05-12T22:20:38Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "6b29109f-a12c-4209-acc6-536178ecd28b",
        "parentId" : "d49c7eb6-4e84-419c-8ad9-4e5d85ed0b52",
        "authorId" : "2a16373d-c60e-410a-bc37-ba7eae4ec2ad",
        "body" : "Thanks for the explanation. I'll update the PR as suggested.",
        "createdAt" : "2020-05-12T16:55:21Z",
        "updatedAt" : "2020-05-12T22:20:38Z",
        "lastEditedBy" : "2a16373d-c60e-410a-bc37-ba7eae4ec2ad",
        "tags" : [
        ]
      }
    ],
    "commit" : "e633d1fb5374fa7faa6cd6e15d90f4f04703aa1b",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +36,40 @@    val listener: Option[AppStatusListener] = None) {\n\n  def applicationInfo(): v1.ApplicationInfo = {\n    try {\n      // The ApplicationInfo may not be available when Spark is starting up."
  },
  {
    "id" : "ed117a97-70eb-4115-a7d4-138b43182500",
    "prId" : 25369,
    "prUrl" : "https://github.com/apache/spark/pull/25369#pullrequestreview-273471839",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "954db411-509e-48cb-b283-183fd301c4b1",
        "parentId" : null,
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "After reading the comment in https://github.com/apache/spark/pull/23088/files#diff-3bd1667878f7bda9a56f95e93a80b475R233, I think it is on purpose that only count the \"SUCCESS\" task when with `InMemoryStore`.\r\ncc @shahidki31 \r\n",
        "createdAt" : "2019-08-07T08:43:55Z",
        "updatedAt" : "2019-08-11T17:04:03Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "054d6d38-c50e-4745-b68b-2dfd9efd4955",
        "parentId" : "954db411-509e-48cb-b283-183fd301c4b1",
        "authorId" : "d0d56018-475d-4da3-93a6-5e843402b7f2",
        "body" : "Thanks @gengliangwang . History server uses `InMemory` store by default. For Disk store case,  this isn't an optimal way for finding success task. I am yet to raise a PR for supporting for Disk store case.  \r\n\r\nI think you just need to add,\r\n `isInMemoryStore: Boolean =  listener.isDefined || store.isInstanceOf[InMemoryStore]`\r\n\r\nWill test your code with all the scenarios.\r\nAny case, this would be temporary as we need to support for Diskstore also.",
        "createdAt" : "2019-08-07T10:26:21Z",
        "updatedAt" : "2019-08-11T17:04:03Z",
        "lastEditedBy" : "d0d56018-475d-4da3-93a6-5e843402b7f2",
        "tags" : [
        ]
      },
      {
        "id" : "b5fa98fa-d82f-4455-9ae3-688bb2eb573f",
        "parentId" : "954db411-509e-48cb-b283-183fd301c4b1",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "@shahidki31 Thanks for the suggestion.\r\n\r\nI am aware that #23088 is to follow the behavior of previous versions of spark.  But I wonder if we can simply show the summary metrics for all the tasks instead of only the \"SUCCESS\" ones, as all the tasks are listed in the task table. By doing that should also make sense to users. The implementation will be simpler and we don't have to worry about the performance of the disk store.\r\nAlso cc @vanzin @srowen ",
        "createdAt" : "2019-08-07T17:39:03Z",
        "updatedAt" : "2019-08-11T17:04:03Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "2749db7f-2e86-43b6-8ec8-df4703951222",
        "parentId" : "954db411-509e-48cb-b283-183fd301c4b1",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "I don't know enough to evaluate this one, sorry. The code change itself looks plausible.",
        "createdAt" : "2019-08-07T18:51:59Z",
        "updatedAt" : "2019-08-11T17:04:03Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "073a00e3-f315-418b-ae26-75937cbc7413",
        "parentId" : "954db411-509e-48cb-b283-183fd301c4b1",
        "authorId" : "d0d56018-475d-4da3-93a6-5e843402b7f2",
        "body" : "@gengliangwang If I understand correctly,  ~~#23008~~ #23088 is actually fixing this issue. right ?(At least history server case).\r\nBecause, `count` is always filtering out the running tasks, as `ExecutorRunTime` will be defined for only finished tasks. But `scan tasks` contains all the tasks, including running tasks. \r\nhttps://github.com/apache/spark/blob/a59fdc4b5783be591a236bfc60d1107caa818412/core/src/main/scala/org/apache/spark/status/AppStatusStore.scala#L169\r\n\r\nhttps://github.com/apache/spark/blob/a59fdc4b5783be591a236bfc60d1107caa818412/core/src/main/scala/org/apache/spark/status/AppStatusStore.scala#L263-L265\r\n\r\nSo, reverting ~~23008~~ 23088 and hence this PR, would not fix the issue? Please correct me if I am wrong.",
        "createdAt" : "2019-08-07T19:14:04Z",
        "updatedAt" : "2019-08-11T17:04:03Z",
        "lastEditedBy" : "d0d56018-475d-4da3-93a6-5e843402b7f2",
        "tags" : [
        ]
      },
      {
        "id" : "6fc09461-ceb7-41cf-a239-933947db2a9b",
        "parentId" : "954db411-509e-48cb-b283-183fd301c4b1",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "@shahidki31 Do you mean #23088?\r\n> as ExecutorRunTime will be defined for only finished tasks\r\n\r\nThat's not true. It is also defined in running tasks as I observed.\r\n\r\n> reverting 23008 and hence this PR, would not fix the issue\r\n\r\nI am not trying to revert anything. Just out of curiosity for the feature.\r\n",
        "createdAt" : "2019-08-08T02:19:05Z",
        "updatedAt" : "2019-08-11T17:04:03Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "679abb38-ae32-442b-9c66-aee9f6a04f3f",
        "parentId" : "954db411-509e-48cb-b283-183fd301c4b1",
        "authorId" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "body" : "I guess the underlying question you guys are asking is: should the metrics count only successful tasks, or all non-failed tasks?\r\n\r\nI don't remember the behavior in 2.2 (which is what #23088 was trying to emulate?), which should have the correct answer.",
        "createdAt" : "2019-08-08T21:19:10Z",
        "updatedAt" : "2019-08-11T17:04:03Z",
        "lastEditedBy" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "tags" : [
        ]
      },
      {
        "id" : "f62ae7a5-52ba-4fe0-a0bb-c29c76c1dae8",
        "parentId" : "954db411-509e-48cb-b283-183fd301c4b1",
        "authorId" : "d0d56018-475d-4da3-93a6-5e843402b7f2",
        "body" : "@vanzin In Spark-2.2, as per this line, it doesn't count metrics of a running task. That was the intend behind #23088 (which eventually fixes this issue as well)\r\nhttps://github.com/apache/spark/blob/7c7d7f6a878b02ece881266ee538f3e1443aa8c1/core/src/main/scala/org/apache/spark/ui/jobs/StagePage.scala#L340\r\n\r\nverified locally also, \r\n `sc.parallelize(1 to 160, 1).map( i => Thread.sleep(1000)).collect()`\r\n\r\n![Screenshot 2019-08-09 at 4 03 09 AM](https://user-images.githubusercontent.com/23054875/62742208-4beb1a80-ba5b-11e9-97a7-b01f9515b5fd.png)\r\n",
        "createdAt" : "2019-08-08T22:39:54Z",
        "updatedAt" : "2019-08-11T17:04:03Z",
        "lastEditedBy" : "d0d56018-475d-4da3-93a6-5e843402b7f2",
        "tags" : [
        ]
      },
      {
        "id" : "03971c79-ca38-4fb4-bd08-3f6ec8bbeada",
        "parentId" : "954db411-509e-48cb-b283-183fd301c4b1",
        "authorId" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "body" : "So sounds like the current behavior is consistent with how Spark has behaved in the past, and this change is proposing a different approach.\r\n\r\nA quick look at the history shows the current behavior has been that in forever...\r\n\r\nAlso, that pointed me at another thing that can be seen in the screenshots. The table header says \"Summary Metrics for X *Completed* Tasks\". So this change would be making that wrong...",
        "createdAt" : "2019-08-08T23:19:57Z",
        "updatedAt" : "2019-08-11T17:04:03Z",
        "lastEditedBy" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "tags" : [
        ]
      },
      {
        "id" : "abe5a887-8d99-4ccf-9350-35f18c285718",
        "parentId" : "954db411-509e-48cb-b283-183fd301c4b1",
        "authorId" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "body" : "Ok, I need to backtrack here a little. My bad, I think I know why I'm confused.\r\n\r\nI've been starting from the PR title, and it confused me a little bit. Could you change it so it describes what the change is doing? e.g. \"Ignore non-succeeded tasks when calculating metric summaries.\"\r\n\r\nLet me re-review trying to ignore that and focusing just on the code.",
        "createdAt" : "2019-08-09T17:23:37Z",
        "updatedAt" : "2019-08-11T17:04:03Z",
        "lastEditedBy" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "tags" : [
        ]
      },
      {
        "id" : "6f8e52fc-330c-4fac-b81a-2ab634ca9353",
        "parentId" : "954db411-509e-48cb-b283-183fd301c4b1",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "@vanzin Got it. I have updated the title.\r\nThe PR itself is about mismatching the kvstore, and leading to the wrong cache for task summary.",
        "createdAt" : "2019-08-09T17:43:09Z",
        "updatedAt" : "2019-08-11T17:04:03Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "dc776cc8-5454-476c-8324-0186d5628178",
        "parentId" : "954db411-509e-48cb-b283-183fd301c4b1",
        "authorId" : "d0d56018-475d-4da3-93a6-5e843402b7f2",
        "body" : "May be we need to add a test in the AppStatusStoreSuite? There all the stores are tested against InMemory store only, I think.",
        "createdAt" : "2019-08-09T17:56:12Z",
        "updatedAt" : "2019-08-11T17:04:03Z",
        "lastEditedBy" : "d0d56018-475d-4da3-93a6-5e843402b7f2",
        "tags" : [
        ]
      },
      {
        "id" : "4ef5eaa0-b4cf-4d3f-afdd-81272914e24a",
        "parentId" : "954db411-509e-48cb-b283-183fd301c4b1",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "@shahidki31 sure, I have added test cases.",
        "createdAt" : "2019-08-11T17:03:53Z",
        "updatedAt" : "2019-08-11T17:04:03Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "08eca6e7909098ae2cabac6d593c41cdf5a7aee0",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +163,167 @@    val count = {\n      Utils.tryWithResource(\n        if (isInMemoryStore) {\n          // For Live UI, we should count the tasks with status \"SUCCESS\" only.\n          store.view(classOf[TaskDataWrapper])"
  },
  {
    "id" : "26647dfa-4c5a-4d8b-a80e-9d49f19d35c1",
    "prId" : 25369,
    "prUrl" : "https://github.com/apache/spark/pull/25369#pullrequestreview-273471831",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "676bcf9a-9aee-4764-963b-96828e3dd747",
        "parentId" : null,
        "authorId" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "body" : "This check isn't whether it's an in-memory store, but whether you're running a live UI (vs. in-memory store in the SHS). Is that your intent? (The method name should match what you intend to check.)",
        "createdAt" : "2019-08-08T21:17:49Z",
        "updatedAt" : "2019-08-11T17:04:03Z",
        "lastEditedBy" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "tags" : [
        ]
      },
      {
        "id" : "f329dfe3-7c7c-4fd8-866a-932aa0e00702",
        "parentId" : "676bcf9a-9aee-4764-963b-96828e3dd747",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "I checked if it is live UI in the first commit. \r\nAs per the comment in https://github.com/apache/spark/pull/23088/files#diff-3bd1667878f7bda9a56f95e93a80b475R233 and https://github.com/apache/spark/pull/25369#discussion_r311479709, I change it to checking if it is InMemoryStore/Live UI. \r\nBut later on, I don't like the idea that live UI is inconsistent with SHS. So I raise a question.",
        "createdAt" : "2019-08-09T03:59:34Z",
        "updatedAt" : "2019-08-11T17:04:03Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "720ca4f7-ccb0-4581-b1aa-f44cf7463313",
        "parentId" : "676bcf9a-9aee-4764-963b-96828e3dd747",
        "authorId" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "body" : "Ok, I looked at this again ignoring the PR title and this makes sense. Sorry for flip-flopping here, but could you put this in a local variable in `taskSummary` with a comment explaining it? e.g.\r\n\r\n```\r\n// SPARK-26119: we only want to consider successful tasks when calculating the metrics summary,\r\n// but currently this is very expensive when using a disk store. So we only trigger the slower code\r\n// path when we know we have all data in memory. This check is an approximation of when the know\r\n// that the data will be in memory.\r\nval isInMemoryStore = store.isInstanceOf[InMemoryStore] || listener.isDefined\r\n```",
        "createdAt" : "2019-08-09T17:28:24Z",
        "updatedAt" : "2019-08-11T17:04:03Z",
        "lastEditedBy" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "tags" : [
        ]
      },
      {
        "id" : "7cfe4cf4-aa73-41d5-8236-c50f12acf8ad",
        "parentId" : "676bcf9a-9aee-4764-963b-96828e3dd747",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "Thanks, I have added comments",
        "createdAt" : "2019-08-11T17:03:35Z",
        "updatedAt" : "2019-08-11T17:04:03Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      }
    ],
    "commit" : "08eca6e7909098ae2cabac6d593c41cdf5a7aee0",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +141,145 @@  // code path when we know we have all data in memory. The following method checks whether all\n  // the data will be in memory.\n  private def isInMemoryStore: Boolean = store.isInstanceOf[InMemoryStore] || listener.isDefined\n\n  /**"
  },
  {
    "id" : "c054c27c-a716-480f-8991-ef528fa4baba",
    "prId" : 24982,
    "prUrl" : "https://github.com/apache/spark/pull/24982#pullrequestreview-255751577",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "21e0ba56-8668-4952-9118-358b1c1e5d9e",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Just `statuses != null && !statuses.isEmpty() && !statuses.contains(j.info.status)`; in fact the middle condition is probably redundant then. Or you don't need `viewWithCondition` if `statuses == null`",
        "createdAt" : "2019-06-28T12:56:03Z",
        "updatedAt" : "2019-06-28T13:00:30Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      }
    ],
    "commit" : "dac3fe3ba6bd8ee270f92109333279d6510a6c43",
    "line" : 29,
    "diffHunk" : "@@ -1,1 +47,51 @@  def jobsList(statuses: JList[JobExecutionStatus]): Seq[v1.JobData] = {\n    store.viewWithCondition(classOf[JobDataWrapper], (j: JobDataWrapper) =>\n      if (statuses != null && !statuses.isEmpty()) statuses.contains(j.info.status) else true\n    ).reverse().asScala.map(_.info).toSeq\n  }"
  },
  {
    "id" : "b7902f60-6b4c-4ae4-999e-2b380551639b",
    "prId" : 24982,
    "prUrl" : "https://github.com/apache/spark/pull/24982#pullrequestreview-255751577",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f244dff4-1025-40ef-9d84-e3ae36893aab",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Remove this",
        "createdAt" : "2019-06-28T12:56:44Z",
        "updatedAt" : "2019-06-28T13:00:30Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      }
    ],
    "commit" : "dac3fe3ba6bd8ee270f92109333279d6510a6c43",
    "line" : 86,
    "diffHunk" : "@@ -1,1 +422,426 @@        store.view(classOf[TaskDataWrapper])\n    }\n      store.view(classOf[TaskDataWrapper])\n    val indexed = sortBy match {\n      case Some(index) =>"
  },
  {
    "id" : "0a6ce7e4-a415-4c61-bc5d-1950da0c4415",
    "prId" : 24982,
    "prUrl" : "https://github.com/apache/spark/pull/24982#pullrequestreview-255751577",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "844756c7-b40d-4d48-9a56-e3cd4778487c",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Use equalsIgnoreCase maybe? but will the statuses vary by case ever?",
        "createdAt" : "2019-06-28T12:57:18Z",
        "updatedAt" : "2019-06-28T13:00:30Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      }
    ],
    "commit" : "dac3fe3ba6bd8ee270f92109333279d6510a6c43",
    "line" : 81,
    "diffHunk" : "@@ -1,1 +417,421 @@      case Some(expected) =>\n        store.viewWithCondition(classOf[TaskDataWrapper], (t: TaskDataWrapper) => {\n          t.status.toLowerCase(Locale.ROOT) == expected.toLowerCase(Locale.ROOT)\n        })\n      case None =>"
  }
]