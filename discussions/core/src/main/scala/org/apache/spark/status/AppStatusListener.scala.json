[
  {
    "id" : "b70542c0-7034-4915-a71d-5d8ccf23f40b",
    "prId" : 30573,
    "prUrl" : "https://github.com/apache/spark/pull/30573#pullrequestreview-554339146",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "22610016-bce3-40a8-8291-4bc59f7e3197",
        "parentId" : null,
        "authorId" : "3a227965-84e0-47cf-9974-11293764f028",
        "body" : "Hmm... executor metrics for each stage should be collected [here](https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/status/AppStatusListener.scala#L980).\r\nBut if the heartbeat interval from an executor is longer than lifetime of a stage, we can't collect the executor metrics for the stage.\r\nSo this change can be one option. What do you think @gengliangwang ?",
        "createdAt" : "2020-12-08T02:27:34Z",
        "updatedAt" : "2020-12-09T13:48:52Z",
        "lastEditedBy" : "3a227965-84e0-47cf-9974-11293764f028",
        "tags" : [
        ]
      },
      {
        "id" : "dce1aa5e-f281-4c97-a046-f2f0599ce457",
        "parentId" : "22610016-bce3-40a8-8291-4bc59f7e3197",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "We collect all data to choose the peak metrics. IMO,  more accuracy is better. \r\nBut I'm not particularly clear about the performance impact of this part, hope more suggestion.",
        "createdAt" : "2020-12-09T09:38:41Z",
        "updatedAt" : "2020-12-09T13:48:52Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "a128f94c-5d95-4363-a18d-fd236c968855",
        "parentId" : "22610016-bce3-40a8-8291-4bc59f7e3197",
        "authorId" : "3a227965-84e0-47cf-9974-11293764f028",
        "body" : "@imback82 Do you have any concern about this change?",
        "createdAt" : "2020-12-11T06:55:56Z",
        "updatedAt" : "2020-12-11T06:55:56Z",
        "lastEditedBy" : "3a227965-84e0-47cf-9974-11293764f028",
        "tags" : [
        ]
      },
      {
        "id" : "e5baf57a-9d04-47fc-b792-ce9700299bc3",
        "parentId" : "22610016-bce3-40a8-8291-4bc59f7e3197",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "This seems fine to me if we need more accurate peak values.",
        "createdAt" : "2020-12-12T01:17:35Z",
        "updatedAt" : "2020-12-12T01:17:35Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      },
      {
        "id" : "daba771a-4365-4487-8d42-cc5638e1d01f",
        "parentId" : "22610016-bce3-40a8-8291-4bc59f7e3197",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "It seems that the first metrics of `peakExecutorMetrics` become 0 instead of -1 after this. @AngersZhuuuu Do you know the reason?",
        "createdAt" : "2020-12-14T09:03:21Z",
        "updatedAt" : "2020-12-14T09:03:22Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "1e285990-434b-4f2b-b3e5-1a907db0d21b",
        "parentId" : "22610016-bce3-40a8-8291-4bc59f7e3197",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> It seems that the first metrics of peakExecutorMetrics become 0 instead of -1 after this. @AngersZhuuuu Do you know the reason?\r\n\r\n`-1` is default peakExecutorMetrics, and with this change,  we will update this value with real task metrics, but why all is `0` depend on the metrics data...",
        "createdAt" : "2020-12-14T11:02:28Z",
        "updatedAt" : "2020-12-14T11:02:39Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "e4dd0fb6-fdf9-40b8-9736-a038dec4097d",
        "parentId" : "22610016-bce3-40a8-8291-4bc59f7e3197",
        "authorId" : "3a227965-84e0-47cf-9974-11293764f028",
        "body" : "> It seems that the first metrics of peakExecutorMetrics become 0 instead of -1 after this. @AngersZhuuuu Do you know the reason?\r\n\r\n@gengliangwang \r\nBy this change, `peakExecutorMetrics` is updated not only `onExecutorMetricsUpdate` but also `onTaskEnd`.\r\nSo, the peak value carried by `SparkListenerTaskEnd` is `0`, the corresponding peak values in `peakExecutorMetrics` is set to `0`.\r\nDo you have any concern?",
        "createdAt" : "2020-12-17T06:46:29Z",
        "updatedAt" : "2020-12-17T06:46:35Z",
        "lastEditedBy" : "3a227965-84e0-47cf-9974-11293764f028",
        "tags" : [
        ]
      },
      {
        "id" : "f24afa75-9abc-46fd-96ab-a3c9ab3f5839",
        "parentId" : "22610016-bce3-40a8-8291-4bc59f7e3197",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "@sarutak No, I am ok with it :)",
        "createdAt" : "2020-12-17T07:22:43Z",
        "updatedAt" : "2020-12-17T07:22:44Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "1e222b1f-23c6-4305-850b-90c16ff13e58",
        "parentId" : "22610016-bce3-40a8-8291-4bc59f7e3197",
        "authorId" : "3a227965-84e0-47cf-9974-11293764f028",
        "body" : "O.K, I'll merge later if there are no objections. Thanks for the response. @gengliangwang ",
        "createdAt" : "2020-12-17T07:32:05Z",
        "updatedAt" : "2020-12-17T07:32:06Z",
        "lastEditedBy" : "3a227965-84e0-47cf-9974-11293764f028",
        "tags" : [
        ]
      }
    ],
    "commit" : "c21be1157c2fe74aec075e4662aec13d0b66597c",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +690,694 @@\n      stage.executorSummary(event.taskInfo.executorId).peakExecutorMetrics\n        .compareAndUpdatePeakValues(event.taskExecutorMetrics)\n      // [SPARK-24415] Wait for all tasks to finish before removing stage from live list\n      val removeStage ="
  },
  {
    "id" : "569b1562-0b12-423c-a566-890d78e8d357",
    "prId" : 29906,
    "prUrl" : "https://github.com/apache/spark/pull/29906#pullrequestreview-501482167",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7f02dc19-7543-4eaa-aec7-1c8781668877",
        "parentId" : null,
        "authorId" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "body" : "Is `setStageExclusionStatus` a better name? It reads more smoothly to me, but I'm not sure if it's less clear.",
        "createdAt" : "2020-10-02T22:55:31Z",
        "updatedAt" : "2020-10-28T13:50:33Z",
        "lastEditedBy" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "tags" : [
        ]
      }
    ],
    "commit" : "b38dd66500cdd4a12f78cca1eeacf116f0ea5b4e",
    "line" : 68,
    "diffHunk" : "@@ -1,1 +327,331 @@  }\n\n  private def setStageExcludedStatus(stage: LiveStage, now: Long, executorIds: String*): Unit = {\n    executorIds.foreach { executorId =>\n      val executorStageSummary = stage.executorSummary(executorId)"
  },
  {
    "id" : "61d2bd6b-0710-4c8a-823c-e983fd1135af",
    "prId" : 29906,
    "prUrl" : "https://github.com/apache/spark/pull/29906#pullrequestreview-514734141",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dd457964-6d92-418f-b54a-c8e57d3beb62",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Do we need to implement these deprecated methods for internal listeners? I assume they are only used for external listeners.",
        "createdAt" : "2020-10-20T03:12:02Z",
        "updatedAt" : "2020-10-28T13:50:33Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "8bbf6176-c82b-489a-b348-aede9f5f707a",
        "parentId" : "dd457964-6d92-418f-b54a-c8e57d3beb62",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "I left them to be completely backwards compatible. This way they are still written to event logs which means older history servers should work with newer files and if people have parsers for those it still works.  I don't think we say the event log if compatible in that way but this gives more compatibility.  \r\nI'm fine with removing them if everyone else thinks its ok.",
        "createdAt" : "2020-10-20T13:54:55Z",
        "updatedAt" : "2020-10-28T13:50:33Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "98537ff2-6871-4797-8916-6f97544b52f2",
        "parentId" : "dd457964-6d92-418f-b54a-c8e57d3beb62",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "IIUC, this is for new HS(history server) + old App compatibility? When a new HS reads old blacklist events for the old App, it can thus call these deprecated methods.\r\n\r\nFor old HS + new App case, I think it's fine since we always post new and old blacklist events, and thus EventLoggingListers writes two blacklist events as well. Therefore, old HS can parse the old blocklist events and ignore the new one.\r\n\r\n\r\nI'm fine to leave them as they are. Maybe, add one more comment to explain why we still need it?\r\n\r\nAnother thought is, we can remove all those deprecated methods both in `AppStatusListener` and `EventLoggingListers` but keep compatibility by handling it in `onOtherEvent`. It's up to you. I'm fine either way.",
        "createdAt" : "2020-10-22T03:10:19Z",
        "updatedAt" : "2020-10-28T13:50:33Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "e5665114-8ace-49e0-a3cf-d37e8157d001",
        "parentId" : "dd457964-6d92-418f-b54a-c8e57d3beb62",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "yes its for new HS reading old app event log compatibility.\r\nI prefer to leave as is this way. I can add a comment.",
        "createdAt" : "2020-10-22T13:37:13Z",
        "updatedAt" : "2020-10-28T13:50:33Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "b38dd66500cdd4a12f78cca1eeacf116f0ea5b4e",
    "line" : 87,
    "diffHunk" : "@@ -1,1 +345,349 @@  }\n\n  override def onNodeBlacklisted(event: SparkListenerNodeBlacklisted): Unit = {\n    updateNodeExcluded(event.hostId, true)\n  }"
  },
  {
    "id" : "f0ef5425-e9f5-4a1a-9d8f-87e7c2784025",
    "prId" : 29906,
    "prUrl" : "https://github.com/apache/spark/pull/29906#pullrequestreview-515797245",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "33f34bfc-123a-4bda-9208-c63ddec913c9",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "I wonder this could lead to the metrics overcounted since we always post two blacklist events?",
        "createdAt" : "2020-10-22T03:16:00Z",
        "updatedAt" : "2020-10-28T13:50:33Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "9ddae70f-3462-40e8-97bc-a333562a6264",
        "parentId" : "33f34bfc-123a-4bda-9208-c63ddec913c9",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "thanks for catching this one, I missed it. I had checked this and most are fine due to using set and just setting status which would already be set, but I somehow missed this one. I'll fix ",
        "createdAt" : "2020-10-22T13:26:10Z",
        "updatedAt" : "2020-10-28T13:50:33Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "fafab946-1f3c-456a-9ce5-aada38f2adc6",
        "parentId" : "33f34bfc-123a-4bda-9208-c63ddec913c9",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "I updated this but I actually found a pre-existing bug where we weren't incrementing this when we excluded a node - which implicitly excludes the executors",
        "createdAt" : "2020-10-23T15:51:16Z",
        "updatedAt" : "2020-10-28T13:50:33Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "b38dd66500cdd4a12f78cca1eeacf116f0ea5b4e",
    "line" : 149,
    "diffHunk" : "@@ -1,1 +402,406 @@      if (excluded) {\n        appStatusSource.foreach(_.BLACKLISTED_EXECUTORS.inc())\n        appStatusSource.foreach(_.EXCLUDED_EXECUTORS.inc())\n      } else {\n        appStatusSource.foreach(_.UNBLACKLISTED_EXECUTORS.inc())"
  },
  {
    "id" : "23f4caa5-dc04-48e6-ac7d-53552eb07327",
    "prId" : 29082,
    "prUrl" : "https://github.com/apache/spark/pull/29082#pullrequestreview-492504272",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "07ed30b1-2da5-43a9-b336-5660854981e9",
        "parentId" : null,
        "authorId" : "3a227965-84e0-47cf-9974-11293764f028",
        "body" : "As I mention in another place, `message` field in `FailureReason` seems to be redundant.\r\nIf we can remove it away,  can we just parse error message and build `FailureReason` in `computeFailureSummary`?",
        "createdAt" : "2020-09-21T11:52:36Z",
        "updatedAt" : "2021-01-26T02:07:13Z",
        "lastEditedBy" : "3a227965-84e0-47cf-9974-11293764f028",
        "tags" : [
        ]
      }
    ],
    "commit" : "769ce48b4f1500e65c9548e2bdc93d5c8e01079f",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +659,663 @@      task.errorMessage = errorMessage\n      task.failureReason = event.reason match {\n        case e: ExceptionFailure =>\n          Some(new v1.FailureReason(e.className, e.description, e.toErrorString))\n        case e: ExecutorLostFailure =>"
  },
  {
    "id" : "35948917-fcb5-4a90-bba2-b9b9cfbaf069",
    "prId" : 29082,
    "prUrl" : "https://github.com/apache/spark/pull/29082#pullrequestreview-515861549",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "51366170-a978-4877-ba60-0ae25c8589bd",
        "parentId" : null,
        "authorId" : "3a227965-84e0-47cf-9974-11293764f028",
        "body" : "As I mentioned in another place, `message` in FailureReason can be redundant.\r\nIf we can remove it, we can just parse error message and build `FailureReason` in `computeFailureSummary` regardless of `event.reason`.",
        "createdAt" : "2020-09-21T12:30:50Z",
        "updatedAt" : "2021-01-26T02:07:13Z",
        "lastEditedBy" : "3a227965-84e0-47cf-9974-11293764f028",
        "tags" : [
        ]
      },
      {
        "id" : "4574160c-84f4-4b9c-918f-593a2b5e067f",
        "parentId" : "51366170-a978-4877-ba60-0ae25c8589bd",
        "authorId" : "cf8c9534-0cf3-4aad-8ead-54c363cfa86e",
        "body" : "Yes, in previous version, I directly passed error message to build failure reason, but I found it hard, complex and error-prone. This due to different failure reason has different format:\r\n1. `ExceptionFailure` is most common and easy one. The message is only exception's message, we can parse it from errorString.\r\n2. `ExecutorLostFailure` errorString has format like `ExecutorLostFailure (executor 72 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 123153 ms`. We can split the message out using keyword `Reason:`\r\n3. `FetchFailed` errorString has format like `FetchFailed($bmAddressString, shuffleId=$shuffleId, mapId=$mapId, reduceId=$reduceId, message=\\n$message\\n)`. We need to parse message from this.\r\n\r\nIn summary, parsing will rely on and couple with specific format of different failure reason. If in the future, the format changed, it's easy to break. So I chose the solution to directly get from TaskEndReason. ",
        "createdAt" : "2020-09-24T16:52:16Z",
        "updatedAt" : "2021-01-26T02:07:13Z",
        "lastEditedBy" : "cf8c9534-0cf3-4aad-8ead-54c363cfa86e",
        "tags" : [
        ]
      },
      {
        "id" : "2acce83b-57d2-473c-943c-65b45ba8a13c",
        "parentId" : "51366170-a978-4877-ba60-0ae25c8589bd",
        "authorId" : "cf8c9534-0cf3-4aad-8ead-54c363cfa86e",
        "body" : "@sarutak Any more comments on this?",
        "createdAt" : "2020-10-23T17:19:05Z",
        "updatedAt" : "2021-01-26T02:07:13Z",
        "lastEditedBy" : "cf8c9534-0cf3-4aad-8ead-54c363cfa86e",
        "tags" : [
        ]
      }
    ],
    "commit" : "769ce48b4f1500e65c9548e2bdc93d5c8e01079f",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +658,662 @@      }\n      task.errorMessage = errorMessage\n      task.failureReason = event.reason match {\n        case e: ExceptionFailure =>\n          Some(new v1.FailureReason(e.className, e.description, e.toErrorString))"
  }
]