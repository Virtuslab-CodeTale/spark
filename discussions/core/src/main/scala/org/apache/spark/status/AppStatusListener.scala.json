[
  {
    "id" : "b70542c0-7034-4915-a71d-5d8ccf23f40b",
    "prId" : 30573,
    "prUrl" : "https://github.com/apache/spark/pull/30573#pullrequestreview-554339146",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "22610016-bce3-40a8-8291-4bc59f7e3197",
        "parentId" : null,
        "authorId" : "3a227965-84e0-47cf-9974-11293764f028",
        "body" : "Hmm... executor metrics for each stage should be collected [here](https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/status/AppStatusListener.scala#L980).\r\nBut if the heartbeat interval from an executor is longer than lifetime of a stage, we can't collect the executor metrics for the stage.\r\nSo this change can be one option. What do you think @gengliangwang ?",
        "createdAt" : "2020-12-08T02:27:34Z",
        "updatedAt" : "2020-12-09T13:48:52Z",
        "lastEditedBy" : "3a227965-84e0-47cf-9974-11293764f028",
        "tags" : [
        ]
      },
      {
        "id" : "dce1aa5e-f281-4c97-a046-f2f0599ce457",
        "parentId" : "22610016-bce3-40a8-8291-4bc59f7e3197",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "We collect all data to choose the peak metrics. IMO,  more accuracy is better. \r\nBut I'm not particularly clear about the performance impact of this part, hope more suggestion.",
        "createdAt" : "2020-12-09T09:38:41Z",
        "updatedAt" : "2020-12-09T13:48:52Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "a128f94c-5d95-4363-a18d-fd236c968855",
        "parentId" : "22610016-bce3-40a8-8291-4bc59f7e3197",
        "authorId" : "3a227965-84e0-47cf-9974-11293764f028",
        "body" : "@imback82 Do you have any concern about this change?",
        "createdAt" : "2020-12-11T06:55:56Z",
        "updatedAt" : "2020-12-11T06:55:56Z",
        "lastEditedBy" : "3a227965-84e0-47cf-9974-11293764f028",
        "tags" : [
        ]
      },
      {
        "id" : "e5baf57a-9d04-47fc-b792-ce9700299bc3",
        "parentId" : "22610016-bce3-40a8-8291-4bc59f7e3197",
        "authorId" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "body" : "This seems fine to me if we need more accurate peak values.",
        "createdAt" : "2020-12-12T01:17:35Z",
        "updatedAt" : "2020-12-12T01:17:35Z",
        "lastEditedBy" : "b14448be-63dd-4b59-ab38-deeb2a38de86",
        "tags" : [
        ]
      },
      {
        "id" : "daba771a-4365-4487-8d42-cc5638e1d01f",
        "parentId" : "22610016-bce3-40a8-8291-4bc59f7e3197",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "It seems that the first metrics of `peakExecutorMetrics` become 0 instead of -1 after this. @AngersZhuuuu Do you know the reason?",
        "createdAt" : "2020-12-14T09:03:21Z",
        "updatedAt" : "2020-12-14T09:03:22Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "1e285990-434b-4f2b-b3e5-1a907db0d21b",
        "parentId" : "22610016-bce3-40a8-8291-4bc59f7e3197",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> It seems that the first metrics of peakExecutorMetrics become 0 instead of -1 after this. @AngersZhuuuu Do you know the reason?\r\n\r\n`-1` is default peakExecutorMetrics, and with this change,  we will update this value with real task metrics, but why all is `0` depend on the metrics data...",
        "createdAt" : "2020-12-14T11:02:28Z",
        "updatedAt" : "2020-12-14T11:02:39Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      },
      {
        "id" : "e4dd0fb6-fdf9-40b8-9736-a038dec4097d",
        "parentId" : "22610016-bce3-40a8-8291-4bc59f7e3197",
        "authorId" : "3a227965-84e0-47cf-9974-11293764f028",
        "body" : "> It seems that the first metrics of peakExecutorMetrics become 0 instead of -1 after this. @AngersZhuuuu Do you know the reason?\r\n\r\n@gengliangwang \r\nBy this change, `peakExecutorMetrics` is updated not only `onExecutorMetricsUpdate` but also `onTaskEnd`.\r\nSo, the peak value carried by `SparkListenerTaskEnd` is `0`, the corresponding peak values in `peakExecutorMetrics` is set to `0`.\r\nDo you have any concern?",
        "createdAt" : "2020-12-17T06:46:29Z",
        "updatedAt" : "2020-12-17T06:46:35Z",
        "lastEditedBy" : "3a227965-84e0-47cf-9974-11293764f028",
        "tags" : [
        ]
      },
      {
        "id" : "f24afa75-9abc-46fd-96ab-a3c9ab3f5839",
        "parentId" : "22610016-bce3-40a8-8291-4bc59f7e3197",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "@sarutak No, I am ok with it :)",
        "createdAt" : "2020-12-17T07:22:43Z",
        "updatedAt" : "2020-12-17T07:22:44Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "1e222b1f-23c6-4305-850b-90c16ff13e58",
        "parentId" : "22610016-bce3-40a8-8291-4bc59f7e3197",
        "authorId" : "3a227965-84e0-47cf-9974-11293764f028",
        "body" : "O.K, I'll merge later if there are no objections. Thanks for the response. @gengliangwang ",
        "createdAt" : "2020-12-17T07:32:05Z",
        "updatedAt" : "2020-12-17T07:32:06Z",
        "lastEditedBy" : "3a227965-84e0-47cf-9974-11293764f028",
        "tags" : [
        ]
      }
    ],
    "commit" : "c21be1157c2fe74aec075e4662aec13d0b66597c",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +690,694 @@\n      stage.executorSummary(event.taskInfo.executorId).peakExecutorMetrics\n        .compareAndUpdatePeakValues(event.taskExecutorMetrics)\n      // [SPARK-24415] Wait for all tasks to finish before removing stage from live list\n      val removeStage ="
  },
  {
    "id" : "569b1562-0b12-423c-a566-890d78e8d357",
    "prId" : 29906,
    "prUrl" : "https://github.com/apache/spark/pull/29906#pullrequestreview-501482167",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7f02dc19-7543-4eaa-aec7-1c8781668877",
        "parentId" : null,
        "authorId" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "body" : "Is `setStageExclusionStatus` a better name? It reads more smoothly to me, but I'm not sure if it's less clear.",
        "createdAt" : "2020-10-02T22:55:31Z",
        "updatedAt" : "2020-10-28T13:50:33Z",
        "lastEditedBy" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "tags" : [
        ]
      }
    ],
    "commit" : "b38dd66500cdd4a12f78cca1eeacf116f0ea5b4e",
    "line" : 68,
    "diffHunk" : "@@ -1,1 +327,331 @@  }\n\n  private def setStageExcludedStatus(stage: LiveStage, now: Long, executorIds: String*): Unit = {\n    executorIds.foreach { executorId =>\n      val executorStageSummary = stage.executorSummary(executorId)"
  },
  {
    "id" : "61d2bd6b-0710-4c8a-823c-e983fd1135af",
    "prId" : 29906,
    "prUrl" : "https://github.com/apache/spark/pull/29906#pullrequestreview-514734141",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dd457964-6d92-418f-b54a-c8e57d3beb62",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Do we need to implement these deprecated methods for internal listeners? I assume they are only used for external listeners.",
        "createdAt" : "2020-10-20T03:12:02Z",
        "updatedAt" : "2020-10-28T13:50:33Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "8bbf6176-c82b-489a-b348-aede9f5f707a",
        "parentId" : "dd457964-6d92-418f-b54a-c8e57d3beb62",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "I left them to be completely backwards compatible. This way they are still written to event logs which means older history servers should work with newer files and if people have parsers for those it still works.  I don't think we say the event log if compatible in that way but this gives more compatibility.  \r\nI'm fine with removing them if everyone else thinks its ok.",
        "createdAt" : "2020-10-20T13:54:55Z",
        "updatedAt" : "2020-10-28T13:50:33Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "98537ff2-6871-4797-8916-6f97544b52f2",
        "parentId" : "dd457964-6d92-418f-b54a-c8e57d3beb62",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "IIUC, this is for new HS(history server) + old App compatibility? When a new HS reads old blacklist events for the old App, it can thus call these deprecated methods.\r\n\r\nFor old HS + new App case, I think it's fine since we always post new and old blacklist events, and thus EventLoggingListers writes two blacklist events as well. Therefore, old HS can parse the old blocklist events and ignore the new one.\r\n\r\n\r\nI'm fine to leave them as they are. Maybe, add one more comment to explain why we still need it?\r\n\r\nAnother thought is, we can remove all those deprecated methods both in `AppStatusListener` and `EventLoggingListers` but keep compatibility by handling it in `onOtherEvent`. It's up to you. I'm fine either way.",
        "createdAt" : "2020-10-22T03:10:19Z",
        "updatedAt" : "2020-10-28T13:50:33Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "e5665114-8ace-49e0-a3cf-d37e8157d001",
        "parentId" : "dd457964-6d92-418f-b54a-c8e57d3beb62",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "yes its for new HS reading old app event log compatibility.\r\nI prefer to leave as is this way. I can add a comment.",
        "createdAt" : "2020-10-22T13:37:13Z",
        "updatedAt" : "2020-10-28T13:50:33Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "b38dd66500cdd4a12f78cca1eeacf116f0ea5b4e",
    "line" : 87,
    "diffHunk" : "@@ -1,1 +345,349 @@  }\n\n  override def onNodeBlacklisted(event: SparkListenerNodeBlacklisted): Unit = {\n    updateNodeExcluded(event.hostId, true)\n  }"
  },
  {
    "id" : "f0ef5425-e9f5-4a1a-9d8f-87e7c2784025",
    "prId" : 29906,
    "prUrl" : "https://github.com/apache/spark/pull/29906#pullrequestreview-515797245",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "33f34bfc-123a-4bda-9208-c63ddec913c9",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "I wonder this could lead to the metrics overcounted since we always post two blacklist events?",
        "createdAt" : "2020-10-22T03:16:00Z",
        "updatedAt" : "2020-10-28T13:50:33Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "9ddae70f-3462-40e8-97bc-a333562a6264",
        "parentId" : "33f34bfc-123a-4bda-9208-c63ddec913c9",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "thanks for catching this one, I missed it. I had checked this and most are fine due to using set and just setting status which would already be set, but I somehow missed this one. I'll fix ",
        "createdAt" : "2020-10-22T13:26:10Z",
        "updatedAt" : "2020-10-28T13:50:33Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "fafab946-1f3c-456a-9ce5-aada38f2adc6",
        "parentId" : "33f34bfc-123a-4bda-9208-c63ddec913c9",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "I updated this but I actually found a pre-existing bug where we weren't incrementing this when we excluded a node - which implicitly excludes the executors",
        "createdAt" : "2020-10-23T15:51:16Z",
        "updatedAt" : "2020-10-28T13:50:33Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "b38dd66500cdd4a12f78cca1eeacf116f0ea5b4e",
    "line" : 149,
    "diffHunk" : "@@ -1,1 +402,406 @@      if (excluded) {\n        appStatusSource.foreach(_.BLACKLISTED_EXECUTORS.inc())\n        appStatusSource.foreach(_.EXCLUDED_EXECUTORS.inc())\n      } else {\n        appStatusSource.foreach(_.UNBLACKLISTED_EXECUTORS.inc())"
  },
  {
    "id" : "23f4caa5-dc04-48e6-ac7d-53552eb07327",
    "prId" : 29082,
    "prUrl" : "https://github.com/apache/spark/pull/29082#pullrequestreview-492504272",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "07ed30b1-2da5-43a9-b336-5660854981e9",
        "parentId" : null,
        "authorId" : "3a227965-84e0-47cf-9974-11293764f028",
        "body" : "As I mention in another place, `message` field in `FailureReason` seems to be redundant.\r\nIf we can remove it away,  can we just parse error message and build `FailureReason` in `computeFailureSummary`?",
        "createdAt" : "2020-09-21T11:52:36Z",
        "updatedAt" : "2021-01-26T02:07:13Z",
        "lastEditedBy" : "3a227965-84e0-47cf-9974-11293764f028",
        "tags" : [
        ]
      }
    ],
    "commit" : "769ce48b4f1500e65c9548e2bdc93d5c8e01079f",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +659,663 @@      task.errorMessage = errorMessage\n      task.failureReason = event.reason match {\n        case e: ExceptionFailure =>\n          Some(new v1.FailureReason(e.className, e.description, e.toErrorString))\n        case e: ExecutorLostFailure =>"
  },
  {
    "id" : "35948917-fcb5-4a90-bba2-b9b9cfbaf069",
    "prId" : 29082,
    "prUrl" : "https://github.com/apache/spark/pull/29082#pullrequestreview-515861549",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "51366170-a978-4877-ba60-0ae25c8589bd",
        "parentId" : null,
        "authorId" : "3a227965-84e0-47cf-9974-11293764f028",
        "body" : "As I mentioned in another place, `message` in FailureReason can be redundant.\r\nIf we can remove it, we can just parse error message and build `FailureReason` in `computeFailureSummary` regardless of `event.reason`.",
        "createdAt" : "2020-09-21T12:30:50Z",
        "updatedAt" : "2021-01-26T02:07:13Z",
        "lastEditedBy" : "3a227965-84e0-47cf-9974-11293764f028",
        "tags" : [
        ]
      },
      {
        "id" : "4574160c-84f4-4b9c-918f-593a2b5e067f",
        "parentId" : "51366170-a978-4877-ba60-0ae25c8589bd",
        "authorId" : "cf8c9534-0cf3-4aad-8ead-54c363cfa86e",
        "body" : "Yes, in previous version, I directly passed error message to build failure reason, but I found it hard, complex and error-prone. This due to different failure reason has different format:\r\n1. `ExceptionFailure` is most common and easy one. The message is only exception's message, we can parse it from errorString.\r\n2. `ExecutorLostFailure` errorString has format like `ExecutorLostFailure (executor 72 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 123153 ms`. We can split the message out using keyword `Reason:`\r\n3. `FetchFailed` errorString has format like `FetchFailed($bmAddressString, shuffleId=$shuffleId, mapId=$mapId, reduceId=$reduceId, message=\\n$message\\n)`. We need to parse message from this.\r\n\r\nIn summary, parsing will rely on and couple with specific format of different failure reason. If in the future, the format changed, it's easy to break. So I chose the solution to directly get from TaskEndReason. ",
        "createdAt" : "2020-09-24T16:52:16Z",
        "updatedAt" : "2021-01-26T02:07:13Z",
        "lastEditedBy" : "cf8c9534-0cf3-4aad-8ead-54c363cfa86e",
        "tags" : [
        ]
      },
      {
        "id" : "2acce83b-57d2-473c-943c-65b45ba8a13c",
        "parentId" : "51366170-a978-4877-ba60-0ae25c8589bd",
        "authorId" : "cf8c9534-0cf3-4aad-8ead-54c363cfa86e",
        "body" : "@sarutak Any more comments on this?",
        "createdAt" : "2020-10-23T17:19:05Z",
        "updatedAt" : "2021-01-26T02:07:13Z",
        "lastEditedBy" : "cf8c9534-0cf3-4aad-8ead-54c363cfa86e",
        "tags" : [
        ]
      }
    ],
    "commit" : "769ce48b4f1500e65c9548e2bdc93d5c8e01079f",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +658,662 @@      }\n      task.errorMessage = errorMessage\n      task.failureReason = event.reason match {\n        case e: ExceptionFailure =>\n          Some(new v1.FailureReason(e.className, e.description, e.toErrorString))"
  },
  {
    "id" : "afa4cb34-c34d-4f81-9e36-032b1b9017a5",
    "prId" : 27306,
    "prUrl" : "https://github.com/apache/spark/pull/27306#pullrequestreview-351523711",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "759aa019-9662-441d-9060-8cb052eb4944",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Can we add a UT for this code logic?",
        "createdAt" : "2020-01-22T05:22:21Z",
        "updatedAt" : "2020-02-03T05:09:39Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "b68f85ad-0b66-4d5b-a0fd-478c94f67b56",
        "parentId" : "759aa019-9662-441d-9060-8cb052eb4944",
        "authorId" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "body" : "It should be easy to add a new test case in `AppStatusListenerSuite` to enforce the behavior.",
        "createdAt" : "2020-01-22T23:24:03Z",
        "updatedAt" : "2020-02-03T05:09:39Z",
        "lastEditedBy" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "tags" : [
        ]
      },
      {
        "id" : "7a4021c9-33c2-43f8-a57f-a51714c5f9ae",
        "parentId" : "759aa019-9662-441d-9060-8cb052eb4944",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "Done in https://github.com/apache/spark/pull/27418/commits/f3fabdc812917c3e13283d380b99656ecc06cd93.",
        "createdAt" : "2020-01-31T14:03:35Z",
        "updatedAt" : "2020-02-03T05:09:39Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "5f123173bc03cb057c97fb98f5ea5879452e00ba",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +777,781 @@      // used memory to avoid overlapped count.\n      exec.usedOnHeap = 0\n      exec.usedOffHeap = 0\n    }\n    exec.isActive = true"
  },
  {
    "id" : "5367abfd-c41c-4d29-ac48-2e44570f707d",
    "prId" : 26821,
    "prUrl" : "https://github.com/apache/spark/pull/26821#pullrequestreview-331022756",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c8344e7e-0e47-4e64-9324-7608f655ab7a",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "This should be placed before `flush()`.",
        "createdAt" : "2019-12-12T08:57:38Z",
        "updatedAt" : "2019-12-12T09:22:05Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "b6b2c5a3cc46d25595bfa7e187846187dd5ac806",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +103,107 @@      val now = System.nanoTime()\n      flush(update(_, now))\n      if (appId != null) {\n        // If incremental parsing is enabled, write the listener data to the store\n        val data = new AppStatusListenerData(appId, attemptId, liveStages, liveJobs,"
  },
  {
    "id" : "c43b1b5f-366c-4094-99df-a3ed7d84b015",
    "prId" : 26821,
    "prUrl" : "https://github.com/apache/spark/pull/26821#pullrequestreview-331022756",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0ac5f52e-88b3-4247-825e-6a4372493979",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "I'd rather have some case class (or just simply tuple) to store the information altogether and name it which clearly representing \"incremental parsing is used\". Checking whether appId is available or not to determine whether incremental parsing is used or not wouldn't be easy to understand for newcomer of the code.",
        "createdAt" : "2019-12-12T09:00:08Z",
        "updatedAt" : "2019-12-12T09:22:05Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "b6b2c5a3cc46d25595bfa7e187846187dd5ac806",
    "line" : 27,
    "diffHunk" : "@@ -1,1 +117,121 @@      // If incremental parsing is enabled, read and update the listener data\n      // from store\n      this.appId = appId\n      this.attemptId = attemptId\n      try {"
  },
  {
    "id" : "6df96974-8563-4588-b051-1f17cc0c2ad6",
    "prId" : 26637,
    "prUrl" : "https://github.com/apache/spark/pull/26637#pullrequestreview-322820295",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "15659de5-4dbf-4ddd-b4ff-257121fd5d66",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Shall we change `stageId == -1`? Of course, CONSTANT will be better like `stageId == SOME_CONSTANT_ID`.",
        "createdAt" : "2019-11-23T22:35:24Z",
        "updatedAt" : "2019-11-26T23:44:39Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "be51c10c-d5de-4a3e-9fc7-dba0431b09a6",
        "parentId" : "15659de5-4dbf-4ddd-b4ff-257121fd5d66",
        "authorId" : "3a227965-84e0-47cf-9974-11293764f028",
        "body" : "Changes in this PR is to care the case when a job has 1 or more partitions (tasks).\r\nSo, I think that `numTasks > 0` makes sense rather than `stageId != -1` .\r\nOr how about making the `while` loop  containing this code block like `while (job.stageIds.nonEmpty && it.hasNext())`\r\nWhat do you think?",
        "createdAt" : "2019-11-25T06:46:12Z",
        "updatedAt" : "2019-11-26T23:44:39Z",
        "lastEditedBy" : "3a227965-84e0-47cf-9974-11293764f028",
        "tags" : [
        ]
      },
      {
        "id" : "28ffe4c6-9b58-404a-a83a-84d76a48145c",
        "parentId" : "15659de5-4dbf-4ddd-b4ff-257121fd5d66",
        "authorId" : "3a227965-84e0-47cf-9974-11293764f028",
        "body" : "FYI:\r\nThis change is to avoid marking a stage which has no partitions as skipped.\r\n<img width=\"1436\" alt=\"スクリーンショット 2019-11-26 17 12 15\" src=\"https://user-images.githubusercontent.com/4736016/69615082-a4304180-1077-11ea-9459-791d8074852b.png\">\r\n",
        "createdAt" : "2019-11-26T09:08:37Z",
        "updatedAt" : "2019-11-26T23:44:39Z",
        "lastEditedBy" : "3a227965-84e0-47cf-9974-11293764f028",
        "tags" : [
        ]
      }
    ],
    "commit" : "dbb73e3f21a7bccd7af3a217303018eb9b206c8b",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +416,420 @@          val stage = e.getValue()\n          // If a stage has no partitions(tasks), the stage should not be marked as skipped.\n          if (v1.StageStatus.PENDING.equals(stage.status) && stage.info.numTasks > 0) {\n            stage.status = v1.StageStatus.SKIPPED\n            job.skippedStages += stage.info.stageId"
  },
  {
    "id" : "2de5f787-826f-4920-bf98-e85450b7a4f3",
    "prId" : 25943,
    "prUrl" : "https://github.com/apache/spark/pull/25943#pullrequestreview-299735323",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "da549ffa-db8b-47b5-8d61-d9d4f92ecc27",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "This may need to cooperate with SPARK-28594's config.",
        "createdAt" : "2019-09-26T12:50:01Z",
        "updatedAt" : "2019-10-16T15:23:01Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "e742d9e8-fa68-4b8d-ae6b-9b79cfeac47d",
        "parentId" : "da549ffa-db8b-47b5-8d61-d9d4f92ecc27",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "I still think it should be run by default to guarantee AppStatusListener and KVStore is in sync, but let's wait for another voices.",
        "createdAt" : "2019-10-04T08:12:02Z",
        "updatedAt" : "2019-10-16T15:23:01Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "19598e40-6763-473d-819e-a610388be7db",
        "parentId" : "da549ffa-db8b-47b5-8d61-d9d4f92ecc27",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Hmm... I've found we don't recover AppStatusSource as well since it will not be passed when `live == false`. If possible I'd put assertion whether KVStore is empty when `live == true` then, but it depends on the possibility. If not, we may have to live with.",
        "createdAt" : "2019-10-07T03:13:21Z",
        "updatedAt" : "2019-10-16T15:23:01Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "bc26265f-f7bf-40c4-a4fe-a2e38b5f0498",
        "parentId" : "da549ffa-db8b-47b5-8d61-d9d4f92ecc27",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "> If possible I'd put assertion whether KVStore is empty when live == true then, but it depends on the possibility. \r\n\r\nI think our current usage of a live(true) `AppStatusListener` guarantees the empty KVStore at the initialization step. So I don't understand well for `depends on the possibility` ? Do I miss something ?\r\n\r\nAnd yet, it seems that we don't have `isEmpty` api for KVStore. Otherwise, I could put an assertion to reach a compromises between us. ",
        "createdAt" : "2019-10-09T14:46:57Z",
        "updatedAt" : "2019-10-16T15:23:01Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "463c2818-ac92-458a-8eef-23de0482b28b",
        "parentId" : "da549ffa-db8b-47b5-8d61-d9d4f92ecc27",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "> I think our current usage of a live(true) AppStatusListener guarantees the empty KVStore at the initialization step. So I don't understand well for depends on the possibility ?\r\n\r\nThat's a \"context\" we might have a chance to break eventually and as a side-effect it will break here. I'm in favor of doing defensive programming: if there're preconditions it should be mentioned anywhere or asserted. But I agree we don't have isEmpty api for KVStore - let's leave it as it is.",
        "createdAt" : "2019-10-09T22:34:19Z",
        "updatedAt" : "2019-10-16T15:23:01Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "46715b3d2c52acd97fdca2e2034393f7e49e1b4f",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +107,111 @@  // visible for tests\n  private[spark] def recoverLiveEntities(): Unit = {\n    if (!live) {\n      kvstore.view(classOf[JobDataWrapper])\n        .asScala.filter(_.info.status == JobExecutionStatus.RUNNING)"
  },
  {
    "id" : "be10bd90-1896-41bf-8745-8fcfd9115765",
    "prId" : 25943,
    "prUrl" : "https://github.com/apache/spark/pull/25943#pullrequestreview-297260524",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "18e4faf3-e332-493b-87b2-026432930320",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "nit: indentation of block for foreach looks to be off",
        "createdAt" : "2019-10-04T08:13:53Z",
        "updatedAt" : "2019-10-16T15:23:01Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "46715b3d2c52acd97fdca2e2034393f7e49e1b4f",
    "line" : 48,
    "diffHunk" : "@@ -1,1 +127,131 @@          val jobs = liveJobs.values.filter(_.stageIds.contains(stageId)).toSeq\n          stageData.toLiveStage(jobs)\n        }.foreach { stage =>\n          val stageId = stage.info.stageId\n          val stageAttempt = stage.info.attemptNumber()"
  },
  {
    "id" : "7411544b-3d6e-4901-82a7-80662aaff0fb",
    "prId" : 25943,
    "prUrl" : "https://github.com/apache/spark/pull/25943#pullrequestreview-297260524",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "05c4640b-1837-4772-9494-7d7d40f241a5",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "nit: same here, but just a suggestion (as the indentation is not dramatically changed)",
        "createdAt" : "2019-10-04T08:17:01Z",
        "updatedAt" : "2019-10-16T15:23:01Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "46715b3d2c52acd97fdca2e2034393f7e49e1b4f",
    "line" : 83,
    "diffHunk" : "@@ -1,1 +162,166 @@        }\n\n      kvstore.view(classOf[RDDStorageInfoWrapper]).asScala\n        .foreach { rddWrapper =>\n          val liveRdd = rddWrapper.toLiveRDD(liveExecutors)"
  },
  {
    "id" : "f8cd78ae-efa5-4f91-89fa-231bbb29c6db",
    "prId" : 25943,
    "prUrl" : "https://github.com/apache/spark/pull/25943#pullrequestreview-297260524",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d32c063d-8190-49a6-9854-15a99b0088f0",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "nit: same here",
        "createdAt" : "2019-10-04T08:17:06Z",
        "updatedAt" : "2019-10-16T15:23:01Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "46715b3d2c52acd97fdca2e2034393f7e49e1b4f",
    "line" : 89,
    "diffHunk" : "@@ -1,1 +168,172 @@        }\n\n      kvstore.view(classOf[PoolData]).asScala.foreach { poolData =>\n        val schedulerPool = poolData.toSchedulerPool\n        pools.put(schedulerPool.name, schedulerPool)"
  },
  {
    "id" : "01f4b30c-1742-47e4-b941-ef8964c64134",
    "prId" : 25943,
    "prUrl" : "https://github.com/apache/spark/pull/25943#pullrequestreview-297260524",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "63884188-bb3e-4da5-ade6-e20566eb0b67",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "nit: unnecessary two empty lines - one empty line is enough",
        "createdAt" : "2019-10-04T08:17:30Z",
        "updatedAt" : "2019-10-16T15:23:01Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "46715b3d2c52acd97fdca2e2034393f7e49e1b4f",
    "line" : 67,
    "diffHunk" : "@@ -1,1 +146,150 @@              }\n            }\n\n          kvstore.view(classOf[TaskDataWrapper])\n            .parent(Array(stageId, stageAttempt))"
  },
  {
    "id" : "33fac5cb-6e44-4a57-bec3-dbc4d099f642",
    "prId" : 25943,
    "prUrl" : "https://github.com/apache/spark/pull/25943#pullrequestreview-300189695",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9c6bb7c0-ebdf-47eb-ab66-30a5512c3a22",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Safer condition would be either `UNKNOWN` or `RUNNING`, but I guess it shouldn't be UNKNOWN so seems OK.",
        "createdAt" : "2019-10-07T02:50:09Z",
        "updatedAt" : "2019-10-16T15:23:01Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "bf1ba6b8-3d00-475d-8b49-6204a3941580",
        "parentId" : "9c6bb7c0-ebdf-47eb-ab66-30a5512c3a22",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Yeah. I didn't see any place we set Job status to `UNKNOWN`.",
        "createdAt" : "2019-10-10T16:13:40Z",
        "updatedAt" : "2019-10-16T15:23:01Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "46715b3d2c52acd97fdca2e2034393f7e49e1b4f",
    "line" : 30,
    "diffHunk" : "@@ -1,1 +109,113 @@    if (!live) {\n      kvstore.view(classOf[JobDataWrapper])\n        .asScala.filter(_.info.status == JobExecutionStatus.RUNNING)\n        .map(_.toLiveJob).foreach(job => liveJobs.put(job.jobId, job))\n"
  },
  {
    "id" : "53a84b57-3a91-4127-9172-4a18390cab2c",
    "prId" : 25943,
    "prUrl" : "https://github.com/apache/spark/pull/25943#pullrequestreview-301353128",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "43ea8f82-da35-4b0a-8319-80a9a347c587",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Logically, `activeTasks > 0` is counted in `liveStages` regardless of status:\r\n\r\nhttps://github.com/apache/spark/blob/5a512e86e94593bc004a35101ad6497e20c13e0a/core/src/main/scala/org/apache/spark/status/AppStatusListener.scala#L605-L614\r\n\r\nhttps://github.com/apache/spark/blob/5a512e86e94593bc004a35101ad6497e20c13e0a/core/src/main/scala/org/apache/spark/status/AppStatusListener.scala#L743-L748\r\n\r\n... except status is SKIPPED:\r\n\r\nhttps://github.com/apache/spark/blob/5a512e86e94593bc004a35101ad6497e20c13e0a/core/src/main/scala/org/apache/spark/status/AppStatusListener.scala#L408-L429\r\n\r\nso the condition is actually much complicated than that.",
        "createdAt" : "2019-10-07T03:20:44Z",
        "updatedAt" : "2019-10-16T15:23:01Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "718d4b5a-ac98-49e0-bd74-eaa4471b72f7",
        "parentId" : "43ea8f82-da35-4b0a-8319-80a9a347c587",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Good catch here ! And I attached another possible condition for this:\r\n\r\n`(stageData.info.numActiveTasks > 0 && stageData.info.status != v1.StageStatus.SKIPPED)`",
        "createdAt" : "2019-10-14T14:44:17Z",
        "updatedAt" : "2019-10-16T15:23:01Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "46715b3d2c52acd97fdca2e2034393f7e49e1b4f",
    "line" : 41,
    "diffHunk" : "@@ -1,1 +120,124 @@      kvstore.view(classOf[StageDataWrapper]).asScala\n        .filter { stageData =>\n          stageData.info.status == v1.StageStatus.PENDING ||\n            stageData.info.status == v1.StageStatus.ACTIVE ||\n            (stageData.info.numActiveTasks > 0 && stageData.info.status != v1.StageStatus.SKIPPED)"
  },
  {
    "id" : "5eac97c8-ca1f-46a8-90ee-53e21cd828d4",
    "prId" : 24303,
    "prUrl" : "https://github.com/apache/spark/pull/24303#pullrequestreview-223355543",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c8324164-4190-4802-a057-2718b0d961d5",
        "parentId" : null,
        "authorId" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "body" : "Hmm... in the bug you mention that job-level data is not being updated. Is that the only case? Because if that's it, then this looks like overkill. You could e.g. update the jobs in the code that handles `event.accumUpdates` above, or even just flush jobs specifically, instead of everything.\r\n\r\nDoing a full flush here seems like overkill and a little expensive when you think about many heartbeats arriving in a short period (even when considering `lastFlushTimeNs`).",
        "createdAt" : "2019-04-04T23:33:21Z",
        "updatedAt" : "2019-04-08T22:25:03Z",
        "lastEditedBy" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "tags" : [
        ]
      },
      {
        "id" : "d710c1d2-6d83-4793-8985-c7dcf54e3ea5",
        "parentId" : "c8324164-4190-4802-a057-2718b0d961d5",
        "authorId" : "2d6b46ba-4100-4c4d-9341-fff5e39647ec",
        "body" : "> Hmm... in the bug you mention that job-level data is not being updated. Is that the only case?\r\n\r\nI also noticed that executor active tasks sometimes could be wrong. That's why I decided to flush everything to make sure we don't miss any places. It's also hard to maintain if we need to manually flush in every place.\r\n\r\nIdeally, we should flush periodically so that it doesn't depend on receiving a Spark event. But then I will need to add a new event type and post to the listener bus. That's overkilled.\r\n\r\n> when you think about many heartbeats arriving in a short period\r\n\r\nAt least there will be at least 100ms between each flush. As long as we process heart beats very fast, most of them won't trigger the flush.",
        "createdAt" : "2019-04-05T00:02:47Z",
        "updatedAt" : "2019-04-08T22:25:03Z",
        "lastEditedBy" : "2d6b46ba-4100-4c4d-9341-fff5e39647ec",
        "tags" : [
        ]
      },
      {
        "id" : "2d8e038d-d700-4a60-8db5-b8a180ca02ce",
        "parentId" : "c8324164-4190-4802-a057-2718b0d961d5",
        "authorId" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "body" : "If the goal is to use the hearbeats as some trigger for flushing, how about using some ratio of the heartbeat period instead of `liveUpdatePeriodNs` to control whether to flush everything?\r\n\r\nReally large apps can get a little backed up when processing hearbeats from lots and lots of busy executors, and this would make it a little worse.",
        "createdAt" : "2019-04-05T00:11:33Z",
        "updatedAt" : "2019-04-08T22:25:03Z",
        "lastEditedBy" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "tags" : [
        ]
      },
      {
        "id" : "f6f6aa8d-7418-4b40-8398-6cfc3e6165b4",
        "parentId" : "c8324164-4190-4802-a057-2718b0d961d5",
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "The update only happens in live UI, which should be fine in general.  For real large apps, will it help by setting `LIVE_ENTITY_UPDATE_PERIOD` to a larger value? Setting a ratio of heartbeat period seems a bit complex.",
        "createdAt" : "2019-04-05T05:37:36Z",
        "updatedAt" : "2019-04-08T22:25:03Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "b47f1310-f063-4f78-8402-f301687621f4",
        "parentId" : "c8324164-4190-4802-a057-2718b0d961d5",
        "authorId" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "body" : "> only happens in live UI\r\n\r\nThe \"don't write to the store all the time\" thing was added specifically to speed up live UIs, because copying + writing the data (even to the memory store) becomes really expensive when you have event storms (think thousands of tasks starting and stopping in a very short period).\r\n\r\n>  setting LIVE_ENTITY_UPDATE_PERIOD to a larger value\r\n\r\nWe should avoid requiring configuration tweaks for things not to break, when possible.",
        "createdAt" : "2019-04-05T15:59:13Z",
        "updatedAt" : "2019-04-08T22:25:03Z",
        "lastEditedBy" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "tags" : [
        ]
      }
    ],
    "commit" : "1c5307161c1731112b8d7b096778d0f56101d04b",
    "line" : 41,
    "diffHunk" : "@@ -1,1 +846,850 @@    // `max(heartbeat interval, liveUpdateMinFlushPeriod)`.\n    if (now - lastFlushTimeNs > liveUpdateMinFlushPeriod) {\n      flush(maybeUpdate(_, now))\n      // Re-get the current system time because `flush` may be slow and `now` is stale.\n      lastFlushTimeNs = System.nanoTime()"
  }
]