[
  {
    "id" : "d27645fd-bf2f-4f58-b0b4-0fe5780fa0bf",
    "prId" : 26911,
    "prUrl" : "https://github.com/apache/spark/pull/26911#pullrequestreview-332800582",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "44cad677-60d8-48c7-b6d2-3eee37e017d7",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Problem here was using a non-ref type in a Java Map. This is only really used for a JobConf anyway",
        "createdAt" : "2019-12-16T19:18:14Z",
        "updatedAt" : "2019-12-18T00:42:58Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      }
    ],
    "commit" : "a6de24a64b8aa23699a7527919389764e935f1e5",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +406,410 @@   * the local process.\n   */\n  def getCachedMetadata(key: String): AnyRef = SparkEnv.get.hadoopJobMetadata.get(key)\n\n  private def putCachedMetadata(key: String, value: AnyRef): Unit ="
  }
]