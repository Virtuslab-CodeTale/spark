[
  {
    "id" : "d27645fd-bf2f-4f58-b0b4-0fe5780fa0bf",
    "prId" : 26911,
    "prUrl" : "https://github.com/apache/spark/pull/26911#pullrequestreview-332800582",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "44cad677-60d8-48c7-b6d2-3eee37e017d7",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Problem here was using a non-ref type in a Java Map. This is only really used for a JobConf anyway",
        "createdAt" : "2019-12-16T19:18:14Z",
        "updatedAt" : "2019-12-18T00:42:58Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      }
    ],
    "commit" : "a6de24a64b8aa23699a7527919389764e935f1e5",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +406,410 @@   * the local process.\n   */\n  def getCachedMetadata(key: String): AnyRef = SparkEnv.get.hadoopJobMetadata.get(key)\n\n  private def putCachedMetadata(key: String, value: AnyRef): Unit ="
  },
  {
    "id" : "892f5247-e1b1-4b9d-81f0-0f8a959e311a",
    "prId" : 25391,
    "prUrl" : "https://github.com/apache/spark/pull/25391#pullrequestreview-272645446",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e301cfc4-7ea1-4e49-b513-496f91ae2ed5",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I am okay with it",
        "createdAt" : "2019-08-08T15:14:11Z",
        "updatedAt" : "2019-08-08T15:14:12Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "4d58719934a1f9374d730f134df21b21058a1275",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +216,220 @@          if (Utils.isFileSplittable(path, codecFactory)) {\n            logWarning(s\"Loading one large file ${path.toString} with only one partition, \" +\n              s\"we can increase partition numbers for improving performance.\")\n          } else {\n            logWarning(s\"Loading one large unsplittable file ${path.toString} with only one \" +"
  },
  {
    "id" : "a1c6eef6-5a4d-49ea-8221-26fcf5618344",
    "prId" : 25134,
    "prUrl" : "https://github.com/apache/spark/pull/25134#pullrequestreview-270096385",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5e500c45-c137-4430-a990-2860b229e7d7",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Is it always `sc.textFile`? Many datasource V1 implementation still uses `hadoopFile` or `newHadoopFile` often.",
        "createdAt" : "2019-08-02T10:11:20Z",
        "updatedAt" : "2019-08-02T10:11:20Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "801c6e30d785142dd5d03716b619dc48b87d125a",
    "line" : 29,
    "diffHunk" : "@@ -1,1 +217,221 @@            logWarning(s\"Loading one large file ${path.toString} with only one partition, \" +\n              s\"we can increase partition numbers by the `minPartitions` argument in method \" +\n              \"`sc.textFile`\")\n          } else {\n            logWarning(s\"Loading one large unsplittable file ${path.toString} with only one \" +"
  },
  {
    "id" : "a3b37903-bde6-40cc-b551-c8249a88604f",
    "prId" : 25134,
    "prUrl" : "https://github.com/apache/spark/pull/25134#pullrequestreview-270097095",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "13e9ed43-cffa-47f1-8911-ed3ff9743bba",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "nit `toString` won't be needed since here's string interpolation.",
        "createdAt" : "2019-08-02T10:13:03Z",
        "updatedAt" : "2019-08-02T10:13:03Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "801c6e30d785142dd5d03716b619dc48b87d125a",
    "line" : 27,
    "diffHunk" : "@@ -1,1 +215,219 @@          val codecFactory = new CompressionCodecFactory(jobConf)\n          if (Utils.isFileSplittable(path, codecFactory)) {\n            logWarning(s\"Loading one large file ${path.toString} with only one partition, \" +\n              s\"we can increase partition numbers by the `minPartitions` argument in method \" +\n              \"`sc.textFile`\")"
  },
  {
    "id" : "aff1a6fd-2e55-4db9-ab41-189201a18315",
    "prId" : 25134,
    "prUrl" : "https://github.com/apache/spark/pull/25134#pullrequestreview-270097153",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8b1e0241-4385-4552-ab4b-f93e20b95773",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "nit: and `s` isn't needed too",
        "createdAt" : "2019-08-02T10:13:11Z",
        "updatedAt" : "2019-08-02T10:13:11Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "801c6e30d785142dd5d03716b619dc48b87d125a",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +216,220 @@          if (Utils.isFileSplittable(path, codecFactory)) {\n            logWarning(s\"Loading one large file ${path.toString} with only one partition, \" +\n              s\"we can increase partition numbers by the `minPartitions` argument in method \" +\n              \"`sc.textFile`\")\n          } else {"
  }
]