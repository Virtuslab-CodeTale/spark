[
  {
    "id" : "24a24744-ee30-495f-8508-ad6916b22e4b",
    "prId" : 32283,
    "prUrl" : "https://github.com/apache/spark/pull/32283#pullrequestreview-708904197",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b3da9ce1-7005-4e69-9e1e-42d296ca0daf",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Can we check if we're in K8s here ?",
        "createdAt" : "2021-07-16T00:14:03Z",
        "updatedAt" : "2021-07-16T00:14:03Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "e58b3228-2845-45ef-b340-2b5e2ee9b6e8",
        "parentId" : "b3da9ce1-7005-4e69-9e1e-42d296ca0daf",
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "+1 to making it k8s specific ...\r\nIdeally, we should be using daemon threads for most of our background work ...",
        "createdAt" : "2021-07-16T01:38:38Z",
        "updatedAt" : "2021-07-16T01:38:38Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "16d73a3a-5e1c-4a52-8973-0de2b37912cc",
        "parentId" : "b3da9ce1-7005-4e69-9e1e-42d296ca0daf",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "+1",
        "createdAt" : "2021-07-16T02:28:20Z",
        "updatedAt" : "2021-07-16T02:28:20Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "4813eff1-b620-4730-9023-b91a4288411b",
        "parentId" : "b3da9ce1-7005-4e69-9e1e-42d296ca0daf",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Since SPARK-34674 was released already via Apache Spark 3.1.2, I will make another JIRA for this, @HyukjinKwon , @mridulm , @Ngone51 .",
        "createdAt" : "2021-07-17T09:27:17Z",
        "updatedAt" : "2021-07-17T09:27:17Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "eb8cc3e3-8662-4d3c-8073-20f09764e08d",
        "parentId" : "b3da9ce1-7005-4e69-9e1e-42d296ca0daf",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Here is the PR.\r\n- https://github.com/apache/spark/pull/33403",
        "createdAt" : "2021-07-17T09:41:54Z",
        "updatedAt" : "2021-07-17T09:41:54Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "814e391a0f7df53c7a1ad96c895c80f5745e591e",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +954,958 @@        throw findCause(t)\n    } finally {\n      if (!isShell(args.primaryResource) && !isSqlShell(args.mainClass) &&\n        !isThriftServer(args.mainClass)) {\n        try {"
  },
  {
    "id" : "6e9361b9-0565-40b6-b8e6-387b3986c3d5",
    "prId" : 32081,
    "prUrl" : "https://github.com/apache/spark/pull/32081#pullrequestreview-707937935",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f63f82d2-ed13-4c8c-95e7-d3f232c28101",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Just reading this and https://github.com/apache/spark/pull/33154, shouldn't we enable this only w/ Kubernates (and also when it's not a Thirftserver, shall, etc.)?\r\n\r\nAlso, I think we might have to add some comments on that.",
        "createdAt" : "2021-07-04T02:18:43Z",
        "updatedAt" : "2021-07-04T02:18:44Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "b5c73a84-41f6-46d0-8caf-2b9a52ceb261",
        "parentId" : "f63f82d2-ed13-4c8c-95e7-d3f232c28101",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "cc @sunpe too FYI",
        "createdAt" : "2021-07-04T02:19:01Z",
        "updatedAt" : "2021-07-04T02:19:01Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "a1a02829-ec8d-4fd9-ac91-b56577ac5c2b",
        "parentId" : "f63f82d2-ed13-4c8c-95e7-d3f232c28101",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "@HyukjinKwon This PR was reverted once and resubmitted https://github.com/apache/spark/pull/32283 (in which Thirftserver and others are excluded).\r\n\r\nBut I agree with you that we should only do this for K8s. Otherwise, it looks like a behavior change to me.",
        "createdAt" : "2021-07-15T15:58:29Z",
        "updatedAt" : "2021-07-15T15:58:30Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "c20ec7a9-7f46-42ae-aacd-42e047bfc3bb",
        "parentId" : "f63f82d2-ed13-4c8c-95e7-d3f232c28101",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Shall we continue the discussion on the latest one, #32282 , please?",
        "createdAt" : "2021-07-15T16:32:37Z",
        "updatedAt" : "2021-07-15T16:32:37Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "754c83bd-7a46-49c9-b006-07d4a3b7ee91",
        "parentId" : "f63f82d2-ed13-4c8c-95e7-d3f232c28101",
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "@dongjoon-hyun that pr looks unrelated, right ? Not sure if I am missing something.",
        "createdAt" : "2021-07-15T17:29:58Z",
        "updatedAt" : "2021-07-15T17:29:59Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "4daa389d-27de-4259-8148-5cae8430cda4",
        "parentId" : "f63f82d2-ed13-4c8c-95e7-d3f232c28101",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Oops. It's a typo of #32283 . I tried to mention the one from @Ngone51 's comment. ",
        "createdAt" : "2021-07-15T21:57:31Z",
        "updatedAt" : "2021-07-15T21:57:31Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "9bdf1481-d28a-4bb4-9cf3-fe8a07425d84",
        "parentId" : "f63f82d2-ed13-4c8c-95e7-d3f232c28101",
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "Thanks @dongjoon-hyun ! Was not sure if there was another pr also I should be looking at :-)",
        "createdAt" : "2021-07-16T01:36:04Z",
        "updatedAt" : "2021-07-16T01:36:04Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      }
    ],
    "commit" : "e3946b459533dbeb92d873d3587cf6af7ca8ad34",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +955,959 @@    } finally {\n      try {\n        SparkContext.getActive.foreach(_.stop())\n      } catch {\n        case e: Throwable => logError(s\"Failed to close SparkContext: $e\")"
  },
  {
    "id" : "45dc4e2b-cbaf-4024-8e1f-4e7788856094",
    "prId" : 32080,
    "prUrl" : "https://github.com/apache/spark/pull/32080#pullrequestreview-630350153",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3d568b0d-55c4-4759-8016-638a61b5559e",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Please remove the extra space at the end of this line.\r\n```\r\n[error] /home/runner/work/spark/spark/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala:1034:30: Whitespace at end of line\r\n```",
        "createdAt" : "2021-04-07T18:13:57Z",
        "updatedAt" : "2021-04-07T18:13:58Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "cdb346533e2314e781d1a145f3635d16d3cc9fe0",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +1032,1036 @@          case e: SparkUserAppException =>\n            exitFn(e.exitCode)\n          case _: Throwable => \n            exitFn(1)\n        }"
  },
  {
    "id" : "b5989623-7a80-454f-90de-213dd4023c69",
    "prId" : 32080,
    "prUrl" : "https://github.com/apache/spark/pull/32080#pullrequestreview-630971609",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2f8b9653-b510-4b47-87a2-e003088087b3",
        "parentId" : null,
        "authorId" : "c89ce77d-ed7a-49e7-811e-5be0290f30a1",
        "body" : "This changes will not fix [the problem](https://issues.apache.org/jira/browse/SPARK-34674) completely.\r\nIf the main() method completes successfully,  the application will still hang in K8S.\r\nCould you consider also call to exitFn in case of successfull?",
        "createdAt" : "2021-04-07T21:44:18Z",
        "updatedAt" : "2021-04-07T21:44:20Z",
        "lastEditedBy" : "c89ce77d-ed7a-49e7-811e-5be0290f30a1",
        "tags" : [
        ]
      },
      {
        "id" : "5242f081-ca3d-4216-8e68-8a9dfccfb9c1",
        "parentId" : "2f8b9653-b510-4b47-87a2-e003088087b3",
        "authorId" : "211509dc-9936-40ab-b814-4a246e4e060e",
        "body" : "Yes, you are right.",
        "createdAt" : "2021-04-08T04:41:26Z",
        "updatedAt" : "2021-04-08T04:41:26Z",
        "lastEditedBy" : "211509dc-9936-40ab-b814-4a246e4e060e",
        "tags" : [
        ]
      }
    ],
    "commit" : "cdb346533e2314e781d1a145f3635d16d3cc9fe0",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1033,1037 @@            exitFn(e.exitCode)\n          case _: Throwable => \n            exitFn(1)\n        }\n      }"
  },
  {
    "id" : "e930a783-8bba-4ee6-b813-a69849f40396",
    "prId" : 31849,
    "prUrl" : "https://github.com/apache/spark/pull/31849#pullrequestreview-615018375",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e10bcd75-e7c5-464b-a2a7-c3d837ceb54e",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "While I agree with the intention (I faced the same problem and had to manually remove cache too), I concern that it will actually make users more confused in a way because it's Maven or Ivy's standard behaviour, and now we're changing how they work by default in Spark, which probably users wouldn't know.\r\n\r\nI know it's very unlikely but some users might want to use one cached snapshot (presumably as they know the behaviours of Maven or Ivy resolvers work). Let's say, one CI regularly publishes snapshot, and users want to test one specific version created at the specific time. After this PR, they are forced to use the newest snapshot always.\r\n",
        "createdAt" : "2021-03-18T04:47:39Z",
        "updatedAt" : "2021-03-18T04:48:12Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "c522647b-284b-43fc-b57e-ef36e0c1540b",
        "parentId" : "e10bcd75-e7c5-464b-a2a7-c3d837ceb54e",
        "authorId" : "6e99a03b-1d60-4065-9e56-3151748ea94e",
        "body" : "The major reason for this change is to reduce frictions for developers, especially new ones, to contribute to Spark external modules. \r\n\r\nI think the behavior proposed here matches the standard ones in Ivy. See \"use a naming convention like a special suffix\" in https://ant.apache.org/ivy/history/latest-milestone/bestpractices.html, and the implementation in ivy's `IBiblioResolver`: https://github.com/apache/ant-ivy/blob/master/src/java/org/apache/ivy/plugins/resolver/IBiblioResolver.java#L88-L92\r\n",
        "createdAt" : "2021-03-18T06:27:00Z",
        "updatedAt" : "2021-03-18T06:27:00Z",
        "lastEditedBy" : "6e99a03b-1d60-4065-9e56-3151748ea94e",
        "tags" : [
        ]
      },
      {
        "id" : "c5f71b0e-1e9f-4e7a-90a1-23da176af5e2",
        "parentId" : "e10bcd75-e7c5-464b-a2a7-c3d837ceb54e",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Thanks @bozhang2820 for correcting me. Yeah, I think it makes sense.",
        "createdAt" : "2021-03-18T06:30:52Z",
        "updatedAt" : "2021-03-18T06:30:52Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "2e8d869e28adfbdc5d54517199005dc21e0d9b9e",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +1155,1159 @@    cr.setName(\"spark-list\")\n    cr.setChangingMatcher(PatternMatcher.REGEXP)\n    cr.setChangingPattern(\".*-SNAPSHOT\")\n\n    val localM2 = new IBiblioResolver"
  },
  {
    "id" : "b774765b-554b-44f6-9534-452e2b272cac",
    "prId" : 30922,
    "prUrl" : "https://github.com/apache/spark/pull/30922#pullrequestreview-561198665",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c4f5970f-1f3a-4134-9c78-d6e352778bc8",
        "parentId" : null,
        "authorId" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "body" : "@AngersZhuuuu  FYI you missed updating the `@return` Scaladoc tag here, as well as the description which explicitly mentions a comma-delimited list. Would you mind submitting a follow-on to update?",
        "createdAt" : "2021-01-04T17:17:29Z",
        "updatedAt" : "2021-01-04T17:18:03Z",
        "lastEditedBy" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "tags" : [
        ]
      }
    ],
    "commit" : "4d8849644ae25ce94e5afbcc9b7270fe909ae869",
    "line" : 33,
    "diffHunk" : "@@ -1,1 +1200,1204 @@   * @param cacheDirectory directory where jars are cached\n   * @return a comma-delimited list of paths for the dependencies\n   */\n  def resolveDependencyPaths(\n      artifacts: Array[AnyRef],"
  },
  {
    "id" : "3ea786c4-976a-4fa8-a830-527b46afecf1",
    "prId" : 30895,
    "prUrl" : "https://github.com/apache/spark/pull/30895#pullrequestreview-557431356",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bba87a32-7ef9-4003-8d0e-2f043adfdd40",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "nit. Just a question. Why this is different from the original patch?\r\n\r\nThe original patch didn't change the order of lines here. So, `pyFiles` should be at the first instead of `jars`.",
        "createdAt" : "2020-12-22T23:18:18Z",
        "updatedAt" : "2020-12-22T23:49:41Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "67f09daa-c09c-4503-aa0c-ef5d97e04dfb",
        "parentId" : "bba87a32-7ef9-4003-8d0e-2f043adfdd40",
        "authorId" : "714bd96c-2aea-4b32-9fc8-de23712eb87d",
        "body" : "Oops, this got it while resolving a conflict with cherry-pick (`spark.executor.instances` was removed in branch-3). I've put it back in order (although I believe it should not affect anything).",
        "createdAt" : "2020-12-22T23:51:09Z",
        "updatedAt" : "2020-12-22T23:51:09Z",
        "lastEditedBy" : "714bd96c-2aea-4b32-9fc8-de23712eb87d",
        "tags" : [
        ]
      },
      {
        "id" : "c3802f22-989a-4116-9079-d3f6f8df064d",
        "parentId" : "bba87a32-7ef9-4003-8d0e-2f043adfdd40",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Thanks!",
        "createdAt" : "2020-12-22T23:53:13Z",
        "updatedAt" : "2020-12-22T23:53:13Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "d621d0dcae6b7153399f9232f44f62b3d99414ee",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +526,530 @@        mergeFn = Some(mergeFileLists(_, _))),\n      OptionAssigner(args.jars, YARN, ALL_DEPLOY_MODES, confKey = \"spark.yarn.dist.jars\",\n        mergeFn = Some(mergeFileLists(_, _))),\n      OptionAssigner(args.files, YARN, ALL_DEPLOY_MODES, confKey = \"spark.yarn.dist.files\",\n        mergeFn = Some(mergeFileLists(_, _))),"
  },
  {
    "id" : "93cd11f8-bc4d-45b7-a85e-bd9ae4ba4c27",
    "prId" : 30735,
    "prUrl" : "https://github.com/apache/spark/pull/30735#pullrequestreview-551848329",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8bd2884b-4eab-44f2-8b51-4d51dfd6f190",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "`spark.files` looks having the same issue too. However, I would like to avoid dealing all together in this PR.",
        "createdAt" : "2020-12-11T13:09:56Z",
        "updatedAt" : "2020-12-13T02:04:32Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "6df88715-519d-4aa9-a723-42766b4db463",
        "parentId" : "8bd2884b-4eab-44f2-8b51-4d51dfd6f190",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "did you file a separate lira then?",
        "createdAt" : "2020-12-14T14:42:40Z",
        "updatedAt" : "2020-12-14T14:42:40Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "5c73cebe-9969-45ec-9b99-64e5994d1221",
        "parentId" : "8bd2884b-4eab-44f2-8b51-4d51dfd6f190",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Sure, will file a JIRA.",
        "createdAt" : "2020-12-14T19:10:06Z",
        "updatedAt" : "2020-12-14T19:10:06Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "2aaf9a86-2381-450f-9081-8ea42dfc4d56",
        "parentId" : "8bd2884b-4eab-44f2-8b51-4d51dfd6f190",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "SPARK-33782",
        "createdAt" : "2020-12-14T20:02:21Z",
        "updatedAt" : "2020-12-14T20:02:21Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "5f13c30a1d6a9df0b48f161d2d5d28b75c149bf4",
    "line" : 31,
    "diffHunk" : "@@ -1,1 +400,404 @@          // SPARK-33748: this mimics the behaviour of Yarn cluster mode. If the driver is running\n          // in cluster mode, the archives should be available in the driver's current working\n          // directory too.\n          Utils.stringToSeq(localArchives).map(Utils.resolveURI).zip(resolvedUris).map {\n            case (localArchive, resolvedUri) =>"
  },
  {
    "id" : "3b46df9c-f293-4ace-8a0a-5f89d43f8364",
    "prId" : 29874,
    "prUrl" : "https://github.com/apache/spark/pull/29874#pullrequestreview-514901312",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1ff16c66-f08f-41b5-85d6-1120c53177d6",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Is the default value always `https://repo1.maven.org/maven2/` for all user environments?",
        "createdAt" : "2020-10-22T15:43:46Z",
        "updatedAt" : "2020-10-22T22:12:54Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "29173c20-d40b-405c-98ed-60ee6cf5f948",
        "parentId" : "1ff16c66-f08f-41b5-85d6-1120c53177d6",
        "authorId" : "d7035c07-41f3-4f6f-aae3-8ef2343ccee6",
        "body" : "Yes. That is the default for `IBiblioResolver`.",
        "createdAt" : "2020-10-22T16:16:22Z",
        "updatedAt" : "2020-10-22T22:12:54Z",
        "lastEditedBy" : "d7035c07-41f3-4f6f-aae3-8ef2343ccee6",
        "tags" : [
        ]
      }
    ],
    "commit" : "cfb86c3608481b8767ad6d104ce3a2884388c13d",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1162,1166 @@    br.setUsepoms(true)\n    val defaultInternalRepo : Option[String] = sys.env.get(\"DEFAULT_ARTIFACT_REPOSITORY\")\n    br.setRoot(defaultInternalRepo.getOrElse(\"https://repo1.maven.org/maven2/\"))\n    br.setName(\"central\")\n    cr.add(br)"
  },
  {
    "id" : "3207ea52-5c9f-4f03-bd8c-308f0f8a42e9",
    "prId" : 29105,
    "prUrl" : "https://github.com/apache/spark/pull/29105#pullrequestreview-448271931",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cffdb0ee-740b-4586-ba96-2b663adf2413",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "We already make `core` module to pass Scala 2.13 compilation three days ago. Is this required?\r\n- https://github.com/apache/spark/commit/3ad4863673fc46080dda963be3055a3e554cfbc7\r\n\r\ncc @srowen ",
        "createdAt" : "2020-07-14T16:31:08Z",
        "updatedAt" : "2020-07-14T16:31:09Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "8d94634872fd234f8c349c7358d26c6618382d86",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +654,658 @@          (deployMode & opt.deployMode) != 0 &&\n          (clusterManager & opt.clusterManager) != 0) {\n        if (opt.clOption != null) { childArgs += opt.clOption += opt.value }\n        if (opt.confKey != null) {\n          if (opt.mergeFn.isDefined && sparkConf.contains(opt.confKey)) {"
  },
  {
    "id" : "1d92e8a6-9368-4d7f-bd62-239dce60ef58",
    "prId" : 28077,
    "prUrl" : "https://github.com/apache/spark/pull/28077#pullrequestreview-384422651",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "177eb1aa-d6e1-4a55-a8c0-c9310f8dce5c",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Just a question. Did we `mergeFileLists` for `deployMode != CLIENT` already?",
        "createdAt" : "2020-03-31T04:20:00Z",
        "updatedAt" : "2020-03-31T04:50:22Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "a0665cb4-6ea7-4bbf-9dae-4fe8b83b5d3b",
        "parentId" : "177eb1aa-d6e1-4a55-a8c0-c9310f8dce5c",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "No, I don't see we merge it for cluster deploy mode.",
        "createdAt" : "2020-03-31T05:22:39Z",
        "updatedAt" : "2020-03-31T05:22:39Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "31db3de3-c975-499b-b249-371c7c938862",
        "parentId" : "177eb1aa-d6e1-4a55-a8c0-c9310f8dce5c",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Got it. Thanks, @viirya .",
        "createdAt" : "2020-03-31T06:01:35Z",
        "updatedAt" : "2020-03-31T06:01:35Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "8b5edefe2ba6c4ab04a00ccb865247e93794ea4c",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +478,482 @@\n    // Non-PySpark applications can need Python dependencies.\n    if (deployMode == CLIENT && clusterManager != YARN) {\n      // The YARN backend handles python files differently, so don't merge the lists.\n      args.files = mergeFileLists(args.files, args.pyFiles)"
  },
  {
    "id" : "0a05ed6a-51a1-4609-b967-5a2052c2e263",
    "prId" : 27422,
    "prUrl" : "https://github.com/apache/spark/pull/27422#pullrequestreview-351759386",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3afedf90-ada3-4c5e-adb9-b1621298a561",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "So, this is adding backing `--proxy-user` in order to propagate it to K8s?",
        "createdAt" : "2020-01-31T20:12:52Z",
        "updatedAt" : "2020-03-04T20:05:49Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "dea285bc-55c2-4f47-9b7e-5689a367cb29",
        "parentId" : "3afedf90-ada3-4c5e-adb9-b1621298a561",
        "authorId" : "1e389084-a301-4224-b5bf-6c9bba25c7d6",
        "body" : "Yes!",
        "createdAt" : "2020-01-31T20:30:02Z",
        "updatedAt" : "2020-03-04T20:05:49Z",
        "lastEditedBy" : "1e389084-a301-4224-b5bf-6c9bba25c7d6",
        "tags" : [
        ]
      }
    ],
    "commit" : "d8d7f7db21c937a0ca470150f39499f1558c648f",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +774,778 @@        }\n      }\n      // Pass the proxyUser to the k8s app so it is possible to add it to the driver args\n      if (args.proxyUser != null) {\n        childArgs += (\"--proxy-user\", args.proxyUser)"
  },
  {
    "id" : "1e8a969e-2789-4fa0-813a-7a8e2211392e",
    "prId" : 26161,
    "prUrl" : "https://github.com/apache/spark/pull/26161#pullrequestreview-306867878",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4ed973c6-5dbe-4ea7-8255-ae1c4c19d246",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "nit. `.map( _ + \" \")` -> `.map(_ + \" \")`",
        "createdAt" : "2019-10-24T21:12:50Z",
        "updatedAt" : "2019-10-24T21:12:50Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "550508339d401750d9f1e74f2bcfcd9c83ed4427",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +791,795 @@    if (sparkConf.get(KILL_ON_OOM_ERROR) && deployMode == CLUSTER) {\n      val driverJavaOptions = sparkConf.getOption(SparkLauncher.DRIVER_EXTRA_JAVA_OPTIONS)\n        .map( _ + \" \")\n        .getOrElse(\"\") + \"-XX:OnOutOfMemoryError=\\\"kill -9 %p\\\"\"\n      sparkConf.set(SparkLauncher.DRIVER_EXTRA_JAVA_OPTIONS, driverJavaOptions)"
  },
  {
    "id" : "c530e09b-f05f-4969-9cc3-a6e3ab21b857",
    "prId" : 26161,
    "prUrl" : "https://github.com/apache/spark/pull/26161#pullrequestreview-306877631",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "33fc3888-a0eb-475a-95c4-e9c6a7ffb292",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Since this is a general PR for `SparkSubmit`, does this work on `Windows`?\r\n`ExitOnOutOfMemoryError` will be a better choice, @skonto .",
        "createdAt" : "2019-10-24T21:17:24Z",
        "updatedAt" : "2019-10-24T21:17:24Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "36b9ace9-0f57-47df-894e-cc520773e0b6",
        "parentId" : "33fc3888-a0eb-475a-95c4-e9c6a7ffb292",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Maybe, I lost some context since this is the 3rd try for this.",
        "createdAt" : "2019-10-24T21:26:19Z",
        "updatedAt" : "2019-10-24T21:26:19Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "3649e72d-5d4d-422d-915f-ea0b9033446b",
        "parentId" : "33fc3888-a0eb-475a-95c4-e9c6a7ffb292",
        "authorId" : "e913f9af-103d-40d4-976e-9aa8e12e7211",
        "body" : "I actually think we should stick w/ `OnOutOfMemoryError`, because `ExitOnOutOfMemoryError` was not present till java 8u92 (discussed earlier here: https://github.com/apache/spark/pull/24796#issuecomment-505063314).  I don't think we specify a minimum version within java 8, so we might have to stick with this.\r\n\r\nbut yeah, we probably have to make sure it doesn't do anything too weird on windows (does spark actually run in anything other than local mode on windows?)",
        "createdAt" : "2019-10-24T21:27:17Z",
        "updatedAt" : "2019-10-24T21:27:17Z",
        "lastEditedBy" : "e913f9af-103d-40d4-976e-9aa8e12e7211",
        "tags" : [
        ]
      },
      {
        "id" : "f8b7e788-7b3b-4d35-aefa-cdf5191dd85d",
        "parentId" : "33fc3888-a0eb-475a-95c4-e9c6a7ffb292",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "For Windows, I prefer to use JVM option. Personally, I don't think Apache Spark 3.0.0 will be used on JDK 8u91 or older. Apache Spark 3.0.0 starts a new age of JDK11. ðŸ˜„ ",
        "createdAt" : "2019-10-24T21:32:24Z",
        "updatedAt" : "2019-10-24T21:32:24Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "550508339d401750d9f1e74f2bcfcd9c83ed4427",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +792,796 @@      val driverJavaOptions = sparkConf.getOption(SparkLauncher.DRIVER_EXTRA_JAVA_OPTIONS)\n        .map( _ + \" \")\n        .getOrElse(\"\") + \"-XX:OnOutOfMemoryError=\\\"kill -9 %p\\\"\"\n      sparkConf.set(SparkLauncher.DRIVER_EXTRA_JAVA_OPTIONS, driverJavaOptions)\n    }"
  }
]