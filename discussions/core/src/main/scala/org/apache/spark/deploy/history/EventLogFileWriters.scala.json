[
  {
    "id" : "68f1b6eb-fc94-4c18-a5c3-d14a0abc14b5",
    "prId" : 27764,
    "prUrl" : "https://github.com/apache/spark/pull/27764#pullrequestreview-376189058",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9856e114-bf79-48fc-b808-9af4c9d27807",
        "parentId" : null,
        "authorId" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "body" : "Actually these permissions are a bit weird for a file; for a file I'd expect 660 (not 770). It's also unnecessary since to delete a file you need write permission to the directory only.",
        "createdAt" : "2020-03-10T21:15:36Z",
        "updatedAt" : "2020-03-17T14:11:08Z",
        "lastEditedBy" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "tags" : [
        ]
      },
      {
        "id" : "ae36a29d-e1a0-4ea1-ae3e-e49b49aebf89",
        "parentId" : "9856e114-bf79-48fc-b808-9af4c9d27807",
        "authorId" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "body" : "Hmm the old `EventLoggingListener` uses 770 also. Maybe it should be changed to 660, and have a separate constant for file permissions and directory permissions.",
        "createdAt" : "2020-03-10T21:18:33Z",
        "updatedAt" : "2020-03-17T14:11:08Z",
        "lastEditedBy" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "tags" : [
        ]
      },
      {
        "id" : "73bde098-a33d-417f-a0ef-731b6ad3cfcc",
        "parentId" : "9856e114-bf79-48fc-b808-9af4c9d27807",
        "authorId" : "92aa9573-b64a-4a86-aebf-54c827df3ed0",
        "body" : "Yeah I just used the existing permissions. Not sure why they were set with the executable bit in the first place. I can change the files to only have 660 if there's no reason not to.",
        "createdAt" : "2020-03-10T23:57:43Z",
        "updatedAt" : "2020-03-17T14:11:08Z",
        "lastEditedBy" : "92aa9573-b64a-4a86-aebf-54c827df3ed0",
        "tags" : [
        ]
      },
      {
        "id" : "5c7da416-b08e-46f7-8457-4eb369d1ff89",
        "parentId" : "9856e114-bf79-48fc-b808-9af4c9d27807",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Yes please go ahead and fix it altogether.",
        "createdAt" : "2020-03-11T00:48:50Z",
        "updatedAt" : "2020-03-17T14:11:08Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "9f72e97b-e2e3-42e7-a0b5-6eb1a02e2991",
        "parentId" : "9856e114-bf79-48fc-b808-9af4c9d27807",
        "authorId" : "92aa9573-b64a-4a86-aebf-54c827df3ed0",
        "body" : "Changed the file permission to 660 and added a separate folder permission for 770",
        "createdAt" : "2020-03-17T16:15:19Z",
        "updatedAt" : "2020-03-17T16:15:19Z",
        "lastEditedBy" : "92aa9573-b64a-4a86-aebf-54c827df3ed0",
        "tags" : [
        ]
      }
    ],
    "commit" : "1499158a6e4188aa82a6ee6f24933ef6a2eed94e",
    "line" : 27,
    "diffHunk" : "@@ -1,1 +366,370 @@    // SPARK-30860: use the class method to avoid the umask causing permission issues\n    val outputStream = FileSystem.create(fileSystem, appStatusPath,\n      EventLogFileWriter.LOG_FILE_PERMISSIONS)\n    // we intentionally create zero-byte file to minimize the cost\n    outputStream.close()"
  },
  {
    "id" : "e8caf9bf-08b7-407d-bdc6-990aad1409ac",
    "prId" : 26416,
    "prUrl" : "https://github.com/apache/spark/pull/26416#pullrequestreview-314590014",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "178bdd03-863a-4a9a-8d91-0729d8a20a0d",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "nit: I'd prefer `.compacted`.",
        "createdAt" : "2019-11-08T13:04:16Z",
        "updatedAt" : "2019-12-26T02:35:39Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "b647079c-b681-441f-bac0-a9fb23f7ae88",
        "parentId" : "178bdd03-863a-4a9a-8d91-0729d8a20a0d",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "I followed the naming of extension where the extension of state store's snapshotted file is \".snapshot\". Let's hear more voices on this.",
        "createdAt" : "2019-11-10T04:44:35Z",
        "updatedAt" : "2019-12-26T02:35:39Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "e1a6e42b73f8d58dbc0a04882f2288d2fae0dad8",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +165,169 @@  // Suffix applied to the names of files still being written by applications.\n  val IN_PROGRESS = \".inprogress\"\n  val COMPACTED = \".compact\"\n\n  val LOG_FILE_PERMISSIONS = new FsPermission(Integer.parseInt(\"770\", 8).toShort)"
  },
  {
    "id" : "ebdf062b-cd39-4ebe-988c-063ebe031d6c",
    "prId" : 26416,
    "prUrl" : "https://github.com/apache/spark/pull/26416#pullrequestreview-317561147",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "35a9a44e-008c-47b7-8870-56adb3d4f45c",
        "parentId" : null,
        "authorId" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "body" : "This is not belonging to this line but the next (so original implementation).\r\nAFAIK `hadoopDataStream.foreach(_.hflush())` is no-op when `spark.eventLog.allowErasureCoding` is `true`. Won't that cause any issue?",
        "createdAt" : "2019-11-14T14:40:07Z",
        "updatedAt" : "2019-12-26T02:35:39Z",
        "lastEditedBy" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "tags" : [
        ]
      },
      {
        "id" : "59520d17-d3a7-4b88-ab36-d0c49dfc2f12",
        "parentId" : "35a9a44e-008c-47b7-8870-56adb3d4f45c",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "I think that's already discussed in #22881 - if I interpreted correctly, we turn off EC for event log by default, and turn on at users' own risk.",
        "createdAt" : "2019-11-15T04:13:48Z",
        "updatedAt" : "2019-12-26T02:35:39Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "5800d72d-5fb5-436b-bed8-cd439bf2b4a1",
        "parentId" : "35a9a44e-008c-47b7-8870-56adb3d4f45c",
        "authorId" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "body" : "Thanks for pointing to the good direction! This is fine then.",
        "createdAt" : "2019-11-15T11:29:23Z",
        "updatedAt" : "2019-12-26T02:35:39Z",
        "lastEditedBy" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "tags" : [
        ]
      }
    ],
    "commit" : "e1a6e42b73f8d58dbc0a04882f2288d2fae0dad8",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +119,123 @@    // scalastyle:on println\n    if (flushLogger) {\n      writer.foreach(_.flush())\n      hadoopDataStream.foreach(_.hflush())\n    }"
  },
  {
    "id" : "e3e21143-7bae-449d-b619-10c736e4be9d",
    "prId" : 25670,
    "prUrl" : "https://github.com/apache/spark/pull/25670#pullrequestreview-294076395",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8f090a7f-5b2e-4ddd-bff7-83fff97a45f4",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Where is this method used ?",
        "createdAt" : "2019-09-26T16:19:36Z",
        "updatedAt" : "2019-10-16T20:56:19Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "87ccf6d0-8923-4554-b368-82b9452791fb",
        "parentId" : "8f090a7f-5b2e-4ddd-bff7-83fff97a45f4",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "They're used in test suites - we need to provide unified way to get log path between single file reader and rolling files reader.",
        "createdAt" : "2019-09-26T20:35:50Z",
        "updatedAt" : "2019-10-16T20:56:19Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "1bf8471f-b49d-4eab-851c-e672277e8273",
        "parentId" : "8f090a7f-5b2e-4ddd-bff7-83fff97a45f4",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Then, would be better to mark as \"For tests only\"",
        "createdAt" : "2019-09-27T01:23:35Z",
        "updatedAt" : "2019-10-16T20:56:19Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "line" : 356,
    "diffHunk" : "@@ -1,1 +354,358 @@  }\n\n  override def logPath: String = logDirForAppPath.toString\n\n  private def createAppStatusFile(inProgress: Boolean): Unit = {"
  },
  {
    "id" : "32a5ea31-92e2-438c-9d09-b20b822bbcdc",
    "prId" : 25670,
    "prUrl" : "https://github.com/apache/spark/pull/25670#pullrequestreview-296570315",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "68703952-ae3f-4bd2-a62a-c590c564aa45",
        "parentId" : null,
        "authorId" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "body" : "This comes from the old code but seems to be somewhat incorrect. If the file exists and `shouldOverwrite` is false, this will not delete the file, as expected.\r\n\r\nBut later, when you do e.g. `new FileOutputStream(uri.getPath)`, the existing file will be overwritten on most file systems (NTFS being the exception, I think).\r\n\r\nNo need to fix that here, though. Also because it's unlikely to happen given how we name files.",
        "createdAt" : "2019-10-02T16:28:10Z",
        "updatedAt" : "2019-10-16T20:56:19Z",
        "lastEditedBy" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "tags" : [
        ]
      },
      {
        "id" : "ba23062a-2da4-493a-9c22-ab6d66f7dc69",
        "parentId" : "68703952-ae3f-4bd2-a62a-c590c564aa45",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "That's a good point. Maybe we haven't defined proper behavior of this case: would we want to fail the application?\r\n\r\nThere might be similar case for if shouldOverwrite is true and fileSystem.delete() returns false. I see fileSystem.delete() will mostly throw IOException when it fails to delete, but at least in javadoc, having 'false' as return value when calling fileSystem.delete() may not only say the file doesn't exist. Javadoc doesn't guarantee that.",
        "createdAt" : "2019-10-02T22:56:39Z",
        "updatedAt" : "2019-10-16T20:56:19Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "line" : 83,
    "diffHunk" : "@@ -1,1 +81,85 @@\n  protected def initLogFile(path: Path)(fnSetupWriter: OutputStream => PrintWriter): Unit = {\n    if (shouldOverwrite && fileSystem.delete(path, true)) {\n      logWarning(s\"Event log $path already exists. Overwriting...\")\n    }"
  }
]