[
  {
    "id" : "68f1b6eb-fc94-4c18-a5c3-d14a0abc14b5",
    "prId" : 27764,
    "prUrl" : "https://github.com/apache/spark/pull/27764#pullrequestreview-376189058",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9856e114-bf79-48fc-b808-9af4c9d27807",
        "parentId" : null,
        "authorId" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "body" : "Actually these permissions are a bit weird for a file; for a file I'd expect 660 (not 770). It's also unnecessary since to delete a file you need write permission to the directory only.",
        "createdAt" : "2020-03-10T21:15:36Z",
        "updatedAt" : "2020-03-17T14:11:08Z",
        "lastEditedBy" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "tags" : [
        ]
      },
      {
        "id" : "ae36a29d-e1a0-4ea1-ae3e-e49b49aebf89",
        "parentId" : "9856e114-bf79-48fc-b808-9af4c9d27807",
        "authorId" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "body" : "Hmm the old `EventLoggingListener` uses 770 also. Maybe it should be changed to 660, and have a separate constant for file permissions and directory permissions.",
        "createdAt" : "2020-03-10T21:18:33Z",
        "updatedAt" : "2020-03-17T14:11:08Z",
        "lastEditedBy" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "tags" : [
        ]
      },
      {
        "id" : "73bde098-a33d-417f-a0ef-731b6ad3cfcc",
        "parentId" : "9856e114-bf79-48fc-b808-9af4c9d27807",
        "authorId" : "92aa9573-b64a-4a86-aebf-54c827df3ed0",
        "body" : "Yeah I just used the existing permissions. Not sure why they were set with the executable bit in the first place. I can change the files to only have 660 if there's no reason not to.",
        "createdAt" : "2020-03-10T23:57:43Z",
        "updatedAt" : "2020-03-17T14:11:08Z",
        "lastEditedBy" : "92aa9573-b64a-4a86-aebf-54c827df3ed0",
        "tags" : [
        ]
      },
      {
        "id" : "5c7da416-b08e-46f7-8457-4eb369d1ff89",
        "parentId" : "9856e114-bf79-48fc-b808-9af4c9d27807",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Yes please go ahead and fix it altogether.",
        "createdAt" : "2020-03-11T00:48:50Z",
        "updatedAt" : "2020-03-17T14:11:08Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "9f72e97b-e2e3-42e7-a0b5-6eb1a02e2991",
        "parentId" : "9856e114-bf79-48fc-b808-9af4c9d27807",
        "authorId" : "92aa9573-b64a-4a86-aebf-54c827df3ed0",
        "body" : "Changed the file permission to 660 and added a separate folder permission for 770",
        "createdAt" : "2020-03-17T16:15:19Z",
        "updatedAt" : "2020-03-17T16:15:19Z",
        "lastEditedBy" : "92aa9573-b64a-4a86-aebf-54c827df3ed0",
        "tags" : [
        ]
      }
    ],
    "commit" : "1499158a6e4188aa82a6ee6f24933ef6a2eed94e",
    "line" : 27,
    "diffHunk" : "@@ -1,1 +366,370 @@    // SPARK-30860: use the class method to avoid the umask causing permission issues\n    val outputStream = FileSystem.create(fileSystem, appStatusPath,\n      EventLogFileWriter.LOG_FILE_PERMISSIONS)\n    // we intentionally create zero-byte file to minimize the cost\n    outputStream.close()"
  },
  {
    "id" : "e8caf9bf-08b7-407d-bdc6-990aad1409ac",
    "prId" : 26416,
    "prUrl" : "https://github.com/apache/spark/pull/26416#pullrequestreview-314590014",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "178bdd03-863a-4a9a-8d91-0729d8a20a0d",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "nit: I'd prefer `.compacted`.",
        "createdAt" : "2019-11-08T13:04:16Z",
        "updatedAt" : "2019-12-26T02:35:39Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "b647079c-b681-441f-bac0-a9fb23f7ae88",
        "parentId" : "178bdd03-863a-4a9a-8d91-0729d8a20a0d",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "I followed the naming of extension where the extension of state store's snapshotted file is \".snapshot\". Let's hear more voices on this.",
        "createdAt" : "2019-11-10T04:44:35Z",
        "updatedAt" : "2019-12-26T02:35:39Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "e1a6e42b73f8d58dbc0a04882f2288d2fae0dad8",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +165,169 @@  // Suffix applied to the names of files still being written by applications.\n  val IN_PROGRESS = \".inprogress\"\n  val COMPACTED = \".compact\"\n\n  val LOG_FILE_PERMISSIONS = new FsPermission(Integer.parseInt(\"770\", 8).toShort)"
  },
  {
    "id" : "ebdf062b-cd39-4ebe-988c-063ebe031d6c",
    "prId" : 26416,
    "prUrl" : "https://github.com/apache/spark/pull/26416#pullrequestreview-317561147",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "35a9a44e-008c-47b7-8870-56adb3d4f45c",
        "parentId" : null,
        "authorId" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "body" : "This is not belonging to this line but the next (so original implementation).\r\nAFAIK `hadoopDataStream.foreach(_.hflush())` is no-op when `spark.eventLog.allowErasureCoding` is `true`. Won't that cause any issue?",
        "createdAt" : "2019-11-14T14:40:07Z",
        "updatedAt" : "2019-12-26T02:35:39Z",
        "lastEditedBy" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "tags" : [
        ]
      },
      {
        "id" : "59520d17-d3a7-4b88-ab36-d0c49dfc2f12",
        "parentId" : "35a9a44e-008c-47b7-8870-56adb3d4f45c",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "I think that's already discussed in #22881 - if I interpreted correctly, we turn off EC for event log by default, and turn on at users' own risk.",
        "createdAt" : "2019-11-15T04:13:48Z",
        "updatedAt" : "2019-12-26T02:35:39Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "5800d72d-5fb5-436b-bed8-cd439bf2b4a1",
        "parentId" : "35a9a44e-008c-47b7-8870-56adb3d4f45c",
        "authorId" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "body" : "Thanks for pointing to the good direction! This is fine then.",
        "createdAt" : "2019-11-15T11:29:23Z",
        "updatedAt" : "2019-12-26T02:35:39Z",
        "lastEditedBy" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "tags" : [
        ]
      }
    ],
    "commit" : "e1a6e42b73f8d58dbc0a04882f2288d2fae0dad8",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +119,123 @@    // scalastyle:on println\n    if (flushLogger) {\n      writer.foreach(_.flush())\n      hadoopDataStream.foreach(_.hflush())\n    }"
  }
]