[
  {
    "id" : "22cc1e9e-fc63-4400-bf70-41b337436698",
    "prId" : 31256,
    "prUrl" : "https://github.com/apache/spark/pull/31256#pullrequestreview-571905313",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "217fea12-d33c-4486-9088-6daeabdcd131",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "When `cachedBlocks.empty == true`, this will be `Long.MaxValue`. That's wrong, isn't it, @hanyucui ?",
        "createdAt" : "2021-01-20T06:08:18Z",
        "updatedAt" : "2021-01-20T06:08:18Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "7d933d98-a1a0-4def-943f-35e388e2024c",
        "parentId" : "217fea12-d33c-4486-9088-6daeabdcd131",
        "authorId" : "b4586f54-4dcf-4d45-99ee-e2fb177c113c",
        "body" : "@dongjoon-hyun Yeah, you are right. This is not a solution but only an illustration. I wish there was a way to prevent this from bothering admins.\r\n\r\nIn my case, I enable dynamic allocation on K8s. I set `spark.dynamicAllocation.shuffleTracking.timeout` to a small number so that I can kill the executors earlier but set `spark.dynamicAllocation.cachedExecutorIdleTimeout` to a larger value since some users would like to keep their cached data around for longer. I expected the executors to be killed when the larger timeout is hit, not the smaller one, but the current behavior is the opposite. I understand different users have different requirements and this might totally make sense for them. No action is needed on your side -- I will think it through first.",
        "createdAt" : "2021-01-20T06:21:13Z",
        "updatedAt" : "2021-01-20T06:21:14Z",
        "lastEditedBy" : "b4586f54-4dcf-4d45-99ee-e2fb177c113c",
        "tags" : [
        ]
      },
      {
        "id" : "b9d8e558-7b13-4e68-a3da-30387216721f",
        "parentId" : "217fea12-d33c-4486-9088-6daeabdcd131",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "I understand your opinion, but this feature is not designed like that originally.",
        "createdAt" : "2021-01-20T06:30:54Z",
        "updatedAt" : "2021-01-20T06:30:54Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "b78d8d6f196b9f5a194acf87f0e2386366d0f712",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +547,551 @@            Long.MaxValue\n          }\n          math.max(_cacheTimeout, _shuffleTimeout)\n        } else {\n          idleTimeoutNs"
  },
  {
    "id" : "85ec08f2-28a7-4ab7-8550-210e2a238a95",
    "prId" : 31125,
    "prUrl" : "https://github.com/apache/spark/pull/31125#pullrequestreview-567155380",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "27773f60-a2b1-4d5c-9752-9217941034a4",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Just double checking you're sure these work in 2.13? b/c I added a lot of calls like this that will be redundant in 2.12 but not 2.13. (Not saying this one is necessary, particularly.)",
        "createdAt" : "2021-01-11T19:47:33Z",
        "updatedAt" : "2021-01-11T19:47:33Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "0db03fc1-c7a5-4f3d-a6f5-9cd0849de42b",
        "parentId" : "27773f60-a2b1-4d5c-9752-9217941034a4",
        "authorId" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "body" : "Yes, I'm sure this conversion is redundant, the type of `shuffleStages` is `Seq[(Int, Int)]` in Scala 2.13, and I run \r\n`mvn clean install -Pscala-2.13 -pl core -am` locally, all test passed in core module\r\n\r\n```\r\nRun completed in 36 minutes, 54 seconds.\r\nTotal number of tests run: 2710\r\nSuites: completed 270, aborted 0\r\nTests: succeeded 2710, failed 0, canceled 4, ignored 7, pending 0\r\nAll tests passed.\r\n```\r\nI will do more local tests to verify other changes today",
        "createdAt" : "2021-01-12T02:30:31Z",
        "updatedAt" : "2021-01-12T02:38:18Z",
        "lastEditedBy" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "tags" : [
        ]
      },
      {
        "id" : "bca6eccc-dfc8-45e9-a87a-87c54d7dcb01",
        "parentId" : "27773f60-a2b1-4d5c-9752-9217941034a4",
        "authorId" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "body" : "`mvn clean install -Phadoop-3.2 -Phive-2.3 -Phadoop-cloud -Pmesos -Pyarn -Pkinesis-asl -Phive-thriftserver -Pspark-ganglia-lgpl -Pkubernetes -Phive -Pscala-2.13 -pl graphx`\r\n\r\nall test passed\r\n\r\n```\r\nRun completed in 2 minutes, 24 seconds.\r\nTotal number of tests run: 108\r\nSuites: completed 19, aborted 0\r\nTests: succeeded 108, failed 0, canceled 0, ignored 0, pending 0\r\nAll tests passed.\r\n```",
        "createdAt" : "2021-01-12T04:07:41Z",
        "updatedAt" : "2021-01-12T04:07:42Z",
        "lastEditedBy" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "tags" : [
        ]
      },
      {
        "id" : "f524be35-ca85-4a33-aeb0-07cbd7eb94ba",
        "parentId" : "27773f60-a2b1-4d5c-9752-9217941034a4",
        "authorId" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "body" : "`mvn clean install -Phadoop-3.2 -Phive-2.3 -Phadoop-cloud -Pmesos -Pyarn -Pkinesis-asl -Phive-thriftserver -Pspark-ganglia-lgpl -Pkubernetes -Phive -Pscala-2.13 -pl mllib\r\n`\r\nall test passed\r\n```\r\nRun completed in 28 minutes, 39 seconds.\r\nTotal number of tests run: 1627\r\nSuites: completed 206, aborted 0\r\nTests: succeeded 1627, failed 0, canceled 0, ignored 7, pending 0\r\nAll tests passed.\r\n```",
        "createdAt" : "2021-01-12T05:54:12Z",
        "updatedAt" : "2021-01-12T05:54:12Z",
        "lastEditedBy" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "tags" : [
        ]
      },
      {
        "id" : "1a253447-b0f9-45d3-901f-c981d82a7c14",
        "parentId" : "27773f60-a2b1-4d5c-9752-9217941034a4",
        "authorId" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "body" : "`mvn clean install -Phadoop-3.2 -Phive-2.3 -Phadoop-cloud -Pmesos -Pyarn -Pkinesis-asl -Phive-thriftserver -Pspark-ganglia-lgpl -Pkubernetes -Phive -Pscala-2.13 -pl mllib-local`\r\n\r\nall test passed.\r\n\r\n```\r\nRun completed in 2 seconds, 18 milliseconds.\r\nTotal number of tests run: 95\r\nSuites: completed 9, aborted 0\r\nTests: succeeded 95, failed 0, canceled 0, ignored 0, pending 0\r\nAll tests passed.\r\n```\r\n",
        "createdAt" : "2021-01-12T06:31:47Z",
        "updatedAt" : "2021-01-12T06:31:47Z",
        "lastEditedBy" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "tags" : [
        ]
      },
      {
        "id" : "86d44aee-4fb9-43ab-97d3-675a68cd226e",
        "parentId" : "27773f60-a2b1-4d5c-9752-9217941034a4",
        "authorId" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "body" : "`mvn clean install -Phadoop-3.2 -Phive-2.3 -Phadoop-cloud -Pmesos -Pyarn -Pkinesis-asl -Phive-thriftserver -Pspark-ganglia-lgpl -Pkubernetes -Phive -Pscala-2.13 -pl sql/catalyst`\r\n\r\nall test passed.\r\n\r\n```\r\nRun completed in 12 minutes, 27 seconds.\r\nTotal number of tests run: 4718\r\nSuites: completed 261, aborted 0\r\nTests: succeeded 4718, failed 0, canceled 0, ignored 5, pending 0\r\nAll tests passed.\r\n```",
        "createdAt" : "2021-01-12T07:01:03Z",
        "updatedAt" : "2021-01-12T07:01:03Z",
        "lastEditedBy" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "tags" : [
        ]
      },
      {
        "id" : "30aa76f6-a393-4206-a567-a1419d33cc2c",
        "parentId" : "27773f60-a2b1-4d5c-9752-9217941034a4",
        "authorId" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "body" : "`mvn clean install -Phadoop-3.2 -Phive-2.3 -Phadoop-cloud -Pmesos -Pyarn -Pkinesis-asl -Phive-thriftserver -Pspark-ganglia-lgpl -Pkubernetes -Phive -Pscala-2.13 -pl sql/core`\r\n\r\nall test passed.\r\n\r\n```\r\nRun completed in 1 hour, 38 minutes, 11 seconds.\r\nTotal number of tests run: 9185\r\nSuites: completed 406, aborted 0\r\nTests: succeeded 9185, failed 0, canceled 1, ignored 51, pending 0\r\nAll tests passed.\r\n```",
        "createdAt" : "2021-01-12T08:54:33Z",
        "updatedAt" : "2021-01-12T08:54:55Z",
        "lastEditedBy" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "tags" : [
        ]
      },
      {
        "id" : "63513e98-4a90-4839-a877-c5b06839dc65",
        "parentId" : "27773f60-a2b1-4d5c-9752-9217941034a4",
        "authorId" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "body" : "some related modules that have not been verified today : `hive`, `hive-thriftserver` and `yarn`",
        "createdAt" : "2021-01-12T12:45:23Z",
        "updatedAt" : "2021-01-12T12:45:23Z",
        "lastEditedBy" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "tags" : [
        ]
      },
      {
        "id" : "c44708dc-ac92-4eb4-984b-09371b839da6",
        "parentId" : "27773f60-a2b1-4d5c-9752-9217941034a4",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Do you want to check out the other modules first and do this all at once?",
        "createdAt" : "2021-01-12T14:51:22Z",
        "updatedAt" : "2021-01-12T14:51:23Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "50bb3272-b612-4d43-9126-5f99f9d42d8c",
        "parentId" : "27773f60-a2b1-4d5c-9752-9217941034a4",
        "authorId" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "body" : "Yes, I'll check these today",
        "createdAt" : "2021-01-13T02:37:11Z",
        "updatedAt" : "2021-01-13T02:37:11Z",
        "lastEditedBy" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "tags" : [
        ]
      },
      {
        "id" : "1702fbf9-0290-489d-942d-566594203e03",
        "parentId" : "27773f60-a2b1-4d5c-9752-9217941034a4",
        "authorId" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "body" : "@srowen All modules are verified and add new Jira SPARK-34098",
        "createdAt" : "2021-01-13T12:03:29Z",
        "updatedAt" : "2021-01-13T12:08:55Z",
        "lastEditedBy" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "tags" : [
        ]
      }
    ],
    "commit" : "a4e44c21657de5c36c3a6fe18eb0ce2f6c6c3749",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +228,232 @@\n    if (updateExecutors) {\n      val activeShuffleIds = shuffleStages.map(_._2)\n      var needTimeoutUpdate = false\n      val activatedExecs = new ExecutorIdCollector()"
  },
  {
    "id" : "1a344104-811a-4328-8c3a-d4d4efe7a0c7",
    "prId" : 30795,
    "prUrl" : "https://github.com/apache/spark/pull/30795#pullrequestreview-555580173",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "df1f35fe-a91b-4c62-9a0c-ed4e17831115",
        "parentId" : null,
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "we should expand this to state excluded for entire application (I realize HealthTracker implies this but would like to be more explicit), does not include excluded within the stage level.",
        "createdAt" : "2020-12-18T15:28:24Z",
        "updatedAt" : "2020-12-18T16:06:17Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "75cdd1e334c15c8af89c9b0d283bedf12d0f33ee",
    "line" : 165,
    "diffHunk" : "@@ -1,1 +574,578 @@    var decommissioning: Boolean = false\n    var hasActiveShuffle: Boolean = false\n    // whether the executor is temporarily excluded by the `HealthTracker`\n    var excluded: Boolean = false\n"
  },
  {
    "id" : "0f4691ce-5e0c-4e62-96b7-6ed95cb078a0",
    "prId" : 30103,
    "prUrl" : "https://github.com/apache/spark/pull/30103#pullrequestreview-532150265",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "90f1d16b-d570-40bf-8ead-e9a3ca53365b",
        "parentId" : null,
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "I don't follow this change.  Here isIdle means we don't have any running tasks and hasActiveshuffle is false. The ids passed in are supposed to be all the active ones which is determined by shuffleToActiveJobs.  It does not mean that shuffleIds is empty. shuffleIds stick around until we get the ShuffleCleanedEvent that comes from context cleaner when RDD isn't referenced anymore. If its referenced even though the job may be done it means someone could use that still so we don't remove it until after the timeout.  Here if we are idle and had active shuffle we update the Timeout, which should be setting it to either cache or storage timeout and then when the ExecutorMonitor comes around to check it should see that its set and time it out regardless of if it has shuffle ids.\r\n\r\nIf there is a bug in there somewhere then  we need to figure out where it is. I do not think is not the correct fix.",
        "createdAt" : "2020-11-13T15:29:05Z",
        "updatedAt" : "2020-11-13T15:30:20Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "fa71f054-6b06-44bc-a528-70c05cc4b792",
        "parentId" : "90f1d16b-d570-40bf-8ead-e9a3ca53365b",
        "authorId" : "19f8cd22-cf85-435f-a650-411ff7836f0b",
        "body" : "thank you for your explaining. i got it .it seems works well . i will close this issue.",
        "createdAt" : "2020-11-17T09:03:43Z",
        "updatedAt" : "2020-11-17T09:03:43Z",
        "lastEditedBy" : "19f8cd22-cf85-435f-a650-411ff7836f0b",
        "tags" : [
        ]
      }
    ],
    "commit" : "2ff44993d976517738363d8d1feca1e2371dd9a3",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +587,591 @@      hasActiveShuffle = ids.exists(shuffleIds.contains)\n      if (hadActiveShuffle && isIdle) {\n        shuffleIds.clear()\n        updateTimeout()\n      }"
  },
  {
    "id" : "85de189e-f5e2-4067-bb4f-c932a2576805",
    "prId" : 29367,
    "prUrl" : "https://github.com/apache/spark/pull/29367#pullrequestreview-462800278",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "97fe4dc3-9422-476f-b75c-869abd4bd47c",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "I am wondering if the name `markExecutorsDecommissioned` would reflect the intent of this function better ?",
        "createdAt" : "2020-08-06T05:49:03Z",
        "updatedAt" : "2020-08-12T19:09:04Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "eefff4d0-bc06-41ad-8c25-6cd48b1fe39f",
        "parentId" : "97fe4dc3-9422-476f-b75c-869abd4bd47c",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "This matches the function for killing executors (`executorsKilled`), so I think keeping the naming convention consistent is better.",
        "createdAt" : "2020-08-06T19:07:09Z",
        "updatedAt" : "2020-08-12T19:09:04Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "e970cb10147fb64533f5088edc3a448b5ef198cf",
    "line" : 31,
    "diffHunk" : "@@ -1,1 +152,156 @@  }\n\n  private[spark] def executorsDecommissioned(ids: Seq[String]): Unit = {\n    ids.foreach { id =>\n      val tracker = executors.get(id)"
  },
  {
    "id" : "eb94c874-ab5d-4c3b-9fe7-8105102238fe",
    "prId" : 29367,
    "prUrl" : "https://github.com/apache/spark/pull/29367#pullrequestreview-463829692",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "75102d7f-b2ce-43e0-9b8b-1fec5c9000dd",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "I didn't quite understand the reason for the change in this hunk: It seems that we should only be changing the KILL message to DECOMMISSION message if I understand the commit message/PR description properly. \r\n\r\nAre we also changing what gets decommissioned or not ?",
        "createdAt" : "2020-08-06T05:52:10Z",
        "updatedAt" : "2020-08-12T19:09:04Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "dc49ab3b-3f4e-40ef-863e-b68cd906eb81",
        "parentId" : "75102d7f-b2ce-43e0-9b8b-1fec5c9000dd",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "So since we now move shuffle files around, we need to keep track of the changes if decommissioning and the executor monitor are both enabled.",
        "createdAt" : "2020-08-06T19:26:11Z",
        "updatedAt" : "2020-08-12T19:09:04Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "6506db55-4e34-4e47-9b1d-235f335b5d13",
        "parentId" : "75102d7f-b2ce-43e0-9b8b-1fec5c9000dd",
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "Got it.",
        "createdAt" : "2020-08-08T22:11:47Z",
        "updatedAt" : "2020-08-12T19:09:04Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      }
    ],
    "commit" : "e970cb10147fb64533f5088edc3a448b5ef198cf",
    "line" : 86,
    "diffHunk" : "@@ -1,1 +362,366 @@    val exec = ensureExecutorIsTracked(event.blockUpdatedInfo.blockManagerId.executorId,\n      UNKNOWN_RESOURCE_PROFILE_ID)\n\n    // Check if it is a shuffle file, or RDD to pick the correct codepath for update\n    if (!event.blockUpdatedInfo.blockId.isInstanceOf[RDDBlockId]) {"
  },
  {
    "id" : "09b70bea-f5f5-4cfa-95af-a9279868385d",
    "prId" : 29367,
    "prUrl" : "https://github.com/apache/spark/pull/29367#pullrequestreview-463829692",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5666fc0a-56da-493f-acb6-cee9ad47afac",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "I am just trying to follow along this code, so pardon me if this is a n00b question: Why are we separately tracking pendingRemoval and decommissioning separately ? Two questions about that:\r\n\r\n- If an executor is marked as decommissioned here, when is it actually removed ? (Outside of dynamic allocation that happens when the executor naturally has a heartbeat failure. ). \r\n- Is my understanding correct that if graceful decommissioning is plugged into dynamic-allocation (this feature) AND the cluster manager supports decommissioning, then pendingRemoval would be empty -- ie executors would only be decommission ?",
        "createdAt" : "2020-08-08T17:16:52Z",
        "updatedAt" : "2020-08-12T19:09:04Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "5e9c14c3-08b1-46eb-9afd-aaf87c4d6e6f",
        "parentId" : "5666fc0a-56da-493f-acb6-cee9ad47afac",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "We also kill executors if they end up blacklisted so I think we want to track them separately.",
        "createdAt" : "2020-08-08T18:31:15Z",
        "updatedAt" : "2020-08-12T19:09:04Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "6c2fe7f9-422b-4a05-a915-3c68c59fc911",
        "parentId" : "5666fc0a-56da-493f-acb6-cee9ad47afac",
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "Thanks for clarifying that !",
        "createdAt" : "2020-08-08T22:11:10Z",
        "updatedAt" : "2020-08-12T19:09:04Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      }
    ],
    "commit" : "e970cb10147fb64533f5088edc3a448b5ef198cf",
    "line" : 77,
    "diffHunk" : "@@ -1,1 +353,357 @@    if (removed != null) {\n      decrementExecResourceProfileCount(removed.resourceProfileId)\n      if (!removed.pendingRemoval || !removed.decommissioning) {\n        nextTimeout.set(Long.MinValue)\n      }"
  },
  {
    "id" : "4943caf3-3b21-4879-973d-b0ce07706b35",
    "prId" : 29367,
    "prUrl" : "https://github.com/apache/spark/pull/29367#pullrequestreview-464613409",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4bb96640-7b84-453e-815b-9ded012f0362",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "Is this comment change intended ?",
        "createdAt" : "2020-08-08T17:17:25Z",
        "updatedAt" : "2020-08-12T19:09:04Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "80f8b437-c941-477b-809f-33e017e05b51",
        "parentId" : "4bb96640-7b84-453e-815b-9ded012f0362",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Yes, since we're eventually going to want to use intelligent metrics to decide who to scale down I'd like us to only track shuffle files that are being used not speculative ones. Doesn't need to be addressed right now which is why it's a TODO.",
        "createdAt" : "2020-08-10T21:52:17Z",
        "updatedAt" : "2020-08-12T19:09:05Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "e970cb10147fb64533f5088edc3a448b5ef198cf",
    "line" : 68,
    "diffHunk" : "@@ -1,1 +324,328 @@      // This means that an executor may be marked as having shuffle data, and thus prevented\n      // from being removed, even though the data may not be used.\n      // TODO: Only track used files (SPARK-31974)\n      if (shuffleTrackingEnabled && event.reason == Success) {\n        stageToShuffleID.get(event.stageId).foreach { shuffleId =>"
  },
  {
    "id" : "aa9a3847-3427-45c0-9498-ea301affb1ea",
    "prId" : 29367,
    "prUrl" : "https://github.com/apache/spark/pull/29367#pullrequestreview-464612539",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "26c6f324-387a-43fb-8ac1-102dec703c3f",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "I went through all of the usages of executor.pendingRemoval and executor.decommissioning flag: They are treated identically right now. That is for all practical purposes an executor being decommissioned is treated the same as an executor pending to be removed. \r\n\r\nDo you have a use case in mind of why you would like to distinguish b/w these two states ? If you don't need to distinguish, the change would become simpler if you treat a decommissioned executor as pending removal. \r\n\r\nI cannot see where this distinction is relevant in this PR, so perhaps you have a future use case in mind for this distinction ?",
        "createdAt" : "2020-08-08T23:54:32Z",
        "updatedAt" : "2020-08-12T19:09:05Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "3ed2c9f0-39dc-4464-a91e-c6a422465fe3",
        "parentId" : "26c6f324-387a-43fb-8ac1-102dec703c3f",
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "^^^ @holdenk ... any thoughts/followup on this ?",
        "createdAt" : "2020-08-10T19:42:55Z",
        "updatedAt" : "2020-08-12T19:09:05Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "3e561251-e1e9-44cc-8510-3e99e5eb4149",
        "parentId" : "26c6f324-387a-43fb-8ac1-102dec703c3f",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Eventually I'd like us to have better logging and metrics around decommissioning and understanding it's impact versus blacklisting, although to be fair that isn't in the short term.",
        "createdAt" : "2020-08-10T21:50:36Z",
        "updatedAt" : "2020-08-12T19:09:05Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "e970cb10147fb64533f5088edc3a448b5ef198cf",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +116,120 @@      timedOutExecs = executors.asScala\n        .filter { case (_, exec) =>\n          !exec.pendingRemoval && !exec.hasActiveShuffle && !exec.decommissioning}\n        .filter { case (_, exec) =>\n          val deadline = exec.timeoutAt"
  },
  {
    "id" : "74ab0733-0880-4622-965b-2f7629203b50",
    "prId" : 28818,
    "prUrl" : "https://github.com/apache/spark/pull/28818#pullrequestreview-436253882",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2ef166d9-9ef2-49cb-bc54-3bf35e4e3ba1",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Since we are touching `ExecutorMonitor`, when do we have a counter operation, `exec.removeShuffle`? In this PR, it seems that `executorsKilled` is used. Is it enough?\r\ncc @dbtsai ",
        "createdAt" : "2020-06-16T22:31:19Z",
        "updatedAt" : "2020-07-23T21:35:22Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "1b2ba6b0-68d3-4e1d-b2ff-7a3cb4f16a3c",
        "parentId" : "2ef166d9-9ef2-49cb-bc54-3bf35e4e3ba1",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Yeah so since we're only doing migrations during decommissioning, whatever shuffle files remain on the host when it dies will do the cleanup. I can't think of why we would need to do a delete operation here as well, but if it would be useful for your follow on work I can add it?",
        "createdAt" : "2020-06-24T00:53:54Z",
        "updatedAt" : "2020-07-23T21:35:22Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "f921ddd3913d8b749abd2d8de645e5d098d73823",
    "line" : 100,
    "diffHunk" : "@@ -1,1 +373,377 @@       */\n      event.blockUpdatedInfo.blockId match {\n        case ShuffleDataBlockId(shuffleId, _, _) => exec.addShuffle(shuffleId)\n        case _ => // For now we only update on data blocks\n      }"
  },
  {
    "id" : "3dc440d9-62c2-4735-bec5-2910b9e5970c",
    "prId" : 26682,
    "prUrl" : "https://github.com/apache/spark/pull/26682#pullrequestreview-349001212",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "78267ef8-343f-400a-9570-079be9bc0e99",
        "parentId" : null,
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "What is the impact if concurrent updates to execResourceProfileCount (via put) result in inconsistency w.r.t execResourceProfileCount itself and w.r.t executors ? (due to the 'put' happening - across this class).\r\nIf we expect execResourceProfileCount for some key to be some value V while updating it to V1, we could use compute() instead and ensure the invariant is satisfied ? Or do we not expect this to happen ?",
        "createdAt" : "2020-01-26T08:47:48Z",
        "updatedAt" : "2020-01-26T08:51:12Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "d26df732-680a-4760-ba68-8b654eab4bd5",
        "parentId" : "78267ef8-343f-400a-9570-079be9bc0e99",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "so that shouldn't happen to execResourceProfileCount  because all of the calls to modify it are from the event listener which is single threaded.  There is a call to get that can happen from separate threads but that won't affect the modify case.   We update both executors and execResourceProfileCount at the same time on adding and removing so those should be in lock step.\r\nI think I can change them to compute if it makes it easier to read?",
        "createdAt" : "2020-01-27T16:44:06Z",
        "updatedAt" : "2020-01-27T16:44:06Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "38fd43b4-bcb4-42f1-b889-901d236d9e26",
        "parentId" : "78267ef8-343f-400a-9570-079be9bc0e99",
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "If all updates are expected to be single threaded, then change to compute makes no sense (it will simply complicate the code) - I probably missed some note indicating this is the expectation !\r\nThanks for elaborating Tom.",
        "createdAt" : "2020-01-27T21:52:22Z",
        "updatedAt" : "2020-01-27T21:52:23Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      }
    ],
    "commit" : "75091102d6bd64a61fb903c8df3edf9ca047e879",
    "line" : 108,
    "diffHunk" : "@@ -1,1 +427,431 @@          s\"count is now $newcount\")\n        new Tracker(resourceProfileId)\n      })\n    // if we had added executor before without knowing the resource profile id, fix it up\n    if (execTracker.resourceProfileId == UNKNOWN_RESOURCE_PROFILE_ID &&"
  }
]