[
  {
    "id" : "22cc1e9e-fc63-4400-bf70-41b337436698",
    "prId" : 31256,
    "prUrl" : "https://github.com/apache/spark/pull/31256#pullrequestreview-571905313",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "217fea12-d33c-4486-9088-6daeabdcd131",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "When `cachedBlocks.empty == true`, this will be `Long.MaxValue`. That's wrong, isn't it, @hanyucui ?",
        "createdAt" : "2021-01-20T06:08:18Z",
        "updatedAt" : "2021-01-20T06:08:18Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "7d933d98-a1a0-4def-943f-35e388e2024c",
        "parentId" : "217fea12-d33c-4486-9088-6daeabdcd131",
        "authorId" : "b4586f54-4dcf-4d45-99ee-e2fb177c113c",
        "body" : "@dongjoon-hyun Yeah, you are right. This is not a solution but only an illustration. I wish there was a way to prevent this from bothering admins.\r\n\r\nIn my case, I enable dynamic allocation on K8s. I set `spark.dynamicAllocation.shuffleTracking.timeout` to a small number so that I can kill the executors earlier but set `spark.dynamicAllocation.cachedExecutorIdleTimeout` to a larger value since some users would like to keep their cached data around for longer. I expected the executors to be killed when the larger timeout is hit, not the smaller one, but the current behavior is the opposite. I understand different users have different requirements and this might totally make sense for them. No action is needed on your side -- I will think it through first.",
        "createdAt" : "2021-01-20T06:21:13Z",
        "updatedAt" : "2021-01-20T06:21:14Z",
        "lastEditedBy" : "b4586f54-4dcf-4d45-99ee-e2fb177c113c",
        "tags" : [
        ]
      },
      {
        "id" : "b9d8e558-7b13-4e68-a3da-30387216721f",
        "parentId" : "217fea12-d33c-4486-9088-6daeabdcd131",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "I understand your opinion, but this feature is not designed like that originally.",
        "createdAt" : "2021-01-20T06:30:54Z",
        "updatedAt" : "2021-01-20T06:30:54Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "b78d8d6f196b9f5a194acf87f0e2386366d0f712",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +547,551 @@            Long.MaxValue\n          }\n          math.max(_cacheTimeout, _shuffleTimeout)\n        } else {\n          idleTimeoutNs"
  },
  {
    "id" : "85ec08f2-28a7-4ab7-8550-210e2a238a95",
    "prId" : 31125,
    "prUrl" : "https://github.com/apache/spark/pull/31125#pullrequestreview-567155380",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "27773f60-a2b1-4d5c-9752-9217941034a4",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Just double checking you're sure these work in 2.13? b/c I added a lot of calls like this that will be redundant in 2.12 but not 2.13. (Not saying this one is necessary, particularly.)",
        "createdAt" : "2021-01-11T19:47:33Z",
        "updatedAt" : "2021-01-11T19:47:33Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "0db03fc1-c7a5-4f3d-a6f5-9cd0849de42b",
        "parentId" : "27773f60-a2b1-4d5c-9752-9217941034a4",
        "authorId" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "body" : "Yes, I'm sure this conversion is redundant, the type of `shuffleStages` is `Seq[(Int, Int)]` in Scala 2.13, and I run \r\n`mvn clean install -Pscala-2.13 -pl core -am` locally, all test passed in core module\r\n\r\n```\r\nRun completed in 36 minutes, 54 seconds.\r\nTotal number of tests run: 2710\r\nSuites: completed 270, aborted 0\r\nTests: succeeded 2710, failed 0, canceled 4, ignored 7, pending 0\r\nAll tests passed.\r\n```\r\nI will do more local tests to verify other changes today",
        "createdAt" : "2021-01-12T02:30:31Z",
        "updatedAt" : "2021-01-12T02:38:18Z",
        "lastEditedBy" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "tags" : [
        ]
      },
      {
        "id" : "bca6eccc-dfc8-45e9-a87a-87c54d7dcb01",
        "parentId" : "27773f60-a2b1-4d5c-9752-9217941034a4",
        "authorId" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "body" : "`mvn clean install -Phadoop-3.2 -Phive-2.3 -Phadoop-cloud -Pmesos -Pyarn -Pkinesis-asl -Phive-thriftserver -Pspark-ganglia-lgpl -Pkubernetes -Phive -Pscala-2.13 -pl graphx`\r\n\r\nall test passed\r\n\r\n```\r\nRun completed in 2 minutes, 24 seconds.\r\nTotal number of tests run: 108\r\nSuites: completed 19, aborted 0\r\nTests: succeeded 108, failed 0, canceled 0, ignored 0, pending 0\r\nAll tests passed.\r\n```",
        "createdAt" : "2021-01-12T04:07:41Z",
        "updatedAt" : "2021-01-12T04:07:42Z",
        "lastEditedBy" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "tags" : [
        ]
      },
      {
        "id" : "f524be35-ca85-4a33-aeb0-07cbd7eb94ba",
        "parentId" : "27773f60-a2b1-4d5c-9752-9217941034a4",
        "authorId" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "body" : "`mvn clean install -Phadoop-3.2 -Phive-2.3 -Phadoop-cloud -Pmesos -Pyarn -Pkinesis-asl -Phive-thriftserver -Pspark-ganglia-lgpl -Pkubernetes -Phive -Pscala-2.13 -pl mllib\r\n`\r\nall test passed\r\n```\r\nRun completed in 28 minutes, 39 seconds.\r\nTotal number of tests run: 1627\r\nSuites: completed 206, aborted 0\r\nTests: succeeded 1627, failed 0, canceled 0, ignored 7, pending 0\r\nAll tests passed.\r\n```",
        "createdAt" : "2021-01-12T05:54:12Z",
        "updatedAt" : "2021-01-12T05:54:12Z",
        "lastEditedBy" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "tags" : [
        ]
      },
      {
        "id" : "1a253447-b0f9-45d3-901f-c981d82a7c14",
        "parentId" : "27773f60-a2b1-4d5c-9752-9217941034a4",
        "authorId" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "body" : "`mvn clean install -Phadoop-3.2 -Phive-2.3 -Phadoop-cloud -Pmesos -Pyarn -Pkinesis-asl -Phive-thriftserver -Pspark-ganglia-lgpl -Pkubernetes -Phive -Pscala-2.13 -pl mllib-local`\r\n\r\nall test passed.\r\n\r\n```\r\nRun completed in 2 seconds, 18 milliseconds.\r\nTotal number of tests run: 95\r\nSuites: completed 9, aborted 0\r\nTests: succeeded 95, failed 0, canceled 0, ignored 0, pending 0\r\nAll tests passed.\r\n```\r\n",
        "createdAt" : "2021-01-12T06:31:47Z",
        "updatedAt" : "2021-01-12T06:31:47Z",
        "lastEditedBy" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "tags" : [
        ]
      },
      {
        "id" : "86d44aee-4fb9-43ab-97d3-675a68cd226e",
        "parentId" : "27773f60-a2b1-4d5c-9752-9217941034a4",
        "authorId" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "body" : "`mvn clean install -Phadoop-3.2 -Phive-2.3 -Phadoop-cloud -Pmesos -Pyarn -Pkinesis-asl -Phive-thriftserver -Pspark-ganglia-lgpl -Pkubernetes -Phive -Pscala-2.13 -pl sql/catalyst`\r\n\r\nall test passed.\r\n\r\n```\r\nRun completed in 12 minutes, 27 seconds.\r\nTotal number of tests run: 4718\r\nSuites: completed 261, aborted 0\r\nTests: succeeded 4718, failed 0, canceled 0, ignored 5, pending 0\r\nAll tests passed.\r\n```",
        "createdAt" : "2021-01-12T07:01:03Z",
        "updatedAt" : "2021-01-12T07:01:03Z",
        "lastEditedBy" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "tags" : [
        ]
      },
      {
        "id" : "30aa76f6-a393-4206-a567-a1419d33cc2c",
        "parentId" : "27773f60-a2b1-4d5c-9752-9217941034a4",
        "authorId" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "body" : "`mvn clean install -Phadoop-3.2 -Phive-2.3 -Phadoop-cloud -Pmesos -Pyarn -Pkinesis-asl -Phive-thriftserver -Pspark-ganglia-lgpl -Pkubernetes -Phive -Pscala-2.13 -pl sql/core`\r\n\r\nall test passed.\r\n\r\n```\r\nRun completed in 1 hour, 38 minutes, 11 seconds.\r\nTotal number of tests run: 9185\r\nSuites: completed 406, aborted 0\r\nTests: succeeded 9185, failed 0, canceled 1, ignored 51, pending 0\r\nAll tests passed.\r\n```",
        "createdAt" : "2021-01-12T08:54:33Z",
        "updatedAt" : "2021-01-12T08:54:55Z",
        "lastEditedBy" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "tags" : [
        ]
      },
      {
        "id" : "63513e98-4a90-4839-a877-c5b06839dc65",
        "parentId" : "27773f60-a2b1-4d5c-9752-9217941034a4",
        "authorId" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "body" : "some related modules that have not been verified today : `hive`, `hive-thriftserver` and `yarn`",
        "createdAt" : "2021-01-12T12:45:23Z",
        "updatedAt" : "2021-01-12T12:45:23Z",
        "lastEditedBy" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "tags" : [
        ]
      },
      {
        "id" : "c44708dc-ac92-4eb4-984b-09371b839da6",
        "parentId" : "27773f60-a2b1-4d5c-9752-9217941034a4",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Do you want to check out the other modules first and do this all at once?",
        "createdAt" : "2021-01-12T14:51:22Z",
        "updatedAt" : "2021-01-12T14:51:23Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "50bb3272-b612-4d43-9126-5f99f9d42d8c",
        "parentId" : "27773f60-a2b1-4d5c-9752-9217941034a4",
        "authorId" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "body" : "Yes, I'll check these today",
        "createdAt" : "2021-01-13T02:37:11Z",
        "updatedAt" : "2021-01-13T02:37:11Z",
        "lastEditedBy" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "tags" : [
        ]
      },
      {
        "id" : "1702fbf9-0290-489d-942d-566594203e03",
        "parentId" : "27773f60-a2b1-4d5c-9752-9217941034a4",
        "authorId" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "body" : "@srowen All modules are verified and add new Jira SPARK-34098",
        "createdAt" : "2021-01-13T12:03:29Z",
        "updatedAt" : "2021-01-13T12:08:55Z",
        "lastEditedBy" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "tags" : [
        ]
      }
    ],
    "commit" : "a4e44c21657de5c36c3a6fe18eb0ce2f6c6c3749",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +228,232 @@\n    if (updateExecutors) {\n      val activeShuffleIds = shuffleStages.map(_._2)\n      var needTimeoutUpdate = false\n      val activatedExecs = new ExecutorIdCollector()"
  },
  {
    "id" : "1a344104-811a-4328-8c3a-d4d4efe7a0c7",
    "prId" : 30795,
    "prUrl" : "https://github.com/apache/spark/pull/30795#pullrequestreview-555580173",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "df1f35fe-a91b-4c62-9a0c-ed4e17831115",
        "parentId" : null,
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "we should expand this to state excluded for entire application (I realize HealthTracker implies this but would like to be more explicit), does not include excluded within the stage level.",
        "createdAt" : "2020-12-18T15:28:24Z",
        "updatedAt" : "2020-12-18T16:06:17Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "75cdd1e334c15c8af89c9b0d283bedf12d0f33ee",
    "line" : 165,
    "diffHunk" : "@@ -1,1 +574,578 @@    var decommissioning: Boolean = false\n    var hasActiveShuffle: Boolean = false\n    // whether the executor is temporarily excluded by the `HealthTracker`\n    var excluded: Boolean = false\n"
  },
  {
    "id" : "0f4691ce-5e0c-4e62-96b7-6ed95cb078a0",
    "prId" : 30103,
    "prUrl" : "https://github.com/apache/spark/pull/30103#pullrequestreview-532150265",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "90f1d16b-d570-40bf-8ead-e9a3ca53365b",
        "parentId" : null,
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "I don't follow this change.  Here isIdle means we don't have any running tasks and hasActiveshuffle is false. The ids passed in are supposed to be all the active ones which is determined by shuffleToActiveJobs.  It does not mean that shuffleIds is empty. shuffleIds stick around until we get the ShuffleCleanedEvent that comes from context cleaner when RDD isn't referenced anymore. If its referenced even though the job may be done it means someone could use that still so we don't remove it until after the timeout.  Here if we are idle and had active shuffle we update the Timeout, which should be setting it to either cache or storage timeout and then when the ExecutorMonitor comes around to check it should see that its set and time it out regardless of if it has shuffle ids.\r\n\r\nIf there is a bug in there somewhere then  we need to figure out where it is. I do not think is not the correct fix.",
        "createdAt" : "2020-11-13T15:29:05Z",
        "updatedAt" : "2020-11-13T15:30:20Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "fa71f054-6b06-44bc-a528-70c05cc4b792",
        "parentId" : "90f1d16b-d570-40bf-8ead-e9a3ca53365b",
        "authorId" : "19f8cd22-cf85-435f-a650-411ff7836f0b",
        "body" : "thank you for your explaining. i got it .it seems works well . i will close this issue.",
        "createdAt" : "2020-11-17T09:03:43Z",
        "updatedAt" : "2020-11-17T09:03:43Z",
        "lastEditedBy" : "19f8cd22-cf85-435f-a650-411ff7836f0b",
        "tags" : [
        ]
      }
    ],
    "commit" : "2ff44993d976517738363d8d1feca1e2371dd9a3",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +587,591 @@      hasActiveShuffle = ids.exists(shuffleIds.contains)\n      if (hadActiveShuffle && isIdle) {\n        shuffleIds.clear()\n        updateTimeout()\n      }"
  },
  {
    "id" : "85de189e-f5e2-4067-bb4f-c932a2576805",
    "prId" : 29367,
    "prUrl" : "https://github.com/apache/spark/pull/29367#pullrequestreview-462800278",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "97fe4dc3-9422-476f-b75c-869abd4bd47c",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "I am wondering if the name `markExecutorsDecommissioned` would reflect the intent of this function better ?",
        "createdAt" : "2020-08-06T05:49:03Z",
        "updatedAt" : "2020-08-12T19:09:04Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "eefff4d0-bc06-41ad-8c25-6cd48b1fe39f",
        "parentId" : "97fe4dc3-9422-476f-b75c-869abd4bd47c",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "This matches the function for killing executors (`executorsKilled`), so I think keeping the naming convention consistent is better.",
        "createdAt" : "2020-08-06T19:07:09Z",
        "updatedAt" : "2020-08-12T19:09:04Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "e970cb10147fb64533f5088edc3a448b5ef198cf",
    "line" : 31,
    "diffHunk" : "@@ -1,1 +152,156 @@  }\n\n  private[spark] def executorsDecommissioned(ids: Seq[String]): Unit = {\n    ids.foreach { id =>\n      val tracker = executors.get(id)"
  },
  {
    "id" : "eb94c874-ab5d-4c3b-9fe7-8105102238fe",
    "prId" : 29367,
    "prUrl" : "https://github.com/apache/spark/pull/29367#pullrequestreview-463829692",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "75102d7f-b2ce-43e0-9b8b-1fec5c9000dd",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "I didn't quite understand the reason for the change in this hunk: It seems that we should only be changing the KILL message to DECOMMISSION message if I understand the commit message/PR description properly. \r\n\r\nAre we also changing what gets decommissioned or not ?",
        "createdAt" : "2020-08-06T05:52:10Z",
        "updatedAt" : "2020-08-12T19:09:04Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "dc49ab3b-3f4e-40ef-863e-b68cd906eb81",
        "parentId" : "75102d7f-b2ce-43e0-9b8b-1fec5c9000dd",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "So since we now move shuffle files around, we need to keep track of the changes if decommissioning and the executor monitor are both enabled.",
        "createdAt" : "2020-08-06T19:26:11Z",
        "updatedAt" : "2020-08-12T19:09:04Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "6506db55-4e34-4e47-9b1d-235f335b5d13",
        "parentId" : "75102d7f-b2ce-43e0-9b8b-1fec5c9000dd",
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "Got it.",
        "createdAt" : "2020-08-08T22:11:47Z",
        "updatedAt" : "2020-08-12T19:09:04Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      }
    ],
    "commit" : "e970cb10147fb64533f5088edc3a448b5ef198cf",
    "line" : 86,
    "diffHunk" : "@@ -1,1 +362,366 @@    val exec = ensureExecutorIsTracked(event.blockUpdatedInfo.blockManagerId.executorId,\n      UNKNOWN_RESOURCE_PROFILE_ID)\n\n    // Check if it is a shuffle file, or RDD to pick the correct codepath for update\n    if (!event.blockUpdatedInfo.blockId.isInstanceOf[RDDBlockId]) {"
  },
  {
    "id" : "09b70bea-f5f5-4cfa-95af-a9279868385d",
    "prId" : 29367,
    "prUrl" : "https://github.com/apache/spark/pull/29367#pullrequestreview-463829692",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5666fc0a-56da-493f-acb6-cee9ad47afac",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "I am just trying to follow along this code, so pardon me if this is a n00b question: Why are we separately tracking pendingRemoval and decommissioning separately ? Two questions about that:\r\n\r\n- If an executor is marked as decommissioned here, when is it actually removed ? (Outside of dynamic allocation that happens when the executor naturally has a heartbeat failure. ). \r\n- Is my understanding correct that if graceful decommissioning is plugged into dynamic-allocation (this feature) AND the cluster manager supports decommissioning, then pendingRemoval would be empty -- ie executors would only be decommission ?",
        "createdAt" : "2020-08-08T17:16:52Z",
        "updatedAt" : "2020-08-12T19:09:04Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "5e9c14c3-08b1-46eb-9afd-aaf87c4d6e6f",
        "parentId" : "5666fc0a-56da-493f-acb6-cee9ad47afac",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "We also kill executors if they end up blacklisted so I think we want to track them separately.",
        "createdAt" : "2020-08-08T18:31:15Z",
        "updatedAt" : "2020-08-12T19:09:04Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "6c2fe7f9-422b-4a05-a915-3c68c59fc911",
        "parentId" : "5666fc0a-56da-493f-acb6-cee9ad47afac",
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "Thanks for clarifying that !",
        "createdAt" : "2020-08-08T22:11:10Z",
        "updatedAt" : "2020-08-12T19:09:04Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      }
    ],
    "commit" : "e970cb10147fb64533f5088edc3a448b5ef198cf",
    "line" : 77,
    "diffHunk" : "@@ -1,1 +353,357 @@    if (removed != null) {\n      decrementExecResourceProfileCount(removed.resourceProfileId)\n      if (!removed.pendingRemoval || !removed.decommissioning) {\n        nextTimeout.set(Long.MinValue)\n      }"
  },
  {
    "id" : "4943caf3-3b21-4879-973d-b0ce07706b35",
    "prId" : 29367,
    "prUrl" : "https://github.com/apache/spark/pull/29367#pullrequestreview-464613409",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4bb96640-7b84-453e-815b-9ded012f0362",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "Is this comment change intended ?",
        "createdAt" : "2020-08-08T17:17:25Z",
        "updatedAt" : "2020-08-12T19:09:04Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "80f8b437-c941-477b-809f-33e017e05b51",
        "parentId" : "4bb96640-7b84-453e-815b-9ded012f0362",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Yes, since we're eventually going to want to use intelligent metrics to decide who to scale down I'd like us to only track shuffle files that are being used not speculative ones. Doesn't need to be addressed right now which is why it's a TODO.",
        "createdAt" : "2020-08-10T21:52:17Z",
        "updatedAt" : "2020-08-12T19:09:05Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "e970cb10147fb64533f5088edc3a448b5ef198cf",
    "line" : 68,
    "diffHunk" : "@@ -1,1 +324,328 @@      // This means that an executor may be marked as having shuffle data, and thus prevented\n      // from being removed, even though the data may not be used.\n      // TODO: Only track used files (SPARK-31974)\n      if (shuffleTrackingEnabled && event.reason == Success) {\n        stageToShuffleID.get(event.stageId).foreach { shuffleId =>"
  },
  {
    "id" : "aa9a3847-3427-45c0-9498-ea301affb1ea",
    "prId" : 29367,
    "prUrl" : "https://github.com/apache/spark/pull/29367#pullrequestreview-464612539",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "26c6f324-387a-43fb-8ac1-102dec703c3f",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "I went through all of the usages of executor.pendingRemoval and executor.decommissioning flag: They are treated identically right now. That is for all practical purposes an executor being decommissioned is treated the same as an executor pending to be removed. \r\n\r\nDo you have a use case in mind of why you would like to distinguish b/w these two states ? If you don't need to distinguish, the change would become simpler if you treat a decommissioned executor as pending removal. \r\n\r\nI cannot see where this distinction is relevant in this PR, so perhaps you have a future use case in mind for this distinction ?",
        "createdAt" : "2020-08-08T23:54:32Z",
        "updatedAt" : "2020-08-12T19:09:05Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "3ed2c9f0-39dc-4464-a91e-c6a422465fe3",
        "parentId" : "26c6f324-387a-43fb-8ac1-102dec703c3f",
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "^^^ @holdenk ... any thoughts/followup on this ?",
        "createdAt" : "2020-08-10T19:42:55Z",
        "updatedAt" : "2020-08-12T19:09:05Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "3e561251-e1e9-44cc-8510-3e99e5eb4149",
        "parentId" : "26c6f324-387a-43fb-8ac1-102dec703c3f",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Eventually I'd like us to have better logging and metrics around decommissioning and understanding it's impact versus blacklisting, although to be fair that isn't in the short term.",
        "createdAt" : "2020-08-10T21:50:36Z",
        "updatedAt" : "2020-08-12T19:09:05Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "e970cb10147fb64533f5088edc3a448b5ef198cf",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +116,120 @@      timedOutExecs = executors.asScala\n        .filter { case (_, exec) =>\n          !exec.pendingRemoval && !exec.hasActiveShuffle && !exec.decommissioning}\n        .filter { case (_, exec) =>\n          val deadline = exec.timeoutAt"
  },
  {
    "id" : "74ab0733-0880-4622-965b-2f7629203b50",
    "prId" : 28818,
    "prUrl" : "https://github.com/apache/spark/pull/28818#pullrequestreview-436253882",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2ef166d9-9ef2-49cb-bc54-3bf35e4e3ba1",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Since we are touching `ExecutorMonitor`, when do we have a counter operation, `exec.removeShuffle`? In this PR, it seems that `executorsKilled` is used. Is it enough?\r\ncc @dbtsai ",
        "createdAt" : "2020-06-16T22:31:19Z",
        "updatedAt" : "2020-07-23T21:35:22Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "1b2ba6b0-68d3-4e1d-b2ff-7a3cb4f16a3c",
        "parentId" : "2ef166d9-9ef2-49cb-bc54-3bf35e4e3ba1",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Yeah so since we're only doing migrations during decommissioning, whatever shuffle files remain on the host when it dies will do the cleanup. I can't think of why we would need to do a delete operation here as well, but if it would be useful for your follow on work I can add it?",
        "createdAt" : "2020-06-24T00:53:54Z",
        "updatedAt" : "2020-07-23T21:35:22Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "f921ddd3913d8b749abd2d8de645e5d098d73823",
    "line" : 100,
    "diffHunk" : "@@ -1,1 +373,377 @@       */\n      event.blockUpdatedInfo.blockId match {\n        case ShuffleDataBlockId(shuffleId, _, _) => exec.addShuffle(shuffleId)\n        case _ => // For now we only update on data blocks\n      }"
  },
  {
    "id" : "3dc440d9-62c2-4735-bec5-2910b9e5970c",
    "prId" : 26682,
    "prUrl" : "https://github.com/apache/spark/pull/26682#pullrequestreview-349001212",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "78267ef8-343f-400a-9570-079be9bc0e99",
        "parentId" : null,
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "What is the impact if concurrent updates to execResourceProfileCount (via put) result in inconsistency w.r.t execResourceProfileCount itself and w.r.t executors ? (due to the 'put' happening - across this class).\r\nIf we expect execResourceProfileCount for some key to be some value V while updating it to V1, we could use compute() instead and ensure the invariant is satisfied ? Or do we not expect this to happen ?",
        "createdAt" : "2020-01-26T08:47:48Z",
        "updatedAt" : "2020-01-26T08:51:12Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "d26df732-680a-4760-ba68-8b654eab4bd5",
        "parentId" : "78267ef8-343f-400a-9570-079be9bc0e99",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "so that shouldn't happen to execResourceProfileCount  because all of the calls to modify it are from the event listener which is single threaded.  There is a call to get that can happen from separate threads but that won't affect the modify case.   We update both executors and execResourceProfileCount at the same time on adding and removing so those should be in lock step.\r\nI think I can change them to compute if it makes it easier to read?",
        "createdAt" : "2020-01-27T16:44:06Z",
        "updatedAt" : "2020-01-27T16:44:06Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "38fd43b4-bcb4-42f1-b889-901d236d9e26",
        "parentId" : "78267ef8-343f-400a-9570-079be9bc0e99",
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "If all updates are expected to be single threaded, then change to compute makes no sense (it will simply complicate the code) - I probably missed some note indicating this is the expectation !\r\nThanks for elaborating Tom.",
        "createdAt" : "2020-01-27T21:52:22Z",
        "updatedAt" : "2020-01-27T21:52:23Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      }
    ],
    "commit" : "75091102d6bd64a61fb903c8df3edf9ca047e879",
    "line" : 108,
    "diffHunk" : "@@ -1,1 +427,431 @@          s\"count is now $newcount\")\n        new Tracker(resourceProfileId)\n      })\n    // if we had added executor before without knowing the resource profile id, fix it up\n    if (execTracker.resourceProfileId == UNKNOWN_RESOURCE_PROFILE_ID &&"
  },
  {
    "id" : "1f8ba4b8-99cf-4f5f-8ecf-7b7ab091c540",
    "prId" : 24817,
    "prUrl" : "https://github.com/apache/spark/pull/24817#pullrequestreview-255334586",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7e0a22ce-c6ee-4108-b218-916dc51ac436",
        "parentId" : null,
        "authorId" : "e913f9af-103d-40d4-976e-9aa8e12e7211",
        "body" : "it is weird that this will be visible to all user listeners as well ...",
        "createdAt" : "2019-06-21T21:39:37Z",
        "updatedAt" : "2019-07-15T18:05:50Z",
        "lastEditedBy" : "e913f9af-103d-40d4-976e-9aa8e12e7211",
        "tags" : [
        ]
      },
      {
        "id" : "878bc7e7-abb1-45a4-bce6-0ffc36d02b87",
        "parentId" : "7e0a22ce-c6ee-4108-b218-916dc51ac436",
        "authorId" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "body" : "Well, all listeners already have to deal with events they don't understand; that's the contract for `onOtherEvent`, since we're allowed to add new events (public or not).\r\n\r\nThe only odd thing here is that this event is not public, so you can't really handle it outside of Spark (without reflection), but I don't really see an easy way to solve this issue differently (outside of adding locking which I don't want to).",
        "createdAt" : "2019-06-24T21:49:30Z",
        "updatedAt" : "2019-07-15T18:05:50Z",
        "lastEditedBy" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "tags" : [
        ]
      },
      {
        "id" : "d1d4c6fc-962e-4c2d-b726-e98ff45eaea6",
        "parentId" : "7e0a22ce-c6ee-4108-b218-916dc51ac436",
        "authorId" : "e913f9af-103d-40d4-976e-9aa8e12e7211",
        "body" : "yeah, that's the weird part, you get this event with a class that is not public.\r\n\r\nThe only things I can think of are (1) to create a new event loop here, which just forwards the listener events and also this new event or (2) create a \"InternalListenerEvent\" which is automatically filtered from all non-internal listeners.  But both of those seem like overkill.",
        "createdAt" : "2019-06-27T16:26:22Z",
        "updatedAt" : "2019-07-15T18:05:50Z",
        "lastEditedBy" : "e913f9af-103d-40d4-976e-9aa8e12e7211",
        "tags" : [
        ]
      }
    ],
    "commit" : "6154bf486e68dbb5a4c16dc9e71030cc20d8ca58",
    "line" : 211,
    "diffHunk" : "@@ -1,1 +354,358 @@    // Because this is called in a completely separate thread, we post a custom event to the\n    // listener bus so that the internal state is safely updated.\n    listenerBus.post(ShuffleCleanedEvent(shuffleId))\n  }\n"
  },
  {
    "id" : "dc82de6e-52dd-49f1-9b09-a63fcf42fc77",
    "prId" : 24817,
    "prUrl" : "https://github.com/apache/spark/pull/24817#pullrequestreview-255465765",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cba887f0-8e98-4910-964a-4464eba602c9",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Shall we add `hasActiveShuffle` in if condition or starts with `if (isIdle)` rather than `if (idleStart >= 0)` ?",
        "createdAt" : "2019-06-27T02:39:49Z",
        "updatedAt" : "2019-07-15T18:05:50Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "f0f62414-cbe9-47d5-8b43-0c36385b1ede",
        "parentId" : "cba887f0-8e98-4910-964a-4464eba602c9",
        "authorId" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "body" : "This was intentional. The `hasActiveShuffle` tracking is not meant to change when the executor times out, just when it should not be removed.\r\n\r\nDoing that would mean that every time a job starts with a shuffle that this executor has data, its timeout would be reset, even if it ends up not running any tasks. (Which is unlikely, but well.)",
        "createdAt" : "2019-06-27T20:35:32Z",
        "updatedAt" : "2019-07-15T18:05:50Z",
        "lastEditedBy" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "tags" : [
        ]
      }
    ],
    "commit" : "6154bf486e68dbb5a4c16dc9e71030cc20d8ca58",
    "line" : 276,
    "diffHunk" : "@@ -1,1 +440,444 @@        val timeout = if (cachedBlocks.nonEmpty || (shuffleIds != null && shuffleIds.nonEmpty)) {\n          val _cacheTimeout = if (cachedBlocks.nonEmpty) storageTimeoutMs else Long.MaxValue\n          val _shuffleTimeout = if (shuffleIds != null && shuffleIds.nonEmpty) {\n            shuffleTimeoutMs\n          } else {"
  },
  {
    "id" : "6eeec189-b643-4fdd-93db-9fdb2336563f",
    "prId" : 24817,
    "prUrl" : "https://github.com/apache/spark/pull/24817#pullrequestreview-255941847",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7c34b8b6-3a1b-42e4-911c-6593cad5f4ef",
        "parentId" : null,
        "authorId" : "e913f9af-103d-40d4-976e-9aa8e12e7211",
        "body" : "Here you're only grabbing the active shuffles of this job.  I don't think that is right, when you've got two concurrent jobs submitted with a distinct set of shuffles.  The first job will need shuffle A on executor X, and then this needs shuffle B on executor Y, and it would look like executor X does not have an active shuffle anymore.\r\n\r\nit might also help to rename `shuffleStages` --> `thisJobShuffleStage`.",
        "createdAt" : "2019-06-27T16:14:42Z",
        "updatedAt" : "2019-07-15T18:05:50Z",
        "lastEditedBy" : "e913f9af-103d-40d4-976e-9aa8e12e7211",
        "tags" : [
        ]
      },
      {
        "id" : "00c334f3-b734-4992-9043-79904335c1c4",
        "parentId" : "7c34b8b6-3a1b-42e4-911c-6593cad5f4ef",
        "authorId" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "body" : "While what you say is true in isolation, the check for `if (!exec.hasActiveShuffle) {` below makes sure the end result is correct. When the second jobs starts, executor X has an active shuffle, so this code would not change that state.",
        "createdAt" : "2019-06-27T21:23:58Z",
        "updatedAt" : "2019-07-15T18:05:50Z",
        "lastEditedBy" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "tags" : [
        ]
      },
      {
        "id" : "f351bdbc-1053-4480-8d57-3b91cb47e5ea",
        "parentId" : "7c34b8b6-3a1b-42e4-911c-6593cad5f4ef",
        "authorId" : "e913f9af-103d-40d4-976e-9aa8e12e7211",
        "body" : "my fault, you are right!",
        "createdAt" : "2019-06-28T19:35:19Z",
        "updatedAt" : "2019-07-15T18:05:50Z",
        "lastEditedBy" : "e913f9af-103d-40d4-976e-9aa8e12e7211",
        "tags" : [
        ]
      }
    ],
    "commit" : "6154bf486e68dbb5a4c16dc9e71030cc20d8ca58",
    "line" : 92,
    "diffHunk" : "@@ -1,1 +181,185 @@\n    if (updateExecutors) {\n      val activeShuffleIds = shuffleStages.map(_._2).toSeq\n      var needTimeoutUpdate = false\n      val activatedExecs = new mutable.ArrayBuffer[String]()"
  },
  {
    "id" : "012f76a5-3617-4703-8d6c-c919a2cd5402",
    "prId" : 24817,
    "prUrl" : "https://github.com/apache/spark/pull/24817#pullrequestreview-255334586",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8c1fb87b-d30e-4103-9ebd-9cba5fa11431",
        "parentId" : null,
        "authorId" : "e913f9af-103d-40d4-976e-9aa8e12e7211",
        "body" : "is it worthwhile to logdebug here too? (with similar comment to above for combining them)",
        "createdAt" : "2019-06-27T16:20:38Z",
        "updatedAt" : "2019-07-15T18:05:50Z",
        "lastEditedBy" : "e913f9af-103d-40d4-976e-9aa8e12e7211",
        "tags" : [
        ]
      }
    ],
    "commit" : "6154bf486e68dbb5a4c16dc9e71030cc20d8ca58",
    "line" : 148,
    "diffHunk" : "@@ -1,1 +237,241 @@      executors.asScala.foreach { case (id, exec) =>\n        if (exec.hasActiveShuffle) {\n          exec.updateActiveShuffles(activeShuffles)\n          if (!exec.hasActiveShuffle) {\n            deactivatedExecs += id"
  },
  {
    "id" : "44268599-4331-4797-8aab-29d46d102174",
    "prId" : 24704,
    "prUrl" : "https://github.com/apache/spark/pull/24704#pullrequestreview-243501678",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e2d53d29-e270-4115-b71f-546946768243",
        "parentId" : null,
        "authorId" : "e913f9af-103d-40d4-976e-9aa8e12e7211",
        "body" : "do you want to do this when `storageLevel.isValid && fetchFromShuffleSvcEnabled && storageLevel.useDisk`?  I If its the first time the block is cached, then its a no-op.  If it has already been cached in memory, then this would no longer apply the cached timeout to this executor.  I guess that is desired?  just want to check.",
        "createdAt" : "2019-05-29T18:03:05Z",
        "updatedAt" : "2019-06-05T01:27:44Z",
        "lastEditedBy" : "e913f9af-103d-40d4-976e-9aa8e12e7211",
        "tags" : [
        ]
      },
      {
        "id" : "ed0ed96b-35e1-47ff-a9ce-242f4cdfbf02",
        "parentId" : "e2d53d29-e270-4115-b71f-546946768243",
        "authorId" : "e913f9af-103d-40d4-976e-9aa8e12e7211",
        "body" : "ok I see this is explicitly checked in a test too.",
        "createdAt" : "2019-05-29T18:40:14Z",
        "updatedAt" : "2019-06-05T01:27:44Z",
        "lastEditedBy" : "e913f9af-103d-40d4-976e-9aa8e12e7211",
        "tags" : [
        ]
      },
      {
        "id" : "d0fac18d-bdf9-46a7-b57b-c2c88e31cce2",
        "parentId" : "e2d53d29-e270-4115-b71f-546946768243",
        "authorId" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "body" : "Yep, that's basically what SPARK-27677 does.",
        "createdAt" : "2019-05-29T20:34:07Z",
        "updatedAt" : "2019-06-05T01:27:44Z",
        "lastEditedBy" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "tags" : [
        ]
      }
    ],
    "commit" : "9ae62d930aec531c5c19a718a76ea4ac01c2695d",
    "line" : 183,
    "diffHunk" : "@@ -1,1 +181,185 @@      }\n    } else {\n      exec.cachedBlocks.get(blockId.rddId).foreach { blocks =>\n        blocks -= blockId.splitIndex\n        if (blocks.isEmpty) {"
  },
  {
    "id" : "83695f87-9476-454e-a0f3-59571e668f3c",
    "prId" : 24704,
    "prUrl" : "https://github.com/apache/spark/pull/24704#pullrequestreview-243401974",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "07f0a9eb-02aa-4f6a-98c5-ca7b8032100e",
        "parentId" : null,
        "authorId" : "e913f9af-103d-40d4-976e-9aa8e12e7211",
        "body" : "would help to add a comment that its rdd_id -> partition_ids",
        "createdAt" : "2019-05-29T18:04:43Z",
        "updatedAt" : "2019-06-05T01:27:44Z",
        "lastEditedBy" : "e913f9af-103d-40d4-976e-9aa8e12e7211",
        "tags" : [
        ]
      }
    ],
    "commit" : "9ae62d930aec531c5c19a718a76ea4ac01c2695d",
    "line" : 253,
    "diffHunk" : "@@ -1,1 +251,255 @@    // Maps RDD IDs to the partition IDs stored in the executor.\n    // This should only be used in the event thread.\n    val cachedBlocks = new mutable.HashMap[Int, mutable.BitSet]()\n\n    // For testing."
  },
  {
    "id" : "423c6287-bb3a-4d63-879b-103fecd68a22",
    "prId" : 24704,
    "prUrl" : "https://github.com/apache/spark/pull/24704#pullrequestreview-244608368",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f8271d74-339b-49ae-9519-1c92ee2ab5f7",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Why set current `nextTimeout` to `MinValue` ? Do you mean we need eagerly recompute timed out executors at next round to `update the list of timed out executors` ? But, as far as I know from the code, each time EAM thread invoke on `timedOutExecutors()`, it gets the list of timed out executors and asks cluster manager to kill them by `ExecutorAllocationClient` immediately. So, if an executor has already been considered timed out and killing after last round, will there be any difference for that executor if we eagerly `update the list of timed out executors` in next round ?\r\n\r\nI guess, here, you mean to handle the case that an event comes making the executor to be non timed out again after the  executor just considered to be timeout nearly **after scanning**. But, as I mentioned above, I think it won't work.  Instead of **after scanning**, I think, maybe, we can do optimization for the case when it happens **during scanning**.  For example, wrapping a `while` loop on scanning and exit until all timed out executors' `timeOut` to be `true`:\r\n\r\n```\r\n// this check guard against event which updates deadline during scanning\r\nwhile(!checkTimedOut(timedOutExecs)) {\r\n ...\r\n timedOutExecs = scanning {\r\n }\r\n ...\r\n}\r\n```\r\n\r\nWDYT ?",
        "createdAt" : "2019-05-30T06:29:31Z",
        "updatedAt" : "2019-06-05T01:27:44Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "861746f9-5523-4656-a30c-4e34bc2d4bfe",
        "parentId" : "f8271d74-339b-49ae-9519-1c92ee2ab5f7",
        "authorId" : "e913f9af-103d-40d4-976e-9aa8e12e7211",
        "body" : "> it gets the list of timed out executors and asks cluster manager to kill them by ExecutorAllocationClient immediately. So, if an executor has already been considered timed out and killing after last round, will there be any difference for that executor if we eagerly update the list of timed out executors in next round ?\r\n\r\nonly the CoarseGrainedSchedulerBackend has a consistent view of which executors are actually running tasks, and so it has ultimate say in whether or not to kill an executor.  You might not have killed an executor earlier because it was actually busy.\r\n\r\nSo here is where you \"catch up\" with the view of the CGSB, so you know that executor shouldn't be considered timed out and requested to be killed next time around.\r\n\r\n(at least I think so, marcelo can confirm ...)",
        "createdAt" : "2019-05-30T17:13:08Z",
        "updatedAt" : "2019-06-05T01:27:44Z",
        "lastEditedBy" : "e913f9af-103d-40d4-976e-9aa8e12e7211",
        "tags" : [
        ]
      },
      {
        "id" : "474e7bed-ab18-4c1f-bd73-0f2b0914b57c",
        "parentId" : "f8271d74-339b-49ae-9519-1c92ee2ab5f7",
        "authorId" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "body" : "What Imran said, with a complication.\r\n\r\nThe scenario here is decidedly rare. The EAM thread sees a timed out executor at the same time a new task is starting on it. So the EAM tries to kill it, but the CGSB says no.\r\n\r\nWhile that's happening the task start event arrives, and the monitor now knows the executor is not idle anymore. But the executor is still there in the list of timed out executors, so on every cycle, the EAM would still try to kill it and the CGSB will keep saying no. Until it becomes idle again (but not timed out!), and then it will be killed, but at the wrong time.\r\n\r\nSo it needs to be removed from the timed out list to avoid that problem.\r\n\r\nThere's still a case where the task start event isn't processed until after the task has actually ended, and the EAM thread runs again and actually manages to kill an executor before it should. But that should be even rarer, and I'm not sure that can be fixed, since it involves synchronization with threads that this code has no control over.",
        "createdAt" : "2019-05-30T18:29:54Z",
        "updatedAt" : "2019-06-05T01:27:44Z",
        "lastEditedBy" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "tags" : [
        ]
      },
      {
        "id" : "e36dea78-b7b5-40de-ba50-1949468524e5",
        "parentId" : "f8271d74-339b-49ae-9519-1c92ee2ab5f7",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Make sense.@squito @vanzin\r\n\r\n> But the executor is still there in the list of timed out executors, so on every cycle, the EAM would still try to kill it and the CGSB will keep saying no. \r\n\r\nI just realized that we have a cached `timedOutExecs` list in monitor. ~~But there's one last point we may overlook. Once an executor has considered to be timed out in a previous scanning round, its `pendingRemoval` would be marked as `true`. So, in next scanning round, we'll filter it out before checking its `deadline`. Then, it won't appear in `timedOutExecs` due to `pendingRemoval` rather than its new `deadline`. We should cover this.~~\r\n\r\nBut, why we need to cache `timedOutExecs` in monitor ? Could we always return a latest timed out execs list to EAM on every cycle(e.g. return empty when now < nextTimeout.get) ?",
        "createdAt" : "2019-05-31T14:16:31Z",
        "updatedAt" : "2019-06-05T01:27:44Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "f7de80f2-3b1a-4a9d-8c96-b481dc818af1",
        "parentId" : "f8271d74-339b-49ae-9519-1c92ee2ab5f7",
        "authorId" : "e913f9af-103d-40d4-976e-9aa8e12e7211",
        "body" : "> So, in next scanning round, we'll filter it out before checking its deadline. Then, it won't appear in timedOutExecs due to pendingRemoval rather than its new deadline. We should cover this.\r\n\r\nI'm not sure I understand this part.  `pendingRemoval` is after the CGSB has acknowledged that it will remove the executor.  We shouldn't need to send any more messages about killing the executor after this.\r\n\r\n> But, why we need to cache timedOutExecs in monitor ? Could we always return a latest timed out execs list to EAM on every cycle(e.g. return empty when now < nextTimeout.get) ?\r\n\r\nI think you might be right about this, I would need to read it more carefully ... but in any case, I think that would make it significantly harder to follow.  Its a lot easier to reason about if `ExecutorMonitor.timedOutExecutors()` consistently returns the right list.",
        "createdAt" : "2019-05-31T16:41:03Z",
        "updatedAt" : "2019-06-05T01:27:44Z",
        "lastEditedBy" : "e913f9af-103d-40d4-976e-9aa8e12e7211",
        "tags" : [
        ]
      },
      {
        "id" : "a2e68a5b-246f-402b-aeb6-1591aa7671e5",
        "parentId" : "f8271d74-339b-49ae-9519-1c92ee2ab5f7",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "> pendingRemoval is after the CGSB has acknowledged that it will remove the executor. We shouldn't need to send any more messages about killing the executor after this.\r\n\r\nI revisit the code, you @squito are right. Thanks for pointing it out.",
        "createdAt" : "2019-05-31T16:55:52Z",
        "updatedAt" : "2019-06-05T01:27:44Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "9ee0ac33-cf8e-40fa-b3e1-721cb9b1b9fa",
        "parentId" : "f8271d74-339b-49ae-9519-1c92ee2ab5f7",
        "authorId" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "body" : "> But, why we need to cache timedOutExecs in monitor ?\r\n\r\nAs explained the a comment where the `timedOutExecs` variable is declared, that's to avoid scanning the whole list of executors every 100ms.",
        "createdAt" : "2019-05-31T17:16:31Z",
        "updatedAt" : "2019-06-05T01:27:44Z",
        "lastEditedBy" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "tags" : [
        ]
      },
      {
        "id" : "fc63fc06-368c-4fc0-8f5f-b537335d8d1d",
        "parentId" : "f8271d74-339b-49ae-9519-1c92ee2ab5f7",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "> that's to avoid scanning the whole list of executors every 100ms.\r\n\r\nThis doesn't make sense. Take for a not appropriate example, if we always got `now >= nextTimeout.get()`, then, we have to scanning every 100ms. I mean, that, we can't avoid `scanning the whole list of executors every 100ms` just by the cached `timedOutExecs`.  We can avoid scanning if and only if `now < nextTimeout.get()` and we can just return empty list to EAM if `now < nextTimeout.get()` without scanning.\r\n",
        "createdAt" : "2019-05-31T17:29:15Z",
        "updatedAt" : "2019-06-05T01:27:44Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "2304f7d8-13a4-42fd-8661-436600dfc103",
        "parentId" : "f8271d74-339b-49ae-9519-1c92ee2ab5f7",
        "authorId" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "body" : "Hmmm, possibly. I think after some recent changes were made to the way I'm using `nextTimeout` it can replace the cached list. Let me take a look.",
        "createdAt" : "2019-05-31T17:44:01Z",
        "updatedAt" : "2019-06-05T01:27:44Z",
        "lastEditedBy" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "tags" : [
        ]
      },
      {
        "id" : "65d4eb01-94c8-4a71-a757-ab2c757c5e62",
        "parentId" : "f8271d74-339b-49ae-9519-1c92ee2ab5f7",
        "authorId" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "body" : "Yeah, this doesn't work because of an edge case.\r\n\r\nWhen calculating the timed out executors, the next timeout is set for when the next /non-timed out/ executor will time out. That is the important part.\r\n\r\nNow imagine that you return 3 timed out executors and the EAM doesn't kill any of them (either because of limits, or because the CGSB doesn't do it). Now you will not get a call to `executorsKilled`, so the next timeout will not be reset.\r\n\r\nWhich means that next time you call `timedOutExecutors()` it will return an empty list, instead of the 3 timed out executors.\r\n\r\nSo you need that cache.",
        "createdAt" : "2019-05-31T22:56:56Z",
        "updatedAt" : "2019-06-05T01:27:44Z",
        "lastEditedBy" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "tags" : [
        ]
      },
      {
        "id" : "124bdb88-19ac-4c7f-a6cf-09eadd3e6d16",
        "parentId" : "f8271d74-339b-49ae-9519-1c92ee2ab5f7",
        "authorId" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "body" : "BTW you could say \"just call `executorsKilled` with an empty list\" but then that means the optimization doesn't work, and you're basically calculating the timed out list on every EAM schedule call.",
        "createdAt" : "2019-05-31T22:59:50Z",
        "updatedAt" : "2019-06-05T01:27:44Z",
        "lastEditedBy" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "tags" : [
        ]
      },
      {
        "id" : "1610ff53-81af-4071-b7dd-39169cef1540",
        "parentId" : "f8271d74-339b-49ae-9519-1c92ee2ab5f7",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Okay, I see the problem here. But, things here I care about is that, with a cached list, EAM may performs unnecessary executors removing for many many times before nextTimeout change. For example, we have 3 executors in monitor, and EAM gets those 3 timeout executors in a single round scanning. That's means, we can not update nextTimeout during this scanning and nextTimeout would just be `Long.MaxValue` before new events(which updates nextTimeout) comes. And, for EAM, before nextTimeout updates, it always gets those 3 cached timed out executors on every 100ms, and performs unnecessary executor removing again and again. This case may be rare, or have less impact on EAM,  but I do worry about this similar possible unnecessary behavior  with that cache list.\r\n\r\nI think remove that cache list can be achieved with a little more change:\r\n\r\n```\r\nif (deadline > now) {\r\n newNextTimeout = math.min(newNextTimeout, deadline)\r\n exec.timedOut = false\r\n false\r\n} else {\r\n exec.timedOut = true\r\n true\r\n}\r\n```\r\n\r\n1. instead of only updating `newNextTimeout` for non timed out executors, we should update it for timed out executors, too\r\n\r\n2. when we get a call to `executorsKilled(executorIds)`,  updates `nextTimeout` comparing to those killed executors' `timeoutAt`, rather than just set to Long.MinValue\r\n\r\n\r\n\r\n",
        "createdAt" : "2019-06-01T16:38:33Z",
        "updatedAt" : "2019-06-05T01:27:44Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "3dfc5ce4-ebf9-4195-b507-47cc02c80494",
        "parentId" : "f8271d74-339b-49ae-9519-1c92ee2ab5f7",
        "authorId" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "body" : "> with a cached list, EAM may performs unnecessary executors removing for many many times before nextTimeout change\r\n\r\nThat's actually the point. If the EAM did not kill the executors, it most probably means that there's an event that will be delivered to the listener and cause the next timeout to be updated (e.g. a task start).\r\n\r\nIf such event doesn't arrive, then the executors are still timed out, and will be always returned to the EAM. Then it may just decide not to kill them again (e.g. because the lower limit of executors has been reached).\r\n\r\nAll that is fine, and cheap computationally.\r\n\r\n> instead of only updating newNextTimeout for non timed out executors, we should update it for timed out executors, too\r\n\r\nThat means that if no executors are killed, you'll be re-scanning the whole executor list on every EAM interval until something changes, which is the exact thing the cache is trying to avoid.",
        "createdAt" : "2019-06-01T17:51:16Z",
        "updatedAt" : "2019-06-05T01:27:44Z",
        "lastEditedBy" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "tags" : [
        ]
      },
      {
        "id" : "05b87a49-8564-40d2-8525-2bd34c4f54bd",
        "parentId" : "f8271d74-339b-49ae-9519-1c92ee2ab5f7",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Make sense.",
        "createdAt" : "2019-06-02T03:29:14Z",
        "updatedAt" : "2019-06-05T01:27:44Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "9ae62d930aec531c5c19a718a76ea4ac01c2695d",
    "line" : 277,
    "diffHunk" : "@@ -1,1 +275,279 @@      // old one, ask the EAM thread to update the list of timed out executors.\n      if (newDeadline > oldDeadline && timedOut) {\n        nextTimeout.set(Long.MinValue)\n      } else {\n        updateNextTimeout(newDeadline)"
  },
  {
    "id" : "23393fc1-bafb-4fd3-8a04-52eb474d7978",
    "prId" : 24704,
    "prUrl" : "https://github.com/apache/spark/pull/24704#pullrequestreview-243875603",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2a066798-f934-4ad1-858f-37c4559aed8b",
        "parentId" : null,
        "authorId" : "e913f9af-103d-40d4-976e-9aa8e12e7211",
        "body" : "I think its worth a comment that this can only be called from the EAM scheduling thread.",
        "createdAt" : "2019-05-30T15:52:11Z",
        "updatedAt" : "2019-06-05T01:27:44Z",
        "lastEditedBy" : "e913f9af-103d-40d4-976e-9aa8e12e7211",
        "tags" : [
        ]
      }
    ],
    "commit" : "9ae62d930aec531c5c19a718a76ea4ac01c2695d",
    "line" : 77,
    "diffHunk" : "@@ -1,1 +75,79 @@   * Should only be called from the EAM thread.\n   */\n  def timedOutExecutors(): Seq[String] = {\n    val now = clock.getTimeMillis()\n    if (now >= nextTimeout.get()) {"
  },
  {
    "id" : "c72a7321-7d87-47ac-8f4b-9f5cfc06c983",
    "prId" : 24704,
    "prUrl" : "https://github.com/apache/spark/pull/24704#pullrequestreview-244334290",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6defe239-fe3b-4121-a65f-09a6e5251e8f",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Change is good here. Previously I thought it overlap with some behavior in Tracker. And, now, it is quite simple and more readable and easy to understand.",
        "createdAt" : "2019-05-31T14:19:23Z",
        "updatedAt" : "2019-06-05T01:27:44Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "9ae62d930aec531c5c19a718a76ea4ac01c2695d",
    "line" : 96,
    "diffHunk" : "@@ -1,1 +94,98 @@            false\n          } else {\n            exec.timedOut = true\n            true\n          }"
  },
  {
    "id" : "f9cce48f-6d08-4c56-8193-ead4f36a9673",
    "prId" : 24704,
    "prUrl" : "https://github.com/apache/spark/pull/24704#pullrequestreview-244423770",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9a534e17-fe05-41f0-b7da-c2fbf8aa83d8",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "```\r\nif (deadline > now) {\r\n newNextTimeout = math.min(newNextTimeout, deadline)\r\n exec.timedOut = false\r\n false\r\n} else {\r\n exec.timedOut = true\r\n true\r\n}\r\n```\r\nMaybe ?\r\n```\r\nif (deadline > now) {\r\n newNextTimeout = math.min(newNextTimeout, deadline)\r\n exec.timedOut = false\r\n} else {\r\n exec.timedOut = true\r\n}\r\nexec.timedOut\r\n```",
        "createdAt" : "2019-05-31T14:21:47Z",
        "updatedAt" : "2019-06-05T01:27:44Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "5091470e-b089-43a6-87ec-bd2f83fa3748",
        "parentId" : "9a534e17-fe05-41f0-b7da-c2fbf8aa83d8",
        "authorId" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "body" : "I was trying to avoid another read of a volatile variable.",
        "createdAt" : "2019-05-31T17:17:55Z",
        "updatedAt" : "2019-06-05T01:27:44Z",
        "lastEditedBy" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "tags" : [
        ]
      }
    ],
    "commit" : "9ae62d930aec531c5c19a718a76ea4ac01c2695d",
    "line" : 98,
    "diffHunk" : "@@ -1,1 +96,100 @@            exec.timedOut = true\n            true\n          }\n        }\n        .keys"
  },
  {
    "id" : "1b090598-904c-42be-b40a-059b0d7c5e0c",
    "prId" : 24704,
    "prUrl" : "https://github.com/apache/spark/pull/24704#pullrequestreview-244428092",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3194c7ea-a2f6-4109-8fd7-b48a2b515b8c",
        "parentId" : null,
        "authorId" : "ad79e8d4-cf06-458d-a4bf-c8cc54588fc6",
        "body" : "`idleStart` seems to be redundant. We could just use `runningTasks == 0`",
        "createdAt" : "2019-05-31T16:09:43Z",
        "updatedAt" : "2019-06-05T01:27:44Z",
        "lastEditedBy" : "ad79e8d4-cf06-458d-a4bf-c8cc54588fc6",
        "tags" : [
        ]
      },
      {
        "id" : "2198ba85-bd8b-46fb-a168-b17aae384f33",
        "parentId" : "3194c7ea-a2f6-4109-8fd7-b48a2b515b8c",
        "authorId" : "e913f9af-103d-40d4-976e-9aa8e12e7211",
        "body" : "I think you need it when updating the timeout, specifically when you drop cached blocks.  You want to track the original time the executor went idle.  (you could make the approximation to just restart the idle time when the blocks get dropped, but might as well do it right)",
        "createdAt" : "2019-05-31T16:24:24Z",
        "updatedAt" : "2019-06-05T01:27:44Z",
        "lastEditedBy" : "e913f9af-103d-40d4-976e-9aa8e12e7211",
        "tags" : [
        ]
      },
      {
        "id" : "b7036358-4bf4-4086-8fb0-73796411c63c",
        "parentId" : "3194c7ea-a2f6-4109-8fd7-b48a2b515b8c",
        "authorId" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "body" : "Yeah, it's not redundant. When checking for idleness, this check is the same as `runningTasks == 0`, but we need both pieces of information for the timeout calculation to be correct.",
        "createdAt" : "2019-05-31T17:26:56Z",
        "updatedAt" : "2019-06-05T01:27:44Z",
        "lastEditedBy" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "tags" : [
        ]
      }
    ],
    "commit" : "9ae62d930aec531c5c19a718a76ea4ac01c2695d",
    "line" : 266,
    "diffHunk" : "@@ -1,1 +264,268 @@    def updateTimeout(): Unit = {\n      val oldDeadline = timeoutAt\n      val newDeadline = if (idleStart >= 0) {\n        idleStart + (if (cachedBlocks.nonEmpty) storageTimeoutMs else idleTimeoutMs)\n      } else {"
  },
  {
    "id" : "fa0767ce-7220-4166-8916-8c9752853874",
    "prId" : 24704,
    "prUrl" : "https://github.com/apache/spark/pull/24704#pullrequestreview-244393247",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8f942acf-73dd-4959-a8d0-d4694a1f2dcb",
        "parentId" : null,
        "authorId" : "ad79e8d4-cf06-458d-a4bf-c8cc54588fc6",
        "body" : "Can replace it with `runningTasks == 0`",
        "createdAt" : "2019-05-31T16:10:25Z",
        "updatedAt" : "2019-06-05T01:27:44Z",
        "lastEditedBy" : "ad79e8d4-cf06-458d-a4bf-c8cc54588fc6",
        "tags" : [
        ]
      }
    ],
    "commit" : "9ae62d930aec531c5c19a718a76ea4ac01c2695d",
    "line" : 256,
    "diffHunk" : "@@ -1,1 +254,258 @@\n    // For testing.\n    def isIdle: Boolean = idleStart >= 0\n\n    def updateRunningTasks(delta: Int): Unit = {"
  },
  {
    "id" : "bcf96ccd-19a1-4666-819b-2869f841145f",
    "prId" : 24704,
    "prUrl" : "https://github.com/apache/spark/pull/24704#pullrequestreview-245126557",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dd68b405-2870-44dc-863e-51c193ddf66c",
        "parentId" : null,
        "authorId" : "e913f9af-103d-40d4-976e-9aa8e12e7211",
        "body" : "I think this should be volatile too -- its accessed by the EAM thread and the listenerbus thread",
        "createdAt" : "2019-06-03T14:46:56Z",
        "updatedAt" : "2019-06-05T01:27:44Z",
        "lastEditedBy" : "e913f9af-103d-40d4-976e-9aa8e12e7211",
        "tags" : [
        ]
      },
      {
        "id" : "3949ee2c-a02c-49da-a4c8-d1b2f22146e1",
        "parentId" : "dd68b405-2870-44dc-863e-51c193ddf66c",
        "authorId" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "body" : "I don't think it needs to. I tried very hard to avoid volatile when not necessary; in this code, it's generally only needed when ordering is important when checking multiple variables. That is not the case for this one, since it doesn't really depend on any other variables.\r\n\r\nAt first I also didn't have `timedOut` and `timeoutAt` as volatiles, but I couldn't 100% convince me that it was not needed. But in this case I'm pretty sure it doesn't do anything useful.",
        "createdAt" : "2019-06-03T17:02:00Z",
        "updatedAt" : "2019-06-05T01:27:44Z",
        "lastEditedBy" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "tags" : [
        ]
      },
      {
        "id" : "21c2461f-115b-4824-8b99-4173ea850d66",
        "parentId" : "dd68b405-2870-44dc-863e-51c193ddf66c",
        "authorId" : "e913f9af-103d-40d4-976e-9aa8e12e7211",
        "body" : "ok, I guess the one access which is in the listener thread is in `onExecutorRemoved()`.  If that sees the stale value, then it will incorrectly set the `nextTimeout`, telling the monitor to recompute the list.  But while that is inefficient, its never incorrect.  And the hope is, its rare enough that its still better to avoid making it volatile.\r\n\r\nis that right?\r\n\r\nI'd say it at least deserves a comment in onExecutorRemoved.",
        "createdAt" : "2019-06-03T20:58:32Z",
        "updatedAt" : "2019-06-05T01:27:44Z",
        "lastEditedBy" : "e913f9af-103d-40d4-976e-9aa8e12e7211",
        "tags" : [
        ]
      },
      {
        "id" : "2c329d45-6eb2-400b-9608-0acac3a295cf",
        "parentId" : "dd68b405-2870-44dc-863e-51c193ddf66c",
        "authorId" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "body" : ">  If that sees the stale value, then it will incorrectly set the nextTimeout\r\n\r\nRight, but notice that isn't really related to the variable being volatile or not, it's just a race that can also occur if the variable is volatile.\r\n\r\nWriting to that variable is an atomic operation. Volatile controls what happens to other reads / writes around that volatile write. That's why it's needed when the order of the reads and writes is important. But here it isn't.",
        "createdAt" : "2019-06-03T21:09:08Z",
        "updatedAt" : "2019-06-05T01:27:44Z",
        "lastEditedBy" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "tags" : [
        ]
      }
    ],
    "commit" : "9ae62d930aec531c5c19a718a76ea4ac01c2695d",
    "line" : 246,
    "diffHunk" : "@@ -1,1 +244,248 @@    @volatile var timedOut: Boolean = false\n\n    var pendingRemoval: Boolean = false\n\n    private var idleStart: Long = -1"
  }
]