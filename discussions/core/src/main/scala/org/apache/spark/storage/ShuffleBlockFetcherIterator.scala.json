[
  {
    "id" : "f7fbcdd8-720c-4b2a-9d43-d073c3098cbc",
    "prId" : 33451,
    "prUrl" : "https://github.com/apache/spark/pull/33451#pullrequestreview-716204558",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d55c9938-e3d5-438e-b2e1-c62ce8a2dac1",
        "parentId" : null,
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "To clarify, with this change, for all fetches after the first failure, we will diagnose (except if cause == disk) ?\r\nIf yes, the change proposed in `diagnoseCorruption` will be more useful.",
        "createdAt" : "2021-07-26T19:35:30Z",
        "updatedAt" : "2021-07-26T19:40:38Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "c798dc9c-7359-47eb-84a9-2fd4e9712982",
        "parentId" : "d55c9938-e3d5-438e-b2e1-c62ce8a2dac1",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "> To clarify, with this change, for all fetches after the first failure, we will diagnose (except if cause == disk) ?\r\n\r\nNo. We will only diagnose if a block is corrupted at second time (those blocks which can be found in corruptedBlocks). But there's an exception for the corruption from `BufferReleasingInputStream`. In the case of `BufferReleasingInputStream`, the data stream has been partially consumed by the downstream RDDs. So, we don't have a chance to retry. In this case, we'd diagnose anyway.\r\n\r\n\r\n(Previously, we'd diagnose when the block corrupted at first time. And we decide whether retry the block depends on the diagnosis result. But this way has a problem that it blocks the fetcher's thread so may introduce regression.\r\nNow, for the block corrupted at first time, we still always retry it (this remains the same behavior as now). If it corrupts again, then we'd diagnose it and throw fetch failure with the cause (if any).)",
        "createdAt" : "2021-07-27T13:28:55Z",
        "updatedAt" : "2021-07-27T14:11:06Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "d54aa003-2f32-4a71-aa41-181504e44525",
        "parentId" : "d55c9938-e3d5-438e-b2e1-c62ce8a2dac1",
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "I phrased it poorly, thanks for clarifying and the added context around `BufferReleasingInputStream` - that is what I was trying to get to: \"To clarify, with this change, for all fetches after the first failure, we will diagnose and retry only if cause != disk\"",
        "createdAt" : "2021-07-27T17:43:44Z",
        "updatedAt" : "2021-07-27T17:44:26Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      }
    ],
    "commit" : "ca1b058b6ccac9178859c56e2f7dd05f4ff68900",
    "line" : 73,
    "diffHunk" : "@@ -1,1 +845,849 @@                if (!(checksumEnabled && corruptedBlocks.contains(blockId))) {\n                  buf.release()\n                }\n\n                if (blockId.isShuffleChunk) {"
  },
  {
    "id" : "05006036-faf2-4533-988c-950faa09615c",
    "prId" : 33451,
    "prUrl" : "https://github.com/apache/spark/pull/33451#pullrequestreview-717092285",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d3c177ad-beb4-4bec-8945-bce792471c53",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "nit:\r\n```\r\nval bufIn = ...\r\nif (checksumEnabled) {\r\n  ...\r\n  checkedIn = ...\r\n  checkedIn\r\n} else {\r\n  bufIn\r\n}\r\n```",
        "createdAt" : "2021-07-28T14:39:19Z",
        "updatedAt" : "2021-07-28T14:39:20Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "ca1b058b6ccac9178859c56e2f7dd05f4ff68900",
    "line" : 59,
    "diffHunk" : "@@ -1,1 +805,809 @@            } else {\n              bufIn\n            }\n          } catch {\n            // The exception could only be throwed by local shuffle block"
  },
  {
    "id" : "5fc6160a-9c3c-4a65-9515-ac59d6d6ec3e",
    "prId" : 33451,
    "prUrl" : "https://github.com/apache/spark/pull/33451#pullrequestreview-717618233",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f698cd27-60ef-493d-aca8-ff1ec7b19470",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "why do we use ms in https://github.com/apache/spark/pull/33451/files#diff-8ada3fbd5ebaae4bc80daf73fab086971d1e8496f7e882e7510b1b15b389ff50R129 but ns here?",
        "createdAt" : "2021-07-28T14:46:46Z",
        "updatedAt" : "2021-07-28T14:46:46Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "898e5387-8819-495a-9b52-2a6b6a400679",
        "parentId" : "f698cd27-60ef-493d-aca8-ff1ec7b19470",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Switched to ns there.",
        "createdAt" : "2021-07-29T02:38:20Z",
        "updatedAt" : "2021-07-29T02:38:20Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "ca1b058b6ccac9178859c56e2f7dd05f4ff68900",
    "line" : 148,
    "diffHunk" : "@@ -1,1 +1036,1040 @@      blockId: BlockId): String = {\n    logInfo(\"Start corruption diagnosis.\")\n    val startTimeNs = System.nanoTime()\n    assert(blockId.isInstanceOf[ShuffleBlockId], s\"Expected ShuffleBlockId, but got $blockId\")\n    val shuffleBlock = blockId.asInstanceOf[ShuffleBlockId]"
  },
  {
    "id" : "6151b4ea-e7fa-4ccd-8ba3-a96f52a7a762",
    "prId" : 33451,
    "prUrl" : "https://github.com/apache/spark/pull/33451#pullrequestreview-717775726",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1c9a8331-8d10-434c-baef-0a98fbaad32d",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I'm confused. If we only need millisecond precision, why do we calculate with nanosecond and convert to millisecond at the end? can't we use `System.currentTimeMillis`?",
        "createdAt" : "2021-07-29T07:19:30Z",
        "updatedAt" : "2021-07-29T07:19:30Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "b56ffc32-841a-45cc-b738-218bd152b92d",
        "parentId" : "1c9a8331-8d10-434c-baef-0a98fbaad32d",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "According to https://github.com/apache/spark/pull/23727, it's recommended to use `System.nanoTime()`. And Spark usually exposes the time as milliseconds so the conversion is used.",
        "createdAt" : "2021-07-29T07:48:28Z",
        "updatedAt" : "2021-07-29T07:48:28Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "ca1b058b6ccac9178859c56e2f7dd05f4ff68900",
    "line" : 165,
    "diffHunk" : "@@ -1,1 +1053,1057 @@        cause = Cause.UNKNOWN_ISSUE\n    }\n    val duration = TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - startTimeNs)\n    val diagnosisResponse = cause match {\n      case Cause.UNSUPPORTED_CHECKSUM_ALGORITHM =>"
  },
  {
    "id" : "3bc39e41-e165-41c5-b6c3-b809c457d247",
    "prId" : 33109,
    "prUrl" : "https://github.com/apache/spark/pull/33109#pullrequestreview-693533439",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "26ad6a60-e28c-422a-a9ec-81d42455f560",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "nit format:\r\n```\r\n    }\r\n\r\n    val remoteBlockBytes = collectedRemoteRequests.map(_.size).sum\r\n    val numRemoteBlocks = collectedRemoteRequests.map(_.blocks.size).sum\r\n    val totalBytes = localBlockBytes + remoteBlockBytes + hostLocalBlockBytes\r\n```\r\n?",
        "createdAt" : "2021-06-28T03:50:37Z",
        "updatedAt" : "2021-06-28T03:50:38Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "359195f53fe1ef4184c687cbb25b69235db1ebb6",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +387,391 @@    val (remoteBlockBytes, numRemoteBlocks) =\n      collectedRemoteRequests.foldLeft((0L, 0))((x, y) => (x._1 + y.size, x._2 + y.blocks.size))\n    val totalBytes = localBlockBytes + remoteBlockBytes + hostLocalBlockBytes\n    assert(numBlocksToFetch == localBlocks.size + hostLocalBlocks.size + numRemoteBlocks,\n      s\"The number of non-empty blocks $numBlocksToFetch doesn't equal to the number of local \" +"
  },
  {
    "id" : "f084fcab-1df2-4b7d-84c3-13ee22742d24",
    "prId" : 32287,
    "prUrl" : "https://github.com/apache/spark/pull/32287#pullrequestreview-653632119",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "46691b38-ba46-47c0-90a5-bc30f9c7f422",
        "parentId" : null,
        "authorId" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "body" : "Was thinking more about this solution and there is a potential problem I see. Once a netty OOM is encountered for some responses, the corresponding requests are deferred and no more remote requests are sent. Now this helps to recover. But we don't change any in-flight remote requests limits. So after the  `isNettyOOMOnShuffle` is reset with a successful remote response, the next burst of remote requests will be sent out at the same rate. This means again there are chances to see netty OOMs and again some of the blocks will be deferred. This introduces more delay and increases the load on shuffle server.\r\n\r\nI think solving this maybe more complex and right now this is just a workaround. But maybe we can do something simpler to reduce the number of requests made after a netty OOM is encountered?\r\n",
        "createdAt" : "2021-04-29T21:48:34Z",
        "updatedAt" : "2021-05-10T10:25:15Z",
        "lastEditedBy" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "tags" : [
        ]
      },
      {
        "id" : "a26a807d-9509-4065-8bf6-96327fc34143",
        "parentId" : "46691b38-ba46-47c0-90a5-bc30f9c7f422",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Yes, that's true. I also considered another way previously, which is to adjust the threshold of in-flight requests dynamically. For example, when OOM throws, the threshold would be reduced to the max number of in-flight requests before OOM. And if OOM happens again, we continue to reduce the threshold. Then, it comes to a question: when do we increase the threshold? When the backlogged requests are too much and OOM has been disappeared for a while? If we go this way, we should be very careful about the adjustment algorithm as it's directly related to the performance.\r\n\r\nLet me think more about it. Thanks!",
        "createdAt" : "2021-04-30T01:55:36Z",
        "updatedAt" : "2021-05-10T10:25:15Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "41fe47a2-ab9d-4e0a-9719-b4ed62754c35",
        "parentId" : "46691b38-ba46-47c0-90a5-bc30f9c7f422",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "So I have limited the reset condition to whether the Netty free memory is larger than `maxReqSizeShuffleToMem` (default 200M), which is more strict than the `averageBlockSize`. I think this would mitigate the issue you mentioned here. WDYT?",
        "createdAt" : "2021-04-30T07:53:30Z",
        "updatedAt" : "2021-05-10T10:25:15Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "20403b26-8ca9-4175-99ed-a759e138bd1a",
        "parentId" : "46691b38-ba46-47c0-90a5-bc30f9c7f422",
        "authorId" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "body" : "Should it again check at line 805 that `freeDirectMemory` > maxReqSizeShuffleToMem?\r\nOtherwise it immediately unsets the flag and sends more requests.",
        "createdAt" : "2021-05-03T06:49:59Z",
        "updatedAt" : "2021-05-10T10:25:15Z",
        "lastEditedBy" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "tags" : [
        ]
      },
      {
        "id" : "df73ce07-2c51-440f-8662-b5ac5b9b5c86",
        "parentId" : "46691b38-ba46-47c0-90a5-bc30f9c7f422",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "> Should it again check at line 805 that freeDirectMemory > maxReqSizeShuffleToMem?\r\n\r\nNo. Otherwise, for the first invocation of `fetchUpToMaxBytes`, the fetching can hang if `isNettyOOMOnShuffle=true.` Because if no requests were sent in the first invocation, there would be no callback on `fetchUpToMaxBytes` later.\r\n",
        "createdAt" : "2021-05-03T15:21:54Z",
        "updatedAt" : "2021-05-10T10:25:15Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "6ecc5106-a648-418a-98df-0d01357b5d85",
        "parentId" : "46691b38-ba46-47c0-90a5-bc30f9c7f422",
        "authorId" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "body" : "> Because if no requests were sent in the first invocation, there would be no callback on fetchUpToMaxBytes later.\r\n\r\nFor the first invocation `fetchUpToMaxBytes` if  `isNettyOOMOnShuffle=true` then that means another iterator saw the NettyOOM as set it to true. This will just unset it.\r\n\r\nAlso how will it hang? let's just say if they are only remote blocks to be fetched and none of the requests are sent initially, `iterator.next()` will keep calling `fetchUpToMaxBytes` as I see it. Eventually when enough freeDirectMemory is available, it will send remote requests.",
        "createdAt" : "2021-05-03T17:49:22Z",
        "updatedAt" : "2021-05-10T10:25:15Z",
        "lastEditedBy" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "tags" : [
        ]
      },
      {
        "id" : "65a53900-8dd8-4836-b68b-451c7b18a798",
        "parentId" : "46691b38-ba46-47c0-90a5-bc30f9c7f422",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "IIUC, it will block at https://github.com/apache/spark/blob/2cb962b132af2683a1a445365636e572c557ac5b/core/src/main/scala/org/apache/spark/storage/ShuffleBlockFetcherIterator.scala#L594",
        "createdAt" : "2021-05-04T13:18:51Z",
        "updatedAt" : "2021-05-10T10:25:15Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "28f3551f-df31-40bb-bfda-f3a9bfa357c9",
        "parentId" : "46691b38-ba46-47c0-90a5-bc30f9c7f422",
        "authorId" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "body" : "Oh I see. It just waits indefinitely here. Can we not change this to `poll(time)` ? If there is nothing,  result will be null and it will call `fetchUpToMaxBytes` again.\r\nWDYT?",
        "createdAt" : "2021-05-04T16:24:32Z",
        "updatedAt" : "2021-05-10T10:25:15Z",
        "lastEditedBy" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "tags" : [
        ]
      },
      {
        "id" : "37f47698-81c7-4cba-b0a4-747aa05d53d6",
        "parentId" : "46691b38-ba46-47c0-90a5-bc30f9c7f422",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "`poll(time)` may work but I think it breaks the existing design. After the change, we would introduce the overhead due to call `fetchUpToMaxBytes` many times while it's not necessary. \r\n",
        "createdAt" : "2021-05-06T16:04:14Z",
        "updatedAt" : "2021-05-10T10:25:15Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "4af6ee73221a55e3cd7239f151ec33d5e802cbe4",
    "line" : 194,
    "diffHunk" : "@@ -1,1 +800,804 @@      }\n    }\n\n    // Send fetch requests up to maxBytesInFlight. If you cannot fetch from a remote host\n    // immediately, defer the request until the next time it can be processed."
  },
  {
    "id" : "13845e79-7cc0-4c4b-b77a-2dca1687e914",
    "prId" : 32287,
    "prUrl" : "https://github.com/apache/spark/pull/32287#pullrequestreview-653556695",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "74b80e45-2a85-4d51-bdf9-7ab3df1efd0c",
        "parentId" : null,
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "I will let @otterc elaborate more; we had discussed whether we can minimize direct memory load in `fetchUpToMaxBytes` due to all 'expensive' deferred blocks being sent out (mostly) together (modulo constraints on `maxBlocksInFlightPerAddress`/`maxBytesInFlight`).\r\n",
        "createdAt" : "2021-05-03T07:50:33Z",
        "updatedAt" : "2021-05-10T10:25:15Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "daa5c801-c03a-4490-afee-c2d60200e4f3",
        "parentId" : "74b80e45-2a85-4d51-bdf9-7ab3df1efd0c",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "I'm not sure about the \"direct memory load\" you mean here. I'll wait for @otterc's explanation.",
        "createdAt" : "2021-05-03T15:23:57Z",
        "updatedAt" : "2021-05-10T10:25:15Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "02a45c7f-05a3-49e6-a72f-51e1103216ef",
        "parentId" : "74b80e45-2a85-4d51-bdf9-7ab3df1efd0c",
        "authorId" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "body" : "This is related to the conversation here:\r\nhttps://github.com/apache/spark/pull/32287#discussion_r623425119\r\nWe were discussing simple ways to reduce the number of remote fetch requests after the OOM. One such thing could be that after the OOM, we just sent the out the requests that were deferred due to the OOM and not send any additional requests. \r\nI am not sure how effective is this going to be though. Since the in-flight limit remains the same, the next call to `fetchUpToMaxBytes` when it sends out non-deferred requests can cause new blocks to OOM.\r\n\r\nAnother simple way could be to modify `isRemoteBlockFetchable` such that after this iterator has seen an OOM, it will also check `bytesInFlight + fetchReqQueue.front.size` < freeDirectorMemory?",
        "createdAt" : "2021-05-03T18:42:14Z",
        "updatedAt" : "2021-05-10T10:25:15Z",
        "lastEditedBy" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "tags" : [
        ]
      },
      {
        "id" : "620b7ed6-0657-4683-8817-0a893115c680",
        "parentId" : "74b80e45-2a85-4d51-bdf9-7ab3df1efd0c",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "So after #32287 (comment), I have changed the unset condition to `freeDirectorMemory > maxReqSizeShuffleToMem (200M by default)`, which I think is already very strict. So, it should avoid the issue you mentioned in  #32287 (comment).\r\n\r\n> Another simple way could be to modify isRemoteBlockFetchable such that after this iterator has seen an OOM, it will also check bytesInFlight + fetchReqQueue.front.size < freeDirectorMemory?\r\n\r\nDo you mean check bytesInFlight + fetchReqQueue.front.size < freeDirectorMemory for all the cases or only when `isNettyOOMOnShuffle=true`?  If you also want to check when `isNettyOOMOnShuffle=false`, I'd like to mention that block size is not equal to the consumed memory size of Netty. As you know,  blocks that bigger than  `maxReqSizeShuffleToMem` would be stored on disk.\r\n",
        "createdAt" : "2021-05-04T13:29:17Z",
        "updatedAt" : "2021-05-10T10:25:15Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "67f82414-ae7d-40d4-984d-46ef3a43b352",
        "parentId" : "74b80e45-2a85-4d51-bdf9-7ab3df1efd0c",
        "authorId" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "body" : "> I have changed the unset condition to freeDirectorMemory > maxReqSizeShuffleToMem (200M by default), which I think is already very strict. So, it should avoid the issue you mentioned in #32287 (comment).\r\n\r\nI don't think it will avoid the issue. This adds more time when the next set of requests are going to be set. However, the next set of requests (including the deferred ones) will still be sent at same frequency, so some of them would again see OOMs. \r\n\r\n> Do you mean check bytesInFlight + fetchReqQueue.front.size < freeDirectorMemory for all the cases or only when isNettyOOMOnShuffle=true? \r\n\r\nI meant that if once this OOM is encountered, after that the iterator checks against freeDirectorMemory as well. If a request.size > `maxReqSizeShuffleToMem` then we can skip the check on it as that is stored on disk.\r\n",
        "createdAt" : "2021-05-04T16:16:44Z",
        "updatedAt" : "2021-05-10T10:25:15Z",
        "lastEditedBy" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "tags" : [
        ]
      },
      {
        "id" : "8253b603-97ea-4e5d-8c40-650c99c50ef3",
        "parentId" : "74b80e45-2a85-4d51-bdf9-7ab3df1efd0c",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "I think there are two different cases here. For example, assuming there're 100 requests.\r\n\r\nFor case a), the OOM threshold might be 80 requests. In this case, after OOMed, the deferred 20 requests shouldn't hit the OOM again. \r\n\r\nFor case b), the OOM threshold might be 20 requests. In this case, there're still 80 deferred requests, which would hit the OOM soon as you mentioned. That being said, I think the current fix would work around the issue in the end. Note that the application would fail before the fix.\r\n\r\nTo improve the current fix further, I think we can do it in a separate PR as I think it's not an easy thing[1] to do (or do you have any other ideas?) and must require more discussion. WDTY?\r\n\r\n1. Even if we skip the case of request > maxReqSizeShuffleToMem, note that the in-memory request size is not strictly equal to the consumed memory size in Netty due to Netty's memory management mechanism (https://www.facebook.com/notes/facebook-engineering/scalable-memory-allocation-using-jemalloc/480222803919). For example, Netty may allocate 16MB for a 9MB block.\r\nAnd there could be multiple tasks fetching concurrently. So we may need to track the total `bytesInFlight` of all tasks rather than the single task itself. And it would require more synchronization among tasks and make the thing more complex.\r\n\r\n\r\n\r\n",
        "createdAt" : "2021-05-06T09:40:09Z",
        "updatedAt" : "2021-05-10T10:25:15Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "451afa08-16c1-4795-b513-88501a75577e",
        "parentId" : "74b80e45-2a85-4d51-bdf9-7ab3df1efd0c",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "It sounds like a good idea to dynamically tune the request frequency, but this doesn't seem like trivial work. I'd vote for doing it in a follow-up with more discussions.",
        "createdAt" : "2021-05-06T13:37:19Z",
        "updatedAt" : "2021-05-10T10:25:15Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "2c9dc99f-6dc1-4e0b-8da7-b6c0b12245eb",
        "parentId" : "74b80e45-2a85-4d51-bdf9-7ab3df1efd0c",
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "If nontrivial, we can definitely push it to a later effort - can you file a jira for it @Ngone51 so that we can track it (and perhaps someone can pick it up later ?).\r\nThis PR is already a marked improvement over what we currently have w.r.t OOM at executor - as long as ESS load does not go up :-)",
        "createdAt" : "2021-05-06T14:25:06Z",
        "updatedAt" : "2021-05-10T10:25:15Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "b2a6c8b2-f118-4903-984f-ef9074614b95",
        "parentId" : "74b80e45-2a85-4d51-bdf9-7ab3df1efd0c",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Filed the JIRA ticket: https://issues.apache.org/jira/browse/SPARK-35330",
        "createdAt" : "2021-05-06T15:22:17Z",
        "updatedAt" : "2021-05-10T10:25:15Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "4af6ee73221a55e3cd7239f151ec33d5e802cbe4",
    "line" : 72,
    "diffHunk" : "@@ -1,1 +270,274 @@        deferredBlocks.clear()\n      }\n    }\n\n    val blockFetchingListener = new BlockFetchingListener {"
  },
  {
    "id" : "09a87b3c-85de-41ee-b893-15c6dfc02a35",
    "prId" : 32287,
    "prUrl" : "https://github.com/apache/spark/pull/32287#pullrequestreview-655486341",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dd58166e-fed6-4eb7-b4de-e9cc8a0e5cbd",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Shall we add a debug message with `else` statement because this code path is changed after this PR? Previously, we do `results.put` always.",
        "createdAt" : "2021-05-08T22:42:23Z",
        "updatedAt" : "2021-05-10T10:25:15Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "2ff84d55-6156-4fb3-bd81-37e5abbcc35e",
        "parentId" : "dd58166e-fed6-4eb7-b4de-e9cc8a0e5cbd",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "I'm not sure which case the `else` statement you want to match here. Do you want:\r\n\r\n```scala\r\nif (!isZombie) {\r\n\r\n} else {\r\n ...\r\n}\r\n```\r\n\r\n? \r\n\r\n(Note that in `onBlockFetchSuccess` we also don't have such `else` statement for zombie case.)\r\n\r\nOr you actually want the else case of not putting results? If so, I think we already have the log info as\r\n```scala\r\nlogInfo(s\"Block $blockId has failed $failureTimes times \" +\r\n                    s\"due to Netty OOM, will retry\")\r\n```",
        "createdAt" : "2021-05-10T10:24:33Z",
        "updatedAt" : "2021-05-10T10:25:15Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "4af6ee73221a55e3cd7239f151ec33d5e802cbe4",
    "line" : 128,
    "diffHunk" : "@@ -1,1 +327,331 @@                deferredBlocks += blockId\n                enqueueDeferredFetchRequestIfNecessary()\n              }\n\n            case _ =>"
  },
  {
    "id" : "765d3e37-1a99-4bc2-b7a7-28a8b1219ad9",
    "prId" : 32287,
    "prUrl" : "https://github.com/apache/spark/pull/32287#pullrequestreview-655028514",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d715c120-b118-4343-ab2a-e9b911ddc63c",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "~`final` is required to use `@inline` properly.~",
        "createdAt" : "2021-05-08T23:24:56Z",
        "updatedAt" : "2021-05-10T10:25:15Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "bc2e38df-f123-4da4-83ba-cbfb54312735",
        "parentId" : "d715c120-b118-4343-ab2a-e9b911ddc63c",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Oh, got it. It's not a member function. So. you didn't use it.",
        "createdAt" : "2021-05-08T23:35:43Z",
        "updatedAt" : "2021-05-10T10:25:15Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "4af6ee73221a55e3cd7239f151ec33d5e802cbe4",
    "line" : 63,
    "diffHunk" : "@@ -1,1 +261,265 @@    val address = req.address\n\n    @inline def enqueueDeferredFetchRequestIfNecessary(): Unit = {\n      if (remainingBlocks.isEmpty && deferredBlocks.nonEmpty) {\n        val blocks = deferredBlocks.map { blockId =>"
  },
  {
    "id" : "ec15c2c7-f5d6-4bbf-a34a-6751d5627305",
    "prId" : 32140,
    "prUrl" : "https://github.com/apache/spark/pull/32140#pullrequestreview-669648092",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ea0bb9c8-0672-4f7d-926b-21c6893d1e18",
        "parentId" : null,
        "authorId" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "body" : "Note to self: Most of this is as before. Have added conditions for shuffleChunks",
        "createdAt" : "2021-05-27T00:46:09Z",
        "updatedAt" : "2021-06-03T04:54:51Z",
        "lastEditedBy" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad89a0208a5e3f880fca502c297362388a104dd7",
    "line" : 387,
    "diffHunk" : "@@ -1,1 +822,826 @@              }\n            } catch {\n              case e: IOException =>\n                buf.release()\n                if (blockId.isShuffleChunk) {"
  },
  {
    "id" : "f3ea0c43-8847-4e9a-b097-86d7dfb0efd1",
    "prId" : 32140,
    "prUrl" : "https://github.com/apache/spark/pull/32140#pullrequestreview-676358897",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fdc6bbe6-1884-4161-a79f-7e3719496a3e",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "How about:\r\n\r\n```scala\r\nlogDebug(\"Sending request for %d blocks (%s) from %s\".format(\r\n      request.blocks.size, Utils.bytesToString(request.size), request.address.hostPort))\r\nif (hasMergedBlocks) {\r\n pushBasedFetchHelper.sendFetchMergedStatusRequest(request)\r\n} else {\r\n sendRequest(request)\r\n numBlocksInFlightPerAddress(remoteAddress) = \r\n   numBlocksInFlightPerAddress.getOrElse(remoteAddress, 0) + request.blocks.size\r\n}\r\n```",
        "createdAt" : "2021-06-09T14:13:27Z",
        "updatedAt" : "2021-06-09T15:22:50Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad89a0208a5e3f880fca502c297362388a104dd7",
    "line" : 518,
    "diffHunk" : "@@ -1,1 +1023,1027 @@    }\n\n    def send(remoteAddress: BlockManagerId, request: FetchRequest): Unit = {\n      if (request.forMergedMetas) {\n        pushBasedFetchHelper.sendFetchMergedStatusRequest(request)"
  },
  {
    "id" : "5a34e403-839c-439f-809a-8658a998e3a5",
    "prId" : 32140,
    "prUrl" : "https://github.com/apache/spark/pull/32140#pullrequestreview-680352296",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7ac7d25a-548b-4207-ae94-1488dbedb195",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Hm..is it possible there's only `FetchRequest(hasMergedBlocks)` at the beginning? In that case, it seems to cause the fetching process to hang.\r\n\r\nWe probably need to call `fetchUpToMaxBytes()` here if `reqsInFlight=0`.",
        "createdAt" : "2021-06-09T15:02:59Z",
        "updatedAt" : "2021-06-09T15:22:50Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "c8c8e373-7caa-4539-a726-00f80c30ac49",
        "parentId" : "7ac7d25a-548b-4207-ae94-1488dbedb195",
        "authorId" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "body" : "> Hm..is it possible there's only FetchRequest(hasMergedBlocks) at the beginning? In that case, it seems to cause the fetching process to hang.\r\n\r\nIt will not cause the fetch process to hang if there is just one FetchRequest with merged blocks.\r\nConsider this example that if there is a FetchRequest for a merged block `ShuffleBlock(0, -1, 0)`,\r\n- the iterator will send out the request to fetch the metadata for this block in `PushBasedFetchHelper.sendFetchMergedStatusRequest`. \r\n- The iterator will wait for a response in the result queue at `results.take()`.\r\n- Once it receives a response, which is either `MergedBlocksMetaFetchResult` or `MergedBlocksMetaFailedFetchResult`, it adds more FetchRequests to the fetch queue and sets `result = null`.\r\n- `fetchUpToMaxBytes()` is always called after processing the response.\r\n- Since `result = null`, while loop repeats and waits again for a response in the result queue.  \r\n\r\nI will also add a UT for this case just to verify this.",
        "createdAt" : "2021-06-09T17:26:32Z",
        "updatedAt" : "2021-06-09T21:09:00Z",
        "lastEditedBy" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "tags" : [
        ]
      },
      {
        "id" : "1ae81302-f137-4537-bd9a-3d890e215feb",
        "parentId" : "7ac7d25a-548b-4207-ae94-1488dbedb195",
        "authorId" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "body" : "I have added a UT as well to verify this  `iterator has just 1 merged block and fails to fetch the meta`. PTAL",
        "createdAt" : "2021-06-09T21:02:42Z",
        "updatedAt" : "2021-06-09T21:02:42Z",
        "lastEditedBy" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "tags" : [
        ]
      },
      {
        "id" : "565c3616-d9f5-46b6-83c8-544ea537763a",
        "parentId" : "7ac7d25a-548b-4207-ae94-1488dbedb195",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "> * fetchUpToMaxBytes() is always called after processing the response.\r\n\r\nI doubt this point. But I'd check the test first since I could miss something.",
        "createdAt" : "2021-06-10T03:20:06Z",
        "updatedAt" : "2021-06-10T03:20:07Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "86f51d7c-55ab-468d-af6b-d85eaa2a6ec5",
        "parentId" : "7ac7d25a-548b-4207-ae94-1488dbedb195",
        "authorId" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "body" : "Actually, this is the existing code which I haven't modified. The while loop inside iterator.next() is as below, so `fetchUpToMaxBytes` is always called after a response is matched and processed.\r\n```\r\n    while (result == null) {\r\n      val startFetchWait = System.nanoTime()\r\n      result = results.take()\r\n      val fetchWaitTime = TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - startFetchWait)\r\n      shuffleMetrics.incFetchWaitTime(fetchWaitTime)\r\n\r\n      result match {...}\r\n \r\n      // Send fetch requests up to maxBytesInFlight\r\n      fetchUpToMaxBytes()\r\n    }\r\n    ```\r\n ",
        "createdAt" : "2021-06-10T04:50:49Z",
        "updatedAt" : "2021-06-10T04:50:50Z",
        "lastEditedBy" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "tags" : [
        ]
      },
      {
        "id" : "bbcd2708-e0dc-44b4-a931-c5740a22773a",
        "parentId" : "7ac7d25a-548b-4207-ae94-1488dbedb195",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Oh, I see. I misread it.",
        "createdAt" : "2021-06-10T05:20:44Z",
        "updatedAt" : "2021-06-10T05:20:45Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad89a0208a5e3f880fca502c297362388a104dd7",
    "line" : 501,
    "diffHunk" : "@@ -1,1 +946,950 @@          fetchRequests ++= additionalRemoteReqs\n          // Set result to null to force another iteration.\n          result = null\n\n        case PushMergedRemoteMetaFailedFetchResult(shuffleId, reduceId, address) =>"
  },
  {
    "id" : "9cafca31-607d-46b5-825d-ea6f30872c65",
    "prId" : 32140,
    "prUrl" : "https://github.com/apache/spark/pull/32140#pullrequestreview-679074787",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "94eb7c18-01bd-480e-ada0-62a0a049ebb0",
        "parentId" : null,
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "Note: Here we are assuming `localBlocks` is empty when method was invoked.",
        "createdAt" : "2021-06-10T16:10:27Z",
        "updatedAt" : "2021-06-10T17:58:43Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad89a0208a5e3f880fca502c297362388a104dd7",
    "line" : 155,
    "diffHunk" : "@@ -1,1 +419,423 @@      pushMergedLocalBlockBytes\n    val blocksToFetchCurrentIteration = numBlocksToFetch - prevNumBlocksToFetch\n    assert(blocksToFetchCurrentIteration == localBlocks.size +\n      numHostLocalBlocks + numRemoteBlocks + pushMergedLocalBlocks.size,\n        s\"The number of non-empty blocks $blocksToFetchCurrentIteration doesn't equal to the sum \" +"
  },
  {
    "id" : "bd674e46-d5d2-447d-a2b9-971c451611f1",
    "prId" : 32140,
    "prUrl" : "https://github.com/apache/spark/pull/32140#pullrequestreview-681231368",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "afbd0a6d-51e0-4f51-9912-d6ae134a282c",
        "parentId" : null,
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "Is `mergeContinuousShuffleBlockIdsIfNeeded` relevant for merged blocks/chunks ?\r\nIf not, any side effects of doing this ?",
        "createdAt" : "2021-06-10T16:15:47Z",
        "updatedAt" : "2021-06-10T17:58:43Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "b91cc71a-d529-4552-8eb0-5f9c9fba12fc",
        "parentId" : "afbd0a6d-51e0-4f51-9912-d6ae134a282c",
        "authorId" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "body" : "> Is mergeContinuousShuffleBlockIdsIfNeeded relevant for merged blocks/chunks ?\r\n\r\nNo, it is not relevant for merged blocks/chunks. For both merged blocks/chunks, I am passing `enabledBatchFetch = false` so `mergeContinuousShuffleBlockIdsIfNeeded` returns the passed in blocks.\r\n\r\nI am not seeing any side effects of reusing this method for merged blocks/chunks. IIUC, this method enforces the limit of `maxBlocksInFlightPerAddress` for a  FetchRequest and is the one that modifies `numBlocksToFetch` for remote requests.",
        "createdAt" : "2021-06-10T20:29:08Z",
        "updatedAt" : "2021-06-10T20:29:08Z",
        "lastEditedBy" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad89a0208a5e3f880fca502c297362388a104dd7",
    "line" : 195,
    "diffHunk" : "@@ -1,1 +455,459 @@      enableBatchFetch: Boolean,\n      forMergedMetas: Boolean = false): ArrayBuffer[FetchBlockInfo] = {\n    val mergedBlocks = mergeContinuousShuffleBlockIdsIfNeeded(curBlocks, enableBatchFetch)\n    numBlocksToFetch += mergedBlocks.size\n    val retBlocks = new ArrayBuffer[FetchBlockInfo]"
  },
  {
    "id" : "4572c8ec-d988-44df-9035-11d6e6103a5b",
    "prId" : 32140,
    "prUrl" : "https://github.com/apache/spark/pull/32140#pullrequestreview-691290168",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d5fd45c5-b419-4537-b3f5-fe25082d318f",
        "parentId" : null,
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "For a small enough chunk, cant this not throw exception ?",
        "createdAt" : "2021-06-23T01:17:38Z",
        "updatedAt" : "2021-06-23T01:27:17Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "6a489fef-35ab-4f49-b5f0-eb4f108f2808",
        "parentId" : "d5fd45c5-b419-4537-b3f5-fe25082d318f",
        "authorId" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "body" : "Merged chunks are going to be 2MB. Only the last chunk in a merged file can be smaller than 2mb. So this can still throw  exception. But I am handling the exception in the following catch block to initiate a fallback. Do you see any issues with this?",
        "createdAt" : "2021-06-23T02:37:16Z",
        "updatedAt" : "2021-06-23T06:34:29Z",
        "lastEditedBy" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "tags" : [
        ]
      },
      {
        "id" : "1d39ec7c-1cf6-40a4-ab8f-0fc2f95a4fab",
        "parentId" : "d5fd45c5-b419-4537-b3f5-fe25082d318f",
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "What I am trying to understand is, we will be initiating a fallback and discarding merge even though there is nothing really wrong here - other than the fact that chunk was too small to decompress - right ? (in case chunk was split at a boundary which causes decompression to fail).\r\nWant to make sure I am not missing something here.\r\n\r\nThe 2mb is configurable, so we can have `maxBytesInFlight` and chunk sizes of different sizes - do we enforce any constraint on these to prevent this sort of issue ?",
        "createdAt" : "2021-06-23T08:06:19Z",
        "updatedAt" : "2021-06-23T08:06:19Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "ffc31e6c-80fa-4ddf-b98f-0820e1ca44fd",
        "parentId" : "d5fd45c5-b419-4537-b3f5-fe25082d318f",
        "authorId" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "body" : "> What I am trying to understand is, we will be initiating a fallback and discarding merge even though there is nothing really wrong here - other than the fact that chunk was too small to decompress - right ? (in case chunk was split at a boundary which causes decompression to fail).\r\n\r\nA shuffle merged chunk contains shuffle blocks in its entirety. It will never contain a partial shuffle block. This is documented for the configuration `minChunkSizeInMergedShuffleFile`\r\n```\r\n  /**\r\n   * The minimum size of a chunk when dividing a merged shuffle file into multiple chunks during\r\n   * push-based shuffle.\r\n   * A merged shuffle file consists of multiple small shuffle blocks. Fetching the\r\n   * complete merged shuffle file in a single response increases the memory requirements for the\r\n   * clients. Instead of serving the entire merged file, the shuffle service serves the\r\n   * merged file in `chunks`. A `chunk` constitutes few shuffle blocks in entirety and this\r\n   * configuration controls how big a chunk can get. A corresponding index file for each merged\r\n   * shuffle file will be generated indicating chunk boundaries.\r\n   */\r\n  public int minChunkSizeInMergedShuffleFile() {\r\n    return Ints.checkedCast(JavaUtils.byteStringAsBytes(\r\n        conf.get(\"spark.shuffle.server.minChunkSizeInMergedShuffleFile\", \"2m\")));\r\n  }\r\n  ```\r\nSo if this fails for a shuffle chunk, it would be because the shuffle chunk was corrupt.",
        "createdAt" : "2021-06-23T13:58:36Z",
        "updatedAt" : "2021-06-23T13:58:36Z",
        "lastEditedBy" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "tags" : [
        ]
      },
      {
        "id" : "5526337e-80aa-4692-a812-36f0227fc944",
        "parentId" : "d5fd45c5-b419-4537-b3f5-fe25082d318f",
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "Thanks for clarifying ! This addresses my concern.",
        "createdAt" : "2021-06-24T02:43:45Z",
        "updatedAt" : "2021-06-24T02:43:46Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad89a0208a5e3f880fca502c297362388a104dd7",
    "line" : 384,
    "diffHunk" : "@@ -1,1 +819,823 @@              if (streamCompressedOrEncrypted && detectCorruptUseExtraMemory) {\n                // TODO: manage the memory used here, and spill it into disk in case of OOM.\n                input = Utils.copyStreamUpTo(input, maxBytesInFlight / 3)\n              }\n            } catch {"
  },
  {
    "id" : "47bcb1a7-c3ff-43af-8a7c-f12850f3d8ad",
    "prId" : 30492,
    "prUrl" : "https://github.com/apache/spark/pull/30492#pullrequestreview-541281695",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1aaeefc8-cc15-4a94-bac4-95a8f1b81aea",
        "parentId" : null,
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Iâ€™m fuzzy on why this change is needed, would the fallback ever be equal to the execid?",
        "createdAt" : "2020-11-30T19:09:47Z",
        "updatedAt" : "2020-11-30T19:56:40Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "8a5aadf3-357a-4129-8db3-ecd5812a13b3",
        "parentId" : "1aaeefc8-cc15-4a94-bac4-95a8f1b81aea",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Yes. This is required for simplicity to make Spark consider the external blocks as a local block.",
        "createdAt" : "2020-11-30T19:54:57Z",
        "updatedAt" : "2020-11-30T19:56:40Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "234c1333-3426-4651-9669-215bf87e4633",
        "parentId" : "1aaeefc8-cc15-4a94-bac4-95a8f1b81aea",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Sounds reasonable",
        "createdAt" : "2020-11-30T20:47:36Z",
        "updatedAt" : "2020-11-30T20:47:36Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "9c9317378eb61b40f766e41dd168d2189f6008c1",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +298,302 @@    val fallback = FallbackStorage.FALLBACK_BLOCK_MANAGER_ID.executorId\n    for ((address, blockInfos) <- blocksByAddress) {\n      if (Seq(blockManager.blockManagerId.executorId, fallback).contains(address.executorId)) {\n        checkBlockSizes(blockInfos)\n        val mergedBlockInfos = mergeContinuousShuffleBlockIdsIfNeeded("
  }
]