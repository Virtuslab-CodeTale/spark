[
  {
    "id" : "babcba12-cead-46af-8edd-a69acc134566",
    "prId" : 31974,
    "prUrl" : "https://github.com/apache/spark/pull/31974#pullrequestreview-645947605",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dd8dd3c8-5049-435d-94c1-af245524570f",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "If this's used for YARN only, shall we move it to `YarnSchedulerBackend`? @tgravescs @SaurabhChawla100 ",
        "createdAt" : "2021-04-23T03:01:57Z",
        "updatedAt" : "2021-04-23T03:02:15Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "f73561f5-b0ff-4a8d-9d54-a7f91ecc8c7b",
        "parentId" : "dd8dd3c8-5049-435d-94c1-af245524570f",
        "authorId" : "11f6c849-b5b9-4c65-990c-525cf572b913",
        "body" : "@Ngone51 - The idea here is to create a framework where any ResourceManager can add their Process info through this MiscellaneousProcessAdded . And the code change is made to provide the info related to yarn-am in case deploy mode client for yarn through this framework.  cc @tgravescs ",
        "createdAt" : "2021-04-23T06:35:55Z",
        "updatedAt" : "2021-04-23T06:35:56Z",
        "lastEditedBy" : "11f6c849-b5b9-4c65-990c-525cf572b913",
        "tags" : [
        ]
      },
      {
        "id" : "b97169ca-423e-4e3f-b677-2ae1aa7c9e50",
        "parentId" : "dd8dd3c8-5049-435d-94c1-af245524570f",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Do we have a clear plan to add support for other resource managers?\r\n",
        "createdAt" : "2021-04-23T13:43:17Z",
        "updatedAt" : "2021-04-23T13:43:18Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "745817ac-cc99-4055-952d-8b321ebfd61e",
        "parentId" : "dd8dd3c8-5049-435d-94c1-af245524570f",
        "authorId" : "11f6c849-b5b9-4c65-990c-525cf572b913",
        "body" : "As of now we are not sure about the process that can be added by the other resource manager.But if there is any process that needs to be shown in the executors page, they can reuse the existing code here.",
        "createdAt" : "2021-04-26T03:25:15Z",
        "updatedAt" : "2021-04-26T03:25:15Z",
        "lastEditedBy" : "11f6c849-b5b9-4c65-990c-525cf572b913",
        "tags" : [
        ]
      },
      {
        "id" : "7557a27d-781e-4420-924e-7770549f04ba",
        "parentId" : "dd8dd3c8-5049-435d-94c1-af245524570f",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "I'm not sure what you had in mind for yarn specific code here?  most of this is in common places so the intent was to make it generic where it could be reused in the future if needed.",
        "createdAt" : "2021-04-27T14:44:06Z",
        "updatedAt" : "2021-04-27T14:44:06Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "eadb6dab935b26ffcae4e85f027ae324b98536ba",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +215,219 @@        makeOffers(executorId)\n\n      case MiscellaneousProcessAdded(time: Long,\n          processId: String, info: MiscellaneousProcessDetails) =>\n        listenerBus.post(SparkListenerMiscellaneousProcessAdded(time, processId, info))"
  },
  {
    "id" : "1c94a8e9-0d77-4145-839f-c23e4a9a1e13",
    "prId" : 31249,
    "prUrl" : "https://github.com/apache/spark/pull/31249#pullrequestreview-574714315",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "83e66de0-b440-4119-b73b-6b6e13c0bec8",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "If you don't mind, shall we revert this change to reduce this patch size?",
        "createdAt" : "2021-01-22T20:16:49Z",
        "updatedAt" : "2021-02-09T18:06:52Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "ad1573c5-5a2b-4b39-97b5-e9874cb27b27",
        "parentId" : "83e66de0-b440-4119-b73b-6b6e13c0bec8",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "So I switched the name to `execs` since it is multiple executors and I was having difficulty reading the code the first time through. It's not strong feelings but I think it's clearer this way (e.g. it's not calling foreach on the executors it's actually calling foreach on the option of list of execs).",
        "createdAt" : "2021-01-22T21:40:19Z",
        "updatedAt" : "2021-02-09T18:06:52Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "37c0323c-b56f-4886-840b-6e05b7defde9",
        "parentId" : "83e66de0-b440-4119-b73b-6b6e13c0bec8",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Got it. It's up to your decision.",
        "createdAt" : "2021-01-22T23:29:49Z",
        "updatedAt" : "2021-02-09T18:06:52Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "41bc1ecb-47c4-4dc1-9475-f6e15fdda25f",
        "parentId" : "83e66de0-b440-4119-b73b-6b6e13c0bec8",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "cool, if your ok with it I find this makes the code more readable so I'll leave as is :)",
        "createdAt" : "2021-01-23T00:54:41Z",
        "updatedAt" : "2021-02-09T18:06:52Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "73bfd55dc3d9b3968658dd874de94c6ce5577252",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +183,187 @@      case KillExecutorsOnHost(host) =>\n        scheduler.getExecutorsAliveOnHost(host).foreach { execs =>\n          killExecutors(execs.toSeq, adjustTargetNumExecutors = false, countFailures = false,\n            force = true)\n        }"
  },
  {
    "id" : "d6d7b6f7-7943-4c96-9b04-d56fface9828",
    "prId" : 31195,
    "prUrl" : "https://github.com/apache/spark/pull/31195#pullrequestreview-569532833",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "19dff483-4010-4c93-b41d-2ad3f7f00888",
        "parentId" : null,
        "authorId" : "8c82da5a-f351-4a37-a8a9-13809311b07b",
        "body" : "nit: `_.registrationTs`",
        "createdAt" : "2021-01-15T18:17:38Z",
        "updatedAt" : "2021-01-15T18:20:05Z",
        "lastEditedBy" : "8c82da5a-f351-4a37-a8a9-13809311b07b",
        "tags" : [
        ]
      },
      {
        "id" : "eddb600d-f890-45a4-8349-a1bb5b404a22",
        "parentId" : "19dff483-4010-4c93-b41d-2ad3f7f00888",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Hi, @jaceklaskowski . In a backport PR, we had better keep the consistency between branches. We can do that later from the `master` branch. \r\n\r\nPlease don't change this part, @attilapiros .",
        "createdAt" : "2021-01-15T18:51:35Z",
        "updatedAt" : "2021-01-15T18:51:35Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "3afa99ae6393e9a3be88c24aa764e793839eec02",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +559,563 @@\n  def getExecutorsWithRegistrationTs(): Map[String, Long] = synchronized {\n    executorDataMap.mapValues(v => v.registrationTs).toMap\n  }\n"
  },
  {
    "id" : "6da85dbd-2ea4-4541-b38f-c7c86fc395cb",
    "prId" : 29817,
    "prUrl" : "https://github.com/apache/spark/pull/29817#pullrequestreview-495104382",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7d2385a3-852f-404c-ad7c-fc0c845fb0e2",
        "parentId" : null,
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Here might be where you break the test suite last time, so double check it.",
        "createdAt" : "2020-09-23T23:10:33Z",
        "updatedAt" : "2020-10-22T13:56:45Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "3c1e033089e392066425139d7ccb52cd501dcc31",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +271,275 @@      // Do not change this code without running the K8s integration suites\n      case ExecutorDecommissioning(executorId) =>\n        logWarning(s\"Received executor $executorId decommissioned message\")\n        context.reply(\n          decommissionExecutor("
  },
  {
    "id" : "ffdfe21d-909b-4d98-8e59-4f1436a95856",
    "prId" : 29817,
    "prUrl" : "https://github.com/apache/spark/pull/29817#pullrequestreview-506177818",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9cd8298e-af2c-448d-95f9-58616969dccc",
        "parentId" : null,
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Not 100% sure, but I do remember that the locking mechanism withLock did manage to deadlock before, it might be good to add a logging statement in here that verifies this block is called in the integration test.",
        "createdAt" : "2020-10-06T21:31:08Z",
        "updatedAt" : "2020-10-22T13:56:45Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "12c51bd9-d8fb-4b9a-99d4-5f9940cb6537",
        "parentId" : "9cd8298e-af2c-448d-95f9-58616969dccc",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "> Not 100% sure, but I do remember that the locking mechanism withLock did manage to deadlock before\r\n\r\nYeah. We need to avoid the deadlock now because we need both CoarseGrainedSchedulerBackend's lock and TaskSchedulerImpl's lock within the same code block. \r\n\r\n\r\n> it might be good to add a logging statement in here that verifies this block is called in the integration test.\r\n\r\nUpdated the logs and the integration test.\r\n",
        "createdAt" : "2020-10-11T13:06:37Z",
        "updatedAt" : "2020-10-22T13:56:45Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "3c1e033089e392066425139d7ccb52cd501dcc31",
    "line" : 43,
    "diffHunk" : "@@ -1,1 +471,475 @@      executorsAndDecomInfo: Array[(String, ExecutorDecommissionInfo)],\n      adjustTargetNumExecutors: Boolean,\n      triggeredByExecutor: Boolean): Seq[String] = withLock {\n    // Do not change this code without running the K8s integration suites\n    val executorsToDecommission = executorsAndDecomInfo.flatMap { case (executorId, decomInfo) =>"
  },
  {
    "id" : "30f4df61-106c-4c34-9a52-c647c2529855",
    "prId" : 29817,
    "prUrl" : "https://github.com/apache/spark/pull/29817#pullrequestreview-506176873",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b8dc41e8-937d-46f5-9b09-8a20743b6996",
        "parentId" : null,
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "I think we might want to move this up because doing the storage decommissioning before the executor it's self is marked for decommissioning could mean that local blocks get stored in a manner where we might not know to migrate them. If you've dug through the code here and you don't believe that happens that's fine.",
        "createdAt" : "2020-10-06T21:34:05Z",
        "updatedAt" : "2020-10-22T13:56:45Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "a9cd13f2-1324-4d9d-b156-8776b9f61cfe",
        "parentId" : "b8dc41e8-937d-46f5-9b09-8a20743b6996",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "We don't really migrate blocks when decommissioning BlockManagers above. We only mark them as being decommissioning at driver side. So I think the problem you mentioned won't exist.",
        "createdAt" : "2020-10-11T12:56:04Z",
        "updatedAt" : "2020-10-22T13:56:45Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "3c1e033089e392066425139d7ccb52cd501dcc31",
    "line" : 102,
    "diffHunk" : "@@ -1,1 +499,503 @@    scheduler.sc.env.blockManager.master.decommissionBlockManagers(executorsToDecommission)\n\n    if (!triggeredByExecutor) {\n      executorsToDecommission.foreach { executorId =>\n        logInfo(s\"Notify executor $executorId to decommissioning.\")"
  },
  {
    "id" : "377c110a-160d-47bb-a6d9-e6ac643d47ad",
    "prId" : 29722,
    "prUrl" : "https://github.com/apache/spark/pull/29722#pullrequestreview-487695120",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "33fdef62-a021-4363-8e12-6a974cc05fdf",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "There is one branch missing from previous code:\r\n```\r\n      case None =>\r\n          // Ignoring the executor since it is not registered.\r\n          logWarning(s\"Attempted to decommission unknown executor $executorId.\")\r\n          return false\r\n```\r\n\r\nshall we guard against missing executor here as well?",
        "createdAt" : "2020-09-14T12:03:26Z",
        "updatedAt" : "2020-09-16T06:57:24Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "85fada49-fd11-40f5-9a3a-624fd212b174",
        "parentId" : "33fdef62-a021-4363-8e12-6a974cc05fdf",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "We don't need it now. Because the whole block is under the same lock protection(`withLock`). And since we've already ensured `isExecutorActive` above, `executorDataMap` must contains the `executorId` at this point.",
        "createdAt" : "2020-09-14T12:20:11Z",
        "updatedAt" : "2020-09-16T06:57:24Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "290d2c08a1adfce3c0c4afe1cb8c9e214b25ea3d",
    "line" : 112,
    "diffHunk" : "@@ -1,1 +498,502 @@      executorsToDecommission.foreach { executorId =>\n        logInfo(s\"Asking executor $executorId to decommissioning.\")\n        executorDataMap(executorId).executorEndpoint.send(DecommissionExecutor)\n      }\n    }"
  },
  {
    "id" : "fc0a02ee-97fe-401c-a8df-13a5086a919c",
    "prId" : 29579,
    "prUrl" : "https://github.com/apache/spark/pull/29579#pullrequestreview-478342553",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "421df321-eddb-40ce-881e-b14092346c7d",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "Just trying to follow why the driver end point message was dropped ?",
        "createdAt" : "2020-08-31T03:23:45Z",
        "updatedAt" : "2020-09-07T13:30:39Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "e214b63f-eacf-4ab8-ad12-63454aeba0e8",
        "parentId" : "421df321-eddb-40ce-881e-b14092346c7d",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "At this point, the executor has been added to `executorsPendingDecommission`, which is considered to be decommissioned already. The  message `DecommissionExecutor` calls `CoarseGrainedSchedulerBackend.decommissionExecutors` indeed. However, it's a NoOp for an executor that is already decommissioned. ",
        "createdAt" : "2020-08-31T04:07:44Z",
        "updatedAt" : "2020-09-07T13:30:39Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "f628cbfe-7755-418d-8811-2a8fa604a855",
        "parentId" : "421df321-eddb-40ce-881e-b14092346c7d",
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "Agreed !",
        "createdAt" : "2020-08-31T06:37:45Z",
        "updatedAt" : "2020-09-07T13:30:39Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      }
    ],
    "commit" : "d2468407f3d02e5a191ad373b5f2201a930cbb09",
    "line" : 68,
    "diffHunk" : "@@ -1,1 +497,501 @@\n    logInfo(s\"Asking executor $executorId to decommissioning.\")\n    scheduler.executorDecommission(executorId, decomInfo)\n    // Send decommission message to the executor (it could have originated on the executor\n    // but not necessarily)."
  },
  {
    "id" : "746f122a-607d-41ac-a0fc-e8c9ae0abb65",
    "prId" : 29579,
    "prUrl" : "https://github.com/apache/spark/pull/29579#pullrequestreview-478342553",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d84732e7-6bf6-4bd6-a1db-4449a392b4b7",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "nit: While you are here, can you please also close the parenthesis here :-) ",
        "createdAt" : "2020-09-01T21:36:07Z",
        "updatedAt" : "2020-09-07T13:30:39Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      }
    ],
    "commit" : "d2468407f3d02e5a191ad373b5f2201a930cbb09",
    "line" : 69,
    "diffHunk" : "@@ -1,1 +498,502 @@    logInfo(s\"Asking executor $executorId to decommissioning.\")\n    scheduler.executorDecommission(executorId, decomInfo)\n    // Send decommission message to the executor (it could have originated on the executor\n    // but not necessarily).\n    CoarseGrainedSchedulerBackend.this.synchronized {"
  },
  {
    "id" : "d0ac6084-ebda-458a-8090-8debd6cd7a55",
    "prId" : 29367,
    "prUrl" : "https://github.com/apache/spark/pull/29367#pullrequestreview-464523845",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5c942d78-1167-44a9-aa66-a9391cd62c17",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "Is the comment above accurate ? It seems we are indeed replacing the executors that are decommissioned when adjustTargetNumExecutors = true. \r\n\r\nOn a related note, should `adjustTargetNumExecutors` be simply renamed as `replaceDecommissionedExecutors` ? to make the meaning be more direct ?",
        "createdAt" : "2020-08-08T17:26:14Z",
        "updatedAt" : "2020-08-12T19:09:04Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "6827c571-5527-44aa-8a77-b6a635d1bb07",
        "parentId" : "5c942d78-1167-44a9-aa66-a9391cd62c17",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Take a look at the comments where this is called and in killExecutors.\r\n\r\nAlso if you look if this flag is enabled it removes the resource from the target resource profile, otherwise it leaves the request as is which means Spark will replace it.",
        "createdAt" : "2020-08-08T18:23:43Z",
        "updatedAt" : "2020-08-12T19:09:04Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "23bec9a9-5f9d-4406-954b-6338787a3def",
        "parentId" : "5c942d78-1167-44a9-aa66-a9391cd62c17",
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "Okay, I understand this better now. `adjustTargetNumExecutors=true` means that the scheduler will not try to replenish the executor. Otherwise, the scheduler still thinks that the application wants the same number of executors as before.\r\n\r\nI would strongly recommend that you please consider extracting out this piece of common code (shared with `killExecutors`) into a separate helper method. Its fairly subtle and having it one place will both help the readability of this PR and future changes.",
        "createdAt" : "2020-08-08T22:07:12Z",
        "updatedAt" : "2020-08-12T19:09:04Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "a87fb55c-0123-4f84-89b1-c704e93e7176",
        "parentId" : "5c942d78-1167-44a9-aa66-a9391cd62c17",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "yeah we could move this into a helper method, that's a good idea.",
        "createdAt" : "2020-08-10T17:51:40Z",
        "updatedAt" : "2020-08-12T19:09:05Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "b359e517-8878-4b3d-b825-161c76f635e6",
        "parentId" : "5c942d78-1167-44a9-aa66-a9391cd62c17",
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "Thanks :-P ",
        "createdAt" : "2020-08-10T19:42:31Z",
        "updatedAt" : "2020-08-12T19:09:05Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      }
    ],
    "commit" : "e970cb10147fb64533f5088edc3a448b5ef198cf",
    "line" : 109,
    "diffHunk" : "@@ -1,1 +476,480 @@\n    // If we don't want to replace the executors we are decommissioning\n    if (adjustTargetNumExecutors) {\n      adjustExecutors(executorsToDecommission.map(_._1))\n    }"
  },
  {
    "id" : "1f6ab5e4-d697-4b8d-a2fd-1d78c0f88d0d",
    "prId" : 29367,
    "prUrl" : "https://github.com/apache/spark/pull/29367#pullrequestreview-465373030",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d1cad35a-9e28-418f-b865-e775be1fb6cb",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "Why is this duplicating the `decommissionExecutor` call above ? I think you are decommissioning twice now: This method is called synchronously by the ExecutorAllocationManager: It sends a message `DecommissionExecutor` to itself, which then calls `decommissionExecutor` on line 425 in this file. This does not smell right to me: both the code duplication and also the double decommissioning.\r\n\r\nI think you want to simply send the message here and let the DriverEndpoint work do the actual decommissioning as before. Ie. only do this: `driverEndpoint.send(DecommissionExecutor(executorId, decomInfo))`\r\n\r\n[Blocker]",
        "createdAt" : "2020-08-08T23:33:08Z",
        "updatedAt" : "2020-08-12T19:09:05Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "e578b09a-f4da-46b2-8cfc-cb714ea1ef91",
        "parentId" : "d1cad35a-9e28-418f-b865-e775be1fb6cb",
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "^^^ @holdenk ... any thoughts/followup on this ?",
        "createdAt" : "2020-08-10T19:42:13Z",
        "updatedAt" : "2020-08-12T19:09:05Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "038b040a-6d79-4184-8c98-52151bd1ad8b",
        "parentId" : "d1cad35a-9e28-418f-b865-e775be1fb6cb",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Yeah should be able to clean up the duplicated code, I'll take a crack tonight.",
        "createdAt" : "2020-08-10T22:38:13Z",
        "updatedAt" : "2020-08-12T19:09:05Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "bdfb1a32-48a8-40ef-8b4e-0abfccf3e37f",
        "parentId" : "d1cad35a-9e28-418f-b865-e775be1fb6cb",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Ok I cleaned up the duplicated function, I'll poke at the possible double driver endpoint send this morning.",
        "createdAt" : "2020-08-11T17:58:18Z",
        "updatedAt" : "2020-08-12T19:09:05Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "53f8f9da-8ad7-44be-a440-da2bd402e079",
        "parentId" : "d1cad35a-9e28-418f-b865-e775be1fb6cb",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "So this is the only non-test place where we call `decommissionBlockManagers`.",
        "createdAt" : "2020-08-11T19:37:41Z",
        "updatedAt" : "2020-08-12T19:09:05Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "850ae1f0-c436-4532-9003-37992c9f4df1",
        "parentId" : "d1cad35a-9e28-418f-b865-e775be1fb6cb",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Looking inside of the EAM I see the call to decom inside of removeExecutors, but I'm not seeing the message.",
        "createdAt" : "2020-08-11T19:40:43Z",
        "updatedAt" : "2020-08-12T19:09:05Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "e970cb10147fb64533f5088edc3a448b5ef198cf",
    "line" : 119,
    "diffHunk" : "@@ -1,1 +486,490 @@\n\n  private def doDecommission(executorId: String,\n      decomInfo: ExecutorDecommissionInfo): Boolean = {\n"
  },
  {
    "id" : "2f4f8e6a-43a0-45a2-8f57-8965990fe877",
    "prId" : 29367,
    "prUrl" : "https://github.com/apache/spark/pull/29367#pullrequestreview-465533760",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "58feb959-3654-4455-bdd2-a51afc90f1df",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "I believe there is still a cycle here: Please trace through how `DecommissionExecutor` message is handled: It will eventually call this doDecommission which will send the message again ... If I understand correctly, this may end up live-locking the driver until the poor executor actually dies for good.\r\n\r\nOne way to break this cycle is to directly call `doDecommission` in the handlers for `DecommissionExecutor` in this class's `receive` and `receiveAndReply` methods, with a special flag that forbids re-sending the message.\r\n\r\n[blocker]",
        "createdAt" : "2020-08-11T21:45:41Z",
        "updatedAt" : "2020-08-12T19:09:05Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "5ecaf483-b8d6-4ad0-a25f-32755c12bb83",
        "parentId" : "58feb959-3654-4455-bdd2-a51afc90f1df",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Ok so to me in CoarseGrainedSchedulerBackend if we receive a `DecommissionExecutor` we call `decommissionExecutor` in the EAM base class which then calls the `decommissionExecutors` in the Scheduler which, if the executor has not been marked as decommissioning, marks the executor as decommissioning and then calls into `doDecomission` which asks the scheduler and driver and block manager (as appopriate) to take on the next steps in decommissioning.\r\n\r\nOn the driver endpoint we will delegate to this same function, and since the executor is already marked as decommissioned we will no-op. We could drop this message from the driver endpoint, but I figured leaving it there incase it's easier for someone to send a no-reply message to the driver endpoint then the boolean ask makes sense.\r\n\r\nWDYT?",
        "createdAt" : "2020-08-11T22:44:15Z",
        "updatedAt" : "2020-08-12T19:09:05Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "e1151e7e-a1ab-414a-9f93-65b7ea5308ff",
        "parentId" : "58feb959-3654-4455-bdd2-a51afc90f1df",
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "You are so RIGHT !!!. Sorry, for not spotting that. In which case I think this is not just fine but also neat in that the code paths for decommissioned are pretty nicely converged now. ",
        "createdAt" : "2020-08-12T01:38:28Z",
        "updatedAt" : "2020-08-12T19:09:05Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      }
    ],
    "commit" : "e970cb10147fb64533f5088edc3a448b5ef198cf",
    "line" : 127,
    "diffHunk" : "@@ -1,1 +494,498 @@      if (driverEndpoint != null) {\n        logInfo(\"Propagating executor decommission to driver.\")\n        driverEndpoint.send(DecommissionExecutor(executorId, decomInfo))\n      }\n    } catch {"
  },
  {
    "id" : "d35e89a7-e540-45ef-bd6b-290adf483972",
    "prId" : 29367,
    "prUrl" : "https://github.com/apache/spark/pull/29367#pullrequestreview-465557158",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0e7339ab-6861-4631-a4c4-58dccf02923c",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "should there be a check for executorsToDecommission.notEmpty ? Otherwise, we will request executors again with no change in the adjustExecutors helper method. Could again lead to some unnecessary strain on the driver. \r\n\r\nNot a big deal because this is one time, since doDecommission isn't called again and again.",
        "createdAt" : "2020-08-12T01:36:10Z",
        "updatedAt" : "2020-08-12T19:09:05Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "d7f0b2a2-58c4-4c72-b3bb-11634da45a88",
        "parentId" : "0e7339ab-6861-4631-a4c4-58dccf02923c",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Let me put that logic inside adjustExecutors :)",
        "createdAt" : "2020-08-12T02:53:53Z",
        "updatedAt" : "2020-08-12T19:09:05Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "e970cb10147fb64533f5088edc3a448b5ef198cf",
    "line" : 109,
    "diffHunk" : "@@ -1,1 +476,480 @@\n    // If we don't want to replace the executors we are decommissioning\n    if (adjustTargetNumExecutors) {\n      adjustExecutors(executorsToDecommission.map(_._1))\n    }"
  },
  {
    "id" : "b64ea4a8-5390-4113-b082-d3f11257138c",
    "prId" : 29211,
    "prUrl" : "https://github.com/apache/spark/pull/29211#pullrequestreview-462318285",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d914b6b7-be6a-41bc-b6d0-55498cceb9e0",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "So we need to prevent duplicate `DecommissionSelf` at the executor side? For now, I don't see we handle duplicate `DecommissionSelf` and it may create duplicate threads as a result.",
        "createdAt" : "2020-08-06T08:58:31Z",
        "updatedAt" : "2020-08-06T09:39:47Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "e81c3fc2d0b8c28cd0aedf08a7ea1e86a153ae49",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +443,447 @@            logError(s\"Unexpected error during decommissioning ${e.toString}\", e)\n        }\n        // Send decommission message to the executor, this may be a duplicate since the executor\n        // could have been the one to notify us. But it's also possible the notification came from\n        // elsewhere and the executor does not yet know."
  },
  {
    "id" : "d6e321d2-2d21-43d4-8c4f-5cc3ad4a898d",
    "prId" : 28746,
    "prUrl" : "https://github.com/apache/spark/pull/28746#pullrequestreview-425902897",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "059a392e-0afb-486c-af39-9f9d27591bcd",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "This change fixes the failure of test `org.apache.spark.launcher.LauncherBackendSuite.standalone/client: launcher handle`. After sleeping one more second, the application launched by the `SparkLauncher` now has a chance to submit tasks to TaskScheduler and call `reviveOffers` on the SchedulerBackend. At the same time, the `SparkLauncher` will ask the application to stop. Therefore, the SchedulerBackend could have been already stopped when it receives `ReviveOffers` messages, which would fail the entire application at the end.\r\n\r\nSo, I use ` Utils.tryLogNonFatalError` to fix it and I think this should be fine since we've already use it at:\r\n\r\nhttps://github.com/apache/spark/blob/c560428fe0113f17362bae2b369910049914696f/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala#L137-L139\r\n\r\n ",
        "createdAt" : "2020-06-08T04:00:45Z",
        "updatedAt" : "2020-07-13T02:06:02Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "59a0595c8177ff801d0f7cbcc1fe6c458bcb844c",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +561,565 @@  }\n\n  override def reviveOffers(): Unit = Utils.tryLogNonFatalError {\n    driverEndpoint.send(ReviveOffers)\n  }"
  },
  {
    "id" : "a326f43d-5975-4d99-aa5c-1cf111869d50",
    "prId" : 28254,
    "prUrl" : "https://github.com/apache/spark/pull/28254#pullrequestreview-396155344",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7265a1ba-b716-4ec7-a7b2-5ebd7973724f",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Can we guarantee that `stop` is called before `kill` in this way?",
        "createdAt" : "2020-04-20T03:30:03Z",
        "updatedAt" : "2020-04-20T03:30:22Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "b2a2ca6a-0f25-4f88-939b-aec1fc51c345",
        "parentId" : "7265a1ba-b716-4ec7-a7b2-5ebd7973724f",
        "authorId" : "832e1988-205f-40f9-89d5-c37ea16224c9",
        "body" : "Yes.",
        "createdAt" : "2020-04-20T05:48:50Z",
        "updatedAt" : "2020-04-20T05:48:50Z",
        "lastEditedBy" : "832e1988-205f-40f9-89d5-c37ea16224c9",
        "tags" : [
        ]
      },
      {
        "id" : "ea0120b2-65b1-46d8-b0e4-94774840b45b",
        "parentId" : "7265a1ba-b716-4ec7-a7b2-5ebd7973724f",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "`StopExecutor` may arrive at executor after `kill` arrive at worker/container due to network delay, isn't it possible?",
        "createdAt" : "2020-04-20T05:51:46Z",
        "updatedAt" : "2020-04-20T05:51:47Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "c4d37111c3bbf1db4eddb7dbaaff487fe82d81ae",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +771,775 @@        if (executorsToKill.nonEmpty) {\n          executorsToKill.foreach(id =>\n            executorDataMap.get(id).foreach(_.executorEndpoint.send(StopExecutor)))\n          _ => doKillExecutors(executorsToKill)\n        } else {"
  },
  {
    "id" : "71363c53-04e0-4ba3-baa0-8fc748c631ec",
    "prId" : 27773,
    "prUrl" : "https://github.com/apache/spark/pull/27773#pullrequestreview-375706495",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "90ab3854-a00d-4005-be1c-b9053986e1aa",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Executors may haven't launched at the time we call `maxNumConcurrentTasks` from `checkBarrierStageWithNumSlots`?",
        "createdAt" : "2020-03-09T08:02:27Z",
        "updatedAt" : "2020-03-19T20:03:38Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "03358259-b5e1-42be-aec1-1d36d768cac2",
        "parentId" : "90ab3854-a00d-4005-be1c-b9053986e1aa",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "I assume that could be the case or that executor number went to 0 if some died, the sum will just return 0. there is no change in logic for that.  Is there something specific you are wondering about?",
        "createdAt" : "2020-03-09T16:00:16Z",
        "updatedAt" : "2020-03-19T20:03:38Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "424a46de-60f3-4ca1-8638-dd5a85d6588a",
        "parentId" : "90ab3854-a00d-4005-be1c-b9053986e1aa",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Let's say user has a barrier job like:\r\n\r\n```\r\nrdd.withResources(rp).barrirer().mapPartition { part =>\r\n  // do some barrier stuff\r\n}.collect()\r\n```\r\nSo, at the time we're `createShuffleMapStage`/`mergeResourceProfilesForStage`/`checkBarrierStageWithNumSlots`, it's possible that some executors haven't launched?\r\n\r\n",
        "createdAt" : "2020-03-10T02:36:50Z",
        "updatedAt" : "2020-03-19T20:03:38Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "8f49d2fc-78d5-4fc6-9282-8fc7c5cf180b",
        "parentId" : "90ab3854-a00d-4005-be1c-b9053986e1aa",
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "Yes, executors need not have launched, launched/running executors might have failed (after computing `rdd` in the example above), or number of executors is < number of partitions in rdd.\r\n`maxNumConcurrentTasks` will tell us how many (max) can be run based on current state.",
        "createdAt" : "2020-03-10T10:54:52Z",
        "updatedAt" : "2020-03-19T20:03:38Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "34267c51-5df4-425e-a2c4-03a410d02d93",
        "parentId" : "90ab3854-a00d-4005-be1c-b9053986e1aa",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Actually, I was not talking about some error cases here. Here, I'm worrying about the normal procedure. And I realize now we already have precaution for it, see: https://github.com/apache/spark/blob/b29829e2abdebdf6fa9dd0a4a4cf4c9d676ee82d/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala#L983-L1004",
        "createdAt" : "2020-03-17T03:26:02Z",
        "updatedAt" : "2020-03-19T20:03:38Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "2a4a73385ee948d81e51091990a37ebbc8010f61",
    "line" : 61,
    "diffHunk" : "@@ -1,1 +615,619 @@  override def maxNumConcurrentTasks(rp: ResourceProfile): Int = synchronized {\n    val cpusPerTask = ResourceProfile.getTaskCpusOrDefaultForProfile(rp, conf)\n    val executorsWithResourceProfile = executorDataMap.values.filter(_.resourceProfileId == rp.id)\n    executorsWithResourceProfile.map(_.totalCores / cpusPerTask).sum\n  }"
  },
  {
    "id" : "8a15d1b2-361b-468c-b234-0a16408532ca",
    "prId" : 27773,
    "prUrl" : "https://github.com/apache/spark/pull/27773#pullrequestreview-371975728",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "27e4e1f7-fb36-4c98-91e6-fd2cf5c5cd39",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "We can get `taskCpus` through this executor's resource profile? e.g. `sc.resourceProfileManager.taskCpusForProfileId(taskSetRpID)`\r\n\r\nThus, we don't need to pass `taskCpus` to executor.",
        "createdAt" : "2020-03-09T08:38:04Z",
        "updatedAt" : "2020-03-19T20:03:38Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "02f3f6a0-2205-4f0d-bd07-79963e3cbabf",
        "parentId" : "27e4e1f7-fb36-4c98-91e6-fd2cf5c5cd39",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "I put it there so that I don't have to worry about race conditions between when we get status updates from executors and when the scheduler removes the tasksets. for instance currently we could remove it from the taskidToTaskSetManager if the executor failed.  In the case the taskid to task set manager is missing we wouldn't know how many cores to properly add back. So to keep away from that race this seems cleaner and also possibly gives us a way to expose to the user in the future if we wanted.  ",
        "createdAt" : "2020-03-09T20:10:40Z",
        "updatedAt" : "2020-03-19T20:03:38Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "2e390ef5-a564-4850-97b0-fb70b675fc49",
        "parentId" : "27e4e1f7-fb36-4c98-91e6-fd2cf5c5cd39",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Oh..oops..I copied example from your code without update and leave you to another direction. Sorry about that.\r\n\r\nActually, I mean, could we do this?\r\n\r\n`sc.resourceProfileManager.taskCpusForProfileId(executorInfo.resourceProfileId)`\r\n\r\nGet resourceProfileId from executorInfo instead of taskSet.",
        "createdAt" : "2020-03-10T02:43:24Z",
        "updatedAt" : "2020-03-19T20:03:38Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "6c75e4c7-bb15-4bc6-a6dd-36c6334db075",
        "parentId" : "27e4e1f7-fb36-4c98-91e6-fd2cf5c5cd39",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "ah, yes we can get it from there. we aren't using it for anything else at this point. ",
        "createdAt" : "2020-03-10T13:59:12Z",
        "updatedAt" : "2020-03-19T20:03:38Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "2a4a73385ee948d81e51091990a37ebbc8010f61",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +149,153 @@              val prof = scheduler.sc.resourceProfileManager.resourceProfileFromId(rpId)\n              val taskCpus = ResourceProfile.getTaskCpusOrDefaultForProfile(prof, conf)\n              executorInfo.freeCores += taskCpus\n              resources.foreach { case (k, v) =>\n                executorInfo.resourcesInfo.get(k).foreach { r =>"
  },
  {
    "id" : "a4b663f2-7218-47f6-a5a0-9be6c76f397d",
    "prId" : 26682,
    "prUrl" : "https://github.com/apache/spark/pull/26682#pullrequestreview-342072932",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4f3e8104-4c5f-432b-b27b-b1eccf6a7c15",
        "parentId" : null,
        "authorId" : "99e31844-a5b3-48a2-af71-412edb607a13",
        "body" : "nit: Can we add `private [spark]` or others?",
        "createdAt" : "2020-01-13T19:08:55Z",
        "updatedAt" : "2020-01-17T01:51:16Z",
        "lastEditedBy" : "99e31844-a5b3-48a2-af71-412edb607a13",
        "tags" : [
        ]
      },
      {
        "id" : "c4d72571-9a75-4664-9b62-b6c57ff19416",
        "parentId" : "4f3e8104-4c5f-432b-b27b-b1eccf6a7c15",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "the entire class is already private[spark]",
        "createdAt" : "2020-01-13T19:11:00Z",
        "updatedAt" : "2020-01-17T01:51:16Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "75091102d6bd64a61fb903c8df3edf9ca047e879",
    "line" : 51,
    "diffHunk" : "@@ -1,1 +577,581 @@\n  // this function is for testing only\n  def getExecutorResourceProfileId(executorId: String): Int = synchronized {\n    val res = executorDataMap.get(executorId)\n    res.map(_.resourceProfileId).getOrElse(ResourceProfile.UNKNOWN_RESOURCE_PROFILE_ID)"
  },
  {
    "id" : "cbf1a20a-e6a9-4bc4-9ab5-8eb4a38e532d",
    "prId" : 26682,
    "prUrl" : "https://github.com/apache/spark/pull/26682#pullrequestreview-343567197",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "591210a6-cef4-41f7-98c4-db35a2c8a031",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Executors do not know what default ResourceProfile is? If default ResourceProfile is derived from Spark configs, do we still need to send a default ResourceProfile to executors? Can executors generate it if it knows it uses default one?",
        "createdAt" : "2020-01-15T06:35:12Z",
        "updatedAt" : "2020-01-17T01:51:16Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "fbb16c15-6aba-4be0-94ff-a2e44cb4bbab",
        "parentId" : "591210a6-cef4-41f7-98c4-db35a2c8a031",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "They can but its really just extra code and an extra code path to have to maintain.   I specifically removed the old code that was generating the default profile on the executor side from the configs because the driver already calculated it and there isn't a reason to have 2 paths to calculate it.  This way it all works the same. Was there something specific here you are concerned with? ",
        "createdAt" : "2020-01-15T14:20:40Z",
        "updatedAt" : "2020-01-17T01:51:16Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "56be2ba7-b6de-47fd-93b6-70d816629355",
        "parentId" : "591210a6-cef4-41f7-98c4-db35a2c8a031",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Oh I see. I had thought that if we don't have special resource need, we don't need such ResourceProfile communication between driver/executors. If for a clean code path and the communication is cheap, I think it should be fine.",
        "createdAt" : "2020-01-15T22:31:41Z",
        "updatedAt" : "2020-01-17T01:51:16Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "75091102d6bd64a61fb903c8df3edf9ca047e879",
    "line" : 42,
    "diffHunk" : "@@ -1,1 +280,284 @@          SparkEnv.get.securityManager.getIOEncryptionKey(),\n          Option(delegationTokens.get()),\n          rp)\n        context.reply(reply)\n    }"
  },
  {
    "id" : "c2bb6170-205f-469a-b5c9-ce8c193c1950",
    "prId" : 26586,
    "prUrl" : "https://github.com/apache/spark/pull/26586#pullrequestreview-343598399",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "514c5e07-b8a3-4a0a-980b-d79ec945033a",
        "parentId" : null,
        "authorId" : "ec40c71b-2099-4668-ada8-f3a78eb61ea4",
        "body" : "Is there a reason we would rather `sendFailure(_)` instead of the exiting the executor with a `RegisterExecutorFailed` message? ",
        "createdAt" : "2020-01-15T23:42:53Z",
        "updatedAt" : "2020-01-15T23:48:12Z",
        "lastEditedBy" : "ec40c71b-2099-4668-ada8-f3a78eb61ea4",
        "tags" : [
        ]
      },
      {
        "id" : "2c98b3fa-6c69-4d31-945d-d8acbdbf9af9",
        "parentId" : "514c5e07-b8a3-4a0a-980b-d79ec945033a",
        "authorId" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "body" : "Explained in the PR description.",
        "createdAt" : "2020-01-15T23:52:03Z",
        "updatedAt" : "2020-01-15T23:52:03Z",
        "lastEditedBy" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "tags" : [
        ]
      }
    ],
    "commit" : "e6f881451d4fb10d23aaa735919278cefe88c0c3",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +215,219 @@          // or if it ignored our blacklist), then we reject that executor immediately.\n          logInfo(s\"Rejecting $executorId as it has been blacklisted.\")\n          context.sendFailure(new IllegalStateException(s\"Executor is blacklisted: $executorId\"))\n        } else {\n          // If the executor's rpc env is not listening for incoming connections, `hostPort`"
  },
  {
    "id" : "0cd93229-b357-4ebb-b396-ed89bcbd5b00",
    "prId" : 26440,
    "prUrl" : "https://github.com/apache/spark/pull/26440#pullrequestreview-348287524",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "263f9bbb-0a1b-41f7-907e-6b3e02cb194b",
        "parentId" : null,
        "authorId" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "body" : "I see you adding things to this set but didn't notice anywhere removing the executor from it.",
        "createdAt" : "2020-01-06T20:31:02Z",
        "updatedAt" : "2020-02-13T23:10:10Z",
        "lastEditedBy" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "tags" : [
        ]
      },
      {
        "id" : "61c096e7-d844-4b49-b919-d6faa69ca621",
        "parentId" : "263f9bbb-0a1b-41f7-907e-6b3e02cb194b",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Good catch, I dropped that in the rebase.",
        "createdAt" : "2020-01-09T22:43:33Z",
        "updatedAt" : "2020-02-13T23:10:10Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "ec8fb7fa-e051-412d-a679-ea22183667f8",
        "parentId" : "263f9bbb-0a1b-41f7-907e-6b3e02cb194b",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Added the cleanup logic back in.",
        "createdAt" : "2020-01-25T00:07:01Z",
        "updatedAt" : "2020-02-13T23:10:10Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "af550303e0b929dc9f7436bcfb36438ff36b8208",
    "line" : 71,
    "diffHunk" : "@@ -1,1 +417,421 @@        // Only bother decommissioning executors which are alive.\n        if (isExecutorActive(executorId)) {\n          executorsPendingDecommission += executorId\n          true\n        } else {"
  },
  {
    "id" : "46a9794b-6f37-4f96-88bc-eafa474bb9f4",
    "prId" : 26440,
    "prUrl" : "https://github.com/apache/spark/pull/26440#pullrequestreview-359201672",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "909c213a-4f05-4184-a8f0-696c60a305ae",
        "parentId" : null,
        "authorId" : "11f6c849-b5b9-4c65-990c-525cf572b913",
        "body" : "Here the entire node will be decommissioned , So instead of checking for each executor id contains in executorsPendingDecommission, can we have the filter on the node level with its HostName as the input parameter. So there will be only one entry of the node hostname that would be added in the some decommission tracker.At the end we don't want any task to be scheduled on the executor running on the node. We can check this in org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.DriverEndpoint#makeOffers",
        "createdAt" : "2020-02-10T12:50:43Z",
        "updatedAt" : "2020-02-13T23:10:10Z",
        "lastEditedBy" : "11f6c849-b5b9-4c65-990c-525cf572b913",
        "tags" : [
        ]
      },
      {
        "id" : "f35b93ee-e310-48de-bc8a-9ee65db0bdb4",
        "parentId" : "909c213a-4f05-4184-a8f0-696c60a305ae",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "We could, but we also use this as a basis for other cluster managers which may not have an entire node decom always (hence executor decommissioning).",
        "createdAt" : "2020-02-10T19:33:18Z",
        "updatedAt" : "2020-02-13T23:10:10Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "c13fe782-4b18-4b20-84bf-ecc03f16aee1",
        "parentId" : "909c213a-4f05-4184-a8f0-696c60a305ae",
        "authorId" : "11f6c849-b5b9-4c65-990c-525cf572b913",
        "body" : "I am considering this scenario where there is spotloss in aws or Preemptible VM loss in GCP, the entire node will not be available and all the executors on that node will not be available.There may be scenario as per the existing code where the code is checking the decommissioning of executors and then API call is send to update the executorsPendingDecommission for each executors and in mean time when the executor information is added into the executorsPendingDecommission and node is lost/ about to be lost. Since there might be some of the enteries of executors that are not added into executorsPendingDecommission and we are still scheduling a task in those executors.\r\n\r\nIf it would have been on the node level, just there is need to add first node info in the NodePendingToDecommission and rest if the other executors running same node will try to add there entry can be ignored. In this there is less chance of the task to be scheduled on any executor on the decommissioning nodes. This is possible at the yarn end where AM can send the request to RM to find the updated states of the node. Not sure if its possible in the case of k8s where each executorpods can also get the information about hostName where that pods are running and pods loss reason. \r\n\r\nIf its not possible to get the above info in the k8s, can we scope this executor decommission to k8s level and also add the node level decommissioning also here where other cluster manager can take the benefit of node level decommissioning",
        "createdAt" : "2020-02-11T08:35:54Z",
        "updatedAt" : "2020-02-13T23:10:10Z",
        "lastEditedBy" : "11f6c849-b5b9-4c65-990c-525cf572b913",
        "tags" : [
        ]
      },
      {
        "id" : "1e8460da-3e09-48e7-a51d-dfbe36bf296d",
        "parentId" : "909c213a-4f05-4184-a8f0-696c60a305ae",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "We could also add node-level information when we implement YARN support, I'm not sure what that would give us over just adding the executors on the node to the current structure.",
        "createdAt" : "2020-02-14T20:30:14Z",
        "updatedAt" : "2020-02-14T20:30:15Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "af550303e0b929dc9f7436bcfb36438ff36b8208",
    "line" : 122,
    "diffHunk" : "@@ -1,1 +603,607 @@      !executorsPendingToRemove.contains(id) &&\n      !executorsPendingLossReason.contains(id) &&\n      !executorsPendingDecommission.contains(id)\n\n  }"
  },
  {
    "id" : "b0cf0dbb-7cbc-4cbe-ac89-d88970a199a3",
    "prId" : 25964,
    "prUrl" : "https://github.com/apache/spark/pull/25964#pullrequestreview-314524042",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9b518ebe-97b4-4e89-a253-985079548a6c",
        "parentId" : null,
        "authorId" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "body" : "Don't you need to call `makeOffers(executorId)` to have the executor get tasks right away, instead of waiting for other events to trigger that?\r\n\r\nSimiarly, the old \"register\" message handler does not need to call `makeOffers` anymore.",
        "createdAt" : "2019-11-09T00:22:01Z",
        "updatedAt" : "2019-11-11T11:21:09Z",
        "lastEditedBy" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "tags" : [
        ]
      },
      {
        "id" : "bc5a6425-4695-49b6-a7ae-14f4ec260172",
        "parentId" : "9b518ebe-97b4-4e89-a253-985079548a6c",
        "authorId" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "body" : "good catch, we shall move the call of `makeOffers` from `RegisterExecutor` to here.",
        "createdAt" : "2019-11-09T00:33:21Z",
        "updatedAt" : "2019-11-11T11:21:09Z",
        "lastEditedBy" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "f329d813af5c329491f73aa74557788c000a57c2",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +191,195 @@        executorDataMap.get(executorId).foreach { data =>\n          data.freeCores = data.totalCores\n        }\n        makeOffers(executorId)\n    }"
  },
  {
    "id" : "1cda7ded-67a7-4911-8435-9b928cc73bff",
    "prId" : 25964,
    "prUrl" : "https://github.com/apache/spark/pull/25964#pullrequestreview-314508366",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1f727706-3552-4799-aac3-336e6a35cee2",
        "parentId" : null,
        "authorId" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "body" : "This works for now, in the meanwhile I'm wondering whether we shall also mark the resources as empty.",
        "createdAt" : "2019-11-09T00:24:41Z",
        "updatedAt" : "2019-11-11T11:21:09Z",
        "lastEditedBy" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "f329d813af5c329491f73aa74557788c000a57c2",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +190,194 @@      case LaunchedExecutor(executorId) =>\n        executorDataMap.get(executorId).foreach { data =>\n          data.freeCores = data.totalCores\n        }\n        makeOffers(executorId)"
  },
  {
    "id" : "919441f9-e972-48e0-a0c9-8a12f9deb928",
    "prId" : 25922,
    "prUrl" : "https://github.com/apache/spark/pull/25922#pullrequestreview-292833472",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9672d263-f482-4b18-93fb-1ceb6febdea0",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "@ConeyLiu So, are these all of them which needs `synchronized`?",
        "createdAt" : "2019-09-25T04:33:23Z",
        "updatedAt" : "2019-09-25T08:20:03Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "a050725e-3d12-44aa-a1a1-8df3fc40985d",
        "parentId" : "9672d263-f482-4b18-93fb-1ceb6febdea0",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "BTW, this method is testing-only. We may not need for this.",
        "createdAt" : "2019-09-25T04:34:49Z",
        "updatedAt" : "2019-09-25T08:20:03Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "3aca0aa2-a516-4a36-bb2d-81282da8f37b",
        "parentId" : "9672d263-f482-4b18-93fb-1ceb6febdea0",
        "authorId" : "28d581bb-a398-46f0-861f-a016fdb56186",
        "body" : "Hi @dongjoon-hyun, thanks for reviewing. Only those who accessing `executorDataMap` out of `DriverEndpoint`. I suggest this method to synchronize too because it will not add too much overhead for the test.",
        "createdAt" : "2019-09-25T05:08:00Z",
        "updatedAt" : "2019-09-25T08:20:03Z",
        "lastEditedBy" : "28d581bb-a398-46f0-861f-a016fdb56186",
        "tags" : [
        ]
      },
      {
        "id" : "65ebca48-7bc7-43d5-9de3-cf59494b1e06",
        "parentId" : "9672d263-f482-4b18-93fb-1ceb6febdea0",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Yep. No problem.",
        "createdAt" : "2019-09-25T05:20:24Z",
        "updatedAt" : "2019-09-25T08:20:03Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "08f29888554d839b1329de49624989911d366984",
    "line" : 41,
    "diffHunk" : "@@ -1,1 +554,558 @@  // this function is for testing only\n  def getExecutorAvailableResources(\n      executorId: String): Map[String, ExecutorResourceInfo] = synchronized {\n    executorDataMap.get(executorId).map(_.resourcesInfo).getOrElse(Map.empty)\n  }"
  },
  {
    "id" : "d7acf085-d297-475c-924d-7f993c9f6251",
    "prId" : 25922,
    "prUrl" : "https://github.com/apache/spark/pull/25922#pullrequestreview-292903302",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3154f05b-9bc4-40a8-91b7-572c72ca44b4",
        "parentId" : null,
        "authorId" : "28d581bb-a398-46f0-861f-a016fdb56186",
        "body" : "Accessing `executorDataMap` in the inherited methods from ThreadSafeRpcEndpoint should also be OK.",
        "createdAt" : "2019-09-25T08:21:16Z",
        "updatedAt" : "2019-09-25T08:21:16Z",
        "lastEditedBy" : "28d581bb-a398-46f0-861f-a016fdb56186",
        "tags" : [
        ]
      }
    ],
    "commit" : "08f29888554d839b1329de49624989911d366984",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +73,77 @@  // protected by `CoarseGrainedSchedulerBackend.this`. Besides, `executorDataMap` should only\n  // be modified in the inherited methods from ThreadSafeRpcEndpoint with protection by\n  // `CoarseGrainedSchedulerBackend.this`.\n  private val executorDataMap = new HashMap[String, ExecutorData]\n"
  },
  {
    "id" : "7f4b64d2-477a-4ae3-90ae-eb12a1f92528",
    "prId" : 25622,
    "prUrl" : "https://github.com/apache/spark/pull/25622#pullrequestreview-281808870",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "39f879f7-6d8f-4725-a282-2465223c03c0",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I think we don't have to change this - it's going to cause a lot of conflicts when we backport but the gain here is virtually nothing. Either way is legitimate.",
        "createdAt" : "2019-08-30T02:56:34Z",
        "updatedAt" : "2019-08-30T02:56:34Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "67cabe3464b632d784919b018fb575f19f2f1d85",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +271,275 @@        val workOffers = activeExecutors.map {\n          case (id, executorData) =>\n            WorkerOffer(id, executorData.executorHost, executorData.freeCores,\n              Some(executorData.executorAddress.hostPort),\n              executorData.resourcesInfo.map { case (rName, rInfo) =>"
  }
]