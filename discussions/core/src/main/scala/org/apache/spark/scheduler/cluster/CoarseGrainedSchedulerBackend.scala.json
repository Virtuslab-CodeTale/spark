[
  {
    "id" : "babcba12-cead-46af-8edd-a69acc134566",
    "prId" : 31974,
    "prUrl" : "https://github.com/apache/spark/pull/31974#pullrequestreview-645947605",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dd8dd3c8-5049-435d-94c1-af245524570f",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "If this's used for YARN only, shall we move it to `YarnSchedulerBackend`? @tgravescs @SaurabhChawla100 ",
        "createdAt" : "2021-04-23T03:01:57Z",
        "updatedAt" : "2021-04-23T03:02:15Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "f73561f5-b0ff-4a8d-9d54-a7f91ecc8c7b",
        "parentId" : "dd8dd3c8-5049-435d-94c1-af245524570f",
        "authorId" : "11f6c849-b5b9-4c65-990c-525cf572b913",
        "body" : "@Ngone51 - The idea here is to create a framework where any ResourceManager can add their Process info through this MiscellaneousProcessAdded . And the code change is made to provide the info related to yarn-am in case deploy mode client for yarn through this framework.  cc @tgravescs ",
        "createdAt" : "2021-04-23T06:35:55Z",
        "updatedAt" : "2021-04-23T06:35:56Z",
        "lastEditedBy" : "11f6c849-b5b9-4c65-990c-525cf572b913",
        "tags" : [
        ]
      },
      {
        "id" : "b97169ca-423e-4e3f-b677-2ae1aa7c9e50",
        "parentId" : "dd8dd3c8-5049-435d-94c1-af245524570f",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Do we have a clear plan to add support for other resource managers?\r\n",
        "createdAt" : "2021-04-23T13:43:17Z",
        "updatedAt" : "2021-04-23T13:43:18Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "745817ac-cc99-4055-952d-8b321ebfd61e",
        "parentId" : "dd8dd3c8-5049-435d-94c1-af245524570f",
        "authorId" : "11f6c849-b5b9-4c65-990c-525cf572b913",
        "body" : "As of now we are not sure about the process that can be added by the other resource manager.But if there is any process that needs to be shown in the executors page, they can reuse the existing code here.",
        "createdAt" : "2021-04-26T03:25:15Z",
        "updatedAt" : "2021-04-26T03:25:15Z",
        "lastEditedBy" : "11f6c849-b5b9-4c65-990c-525cf572b913",
        "tags" : [
        ]
      },
      {
        "id" : "7557a27d-781e-4420-924e-7770549f04ba",
        "parentId" : "dd8dd3c8-5049-435d-94c1-af245524570f",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "I'm not sure what you had in mind for yarn specific code here?  most of this is in common places so the intent was to make it generic where it could be reused in the future if needed.",
        "createdAt" : "2021-04-27T14:44:06Z",
        "updatedAt" : "2021-04-27T14:44:06Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "eadb6dab935b26ffcae4e85f027ae324b98536ba",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +215,219 @@        makeOffers(executorId)\n\n      case MiscellaneousProcessAdded(time: Long,\n          processId: String, info: MiscellaneousProcessDetails) =>\n        listenerBus.post(SparkListenerMiscellaneousProcessAdded(time, processId, info))"
  },
  {
    "id" : "1c94a8e9-0d77-4145-839f-c23e4a9a1e13",
    "prId" : 31249,
    "prUrl" : "https://github.com/apache/spark/pull/31249#pullrequestreview-574714315",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "83e66de0-b440-4119-b73b-6b6e13c0bec8",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "If you don't mind, shall we revert this change to reduce this patch size?",
        "createdAt" : "2021-01-22T20:16:49Z",
        "updatedAt" : "2021-02-09T18:06:52Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "ad1573c5-5a2b-4b39-97b5-e9874cb27b27",
        "parentId" : "83e66de0-b440-4119-b73b-6b6e13c0bec8",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "So I switched the name to `execs` since it is multiple executors and I was having difficulty reading the code the first time through. It's not strong feelings but I think it's clearer this way (e.g. it's not calling foreach on the executors it's actually calling foreach on the option of list of execs).",
        "createdAt" : "2021-01-22T21:40:19Z",
        "updatedAt" : "2021-02-09T18:06:52Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "37c0323c-b56f-4886-840b-6e05b7defde9",
        "parentId" : "83e66de0-b440-4119-b73b-6b6e13c0bec8",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Got it. It's up to your decision.",
        "createdAt" : "2021-01-22T23:29:49Z",
        "updatedAt" : "2021-02-09T18:06:52Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "41bc1ecb-47c4-4dc1-9475-f6e15fdda25f",
        "parentId" : "83e66de0-b440-4119-b73b-6b6e13c0bec8",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "cool, if your ok with it I find this makes the code more readable so I'll leave as is :)",
        "createdAt" : "2021-01-23T00:54:41Z",
        "updatedAt" : "2021-02-09T18:06:52Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "73bfd55dc3d9b3968658dd874de94c6ce5577252",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +183,187 @@      case KillExecutorsOnHost(host) =>\n        scheduler.getExecutorsAliveOnHost(host).foreach { execs =>\n          killExecutors(execs.toSeq, adjustTargetNumExecutors = false, countFailures = false,\n            force = true)\n        }"
  },
  {
    "id" : "d6d7b6f7-7943-4c96-9b04-d56fface9828",
    "prId" : 31195,
    "prUrl" : "https://github.com/apache/spark/pull/31195#pullrequestreview-569532833",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "19dff483-4010-4c93-b41d-2ad3f7f00888",
        "parentId" : null,
        "authorId" : "8c82da5a-f351-4a37-a8a9-13809311b07b",
        "body" : "nit: `_.registrationTs`",
        "createdAt" : "2021-01-15T18:17:38Z",
        "updatedAt" : "2021-01-15T18:20:05Z",
        "lastEditedBy" : "8c82da5a-f351-4a37-a8a9-13809311b07b",
        "tags" : [
        ]
      },
      {
        "id" : "eddb600d-f890-45a4-8349-a1bb5b404a22",
        "parentId" : "19dff483-4010-4c93-b41d-2ad3f7f00888",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Hi, @jaceklaskowski . In a backport PR, we had better keep the consistency between branches. We can do that later from the `master` branch. \r\n\r\nPlease don't change this part, @attilapiros .",
        "createdAt" : "2021-01-15T18:51:35Z",
        "updatedAt" : "2021-01-15T18:51:35Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "3afa99ae6393e9a3be88c24aa764e793839eec02",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +559,563 @@\n  def getExecutorsWithRegistrationTs(): Map[String, Long] = synchronized {\n    executorDataMap.mapValues(v => v.registrationTs).toMap\n  }\n"
  },
  {
    "id" : "6da85dbd-2ea4-4541-b38f-c7c86fc395cb",
    "prId" : 29817,
    "prUrl" : "https://github.com/apache/spark/pull/29817#pullrequestreview-495104382",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7d2385a3-852f-404c-ad7c-fc0c845fb0e2",
        "parentId" : null,
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Here might be where you break the test suite last time, so double check it.",
        "createdAt" : "2020-09-23T23:10:33Z",
        "updatedAt" : "2020-10-22T13:56:45Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "3c1e033089e392066425139d7ccb52cd501dcc31",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +271,275 @@      // Do not change this code without running the K8s integration suites\n      case ExecutorDecommissioning(executorId) =>\n        logWarning(s\"Received executor $executorId decommissioned message\")\n        context.reply(\n          decommissionExecutor("
  },
  {
    "id" : "ffdfe21d-909b-4d98-8e59-4f1436a95856",
    "prId" : 29817,
    "prUrl" : "https://github.com/apache/spark/pull/29817#pullrequestreview-506177818",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9cd8298e-af2c-448d-95f9-58616969dccc",
        "parentId" : null,
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Not 100% sure, but I do remember that the locking mechanism withLock did manage to deadlock before, it might be good to add a logging statement in here that verifies this block is called in the integration test.",
        "createdAt" : "2020-10-06T21:31:08Z",
        "updatedAt" : "2020-10-22T13:56:45Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "12c51bd9-d8fb-4b9a-99d4-5f9940cb6537",
        "parentId" : "9cd8298e-af2c-448d-95f9-58616969dccc",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "> Not 100% sure, but I do remember that the locking mechanism withLock did manage to deadlock before\r\n\r\nYeah. We need to avoid the deadlock now because we need both CoarseGrainedSchedulerBackend's lock and TaskSchedulerImpl's lock within the same code block. \r\n\r\n\r\n> it might be good to add a logging statement in here that verifies this block is called in the integration test.\r\n\r\nUpdated the logs and the integration test.\r\n",
        "createdAt" : "2020-10-11T13:06:37Z",
        "updatedAt" : "2020-10-22T13:56:45Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "3c1e033089e392066425139d7ccb52cd501dcc31",
    "line" : 43,
    "diffHunk" : "@@ -1,1 +471,475 @@      executorsAndDecomInfo: Array[(String, ExecutorDecommissionInfo)],\n      adjustTargetNumExecutors: Boolean,\n      triggeredByExecutor: Boolean): Seq[String] = withLock {\n    // Do not change this code without running the K8s integration suites\n    val executorsToDecommission = executorsAndDecomInfo.flatMap { case (executorId, decomInfo) =>"
  },
  {
    "id" : "30f4df61-106c-4c34-9a52-c647c2529855",
    "prId" : 29817,
    "prUrl" : "https://github.com/apache/spark/pull/29817#pullrequestreview-506176873",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b8dc41e8-937d-46f5-9b09-8a20743b6996",
        "parentId" : null,
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "I think we might want to move this up because doing the storage decommissioning before the executor it's self is marked for decommissioning could mean that local blocks get stored in a manner where we might not know to migrate them. If you've dug through the code here and you don't believe that happens that's fine.",
        "createdAt" : "2020-10-06T21:34:05Z",
        "updatedAt" : "2020-10-22T13:56:45Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "a9cd13f2-1324-4d9d-b156-8776b9f61cfe",
        "parentId" : "b8dc41e8-937d-46f5-9b09-8a20743b6996",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "We don't really migrate blocks when decommissioning BlockManagers above. We only mark them as being decommissioning at driver side. So I think the problem you mentioned won't exist.",
        "createdAt" : "2020-10-11T12:56:04Z",
        "updatedAt" : "2020-10-22T13:56:45Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "3c1e033089e392066425139d7ccb52cd501dcc31",
    "line" : 102,
    "diffHunk" : "@@ -1,1 +499,503 @@    scheduler.sc.env.blockManager.master.decommissionBlockManagers(executorsToDecommission)\n\n    if (!triggeredByExecutor) {\n      executorsToDecommission.foreach { executorId =>\n        logInfo(s\"Notify executor $executorId to decommissioning.\")"
  },
  {
    "id" : "377c110a-160d-47bb-a6d9-e6ac643d47ad",
    "prId" : 29722,
    "prUrl" : "https://github.com/apache/spark/pull/29722#pullrequestreview-487695120",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "33fdef62-a021-4363-8e12-6a974cc05fdf",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "There is one branch missing from previous code:\r\n```\r\n      case None =>\r\n          // Ignoring the executor since it is not registered.\r\n          logWarning(s\"Attempted to decommission unknown executor $executorId.\")\r\n          return false\r\n```\r\n\r\nshall we guard against missing executor here as well?",
        "createdAt" : "2020-09-14T12:03:26Z",
        "updatedAt" : "2020-09-16T06:57:24Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "85fada49-fd11-40f5-9a3a-624fd212b174",
        "parentId" : "33fdef62-a021-4363-8e12-6a974cc05fdf",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "We don't need it now. Because the whole block is under the same lock protection(`withLock`). And since we've already ensured `isExecutorActive` above, `executorDataMap` must contains the `executorId` at this point.",
        "createdAt" : "2020-09-14T12:20:11Z",
        "updatedAt" : "2020-09-16T06:57:24Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "290d2c08a1adfce3c0c4afe1cb8c9e214b25ea3d",
    "line" : 112,
    "diffHunk" : "@@ -1,1 +498,502 @@      executorsToDecommission.foreach { executorId =>\n        logInfo(s\"Asking executor $executorId to decommissioning.\")\n        executorDataMap(executorId).executorEndpoint.send(DecommissionExecutor)\n      }\n    }"
  },
  {
    "id" : "fc0a02ee-97fe-401c-a8df-13a5086a919c",
    "prId" : 29579,
    "prUrl" : "https://github.com/apache/spark/pull/29579#pullrequestreview-478342553",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "421df321-eddb-40ce-881e-b14092346c7d",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "Just trying to follow why the driver end point message was dropped ?",
        "createdAt" : "2020-08-31T03:23:45Z",
        "updatedAt" : "2020-09-07T13:30:39Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "e214b63f-eacf-4ab8-ad12-63454aeba0e8",
        "parentId" : "421df321-eddb-40ce-881e-b14092346c7d",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "At this point, the executor has been added to `executorsPendingDecommission`, which is considered to be decommissioned already. The  message `DecommissionExecutor` calls `CoarseGrainedSchedulerBackend.decommissionExecutors` indeed. However, it's a NoOp for an executor that is already decommissioned. ",
        "createdAt" : "2020-08-31T04:07:44Z",
        "updatedAt" : "2020-09-07T13:30:39Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "f628cbfe-7755-418d-8811-2a8fa604a855",
        "parentId" : "421df321-eddb-40ce-881e-b14092346c7d",
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "Agreed !",
        "createdAt" : "2020-08-31T06:37:45Z",
        "updatedAt" : "2020-09-07T13:30:39Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      }
    ],
    "commit" : "d2468407f3d02e5a191ad373b5f2201a930cbb09",
    "line" : 68,
    "diffHunk" : "@@ -1,1 +497,501 @@\n    logInfo(s\"Asking executor $executorId to decommissioning.\")\n    scheduler.executorDecommission(executorId, decomInfo)\n    // Send decommission message to the executor (it could have originated on the executor\n    // but not necessarily)."
  },
  {
    "id" : "746f122a-607d-41ac-a0fc-e8c9ae0abb65",
    "prId" : 29579,
    "prUrl" : "https://github.com/apache/spark/pull/29579#pullrequestreview-478342553",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d84732e7-6bf6-4bd6-a1db-4449a392b4b7",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "nit: While you are here, can you please also close the parenthesis here :-) ",
        "createdAt" : "2020-09-01T21:36:07Z",
        "updatedAt" : "2020-09-07T13:30:39Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      }
    ],
    "commit" : "d2468407f3d02e5a191ad373b5f2201a930cbb09",
    "line" : 69,
    "diffHunk" : "@@ -1,1 +498,502 @@    logInfo(s\"Asking executor $executorId to decommissioning.\")\n    scheduler.executorDecommission(executorId, decomInfo)\n    // Send decommission message to the executor (it could have originated on the executor\n    // but not necessarily).\n    CoarseGrainedSchedulerBackend.this.synchronized {"
  },
  {
    "id" : "d0ac6084-ebda-458a-8090-8debd6cd7a55",
    "prId" : 29367,
    "prUrl" : "https://github.com/apache/spark/pull/29367#pullrequestreview-464523845",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5c942d78-1167-44a9-aa66-a9391cd62c17",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "Is the comment above accurate ? It seems we are indeed replacing the executors that are decommissioned when adjustTargetNumExecutors = true. \r\n\r\nOn a related note, should `adjustTargetNumExecutors` be simply renamed as `replaceDecommissionedExecutors` ? to make the meaning be more direct ?",
        "createdAt" : "2020-08-08T17:26:14Z",
        "updatedAt" : "2020-08-12T19:09:04Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "6827c571-5527-44aa-8a77-b6a635d1bb07",
        "parentId" : "5c942d78-1167-44a9-aa66-a9391cd62c17",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Take a look at the comments where this is called and in killExecutors.\r\n\r\nAlso if you look if this flag is enabled it removes the resource from the target resource profile, otherwise it leaves the request as is which means Spark will replace it.",
        "createdAt" : "2020-08-08T18:23:43Z",
        "updatedAt" : "2020-08-12T19:09:04Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "23bec9a9-5f9d-4406-954b-6338787a3def",
        "parentId" : "5c942d78-1167-44a9-aa66-a9391cd62c17",
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "Okay, I understand this better now. `adjustTargetNumExecutors=true` means that the scheduler will not try to replenish the executor. Otherwise, the scheduler still thinks that the application wants the same number of executors as before.\r\n\r\nI would strongly recommend that you please consider extracting out this piece of common code (shared with `killExecutors`) into a separate helper method. Its fairly subtle and having it one place will both help the readability of this PR and future changes.",
        "createdAt" : "2020-08-08T22:07:12Z",
        "updatedAt" : "2020-08-12T19:09:04Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "a87fb55c-0123-4f84-89b1-c704e93e7176",
        "parentId" : "5c942d78-1167-44a9-aa66-a9391cd62c17",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "yeah we could move this into a helper method, that's a good idea.",
        "createdAt" : "2020-08-10T17:51:40Z",
        "updatedAt" : "2020-08-12T19:09:05Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "b359e517-8878-4b3d-b825-161c76f635e6",
        "parentId" : "5c942d78-1167-44a9-aa66-a9391cd62c17",
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "Thanks :-P ",
        "createdAt" : "2020-08-10T19:42:31Z",
        "updatedAt" : "2020-08-12T19:09:05Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      }
    ],
    "commit" : "e970cb10147fb64533f5088edc3a448b5ef198cf",
    "line" : 109,
    "diffHunk" : "@@ -1,1 +476,480 @@\n    // If we don't want to replace the executors we are decommissioning\n    if (adjustTargetNumExecutors) {\n      adjustExecutors(executorsToDecommission.map(_._1))\n    }"
  },
  {
    "id" : "1f6ab5e4-d697-4b8d-a2fd-1d78c0f88d0d",
    "prId" : 29367,
    "prUrl" : "https://github.com/apache/spark/pull/29367#pullrequestreview-465373030",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d1cad35a-9e28-418f-b865-e775be1fb6cb",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "Why is this duplicating the `decommissionExecutor` call above ? I think you are decommissioning twice now: This method is called synchronously by the ExecutorAllocationManager: It sends a message `DecommissionExecutor` to itself, which then calls `decommissionExecutor` on line 425 in this file. This does not smell right to me: both the code duplication and also the double decommissioning.\r\n\r\nI think you want to simply send the message here and let the DriverEndpoint work do the actual decommissioning as before. Ie. only do this: `driverEndpoint.send(DecommissionExecutor(executorId, decomInfo))`\r\n\r\n[Blocker]",
        "createdAt" : "2020-08-08T23:33:08Z",
        "updatedAt" : "2020-08-12T19:09:05Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "e578b09a-f4da-46b2-8cfc-cb714ea1ef91",
        "parentId" : "d1cad35a-9e28-418f-b865-e775be1fb6cb",
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "^^^ @holdenk ... any thoughts/followup on this ?",
        "createdAt" : "2020-08-10T19:42:13Z",
        "updatedAt" : "2020-08-12T19:09:05Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "038b040a-6d79-4184-8c98-52151bd1ad8b",
        "parentId" : "d1cad35a-9e28-418f-b865-e775be1fb6cb",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Yeah should be able to clean up the duplicated code, I'll take a crack tonight.",
        "createdAt" : "2020-08-10T22:38:13Z",
        "updatedAt" : "2020-08-12T19:09:05Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "bdfb1a32-48a8-40ef-8b4e-0abfccf3e37f",
        "parentId" : "d1cad35a-9e28-418f-b865-e775be1fb6cb",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Ok I cleaned up the duplicated function, I'll poke at the possible double driver endpoint send this morning.",
        "createdAt" : "2020-08-11T17:58:18Z",
        "updatedAt" : "2020-08-12T19:09:05Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "53f8f9da-8ad7-44be-a440-da2bd402e079",
        "parentId" : "d1cad35a-9e28-418f-b865-e775be1fb6cb",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "So this is the only non-test place where we call `decommissionBlockManagers`.",
        "createdAt" : "2020-08-11T19:37:41Z",
        "updatedAt" : "2020-08-12T19:09:05Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "850ae1f0-c436-4532-9003-37992c9f4df1",
        "parentId" : "d1cad35a-9e28-418f-b865-e775be1fb6cb",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Looking inside of the EAM I see the call to decom inside of removeExecutors, but I'm not seeing the message.",
        "createdAt" : "2020-08-11T19:40:43Z",
        "updatedAt" : "2020-08-12T19:09:05Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "e970cb10147fb64533f5088edc3a448b5ef198cf",
    "line" : 119,
    "diffHunk" : "@@ -1,1 +486,490 @@\n\n  private def doDecommission(executorId: String,\n      decomInfo: ExecutorDecommissionInfo): Boolean = {\n"
  },
  {
    "id" : "2f4f8e6a-43a0-45a2-8f57-8965990fe877",
    "prId" : 29367,
    "prUrl" : "https://github.com/apache/spark/pull/29367#pullrequestreview-465533760",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "58feb959-3654-4455-bdd2-a51afc90f1df",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "I believe there is still a cycle here: Please trace through how `DecommissionExecutor` message is handled: It will eventually call this doDecommission which will send the message again ... If I understand correctly, this may end up live-locking the driver until the poor executor actually dies for good.\r\n\r\nOne way to break this cycle is to directly call `doDecommission` in the handlers for `DecommissionExecutor` in this class's `receive` and `receiveAndReply` methods, with a special flag that forbids re-sending the message.\r\n\r\n[blocker]",
        "createdAt" : "2020-08-11T21:45:41Z",
        "updatedAt" : "2020-08-12T19:09:05Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "5ecaf483-b8d6-4ad0-a25f-32755c12bb83",
        "parentId" : "58feb959-3654-4455-bdd2-a51afc90f1df",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Ok so to me in CoarseGrainedSchedulerBackend if we receive a `DecommissionExecutor` we call `decommissionExecutor` in the EAM base class which then calls the `decommissionExecutors` in the Scheduler which, if the executor has not been marked as decommissioning, marks the executor as decommissioning and then calls into `doDecomission` which asks the scheduler and driver and block manager (as appopriate) to take on the next steps in decommissioning.\r\n\r\nOn the driver endpoint we will delegate to this same function, and since the executor is already marked as decommissioned we will no-op. We could drop this message from the driver endpoint, but I figured leaving it there incase it's easier for someone to send a no-reply message to the driver endpoint then the boolean ask makes sense.\r\n\r\nWDYT?",
        "createdAt" : "2020-08-11T22:44:15Z",
        "updatedAt" : "2020-08-12T19:09:05Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "e1151e7e-a1ab-414a-9f93-65b7ea5308ff",
        "parentId" : "58feb959-3654-4455-bdd2-a51afc90f1df",
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "You are so RIGHT !!!. Sorry, for not spotting that. In which case I think this is not just fine but also neat in that the code paths for decommissioned are pretty nicely converged now. ",
        "createdAt" : "2020-08-12T01:38:28Z",
        "updatedAt" : "2020-08-12T19:09:05Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      }
    ],
    "commit" : "e970cb10147fb64533f5088edc3a448b5ef198cf",
    "line" : 127,
    "diffHunk" : "@@ -1,1 +494,498 @@      if (driverEndpoint != null) {\n        logInfo(\"Propagating executor decommission to driver.\")\n        driverEndpoint.send(DecommissionExecutor(executorId, decomInfo))\n      }\n    } catch {"
  },
  {
    "id" : "d35e89a7-e540-45ef-bd6b-290adf483972",
    "prId" : 29367,
    "prUrl" : "https://github.com/apache/spark/pull/29367#pullrequestreview-465557158",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0e7339ab-6861-4631-a4c4-58dccf02923c",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "should there be a check for executorsToDecommission.notEmpty ? Otherwise, we will request executors again with no change in the adjustExecutors helper method. Could again lead to some unnecessary strain on the driver. \r\n\r\nNot a big deal because this is one time, since doDecommission isn't called again and again.",
        "createdAt" : "2020-08-12T01:36:10Z",
        "updatedAt" : "2020-08-12T19:09:05Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "d7f0b2a2-58c4-4c72-b3bb-11634da45a88",
        "parentId" : "0e7339ab-6861-4631-a4c4-58dccf02923c",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Let me put that logic inside adjustExecutors :)",
        "createdAt" : "2020-08-12T02:53:53Z",
        "updatedAt" : "2020-08-12T19:09:05Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "e970cb10147fb64533f5088edc3a448b5ef198cf",
    "line" : 109,
    "diffHunk" : "@@ -1,1 +476,480 @@\n    // If we don't want to replace the executors we are decommissioning\n    if (adjustTargetNumExecutors) {\n      adjustExecutors(executorsToDecommission.map(_._1))\n    }"
  },
  {
    "id" : "b64ea4a8-5390-4113-b082-d3f11257138c",
    "prId" : 29211,
    "prUrl" : "https://github.com/apache/spark/pull/29211#pullrequestreview-462318285",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d914b6b7-be6a-41bc-b6d0-55498cceb9e0",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "So we need to prevent duplicate `DecommissionSelf` at the executor side? For now, I don't see we handle duplicate `DecommissionSelf` and it may create duplicate threads as a result.",
        "createdAt" : "2020-08-06T08:58:31Z",
        "updatedAt" : "2020-08-06T09:39:47Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "e81c3fc2d0b8c28cd0aedf08a7ea1e86a153ae49",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +443,447 @@            logError(s\"Unexpected error during decommissioning ${e.toString}\", e)\n        }\n        // Send decommission message to the executor, this may be a duplicate since the executor\n        // could have been the one to notify us. But it's also possible the notification came from\n        // elsewhere and the executor does not yet know."
  }
]