[
  {
    "id" : "babcba12-cead-46af-8edd-a69acc134566",
    "prId" : 31974,
    "prUrl" : "https://github.com/apache/spark/pull/31974#pullrequestreview-645947605",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dd8dd3c8-5049-435d-94c1-af245524570f",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "If this's used for YARN only, shall we move it to `YarnSchedulerBackend`? @tgravescs @SaurabhChawla100 ",
        "createdAt" : "2021-04-23T03:01:57Z",
        "updatedAt" : "2021-04-23T03:02:15Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "f73561f5-b0ff-4a8d-9d54-a7f91ecc8c7b",
        "parentId" : "dd8dd3c8-5049-435d-94c1-af245524570f",
        "authorId" : "11f6c849-b5b9-4c65-990c-525cf572b913",
        "body" : "@Ngone51 - The idea here is to create a framework where any ResourceManager can add their Process info through this MiscellaneousProcessAdded . And the code change is made to provide the info related to yarn-am in case deploy mode client for yarn through this framework.  cc @tgravescs ",
        "createdAt" : "2021-04-23T06:35:55Z",
        "updatedAt" : "2021-04-23T06:35:56Z",
        "lastEditedBy" : "11f6c849-b5b9-4c65-990c-525cf572b913",
        "tags" : [
        ]
      },
      {
        "id" : "b97169ca-423e-4e3f-b677-2ae1aa7c9e50",
        "parentId" : "dd8dd3c8-5049-435d-94c1-af245524570f",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Do we have a clear plan to add support for other resource managers?\r\n",
        "createdAt" : "2021-04-23T13:43:17Z",
        "updatedAt" : "2021-04-23T13:43:18Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "745817ac-cc99-4055-952d-8b321ebfd61e",
        "parentId" : "dd8dd3c8-5049-435d-94c1-af245524570f",
        "authorId" : "11f6c849-b5b9-4c65-990c-525cf572b913",
        "body" : "As of now we are not sure about the process that can be added by the other resource manager.But if there is any process that needs to be shown in the executors page, they can reuse the existing code here.",
        "createdAt" : "2021-04-26T03:25:15Z",
        "updatedAt" : "2021-04-26T03:25:15Z",
        "lastEditedBy" : "11f6c849-b5b9-4c65-990c-525cf572b913",
        "tags" : [
        ]
      },
      {
        "id" : "7557a27d-781e-4420-924e-7770549f04ba",
        "parentId" : "dd8dd3c8-5049-435d-94c1-af245524570f",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "I'm not sure what you had in mind for yarn specific code here?  most of this is in common places so the intent was to make it generic where it could be reused in the future if needed.",
        "createdAt" : "2021-04-27T14:44:06Z",
        "updatedAt" : "2021-04-27T14:44:06Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "eadb6dab935b26ffcae4e85f027ae324b98536ba",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +215,219 @@        makeOffers(executorId)\n\n      case MiscellaneousProcessAdded(time: Long,\n          processId: String, info: MiscellaneousProcessDetails) =>\n        listenerBus.post(SparkListenerMiscellaneousProcessAdded(time, processId, info))"
  },
  {
    "id" : "1c94a8e9-0d77-4145-839f-c23e4a9a1e13",
    "prId" : 31249,
    "prUrl" : "https://github.com/apache/spark/pull/31249#pullrequestreview-574714315",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "83e66de0-b440-4119-b73b-6b6e13c0bec8",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "If you don't mind, shall we revert this change to reduce this patch size?",
        "createdAt" : "2021-01-22T20:16:49Z",
        "updatedAt" : "2021-02-09T18:06:52Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "ad1573c5-5a2b-4b39-97b5-e9874cb27b27",
        "parentId" : "83e66de0-b440-4119-b73b-6b6e13c0bec8",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "So I switched the name to `execs` since it is multiple executors and I was having difficulty reading the code the first time through. It's not strong feelings but I think it's clearer this way (e.g. it's not calling foreach on the executors it's actually calling foreach on the option of list of execs).",
        "createdAt" : "2021-01-22T21:40:19Z",
        "updatedAt" : "2021-02-09T18:06:52Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "37c0323c-b56f-4886-840b-6e05b7defde9",
        "parentId" : "83e66de0-b440-4119-b73b-6b6e13c0bec8",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Got it. It's up to your decision.",
        "createdAt" : "2021-01-22T23:29:49Z",
        "updatedAt" : "2021-02-09T18:06:52Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "41bc1ecb-47c4-4dc1-9475-f6e15fdda25f",
        "parentId" : "83e66de0-b440-4119-b73b-6b6e13c0bec8",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "cool, if your ok with it I find this makes the code more readable so I'll leave as is :)",
        "createdAt" : "2021-01-23T00:54:41Z",
        "updatedAt" : "2021-02-09T18:06:52Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "73bfd55dc3d9b3968658dd874de94c6ce5577252",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +183,187 @@      case KillExecutorsOnHost(host) =>\n        scheduler.getExecutorsAliveOnHost(host).foreach { execs =>\n          killExecutors(execs.toSeq, adjustTargetNumExecutors = false, countFailures = false,\n            force = true)\n        }"
  },
  {
    "id" : "d6d7b6f7-7943-4c96-9b04-d56fface9828",
    "prId" : 31195,
    "prUrl" : "https://github.com/apache/spark/pull/31195#pullrequestreview-569532833",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "19dff483-4010-4c93-b41d-2ad3f7f00888",
        "parentId" : null,
        "authorId" : "8c82da5a-f351-4a37-a8a9-13809311b07b",
        "body" : "nit: `_.registrationTs`",
        "createdAt" : "2021-01-15T18:17:38Z",
        "updatedAt" : "2021-01-15T18:20:05Z",
        "lastEditedBy" : "8c82da5a-f351-4a37-a8a9-13809311b07b",
        "tags" : [
        ]
      },
      {
        "id" : "eddb600d-f890-45a4-8349-a1bb5b404a22",
        "parentId" : "19dff483-4010-4c93-b41d-2ad3f7f00888",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Hi, @jaceklaskowski . In a backport PR, we had better keep the consistency between branches. We can do that later from the `master` branch. \r\n\r\nPlease don't change this part, @attilapiros .",
        "createdAt" : "2021-01-15T18:51:35Z",
        "updatedAt" : "2021-01-15T18:51:35Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "3afa99ae6393e9a3be88c24aa764e793839eec02",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +559,563 @@\n  def getExecutorsWithRegistrationTs(): Map[String, Long] = synchronized {\n    executorDataMap.mapValues(v => v.registrationTs).toMap\n  }\n"
  },
  {
    "id" : "6da85dbd-2ea4-4541-b38f-c7c86fc395cb",
    "prId" : 29817,
    "prUrl" : "https://github.com/apache/spark/pull/29817#pullrequestreview-495104382",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7d2385a3-852f-404c-ad7c-fc0c845fb0e2",
        "parentId" : null,
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Here might be where you break the test suite last time, so double check it.",
        "createdAt" : "2020-09-23T23:10:33Z",
        "updatedAt" : "2020-10-22T13:56:45Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "3c1e033089e392066425139d7ccb52cd501dcc31",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +271,275 @@      // Do not change this code without running the K8s integration suites\n      case ExecutorDecommissioning(executorId) =>\n        logWarning(s\"Received executor $executorId decommissioned message\")\n        context.reply(\n          decommissionExecutor("
  },
  {
    "id" : "ffdfe21d-909b-4d98-8e59-4f1436a95856",
    "prId" : 29817,
    "prUrl" : "https://github.com/apache/spark/pull/29817#pullrequestreview-506177818",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9cd8298e-af2c-448d-95f9-58616969dccc",
        "parentId" : null,
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Not 100% sure, but I do remember that the locking mechanism withLock did manage to deadlock before, it might be good to add a logging statement in here that verifies this block is called in the integration test.",
        "createdAt" : "2020-10-06T21:31:08Z",
        "updatedAt" : "2020-10-22T13:56:45Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "12c51bd9-d8fb-4b9a-99d4-5f9940cb6537",
        "parentId" : "9cd8298e-af2c-448d-95f9-58616969dccc",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "> Not 100% sure, but I do remember that the locking mechanism withLock did manage to deadlock before\r\n\r\nYeah. We need to avoid the deadlock now because we need both CoarseGrainedSchedulerBackend's lock and TaskSchedulerImpl's lock within the same code block. \r\n\r\n\r\n> it might be good to add a logging statement in here that verifies this block is called in the integration test.\r\n\r\nUpdated the logs and the integration test.\r\n",
        "createdAt" : "2020-10-11T13:06:37Z",
        "updatedAt" : "2020-10-22T13:56:45Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "3c1e033089e392066425139d7ccb52cd501dcc31",
    "line" : 43,
    "diffHunk" : "@@ -1,1 +471,475 @@      executorsAndDecomInfo: Array[(String, ExecutorDecommissionInfo)],\n      adjustTargetNumExecutors: Boolean,\n      triggeredByExecutor: Boolean): Seq[String] = withLock {\n    // Do not change this code without running the K8s integration suites\n    val executorsToDecommission = executorsAndDecomInfo.flatMap { case (executorId, decomInfo) =>"
  },
  {
    "id" : "30f4df61-106c-4c34-9a52-c647c2529855",
    "prId" : 29817,
    "prUrl" : "https://github.com/apache/spark/pull/29817#pullrequestreview-506176873",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b8dc41e8-937d-46f5-9b09-8a20743b6996",
        "parentId" : null,
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "I think we might want to move this up because doing the storage decommissioning before the executor it's self is marked for decommissioning could mean that local blocks get stored in a manner where we might not know to migrate them. If you've dug through the code here and you don't believe that happens that's fine.",
        "createdAt" : "2020-10-06T21:34:05Z",
        "updatedAt" : "2020-10-22T13:56:45Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "a9cd13f2-1324-4d9d-b156-8776b9f61cfe",
        "parentId" : "b8dc41e8-937d-46f5-9b09-8a20743b6996",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "We don't really migrate blocks when decommissioning BlockManagers above. We only mark them as being decommissioning at driver side. So I think the problem you mentioned won't exist.",
        "createdAt" : "2020-10-11T12:56:04Z",
        "updatedAt" : "2020-10-22T13:56:45Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "3c1e033089e392066425139d7ccb52cd501dcc31",
    "line" : 102,
    "diffHunk" : "@@ -1,1 +499,503 @@    scheduler.sc.env.blockManager.master.decommissionBlockManagers(executorsToDecommission)\n\n    if (!triggeredByExecutor) {\n      executorsToDecommission.foreach { executorId =>\n        logInfo(s\"Notify executor $executorId to decommissioning.\")"
  },
  {
    "id" : "377c110a-160d-47bb-a6d9-e6ac643d47ad",
    "prId" : 29722,
    "prUrl" : "https://github.com/apache/spark/pull/29722#pullrequestreview-487695120",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "33fdef62-a021-4363-8e12-6a974cc05fdf",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "There is one branch missing from previous code:\r\n```\r\n      case None =>\r\n          // Ignoring the executor since it is not registered.\r\n          logWarning(s\"Attempted to decommission unknown executor $executorId.\")\r\n          return false\r\n```\r\n\r\nshall we guard against missing executor here as well?",
        "createdAt" : "2020-09-14T12:03:26Z",
        "updatedAt" : "2020-09-16T06:57:24Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "85fada49-fd11-40f5-9a3a-624fd212b174",
        "parentId" : "33fdef62-a021-4363-8e12-6a974cc05fdf",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "We don't need it now. Because the whole block is under the same lock protection(`withLock`). And since we've already ensured `isExecutorActive` above, `executorDataMap` must contains the `executorId` at this point.",
        "createdAt" : "2020-09-14T12:20:11Z",
        "updatedAt" : "2020-09-16T06:57:24Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "290d2c08a1adfce3c0c4afe1cb8c9e214b25ea3d",
    "line" : 112,
    "diffHunk" : "@@ -1,1 +498,502 @@      executorsToDecommission.foreach { executorId =>\n        logInfo(s\"Asking executor $executorId to decommissioning.\")\n        executorDataMap(executorId).executorEndpoint.send(DecommissionExecutor)\n      }\n    }"
  },
  {
    "id" : "fc0a02ee-97fe-401c-a8df-13a5086a919c",
    "prId" : 29579,
    "prUrl" : "https://github.com/apache/spark/pull/29579#pullrequestreview-478342553",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "421df321-eddb-40ce-881e-b14092346c7d",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "Just trying to follow why the driver end point message was dropped ?",
        "createdAt" : "2020-08-31T03:23:45Z",
        "updatedAt" : "2020-09-07T13:30:39Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "e214b63f-eacf-4ab8-ad12-63454aeba0e8",
        "parentId" : "421df321-eddb-40ce-881e-b14092346c7d",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "At this point, the executor has been added to `executorsPendingDecommission`, which is considered to be decommissioned already. The  message `DecommissionExecutor` calls `CoarseGrainedSchedulerBackend.decommissionExecutors` indeed. However, it's a NoOp for an executor that is already decommissioned. ",
        "createdAt" : "2020-08-31T04:07:44Z",
        "updatedAt" : "2020-09-07T13:30:39Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "f628cbfe-7755-418d-8811-2a8fa604a855",
        "parentId" : "421df321-eddb-40ce-881e-b14092346c7d",
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "Agreed !",
        "createdAt" : "2020-08-31T06:37:45Z",
        "updatedAt" : "2020-09-07T13:30:39Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      }
    ],
    "commit" : "d2468407f3d02e5a191ad373b5f2201a930cbb09",
    "line" : 68,
    "diffHunk" : "@@ -1,1 +497,501 @@\n    logInfo(s\"Asking executor $executorId to decommissioning.\")\n    scheduler.executorDecommission(executorId, decomInfo)\n    // Send decommission message to the executor (it could have originated on the executor\n    // but not necessarily)."
  },
  {
    "id" : "746f122a-607d-41ac-a0fc-e8c9ae0abb65",
    "prId" : 29579,
    "prUrl" : "https://github.com/apache/spark/pull/29579#pullrequestreview-478342553",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d84732e7-6bf6-4bd6-a1db-4449a392b4b7",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "nit: While you are here, can you please also close the parenthesis here :-) ",
        "createdAt" : "2020-09-01T21:36:07Z",
        "updatedAt" : "2020-09-07T13:30:39Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      }
    ],
    "commit" : "d2468407f3d02e5a191ad373b5f2201a930cbb09",
    "line" : 69,
    "diffHunk" : "@@ -1,1 +498,502 @@    logInfo(s\"Asking executor $executorId to decommissioning.\")\n    scheduler.executorDecommission(executorId, decomInfo)\n    // Send decommission message to the executor (it could have originated on the executor\n    // but not necessarily).\n    CoarseGrainedSchedulerBackend.this.synchronized {"
  }
]