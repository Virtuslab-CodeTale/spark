[
  {
    "id" : "2808add8-1a74-451e-a72c-f0bc0e519645",
    "prId" : 25577,
    "prUrl" : "https://github.com/apache/spark/pull/25577#pullrequestreview-280144897",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d34518ac-a4ee-4ee1-aa4f-0c967842f155",
        "parentId" : null,
        "authorId" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "body" : "Is it thread-safe to create a new `CountDownLatch` here?",
        "createdAt" : "2019-08-27T03:37:04Z",
        "updatedAt" : "2019-08-27T11:55:55Z",
        "lastEditedBy" : "15acce7d-132b-48c9-894b-9bc3f4c2b09d",
        "tags" : [
        ]
      },
      {
        "id" : "cdca0472-0ae4-45b2-9d13-74ec1993516c",
        "parentId" : "d34518ac-a4ee-4ee1-aa4f-0c967842f155",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Since all events are processed in the single dispatchThread, so we'll only have that one thread to create new `CountDownLatch`. But I just realize that variable `latch` maybe override if the number of backlog events exceeds checkpoint batch size while last checkpoint is still in progress. So, I update the `shouldCheckpoint` condition to `isDone && (finish || processedEventsNum - lastRecordEventsNum >= batchSize)` to cover that corner case.",
        "createdAt" : "2019-08-27T12:05:43Z",
        "updatedAt" : "2019-08-27T12:05:43Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "a08c3e90c1553f81dae6fcf99e2179cfd95f93f9",
    "line" : 85,
    "diffHunk" : "@@ -1,1 +83,87 @@      // flush to make sure that all processed events' related data have write into InMemoryStore\n      listener.flush(listener.update(_, System.nanoTime()))\n      latch = new CountDownLatch(1)\n      lastRecordEventsNum = processedEventsNum\n      if (finish) {"
  }
]