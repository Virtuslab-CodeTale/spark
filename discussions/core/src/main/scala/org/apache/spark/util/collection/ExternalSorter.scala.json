[
  {
    "id" : "ea6afeef-483f-4bc6-b2b3-fd27393aab75",
    "prId" : 31480,
    "prUrl" : "https://github.com/apache/spark/pull/31480#pullrequestreview-612901216",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8d5ed328-3ab9-46ab-9cab-1ee526281dc4",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "I think metrics updates implicitly require records inserting, otherwise, it's meaningless to update. So, shall we do `insertAll` here too? e.g.,\r\n\r\n```scala\r\ndef completionIterator(records: Iterator[Product2[K, V]]): Iterator[Product2[K, C]] = {\r\n  insertAll(records)\r\n  context.taskMetrics().incMemoryBytesSpilled(memoryBytesSpilled)\r\n  ...\r\n}\r\n```\r\n",
        "createdAt" : "2021-03-15T14:23:53Z",
        "updatedAt" : "2021-03-19T05:46:28Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "adf90381-722e-4df6-909f-db1ab794f4a6",
        "parentId" : "8d5ed328-3ab9-46ab-9cab-1ee526281dc4",
        "authorId" : "685f805b-e4fa-4f21-b066-58afcecf9ce6",
        "body" : "I am ok to add `records` here, but perfer another method name 'insertAllAndUpdateMetrics'",
        "createdAt" : "2021-03-16T01:52:07Z",
        "updatedAt" : "2021-03-19T05:46:28Z",
        "lastEditedBy" : "685f805b-e4fa-4f21-b066-58afcecf9ce6",
        "tags" : [
        ]
      },
      {
        "id" : "3e12f2e6-ff80-40a3-96b7-46bdaae2249d",
        "parentId" : "8d5ed328-3ab9-46ab-9cab-1ee526281dc4",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Yeah, I was thinking about the renaming too. Maybe, `insertAllWithMetricsUpdated`?",
        "createdAt" : "2021-03-16T02:47:30Z",
        "updatedAt" : "2021-03-19T05:46:28Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "d6991c55-4677-4fe4-a72a-ae706a4ae1b5",
        "parentId" : "8d5ed328-3ab9-46ab-9cab-1ee526281dc4",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Ah, I just saw you already renamed the method name. Should be good enough. You can ignore mine.",
        "createdAt" : "2021-03-16T06:56:09Z",
        "updatedAt" : "2021-03-19T05:46:28Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "83947ab67d22d62f17f774451fdd5d4f9616000a",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +683,687 @@    context.taskMetrics().incMemoryBytesSpilled(memoryBytesSpilled)\n    context.taskMetrics().incDiskBytesSpilled(diskBytesSpilled)\n    context.taskMetrics().incPeakExecutionMemory(peakMemoryUsedBytes)\n    // Use completion callback to stop sorter if task was finished/cancelled.\n    context.addTaskCompletionListener[Unit](_ => stop())"
  },
  {
    "id" : "c27e1610-b6fe-402c-9586-7bda462d93c6",
    "prId" : 25342,
    "prUrl" : "https://github.com/apache/spark/pull/25342#pullrequestreview-279939901",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5f2449c4-4e67-452d-9eeb-a9d5854620d7",
        "parentId" : null,
        "authorId" : "e913f9af-103d-40d4-976e-9aa8e12e7211",
        "body" : "can't that test just call `sorter.writePartitionedMapOutput(..., new LocalDiskMapOutputWriter(...))` ?  Anyway fine to leave it for the follow up jira.",
        "createdAt" : "2019-08-27T03:37:01Z",
        "updatedAt" : "2019-08-28T21:40:35Z",
        "lastEditedBy" : "e913f9af-103d-40d4-976e-9aa8e12e7211",
        "tags" : [
        ]
      }
    ],
    "commit" : "d4831577120f73b6bfe20ca887659566abcabb31",
    "line" : 28,
    "diffHunk" : "@@ -1,1 +674,678 @@\n  /**\n   * TODO(SPARK-28764): remove this, as this is only used by UnsafeRowSerializerSuite in the SQL\n   * project. We should figure out an alternative way to test that so that we can remove this\n   * otherwise unused code path."
  }
]