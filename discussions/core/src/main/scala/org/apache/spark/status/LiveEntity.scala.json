[
  {
    "id" : "036c5f2a-3e30-4f22-b62c-36fd1b84ff7f",
    "prId" : 31974,
    "prUrl" : "https://github.com/apache/spark/pull/31974#pullrequestreview-637057695",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a7110c2b-5048-47e3-a014-278f924e641c",
        "parentId" : null,
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "why do we need both host and hostPort?",
        "createdAt" : "2021-04-15T13:56:03Z",
        "updatedAt" : "2021-04-20T05:59:58Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "75992d8c-f0e9-4713-b28c-40477ba2a864",
        "parentId" : "a7110c2b-5048-47e3-a014-278f924e641c",
        "authorId" : "11f6c849-b5b9-4c65-990c-525cf572b913",
        "body" : "kept only hostPort removed Host",
        "createdAt" : "2021-04-15T19:31:52Z",
        "updatedAt" : "2021-04-20T05:59:58Z",
        "lastEditedBy" : "11f6c849-b5b9-4c65-990c-525cf572b913",
        "tags" : [
        ]
      }
    ],
    "commit" : "eadb6dab935b26ffcae4e85f027ae324b98536ba",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +920,924 @@    creationTime: Long) extends LiveEntity {\n\n  var hostPort: String = null\n  var isActive = true\n  var totalCores = 0"
  },
  {
    "id" : "635534d9-b172-42ad-8c69-39e756de10da",
    "prId" : 28061,
    "prUrl" : "https://github.com/apache/spark/pull/28061#pullrequestreview-387869598",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "58484479-9835-4b4b-95b0-881441b5216e",
        "parentId" : null,
        "authorId" : "3a227965-84e0-47cf-9974-11293764f028",
        "body" : "How about `executorMetrics.foreach(this.executorMetrics.compareAndUpdatePeakValues)`?",
        "createdAt" : "2020-04-05T22:42:18Z",
        "updatedAt" : "2020-04-05T23:06:35Z",
        "lastEditedBy" : "3a227965-84e0-47cf-9974-11293764f028",
        "tags" : [
        ]
      }
    ],
    "commit" : "bbea4f9f929db25d76eb5b137f7f0ae6eb5ea4e4",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +142,146 @@    executorMetrics.foreach {\n      this.executorMetrics.compareAndUpdatePeakValues\n    }\n    if (metrics != null) {\n      val old = this.metrics"
  },
  {
    "id" : "4413218c-9553-4f1f-a0ca-3da14affcb24",
    "prId" : 27038,
    "prUrl" : "https://github.com/apache/spark/pull/27038#pullrequestreview-337001876",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "32fdd0a7-477e-4e7b-83d3-81fc138480f8",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Would it be better if we add a config for this?",
        "createdAt" : "2019-12-29T08:14:09Z",
        "updatedAt" : "2019-12-30T22:16:49Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "a4ea2d4d-8b0a-48cb-bce7-8ac4411ecd87",
        "parentId" : "32fdd0a7-477e-4e7b-83d3-81fc138480f8",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "I'm not sure if a config here is useful. Will we want to set more or less elements of collection accumulators in api or ui? cc @cloud-fan ",
        "createdAt" : "2019-12-29T17:23:40Z",
        "updatedAt" : "2019-12-30T22:16:49Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "381a9b6c-f383-4912-968a-48f63ff357b6",
        "parentId" : "32fdd0a7-477e-4e7b-83d3-81fc138480f8",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Why don't we just use existing configuration `spark.sql.debug.maxToStringFields`?",
        "createdAt" : "2019-12-30T05:14:11Z",
        "updatedAt" : "2019-12-30T22:16:49Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "d698fd37-a5b2-4eba-8756-ed6b61a31d42",
        "parentId" : "32fdd0a7-477e-4e7b-83d3-81fc138480f8",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Oops, it's not only SQL.",
        "createdAt" : "2019-12-30T05:15:47Z",
        "updatedAt" : "2019-12-30T22:16:49Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "b5c61c39-acb4-4144-b31b-b74b23eff94e",
        "parentId" : "32fdd0a7-477e-4e7b-83d3-81fc138480f8",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "yea. :)",
        "createdAt" : "2019-12-30T05:16:34Z",
        "updatedAt" : "2019-12-30T22:16:49Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "c309de82d01f7b5d341a440522e499997772bdd5",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +636,640 @@      // takes much more memory (e.g. long => string of it) and cause OOM.\n      // So we only show first few elements.\n      if (list.size() > 5) {\n        list.asScala.take(5).mkString(\"[\", \",\", \",\" + \"... \" + (list.size() - 5) + \" more items]\")\n      } else {"
  },
  {
    "id" : "c7c85f5b-04c0-4deb-98b1-97db19f5071b",
    "prId" : 27038,
    "prUrl" : "https://github.com/apache/spark/pull/27038#pullrequestreview-337006447",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f3a17543-82c3-4861-a6c5-9ceb0900cca4",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Can we add a JIRA ID and why we only abbreviate this case alone?",
        "createdAt" : "2019-12-30T05:53:17Z",
        "updatedAt" : "2019-12-30T22:16:49Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "a5abe6ea-fd6c-4d98-93f7-2052c4e8a2cb",
        "parentId" : "f3a17543-82c3-4861-a6c5-9ceb0900cca4",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Yes. Good advice.",
        "createdAt" : "2019-12-30T06:06:48Z",
        "updatedAt" : "2019-12-30T22:16:49Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "c309de82d01f7b5d341a440522e499997772bdd5",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +632,636 @@\n  private def accuValuetoString(value: Any): String = value match {\n    case list: java.util.List[_] =>\n      // SPARK-30379: For collection accumulator, string representation might\n      // takes much more memory (e.g. long => string of it) and cause OOM."
  },
  {
    "id" : "607d0771-2e24-4b92-84ad-7ba21e4f683c",
    "prId" : 27038,
    "prUrl" : "https://github.com/apache/spark/pull/27038#pullrequestreview-337006400",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fd04c5fd-186e-4458-94be-9fd76631f46c",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Does 5 looks good enough in UI? I haven't tested it by myself but thought it's faster just to ask.",
        "createdAt" : "2019-12-30T05:56:10Z",
        "updatedAt" : "2019-12-30T22:16:49Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "d1182075-2f16-49ac-b00b-706f76333dd8",
        "parentId" : "fd04c5fd-186e-4458-94be-9fd76631f46c",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "This is a number for avoiding OOM issue. For UI, good or not might be case by case. Although it sounds okay to add a config, I feel it is not a big deal to adjust this number in UI and api. I suggest to leave it as 5 and if we need to configure it, we can add a config later.",
        "createdAt" : "2019-12-30T06:06:19Z",
        "updatedAt" : "2019-12-30T22:16:49Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "c309de82d01f7b5d341a440522e499997772bdd5",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +636,640 @@      // takes much more memory (e.g. long => string of it) and cause OOM.\n      // So we only show first few elements.\n      if (list.size() > 5) {\n        list.asScala.take(5).mkString(\"[\", \",\", \",\" + \"... \" + (list.size() - 5) + \" more items]\")\n      } else {"
  },
  {
    "id" : "37c8b7ae-17fa-4f32-a3bd-60a5c74c905a",
    "prId" : 27038,
    "prUrl" : "https://github.com/apache/spark/pull/27038#pullrequestreview-337253282",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5d324753-2fef-48d2-ac74-bf59e1721070",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Just found that in UI, the tasks of Stage tab shows `update` too. So we also need to truncate update value.",
        "createdAt" : "2019-12-30T22:08:31Z",
        "updatedAt" : "2019-12-30T22:16:49Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "762cef69-e471-4268-86ff-efeb6c94b5a1",
        "parentId" : "5d324753-2fef-48d2-ac74-bf59e1721070",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Shown in the pr description.",
        "createdAt" : "2019-12-30T22:10:46Z",
        "updatedAt" : "2019-12-30T22:16:49Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "c309de82d01f7b5d341a440522e499997772bdd5",
    "line" : 38,
    "diffHunk" : "@@ -1,1 +655,659 @@          acc.id,\n          acc.name.map(weakIntern).orNull,\n          acc.update.map(accuValuetoString),\n          acc.value.map(accuValuetoString).orNull)\n      }"
  },
  {
    "id" : "da26536d-91a1-4795-a37c-2751915ff3c9",
    "prId" : 27026,
    "prUrl" : "https://github.com/apache/spark/pull/27026#pullrequestreview-359392125",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7de26082-0a63-474a-b4e9-b117d394b0b5",
        "parentId" : null,
        "authorId" : "51be1be0-7c72-4f32-a21f-91b1707ec363",
        "body" : "vars are not very cool in scala... how about something like this?\r\n\r\n```\r\nval (duration, executorRunTime) = if (info.finished) {\r\n      (info.duration, metrics.executorRunTime)\r\n    } else {\r\n      val timeRunning = info.timeRunning(lastUpdateTime.getOrElse(System.currentTimeMillis()))\r\n      (timeRunning, timeRunning)\r\n    }\r\n```",
        "createdAt" : "2020-02-16T09:52:22Z",
        "updatedAt" : "2020-02-16T10:05:38Z",
        "lastEditedBy" : "51be1be0-7c72-4f32-a21f-91b1707ec363",
        "tags" : [
        ]
      }
    ],
    "commit" : "a63a4bb4ac36e98e45dc999cb50dcbf4e5ea6794",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +179,183 @@    var executorRunTime: Long = 0\n\n    if (info.finished) {\n      duration = info.duration\n      executorRunTime = metrics.executorRunTime"
  },
  {
    "id" : "b453a9a5-8877-4f48-8704-3fda40a3ac6b",
    "prId" : 25943,
    "prUrl" : "https://github.com/apache/spark/pull/25943#pullrequestreview-301993806",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6ffa87e5-eacd-437b-9f21-12ae544215c8",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Could we add `completedIndices` and `completedStages` into new KVStore entity class, and restore LiveJob from both JobDataWrapper and new class? I guess you would like to avoid modifying JobDataWrapper, but I'd worry the state of instance differs from origin vs restored so would like to explore new option here.",
        "createdAt" : "2019-10-07T08:58:33Z",
        "updatedAt" : "2019-10-16T15:23:01Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "4f0bc73d-ccb6-40e9-aca7-318a2954119c",
        "parentId" : "6ffa87e5-eacd-437b-9f21-12ae544215c8",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "I don't mind adding these to JobDataWrapper as well - it could bring problems on backward compatibility though. It sounds me as tradeoff between \"possibly inaccurate\" vs \"no backward compatibility\".",
        "createdAt" : "2019-10-08T01:10:21Z",
        "updatedAt" : "2019-10-16T15:23:01Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "6e0cc4c6-0b14-41c6-9c06-de3fa40a2b52",
        "parentId" : "6ffa87e5-eacd-437b-9f21-12ae544215c8",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "I think adding `completedIndices` and `completedStages` to `JobDataWrapper` would be OK since `JobDataWrapper` is not under `api.v1` package ?\r\n\r\nAnd, I think correctness is aways the first thing we need to take care of.",
        "createdAt" : "2019-10-15T15:19:03Z",
        "updatedAt" : "2019-10-16T15:23:01Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "46715b3d2c52acd97fdca2e2034393f7e49e1b4f",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +75,79 @@  // Holds both the stage ID and the task index, packed into a single long value.\n  val completedIndices = new OpenHashSet[Long]()\n  // will only be set when recover LiveJob is needed.\n  var numCompletedIndices = 0\n"
  },
  {
    "id" : "723c1d97-61b7-40cd-8748-afc265c78390",
    "prId" : 25943,
    "prUrl" : "https://github.com/apache/spark/pull/25943#pullrequestreview-298501257",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "58832e45-d0c0-46f4-9cd4-9a73746ea00b",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "same here: Could we add `completedIndices` into new KVStore entity class, and restore LiveStage from both StageDataWrapper and new class? Adding it to the  `StageDataWrapper` is also fine for me.",
        "createdAt" : "2019-10-08T02:09:42Z",
        "updatedAt" : "2019-10-16T15:23:01Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "46715b3d2c52acd97fdca2e2034393f7e49e1b4f",
    "line" : 103,
    "diffHunk" : "@@ -1,1 +375,379 @@  var failedTasks = 0\n  val completedIndices = new OpenHashSet[Int]()\n  // will only be set when recover LiveStage is needed.\n  var numCompletedIndices = 0\n"
  },
  {
    "id" : "6966ffb5-6fea-4c82-8b20-afab54834ccd",
    "prId" : 25779,
    "prUrl" : "https://github.com/apache/spark/pull/25779#pullrequestreview-298239104",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "89fe4473-ca2e-4baf-b15d-5b603efea687",
        "parentId" : null,
        "authorId" : "e913f9af-103d-40d4-976e-9aa8e12e7211",
        "body" : "A stage may have been submitted before the RDD was persisted at all, then another stage submitted after the RDD is persisted, so its not actually immutable.  You wouldn't properly capture that here.\r\n\r\n(probably not the optimal thing for the user to do, but I've seen weirder things ...)",
        "createdAt" : "2019-10-04T21:22:45Z",
        "updatedAt" : "2019-10-04T22:16:52Z",
        "lastEditedBy" : "e913f9af-103d-40d4-976e-9aa8e12e7211",
        "tags" : [
        ]
      },
      {
        "id" : "985b305b-03fd-4632-8f9e-5609593ea000",
        "parentId" : "89fe4473-ca2e-4baf-b15d-5b603efea687",
        "authorId" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "body" : "Actually that works fine, because the listener does not track RDDs that are not persisted; so there wouldn't be a live RDD for the first stage in your example; it would be created when the second stage is submitted, and at that point the storage level cannot be changed further.\r\n\r\nI'll update the comment (maybe even add a unit test).",
        "createdAt" : "2019-10-04T22:03:53Z",
        "updatedAt" : "2019-10-04T22:16:52Z",
        "lastEditedBy" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "tags" : [
        ]
      },
      {
        "id" : "9818d759-9441-4334-b518-1b060a81654f",
        "parentId" : "89fe4473-ca2e-4baf-b15d-5b603efea687",
        "authorId" : "e913f9af-103d-40d4-976e-9aa8e12e7211",
        "body" : "makes sense, thanks for adding the test too.",
        "createdAt" : "2019-10-07T16:02:00Z",
        "updatedAt" : "2019-10-07T16:02:05Z",
        "lastEditedBy" : "e913f9af-103d-40d4-976e-9aa8e12e7211",
        "tags" : [
        ]
      }
    ],
    "commit" : "b95baec2297254707fecf6a819ecd86b088a68d9",
    "line" : 52,
    "diffHunk" : "@@ -1,1 +535,539 @@ * RDDs, this covers the case where an early stage is run on the unpersisted RDD, and a later stage\n * it started after the RDD is marked for caching.\n */\nprivate class LiveRDD(val info: RDDInfo, storageLevel: StorageLevel) extends LiveEntity {\n"
  },
  {
    "id" : "6264f9ad-f636-4d99-9fe9-5caf3d379749",
    "prId" : 25779,
    "prUrl" : "https://github.com/apache/spark/pull/25779#pullrequestreview-297737471",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "226fafb5-acc9-4ea1-a39f-bb6268506f56",
        "parentId" : null,
        "authorId" : "e913f9af-103d-40d4-976e-9aa8e12e7211",
        "body" : "It would help me a bit if this was called 'requestedStorageLevel' and in RDDPartitionInfo it was called 'effectiveStorageLevel'.  I guess its not worth changing RDDPartitionInfo, but maybe just a comment along those lines.",
        "createdAt" : "2019-10-04T21:25:03Z",
        "updatedAt" : "2019-10-04T22:16:52Z",
        "lastEditedBy" : "e913f9af-103d-40d4-976e-9aa8e12e7211",
        "tags" : [
        ]
      },
      {
        "id" : "7dcf74d6-3d13-4e3e-8105-2ced92ac1c39",
        "parentId" : "226fafb5-acc9-4ea1-a39f-bb6268506f56",
        "authorId" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "body" : "Sure.",
        "createdAt" : "2019-10-04T22:04:08Z",
        "updatedAt" : "2019-10-04T22:16:52Z",
        "lastEditedBy" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "tags" : [
        ]
      }
    ],
    "commit" : "b95baec2297254707fecf6a819ecd86b088a68d9",
    "line" : 20,
    "diffHunk" : "@@ -1,1 +465,469 @@ * by the application.\n */\nprivate class LiveRDDPartition(val blockName: String, rddLevel: StorageLevel) {\n\n  import LiveEntityHelpers._"
  }
]