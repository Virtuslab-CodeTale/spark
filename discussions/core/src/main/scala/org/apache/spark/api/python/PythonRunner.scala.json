[
  {
    "id" : "6b1f8548-a67d-42ee-be42-b6d8af3f9c42",
    "prId" : 28085,
    "prUrl" : "https://github.com/apache/spark/pull/28085#pullrequestreview-392955643",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5a01493b-0a1a-4549-b8e6-8213570f3c04",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Can I ask why we divide Python worker's memory by the number of cores? ",
        "createdAt" : "2020-04-13T06:03:06Z",
        "updatedAt" : "2020-04-22T14:03:45Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "db2b70c8-7323-4541-b035-1bdbaeb56104",
        "parentId" : "5a01493b-0a1a-4549-b8e6-8213570f3c04",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "this was preexisting functionality (https://github.com/apache/spark/pull/28085/files#diff-6bc32eb2bef385137d7c16fc2c75e8b4L88) , I just changed to make it work with the resource profiles. From my understanding its just splitting the memory equally because you get a python worker per task. I thought the comment did decent job of relaying that, but maybe we need to clarify?    Thinking about this some more, there might actually be a bug here (was here before my changes) if the spark.task.cpus is > 1 because the max tasks you get couldn't be equal to number of cores, so your are splitting the memory to much.  I can file a separate jira to look at that though.",
        "createdAt" : "2020-04-13T16:32:03Z",
        "updatedAt" : "2020-04-22T14:03:45Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "f0b3578d-1c5f-478a-8212-394cab50c3aa",
        "parentId" : "5a01493b-0a1a-4549-b8e6-8213570f3c04",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Yes ... let's just loop this in a separate JIRA if you don't mind.\r\n\r\nI wary of this Python memory configuration which needed a bunch of followups and separate investigation (see SPARK-25004) when it was first added. This configuration also made Spark 2.4.0 useless on Windows (SPARK-26080).\r\n\r\nThis configuration is even incomplete and conflicts with 'spark.python.worker.memory' configuration, see also SPARK-26679.\r\n\r\n",
        "createdAt" : "2020-04-14T00:51:29Z",
        "updatedAt" : "2020-04-22T14:03:45Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "799f8119-fe8a-4f35-8bbc-01c541c370a5",
        "parentId" : "5a01493b-0a1a-4549-b8e6-8213570f3c04",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "https://issues.apache.org/jira/browse/SPARK-31444",
        "createdAt" : "2020-04-14T14:00:26Z",
        "updatedAt" : "2020-04-22T14:03:45Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "354fb0c09ff9ed6be985f0ca7a8b6fae835303a2",
    "line" : 25,
    "diffHunk" : "@@ -1,1 +108,112 @@  // number of concurrent tasks, which is determined by the number of cores in this executor.\n  private def getWorkerMemoryMb(mem: Option[Long], cores: Int): Option[Long] = {\n    mem.map(_ / cores)\n  }\n"
  },
  {
    "id" : "deb4fc86-e5a2-4ee9-8c85-e016cf60cdb6",
    "prId" : 27951,
    "prUrl" : "https://github.com/apache/spark/pull/27951#pullrequestreview-377112442",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b5a6b5f2-d6ef-4c92-bdb4-55767c0691d9",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "You probably don't even need these types here and below, but it won't matter",
        "createdAt" : "2020-03-18T18:03:39Z",
        "updatedAt" : "2020-03-18T18:03:41Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      }
    ],
    "commit" : "baab9655153a1c20bfc8831c54dbf8b19e644e7d",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +421,425 @@            result = BarrierTaskContextMessageProtocol.BARRIER_RESULT_SUCCESS\n          case BarrierTaskContextMessageProtocol.ALL_GATHER_FUNCTION =>\n            val messages: Array[String] = context.asInstanceOf[BarrierTaskContext].allGather(\n              message\n            )"
  },
  {
    "id" : "2783ca03-a833-4ca8-80cc-92fb71cb9f30",
    "prId" : 26953,
    "prUrl" : "https://github.com/apache/spark/pull/26953#pullrequestreview-578856679",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "51331037-a3cd-48fb-86d0-a979868dd513",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Have a couple of questions. \r\n\r\n- Is it possible to merge it with `SQLMetric`? It would be nicer if UI shows it as well.\r\n- Is it possible to integrate with existing Python profiler? The current read time isn't purely Python execution time. It includes socket IO time which can potentially be large.",
        "createdAt" : "2020-01-17T06:27:11Z",
        "updatedAt" : "2020-11-13T09:19:59Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "a0dbe594-513c-46ad-982e-f48c6221cfc8",
        "parentId" : "51331037-a3cd-48fb-86d0-a979868dd513",
        "authorId" : "acf5aefc-4c46-451e-a28d-492ceaffd160",
        "body" : "- I like the idea of adding SQLMetrics for Python UDF instrumentation and use them in the WEBUI. However, I think the work would rather fit for a separate JIRA/PR. The implementation details and the overhead of SQLMetrics are different from Dropwizard-based metrics, so probably we would like to have only a limited number of SQLMetrics instrumenting task activities in this area. Also the implementation of SQLMetrics for `[[PythonUDF]]` execution may require some important changes to the current plan evaluation code.\r\n\r\n- It is indeed the case that the “read time from worker” which is exposed to the users via the dropwizard library as “FetchResultsTimeFromWorkers” contains both socket I/O + deserialization time and Python UDF execution time. Measuring on the Python side could allow to separate the 2 time components, however currently I don’t see how to make a lightweight implementation for that. Python profiler has the possibility to measure on the Python side as you mentioned, but I see its usage more for debugging, while the proposed instrumentation is lightweight and intended to be used for production use cases too. Maybe future work can address this case if there is need?\r\n",
        "createdAt" : "2020-01-22T16:07:06Z",
        "updatedAt" : "2020-11-13T09:19:59Z",
        "lastEditedBy" : "acf5aefc-4c46-451e-a28d-492ceaffd160",
        "tags" : [
        ]
      },
      {
        "id" : "429d813b-f4c4-4813-91a1-a1b68b8e7c26",
        "parentId" : "51331037-a3cd-48fb-86d0-a979868dd513",
        "authorId" : "acf5aefc-4c46-451e-a28d-492ceaffd160",
        "body" : "@HyukjinKwon I have finally managed to work on your suggestion to instrument Python execution using SQL Metrics, so that users can see the metrics via the WebUI. See [SPARK-34265]. I imagine that I could later refactor the work on this PR based on that.",
        "createdAt" : "2021-01-27T16:06:56Z",
        "updatedAt" : "2021-01-27T16:06:56Z",
        "lastEditedBy" : "acf5aefc-4c46-451e-a28d-492ceaffd160",
        "tags" : [
        ]
      },
      {
        "id" : "974e71cf-70bb-4896-b700-ab5d98d00608",
        "parentId" : "51331037-a3cd-48fb-86d0-a979868dd513",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "PR link: https://github.com/apache/spark/pull/31367",
        "createdAt" : "2021-01-29T00:39:34Z",
        "updatedAt" : "2021-01-29T00:39:34Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      }
    ],
    "commit" : "750b973b8e810572c7a15e5251d56d6c58066661",
    "line" : 30,
    "diffHunk" : "@@ -1,1 +479,483 @@        nextObj = read()\n        val deltaTime = System.nanoTime()-startTime\n        PythonMetrics.incFromWorkerReadTime(deltaTime)\n        hasNext\n      } else {"
  },
  {
    "id" : "5ce4301e-f6a5-482e-b6a9-fe1e03957faf",
    "prId" : 25545,
    "prUrl" : "https://github.com/apache/spark/pull/25545#pullrequestreview-280977955",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "eaff1b3d-d820-48a0-a5e9-7496c1bb1c24",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Can we avoid to say it targets pandas/numpy? This PR I guess then targets to fix a general issue rather than one specific issue found in numpy.",
        "createdAt" : "2019-08-25T02:15:32Z",
        "updatedAt" : "2019-08-25T02:15:33Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "5f441f9d-3738-4fef-821d-7f805776f3c4",
        "parentId" : "eaff1b3d-d820-48a0-a5e9-7496c1bb1c24",
        "authorId" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "body" : "The first line of the comment explains the fix, the second gives context about when this happens.\r\n\r\nIf you would like to clarify further, please open a PR against my branch.",
        "createdAt" : "2019-08-26T15:58:41Z",
        "updatedAt" : "2019-08-26T15:58:41Z",
        "lastEditedBy" : "0fc9f1bc-0097-451e-915f-52da69a366f3",
        "tags" : [
        ]
      },
      {
        "id" : "cd894c8e-3ed1-450c-a891-276770352927",
        "parentId" : "eaff1b3d-d820-48a0-a5e9-7496c1bb1c24",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Can we clarify here? What does \"limit the OpenMP thread pool to the number of cores assigned to this executor\" means in general PySpark jobs?",
        "createdAt" : "2019-08-28T16:32:18Z",
        "updatedAt" : "2019-08-28T16:32:18Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "63cbde95-2bbc-4ce8-9c12-610e58031401",
        "parentId" : "eaff1b3d-d820-48a0-a5e9-7496c1bb1c24",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "I think it's pretty straightforward. This env variable controls how many threads OpenMP uses, and it shouldn't be more than the number of cores the executor is allowed to use, of course. However its default, unset, will sometimes use more than the allowed number of cores. So it is set to the number of allowed cores if not set.\r\n\r\nI agree it's broader than numpy. However the change to Pyspark would mostly improve the situation for numpy users (by extension, pandas) specifically. I don't think it matters so much; we could remove the commentary and point to the JIRA or something.",
        "createdAt" : "2019-08-28T16:53:18Z",
        "updatedAt" : "2019-08-28T16:53:18Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "60bd7041-5f7f-4509-bf3a-15213955a08b",
        "parentId" : "eaff1b3d-d820-48a0-a5e9-7496c1bb1c24",
        "authorId" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "body" : "I think the comment sounds fine. Even though the issue is broader than numpy, it seems like it's the biggest offender and it's good to have a little background info.",
        "createdAt" : "2019-08-28T17:12:28Z",
        "updatedAt" : "2019-08-28T17:26:45Z",
        "lastEditedBy" : "0c293c45-22a6-4358-8a40-adbf7c470575",
        "tags" : [
        ]
      }
    ],
    "commit" : "4655bce47949dc2777c3e9dd3c764d8582002ca8",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +111,115 @@      // SPARK-28843: limit the OpenMP thread pool to the number of cores assigned to this executor\n      // this avoids high memory consumption with pandas/numpy because of a large OpenMP thread pool\n      // see https://github.com/numpy/numpy/issues/10455\n      conf.getOption(\"spark.executor.cores\").foreach(envVars.put(\"OMP_NUM_THREADS\", _))\n    }"
  },
  {
    "id" : "982d3294-2738-446d-a990-c037c26699e2",
    "prId" : 25545,
    "prUrl" : "https://github.com/apache/spark/pull/25545#pullrequestreview-280970189",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c572f868-0e00-46e0-a8ec-e470bbd952ed",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Seems `spark.executor.cores` can mean different things, for instance,\r\n\r\n> spark.executor.cores | 1 in YARN mode, all the available cores on the worker in standalone\r\n\r\nCan we investigate such cases?\r\n",
        "createdAt" : "2019-08-26T00:53:52Z",
        "updatedAt" : "2019-08-26T00:53:52Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "5f3c34d9-86bd-4444-abd4-f531b3693163",
        "parentId" : "c572f868-0e00-46e0-a8ec-e470bbd952ed",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "1 is just the default in YARN; the meaning is the same everywhere.\r\n\r\nLet me break it down further, to illustrate. All this is doing is ensuring that a process isn't using more cores than it should, which ought to always be a good thing. (And it saves memory along the way.) Suppose there's a 16-core machine.\r\n\r\nCase 1: JVM Spark\r\na) `spark.executor.cores` = 16. There is one JVM using all cores.\r\nb) `spark.executor.cores` = 4. There are (up to) 4 JVMs using 4 cores each.\r\n\r\nCase 2: Pyspark\r\na) `spark.executor.cores` = 16. There are 16 Python processes\r\nb) `spark.executor.cores` = 4. There are still 16 Python processes.\r\n\r\nIn case 1a, imagine using MLlib that uses OpenBLAS or MKL. By default, OpenMP will use all 16 cores now. This is fine, and does not change with this change. In case 1b, each JVM will use 16 cores, so OpenMP will attempt to use 64 total threads (to my understanding here), right now. This change would make this use 16 cores (4 x 4). That's better.\r\n\r\nIn case 2a and 2b, imagine using numpy. 256 threads will be used in total on the machine! That's bad; it's a little slower because of all the context switching, but also uses more memory. This change does not however help case 2a. It does help 2b, where 'at least' only 64 threads are started.\r\n\r\nThe more aggressive change would be to set the default to 1, always, for Pyspark as well as this matches the execution better. However, this is at least a more conservative step to merely cap it at the number of allocated executor cores.\r\n\r\n\r\nYes, the situation isn't as bad if the executor isn't actually fully utilized, but, I don't think we should optimize for that case? at least, this more conservative change still errs on the side of over-committing the cores at the cost of memory, just not nearly as extremely as the default.",
        "createdAt" : "2019-08-26T14:22:58Z",
        "updatedAt" : "2019-08-26T14:22:58Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "83339a90-6004-405a-8190-05f03ad7079d",
        "parentId" : "c572f868-0e00-46e0-a8ec-e470bbd952ed",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Yes, so the problem here is that the number is somewhat a bit arbitrary. It should be yes 1 or up to users control.\r\n\r\nAm I understanding correctly that, say in 16 cores machine,\r\n\r\nspark.executor.cores=1-> all cores, OMP_NUM_THREADS=1 -> one process with 1 thread\r\nspark.executor.cores=2 -> two cores, OMP_NUM_THREADS=2 -> one process with 2 threads\r\nspark.executor.cores=3 -> three cores, OMP_NUM_THREADS=3 -> one process with 3 threads\r\n...\r\nspark.executor.cores=16-> all cores, OMP_NUM_THREADS=16 -> one process with 16 threads\r\n\r\nis an expected behaviour?\r\n\r\n\r\nUDF or RDD APIs cannot be optimized because we don't know what's users are going to do in in this box. And here we're putting an assumption on that.\r\n\r\n",
        "createdAt" : "2019-08-28T16:39:16Z",
        "updatedAt" : "2019-08-28T16:39:16Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "32fe8f60-0cef-4ab7-bfbf-309eb4e47c48",
        "parentId" : "c572f868-0e00-46e0-a8ec-e470bbd952ed",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Also, to be clear, this is already configurable. If we're going to put some default number for all general PySpark computation, please pease let's clarify what it means in PR description and upgrade migration guide.",
        "createdAt" : "2019-08-28T16:40:39Z",
        "updatedAt" : "2019-08-28T16:40:39Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "ab00d753-87d7-4305-b0fa-530fd356fa13",
        "parentId" : "c572f868-0e00-46e0-a8ec-e470bbd952ed",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "It remains up to users to control if desired. It's preventing it from using a pool that is clearly too big: nothing should cause a job to try to use more cores than the _executor_ is allowed to. This is the most conservative possible change that just makes a clearly wrong setting better in some cases. It is indeed likely in some cases that a lower setting is even better, but, we don't force that here as it 'depends' a bit.\r\n\r\nSee my post above. For Pyspark for example, with 4 x 4-core executors on a 16-core machines, in Pyspark, you get _256_ threads before, and 64 after. Even 64 is too high, but better. It doesn't help a 1 x 16-core executor in Pyspark.\r\n\r\nAre you arguing for the 'more aggressive' change, to force it to 1 if unset in Pyspark? that's also coherent. But then I don't see why the less aggressive change is a problem.",
        "createdAt" : "2019-08-28T16:58:07Z",
        "updatedAt" : "2019-08-28T16:58:07Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      }
    ],
    "commit" : "4655bce47949dc2777c3e9dd3c764d8582002ca8",
    "line" : 9,
    "diffHunk" : "@@ -1,1 +112,116 @@      // this avoids high memory consumption with pandas/numpy because of a large OpenMP thread pool\n      // see https://github.com/numpy/numpy/issues/10455\n      conf.getOption(\"spark.executor.cores\").foreach(envVars.put(\"OMP_NUM_THREADS\", _))\n    }\n    envVars.put(\"SPARK_LOCAL_DIRS\", localdir) // it's also used in monitor thread"
  }
]