[
  {
    "id" : "6b1f8548-a67d-42ee-be42-b6d8af3f9c42",
    "prId" : 28085,
    "prUrl" : "https://github.com/apache/spark/pull/28085#pullrequestreview-392955643",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5a01493b-0a1a-4549-b8e6-8213570f3c04",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Can I ask why we divide Python worker's memory by the number of cores? ",
        "createdAt" : "2020-04-13T06:03:06Z",
        "updatedAt" : "2020-04-22T14:03:45Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "db2b70c8-7323-4541-b035-1bdbaeb56104",
        "parentId" : "5a01493b-0a1a-4549-b8e6-8213570f3c04",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "this was preexisting functionality (https://github.com/apache/spark/pull/28085/files#diff-6bc32eb2bef385137d7c16fc2c75e8b4L88) , I just changed to make it work with the resource profiles. From my understanding its just splitting the memory equally because you get a python worker per task. I thought the comment did decent job of relaying that, but maybe we need to clarify?    Thinking about this some more, there might actually be a bug here (was here before my changes) if the spark.task.cpus is > 1 because the max tasks you get couldn't be equal to number of cores, so your are splitting the memory to much.  I can file a separate jira to look at that though.",
        "createdAt" : "2020-04-13T16:32:03Z",
        "updatedAt" : "2020-04-22T14:03:45Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "f0b3578d-1c5f-478a-8212-394cab50c3aa",
        "parentId" : "5a01493b-0a1a-4549-b8e6-8213570f3c04",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Yes ... let's just loop this in a separate JIRA if you don't mind.\r\n\r\nI wary of this Python memory configuration which needed a bunch of followups and separate investigation (see SPARK-25004) when it was first added. This configuration also made Spark 2.4.0 useless on Windows (SPARK-26080).\r\n\r\nThis configuration is even incomplete and conflicts with 'spark.python.worker.memory' configuration, see also SPARK-26679.\r\n\r\n",
        "createdAt" : "2020-04-14T00:51:29Z",
        "updatedAt" : "2020-04-22T14:03:45Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "799f8119-fe8a-4f35-8bbc-01c541c370a5",
        "parentId" : "5a01493b-0a1a-4549-b8e6-8213570f3c04",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "https://issues.apache.org/jira/browse/SPARK-31444",
        "createdAt" : "2020-04-14T14:00:26Z",
        "updatedAt" : "2020-04-22T14:03:45Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "354fb0c09ff9ed6be985f0ca7a8b6fae835303a2",
    "line" : 25,
    "diffHunk" : "@@ -1,1 +108,112 @@  // number of concurrent tasks, which is determined by the number of cores in this executor.\n  private def getWorkerMemoryMb(mem: Option[Long], cores: Int): Option[Long] = {\n    mem.map(_ / cores)\n  }\n"
  },
  {
    "id" : "deb4fc86-e5a2-4ee9-8c85-e016cf60cdb6",
    "prId" : 27951,
    "prUrl" : "https://github.com/apache/spark/pull/27951#pullrequestreview-377112442",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b5a6b5f2-d6ef-4c92-bdb4-55767c0691d9",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "You probably don't even need these types here and below, but it won't matter",
        "createdAt" : "2020-03-18T18:03:39Z",
        "updatedAt" : "2020-03-18T18:03:41Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      }
    ],
    "commit" : "baab9655153a1c20bfc8831c54dbf8b19e644e7d",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +421,425 @@            result = BarrierTaskContextMessageProtocol.BARRIER_RESULT_SUCCESS\n          case BarrierTaskContextMessageProtocol.ALL_GATHER_FUNCTION =>\n            val messages: Array[String] = context.asInstanceOf[BarrierTaskContext].allGather(\n              message\n            )"
  }
]