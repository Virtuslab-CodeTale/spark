[
  {
    "id" : "15c8a658-dea0-4955-a3f3-6a4d3fb4b62e",
    "prId" : 33251,
    "prUrl" : "https://github.com/apache/spark/pull/33251#pullrequestreview-703342839",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "594199f1-aafa-4ada-9731-d93569c82cd0",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "I wasn't clear why this had to change?",
        "createdAt" : "2021-07-09T18:13:49Z",
        "updatedAt" : "2021-07-09T18:15:27Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "81bd8e32-04f0-4161-9167-aa3de0aa7532",
        "parentId" : "594199f1-aafa-4ada-9731-d93569c82cd0",
        "authorId" : "3f417633-f870-4fee-b580-ac9ce146075f",
        "body" : "The test was failing with `NullPointerException` cause the `SparkEnv.get` wasn't initialized.\r\nConsidering that the `RemoteBlockDownloadFileManager` accepts an instance of `BlockManager` which already has `SecurityManager` it seemed logical to just pass the `encryptionKey` too so it didn't rely on the static env in the lazy val.\r\n\r\nAs I understand the alternative would be to construct a mock environment and call the static `SparkEnv.set` and I see that this approach is taken in other places, but I do not understand how it works reliably without removing it after use - it's just left there. Hence I decided to remove the reliance on the static env so it's easier to test.",
        "createdAt" : "2021-07-09T19:38:11Z",
        "updatedAt" : "2021-07-09T19:55:52Z",
        "lastEditedBy" : "3f417633-f870-4fee-b580-ac9ce146075f",
        "tags" : [
        ]
      }
    ],
    "commit" : "74c374fc45279f378d965d421298864b2ca6794d",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +256,260 @@  // Exposed for test\n  private[storage] val remoteBlockTempFileManager =\n    new BlockManager.RemoteBlockDownloadFileManager(\n      this,\n      securityManager.getIOEncryptionKey())"
  },
  {
    "id" : "e3249ebb-b1ad-4188-ab8d-4803cc86ceb6",
    "prId" : 33078,
    "prUrl" : "https://github.com/apache/spark/pull/33078#pullrequestreview-698737470",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7c99b7b3-c764-4558-a287-28bcae65bfb4",
        "parentId" : null,
        "authorId" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "body" : "Is there a UT that verifies the message when pushbased shuffle is enabled?",
        "createdAt" : "2021-06-29T06:58:04Z",
        "updatedAt" : "2021-06-29T06:58:51Z",
        "lastEditedBy" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "tags" : [
        ]
      },
      {
        "id" : "cd182bc3-c1a1-4f97-b9b9-5da1ffc30f2c",
        "parentId" : "7c99b7b3-c764-4558-a287-28bcae65bfb4",
        "authorId" : "e942fa3e-a5b9-4a9f-b9a5-a1afd3c29b2c",
        "body" : "Added UT.",
        "createdAt" : "2021-07-05T01:32:43Z",
        "updatedAt" : "2021-07-05T01:32:43Z",
        "lastEditedBy" : "e942fa3e-a5b9-4a9f-b9a5-a1afd3c29b2c",
        "tags" : [
        ]
      }
    ],
    "commit" : "53109918cbdbdba2fe79f38a991c171efec7e85f",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +544,548 @@      } else {\n        shuffleManager.getClass.getName\n      }\n    val shuffleConfig = new ExecutorShuffleInfo(\n      diskBlockManager.localDirsString,"
  },
  {
    "id" : "410e8a54-df98-4e65-b460-af8bd80215a7",
    "prId" : 30492,
    "prUrl" : "https://github.com/apache/spark/pull/30492#pullrequestreview-541269445",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "992abbb5-74a3-4e46-b524-d0899b10fd47",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Can you explain the reasoning here? So once any executor cannot get local shuffle block, it will always try to read from fallback storage?",
        "createdAt" : "2020-11-26T18:04:54Z",
        "updatedAt" : "2020-11-30T19:56:40Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "326d2ed0-92f3-4510-8837-e323d93e2c88",
        "parentId" : "992abbb5-74a3-4e46-b524-d0899b10fd47",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Yes. If `STORAGE_DECOMMISSION_FALLBACK_STORAGE_PATH` is defined, it tried to fallback. Currently, it happens when `FALLBACK_BLOCK_MANAGER_ID` has the blocks.",
        "createdAt" : "2020-11-27T01:32:30Z",
        "updatedAt" : "2020-11-30T19:56:40Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "98a8d4e4-a403-44e9-a90a-6e64a5ce50b4",
        "parentId" : "992abbb5-74a3-4e46-b524-d0899b10fd47",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Is there any check if `FALLBACK_BLOCK_MANAGER_ID` has the blocks before calling `FallbackStorage.read`? Looks like if `getBlockData` throws `IOException`, the block manager will just go to read from fallback storage.",
        "createdAt" : "2020-11-27T01:43:57Z",
        "updatedAt" : "2020-11-30T19:56:40Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "fa2dbdaf-548f-4c89-a885-33a8f66c288e",
        "parentId" : "992abbb5-74a3-4e46-b524-d0899b10fd47",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "It's already `IOException` and `FallbackStorage.read` throws `IOException` for non-exist files and it's fine and legitimate in the `WorkerDecomission` context, @viirya . Please note the following.\r\n- The whole Worker decommission (including `shuffle and rdd storage decommission`) is designed as a best-effort approach.\r\n- It's because the main use case is K8s graceful shutdown with the default period (`30s`). We can increase the period, but we cannot set it to the infinite value technically. It means executor hangs in case of disk full situation.\r\n- What we are aiming is to rescue data as much as possible, but `100%` is not guaranteed always.\r\n- Lastly, data block selection(shuffle or rdd) was a random-order from the beginning. It became worse when there are multiple shuffle across multiple executors.\r\n\r\nDue to the above reasons, we introduced `SPARK-33387 Support ordered shuffle block migration` to rescue as complete as possible. However, we can still easily imagine that multiple shuffles coexist on an executor in a skewed manner. While the other executor succeeds to complete the migration, the skewed executor may fail to complete the migration in the given grace period.",
        "createdAt" : "2020-11-29T00:17:37Z",
        "updatedAt" : "2020-11-30T19:56:40Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "17fc33dd-fb13-4ac0-a2ac-80db4580ca49",
        "parentId" : "992abbb5-74a3-4e46-b524-d0899b10fd47",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "I think this would probably fail quickly if the external storage doesn’t have the block right? So no need to do a separate check to see if the block is present, the failure message is good enough.",
        "createdAt" : "2020-11-30T19:03:18Z",
        "updatedAt" : "2020-11-30T19:56:40Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "03212650-a726-4d11-bb08-f59483046bb2",
        "parentId" : "992abbb5-74a3-4e46-b524-d0899b10fd47",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Yes, it does, @holdenk .",
        "createdAt" : "2020-11-30T19:58:24Z",
        "updatedAt" : "2020-11-30T19:58:24Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "bfa011d1-d6c7-4b06-8a3b-1ddec2d1c39a",
        "parentId" : "992abbb5-74a3-4e46-b524-d0899b10fd47",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Make sense. Thanks @holdenk ",
        "createdAt" : "2020-11-30T20:29:20Z",
        "updatedAt" : "2020-11-30T20:29:20Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "9c9317378eb61b40f766e41dd168d2189f6008c1",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +636,640 @@          } else {\n            throw e\n          }\n      }\n    } else {"
  },
  {
    "id" : "f4a877e9-723e-4fa5-a4a3-7b6e57e494a3",
    "prId" : 30492,
    "prUrl" : "https://github.com/apache/spark/pull/30492#pullrequestreview-541280945",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c2a7420c-1ed0-4d2d-8fe8-f62f68e8c898",
        "parentId" : null,
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Nit/ question: Could we move the if up as part of the case and avoid the need for explicit rethrow?",
        "createdAt" : "2020-11-30T19:01:14Z",
        "updatedAt" : "2020-11-30T19:56:40Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "fc009249-b435-4828-a944-76083cb5a379",
        "parentId" : "c2a7420c-1ed0-4d2d-8fe8-f62f68e8c898",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "It's because we should access the normal access path by default. We are able to use `Fallback` path only if the normal access path fail because `conf.get(config.STORAGE_DECOMMISSION_FALLBACK_STORAGE_PATH).isDefined` doesn't mean the fallback storage has the data.",
        "createdAt" : "2020-11-30T19:53:17Z",
        "updatedAt" : "2020-11-30T19:56:40Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "2073b9df-2c1f-45eb-9a26-b72671a73a70",
        "parentId" : "c2a7420c-1ed0-4d2d-8fe8-f62f68e8c898",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Please let me know if I misunderstand your comment.",
        "createdAt" : "2020-11-30T19:57:46Z",
        "updatedAt" : "2020-11-30T19:57:46Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "7872315b-b7e5-4223-9882-0224bcc768fc",
        "parentId" : "c2a7420c-1ed0-4d2d-8fe8-f62f68e8c898",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Yeah so I mean we still have a try/catch just the case statement has the if as part of it (eg “ Using if expressions in case statements”)",
        "createdAt" : "2020-11-30T20:46:33Z",
        "updatedAt" : "2020-11-30T20:46:33Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "9c9317378eb61b40f766e41dd168d2189f6008c1",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +631,635 @@        shuffleManager.shuffleBlockResolver.getBlockData(blockId)\n      } catch {\n        case e: IOException =>\n          if (conf.get(config.STORAGE_DECOMMISSION_FALLBACK_STORAGE_PATH).isDefined) {\n            FallbackStorage.read(conf, blockId)"
  },
  {
    "id" : "a6aa1bd3-0a08-4898-9797-60d82f546c0a",
    "prId" : 29817,
    "prUrl" : "https://github.com/apache/spark/pull/29817#pullrequestreview-503374099",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b2328842-e847-4021-bdfe-fab90e02e048",
        "parentId" : null,
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Why?",
        "createdAt" : "2020-09-23T23:12:05Z",
        "updatedAt" : "2020-10-22T13:56:45Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "aaa37f7c-1c25-4136-8c9f-d52fac222d40",
        "parentId" : "b2328842-e847-4021-bdfe-fab90e02e048",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "?",
        "createdAt" : "2020-09-24T02:43:29Z",
        "updatedAt" : "2020-10-22T13:56:45Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "90316493-aa30-43c1-9880-dedfc916b51c",
        "parentId" : "b2328842-e847-4021-bdfe-fab90e02e048",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Why did you make this change?",
        "createdAt" : "2020-09-24T16:02:18Z",
        "updatedAt" : "2020-10-22T13:56:45Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "6f73e3d9-e800-4057-83b6-794295f59813",
        "parentId" : "b2328842-e847-4021-bdfe-fab90e02e048",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "If I unsterstand your question correctly:\r\n\r\nWe didn't really change the `decommissionBlockManager`. The original `decommissionBlockManager` has been renamed to `decommissionSelf` to avoid the naming collision.",
        "createdAt" : "2020-09-28T08:39:26Z",
        "updatedAt" : "2020-10-22T13:56:45Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "af9d5c51-d04c-4a2e-906f-b35b5d205227",
        "parentId" : "b2328842-e847-4021-bdfe-fab90e02e048",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Makes sense, although maybe introducing a new name instead of changing the use of a previous function name would be easier for verifying.",
        "createdAt" : "2020-10-06T21:36:37Z",
        "updatedAt" : "2020-10-22T13:56:45Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "3c1e033089e392066425139d7ccb52cd501dcc31",
    "line" : 25,
    "diffHunk" : "@@ -1,1 +1811,1815 @@  }\n\n  def decommissionBlockManager(): Unit = storageEndpoint.ask(DecommissionBlockManager)\n\n  private[spark] def decommissionSelf(): Unit = synchronized {"
  },
  {
    "id" : "d0b10f18-6225-4e2c-a346-4a4b505d9825",
    "prId" : 29722,
    "prUrl" : "https://github.com/apache/spark/pull/29722#pullrequestreview-487423840",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5b2b550f-ccf9-4a7e-bc3c-b023b043177e",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "Didn't follow the need for the name change.",
        "createdAt" : "2020-09-12T01:56:01Z",
        "updatedAt" : "2020-09-16T06:57:24Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "18d6cc2f-30ee-4257-bfd6-a7c6da2a1bbf",
        "parentId" : "5b2b550f-ccf9-4a7e-bc3c-b023b043177e",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "It conflicts with the newly added \"decommissionBlockManager()\".",
        "createdAt" : "2020-09-14T05:26:36Z",
        "updatedAt" : "2020-09-16T06:57:24Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "290d2c08a1adfce3c0c4afe1cb8c9e214b25ea3d",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +1812,1816 @@  def decommissionBlockManager(): Unit = storageEndpoint.ask(DecommissionBlockManager)\n\n  private[spark] def decommissionSelf(): Unit = synchronized {\n    decommissioner match {\n      case None =>"
  },
  {
    "id" : "065a02de-5c7a-444f-b57d-3252ffabf494",
    "prId" : 29211,
    "prUrl" : "https://github.com/apache/spark/pull/29211#pullrequestreview-454592436",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f8e77ce9-7c0f-4b91-919b-476926dee874",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "What do you think of directly exposing the \"BlocksMigrated\" and \"BlocksToMigrate\" instead of a single boolean ? I think it will provide more debugging value than a single flag. \r\n\r\nie, instead of returning \"BlocksMigrated >= BlocksToMigrate\", return the two counters directly.\r\n\r\nOr maybe this may be a lot more work because you want to track this for both shuffle as well as persisted blocks.\r\n\r\n",
        "createdAt" : "2020-07-24T00:04:29Z",
        "updatedAt" : "2020-08-05T18:39:14Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "b020bf29-2d62-4efc-87fb-b44b40d437f1",
        "parentId" : "f8e77ce9-7c0f-4b91-919b-476926dee874",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "I'd really rather not. I think the level above us should only be concerned if the blocks are done migrating or not. If we expose that up a level the logic is going to get even more complicated cross-class boundaries.",
        "createdAt" : "2020-07-24T00:47:34Z",
        "updatedAt" : "2020-08-05T18:39:14Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "2572ca14-ec27-4e50-abde-2de9fc6b877f",
        "parentId" : "f8e77ce9-7c0f-4b91-919b-476926dee874",
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "Cool !. I am always anal about logging to a fault :-P ",
        "createdAt" : "2020-07-24T01:27:50Z",
        "updatedAt" : "2020-08-05T18:39:14Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      }
    ],
    "commit" : "e81c3fc2d0b8c28cd0aedf08a7ea1e86a153ae49",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +1827,1831 @@   *  If there are any tasks running since that time the boolean may be incorrect.\n   */\n  private[spark] def lastMigrationInfo(): (Long, Boolean) = {\n    decommissioner.map(_.lastMigrationInfo()).getOrElse((0, false))\n  }"
  },
  {
    "id" : "5a69db39-f1a7-4ed9-aea3-b54f1d334205",
    "prId" : 28911,
    "prUrl" : "https://github.com/apache/spark/pull/28911#pullrequestreview-478568555",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "85eb1bfd-8a06-4de5-ab25-d240a71de829",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Just a question. Why do we switch from `Java` Map to `Scala` Map in this PR? Is it required?",
        "createdAt" : "2020-08-29T22:10:23Z",
        "updatedAt" : "2020-09-01T06:44:19Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "777e4bc3-7f22-4217-a329-de72357ae86a",
        "parentId" : "85eb1bfd-8a06-4de5-ab25-d240a71de829",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "It's required by `fetchMultipleHostLocalBlocks`.  Actually, we could also do the Jave to Scala map conversion before calling  `fetchMultipleHostLocalBlocks` but leaving `Try[java.util.Map[String, Array[String]]] => Unit` unchanged.\r\n\r\nBut I decided to do the conversion here just because this class already imported `scala.collection.JavaConverters._`. It has no big difference to do the conversion here or there.",
        "createdAt" : "2020-08-31T12:25:32Z",
        "updatedAt" : "2020-09-01T06:44:20Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "a23ab1721b1225e0a95fb27660fe81847287c622",
    "line" : 30,
    "diffHunk" : "@@ -1,1 +138,142 @@      port: Int,\n      executorIds: Array[String])(\n      callback: Try[Map[String, Array[String]]] => Unit): Unit = {\n    val hostLocalDirsCompletable = new CompletableFuture[java.util.Map[String, Array[String]]]\n    blockStoreClient.getHostLocalDirs("
  },
  {
    "id" : "5c0453af-aa4a-47b8-a1bf-0bcf19bd058d",
    "prId" : 28911,
    "prUrl" : "https://github.com/apache/spark/pull/28911#pullrequestreview-479418592",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9c339821-696d-45d4-9bb3-4b2874864220",
        "parentId" : null,
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Why did the scope change here?",
        "createdAt" : "2020-08-31T17:11:50Z",
        "updatedAt" : "2020-09-01T06:44:20Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "e19aca12-6660-4e73-b57f-c33297a91567",
        "parentId" : "9c339821-696d-45d4-9bb3-4b2874864220",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "It's required by `val port = blockManager.externalShuffleServicePort` within `ShuffleBlockFetcherIterator`.",
        "createdAt" : "2020-09-01T05:52:27Z",
        "updatedAt" : "2020-09-01T06:44:20Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "a23ab1721b1225e0a95fb27660fe81847287c622",
    "line" : 60,
    "diffHunk" : "@@ -1,1 +212,216 @@  private val maxOffHeapMemory = memoryManager.maxOffHeapStorageMemory\n\n  private[spark] val externalShuffleServicePort = StorageUtils.externalShuffleServicePort(conf)\n\n  var blockManagerId: BlockManagerId = _"
  },
  {
    "id" : "3dbbf9af-2590-4d88-9bfe-4e0aaaba2ac8",
    "prId" : 28817,
    "prUrl" : "https://github.com/apache/spark/pull/28817#pullrequestreview-430179975",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "aaf7ab3b-bab7-4977-9b79-adc5824887cd",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "I think that the comment needs to be updated to reflect what the return Boolean indicates.",
        "createdAt" : "2020-06-13T23:37:09Z",
        "updatedAt" : "2020-06-14T01:11:18Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "6b7cb08c-8996-4263-a36e-e9b6c9b780f5",
        "parentId" : "aaf7ab3b-bab7-4977-9b79-adc5824887cd",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Good catch",
        "createdAt" : "2020-06-14T01:10:34Z",
        "updatedAt" : "2020-06-14T01:11:18Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "a2c055715ccf2992e399cef3768b1299c24d9a82",
    "line" : 172,
    "diffHunk" : "@@ -1,1 +1890,1894 @@   * Note: this does not delete the shuffle files in-case there is an in-progress fetch\n   * but rather shadows them.\n   * Requires an Indexed based shuffle resolver.\n   *\n   * @return true if we have not migrated all shuffle blocks, false otherwise."
  },
  {
    "id" : "f3cdaf76-c08a-418a-82b2-e26aa10be894",
    "prId" : 28817,
    "prUrl" : "https://github.com/apache/spark/pull/28817#pullrequestreview-430179582",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8a719425-87f4-43ac-8e50-d75ea91e7ff2",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "Just to make sure I am totally understanding this: You mean that the running tasks that were already running when the decommissioning was started at the executor ? Because, I think we refuse launching new tasks when the decommissioning has started, so the new blocks being written must be written by already running tasks. Did I get this right ?\r\n\r\nAlso, just to confirm I am still following along: I don't see this case handled in the existing BlockManagerSuite: I believe we are not testing writing new blocks while the decom/offload is in progress.",
        "createdAt" : "2020-06-13T23:41:06Z",
        "updatedAt" : "2020-06-14T01:11:18Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "6460a055-46ac-42f2-9aad-22cd784ad35f",
        "parentId" : "8a719425-87f4-43ac-8e50-d75ea91e7ff2",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "It is covered, you can verify this by disabling this logic and seeing the test fail (albiet you'll have to run the test a few times because it becomes a race condition). Look at the \"migrateDuring\" flag for details.",
        "createdAt" : "2020-06-14T00:57:30Z",
        "updatedAt" : "2020-06-14T01:11:18Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "a2c055715ccf2992e399cef3768b1299c24d9a82",
    "line" : 252,
    "diffHunk" : "@@ -1,1 +2052,2056 @@  private[spark] class BlockManagerDecommissionManager(conf: SparkConf) {\n    @volatile private var stopped = false\n    // Since running tasks can add more blocks this can change.\n    @volatile var allBlocksMigrated = false\n    var previousBlocksLeft = true"
  },
  {
    "id" : "69521326-9285-4a97-a641-27fcfd936fe8",
    "prId" : 28708,
    "prUrl" : "https://github.com/apache/spark/pull/28708#pullrequestreview-447738372",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "100e536c-f7ff-4c77-b75a-c5b88df8f517",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "Just for my understanding: why is decommissioner volatile now ? Would a comment help ?",
        "createdAt" : "2020-06-19T01:48:31Z",
        "updatedAt" : "2020-07-17T23:45:20Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "4dfed59e-4b07-4647-bff0-652f14d6c2f2",
        "parentId" : "100e536c-f7ff-4c77-b75a-c5b88df8f517",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "If we receive two decommissioning messages at the same time. we don't want to launch two of them.",
        "createdAt" : "2020-06-19T03:05:52Z",
        "updatedAt" : "2020-07-17T23:45:20Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "6495f264-30fc-4e28-8ddf-c264ae1b4de8",
        "parentId" : "100e536c-f7ff-4c77-b75a-c5b88df8f517",
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "Hmm, the creation to `decommissioner` is guarded by a lock. ",
        "createdAt" : "2020-06-19T03:50:44Z",
        "updatedAt" : "2020-07-17T23:45:20Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "45d246f8-a5d6-4e49-88bc-d69554c5218a",
        "parentId" : "100e536c-f7ff-4c77-b75a-c5b88df8f517",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "That's true. If we drop it we might also accept remove block puts after we've started decommissioning though. Depends on how much we want to avoid that.",
        "createdAt" : "2020-06-29T20:58:04Z",
        "updatedAt" : "2020-07-17T23:45:20Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "5b0bac83-6465-4406-b72c-481a6c0f01aa",
        "parentId" : "100e536c-f7ff-4c77-b75a-c5b88df8f517",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "I think I'm going to leave it volatile for now, I'd like to avoid remote block puts once we're in decommissioning because we depend on not getting new blocks except from tasks to figure out when it is safe to exit.",
        "createdAt" : "2020-07-14T02:20:31Z",
        "updatedAt" : "2020-07-17T23:45:20Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "8494bdd94285c7cc5a41e151da920710be7f4671",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +246,250 @@\n  // This is volatile since if it's defined we should not accept remote blocks.\n  @volatile private var decommissioner: Option[BlockManagerDecommissioner] = None\n\n  // A DownloadFileManager used to track all the files of remote blocks which are above the"
  },
  {
    "id" : "2a901300-cbf3-457a-a55b-eaeb872db352",
    "prId" : 28708,
    "prUrl" : "https://github.com/apache/spark/pull/28708#pullrequestreview-433770929",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3dfe86a2-a3aa-4156-b97c-86f56247b88d",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "Ah I see: That's why the migratableResolver is a lazy val. \r\n\r\nIs the blockId pertinent to log here ? I believe that the class cast exception would be thrown when the resolver is not of the correct type on the first access. Would a regular function help here, or perhaps the class cast exception check can be moved inside of the migratableResolver assignment ?",
        "createdAt" : "2020-06-19T01:56:37Z",
        "updatedAt" : "2020-07-17T23:45:20Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "6d25d548-5bc7-404a-8c00-1bcc67cf465e",
        "parentId" : "3dfe86a2-a3aa-4156-b97c-86f56247b88d",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Yeah were logging which kind of block we received. We only really care about the type but it's easier to just print the blockId its self.",
        "createdAt" : "2020-06-19T03:07:52Z",
        "updatedAt" : "2020-07-17T23:45:20Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "8494bdd94285c7cc5a41e151da920710be7f4671",
    "line" : 74,
    "diffHunk" : "@@ -1,1 +673,677 @@        return migratableResolver.putShuffleBlockAsStream(blockId, serializerManager)\n      } catch {\n        case e: ClassCastException => throw new SparkException(\n          s\"Unexpected shuffle block ${blockId} with unsupported shuffle \" +\n          s\"resolver ${shuffleManager.shuffleBlockResolver}\")"
  }
]