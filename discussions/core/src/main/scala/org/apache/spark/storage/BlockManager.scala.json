[
  {
    "id" : "15c8a658-dea0-4955-a3f3-6a4d3fb4b62e",
    "prId" : 33251,
    "prUrl" : "https://github.com/apache/spark/pull/33251#pullrequestreview-703342839",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "594199f1-aafa-4ada-9731-d93569c82cd0",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "I wasn't clear why this had to change?",
        "createdAt" : "2021-07-09T18:13:49Z",
        "updatedAt" : "2021-07-09T18:15:27Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "81bd8e32-04f0-4161-9167-aa3de0aa7532",
        "parentId" : "594199f1-aafa-4ada-9731-d93569c82cd0",
        "authorId" : "3f417633-f870-4fee-b580-ac9ce146075f",
        "body" : "The test was failing with `NullPointerException` cause the `SparkEnv.get` wasn't initialized.\r\nConsidering that the `RemoteBlockDownloadFileManager` accepts an instance of `BlockManager` which already has `SecurityManager` it seemed logical to just pass the `encryptionKey` too so it didn't rely on the static env in the lazy val.\r\n\r\nAs I understand the alternative would be to construct a mock environment and call the static `SparkEnv.set` and I see that this approach is taken in other places, but I do not understand how it works reliably without removing it after use - it's just left there. Hence I decided to remove the reliance on the static env so it's easier to test.",
        "createdAt" : "2021-07-09T19:38:11Z",
        "updatedAt" : "2021-07-09T19:55:52Z",
        "lastEditedBy" : "3f417633-f870-4fee-b580-ac9ce146075f",
        "tags" : [
        ]
      }
    ],
    "commit" : "74c374fc45279f378d965d421298864b2ca6794d",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +256,260 @@  // Exposed for test\n  private[storage] val remoteBlockTempFileManager =\n    new BlockManager.RemoteBlockDownloadFileManager(\n      this,\n      securityManager.getIOEncryptionKey())"
  },
  {
    "id" : "e3249ebb-b1ad-4188-ab8d-4803cc86ceb6",
    "prId" : 33078,
    "prUrl" : "https://github.com/apache/spark/pull/33078#pullrequestreview-698737470",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7c99b7b3-c764-4558-a287-28bcae65bfb4",
        "parentId" : null,
        "authorId" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "body" : "Is there a UT that verifies the message when pushbased shuffle is enabled?",
        "createdAt" : "2021-06-29T06:58:04Z",
        "updatedAt" : "2021-06-29T06:58:51Z",
        "lastEditedBy" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "tags" : [
        ]
      },
      {
        "id" : "cd182bc3-c1a1-4f97-b9b9-5da1ffc30f2c",
        "parentId" : "7c99b7b3-c764-4558-a287-28bcae65bfb4",
        "authorId" : "e942fa3e-a5b9-4a9f-b9a5-a1afd3c29b2c",
        "body" : "Added UT.",
        "createdAt" : "2021-07-05T01:32:43Z",
        "updatedAt" : "2021-07-05T01:32:43Z",
        "lastEditedBy" : "e942fa3e-a5b9-4a9f-b9a5-a1afd3c29b2c",
        "tags" : [
        ]
      }
    ],
    "commit" : "53109918cbdbdba2fe79f38a991c171efec7e85f",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +544,548 @@      } else {\n        shuffleManager.getClass.getName\n      }\n    val shuffleConfig = new ExecutorShuffleInfo(\n      diskBlockManager.localDirsString,"
  },
  {
    "id" : "410e8a54-df98-4e65-b460-af8bd80215a7",
    "prId" : 30492,
    "prUrl" : "https://github.com/apache/spark/pull/30492#pullrequestreview-541269445",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "992abbb5-74a3-4e46-b524-d0899b10fd47",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Can you explain the reasoning here? So once any executor cannot get local shuffle block, it will always try to read from fallback storage?",
        "createdAt" : "2020-11-26T18:04:54Z",
        "updatedAt" : "2020-11-30T19:56:40Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "326d2ed0-92f3-4510-8837-e323d93e2c88",
        "parentId" : "992abbb5-74a3-4e46-b524-d0899b10fd47",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Yes. If `STORAGE_DECOMMISSION_FALLBACK_STORAGE_PATH` is defined, it tried to fallback. Currently, it happens when `FALLBACK_BLOCK_MANAGER_ID` has the blocks.",
        "createdAt" : "2020-11-27T01:32:30Z",
        "updatedAt" : "2020-11-30T19:56:40Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "98a8d4e4-a403-44e9-a90a-6e64a5ce50b4",
        "parentId" : "992abbb5-74a3-4e46-b524-d0899b10fd47",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Is there any check if `FALLBACK_BLOCK_MANAGER_ID` has the blocks before calling `FallbackStorage.read`? Looks like if `getBlockData` throws `IOException`, the block manager will just go to read from fallback storage.",
        "createdAt" : "2020-11-27T01:43:57Z",
        "updatedAt" : "2020-11-30T19:56:40Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "fa2dbdaf-548f-4c89-a885-33a8f66c288e",
        "parentId" : "992abbb5-74a3-4e46-b524-d0899b10fd47",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "It's already `IOException` and `FallbackStorage.read` throws `IOException` for non-exist files and it's fine and legitimate in the `WorkerDecomission` context, @viirya . Please note the following.\r\n- The whole Worker decommission (including `shuffle and rdd storage decommission`) is designed as a best-effort approach.\r\n- It's because the main use case is K8s graceful shutdown with the default period (`30s`). We can increase the period, but we cannot set it to the infinite value technically. It means executor hangs in case of disk full situation.\r\n- What we are aiming is to rescue data as much as possible, but `100%` is not guaranteed always.\r\n- Lastly, data block selection(shuffle or rdd) was a random-order from the beginning. It became worse when there are multiple shuffle across multiple executors.\r\n\r\nDue to the above reasons, we introduced `SPARK-33387 Support ordered shuffle block migration` to rescue as complete as possible. However, we can still easily imagine that multiple shuffles coexist on an executor in a skewed manner. While the other executor succeeds to complete the migration, the skewed executor may fail to complete the migration in the given grace period.",
        "createdAt" : "2020-11-29T00:17:37Z",
        "updatedAt" : "2020-11-30T19:56:40Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "17fc33dd-fb13-4ac0-a2ac-80db4580ca49",
        "parentId" : "992abbb5-74a3-4e46-b524-d0899b10fd47",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "I think this would probably fail quickly if the external storage doesn’t have the block right? So no need to do a separate check to see if the block is present, the failure message is good enough.",
        "createdAt" : "2020-11-30T19:03:18Z",
        "updatedAt" : "2020-11-30T19:56:40Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "03212650-a726-4d11-bb08-f59483046bb2",
        "parentId" : "992abbb5-74a3-4e46-b524-d0899b10fd47",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Yes, it does, @holdenk .",
        "createdAt" : "2020-11-30T19:58:24Z",
        "updatedAt" : "2020-11-30T19:58:24Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "bfa011d1-d6c7-4b06-8a3b-1ddec2d1c39a",
        "parentId" : "992abbb5-74a3-4e46-b524-d0899b10fd47",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "Make sense. Thanks @holdenk ",
        "createdAt" : "2020-11-30T20:29:20Z",
        "updatedAt" : "2020-11-30T20:29:20Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "9c9317378eb61b40f766e41dd168d2189f6008c1",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +636,640 @@          } else {\n            throw e\n          }\n      }\n    } else {"
  },
  {
    "id" : "f4a877e9-723e-4fa5-a4a3-7b6e57e494a3",
    "prId" : 30492,
    "prUrl" : "https://github.com/apache/spark/pull/30492#pullrequestreview-541280945",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c2a7420c-1ed0-4d2d-8fe8-f62f68e8c898",
        "parentId" : null,
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Nit/ question: Could we move the if up as part of the case and avoid the need for explicit rethrow?",
        "createdAt" : "2020-11-30T19:01:14Z",
        "updatedAt" : "2020-11-30T19:56:40Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "fc009249-b435-4828-a944-76083cb5a379",
        "parentId" : "c2a7420c-1ed0-4d2d-8fe8-f62f68e8c898",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "It's because we should access the normal access path by default. We are able to use `Fallback` path only if the normal access path fail because `conf.get(config.STORAGE_DECOMMISSION_FALLBACK_STORAGE_PATH).isDefined` doesn't mean the fallback storage has the data.",
        "createdAt" : "2020-11-30T19:53:17Z",
        "updatedAt" : "2020-11-30T19:56:40Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "2073b9df-2c1f-45eb-9a26-b72671a73a70",
        "parentId" : "c2a7420c-1ed0-4d2d-8fe8-f62f68e8c898",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Please let me know if I misunderstand your comment.",
        "createdAt" : "2020-11-30T19:57:46Z",
        "updatedAt" : "2020-11-30T19:57:46Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "7872315b-b7e5-4223-9882-0224bcc768fc",
        "parentId" : "c2a7420c-1ed0-4d2d-8fe8-f62f68e8c898",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Yeah so I mean we still have a try/catch just the case statement has the if as part of it (eg “ Using if expressions in case statements”)",
        "createdAt" : "2020-11-30T20:46:33Z",
        "updatedAt" : "2020-11-30T20:46:33Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "9c9317378eb61b40f766e41dd168d2189f6008c1",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +631,635 @@        shuffleManager.shuffleBlockResolver.getBlockData(blockId)\n      } catch {\n        case e: IOException =>\n          if (conf.get(config.STORAGE_DECOMMISSION_FALLBACK_STORAGE_PATH).isDefined) {\n            FallbackStorage.read(conf, blockId)"
  },
  {
    "id" : "a6aa1bd3-0a08-4898-9797-60d82f546c0a",
    "prId" : 29817,
    "prUrl" : "https://github.com/apache/spark/pull/29817#pullrequestreview-503374099",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b2328842-e847-4021-bdfe-fab90e02e048",
        "parentId" : null,
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Why?",
        "createdAt" : "2020-09-23T23:12:05Z",
        "updatedAt" : "2020-10-22T13:56:45Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "aaa37f7c-1c25-4136-8c9f-d52fac222d40",
        "parentId" : "b2328842-e847-4021-bdfe-fab90e02e048",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "?",
        "createdAt" : "2020-09-24T02:43:29Z",
        "updatedAt" : "2020-10-22T13:56:45Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "90316493-aa30-43c1-9880-dedfc916b51c",
        "parentId" : "b2328842-e847-4021-bdfe-fab90e02e048",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Why did you make this change?",
        "createdAt" : "2020-09-24T16:02:18Z",
        "updatedAt" : "2020-10-22T13:56:45Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "6f73e3d9-e800-4057-83b6-794295f59813",
        "parentId" : "b2328842-e847-4021-bdfe-fab90e02e048",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "If I unsterstand your question correctly:\r\n\r\nWe didn't really change the `decommissionBlockManager`. The original `decommissionBlockManager` has been renamed to `decommissionSelf` to avoid the naming collision.",
        "createdAt" : "2020-09-28T08:39:26Z",
        "updatedAt" : "2020-10-22T13:56:45Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "af9d5c51-d04c-4a2e-906f-b35b5d205227",
        "parentId" : "b2328842-e847-4021-bdfe-fab90e02e048",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Makes sense, although maybe introducing a new name instead of changing the use of a previous function name would be easier for verifying.",
        "createdAt" : "2020-10-06T21:36:37Z",
        "updatedAt" : "2020-10-22T13:56:45Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "3c1e033089e392066425139d7ccb52cd501dcc31",
    "line" : 25,
    "diffHunk" : "@@ -1,1 +1811,1815 @@  }\n\n  def decommissionBlockManager(): Unit = storageEndpoint.ask(DecommissionBlockManager)\n\n  private[spark] def decommissionSelf(): Unit = synchronized {"
  },
  {
    "id" : "d0b10f18-6225-4e2c-a346-4a4b505d9825",
    "prId" : 29722,
    "prUrl" : "https://github.com/apache/spark/pull/29722#pullrequestreview-487423840",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5b2b550f-ccf9-4a7e-bc3c-b023b043177e",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "Didn't follow the need for the name change.",
        "createdAt" : "2020-09-12T01:56:01Z",
        "updatedAt" : "2020-09-16T06:57:24Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "18d6cc2f-30ee-4257-bfd6-a7c6da2a1bbf",
        "parentId" : "5b2b550f-ccf9-4a7e-bc3c-b023b043177e",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "It conflicts with the newly added \"decommissionBlockManager()\".",
        "createdAt" : "2020-09-14T05:26:36Z",
        "updatedAt" : "2020-09-16T06:57:24Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "290d2c08a1adfce3c0c4afe1cb8c9e214b25ea3d",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +1812,1816 @@  def decommissionBlockManager(): Unit = storageEndpoint.ask(DecommissionBlockManager)\n\n  private[spark] def decommissionSelf(): Unit = synchronized {\n    decommissioner match {\n      case None =>"
  },
  {
    "id" : "065a02de-5c7a-444f-b57d-3252ffabf494",
    "prId" : 29211,
    "prUrl" : "https://github.com/apache/spark/pull/29211#pullrequestreview-454592436",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f8e77ce9-7c0f-4b91-919b-476926dee874",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "What do you think of directly exposing the \"BlocksMigrated\" and \"BlocksToMigrate\" instead of a single boolean ? I think it will provide more debugging value than a single flag. \r\n\r\nie, instead of returning \"BlocksMigrated >= BlocksToMigrate\", return the two counters directly.\r\n\r\nOr maybe this may be a lot more work because you want to track this for both shuffle as well as persisted blocks.\r\n\r\n",
        "createdAt" : "2020-07-24T00:04:29Z",
        "updatedAt" : "2020-08-05T18:39:14Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "b020bf29-2d62-4efc-87fb-b44b40d437f1",
        "parentId" : "f8e77ce9-7c0f-4b91-919b-476926dee874",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "I'd really rather not. I think the level above us should only be concerned if the blocks are done migrating or not. If we expose that up a level the logic is going to get even more complicated cross-class boundaries.",
        "createdAt" : "2020-07-24T00:47:34Z",
        "updatedAt" : "2020-08-05T18:39:14Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "2572ca14-ec27-4e50-abde-2de9fc6b877f",
        "parentId" : "f8e77ce9-7c0f-4b91-919b-476926dee874",
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "Cool !. I am always anal about logging to a fault :-P ",
        "createdAt" : "2020-07-24T01:27:50Z",
        "updatedAt" : "2020-08-05T18:39:14Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      }
    ],
    "commit" : "e81c3fc2d0b8c28cd0aedf08a7ea1e86a153ae49",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +1827,1831 @@   *  If there are any tasks running since that time the boolean may be incorrect.\n   */\n  private[spark] def lastMigrationInfo(): (Long, Boolean) = {\n    decommissioner.map(_.lastMigrationInfo()).getOrElse((0, false))\n  }"
  },
  {
    "id" : "5a69db39-f1a7-4ed9-aea3-b54f1d334205",
    "prId" : 28911,
    "prUrl" : "https://github.com/apache/spark/pull/28911#pullrequestreview-478568555",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "85eb1bfd-8a06-4de5-ab25-d240a71de829",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Just a question. Why do we switch from `Java` Map to `Scala` Map in this PR? Is it required?",
        "createdAt" : "2020-08-29T22:10:23Z",
        "updatedAt" : "2020-09-01T06:44:19Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "777e4bc3-7f22-4217-a329-de72357ae86a",
        "parentId" : "85eb1bfd-8a06-4de5-ab25-d240a71de829",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "It's required by `fetchMultipleHostLocalBlocks`.  Actually, we could also do the Jave to Scala map conversion before calling  `fetchMultipleHostLocalBlocks` but leaving `Try[java.util.Map[String, Array[String]]] => Unit` unchanged.\r\n\r\nBut I decided to do the conversion here just because this class already imported `scala.collection.JavaConverters._`. It has no big difference to do the conversion here or there.",
        "createdAt" : "2020-08-31T12:25:32Z",
        "updatedAt" : "2020-09-01T06:44:20Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "a23ab1721b1225e0a95fb27660fe81847287c622",
    "line" : 30,
    "diffHunk" : "@@ -1,1 +138,142 @@      port: Int,\n      executorIds: Array[String])(\n      callback: Try[Map[String, Array[String]]] => Unit): Unit = {\n    val hostLocalDirsCompletable = new CompletableFuture[java.util.Map[String, Array[String]]]\n    blockStoreClient.getHostLocalDirs("
  },
  {
    "id" : "5c0453af-aa4a-47b8-a1bf-0bcf19bd058d",
    "prId" : 28911,
    "prUrl" : "https://github.com/apache/spark/pull/28911#pullrequestreview-479418592",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9c339821-696d-45d4-9bb3-4b2874864220",
        "parentId" : null,
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Why did the scope change here?",
        "createdAt" : "2020-08-31T17:11:50Z",
        "updatedAt" : "2020-09-01T06:44:20Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "e19aca12-6660-4e73-b57f-c33297a91567",
        "parentId" : "9c339821-696d-45d4-9bb3-4b2874864220",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "It's required by `val port = blockManager.externalShuffleServicePort` within `ShuffleBlockFetcherIterator`.",
        "createdAt" : "2020-09-01T05:52:27Z",
        "updatedAt" : "2020-09-01T06:44:20Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "a23ab1721b1225e0a95fb27660fe81847287c622",
    "line" : 60,
    "diffHunk" : "@@ -1,1 +212,216 @@  private val maxOffHeapMemory = memoryManager.maxOffHeapStorageMemory\n\n  private[spark] val externalShuffleServicePort = StorageUtils.externalShuffleServicePort(conf)\n\n  var blockManagerId: BlockManagerId = _"
  },
  {
    "id" : "3dbbf9af-2590-4d88-9bfe-4e0aaaba2ac8",
    "prId" : 28817,
    "prUrl" : "https://github.com/apache/spark/pull/28817#pullrequestreview-430179975",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "aaf7ab3b-bab7-4977-9b79-adc5824887cd",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "I think that the comment needs to be updated to reflect what the return Boolean indicates.",
        "createdAt" : "2020-06-13T23:37:09Z",
        "updatedAt" : "2020-06-14T01:11:18Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "6b7cb08c-8996-4263-a36e-e9b6c9b780f5",
        "parentId" : "aaf7ab3b-bab7-4977-9b79-adc5824887cd",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Good catch",
        "createdAt" : "2020-06-14T01:10:34Z",
        "updatedAt" : "2020-06-14T01:11:18Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "a2c055715ccf2992e399cef3768b1299c24d9a82",
    "line" : 172,
    "diffHunk" : "@@ -1,1 +1890,1894 @@   * Note: this does not delete the shuffle files in-case there is an in-progress fetch\n   * but rather shadows them.\n   * Requires an Indexed based shuffle resolver.\n   *\n   * @return true if we have not migrated all shuffle blocks, false otherwise."
  },
  {
    "id" : "f3cdaf76-c08a-418a-82b2-e26aa10be894",
    "prId" : 28817,
    "prUrl" : "https://github.com/apache/spark/pull/28817#pullrequestreview-430179582",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8a719425-87f4-43ac-8e50-d75ea91e7ff2",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "Just to make sure I am totally understanding this: You mean that the running tasks that were already running when the decommissioning was started at the executor ? Because, I think we refuse launching new tasks when the decommissioning has started, so the new blocks being written must be written by already running tasks. Did I get this right ?\r\n\r\nAlso, just to confirm I am still following along: I don't see this case handled in the existing BlockManagerSuite: I believe we are not testing writing new blocks while the decom/offload is in progress.",
        "createdAt" : "2020-06-13T23:41:06Z",
        "updatedAt" : "2020-06-14T01:11:18Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "6460a055-46ac-42f2-9aad-22cd784ad35f",
        "parentId" : "8a719425-87f4-43ac-8e50-d75ea91e7ff2",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "It is covered, you can verify this by disabling this logic and seeing the test fail (albiet you'll have to run the test a few times because it becomes a race condition). Look at the \"migrateDuring\" flag for details.",
        "createdAt" : "2020-06-14T00:57:30Z",
        "updatedAt" : "2020-06-14T01:11:18Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "a2c055715ccf2992e399cef3768b1299c24d9a82",
    "line" : 252,
    "diffHunk" : "@@ -1,1 +2052,2056 @@  private[spark] class BlockManagerDecommissionManager(conf: SparkConf) {\n    @volatile private var stopped = false\n    // Since running tasks can add more blocks this can change.\n    @volatile var allBlocksMigrated = false\n    var previousBlocksLeft = true"
  },
  {
    "id" : "69521326-9285-4a97-a641-27fcfd936fe8",
    "prId" : 28708,
    "prUrl" : "https://github.com/apache/spark/pull/28708#pullrequestreview-447738372",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "100e536c-f7ff-4c77-b75a-c5b88df8f517",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "Just for my understanding: why is decommissioner volatile now ? Would a comment help ?",
        "createdAt" : "2020-06-19T01:48:31Z",
        "updatedAt" : "2020-07-17T23:45:20Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "4dfed59e-4b07-4647-bff0-652f14d6c2f2",
        "parentId" : "100e536c-f7ff-4c77-b75a-c5b88df8f517",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "If we receive two decommissioning messages at the same time. we don't want to launch two of them.",
        "createdAt" : "2020-06-19T03:05:52Z",
        "updatedAt" : "2020-07-17T23:45:20Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "6495f264-30fc-4e28-8ddf-c264ae1b4de8",
        "parentId" : "100e536c-f7ff-4c77-b75a-c5b88df8f517",
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "Hmm, the creation to `decommissioner` is guarded by a lock. ",
        "createdAt" : "2020-06-19T03:50:44Z",
        "updatedAt" : "2020-07-17T23:45:20Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "45d246f8-a5d6-4e49-88bc-d69554c5218a",
        "parentId" : "100e536c-f7ff-4c77-b75a-c5b88df8f517",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "That's true. If we drop it we might also accept remove block puts after we've started decommissioning though. Depends on how much we want to avoid that.",
        "createdAt" : "2020-06-29T20:58:04Z",
        "updatedAt" : "2020-07-17T23:45:20Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "5b0bac83-6465-4406-b72c-481a6c0f01aa",
        "parentId" : "100e536c-f7ff-4c77-b75a-c5b88df8f517",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "I think I'm going to leave it volatile for now, I'd like to avoid remote block puts once we're in decommissioning because we depend on not getting new blocks except from tasks to figure out when it is safe to exit.",
        "createdAt" : "2020-07-14T02:20:31Z",
        "updatedAt" : "2020-07-17T23:45:20Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "8494bdd94285c7cc5a41e151da920710be7f4671",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +246,250 @@\n  // This is volatile since if it's defined we should not accept remote blocks.\n  @volatile private var decommissioner: Option[BlockManagerDecommissioner] = None\n\n  // A DownloadFileManager used to track all the files of remote blocks which are above the"
  },
  {
    "id" : "2a901300-cbf3-457a-a55b-eaeb872db352",
    "prId" : 28708,
    "prUrl" : "https://github.com/apache/spark/pull/28708#pullrequestreview-433770929",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3dfe86a2-a3aa-4156-b97c-86f56247b88d",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "Ah I see: That's why the migratableResolver is a lazy val. \r\n\r\nIs the blockId pertinent to log here ? I believe that the class cast exception would be thrown when the resolver is not of the correct type on the first access. Would a regular function help here, or perhaps the class cast exception check can be moved inside of the migratableResolver assignment ?",
        "createdAt" : "2020-06-19T01:56:37Z",
        "updatedAt" : "2020-07-17T23:45:20Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "6d25d548-5bc7-404a-8c00-1bcc67cf465e",
        "parentId" : "3dfe86a2-a3aa-4156-b97c-86f56247b88d",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Yeah were logging which kind of block we received. We only really care about the type but it's easier to just print the blockId its self.",
        "createdAt" : "2020-06-19T03:07:52Z",
        "updatedAt" : "2020-07-17T23:45:20Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "8494bdd94285c7cc5a41e151da920710be7f4671",
    "line" : 74,
    "diffHunk" : "@@ -1,1 +673,677 @@        return migratableResolver.putShuffleBlockAsStream(blockId, serializerManager)\n      } catch {\n        case e: ClassCastException => throw new SparkException(\n          s\"Unexpected shuffle block ${blockId} with unsupported shuffle \" +\n          s\"resolver ${shuffleManager.shuffleBlockResolver}\")"
  },
  {
    "id" : "2caecf74-fa52-42cd-bcf6-2c7a44c1b243",
    "prId" : 28482,
    "prUrl" : "https://github.com/apache/spark/pull/28482#pullrequestreview-412051156",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f20c7c79-ac8d-4fa9-b4bd-2237eef3e695",
        "parentId" : null,
        "authorId" : "ad4cb313-47b2-4cfa-9c1f-0a3596794a67",
        "body" : "Can this explicit catching of InterruptedException `case e: InterruptedException =>` be removed? InterruptException will not be caught by `case NonFatal(t)` here so it will automatically be thrown back?",
        "createdAt" : "2020-05-14T17:51:13Z",
        "updatedAt" : "2020-05-14T17:51:14Z",
        "lastEditedBy" : "ad4cb313-47b2-4cfa-9c1f-0a3596794a67",
        "tags" : [
        ]
      },
      {
        "id" : "5ab207b6-79af-417c-a3a6-7ef602af3e67",
        "parentId" : "f20c7c79-ac8d-4fa9-b4bd-2237eef3e695",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "That's probably ok",
        "createdAt" : "2020-05-14T17:54:07Z",
        "updatedAt" : "2020-05-14T17:54:07Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "3b3a81db43ffaa0312b5bd110a68fb6fda349794",
    "line" : 193,
    "diffHunk" : "@@ -1,1 +1923,1927 @@              decommissionRddCacheBlocks()\n            } catch {\n              case e: InterruptedException =>\n                logInfo(\"Interrupted during migration, re-throwing\")\n                throw e"
  },
  {
    "id" : "65dad189-9884-400c-8dc8-8e2638a3013a",
    "prId" : 28482,
    "prUrl" : "https://github.com/apache/spark/pull/28482#pullrequestreview-412062586",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b520c71a-b915-452e-8b56-531b462931f4",
        "parentId" : null,
        "authorId" : "ad4cb313-47b2-4cfa-9c1f-0a3596794a67",
        "body" : "When the thread is Interrupted, the InterruptedException is caught and stopped is set to true and while loop will exit. So do we need to add `!Thread.interrupted()` explicitly? Or is this just for being more defensive here?",
        "createdAt" : "2020-05-14T17:52:35Z",
        "updatedAt" : "2020-05-14T17:52:58Z",
        "lastEditedBy" : "ad4cb313-47b2-4cfa-9c1f-0a3596794a67",
        "tags" : [
        ]
      },
      {
        "id" : "4ff6486b-cdc3-4fb9-bec4-e7d6d3277c12",
        "parentId" : "b520c71a-b915-452e-8b56-531b462931f4",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "This is incase one of our sub functions is capturing the thread interruption (either now or in the future).",
        "createdAt" : "2020-05-14T17:53:16Z",
        "updatedAt" : "2020-05-14T17:53:16Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "7bbceb90-333f-469b-96f1-cd60f73b46cf",
        "parentId" : "b520c71a-b915-452e-8b56-531b462931f4",
        "authorId" : "ad4cb313-47b2-4cfa-9c1f-0a3596794a67",
        "body" : "got it.",
        "createdAt" : "2020-05-14T18:08:57Z",
        "updatedAt" : "2020-05-14T18:08:57Z",
        "lastEditedBy" : "ad4cb313-47b2-4cfa-9c1f-0a3596794a67",
        "tags" : [
        ]
      }
    ],
    "commit" : "3b3a81db43ffaa0312b5bd110a68fb6fda349794",
    "line" : 187,
    "diffHunk" : "@@ -1,1 +1917,1921 @@          while (blockManagerDecommissioning &&\n            !stopped &&\n            !Thread.interrupted() &&\n            failures < 20) {\n            logDebug(\"Attempting to replicate all cached RDD blocks\")"
  },
  {
    "id" : "303b4635-663f-4652-bb0c-6a561b47baf8",
    "prId" : 28482,
    "prUrl" : "https://github.com/apache/spark/pull/28482#pullrequestreview-412062312",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "117d4509-b4ce-404d-8237-f2a6da939671",
        "parentId" : null,
        "authorId" : "ad4cb313-47b2-4cfa-9c1f-0a3596794a67",
        "body" : "InterruptedException is not part of `NonFatal(e)`, so won't that be thrown away anyway without this change also?",
        "createdAt" : "2020-05-14T18:08:33Z",
        "updatedAt" : "2020-05-14T18:08:33Z",
        "lastEditedBy" : "ad4cb313-47b2-4cfa-9c1f-0a3596794a67",
        "tags" : [
        ]
      }
    ],
    "commit" : "3b3a81db43ffaa0312b5bd110a68fb6fda349794",
    "line" : 89,
    "diffHunk" : "@@ -1,1 +1655,1659 @@      } catch {\n        // Rethrow interrupt exception\n        case e: InterruptedException =>\n          throw e\n        // Everything else we may retry"
  },
  {
    "id" : "bfb11f2e-9508-4646-8f6d-c8731d47c9bd",
    "prId" : 28370,
    "prUrl" : "https://github.com/apache/spark/pull/28370#pullrequestreview-405189403",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "eaa61973-7a81-4213-8a3d-85d5cfc94640",
        "parentId" : null,
        "authorId" : "31c8cfa2-9d5f-412b-80d0-c7d204dd8cd1",
        "body" : "Is there a possibility that get peers might return some blockManagerIDs to which this block has already been replicated to?",
        "createdAt" : "2020-05-02T14:50:54Z",
        "updatedAt" : "2020-05-14T19:07:46Z",
        "lastEditedBy" : "31c8cfa2-9d5f-412b-80d0-c7d204dd8cd1",
        "tags" : [
        ]
      },
      {
        "id" : "b73a8ded-6deb-4fb6-aa43-45f3063cebfd",
        "parentId" : "eaa61973-7a81-4213-8a3d-85d5cfc94640",
        "authorId" : "31c8cfa2-9d5f-412b-80d0-c7d204dd8cd1",
        "body" : "I think getPeers Handles it. If so kindly ignore it.",
        "createdAt" : "2020-05-02T15:01:41Z",
        "updatedAt" : "2020-05-14T19:07:46Z",
        "lastEditedBy" : "31c8cfa2-9d5f-412b-80d0-c7d204dd8cd1",
        "tags" : [
        ]
      },
      {
        "id" : "bf360dff-609b-4283-80a6-439b84ae0df1",
        "parentId" : "eaa61973-7a81-4213-8a3d-85d5cfc94640",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "I don't think getPeers handles that situation currently, it's handled inside of blockReplicationPolicy `prioritize` which is called inside of `replicate`.",
        "createdAt" : "2020-05-04T17:20:23Z",
        "updatedAt" : "2020-05-14T19:07:46Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "c34305662cd91d05b5160c7de72e35e4108b6755",
    "line" : 52,
    "diffHunk" : "@@ -1,1 +1581,1585 @@      // we know we are called as a result of an executor removal or because the current executor\n      // is getting decommissioned. so we refresh peer cache before trying replication, we won't\n      // try to replicate to a missing executor/another decommissioning executor\n      getPeers(forceFetch = true)\n      try {"
  },
  {
    "id" : "63e1845c-e758-44aa-8c76-b974711c8b93",
    "prId" : 28370,
    "prUrl" : "https://github.com/apache/spark/pull/28370#pullrequestreview-405384854",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ecbd4b83-e088-4164-be94-a408f2048c0c",
        "parentId" : null,
        "authorId" : "31c8cfa2-9d5f-412b-80d0-c7d204dd8cd1",
        "body" : "Just a though.. should it be part of a thread pool rather than a single thread doing it?",
        "createdAt" : "2020-05-02T15:02:57Z",
        "updatedAt" : "2020-05-14T19:07:46Z",
        "lastEditedBy" : "31c8cfa2-9d5f-412b-80d0-c7d204dd8cd1",
        "tags" : [
        ]
      },
      {
        "id" : "c122e619-e19d-4029-8593-d70b00ced834",
        "parentId" : "ecbd4b83-e088-4164-be94-a408f2048c0c",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "This part of the loop I think is fine with a single thread (it's just checking which blocks need to get migrated), I think down on L1807 we could explore something else like a threadpool or a producer/consumer model, but I don't think we need to block the PR on that.",
        "createdAt" : "2020-05-04T22:12:52Z",
        "updatedAt" : "2020-05-14T19:07:46Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "c34305662cd91d05b5160c7de72e35e4108b6755",
    "line" : 126,
    "diffHunk" : "@@ -1,1 +1795,1799 @@   * Visible for testing\n   */\n  def decommissionRddCacheBlocks(): Unit = {\n    val replicateBlocksInfo = master.getReplicateInfoForRDDBlocks(blockManagerId)\n"
  },
  {
    "id" : "8e8affaf-661d-4cfd-8134-b33626a6788f",
    "prId" : 28370,
    "prUrl" : "https://github.com/apache/spark/pull/28370#pullrequestreview-413228507",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4fa8f215-8b29-4cd1-ad9f-fbcb7b4df553",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "use `map`?",
        "createdAt" : "2020-05-15T13:15:32Z",
        "updatedAt" : "2020-05-15T13:55:54Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "4d3f680e-ac4e-45a2-95af-ba83af46fe60",
        "parentId" : "4fa8f215-8b29-4cd1-ad9f-fbcb7b4df553",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Using `map` would give us back an `Option[Boolean]` and we just want a `boolean`",
        "createdAt" : "2020-05-15T17:18:50Z",
        "updatedAt" : "2020-05-15T17:18:50Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "876441e6-c869-493a-8515-d3129161ac6c",
        "parentId" : "4fa8f215-8b29-4cd1-ad9f-fbcb7b4df553",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "I see.",
        "createdAt" : "2020-05-18T01:44:33Z",
        "updatedAt" : "2020-05-18T01:44:33Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "c34305662cd91d05b5160c7de72e35e4108b6755",
    "line" : 40,
    "diffHunk" : "@@ -1,1 +1571,1575 @@      maxReplicationFailures: Option[Int] = None): Boolean = {\n    logInfo(s\"Using $blockManagerId to pro-actively replicate $blockId\")\n    blockInfoManager.lockForReading(blockId).forall { info =>\n      val data = doGetLocalBytes(blockId, info)\n      val storageLevel = StorageLevel("
  },
  {
    "id" : "e499ba3e-cb96-4802-a7f8-0fb1b9b1aabb",
    "prId" : 28370,
    "prUrl" : "https://github.com/apache/spark/pull/28370#pullrequestreview-413277238",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cf67a840-6bcc-44ca-adf8-4a9041ebb80b",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Do we really need a wrapped manager class? It seems overkill to me.",
        "createdAt" : "2020-05-15T13:22:40Z",
        "updatedAt" : "2020-05-15T13:55:54Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "9962e4f7-bbe0-4f54-8bf8-a58050a816aa",
        "parentId" : "cf67a840-6bcc-44ca-adf8-4a9041ebb80b",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "For the first part I'm ambivalent, but given that we also want to migrate shuffle blocks after I think having a manager is ok.",
        "createdAt" : "2020-05-15T17:37:39Z",
        "updatedAt" : "2020-05-15T17:37:40Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "62775523-f9da-4252-8931-ba18d16bb8eb",
        "parentId" : "cf67a840-6bcc-44ca-adf8-4a9041ebb80b",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "We should implement it step by step and could always do refactor later. Or, we should at least add a todo ticket to explain why we need this and what we plan to do next. Otherwise, I  am really -1 on this kind of change. ",
        "createdAt" : "2020-05-18T01:48:10Z",
        "updatedAt" : "2020-05-18T01:48:10Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "2e986b2c-1ad1-4c19-9c64-8208c83e14dc",
        "parentId" : "cf67a840-6bcc-44ca-adf8-4a9041ebb80b",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "So there are two conversations I want to have about this with you @Ngone51 now to make sure I'm understanding what you're trying to express.\r\n\r\nThere already is a second follow up PR that extends the  `BlockManagerDecommissionManager` already exists, so I'm not sure I agree with your reasoning. If it was only just for some possible future implementation that didn't already exist I'd be more inclined to simplify. Maybe you can take a look at https://issues.apache.org/jira/browse/SPARK-20624 , https://issues.apache.org/jira/browse/SPARK-20629 and https://github.com/apache/spark/pull/28331 for context.\r\n\r\nI want to understand your -1 here because that has some pretty strong meanings in the context of a code change. A -1 is generally viewed as expressing a veto, which I don't believe you have in the project (of course I was out for a month in the hospital last year so if you do please let point me to thread). Even if you don't have a veto in the project is it your intention to say that if you did have a veto you would block this code change? A veto is generally a very strong expression, and I'm worried I'm not understanding your reasoning since this seems like a relatively minor issue.",
        "createdAt" : "2020-05-18T02:08:28Z",
        "updatedAt" : "2020-05-18T02:08:28Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "81ace5c9-8f22-40b0-b2c2-09337eed2cfc",
        "parentId" : "cf67a840-6bcc-44ca-adf8-4a9041ebb80b",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Also, I understand text-only communication can have more misunderstandings, if you want to find a time this week when we're both free to jump on a call to clarify this (and we can write back our understanding here so it's recorded for people to understand what we talked about), I'd be more than happy to.",
        "createdAt" : "2020-05-18T02:11:35Z",
        "updatedAt" : "2020-05-18T02:11:36Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "e0e7652e-474a-4192-b17e-1f09f508b997",
        "parentId" : "cf67a840-6bcc-44ca-adf8-4a9041ebb80b",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "For a new reviewer (e.g. me) on a big topic, it's not always possible to know every detail(even worse, when there's no design doc). So it's the author's responsibility to give more context. For example, leaving todo JIRA tickets in the code comment or reply to give more information. But without sufficient context here, I really think \"this change\", wrapping a manager around a thread, doesn't make sense to me.\r\n\r\n\r\n\r\nAs for \"-1\", it really represents my personal opinion. I should say \"I don't like this change\" if \"-1\" means a lot for the community.",
        "createdAt" : "2020-05-18T02:35:19Z",
        "updatedAt" : "2020-05-18T02:35:20Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "e2c3865b-4358-46ce-a2a9-edb008c848f1",
        "parentId" : "cf67a840-6bcc-44ca-adf8-4a9041ebb80b",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "As a reviewer it’s expected that you would read the issue before asking for a follow up issue in a blocking manner.",
        "createdAt" : "2020-05-18T03:46:11Z",
        "updatedAt" : "2020-05-18T03:46:11Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "7c64d046-5585-46f2-9695-a1de4bb78bf7",
        "parentId" : "cf67a840-6bcc-44ca-adf8-4a9041ebb80b",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Of course, I did. But I still don't get it and I think it' not always possible that a reviewer could know the sub-issue is mean to be a follow up for some specific codes without design document/code comments around here.\r\n\r\n",
        "createdAt" : "2020-05-18T04:15:27Z",
        "updatedAt" : "2020-05-18T04:15:28Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "7487d927-7ff3-443a-9baa-f23bedc42529",
        "parentId" : "cf67a840-6bcc-44ca-adf8-4a9041ebb80b",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "So if you look at the parent issue you can see there is another sub issue that says migrate shuffle blocks. It’s ok to ask for a follow up even if there is one (we all miss things in reading), but attempt to vote a -1 has a higher bar than just asking for something.",
        "createdAt" : "2020-05-18T05:18:56Z",
        "updatedAt" : "2020-05-18T05:18:56Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "c34305662cd91d05b5160c7de72e35e4108b6755",
    "line" : 176,
    "diffHunk" : "@@ -1,1 +1906,1910 @@   * It creates a Thread to retry offloading all RDD cache blocks\n   */\n  private class BlockManagerDecommissionManager(conf: SparkConf) {\n    @volatile private var stopped = false\n    private val sleepInterval = conf.get("
  },
  {
    "id" : "0474cdde-beee-4dcd-b170-8f70030d7075",
    "prId" : 28370,
    "prUrl" : "https://github.com/apache/spark/pull/28370#pullrequestreview-413230643",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "29e0626e-2764-4be5-9219-6505981d2352",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Use `Runnable` for the decommissioning and `ThreadUtils` to execute the `Runnable`?",
        "createdAt" : "2020-05-15T13:23:55Z",
        "updatedAt" : "2020-05-15T13:55:54Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "0940cb0b-61aa-4e79-ae43-232084034a65",
        "parentId" : "29e0626e-2764-4be5-9219-6505981d2352",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Looking at our code we seem to be roughly split on `Thread` versus `Runnable` usage. I think `Runnable` would make more sense if we were submitting this to an execution pool, but since we have a single thread and there is no reason to scale up the number of threads I don't see the need for that change.",
        "createdAt" : "2020-05-15T17:22:06Z",
        "updatedAt" : "2020-05-15T17:22:07Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "88824b0b-86a8-46aa-a4b1-139fdd0496b4",
        "parentId" : "29e0626e-2764-4be5-9219-6505981d2352",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "We always use `ThreadUtils.newDaemonSingleThreadExecutor` for the single runnable.",
        "createdAt" : "2020-05-15T17:31:04Z",
        "updatedAt" : "2020-05-15T17:31:05Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "bedd185e-88f6-41a8-a48b-ac58623f8153",
        "parentId" : "29e0626e-2764-4be5-9219-6505981d2352",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "`grep -r \"new Thread\" ./core/src/main | wc -l` returns 36\r\n`grep -r ThreadUtils.newDaemonSingleThreadExecutor ./core/src/main |wc -l` returns 4",
        "createdAt" : "2020-05-15T17:35:57Z",
        "updatedAt" : "2020-05-15T17:35:58Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "93fd849d-72a0-4b50-a519-6f1db5e2ec20",
        "parentId" : "29e0626e-2764-4be5-9219-6505981d2352",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Ah..that's good point but just wondering how many of them are chosen after realizing `ThreadUtils.newDaemonSingleThreadExecutor`. \r\n\r\nBTW, you'd better grep \"new Thread(\" to exclude ThreadLocal declaration.",
        "createdAt" : "2020-05-15T18:07:38Z",
        "updatedAt" : "2020-05-15T18:07:38Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "dd85695a-99b8-49b7-a70b-8de3a61b918f",
        "parentId" : "29e0626e-2764-4be5-9219-6505981d2352",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Even that returns 36 in core.",
        "createdAt" : "2020-05-18T01:55:29Z",
        "updatedAt" : "2020-05-18T01:55:30Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "c34305662cd91d05b5160c7de72e35e4108b6755",
    "line" : 206,
    "diffHunk" : "@@ -1,1 +1936,1940 @@    }\n    blockReplicationThread.setDaemon(true)\n    blockReplicationThread.setName(\"block-replication-thread\")\n\n    def start(): Unit = {"
  },
  {
    "id" : "27a17d49-bf38-479e-a13f-2397b4a26422",
    "prId" : 28370,
    "prUrl" : "https://github.com/apache/spark/pull/28370#pullrequestreview-412829427",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2e223964-01a7-431b-aab5-5f2e0931b792",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Don't you need to set `stop=true` here?\r\n\r\nOr you mean we need to do multiple time `decommissionRddCacheBlocks`? If so, why we need to do it for multiple times? There should be no rdd blocks change after decommissioning?",
        "createdAt" : "2020-05-15T13:26:19Z",
        "updatedAt" : "2020-05-15T13:55:54Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "5733df51-353c-418e-bc93-d21d807ebb8d",
        "parentId" : "2e223964-01a7-431b-aab5-5f2e0931b792",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "We don't set `stop=true` here because we loop through this multiple times. It is possible that not all blocks will replicate in the first iteration and also possible that more blocks are stored while were decommissioning (e.g. any existing running tasks which have a persist).",
        "createdAt" : "2020-05-15T17:18:42Z",
        "updatedAt" : "2020-05-15T17:18:43Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "c34305662cd91d05b5160c7de72e35e4108b6755",
    "line" : 190,
    "diffHunk" : "@@ -1,1 +1920,1924 @@          try {\n            logDebug(\"Attempting to replicate all cached RDD blocks\")\n            decommissionRddCacheBlocks()\n            logInfo(\"Attempt to replicate all cached blocks done\")\n            Thread.sleep(sleepInterval)"
  },
  {
    "id" : "15453e87-773d-4af7-b036-0045cd70cb4a",
    "prId" : 28370,
    "prUrl" : "https://github.com/apache/spark/pull/28370#pullrequestreview-413816593",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d6fd4949-227c-4bca-83a2-befeded6cc46",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Could we add attempt number to the log?",
        "createdAt" : "2020-05-18T03:15:14Z",
        "updatedAt" : "2020-05-18T03:15:14Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "3567e9b8-3b8c-4ac7-86ae-8d9d57a8e9cf",
        "parentId" : "d6fd4949-227c-4bca-83a2-befeded6cc46",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Added to https://issues.apache.org/jira/browse/SPARK-31555",
        "createdAt" : "2020-05-18T17:56:20Z",
        "updatedAt" : "2020-05-18T17:56:20Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "c34305662cd91d05b5160c7de72e35e4108b6755",
    "line" : 189,
    "diffHunk" : "@@ -1,1 +1919,1923 @@          && failures < 20) {\n          try {\n            logDebug(\"Attempting to replicate all cached RDD blocks\")\n            decommissionRddCacheBlocks()\n            logInfo(\"Attempt to replicate all cached blocks done\")"
  },
  {
    "id" : "c6c6e891-e4b1-4db1-b257-fc41ef0e2474",
    "prId" : 28370,
    "prUrl" : "https://github.com/apache/spark/pull/28370#pullrequestreview-413254974",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0d9a84e8-e9b9-4139-9bbb-2c254d3a0f18",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "attempt number?",
        "createdAt" : "2020-05-18T03:15:34Z",
        "updatedAt" : "2020-05-18T03:15:35Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "fb019a7d-6b0e-4b19-b995-0df09d8e8c2e",
        "parentId" : "0d9a84e8-e9b9-4139-9bbb-2c254d3a0f18",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "I’d say fine to do in a follow up but if we want to add the attempt number here go for it (but I won’t hold off on merging for that).",
        "createdAt" : "2020-05-18T03:47:17Z",
        "updatedAt" : "2020-05-18T03:47:18Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "c34305662cd91d05b5160c7de72e35e4108b6755",
    "line" : 191,
    "diffHunk" : "@@ -1,1 +1921,1925 @@            logDebug(\"Attempting to replicate all cached RDD blocks\")\n            decommissionRddCacheBlocks()\n            logInfo(\"Attempt to replicate all cached blocks done\")\n            Thread.sleep(sleepInterval)\n          } catch {"
  },
  {
    "id" : "edf18b14-64c0-41c9-9b5d-f97747a008f0",
    "prId" : 28370,
    "prUrl" : "https://github.com/apache/spark/pull/28370#pullrequestreview-413248247",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "292eaa6c-51f5-4251-86ed-977619598f11",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "attempt number?",
        "createdAt" : "2020-05-18T03:15:47Z",
        "updatedAt" : "2020-05-18T03:15:47Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "c34305662cd91d05b5160c7de72e35e4108b6755",
    "line" : 200,
    "diffHunk" : "@@ -1,1 +1930,1934 @@              failures += 1\n              logError(\"Error occurred while trying to replicate cached RDD blocks\" +\n                s\" for block manager decommissioning (failure count: $failures)\", e)\n          }\n        }"
  },
  {
    "id" : "2255f6d8-e709-4a6f-8609-13847415da22",
    "prId" : 28370,
    "prUrl" : "https://github.com/apache/spark/pull/28370#pullrequestreview-413819170",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a86491ca-8f25-4527-b511-1e7292732c15",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Do we really need this? \r\n\r\n1. In `stop()`, `stopped` is set before `interrupt()`. \r\n\r\n2. if the thread is doing decommission or sleeping when `stop()` is called, we'll capture `InterruptedException` later and set `stopped` to true.",
        "createdAt" : "2020-05-18T03:22:15Z",
        "updatedAt" : "2020-05-18T03:22:15Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "dcadf2cf-6121-4d95-8117-660c1fbddf68",
        "parentId" : "a86491ca-8f25-4527-b511-1e7292732c15",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Unless the interrupt exception is caught inside of the block transfer",
        "createdAt" : "2020-05-18T03:48:12Z",
        "updatedAt" : "2020-05-18T03:48:12Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "81ab4c61-4390-4eff-abe3-f5de3dc207bc",
        "parentId" : "a86491ca-8f25-4527-b511-1e7292732c15",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "> Unless the interrupt exception is caught inside of the block transfer\r\n\r\nIf there's an `InterruptedException` captured by block transfer, then `Thread.interrupted()` inside `blockReplicationThread` would return false. And what do you expect for this case?\r\n\r\nIf you want the decommission thread stop, then, `Thread.interrupted()` won't work;\r\n\r\nOr if you want the decommission thread to keep working, then, `Thread.interrupted()` is useless because the status has already been cleared(unless block transfer set it to interrupted again).\r\n\r\n\r\n\r\n",
        "createdAt" : "2020-05-18T04:43:03Z",
        "updatedAt" : "2020-05-18T04:44:54Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "9dbc154c-014d-4fdf-84fc-a9aeb8eb35a4",
        "parentId" : "a86491ca-8f25-4527-b511-1e7292732c15",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "If an interrupt exception is caught the thread would still be marked as interrupted",
        "createdAt" : "2020-05-18T05:12:44Z",
        "updatedAt" : "2020-05-18T05:12:45Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "19eaf1eb-d458-4ca9-bad6-94659d6fc830",
        "parentId" : "a86491ca-8f25-4527-b511-1e7292732c15",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "I believe in normal cases like `wait`, `sleep`, the status will be cleared according to JDK doc:\r\n\r\n```\r\n* @throws  InterruptedException\r\n*          if any thread has interrupted the current thread. The\r\n*          <i>interrupted status</i> of the current thread is\r\n*          cleared when this exception is thrown.\r\n```\r\n\r\nAnd even if the status is not cleared in other cases, the following `Thread.sleep(sleepInterval)` will throw `InterruptedException` firstly and set `stopped` to true and `Thread.interrupted()` still does not take effect.",
        "createdAt" : "2020-05-18T06:39:51Z",
        "updatedAt" : "2020-05-18T06:40:11Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "fb1de929-4197-45b8-9af0-4718a6fb931d",
        "parentId" : "a86491ca-8f25-4527-b511-1e7292732c15",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "So that block of text indicates that the interrupted status is cleared when the exception is thrown. Which means it would be possible for the thread to be in an interrupted status without having the exception thrown. Worst case scenario: this check is not needed but also causes no significant harm",
        "createdAt" : "2020-05-18T18:00:08Z",
        "updatedAt" : "2020-05-18T18:00:09Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "c34305662cd91d05b5160c7de72e35e4108b6755",
    "line" : 186,
    "diffHunk" : "@@ -1,1 +1916,1920 @@        while (blockManagerDecommissioning\n          && !stopped\n          && !Thread.interrupted()\n          && failures < 20) {\n          try {"
  },
  {
    "id" : "fbbab8e1-652e-47f7-8c42-ceba2c053814",
    "prId" : 28370,
    "prUrl" : "https://github.com/apache/spark/pull/28370#pullrequestreview-436083357",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b5870d35-1e39-4760-92a5-fb0b6b6055e9",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "@prakharjain09 / @holdenk, Thank you for this improvement. I had a question please: \r\n\r\n(I am still new to this code paths and I am not totally sure of what I am talking about. So if there is something I am missing please help me fill the gaps :-). )\r\n\r\nI notice that `replicateBlock` is already called during the executor removal codepath. ie, `org.apache.spark.storage.BlockManagerMasterEndpoint#removeBlockManager` does the replication to other peers if `spark.storage.replication.proactive=true`. This seemed to have been implemented in [SPARK-15355](https://issues.apache.org/jira/browse/SPARK-15355). And `org.apache.spark.storage.BlockManagerMasterEndpoint#removeBlockManager` is triggered when the executor is \"eventually\" lost.\r\n\r\nI understand it is a bit late to do the replication when the executor is indeed lost: Since decommissioning as implemented in https://github.com/apache/spark/pull/26440 does not really trigger eager executor loss. We instead merely stop scheduling on the decom'd executor and let it be shot down out of band. Which means that the replication triggered in SPARK-15355 would be too late.\r\n\r\nI like the approach taken in this PR to eagerly tell the executor (block-manager) to start replication when the decom is first initiated, to give it more time to be useful. But I wonder if you implemented this somewhat differently by leveraging the existing eager [replication loop](https://github.com/apache/spark/pull/14412/files#diff-186864190089a718680accb51de5f0d4R215) ?.\r\n\r\nThanks !",
        "createdAt" : "2020-06-11T17:43:27Z",
        "updatedAt" : "2020-06-11T17:45:01Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "59583687-66f0-4a35-84bb-31fef39d3522",
        "parentId" : "b5870d35-1e39-4760-92a5-fb0b6b6055e9",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "So the existing block replication is for the case where blocks we stored on two machines and due to executor loss are now down to one machine so they are replicated. It's not useless but it doesn't solve the same core problem.",
        "createdAt" : "2020-06-23T19:19:02Z",
        "updatedAt" : "2020-06-23T19:19:03Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "c34305662cd91d05b5160c7de72e35e4108b6755",
    "line" : 32,
    "diffHunk" : "@@ -1,1 +1565,1569 @@   * @return whether block was successfully replicated or not\n   */\n  def replicateBlock(\n      blockId: BlockId,\n      existingReplicas: Set[BlockManagerId],"
  },
  {
    "id" : "6acc74ec-b845-4022-a48f-2671eeea11f8",
    "prId" : 28331,
    "prUrl" : "https://github.com/apache/spark/pull/28331#pullrequestreview-418578500",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d5524c20-1efe-4a1f-89aa-364d67cbdd94",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "I don't find `decommissionBlockManager` in current master branch?",
        "createdAt" : "2020-05-03T06:18:19Z",
        "updatedAt" : "2020-06-02T18:20:15Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "c71a6d3a-6e28-4a65-9bf3-db05e62eb43d",
        "parentId" : "d5524c20-1efe-4a1f-89aa-364d67cbdd94",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Yeah this was added in the cache block migration PR (now merged)",
        "createdAt" : "2020-05-26T18:45:12Z",
        "updatedAt" : "2020-06-02T18:20:16Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "a3aa8ebe11de6011fc77daa5b805af1193992d74",
    "line" : 82,
    "diffHunk" : "@@ -1,1 +1808,1812 @@      blockManagerDecommissioning = true\n      decommissionManager = Some(new BlockManagerDecommissionManager(conf))\n      decommissionManager.foreach(_.start())\n    } else {\n      logDebug(\"Block manager already in decommissioning state\")"
  },
  {
    "id" : "8e82e4ce-778f-4cf3-811d-e13f350786ed",
    "prId" : 28331,
    "prUrl" : "https://github.com/apache/spark/pull/28331#pullrequestreview-420679448",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c2d7cda6-0f35-44dd-b61d-f37518fe90db",
        "parentId" : null,
        "authorId" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "body" : "The `(blockId.isShuffle || blockId.isInternalShuffle)` covers:\r\n- `ShuffleBlockId`\r\n- `ShuffleBlockBatchId`\r\n- `ShuffleDataBlockId`\r\n- `ShuffleIndexBlockId`\r\n\r\nBut the `putShuffleBlockAsStream()` is prepared for only these two:\r\n- `ShuffleIndexBlockId`\r\nhttps://github.com/apache/spark/blob/be2a5e736e051ca0497906b2a2e904c7b4033596/core/src/main/scala/org/apache/spark/shuffle/IndexShuffleBlockResolver.scala#L182\r\n- `ShuffleBlockBatchId` \r\nhttps://github.com/apache/spark/blob/be2a5e736e051ca0497906b2a2e904c7b4033596/core/src/main/scala/org/apache/spark/shuffle/IndexShuffleBlockResolver.scala#L184\r\n\r\nAnd as I know shuffle block batch IDs are used for fetching blocks where for the same map ID continuous reducer IDs are requested. To merge those continues block ids together and fetch them with one request so I assume they are not what we need here. \r\n\r\nFinally the `getMigrationBlocks()` returns only a `ShuffleIndexBlockId ` and a `ShuffleDataBlockId` which is correct.\r\n\r\nWe should harmonise three places (methods) regarding the block IDs they are handling: use the `isInternalShuffle` within this condition and fix `putShuffleBlockAsStream()`.\r\n",
        "createdAt" : "2020-05-07T12:36:14Z",
        "updatedAt" : "2020-06-02T18:20:15Z",
        "lastEditedBy" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "tags" : [
        ]
      },
      {
        "id" : "f7b90cec-4222-480a-bb05-3f753d854e24",
        "parentId" : "c2d7cda6-0f35-44dd-b61d-f37518fe90db",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "This is a good point, I wanted to delegate all shuffle blocks for when we generalize the interface a bit but I haven't done that yet. I'll revisit this once we get the cache block sorted out.",
        "createdAt" : "2020-05-07T21:43:58Z",
        "updatedAt" : "2020-06-02T18:20:15Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "6e0008ce-3063-4d45-b30c-7c03dd085afc",
        "parentId" : "c2d7cda6-0f35-44dd-b61d-f37518fe90db",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "So I've changed the interface to be more general, so I think probably this makes sense now. For the Index resolver we're still only going to see Data & index blocks, but if other resolvers implement the trait then we might see other blocks transfered.",
        "createdAt" : "2020-05-29T06:19:46Z",
        "updatedAt" : "2020-06-02T18:20:16Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "a3aa8ebe11de6011fc77daa5b805af1193992d74",
    "line" : 40,
    "diffHunk" : "@@ -1,1 +659,663 @@      classTag: ClassTag[_]): StreamCallbackWithID = {\n    // Delegate shuffle blocks here to resolver if supported\n    if (blockId.isShuffle || blockId.isInternalShuffle) {\n      logInfo(s\"Putting shuffle block ${blockId}\")\n      try {"
  },
  {
    "id" : "81be4847-3332-4f43-9290-8165e3855994",
    "prId" : 27864,
    "prUrl" : "https://github.com/apache/spark/pull/27864#pullrequestreview-374643666",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f8d534e7-578f-4644-b96e-9864a1cf591e",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Shall we have a `logDebug` when it's already decommissioning (`blockManagerDecommissioning = true`)?",
        "createdAt" : "2020-03-13T17:30:56Z",
        "updatedAt" : "2020-04-23T14:23:15Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "277d9b79-7ac8-4d02-8d07-639bb166c31f",
        "parentId" : "f8d534e7-578f-4644-b96e-9864a1cf591e",
        "authorId" : "ad4cb313-47b2-4cfa-9c1f-0a3596794a67",
        "body" : "done.",
        "createdAt" : "2020-03-13T22:04:11Z",
        "updatedAt" : "2020-04-23T14:23:15Z",
        "lastEditedBy" : "ad4cb313-47b2-4cfa-9c1f-0a3596794a67",
        "tags" : [
        ]
      }
    ],
    "commit" : "bb324f946019b8d700c517cc5eb2f7c11dc70cfc",
    "line" : 101,
    "diffHunk" : "@@ -1,1 +1777,1781 @@\n  def decommissionBlockManager(): Unit = {\n    if (!blockManagerDecommissioning) {\n      logInfo(\"Starting block manager decommissioning process\")\n      blockManagerDecommissioning = true"
  },
  {
    "id" : "16e5af17-73c7-4a2a-a8bf-38fb1c5e1f85",
    "prId" : 27864,
    "prUrl" : "https://github.com/apache/spark/pull/27864#pullrequestreview-399473744",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1a0ee909-9c06-4df4-a3b5-8187335e3735",
        "parentId" : null,
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "I don't know if we need it but replicateBlock is blocking and it seems like maybe async + futures might help us migrate more blocks? Especially if one host is underload we might block on sending a block to that host before we move forward.",
        "createdAt" : "2020-04-06T21:00:29Z",
        "updatedAt" : "2020-04-23T14:23:15Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "c246d142-480d-4e94-9bfd-a38dd2dd06f3",
        "parentId" : "1a0ee909-9c06-4df4-a3b5-8187335e3735",
        "authorId" : "ad4cb313-47b2-4cfa-9c1f-0a3596794a67",
        "body" : "@holdenk I am not sure how this will behave when multiple executors on same host machine are decommissioning. And each one of them is doing it in parallel - may cause some sort of network congestion? I have updated code to do replication in ThreadPool of size 4. Maybe we should make this configurable? any suggestions?\r\n",
        "createdAt" : "2020-04-23T14:29:14Z",
        "updatedAt" : "2020-04-23T14:35:46Z",
        "lastEditedBy" : "ad4cb313-47b2-4cfa-9c1f-0a3596794a67",
        "tags" : [
        ]
      },
      {
        "id" : "4fc99724-f129-4594-8bae-230b48ab3cc5",
        "parentId" : "1a0ee909-9c06-4df4-a3b5-8187335e3735",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Network congestion is certainly a possibility, I think that for now this strike a good balance between simple code and avoiding hanging all transfers if we have one slow target host. We can revisit this in the future if it turns out we need more control in production environments. Sound good?",
        "createdAt" : "2020-04-23T21:02:14Z",
        "updatedAt" : "2020-04-23T21:07:09Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "bb324f946019b8d700c517cc5eb2f7c11dc70cfc",
    "line" : 132,
    "diffHunk" : "@@ -1,1 +1808,1812 @@      replicateBlocksInfo, \"decommissionRddCacheBlocks\", 4) {\n      case ReplicateBlock(blockId, existingReplicas, maxReplicas) =>\n        val replicatedSuccessfully = replicateBlock(\n          blockId,\n          existingReplicas.toSet,"
  },
  {
    "id" : "ea47baa2-ade8-4ae0-a9f3-292df565efa4",
    "prId" : 27864,
    "prUrl" : "https://github.com/apache/spark/pull/27864#pullrequestreview-389982961",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cb24fbec-723a-403e-a270-515eaa54aa5a",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Shall we have a `logInfo` to match with line 1933, `logInfo(\"Stopping cache replication thread\")`?",
        "createdAt" : "2020-04-07T18:30:33Z",
        "updatedAt" : "2020-04-23T14:23:15Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "ec4e6fe0-9f26-4d05-8af1-e0e669c313d8",
        "parentId" : "cb24fbec-723a-403e-a270-515eaa54aa5a",
        "authorId" : "ad4cb313-47b2-4cfa-9c1f-0a3596794a67",
        "body" : "done.",
        "createdAt" : "2020-04-08T13:35:41Z",
        "updatedAt" : "2020-04-23T14:23:15Z",
        "lastEditedBy" : "ad4cb313-47b2-4cfa-9c1f-0a3596794a67",
        "tags" : [
        ]
      },
      {
        "id" : "ad73daa2-59ef-4d97-8116-1154268aa608",
        "parentId" : "cb24fbec-723a-403e-a270-515eaa54aa5a",
        "authorId" : "ad4cb313-47b2-4cfa-9c1f-0a3596794a67",
        "body" : "added logInfo.",
        "createdAt" : "2020-04-08T13:39:04Z",
        "updatedAt" : "2020-04-23T14:23:15Z",
        "lastEditedBy" : "ad4cb313-47b2-4cfa-9c1f-0a3596794a67",
        "tags" : [
        ]
      }
    ],
    "commit" : "bb324f946019b8d700c517cc5eb2f7c11dc70cfc",
    "line" : 188,
    "diffHunk" : "@@ -1,1 +1925,1929 @@    blockReplicationThread.setName(\"block-replication-thread\")\n\n    def start(): Unit = {\n      logInfo(\"Starting block replication thread\")\n      blockReplicationThread.start()"
  },
  {
    "id" : "c6f99103-1a20-43f6-996c-2aad3a1396b1",
    "prId" : 25262,
    "prUrl" : "https://github.com/apache/spark/pull/25262#pullrequestreview-269085709",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "13dc5f21-058c-43ae-9f4f-51dcf825fd65",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "I think you only need to change scope of TempFileBasedBlockStoreUpdater, not BlockStoreUpdater?",
        "createdAt" : "2019-07-31T03:33:45Z",
        "updatedAt" : "2019-07-31T15:21:29Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "31f018ef-2495-4634-a723-404bdd3c5b75",
        "parentId" : "13dc5f21-058c-43ae-9f4f-51dcf825fd65",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "I'm afraid not. if we revive the scope here, compiler would say:\r\n\r\n```\r\n[error] /Users/wuyi/workspace/spark/core/src/main/scala/org/apache/spark/storage/BlockManager.scala:368: private class BlockStoreUpdater escapes its defining scope as part of type BlockManager.this.BlockStoreUpdater[T]\r\n[error]     extends BlockStoreUpdater[T](blockSize, blockId, level, classTag, tellMaster, keepReadLock) {\r\n[error]             ^\r\n[error] /Users/wuyi/workspace/spark/core/src/main/scala/org/apache/spark/storage/BlockManager.scala:368: no arguments allowed for nullary constructor Object: ()Object\r\n[error]     extends BlockStoreUpdater[T](blockSize, blockId, level, classTag, tellMaster, keepReadLock) {\r\n[error]                                  ^\r\n[error] /Users/wuyi/workspace/spark/core/src/main/scala/org/apache/spark/storage/BlockManager.scala:383: value save is not a member of Product with Serializable\r\n[error]       val res = super.save()\r\n[error]                       ^\r\n[error] three errors found\r\n[error] Compile failed at 2019-7-31 23:08:25 [36.500s]\r\n```",
        "createdAt" : "2019-07-31T15:13:15Z",
        "updatedAt" : "2019-07-31T15:21:29Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "5cdbab3659edd219bff462ce4d004724cff68e10",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +212,216 @@   * @param blockSize the decrypted size of the block\n   */\n  private[spark] abstract class BlockStoreUpdater[T](\n      blockSize: Long,\n      blockId: BlockId,"
  },
  {
    "id" : "c1e29b1e-f23a-43dc-89ca-9e88846eeafc",
    "prId" : 24699,
    "prUrl" : "https://github.com/apache/spark/pull/24699#pullrequestreview-243016283",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4159a5e4-31f2-416e-9d64-bc31ae478131",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "nit: a missing `,` before `for example`.",
        "createdAt" : "2019-05-29T02:49:40Z",
        "updatedAt" : "2019-06-17T13:02:04Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "7f0f360dfb734c44ed2773c60d340d47b17f33fb",
    "line" : 36,
    "diffHunk" : "@@ -1,1 +994,998 @@   * Release a lock on the given block with explicit TaskContext.\n   * The param `taskContext` should be passed in case we can't get the correct TaskContext,\n   * for example, the input iterator of a cached RDD iterates to the end in a child\n   * thread.\n   */"
  },
  {
    "id" : "222d3597-331e-40ed-8841-c7da59e66046",
    "prId" : 24554,
    "prUrl" : "https://github.com/apache/spark/pull/24554#pullrequestreview-236246355",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f6bdd1de-c004-46e9-a4c5-27640642f626",
        "parentId" : null,
        "authorId" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "body" : "Imran already commented on this, but it would be good to know what happens if this file disappears after this point. Because `EncryptedManagedBuffer`/`EncryptedBlockData` will not open the file right away, so you may get some exception later, and it would be good to know that the rest of the code will recover properly from that.\r\n\r\n(I guess it wouldn't be worse than getting an `IOException` while reading your own cached block, but doesn't hurt to check.)",
        "createdAt" : "2019-05-10T18:40:41Z",
        "updatedAt" : "2019-06-07T17:58:12Z",
        "lastEditedBy" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "tags" : [
        ]
      }
    ],
    "commit" : "261fc767a23061844e32b1a7ec0e8fd58c42e12c",
    "line" : 164,
    "diffHunk" : "@@ -1,1 +1004,1008 @@      blockSize: Long): Option[ManagedBuffer] = {\n    val file = ExecutorDiskUtils.getFile(localDirs, subDirsPerLocalDir, blockId.name)\n    if (file.exists()) {\n      val mangedBuffer = securityManager.getIOEncryptionKey() match {\n        case Some(key) =>"
  },
  {
    "id" : "0a6051de-e77b-497e-ae0d-d59226072222",
    "prId" : 24499,
    "prUrl" : "https://github.com/apache/spark/pull/24499#pullrequestreview-238814473",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fd523f86-d593-4a67-8f70-59914a35a2d4",
        "parentId" : null,
        "authorId" : "e913f9af-103d-40d4-976e-9aa8e12e7211",
        "body" : "if you're not using the external shuffle client, where is the shuffle client getting initialized now?",
        "createdAt" : "2019-05-16T21:31:26Z",
        "updatedAt" : "2019-05-23T11:27:05Z",
        "lastEditedBy" : "e913f9af-103d-40d4-976e-9aa8e12e7211",
        "tags" : [
        ]
      },
      {
        "id" : "c77ad462-c62d-41a2-8a1a-3d1a1b341515",
        "parentId" : "fd523f86-d593-4a67-8f70-59914a35a2d4",
        "authorId" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "body" : "There were two `init` methods for `BlockTransferService`:\r\n1) One with the `appID` was inherited from `ShuffleClient` and had an empty body:\r\nhttps://github.com/apache/spark/blob/5476f6c078a05d8d2dd31acb513d312c9f28dd2c/common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/ShuffleClient.java#L28-L33\r\n\r\nAnd **I have removed this one** (as it was probably just to avoid an if condition and a casting of `externalShuffleClient` to `ExternalShuffleClient` but now we have the if condition within the `Option[ExternalShuffleClient]`).\r\n\r\n2) The other `init` is: `org.apache.spark.network.BlockTransferService#init` which gets a `BlockDataManager`. This one is called right before your comment in the line 397 (new numbering):\r\nhttps://github.com/apache/spark/blob/a849554eba22db846bc00efc5b36e2438b481f6a/core/src/main/scala/org/apache/spark/storage/BlockManager.scala#L396-L400\r\n\r\nThese double `init` was a bit misleading, I am positive this even improves the readability. Otherwise the `available only after [[init]] is invoked` appearing in many comments before the `BlockTransferService` methods should even mention by which of the `init` method exactly. \r\n",
        "createdAt" : "2019-05-17T09:06:37Z",
        "updatedAt" : "2019-05-23T11:27:05Z",
        "lastEditedBy" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "tags" : [
        ]
      }
    ],
    "commit" : "faa583f88b410fc7ededafe36bcb1ef878482d44",
    "line" : 68,
    "diffHunk" : "@@ -1,1 +397,401 @@    blockTransferService.init(this)\n    externalShuffleClient.foreach { shuffleClient =>\n      shuffleClient.init(appId)\n    }\n    blockReplicationPolicy = {"
  }
]