[
  {
    "id" : "57fc64b8-f9d2-4fb2-9b4f-40391728c126",
    "prId" : 30842,
    "prUrl" : "https://github.com/apache/spark/pull/30842#pullrequestreview-555634331",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8953a66e-4eb3-42fa-a381-d0d8d3c8c086",
        "parentId" : null,
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "it would be nice to add a comment here describing notStale and why we need it.",
        "createdAt" : "2020-12-18T14:10:30Z",
        "updatedAt" : "2020-12-18T16:26:27Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "493e98aa-0cd3-40d6-9cca-59dad71afd0e",
        "parentId" : "8953a66e-4eb3-42fa-a381-d0d8d3c8c086",
        "authorId" : "25fa223a-19ad-476f-91b0-c26932236a2b",
        "body" : "@tgravescs thank you for the review!\r\nPR has been updated with a comment.",
        "createdAt" : "2020-12-18T16:28:04Z",
        "updatedAt" : "2020-12-18T16:28:04Z",
        "lastEditedBy" : "25fa223a-19ad-476f-91b0-c26932236a2b",
        "tags" : [
        ]
      }
    ],
    "commit" : "c2b8c2d24ffba385ad345f838b0a9ce1e562d93b",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +465,469 @@      // right after this check and before the check for stale entities will be identified as stale\n      // and will be deleted from the UI until the next 'checkForLogs' run.\n      val notStale = mutable.HashSet[String]()\n      val updated = Option(fs.listStatus(new Path(logDir))).map(_.toSeq).getOrElse(Nil)\n        .filter { entry => !isBlacklisted(entry.getPath) }"
  },
  {
    "id" : "b5fb4de3-1ad3-4754-a36c-d5c32dd11f9e",
    "prId" : 30353,
    "prUrl" : "https://github.com/apache/spark/pull/30353#pullrequestreview-530807185",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a683f3c3-9e35-436e-9b42-51e8d86302c9",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Hm, these all look like a performance non-sensitive code path. Is it worthwhile to sweep and fix? I am not sure because it will make more conflicts when reverting, backporting, etc. `Option == Some(id)` can be sort of more readable in a way.",
        "createdAt" : "2020-11-12T12:32:56Z",
        "updatedAt" : "2020-11-13T11:47:20Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "da60323a-ce54-443c-b300-ea498b6a49dc",
        "parentId" : "a683f3c3-9e35-436e-9b42-51e8d86302c9",
        "authorId" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "body" : "mainly to reduce unnecessary object creation, if the readability is affected, it can be left unchanged",
        "createdAt" : "2020-11-12T12:57:13Z",
        "updatedAt" : "2020-11-13T11:47:20Z",
        "lastEditedBy" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "tags" : [
        ]
      },
      {
        "id" : "fd20dcda-23a2-4081-9d7d-876c24f9d276",
        "parentId" : "a683f3c3-9e35-436e-9b42-51e8d86302c9",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "I have the same comment; I don't think it matters much either way. If it's just 14 instances, OK.",
        "createdAt" : "2020-11-15T15:07:50Z",
        "updatedAt" : "2020-11-15T15:07:51Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      }
    ],
    "commit" : "e95295ad35fe472decb4814c45096fb4a5040a51",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +666,670 @@      // If no attempt is specified, or there is no attemptId for attempts, return all attempts\n      attemptId\n        .map { id => app.attempts.filter(_.info.attemptId.contains(id)) }\n        .getOrElse(app.attempts)\n        .foreach { attempt =>"
  },
  {
    "id" : "8b5a29e8-857b-4a41-ab22-4a169cb45500",
    "prId" : 30037,
    "prUrl" : "https://github.com/apache/spark/pull/30037#pullrequestreview-508312899",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8e9e97af-c304-4428-bffc-5af0e6b7a040",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "I roughly remember this was pointed out earlier, but we wanted more elegant code and during finding we forgot the actual issue. Happy to see it fixed finally.",
        "createdAt" : "2020-10-14T12:54:31Z",
        "updatedAt" : "2020-10-14T13:13:33Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "0efba0ad3972f3031d5d4c4a14c88d0e3c8f37e7",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +539,543 @@              } catch {\n                case _: FileNotFoundException => false\n                case NonFatal(e) =>\n                  logWarning(s\"Error while reading new log ${reader.rootPath}\", e)\n                  false"
  },
  {
    "id" : "a3ed4ec9-fd79-4728-9529-b8cc1604fd5e",
    "prId" : 29630,
    "prUrl" : "https://github.com/apache/spark/pull/29630#pullrequestreview-481622962",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "24b75032-3396-4677-904f-6f37e43400f4",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "This is playing as a \"lock\" for the log file. You can see where the lock flag is accessed or/and modified.",
        "createdAt" : "2020-09-03T08:41:43Z",
        "updatedAt" : "2020-09-03T08:41:43Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "5cce482ac0a623aaa9d4657995bb2f5574a9a9a4",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +474,478 @@      val updated = Option(fs.globStatus(new Path(logDir + \"/*\"))).map(_.toSeq).getOrElse(Nil)\n        .filter { entry => isAccessible(entry.getPath) }\n        .filter { entry => !isProcessing(entry.getPath) }\n        .flatMap { entry => EventLogFileReader(fs, entry) }\n        .filter { reader =>"
  },
  {
    "id" : "46f99457-622b-417a-85fd-bc5afbae26b2",
    "prId" : 29630,
    "prUrl" : "https://github.com/apache/spark/pull/29630#pullrequestreview-484304220",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "84700f66-ae0c-4871-9b4e-f3f03b27ad3a",
        "parentId" : null,
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "yeah I don't quite get this, you are requiring all the directories to be under a single directory, I guess that makes this logic easier, but at the same time why the restriction and not have a list?  If we are going to support multiple directories and make sure it works I don't see the reason to have this restriction.  What if people have multiple clusters writing to different HDFS filesystems for instance.  \r\n\r\nI agree with @HeartSaVioR if we are going to support multiple directories we need to have a thorough look at all the logic here to make sure no other problems. I guess in this case you are using a single filesystem?\r\n\r\nI think we need to flush out more of the overall goals and design first",
        "createdAt" : "2020-09-03T13:32:52Z",
        "updatedAt" : "2020-09-03T13:32:52Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "89c68049-79e7-4af0-95b9-190e542c3145",
        "parentId" : "84700f66-ae0c-4871-9b4e-f3f03b27ad3a",
        "authorId" : "8b74a389-1f07-40ba-bd69-8c2de0d3c8bb",
        "body" : "Yes, we are using a single file system in this case. This feature is useful when using external file systems for log data because then multiple directories will correspond to multiple clusters.\r\n\r\nWe could have a list too. Looking into that option.",
        "createdAt" : "2020-09-03T17:57:17Z",
        "updatedAt" : "2020-09-03T17:57:17Z",
        "lastEditedBy" : "8b74a389-1f07-40ba-bd69-8c2de0d3c8bb",
        "tags" : [
        ]
      },
      {
        "id" : "b99542b9-2be4-4344-a654-84b8de4caa28",
        "parentId" : "84700f66-ae0c-4871-9b4e-f3f03b27ad3a",
        "authorId" : "8b74a389-1f07-40ba-bd69-8c2de0d3c8bb",
        "body" : "The reason we chose to go with glob-pattern is that our service creates short-lived YARN clusters on which Spark applications are run. Log data goes to directories in a remote file-system. Since the number of these clusters can be really large, glob pattern would better fit our use-case, than a static list would.",
        "createdAt" : "2020-09-08T16:23:51Z",
        "updatedAt" : "2020-09-08T16:23:52Z",
        "lastEditedBy" : "8b74a389-1f07-40ba-bd69-8c2de0d3c8bb",
        "tags" : [
        ]
      }
    ],
    "commit" : "5cce482ac0a623aaa9d4657995bb2f5574a9a9a4",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +472,476 @@      logDebug(s\"Scanning $logDir with lastScanTime==$lastScanTime\")\n\n      val updated = Option(fs.globStatus(new Path(logDir + \"/*\"))).map(_.toSeq).getOrElse(Nil)\n        .filter { entry => isAccessible(entry.getPath) }\n        .filter { entry => !isProcessing(entry.getPath) }"
  },
  {
    "id" : "0c81582e-f21e-47a8-97a5-9ceaf8c85957",
    "prId" : 29350,
    "prUrl" : "https://github.com/apache/spark/pull/29350#pullrequestreview-461389982",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ca1a0a16-66d8-48df-9fbd-e91e9e68c8a8",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "nit: I'd have empty new line after `}`.",
        "createdAt" : "2020-08-05T05:13:38Z",
        "updatedAt" : "2020-08-05T06:32:07Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "2cf0f6de-f65f-4500-a588-bcdad19bd9ab",
        "parentId" : "ca1a0a16-66d8-48df-9fbd-e91e9e68c8a8",
        "authorId" : "e9672703-dc17-430b-a47b-52d125d0cc78",
        "body" : "added\r\n\r\n> nit: I'd have empty new line after `}`.\r\n\r\n",
        "createdAt" : "2020-08-05T06:59:27Z",
        "updatedAt" : "2020-08-05T06:59:27Z",
        "lastEditedBy" : "e9672703-dc17-430b-a47b-52d125d0cc78",
        "tags" : [
        ]
      }
    ],
    "commit" : "d1bf4caa30231a41fd4e6025c34af71c5f15e07e",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +540,544 @@              }\n\n            case _: FileNotFoundException =>\n              false\n          }"
  }
]