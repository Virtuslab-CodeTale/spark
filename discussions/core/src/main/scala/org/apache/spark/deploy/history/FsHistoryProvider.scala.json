[
  {
    "id" : "57fc64b8-f9d2-4fb2-9b4f-40391728c126",
    "prId" : 30842,
    "prUrl" : "https://github.com/apache/spark/pull/30842#pullrequestreview-555634331",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8953a66e-4eb3-42fa-a381-d0d8d3c8c086",
        "parentId" : null,
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "it would be nice to add a comment here describing notStale and why we need it.",
        "createdAt" : "2020-12-18T14:10:30Z",
        "updatedAt" : "2020-12-18T16:26:27Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "493e98aa-0cd3-40d6-9cca-59dad71afd0e",
        "parentId" : "8953a66e-4eb3-42fa-a381-d0d8d3c8c086",
        "authorId" : "25fa223a-19ad-476f-91b0-c26932236a2b",
        "body" : "@tgravescs thank you for the review!\r\nPR has been updated with a comment.",
        "createdAt" : "2020-12-18T16:28:04Z",
        "updatedAt" : "2020-12-18T16:28:04Z",
        "lastEditedBy" : "25fa223a-19ad-476f-91b0-c26932236a2b",
        "tags" : [
        ]
      }
    ],
    "commit" : "c2b8c2d24ffba385ad345f838b0a9ce1e562d93b",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +465,469 @@      // right after this check and before the check for stale entities will be identified as stale\n      // and will be deleted from the UI until the next 'checkForLogs' run.\n      val notStale = mutable.HashSet[String]()\n      val updated = Option(fs.listStatus(new Path(logDir))).map(_.toSeq).getOrElse(Nil)\n        .filter { entry => !isBlacklisted(entry.getPath) }"
  },
  {
    "id" : "b5fb4de3-1ad3-4754-a36c-d5c32dd11f9e",
    "prId" : 30353,
    "prUrl" : "https://github.com/apache/spark/pull/30353#pullrequestreview-530807185",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a683f3c3-9e35-436e-9b42-51e8d86302c9",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Hm, these all look like a performance non-sensitive code path. Is it worthwhile to sweep and fix? I am not sure because it will make more conflicts when reverting, backporting, etc. `Option == Some(id)` can be sort of more readable in a way.",
        "createdAt" : "2020-11-12T12:32:56Z",
        "updatedAt" : "2020-11-13T11:47:20Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "da60323a-ce54-443c-b300-ea498b6a49dc",
        "parentId" : "a683f3c3-9e35-436e-9b42-51e8d86302c9",
        "authorId" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "body" : "mainly to reduce unnecessary object creation, if the readability is affected, it can be left unchanged",
        "createdAt" : "2020-11-12T12:57:13Z",
        "updatedAt" : "2020-11-13T11:47:20Z",
        "lastEditedBy" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "tags" : [
        ]
      },
      {
        "id" : "fd20dcda-23a2-4081-9d7d-876c24f9d276",
        "parentId" : "a683f3c3-9e35-436e-9b42-51e8d86302c9",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "I have the same comment; I don't think it matters much either way. If it's just 14 instances, OK.",
        "createdAt" : "2020-11-15T15:07:50Z",
        "updatedAt" : "2020-11-15T15:07:51Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      }
    ],
    "commit" : "e95295ad35fe472decb4814c45096fb4a5040a51",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +666,670 @@      // If no attempt is specified, or there is no attemptId for attempts, return all attempts\n      attemptId\n        .map { id => app.attempts.filter(_.info.attemptId.contains(id)) }\n        .getOrElse(app.attempts)\n        .foreach { attempt =>"
  },
  {
    "id" : "8b5a29e8-857b-4a41-ab22-4a169cb45500",
    "prId" : 30037,
    "prUrl" : "https://github.com/apache/spark/pull/30037#pullrequestreview-508312899",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8e9e97af-c304-4428-bffc-5af0e6b7a040",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "I roughly remember this was pointed out earlier, but we wanted more elegant code and during finding we forgot the actual issue. Happy to see it fixed finally.",
        "createdAt" : "2020-10-14T12:54:31Z",
        "updatedAt" : "2020-10-14T13:13:33Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "0efba0ad3972f3031d5d4c4a14c88d0e3c8f37e7",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +539,543 @@              } catch {\n                case _: FileNotFoundException => false\n                case NonFatal(e) =>\n                  logWarning(s\"Error while reading new log ${reader.rootPath}\", e)\n                  false"
  },
  {
    "id" : "a3ed4ec9-fd79-4728-9529-b8cc1604fd5e",
    "prId" : 29630,
    "prUrl" : "https://github.com/apache/spark/pull/29630#pullrequestreview-481622962",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "24b75032-3396-4677-904f-6f37e43400f4",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "This is playing as a \"lock\" for the log file. You can see where the lock flag is accessed or/and modified.",
        "createdAt" : "2020-09-03T08:41:43Z",
        "updatedAt" : "2020-09-03T08:41:43Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "5cce482ac0a623aaa9d4657995bb2f5574a9a9a4",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +474,478 @@      val updated = Option(fs.globStatus(new Path(logDir + \"/*\"))).map(_.toSeq).getOrElse(Nil)\n        .filter { entry => isAccessible(entry.getPath) }\n        .filter { entry => !isProcessing(entry.getPath) }\n        .flatMap { entry => EventLogFileReader(fs, entry) }\n        .filter { reader =>"
  },
  {
    "id" : "46f99457-622b-417a-85fd-bc5afbae26b2",
    "prId" : 29630,
    "prUrl" : "https://github.com/apache/spark/pull/29630#pullrequestreview-484304220",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "84700f66-ae0c-4871-9b4e-f3f03b27ad3a",
        "parentId" : null,
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "yeah I don't quite get this, you are requiring all the directories to be under a single directory, I guess that makes this logic easier, but at the same time why the restriction and not have a list?  If we are going to support multiple directories and make sure it works I don't see the reason to have this restriction.  What if people have multiple clusters writing to different HDFS filesystems for instance.  \r\n\r\nI agree with @HeartSaVioR if we are going to support multiple directories we need to have a thorough look at all the logic here to make sure no other problems. I guess in this case you are using a single filesystem?\r\n\r\nI think we need to flush out more of the overall goals and design first",
        "createdAt" : "2020-09-03T13:32:52Z",
        "updatedAt" : "2020-09-03T13:32:52Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "89c68049-79e7-4af0-95b9-190e542c3145",
        "parentId" : "84700f66-ae0c-4871-9b4e-f3f03b27ad3a",
        "authorId" : "8b74a389-1f07-40ba-bd69-8c2de0d3c8bb",
        "body" : "Yes, we are using a single file system in this case. This feature is useful when using external file systems for log data because then multiple directories will correspond to multiple clusters.\r\n\r\nWe could have a list too. Looking into that option.",
        "createdAt" : "2020-09-03T17:57:17Z",
        "updatedAt" : "2020-09-03T17:57:17Z",
        "lastEditedBy" : "8b74a389-1f07-40ba-bd69-8c2de0d3c8bb",
        "tags" : [
        ]
      },
      {
        "id" : "b99542b9-2be4-4344-a654-84b8de4caa28",
        "parentId" : "84700f66-ae0c-4871-9b4e-f3f03b27ad3a",
        "authorId" : "8b74a389-1f07-40ba-bd69-8c2de0d3c8bb",
        "body" : "The reason we chose to go with glob-pattern is that our service creates short-lived YARN clusters on which Spark applications are run. Log data goes to directories in a remote file-system. Since the number of these clusters can be really large, glob pattern would better fit our use-case, than a static list would.",
        "createdAt" : "2020-09-08T16:23:51Z",
        "updatedAt" : "2020-09-08T16:23:52Z",
        "lastEditedBy" : "8b74a389-1f07-40ba-bd69-8c2de0d3c8bb",
        "tags" : [
        ]
      }
    ],
    "commit" : "5cce482ac0a623aaa9d4657995bb2f5574a9a9a4",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +472,476 @@      logDebug(s\"Scanning $logDir with lastScanTime==$lastScanTime\")\n\n      val updated = Option(fs.globStatus(new Path(logDir + \"/*\"))).map(_.toSeq).getOrElse(Nil)\n        .filter { entry => isAccessible(entry.getPath) }\n        .filter { entry => !isProcessing(entry.getPath) }"
  },
  {
    "id" : "0c81582e-f21e-47a8-97a5-9ceaf8c85957",
    "prId" : 29350,
    "prUrl" : "https://github.com/apache/spark/pull/29350#pullrequestreview-461389982",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ca1a0a16-66d8-48df-9fbd-e91e9e68c8a8",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "nit: I'd have empty new line after `}`.",
        "createdAt" : "2020-08-05T05:13:38Z",
        "updatedAt" : "2020-08-05T06:32:07Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "2cf0f6de-f65f-4500-a588-bcdad19bd9ab",
        "parentId" : "ca1a0a16-66d8-48df-9fbd-e91e9e68c8a8",
        "authorId" : "e9672703-dc17-430b-a47b-52d125d0cc78",
        "body" : "added\r\n\r\n> nit: I'd have empty new line after `}`.\r\n\r\n",
        "createdAt" : "2020-08-05T06:59:27Z",
        "updatedAt" : "2020-08-05T06:59:27Z",
        "lastEditedBy" : "e9672703-dc17-430b-a47b-52d125d0cc78",
        "tags" : [
        ]
      }
    ],
    "commit" : "d1bf4caa30231a41fd4e6025c34af71c5f15e07e",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +540,544 @@              }\n\n            case _: FileNotFoundException =>\n              false\n          }"
  },
  {
    "id" : "e8d67d10-ad80-42fe-b3ea-d0f5ff89becd",
    "prId" : 28874,
    "prUrl" : "https://github.com/apache/spark/pull/28874#pullrequestreview-438437522",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "16465e28-0c28-45ce-9de1-cd0e1cfd4cfe",
        "parentId" : null,
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "I think this should function should be isInAccessible or you have to change its logic. You added the ! but the isAccessible function does the same thing the isBlacklisted did.  Probably why tests are failing.",
        "createdAt" : "2020-06-25T13:23:08Z",
        "updatedAt" : "2020-07-14T15:31:39Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "9b7c3ef6-8b69-4718-b9a0-f98117b1e88b",
        "parentId" : "16465e28-0c28-45ce-9de1-cd0e1cfd4cfe",
        "authorId" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "body" : "Thanks for the catch, my mistake. You're right that I remembered to update the callsites but not the definition.",
        "createdAt" : "2020-06-26T16:34:46Z",
        "updatedAt" : "2020-07-14T15:31:39Z",
        "lastEditedBy" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "tags" : [
        ]
      }
    ],
    "commit" : "1ba58fb285cc0a1ff28e452a61c8521d5b308a32",
    "line" : 69,
    "diffHunk" : "@@ -1,1 +1237,1241 @@  private def deleteLog(fs: FileSystem, log: Path): Boolean = {\n    var deleted = false\n    if (!isAccessible(log)) {\n      logDebug(s\"Skipping deleting $log as we don't have permissions on it.\")\n    } else {"
  },
  {
    "id" : "eb36a401-e000-437b-bd57-db07d218edee",
    "prId" : 28412,
    "prUrl" : "https://github.com/apache/spark/pull/28412#pullrequestreview-431045908",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f6cfcb4b-e34f-462e-83d9-8394c5e256d6",
        "parentId" : null,
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "I find this function hard to read now. I would say at least put a comment here and possibly split the below logic into a separate function for easier readability.",
        "createdAt" : "2020-06-15T14:21:37Z",
        "updatedAt" : "2020-07-06T21:32:29Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "094fa272-1230-4ea9-b977-0b33d430ec09",
        "parentId" : "f6cfcb4b-e34f-462e-83d9-8394c5e256d6",
        "authorId" : "788d6907-ae49-42f5-95b7-ef69c1460937",
        "body" : "Addressed. Split the logic below to a new function createLevelDBStore().",
        "createdAt" : "2020-06-15T22:52:41Z",
        "updatedAt" : "2020-07-06T21:32:29Z",
        "lastEditedBy" : "788d6907-ae49-42f5-95b7-ef69c1460937",
        "tags" : [
        ]
      }
    ],
    "commit" : "b71a923e4554e814ee923b64d1b22ce42825378c",
    "line" : 45,
    "diffHunk" : "@@ -1,1 +1188,1192 @@      }\n    }\n\n    createLevelDBStore(dm, appId, attempt, metadata)\n  }"
  },
  {
    "id" : "2fa6637b-5310-4acc-bfd4-39fa6b06d647",
    "prId" : 27208,
    "prUrl" : "https://github.com/apache/spark/pull/27208#pullrequestreview-346933689",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "908cef6c-4e31-4e3b-a085-06a9fd8288cc",
        "parentId" : null,
        "authorId" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "body" : "So one thing that feels a tiny bit odd is that when deciding whether to compact, you're actually considering the last log file, which you won't consider during actual compaction, right?\r\n\r\nWouldn't that cause unnecessary (or too aggressive) compaction at the end of the application, when potentially a bunch of jobs finish and \"release\" lots of tasks, inflating the compation scoe?",
        "createdAt" : "2020-01-22T16:50:59Z",
        "updatedAt" : "2020-01-28T01:55:03Z",
        "lastEditedBy" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "tags" : [
        ]
      },
      {
        "id" : "b7388159-2d91-42e1-a3e4-6f2e93d6373f",
        "parentId" : "908cef6c-4e31-4e3b-a085-06a9fd8288cc",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "> So one thing that feels a tiny bit odd is that when deciding whether to compact, you're actually considering the last log file, which you won't consider during actual compaction, right?\r\n> Wouldn't that cause unnecessary (or too aggressive) compaction at the end of the application, when potentially a bunch of jobs finish and \"release\" lots of tasks, inflating the compation scoe?\r\n\r\nThat's the intention that callers of compactor don't care about how many files are actually affected. Callers of compactor just need to know that same list of log files would bring same result, unless it fails and throws exception. How many files are excluded in compaction is just a configuration, and the last log file should be excluded is an implementation detail. (We prevent it in both configuration and compactor via having 1 as min value for max retain log file.)\r\n\r\nCompactor will ignore the last log file in any way as configured, so unless the rare case where the log is rolled just before the app is finished, it won't happen. And most probably end users would avoid to set the value to 1 if they read the doc and understand how it works.",
        "createdAt" : "2020-01-22T22:00:16Z",
        "updatedAt" : "2020-01-28T01:55:03Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "ddd67885dd2c861481287b127a51c6159c2b8fbf",
    "line" : 147,
    "diffHunk" : "@@ -1,1 +825,829 @@                info.lastEvaluatedForCompaction.get < lastIndex) {\n              // haven't tried compaction for this index, do compaction\n              fileCompactor.compact(reader.listEventLogFiles)\n              listing.write(info.copy(lastEvaluatedForCompaction = Some(lastIndex)))\n            }"
  },
  {
    "id" : "f88f30c2-da7d-4165-abf7-b6cc7808d50b",
    "prId" : 27208,
    "prUrl" : "https://github.com/apache/spark/pull/27208#pullrequestreview-348364844",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c29efad0-42d2-486b-8197-dd4c8102d77d",
        "parentId" : null,
        "authorId" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "body" : "Hmm, perhaps this method could take care of calling `endProcessing` too (e.g. by wrapping the task)? Should be just a small adjustment at the call site.",
        "createdAt" : "2020-01-23T16:49:35Z",
        "updatedAt" : "2020-01-28T01:55:03Z",
        "lastEditedBy" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "tags" : [
        ]
      },
      {
        "id" : "73762bd3-410d-4ada-a88f-a05a663e95f5",
        "parentId" : "c29efad0-42d2-486b-8197-dd4c8102d77d",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "`finally` statement in `mergeApplicationListing` makes it be complicated, because we should handle the reentrance of lock. (We're submitting another task in the task.)\r\n\r\nIf we move endProcessing from `finally` statement in `mergeApplicationListing` to the end of task here, `processing` on the compaction task would be executed earlier than calling `endProcessing` on the listing task. Marking lock from compaction task would succeed but effectively no-op, and releasing lock from listing task would remove the mark for compaction task as well, which makes the compaction task run without proper lock.\r\n\r\nSo either we need to make lock much smarter, or document the requirement on caller side. I'm feeling that former one is more complicated than latter one.",
        "createdAt" : "2020-01-26T08:26:49Z",
        "updatedAt" : "2020-01-28T01:55:03Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "ddd67885dd2c861481287b127a51c6159c2b8fbf",
    "line" : 276,
    "diffHunk" : "@@ -1,1 +1239,1243 @@  }\n\n  /** NOTE: 'task' should ensure it executes 'endProcessing' at the end */\n  private def submitLogProcessTask(rootPath: Path)(task: Runnable): Unit = {\n    try {"
  },
  {
    "id" : "337c39d8-76ae-4310-b3a6-33727ad75474",
    "prId" : 26821,
    "prUrl" : "https://github.com/apache/spark/pull/26821#pullrequestreview-331022756",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7d38e20a-84f1-40f5-99ed-8668e63cbd26",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "We seem to not add `is`/`are` in the name of value even it's expected to return Boolean. Nearest example is `fastInProgressParsing`.",
        "createdAt" : "2019-12-12T06:54:26Z",
        "updatedAt" : "2019-12-12T09:22:04Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "b6b2c5a3cc46d25595bfa7e187846187dd5ac806",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +134,138 @@  // `IncrmentalInfo`. Whenever a new event come, parsing will happen from the line it\n  // read last time. Currently it supports inmemory store. TODO: Support for disk store.\n  private val isIncrementalParsingEnabled = storePath.isEmpty &&\n    conf.get(History.INCREMENTAL_PARSING_ENABLED)\n  private val storeMap = new ConcurrentHashMap[(String, Option[String]), KVStore]()"
  },
  {
    "id" : "64e8a1ee-96ba-402e-8659-3e21ae048aaf",
    "prId" : 26821,
    "prUrl" : "https://github.com/apache/spark/pull/26821#pullrequestreview-331022756",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0ca1bdce-7a87-4bc2-addc-c23341555dff",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Please place TODO as separate line of comment so that it helps IDE to highlight.",
        "createdAt" : "2019-12-12T06:55:01Z",
        "updatedAt" : "2019-12-12T09:22:04Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "b6b2c5a3cc46d25595bfa7e187846187dd5ac806",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +133,137 @@  // during invalidate UI or detached UI. Metadata of the event read will store in the\n  // `IncrmentalInfo`. Whenever a new event come, parsing will happen from the line it\n  // read last time. Currently it supports inmemory store. TODO: Support for disk store.\n  private val isIncrementalParsingEnabled = storePath.isEmpty &&\n    conf.get(History.INCREMENTAL_PARSING_ENABLED)"
  },
  {
    "id" : "bddfedc2-b744-476c-bd23-385e4ce368a4",
    "prId" : 26821,
    "prUrl" : "https://github.com/apache/spark/pull/26821#pullrequestreview-331022756",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c9916c6f-3023-43e4-bdfb-ab03da12e3f1",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "1. nit: `incremental parsing support configuration` seems odd. IMHO, just `incremental parsing` would work.\r\n2. there're two kinds of stores in FsHistoryProvider, so maybe better to clarify here; `underlying APP kvstore` or some better words?",
        "createdAt" : "2019-12-12T07:04:13Z",
        "updatedAt" : "2019-12-12T09:22:04Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "b6b2c5a3cc46d25595bfa7e187846187dd5ac806",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +130,134 @@  private val storePath = conf.get(LOCAL_STORE_DIR).map(new File(_))\n  private val fastInProgressParsing = conf.get(FAST_IN_PROGRESS_PARSING)\n  // If incremental parsing support configuration is enabled, underlying store will not close\n  // during invalidate UI or detached UI. Metadata of the event read will store in the\n  // `IncrmentalInfo`. Whenever a new event come, parsing will happen from the line it"
  },
  {
    "id" : "77c40469-063e-4ee8-ba89-d4f8091a0224",
    "prId" : 26821,
    "prUrl" : "https://github.com/apache/spark/pull/26821#pullrequestreview-331022756",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "018c6d63-f9a5-40cf-b5e1-8e59552ccbae",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "nit: invalidating / detaching\r\nnit2: `Metadata of reading event log` (maybe there should be better words...) will be stored",
        "createdAt" : "2019-12-12T07:05:33Z",
        "updatedAt" : "2019-12-12T09:22:04Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "b6b2c5a3cc46d25595bfa7e187846187dd5ac806",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +131,135 @@  private val fastInProgressParsing = conf.get(FAST_IN_PROGRESS_PARSING)\n  // If incremental parsing support configuration is enabled, underlying store will not close\n  // during invalidate UI or detached UI. Metadata of the event read will store in the\n  // `IncrmentalInfo`. Whenever a new event come, parsing will happen from the line it\n  // read last time. Currently it supports inmemory store. TODO: Support for disk store."
  },
  {
    "id" : "9fd793c3-d67f-4638-baf5-9804cad7fcdb",
    "prId" : 26821,
    "prUrl" : "https://github.com/apache/spark/pull/26821#pullrequestreview-331022756",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b7607599-b68c-4866-b9ee-4b5c753b7bda",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "I guess `incremental parsing` already explains this sentence, looks redundant. Moreover, technically, parsing doesn't start immediately when a new event comes.",
        "createdAt" : "2019-12-12T07:10:36Z",
        "updatedAt" : "2019-12-12T09:22:04Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "b6b2c5a3cc46d25595bfa7e187846187dd5ac806",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +132,136 @@  // If incremental parsing support configuration is enabled, underlying store will not close\n  // during invalidate UI or detached UI. Metadata of the event read will store in the\n  // `IncrmentalInfo`. Whenever a new event come, parsing will happen from the line it\n  // read last time. Currently it supports inmemory store. TODO: Support for disk store.\n  private val isIncrementalParsingEnabled = storePath.isEmpty &&"
  },
  {
    "id" : "179cbf88-fac2-4866-9955-1fe1dda64f68",
    "prId" : 26821,
    "prUrl" : "https://github.com/apache/spark/pull/26821#pullrequestreview-331022756",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c5a514e6-5bce-49dd-9f5d-9bb5cc6aa3c1",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Looks redundant as we explained it already in above. Either one would be sufficient.",
        "createdAt" : "2019-12-12T07:13:27Z",
        "updatedAt" : "2019-12-12T09:22:05Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "b6b2c5a3cc46d25595bfa7e187846187dd5ac806",
    "line" : 34,
    "diffHunk" : "@@ -1,1 +419,423 @@      activeUIs.remove((appId, attemptId))\n    }\n    // If incremental parsing is enabled, will not close the underlying store.\n    if (!isIncrementalParsingEnabled) {\n      uiOption.foreach { loadedUI =>"
  },
  {
    "id" : "58df8794-86c1-4634-9460-ce7c70b516a6",
    "prId" : 26821,
    "prUrl" : "https://github.com/apache/spark/pull/26821#pullrequestreview-331022756",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f37cf58c-afd9-4bf8-846b-f9147acb5a8f",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "nit: `IncrementalMetaInfo`. \r\n\r\nBut as I commented earlier, I don't see a reason why this is needed. This will be required when we support incremental parsing upon SHS restart.",
        "createdAt" : "2019-12-12T07:16:45Z",
        "updatedAt" : "2019-12-12T09:22:05Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "b6b2c5a3cc46d25595bfa7e187846187dd5ac806",
    "line" : 214,
    "diffHunk" : "@@ -1,1 +1238,1242 @@    isComplete: Boolean)\n\nprivate[history] case class IncrimentalMetaInfo(\n    appId: String,\n    attemptId: Option[String],"
  },
  {
    "id" : "3b99c7e8-a509-4735-9b40-b79bcfc29e03",
    "prId" : 26821,
    "prUrl" : "https://github.com/apache/spark/pull/26821#pullrequestreview-331022756",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6fb52c93-ad2e-4d8b-9623-3382008040a6",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "I don't see a reason to maintain metadata of reading in kvstore, given we lose everything upon SHS restart. You may want to store it in another HashMap, or even simply just store it within storeMap. (You may want to have another class for data structure of storeMap entity.)",
        "createdAt" : "2019-12-12T07:20:10Z",
        "updatedAt" : "2019-12-12T09:22:05Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "b6b2c5a3cc46d25595bfa7e187846187dd5ac806",
    "line" : 69,
    "diffHunk" : "@@ -1,1 +633,637 @@          }\n\n         if (isIncrementalParsingEnabled) {\n           try {\n             listing.delete(classOf[IncrimentalMetaInfo], Array(Some(appId), attemptId))"
  },
  {
    "id" : "7687c6d7-2d5a-4271-ae4b-dfedc2ec78b5",
    "prId" : 26821,
    "prUrl" : "https://github.com/apache/spark/pull/26821#pullrequestreview-331022756",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a92a0557-257c-4e9b-a73d-25d5a93f93f4",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Same here: looks redundant as we explained it already in above. Either one would be sufficient.",
        "createdAt" : "2019-12-12T07:25:01Z",
        "updatedAt" : "2019-12-12T09:22:05Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "b6b2c5a3cc46d25595bfa7e187846187dd5ac806",
    "line" : 102,
    "diffHunk" : "@@ -1,1 +814,818 @@      activeUIs.get((appId, attemptId)).foreach { ui =>\n        ui.invalidate()\n        // If incremental parsing is enabled, will not close the underlying store\n        // on invalidate UI.\n        if (!isIncrementalParsingEnabled) {"
  },
  {
    "id" : "6d9e0b7d-11a2-4c5e-aad4-b696a0c9c77b",
    "prId" : 26821,
    "prUrl" : "https://github.com/apache/spark/pull/26821#pullrequestreview-331022756",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fbdc6a4e-1b0b-471a-a39e-2232ee18ef9d",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "nit: `incimentInfo` -> `incrementInfo`. ",
        "createdAt" : "2019-12-12T07:26:21Z",
        "updatedAt" : "2019-12-12T09:22:05Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "b6b2c5a3cc46d25595bfa7e187846187dd5ac806",
    "line" : 116,
    "diffHunk" : "@@ -1,1 +974,978 @@      reader: EventLogFileReader,\n      lastUpdated: Long,\n      incrimentInfo: Option[IncrimentalMetaInfo] = None): Unit = {\n    // Disable async updates, since they cause higher memory usage, and it's ok to take longer\n    // to parse the event logs in the SHS."
  },
  {
    "id" : "27359114-8221-4f6e-bd7c-5868c98615a6",
    "prId" : 26821,
    "prUrl" : "https://github.com/apache/spark/pull/26821#pullrequestreview-331022756",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "41ba0a2c-7f7d-4417-babe-152603bed7e4",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Personally I'd add a new abstraction (like `RestorableSparkListener` or `SnapshottableSparkListener`... better words are welcome) which has a `initialize` method so that we can call `initialize` against listener instead of plugin. But given more changes are required, let's hear more voices on committers.",
        "createdAt" : "2019-12-12T07:36:56Z",
        "updatedAt" : "2019-12-12T09:22:05Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "b6b2c5a3cc46d25595bfa7e187846187dd5ac806",
    "line" : 133,
    "diffHunk" : "@@ -1,1 +990,994 @@      listener <- plugin.createListeners(conf, trackingStore)\n    } {\n      incrimentInfo.foreach(info => plugin.initialize(listener, info.appId, info.attemptId))\n      replayBus.addListener(listener)\n    }"
  },
  {
    "id" : "71e74771-c32c-422e-b1e1-4562d1fadbbe",
    "prId" : 26821,
    "prUrl" : "https://github.com/apache/spark/pull/26821#pullrequestreview-331022756",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3461c866-71e8-4c6c-8fbf-98df860a42f4",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "`logFiles.drop(fileToStart).foreach` would be enough to filter out already read files.",
        "createdAt" : "2019-12-12T07:40:31Z",
        "updatedAt" : "2019-12-12T09:22:05Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "b6b2c5a3cc46d25595bfa7e187846187dd5ac806",
    "line" : 154,
    "diffHunk" : "@@ -1,1 +1018,1022 @@    var lineToSkip = info.map(_.lineToSkip).getOrElse(-1)\n    val fileToStart = info.map(_.fileIndex).getOrElse(0)\n    var fileIndex = 0\n    logFiles.foreach { file =>\n      if (continueReplay && fileIndex >= fileToStart) {"
  },
  {
    "id" : "0c8b1331-c73e-4215-9a0e-ee6a64d4c94e",
    "prId" : 26821,
    "prUrl" : "https://github.com/apache/spark/pull/26821#pullrequestreview-331022756",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e179697a-2dcd-4567-864d-57e970f55266",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "We may want to discard `store` when there's any exception thrown from `rebuildAppStore()`, and then IncrementalMetaInfo also becomes invalid. They should be discarded from both storeMap and listing.",
        "createdAt" : "2019-12-12T07:57:42Z",
        "updatedAt" : "2019-12-12T09:22:05Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "b6b2c5a3cc46d25595bfa7e187846187dd5ac806",
    "line" : 203,
    "diffHunk" : "@@ -1,1 +1160,1164 @@        Some(info)\n    }\n    rebuildAppStore(store, reader, attempt.info.lastUpdated.getTime(), info)\n    if (isIncrementalParsingEnabled) {\n      storeMap.put(appId -> attempt.info.attemptId, store)"
  },
  {
    "id" : "4c1a8696-df52-45c3-ae8b-6b28a03dd3dc",
    "prId" : 26821,
    "prUrl" : "https://github.com/apache/spark/pull/26821#pullrequestreview-331022756",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "06aebcb1-bb68-4a8a-9138-a2f90652150e",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "The code is hard to read.\r\n\r\n1. `reader` can be initialized in first line\r\n2. it would be just better to have a big `if-else` as I don't think there're enough of sharing code between twos.",
        "createdAt" : "2019-12-12T08:13:30Z",
        "updatedAt" : "2019-12-12T09:22:05Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "b6b2c5a3cc46d25595bfa7e187846187dd5ac806",
    "line" : 183,
    "diffHunk" : "@@ -1,1 +1141,1145 @@\n  private def createInMemoryStore(appId: String, attempt: AttemptInfoWrapper): KVStore = {\n    val store = if (isIncrementalParsingEnabled) {\n      storeMap.getOrDefault(appId -> attempt.info.attemptId, new InMemoryStore())\n    } else {"
  },
  {
    "id" : "50399a49-f7b2-4c90-ad63-41da8d2813bc",
    "prId" : 26821,
    "prUrl" : "https://github.com/apache/spark/pull/26821#pullrequestreview-331022756",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7d74bd6b-bd08-43ae-8fa6-4ac3faef9d77",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "I'm not sure here is a good place to update the IncrementalMetaInfo:\r\n\r\n1. The method doesn't know whether replaying events will be reflected in app's KV store. I know `info` will be None for the case, but still not easy to understand unless knowing full picture.\r\n\r\n2. The logic seems to be broken when there's error being thrown. e.g. If this reads 3 files successfully and throws error on reading 4th file, store may reflect some events in 4th file (though I think it should be discarded), where it doesn't update IncrementalMetaInfo to reflect the last status of store.\r\n\r\nSo it may be better to propagate the information to the caller and let caller deal with it.\r\n\r\nAnd as a side note this will not work with compaction (#26416) for various reasons:\r\n\r\n1) Listing log files will not start with index 0 if there's any compact file.\r\n2) Log file for the index which should restart from may be removed (either compacted into higher index of compact file, or equal index of compact file). In either way, it should be replayed from the start (though compact log file would be small enough to replay quickly).\r\n\r\nBut let's put aside of that - if #26416 becomes merged earlier it should be reconsidered.",
        "createdAt" : "2019-12-12T08:33:00Z",
        "updatedAt" : "2019-12-12T09:22:05Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "b6b2c5a3cc46d25595bfa7e187846187dd5ac806",
    "line" : 164,
    "diffHunk" : "@@ -1,1 +1025,1029 @@            maybeTruncated = maybeTruncated, eventsFilter = eventsFilter, lineToSkip)\n          continueReplay = result.success\n          // We need to reset the lineToSkip to -1 as we need to parse next file from the beginning\n          lineToSkip = -1\n          if (info.isDefined && (!continueReplay || fileIndex >= logFiles.size -1)) {"
  }
]