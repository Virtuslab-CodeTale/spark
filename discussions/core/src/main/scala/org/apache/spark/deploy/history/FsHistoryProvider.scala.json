[
  {
    "id" : "57fc64b8-f9d2-4fb2-9b4f-40391728c126",
    "prId" : 30842,
    "prUrl" : "https://github.com/apache/spark/pull/30842#pullrequestreview-555634331",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8953a66e-4eb3-42fa-a381-d0d8d3c8c086",
        "parentId" : null,
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "it would be nice to add a comment here describing notStale and why we need it.",
        "createdAt" : "2020-12-18T14:10:30Z",
        "updatedAt" : "2020-12-18T16:26:27Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "493e98aa-0cd3-40d6-9cca-59dad71afd0e",
        "parentId" : "8953a66e-4eb3-42fa-a381-d0d8d3c8c086",
        "authorId" : "25fa223a-19ad-476f-91b0-c26932236a2b",
        "body" : "@tgravescs thank you for the review!\r\nPR has been updated with a comment.",
        "createdAt" : "2020-12-18T16:28:04Z",
        "updatedAt" : "2020-12-18T16:28:04Z",
        "lastEditedBy" : "25fa223a-19ad-476f-91b0-c26932236a2b",
        "tags" : [
        ]
      }
    ],
    "commit" : "c2b8c2d24ffba385ad345f838b0a9ce1e562d93b",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +465,469 @@      // right after this check and before the check for stale entities will be identified as stale\n      // and will be deleted from the UI until the next 'checkForLogs' run.\n      val notStale = mutable.HashSet[String]()\n      val updated = Option(fs.listStatus(new Path(logDir))).map(_.toSeq).getOrElse(Nil)\n        .filter { entry => !isBlacklisted(entry.getPath) }"
  },
  {
    "id" : "b5fb4de3-1ad3-4754-a36c-d5c32dd11f9e",
    "prId" : 30353,
    "prUrl" : "https://github.com/apache/spark/pull/30353#pullrequestreview-530807185",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a683f3c3-9e35-436e-9b42-51e8d86302c9",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Hm, these all look like a performance non-sensitive code path. Is it worthwhile to sweep and fix? I am not sure because it will make more conflicts when reverting, backporting, etc. `Option == Some(id)` can be sort of more readable in a way.",
        "createdAt" : "2020-11-12T12:32:56Z",
        "updatedAt" : "2020-11-13T11:47:20Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "da60323a-ce54-443c-b300-ea498b6a49dc",
        "parentId" : "a683f3c3-9e35-436e-9b42-51e8d86302c9",
        "authorId" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "body" : "mainly to reduce unnecessary object creation, if the readability is affected, it can be left unchanged",
        "createdAt" : "2020-11-12T12:57:13Z",
        "updatedAt" : "2020-11-13T11:47:20Z",
        "lastEditedBy" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "tags" : [
        ]
      },
      {
        "id" : "fd20dcda-23a2-4081-9d7d-876c24f9d276",
        "parentId" : "a683f3c3-9e35-436e-9b42-51e8d86302c9",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "I have the same comment; I don't think it matters much either way. If it's just 14 instances, OK.",
        "createdAt" : "2020-11-15T15:07:50Z",
        "updatedAt" : "2020-11-15T15:07:51Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      }
    ],
    "commit" : "e95295ad35fe472decb4814c45096fb4a5040a51",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +666,670 @@      // If no attempt is specified, or there is no attemptId for attempts, return all attempts\n      attemptId\n        .map { id => app.attempts.filter(_.info.attemptId.contains(id)) }\n        .getOrElse(app.attempts)\n        .foreach { attempt =>"
  },
  {
    "id" : "8b5a29e8-857b-4a41-ab22-4a169cb45500",
    "prId" : 30037,
    "prUrl" : "https://github.com/apache/spark/pull/30037#pullrequestreview-508312899",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8e9e97af-c304-4428-bffc-5af0e6b7a040",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "I roughly remember this was pointed out earlier, but we wanted more elegant code and during finding we forgot the actual issue. Happy to see it fixed finally.",
        "createdAt" : "2020-10-14T12:54:31Z",
        "updatedAt" : "2020-10-14T13:13:33Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "0efba0ad3972f3031d5d4c4a14c88d0e3c8f37e7",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +539,543 @@              } catch {\n                case _: FileNotFoundException => false\n                case NonFatal(e) =>\n                  logWarning(s\"Error while reading new log ${reader.rootPath}\", e)\n                  false"
  },
  {
    "id" : "a3ed4ec9-fd79-4728-9529-b8cc1604fd5e",
    "prId" : 29630,
    "prUrl" : "https://github.com/apache/spark/pull/29630#pullrequestreview-481622962",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "24b75032-3396-4677-904f-6f37e43400f4",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "This is playing as a \"lock\" for the log file. You can see where the lock flag is accessed or/and modified.",
        "createdAt" : "2020-09-03T08:41:43Z",
        "updatedAt" : "2020-09-03T08:41:43Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "5cce482ac0a623aaa9d4657995bb2f5574a9a9a4",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +474,478 @@      val updated = Option(fs.globStatus(new Path(logDir + \"/*\"))).map(_.toSeq).getOrElse(Nil)\n        .filter { entry => isAccessible(entry.getPath) }\n        .filter { entry => !isProcessing(entry.getPath) }\n        .flatMap { entry => EventLogFileReader(fs, entry) }\n        .filter { reader =>"
  },
  {
    "id" : "46f99457-622b-417a-85fd-bc5afbae26b2",
    "prId" : 29630,
    "prUrl" : "https://github.com/apache/spark/pull/29630#pullrequestreview-484304220",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "84700f66-ae0c-4871-9b4e-f3f03b27ad3a",
        "parentId" : null,
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "yeah I don't quite get this, you are requiring all the directories to be under a single directory, I guess that makes this logic easier, but at the same time why the restriction and not have a list?  If we are going to support multiple directories and make sure it works I don't see the reason to have this restriction.  What if people have multiple clusters writing to different HDFS filesystems for instance.  \r\n\r\nI agree with @HeartSaVioR if we are going to support multiple directories we need to have a thorough look at all the logic here to make sure no other problems. I guess in this case you are using a single filesystem?\r\n\r\nI think we need to flush out more of the overall goals and design first",
        "createdAt" : "2020-09-03T13:32:52Z",
        "updatedAt" : "2020-09-03T13:32:52Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "89c68049-79e7-4af0-95b9-190e542c3145",
        "parentId" : "84700f66-ae0c-4871-9b4e-f3f03b27ad3a",
        "authorId" : "8b74a389-1f07-40ba-bd69-8c2de0d3c8bb",
        "body" : "Yes, we are using a single file system in this case. This feature is useful when using external file systems for log data because then multiple directories will correspond to multiple clusters.\r\n\r\nWe could have a list too. Looking into that option.",
        "createdAt" : "2020-09-03T17:57:17Z",
        "updatedAt" : "2020-09-03T17:57:17Z",
        "lastEditedBy" : "8b74a389-1f07-40ba-bd69-8c2de0d3c8bb",
        "tags" : [
        ]
      },
      {
        "id" : "b99542b9-2be4-4344-a654-84b8de4caa28",
        "parentId" : "84700f66-ae0c-4871-9b4e-f3f03b27ad3a",
        "authorId" : "8b74a389-1f07-40ba-bd69-8c2de0d3c8bb",
        "body" : "The reason we chose to go with glob-pattern is that our service creates short-lived YARN clusters on which Spark applications are run. Log data goes to directories in a remote file-system. Since the number of these clusters can be really large, glob pattern would better fit our use-case, than a static list would.",
        "createdAt" : "2020-09-08T16:23:51Z",
        "updatedAt" : "2020-09-08T16:23:52Z",
        "lastEditedBy" : "8b74a389-1f07-40ba-bd69-8c2de0d3c8bb",
        "tags" : [
        ]
      }
    ],
    "commit" : "5cce482ac0a623aaa9d4657995bb2f5574a9a9a4",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +472,476 @@      logDebug(s\"Scanning $logDir with lastScanTime==$lastScanTime\")\n\n      val updated = Option(fs.globStatus(new Path(logDir + \"/*\"))).map(_.toSeq).getOrElse(Nil)\n        .filter { entry => isAccessible(entry.getPath) }\n        .filter { entry => !isProcessing(entry.getPath) }"
  },
  {
    "id" : "0c81582e-f21e-47a8-97a5-9ceaf8c85957",
    "prId" : 29350,
    "prUrl" : "https://github.com/apache/spark/pull/29350#pullrequestreview-461389982",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ca1a0a16-66d8-48df-9fbd-e91e9e68c8a8",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "nit: I'd have empty new line after `}`.",
        "createdAt" : "2020-08-05T05:13:38Z",
        "updatedAt" : "2020-08-05T06:32:07Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "2cf0f6de-f65f-4500-a588-bcdad19bd9ab",
        "parentId" : "ca1a0a16-66d8-48df-9fbd-e91e9e68c8a8",
        "authorId" : "e9672703-dc17-430b-a47b-52d125d0cc78",
        "body" : "added\r\n\r\n> nit: I'd have empty new line after `}`.\r\n\r\n",
        "createdAt" : "2020-08-05T06:59:27Z",
        "updatedAt" : "2020-08-05T06:59:27Z",
        "lastEditedBy" : "e9672703-dc17-430b-a47b-52d125d0cc78",
        "tags" : [
        ]
      }
    ],
    "commit" : "d1bf4caa30231a41fd4e6025c34af71c5f15e07e",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +540,544 @@              }\n\n            case _: FileNotFoundException =>\n              false\n          }"
  },
  {
    "id" : "e8d67d10-ad80-42fe-b3ea-d0f5ff89becd",
    "prId" : 28874,
    "prUrl" : "https://github.com/apache/spark/pull/28874#pullrequestreview-438437522",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "16465e28-0c28-45ce-9de1-cd0e1cfd4cfe",
        "parentId" : null,
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "I think this should function should be isInAccessible or you have to change its logic. You added the ! but the isAccessible function does the same thing the isBlacklisted did.  Probably why tests are failing.",
        "createdAt" : "2020-06-25T13:23:08Z",
        "updatedAt" : "2020-07-14T15:31:39Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "9b7c3ef6-8b69-4718-b9a0-f98117b1e88b",
        "parentId" : "16465e28-0c28-45ce-9de1-cd0e1cfd4cfe",
        "authorId" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "body" : "Thanks for the catch, my mistake. You're right that I remembered to update the callsites but not the definition.",
        "createdAt" : "2020-06-26T16:34:46Z",
        "updatedAt" : "2020-07-14T15:31:39Z",
        "lastEditedBy" : "ddb80038-2da6-4937-8c45-4d52fbe0300f",
        "tags" : [
        ]
      }
    ],
    "commit" : "1ba58fb285cc0a1ff28e452a61c8521d5b308a32",
    "line" : 69,
    "diffHunk" : "@@ -1,1 +1237,1241 @@  private def deleteLog(fs: FileSystem, log: Path): Boolean = {\n    var deleted = false\n    if (!isAccessible(log)) {\n      logDebug(s\"Skipping deleting $log as we don't have permissions on it.\")\n    } else {"
  },
  {
    "id" : "eb36a401-e000-437b-bd57-db07d218edee",
    "prId" : 28412,
    "prUrl" : "https://github.com/apache/spark/pull/28412#pullrequestreview-431045908",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f6cfcb4b-e34f-462e-83d9-8394c5e256d6",
        "parentId" : null,
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "I find this function hard to read now. I would say at least put a comment here and possibly split the below logic into a separate function for easier readability.",
        "createdAt" : "2020-06-15T14:21:37Z",
        "updatedAt" : "2020-07-06T21:32:29Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "094fa272-1230-4ea9-b977-0b33d430ec09",
        "parentId" : "f6cfcb4b-e34f-462e-83d9-8394c5e256d6",
        "authorId" : "788d6907-ae49-42f5-95b7-ef69c1460937",
        "body" : "Addressed. Split the logic below to a new function createLevelDBStore().",
        "createdAt" : "2020-06-15T22:52:41Z",
        "updatedAt" : "2020-07-06T21:32:29Z",
        "lastEditedBy" : "788d6907-ae49-42f5-95b7-ef69c1460937",
        "tags" : [
        ]
      }
    ],
    "commit" : "b71a923e4554e814ee923b64d1b22ce42825378c",
    "line" : 45,
    "diffHunk" : "@@ -1,1 +1188,1192 @@      }\n    }\n\n    createLevelDBStore(dm, appId, attempt, metadata)\n  }"
  },
  {
    "id" : "2fa6637b-5310-4acc-bfd4-39fa6b06d647",
    "prId" : 27208,
    "prUrl" : "https://github.com/apache/spark/pull/27208#pullrequestreview-346933689",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "908cef6c-4e31-4e3b-a085-06a9fd8288cc",
        "parentId" : null,
        "authorId" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "body" : "So one thing that feels a tiny bit odd is that when deciding whether to compact, you're actually considering the last log file, which you won't consider during actual compaction, right?\r\n\r\nWouldn't that cause unnecessary (or too aggressive) compaction at the end of the application, when potentially a bunch of jobs finish and \"release\" lots of tasks, inflating the compation scoe?",
        "createdAt" : "2020-01-22T16:50:59Z",
        "updatedAt" : "2020-01-28T01:55:03Z",
        "lastEditedBy" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "tags" : [
        ]
      },
      {
        "id" : "b7388159-2d91-42e1-a3e4-6f2e93d6373f",
        "parentId" : "908cef6c-4e31-4e3b-a085-06a9fd8288cc",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "> So one thing that feels a tiny bit odd is that when deciding whether to compact, you're actually considering the last log file, which you won't consider during actual compaction, right?\r\n> Wouldn't that cause unnecessary (or too aggressive) compaction at the end of the application, when potentially a bunch of jobs finish and \"release\" lots of tasks, inflating the compation scoe?\r\n\r\nThat's the intention that callers of compactor don't care about how many files are actually affected. Callers of compactor just need to know that same list of log files would bring same result, unless it fails and throws exception. How many files are excluded in compaction is just a configuration, and the last log file should be excluded is an implementation detail. (We prevent it in both configuration and compactor via having 1 as min value for max retain log file.)\r\n\r\nCompactor will ignore the last log file in any way as configured, so unless the rare case where the log is rolled just before the app is finished, it won't happen. And most probably end users would avoid to set the value to 1 if they read the doc and understand how it works.",
        "createdAt" : "2020-01-22T22:00:16Z",
        "updatedAt" : "2020-01-28T01:55:03Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "ddd67885dd2c861481287b127a51c6159c2b8fbf",
    "line" : 147,
    "diffHunk" : "@@ -1,1 +825,829 @@                info.lastEvaluatedForCompaction.get < lastIndex) {\n              // haven't tried compaction for this index, do compaction\n              fileCompactor.compact(reader.listEventLogFiles)\n              listing.write(info.copy(lastEvaluatedForCompaction = Some(lastIndex)))\n            }"
  },
  {
    "id" : "f88f30c2-da7d-4165-abf7-b6cc7808d50b",
    "prId" : 27208,
    "prUrl" : "https://github.com/apache/spark/pull/27208#pullrequestreview-348364844",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c29efad0-42d2-486b-8197-dd4c8102d77d",
        "parentId" : null,
        "authorId" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "body" : "Hmm, perhaps this method could take care of calling `endProcessing` too (e.g. by wrapping the task)? Should be just a small adjustment at the call site.",
        "createdAt" : "2020-01-23T16:49:35Z",
        "updatedAt" : "2020-01-28T01:55:03Z",
        "lastEditedBy" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "tags" : [
        ]
      },
      {
        "id" : "73762bd3-410d-4ada-a88f-a05a663e95f5",
        "parentId" : "c29efad0-42d2-486b-8197-dd4c8102d77d",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "`finally` statement in `mergeApplicationListing` makes it be complicated, because we should handle the reentrance of lock. (We're submitting another task in the task.)\r\n\r\nIf we move endProcessing from `finally` statement in `mergeApplicationListing` to the end of task here, `processing` on the compaction task would be executed earlier than calling `endProcessing` on the listing task. Marking lock from compaction task would succeed but effectively no-op, and releasing lock from listing task would remove the mark for compaction task as well, which makes the compaction task run without proper lock.\r\n\r\nSo either we need to make lock much smarter, or document the requirement on caller side. I'm feeling that former one is more complicated than latter one.",
        "createdAt" : "2020-01-26T08:26:49Z",
        "updatedAt" : "2020-01-28T01:55:03Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "ddd67885dd2c861481287b127a51c6159c2b8fbf",
    "line" : 276,
    "diffHunk" : "@@ -1,1 +1239,1243 @@  }\n\n  /** NOTE: 'task' should ensure it executes 'endProcessing' at the end */\n  private def submitLogProcessTask(rootPath: Path)(task: Runnable): Unit = {\n    try {"
  },
  {
    "id" : "337c39d8-76ae-4310-b3a6-33727ad75474",
    "prId" : 26821,
    "prUrl" : "https://github.com/apache/spark/pull/26821#pullrequestreview-331022756",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7d38e20a-84f1-40f5-99ed-8668e63cbd26",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "We seem to not add `is`/`are` in the name of value even it's expected to return Boolean. Nearest example is `fastInProgressParsing`.",
        "createdAt" : "2019-12-12T06:54:26Z",
        "updatedAt" : "2019-12-12T09:22:04Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "b6b2c5a3cc46d25595bfa7e187846187dd5ac806",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +134,138 @@  // `IncrmentalInfo`. Whenever a new event come, parsing will happen from the line it\n  // read last time. Currently it supports inmemory store. TODO: Support for disk store.\n  private val isIncrementalParsingEnabled = storePath.isEmpty &&\n    conf.get(History.INCREMENTAL_PARSING_ENABLED)\n  private val storeMap = new ConcurrentHashMap[(String, Option[String]), KVStore]()"
  },
  {
    "id" : "64e8a1ee-96ba-402e-8659-3e21ae048aaf",
    "prId" : 26821,
    "prUrl" : "https://github.com/apache/spark/pull/26821#pullrequestreview-331022756",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0ca1bdce-7a87-4bc2-addc-c23341555dff",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Please place TODO as separate line of comment so that it helps IDE to highlight.",
        "createdAt" : "2019-12-12T06:55:01Z",
        "updatedAt" : "2019-12-12T09:22:04Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "b6b2c5a3cc46d25595bfa7e187846187dd5ac806",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +133,137 @@  // during invalidate UI or detached UI. Metadata of the event read will store in the\n  // `IncrmentalInfo`. Whenever a new event come, parsing will happen from the line it\n  // read last time. Currently it supports inmemory store. TODO: Support for disk store.\n  private val isIncrementalParsingEnabled = storePath.isEmpty &&\n    conf.get(History.INCREMENTAL_PARSING_ENABLED)"
  },
  {
    "id" : "bddfedc2-b744-476c-bd23-385e4ce368a4",
    "prId" : 26821,
    "prUrl" : "https://github.com/apache/spark/pull/26821#pullrequestreview-331022756",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c9916c6f-3023-43e4-bdfb-ab03da12e3f1",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "1. nit: `incremental parsing support configuration` seems odd. IMHO, just `incremental parsing` would work.\r\n2. there're two kinds of stores in FsHistoryProvider, so maybe better to clarify here; `underlying APP kvstore` or some better words?",
        "createdAt" : "2019-12-12T07:04:13Z",
        "updatedAt" : "2019-12-12T09:22:04Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "b6b2c5a3cc46d25595bfa7e187846187dd5ac806",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +130,134 @@  private val storePath = conf.get(LOCAL_STORE_DIR).map(new File(_))\n  private val fastInProgressParsing = conf.get(FAST_IN_PROGRESS_PARSING)\n  // If incremental parsing support configuration is enabled, underlying store will not close\n  // during invalidate UI or detached UI. Metadata of the event read will store in the\n  // `IncrmentalInfo`. Whenever a new event come, parsing will happen from the line it"
  },
  {
    "id" : "77c40469-063e-4ee8-ba89-d4f8091a0224",
    "prId" : 26821,
    "prUrl" : "https://github.com/apache/spark/pull/26821#pullrequestreview-331022756",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "018c6d63-f9a5-40cf-b5e1-8e59552ccbae",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "nit: invalidating / detaching\r\nnit2: `Metadata of reading event log` (maybe there should be better words...) will be stored",
        "createdAt" : "2019-12-12T07:05:33Z",
        "updatedAt" : "2019-12-12T09:22:04Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "b6b2c5a3cc46d25595bfa7e187846187dd5ac806",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +131,135 @@  private val fastInProgressParsing = conf.get(FAST_IN_PROGRESS_PARSING)\n  // If incremental parsing support configuration is enabled, underlying store will not close\n  // during invalidate UI or detached UI. Metadata of the event read will store in the\n  // `IncrmentalInfo`. Whenever a new event come, parsing will happen from the line it\n  // read last time. Currently it supports inmemory store. TODO: Support for disk store."
  },
  {
    "id" : "9fd793c3-d67f-4638-baf5-9804cad7fcdb",
    "prId" : 26821,
    "prUrl" : "https://github.com/apache/spark/pull/26821#pullrequestreview-331022756",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b7607599-b68c-4866-b9ee-4b5c753b7bda",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "I guess `incremental parsing` already explains this sentence, looks redundant. Moreover, technically, parsing doesn't start immediately when a new event comes.",
        "createdAt" : "2019-12-12T07:10:36Z",
        "updatedAt" : "2019-12-12T09:22:04Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "b6b2c5a3cc46d25595bfa7e187846187dd5ac806",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +132,136 @@  // If incremental parsing support configuration is enabled, underlying store will not close\n  // during invalidate UI or detached UI. Metadata of the event read will store in the\n  // `IncrmentalInfo`. Whenever a new event come, parsing will happen from the line it\n  // read last time. Currently it supports inmemory store. TODO: Support for disk store.\n  private val isIncrementalParsingEnabled = storePath.isEmpty &&"
  },
  {
    "id" : "179cbf88-fac2-4866-9955-1fe1dda64f68",
    "prId" : 26821,
    "prUrl" : "https://github.com/apache/spark/pull/26821#pullrequestreview-331022756",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c5a514e6-5bce-49dd-9f5d-9bb5cc6aa3c1",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Looks redundant as we explained it already in above. Either one would be sufficient.",
        "createdAt" : "2019-12-12T07:13:27Z",
        "updatedAt" : "2019-12-12T09:22:05Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "b6b2c5a3cc46d25595bfa7e187846187dd5ac806",
    "line" : 34,
    "diffHunk" : "@@ -1,1 +419,423 @@      activeUIs.remove((appId, attemptId))\n    }\n    // If incremental parsing is enabled, will not close the underlying store.\n    if (!isIncrementalParsingEnabled) {\n      uiOption.foreach { loadedUI =>"
  },
  {
    "id" : "58df8794-86c1-4634-9460-ce7c70b516a6",
    "prId" : 26821,
    "prUrl" : "https://github.com/apache/spark/pull/26821#pullrequestreview-331022756",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f37cf58c-afd9-4bf8-846b-f9147acb5a8f",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "nit: `IncrementalMetaInfo`. \r\n\r\nBut as I commented earlier, I don't see a reason why this is needed. This will be required when we support incremental parsing upon SHS restart.",
        "createdAt" : "2019-12-12T07:16:45Z",
        "updatedAt" : "2019-12-12T09:22:05Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "b6b2c5a3cc46d25595bfa7e187846187dd5ac806",
    "line" : 214,
    "diffHunk" : "@@ -1,1 +1238,1242 @@    isComplete: Boolean)\n\nprivate[history] case class IncrimentalMetaInfo(\n    appId: String,\n    attemptId: Option[String],"
  },
  {
    "id" : "3b99c7e8-a509-4735-9b40-b79bcfc29e03",
    "prId" : 26821,
    "prUrl" : "https://github.com/apache/spark/pull/26821#pullrequestreview-331022756",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6fb52c93-ad2e-4d8b-9623-3382008040a6",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "I don't see a reason to maintain metadata of reading in kvstore, given we lose everything upon SHS restart. You may want to store it in another HashMap, or even simply just store it within storeMap. (You may want to have another class for data structure of storeMap entity.)",
        "createdAt" : "2019-12-12T07:20:10Z",
        "updatedAt" : "2019-12-12T09:22:05Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "b6b2c5a3cc46d25595bfa7e187846187dd5ac806",
    "line" : 69,
    "diffHunk" : "@@ -1,1 +633,637 @@          }\n\n         if (isIncrementalParsingEnabled) {\n           try {\n             listing.delete(classOf[IncrimentalMetaInfo], Array(Some(appId), attemptId))"
  },
  {
    "id" : "7687c6d7-2d5a-4271-ae4b-dfedc2ec78b5",
    "prId" : 26821,
    "prUrl" : "https://github.com/apache/spark/pull/26821#pullrequestreview-331022756",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a92a0557-257c-4e9b-a73d-25d5a93f93f4",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Same here: looks redundant as we explained it already in above. Either one would be sufficient.",
        "createdAt" : "2019-12-12T07:25:01Z",
        "updatedAt" : "2019-12-12T09:22:05Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "b6b2c5a3cc46d25595bfa7e187846187dd5ac806",
    "line" : 102,
    "diffHunk" : "@@ -1,1 +814,818 @@      activeUIs.get((appId, attemptId)).foreach { ui =>\n        ui.invalidate()\n        // If incremental parsing is enabled, will not close the underlying store\n        // on invalidate UI.\n        if (!isIncrementalParsingEnabled) {"
  },
  {
    "id" : "6d9e0b7d-11a2-4c5e-aad4-b696a0c9c77b",
    "prId" : 26821,
    "prUrl" : "https://github.com/apache/spark/pull/26821#pullrequestreview-331022756",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fbdc6a4e-1b0b-471a-a39e-2232ee18ef9d",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "nit: `incimentInfo` -> `incrementInfo`. ",
        "createdAt" : "2019-12-12T07:26:21Z",
        "updatedAt" : "2019-12-12T09:22:05Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "b6b2c5a3cc46d25595bfa7e187846187dd5ac806",
    "line" : 116,
    "diffHunk" : "@@ -1,1 +974,978 @@      reader: EventLogFileReader,\n      lastUpdated: Long,\n      incrimentInfo: Option[IncrimentalMetaInfo] = None): Unit = {\n    // Disable async updates, since they cause higher memory usage, and it's ok to take longer\n    // to parse the event logs in the SHS."
  },
  {
    "id" : "27359114-8221-4f6e-bd7c-5868c98615a6",
    "prId" : 26821,
    "prUrl" : "https://github.com/apache/spark/pull/26821#pullrequestreview-331022756",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "41ba0a2c-7f7d-4417-babe-152603bed7e4",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Personally I'd add a new abstraction (like `RestorableSparkListener` or `SnapshottableSparkListener`... better words are welcome) which has a `initialize` method so that we can call `initialize` against listener instead of plugin. But given more changes are required, let's hear more voices on committers.",
        "createdAt" : "2019-12-12T07:36:56Z",
        "updatedAt" : "2019-12-12T09:22:05Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "b6b2c5a3cc46d25595bfa7e187846187dd5ac806",
    "line" : 133,
    "diffHunk" : "@@ -1,1 +990,994 @@      listener <- plugin.createListeners(conf, trackingStore)\n    } {\n      incrimentInfo.foreach(info => plugin.initialize(listener, info.appId, info.attemptId))\n      replayBus.addListener(listener)\n    }"
  },
  {
    "id" : "71e74771-c32c-422e-b1e1-4562d1fadbbe",
    "prId" : 26821,
    "prUrl" : "https://github.com/apache/spark/pull/26821#pullrequestreview-331022756",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3461c866-71e8-4c6c-8fbf-98df860a42f4",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "`logFiles.drop(fileToStart).foreach` would be enough to filter out already read files.",
        "createdAt" : "2019-12-12T07:40:31Z",
        "updatedAt" : "2019-12-12T09:22:05Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "b6b2c5a3cc46d25595bfa7e187846187dd5ac806",
    "line" : 154,
    "diffHunk" : "@@ -1,1 +1018,1022 @@    var lineToSkip = info.map(_.lineToSkip).getOrElse(-1)\n    val fileToStart = info.map(_.fileIndex).getOrElse(0)\n    var fileIndex = 0\n    logFiles.foreach { file =>\n      if (continueReplay && fileIndex >= fileToStart) {"
  },
  {
    "id" : "0c8b1331-c73e-4215-9a0e-ee6a64d4c94e",
    "prId" : 26821,
    "prUrl" : "https://github.com/apache/spark/pull/26821#pullrequestreview-331022756",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e179697a-2dcd-4567-864d-57e970f55266",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "We may want to discard `store` when there's any exception thrown from `rebuildAppStore()`, and then IncrementalMetaInfo also becomes invalid. They should be discarded from both storeMap and listing.",
        "createdAt" : "2019-12-12T07:57:42Z",
        "updatedAt" : "2019-12-12T09:22:05Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "b6b2c5a3cc46d25595bfa7e187846187dd5ac806",
    "line" : 203,
    "diffHunk" : "@@ -1,1 +1160,1164 @@        Some(info)\n    }\n    rebuildAppStore(store, reader, attempt.info.lastUpdated.getTime(), info)\n    if (isIncrementalParsingEnabled) {\n      storeMap.put(appId -> attempt.info.attemptId, store)"
  },
  {
    "id" : "4c1a8696-df52-45c3-ae8b-6b28a03dd3dc",
    "prId" : 26821,
    "prUrl" : "https://github.com/apache/spark/pull/26821#pullrequestreview-331022756",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "06aebcb1-bb68-4a8a-9138-a2f90652150e",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "The code is hard to read.\r\n\r\n1. `reader` can be initialized in first line\r\n2. it would be just better to have a big `if-else` as I don't think there're enough of sharing code between twos.",
        "createdAt" : "2019-12-12T08:13:30Z",
        "updatedAt" : "2019-12-12T09:22:05Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "b6b2c5a3cc46d25595bfa7e187846187dd5ac806",
    "line" : 183,
    "diffHunk" : "@@ -1,1 +1141,1145 @@\n  private def createInMemoryStore(appId: String, attempt: AttemptInfoWrapper): KVStore = {\n    val store = if (isIncrementalParsingEnabled) {\n      storeMap.getOrDefault(appId -> attempt.info.attemptId, new InMemoryStore())\n    } else {"
  },
  {
    "id" : "50399a49-f7b2-4c90-ad63-41da8d2813bc",
    "prId" : 26821,
    "prUrl" : "https://github.com/apache/spark/pull/26821#pullrequestreview-331022756",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7d74bd6b-bd08-43ae-8fa6-4ac3faef9d77",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "I'm not sure here is a good place to update the IncrementalMetaInfo:\r\n\r\n1. The method doesn't know whether replaying events will be reflected in app's KV store. I know `info` will be None for the case, but still not easy to understand unless knowing full picture.\r\n\r\n2. The logic seems to be broken when there's error being thrown. e.g. If this reads 3 files successfully and throws error on reading 4th file, store may reflect some events in 4th file (though I think it should be discarded), where it doesn't update IncrementalMetaInfo to reflect the last status of store.\r\n\r\nSo it may be better to propagate the information to the caller and let caller deal with it.\r\n\r\nAnd as a side note this will not work with compaction (#26416) for various reasons:\r\n\r\n1) Listing log files will not start with index 0 if there's any compact file.\r\n2) Log file for the index which should restart from may be removed (either compacted into higher index of compact file, or equal index of compact file). In either way, it should be replayed from the start (though compact log file would be small enough to replay quickly).\r\n\r\nBut let's put aside of that - if #26416 becomes merged earlier it should be reconsidered.",
        "createdAt" : "2019-12-12T08:33:00Z",
        "updatedAt" : "2019-12-12T09:22:05Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "b6b2c5a3cc46d25595bfa7e187846187dd5ac806",
    "line" : 164,
    "diffHunk" : "@@ -1,1 +1025,1029 @@            maybeTruncated = maybeTruncated, eventsFilter = eventsFilter, lineToSkip)\n          continueReplay = result.success\n          // We need to reset the lineToSkip to -1 as we need to parse next file from the beginning\n          lineToSkip = -1\n          if (info.isDefined && (!continueReplay || fileIndex >= logFiles.size -1)) {"
  },
  {
    "id" : "887f3b34-d52c-45a3-adb7-c9f1965ec255",
    "prId" : 26416,
    "prUrl" : "https://github.com/apache/spark/pull/26416#pullrequestreview-337909022",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "984a8c99-dc5b-449e-a683-c7ad82080284",
        "parentId" : null,
        "authorId" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "body" : "I started trying to track down how this works and it's all over the place. The fact that the code around this is all new from the previous patch also makes it really hard to do incremental reviews. This is a huge PR and when significant parts of it change every time an update is made, it makes reviews really slow.\r\n\r\nHave you tried to break this down into more incremental PRs?\r\n\r\nFor example, I hardly ever get into the `FsHistoryProvider` part before I start questioning why previous code behaves the way it does. Kinda feels like all the filtering and compaction code could be added before it's hooked up to the provider.\r\n\r\nThis particular code that does caching of the compactor is scattered all over the place. Here you cache the instances, but there's a bunch of code and exception handling in the compactor code itself to deal with the fact that the internal compactor state may not be reusable.\r\n\r\nThat to me screams like a bad abstraction, and that you should be thinking differently about how this caching is done, and the API between the provider and the compactor.\r\n\r\nAnother thing I noticed is that you only seem to remove things from this cache when logs are cleaned up, but not e.g. when an application finishes (and the code can make the final compaction).",
        "createdAt" : "2020-01-03T00:31:42Z",
        "updatedAt" : "2020-01-03T00:38:05Z",
        "lastEditedBy" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "tags" : [
        ]
      }
    ],
    "commit" : "e1a6e42b73f8d58dbc0a04882f2288d2fae0dad8",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +161,165 @@\n  // Visible for testing.\n  private[history] val logToCompactor = new mutable.HashMap[String, EventLogFileCompactor]\n\n  // Used to store the paths, which are being processed. This enable the replay log tasks execute"
  },
  {
    "id" : "02670115-1cb8-434a-94d9-fbc4cebd5c7a",
    "prId" : 26397,
    "prUrl" : "https://github.com/apache/spark/pull/26397#pullrequestreview-314115148",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "27b62144-fe6c-4ca2-b8a5-f28d5433c9ac",
        "parentId" : null,
        "authorId" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "body" : "Nit: There are couple of unused imports at the beginning of the file. Maybe worth to clean them up.",
        "createdAt" : "2019-11-07T15:52:28Z",
        "updatedAt" : "2019-11-10T04:57:07Z",
        "lastEditedBy" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "tags" : [
        ]
      },
      {
        "id" : "07926898-9d42-4f6b-9191-dcc3befb716a",
        "parentId" : "27b62144-fe6c-4ca2-b8a5-f28d5433c9ac",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "If it's occurred from here I'll remove. If not, it might make other PRs be broken, so I might take it carefully.",
        "createdAt" : "2019-11-08T01:07:35Z",
        "updatedAt" : "2019-11-10T04:57:07Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "2eb35c07-146f-4ea6-8559-5260608a7d06",
        "parentId" : "27b62144-fe6c-4ca2-b8a5-f28d5433c9ac",
        "authorId" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "body" : "Since `FsHistoryProvider.scala` is one of the main target of this development (I mean not the PR) it would be good to clean it up somewhere. What do you think where should we do it?",
        "createdAt" : "2019-11-08T08:58:27Z",
        "updatedAt" : "2019-11-10T04:57:07Z",
        "lastEditedBy" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "tags" : [
        ]
      },
      {
        "id" : "9783e7a4-f9cf-4333-a4ea-31d09f623643",
        "parentId" : "27b62144-fe6c-4ca2-b8a5-f28d5433c9ac",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "I guess it's OK to do with minor PR, but commiter could finally judge the worth. (treat my comment as 2 cents)",
        "createdAt" : "2019-11-08T09:56:46Z",
        "updatedAt" : "2019-11-10T04:57:07Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "25a87ba0-6cee-4356-ba02-9c7e1b76bdcd",
        "parentId" : "27b62144-fe6c-4ca2-b8a5-f28d5433c9ac",
        "authorId" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "body" : "I agree with this resolution. Such way we separate the concerns. I've created https://github.com/apache/spark/pull/26436, in case of disagreement it can be just dropped.",
        "createdAt" : "2019-11-08T10:28:13Z",
        "updatedAt" : "2019-11-10T04:57:07Z",
        "lastEditedBy" : "e6f86365-3ac2-48b4-94f9-21ce737cf1ac",
        "tags" : [
        ]
      }
    ],
    "commit" : "698a6571e6f798b74d8469be32b41999c5397d60",
    "line" : 1,
    "diffHunk" : "@@ -1,1 +16,20 @@ */\n\npackage org.apache.spark.deploy.history\n\nimport java.io.{File, FileNotFoundException, IOException}"
  },
  {
    "id" : "e7687ed9-dcac-4020-8714-acc6007a855c",
    "prId" : 25797,
    "prUrl" : "https://github.com/apache/spark/pull/25797#pullrequestreview-288406938",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9dd772f7-f5be-43b8-91ea-044e1d98c32d",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "You may want to recheck all expressions in catch statement are thread-safe, for sure. (I don't know whether it's thread safe or not, but the logic is now executed from multiple threads concurrently while it was executed from single thread.)",
        "createdAt" : "2019-09-16T01:06:23Z",
        "updatedAt" : "2019-12-14T16:41:25Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "73ea9bff-4fd3-4967-91d2-aa1e3f90c831",
        "parentId" : "9dd772f7-f5be-43b8-91ea-044e1d98c32d",
        "authorId" : "b6393788-12cc-4066-9294-857f4baec092",
        "body" : "Hi, I have checked these expressions here and I think they are thread-safe.\r\n\r\n- blacklist is a ConcurrentHashMap and blackList(path) is thread-safe.\r\n- listing.delete() is thread-safe.\r\n- endProcess is a remove operation for ConcurrentSkipListSet and it is thread-safe.\r\n- pendingReplayTasksCount is an AtomicInteger and it is thread-safe.",
        "createdAt" : "2019-09-16T03:32:09Z",
        "updatedAt" : "2019-12-14T16:41:25Z",
        "lastEditedBy" : "b6393788-12cc-4066-9294-857f4baec092",
        "tags" : [
        ]
      }
    ],
    "commit" : "e9ebb6fd690c6e3a786b3285058c59850107386b",
    "line" : 122,
    "diffHunk" : "@@ -1,1 +670,674 @@      }\n    } catch {\n      case e: InterruptedException =>\n        throw e\n      case e: AccessControlException =>"
  },
  {
    "id" : "d3e782df-54dc-474c-81d8-b791a8087951",
    "prId" : 25797,
    "prUrl" : "https://github.com/apache/spark/pull/25797#pullrequestreview-293463196",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "13b3864f-e5c4-4f89-8d98-e1ad9cc9cf22",
        "parentId" : null,
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Looks like the name needs to be changed - `processing(String)` would \"retrieve\" value, whereas `processing(Path)` would \"add\" the value - same name on different operations. \r\n\r\nMaybe `addProcessing` or better naming? Even I guess it might be better if we have a new class to extract such functionality out of FsHistoryProvider and composite it, though I agree it's too simple to extract out.",
        "createdAt" : "2019-09-25T22:25:37Z",
        "updatedAt" : "2019-12-14T16:41:25Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "869a5360-78d8-4281-95b2-88825ac2e275",
        "parentId" : "13b3864f-e5c4-4f89-8d98-e1ad9cc9cf22",
        "authorId" : "b6393788-12cc-4066-9294-857f4baec092",
        "body" : "thanks for you suggestion, the name was referred that of `blacklist`.\r\nI think the function of `processing` is similar with `blacklist`, so how about just keeping consistent with `blacklist`?\r\nhttps://github.com/apache/spark/blob/21db2f86f7c196c535e8f0b5675ae48cb2c372f7/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala#L164-L173\r\n\r\n",
        "createdAt" : "2019-09-26T02:56:12Z",
        "updatedAt" : "2019-12-14T16:41:25Z",
        "lastEditedBy" : "b6393788-12cc-4066-9294-857f4baec092",
        "tags" : [
        ]
      },
      {
        "id" : "53eb4d0c-217e-4ef3-bff2-0ee82901ed81",
        "parentId" : "13b3864f-e5c4-4f89-8d98-e1ad9cc9cf22",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Ah OK didn't recognize it. Looks OK to me.",
        "createdAt" : "2019-09-26T03:07:38Z",
        "updatedAt" : "2019-12-14T16:41:25Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "e9ebb6fd690c6e3a786b3285058c59850107386b",
    "line" : 31,
    "diffHunk" : "@@ -1,1 +172,176 @@  }\n\n  private def processing(path: Path): Unit = {\n    processing.add(path.getName)\n  }"
  },
  {
    "id" : "7ba47711-33d7-4eda-add5-c465b135f0f1",
    "prId" : 25797,
    "prUrl" : "https://github.com/apache/spark/pull/25797#pullrequestreview-331802850",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "32061b5e-9b66-4e2e-a15a-3cd8d410cce7",
        "parentId" : null,
        "authorId" : "b6393788-12cc-4066-9294-857f4baec092",
        "body" : "Here the logic is consistent with cleanLogs().\r\nBut, I think there is an overlap between `app.oldestAttempt() <= maxTime` and `attempt.info.lastUpdated.getTime() >= maxTime`, even it does not matter.",
        "createdAt" : "2019-12-13T11:05:01Z",
        "updatedAt" : "2019-12-14T16:41:25Z",
        "lastEditedBy" : "b6393788-12cc-4066-9294-857f4baec092",
        "tags" : [
        ]
      }
    ],
    "commit" : "e9ebb6fd690c6e3a786b3285058c59850107386b",
    "line" : 176,
    "diffHunk" : "@@ -1,1 +837,841 @@    log.appId.foreach { appId =>\n      val app = listing.read(classOf[ApplicationInfoWrapper], appId)\n      if (app.oldestAttempt() <= maxTime) {\n        val (remaining, toDelete) = app.attempts.partition { attempt =>\n          attempt.info.lastUpdated.getTime() >= maxTime"
  },
  {
    "id" : "0860329f-4519-45a9-9ba9-fd9741c97738",
    "prId" : 25797,
    "prUrl" : "https://github.com/apache/spark/pull/25797#pullrequestreview-332222148",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "58a7a6a5-2871-4204-a28a-1b0c93555b2b",
        "parentId" : null,
        "authorId" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "body" : "Related to my previous comment, but since this method is called after `doMergeApplicationListing`, and that method does not throw exceptions for invalid apps (= those without an ID), as far as I know, this looks like dead code.",
        "createdAt" : "2019-12-14T01:02:35Z",
        "updatedAt" : "2019-12-14T16:41:25Z",
        "lastEditedBy" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "tags" : [
        ]
      },
      {
        "id" : "3dd10f2a-f60c-4b4f-a87b-91a4fb9f0a80",
        "parentId" : "58a7a6a5-2871-4204-a28a-1b0c93555b2b",
        "authorId" : "b6393788-12cc-4066-9294-857f4baec092",
        "body" : "Thanks very much.\r\nI have modified the unit test.\r\nAnd after debugging, It seems that it is not dead code.",
        "createdAt" : "2019-12-14T16:47:49Z",
        "updatedAt" : "2019-12-14T16:47:49Z",
        "lastEditedBy" : "b6393788-12cc-4066-9294-857f4baec092",
        "tags" : [
        ]
      }
    ],
    "commit" : "e9ebb6fd690c6e3a786b3285058c59850107386b",
    "line" : 168,
    "diffHunk" : "@@ -1,1 +829,833 @@    val log = listing.read(classOf[LogInfo], logPath)\n\n    if (log.lastProcessed <= maxTime && log.appId.isEmpty) {\n      logInfo(s\"Deleting invalid / corrupt event log ${log.logPath}\")\n      deleteLog(fs, new Path(log.logPath))"
  },
  {
    "id" : "dfae6922-c3ab-47dd-bb22-8ebb35ae5e3a",
    "prId" : 25670,
    "prUrl" : "https://github.com/apache/spark/pull/25670#pullrequestreview-297766157",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "15e40544-a238-4dea-89a0-f7b8606ef7e6",
        "parentId" : null,
        "authorId" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "body" : "It would be good to check what happens when you have an existing listing db with the old version of this type, and the new code loads it. Is this field properly initialized to `None`, or does it become `null`, which would cause problems?",
        "createdAt" : "2019-10-02T16:48:55Z",
        "updatedAt" : "2019-10-16T20:56:19Z",
        "lastEditedBy" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "tags" : [
        ]
      },
      {
        "id" : "533d97ed-32d5-47dc-ae33-e0202c00c639",
        "parentId" : "15e40544-a238-4dea-89a0-f7b8606ef7e6",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "I missed to check this. Will check and fix if it causes problem via adding UT which loads LevelDB from Spark 2.4.4 and read more applications.",
        "createdAt" : "2019-10-03T03:01:49Z",
        "updatedAt" : "2019-10-16T20:56:19Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "cc353dae-4fe8-4391-b477-0022f5ba245d",
        "parentId" : "15e40544-a238-4dea-89a0-f7b8606ef7e6",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "I've crafted some test code to experiment this: \r\n\r\n```\r\n    val newLogDir = new File(\"/spark-dist/releases/spark-2.4.4-bin-hadoop2.7/eventlogs\")\r\n    val localStoreDir = new File(\"/spark-dist/releases/spark-2.4.4-bin-hadoop2.7/history-dir\")\r\n    val newLocalStoreDir = new File(testDir, \"localstore\")\r\n    newLocalStoreDir.mkdirs()\r\n    FileUtils.copyDirectory(localStoreDir, newLocalStoreDir)\r\n\r\n    val conf = createTestConf()\r\n    conf.set(HISTORY_LOG_DIR, newLogDir.getAbsolutePath)\r\n    conf.set(LOCAL_STORE_DIR, newLocalStoreDir.getAbsolutePath)\r\n\r\n    val provider = new FsHistoryProvider(conf)\r\n    provider.checkForLogs()\r\n```\r\n\r\nHere the directory directly points to the eventlogs/history local store directory in Spark 2.4.4 since metadata of listing contains the path and Spark ignores the content of local store if metadata doesn't match. If we want to include this to new UT, some modification might be needed. We tend to add UT if we want to ensure the new change is compatible with old version, so seem to be worth to add.\r\n\r\nI've set breakpoint to `FsHistoryProvider.checkForLogs`, and observed lastIndex is set to `None` (so it seems to be safe), but `logType` is set to `null` which we don't expect.\r\n\r\nWould we be better to increase `CURRENT_LISTING_VERSION` to deal with this, or manually set to `EventLogs` when it's null? Maybe better to deal with new PR though.",
        "createdAt" : "2019-10-04T01:38:42Z",
        "updatedAt" : "2019-10-16T20:56:19Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "9bc20a8b-bf3b-43e8-9657-eeef4ef5553e",
        "parentId" : "15e40544-a238-4dea-89a0-f7b8606ef7e6",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Just realized that FsHistoryProvider does some care on `logType` being `null`. I wouldn't expect LogType.Value is null and feel safer to correct the value, but given now FsHistoryProvider accesses logType correctly, that's OK. I'd prefer to add UT to check the values of field of LogInfo when reading from Spark 2.4.4 to make sure we're not missing anything, but it's just me.",
        "createdAt" : "2019-10-04T03:05:19Z",
        "updatedAt" : "2019-10-16T20:56:19Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      },
      {
        "id" : "10d8a9fb-e3fc-479e-819c-638d2a9ec4bd",
        "parentId" : "15e40544-a238-4dea-89a0-f7b8606ef7e6",
        "authorId" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "body" : "You don't need to create a disk store with the old Spark version to test this. This is basically a test of deserializing an old JSON payload using `KVStoreScalaSerializer`; feed it JSON without the new field, and see what comes out of it.",
        "createdAt" : "2019-10-04T16:21:43Z",
        "updatedAt" : "2019-10-16T20:56:19Z",
        "lastEditedBy" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "tags" : [
        ]
      },
      {
        "id" : "fde218f8-3020-4911-9da5-9f2c4a6f0b37",
        "parentId" : "15e40544-a238-4dea-89a0-f7b8606ef7e6",
        "authorId" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "body" : "Good point. In other tests which we have modified something and worried about compatibility, we stored the old data and load from there so I thought about the same. Your suggestion would be pretty much simpler. Thanks!",
        "createdAt" : "2019-10-05T00:50:17Z",
        "updatedAt" : "2019-10-16T20:56:19Z",
        "lastEditedBy" : "0c28e5da-df9b-4076-bb67-3b6878f1f4ce",
        "tags" : [
        ]
      }
    ],
    "commit" : "a2f631d88504ed360f3f3d3bcb3ceffb83f9c75f",
    "line" : 365,
    "diffHunk" : "@@ -1,1 +1171,1175 @@    attemptId: Option[String],\n    fileSize: Long,\n    lastIndex: Option[Long],\n    isComplete: Boolean)\n"
  }
]