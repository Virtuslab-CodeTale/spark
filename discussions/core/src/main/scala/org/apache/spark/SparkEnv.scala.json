[
  {
    "id" : "1fd122bd-7689-4863-aa65-c5cc9ca5e3e8",
    "prId" : 30725,
    "prUrl" : "https://github.com/apache/spark/pull/30725#pullrequestreview-556128709",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2c517ef4-88b4-4068-8c36-cde526633a8c",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "is it better to put a size limitation for this cache? then soft reference should also be fine.",
        "createdAt" : "2020-12-16T08:34:07Z",
        "updatedAt" : "2020-12-16T08:34:25Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "34cc4d6d-ba7a-4c2c-979f-a822dec74ebf",
        "parentId" : "2c517ef4-88b4-4068-8c36-cde526633a8c",
        "authorId" : "8de20372-3c43-4f6b-838b-45ccb852019b",
        "body" : "Use limited size, soft-reference cache can reduce the number of YGC than weak-reference.\r\nBut what size should it be limited to?\r\nIn fact, the driver rarely has the opportunity to reuse the jobconf of the cache, and it makes sense to share the jobconf in the executor.",
        "createdAt" : "2020-12-16T09:29:20Z",
        "updatedAt" : "2020-12-16T09:29:20Z",
        "lastEditedBy" : "8de20372-3c43-4f6b-838b-45ccb852019b",
        "tags" : [
        ]
      },
      {
        "id" : "a8603d02-4920-4f0d-b9bd-a2de138a065d",
        "parentId" : "2c517ef4-88b4-4068-8c36-cde526633a8c",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Whether or not a limit is in place (which could also be a good back-stop to prevent a huge cache), this could be fine - I think the only risk is that weak references are quite readily reclaimed, so this risks losing most of the caching.",
        "createdAt" : "2020-12-21T04:55:17Z",
        "updatedAt" : "2020-12-21T04:55:17Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      }
    ],
    "commit" : "5c125cdd2af39d8de5187d11f4172c0cf8fdd6f9",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +78,82 @@  // (e.g., HadoopRDD uses this to cache JobConfs).\n  private[spark] val hadoopJobMetadata =\n    CacheBuilder.newBuilder().weakValues().build[String, AnyRef]().asMap()\n\n  private[spark] var driverTmpDir: Option[String] = None"
  },
  {
    "id" : "4fd3fe65-dd1a-4949-9dac-5c3742f97cda",
    "prId" : 28708,
    "prUrl" : "https://github.com/apache/spark/pull/28708#pullrequestreview-433770929",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "032f33c6-e990-483c-b7a9-ea28b3fce5df",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "Should there be a `isDriver` check here ? I believe the MapOutputTrackerMaster is only available on the driver ?",
        "createdAt" : "2020-06-19T01:22:45Z",
        "updatedAt" : "2020-07-17T23:45:20Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "03470695-c669-4717-9835-fe4e9192506d",
        "parentId" : "032f33c6-e990-483c-b7a9-ea28b3fce5df",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "`registerOrLookupEndpoint` does the trick, if the BlockManagerMasterEndpoint is being constructed the `mapOutputTracker` is the `mapOutputTrackerMaster`.",
        "createdAt" : "2020-06-19T03:23:08Z",
        "updatedAt" : "2020-07-17T23:45:20Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "8494bdd94285c7cc5a41e151da920710be7f4671",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +369,373 @@            None\n          }, blockManagerInfo,\n          mapOutputTracker.asInstanceOf[MapOutputTrackerMaster])),\n      registerOrLookupEndpoint(\n        BlockManagerMaster.DRIVER_HEARTBEAT_ENDPOINT_NAME,"
  },
  {
    "id" : "d15c1d43-429f-432d-89bd-03c3e2d4f1fc",
    "prId" : 26911,
    "prUrl" : "https://github.com/apache/spark/pull/26911#pullrequestreview-332800582",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bd260fd5-646e-47cc-9d91-cd8d95066bdc",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "This object was really a cache anyway and it was recommended to use CacheBuilder instead of MapMaker.",
        "createdAt" : "2019-12-16T19:17:44Z",
        "updatedAt" : "2019-12-18T00:42:58Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      }
    ],
    "commit" : "a6de24a64b8aa23699a7527919389764e935f1e5",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +77,81 @@  // A general, soft-reference map for metadata needed during HadoopRDD split computation\n  // (e.g., HadoopFileRDD uses this to cache JobConfs and InputFormats).\n  private[spark] val hadoopJobMetadata =\n    CacheBuilder.newBuilder().softValues().build[String, AnyRef]().asMap()\n"
  }
]