[
  {
    "id" : "d58300ca-75eb-46ae-b2a1-287aaf28810a",
    "prId" : 31373,
    "prUrl" : "https://github.com/apache/spark/pull/31373#pullrequestreview-578575706",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b9095a8f-474b-4248-bf7d-3c83c1b27f33",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "This sounds like a race condition that happens during SparkContext shutdown. So it's also possible that the SparkContext is stopped right after we sent the `HeartbeatResponse`. In that case, IIUC, the issue will still exist.\r\n\r\nDoes the current behavior cause any real issue?",
        "createdAt" : "2021-01-28T09:45:09Z",
        "updatedAt" : "2021-01-28T09:45:15Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "7e3cbcf9-874a-4a89-8dac-93d4130c1a7e",
        "parentId" : "b9095a8f-474b-4248-bf7d-3c83c1b27f33",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Thank. you for review, @Ngone51 . That's true. In the production environment, I hit those intermediate status. And, this will help us simplify the situation.\r\n\r\n1. The case you mentioned, `Send HeartbeatResponse and  sc.stop invoked`, is a normal situation. The users don't complain about this.\r\n2. The case in this PR, `sc.stop invoked and Spark works inefficiently by sending HeartbeatResponse(true)` is a problem. The users complain about this.\r\n\r\nFor the following, yes. We are After the apps `sc.stop` takes a longer time than we expect.\r\n> Does the current behavior cause any real issue?",
        "createdAt" : "2021-01-28T17:31:37Z",
        "updatedAt" : "2021-01-28T17:51:35Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "510a505bd226d684ae771b911e0eed04946019d6",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +129,133 @@    // Messages received from executors\n    case heartbeat @ Heartbeat(executorId, accumUpdates, blockManagerId, executorUpdates) =>\n      var reregisterBlockManager = !sc.isStopped\n      if (scheduler != null) {\n        if (executorLastSeen.contains(executorId)) {"
  },
  {
    "id" : "70dde018-1864-4e51-89b0-63d521b3c239",
    "prId" : 30547,
    "prUrl" : "https://github.com/apache/spark/pull/30547#pullrequestreview-542467247",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a31200bc-0051-47e3-9506-69bf35e13325",
        "parentId" : null,
        "authorId" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "body" : "@HyukjinKwon  There are two reasons for change from \r\n```\r\nsc.conf.get(\r\n  config.STORAGE_BLOCKMANAGER_HEARTBEAT_TIMEOUT\r\n).getOrElse(sc.conf.getTimeAsMs(Network.NETWORK_TIMEOUT.key))\r\n```\r\nto \r\n\r\n```\r\nsc.conf.get(\r\n    config.STORAGE_BLOCKMANAGER_HEARTBEAT_TIMEOUT\r\n  ).getOrElse(Utils.timeStringAsMs(s\"${sc.conf.get(Network.NETWORK_TIMEOUT)}s\"))\r\n```\r\nï¼š\r\n\r\n- Maybe `Network.NETWORK_TIMEOUT` is not configured, need a default value\r\n\r\n- TimeUnit of `Network.NETWORK_TIMEOUT` is `TimeUnit.SECONDS` , so if user configure  `Network.NETWORK_TIMEOUT`  to `200` not `200s`, the result of `executorTimeoutMs` will be `200ms` not `200000ms` when use `sc.conf.getTimeAsMs(Network.NETWORK_TIMEOUT.key)`, it should be wrong",
        "createdAt" : "2020-12-02T03:13:57Z",
        "updatedAt" : "2020-12-02T03:14:11Z",
        "lastEditedBy" : "c7afbd1a-ab9f-4878-b837-32685ae783b0",
        "tags" : [
        ]
      },
      {
        "id" : "197faa0b-94e8-4a97-94a1-68e631441ef1",
        "parentId" : "a31200bc-0051-47e3-9506-69bf35e13325",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Yeah, that's fine.",
        "createdAt" : "2020-12-02T03:14:52Z",
        "updatedAt" : "2020-12-02T03:14:53Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "908863543372655091b8f6114b8ec5ec0290763e",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +83,87 @@  private val executorTimeoutMs = sc.conf.get(\n    config.STORAGE_BLOCKMANAGER_HEARTBEAT_TIMEOUT\n  ).getOrElse(Utils.timeStringAsMs(s\"${sc.conf.get(Network.NETWORK_TIMEOUT)}s\"))\n\n  private val checkTimeoutIntervalMs = sc.conf.get(Network.NETWORK_TIMEOUT_INTERVAL)"
  },
  {
    "id" : "90482634-865f-4cf1-983a-6fd12a6ca30a",
    "prId" : 30131,
    "prUrl" : "https://github.com/apache/spark/pull/30131#pullrequestreview-516492535",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "03fe60dd-443b-4df7-a5eb-3d5c12e64d1a",
        "parentId" : null,
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "call for clean dead host, also just need with delay.",
        "createdAt" : "2020-10-26T06:25:41Z",
        "updatedAt" : "2020-11-26T00:49:06Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "5643ec267a339877c1004aae9e1ed8ae1f8f6cca",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +105,109 @@  override def onStart(): Unit = {\n    timeoutCheckingTask = eventLoopThread.scheduleWithFixedDelay(\n      () => Utils.tryLogNonFatalError { Option(self).foreach(_.ask[Boolean](ExpireDeadHosts)) },\n      0, checkTimeoutIntervalMs, TimeUnit.MILLISECONDS)\n  }"
  },
  {
    "id" : "d23e962b-1d8b-4d6c-9633-c8af836c16e3",
    "prId" : 26980,
    "prUrl" : "https://github.com/apache/spark/pull/26980#pullrequestreview-336054669",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d23c88b7-b24b-4215-b775-5e764605a92f",
        "parentId" : null,
        "authorId" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "body" : "Why not put this into TaskSchedulerImpl.executorLost() ?",
        "createdAt" : "2019-12-23T19:30:06Z",
        "updatedAt" : "2019-12-27T06:51:59Z",
        "lastEditedBy" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "tags" : [
        ]
      },
      {
        "id" : "14926e6e-9985-4d39-9af3-c2d60b5ae8f1",
        "parentId" : "d23c88b7-b24b-4215-b775-5e764605a92f",
        "authorId" : "53798d1f-01ee-4eeb-9fde-46adb6017627",
        "body" : "I think that causes a loop / cycle because `CoarseGrainedSchedulerBackend.removeExecutor` already calls `TaskSchedulerImpl.executorLost`.",
        "createdAt" : "2019-12-24T01:27:50Z",
        "updatedAt" : "2019-12-27T06:51:59Z",
        "lastEditedBy" : "53798d1f-01ee-4eeb-9fde-46adb6017627",
        "tags" : [
        ]
      },
      {
        "id" : "589dc535-6adc-4327-a2ae-656dd9446f3b",
        "parentId" : "d23c88b7-b24b-4215-b775-5e764605a92f",
        "authorId" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "body" : "ah, i see",
        "createdAt" : "2019-12-24T01:34:57Z",
        "updatedAt" : "2019-12-27T06:51:59Z",
        "lastEditedBy" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "5f2f86ae4d1f44a12088f7f7a4cb43e9a1189837",
    "line" : 30,
    "diffHunk" : "@@ -1,1 +216,220 @@            // 2) call scheduler.executorLost() underlying to fail any tasks assigned to\n            //    those executors to avoid app hang\n            sc.schedulerBackend match {\n              case backend: CoarseGrainedSchedulerBackend =>\n                backend.driverEndpoint.send(RemoveExecutor(executorId,"
  },
  {
    "id" : "b56f4b05-a5ad-4b5d-b3f1-c894fdb6ff81",
    "prId" : 26980,
    "prUrl" : "https://github.com/apache/spark/pull/26980#pullrequestreview-336676539",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ac7b5508-4e51-4a01-a2f5-c8a2e555238d",
        "parentId" : null,
        "authorId" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "body" : "I'd just mention we shall explicitly remove the information of the killed executors from scheduler backend instead of waiting for Disconnect message, to avoid task hang. ",
        "createdAt" : "2019-12-27T05:55:47Z",
        "updatedAt" : "2019-12-27T06:51:59Z",
        "lastEditedBy" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "5f2f86ae4d1f44a12088f7f7a4cb43e9a1189837",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +209,213 @@            // so do not simply call `sc.killExecutor` here (SPARK-8119)\n            sc.killAndReplaceExecutor(executorId)\n            // SPARK-27348: in case of the executors which are not gracefully shut down,\n            // we should remove lost executors from CoarseGrainedSchedulerBackend manually\n            // here to guarantee two things:"
  },
  {
    "id" : "e032ee76-a7f3-4472-9e7d-8e45b10ee6fb",
    "prId" : 26938,
    "prUrl" : "https://github.com/apache/spark/pull/26938#pullrequestreview-335819376",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "97a9154f-6d8b-4fed-a254-c48fcc416ac7",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "I think this does not resolve the issue completely. Because when you look into `sc.killAndReplaceExecutor` below, you can see that we've already try to mark it as \"pending to remove\". Though it happens in another separate thread, but I don't see big difference according to this issue. WDYT?",
        "createdAt" : "2019-12-20T13:32:43Z",
        "updatedAt" : "2019-12-20T13:33:39Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "af6de75b-63b3-477b-8b84-f341e1b2fef6",
        "parentId" : "97a9154f-6d8b-4fed-a254-c48fcc416ac7",
        "authorId" : "78c98141-e5a6-4d72-a74b-e121b62fdbf4",
        "body" : "> `sc.killAndReplaceExecutor` already try to mark it as \"pending to remove\" this is right, but the task has rescheduled at this executor agait at this time, the executor must be removed from the ExecutorBackend to avoid. \r\nFor example, it will `disableExecutor` in `CoarseGrainedSchedulerBackend` if the driver lost connection from executor, `disableExecutor` mark the executor dead, and then to reschedule the task on the lost connection executors.",
        "createdAt" : "2019-12-21T03:12:47Z",
        "updatedAt" : "2019-12-21T03:39:59Z",
        "lastEditedBy" : "78c98141-e5a6-4d72-a74b-e121b62fdbf4",
        "tags" : [
        ]
      },
      {
        "id" : "491644de-ae4a-43e7-b81d-e6317f5f8fb5",
        "parentId" : "97a9154f-6d8b-4fed-a254-c48fcc416ac7",
        "authorId" : "78c98141-e5a6-4d72-a74b-e121b62fdbf4",
        "body" : "The task can reschedule at this executor before mark it as \"pending to remove\".",
        "createdAt" : "2019-12-21T03:14:26Z",
        "updatedAt" : "2019-12-21T03:14:26Z",
        "lastEditedBy" : "78c98141-e5a6-4d72-a74b-e121b62fdbf4",
        "tags" : [
        ]
      },
      {
        "id" : "ebe51c5f-c1f3-4a01-93af-f99d318f08cc",
        "parentId" : "97a9154f-6d8b-4fed-a254-c48fcc416ac7",
        "authorId" : "78c98141-e5a6-4d72-a74b-e121b62fdbf4",
        "body" : "We add the executor before rescheduler can avoid the tasks to rescheduler the bad executors",
        "createdAt" : "2019-12-21T09:59:34Z",
        "updatedAt" : "2019-12-21T09:59:35Z",
        "lastEditedBy" : "78c98141-e5a6-4d72-a74b-e121b62fdbf4",
        "tags" : [
        ]
      },
      {
        "id" : "e5d14c69-8c2c-4eac-902f-3871e1b726c0",
        "parentId" : "97a9154f-6d8b-4fed-a254-c48fcc416ac7",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Hi, @seayoun. After a second think, I think this fix can really avoid app hang, though it can not clean up dead records in `CoarseGrainedSchedulerBackend`. \r\n\r\nAs you may have realized that a same issue has been posted in SPARK-27348 and its author(@xuanyuanking) has assigned to me recently. And it's really coincidental. And I really appreciate that you would like my proposal. Actually, the main idea is still base on original author's contribution.\r\n\r\nAlso, thanks for review.\r\n",
        "createdAt" : "2019-12-23T11:19:10Z",
        "updatedAt" : "2019-12-23T11:19:11Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "036dcc2aafb51862feae95c5502279ef29a7ee41",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +207,211 @@              // to avoid reschedule task on this executor again\n              if (!backend.executorsPendingToRemove.contains(executorId)) {\n                backend.executorsPendingToRemove(executorId) = false\n              }\n            }"
  }
]