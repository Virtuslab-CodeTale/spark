[
  {
    "id" : "1a9af7b7-0fd6-4683-964a-9a7435d995e1",
    "prId" : 33529,
    "prUrl" : "https://github.com/apache/spark/pull/33529#pullrequestreview-717295595",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "427a3739-7ed0-4b08-880e-0183bb28a08b",
        "parentId" : null,
        "authorId" : "b3ba992a-312c-46eb-b3c3-8d861d15ac40",
        "body" : "The error message seems changed here.",
        "createdAt" : "2021-07-28T18:26:59Z",
        "updatedAt" : "2021-07-28T18:28:24Z",
        "lastEditedBy" : "b3ba992a-312c-46eb-b3c3-8d861d15ac40",
        "tags" : [
        ]
      }
    ],
    "commit" : "370e975818f827eddf522243a56d64d2205f08c6",
    "line" : 41,
    "diffHunk" : "@@ -1,1 +920,924 @@        // must mean the error is during registration.\n        // It might be good to do something smarter here in the future.\n        throw SparkCoreErrors.clusterSchedulerError(message)\n      }\n    }"
  },
  {
    "id" : "48f0d729-ba23-4a8e-9a1d-2e5b7722ebd6",
    "prId" : 29788,
    "prUrl" : "https://github.com/apache/spark/pull/29788#pullrequestreview-491637014",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "544efe40-a3d4-4a94-8ee2-b2cfa01e55ad",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "instead of 'shutdown', should we say 'is finally lost' ? To be more accurate in this setting.\r\n\r\n+1 on this change to avoid log spam.",
        "createdAt" : "2020-09-18T17:25:06Z",
        "updatedAt" : "2020-09-18T17:38:05Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      }
    ],
    "commit" : "9bebdd4f2846f12f8ab13279c8cece151e8edfd0",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +973,977 @@    case ExecutorDecommission(reason, _) =>\n      // use logInfo instead of logError as the loss of decommissioned executor is what we expect\n      logInfo(s\"Decommissioned executor $executorId on $hostPort shutdown: $reason\")\n    case _ =>\n      logError(s\"Lost executor $executorId on $hostPort: $reason\")"
  },
  {
    "id" : "adb7dc69-0088-4eb7-a1a7-adf54af1a388",
    "prId" : 29468,
    "prUrl" : "https://github.com/apache/spark/pull/29468#pullrequestreview-470161779",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b74e793e-5aa8-4cad-80a4-6589d0646706",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "I haven't looked at this code too deeply, but does it make sense it eagerly remove the decommissioned executor from hostToExecutors and recomputeLocality and thus rebuild hostsByRack etc ? \r\n\r\n\r\n",
        "createdAt" : "2020-08-18T17:33:37Z",
        "updatedAt" : "2020-08-20T06:22:17Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "e89b694b-ede2-4078-870b-b63437f30b47",
        "parentId" : "b74e793e-5aa8-4cad-80a4-6589d0646706",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "`hostToExecutors` will also be used to handle executor removal and blacklisting stuff. In other words, a decommissioned executor could also be removed or blacklisted at the same time. So, remove the executor from `hostToExecutors` might also affect other places. I don't want to mix them together to make things complex.\r\n\r\nBesides, similar to `executorIdToRunningTaskIds`, a decommissioned executor could also have running tasks on it. So we don't eagerly remove it from `executorIdToRunningTaskIds` too.",
        "createdAt" : "2020-08-19T03:36:09Z",
        "updatedAt" : "2020-08-20T06:22:17Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "cb921947-4b86-46cb-b452-516b5e892344",
        "parentId" : "b74e793e-5aa8-4cad-80a4-6589d0646706",
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "Got it. Thanks for considering it.",
        "createdAt" : "2020-08-19T06:35:05Z",
        "updatedAt" : "2020-08-20T06:22:17Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      }
    ],
    "commit" : "65f6e0525b2d1472c1e6956587253e5ae6263676",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +1072,1076 @@\n  def hasHostAliveOnRack(rack: String): Boolean = synchronized {\n    hostsByRack.get(rack)\n      .exists(hosts => hosts.exists(h => !isHostDecommissioned(h)))\n  }"
  },
  {
    "id" : "df9cf048-0b84-4508-88f8-c20b6f12ebc6",
    "prId" : 29468,
    "prUrl" : "https://github.com/apache/spark/pull/29468#pullrequestreview-470179946",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "688f9374-00bc-403c-a4e2-1e1c0c5469a0",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "I don't think this method needs to be changed: It only seems to be used when the HDFS split is cached in memory by Hadoop. Perhaps we should restrict to changing only the methods in the `computeValidLocalityLevels` call path ?",
        "createdAt" : "2020-08-18T18:13:34Z",
        "updatedAt" : "2020-08-20T06:22:17Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "831da774-dda0-4ddf-a6ea-ccb9aeef4873",
        "parentId" : "688f9374-00bc-403c-a4e2-1e1c0c5469a0",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "hmm..I think this more like a problem of definition or something like that. In this PR, what I'm trying to do is to strengthen the definition of the active status for the executor or host. That is, for an active executor or host, it should not be decommissioned at the same time. This function is getting the active host, so we should filter out decommissioned ones. I think this's quite obvious. And this function is used by multiple tests.",
        "createdAt" : "2020-08-19T03:46:17Z",
        "updatedAt" : "2020-08-20T06:22:17Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "1e78b6b1-b561-4d7d-a0c1-d764a9410280",
        "parentId" : "688f9374-00bc-403c-a4e2-1e1c0c5469a0",
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "I am just a bit wary of regressions and I am not too sure about the unit test coverage here. So generally like the code changes to be minimal. But I am okay with this change if you think it is safe. ",
        "createdAt" : "2020-08-19T06:36:59Z",
        "updatedAt" : "2020-08-20T06:22:17Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "3506c6f5-61c1-410e-83cd-c228991bd409",
        "parentId" : "688f9374-00bc-403c-a4e2-1e1c0c5469a0",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "It's super weird if `getExecutorsAliveOnHost` and `hasExecutorsAliveOnHost` are not consistent.",
        "createdAt" : "2020-08-19T07:08:39Z",
        "updatedAt" : "2020-08-20T06:22:17Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "65f6e0525b2d1472c1e6956587253e5ae6263676",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1063,1067 @@\n  def getExecutorsAliveOnHost(host: String): Option[Set[String]] = synchronized {\n    hostToExecutors.get(host).map(_.filterNot(isExecutorDecommissioned)).map(_.toSet)\n  }\n"
  },
  {
    "id" : "b89b8f1b-6872-42bd-9b81-86962ec66f13",
    "prId" : 29468,
    "prUrl" : "https://github.com/apache/spark/pull/29468#pullrequestreview-470647653",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "44856237-8add-4219-bc0d-39f26c4e8106",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "nit: Should these two methods be declared final : We don't really want to override them and instead want to override the helper methods it calls.",
        "createdAt" : "2020-08-19T16:41:51Z",
        "updatedAt" : "2020-08-20T06:22:17Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      }
    ],
    "commit" : "65f6e0525b2d1472c1e6956587253e5ae6263676",
    "line" : 29,
    "diffHunk" : "@@ -1,1 +1084,1088 @@  }\n\n  // exposed for test\n  protected final def isExecutorDecommissioned(execId: String): Boolean =\n    getExecutorDecommissionInfo(execId).nonEmpty"
  },
  {
    "id" : "2314065b-6721-486d-8ef9-c7357f006a31",
    "prId" : 29395,
    "prUrl" : "https://github.com/apache/spark/pull/29395#pullrequestreview-464262549",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "51402bd9-fddb-45c5-a002-0d409f1454cb",
        "parentId" : null,
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "it would be nice to add a comment (like master branch) that this is only used with barrier scheduling, availableSlots not used otherwise.",
        "createdAt" : "2020-08-10T14:09:32Z",
        "updatedAt" : "2020-08-18T01:49:27Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "daa205dc9f257f42b794767601fd844dd596e374",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +449,453 @@      // we only need to calculate available slots if using barrier scheduling, otherwise the\n      // value is -1\n      val availableSlots = if (taskSet.isBarrier) {\n        val availableResourcesAmount = availableResources.map { resourceMap =>\n          // note that the addresses here have been expanded according to the numParts"
  },
  {
    "id" : "aa0e77d7-6e74-46e3-b207-68c4335c68b4",
    "prId" : 29014,
    "prUrl" : "https://github.com/apache/spark/pull/29014#pullrequestreview-452918174",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1675bb71-9ba8-47e6-a87d-2b6c631dda4e",
        "parentId" : null,
        "authorId" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "body" : "~~Is there a reason to use the lock on the whole `TaskSchedulerImpl` instance? If there is none then we can change the `HashMap` to a `ConcurrentHashMap` and here we can use a `putIfAbsent`.~~\r\n",
        "createdAt" : "2020-07-20T13:50:45Z",
        "updatedAt" : "2020-07-29T01:25:46Z",
        "lastEditedBy" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "tags" : [
        ]
      },
      {
        "id" : "dea65fbb-9f3f-4721-8d27-4d0660bccea7",
        "parentId" : "1675bb71-9ba8-47e6-a87d-2b6c631dda4e",
        "authorId" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "body" : "Is there any guarantee that a `ExecutorDecommissionInfo` with `isHostDecommissioned=false` is never followed by another `ExecutorDecommissionInfo` where `isHostDecommissioned=true`?  ",
        "createdAt" : "2020-07-20T15:48:07Z",
        "updatedAt" : "2020-07-29T01:25:46Z",
        "lastEditedBy" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "tags" : [
        ]
      },
      {
        "id" : "fea02f2a-a354-43cf-b66c-1418fd76c9b3",
        "parentId" : "1675bb71-9ba8-47e6-a87d-2b6c631dda4e",
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "That would be a bug: isHostDecommissioned is ultimately decided by the cluster manager. It knows for sure whether or not the host is decommissioned with the executor. It shouldn't change its mind :-). I can add an assert in the Driver but it would be a bit of an overkill. ",
        "createdAt" : "2020-07-21T01:08:30Z",
        "updatedAt" : "2020-07-29T01:25:46Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "d1e7f03e-ba0b-45f9-963a-b92b69700067",
        "parentId" : "1675bb71-9ba8-47e6-a87d-2b6c631dda4e",
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "Changed this to \"last writer wins\"",
        "createdAt" : "2020-07-21T02:04:48Z",
        "updatedAt" : "2020-07-29T01:25:46Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "5238bef5-fff2-49ac-b526-03e1e2796bc9",
        "parentId" : "1675bb71-9ba8-47e6-a87d-2b6c631dda4e",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "So my concern here is that if a task is scheduled part of the way through decommissioning the executor *could* send back a decommission message with the host lost. Maybe use a getOrElse to make sure if we've ever learnt the entire host is going away we don't forget that?",
        "createdAt" : "2020-07-21T20:47:54Z",
        "updatedAt" : "2020-07-29T01:25:46Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "a16f06b6-f0cc-4c46-81ca-97b5dc3bf72b",
        "parentId" : "1675bb71-9ba8-47e6-a87d-2b6c631dda4e",
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "Ah, I see your point -- its because of the DecommissionExecutor messages that the executor sends to the driver when work is assigned to it. And there, it can reply back with a hostDecommissioned = false, because the executor itself does not know if the host is going away or not (only the cluster manager knows). \r\n\r\nI like your suggestion of doing this \"merging\" semantics, it is easier/less-work than plumbing host Lost to the executor :-) ",
        "createdAt" : "2020-07-22T00:33:19Z",
        "updatedAt" : "2020-07-29T01:25:46Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      }
    ],
    "commit" : "dbc478b0d96de0f1a48f93d90f1e648229b5790c",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +942,946 @@  override def executorDecommission(\n      executorId: String, decommissionInfo: ExecutorDecommissionInfo): Unit = {\n    synchronized {\n      // Don't bother noting decommissioning for executors that we don't know about\n      if (executorIdToHost.contains(executorId)) {"
  },
  {
    "id" : "d51419fe-2660-4b71-8774-ec02db1afef3",
    "prId" : 29014,
    "prUrl" : "https://github.com/apache/spark/pull/29014#pullrequestreview-451741240",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c698e332-c004-4a78-9a86-8824895b5323",
        "parentId" : null,
        "authorId" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "body" : "~~There is no synchronized here?~~",
        "createdAt" : "2020-07-20T15:50:51Z",
        "updatedAt" : "2020-07-29T01:25:46Z",
        "lastEditedBy" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "tags" : [
        ]
      },
      {
        "id" : "8f8579f4-349f-48d6-a58f-922414cc5efc",
        "parentId" : "c698e332-c004-4a78-9a86-8824895b5323",
        "authorId" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "body" : "Oh I see, the synchronized is at the caller place in: `executorLost`. This way it is you can leave the HashMap as it is.",
        "createdAt" : "2020-07-20T15:54:54Z",
        "updatedAt" : "2020-07-29T01:25:46Z",
        "lastEditedBy" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "tags" : [
        ]
      }
    ],
    "commit" : "dbc478b0d96de0f1a48f93d90f1e648229b5790c",
    "line" : 58,
    "diffHunk" : "@@ -1,1 +1067,1071 @@    }\n\n    executorsPendingDecommission -= executorId\n\n    if (reason != LossReasonPending) {"
  },
  {
    "id" : "2cabb246-f63b-432c-9b3f-58dd01ec5bc4",
    "prId" : 28476,
    "prUrl" : "https://github.com/apache/spark/pull/28476#pullrequestreview-408034964",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0589d146-9656-4129-8ebb-69aa6f7b444f",
        "parentId" : null,
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "I'm not sure if we should recommend disabling blacklisting.  If it was supposed to be blacklisted assumption is something was wrong with it so job would probably fail anyway.  Maybe we should just say it may have been blacklisted like before.",
        "createdAt" : "2020-05-07T14:11:35Z",
        "updatedAt" : "2020-05-07T14:11:36Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "7880e746-f9ba-4dd3-a76a-aa00c87bf59c",
        "parentId" : "0589d146-9656-4129-8ebb-69aa6f7b444f",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Thanks for review.\r\n\r\nI realized a fact that blacklisting actually does not work for barrier taskset. As you may know, blacklisting only takes effect when there's failed task. But for a barrier task set, it will be marked as zombie once there's any failed task and we don't consider blacklisting for a zombie task set, see:\r\n\r\nhttps://github.com/apache/spark/blob/8b4862953a879a9b3ba6f57e669efc383df68b7c/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala#L885-L894\r\n\r\nSo, I think blacklisting actually won't cause partial tasks launching. I will close this PR.",
        "createdAt" : "2020-05-08T06:48:54Z",
        "updatedAt" : "2020-05-08T06:50:11Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "94f29254-49d2-4092-87d1-375108f41736",
        "parentId" : "0589d146-9656-4129-8ebb-69aa6f7b444f",
        "authorId" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "body" : "+1",
        "createdAt" : "2020-05-08T06:49:49Z",
        "updatedAt" : "2020-05-08T06:49:49Z",
        "lastEditedBy" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "9df2bf0563c684dd732e0682b1f460f37fef495f",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +692,696 @@                s\"blacklisting is enabled, as barrier execution currently does not work \" +\n                s\"gracefully with them. We highly recommend you to disable delay scheduling \" +\n                s\"by setting spark.locality.wait=0 or disable blacklisting by setting \" +\n                s\"spark.blacklist.enabled=false as a workaround if you see this error frequently.\"\n            logWarning(errorMsg)"
  },
  {
    "id" : "f56934b4-2912-4d77-bf80-f2688e574d30",
    "prId" : 28476,
    "prUrl" : "https://github.com/apache/spark/pull/28476#pullrequestreview-408034929",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ac436325-c0d3-4a9a-ac16-044752ebd7b9",
        "parentId" : null,
        "authorId" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "body" : "You might also want to check whether blacklist is actually enabled here. ",
        "createdAt" : "2020-05-08T06:49:44Z",
        "updatedAt" : "2020-05-08T06:49:44Z",
        "lastEditedBy" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "9df2bf0563c684dd732e0682b1f460f37fef495f",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +693,697 @@                s\"gracefully with them. We highly recommend you to disable delay scheduling \" +\n                s\"by setting spark.locality.wait=0 or disable blacklisting by setting \" +\n                s\"spark.blacklist.enabled=false as a workaround if you see this error frequently.\"\n            logWarning(errorMsg)\n            taskSet.abort(errorMsg)"
  },
  {
    "id" : "42402156-eedf-49d2-9c82-d76aa9f3ee84",
    "prId" : 28287,
    "prUrl" : "https://github.com/apache/spark/pull/28287#pullrequestreview-445047768",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "69731f96-30c2-43a4-9534-b641400c4520",
        "parentId" : null,
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "I think the above comment needs to be updated",
        "createdAt" : "2020-07-08T20:56:17Z",
        "updatedAt" : "2020-07-22T17:58:20Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "d6f1e73ae49d9641a8efcb1f5e9016bd378a70c6",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +653,657 @@              // unschedulable tasks else we will abort immediately.\n              executorIdToRunningTaskIds.find(x => !isExecutorBusy(x._1)) match {\n                case Some ((executorId, _)) =>\n                  if (!unschedulableTaskSetToExpiryTime.contains(taskSet)) {\n                    blacklistTrackerOpt.foreach(blt => blt.killBlacklistedIdleExecutor(executorId))"
  },
  {
    "id" : "75a9869c-b3cb-4664-92a9-8abb158807b5",
    "prId" : 28257,
    "prUrl" : "https://github.com/apache/spark/pull/28257#pullrequestreview-398716988",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3abc0a1b-c346-4e63-82e6-18cccf4cfb00",
        "parentId" : null,
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "Any particular reason to change this ?",
        "createdAt" : "2020-04-22T03:24:19Z",
        "updatedAt" : "2020-04-23T16:40:31Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "41ebc4a0-bf2e-4571-9bf1-ced6765ea6e0",
        "parentId" : "3abc0a1b-c346-4e63-82e6-18cccf4cfb00",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "we don't need to pass the raid in anymore since its in the task set which we need now",
        "createdAt" : "2020-04-22T14:14:45Z",
        "updatedAt" : "2020-04-23T16:40:31Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "aee79a45-17bb-4b39-b808-18bb4d2b8be2",
        "parentId" : "3abc0a1b-c346-4e63-82e6-18cccf4cfb00",
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "True, but I was trying to make sense of whether it was relevant to the fix or not.\r\nLooks like an unrelated cleanup",
        "createdAt" : "2020-04-23T02:24:36Z",
        "updatedAt" : "2020-04-23T16:40:31Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "3bca1a62-f132-46b3-a686-4311ba4b0295",
        "parentId" : "3abc0a1b-c346-4e63-82e6-18cccf4cfb00",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "We need `taskSet: TaskSetManager` now because we'll use it to abort the task set below.",
        "createdAt" : "2020-04-23T02:27:01Z",
        "updatedAt" : "2020-04-23T16:40:31Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "e949871e-21b4-4279-b0d4-e28aa6638729",
        "parentId" : "3abc0a1b-c346-4e63-82e6-18cccf4cfb00",
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "Ah ! Yes, I knew I missed something :-)\r\nThanks",
        "createdAt" : "2020-04-23T02:29:07Z",
        "updatedAt" : "2020-04-23T16:40:31Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      }
    ],
    "commit" : "ac9015d1df4ee37504b602111267422461e4262d",
    "line" : 41,
    "diffHunk" : "@@ -1,1 +475,479 @@    val resourceProfile = sc.resourceProfileManager.resourceProfileFromId(\n      taskSet.taskSet.resourceProfileId)\n    val offersForResourceProfile = resourceProfileIds.zipWithIndex.filter { case (id, _) =>\n      (id == resourceProfile.id)\n    }"
  },
  {
    "id" : "3dae5e9b-2583-4d09-a48a-0d568c80991f",
    "prId" : 28257,
    "prUrl" : "https://github.com/apache/spark/pull/28257#pullrequestreview-399477062",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1af74711-810c-479c-bbd4-87fa88075cf5",
        "parentId" : null,
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "The exception change is fine here ? +CC @tgravescs ",
        "createdAt" : "2020-04-23T02:27:12Z",
        "updatedAt" : "2020-04-23T16:40:31Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "29e52ce0-0e2d-40a0-a745-19bffc90a0b2",
        "parentId" : "1af74711-810c-479c-bbd4-87fa88075cf5",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Throw exception should be fine. But having another think, I'm afraid that it might lead to the following developers fall into the same trap in the future. They may not be aware of `taskSet.abort` but only do exception throw.\r\n",
        "createdAt" : "2020-04-23T16:16:42Z",
        "updatedAt" : "2020-04-23T16:40:31Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "43f7bab2-4b03-4f61-8886-295d6bf8d7f2",
        "parentId" : "1af74711-810c-479c-bbd4-87fa88075cf5",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Let me add comments to `TaskSchedulerImpl` as precaution. ",
        "createdAt" : "2020-04-23T16:21:14Z",
        "updatedAt" : "2020-04-23T16:40:31Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "443cdb1d-0009-4642-ae57-3db7bac31c02",
        "parentId" : "1af74711-810c-479c-bbd4-87fa88075cf5",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "yea that should be fine, as it will just be caught and then logged.",
        "createdAt" : "2020-04-23T21:07:30Z",
        "updatedAt" : "2020-04-23T21:07:31Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "ac9015d1df4ee37504b602111267422461e4262d",
    "line" : 103,
    "diffHunk" : "@@ -1,1 +760,764 @@                  \"taskIdToTaskSetManager.contains(tid) <=> taskIdToExecutorId.contains(tid)\"\n                taskSet.abort(errorMsg)\n                throw new SparkException(errorMsg)\n              })\n              if (executorIdToRunningTaskIds.contains(execId)) {"
  },
  {
    "id" : "4a602c19-5a68-4b5a-98bc-40b1c83d1524",
    "prId" : 27773,
    "prUrl" : "https://github.com/apache/spark/pull/27773#pullrequestreview-371403455",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3dcf60c4-c6ad-41fb-8dcb-47250ede1859",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Seem this is not necessary anymore since we've checked in `resourcesMeetTaskRequirements`. But I'm OK to keep it, though.",
        "createdAt" : "2020-03-09T09:03:22Z",
        "updatedAt" : "2020-03-19T20:03:38Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "73ded64e-ebb1-42c5-b5dc-c1b9f23d0e2d",
        "parentId" : "3dcf60c4-c6ad-41fb-8dcb-47250ede1859",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "this was always the case - even when cpu only, we left it when adding the original resource scheduling code as it doesn't hurt and just double check if something really unexpected happens",
        "createdAt" : "2020-03-09T17:54:39Z",
        "updatedAt" : "2020-03-19T20:03:38Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "2a4a73385ee948d81e51091990a37ebbc8010f61",
    "line" : 76,
    "diffHunk" : "@@ -1,1 +358,362 @@              executorIdToRunningTaskIds(execId).add(tid)\n              availableCpus(i) -= taskCpus\n              assert(availableCpus(i) >= 0)\n              task.resources.foreach { case (rName, rInfo) =>\n                // Remove the first n elements from availableResources addresses, these removed"
  },
  {
    "id" : "b20394fe-d2fc-4cc6-8603-56461e4d2891",
    "prId" : 27773,
    "prUrl" : "https://github.com/apache/spark/pull/27773#pullrequestreview-372450271",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1435bda0-009c-44d6-8421-275e633f63c3",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Maybe we can pass in the target slots and stop calculation once we reach the target. ",
        "createdAt" : "2020-03-09T09:35:05Z",
        "updatedAt" : "2020-03-19T20:03:38Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "7a6af9bc-110c-4134-aa20-a808b80ba905",
        "parentId" : "1435bda0-009c-44d6-8421-275e633f63c3",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "we could. the code would become a  loop (adding up as you went) and not like a map.sum().  honestly I just did what was there before and was thinking that barrier doesn't support dynamic allocation at this point (unless it got checked in without me seeing it?) so the number of executors is more likely to be less then what you need, not more.  \r\nDo you see other cases?\r\nI guess  when barrier scheduling starts to support dynamic allocation this could be more of an issue if you run a large ETL stage and then to smaller number with ML. \r\nI'm fine either way, if you think its an issue now I will change or we could wait til dynamic allocation is supported, what are you thoughts?  (note that I am changing that logic as it had a bug that Mridul found in his review) ",
        "createdAt" : "2020-03-09T21:32:52Z",
        "updatedAt" : "2020-03-19T20:03:38Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "11349df1-1b58-4223-aa71-dbc840e8d8b3",
        "parentId" : "1435bda0-009c-44d6-8421-275e633f63c3",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "> unless it got checked in without me seeing it?\r\n\r\nBarrier mode doesn't support dynamic allocation yet.\r\n\r\n\r\nBut sorry I don't quite understand your concern here.\r\n\r\nWith my understanding, `calculateAvailableSlots` only gives a chance to barrier stage to try to launch all tasks at the same time. But it never guarantee that barrier stage can be successfully launched even if there's enough slots(e.g. due to delay scheduling). So, I mean, whether we calculate total or partial sum of resource slots, it should makes no difference for barrier stage scheduling.\r\n\r\n",
        "createdAt" : "2020-03-10T03:15:51Z",
        "updatedAt" : "2020-03-19T20:03:38Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "1e30d88a-14d5-4c3c-88ed-053a7d1f0a8d",
        "parentId" : "1435bda0-009c-44d6-8421-275e633f63c3",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "Barrier just needs to make sure there are X slots available, if there are not it skips that round of scheduling. \r\nWhat I'm saying is this is how the code did it before (calculate all slots) and in the scenario people are using barrier scheduling, they can't use dynamic allocation, so I would  expect the user to request X executors up front so it is very unlikely the job would have X + Y executors, so even though calculateAvailableSlots would add up all slots, I wouldn't expect that to be more then X. Meaning calculateAvailableSlots isn't wasting any time adding in extra slots because they wouldn't be there.",
        "createdAt" : "2020-03-10T13:17:54Z",
        "updatedAt" : "2020-03-19T20:03:38Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "0777d434-3c21-4f17-ad43-f1befcd96237",
        "parentId" : "1435bda0-009c-44d6-8421-275e633f63c3",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Ok, I see.",
        "createdAt" : "2020-03-11T03:07:47Z",
        "updatedAt" : "2020-03-19T20:03:38Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "2a4a73385ee948d81e51091990a37ebbc8010f61",
    "line" : 157,
    "diffHunk" : "@@ -1,1 +428,432 @@  // Use the resource that the resourceProfile has as the limiting resource to calculate the\n  // total number of slots available based on the current offers.\n  private def calculateAvailableSlots(\n      resourceProfileIds: Array[Int],\n      availableCpus: Array[Int],"
  },
  {
    "id" : "06615aee-ecd5-4469-b0d2-6d4bcd1aff88",
    "prId" : 27207,
    "prUrl" : "https://github.com/apache/spark/pull/27207#pullrequestreview-354779339",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "080f148b-fcb2-4810-aaf5-d3d9932becb0",
        "parentId" : null,
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "lets add a detailed comment about the new locality wait functionality",
        "createdAt" : "2020-02-04T16:03:46Z",
        "updatedAt" : "2020-04-03T05:29:21Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "2298ed0b-3f8c-4694-89bc-d937c5d595b0",
        "parentId" : "080f148b-fcb2-4810-aaf5-d3d9932becb0",
        "authorId" : "29a3e834-d9e3-42dc-ab25-20ba332ef3f2",
        "body" : "where do you suggest the documentation go? in the doc of `spark.locality.wait`  configuration inside package.scala?",
        "createdAt" : "2020-02-05T04:54:16Z",
        "updatedAt" : "2020-04-03T05:29:21Z",
        "lastEditedBy" : "29a3e834-d9e3-42dc-ab25-20ba332ef3f2",
        "tags" : [
        ]
      },
      {
        "id" : "2b0c093e-72aa-4569-9bc9-d5cc2cbe057f",
        "parentId" : "080f148b-fcb2-4810-aaf5-d3d9932becb0",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "I was thinking developer docs  in TaskSchedulerImpl somehwere, perhaps an overall description at the top in the javadoc. ",
        "createdAt" : "2020-02-06T21:19:36Z",
        "updatedAt" : "2020-04-03T05:29:22Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "24c8ad9ae23d360c7bb5bc813d04e6f5f6715d93",
    "line" : 188,
    "diffHunk" : "@@ -1,1 +615,619 @@        }\n\n        if (!legacyLocalityWaitReset) {\n          if (noDelaySchedulingRejects && launchedAnyTask) {\n            if (isAllFreeResources || resetOnPreviousOffer.getOrElse(taskSet.taskSet, true)) {"
  },
  {
    "id" : "fe6efe9c-ff0f-4ce7-831d-7ce48b9d597d",
    "prId" : 27207,
    "prUrl" : "https://github.com/apache/spark/pull/27207#pullrequestreview-398792547",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "471874a7-1d2d-45b3-b6f9-ab4f6c2333f2",
        "parentId" : null,
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "If returning true, I think it means no delay schedule rejects, not had delay schedule rejects.",
        "createdAt" : "2020-04-23T04:49:49Z",
        "updatedAt" : "2020-04-23T04:49:49Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "d4b2d6de-b8b2-438c-8362-91357502403a",
        "parentId" : "471874a7-1d2d-45b3-b6f9-ab4f6c2333f2",
        "authorId" : "29a3e834-d9e3-42dc-ab25-20ba332ef3f2",
        "body" : "thanks for catching. The code changed mid PR and missed updating the doc.",
        "createdAt" : "2020-04-23T06:28:03Z",
        "updatedAt" : "2020-04-23T06:28:03Z",
        "lastEditedBy" : "29a3e834-d9e3-42dc-ab25-20ba332ef3f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "24c8ad9ae23d360c7bb5bc813d04e6f5f6715d93",
    "line" : 78,
    "diffHunk" : "@@ -1,1 +356,360 @@   * @param tasks tasks scheduled per offer, value at index 'i' corresponds to shuffledOffers[i]\n   * @param addressesWithDescs tasks scheduler per host:port, used for barrier tasks\n   * @return tuple of (had delay schedule rejects?, option of min locality of launched task)\n   */\n  private def resourceOfferSingleTaskSet("
  },
  {
    "id" : "db6d2e12-9dd5-4a27-b695-eff9ebba338e",
    "prId" : 27207,
    "prUrl" : "https://github.com/apache/spark/pull/27207#pullrequestreview-405065976",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2cdb9cb1-06b0-4c22-ae5b-e5e1aaf88b3b",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "Can we update the description of this function and explain the parameter \"isAllFreeResources\"?\r\n\r\n",
        "createdAt" : "2020-04-26T06:02:55Z",
        "updatedAt" : "2020-04-26T06:02:56Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "093bdaae-00d4-4315-a123-28c718219ed6",
        "parentId" : "2cdb9cb1-06b0-4c22-ae5b-e5e1aaf88b3b",
        "authorId" : "29a3e834-d9e3-42dc-ab25-20ba332ef3f2",
        "body" : "yes I can add something like: if true, then the parameter offers contains all workers and their free resources. See delay scheduling comments in class description.",
        "createdAt" : "2020-05-02T06:36:29Z",
        "updatedAt" : "2020-05-02T06:36:29Z",
        "lastEditedBy" : "29a3e834-d9e3-42dc-ab25-20ba332ef3f2",
        "tags" : [
        ]
      },
      {
        "id" : "4dfa25a9-b250-4bca-b9a9-d6813d31a93e",
        "parentId" : "2cdb9cb1-06b0-4c22-ae5b-e5e1aaf88b3b",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "is there are changes suggested and required, please file a separate new jira for this and link them.  this pr has been merged and we have had to many followups at this point.",
        "createdAt" : "2020-05-04T14:54:47Z",
        "updatedAt" : "2020-05-04T14:54:47Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "24c8ad9ae23d360c7bb5bc813d04e6f5f6715d93",
    "line" : 156,
    "diffHunk" : "@@ -1,1 +521,525 @@   * Called by cluster manager to offer resources on slaves. We respond by asking our active task\n   * sets for tasks in order of priority. We fill each node with tasks in a round-robin manner so\n   * that tasks are balanced across the cluster.\n   */\n  def resourceOffers("
  },
  {
    "id" : "f4ae509a-ffbd-4275-a5a6-5b0877807394",
    "prId" : 27207,
    "prUrl" : "https://github.com/apache/spark/pull/27207#pullrequestreview-404505237",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "be73ce13-e191-4e93-9fd5-a7175454fb2f",
        "parentId" : null,
        "authorId" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "body" : "Do we need to set the default value?",
        "createdAt" : "2020-04-26T06:03:10Z",
        "updatedAt" : "2020-04-26T06:03:10Z",
        "lastEditedBy" : "d19fc546-1296-40ce-befb-9eca847aeceb",
        "tags" : [
        ]
      },
      {
        "id" : "c1970fab-3cee-4cac-a92c-7eb7bf50bfc0",
        "parentId" : "be73ce13-e191-4e93-9fd5-a7175454fb2f",
        "authorId" : "29a3e834-d9e3-42dc-ab25-20ba332ef3f2",
        "body" : "I think I originally did this to not break the api + maintain something closer to previous behavior for callers who hadn't migrated to setting it to false.\r\nLemme know if this is the wrong approach.",
        "createdAt" : "2020-05-02T06:39:24Z",
        "updatedAt" : "2020-05-02T06:39:25Z",
        "lastEditedBy" : "29a3e834-d9e3-42dc-ab25-20ba332ef3f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "24c8ad9ae23d360c7bb5bc813d04e6f5f6715d93",
    "line" : 161,
    "diffHunk" : "@@ -1,1 +525,529 @@  def resourceOffers(\n      offers: IndexedSeq[WorkerOffer],\n      isAllFreeResources: Boolean = true): Seq[Seq[TaskDescription]] = synchronized {\n    // Mark each slave as alive and remember its hostname\n    // Also track if new executor is added"
  },
  {
    "id" : "1b66bc76-0385-41f9-8057-70060ee949a2",
    "prId" : 26696,
    "prUrl" : "https://github.com/apache/spark/pull/26696#pullrequestreview-329724204",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0979ed2d-2b33-4508-b615-de1c500e60e1",
        "parentId" : null,
        "authorId" : "e913f9af-103d-40d4-976e-9aa8e12e7211",
        "body" : "I think that rather than calling `rootPool.updateAvailableSlots()` on every `resourceOffer`, it would be more efficient to just call it when the total number has changed -- here and in the `resourceOffer` branch where its updated.  Might not matter much, hopefully `updateAvailableSlots()` isn't too expensive, but still I think that would be better.",
        "createdAt" : "2019-12-09T16:26:52Z",
        "updatedAt" : "2019-12-19T05:50:56Z",
        "lastEditedBy" : "e913f9af-103d-40d4-976e-9aa8e12e7211",
        "tags" : [
        ]
      },
      {
        "id" : "1f15852f-5e01-4f5d-a6ca-ed4c2ae5c942",
        "parentId" : "0979ed2d-2b33-4508-b615-de1c500e60e1",
        "authorId" : "29a3e834-d9e3-42dc-ab25-20ba332ef3f2",
        "body" : "updateAvailableSlots is dependent on both the total number of slots + what tasksets/pools are present. Will take a look to see how feasible doing the update incrementally is.",
        "createdAt" : "2019-12-10T10:17:43Z",
        "updatedAt" : "2019-12-19T05:50:56Z",
        "lastEditedBy" : "29a3e834-d9e3-42dc-ab25-20ba332ef3f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "f9a85f9475d5851f5bdd79e18141950330421445",
    "line" : 56,
    "diffHunk" : "@@ -1,1 +817,821 @@      taskIds.foreach(cleanupTaskState)\n    }\n    executorIdToCores.remove(executorId).foreach(totalSlots -= _ / CPUS_PER_TASK)\n\n    val host = executorIdToHost(executorId)"
  },
  {
    "id" : "2b276a5a-023e-4d57-8c39-c7f681346cec",
    "prId" : 26696,
    "prUrl" : "https://github.com/apache/spark/pull/26696#pullrequestreview-333051758",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "103aa9ba-a4d8-44f9-ad65-d6b5f449b404",
        "parentId" : null,
        "authorId" : "29a3e834-d9e3-42dc-ab25-20ba332ef3f2",
        "body" : "can anyone confirm whether it is true that the first resource offer for an executor will include all cores?\r\neven if this is true it feels odd to rely on it.",
        "createdAt" : "2019-12-17T06:16:26Z",
        "updatedAt" : "2019-12-19T05:50:56Z",
        "lastEditedBy" : "29a3e834-d9e3-42dc-ab25-20ba332ef3f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "f9a85f9475d5851f5bdd79e18141950330421445",
    "line" : 37,
    "diffHunk" : "@@ -1,1 +407,411 @@        hostToExecutors(o.host) += o.executorId\n        executorAdded(o.executorId, o.host)\n        // Assumes the first offer will include all cores (free cores == all cores)\n        executorIdToCores(o.executorId) = o.cores\n        totalSlots += o.cores / CPUS_PER_TASK"
  },
  {
    "id" : "d3f5ba7f-7dd7-41b5-9f3e-bca06c2e8095",
    "prId" : 25946,
    "prUrl" : "https://github.com/apache/spark/pull/25946#pullrequestreview-293951485",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "422aae12-5ec9-43b6-82b6-987d2d4d4eee",
        "parentId" : null,
        "authorId" : "e5beb795-ac0e-4bd6-ad66-708215b8ae58",
        "body" : "note: in https://github.com/apache/spark/pull/25946/files#diff-d4000438827afe3a185ae75b24987a61R455 the `else` could become an `else if (availableSlots > 0)` to avoid iterating over the remaining task sets when all available slots have been taken already, but that's a micro-optimization orthogonal to the fix.",
        "createdAt" : "2019-09-26T19:21:09Z",
        "updatedAt" : "2019-09-27T10:34:05Z",
        "lastEditedBy" : "e5beb795-ac0e-4bd6-ad66-708215b8ae58",
        "tags" : [
        ]
      },
      {
        "id" : "ddedb19a-81e3-4a5e-95a1-c760bc85afbb",
        "parentId" : "422aae12-5ec9-43b6-82b6-987d2d4d4eee",
        "authorId" : "e5beb795-ac0e-4bd6-ad66-708215b8ae58",
        "body" : "to avoid reiterating over the availableCpus array, we could also have one var that gets passed and updated inside resourceOfferSingleTaskSet, but that would likely be an overoptimization of this O(#num_offers) sum.",
        "createdAt" : "2019-09-26T19:26:18Z",
        "updatedAt" : "2019-09-27T10:34:05Z",
        "lastEditedBy" : "e5beb795-ac0e-4bd6-ad66-708215b8ae58",
        "tags" : [
        ]
      }
    ],
    "commit" : "f5b4f4805e494d95311d7c46d6635bec6cfeef8b",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +444,448 @@    // NOTE: the preferredLocality order: PROCESS_LOCAL, NODE_LOCAL, NO_PREF, RACK_LOCAL, ANY\n    for (taskSet <- sortedTaskSets) {\n      val availableSlots = availableCpus.map(c => c / CPUS_PER_TASK).sum\n      // Skip the barrier taskSet if the available slots are less than the number of pending tasks.\n      if (taskSet.isBarrier && availableSlots < taskSet.numTasks) {"
  },
  {
    "id" : "3edbe0e1-3786-4db7-9260-0adb635362d8",
    "prId" : 24374,
    "prUrl" : "https://github.com/apache/spark/pull/24374#pullrequestreview-244350034",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0138bc42-366f-47eb-8b2a-bf7a2278a96a",
        "parentId" : null,
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "I think its a bit out of place that we are decrementing the addresses  available in the resourceOffer function since everything else does its bookkeeping right here.  I don't think that is a blocker for this though and the way ",
        "createdAt" : "2019-05-23T21:52:53Z",
        "updatedAt" : "2019-06-04T21:38:24Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "e41a56e5-1880-4f64-82bf-e40271647b9d",
        "parentId" : "0138bc42-366f-47eb-8b2a-bf7a2278a96a",
        "authorId" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "body" : "I just feel if we only bookkeep by count here and actually allocate the addresses in SchedulerBackend then we need to pay slightly more attention to ensure these two places are synchronized. After all I'm neutral here and would be happy to go another way if someone feels strongly about this.",
        "createdAt" : "2019-05-23T23:48:02Z",
        "updatedAt" : "2019-06-04T21:38:24Z",
        "lastEditedBy" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "tags" : [
        ]
      },
      {
        "id" : "94606c9d-668e-4e6a-8104-58bac7c8bd2a",
        "parentId" : "0138bc42-366f-47eb-8b2a-bf7a2278a96a",
        "authorId" : "0c812942-02cb-4975-9748-394d1387affa",
        "body" : "I like @tgravescs 's proposal or at least putting an inline comment after L352 saying where we update the addresses.",
        "createdAt" : "2019-05-24T06:16:35Z",
        "updatedAt" : "2019-06-04T21:38:24Z",
        "lastEditedBy" : "0c812942-02cb-4975-9748-394d1387affa",
        "tags" : [
        ]
      },
      {
        "id" : "c4d59549-9c95-4af4-9f85-ebfd3d8daec6",
        "parentId" : "0138bc42-366f-47eb-8b2a-bf7a2278a96a",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "I'd be ok with a comment for now.  I wasn't suggesting only book keeping by count, I meant do the actual acquire addresses here, but you would have to make other changes for that to happen.  \r\nOne thing we could do is make a separate function in TaskSchedulerImpl for the resource (including cpu's) accounting and then have TaskSetManager.resourceOffer call into that to acquire the resources.\r\nAnother option is to have the TaskSetManager get some addresses but only do the actual accounting to remove them from the available here.",
        "createdAt" : "2019-05-24T14:21:36Z",
        "updatedAt" : "2019-06-04T21:38:24Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "5a0bd919-eed3-4e44-aaca-b9ce8cd1ea31",
        "parentId" : "0138bc42-366f-47eb-8b2a-bf7a2278a96a",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "Add the comment here about accounting of resources done inside TaskSetManager.resourceOffer",
        "createdAt" : "2019-05-31T16:03:17Z",
        "updatedAt" : "2019-06-04T21:38:24Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "82cd1e35bdc7caaddb4122060d8fd6b98893cbb6",
    "line" : 39,
    "diffHunk" : "@@ -1,1 +346,350 @@            tasks(i) += task\n            val tid = task.taskId\n            taskIdToTaskSetManager.put(tid, taskSet)\n            taskIdToExecutorId(tid) = execId\n            executorIdToRunningTaskIds(execId).add(tid)"
  }
]