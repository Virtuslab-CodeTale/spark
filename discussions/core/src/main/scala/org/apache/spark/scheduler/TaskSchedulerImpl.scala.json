[
  {
    "id" : "1a9af7b7-0fd6-4683-964a-9a7435d995e1",
    "prId" : 33529,
    "prUrl" : "https://github.com/apache/spark/pull/33529#pullrequestreview-717295595",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "427a3739-7ed0-4b08-880e-0183bb28a08b",
        "parentId" : null,
        "authorId" : "b3ba992a-312c-46eb-b3c3-8d861d15ac40",
        "body" : "The error message seems changed here.",
        "createdAt" : "2021-07-28T18:26:59Z",
        "updatedAt" : "2021-07-28T18:28:24Z",
        "lastEditedBy" : "b3ba992a-312c-46eb-b3c3-8d861d15ac40",
        "tags" : [
        ]
      }
    ],
    "commit" : "370e975818f827eddf522243a56d64d2205f08c6",
    "line" : 41,
    "diffHunk" : "@@ -1,1 +920,924 @@        // must mean the error is during registration.\n        // It might be good to do something smarter here in the future.\n        throw SparkCoreErrors.clusterSchedulerError(message)\n      }\n    }"
  },
  {
    "id" : "48f0d729-ba23-4a8e-9a1d-2e5b7722ebd6",
    "prId" : 29788,
    "prUrl" : "https://github.com/apache/spark/pull/29788#pullrequestreview-491637014",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "544efe40-a3d4-4a94-8ee2-b2cfa01e55ad",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "instead of 'shutdown', should we say 'is finally lost' ? To be more accurate in this setting.\r\n\r\n+1 on this change to avoid log spam.",
        "createdAt" : "2020-09-18T17:25:06Z",
        "updatedAt" : "2020-09-18T17:38:05Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      }
    ],
    "commit" : "9bebdd4f2846f12f8ab13279c8cece151e8edfd0",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +973,977 @@    case ExecutorDecommission(reason, _) =>\n      // use logInfo instead of logError as the loss of decommissioned executor is what we expect\n      logInfo(s\"Decommissioned executor $executorId on $hostPort shutdown: $reason\")\n    case _ =>\n      logError(s\"Lost executor $executorId on $hostPort: $reason\")"
  },
  {
    "id" : "adb7dc69-0088-4eb7-a1a7-adf54af1a388",
    "prId" : 29468,
    "prUrl" : "https://github.com/apache/spark/pull/29468#pullrequestreview-470161779",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b74e793e-5aa8-4cad-80a4-6589d0646706",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "I haven't looked at this code too deeply, but does it make sense it eagerly remove the decommissioned executor from hostToExecutors and recomputeLocality and thus rebuild hostsByRack etc ? \r\n\r\n\r\n",
        "createdAt" : "2020-08-18T17:33:37Z",
        "updatedAt" : "2020-08-20T06:22:17Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "e89b694b-ede2-4078-870b-b63437f30b47",
        "parentId" : "b74e793e-5aa8-4cad-80a4-6589d0646706",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "`hostToExecutors` will also be used to handle executor removal and blacklisting stuff. In other words, a decommissioned executor could also be removed or blacklisted at the same time. So, remove the executor from `hostToExecutors` might also affect other places. I don't want to mix them together to make things complex.\r\n\r\nBesides, similar to `executorIdToRunningTaskIds`, a decommissioned executor could also have running tasks on it. So we don't eagerly remove it from `executorIdToRunningTaskIds` too.",
        "createdAt" : "2020-08-19T03:36:09Z",
        "updatedAt" : "2020-08-20T06:22:17Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "cb921947-4b86-46cb-b452-516b5e892344",
        "parentId" : "b74e793e-5aa8-4cad-80a4-6589d0646706",
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "Got it. Thanks for considering it.",
        "createdAt" : "2020-08-19T06:35:05Z",
        "updatedAt" : "2020-08-20T06:22:17Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      }
    ],
    "commit" : "65f6e0525b2d1472c1e6956587253e5ae6263676",
    "line" : 16,
    "diffHunk" : "@@ -1,1 +1072,1076 @@\n  def hasHostAliveOnRack(rack: String): Boolean = synchronized {\n    hostsByRack.get(rack)\n      .exists(hosts => hosts.exists(h => !isHostDecommissioned(h)))\n  }"
  },
  {
    "id" : "df9cf048-0b84-4508-88f8-c20b6f12ebc6",
    "prId" : 29468,
    "prUrl" : "https://github.com/apache/spark/pull/29468#pullrequestreview-470179946",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "688f9374-00bc-403c-a4e2-1e1c0c5469a0",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "I don't think this method needs to be changed: It only seems to be used when the HDFS split is cached in memory by Hadoop. Perhaps we should restrict to changing only the methods in the `computeValidLocalityLevels` call path ?",
        "createdAt" : "2020-08-18T18:13:34Z",
        "updatedAt" : "2020-08-20T06:22:17Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "831da774-dda0-4ddf-a6ea-ccb9aeef4873",
        "parentId" : "688f9374-00bc-403c-a4e2-1e1c0c5469a0",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "hmm..I think this more like a problem of definition or something like that. In this PR, what I'm trying to do is to strengthen the definition of the active status for the executor or host. That is, for an active executor or host, it should not be decommissioned at the same time. This function is getting the active host, so we should filter out decommissioned ones. I think this's quite obvious. And this function is used by multiple tests.",
        "createdAt" : "2020-08-19T03:46:17Z",
        "updatedAt" : "2020-08-20T06:22:17Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "1e78b6b1-b561-4d7d-a0c1-d764a9410280",
        "parentId" : "688f9374-00bc-403c-a4e2-1e1c0c5469a0",
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "I am just a bit wary of regressions and I am not too sure about the unit test coverage here. So generally like the code changes to be minimal. But I am okay with this change if you think it is safe. ",
        "createdAt" : "2020-08-19T06:36:59Z",
        "updatedAt" : "2020-08-20T06:22:17Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "3506c6f5-61c1-410e-83cd-c228991bd409",
        "parentId" : "688f9374-00bc-403c-a4e2-1e1c0c5469a0",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "It's super weird if `getExecutorsAliveOnHost` and `hasExecutorsAliveOnHost` are not consistent.",
        "createdAt" : "2020-08-19T07:08:39Z",
        "updatedAt" : "2020-08-20T06:22:17Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "65f6e0525b2d1472c1e6956587253e5ae6263676",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1063,1067 @@\n  def getExecutorsAliveOnHost(host: String): Option[Set[String]] = synchronized {\n    hostToExecutors.get(host).map(_.filterNot(isExecutorDecommissioned)).map(_.toSet)\n  }\n"
  },
  {
    "id" : "b89b8f1b-6872-42bd-9b81-86962ec66f13",
    "prId" : 29468,
    "prUrl" : "https://github.com/apache/spark/pull/29468#pullrequestreview-470647653",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "44856237-8add-4219-bc0d-39f26c4e8106",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "nit: Should these two methods be declared final : We don't really want to override them and instead want to override the helper methods it calls.",
        "createdAt" : "2020-08-19T16:41:51Z",
        "updatedAt" : "2020-08-20T06:22:17Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      }
    ],
    "commit" : "65f6e0525b2d1472c1e6956587253e5ae6263676",
    "line" : 29,
    "diffHunk" : "@@ -1,1 +1084,1088 @@  }\n\n  // exposed for test\n  protected final def isExecutorDecommissioned(execId: String): Boolean =\n    getExecutorDecommissionInfo(execId).nonEmpty"
  },
  {
    "id" : "2314065b-6721-486d-8ef9-c7357f006a31",
    "prId" : 29395,
    "prUrl" : "https://github.com/apache/spark/pull/29395#pullrequestreview-464262549",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "51402bd9-fddb-45c5-a002-0d409f1454cb",
        "parentId" : null,
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "it would be nice to add a comment (like master branch) that this is only used with barrier scheduling, availableSlots not used otherwise.",
        "createdAt" : "2020-08-10T14:09:32Z",
        "updatedAt" : "2020-08-18T01:49:27Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "daa205dc9f257f42b794767601fd844dd596e374",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +449,453 @@      // we only need to calculate available slots if using barrier scheduling, otherwise the\n      // value is -1\n      val availableSlots = if (taskSet.isBarrier) {\n        val availableResourcesAmount = availableResources.map { resourceMap =>\n          // note that the addresses here have been expanded according to the numParts"
  },
  {
    "id" : "aa0e77d7-6e74-46e3-b207-68c4335c68b4",
    "prId" : 29014,
    "prUrl" : "https://github.com/apache/spark/pull/29014#pullrequestreview-452918174",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1675bb71-9ba8-47e6-a87d-2b6c631dda4e",
        "parentId" : null,
        "authorId" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "body" : "~~Is there a reason to use the lock on the whole `TaskSchedulerImpl` instance? If there is none then we can change the `HashMap` to a `ConcurrentHashMap` and here we can use a `putIfAbsent`.~~\r\n",
        "createdAt" : "2020-07-20T13:50:45Z",
        "updatedAt" : "2020-07-29T01:25:46Z",
        "lastEditedBy" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "tags" : [
        ]
      },
      {
        "id" : "dea65fbb-9f3f-4721-8d27-4d0660bccea7",
        "parentId" : "1675bb71-9ba8-47e6-a87d-2b6c631dda4e",
        "authorId" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "body" : "Is there any guarantee that a `ExecutorDecommissionInfo` with `isHostDecommissioned=false` is never followed by another `ExecutorDecommissionInfo` where `isHostDecommissioned=true`?  ",
        "createdAt" : "2020-07-20T15:48:07Z",
        "updatedAt" : "2020-07-29T01:25:46Z",
        "lastEditedBy" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "tags" : [
        ]
      },
      {
        "id" : "fea02f2a-a354-43cf-b66c-1418fd76c9b3",
        "parentId" : "1675bb71-9ba8-47e6-a87d-2b6c631dda4e",
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "That would be a bug: isHostDecommissioned is ultimately decided by the cluster manager. It knows for sure whether or not the host is decommissioned with the executor. It shouldn't change its mind :-). I can add an assert in the Driver but it would be a bit of an overkill. ",
        "createdAt" : "2020-07-21T01:08:30Z",
        "updatedAt" : "2020-07-29T01:25:46Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "d1e7f03e-ba0b-45f9-963a-b92b69700067",
        "parentId" : "1675bb71-9ba8-47e6-a87d-2b6c631dda4e",
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "Changed this to \"last writer wins\"",
        "createdAt" : "2020-07-21T02:04:48Z",
        "updatedAt" : "2020-07-29T01:25:46Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "5238bef5-fff2-49ac-b526-03e1e2796bc9",
        "parentId" : "1675bb71-9ba8-47e6-a87d-2b6c631dda4e",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "So my concern here is that if a task is scheduled part of the way through decommissioning the executor *could* send back a decommission message with the host lost. Maybe use a getOrElse to make sure if we've ever learnt the entire host is going away we don't forget that?",
        "createdAt" : "2020-07-21T20:47:54Z",
        "updatedAt" : "2020-07-29T01:25:46Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "a16f06b6-f0cc-4c46-81ca-97b5dc3bf72b",
        "parentId" : "1675bb71-9ba8-47e6-a87d-2b6c631dda4e",
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "Ah, I see your point -- its because of the DecommissionExecutor messages that the executor sends to the driver when work is assigned to it. And there, it can reply back with a hostDecommissioned = false, because the executor itself does not know if the host is going away or not (only the cluster manager knows). \r\n\r\nI like your suggestion of doing this \"merging\" semantics, it is easier/less-work than plumbing host Lost to the executor :-) ",
        "createdAt" : "2020-07-22T00:33:19Z",
        "updatedAt" : "2020-07-29T01:25:46Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      }
    ],
    "commit" : "dbc478b0d96de0f1a48f93d90f1e648229b5790c",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +942,946 @@  override def executorDecommission(\n      executorId: String, decommissionInfo: ExecutorDecommissionInfo): Unit = {\n    synchronized {\n      // Don't bother noting decommissioning for executors that we don't know about\n      if (executorIdToHost.contains(executorId)) {"
  },
  {
    "id" : "d51419fe-2660-4b71-8774-ec02db1afef3",
    "prId" : 29014,
    "prUrl" : "https://github.com/apache/spark/pull/29014#pullrequestreview-451741240",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c698e332-c004-4a78-9a86-8824895b5323",
        "parentId" : null,
        "authorId" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "body" : "~~There is no synchronized here?~~",
        "createdAt" : "2020-07-20T15:50:51Z",
        "updatedAt" : "2020-07-29T01:25:46Z",
        "lastEditedBy" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "tags" : [
        ]
      },
      {
        "id" : "8f8579f4-349f-48d6-a58f-922414cc5efc",
        "parentId" : "c698e332-c004-4a78-9a86-8824895b5323",
        "authorId" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "body" : "Oh I see, the synchronized is at the caller place in: `executorLost`. This way it is you can leave the HashMap as it is.",
        "createdAt" : "2020-07-20T15:54:54Z",
        "updatedAt" : "2020-07-29T01:25:46Z",
        "lastEditedBy" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "tags" : [
        ]
      }
    ],
    "commit" : "dbc478b0d96de0f1a48f93d90f1e648229b5790c",
    "line" : 58,
    "diffHunk" : "@@ -1,1 +1067,1071 @@    }\n\n    executorsPendingDecommission -= executorId\n\n    if (reason != LossReasonPending) {"
  }
]