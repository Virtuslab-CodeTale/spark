[
  {
    "id" : "5df4914a-18f9-4168-a280-d035caa122fc",
    "prId" : 33759,
    "prUrl" : "https://github.com/apache/spark/pull/33759#pullrequestreview-731662363",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b59b5b87-b103-4edf-9959-924c77d8ea54",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "can we add some comments to explain when can this happen?",
        "createdAt" : "2021-08-17T09:35:54Z",
        "updatedAt" : "2021-08-17T09:35:55Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "e5a12bbf-c730-43a2-a0ed-fffc5960af15",
        "parentId" : "b59b5b87-b103-4edf-9959-924c77d8ea54",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "added",
        "createdAt" : "2021-08-17T11:48:53Z",
        "updatedAt" : "2021-08-17T11:48:54Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "57dc0ca8672f8e981f770752e89f705d8d567e6a",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +205,209 @@          // `executor` can be null if there's any error in `CoarseGrainedExecutorBackend.onStart`\n          // or fail to create `Executor`.\n          if (executor == null) {\n            System.exit(1)\n          } else {"
  },
  {
    "id" : "07475686-945b-4a22-84d2-65e0551dedbb",
    "prId" : 33028,
    "prUrl" : "https://github.com/apache/spark/pull/33028#pullrequestreview-695642220",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8600e946-dafe-43bf-80bb-6519a766b70d",
        "parentId" : null,
        "authorId" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "body" : "If stopping is true, we should add a log saying we entered the `exitExecutor` function, but skipped actual actions because we already tried to exit before.",
        "createdAt" : "2021-06-23T19:17:44Z",
        "updatedAt" : "2021-06-23T19:17:45Z",
        "lastEditedBy" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "tags" : [
        ]
      },
      {
        "id" : "4a902bdd-a563-4677-b73f-00957752e86f",
        "parentId" : "8600e946-dafe-43bf-80bb-6519a766b70d",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Added.",
        "createdAt" : "2021-06-30T02:23:43Z",
        "updatedAt" : "2021-06-30T02:23:43Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "2af82e5d4b83781194c9f0ebd51ada3be04f9279",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +272,276 @@                             throwable: Throwable = null,\n                             notifyDriver: Boolean = true) = {\n    if (stopping.compareAndSet(false, true)) {\n      val message = \"Executor self-exiting due to : \" + reason\n      if (throwable != null) {"
  },
  {
    "id" : "972f71bc-cdfe-424b-bf0a-d27f654b85d1",
    "prId" : 32287,
    "prUrl" : "https://github.com/apache/spark/pull/32287#pullrequestreview-654118765",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9945c97e-199f-4e98-9fd1-6a6e0f2b34f6",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "I tried to write a unit test for this change but realized it's not easy while I was trying. So I gave it up. But I have done the manual test and looks fine. cc @mridulm ",
        "createdAt" : "2021-05-07T06:18:04Z",
        "updatedAt" : "2021-05-10T10:25:15Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "4af6ee73221a55e3cd7239f151ec33d5e802cbe4",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +97,101 @@          s\"'${MAX_REMOTE_BLOCK_SIZE_FETCH_TO_MEM.key}', but got \" +\n          s\"${PlatformDependent.maxDirectMemory()} bytes < \" +\n          s\"${env.conf.get(MAX_REMOTE_BLOCK_SIZE_FETCH_TO_MEM)}\")\n      }\n"
  },
  {
    "id" : "13967d02-36e2-4e75-b36b-4c85f9293fe4",
    "prId" : 31102,
    "prUrl" : "https://github.com/apache/spark/pull/31102#pullrequestreview-576116782",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bd9b6bb0-1f20-4f9d-b8f5-c6e1129e0186",
        "parentId" : null,
        "authorId" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "body" : "Based on the above lines does `STORAGE_DECOMMISSION_ENABLED` has any role now? \r\nSo what about removing this config?",
        "createdAt" : "2021-01-15T14:19:43Z",
        "updatedAt" : "2021-03-23T02:08:10Z",
        "lastEditedBy" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "tags" : [
        ]
      },
      {
        "id" : "f51ed1f7-e406-470c-8618-c5a47ed5dd98",
        "parentId" : "bd9b6bb0-1f20-4f9d-b8f5-c6e1129e0186",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Thatâ€˜s my concern too. But removal involves the API change. I intentionally try to avoid that in this refactor PR. But I still think it's worth a try in a separate PR, especially since the 3.1 hasn't released yet. cc @holdenk what's your opinion?",
        "createdAt" : "2021-01-20T09:38:59Z",
        "updatedAt" : "2021-03-23T02:08:10Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "251c52cf-4c48-4b98-8a40-7514a94e4c0c",
        "parentId" : "bd9b6bb0-1f20-4f9d-b8f5-c6e1129e0186",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "kindly ping @holdenk ",
        "createdAt" : "2021-01-26T08:29:20Z",
        "updatedAt" : "2021-03-23T02:08:10Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "88a205ec8af63f744fdbee8b4e61cb718d3ec207",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +299,303 @@      val migrationEnabled = env.conf.get(STORAGE_DECOMMISSION_ENABLED) &&\n        (env.conf.get(STORAGE_DECOMMISSION_RDD_BLOCKS_ENABLED) ||\n          env.conf.get(STORAGE_DECOMMISSION_SHUFFLE_BLOCKS_ENABLED))\n      if (migrationEnabled) {\n        env.blockManager.decommissionBlockManager()"
  },
  {
    "id" : "b6fb681b-899d-40b4-a7df-b50a44264936",
    "prId" : 31102,
    "prUrl" : "https://github.com/apache/spark/pull/31102#pullrequestreview-569300026",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "59bc5f3c-1a9e-45d4-babb-1f109955b73e",
        "parentId" : null,
        "authorId" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "body" : "By removing the `STORAGE_DECOMMISSION_ENABLED` config this part would simply disappear. ",
        "createdAt" : "2021-01-15T14:21:58Z",
        "updatedAt" : "2021-03-23T02:08:10Z",
        "lastEditedBy" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "tags" : [
        ]
      }
    ],
    "commit" : "88a205ec8af63f744fdbee8b4e61cb718d3ec207",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +302,306 @@      if (migrationEnabled) {\n        env.blockManager.decommissionBlockManager()\n      } else if (env.conf.get(STORAGE_DECOMMISSION_ENABLED)) {\n        logError(s\"Storage decommissioning attempted but neither \" +\n          s\"${STORAGE_DECOMMISSION_SHUFFLE_BLOCKS_ENABLED.key} or \" +"
  },
  {
    "id" : "83ceb6ec-196d-4061-8bbc-cdd655300336",
    "prId" : 29817,
    "prUrl" : "https://github.com/apache/spark/pull/29817#pullrequestreview-514334442",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d6c5da61-92d7-4cec-bf9c-cb3eb29d1e4f",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "This part is new. I think it makes sense to skip executor decommissioning if the driver side rejects to do so.",
        "createdAt" : "2020-10-22T03:28:25Z",
        "updatedAt" : "2020-10-22T13:56:45Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "3c1e033089e392066425139d7ccb52cd501dcc31",
    "line" : 63,
    "diffHunk" : "@@ -1,1 +216,220 @@          // Tell driver that we are starting decommissioning so it stops trying to schedule us\n          driverNotified = driverRef.askSync[Boolean](ExecutorDecommissioning(executorId))\n          if (driverNotified) decommissionSelf()\n        }\n      } catch {"
  },
  {
    "id" : "46ce0364-b07c-40bf-b943-f3de49dd3acd",
    "prId" : 29722,
    "prUrl" : "https://github.com/apache/spark/pull/29722#pullrequestreview-487440188",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fe5b20bc-1b81-470a-be24-99ce1f23a96f",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "I see, so is this the now ONLY place where block manager decommissioning is triggered ? Good.",
        "createdAt" : "2020-09-12T01:32:24Z",
        "updatedAt" : "2020-09-16T06:57:24Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "cbbbc6c2-209c-4d81-af2b-e66824b017e3",
        "parentId" : "fe5b20bc-1b81-470a-be24-99ce1f23a96f",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Yes.",
        "createdAt" : "2020-09-14T06:12:19Z",
        "updatedAt" : "2020-09-16T06:57:24Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "290d2c08a1adfce3c0c4afe1cb8c9e214b25ea3d",
    "line" : 93,
    "diffHunk" : "@@ -1,1 +279,283 @@    try {\n      decommissioned = true\n      if (env.conf.get(STORAGE_DECOMMISSION_ENABLED)) {\n        env.blockManager.decommissionBlockManager()\n      }"
  },
  {
    "id" : "e70e6612-6b01-4f9b-a262-61784d54acfe",
    "prId" : 29422,
    "prUrl" : "https://github.com/apache/spark/pull/29422#pullrequestreview-467905268",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1a20a1f4-6576-4aa7-83a2-7b2ccc0e22f5",
        "parentId" : null,
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Maybe just add a comment here that this is for testing only.",
        "createdAt" : "2020-08-14T17:41:37Z",
        "updatedAt" : "2020-08-17T23:52:14Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "986a539a-4860-4176-a86c-be9655ebc94b",
        "parentId" : "1a20a1f4-6576-4aa7-83a2-7b2ccc0e22f5",
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "sure",
        "createdAt" : "2020-08-14T22:26:25Z",
        "updatedAt" : "2020-08-17T23:52:14Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      }
    ],
    "commit" : "df128e507a2c7bd11d33197fca6b4fa10f4e9256",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +297,301 @@          // This config is internal and only used by unit tests to force an executor\n          // to hang around for longer when decommissioned.\n          val initialSleepMillis = env.conf.getInt(\n            \"spark.test.executor.decommission.initial.sleep.millis\", sleep_time)\n          if (initialSleepMillis > 0) {"
  },
  {
    "id" : "8125c1e3-06a9-498b-853c-5fb42d90f68f",
    "prId" : 29422,
    "prUrl" : "https://github.com/apache/spark/pull/29422#pullrequestreview-467905225",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a5ea5bea-10c3-4a4e-8714-95d8f2b1bbe5",
        "parentId" : null,
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "This was moved so initial sleep time didn't have sleep_time added to it on the first pass through right? Nothing else?",
        "createdAt" : "2020-08-14T17:42:45Z",
        "updatedAt" : "2020-08-17T23:52:14Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "a45b500e-8d44-4b02-938d-4c57f89de701",
        "parentId" : "a5ea5bea-10c3-4a4e-8714-95d8f2b1bbe5",
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "Yeah. No semantic change. We are still by default waiting for sleep_time the first time and the last time around (it is an infinite while loop that can only exit via an `exit(1)` -- via process death). I just wanted the first sleep interval to be configurable for testing. But no production change to the shutdown thread. ",
        "createdAt" : "2020-08-14T22:26:15Z",
        "updatedAt" : "2020-08-17T23:52:14Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      }
    ],
    "commit" : "df128e507a2c7bd11d33197fca6b4fa10f4e9256",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +329,333 @@              lastTaskRunningTime = System.nanoTime()\n            }\n            Thread.sleep(sleep_time)\n          }\n        }"
  },
  {
    "id" : "daa5ea99-66e2-485b-9bb9-c5e89431698c",
    "prId" : 29211,
    "prUrl" : "https://github.com/apache/spark/pull/29211#pullrequestreview-455109853",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a2e4e222-e468-401e-aab8-5165b37fb5ca",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "nit: In the spirit of minimizing code diff, is there is a strong reason why this decommissioned flag was moved down from line 67 ?",
        "createdAt" : "2020-07-24T17:49:54Z",
        "updatedAt" : "2020-08-05T18:39:14Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "a44ec88c-56b8-4953-a337-806781c2165c",
        "parentId" : "a2e4e222-e468-401e-aab8-5165b37fb5ca",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "I didn't consider it related to the variables it was grouped with strongly enough so I moved it to stand alone.",
        "createdAt" : "2020-07-24T18:45:29Z",
        "updatedAt" : "2020-08-05T18:39:14Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "e81c3fc2d0b8c28cd0aedf08a7ea1e86a153ae49",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +80,84 @@  private[executor] val taskResources = new mutable.HashMap[Long, Map[String, ResourceInformation]]\n\n  @volatile private var decommissioned = false\n\n  override def onStart(): Unit = {"
  },
  {
    "id" : "666d8488-f487-4617-8b87-eef151a8a70d",
    "prId" : 29211,
    "prUrl" : "https://github.com/apache/spark/pull/29211#pullrequestreview-462828347",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "af033981-81ba-4b22-9a9b-da280c8a8177",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "I'm wondering if we need to wait for tasks to finish if storage decommission is disabled. I mean, for a shuffle map task and result task with indirect result, their outputs still base on blocks. As a result, we'd spend time waiting for them to finish but get nothing good in return.",
        "createdAt" : "2020-08-06T09:06:34Z",
        "updatedAt" : "2020-08-06T09:39:47Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "371b7f5a-48f0-43a5-8c09-c073ec0ce8c3",
        "parentId" : "af033981-81ba-4b22-9a9b-da280c8a8177",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Unless it's an action. If someone just wants things to exit immediately they should not enable decommissioning.",
        "createdAt" : "2020-08-06T19:50:08Z",
        "updatedAt" : "2020-08-06T19:50:08Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "e81c3fc2d0b8c28cd0aedf08a7ea1e86a153ae49",
    "line" : 63,
    "diffHunk" : "@@ -1,1 +313,317 @@              } else {\n                logInfo(\"No running tasks, no block migration configured, stopping.\")\n                exitExecutor(0, \"Finished decommissioning\", notifyDriver = true)\n              }\n            } else {"
  },
  {
    "id" : "5770ce3a-35d2-48a5-9127-d7a386010522",
    "prId" : 29211,
    "prUrl" : "https://github.com/apache/spark/pull/29211#pullrequestreview-463830980",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "80e2c1ae-78cd-4aab-b987-6d1e2369560a",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "I think you missed a s\" here ... the string interpolation isn't happening",
        "createdAt" : "2020-08-08T22:41:24Z",
        "updatedAt" : "2020-08-08T22:41:24Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      }
    ],
    "commit" : "e81c3fc2d0b8c28cd0aedf08a7ea1e86a153ae49",
    "line" : 66,
    "diffHunk" : "@@ -1,1 +316,320 @@              }\n            } else {\n              logInfo(\"Blocked from shutdown by running ${executor.numRunningtasks} tasks\")\n              // If there is a running task it could store blocks, so make sure we wait for a\n              // migration loop to complete after the last task is done."
  }
]