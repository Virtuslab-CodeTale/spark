[
  {
    "id" : "5df4914a-18f9-4168-a280-d035caa122fc",
    "prId" : 33759,
    "prUrl" : "https://github.com/apache/spark/pull/33759#pullrequestreview-731662363",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b59b5b87-b103-4edf-9959-924c77d8ea54",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "can we add some comments to explain when can this happen?",
        "createdAt" : "2021-08-17T09:35:54Z",
        "updatedAt" : "2021-08-17T09:35:55Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "e5a12bbf-c730-43a2-a0ed-fffc5960af15",
        "parentId" : "b59b5b87-b103-4edf-9959-924c77d8ea54",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "added",
        "createdAt" : "2021-08-17T11:48:53Z",
        "updatedAt" : "2021-08-17T11:48:54Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "57dc0ca8672f8e981f770752e89f705d8d567e6a",
    "line" : 11,
    "diffHunk" : "@@ -1,1 +205,209 @@          // `executor` can be null if there's any error in `CoarseGrainedExecutorBackend.onStart`\n          // or fail to create `Executor`.\n          if (executor == null) {\n            System.exit(1)\n          } else {"
  },
  {
    "id" : "07475686-945b-4a22-84d2-65e0551dedbb",
    "prId" : 33028,
    "prUrl" : "https://github.com/apache/spark/pull/33028#pullrequestreview-695642220",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8600e946-dafe-43bf-80bb-6519a766b70d",
        "parentId" : null,
        "authorId" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "body" : "If stopping is true, we should add a log saying we entered the `exitExecutor` function, but skipped actual actions because we already tried to exit before.",
        "createdAt" : "2021-06-23T19:17:44Z",
        "updatedAt" : "2021-06-23T19:17:45Z",
        "lastEditedBy" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "tags" : [
        ]
      },
      {
        "id" : "4a902bdd-a563-4677-b73f-00957752e86f",
        "parentId" : "8600e946-dafe-43bf-80bb-6519a766b70d",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Added.",
        "createdAt" : "2021-06-30T02:23:43Z",
        "updatedAt" : "2021-06-30T02:23:43Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "2af82e5d4b83781194c9f0ebd51ada3be04f9279",
    "line" : 19,
    "diffHunk" : "@@ -1,1 +272,276 @@                             throwable: Throwable = null,\n                             notifyDriver: Boolean = true) = {\n    if (stopping.compareAndSet(false, true)) {\n      val message = \"Executor self-exiting due to : \" + reason\n      if (throwable != null) {"
  },
  {
    "id" : "972f71bc-cdfe-424b-bf0a-d27f654b85d1",
    "prId" : 32287,
    "prUrl" : "https://github.com/apache/spark/pull/32287#pullrequestreview-654118765",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9945c97e-199f-4e98-9fd1-6a6e0f2b34f6",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "I tried to write a unit test for this change but realized it's not easy while I was trying. So I gave it up. But I have done the manual test and looks fine. cc @mridulm ",
        "createdAt" : "2021-05-07T06:18:04Z",
        "updatedAt" : "2021-05-10T10:25:15Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "4af6ee73221a55e3cd7239f151ec33d5e802cbe4",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +97,101 @@          s\"'${MAX_REMOTE_BLOCK_SIZE_FETCH_TO_MEM.key}', but got \" +\n          s\"${PlatformDependent.maxDirectMemory()} bytes < \" +\n          s\"${env.conf.get(MAX_REMOTE_BLOCK_SIZE_FETCH_TO_MEM)}\")\n      }\n"
  },
  {
    "id" : "13967d02-36e2-4e75-b36b-4c85f9293fe4",
    "prId" : 31102,
    "prUrl" : "https://github.com/apache/spark/pull/31102#pullrequestreview-576116782",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bd9b6bb0-1f20-4f9d-b8f5-c6e1129e0186",
        "parentId" : null,
        "authorId" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "body" : "Based on the above lines does `STORAGE_DECOMMISSION_ENABLED` has any role now? \r\nSo what about removing this config?",
        "createdAt" : "2021-01-15T14:19:43Z",
        "updatedAt" : "2021-03-23T02:08:10Z",
        "lastEditedBy" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "tags" : [
        ]
      },
      {
        "id" : "f51ed1f7-e406-470c-8618-c5a47ed5dd98",
        "parentId" : "bd9b6bb0-1f20-4f9d-b8f5-c6e1129e0186",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Thatâ€˜s my concern too. But removal involves the API change. I intentionally try to avoid that in this refactor PR. But I still think it's worth a try in a separate PR, especially since the 3.1 hasn't released yet. cc @holdenk what's your opinion?",
        "createdAt" : "2021-01-20T09:38:59Z",
        "updatedAt" : "2021-03-23T02:08:10Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "251c52cf-4c48-4b98-8a40-7514a94e4c0c",
        "parentId" : "bd9b6bb0-1f20-4f9d-b8f5-c6e1129e0186",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "kindly ping @holdenk ",
        "createdAt" : "2021-01-26T08:29:20Z",
        "updatedAt" : "2021-03-23T02:08:10Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "88a205ec8af63f744fdbee8b4e61cb718d3ec207",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +299,303 @@      val migrationEnabled = env.conf.get(STORAGE_DECOMMISSION_ENABLED) &&\n        (env.conf.get(STORAGE_DECOMMISSION_RDD_BLOCKS_ENABLED) ||\n          env.conf.get(STORAGE_DECOMMISSION_SHUFFLE_BLOCKS_ENABLED))\n      if (migrationEnabled) {\n        env.blockManager.decommissionBlockManager()"
  },
  {
    "id" : "b6fb681b-899d-40b4-a7df-b50a44264936",
    "prId" : 31102,
    "prUrl" : "https://github.com/apache/spark/pull/31102#pullrequestreview-569300026",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "59bc5f3c-1a9e-45d4-babb-1f109955b73e",
        "parentId" : null,
        "authorId" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "body" : "By removing the `STORAGE_DECOMMISSION_ENABLED` config this part would simply disappear. ",
        "createdAt" : "2021-01-15T14:21:58Z",
        "updatedAt" : "2021-03-23T02:08:10Z",
        "lastEditedBy" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "tags" : [
        ]
      }
    ],
    "commit" : "88a205ec8af63f744fdbee8b4e61cb718d3ec207",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +302,306 @@      if (migrationEnabled) {\n        env.blockManager.decommissionBlockManager()\n      } else if (env.conf.get(STORAGE_DECOMMISSION_ENABLED)) {\n        logError(s\"Storage decommissioning attempted but neither \" +\n          s\"${STORAGE_DECOMMISSION_SHUFFLE_BLOCKS_ENABLED.key} or \" +"
  },
  {
    "id" : "83ceb6ec-196d-4061-8bbc-cdd655300336",
    "prId" : 29817,
    "prUrl" : "https://github.com/apache/spark/pull/29817#pullrequestreview-514334442",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d6c5da61-92d7-4cec-bf9c-cb3eb29d1e4f",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "This part is new. I think it makes sense to skip executor decommissioning if the driver side rejects to do so.",
        "createdAt" : "2020-10-22T03:28:25Z",
        "updatedAt" : "2020-10-22T13:56:45Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "3c1e033089e392066425139d7ccb52cd501dcc31",
    "line" : 63,
    "diffHunk" : "@@ -1,1 +216,220 @@          // Tell driver that we are starting decommissioning so it stops trying to schedule us\n          driverNotified = driverRef.askSync[Boolean](ExecutorDecommissioning(executorId))\n          if (driverNotified) decommissionSelf()\n        }\n      } catch {"
  },
  {
    "id" : "46ce0364-b07c-40bf-b943-f3de49dd3acd",
    "prId" : 29722,
    "prUrl" : "https://github.com/apache/spark/pull/29722#pullrequestreview-487440188",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "fe5b20bc-1b81-470a-be24-99ce1f23a96f",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "I see, so is this the now ONLY place where block manager decommissioning is triggered ? Good.",
        "createdAt" : "2020-09-12T01:32:24Z",
        "updatedAt" : "2020-09-16T06:57:24Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "cbbbc6c2-209c-4d81-af2b-e66824b017e3",
        "parentId" : "fe5b20bc-1b81-470a-be24-99ce1f23a96f",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Yes.",
        "createdAt" : "2020-09-14T06:12:19Z",
        "updatedAt" : "2020-09-16T06:57:24Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "290d2c08a1adfce3c0c4afe1cb8c9e214b25ea3d",
    "line" : 93,
    "diffHunk" : "@@ -1,1 +279,283 @@    try {\n      decommissioned = true\n      if (env.conf.get(STORAGE_DECOMMISSION_ENABLED)) {\n        env.blockManager.decommissionBlockManager()\n      }"
  },
  {
    "id" : "e70e6612-6b01-4f9b-a262-61784d54acfe",
    "prId" : 29422,
    "prUrl" : "https://github.com/apache/spark/pull/29422#pullrequestreview-467905268",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1a20a1f4-6576-4aa7-83a2-7b2ccc0e22f5",
        "parentId" : null,
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Maybe just add a comment here that this is for testing only.",
        "createdAt" : "2020-08-14T17:41:37Z",
        "updatedAt" : "2020-08-17T23:52:14Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "986a539a-4860-4176-a86c-be9655ebc94b",
        "parentId" : "1a20a1f4-6576-4aa7-83a2-7b2ccc0e22f5",
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "sure",
        "createdAt" : "2020-08-14T22:26:25Z",
        "updatedAt" : "2020-08-17T23:52:14Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      }
    ],
    "commit" : "df128e507a2c7bd11d33197fca6b4fa10f4e9256",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +297,301 @@          // This config is internal and only used by unit tests to force an executor\n          // to hang around for longer when decommissioned.\n          val initialSleepMillis = env.conf.getInt(\n            \"spark.test.executor.decommission.initial.sleep.millis\", sleep_time)\n          if (initialSleepMillis > 0) {"
  },
  {
    "id" : "8125c1e3-06a9-498b-853c-5fb42d90f68f",
    "prId" : 29422,
    "prUrl" : "https://github.com/apache/spark/pull/29422#pullrequestreview-467905225",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a5ea5bea-10c3-4a4e-8714-95d8f2b1bbe5",
        "parentId" : null,
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "This was moved so initial sleep time didn't have sleep_time added to it on the first pass through right? Nothing else?",
        "createdAt" : "2020-08-14T17:42:45Z",
        "updatedAt" : "2020-08-17T23:52:14Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "a45b500e-8d44-4b02-938d-4c57f89de701",
        "parentId" : "a5ea5bea-10c3-4a4e-8714-95d8f2b1bbe5",
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "Yeah. No semantic change. We are still by default waiting for sleep_time the first time and the last time around (it is an infinite while loop that can only exit via an `exit(1)` -- via process death). I just wanted the first sleep interval to be configurable for testing. But no production change to the shutdown thread. ",
        "createdAt" : "2020-08-14T22:26:15Z",
        "updatedAt" : "2020-08-17T23:52:14Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      }
    ],
    "commit" : "df128e507a2c7bd11d33197fca6b4fa10f4e9256",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +329,333 @@              lastTaskRunningTime = System.nanoTime()\n            }\n            Thread.sleep(sleep_time)\n          }\n        }"
  },
  {
    "id" : "daa5ea99-66e2-485b-9bb9-c5e89431698c",
    "prId" : 29211,
    "prUrl" : "https://github.com/apache/spark/pull/29211#pullrequestreview-455109853",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a2e4e222-e468-401e-aab8-5165b37fb5ca",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "nit: In the spirit of minimizing code diff, is there is a strong reason why this decommissioned flag was moved down from line 67 ?",
        "createdAt" : "2020-07-24T17:49:54Z",
        "updatedAt" : "2020-08-05T18:39:14Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "a44ec88c-56b8-4953-a337-806781c2165c",
        "parentId" : "a2e4e222-e468-401e-aab8-5165b37fb5ca",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "I didn't consider it related to the variables it was grouped with strongly enough so I moved it to stand alone.",
        "createdAt" : "2020-07-24T18:45:29Z",
        "updatedAt" : "2020-08-05T18:39:14Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "e81c3fc2d0b8c28cd0aedf08a7ea1e86a153ae49",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +80,84 @@  private[executor] val taskResources = new mutable.HashMap[Long, Map[String, ResourceInformation]]\n\n  @volatile private var decommissioned = false\n\n  override def onStart(): Unit = {"
  },
  {
    "id" : "666d8488-f487-4617-8b87-eef151a8a70d",
    "prId" : 29211,
    "prUrl" : "https://github.com/apache/spark/pull/29211#pullrequestreview-462828347",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "af033981-81ba-4b22-9a9b-da280c8a8177",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "I'm wondering if we need to wait for tasks to finish if storage decommission is disabled. I mean, for a shuffle map task and result task with indirect result, their outputs still base on blocks. As a result, we'd spend time waiting for them to finish but get nothing good in return.",
        "createdAt" : "2020-08-06T09:06:34Z",
        "updatedAt" : "2020-08-06T09:39:47Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "371b7f5a-48f0-43a5-8c09-c073ec0ce8c3",
        "parentId" : "af033981-81ba-4b22-9a9b-da280c8a8177",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Unless it's an action. If someone just wants things to exit immediately they should not enable decommissioning.",
        "createdAt" : "2020-08-06T19:50:08Z",
        "updatedAt" : "2020-08-06T19:50:08Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "e81c3fc2d0b8c28cd0aedf08a7ea1e86a153ae49",
    "line" : 63,
    "diffHunk" : "@@ -1,1 +313,317 @@              } else {\n                logInfo(\"No running tasks, no block migration configured, stopping.\")\n                exitExecutor(0, \"Finished decommissioning\", notifyDriver = true)\n              }\n            } else {"
  },
  {
    "id" : "5770ce3a-35d2-48a5-9127-d7a386010522",
    "prId" : 29211,
    "prUrl" : "https://github.com/apache/spark/pull/29211#pullrequestreview-463830980",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "80e2c1ae-78cd-4aab-b987-6d1e2369560a",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "I think you missed a s\" here ... the string interpolation isn't happening",
        "createdAt" : "2020-08-08T22:41:24Z",
        "updatedAt" : "2020-08-08T22:41:24Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      }
    ],
    "commit" : "e81c3fc2d0b8c28cd0aedf08a7ea1e86a153ae49",
    "line" : 66,
    "diffHunk" : "@@ -1,1 +316,320 @@              }\n            } else {\n              logInfo(\"Blocked from shutdown by running ${executor.numRunningtasks} tasks\")\n              // If there is a running task it could store blocks, so make sure we wait for a\n              // migration loop to complete after the last task is done."
  },
  {
    "id" : "2561e9ef-e7fa-4537-8f3f-5cb9d48b3d14",
    "prId" : 29032,
    "prUrl" : "https://github.com/apache/spark/pull/29032#pullrequestreview-452044883",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6ad1e8d2-b484-4c57-b910-d8c1834ee9ab",
        "parentId" : null,
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Is there a reason we broke this on two lines (here and elsewhere)? I don't have strong feelings just if theres been a change to the style want to make sure I'm in the loop.",
        "createdAt" : "2020-07-20T21:19:42Z",
        "updatedAt" : "2020-07-22T05:41:05Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "11576182-3bb0-496f-9fe5-665fc1fba956",
        "parentId" : "6ad1e8d2-b484-4c57-b910-d8c1834ee9ab",
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "It's not broken into two lines for style. I wanted to reuse the \"msg\" down below. Same deal elsewhere in this PR. ",
        "createdAt" : "2020-07-21T00:24:44Z",
        "updatedAt" : "2020-07-22T05:41:05Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      }
    ],
    "commit" : "88060be29d84e9d26a14d35c33f906a3f49434ef",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +168,172 @@        if (decommissioned) {\n          val msg = \"Asked to launch a task while decommissioned.\"\n          logError(msg)\n          driver match {\n            case Some(endpoint) =>"
  },
  {
    "id" : "65480401-c27b-4c96-b6f7-9c121a87d829",
    "prId" : 28817,
    "prUrl" : "https://github.com/apache/spark/pull/28817#pullrequestreview-430179466",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "26e99481-f231-4013-bdad-1632cbeb5d42",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "Should this variable be marked volatile ?",
        "createdAt" : "2020-06-13T23:15:05Z",
        "updatedAt" : "2020-06-14T01:11:18Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "e3f0e33e-279b-45c6-9b51-fc433be5ac26",
        "parentId" : "26e99481-f231-4013-bdad-1632cbeb5d42",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "I don't think so, this will only be accessed in one thread.",
        "createdAt" : "2020-06-14T00:53:52Z",
        "updatedAt" : "2020-06-14T01:11:18Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "a2c055715ccf2992e399cef3768b1299c24d9a82",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +263,267 @@  }\n\n  private var previousAllBlocksMigrated = false\n  private def shutdownIfDone(): Unit = {\n    val numRunningTasks = executor.numRunningTasks"
  },
  {
    "id" : "ae6e9787-0356-4bc2-b361-2dff792b0720",
    "prId" : 28817,
    "prUrl" : "https://github.com/apache/spark/pull/28817#pullrequestreview-430179435",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "569da528-b5a1-448e-87d9-b42e069e84c2",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "I think this logic of previousAllBlocks and allBlocks migrated is a bit confusing. Its not clear why the previous state has to be considered. I wonder if the following code can make this \"history\" aspect a bit clearer:\r\n\r\n```\r\nval allBlocksMigrated = !env.conf.get(STORAGE_DECOMMISSION_ENABLED) ||\r\n      env.blockManager.decommissionManager.map(_.allBlocksMigrated).orElse(false)\r\nval exitCondition = allBlocksMigrated && numRunningTasks == 0\r\nif (exitCondition) { exitExecutor(...) }\r\n```\r\n\r\nAlso, should we really be checking for numRunningTasks here ? What if some race condition caused some tasks to be scheduled onto us while we were marked for decom ?\r\n\r\nFinally, should there be a timeout for how much time the executor will stay alive in decommissioned state ? ",
        "createdAt" : "2020-06-13T23:23:18Z",
        "updatedAt" : "2020-06-14T01:11:18Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "75d133b2-98b8-4bf0-a862-6f9676f0d0fc",
        "parentId" : "569da528-b5a1-448e-87d9-b42e069e84c2",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "If a task is scheduled before we are asked to decom. You can verify this is covered by taking the logic out and watching the tests fail :) (There's an ungly thread sleep in the tests to make this possible).\r\n\r\nSince the block migrations are not atomic, I do think we need the 2x logic, unfortunately, think of this situation:\r\n1) Task launches on executor\r\n2) Executor asked to decomission\r\n3) All blocks currently stored on executor are migrated\r\n4) Task stores a block\r\n5) We check numTasks & see all blocks are migrated, then exit without migrating the block stored by task #4.\r\n\r\nNow that being said that's probably a corner case, and arguably not super important since we're really only doing best effort, but I think for the overhead of one extra boolean it's worth it to cover this corner case.",
        "createdAt" : "2020-06-14T00:52:41Z",
        "updatedAt" : "2020-06-14T01:11:18Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "a2c055715ccf2992e399cef3768b1299c24d9a82",
    "line" : 45,
    "diffHunk" : "@@ -1,1 +283,287 @@      }\n    } else {\n      // If there's a running task it could store blocks.\n      previousAllBlocksMigrated = false\n    }"
  },
  {
    "id" : "0687f2e6-47ab-485c-968b-d4f5b0a9abb6",
    "prId" : 28817,
    "prUrl" : "https://github.com/apache/spark/pull/28817#pullrequestreview-431059147",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5197f6e6-0023-4724-838d-522ac0b1b299",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "`exitExecutor` asynchronously sends RemoveExecutor to the driver. Does that actually make it to the driver ? There is also this question about if we should be using the same `Shutdown`/`StopExecutor` codepath for doing the stopping ? (But althought it seems that we do want to intimate to the driver that the executor is being removed). \r\n\r\nInterestingly, the driver does indeed respond back with a `StopExecutor` and does trigger the clean shutdown path in the executor, but again I wonder if it is too late for it. Perhaps we shouldn't be calling `System.exit` here ?\r\n\r\nAlso, as currently written, this `exitExecutor` could cause job failures: Since the `TaskSchedulerImpl` will treat the `ExecutorLossReason` send by the executor to the driver as an `exitCausedByApp` and thus penalize the task. Instead, I think we shouldn't penalize the running job on a planned executor decommission. One workaround might be to actually respond back to the driver with `ExecutorDecommission` (which is not used elsewhere currently) and then handle that specifically in the `TaskSchedulerImpl`'s determination of `exitCausedByApp`.",
        "createdAt" : "2020-06-14T00:19:30Z",
        "updatedAt" : "2020-06-14T01:11:18Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "3ee16ffe-c3bb-46e0-bf76-714a7bc9d13f",
        "parentId" : "5197f6e6-0023-4724-838d-522ac0b1b299",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "So it's my understanding the `TaskSchedulerImpl` shouldn't have any job failures because we've waited for all the tasks on the executor to finish before calling this code path. Unless is there something I've missed there?\r\n\r\nI think swapping out exit executor for instead telling the driver to stop the executor and avoiding the `system.exit` makes sense either way though.",
        "createdAt" : "2020-06-14T01:04:35Z",
        "updatedAt" : "2020-06-14T01:11:18Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "a745f4f0-7dc2-472f-91f6-5b06d2e203d7",
        "parentId" : "5197f6e6-0023-4724-838d-522ac0b1b299",
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "I was talking about the case where we get shot down before we had a chance to cleanly exit on line 276. Say for example, some time out expires and the executor/node is brought down. \r\n\r\nAre `decom.sh` and `decommission-slave.sh` expected to wait until the executor/worker process has properly shut down ? I think they have some timeouts in them to kill the executor ? Or consider a spot kill scenario where you got some warning (like 2 minutes) and then the machine is yanked out. \r\n\r\nIn this case, the executor will eventually be marked loss via a heartbeat/timeout. And that loss would be deemed as the fault of the task, and could cause job failures. I am wondering if we can fix that scenario of an unclean exit ? \r\n\r\nOne workaround I suggested above was to send a message to the driver saying that the executor is going to go away soon. When that happens (in a clean or unclean way), that loss shouldn't be attributed to the task. \r\n\r\nPerhaps this unclean executor loss/timeout handling is follow up work ? We (or rather I) can create Jira's for this under the parent ticket :-). ",
        "createdAt" : "2020-06-14T03:46:51Z",
        "updatedAt" : "2020-06-14T03:46:51Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "8d593592-c51e-4a9f-acd0-a1f6479cba2b",
        "parentId" : "5197f6e6-0023-4724-838d-522ac0b1b299",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Sure, although I _think_ this behaviour is covered by the changes in https://github.com/apache/spark/pull/26440/files (we only increment failures if the executors previous state was not decommissioning).",
        "createdAt" : "2020-06-14T18:50:17Z",
        "updatedAt" : "2020-06-14T18:50:17Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "34b1b3ae-0b02-4a17-ad5c-d80eabe5929a",
        "parentId" : "5197f6e6-0023-4724-838d-522ac0b1b299",
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "Can you please double check that ? I couldn't find this behavior when scouring TaskSchedulerImpl,  and TaskSetManager. The only place we check for an executor being decommissioned in that PR is when scheduling tasks (in CoarseGrainedSchedulerBackend#isExecutorActive). Thanks !",
        "createdAt" : "2020-06-14T19:12:07Z",
        "updatedAt" : "2020-06-14T19:12:07Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "5c34fe6a-e175-4b5b-b598-6c07172ce9c3",
        "parentId" : "5197f6e6-0023-4724-838d-522ac0b1b299",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Can you point to where in TaskSchedulerImpl it's going to fail the job? `core/src/main/scala/org/apache/spark/deploy/master/Master.scala` is where the current code is, but there might be an additional case that needs to be covered.",
        "createdAt" : "2020-06-15T19:41:14Z",
        "updatedAt" : "2020-06-15T19:41:14Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "8d4769a6-5b7e-4f15-9b7b-117a23c6f114",
        "parentId" : "5197f6e6-0023-4724-838d-522ac0b1b299",
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "https://github.com/apache/spark/blob/a2c055715ccf2992e399cef3768b1299c24d9a82/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala#L980\r\n\r\nIn this match block, we will hit the default case which will treat the failure as having been caused by the app and thus penalize it. \r\n\r\nThis routine is called from `TaskScheduler.executorLost`",
        "createdAt" : "2020-06-15T20:02:05Z",
        "updatedAt" : "2020-06-15T20:02:06Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "a6e13f0f-e600-4557-aad3-3d7c290f1f77",
        "parentId" : "5197f6e6-0023-4724-838d-522ac0b1b299",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Thanks :) If you want to make a PR for that I'd be happy to review/merge since I think that would not depend on any of the in-flight PRs just the current code in master.",
        "createdAt" : "2020-06-15T22:39:20Z",
        "updatedAt" : "2020-06-15T22:39:20Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "f64d47be-ac14-4ad6-ba67-6af319be6fe9",
        "parentId" : "5197f6e6-0023-4724-838d-522ac0b1b299",
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "Absolutely !. Thanks",
        "createdAt" : "2020-06-15T23:28:19Z",
        "updatedAt" : "2020-06-15T23:28:19Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      }
    ],
    "commit" : "a2c055715ccf2992e399cef3768b1299c24d9a82",
    "line" : 34,
    "diffHunk" : "@@ -1,1 +275,279 @@        if (allBlocksMigrated && previousAllBlocksMigrated) {\n          logInfo(\"No running tasks, all blocks migrated, stopping.\")\n          exitExecutor(0, \"Finished decommissioning\", notifyDriver = true)\n        }\n        previousAllBlocksMigrated = allBlocksMigrated"
  },
  {
    "id" : "7ed6b029-0751-46ae-b486-998a7df5d573",
    "prId" : 26440,
    "prUrl" : "https://github.com/apache/spark/pull/26440#pullrequestreview-315654242",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "724ded9e-65c5-499e-9fa6-df93489c9366",
        "parentId" : null,
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "is there an application level config to enable decommissioning?  For instance YARN doesn't need this if YARN is telling the driver",
        "createdAt" : "2019-11-08T21:45:17Z",
        "updatedAt" : "2020-02-13T23:10:10Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "e1ea350a-005d-4343-9444-a777dc54ab2a",
        "parentId" : "724ded9e-65c5-499e-9fa6-df93489c9366",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "That's a good point. Right now we register SIGPWR regardless, but we only really need it on K8s & stand alone. Do you think there is any harm in having SIGPWR registered in YARN deployments?",
        "createdAt" : "2019-11-08T22:43:51Z",
        "updatedAt" : "2020-02-13T23:10:10Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "7eb3c353-30db-48da-8483-62dc8951c3d8",
        "parentId" : "724ded9e-65c5-499e-9fa6-df93489c9366",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "I don't know of any specific issues with having it registered on YARN. I was just thinking if not needed no reason to register just in case something unexpected happened.  If we don't need the app config for anything else then at this point I think we can leave it.\r\n",
        "createdAt" : "2019-11-11T15:00:13Z",
        "updatedAt" : "2020-02-13T23:10:10Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "ad3ae50e-844f-4172-b734-65dd42714638",
        "parentId" : "724ded9e-65c5-499e-9fa6-df93489c9366",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "I think the only situations in which SIGPWR would get sent are from our script or system shutdown. Now if this a graceful system shutdown which didn't fall back to an unblockable signal that might be annoying, but I think on servers it should be fine. But lets leave this comment here incase anyone else has a counter point.",
        "createdAt" : "2019-11-12T16:10:35Z",
        "updatedAt" : "2020-02-13T23:10:10Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "af550303e0b929dc9f7436bcfb36438ff36b8208",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +83,87 @@  override def onStart(): Unit = {\n    logInfo(\"Registering PWR handler.\")\n    SignalUtils.register(\"PWR\")(decommissionSelf)\n\n    logInfo(\"Connecting to driver: \" + driverUrl)"
  },
  {
    "id" : "7c48fa66-6372-4cda-a7bd-5804b7043c9a",
    "prId" : 26440,
    "prUrl" : "https://github.com/apache/spark/pull/26440#pullrequestreview-359200340",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "545f4042-71da-4007-8ce0-143911820158",
        "parentId" : null,
        "authorId" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "body" : "I think that instead of doing this here, it should be done in `onStart` where the driver reference is created. That means the decommission message is sent to the driver as soon as possible after the signal arrives, instead of waiting for the driver to try to use the executor for something.\r\n\r\n(That also means this block can go away and you can just keep the log message in `Executor.scala`.)",
        "createdAt" : "2020-01-06T20:25:16Z",
        "updatedAt" : "2020-02-13T23:10:10Z",
        "lastEditedBy" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "tags" : [
        ]
      },
      {
        "id" : "0ced7175-da60-4e77-a341-11cff511b73a",
        "parentId" : "545f4042-71da-4007-8ce0-143911820158",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "So what about when we are scaling down after the driver reference is created?",
        "createdAt" : "2020-01-09T22:40:55Z",
        "updatedAt" : "2020-02-13T23:10:10Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "d1208498-3669-4952-8c22-383ebea75aec",
        "parentId" : "545f4042-71da-4007-8ce0-143911820158",
        "authorId" : "31c8cfa2-9d5f-412b-80d0-c7d204dd8cd1",
        "body" : "https://docs.google.com/document/d/1xVO1b6KAwdUhjEJBolVPl9C6sLj7oOveErwDSYdT-pE/edit?disco=AAAAI73a0FM\r\nI have marked a comment on this in design doc as well. I think it can be handled by the driver by not allocating tasks to the executor at the first place.\r\nWhen driver is aware of the possible decommission of the node, it can stop allocating tasks to this executor. A small code change in the driver's org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.DriverEndpoint#makeOffers \r\n```\r\n       // Filter out executors on decommissioning worker\r\n        val activeExecutors = executorDataMap.filterKeys(isExecutorActive)\r\n                                 .filter(x => !isNodeDecommissioning(x._2.executorHost))\r\n```\r\nFor this there can be DecommissionTracker ( in the same lines as BlacklistTracker). The DT is filled when the driver is informed of the decommissioning is informed on the host. As comment on the design I have tried to elaborate the flow of populating DT.\r\n```\r\n  private def isNodeDecommissioning(hostname: String): Boolean = {\r\n    decommissionTracker match {\r\n      case None => return false\r\n      case Some(decommissionTracker) => return decommissionTracker.isNodeDecommissioning(hostname)\r\n    }\r\n  }\r\n}\r\n```\r\n",
        "createdAt" : "2020-02-10T06:32:55Z",
        "updatedAt" : "2020-02-13T23:10:10Z",
        "lastEditedBy" : "31c8cfa2-9d5f-412b-80d0-c7d204dd8cd1",
        "tags" : [
        ]
      },
      {
        "id" : "5ff14b67-11c3-4908-b2d2-953e6f65489a",
        "parentId" : "545f4042-71da-4007-8ce0-143911820158",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "So @itskals on Kubernetes and in standalone the driver won't know first (and those the only parts implemented in this PR). Certainly, when we go to implement YARN support we could try and short circuit talking to the executor -- however to enable later things like migrations I'd still want to send the message.\r\n\r\nThis could make a difference when we implement YARN support and I've added a note in the design doc so that we don't skip it then.\r\n\r\n",
        "createdAt" : "2020-02-14T20:27:22Z",
        "updatedAt" : "2020-02-14T20:27:22Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "af550303e0b929dc9f7436bcfb36438ff36b8208",
    "line" : 34,
    "diffHunk" : "@@ -1,1 +168,172 @@          logError(\"Asked to launch a task while decommissioned.\")\n          driver match {\n            case Some(endpoint) =>\n              logInfo(\"Sending DecommissionExecutor to driver.\")\n              endpoint.send(DecommissionExecutor(executorId))"
  },
  {
    "id" : "2158f5d7-2566-455a-9b6f-27e5466303d8",
    "prId" : 26440,
    "prUrl" : "https://github.com/apache/spark/pull/26440#pullrequestreview-356205639",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1f55b4e1-1d88-4ec5-a876-47da344d77f3",
        "parentId" : null,
        "authorId" : "11f6c849-b5b9-4c65-990c-525cf572b913",
        "body" : "Instead of decommission Executor, Can we have Entire node decommission\r\neg.  driver.get.askSync[Boolean](AddNodeToDecommission(hostname, terminationTime, NodeLossReason))",
        "createdAt" : "2020-02-10T13:20:57Z",
        "updatedAt" : "2020-02-13T23:10:10Z",
        "lastEditedBy" : "11f6c849-b5b9-4c65-990c-525cf572b913",
        "tags" : [
        ]
      },
      {
        "id" : "5dff0c45-3370-4a7d-83b0-bfb8ae98a950",
        "parentId" : "1f55b4e1-1d88-4ec5-a876-47da344d77f3",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Same as previous comment, in standalone only sure, but in YARN/K8s we could see individual executors decommission.",
        "createdAt" : "2020-02-10T19:33:57Z",
        "updatedAt" : "2020-02-13T23:10:10Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "af550303e0b929dc9f7436bcfb36438ff36b8208",
    "line" : 55,
    "diffHunk" : "@@ -1,1 +264,268 @@      // Tell master we are are decommissioned so it stops trying to schedule us\n      if (driver.nonEmpty) {\n        driver.get.askSync[Boolean](DecommissionExecutor(executorId))\n      } else {\n        logError(\"No driver to message decommissioning.\")"
  },
  {
    "id" : "0c724fd3-f2f4-4cd5-b64f-2041769f72fe",
    "prId" : 26331,
    "prUrl" : "https://github.com/apache/spark/pull/26331#pullrequestreview-316525163",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ee57525a-f487-4a41-9cce-c1c1aba2387d",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "We need a new test case for this code path, @nishchalv .",
        "createdAt" : "2019-11-01T18:07:28Z",
        "updatedAt" : "2019-11-13T19:50:48Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "ceecb47b-8dc4-4f49-9a9c-40f8185f17dc",
        "parentId" : "ee57525a-f487-4a41-9cce-c1c1aba2387d",
        "authorId" : "41f6c9df-5900-4bfb-a80f-4aa0caafef27",
        "body" : "@dongjoon-hyun Added couple of test cases.",
        "createdAt" : "2019-11-13T19:51:25Z",
        "updatedAt" : "2019-11-13T19:51:26Z",
        "lastEditedBy" : "41f6c9df-5900-4bfb-a80f-4aa0caafef27",
        "tags" : [
        ]
      },
      {
        "id" : "f0996930-f65a-48d1-a1fa-ebd45ecf3d52",
        "parentId" : "ee57525a-f487-4a41-9cce-c1c1aba2387d",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Thank you!",
        "createdAt" : "2019-11-13T20:12:05Z",
        "updatedAt" : "2019-11-13T20:12:05Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "c2a2b95dbd0896377f4c1573aa141e1dc0b6dd52",
    "line" : 73,
    "diffHunk" : "@@ -1,1 +374,378 @@\n    if (bindAddress == null) {\n      bindAddress = hostname\n    }\n"
  },
  {
    "id" : "1093f672-556b-433e-9f90-4b15fd36f61e",
    "prId" : 26331,
    "prUrl" : "https://github.com/apache/spark/pull/26331#pullrequestreview-314508485",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9feadd73-5de1-4d65-a875-697271aaaba9",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Do we need this line in this PR?",
        "createdAt" : "2019-11-01T18:08:50Z",
        "updatedAt" : "2019-11-13T19:50:48Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "2dbeb5ab-ef36-41f8-801a-820984bd5364",
        "parentId" : "9feadd73-5de1-4d65-a875-697271aaaba9",
        "authorId" : "677aa336-324b-4b93-8300-21f8bd378f26",
        "body" : "I think this is needed unless we add extra method in `RpcEnv` object.",
        "createdAt" : "2019-11-08T23:18:17Z",
        "updatedAt" : "2019-11-13T19:50:48Z",
        "lastEditedBy" : "677aa336-324b-4b93-8300-21f8bd378f26",
        "tags" : [
        ]
      }
    ],
    "commit" : "c2a2b95dbd0896377f4c1573aa141e1dc0b6dd52",
    "line" : 34,
    "diffHunk" : "@@ -1,1 +267,271 @@        executorConf,\n        new SecurityManager(executorConf),\n        numUsableCores = 0,\n        clientMode = true)\n"
  },
  {
    "id" : "94edccf3-adbf-4f42-bfc0-23be1b2a991d",
    "prId" : 24702,
    "prUrl" : "https://github.com/apache/spark/pull/24702#pullrequestreview-249403153",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b10a90f5-7e5d-4a12-9b76-c496ececb532",
        "parentId" : null,
        "authorId" : "99156419-7ce7-4671-b858-d0b5c4711f88",
        "body" : "this changes not just for k8s, right?",
        "createdAt" : "2019-05-24T17:26:55Z",
        "updatedAt" : "2019-06-17T15:06:38Z",
        "lastEditedBy" : "99156419-7ce7-4671-b858-d0b5c4711f88",
        "tags" : [
        ]
      },
      {
        "id" : "b872fd26-64b4-4717-9c63-452db8760f9c",
        "parentId" : "b10a90f5-7e5d-4a12-9b76-c496ececb532",
        "authorId" : "e4c929a6-8e22-4cc2-8ddc-d3c60492cde9",
        "body" : "Hi @felixcheung \r\nIt's the executor used for kubernetes but It canbe called for different resource managers.\r\nI Labeled it as K8s because it also modifies the docker images used in k8s. Not sure what would be the best component. For instance YarnCoarseGrainedExecutorBackend extends this class.",
        "createdAt" : "2019-05-24T18:13:28Z",
        "updatedAt" : "2019-06-17T15:06:38Z",
        "lastEditedBy" : "e4c929a6-8e22-4cc2-8ddc-d3c60492cde9",
        "tags" : [
        ]
      },
      {
        "id" : "e9b1d52a-11d1-4f6b-a188-a25927794c42",
        "parentId" : "b10a90f5-7e5d-4a12-9b76-c496ececb532",
        "authorId" : "99156419-7ce7-4671-b858-d0b5c4711f88",
        "body" : "yes, that's what I mean. in case of shared class, the PR should tag all of them",
        "createdAt" : "2019-05-25T03:53:46Z",
        "updatedAt" : "2019-06-17T15:06:38Z",
        "lastEditedBy" : "99156419-7ce7-4671-b858-d0b5c4711f88",
        "tags" : [
        ]
      },
      {
        "id" : "323ba8ea-7973-45b8-9564-131b52774236",
        "parentId" : "b10a90f5-7e5d-4a12-9b76-c496ececb532",
        "authorId" : "e4c929a6-8e22-4cc2-8ddc-d3c60492cde9",
        "body" : "Hi @felixcheung \r\nThen this patch belongs to core  and Kubernetes due to the java property in the docker image.  Looking at https://spark-prs.appspot.com/ for the categories.\r\n",
        "createdAt" : "2019-05-27T09:26:35Z",
        "updatedAt" : "2019-06-17T15:06:38Z",
        "lastEditedBy" : "e4c929a6-8e22-4cc2-8ddc-d3c60492cde9",
        "tags" : [
        ]
      },
      {
        "id" : "2546864f-c716-4afd-b89f-f66e6f382065",
        "parentId" : "b10a90f5-7e5d-4a12-9b76-c496ececb532",
        "authorId" : "e4c929a6-8e22-4cc2-8ddc-d3c60492cde9",
        "body" : "Hi @felixcheung  any feedback?",
        "createdAt" : "2019-05-30T09:42:21Z",
        "updatedAt" : "2019-06-17T15:06:38Z",
        "lastEditedBy" : "e4c929a6-8e22-4cc2-8ddc-d3c60492cde9",
        "tags" : [
        ]
      },
      {
        "id" : "ce3174d6-1fed-4673-bd7e-b06b9d9f1ac2",
        "parentId" : "b10a90f5-7e5d-4a12-9b76-c496ececb532",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "I think this can still just be simpler.\r\n```\r\nvar driver: RpcEndpointRef = null\r\nfor (i <- 0 until 3) {\r\n  try {\r\n    driver = ...\r\n  } catch {\r\n    case e: Throwable => // continue\r\n  }\r\n}\r\nrequire(driver != null, \"...\")\r\n```",
        "createdAt" : "2019-06-10T13:04:05Z",
        "updatedAt" : "2019-06-17T15:06:38Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "bee8de09-21c0-44fc-8e75-a6b9882865f5",
        "parentId" : "b10a90f5-7e5d-4a12-9b76-c496ececb532",
        "authorId" : "e4c929a6-8e22-4cc2-8ddc-d3c60492cde9",
        "body" : "Actually If we don't add the conditions int  the for loop, it will try to get many connections, even if they are succeeding, so I fixed the other way.",
        "createdAt" : "2019-06-11T11:05:33Z",
        "updatedAt" : "2019-06-17T15:06:38Z",
        "lastEditedBy" : "e4c929a6-8e22-4cc2-8ddc-d3c60492cde9",
        "tags" : [
        ]
      },
      {
        "id" : "ed3a3fd7-5d48-4030-a7e0-49e2ec7bb42b",
        "parentId" : "b10a90f5-7e5d-4a12-9b76-c496ececb532",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Oops @jlpedrosa I meant to retain the condition in the loop, but otherwise I find the code right above simpler. However I like retaining the original exception. Just please pay attention to the style. `driver = ` indent is off and needs to say `if (i == nTries - 1)`",
        "createdAt" : "2019-06-12T15:37:12Z",
        "updatedAt" : "2019-06-17T15:06:38Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "51f314d4-2e2b-41e3-9829-89c562695696",
        "parentId" : "b10a90f5-7e5d-4a12-9b76-c496ececb532",
        "authorId" : "e4c929a6-8e22-4cc2-8ddc-d3c60492cde9",
        "body" : "@srowen \r\nFixed the indentation.  Please double check.",
        "createdAt" : "2019-06-13T08:55:38Z",
        "updatedAt" : "2019-06-17T15:06:38Z",
        "lastEditedBy" : "e4c929a6-8e22-4cc2-8ddc-d3c60492cde9",
        "tags" : [
        ]
      },
      {
        "id" : "5f747546-9a05-4f63-afc5-b3cb00c39cf8",
        "parentId" : "b10a90f5-7e5d-4a12-9b76-c496ececb532",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "It looks the same; did you push?",
        "createdAt" : "2019-06-13T12:39:45Z",
        "updatedAt" : "2019-06-17T15:06:38Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "040c4daf-60ee-49d3-b6cb-917e89ddfbef",
        "parentId" : "b10a90f5-7e5d-4a12-9b76-c496ececb532",
        "authorId" : "e4c929a6-8e22-4cc2-8ddc-d3c60492cde9",
        "body" : "@srowen  I think it's in the right place, I'm rewriting history to clean the multiple commits. I think it is in the right place... ",
        "createdAt" : "2019-06-13T14:50:24Z",
        "updatedAt" : "2019-06-17T15:06:38Z",
        "lastEditedBy" : "e4c929a6-8e22-4cc2-8ddc-d3c60492cde9",
        "tags" : [
        ]
      }
    ],
    "commit" : "edbc135803067128f736f68871b960b2482f0558",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +294,298 @@            throw e\n          }\n        }\n      }\n"
  },
  {
    "id" : "bb6121ff-33ef-48e5-9a61-4e0fc1bff4f3",
    "prId" : 24615,
    "prUrl" : "https://github.com/apache/spark/pull/24615#pullrequestreview-240248604",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a28d750d-25ba-477e-a93c-3310144ad653",
        "parentId" : null,
        "authorId" : "0c812942-02cb-4975-9748-394d1387affa",
        "body" : "`SPARK_EXECUTOR_RESOURCE_PREFIX`?",
        "createdAt" : "2019-05-21T18:54:11Z",
        "updatedAt" : "2019-05-23T14:59:29Z",
        "lastEditedBy" : "0c812942-02cb-4975-9748-394d1387affa",
        "tags" : [
        ]
      },
      {
        "id" : "f6457886-5fef-4dfc-8317-48a8736b88c9",
        "parentId" : "a28d750d-25ba-477e-a93c-3310144ad653",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "No, I only want to look for resources if a task we are running requires them.  If no tasks require other resources, then its just overhead for us.",
        "createdAt" : "2019-05-21T19:17:49Z",
        "updatedAt" : "2019-05-23T14:59:29Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "768392ca83c29a2ef6b3ecb2d175cccd20ffa0ab",
    "line" : 49,
    "diffHunk" : "@@ -1,1 +87,91 @@  def parseOrFindResources(resourcesFile: Option[String]): Map[String, ResourceInformation] = {\n    // only parse the resources if a task requires them\n    val resourceInfo = if (env.conf.getAllWithPrefix(SPARK_TASK_RESOURCE_PREFIX).nonEmpty) {\n      val actualExecResources = resourcesFile.map { resourceFileStr => {\n        val source = new BufferedInputStream(new FileInputStream(resourceFileStr))"
  },
  {
    "id" : "7192f812-20f1-4376-bc29-fa7e00e16a14",
    "prId" : 24406,
    "prUrl" : "https://github.com/apache/spark/pull/24406#pullrequestreview-233681117",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5fe4fc15-6f55-4002-b0e9-7969d73b2e8d",
        "parentId" : null,
        "authorId" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "body" : "nit: If `resourceFile` is provided but user didn't specify task resource requirement shall we print a warning message?",
        "createdAt" : "2019-05-03T01:48:25Z",
        "updatedAt" : "2019-05-14T02:27:16Z",
        "lastEditedBy" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "tags" : [
        ]
      },
      {
        "id" : "4ece3c62-daf7-417a-b026-d3cb831fd779",
        "parentId" : "5fe4fc15-6f55-4002-b0e9-7969d73b2e8d",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "will add one",
        "createdAt" : "2019-05-03T21:33:55Z",
        "updatedAt" : "2019-05-14T02:27:16Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "b9dacef1d7d47d19df300628d3841b3a13c03547",
    "line" : 141,
    "diffHunk" : "@@ -1,1 +165,169 @@\n      execResources\n    } else {\n      if (resourcesFile.nonEmpty) {\n        logWarning(s\"A resources file was specified but the application is not configured \" +"
  },
  {
    "id" : "419092a2-b121-4887-8dc3-3161f3b7b10a",
    "prId" : 24406,
    "prUrl" : "https://github.com/apache/spark/pull/24406#pullrequestreview-234544177",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f96da207-1934-4a88-bdfb-ea52e37d15ba",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "Is it correct to say this is only used in standalone mode? this is how the master tells executors what resource it can use? if that's right, how would this work for other resource managers -- simply, in some other RM-specific way? ",
        "createdAt" : "2019-05-06T22:06:48Z",
        "updatedAt" : "2019-05-14T02:27:16Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "48a6931e-0cdc-4944-ba9b-acf9c6cfb799",
        "parentId" : "f96da207-1934-4a88-bdfb-ea52e37d15ba",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "Yes, the intention is for this to be used by standalone mode.  It has no support for containers or isolation like the other resource managers have so it will pass in a file specifying which resources for that container to use. In the GPU case if the node has 4 gpu's and the executor asked for one, it would pass in one GPU resource and the id of that 1 gpu it should use.  There is a separate jira to add that to standalone mode.\r\nThe other resources managers I believe all support some sort of container and isolation such that when the executor starts up it can just use the discoveryScript  to auto discover what resources it has.  We have tested on kubernetes and yarn, I don't have a mesos setup to test on but someone I believe volunteered to work on that side.\r\nYou can obviously run other resources manager like YARN without docker and isolation and in that case this wouldn't work, but YARN doesn't give you any other option.  It doesn't tell you what GPU you received for instance, so in the documentation we will put the limitations and required that yarn/kubernetes, etc must be configured in such a way for this to work.  If you are running YARN without isolation then you could also come up with discovery script to make this work, similar to the hacks users are doing now on YARN with gpu scheduling.",
        "createdAt" : "2019-05-07T14:18:04Z",
        "updatedAt" : "2019-05-14T02:27:16Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "b9dacef1d7d47d19df300628d3841b3a13c03547",
    "line" : 34,
    "diffHunk" : "@@ -1,1 +54,58 @@    userClassPath: Seq[URL],\n    env: SparkEnv,\n    resourcesFile: Option[String])\n  extends ThreadSafeRpcEndpoint with ExecutorBackend with Logging {\n"
  },
  {
    "id" : "713f3a12-73e8-4d94-b494-76c84dbb624c",
    "prId" : 24406,
    "prUrl" : "https://github.com/apache/spark/pull/24406#pullrequestreview-235747491",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1dd68203-524b-4fc7-8770-a8d613c5357b",
        "parentId" : null,
        "authorId" : "0c812942-02cb-4975-9748-394d1387affa",
        "body" : "minor: `parse(source).extract[Seq[ResourceInformation]]`",
        "createdAt" : "2019-05-07T22:23:53Z",
        "updatedAt" : "2019-05-14T02:27:16Z",
        "lastEditedBy" : "0c812942-02cb-4975-9748-394d1387affa",
        "tags" : [
        ]
      },
      {
        "id" : "f5b23fd2-950b-43e5-8e67-2a1a7e3cfe08",
        "parentId" : "1dd68203-524b-4fc7-8770-a8d613c5357b",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "this code changed due to ResourceInformation not being case class anymore",
        "createdAt" : "2019-05-09T18:18:46Z",
        "updatedAt" : "2019-05-14T02:27:16Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "b9dacef1d7d47d19df300628d3841b3a13c03547",
    "line" : 107,
    "diffHunk" : "@@ -1,1 +131,135 @@        val source = new BufferedInputStream(new FileInputStream(resourceFileStr))\n        val resourceMap = try {\n          val parsedJson = parse(source).asInstanceOf[JArray].arr\n          parsedJson.map { json =>\n            val name = (json \\ \"name\").extract[String]"
  },
  {
    "id" : "43c8eb6b-11d8-4066-86b0-56a0631f1e5b",
    "prId" : 24406,
    "prUrl" : "https://github.com/apache/spark/pull/24406#pullrequestreview-237227143",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f0c91d4d-9db9-47fd-acb1-8d99117bdafc",
        "parentId" : null,
        "authorId" : "0c812942-02cb-4975-9748-394d1387affa",
        "body" : "Had an offline discussion with @WeichenXu123 . He suggested refactoring this check to make it easier to read. Now the arguments are:\r\n\r\n* `reqResourcesAndCounts`: request per task (not per executor)\r\n* `actualResources`: resources allocated per executor\r\n\r\nIt is not easy to tell from the variable names and hence make the code harder to read. Basically we need the following:\r\n\r\n1. number allocated per executor cannot be smaller than requested count for each resource name\r\n2. requested count for executor cannot be smaller than requested count for task for each resource name. Note that this doesn't require resource discovery.\r\n3. the set of requested resource names for executors should match the set of requested resource names for tasks.\r\n\r\nIt would be nice to refactor the method into those three. We can also do it in a follow-up PR.",
        "createdAt" : "2019-05-14T03:53:09Z",
        "updatedAt" : "2019-05-14T03:54:04Z",
        "lastEditedBy" : "0c812942-02cb-4975-9748-394d1387affa",
        "tags" : [
        ]
      },
      {
        "id" : "153407fa-1f31-4352-b70d-8c3383dea8b5",
        "parentId" : "f0c91d4d-9db9-47fd-acb1-8d99117bdafc",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "I added a comment  to the driver jira since it needs to be refactored for that anyway:\r\nhttps://issues.apache.org/jira/browse/SPARK-27488",
        "createdAt" : "2019-05-14T13:02:09Z",
        "updatedAt" : "2019-05-14T13:02:09Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "b9dacef1d7d47d19df300628d3841b3a13c03547",
    "line" : 64,
    "diffHunk" : "@@ -1,1 +88,92 @@  // mismatches between what the user requested and what resource manager gave or\n  // what the discovery script found.\n  private def checkResourcesMeetRequirements(\n      resourceConfigPrefix: String,\n      reqResourcesAndCounts: Array[(String, String)],"
  }
]