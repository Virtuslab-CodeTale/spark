[
  {
    "id" : "e187b627-256d-433a-a3ab-1d5b307f5c17",
    "prId" : 30389,
    "prUrl" : "https://github.com/apache/spark/pull/30389#pullrequestreview-536169178",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "f40b3708-fdd3-4341-b185-68b1095ac1a1",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Okay, actually the test in `PythonRDDSuite` was related.\r\n\r\nIn `python server error handling` test we don't initialize `SparkEnv`  because we don't create `SparkContext` or executor there by default, but `SparkEnv.get` was used in the previous change which ended up with NPE.\r\n\r\nNow, I made a change to work around by using `SparkConf` directly here.",
        "createdAt" : "2020-11-23T02:38:00Z",
        "updatedAt" : "2020-11-23T02:38:01Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "5684646e-15c6-49c9-93b5-f942354f476f",
        "parentId" : "f40b3708-fdd3-4341-b185-68b1095ac1a1",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Got it~",
        "createdAt" : "2020-11-23T04:13:10Z",
        "updatedAt" : "2020-11-23T04:13:10Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "363f3bb07ccd34cf3d70303a5512595cc59b603b",
    "line" : 32,
    "diffHunk" : "@@ -1,1 +52,56 @@    val serverSocket = new ServerSocket(0, 1, InetAddress.getByAddress(Array(127, 0, 0, 1)))\n    // Close the socket if no connection in the configured seconds\n    val timeout = authHelper.conf.get(PYTHON_AUTH_SOCKET_TIMEOUT).toInt\n    logTrace(s\"Setting timeout to $timeout sec\")\n    serverSocket.setSoTimeout(timeout * 1000)"
  }
]