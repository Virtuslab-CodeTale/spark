[
  {
    "id" : "5eae8605-2016-49e4-98fb-102ba3c626ad",
    "prId" : 29870,
    "prUrl" : "https://github.com/apache/spark/pull/29870#pullrequestreview-497385675",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d319515d-783f-4c63-af0a-54a467832fea",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Isn't it virtually same?",
        "createdAt" : "2020-09-27T08:40:55Z",
        "updatedAt" : "2020-09-27T08:40:55Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "7b6744d4-be58-4f28-b99f-d64a6aed4eb4",
        "parentId" : "d319515d-783f-4c63-af0a-54a467832fea",
        "authorId" : "da023277-487f-41ab-9359-5ee7378aba9e",
        "body" : "\"no less\" than is slightly confusing as the application fails if both values are same, hence changing it to greater. Please let me know if it makes sense. ",
        "createdAt" : "2020-09-27T15:56:54Z",
        "updatedAt" : "2020-09-27T15:56:54Z",
        "lastEditedBy" : "da023277-487f-41ab-9359-5ee7378aba9e",
        "tags" : [
        ]
      },
      {
        "id" : "0189671d-ef04-4f3d-87cf-64f63d175723",
        "parentId" : "d319515d-783f-4c63-af0a-54a467832fea",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Okay. it's probably because English isn't my first language. cc @srowen.",
        "createdAt" : "2020-09-28T03:54:08Z",
        "updatedAt" : "2020-09-28T03:54:09Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "e30fd780-8904-43cc-813e-04a8536419a1",
        "parentId" : "d319515d-783f-4c63-af0a-54a467832fea",
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "\"no less\" would be \"greater than or equal to\" and the check does look for \"greater than\" so the correction is OK.",
        "createdAt" : "2020-09-28T06:33:35Z",
        "updatedAt" : "2020-09-28T06:33:36Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "64f2b578-e472-4bc6-848e-bcaa691c1655",
        "parentId" : "d319515d-783f-4c63-af0a-54a467832fea",
        "authorId" : "da023277-487f-41ab-9359-5ee7378aba9e",
        "body" : "@srowen Thanks for checking it out, yes, that was the intent. Could you please help in closing this PR? ",
        "createdAt" : "2020-09-28T10:33:12Z",
        "updatedAt" : "2020-09-28T10:33:13Z",
        "lastEditedBy" : "da023277-487f-41ab-9359-5ee7378aba9e",
        "tags" : [
        ]
      }
    ],
    "commit" : "b113ead148ac798f6652dc0c005d3e3c74e5af2e",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +569,573 @@    // it will almost always cause ExecutorLostFailure. See SPARK-22754.\n    require(executorTimeoutThresholdMs > executorHeartbeatIntervalMs, \"The value of \" +\n      s\"${networkTimeout}=${executorTimeoutThresholdMs}ms must be greater than the value of \" +\n      s\"${EXECUTOR_HEARTBEAT_INTERVAL.key}=${executorHeartbeatIntervalMs}ms.\")\n  }"
  },
  {
    "id" : "41977c23-1f64-48a5-b3e3-31229c10862e",
    "prId" : 27727,
    "prUrl" : "https://github.com/apache/spark/pull/27727#pullrequestreview-365904126",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "68293bb1-486c-4474-ab57-e4d7d32ce91b",
        "parentId" : null,
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "this is wrong, you are just saying the alternate config is the same as the current config.\r\nAre you just trying to remove spark.yarn.driver.memoryOverhead reference altogether?",
        "createdAt" : "2020-02-27T17:01:14Z",
        "updatedAt" : "2020-02-27T17:01:14Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "5fa41c8b-a3fc-4083-8d53-7e40494bb46f",
        "parentId" : "68293bb1-486c-4474-ab57-e4d7d32ce91b",
        "authorId" : "f2b84c73-5fe1-4d69-9ebc-216d224cf406",
        "body" : "@tgravescs you're right, the idea t is to eliminate the old configuration for the following versions",
        "createdAt" : "2020-02-27T18:39:48Z",
        "updatedAt" : "2020-02-27T18:39:48Z",
        "lastEditedBy" : "f2b84c73-5fe1-4d69-9ebc-216d224cf406",
        "tags" : [
        ]
      }
    ],
    "commit" : "e3a2a7a83da7885a5dcd25e173b52ee606c28933",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +690,694 @@      AlternateConfig(\"spark.scheduler.listenerbus.eventqueue.size\", \"2.3\")),\n    DRIVER_MEMORY_OVERHEAD.key -> Seq(\n      AlternateConfig(\"spark.driver.memoryOverhead\", \"2.3\")),\n    EXECUTOR_MEMORY_OVERHEAD.key -> Seq(\n      AlternateConfig(\"spark.executor.memoryOverhead\", \"2.3\")),"
  },
  {
    "id" : "eeecfbd9-a298-4c5d-a842-35b93d16e3bc",
    "prId" : 27463,
    "prUrl" : "https://github.com/apache/spark/pull/27463#pullrequestreview-354214544",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "61680f08-0d35-49e6-a96c-85fef81fdda2",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "will spark log warning message when these 2 alternatives are used? If not we still need to put them in `deprecatedConfigs`",
        "createdAt" : "2020-02-06T05:29:34Z",
        "updatedAt" : "2020-02-06T05:54:45Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "9bdb995e-b0d2-4ac0-aca8-4c078177b09c",
        "parentId" : "61680f08-0d35-49e6-a96c-85fef81fdda2",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "It will when user use old one, since deprecation warning also includes `configsWithAlternatives`:\r\nhttps://github.com/apache/spark/blob/77510c602a0141ac2bdf7576b5fbb38724637535/core/src/main/scala/org/apache/spark/SparkConf.scala#L753-L766",
        "createdAt" : "2020-02-06T05:43:58Z",
        "updatedAt" : "2020-02-06T05:54:45Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "8de1b17d-3b75-4c42-89d7-2516f3e05472",
        "parentId" : "61680f08-0d35-49e6-a96c-85fef81fdda2",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "I tested locally, Spark will log warning messages.",
        "createdAt" : "2020-02-06T05:55:30Z",
        "updatedAt" : "2020-02-06T05:55:30Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "8f7540e4cb1336f99889ddf2d449f3ee59f9b491",
    "line" : 6,
    "diffHunk" : "@@ -1,1 +686,690 @@    MAX_REMOTE_BLOCK_SIZE_FETCH_TO_MEM.key -> Seq(\n      AlternateConfig(\"spark.reducer.maxReqSizeShuffleToMem\", \"2.3\"),\n      AlternateConfig(\"spark.maxRemoteBlockSizeFetchToMem\", \"3.0\")),\n    LISTENER_BUS_EVENT_QUEUE_CAPACITY.key -> Seq(\n      AlternateConfig(\"spark.scheduler.listenerbus.eventqueue.size\", \"2.3\")),"
  },
  {
    "id" : "acea9c60-125b-45c6-b788-5e3c8f48a790",
    "prId" : 26390,
    "prUrl" : "https://github.com/apache/spark/pull/26390#pullrequestreview-315940322",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1abef7ee-2d53-4e1d-aa8f-f1be3c77dbdb",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@vanzin, not related to this PR but do you know why it's called deprecated instead of removed config?",
        "createdAt" : "2019-11-08T01:59:25Z",
        "updatedAt" : "2019-11-12T17:40:51Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "89a10bb0-e255-4520-9464-a3b147c6a4cb",
        "parentId" : "1abef7ee-2d53-4e1d-aa8f-f1be3c77dbdb",
        "authorId" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "body" : "Because the name of the variable hardly matters, what you want is a message that explains what's happening with that config (which is what the user sees). It was initially created for deprecated configs but works just as well for removed ones that we want to warn about.",
        "createdAt" : "2019-11-12T17:33:53Z",
        "updatedAt" : "2019-11-12T17:40:51Z",
        "lastEditedBy" : "0b3111e2-9584-4813-8cb5-8f40e119f71c",
        "tags" : [
        ]
      },
      {
        "id" : "755f2ea4-00e0-4e49-aa02-3e8f0f5ba8cf",
        "parentId" : "1abef7ee-2d53-4e1d-aa8f-f1be3c77dbdb",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Ah, thanks. It might be better to rename it later then :-).",
        "createdAt" : "2019-11-13T00:49:54Z",
        "updatedAt" : "2019-11-13T00:49:55Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "ad132575ab7120e78f68785a0d41a97a19dd48b1",
    "line" : 1,
    "diffHunk" : "@@ -1,1 +617,621 @@      DeprecatedConfig(\"spark.executor.port\", \"2.0.0\", \"Not used anymore\"),\n      DeprecatedConfig(\"spark.shuffle.service.index.cache.entries\", \"2.3.0\",\n        \"Not used anymore. Please use spark.shuffle.service.index.cache.size\"),\n      DeprecatedConfig(\"spark.yarn.credentials.file.retention.count\", \"2.4.0\", \"Not used anymore.\"),\n      DeprecatedConfig(\"spark.yarn.credentials.file.retention.days\", \"2.4.0\", \"Not used anymore.\"),"
  }
]