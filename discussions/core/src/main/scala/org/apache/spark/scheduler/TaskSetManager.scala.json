[
  {
    "id" : "6fb869ec-40b5-41dd-89f4-3d7e8097c0f4",
    "prId" : 30650,
    "prUrl" : "https://github.com/apache/spark/pull/30650#pullrequestreview-563196665",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dd8836da-638c-4b80-a459-fa72485c4cfa",
        "parentId" : null,
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "Do this for barrier tasks only if schedule goes through ?",
        "createdAt" : "2020-12-23T19:03:03Z",
        "updatedAt" : "2021-02-18T15:35:53Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "0e6103b4-737c-4dc9-819a-ecbd58070faf",
        "parentId" : "dd8836da-638c-4b80-a459-fa72485c4cfa",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "This's the legacy delay scheduling behavior(disabled by default). In the new delay scheduling, we do reset only if all barrier tasks get scheduled:\r\nhttps://github.com/apache/spark/blob/e3058ba17cb4512537953eb4ded884e24ee93ba2/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala#L589-L593\r\n\r\n(For the barrier taskset, `noDelaySchedulingRejects` means all tasks get scheduled in a single resourceOffers round.)\r\n\r\n\r\nFor the legacy delay scheduling, I'd prefer to keep the same behavior as it is.\r\n",
        "createdAt" : "2020-12-28T12:06:49Z",
        "updatedAt" : "2021-02-18T15:35:53Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "a404e56c-327a-483e-b761-fca699cdd7e8",
        "parentId" : "dd8836da-638c-4b80-a459-fa72485c4cfa",
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "When `legacyLocalityWaitReset` = `true` and barrier task cant be scheduled, there is a behavior change here. Right ? The reset goes through even though nothing was scheduled ?",
        "createdAt" : "2020-12-29T02:19:31Z",
        "updatedAt" : "2021-02-18T15:35:53Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "a9238ffe-b5dd-4a6f-818e-c712d200dc34",
        "parentId" : "dd8836da-638c-4b80-a459-fa72485c4cfa",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "We do reset because we assume the task would get launched like the normal task and let the legacy delay scheduling take effect. The legacy delay scheduling actually doesn't take effect if we don't reset for the barrier task here. Right? Although it actually improves the possibility for the barrier taskset to get launched.\r\n\r\nTo prevent the behavior change, I think we might need to correct the locality level(like reverting the `reset` we did before) if the barrier taskset failed to get launched at the end of a single resourceOffer round. WDYT?\r\n\r\n\r\n(BTW, it's meaningless to reset the timer after all barrier tasks that are already scheduled/launched.)\r\n",
        "createdAt" : "2020-12-30T04:03:47Z",
        "updatedAt" : "2021-02-18T15:35:53Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "54f3fe8e-3d25-4920-83ad-6bf313d0ab55",
        "parentId" : "dd8836da-638c-4b80-a459-fa72485c4cfa",
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "If there are multiple tsm's (stages), the reset will adversely affect the scheduling for the barrier stage, no ?\r\nI agree, once barrier stage is scheduled, it does not matter (but then this wont get triggered anyway in that case if I am not wrong).\r\n\r\n> To prevent the behavior change, I think we might need to correct the locality level(like reverting the reset we did before) if the barrier taskset failed to get launched at the end of a single resourceOffer round. WDYT?\r\n\r\nI agree, this should be sufficient.",
        "createdAt" : "2020-12-30T20:48:45Z",
        "updatedAt" : "2021-02-18T15:35:53Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "78637016-266f-4c48-a317-f44cacd129df",
        "parentId" : "dd8836da-638c-4b80-a459-fa72485c4cfa",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "> If there are multiple tsm's (stages), the reset will adversely affect the scheduling for the barrier stage, no ?\r\n\r\nNo. The reset should only affect the tsm itself.",
        "createdAt" : "2021-01-04T05:24:14Z",
        "updatedAt" : "2021-02-18T15:35:53Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "78226c76-994a-4d0d-808f-2aacd30f9bb2",
        "parentId" : "dd8836da-638c-4b80-a459-fa72485c4cfa",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "So as @mridulm pointed out, since the legacy locality wait is reset  every time we could schedule something I think that means you will never look at other levels, correct?  I think this was behavior before but that doesn't seem right. I think that means you could have enough slots available and some are node local but you won't ever be able to use the ones that aren't.  Please correct me if I'm wrong.  I guess that could be separate issue since same behavior as before.\r\n",
        "createdAt" : "2021-01-05T15:17:11Z",
        "updatedAt" : "2021-02-18T15:35:53Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "9c2ec416-b21c-48f0-9b15-fc9e2e74b8d5",
        "parentId" : "dd8836da-638c-4b80-a459-fa72485c4cfa",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "> So as @mridulm pointed out, since the legacy locality wait is reset every time we could schedule something I think that means you will never look at other levels, correct?\r\n\r\nRight (although there's a corner case that we could look at other levels when the current level doesn't have any pending tasks). So in legacy mode, it's very strict for the barrier tsm to get launched.\r\n\r\n> I think this was behavior before but that doesn't seem right.\r\n\r\nI think that's why we have the new version of delay scheduling.",
        "createdAt" : "2021-01-07T03:42:55Z",
        "updatedAt" : "2021-02-18T15:35:53Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "ca14a419-7f11-42ca-95bf-2ae5025e1b39",
        "parentId" : "dd8836da-638c-4b80-a459-fa72485c4cfa",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "> I think that means you could have enough slots available and some are node local but you won't ever be able to use the ones that aren't.\r\n\r\nThis may lead to the barrier tsm gets hang? (But if we don't revert the locality level at the time we reverting resources, it would not hang since the level will go up as time goes by.)",
        "createdAt" : "2021-01-07T04:02:35Z",
        "updatedAt" : "2021-02-18T15:35:53Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "fead0790b2ed5bc27abf6f46de3ae8e84f3ac640",
    "line" : 133,
    "diffHunk" : "@@ -1,1 +456,460 @@            dequeuedTaskIndex = Some(index)\n            if (legacyLocalityWaitReset && maxLocality != TaskLocality.NO_PREF) {\n              resetDelayScheduleTimer(Some(taskLocality))\n            }\n            if (isBarrier) {"
  },
  {
    "id" : "62c27558-2d18-4974-8dea-dff3c969f454",
    "prId" : 29788,
    "prUrl" : "https://github.com/apache/spark/pull/29788#pullrequestreview-491637014",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b83bd1fd-a67e-4a3e-9c12-df547adf8eb7",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "I am wondering if we should instead pattern match in a separate arm like: \r\n\r\n```\r\n_ @ ExecutorDecommission => false\r\n```\r\n\r\nTo avoid having to change the case arms when we make changes to the structure definitions.",
        "createdAt" : "2020-09-18T17:26:37Z",
        "updatedAt" : "2020-09-18T17:38:05Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      }
    ],
    "commit" : "9bebdd4f2846f12f8ab13279c8cece151e8edfd0",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +992,996 @@      val exitCausedByApp: Boolean = reason match {\n        case exited: ExecutorExited => exited.exitCausedByApp\n        case ExecutorKilled | ExecutorDecommission(_, _) => false\n        case ExecutorProcessLost(_, _, false) => false\n        case _ => true"
  },
  {
    "id" : "61c3a067-63aa-4871-b06c-932636bccfc4",
    "prId" : 29276,
    "prUrl" : "https://github.com/apache/spark/pull/29276#pullrequestreview-458539050",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "41b317db-7e74-423e-8e46-f11026e78fa7",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "This makes sense to me, as shuffle map task returns `MapStatus`. We won't keep the task results, we just update map output tracker, update accumulators, update metrics, and throw the task result away. So we don't need to sum the tasks result size and check it.\r\n\r\nWhat do you think? @tgravescs @squito @holdenk  @jiangxb1987 @Ngone51 ",
        "createdAt" : "2020-07-29T03:16:57Z",
        "updatedAt" : "2020-08-11T03:01:04Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "408563d2-e250-44a2-bc22-a601c0aff36c",
        "parentId" : "41b317db-7e74-423e-8e46-f11026e78fa7",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "According to the JIRA, does this affect only 3.0.0?",
        "createdAt" : "2020-07-29T03:18:16Z",
        "updatedAt" : "2020-08-11T03:01:04Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "67818f3e-7a6a-4722-8085-52c3b290cc46",
        "parentId" : "41b317db-7e74-423e-8e46-f11026e78fa7",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "It affects all the versions, but it's not a serious problem (users can set a very high max result size conf). I think we don't need to backport.",
        "createdAt" : "2020-07-29T03:21:07Z",
        "updatedAt" : "2020-08-11T03:01:04Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "954c33fc-9db7-4c30-b51e-593bf2032f4a",
        "parentId" : "41b317db-7e74-423e-8e46-f11026e78fa7",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Yeah, make sense to me, too.",
        "createdAt" : "2020-07-29T03:34:18Z",
        "updatedAt" : "2020-08-11T03:01:04Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "f361ac46-d540-4804-99c8-7e5c1f0256a7",
        "parentId" : "41b317db-7e74-423e-8e46-f11026e78fa7",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "yes this seems reasonable.  It would be nice to update the comment on the function as well as put description in jira and in this PR to include more details about the shuffle map status and I assume you actually his this case in an application.",
        "createdAt" : "2020-07-29T14:18:24Z",
        "updatedAt" : "2020-08-11T03:01:04Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "e75b4bf8-c749-4c63-9f30-c8f323e93104",
        "parentId" : "41b317db-7e74-423e-8e46-f11026e78fa7",
        "authorId" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "body" : "LGTM too",
        "createdAt" : "2020-07-29T20:36:10Z",
        "updatedAt" : "2020-08-11T03:01:04Z",
        "lastEditedBy" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "tags" : [
        ]
      },
      {
        "id" : "fd4e41d4-95b5-45b5-90d3-5c0d2861906e",
        "parentId" : "41b317db-7e74-423e-8e46-f11026e78fa7",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "But seems we will save the MapStatus in MapOutputTracker. Don't we?",
        "createdAt" : "2020-07-29T22:09:53Z",
        "updatedAt" : "2020-08-11T03:01:04Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "7d842d4e-9e37-4041-af21-2872dc4df758",
        "parentId" : "41b317db-7e74-423e-8e46-f11026e78fa7",
        "authorId" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "body" : "Then we can count the MapStatus size only in the total task result size?",
        "createdAt" : "2020-07-29T23:41:05Z",
        "updatedAt" : "2020-08-11T03:01:04Z",
        "lastEditedBy" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "tags" : [
        ]
      },
      {
        "id" : "6cda80c1-9085-412d-badc-214598c43b54",
        "parentId" : "41b317db-7e74-423e-8e46-f11026e78fa7",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "> But seems we will save the MapStatus in MapOutputTracker. Don't we?\r\n\r\nWe do, but it's a global thing. Limiting the total size of one stage doesn't help too much. I believe the check here was to prevent too much data in `rdd.collect`.",
        "createdAt" : "2020-07-30T14:57:51Z",
        "updatedAt" : "2020-08-11T03:01:04Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "eab35886-7f00-48be-bc67-629a532d873b",
        "parentId" : "41b317db-7e74-423e-8e46-f11026e78fa7",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "OK, make sense.",
        "createdAt" : "2020-07-30T15:56:06Z",
        "updatedAt" : "2020-08-11T03:01:04Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "e1e53ef258013df2f85573de4542d36105ef852b",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +699,703 @@    totalResultSize += size\n    calculatedTasks += 1\n    if (!isShuffleMapTasks && maxResultSize > 0 && totalResultSize > maxResultSize) {\n      val msg = s\"Total size of serialized results of ${calculatedTasks} tasks \" +\n        s\"(${Utils.bytesToString(totalResultSize)}) is bigger than ${config.MAX_RESULT_SIZE.key} \" +"
  },
  {
    "id" : "b1b85476-5fe1-4701-8c88-89938c0e04a4",
    "prId" : 28994,
    "prUrl" : "https://github.com/apache/spark/pull/28994#pullrequestreview-450384670",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a49b2245-7d8c-4ce3-9957-d537bbaff437",
        "parentId" : null,
        "authorId" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "body" : "how about recordsWritten? Should that also be considered wrt progress same wrt shuffleRecordsWritten?",
        "createdAt" : "2020-07-17T05:38:38Z",
        "updatedAt" : "2020-07-17T05:42:18Z",
        "lastEditedBy" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "tags" : [
        ]
      },
      {
        "id" : "eaf61e47-8fd1-49ec-b3f4-e37a7df2adca",
        "parentId" : "a49b2245-7d8c-4ce3-9957-d537bbaff437",
        "authorId" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "body" : "Even cache can also take time when written to disk, does that need to be taken into consideration? Similarly GC time, shuffle read blocked time etc. could also impact task progress",
        "createdAt" : "2020-07-17T05:41:25Z",
        "updatedAt" : "2020-07-17T05:42:18Z",
        "lastEditedBy" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "tags" : [
        ]
      }
    ],
    "commit" : "7519ef5fdc91a7b0f051a97de73055e0506fba47",
    "line" : 105,
    "diffHunk" : "@@ -1,1 +1173,1177 @@        if (task.inputMetrics != null) {\n          sumInputRecords += task.inputMetrics.recordsRead\n        }\n        if (task.shuffleReadMetrics != null) {\n          sumShuffleReadRecords += task.shuffleReadMetrics.recordsRead"
  },
  {
    "id" : "80282d14-0ac1-4745-a6a9-771c95163d0e",
    "prId" : 28994,
    "prUrl" : "https://github.com/apache/spark/pull/28994#pullrequestreview-450384670",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "938180f6-0fe2-4a8a-8929-7ca1340ffc3f",
        "parentId" : null,
        "authorId" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "body" : "would it make sense to add taskProgress as part of taskMetrics that way it can also be shown in SparkUI? Although taskProgress for tasks which doesn't involve input/output/shuffle records would be hard to measure?",
        "createdAt" : "2020-07-17T05:40:13Z",
        "updatedAt" : "2020-07-17T05:42:18Z",
        "lastEditedBy" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "tags" : [
        ]
      }
    ],
    "commit" : "7519ef5fdc91a7b0f051a97de73055e0506fba47",
    "line" : 128,
    "diffHunk" : "@@ -1,1 +1196,1200 @@        taskData(tid).taskMetrics.isDefined) {\n        val taskMetrics = taskData(tid).taskMetrics.get\n        val currentTaskProgressRate = (taskMetrics.inputMetrics.recordsRead +\n          taskMetrics.shuffleReadMetrics.recordsRead) / (runtimeMs / 1000.0)\n        val progressThreshold ="
  },
  {
    "id" : "613f59df-cb1e-4420-a9f4-c0a6a215d6e4",
    "prId" : 28656,
    "prUrl" : "https://github.com/apache/spark/pull/28656#pullrequestreview-419724446",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3147e6cd-39db-49a3-ab6a-22f5d69c09f5",
        "parentId" : null,
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "I think we should change this to only happen when executorAdded. this is also called on lost and decommission and it doesn't make sense to go to \"lower\" level.  Note we may want to stay away from saying higher in the comment below. The code values, lower is actually more strict - meaning process is lowest value. so perhaps don't say higher or lower but say more local or less local.  Perhaps pass in parameter from executorAdded.",
        "createdAt" : "2020-05-27T22:24:59Z",
        "updatedAt" : "2020-05-29T07:49:36Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "5c7ba50b-f9ab-4770-9626-5637f724cd5d",
        "parentId" : "3147e6cd-39db-49a3-ab6a-22f5d69c09f5",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "> I think we should change this to only happen when executorAdded. this is also called on lost and decommission and it doesn't make sense to go to \"lower\" level. \r\n\r\nI think it's impossible to go to \"lower\" or more local level in case of lost and decommission. Lost and decommission would remove executors, so the locality levels can only be less compared to the previous locality levels. It also means, lost and decommission will not add new more local levels.",
        "createdAt" : "2020-05-28T01:50:07Z",
        "updatedAt" : "2020-05-29T07:49:36Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "430c8ef9-82af-4eff-b1b8-25c5d837b160",
        "parentId" : "3147e6cd-39db-49a3-ab6a-22f5d69c09f5",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "> so perhaps don't say higher or lower but say more local or less local.\r\n\r\nYea, good point!",
        "createdAt" : "2020-05-28T01:51:04Z",
        "updatedAt" : "2020-05-29T07:49:36Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "8744f1ec02db28975a80410b5a01d2c2557ad216",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +1114,1118 @@    localityWaits = myLocalityLevels.map(getLocalityWait)\n    currentLocalityIndex = getLocalityIndex(previousLocalityLevel)\n    if (currentLocalityIndex > previousLocalityIndex) {\n      // SPARK-31837: If the new level is more local, shift to the new most local locality\n      // level in terms of better data locality. For example, say the previous locality"
  },
  {
    "id" : "765a1cef-cda0-49b7-827f-516247adfc7a",
    "prId" : 28656,
    "prUrl" : "https://github.com/apache/spark/pull/28656#pullrequestreview-421012127",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8c441afe-6490-40cf-8a7f-79ac190d9c6e",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Hi all, there's a defect in the previous implement(always reset `currentLocalityIndex` to 0). Think about such a case, say we have locality levels [PROCESS, NODE, ANY] and current locality level is ANY. After recompute, we might have locality levels [PROCESS, NODE, RACK, ANY]. In this case, I think we'd better shift to RACK level instead of  PROCESS level, since the TaskSetManager has been already delayed for a while on known levels(PROCESS, NODE). So with this update, I think it could also ease our concern on the possible perf regression introduced by aggressive locality level resetting. @bmarcott @tgravescs @cloud-fan ",
        "createdAt" : "2020-05-29T07:58:32Z",
        "updatedAt" : "2020-05-29T14:25:36Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "9fd24bb0-bbef-42bf-a0bb-752b404e1477",
        "parentId" : "8c441afe-6490-40cf-8a7f-79ac190d9c6e",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "yes this is one of the cases I was referring to. Ideally you would never run into this case because a host is on a rack so you would always have it. Unfortunately Spark defaults the rack to None so you can. I was going to improve upon it in the jira I filed. We can certainly handle some here if you want",
        "createdAt" : "2020-05-29T14:19:42Z",
        "updatedAt" : "2020-05-29T14:19:43Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "a5581087-99c3-4cca-b673-1bfa2104996f",
        "parentId" : "8c441afe-6490-40cf-8a7f-79ac190d9c6e",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "> We can certainly handle some here if you want\r\n\r\nWhat do you mean by \"handle some here\"? I read your JIRA and don't find the specific solution that could be added to this PR. Could you please elaborate more? ",
        "createdAt" : "2020-05-29T14:28:53Z",
        "updatedAt" : "2020-05-29T14:28:53Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "c6f08691-db09-4120-8a83-655a8d0a4f29",
        "parentId" : "8c441afe-6490-40cf-8a7f-79ac190d9c6e",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "I just mean there are a bunch of corner cases and I don't think resetting this on every executor added is ideal.  I did not list out all of them.  On Yarn it actually defaults to default-rack rather then None so it actually won't have this issue because every node has a rack.  I agree that the code you have here is an improvement to handle the rack case.",
        "createdAt" : "2020-05-29T14:33:26Z",
        "updatedAt" : "2020-05-29T14:33:26Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "8744f1ec02db28975a80410b5a01d2c2557ad216",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +1119,1123 @@      // levels are [PROCESS, NODE, ANY] and current level is ANY. After recompute, the\n      // locality levels are [PROCESS, NODE, RACK, ANY]. Then, we'll shift to RACK level.\n      currentLocalityIndex = getLocalityIndex(myLocalityLevels.diff(previousMyLocalityLevels).head)\n    }\n  }"
  },
  {
    "id" : "b8520b42-37d2-4749-89fd-2c538a770aea",
    "prId" : 28619,
    "prUrl" : "https://github.com/apache/spark/pull/28619#pullrequestreview-418831153",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ef55782e-042c-4b6e-9cc7-7d0a99b78370",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "We need to remove the corresponding task from `tidToExecutorKillTimeMapping` if `speculated` is true?",
        "createdAt" : "2020-05-26T05:51:34Z",
        "updatedAt" : "2020-06-15T14:14:24Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "dec43382-7cf3-4801-94d7-930246b85706",
        "parentId" : "ef55782e-042c-4b6e-9cc7-7d0a99b78370",
        "authorId" : "ad4cb313-47b2-4cfa-9c1f-0a3596794a67",
        "body" : "We are removing entries from tidToExecutorKillTimeMapping at time when it is removed from `runningTasksSet`.",
        "createdAt" : "2020-05-26T15:03:36Z",
        "updatedAt" : "2020-06-15T14:14:24Z",
        "lastEditedBy" : "ad4cb313-47b2-4cfa-9c1f-0a3596794a67",
        "tags" : [
        ]
      },
      {
        "id" : "4f6ca1b6-b55d-45de-9eef-6d06d5649a26",
        "parentId" : "ef55782e-042c-4b6e-9cc7-7d0a99b78370",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Then, is it possible that we add redundant speculate tasks for the same tid?",
        "createdAt" : "2020-05-26T15:38:56Z",
        "updatedAt" : "2020-06-15T14:14:24Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "f47a5700-430e-4dcf-84ff-0277a67cc94d",
        "parentId" : "ef55782e-042c-4b6e-9cc7-7d0a99b78370",
        "authorId" : "ad4cb313-47b2-4cfa-9c1f-0a3596794a67",
        "body" : "The `copiesRunning(index) == 1` condition inside `checkAndSubmitSpeculatableTask` will prevent it. Irrespective of executor decommissioning, that check makes sure that only 1 speculatable task can run for each unique index.",
        "createdAt" : "2020-05-26T17:09:14Z",
        "updatedAt" : "2020-06-15T14:14:24Z",
        "lastEditedBy" : "ad4cb313-47b2-4cfa-9c1f-0a3596794a67",
        "tags" : [
        ]
      },
      {
        "id" : "ab84ab4e-6b71-4439-b7c7-e900d380283b",
        "parentId" : "ef55782e-042c-4b6e-9cc7-7d0a99b78370",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "I mean if it's possible to add redundant speculate tasks to `speculatableTasks` rather than running multiple  speculate tasks for the same tid at the same time.\r\n\r\nI think the actual prevention is `!speculatableTasks.contains(index)` in `checkAndSubmitSpeculatableTask`, but never mind since `speculatableTasks` is a `HashSet`.",
        "createdAt" : "2020-05-27T03:22:35Z",
        "updatedAt" : "2020-06-15T14:14:24Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "d87b311be85819ae884e2a24d94926fdd51165de",
    "line" : 48,
    "diffHunk" : "@@ -1,1 +1056,1060 @@            taskEndTimeBasedOnMedianDuration\n          if (canExceedDeadline) {\n            speculated = checkAndSubmitSpeculatableTask(tid, time, 0)\n          }\n        }"
  },
  {
    "id" : "dcec1d24-0dcd-4f8c-b266-d6445c647aed",
    "prId" : 28619,
    "prUrl" : "https://github.com/apache/spark/pull/28619#pullrequestreview-430685101",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bbb1f727-18f9-4dfb-a609-bb4910913a3f",
        "parentId" : null,
        "authorId" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "body" : "I am just curious why this solution (introducing the kill time per task ID map: `tidToExecutorKillTimeMapping`) is chosen over storing the kill time per executor (`executorToKillTimeMapping` or something like that). In the later case here would be  something like: \r\n\r\n```scala\r\nif (!speculated && executorToKillTimeMapping.nonEmpty) {\r\n  val taskInfo = taskInfos(tid)\r\n  executorToKillTimeMapping.get(taskInfo.executorId).foreach {  executorKillTime =>\r\n  ...\r\n  }\r\n}\r\n```",
        "createdAt" : "2020-06-15T11:24:00Z",
        "updatedAt" : "2020-06-15T14:14:24Z",
        "lastEditedBy" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "tags" : [
        ]
      },
      {
        "id" : "1d37e673-ea65-4bf8-8be6-5138305f1c19",
        "parentId" : "bbb1f727-18f9-4dfb-a609-bb4910913a3f",
        "authorId" : "ad4cb313-47b2-4cfa-9c1f-0a3596794a67",
        "body" : "Yes - We can store executorToKillTimeMapping also. In that case, we have to clean the map when executor is removed. For that we may have to make changes to pass \"removeExecutor\" signal till TaskSetManager.\r\n\r\nI didn't see much difference in both the approaches as total entries we will be creating for a given executor in current approach will be equal to total tasks running for that executor (which shouldn't be a lot).",
        "createdAt" : "2020-06-15T14:14:34Z",
        "updatedAt" : "2020-06-15T14:14:35Z",
        "lastEditedBy" : "ad4cb313-47b2-4cfa-9c1f-0a3596794a67",
        "tags" : [
        ]
      }
    ],
    "commit" : "d87b311be85819ae884e2a24d94926fdd51165de",
    "line" : 40,
    "diffHunk" : "@@ -1,1 +1048,1052 @@      for (tid <- runningTasksSet) {\n        var speculated = checkAndSubmitSpeculatableTask(tid, time, threshold)\n        if (!speculated && tidToExecutorKillTimeMapping.contains(tid)) {\n          // Check whether this task will finish before the exectorKillTime assuming\n          // it will take medianDuration overall. If this task cannot finish within"
  },
  {
    "id" : "b24d4ece-ffe1-4aae-bbfb-843b51b023c1",
    "prId" : 27871,
    "prUrl" : "https://github.com/apache/spark/pull/27871#pullrequestreview-375580257",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b85798ef-3649-4521-9c87-c802fc0c78cb",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Although we has a similar one inside `AppStatusListener` already, this means another implicit dependency to `sql` module.",
        "createdAt" : "2020-03-16T21:00:58Z",
        "updatedAt" : "2020-03-16T21:00:58Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "5c9bb717-d175-47e9-9407-bb3d9ab48e0d",
        "parentId" : "b85798ef-3649-4521-9c87-c802fc0c78cb",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "BTW, you may want to define `private val SQL_EXECUTION_ID_KEY = \"spark.sql.execution.id\"` like `AppStatusListener`. Or, if we need this, we may need to define this at the more higher common place.",
        "createdAt" : "2020-03-16T21:02:21Z",
        "updatedAt" : "2020-03-16T21:02:21Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "001b2340859c02be845598869bacc7e96ac34d2f",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +113,117 @@  val minShare = 0\n  val priority = if (taskSet.properties != null) {\n    taskSet.properties.getProperty(\"spark.sql.execution.id\", \"0\").toLong\n  } else 0L\n  val jobId = taskSet.priority"
  },
  {
    "id" : "1df74710-a88c-41b5-b0cb-b293fce0d5fb",
    "prId" : 27636,
    "prUrl" : "https://github.com/apache/spark/pull/27636#pullrequestreview-367204848",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7e314991-452d-4c75-ab76-5b20e081aed8",
        "parentId" : null,
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "can you clarify?",
        "createdAt" : "2020-02-28T20:23:31Z",
        "updatedAt" : "2020-06-11T10:43:18Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "8c1445b6-585d-41a6-a559-ad2f11e84de8",
        "parentId" : "7e314991-452d-4c75-ab76-5b20e081aed8",
        "authorId" : "11f6c849-b5b9-4c65-990c-525cf572b913",
        "body" : "added the description in the code",
        "createdAt" : "2020-03-02T13:41:19Z",
        "updatedAt" : "2020-06-11T10:43:18Z",
        "lastEditedBy" : "11f6c849-b5b9-4c65-990c-525cf572b913",
        "tags" : [
        ]
      }
    ],
    "commit" : "c476e5271ca8b8fd3c401980a080334b8d7b9a36",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +816,820 @@            fetchFailed.bmAddress.host, fetchFailed.bmAddress.executorId))\n\n          // Do account fetch failure exception raised by decommissioned\n          // node against stage failure. Here the logic is to specify,\n          // if the task failed due to fetchFailed of decommission nodes than"
  },
  {
    "id" : "0abb3557-75c2-48cc-a833-ee8986b8272b",
    "prId" : 27455,
    "prUrl" : "https://github.com/apache/spark/pull/27455#pullrequestreview-353723549",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "425d45f8-50f3-4a01-b8f0-148e15baf9a6",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "A zombie TSM may call this while handling failed task.",
        "createdAt" : "2020-02-05T08:43:15Z",
        "updatedAt" : "2020-02-05T13:59:26Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "84da7270-3ccb-48b2-a886-4225e3aca608",
        "parentId" : "425d45f8-50f3-4a01-b8f0-148e15baf9a6",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "can you put it as code comment?",
        "createdAt" : "2020-02-05T13:44:28Z",
        "updatedAt" : "2020-02-05T13:59:26Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "91b83d2425ab9c5e419be1fa029f38293e542871",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +231,235 @@      speculatable: Boolean = false): Unit = {\n    // A zombie TaskSetManager may reach here while handling failed task.\n    if (isZombie) return\n    val pendingTaskSetToAddTo = if (speculatable) pendingSpeculatableTasks else pendingTasks\n    for (loc <- tasks(index).preferredLocations) {"
  },
  {
    "id" : "b8f0954e-1ef8-44f6-9a26-dd1dd63b2821",
    "prId" : 27455,
    "prUrl" : "https://github.com/apache/spark/pull/27455#pullrequestreview-353548377",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a68f1152-59e3-4847-9eef-b7aba743f2bf",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "A zombie TSM may call this while `executorLost` happens.",
        "createdAt" : "2020-02-05T08:44:18Z",
        "updatedAt" : "2020-02-05T13:59:26Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "638b0249-22a8-445d-a04f-6450c23beebb",
        "parentId" : "a68f1152-59e3-4847-9eef-b7aba743f2bf",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "shall we avoid calling `executorLost` for zombie TSM?",
        "createdAt" : "2020-02-05T08:48:10Z",
        "updatedAt" : "2020-02-05T13:59:26Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "c4391f2b-18f5-440a-aa98-319998c71b3a",
        "parentId" : "a68f1152-59e3-4847-9eef-b7aba743f2bf",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "We need it because there may be some running tasks on that lost executor for the zombie TSM and the zombie TSM should handle them properly as failed tasks:\r\nhttps://github.com/apache/spark/blob/e1ea806b3075d279b5f08a29fe4c1ad6d3c4191a/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala#L938-L948",
        "createdAt" : "2020-02-05T08:53:17Z",
        "updatedAt" : "2020-02-05T13:59:26Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "91b83d2425ab9c5e419be1fa029f38293e542871",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +1086,1090 @@  def recomputeLocality(): Unit = {\n    // A zombie TaskSetManager may reach here while executorLost happens\n    if (isZombie) return\n    val previousLocalityLevel = myLocalityLevels(currentLocalityIndex)\n    myLocalityLevels = computeValidLocalityLevels()"
  },
  {
    "id" : "8cef00d3-8d30-43bb-9c75-f7ce132b2bd7",
    "prId" : 27455,
    "prUrl" : "https://github.com/apache/spark/pull/27455#pullrequestreview-353723629",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1d811805-e6ce-4c88-b3ef-dc078366558b",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "ditto",
        "createdAt" : "2020-02-05T13:44:35Z",
        "updatedAt" : "2020-02-05T13:59:26Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "91b83d2425ab9c5e419be1fa029f38293e542871",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +1086,1090 @@  def recomputeLocality(): Unit = {\n    // A zombie TaskSetManager may reach here while executorLost happens\n    if (isZombie) return\n    val previousLocalityLevel = myLocalityLevels(currentLocalityIndex)\n    myLocalityLevels = computeValidLocalityLevels()"
  },
  {
    "id" : "4b717337-dad2-4d96-b7cc-fac2199c55f5",
    "prId" : 27207,
    "prUrl" : "https://github.com/apache/spark/pull/27207#pullrequestreview-353075864",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b9c93528-3175-4c11-a2a6-0eeb1f2e70d5",
        "parentId" : null,
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "add @return s and describe return values",
        "createdAt" : "2020-02-04T16:12:04Z",
        "updatedAt" : "2020-04-03T05:29:21Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "24c8ad9ae23d360c7bb5bc813d04e6f5f6715d93",
    "line" : 33,
    "diffHunk" : "@@ -1,1 +405,409 @@   * @param execId the executor Id of the offered resource\n   * @param host  the host Id of the offered resource\n   * @param maxLocality the maximum locality we want to schedule the tasks at\n   *\n   * @return Tuple containing:"
  },
  {
    "id" : "2b27740b-68a7-4561-ab84-8a2b2ff8368a",
    "prId" : 27207,
    "prUrl" : "https://github.com/apache/spark/pull/27207#pullrequestreview-366940362",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c8f63c0c-e4ea-4074-8429-b3700be4138a",
        "parentId" : null,
        "authorId" : "29a3e834-d9e3-42dc-ab25-20ba332ef3f2",
        "body" : "@tgravescs \r\nIt turns out that AFAICT there is no case that would hit empty task description, ANY locality, no pending tasks, yet there are pending speculative tasks, because once there are no regular pending tasks, the speculative tasks don't follow delay scheduling (you'd only get empty task if there were none pending). The locality level is only determined by non speculative pending tasks. See [here](https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala#L543-L546) \r\n\r\nI ended up not adding a test because it was complex and ugly (given it isn't currently possible).\r\nIt required overriding internal fields of TSM in odd ways.\r\nThink it is better to add an ugly test, take out the check on speculative tasks, or leave as is?\r\n",
        "createdAt" : "2020-03-02T04:03:19Z",
        "updatedAt" : "2020-04-03T05:29:22Z",
        "lastEditedBy" : "29a3e834-d9e3-42dc-ab25-20ba332ef3f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "24c8ad9ae23d360c7bb5bc813d04e6f5f6715d93",
    "line" : 78,
    "diffHunk" : "@@ -1,1 +494,498 @@          serializedTask)\n      }\n      val hasPendingTasks = pendingTasks.all.nonEmpty || pendingSpeculatableTasks.all.nonEmpty\n      val hasScheduleDelayReject =\n        taskDescription.isEmpty &&"
  },
  {
    "id" : "ef989807-a6ce-46b5-9063-d8c557bd8fb3",
    "prId" : 27126,
    "prUrl" : "https://github.com/apache/spark/pull/27126#pullrequestreview-339873244",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e2a11fc7-31c5-44e9-8842-81d28cc770a9",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Will this case fails this check:\r\n\r\nhttps://github.com/apache/spark/blob/e1ea806b3075d279b5f08a29fe4c1ad6d3c4191a/core/src/main/scala/org/apache/spark/SparkContext.scala#L2737\r\n\r\nIt seems `checkResourcesPerTask` also doesn't handle this case properly. So, `CPUS_PER_TASK` greater than 1 should fail the check. \r\n",
        "createdAt" : "2020-01-08T07:40:05Z",
        "updatedAt" : "2020-01-08T07:40:09Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "fcba4165-6f0b-4d85-b1c9-dbd062050aa8",
        "parentId" : "e2a11fc7-31c5-44e9-8842-81d28cc770a9",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "I'm fixing that issue with https://issues.apache.org/jira/browse/SPARK-30446",
        "createdAt" : "2020-01-08T13:47:55Z",
        "updatedAt" : "2020-01-08T13:48:01Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "5299913e8f42785ba6e5f7a4965cd9ffbab99812",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +91,95 @@  // the value of the conf would 1 by default. However, the executor would use all the cores on\n  // the worker. Therefore, CPUS_PER_TASK is okay to be greater than 1 without setting #cores.\n  // To handle this case, we assume the minimum number of slots is 1.\n  // TODO: use the actual number of slots for standalone mode.\n  val speculationTasksLessEqToSlots ="
  },
  {
    "id" : "8e886309-10c6-444e-8845-9b1b9365dcfd",
    "prId" : 27072,
    "prUrl" : "https://github.com/apache/spark/pull/27072#pullrequestreview-360850500",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "01c3cfd5-44af-4e9c-bbce-2b29679563a0",
        "parentId" : null,
        "authorId" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "body" : "This log info mainly inform the user that we won't re=execute the failed task, it makes little sense to differentiate between FetchFailed tasks and speculative tasks. ",
        "createdAt" : "2020-02-19T05:28:54Z",
        "updatedAt" : "2020-02-19T05:28:55Z",
        "lastEditedBy" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "0a46659526250dc7c2de4b677e2781cfcef3cc81",
    "line" : 55,
    "diffHunk" : "@@ -1,1 +890,894 @@    }\n\n    if (fetchFailedIndex.contains(index)) {\n      logInfo(s\"Task ${info.id} in stage ${taskSet.id} (TID $tid) failed (because the task \" +\n        s\"failed with a shuffle data fetch failure, so the previous stage needs to be re-run).\")"
  },
  {
    "id" : "4b9d8ef4-317c-4974-a5bb-1fddb10dd839",
    "prId" : 26975,
    "prUrl" : "https://github.com/apache/spark/pull/26975#pullrequestreview-336367343",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "35bc1108-4442-412d-9a3b-b7cb7839a5e1",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "let's add some code comments here to explain what's going on. e.g. we may have a running task whose partition has been marked as successful, because this partition has another task in another stage attempt.",
        "createdAt" : "2019-12-25T13:47:34Z",
        "updatedAt" : "2019-12-30T03:10:12Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "8678d4c8-13d2-42b8-97c9-513dd98dc8d5",
        "parentId" : "35bc1108-4442-412d-9a3b-b7cb7839a5e1",
        "authorId" : "78c98141-e5a6-4d72-a74b-e121b62fdbf4",
        "body" : "ok, tks!",
        "createdAt" : "2019-12-25T14:06:21Z",
        "updatedAt" : "2019-12-30T03:10:12Z",
        "lastEditedBy" : "78c98141-e5a6-4d72-a74b-e121b62fdbf4",
        "tags" : [
        ]
      }
    ],
    "commit" : "92e30c3d4ae52a8ce78bc10328446c5149a38e55",
    "line" : 8,
    "diffHunk" : "@@ -1,1 +943,947 @@        // this partition has another task completed in another stage attempt.\n        // We treat it as a running task and will call handleFailedTask later.\n        if (successful(index) && !info.running && !killedByOtherAttempt.contains(tid)) {\n          successful(index) = false\n          copiesRunning(index) -= 1"
  },
  {
    "id" : "2c27f158-50ab-4e05-bec4-adb11c0fd7b4",
    "prId" : 24374,
    "prUrl" : "https://github.com/apache/spark/pull/24374#pullrequestreview-241563342",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0e907d17-6109-4bc4-95a8-462624eff53f",
        "parentId" : null,
        "authorId" : "0c812942-02cb-4975-9748-394d1387affa",
        "body" : "Can we use `immutable.Map` instead of `Map` in this file? It should really be `Map` and `mutable.*` for others.",
        "createdAt" : "2019-05-24T06:20:16Z",
        "updatedAt" : "2019-06-04T21:38:24Z",
        "lastEditedBy" : "0c812942-02cb-4975-9748-394d1387affa",
        "tags" : [
        ]
      }
    ],
    "commit" : "82cd1e35bdc7caaddb4122060d8fd6b98893cbb6",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +22,26 @@import java.util.concurrent.ConcurrentLinkedQueue\n\nimport scala.collection.immutable.Map\nimport scala.collection.mutable.{ArrayBuffer, HashMap, HashSet}\nimport scala.math.max"
  }
]