[
  {
    "id" : "6fb869ec-40b5-41dd-89f4-3d7e8097c0f4",
    "prId" : 30650,
    "prUrl" : "https://github.com/apache/spark/pull/30650#pullrequestreview-563196665",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dd8836da-638c-4b80-a459-fa72485c4cfa",
        "parentId" : null,
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "Do this for barrier tasks only if schedule goes through ?",
        "createdAt" : "2020-12-23T19:03:03Z",
        "updatedAt" : "2021-02-18T15:35:53Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "0e6103b4-737c-4dc9-819a-ecbd58070faf",
        "parentId" : "dd8836da-638c-4b80-a459-fa72485c4cfa",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "This's the legacy delay scheduling behavior(disabled by default). In the new delay scheduling, we do reset only if all barrier tasks get scheduled:\r\nhttps://github.com/apache/spark/blob/e3058ba17cb4512537953eb4ded884e24ee93ba2/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala#L589-L593\r\n\r\n(For the barrier taskset, `noDelaySchedulingRejects` means all tasks get scheduled in a single resourceOffers round.)\r\n\r\n\r\nFor the legacy delay scheduling, I'd prefer to keep the same behavior as it is.\r\n",
        "createdAt" : "2020-12-28T12:06:49Z",
        "updatedAt" : "2021-02-18T15:35:53Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "a404e56c-327a-483e-b761-fca699cdd7e8",
        "parentId" : "dd8836da-638c-4b80-a459-fa72485c4cfa",
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "When `legacyLocalityWaitReset` = `true` and barrier task cant be scheduled, there is a behavior change here. Right ? The reset goes through even though nothing was scheduled ?",
        "createdAt" : "2020-12-29T02:19:31Z",
        "updatedAt" : "2021-02-18T15:35:53Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "a9238ffe-b5dd-4a6f-818e-c712d200dc34",
        "parentId" : "dd8836da-638c-4b80-a459-fa72485c4cfa",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "We do reset because we assume the task would get launched like the normal task and let the legacy delay scheduling take effect. The legacy delay scheduling actually doesn't take effect if we don't reset for the barrier task here. Right? Although it actually improves the possibility for the barrier taskset to get launched.\r\n\r\nTo prevent the behavior change, I think we might need to correct the locality level(like reverting the `reset` we did before) if the barrier taskset failed to get launched at the end of a single resourceOffer round. WDYT?\r\n\r\n\r\n(BTW, it's meaningless to reset the timer after all barrier tasks that are already scheduled/launched.)\r\n",
        "createdAt" : "2020-12-30T04:03:47Z",
        "updatedAt" : "2021-02-18T15:35:53Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "54f3fe8e-3d25-4920-83ad-6bf313d0ab55",
        "parentId" : "dd8836da-638c-4b80-a459-fa72485c4cfa",
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "If there are multiple tsm's (stages), the reset will adversely affect the scheduling for the barrier stage, no ?\r\nI agree, once barrier stage is scheduled, it does not matter (but then this wont get triggered anyway in that case if I am not wrong).\r\n\r\n> To prevent the behavior change, I think we might need to correct the locality level(like reverting the reset we did before) if the barrier taskset failed to get launched at the end of a single resourceOffer round. WDYT?\r\n\r\nI agree, this should be sufficient.",
        "createdAt" : "2020-12-30T20:48:45Z",
        "updatedAt" : "2021-02-18T15:35:53Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "78637016-266f-4c48-a317-f44cacd129df",
        "parentId" : "dd8836da-638c-4b80-a459-fa72485c4cfa",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "> If there are multiple tsm's (stages), the reset will adversely affect the scheduling for the barrier stage, no ?\r\n\r\nNo. The reset should only affect the tsm itself.",
        "createdAt" : "2021-01-04T05:24:14Z",
        "updatedAt" : "2021-02-18T15:35:53Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "78226c76-994a-4d0d-808f-2aacd30f9bb2",
        "parentId" : "dd8836da-638c-4b80-a459-fa72485c4cfa",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "So as @mridulm pointed out, since the legacy locality wait is reset  every time we could schedule something I think that means you will never look at other levels, correct?  I think this was behavior before but that doesn't seem right. I think that means you could have enough slots available and some are node local but you won't ever be able to use the ones that aren't.  Please correct me if I'm wrong.  I guess that could be separate issue since same behavior as before.\r\n",
        "createdAt" : "2021-01-05T15:17:11Z",
        "updatedAt" : "2021-02-18T15:35:53Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "9c2ec416-b21c-48f0-9b15-fc9e2e74b8d5",
        "parentId" : "dd8836da-638c-4b80-a459-fa72485c4cfa",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "> So as @mridulm pointed out, since the legacy locality wait is reset every time we could schedule something I think that means you will never look at other levels, correct?\r\n\r\nRight (although there's a corner case that we could look at other levels when the current level doesn't have any pending tasks). So in legacy mode, it's very strict for the barrier tsm to get launched.\r\n\r\n> I think this was behavior before but that doesn't seem right.\r\n\r\nI think that's why we have the new version of delay scheduling.",
        "createdAt" : "2021-01-07T03:42:55Z",
        "updatedAt" : "2021-02-18T15:35:53Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "ca14a419-7f11-42ca-95bf-2ae5025e1b39",
        "parentId" : "dd8836da-638c-4b80-a459-fa72485c4cfa",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "> I think that means you could have enough slots available and some are node local but you won't ever be able to use the ones that aren't.\r\n\r\nThis may lead to the barrier tsm gets hang? (But if we don't revert the locality level at the time we reverting resources, it would not hang since the level will go up as time goes by.)",
        "createdAt" : "2021-01-07T04:02:35Z",
        "updatedAt" : "2021-02-18T15:35:53Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "fead0790b2ed5bc27abf6f46de3ae8e84f3ac640",
    "line" : 133,
    "diffHunk" : "@@ -1,1 +456,460 @@            dequeuedTaskIndex = Some(index)\n            if (legacyLocalityWaitReset && maxLocality != TaskLocality.NO_PREF) {\n              resetDelayScheduleTimer(Some(taskLocality))\n            }\n            if (isBarrier) {"
  },
  {
    "id" : "62c27558-2d18-4974-8dea-dff3c969f454",
    "prId" : 29788,
    "prUrl" : "https://github.com/apache/spark/pull/29788#pullrequestreview-491637014",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b83bd1fd-a67e-4a3e-9c12-df547adf8eb7",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "I am wondering if we should instead pattern match in a separate arm like: \r\n\r\n```\r\n_ @ ExecutorDecommission => false\r\n```\r\n\r\nTo avoid having to change the case arms when we make changes to the structure definitions.",
        "createdAt" : "2020-09-18T17:26:37Z",
        "updatedAt" : "2020-09-18T17:38:05Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      }
    ],
    "commit" : "9bebdd4f2846f12f8ab13279c8cece151e8edfd0",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +992,996 @@      val exitCausedByApp: Boolean = reason match {\n        case exited: ExecutorExited => exited.exitCausedByApp\n        case ExecutorKilled | ExecutorDecommission(_, _) => false\n        case ExecutorProcessLost(_, _, false) => false\n        case _ => true"
  },
  {
    "id" : "61c3a067-63aa-4871-b06c-932636bccfc4",
    "prId" : 29276,
    "prUrl" : "https://github.com/apache/spark/pull/29276#pullrequestreview-458539050",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "41b317db-7e74-423e-8e46-f11026e78fa7",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "This makes sense to me, as shuffle map task returns `MapStatus`. We won't keep the task results, we just update map output tracker, update accumulators, update metrics, and throw the task result away. So we don't need to sum the tasks result size and check it.\r\n\r\nWhat do you think? @tgravescs @squito @holdenk  @jiangxb1987 @Ngone51 ",
        "createdAt" : "2020-07-29T03:16:57Z",
        "updatedAt" : "2020-08-11T03:01:04Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "408563d2-e250-44a2-bc22-a601c0aff36c",
        "parentId" : "41b317db-7e74-423e-8e46-f11026e78fa7",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "According to the JIRA, does this affect only 3.0.0?",
        "createdAt" : "2020-07-29T03:18:16Z",
        "updatedAt" : "2020-08-11T03:01:04Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "67818f3e-7a6a-4722-8085-52c3b290cc46",
        "parentId" : "41b317db-7e74-423e-8e46-f11026e78fa7",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "It affects all the versions, but it's not a serious problem (users can set a very high max result size conf). I think we don't need to backport.",
        "createdAt" : "2020-07-29T03:21:07Z",
        "updatedAt" : "2020-08-11T03:01:04Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "954c33fc-9db7-4c30-b51e-593bf2032f4a",
        "parentId" : "41b317db-7e74-423e-8e46-f11026e78fa7",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Yeah, make sense to me, too.",
        "createdAt" : "2020-07-29T03:34:18Z",
        "updatedAt" : "2020-08-11T03:01:04Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "f361ac46-d540-4804-99c8-7e5c1f0256a7",
        "parentId" : "41b317db-7e74-423e-8e46-f11026e78fa7",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "yes this seems reasonable.  It would be nice to update the comment on the function as well as put description in jira and in this PR to include more details about the shuffle map status and I assume you actually his this case in an application.",
        "createdAt" : "2020-07-29T14:18:24Z",
        "updatedAt" : "2020-08-11T03:01:04Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "e75b4bf8-c749-4c63-9f30-c8f323e93104",
        "parentId" : "41b317db-7e74-423e-8e46-f11026e78fa7",
        "authorId" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "body" : "LGTM too",
        "createdAt" : "2020-07-29T20:36:10Z",
        "updatedAt" : "2020-08-11T03:01:04Z",
        "lastEditedBy" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "tags" : [
        ]
      },
      {
        "id" : "fd4e41d4-95b5-45b5-90d3-5c0d2861906e",
        "parentId" : "41b317db-7e74-423e-8e46-f11026e78fa7",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "But seems we will save the MapStatus in MapOutputTracker. Don't we?",
        "createdAt" : "2020-07-29T22:09:53Z",
        "updatedAt" : "2020-08-11T03:01:04Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "7d842d4e-9e37-4041-af21-2872dc4df758",
        "parentId" : "41b317db-7e74-423e-8e46-f11026e78fa7",
        "authorId" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "body" : "Then we can count the MapStatus size only in the total task result size?",
        "createdAt" : "2020-07-29T23:41:05Z",
        "updatedAt" : "2020-08-11T03:01:04Z",
        "lastEditedBy" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "tags" : [
        ]
      },
      {
        "id" : "6cda80c1-9085-412d-badc-214598c43b54",
        "parentId" : "41b317db-7e74-423e-8e46-f11026e78fa7",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "> But seems we will save the MapStatus in MapOutputTracker. Don't we?\r\n\r\nWe do, but it's a global thing. Limiting the total size of one stage doesn't help too much. I believe the check here was to prevent too much data in `rdd.collect`.",
        "createdAt" : "2020-07-30T14:57:51Z",
        "updatedAt" : "2020-08-11T03:01:04Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "eab35886-7f00-48be-bc67-629a532d873b",
        "parentId" : "41b317db-7e74-423e-8e46-f11026e78fa7",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "OK, make sense.",
        "createdAt" : "2020-07-30T15:56:06Z",
        "updatedAt" : "2020-08-11T03:01:04Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "e1e53ef258013df2f85573de4542d36105ef852b",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +699,703 @@    totalResultSize += size\n    calculatedTasks += 1\n    if (!isShuffleMapTasks && maxResultSize > 0 && totalResultSize > maxResultSize) {\n      val msg = s\"Total size of serialized results of ${calculatedTasks} tasks \" +\n        s\"(${Utils.bytesToString(totalResultSize)}) is bigger than ${config.MAX_RESULT_SIZE.key} \" +"
  }
]