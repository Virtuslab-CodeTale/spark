[
  {
    "id" : "6fb869ec-40b5-41dd-89f4-3d7e8097c0f4",
    "prId" : 30650,
    "prUrl" : "https://github.com/apache/spark/pull/30650#pullrequestreview-563196665",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "dd8836da-638c-4b80-a459-fa72485c4cfa",
        "parentId" : null,
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "Do this for barrier tasks only if schedule goes through ?",
        "createdAt" : "2020-12-23T19:03:03Z",
        "updatedAt" : "2021-02-18T15:35:53Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "0e6103b4-737c-4dc9-819a-ecbd58070faf",
        "parentId" : "dd8836da-638c-4b80-a459-fa72485c4cfa",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "This's the legacy delay scheduling behavior(disabled by default). In the new delay scheduling, we do reset only if all barrier tasks get scheduled:\r\nhttps://github.com/apache/spark/blob/e3058ba17cb4512537953eb4ded884e24ee93ba2/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala#L589-L593\r\n\r\n(For the barrier taskset, `noDelaySchedulingRejects` means all tasks get scheduled in a single resourceOffers round.)\r\n\r\n\r\nFor the legacy delay scheduling, I'd prefer to keep the same behavior as it is.\r\n",
        "createdAt" : "2020-12-28T12:06:49Z",
        "updatedAt" : "2021-02-18T15:35:53Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "a404e56c-327a-483e-b761-fca699cdd7e8",
        "parentId" : "dd8836da-638c-4b80-a459-fa72485c4cfa",
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "When `legacyLocalityWaitReset` = `true` and barrier task cant be scheduled, there is a behavior change here. Right ? The reset goes through even though nothing was scheduled ?",
        "createdAt" : "2020-12-29T02:19:31Z",
        "updatedAt" : "2021-02-18T15:35:53Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "a9238ffe-b5dd-4a6f-818e-c712d200dc34",
        "parentId" : "dd8836da-638c-4b80-a459-fa72485c4cfa",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "We do reset because we assume the task would get launched like the normal task and let the legacy delay scheduling take effect. The legacy delay scheduling actually doesn't take effect if we don't reset for the barrier task here. Right? Although it actually improves the possibility for the barrier taskset to get launched.\r\n\r\nTo prevent the behavior change, I think we might need to correct the locality level(like reverting the `reset` we did before) if the barrier taskset failed to get launched at the end of a single resourceOffer round. WDYT?\r\n\r\n\r\n(BTW, it's meaningless to reset the timer after all barrier tasks that are already scheduled/launched.)\r\n",
        "createdAt" : "2020-12-30T04:03:47Z",
        "updatedAt" : "2021-02-18T15:35:53Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "54f3fe8e-3d25-4920-83ad-6bf313d0ab55",
        "parentId" : "dd8836da-638c-4b80-a459-fa72485c4cfa",
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "If there are multiple tsm's (stages), the reset will adversely affect the scheduling for the barrier stage, no ?\r\nI agree, once barrier stage is scheduled, it does not matter (but then this wont get triggered anyway in that case if I am not wrong).\r\n\r\n> To prevent the behavior change, I think we might need to correct the locality level(like reverting the reset we did before) if the barrier taskset failed to get launched at the end of a single resourceOffer round. WDYT?\r\n\r\nI agree, this should be sufficient.",
        "createdAt" : "2020-12-30T20:48:45Z",
        "updatedAt" : "2021-02-18T15:35:53Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "78637016-266f-4c48-a317-f44cacd129df",
        "parentId" : "dd8836da-638c-4b80-a459-fa72485c4cfa",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "> If there are multiple tsm's (stages), the reset will adversely affect the scheduling for the barrier stage, no ?\r\n\r\nNo. The reset should only affect the tsm itself.",
        "createdAt" : "2021-01-04T05:24:14Z",
        "updatedAt" : "2021-02-18T15:35:53Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "78226c76-994a-4d0d-808f-2aacd30f9bb2",
        "parentId" : "dd8836da-638c-4b80-a459-fa72485c4cfa",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "So as @mridulm pointed out, since the legacy locality wait is reset  every time we could schedule something I think that means you will never look at other levels, correct?  I think this was behavior before but that doesn't seem right. I think that means you could have enough slots available and some are node local but you won't ever be able to use the ones that aren't.  Please correct me if I'm wrong.  I guess that could be separate issue since same behavior as before.\r\n",
        "createdAt" : "2021-01-05T15:17:11Z",
        "updatedAt" : "2021-02-18T15:35:53Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "9c2ec416-b21c-48f0-9b15-fc9e2e74b8d5",
        "parentId" : "dd8836da-638c-4b80-a459-fa72485c4cfa",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "> So as @mridulm pointed out, since the legacy locality wait is reset every time we could schedule something I think that means you will never look at other levels, correct?\r\n\r\nRight (although there's a corner case that we could look at other levels when the current level doesn't have any pending tasks). So in legacy mode, it's very strict for the barrier tsm to get launched.\r\n\r\n> I think this was behavior before but that doesn't seem right.\r\n\r\nI think that's why we have the new version of delay scheduling.",
        "createdAt" : "2021-01-07T03:42:55Z",
        "updatedAt" : "2021-02-18T15:35:53Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "ca14a419-7f11-42ca-95bf-2ae5025e1b39",
        "parentId" : "dd8836da-638c-4b80-a459-fa72485c4cfa",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "> I think that means you could have enough slots available and some are node local but you won't ever be able to use the ones that aren't.\r\n\r\nThis may lead to the barrier tsm gets hang? (But if we don't revert the locality level at the time we reverting resources, it would not hang since the level will go up as time goes by.)",
        "createdAt" : "2021-01-07T04:02:35Z",
        "updatedAt" : "2021-02-18T15:35:53Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "fead0790b2ed5bc27abf6f46de3ae8e84f3ac640",
    "line" : 133,
    "diffHunk" : "@@ -1,1 +456,460 @@            dequeuedTaskIndex = Some(index)\n            if (legacyLocalityWaitReset && maxLocality != TaskLocality.NO_PREF) {\n              resetDelayScheduleTimer(Some(taskLocality))\n            }\n            if (isBarrier) {"
  },
  {
    "id" : "62c27558-2d18-4974-8dea-dff3c969f454",
    "prId" : 29788,
    "prUrl" : "https://github.com/apache/spark/pull/29788#pullrequestreview-491637014",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b83bd1fd-a67e-4a3e-9c12-df547adf8eb7",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "I am wondering if we should instead pattern match in a separate arm like: \r\n\r\n```\r\n_ @ ExecutorDecommission => false\r\n```\r\n\r\nTo avoid having to change the case arms when we make changes to the structure definitions.",
        "createdAt" : "2020-09-18T17:26:37Z",
        "updatedAt" : "2020-09-18T17:38:05Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      }
    ],
    "commit" : "9bebdd4f2846f12f8ab13279c8cece151e8edfd0",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +992,996 @@      val exitCausedByApp: Boolean = reason match {\n        case exited: ExecutorExited => exited.exitCausedByApp\n        case ExecutorKilled | ExecutorDecommission(_, _) => false\n        case ExecutorProcessLost(_, _, false) => false\n        case _ => true"
  },
  {
    "id" : "61c3a067-63aa-4871-b06c-932636bccfc4",
    "prId" : 29276,
    "prUrl" : "https://github.com/apache/spark/pull/29276#pullrequestreview-458539050",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "41b317db-7e74-423e-8e46-f11026e78fa7",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "This makes sense to me, as shuffle map task returns `MapStatus`. We won't keep the task results, we just update map output tracker, update accumulators, update metrics, and throw the task result away. So we don't need to sum the tasks result size and check it.\r\n\r\nWhat do you think? @tgravescs @squito @holdenk  @jiangxb1987 @Ngone51 ",
        "createdAt" : "2020-07-29T03:16:57Z",
        "updatedAt" : "2020-08-11T03:01:04Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "408563d2-e250-44a2-bc22-a601c0aff36c",
        "parentId" : "41b317db-7e74-423e-8e46-f11026e78fa7",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "According to the JIRA, does this affect only 3.0.0?",
        "createdAt" : "2020-07-29T03:18:16Z",
        "updatedAt" : "2020-08-11T03:01:04Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "67818f3e-7a6a-4722-8085-52c3b290cc46",
        "parentId" : "41b317db-7e74-423e-8e46-f11026e78fa7",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "It affects all the versions, but it's not a serious problem (users can set a very high max result size conf). I think we don't need to backport.",
        "createdAt" : "2020-07-29T03:21:07Z",
        "updatedAt" : "2020-08-11T03:01:04Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "954c33fc-9db7-4c30-b51e-593bf2032f4a",
        "parentId" : "41b317db-7e74-423e-8e46-f11026e78fa7",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Yeah, make sense to me, too.",
        "createdAt" : "2020-07-29T03:34:18Z",
        "updatedAt" : "2020-08-11T03:01:04Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "f361ac46-d540-4804-99c8-7e5c1f0256a7",
        "parentId" : "41b317db-7e74-423e-8e46-f11026e78fa7",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "yes this seems reasonable.  It would be nice to update the comment on the function as well as put description in jira and in this PR to include more details about the shuffle map status and I assume you actually his this case in an application.",
        "createdAt" : "2020-07-29T14:18:24Z",
        "updatedAt" : "2020-08-11T03:01:04Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "e75b4bf8-c749-4c63-9f30-c8f323e93104",
        "parentId" : "41b317db-7e74-423e-8e46-f11026e78fa7",
        "authorId" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "body" : "LGTM too",
        "createdAt" : "2020-07-29T20:36:10Z",
        "updatedAt" : "2020-08-11T03:01:04Z",
        "lastEditedBy" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "tags" : [
        ]
      },
      {
        "id" : "fd4e41d4-95b5-45b5-90d3-5c0d2861906e",
        "parentId" : "41b317db-7e74-423e-8e46-f11026e78fa7",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "But seems we will save the MapStatus in MapOutputTracker. Don't we?",
        "createdAt" : "2020-07-29T22:09:53Z",
        "updatedAt" : "2020-08-11T03:01:04Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      },
      {
        "id" : "7d842d4e-9e37-4041-af21-2872dc4df758",
        "parentId" : "41b317db-7e74-423e-8e46-f11026e78fa7",
        "authorId" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "body" : "Then we can count the MapStatus size only in the total task result size?",
        "createdAt" : "2020-07-29T23:41:05Z",
        "updatedAt" : "2020-08-11T03:01:04Z",
        "lastEditedBy" : "4916859c-0e27-4e9d-ac39-ad95bc1382d3",
        "tags" : [
        ]
      },
      {
        "id" : "6cda80c1-9085-412d-badc-214598c43b54",
        "parentId" : "41b317db-7e74-423e-8e46-f11026e78fa7",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "> But seems we will save the MapStatus in MapOutputTracker. Don't we?\r\n\r\nWe do, but it's a global thing. Limiting the total size of one stage doesn't help too much. I believe the check here was to prevent too much data in `rdd.collect`.",
        "createdAt" : "2020-07-30T14:57:51Z",
        "updatedAt" : "2020-08-11T03:01:04Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "eab35886-7f00-48be-bc67-629a532d873b",
        "parentId" : "41b317db-7e74-423e-8e46-f11026e78fa7",
        "authorId" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "body" : "OK, make sense.",
        "createdAt" : "2020-07-30T15:56:06Z",
        "updatedAt" : "2020-08-11T03:01:04Z",
        "lastEditedBy" : "5e28de51-305c-4276-b3a0-78bcc74eb6f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "e1e53ef258013df2f85573de4542d36105ef852b",
    "line" : 21,
    "diffHunk" : "@@ -1,1 +699,703 @@    totalResultSize += size\n    calculatedTasks += 1\n    if (!isShuffleMapTasks && maxResultSize > 0 && totalResultSize > maxResultSize) {\n      val msg = s\"Total size of serialized results of ${calculatedTasks} tasks \" +\n        s\"(${Utils.bytesToString(totalResultSize)}) is bigger than ${config.MAX_RESULT_SIZE.key} \" +"
  },
  {
    "id" : "b1b85476-5fe1-4701-8c88-89938c0e04a4",
    "prId" : 28994,
    "prUrl" : "https://github.com/apache/spark/pull/28994#pullrequestreview-450384670",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a49b2245-7d8c-4ce3-9957-d537bbaff437",
        "parentId" : null,
        "authorId" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "body" : "how about recordsWritten? Should that also be considered wrt progress same wrt shuffleRecordsWritten?",
        "createdAt" : "2020-07-17T05:38:38Z",
        "updatedAt" : "2020-07-17T05:42:18Z",
        "lastEditedBy" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "tags" : [
        ]
      },
      {
        "id" : "eaf61e47-8fd1-49ec-b3f4-e37a7df2adca",
        "parentId" : "a49b2245-7d8c-4ce3-9957-d537bbaff437",
        "authorId" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "body" : "Even cache can also take time when written to disk, does that need to be taken into consideration? Similarly GC time, shuffle read blocked time etc. could also impact task progress",
        "createdAt" : "2020-07-17T05:41:25Z",
        "updatedAt" : "2020-07-17T05:42:18Z",
        "lastEditedBy" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "tags" : [
        ]
      }
    ],
    "commit" : "7519ef5fdc91a7b0f051a97de73055e0506fba47",
    "line" : 105,
    "diffHunk" : "@@ -1,1 +1173,1177 @@        if (task.inputMetrics != null) {\n          sumInputRecords += task.inputMetrics.recordsRead\n        }\n        if (task.shuffleReadMetrics != null) {\n          sumShuffleReadRecords += task.shuffleReadMetrics.recordsRead"
  },
  {
    "id" : "80282d14-0ac1-4745-a6a9-771c95163d0e",
    "prId" : 28994,
    "prUrl" : "https://github.com/apache/spark/pull/28994#pullrequestreview-450384670",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "938180f6-0fe2-4a8a-8929-7ca1340ffc3f",
        "parentId" : null,
        "authorId" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "body" : "would it make sense to add taskProgress as part of taskMetrics that way it can also be shown in SparkUI? Although taskProgress for tasks which doesn't involve input/output/shuffle records would be hard to measure?",
        "createdAt" : "2020-07-17T05:40:13Z",
        "updatedAt" : "2020-07-17T05:42:18Z",
        "lastEditedBy" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "tags" : [
        ]
      }
    ],
    "commit" : "7519ef5fdc91a7b0f051a97de73055e0506fba47",
    "line" : 128,
    "diffHunk" : "@@ -1,1 +1196,1200 @@        taskData(tid).taskMetrics.isDefined) {\n        val taskMetrics = taskData(tid).taskMetrics.get\n        val currentTaskProgressRate = (taskMetrics.inputMetrics.recordsRead +\n          taskMetrics.shuffleReadMetrics.recordsRead) / (runtimeMs / 1000.0)\n        val progressThreshold ="
  },
  {
    "id" : "613f59df-cb1e-4420-a9f4-c0a6a215d6e4",
    "prId" : 28656,
    "prUrl" : "https://github.com/apache/spark/pull/28656#pullrequestreview-419724446",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3147e6cd-39db-49a3-ab6a-22f5d69c09f5",
        "parentId" : null,
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "I think we should change this to only happen when executorAdded. this is also called on lost and decommission and it doesn't make sense to go to \"lower\" level.  Note we may want to stay away from saying higher in the comment below. The code values, lower is actually more strict - meaning process is lowest value. so perhaps don't say higher or lower but say more local or less local.  Perhaps pass in parameter from executorAdded.",
        "createdAt" : "2020-05-27T22:24:59Z",
        "updatedAt" : "2020-05-29T07:49:36Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "5c7ba50b-f9ab-4770-9626-5637f724cd5d",
        "parentId" : "3147e6cd-39db-49a3-ab6a-22f5d69c09f5",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "> I think we should change this to only happen when executorAdded. this is also called on lost and decommission and it doesn't make sense to go to \"lower\" level. \r\n\r\nI think it's impossible to go to \"lower\" or more local level in case of lost and decommission. Lost and decommission would remove executors, so the locality levels can only be less compared to the previous locality levels. It also means, lost and decommission will not add new more local levels.",
        "createdAt" : "2020-05-28T01:50:07Z",
        "updatedAt" : "2020-05-29T07:49:36Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "430c8ef9-82af-4eff-b1b8-25c5d837b160",
        "parentId" : "3147e6cd-39db-49a3-ab6a-22f5d69c09f5",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "> so perhaps don't say higher or lower but say more local or less local.\r\n\r\nYea, good point!",
        "createdAt" : "2020-05-28T01:51:04Z",
        "updatedAt" : "2020-05-29T07:49:36Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "8744f1ec02db28975a80410b5a01d2c2557ad216",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +1114,1118 @@    localityWaits = myLocalityLevels.map(getLocalityWait)\n    currentLocalityIndex = getLocalityIndex(previousLocalityLevel)\n    if (currentLocalityIndex > previousLocalityIndex) {\n      // SPARK-31837: If the new level is more local, shift to the new most local locality\n      // level in terms of better data locality. For example, say the previous locality"
  },
  {
    "id" : "765a1cef-cda0-49b7-827f-516247adfc7a",
    "prId" : 28656,
    "prUrl" : "https://github.com/apache/spark/pull/28656#pullrequestreview-421012127",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8c441afe-6490-40cf-8a7f-79ac190d9c6e",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Hi all, there's a defect in the previous implement(always reset `currentLocalityIndex` to 0). Think about such a case, say we have locality levels [PROCESS, NODE, ANY] and current locality level is ANY. After recompute, we might have locality levels [PROCESS, NODE, RACK, ANY]. In this case, I think we'd better shift to RACK level instead of  PROCESS level, since the TaskSetManager has been already delayed for a while on known levels(PROCESS, NODE). So with this update, I think it could also ease our concern on the possible perf regression introduced by aggressive locality level resetting. @bmarcott @tgravescs @cloud-fan ",
        "createdAt" : "2020-05-29T07:58:32Z",
        "updatedAt" : "2020-05-29T14:25:36Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "9fd24bb0-bbef-42bf-a0bb-752b404e1477",
        "parentId" : "8c441afe-6490-40cf-8a7f-79ac190d9c6e",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "yes this is one of the cases I was referring to. Ideally you would never run into this case because a host is on a rack so you would always have it. Unfortunately Spark defaults the rack to None so you can. I was going to improve upon it in the jira I filed. We can certainly handle some here if you want",
        "createdAt" : "2020-05-29T14:19:42Z",
        "updatedAt" : "2020-05-29T14:19:43Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "a5581087-99c3-4cca-b673-1bfa2104996f",
        "parentId" : "8c441afe-6490-40cf-8a7f-79ac190d9c6e",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "> We can certainly handle some here if you want\r\n\r\nWhat do you mean by \"handle some here\"? I read your JIRA and don't find the specific solution that could be added to this PR. Could you please elaborate more? ",
        "createdAt" : "2020-05-29T14:28:53Z",
        "updatedAt" : "2020-05-29T14:28:53Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "c6f08691-db09-4120-8a83-655a8d0a4f29",
        "parentId" : "8c441afe-6490-40cf-8a7f-79ac190d9c6e",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "I just mean there are a bunch of corner cases and I don't think resetting this on every executor added is ideal.  I did not list out all of them.  On Yarn it actually defaults to default-rack rather then None so it actually won't have this issue because every node has a rack.  I agree that the code you have here is an improvement to handle the rack case.",
        "createdAt" : "2020-05-29T14:33:26Z",
        "updatedAt" : "2020-05-29T14:33:26Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "8744f1ec02db28975a80410b5a01d2c2557ad216",
    "line" : 15,
    "diffHunk" : "@@ -1,1 +1119,1123 @@      // levels are [PROCESS, NODE, ANY] and current level is ANY. After recompute, the\n      // locality levels are [PROCESS, NODE, RACK, ANY]. Then, we'll shift to RACK level.\n      currentLocalityIndex = getLocalityIndex(myLocalityLevels.diff(previousMyLocalityLevels).head)\n    }\n  }"
  },
  {
    "id" : "b8520b42-37d2-4749-89fd-2c538a770aea",
    "prId" : 28619,
    "prUrl" : "https://github.com/apache/spark/pull/28619#pullrequestreview-418831153",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ef55782e-042c-4b6e-9cc7-7d0a99b78370",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "We need to remove the corresponding task from `tidToExecutorKillTimeMapping` if `speculated` is true?",
        "createdAt" : "2020-05-26T05:51:34Z",
        "updatedAt" : "2020-06-15T14:14:24Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "dec43382-7cf3-4801-94d7-930246b85706",
        "parentId" : "ef55782e-042c-4b6e-9cc7-7d0a99b78370",
        "authorId" : "ad4cb313-47b2-4cfa-9c1f-0a3596794a67",
        "body" : "We are removing entries from tidToExecutorKillTimeMapping at time when it is removed from `runningTasksSet`.",
        "createdAt" : "2020-05-26T15:03:36Z",
        "updatedAt" : "2020-06-15T14:14:24Z",
        "lastEditedBy" : "ad4cb313-47b2-4cfa-9c1f-0a3596794a67",
        "tags" : [
        ]
      },
      {
        "id" : "4f6ca1b6-b55d-45de-9eef-6d06d5649a26",
        "parentId" : "ef55782e-042c-4b6e-9cc7-7d0a99b78370",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Then, is it possible that we add redundant speculate tasks for the same tid?",
        "createdAt" : "2020-05-26T15:38:56Z",
        "updatedAt" : "2020-06-15T14:14:24Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "f47a5700-430e-4dcf-84ff-0277a67cc94d",
        "parentId" : "ef55782e-042c-4b6e-9cc7-7d0a99b78370",
        "authorId" : "ad4cb313-47b2-4cfa-9c1f-0a3596794a67",
        "body" : "The `copiesRunning(index) == 1` condition inside `checkAndSubmitSpeculatableTask` will prevent it. Irrespective of executor decommissioning, that check makes sure that only 1 speculatable task can run for each unique index.",
        "createdAt" : "2020-05-26T17:09:14Z",
        "updatedAt" : "2020-06-15T14:14:24Z",
        "lastEditedBy" : "ad4cb313-47b2-4cfa-9c1f-0a3596794a67",
        "tags" : [
        ]
      },
      {
        "id" : "ab84ab4e-6b71-4439-b7c7-e900d380283b",
        "parentId" : "ef55782e-042c-4b6e-9cc7-7d0a99b78370",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "I mean if it's possible to add redundant speculate tasks to `speculatableTasks` rather than running multiple  speculate tasks for the same tid at the same time.\r\n\r\nI think the actual prevention is `!speculatableTasks.contains(index)` in `checkAndSubmitSpeculatableTask`, but never mind since `speculatableTasks` is a `HashSet`.",
        "createdAt" : "2020-05-27T03:22:35Z",
        "updatedAt" : "2020-06-15T14:14:24Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "d87b311be85819ae884e2a24d94926fdd51165de",
    "line" : 48,
    "diffHunk" : "@@ -1,1 +1056,1060 @@            taskEndTimeBasedOnMedianDuration\n          if (canExceedDeadline) {\n            speculated = checkAndSubmitSpeculatableTask(tid, time, 0)\n          }\n        }"
  },
  {
    "id" : "dcec1d24-0dcd-4f8c-b266-d6445c647aed",
    "prId" : 28619,
    "prUrl" : "https://github.com/apache/spark/pull/28619#pullrequestreview-430685101",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bbb1f727-18f9-4dfb-a609-bb4910913a3f",
        "parentId" : null,
        "authorId" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "body" : "I am just curious why this solution (introducing the kill time per task ID map: `tidToExecutorKillTimeMapping`) is chosen over storing the kill time per executor (`executorToKillTimeMapping` or something like that). In the later case here would be  something like: \r\n\r\n```scala\r\nif (!speculated && executorToKillTimeMapping.nonEmpty) {\r\n  val taskInfo = taskInfos(tid)\r\n  executorToKillTimeMapping.get(taskInfo.executorId).foreach {  executorKillTime =>\r\n  ...\r\n  }\r\n}\r\n```",
        "createdAt" : "2020-06-15T11:24:00Z",
        "updatedAt" : "2020-06-15T14:14:24Z",
        "lastEditedBy" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "tags" : [
        ]
      },
      {
        "id" : "1d37e673-ea65-4bf8-8be6-5138305f1c19",
        "parentId" : "bbb1f727-18f9-4dfb-a609-bb4910913a3f",
        "authorId" : "ad4cb313-47b2-4cfa-9c1f-0a3596794a67",
        "body" : "Yes - We can store executorToKillTimeMapping also. In that case, we have to clean the map when executor is removed. For that we may have to make changes to pass \"removeExecutor\" signal till TaskSetManager.\r\n\r\nI didn't see much difference in both the approaches as total entries we will be creating for a given executor in current approach will be equal to total tasks running for that executor (which shouldn't be a lot).",
        "createdAt" : "2020-06-15T14:14:34Z",
        "updatedAt" : "2020-06-15T14:14:35Z",
        "lastEditedBy" : "ad4cb313-47b2-4cfa-9c1f-0a3596794a67",
        "tags" : [
        ]
      }
    ],
    "commit" : "d87b311be85819ae884e2a24d94926fdd51165de",
    "line" : 40,
    "diffHunk" : "@@ -1,1 +1048,1052 @@      for (tid <- runningTasksSet) {\n        var speculated = checkAndSubmitSpeculatableTask(tid, time, threshold)\n        if (!speculated && tidToExecutorKillTimeMapping.contains(tid)) {\n          // Check whether this task will finish before the exectorKillTime assuming\n          // it will take medianDuration overall. If this task cannot finish within"
  },
  {
    "id" : "b24d4ece-ffe1-4aae-bbfb-843b51b023c1",
    "prId" : 27871,
    "prUrl" : "https://github.com/apache/spark/pull/27871#pullrequestreview-375580257",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b85798ef-3649-4521-9c87-c802fc0c78cb",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Although we has a similar one inside `AppStatusListener` already, this means another implicit dependency to `sql` module.",
        "createdAt" : "2020-03-16T21:00:58Z",
        "updatedAt" : "2020-03-16T21:00:58Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "5c9bb717-d175-47e9-9407-bb3d9ab48e0d",
        "parentId" : "b85798ef-3649-4521-9c87-c802fc0c78cb",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "BTW, you may want to define `private val SQL_EXECUTION_ID_KEY = \"spark.sql.execution.id\"` like `AppStatusListener`. Or, if we need this, we may need to define this at the more higher common place.",
        "createdAt" : "2020-03-16T21:02:21Z",
        "updatedAt" : "2020-03-16T21:02:21Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "001b2340859c02be845598869bacc7e96ac34d2f",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +113,117 @@  val minShare = 0\n  val priority = if (taskSet.properties != null) {\n    taskSet.properties.getProperty(\"spark.sql.execution.id\", \"0\").toLong\n  } else 0L\n  val jobId = taskSet.priority"
  },
  {
    "id" : "1df74710-a88c-41b5-b0cb-b293fce0d5fb",
    "prId" : 27636,
    "prUrl" : "https://github.com/apache/spark/pull/27636#pullrequestreview-367204848",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "7e314991-452d-4c75-ab76-5b20e081aed8",
        "parentId" : null,
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "can you clarify?",
        "createdAt" : "2020-02-28T20:23:31Z",
        "updatedAt" : "2020-06-11T10:43:18Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "8c1445b6-585d-41a6-a559-ad2f11e84de8",
        "parentId" : "7e314991-452d-4c75-ab76-5b20e081aed8",
        "authorId" : "11f6c849-b5b9-4c65-990c-525cf572b913",
        "body" : "added the description in the code",
        "createdAt" : "2020-03-02T13:41:19Z",
        "updatedAt" : "2020-06-11T10:43:18Z",
        "lastEditedBy" : "11f6c849-b5b9-4c65-990c-525cf572b913",
        "tags" : [
        ]
      }
    ],
    "commit" : "c476e5271ca8b8fd3c401980a080334b8d7b9a36",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +816,820 @@            fetchFailed.bmAddress.host, fetchFailed.bmAddress.executorId))\n\n          // Do account fetch failure exception raised by decommissioned\n          // node against stage failure. Here the logic is to specify,\n          // if the task failed due to fetchFailed of decommission nodes than"
  }
]