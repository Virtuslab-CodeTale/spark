[
  {
    "id" : "458b56be-8aa0-4312-9463-a34a6a28f025",
    "prId" : 30956,
    "prUrl" : "https://github.com/apache/spark/pull/30956#pullrequestreview-559370922",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "5fd25fb4-4855-4b0f-9ba8-18947a931a8f",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "In non-testing mode, `client.killExecutors` will be invoked. But in testing mode, it's no longer invoked. So we now use the `executorMonitor.executorsPendingToRemove()` to perform the same check.",
        "createdAt" : "2020-12-29T04:04:26Z",
        "updatedAt" : "2020-12-29T04:04:26Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "ba0a4bca5a21417c78bac6626f4e1f6646c68a7b",
    "line" : 27,
    "diffHunk" : "@@ -1,1 +1627,1631 @@    assert(numExecutorsTargetForDefaultProfileId(manager) === 1)\n    // here's the important verify -- we did kill the executors, but did not adjust the target count\n    assert(manager.executorMonitor.executorsPendingToRemove() === Set(\"executor-1\"))\n  }\n"
  },
  {
    "id" : "5526d02b-dcd6-4253-951f-58c34c524684",
    "prId" : 29367,
    "prUrl" : "https://github.com/apache/spark/pull/29367#pullrequestreview-462895648",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0e54edfe-829e-4a70-b367-a75d8df231f0",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "Curious what 2020 means here and why is it so ?",
        "createdAt" : "2020-08-06T05:54:26Z",
        "updatedAt" : "2020-08-12T19:09:04Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      },
      {
        "id" : "215d4630-b5f8-40f6-8a31-6fb4ee7b737d",
        "parentId" : "0e54edfe-829e-4a70-b367-a75d8df231f0",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "It's the starting point used in the corresponding test with kill, didn't see a reason to change it for decom.",
        "createdAt" : "2020-08-06T19:31:30Z",
        "updatedAt" : "2020-08-12T19:09:04Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "d84a5816-5e5c-4a94-b265-4939b01cb2af",
        "parentId" : "0e54edfe-829e-4a70-b367-a75d8df231f0",
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "sure. ",
        "createdAt" : "2020-08-06T21:40:43Z",
        "updatedAt" : "2020-08-12T19:09:04Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      }
    ],
    "commit" : "e970cb10147fb64533f5088edc3a448b5ef198cf",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +1273,1277 @@\n  test(\"mock polling loop remove with decommissioning\") {\n    val clock = new ManualClock(2020L)\n    val manager = createManager(createConf(1, 20, 1, true), clock = clock)\n"
  },
  {
    "id" : "5bdc9ff0-de1e-4ec7-9f3d-bf66d58e7d0a",
    "prId" : 29225,
    "prUrl" : "https://github.com/apache/spark/pull/29225#pullrequestreview-470467859",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9c473a80-ef0a-4594-9806-cc52f24891bb",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "So, this is for the same reason with SPARK-22864 at line 1604?",
        "createdAt" : "2020-07-24T17:36:35Z",
        "updatedAt" : "2020-07-24T17:36:35Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "a7255ddd-4489-4842-ba4c-b1cc3b2c8fa6",
        "parentId" : "9c473a80-ef0a-4594-9806-cc52f24891bb",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "@tgravescs  It happens again at: https://github.com/apache/spark/pull/29418/checks?check_run_id=975304054\r\n\r\nBTW, this change only prevents the automatic invocation of `schedule()` for the second time but the first time invocation of `schedule()` always happens because the `initialDelay` is 0?\r\n\r\nhttps://github.com/apache/spark/blob/8f0fef18438aa8fb07f5ed885ffad1339992f102/core/src/main/scala/org/apache/spark/ExecutorAllocationManager.scala#L250",
        "createdAt" : "2020-08-17T03:18:48Z",
        "updatedAt" : "2020-08-17T03:18:52Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "eff1124e-fa21-47e5-a8b9-0120266f4c28",
        "parentId" : "9c473a80-ef0a-4594-9806-cc52f24891bb",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "I'm not seeing. a test report in that run?\r\nYes but it seemed like the failures we were seeing before we after an interation or two or the test starting, although it did vary. If we can see the logs from it we should be able to see timing and possibly some log messages to tell us if that is the problem.  ",
        "createdAt" : "2020-08-17T13:29:25Z",
        "updatedAt" : "2020-08-17T13:29:26Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "f7cfd02c-864a-4fa3-9f2a-b18b705e9d6e",
        "parentId" : "9c473a80-ef0a-4594-9806-cc52f24891bb",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "> I'm not seeing. a test report in that run?\r\n\r\nYou need to scroll to the end of ` Run tests: core, unsafe, kvstore, avro` section to see the failed tests.\r\n\r\n> Yes but it seemed like the failures we were seeing before we after an interation or two or the test starting, although it did vary. \r\n\r\nOk, not aware of that...\r\n\r\n> If we can see the logs from it we should be able to see timing and possibly some log messages to tell us if that is the problem.\r\n\r\n@tgravescs A feasible way to debug it is to open a PR in your own fork repository and adds some `println` in the source code.  \r\n\r\n@HyukjinKwon I think we still can not see the complete logs in GithubActions like the `unit-test.log` in Jenkins. I mean, even we download the archive logs after checks finished.\r\n\r\n",
        "createdAt" : "2020-08-17T13:39:23Z",
        "updatedAt" : "2020-08-17T13:39:23Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "4b4f913a-908b-4574-a224-a0f8a7f406e3",
        "parentId" : "9c473a80-ef0a-4594-9806-cc52f24891bb",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I think I can try to upload that specific log file as an artifact (so we can download) Let me take a look in coming few days.",
        "createdAt" : "2020-08-17T15:23:14Z",
        "updatedAt" : "2020-08-17T15:23:32Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "e8a64537-ae22-4015-a631-36982fa4837d",
        "parentId" : "9c473a80-ef0a-4594-9806-cc52f24891bb",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "BTW, the `unit-tests.log` file is generated by `log4j`:\r\n\r\nhttps://github.com/apache/spark/blob/b33066f42bd474f5f80b14221f97d09a76e0b398/core/src/test/resources/log4j.properties#L23",
        "createdAt" : "2020-08-18T01:41:50Z",
        "updatedAt" : "2020-08-18T01:41:51Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "8a35243a-2b12-41cd-9e0b-2570089206fa",
        "parentId" : "9c473a80-ef0a-4594-9806-cc52f24891bb",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "thanks for updating and uploading the logs, let me know if you see a build that has this test failure and I'll look at the log",
        "createdAt" : "2020-08-19T13:40:31Z",
        "updatedAt" : "2020-08-19T13:40:31Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "3af6ab923914575a85a097d6b75d69cee26af275",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +1604,1608 @@      // SPARK-22864: effectively disable the allocation schedule by setting the period to a\n      // really long value.\n      .set(TEST_SCHEDULE_INTERVAL, 30000L)\n    sparkConf\n  }"
  },
  {
    "id" : "7349aeb1-91d6-43bd-9a76-5b81933a4671",
    "prId" : 27223,
    "prUrl" : "https://github.com/apache/spark/pull/27223#pullrequestreview-348008303",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "760c5be2-5606-4d00-ac71-2e51d9a14e38",
        "parentId" : null,
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "wait why did this number change to 5? none of the tasks finished between this and the previous check",
        "createdAt" : "2020-01-23T22:47:51Z",
        "updatedAt" : "2020-01-27T18:47:04Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "1c5e862a-91e0-4f78-b791-650570ba36fa",
        "parentId" : "760c5be2-5606-4d00-ac71-2e51d9a14e38",
        "authorId" : "0387304e-5de3-452a-8609-ecd82ff5bfbe",
        "body" : "@tgravescs The number of pending speculative tasks becomes 0 now. So now we are returning `maxNeeded` instead of `maxNeeded+1`, which is 5.",
        "createdAt" : "2020-01-24T09:31:07Z",
        "updatedAt" : "2020-01-27T18:47:04Z",
        "lastEditedBy" : "0387304e-5de3-452a-8609-ecd82ff5bfbe",
        "tags" : [
        ]
      },
      {
        "id" : "4a5ff44b-8f9c-4598-ad1d-eda204e5e1f3",
        "parentId" : "760c5be2-5606-4d00-ac71-2e51d9a14e38",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "ok makes sense, can you add comment.",
        "createdAt" : "2020-01-24T14:56:54Z",
        "updatedAt" : "2020-01-27T18:47:04Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "3ea1175a4d7ccf71095a48945fa8946558b83c21",
    "line" : 44,
    "diffHunk" : "@@ -1,1 +304,308 @@    clock.advance(1000)\n    manager invokePrivate _updateAndSyncNumExecutorsTarget(clock.nanoTime())\n    assert(numExecutorsTarget(manager) == 5) // At this point, we still have 6 executors running\n    assert(maxNumExecutorsNeeded(manager) == 5)\n"
  },
  {
    "id" : "d49cdc0d-1dc2-44bf-8d20-6b1aa1d20947",
    "prId" : 24497,
    "prUrl" : "https://github.com/apache/spark/pull/24497#pullrequestreview-243383697",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e46a28ac-05ab-476b-b4b8-de6f64e7b46e",
        "parentId" : null,
        "authorId" : "e913f9af-103d-40d4-976e-9aa8e12e7211",
        "body" : "\"We count completions from the first zombie attempt\"",
        "createdAt" : "2019-05-29T16:43:42Z",
        "updatedAt" : "2019-06-24T03:38:58Z",
        "lastEditedBy" : "e913f9af-103d-40d4-976e-9aa8e12e7211",
        "tags" : [
        ]
      }
    ],
    "commit" : "b91965098c3c2b71d5adda9d501ee46cf14831ed",
    "line" : 33,
    "diffHunk" : "@@ -1,1 +282,286 @@\n    // submit another attempt for the stage.  We count completions from the first zombie attempt\n    val stageAttempt1 = createStageInfo(stage.stageId, 5, attemptId = 1)\n    post(SparkListenerStageSubmitted(stageAttempt1))\n    post(SparkListenerTaskEnd(0, 0, null, Success, taskInfo1, null))"
  }
]