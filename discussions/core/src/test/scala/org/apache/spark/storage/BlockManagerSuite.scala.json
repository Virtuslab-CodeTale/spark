[
  {
    "id" : "218febca-9f6e-459c-a6a3-489546066a07",
    "prId" : 33425,
    "prUrl" : "https://github.com/apache/spark/pull/33425#pullrequestreview-714234740",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4eb2a971-ee01-4982-b4b0-c307ea60a7da",
        "parentId" : null,
        "authorId" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "body" : "Can we extend this UT to verify the driver host is excluded? It will ensure that any future changes will not change this.",
        "createdAt" : "2021-07-20T05:43:51Z",
        "updatedAt" : "2021-07-20T05:44:25Z",
        "lastEditedBy" : "9953a478-bdd8-4f15-be59-f6f654ff85ae",
        "tags" : [
        ]
      },
      {
        "id" : "5e6a4310-29d6-42b4-b080-910f472ac731",
        "parentId" : "4eb2a971-ee01-4982-b4b0-c307ea60a7da",
        "authorId" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "body" : "I tried doing that, currently the BlockManagerMaster is mocked, therefore it doesn't register due to which it won't be added as part of `blockManagerIdByExecutor`. If not the test should have failed in the first place itself, as we are explicitly having tests checking for the hosts. But I will see if there is any other way add tests for this thing.",
        "createdAt" : "2021-07-20T17:33:46Z",
        "updatedAt" : "2021-07-20T17:33:46Z",
        "lastEditedBy" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "tags" : [
        ]
      },
      {
        "id" : "17186400-f978-404c-bc04-f32147f0ef31",
        "parentId" : "4eb2a971-ee01-4982-b4b0-c307ea60a7da",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Sounds fine.",
        "createdAt" : "2021-07-21T01:58:59Z",
        "updatedAt" : "2021-07-21T01:58:59Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "6a6c86c0-fc8f-4835-82a4-4e694c28ec66",
        "parentId" : "4eb2a971-ee01-4982-b4b0-c307ea60a7da",
        "authorId" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "body" : "@Ngone51 Any suggestions? or is this fine as it is? I don't think there is any other way if not we need to significantly refactor the tests or something.",
        "createdAt" : "2021-07-21T17:16:38Z",
        "updatedAt" : "2021-07-21T17:16:38Z",
        "lastEditedBy" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "tags" : [
        ]
      },
      {
        "id" : "e898593e-326c-4bf3-a32d-9101eb498668",
        "parentId" : "4eb2a971-ee01-4982-b4b0-c307ea60a7da",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "It's fine as it is.",
        "createdAt" : "2021-07-24T13:32:35Z",
        "updatedAt" : "2021-07-24T13:32:35Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "e3ef56393e612e6803b88fbce5c87178743528bc",
    "line" : 18,
    "diffHunk" : "@@ -1,1 +2099,2103 @@    master.removeShufflePushMergerLocation(\"hostA\")\n    assert(master.getShufflePushMergerLocations(4, Set.empty).map(_.host).sorted ===\n      Seq(\"hostB\", \"hostC\", \"hostD\").sorted)\n  }\n"
  },
  {
    "id" : "322affbe-09da-4824-b656-f344e0034e7d",
    "prId" : 33425,
    "prUrl" : "https://github.com/apache/spark/pull/33425#pullrequestreview-715176315",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "3cd610a7-2649-4376-ab14-644ea97e1255",
        "parentId" : null,
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "Add a driver block manager to this test ? It will catch the issue being fixed in `BlockManagerMasterEndpoint`, right ?\r\n",
        "createdAt" : "2021-07-25T06:28:08Z",
        "updatedAt" : "2021-07-25T06:28:08Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      },
      {
        "id" : "8c3de49b-a6aa-48d9-aab6-87355d49421b",
        "parentId" : "3cd610a7-2649-4376-ab14-644ea97e1255",
        "authorId" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "body" : "I tried adding test but like I mentioned in this [comment](https://github.com/apache/spark/pull/33425#discussion_r673340152) currently `BlockManagerMaster` is mocked due to which it doesn't registered and won't be in `blockManagerIdByExecutor`. That is why I didn't add the test.",
        "createdAt" : "2021-07-25T15:01:10Z",
        "updatedAt" : "2021-07-25T15:01:10Z",
        "lastEditedBy" : "4da55a4a-8a34-454a-9445-fb8101f62652",
        "tags" : [
        ]
      },
      {
        "id" : "57a4b686-b142-4fce-82ba-a127861f06f5",
        "parentId" : "3cd610a7-2649-4376-ab14-644ea97e1255",
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "Why would this not work ?\r\n\r\n```\r\n    assert(master.getExecutorEndpointRef(SparkContext.DRIVER_IDENTIFIER).isEmpty)\r\n    makeBlockManager(100, SparkContext.DRIVER_IDENTIFIER,\r\n      transferService = Some(new MockBlockTransferService(10, \"host0\")))\r\n    assert(master.getExecutorEndpointRef(SparkContext.DRIVER_IDENTIFIER).isDefined)\r\n```",
        "createdAt" : "2021-07-26T18:50:33Z",
        "updatedAt" : "2021-07-26T18:50:33Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      }
    ],
    "commit" : "e3ef56393e612e6803b88fbce5c87178743528bc",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +2094,2098 @@    assert(master.getShufflePushMergerLocations(3, Set.empty).size == 3)\n    assert(master.getShufflePushMergerLocations(3, Set.empty).map(_.host).sorted ===\n      Seq(\"hostC\", \"hostB\", \"hostD\").sorted)\n    assert(master.getShufflePushMergerLocations(4, Set.empty).map(_.host).sorted ===\n      Seq(\"hostB\", \"hostA\", \"hostC\", \"hostD\").sorted)"
  },
  {
    "id" : "b8bcc321-b5cb-4e72-91df-64fa9a39125d",
    "prId" : 33425,
    "prUrl" : "https://github.com/apache/spark/pull/33425#pullrequestreview-715340506",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2ea17467-6c01-4c2f-8393-513480390019",
        "parentId" : null,
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "This does not look right; we cant have it both empty and defined :-)\r\nIt will be defined only after we add the driver.",
        "createdAt" : "2021-07-26T22:43:44Z",
        "updatedAt" : "2021-07-26T22:44:02Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      }
    ],
    "commit" : "e3ef56393e612e6803b88fbce5c87178743528bc",
    "line" : 7,
    "diffHunk" : "@@ -1,1 +2087,2091 @@    assert(master.getExecutorEndpointRef(SparkContext.DRIVER_IDENTIFIER).isEmpty)\n    makeBlockManager(100, SparkContext.DRIVER_IDENTIFIER,\n      transferService = Some(new MockBlockTransferService(10, \"host-driver\")))\n    assert(master.getExecutorEndpointRef(SparkContext.DRIVER_IDENTIFIER).isDefined)\n    master.removeExecutor(\"execA\")"
  },
  {
    "id" : "13a2b81d-977c-45da-90a2-5e202879c212",
    "prId" : 32727,
    "prUrl" : "https://github.com/apache/spark/pull/32727#pullrequestreview-675731162",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a7fd665d-a1a2-4bf4-8da9-07b7b5a954ba",
        "parentId" : null,
        "authorId" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "body" : "As I see the test body here is the same as the one below (\"test migration of shuffle blocks during decommissioning - no limit\") so are we calling `testShuffleBlockDecommissioning(None, true)` twice with just a different test name. Is not it?\r\n\r\nIf I am right then we should come up with a common name for those two tests and keep only one with the new name as testing is already takes considerable time and we should avoid duplicating the same tests.",
        "createdAt" : "2021-06-03T02:36:16Z",
        "updatedAt" : "2021-06-03T02:45:06Z",
        "lastEditedBy" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "tags" : [
        ]
      },
      {
        "id" : "8d89201b-f6c3-47d3-a7e8-afa102a82e84",
        "parentId" : "a7fd665d-a1a2-4bf4-8da9-07b7b5a954ba",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Ya, you are correct. During refactoring to minimize the patch, I missed the duplication. I'll remove new test case.\r\n> As I see the test body here is the same as the one below (\"test migration of shuffle blocks during decommissioning - no limit\") so are we calling testShuffleBlockDecommissioning(None, true) twice with just a different test name. Is not it?",
        "createdAt" : "2021-06-03T21:17:19Z",
        "updatedAt" : "2021-06-03T21:17:19Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "a1700a9c135a3d982a887cba4ef321b366157efb",
    "line" : 34,
    "diffHunk" : "@@ -1,1 +1994,1998 @@\n  test(\"SPARK-35589: test migration of index-only shuffle blocks during decommissioning\") {\n    testShuffleBlockDecommissioning(None, true)\n  }\n"
  },
  {
    "id" : "1340de0a-8922-424e-9eeb-652e263fa9a5",
    "prId" : 31493,
    "prUrl" : "https://github.com/apache/spark/pull/31493#pullrequestreview-585760292",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "807992b0-b484-40b0-8f80-a305919afbfe",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "I think this can not ensure that the block fails due to the disk size limitation. Block migration may doesn't finish within 1 second.",
        "createdAt" : "2021-02-06T14:22:35Z",
        "updatedAt" : "2021-02-08T20:53:03Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "fc90a012-1090-4b5a-983e-dbbadd891821",
        "parentId" : "807992b0-b484-40b0-8f80-a305919afbfe",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Yeah it does, in this situation we assert above in the normal case that the blocks migrate within one second.",
        "createdAt" : "2021-02-08T17:44:17Z",
        "updatedAt" : "2021-02-08T20:53:03Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "69b25406b9d5ee1f0862f06614f579a4ed9a1e03",
    "line" : 59,
    "diffHunk" : "@@ -1,1 +1978,1982 @@          === shuffleIndexBlockContent)\n      } else {\n        Thread.sleep(1000)\n        assert(mapOutputTracker.shuffleStatuses(0).mapStatuses(0).location === bm1.blockManagerId)\n      }"
  },
  {
    "id" : "20cbd200-1ca8-436c-aae5-eb7cc675a3d6",
    "prId" : 30164,
    "prUrl" : "https://github.com/apache/spark/pull/30164#pullrequestreview-529106672",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "cf2b5322-8bdf-48c1-9fbc-5e218ecc5a57",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "If we want to get 4 merges, it should also include `hostA`? If yes, shall we also test this case?",
        "createdAt" : "2020-11-12T13:54:25Z",
        "updatedAt" : "2020-11-20T00:32:25Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "5ce29340c8aa4a6aadfc8d00cb5053a9be9aa839",
    "line" : 47,
    "diffHunk" : "@@ -1,1 +2011,2015 @@    master.removeExecutor(\"execE\")\n\n    assert(master.getShufflePushMergerLocations(3, Set.empty).size == 3)\n    assert(master.getShufflePushMergerLocations(3, Set.empty).map(_.host).sorted ===\n      Seq(\"hostC\", \"hostB\", \"hostD\").sorted)"
  },
  {
    "id" : "dccf8c53-1d09-4e68-9532-ff0bacc44485",
    "prId" : 28924,
    "prUrl" : "https://github.com/apache/spark/pull/28924#pullrequestreview-439881919",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "567155bc-c956-4abf-8352-d79836f62f30",
        "parentId" : null,
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Any particular reason why 5?",
        "createdAt" : "2020-06-29T22:58:23Z",
        "updatedAt" : "2020-07-10T07:38:55Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "a6c268c3-d771-488e-ae5a-9b564c64e4ce",
        "parentId" : "567155bc-c956-4abf-8352-d79836f62f30",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "In the newly added tests, we need to simulate the timeout error from BlockManager. But at the same time, we also don't want the test run too long since the default timeout value is 120s. Therefore, we choose a quite short timeout for the tests. On the other hand, we don't set it to a smaller value, e.g. 1s, which may cause test flaky.\r\n\r\n\r\nNote that the best way to set the timeout value is to set it for the newly added tests locally instead of setting it globally. However, with the limitation of the current test framework in Core side, it's hard to set it locally since it requires more changes.",
        "createdAt" : "2020-06-30T10:27:04Z",
        "updatedAt" : "2020-07-10T07:38:55Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "319d9257b200bb087afe15aace8b66d96925ce93",
    "line" : 24,
    "diffHunk" : "@@ -1,1 +95,99 @@      .set(Kryo.KRYO_SERIALIZER_BUFFER_SIZE.key, \"1m\")\n      .set(STORAGE_UNROLL_MEMORY_THRESHOLD, 512L)\n      .set(Network.RPC_ASK_TIMEOUT, \"5s\")\n  }\n"
  },
  {
    "id" : "a1e2cfb6-e394-4c69-b154-48d271a43857",
    "prId" : 28924,
    "prUrl" : "https://github.com/apache/spark/pull/28924#pullrequestreview-446166929",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "72c231ec-c2e7-42cd-8a77-e6be31cacc0b",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "what does `withLost` means?",
        "createdAt" : "2020-07-09T11:59:03Z",
        "updatedAt" : "2020-07-10T07:38:55Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "5a3f2263-d6a0-47ed-9d63-df41e7f79a36",
        "parentId" : "72c231ec-c2e7-42cd-8a77-e6be31cacc0b",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "We always set up three roles, driver, executor-1, executor-2 before these two tests.  `withLost` here indicates whether to initialize the executor-2 as a lost executor in terms of driver's view.",
        "createdAt" : "2020-07-10T07:04:43Z",
        "updatedAt" : "2020-07-10T07:38:55Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "319d9257b200bb087afe15aace8b66d96925ce93",
    "line" : 102,
    "diffHunk" : "@@ -1,1 +240,244 @@      Map.empty, 0))\n\n    if (!withLost) {\n      driverEndpoint.askSync[Boolean](CoarseGrainedClusterMessages.RegisterExecutor(\n        bm2Id.executorId, null, bm1Id.host, 1, Map.empty, Map.empty, Map.empty, 0))"
  },
  {
    "id" : "9e661cea-2e66-4623-b3d6-4998fa2276d1",
    "prId" : 28924,
    "prUrl" : "https://github.com/apache/spark/pull/28924#pullrequestreview-446169822",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c3d8e517-4b70-4b55-87c8-546e6bf9fcde",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "can we add comment to explain what `withLost` means?",
        "createdAt" : "2020-07-10T07:06:43Z",
        "updatedAt" : "2020-07-10T07:38:55Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "f9fc7374-e4a3-4ae8-8294-476235e9deca",
        "parentId" : "c3d8e517-4b70-4b55-87c8-546e6bf9fcde",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "ok sure.",
        "createdAt" : "2020-07-10T07:09:52Z",
        "updatedAt" : "2020-07-10T07:38:55Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "319d9257b200bb087afe15aace8b66d96925ce93",
    "line" : 54,
    "diffHunk" : "@@ -1,1 +192,196 @@   * register the executor-2 normally.\n   */\n  private def setupBlockManagerMasterWithBlocks(withLost: Boolean): Unit = {\n    // set up a simple DriverEndpoint which simply adds executorIds and\n    // checks whether a certain executorId has been added before."
  },
  {
    "id" : "93ec1cc2-05d2-417d-adb4-ebb48d074145",
    "prId" : 27864,
    "prUrl" : "https://github.com/apache/spark/pull/27864#pullrequestreview-388288981",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "27e5112e-46c7-49d6-9510-f575071e5a18",
        "parentId" : null,
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "I'd like to see a test where we decomission a block manager and execute a task that attempts to cache the result. I'm worried the current structure might cause that task to fail unnecessarily.",
        "createdAt" : "2020-04-02T19:13:55Z",
        "updatedAt" : "2020-04-23T14:23:15Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "97312b1a-c509-4bc5-9620-2b27850fe769",
        "parentId" : "27e5112e-46c7-49d6-9510-f575071e5a18",
        "authorId" : "ad4cb313-47b2-4cfa-9c1f-0a3596794a67",
        "body" : "Yes. Updated the code. Added new test to specifically test this scenario in BlockManagerDecommissionSuite.",
        "createdAt" : "2020-04-06T14:11:48Z",
        "updatedAt" : "2020-04-23T14:23:15Z",
        "lastEditedBy" : "ad4cb313-47b2-4cfa-9c1f-0a3596794a67",
        "tags" : [
        ]
      }
    ],
    "commit" : "bb324f946019b8d700c517cc5eb2f7c11dc70cfc",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +1707,1711 @@  }\n\n  test(\"test decommission block manager should not be part of peers\") {\n    val exec1 = \"exec1\"\n    val exec2 = \"exec2\""
  },
  {
    "id" : "edc6a8a2-2f08-4fca-8ef3-9d69a5a76def",
    "prId" : 27306,
    "prUrl" : "https://github.com/apache/spark/pull/27306#pullrequestreview-353535057",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c5fe820e-5253-4c25-969c-459f7d744a55",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "how do we make sure this request returns false?",
        "createdAt" : "2020-02-04T11:23:06Z",
        "updatedAt" : "2020-02-04T11:23:06Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "84bab1c3-26cd-478d-9f55-22f367a301ad",
        "parentId" : "c5fe820e-5253-4c25-969c-459f7d744a55",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Because the `BlockManagerId(\"1\", \"host1\", 100)` hasn't registered at driver. And driver will response `false` for the block from a non-existent block manager.",
        "createdAt" : "2020-02-05T07:46:55Z",
        "updatedAt" : "2020-02-05T07:46:55Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "022a3536-8d65-4137-9e54-3a8f8f8cdc5d",
        "parentId" : "c5fe820e-5253-4c25-969c-459f7d744a55",
        "authorId" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "body" : "Yep, exactly as Yi said.",
        "createdAt" : "2020-02-05T08:28:44Z",
        "updatedAt" : "2020-02-05T08:28:45Z",
        "lastEditedBy" : "cd38bd5a-0fae-4d8e-8acd-36dc13753759",
        "tags" : [
        ]
      }
    ],
    "commit" : "5f123173bc03cb057c97fb98f5ea5879452e00ba",
    "line" : 45,
    "diffHunk" : "@@ -1,1 +1701,1705 @@    val updateInfo = UpdateBlockInfo(BlockManagerId(\"1\", \"host1\", 100),\n      BlockId(\"test_1\"), StorageLevel.MEMORY_ONLY, 1, 1)\n    val result = master.driverEndpoint.askSync[Boolean](updateInfo)\n\n    assert(!result)"
  }
]