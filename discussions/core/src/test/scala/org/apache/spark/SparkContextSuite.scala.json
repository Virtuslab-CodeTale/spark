[
  {
    "id" : "c355eaa4-7673-4c8e-b4c6-762f2be25d04",
    "prId" : 31515,
    "prUrl" : "https://github.com/apache/spark/pull/31515#pullrequestreview-585102643",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a0fda6eb-3a50-4b11-8d6f-11fb8f58c674",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "This is still a `Hadoop` conf although it's `Overlay` property.",
        "createdAt" : "2021-02-08T00:30:11Z",
        "updatedAt" : "2021-02-08T00:30:11Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "189cddf23b30147812ce3761573f49914380cd82",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +1156,1160 @@    val bufferKey = \"io.file.buffer.size\"\n    val hadoopConf0 = new Configuration()\n    hadoopConf0.set(testKey, \"/tmp/hive_zero\")\n\n    val hiveConfFile = Utils.getContextOrSparkClassLoader.getResource(\"hive-site.xml\")"
  },
  {
    "id" : "ffb12959-c994-4ac9-9734-edb434510094",
    "prId" : 31515,
    "prUrl" : "https://github.com/apache/spark/pull/31515#pullrequestreview-585124206",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ddb34ae3-1c2f-45d9-b489-ba0f07d0ea63",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "This is okay because Apache Spark UT aims to test line 1170 ~ 1172.\r\n```\r\n    assert(sc.hadoopConfiguration.get(testKey) === \"/tmp/hive_one\",\r\n      \"hive configs have higher priority than hadoop ones \")\r\n    assert(sc.hadoopConfiguration.get(bufferKey).toInt === 65536,\r\n      \"spark configs have higher priority than hive ones\")\r\n```",
        "createdAt" : "2021-02-08T00:33:14Z",
        "updatedAt" : "2021-02-08T00:33:14Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "6b83c108-a721-4596-833e-5b2960ae7267",
        "parentId" : "ddb34ae3-1c2f-45d9-b489-ba0f07d0ea63",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "w/ this change, `\"/tmp/hive_one\"` in `hive-site.xml` overlays `\"/tmp/hive_{user}\"` not  `\"/tmp/hive_zero\"`, which happens in `SparkHadoopUtil.appendS3AndSparkHadoopHiveConfigurations`.\r\n\r\nAs `hadoopConf0` is not passed to `SparkHadoopUtil`, while `core-site.xml` is, so here to reduce test flakiness, we can remove those `assert`s for `hadoopConf0`, as they are just used to check `hive-site.xml` overrides `core-site.xml`. ",
        "createdAt" : "2021-02-08T01:26:21Z",
        "updatedAt" : "2021-02-08T01:26:21Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "9485901e-de90-43ae-93ca-bd7c95c333c3",
        "parentId" : "ddb34ae3-1c2f-45d9-b489-ba0f07d0ea63",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@yaooqinn, can you open a PR for that, or push some changes into this PR? I think the test is not flaky but broken. It doesn't look obvious because we don't always run YarnClusterSuite.",
        "createdAt" : "2021-02-08T02:27:16Z",
        "updatedAt" : "2021-02-08T02:27:16Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "189cddf23b30147812ce3761573f49914380cd82",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +1161,1165 @@    assert(hiveConfFile != null)\n    hadoopConf0.addResource(hiveConfFile)\n    assert(hadoopConf0.get(testKey) === \"/tmp/hive_zero\")\n    assert(hadoopConf0.get(bufferKey) === \"201811\")\n"
  },
  {
    "id" : "48bf0170-d4ee-4030-af54-3bc17dd99a3d",
    "prId" : 31460,
    "prUrl" : "https://github.com/apache/spark/pull/31460#pullrequestreview-582488988",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "18ab7cdb-f77c-41e5-9a5a-010be2d8e6aa",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "For good measure, do we want to also test that the default io.file.buffer.size you get from a plain Spark configuration is in fact 65536?",
        "createdAt" : "2021-02-03T15:14:24Z",
        "updatedAt" : "2021-02-04T02:17:18Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "024c531b-e2da-4b7d-804e-14b338dce446",
        "parentId" : "18ab7cdb-f77c-41e5-9a5a-010be2d8e6aa",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "I guess that will cause test flakiness if it gets a nondefault value somewhere due to the non-deterministic test order in CIs. I did a pre-check for loading hive-site.xml explicitly to ensure it gets loaded but not pollute the final result.\r\n\r\n",
        "createdAt" : "2021-02-03T15:27:27Z",
        "updatedAt" : "2021-02-04T02:17:18Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "ddecaedd-674b-4741-b411-d1e5584eed4f",
        "parentId" : "18ab7cdb-f77c-41e5-9a5a-010be2d8e6aa",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "oh, I can just set it explicitly too",
        "createdAt" : "2021-02-03T15:28:31Z",
        "updatedAt" : "2021-02-04T02:17:18Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "58a532e4e0ebaeb6f351fa347d1ce4da90d2b1e5",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +1152,1156 @@  }\n\n  test(\"SPARK-34346: hadoop configuration priority for spark/hive/hadoop configs\") {\n    val testKey = \"hadoop.tmp.dir\"\n    val bufferKey = \"io.file.buffer.size\""
  },
  {
    "id" : "57d1db02-5d93-485d-8c61-13836c2b3ac3",
    "prId" : 31460,
    "prUrl" : "https://github.com/apache/spark/pull/31460#pullrequestreview-582549152",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "72a6b4e3-6788-450c-9973-36382d0d4b49",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "shouldn't this be `20181117`?",
        "createdAt" : "2021-02-03T16:07:02Z",
        "updatedAt" : "2021-02-04T02:17:18Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "47fd9c76-5207-49e5-a0e1-8f4f9dd83071",
        "parentId" : "72a6b4e3-6788-450c-9973-36382d0d4b49",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "here is the same sparkConf w/ two more configsï¼Œwillnot change to respect spark.hadoop.xxx",
        "createdAt" : "2021-02-03T16:18:01Z",
        "updatedAt" : "2021-02-04T02:17:18Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "2469d214-c0c2-4dbe-b2bc-30236009b14c",
        "parentId" : "72a6b4e3-6788-450c-9973-36382d0d4b49",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "OK, so for the buffer size, we ignore everything but only respect the `BUFFER_SIZE` config.",
        "createdAt" : "2021-02-03T16:23:44Z",
        "updatedAt" : "2021-02-04T02:17:18Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "58a532e4e0ebaeb6f351fa347d1ce4da90d2b1e5",
    "line" : 42,
    "diffHunk" : "@@ -1,1 +1181,1185 @@    assert(sc.hadoopConfiguration.get(testKey) === \"/tmp/hive_two\",\n      \"spark.hadoop configs have higher priority than hive/hadoop ones\")\n    assert(sc.hadoopConfiguration.get(bufferKey).toInt === 65536,\n      \"spark configs have higher priority than spark.hadoop configs\")\n  }"
  },
  {
    "id" : "783638b5-c041-4583-8af3-bcd445c12269",
    "prId" : 30486,
    "prUrl" : "https://github.com/apache/spark/pull/30486#pullrequestreview-541372402",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e82222a4-3eae-45a5-a33b-e249a2ff6d9b",
        "parentId" : null,
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "are we relying on existing test for tar.gz and tar?",
        "createdAt" : "2020-11-30T15:24:09Z",
        "updatedAt" : "2020-11-30T15:33:46Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "ca53eb46-2afd-417b-a7d3-d7d57126ae54",
        "parentId" : "e82222a4-3eae-45a5-a33b-e249a2ff6d9b",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I manually tested tar.gz and tgz cases, and the cases without extensions too. The unpack method is a copy from Hadoop so I did not exhaustively test but I can add if that looks better to add the case.",
        "createdAt" : "2020-11-30T23:19:42Z",
        "updatedAt" : "2020-11-30T23:19:42Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "e35e94cad2888c98e03feb391a8dfcca8afa365c",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +169,173 @@\n      val jarFile = new File(dir, \"test!@$jar.jar\")\n      val zipFile = new File(dir, \"test-zip.zip\")\n      val relativePath1 =\n        s\"${zipFile.getParent}/../${zipFile.getParentFile.getName}/${zipFile.getName}\""
  },
  {
    "id" : "128a0f0c-54b9-4998-92d7-f9658d36c122",
    "prId" : 29966,
    "prUrl" : "https://github.com/apache/spark/pull/29966#pullrequestreview-542441920",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "24935b41-e579-428a-acdf-32d7977edfa3",
        "parentId" : null,
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Could you add the jars first, then check if the dependent jars co-exist in `sc.listJars`?",
        "createdAt" : "2020-12-02T01:01:24Z",
        "updatedAt" : "2020-12-24T01:53:11Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "a5c317b4-d1cb-43a1-9a43-2a9a1298adc3",
        "parentId" : "24935b41-e579-428a-acdf-32d7977edfa3",
        "authorId" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "body" : "Btw, this behaviour is the same with hive?",
        "createdAt" : "2020-12-02T01:01:49Z",
        "updatedAt" : "2020-12-24T01:53:11Z",
        "lastEditedBy" : "044062b3-9d96-4ec3-a1e2-3a2f595bbc02",
        "tags" : [
        ]
      },
      {
        "id" : "95fb0fb6-b6e1-425c-831e-23993e640d85",
        "parentId" : "24935b41-e579-428a-acdf-32d7977edfa3",
        "authorId" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "body" : "> Could you add the jars first, then check if the dependent jars co-exist in `sc.listJars`?\r\n\r\nDone",
        "createdAt" : "2020-12-02T02:02:49Z",
        "updatedAt" : "2020-12-24T01:53:11Z",
        "lastEditedBy" : "594db420-88ec-4875-8cd3-0d8b0bec131c",
        "tags" : [
        ]
      }
    ],
    "commit" : "4c44daecc7c77527e150d30051170f7ee8667f70",
    "line" : 67,
    "diffHunk" : "@@ -1,1 +1098,1102 @@    sc.addJar(\"ivy://org.apache.hive:hive-storage-api:2.6.0\")\n    assert(sc.listJars().exists(_.contains(\"org.apache.hive_hive-storage-api-2.7.0.jar\")))\n    assert(sc.listJars().exists(_.contains(\"org.apache.hive_hive-storage-api-2.6.0.jar\")))\n  }\n"
  },
  {
    "id" : "39c33146-61a3-419c-b35e-5b3029357614",
    "prId" : 27138,
    "prUrl" : "https://github.com/apache/spark/pull/27138#pullrequestreview-340152260",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b2573822-534a-4a33-ac39-37a73fe5af26",
        "parentId" : null,
        "authorId" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "body" : "IIUC we require 3 cores on each executor in this case.",
        "createdAt" : "2020-01-08T20:58:23Z",
        "updatedAt" : "2020-01-08T21:10:46Z",
        "lastEditedBy" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "tags" : [
        ]
      },
      {
        "id" : "6c01c0dd-0d00-4710-8aa9-a1ef4ab10206",
        "parentId" : "b2573822-534a-4a33-ac39-37a73fe5af26",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "the test passes with it like this. The requirement looks like its 3 executors, not 3 tasks per executor and it waits for all executors to be up.",
        "createdAt" : "2020-01-08T21:09:46Z",
        "updatedAt" : "2020-01-08T21:10:46Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "bc490001-a1d8-4906-94c0-17881ed551db",
        "parentId" : "b2573822-534a-4a33-ac39-37a73fe5af26",
        "authorId" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "body" : "ah yes",
        "createdAt" : "2020-01-08T21:20:39Z",
        "updatedAt" : "2020-01-08T21:20:39Z",
        "lastEditedBy" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "b0ddd1c7f807339bb3b959a4b503ce5877d5d292",
    "line" : 32,
    "diffHunk" : "@@ -1,1 +928,932 @@\n      val conf = new SparkConf()\n        .setMaster(\"local-cluster[3, 1, 1024]\")\n        .setAppName(\"test-cluster\")\n        .set(WORKER_GPU_ID.amountConf, \"3\")"
  },
  {
    "id" : "0e8195e0-1bf5-4837-a4fb-b7b28be46383",
    "prId" : 25047,
    "prUrl" : "https://github.com/apache/spark/pull/25047#pullrequestreview-267202457",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4f06a979-d355-4288-9bd8-2e5bf4e978ec",
        "parentId" : null,
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "I think one of these was intentionally this way as it is allowed to find more resources then requested.  Perhaps put this back and add comment.",
        "createdAt" : "2019-07-16T16:40:31Z",
        "updatedAt" : "2019-08-09T02:27:38Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "1e63ee85-e8a9-4f8c-9acf-c34b4561b26c",
        "parentId" : "4f06a979-d355-4288-9bd8-2e5bf4e978ec",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "The problem is, what does `sparkContext.resources()` should mean when we get Standalone resources scheduling in ? discovered resources or assigned resources ?  This changed to '3', otherwise it failed on:\r\n\r\n`assert(sc.resources.get(GPU).get.addresses === Array(\"0\", \"1\", \"8\"))` \r\n\r\nat L784.",
        "createdAt" : "2019-07-18T13:32:19Z",
        "updatedAt" : "2019-08-09T02:27:38Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "c325ba7d-b034-45e3-97d7-9df4498bfb4c",
        "parentId" : "4f06a979-d355-4288-9bd8-2e5bf4e978ec",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "ah I see.  ok  lets leave this.  STandalone mode right now its assigned.",
        "createdAt" : "2019-07-26T16:34:12Z",
        "updatedAt" : "2019-08-09T02:27:38Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "15a9897131f06ec115b13d97e86c497ef36dace8",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +784,788 @@        .setMaster(\"local-cluster[1, 1, 1024]\")\n        .setAppName(\"test-cluster\")\n      conf.set(DRIVER_GPU_ID.amountConf, \"3\")\n      conf.set(DRIVER_GPU_ID.discoveryScriptConf, scriptPath)\n"
  },
  {
    "id" : "1ea2a6a3-3d9a-4e6f-82eb-5d76324f1d3f",
    "prId" : 25047,
    "prUrl" : "https://github.com/apache/spark/pull/25047#pullrequestreview-267202457",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9ee466f3-e5ca-494c-aac4-6a0b972fa500",
        "parentId" : null,
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "why was this changed?",
        "createdAt" : "2019-07-16T16:40:55Z",
        "updatedAt" : "2019-08-09T02:27:38Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "6cf53481-01ba-461b-b227-0e0c330642c6",
        "parentId" : "9ee466f3-e5ca-494c-aac4-6a0b972fa500",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Previously, this discoveryScript was used for those 3 executors directly. And now, when we get Standalone resources scheduling in, this discoveryScript should be used by Worker, otherwise executor can't be launched due to no confined resources for worker.  As a result, we need to prepare 9 isolated resource addresses for 3 workers in the same host  in order to launch 3 executor.",
        "createdAt" : "2019-07-18T13:36:53Z",
        "updatedAt" : "2019-08-09T02:27:38Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "90f773ef-c5c0-441a-9f0e-75adc2d0221c",
        "parentId" : "9ee466f3-e5ca-494c-aac4-6a0b972fa500",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "ok",
        "createdAt" : "2019-07-26T16:34:48Z",
        "updatedAt" : "2019-08-09T02:27:38Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "15a9897131f06ec115b13d97e86c497ef36dace8",
    "line" : 23,
    "diffHunk" : "@@ -1,1 +851,855 @@    withTempDir { dir =>\n      val discoveryScript = createTempScriptWithExpectedOutput(dir, \"resourceDiscoveryScript\",\n        \"\"\"{\"name\": \"gpu\",\"addresses\":[\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\"]}\"\"\")\n\n      val conf = new SparkConf()"
  },
  {
    "id" : "6e29b476-a019-48ae-a1c3-c8e7e221bd1c",
    "prId" : 24374,
    "prUrl" : "https://github.com/apache/spark/pull/24374#pullrequestreview-244562973",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8278e740-57de-4cd4-b369-884bb84238fc",
        "parentId" : null,
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "add in     assume(!(Utils.isWindows))  so it skips on windows",
        "createdAt" : "2019-05-31T15:18:52Z",
        "updatedAt" : "2019-06-04T21:38:24Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "70bc63bc-3333-4fd9-aa2e-58853f21d044",
        "parentId" : "8278e740-57de-4cd4-b369-884bb84238fc",
        "authorId" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "body" : "updated",
        "createdAt" : "2019-06-01T00:36:42Z",
        "updatedAt" : "2019-06-04T21:38:24Z",
        "lastEditedBy" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "82cd1e35bdc7caaddb4122060d8fd6b98893cbb6",
    "line" : 111,
    "diffHunk" : "@@ -1,1 +863,867 @@  test(\"test resource scheduling under local-cluster mode\") {\n    import org.apache.spark.TestUtils._\n\n    assume(!(Utils.isWindows))\n    withTempDir { dir =>"
  }
]