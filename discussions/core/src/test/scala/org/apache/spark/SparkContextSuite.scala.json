[
  {
    "id" : "c355eaa4-7673-4c8e-b4c6-762f2be25d04",
    "prId" : 31515,
    "prUrl" : "https://github.com/apache/spark/pull/31515#pullrequestreview-585102643",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "a0fda6eb-3a50-4b11-8d6f-11fb8f58c674",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "This is still a `Hadoop` conf although it's `Overlay` property.",
        "createdAt" : "2021-02-08T00:30:11Z",
        "updatedAt" : "2021-02-08T00:30:11Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      }
    ],
    "commit" : "189cddf23b30147812ce3761573f49914380cd82",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +1156,1160 @@    val bufferKey = \"io.file.buffer.size\"\n    val hadoopConf0 = new Configuration()\n    hadoopConf0.set(testKey, \"/tmp/hive_zero\")\n\n    val hiveConfFile = Utils.getContextOrSparkClassLoader.getResource(\"hive-site.xml\")"
  },
  {
    "id" : "ffb12959-c994-4ac9-9734-edb434510094",
    "prId" : 31515,
    "prUrl" : "https://github.com/apache/spark/pull/31515#pullrequestreview-585124206",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ddb34ae3-1c2f-45d9-b489-ba0f07d0ea63",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "This is okay because Apache Spark UT aims to test line 1170 ~ 1172.\r\n```\r\n    assert(sc.hadoopConfiguration.get(testKey) === \"/tmp/hive_one\",\r\n      \"hive configs have higher priority than hadoop ones \")\r\n    assert(sc.hadoopConfiguration.get(bufferKey).toInt === 65536,\r\n      \"spark configs have higher priority than hive ones\")\r\n```",
        "createdAt" : "2021-02-08T00:33:14Z",
        "updatedAt" : "2021-02-08T00:33:14Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "6b83c108-a721-4596-833e-5b2960ae7267",
        "parentId" : "ddb34ae3-1c2f-45d9-b489-ba0f07d0ea63",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "w/ this change, `\"/tmp/hive_one\"` in `hive-site.xml` overlays `\"/tmp/hive_{user}\"` not  `\"/tmp/hive_zero\"`, which happens in `SparkHadoopUtil.appendS3AndSparkHadoopHiveConfigurations`.\r\n\r\nAs `hadoopConf0` is not passed to `SparkHadoopUtil`, while `core-site.xml` is, so here to reduce test flakiness, we can remove those `assert`s for `hadoopConf0`, as they are just used to check `hive-site.xml` overrides `core-site.xml`. ",
        "createdAt" : "2021-02-08T01:26:21Z",
        "updatedAt" : "2021-02-08T01:26:21Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "9485901e-de90-43ae-93ca-bd7c95c333c3",
        "parentId" : "ddb34ae3-1c2f-45d9-b489-ba0f07d0ea63",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "@yaooqinn, can you open a PR for that, or push some changes into this PR? I think the test is not flaky but broken. It doesn't look obvious because we don't always run YarnClusterSuite.",
        "createdAt" : "2021-02-08T02:27:16Z",
        "updatedAt" : "2021-02-08T02:27:16Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "189cddf23b30147812ce3761573f49914380cd82",
    "line" : 10,
    "diffHunk" : "@@ -1,1 +1161,1165 @@    assert(hiveConfFile != null)\n    hadoopConf0.addResource(hiveConfFile)\n    assert(hadoopConf0.get(testKey) === \"/tmp/hive_zero\")\n    assert(hadoopConf0.get(bufferKey) === \"201811\")\n"
  },
  {
    "id" : "48bf0170-d4ee-4030-af54-3bc17dd99a3d",
    "prId" : 31460,
    "prUrl" : "https://github.com/apache/spark/pull/31460#pullrequestreview-582488988",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "18ab7cdb-f77c-41e5-9a5a-010be2d8e6aa",
        "parentId" : null,
        "authorId" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "body" : "For good measure, do we want to also test that the default io.file.buffer.size you get from a plain Spark configuration is in fact 65536?",
        "createdAt" : "2021-02-03T15:14:24Z",
        "updatedAt" : "2021-02-04T02:17:18Z",
        "lastEditedBy" : "707cf6e1-750c-4aea-8774-f79c13fb7add",
        "tags" : [
        ]
      },
      {
        "id" : "024c531b-e2da-4b7d-804e-14b338dce446",
        "parentId" : "18ab7cdb-f77c-41e5-9a5a-010be2d8e6aa",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "I guess that will cause test flakiness if it gets a nondefault value somewhere due to the non-deterministic test order in CIs. I did a pre-check for loading hive-site.xml explicitly to ensure it gets loaded but not pollute the final result.\r\n\r\n",
        "createdAt" : "2021-02-03T15:27:27Z",
        "updatedAt" : "2021-02-04T02:17:18Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "ddecaedd-674b-4741-b411-d1e5584eed4f",
        "parentId" : "18ab7cdb-f77c-41e5-9a5a-010be2d8e6aa",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "oh, I can just set it explicitly too",
        "createdAt" : "2021-02-03T15:28:31Z",
        "updatedAt" : "2021-02-04T02:17:18Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      }
    ],
    "commit" : "58a532e4e0ebaeb6f351fa347d1ce4da90d2b1e5",
    "line" : 13,
    "diffHunk" : "@@ -1,1 +1152,1156 @@  }\n\n  test(\"SPARK-34346: hadoop configuration priority for spark/hive/hadoop configs\") {\n    val testKey = \"hadoop.tmp.dir\"\n    val bufferKey = \"io.file.buffer.size\""
  },
  {
    "id" : "57d1db02-5d93-485d-8c61-13836c2b3ac3",
    "prId" : 31460,
    "prUrl" : "https://github.com/apache/spark/pull/31460#pullrequestreview-582549152",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "72a6b4e3-6788-450c-9973-36382d0d4b49",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "shouldn't this be `20181117`?",
        "createdAt" : "2021-02-03T16:07:02Z",
        "updatedAt" : "2021-02-04T02:17:18Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "47fd9c76-5207-49e5-a0e1-8f4f9dd83071",
        "parentId" : "72a6b4e3-6788-450c-9973-36382d0d4b49",
        "authorId" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "body" : "here is the same sparkConf w/ two more configsï¼Œwillnot change to respect spark.hadoop.xxx",
        "createdAt" : "2021-02-03T16:18:01Z",
        "updatedAt" : "2021-02-04T02:17:18Z",
        "lastEditedBy" : "c62ded40-3015-4888-8e91-d671d0f615be",
        "tags" : [
        ]
      },
      {
        "id" : "2469d214-c0c2-4dbe-b2bc-30236009b14c",
        "parentId" : "72a6b4e3-6788-450c-9973-36382d0d4b49",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "OK, so for the buffer size, we ignore everything but only respect the `BUFFER_SIZE` config.",
        "createdAt" : "2021-02-03T16:23:44Z",
        "updatedAt" : "2021-02-04T02:17:18Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      }
    ],
    "commit" : "58a532e4e0ebaeb6f351fa347d1ce4da90d2b1e5",
    "line" : 42,
    "diffHunk" : "@@ -1,1 +1181,1185 @@    assert(sc.hadoopConfiguration.get(testKey) === \"/tmp/hive_two\",\n      \"spark.hadoop configs have higher priority than hive/hadoop ones\")\n    assert(sc.hadoopConfiguration.get(bufferKey).toInt === 65536,\n      \"spark configs have higher priority than spark.hadoop configs\")\n  }"
  },
  {
    "id" : "783638b5-c041-4583-8af3-bcd445c12269",
    "prId" : 30486,
    "prUrl" : "https://github.com/apache/spark/pull/30486#pullrequestreview-541372402",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "e82222a4-3eae-45a5-a33b-e249a2ff6d9b",
        "parentId" : null,
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "are we relying on existing test for tar.gz and tar?",
        "createdAt" : "2020-11-30T15:24:09Z",
        "updatedAt" : "2020-11-30T15:33:46Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      },
      {
        "id" : "ca53eb46-2afd-417b-a7d3-d7d57126ae54",
        "parentId" : "e82222a4-3eae-45a5-a33b-e249a2ff6d9b",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I manually tested tar.gz and tgz cases, and the cases without extensions too. The unpack method is a copy from Hadoop so I did not exhaustively test but I can add if that looks better to add the case.",
        "createdAt" : "2020-11-30T23:19:42Z",
        "updatedAt" : "2020-11-30T23:19:42Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "e35e94cad2888c98e03feb391a8dfcca8afa365c",
    "line" : 12,
    "diffHunk" : "@@ -1,1 +169,173 @@\n      val jarFile = new File(dir, \"test!@$jar.jar\")\n      val zipFile = new File(dir, \"test-zip.zip\")\n      val relativePath1 =\n        s\"${zipFile.getParent}/../${zipFile.getParentFile.getName}/${zipFile.getName}\""
  }
]