[
  {
    "id" : "84712b59-035c-4e1b-ac75-fa2541d2d9c6",
    "prId" : 28053,
    "prUrl" : "https://github.com/apache/spark/pull/28053#pullrequestreview-383919631",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1d5f4b25-e27f-4b8e-8449-c3f8e1181c97",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "It's not related to this PR, but just wondering why we use different API name between executor and task for cpu/cores. Maybe it's follow original confs?",
        "createdAt" : "2020-03-30T07:43:34Z",
        "updatedAt" : "2020-04-01T19:28:46Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "a92114d4-f4a2-43dd-931c-a9fdd02f1807",
        "parentId" : "1d5f4b25-e27f-4b8e-8449-c3f8e1181c97",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "yes, that is the way the spark confs are defined. task config is called spark.task.cpus for some reason. I have no idea why, its always been that way, but wanted to follow that convention",
        "createdAt" : "2020-03-30T14:36:10Z",
        "updatedAt" : "2020-04-01T19:28:46Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "452378be77b5371feb7d974776ad411b9e2bc232",
    "line" : 14,
    "diffHunk" : "@@ -1,1 +109,113 @@      ereqs.cores(i).memory(\"4g\").memoryOverhead(\"2000m\")\n      val treqs = new TaskResourceRequests()\n      treqs.cpus(i)\n      rprofBuilder.require(ereqs).require(treqs)\n      val rprof = rprofBuilder.build"
  }
]