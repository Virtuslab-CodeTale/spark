[
  {
    "id" : "56d725a4-1385-48e4-814e-66966b992140",
    "prId" : 28038,
    "prUrl" : "https://github.com/apache/spark/pull/28038#pullrequestreview-386653081",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ede065ea-7277-4b73-a7e5-8f94d2da9a49",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Thank you for adding this.",
        "createdAt" : "2020-03-31T00:45:03Z",
        "updatedAt" : "2020-04-06T22:07:58Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "3baabff2-9412-4c3c-b37b-add84d15bcc8",
        "parentId" : "ede065ea-7277-4b73-a7e5-8f94d2da9a49",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Sure, it's just what we had in the ALS suite but this is a better place for it.",
        "createdAt" : "2020-04-02T17:39:08Z",
        "updatedAt" : "2020-04-06T22:07:58Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "29019c88e29b85a2d549dd889c018896fa3e10af",
    "line" : 32,
    "diffHunk" : "@@ -1,1 +30,34 @@\n\nclass RDDCleanerSuite extends SparkFunSuite with BeforeAndAfterEach {\n  override def beforeEach(): Unit = {\n    super.beforeEach()"
  },
  {
    "id" : "1cb36361-c78c-4596-8184-50fce1ee9511",
    "prId" : 28038,
    "prUrl" : "https://github.com/apache/spark/pull/28038#pullrequestreview-386691770",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "6d9d6eee-388e-456d-8837-de91dcc2d8fd",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Could you also assert files before clean?",
        "createdAt" : "2020-03-31T02:27:41Z",
        "updatedAt" : "2020-04-06T22:07:58Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "25fdc344-beed-458a-bbf5-99b9e16fd003",
        "parentId" : "6d9d6eee-388e-456d-8837-de91dcc2d8fd",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Sure, done.",
        "createdAt" : "2020-04-02T18:32:12Z",
        "updatedAt" : "2020-04-06T22:07:58Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "29019c88e29b85a2d549dd889c018896fa3e10af",
    "line" : 66,
    "diffHunk" : "@@ -1,1 +64,68 @@        keysOnly.cleanShuffleDependencies(true)\n        val resultingFiles = getAllFiles\n        assert(resultingFiles === Set())\n        // Ensure running count again works fine even if we kill the shuffle files.\n        assert(keysOnly.count() === 20)"
  },
  {
    "id" : "4d108b8a-ebf6-4e3b-bfcd-aeccba078db4",
    "prId" : 28038,
    "prUrl" : "https://github.com/apache/spark/pull/28038#pullrequestreview-386691280",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ea65440d-c9d9-4636-a494-6f7e06c9eb05",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "What happens when we have multiple shuffles?",
        "createdAt" : "2020-03-31T22:17:13Z",
        "updatedAt" : "2020-04-06T22:07:58Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "0513bd99-3b43-443e-b598-32184f41305b",
        "parentId" : "ea65440d-c9d9-4636-a494-6f7e06c9eb05",
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Like a reduceByKey then change the key and some other operation? It will recurse and clean up both shuffles, unless one of them is cached.",
        "createdAt" : "2020-04-02T18:31:33Z",
        "updatedAt" : "2020-04-06T22:07:58Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "29019c88e29b85a2d549dd889c018896fa3e10af",
    "line" : 60,
    "diffHunk" : "@@ -1,1 +58,62 @@        val input = sc.parallelize(1 to 1000)\n        val keyed = input.map(x => (x % 20, 1))\n        val shuffled = keyed.reduceByKey(_ + _)\n        val keysOnly = shuffled.keys\n        keysOnly.count()"
  }
]