[
  {
    "id" : "e8674628-93d4-4f25-be61-e130d4d164a2",
    "prId" : 29422,
    "prUrl" : "https://github.com/apache/spark/pull/29422#pullrequestreview-467755915",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "22ba400a-aefd-49ce-9b6d-d1990c718583",
        "parentId" : null,
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "Nice fix / cleanup",
        "createdAt" : "2020-08-14T17:53:14Z",
        "updatedAt" : "2020-08-17T23:52:14Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      }
    ],
    "commit" : "df128e507a2c7bd11d33197fca6b4fa10f4e9256",
    "line" : 4,
    "diffHunk" : "@@ -1,1 +85,89 @@  }\n\n  // Unlike TestUtils.withListener, it also waits for the job to be done\n  def withListener(sc: SparkContext, listener: RootStageAwareListener)\n                  (body: SparkListener => Unit): Unit = {"
  },
  {
    "id" : "12bd955b-e12a-4e76-b8d6-767ee220641d",
    "prId" : 29422,
    "prUrl" : "https://github.com/apache/spark/pull/29422#pullrequestreview-467903300",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "83720272-09f3-4042-a95a-f71994589914",
        "parentId" : null,
        "authorId" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "body" : "I assume this is for speed up right?",
        "createdAt" : "2020-08-14T17:53:49Z",
        "updatedAt" : "2020-08-17T23:52:14Z",
        "lastEditedBy" : "ae55b635-dd0e-41aa-9272-372de9a35f38",
        "tags" : [
        ]
      },
      {
        "id" : "d41bd4b5-888d-4703-b033-ca5be49caee9",
        "parentId" : "83720272-09f3-4042-a95a-f71994589914",
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "Exactly. Got tired of waiting for the test to run and trying to cut slack.",
        "createdAt" : "2020-08-14T22:20:06Z",
        "updatedAt" : "2020-08-17T23:52:14Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      }
    ],
    "commit" : "df128e507a2c7bd11d33197fca6b4fa10f4e9256",
    "line" : 74,
    "diffHunk" : "@@ -1,1 +242,246 @@          val sleepTimeSeconds = if (executorId == executorToDecom) 10 else 1\n          Thread.sleep(sleepTimeSeconds * 1000L)\n        }\n        List(1).iterator\n      }, preservesPartitioning = true)"
  },
  {
    "id" : "de688d5d-84f0-4288-b7ee-04ddfefe3226",
    "prId" : 29422,
    "prUrl" : "https://github.com/apache/spark/pull/29422#pullrequestreview-489481854",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "01186a4b-c647-4f62-8425-0e91c60f9013",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "Quick note: this test seems flaky:\r\n\r\n```\r\nsbt.ForkMain$ForkError: org.scalatest.exceptions.TestFailedException: 9 did not equal 6 Expected 6 tasks but got List(0:0:0:0-SUCCESS, 0:0:1:0-SUCCESS, 1:0:0:0-FAILED, 0:1:0:0-SUCCESS, 0:1:1:0-SUCCESS, 1:1:0:0-FAILED, 0:2:0:0-SUCCESS, 0:2:1:0-SUCCESS, 1:2:0:0-SUCCESS)\r\n\tat org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:472)\r\n\tat org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:471)\r\n\tat org.scalatest.Assertions$.newAssertionFailedException(Assertions.scala:1231)\r\n\tat org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:1295)\r\n\tat org.apache.spark.deploy.DecommissionWorkerSuite.testFetchFailures(DecommissionWorkerSuite.scala:267)\r\n\tat org.apache.spark.deploy.DecommissionWorkerSuite.$anonfun$new$14(DecommissionWorkerSuite.scala:275)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)\r\n\tat org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)\r\n\tat org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)\r\n\tat org.scalatest.Transformer.apply(Transformer.scala:22)\r\n\tat org.scalatest.Transformer.apply(Transformer.scala:20)\r\n\tat org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:189)\r\n\tat org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:176)\r\n\tat org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:187)\r\n\tat org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:199)\r\n\tat org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)\r\n\tat org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:199)\r\n\tat org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:181)\r\n\tat org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(SparkFunSuite.scala:61)\r\n\tat org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234)\r\n\tat org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227)\r\n\tat org.apache.spark.SparkFunSuite.runTest(SparkFunSuite.scala:61)\r\n\tat org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:232)\r\n\tat org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)\r\n\tat scala.collection.immutable.List.foreach(List.scala:392)\r\n\tat org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)\r\n\tat org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)\r\n```\r\n\r\nI'll file a JIRA if I see more often.",
        "createdAt" : "2020-09-16T10:05:34Z",
        "updatedAt" : "2020-09-16T10:05:34Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "d8660a61-0f2a-4c46-a8c7-e94ddbecb91f",
        "parentId" : "01186a4b-c647-4f62-8425-0e91c60f9013",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "@HyukjinKwon Do you have a link for the test job? I want to take a look at it.",
        "createdAt" : "2020-09-16T10:08:46Z",
        "updatedAt" : "2020-09-16T10:08:47Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "61c96207-a6ff-4c92-9255-c1052ba48b02",
        "parentId" : "01186a4b-c647-4f62-8425-0e91c60f9013",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I retriggered the job ... I will let you know when I see one more time ..",
        "createdAt" : "2020-09-16T10:18:42Z",
        "updatedAt" : "2020-09-16T10:18:48Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "df128e507a2c7bd11d33197fca6b4fa10f4e9256",
    "line" : 93,
    "diffHunk" : "@@ -1,1 +272,276 @@  }\n\n  test(\"decommission eager workers ensure that fetch failures lead to rerun\") {\n    testFetchFailures(0)\n  }"
  },
  {
    "id" : "26517e30-132a-43d2-acb0-9e2670ebb31a",
    "prId" : 29014,
    "prUrl" : "https://github.com/apache/spark/pull/29014#pullrequestreview-452515702",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "73cfcb0f-b0ab-440e-81a6-ba922cb3251a",
        "parentId" : null,
        "authorId" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "body" : "The same as before (move the assert here and rewrite it). \r\n\r\nHere it is more important as the block scopes are too deep so move all the `assert`s up where the `else` only contains assertions only.",
        "createdAt" : "2020-07-21T14:17:14Z",
        "updatedAt" : "2020-07-29T01:25:46Z",
        "lastEditedBy" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "tags" : [
        ]
      },
      {
        "id" : "0deb1ec7-a434-46c8-adf2-d1781b600748",
        "parentId" : "73cfcb0f-b0ab-440e-81a6-ba922cb3251a",
        "authorId" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "body" : "oh I see that comment I am referring to is lost let me add it again\r\n\r\n",
        "createdAt" : "2020-07-21T14:29:00Z",
        "updatedAt" : "2020-07-29T01:25:46Z",
        "lastEditedBy" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "tags" : [
        ]
      }
    ],
    "commit" : "dbc478b0d96de0f1a48f93d90f1e648229b5790c",
    "line" : 157,
    "diffHunk" : "@@ -1,1 +155,159 @@        override def handleRootTaskEnd(taskEnd: SparkListenerTaskEnd): Unit = {\n          val taskInfo = taskEnd.taskInfo\n          if (taskInfo.index == 0) {\n            if (workerForTask0Decommissioned.compareAndSet(false, true)) {\n              val workerInfo = executorIdToWorkerInfo(taskInfo.executorId)"
  },
  {
    "id" : "40112e5b-383f-4c46-9db3-2e048283a04e",
    "prId" : 29014,
    "prUrl" : "https://github.com/apache/spark/pull/29014#pullrequestreview-455225734",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "1ac17aab-e63f-429e-aa33-f336095f4a2e",
        "parentId" : null,
        "authorId" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "body" : "One idea for fetch failure (I have not tried it yet): from workers you can access the worker then you have the `workDirPath` the executors on the workers are using subdirs of this work dir path. If you can delete its content (or just the shuffle files) between the stages I assume you will have a proper fetch failed.\r\n\r\n ",
        "createdAt" : "2020-07-24T17:49:36Z",
        "updatedAt" : "2020-07-29T01:25:46Z",
        "lastEditedBy" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "tags" : [
        ]
      },
      {
        "id" : "295a2cd9-ea3a-4824-8156-95e49e4fe62c",
        "parentId" : "1ac17aab-e63f-429e-aa33-f336095f4a2e",
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "I spend several hours on this and couldn't get it to work :-(.  I understand that getting a proper fetch failure would obviously be more close to actual scenario here, but I couldn't get it working. Please let me know if this is a blocker.",
        "createdAt" : "2020-07-24T23:01:00Z",
        "updatedAt" : "2020-07-29T01:25:46Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      }
    ],
    "commit" : "dbc478b0d96de0f1a48f93d90f1e648229b5790c",
    "line" : 209,
    "diffHunk" : "@@ -1,1 +207,211 @@    // fetch failure instead of an executor lost. Since it is hard to \"trigger a fetch failure\",\n    // we manually raise the FetchFailed exception when the 2nd stage's task runs and require that\n    // fetch failure to trigger a recomputation.\n    logInfo(s\"Will try to decommission the task running on executor $executorToDecom\")\n    val listener = new RootStageAwareListener {"
  }
]