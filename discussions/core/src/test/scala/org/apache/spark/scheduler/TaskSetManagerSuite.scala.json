[
  {
    "id" : "3ca406ef-5798-4b4d-a8d1-7b681f2f89b2",
    "prId" : 29468,
    "prUrl" : "https://github.com/apache/spark/pull/29468#pullrequestreview-470647653",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ad5b3034-e35f-4110-8dc8-9eafdaf5e503",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "I think you should also call `sched.executorDecommission(exec1, ExecutorDecommissionInfo(\"test\", true))` (ie for exec1), to mimic the production behavior which would always decommission all executors on a decommissioned host.\r\n\r\nIn addition, I think we should also test decommissioning when the entire host hasn't been taken down ? ",
        "createdAt" : "2020-08-19T16:51:04Z",
        "updatedAt" : "2020-08-20T06:22:17Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      }
    ],
    "commit" : "65f6e0525b2d1472c1e6956587253e5ae6263676",
    "line" : 45,
    "diffHunk" : "@@ -1,1 +675,679 @@\n    // Decommission all executors on host0, to mimic CoarseGrainedSchedulerBackend.\n    sched.executorDecommission(exec0, ExecutorDecommissionInfo(\"test\", true))\n    sched.executorDecommission(exec1, ExecutorDecommissionInfo(\"test\", true))\n"
  },
  {
    "id" : "710c3b8d-c881-492c-9651-8233dae58832",
    "prId" : 29240,
    "prUrl" : "https://github.com/apache/spark/pull/29240#pullrequestreview-455461842",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b84c2058-5db4-4119-8218-e6810c3c3af4",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "LGTM. Maybe it would have been better if we leave a comment so people don't mistakenly remove for the time being :-).",
        "createdAt" : "2020-07-27T01:47:48Z",
        "updatedAt" : "2020-07-27T01:47:49Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "cb5667a1-bb10-4be0-bdfb-58e89c24c995",
        "parentId" : "b84c2058-5db4-4119-8218-e6810c3c3af4",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Thanks. Ya. It's a general problem in Scala 2.13 because we don't have any CIs. Not only tests, Scala 2.13 compilation itself is already broken in `sql/catalyst`. It's sad that there is no way to protect properly. :(\r\n\r\nMaybe, we had better consider enable Scala 2.13 CI partially for compilation and testing.\r\n\r\nWDYT, @HyukjinKwon and @srowen ?",
        "createdAt" : "2020-07-27T02:11:19Z",
        "updatedAt" : "2020-07-27T02:11:20Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "d9c48a03-5b6e-4eaf-8932-498d6dac3c95",
        "parentId" : "b84c2058-5db4-4119-8218-e6810c3c3af4",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I haven't tested it by myself yet so I don't know the stability about 2.13 yet but +1 for the idea.",
        "createdAt" : "2020-07-27T04:17:31Z",
        "updatedAt" : "2020-07-27T04:17:31Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "1cf43b9b8b909baba486c004169c973fbbd0a18b",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +32,36 @@import org.scalatest.concurrent.Eventually\n\nimport org.apache.spark.{FakeSchedulerBackend => _, _}\nimport org.apache.spark.internal.Logging\nimport org.apache.spark.internal.config"
  }
]