[
  {
    "id" : "3ca406ef-5798-4b4d-a8d1-7b681f2f89b2",
    "prId" : 29468,
    "prUrl" : "https://github.com/apache/spark/pull/29468#pullrequestreview-470647653",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "ad5b3034-e35f-4110-8dc8-9eafdaf5e503",
        "parentId" : null,
        "authorId" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "body" : "I think you should also call `sched.executorDecommission(exec1, ExecutorDecommissionInfo(\"test\", true))` (ie for exec1), to mimic the production behavior which would always decommission all executors on a decommissioned host.\r\n\r\nIn addition, I think we should also test decommissioning when the entire host hasn't been taken down ? ",
        "createdAt" : "2020-08-19T16:51:04Z",
        "updatedAt" : "2020-08-20T06:22:17Z",
        "lastEditedBy" : "711a9ca0-8b90-435a-bf87-eb621aa1db13",
        "tags" : [
        ]
      }
    ],
    "commit" : "65f6e0525b2d1472c1e6956587253e5ae6263676",
    "line" : 45,
    "diffHunk" : "@@ -1,1 +675,679 @@\n    // Decommission all executors on host0, to mimic CoarseGrainedSchedulerBackend.\n    sched.executorDecommission(exec0, ExecutorDecommissionInfo(\"test\", true))\n    sched.executorDecommission(exec1, ExecutorDecommissionInfo(\"test\", true))\n"
  },
  {
    "id" : "710c3b8d-c881-492c-9651-8233dae58832",
    "prId" : 29240,
    "prUrl" : "https://github.com/apache/spark/pull/29240#pullrequestreview-455461842",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b84c2058-5db4-4119-8218-e6810c3c3af4",
        "parentId" : null,
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "LGTM. Maybe it would have been better if we leave a comment so people don't mistakenly remove for the time being :-).",
        "createdAt" : "2020-07-27T01:47:48Z",
        "updatedAt" : "2020-07-27T01:47:49Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      },
      {
        "id" : "cb5667a1-bb10-4be0-bdfb-58e89c24c995",
        "parentId" : "b84c2058-5db4-4119-8218-e6810c3c3af4",
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Thanks. Ya. It's a general problem in Scala 2.13 because we don't have any CIs. Not only tests, Scala 2.13 compilation itself is already broken in `sql/catalyst`. It's sad that there is no way to protect properly. :(\r\n\r\nMaybe, we had better consider enable Scala 2.13 CI partially for compilation and testing.\r\n\r\nWDYT, @HyukjinKwon and @srowen ?",
        "createdAt" : "2020-07-27T02:11:19Z",
        "updatedAt" : "2020-07-27T02:11:20Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "d9c48a03-5b6e-4eaf-8932-498d6dac3c95",
        "parentId" : "b84c2058-5db4-4119-8218-e6810c3c3af4",
        "authorId" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "body" : "I haven't tested it by myself yet so I don't know the stability about 2.13 yet but +1 for the idea.",
        "createdAt" : "2020-07-27T04:17:31Z",
        "updatedAt" : "2020-07-27T04:17:31Z",
        "lastEditedBy" : "5e07d5ab-b4b3-41e4-beca-fefd635980c6",
        "tags" : [
        ]
      }
    ],
    "commit" : "1cf43b9b8b909baba486c004169c973fbbd0a18b",
    "line" : 5,
    "diffHunk" : "@@ -1,1 +32,36 @@import org.scalatest.concurrent.Eventually\n\nimport org.apache.spark.{FakeSchedulerBackend => _, _}\nimport org.apache.spark.internal.Logging\nimport org.apache.spark.internal.config"
  },
  {
    "id" : "d59f3c18-f8e2-4e99-bf20-f4bd2f67abed",
    "prId" : 28619,
    "prUrl" : "https://github.com/apache/spark/pull/28619#pullrequestreview-418426154",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4bb4abd1-bfa4-4f5c-8c26-b06f8b364f8b",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "It seems `startedTasks` is updated asynchronically by `FakeDAGScheduler`. Maybe we could use `eventually` to pretend the potential failure.",
        "createdAt" : "2020-05-26T05:45:15Z",
        "updatedAt" : "2020-06-15T14:14:24Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "cd323a70-4173-4325-a29f-c4a89c597448",
        "parentId" : "4bb4abd1-bfa4-4f5c-8c26-b06f8b364f8b",
        "authorId" : "ad4cb313-47b2-4cfa-9c1f-0a3596794a67",
        "body" : "`dagScheduler.taskStarted` is called as part of `resourceOffer` and the overridden taskStarted method of FakeDagScheduler appends it to `startedTasks` set. So it seems synchronus only?",
        "createdAt" : "2020-05-26T14:59:28Z",
        "updatedAt" : "2020-06-15T14:14:24Z",
        "lastEditedBy" : "ad4cb313-47b2-4cfa-9c1f-0a3596794a67",
        "tags" : [
        ]
      },
      {
        "id" : "adf5a33a-c9bb-43a1-b48e-b34b3e7f71f8",
        "parentId" : "4bb4abd1-bfa4-4f5c-8c26-b06f8b364f8b",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "I see. you're right.",
        "createdAt" : "2020-05-26T15:37:14Z",
        "updatedAt" : "2020-06-15T14:14:24Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "d87b311be85819ae884e2a24d94926fdd51165de",
    "line" : 35,
    "diffHunk" : "@@ -1,1 +1943,1947 @@    assert(taskOption3.get.executorId === \"exec2\")\n\n    assert(sched.startedTasks.toSet === Set(0, 1, 2, 3))\n\n    clock.advance(4*1000) // time = 10s"
  },
  {
    "id" : "c9aeb882-fdb8-48d7-a697-696253fd4e6a",
    "prId" : 28619,
    "prUrl" : "https://github.com/apache/spark/pull/28619#pullrequestreview-418386530",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "9ba3b1f9-d869-451e-a4f7-554b06685914",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Could you assert `tidToExecutorKillTimeMapping` here?",
        "createdAt" : "2020-05-26T05:46:48Z",
        "updatedAt" : "2020-06-15T14:14:24Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "15e7aec7-6bf7-48af-8925-70b2a14e2852",
        "parentId" : "9ba3b1f9-d869-451e-a4f7-554b06685914",
        "authorId" : "ad4cb313-47b2-4cfa-9c1f-0a3596794a67",
        "body" : "done.",
        "createdAt" : "2020-05-26T14:59:38Z",
        "updatedAt" : "2020-06-15T14:14:24Z",
        "lastEditedBy" : "ad4cb313-47b2-4cfa-9c1f-0a3596794a67",
        "tags" : [
        ]
      }
    ],
    "commit" : "d87b311be85819ae884e2a24d94926fdd51165de",
    "line" : 53,
    "diffHunk" : "@@ -1,1 +1961,1965 @@    // executorDecommissionSpeculationTriggerTimeoutOpt\n    // (TASK 2 -> 15, TASK 3 -> 15)\n    manager.executorDecommission(\"exec2\")\n    assert(manager.tidToExecutorKillTimeMapping.keySet === Set(2, 3))\n    assert(manager.tidToExecutorKillTimeMapping(2) === 15*1000)"
  },
  {
    "id" : "01834924-3b45-459b-9b18-e486c39cb19b",
    "prId" : 27116,
    "prUrl" : "https://github.com/apache/spark/pull/27116#pullrequestreview-339793690",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b0e28a89-b1f8-4964-952e-34a8063b29e1",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "how can we schedule the task before receiving resource offers?",
        "createdAt" : "2020-01-07T14:45:38Z",
        "updatedAt" : "2020-01-08T05:49:09Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "59f97c5a-b1f3-4014-aedf-a366f2cef9d0",
        "parentId" : "b0e28a89-b1f8-4964-952e-34a8063b29e1",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Because this test is kind of somehow hacky. Instead of using mock  schedulerBackend and taskScheduler, we use the real `CoarseGrainedSchedulerBackend` and `TaskSchedulerImpl` from a `SparkContext` due to the reason that we want use `reset()` within `CoarseGrainedSchedulerBackend`. As a result, when we call `sched.submitTasks(taskSet)`, it will schedule tasks immediately.\r\n\r\nI shall put this background in the comment, too.",
        "createdAt" : "2020-01-08T01:57:13Z",
        "updatedAt" : "2020-01-08T05:49:09Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "8fb098c6-661f-47c8-aae9-c9128fc2419b",
        "parentId" : "b0e28a89-b1f8-4964-952e-34a8063b29e1",
        "authorId" : "2c1bd8aa-c665-4159-a689-57743063347c",
        "body" : "But how does changing number of tasks from 2 to 4 fix this.? as per your comment here, https://github.com/apache/spark/pull/27115#discussion_r363722012 this still remains flaky. Please refer https://github.com/apache/spark/pull/27115 for proper fix instead",
        "createdAt" : "2020-01-08T02:29:59Z",
        "updatedAt" : "2020-01-08T05:49:09Z",
        "lastEditedBy" : "2c1bd8aa-c665-4159-a689-57743063347c",
        "tags" : [
        ]
      },
      {
        "id" : "da897d1d-bf4a-444f-94b1-012df13d18e9",
        "parentId" : "b0e28a89-b1f8-4964-952e-34a8063b29e1",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Because there's only two executors will be launched here. So, 2 tasks can be scheduled automatically, while other 2 tasks needs to be scheduled manually(a.k.a `resourceOffer`).",
        "createdAt" : "2020-01-08T02:37:43Z",
        "updatedAt" : "2020-01-08T05:49:09Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "bfb28ba0-500e-4b3a-9ae1-07637174cfb1",
        "parentId" : "b0e28a89-b1f8-4964-952e-34a8063b29e1",
        "authorId" : "2c1bd8aa-c665-4159-a689-57743063347c",
        "body" : "There is no guarantee that the first two will not finish before calling `resourceOffer`, this fix just reduces the window of the failure but doesn't fix it. To resolve this scenario https://github.com/apache/spark/pull/27115 uses tasks which do not finish.",
        "createdAt" : "2020-01-08T02:40:16Z",
        "updatedAt" : "2020-01-08T05:49:09Z",
        "lastEditedBy" : "2c1bd8aa-c665-4159-a689-57743063347c",
        "tags" : [
        ]
      },
      {
        "id" : "fcd3af4c-0912-4ea6-aafc-e3b1f3a483c1",
        "parentId" : "b0e28a89-b1f8-4964-952e-34a8063b29e1",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "You're right as this point and we shall workaround it by increasing revive interval of `CoarseGrainedSchedulerBackend`.\r\n\r\nAs for your fix, I think it's kinda overkill but nothing else.",
        "createdAt" : "2020-01-08T05:51:59Z",
        "updatedAt" : "2020-01-08T05:52:21Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "a2ce62c4-23ba-4166-95a0-797fbaebcbc5",
        "parentId" : "b0e28a89-b1f8-4964-952e-34a8063b29e1",
        "authorId" : "2c1bd8aa-c665-4159-a689-57743063347c",
        "body" : "I don't understand how is mine a overkill when actually what should matter is better approach for fix always. As i mentioned earlier, this PR duplicates development and review effort for https://github.com/apache/spark/pull/27115 then why still insist here.?",
        "createdAt" : "2020-01-08T11:07:46Z",
        "updatedAt" : "2020-01-08T11:07:46Z",
        "lastEditedBy" : "2c1bd8aa-c665-4159-a689-57743063347c",
        "tags" : [
        ]
      }
    ],
    "commit" : "b13e70594f3fd3a63bb903bd8b351e636ce565e2",
    "line" : 17,
    "diffHunk" : "@@ -1,1 +1925,1929 @@    // due to the reason that we want to use reset() of `CoarseGrainedSchedulerBackend`.\n    // So, it's possible that tasks have already been scheduled after we call\n    // `sched.submitTasks(taskSet)` but before `manager.resourceOffer`. In this case,\n    // we'd get None task after `resourceOffer`. So, here, we intentionally set up a\n    // 4 tasks TaskSet in order to feed 2 tasks to the executors and leave another two"
  },
  {
    "id" : "ef4ae9ec-968b-4720-b41f-3bf0d1824324",
    "prId" : 27115,
    "prUrl" : "https://github.com/apache/spark/pull/27115#pullrequestreview-339816185",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "b11cc640-05a9-410e-b4a1-a5e2e928ea74",
        "parentId" : null,
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "what can go wrong if we don't call `eventually`? The task is long-running now.",
        "createdAt" : "2020-01-08T11:40:29Z",
        "updatedAt" : "2020-01-08T16:54:16Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "05dd4fa1-c10a-4746-84bf-256960826e3c",
        "parentId" : "b11cc640-05a9-410e-b4a1-a5e2e928ea74",
        "authorId" : "2c1bd8aa-c665-4159-a689-57743063347c",
        "body" : "As `org.apache.spark.scheduler.TaskSetManager#taskInfos` will be filled in the actual scheduling cycle via `org.apache.spark.scheduler.TaskSetManager#resourceOffer` this eventual loops just waits for it to be scheduled",
        "createdAt" : "2020-01-08T11:55:17Z",
        "updatedAt" : "2020-01-08T16:54:16Z",
        "lastEditedBy" : "2c1bd8aa-c665-4159-a689-57743063347c",
        "tags" : [
        ]
      }
    ],
    "commit" : "b03d2055815814ff4ba8039a0493c637ba3e130d",
    "line" : 25,
    "diffHunk" : "@@ -1,1 +1930,1934 @@    val manager = sched.invokePrivate(taskSetManagers()).get(stageId).get(stageAttemptId)\n\n    val (task0, task1) = eventually(timeout(10.seconds), interval(100.milliseconds)) {\n      (manager.taskInfos(0), manager.taskInfos(1))\n    }"
  },
  {
    "id" : "c09ee938-53c0-477a-abec-7160524f1b35",
    "prId" : 27017,
    "prUrl" : "https://github.com/apache/spark/pull/27017#pullrequestreview-336895983",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "76bf6d26-d857-482f-8d9c-328f2718f1a9",
        "parentId" : null,
        "authorId" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "body" : "why do we still need this config?",
        "createdAt" : "2019-12-28T01:28:42Z",
        "updatedAt" : "2020-01-03T07:48:21Z",
        "lastEditedBy" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "tags" : [
        ]
      },
      {
        "id" : "3a312432-b5a4-42a3-82e4-aec8363e19a6",
        "parentId" : "76bf6d26-d857-482f-8d9c-328f2718f1a9",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "In order to create at most 2 executors at the beginning...Though, this may not necessary.. ",
        "createdAt" : "2019-12-28T01:40:20Z",
        "updatedAt" : "2020-01-03T07:48:21Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "4fdb7cb15b8e285919b26e97eba4ad65c5d970e0",
    "line" : 48,
    "diffHunk" : "@@ -1,1 +1911,1915 @@      .setMaster(\"local-cluster[2, 1, 2048]\")\n      // allow to set up at most two executors\n      .set(\"spark.cores.max\", \"2\")\n      .setAppName(\"CoarseGrainedSchedulerBackend.reset\")\n    sc = new SparkContext(conf)"
  },
  {
    "id" : "d7a597c4-0c2f-43b0-a7bb-8c0f5ef9620c",
    "prId" : 26614,
    "prUrl" : "https://github.com/apache/spark/pull/26614#pullrequestreview-328218546",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "25ed537c-8791-45a7-ae5e-d1c5ff699422",
        "parentId" : null,
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "it would be nice to add anther  test here that that test interaction of the speculative configs.   Meaning I have both the threshold set and the speculation quantile is smaller, the threshold can still apply and vice versa, the quantile can still apply.",
        "createdAt" : "2019-12-06T14:24:26Z",
        "updatedAt" : "2019-12-09T06:27:03Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "181bd8944814a4d21175272aeadad7603b3f639d",
    "line" : 80,
    "diffHunk" : "@@ -1,1 +1855,1859 @@      \"if there are too many tasks in the stage even though time threshold is provided\") {\n    testSpeculationDurationThreshold(true, 2, 1)\n  }\n\n  test(\"SPARK-29976 Regular speculation configs should still take effect even when a \" +"
  }
]