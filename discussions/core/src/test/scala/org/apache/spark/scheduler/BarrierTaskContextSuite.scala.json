[
  {
    "id" : "f16432ea-0f43-4a01-a2f5-08a4c678f58a",
    "prId" : 28839,
    "prUrl" : "https://github.com/apache/spark/pull/28839#pullrequestreview-432088741",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4de6a6dc-d78d-40fd-bb6a-d7649251f6e1",
        "parentId" : null,
        "authorId" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "body" : "Now that we have waited until all the executors has been launched before we submit any jobs, thus upon the first time we try to offer the resources to the pending tasks, we should expect all the executors are available and the locality preference should be satisfied, this shouldn't change following different locality wait time. cc @Ngone51 ",
        "createdAt" : "2020-06-17T05:58:54Z",
        "updatedAt" : "2020-06-17T06:00:08Z",
        "lastEditedBy" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "tags" : [
        ]
      },
      {
        "id" : "d3a481f4-eff4-4b08-a369-fa53825d271b",
        "parentId" : "4de6a6dc-d78d-40fd-bb6a-d7649251f6e1",
        "authorId" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "body" : "I can't remember how far we backported the fix from @Ngone51 , is it in branch 2.4?",
        "createdAt" : "2020-06-17T06:02:48Z",
        "updatedAt" : "2020-06-17T06:02:48Z",
        "lastEditedBy" : "b1f2cebb-db23-4759-b446-886279d07e99",
        "tags" : [
        ]
      },
      {
        "id" : "2d3943b2-39ac-47f6-8c10-f9a51e6bcd8b",
        "parentId" : "4de6a6dc-d78d-40fd-bb6a-d7649251f6e1",
        "authorId" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "body" : "The new delay scheduling update didn't go to 3.0 and previous versions, so this is a separated issue.",
        "createdAt" : "2020-06-17T06:18:50Z",
        "updatedAt" : "2020-06-17T06:18:50Z",
        "lastEditedBy" : "bc001dd4-5224-4ca1-afdd-49d69783d7f2",
        "tags" : [
        ]
      }
    ],
    "commit" : "70245934226748d5ecd34f77fe0372c148f5f2b5",
    "line" : 22,
    "diffHunk" : "@@ -1,1 +276,280 @@\n  test(\"SPARK-31485: barrier stage should fail if only partial tasks are launched\") {\n    val conf = new SparkConf().set(LOCALITY_WAIT_PROCESS.key, \"10s\")\n    initLocalClusterSparkContext(2, conf)\n    val rdd0 = sc.parallelize(Seq(0, 1, 2, 3), 2)"
  }
]