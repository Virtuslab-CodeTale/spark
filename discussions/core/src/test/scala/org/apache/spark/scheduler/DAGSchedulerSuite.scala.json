[
  {
    "id" : "119adb07-e764-461e-bbf6-8f6b30f9ef57",
    "prId" : 30691,
    "prUrl" : "https://github.com/apache/spark/pull/30691#pullrequestreview-671230749",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "86fd3a71-e740-4d18-85c0-089896e5d2e1",
        "parentId" : null,
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "Add a check here to ensure merge results were registered.\r\n`assert(mapOutputTracker.getNumAvailableMergeResults(shuffleDep.shuffleId) == parts.size)`",
        "createdAt" : "2021-05-28T12:07:29Z",
        "updatedAt" : "2021-05-28T12:07:29Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      }
    ],
    "commit" : "35d06150f37e01256314af4956c6e8fdcf7244b4",
    "line" : 341,
    "diffHunk" : "@@ -1,1 +3685,3689 @@      case (task, _) =>\n        (Success, makeMapStatus(\"hostA\", parts))\n    }.toSeq)\n\n    assert(mapOutputTracker.getNumAvailableMergeResults(shuffleDep.shuffleId) == parts)"
  },
  {
    "id" : "89f3ca07-e0cd-479b-a0c9-4e4455c2f646",
    "prId" : 28917,
    "prUrl" : "https://github.com/apache/spark/pull/28917#pullrequestreview-439653938",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "da0194b9-106a-4d09-b689-ba70230e1d22",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Shall we simulate the usage of `withSQLConf` instead of integrating the confs with `test()`?",
        "createdAt" : "2020-06-29T01:42:31Z",
        "updatedAt" : "2020-07-15T07:43:53Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "68fa0fc3-c18e-4453-a33d-fa443ac02363",
        "parentId" : "da0194b9-106a-4d09-b689-ba70230e1d22",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "Most SQL-related configuration parameters can be changed dynamically, but most of Core's parameters are static.",
        "createdAt" : "2020-06-30T04:01:52Z",
        "updatedAt" : "2020-07-15T07:43:53Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ec0d8d00b64662343dc6b3945dc5999343b699a7",
    "line" : 38,
    "diffHunk" : "@@ -1,1 +289,293 @@  }\n\n  private def testWithSparkConf(testName: String, testTags: Tag*)\n      (pairs: (String, String)*)(testFun: => Any)(implicit pos: Position): Unit = {\n    super.test(testName, testTags: _*) {"
  },
  {
    "id" : "40883614-148e-4175-a48d-cdfa01dd0522",
    "prId" : 28848,
    "prUrl" : "https://github.com/apache/spark/pull/28848#pullrequestreview-433763435",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0facbafb-5b19-43da-9e6e-9ffb5377e583",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "The test framework will call afterEach automatically. So you don't need to call it manually. ",
        "createdAt" : "2020-06-17T14:47:26Z",
        "updatedAt" : "2020-07-15T00:52:03Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "f85ed1a6-e23a-4889-96bd-24db8d9b8c15",
        "parentId" : "0facbafb-5b19-43da-9e6e-9ffb5377e583",
        "authorId" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "body" : "It is called by explicitly as the `init` which is called by the `beforeEach` was called with the `SHUFFLE_SERVICE_ENABLED` default config. So that one should be cleaned up and a new initialisation is needed see a few lines below.\r\n\r\nSome tests follow this pattern within this suite. ",
        "createdAt" : "2020-06-17T15:42:11Z",
        "updatedAt" : "2020-07-15T00:52:03Z",
        "lastEditedBy" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "tags" : [
        ]
      },
      {
        "id" : "8235625b-52bc-49e2-b911-4491c850fbee",
        "parentId" : "0facbafb-5b19-43da-9e6e-9ffb5377e583",
        "authorId" : "119f7711-a251-4127-b455-51a922a097f1",
        "body" : "Attila summed it up.",
        "createdAt" : "2020-06-17T16:33:49Z",
        "updatedAt" : "2020-07-15T00:52:03Z",
        "lastEditedBy" : "119f7711-a251-4127-b455-51a922a097f1",
        "tags" : [
        ]
      },
      {
        "id" : "1e4305dd-2baf-48a0-b8fb-7ef9ac26c453",
        "parentId" : "0facbafb-5b19-43da-9e6e-9ffb5377e583",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "I see, make sense.",
        "createdAt" : "2020-06-19T02:28:20Z",
        "updatedAt" : "2020-07-15T00:52:03Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "0e0086288f6279569e8a11cef9d928b87c40469b",
    "line" : 84,
    "diffHunk" : "@@ -1,1 +563,567 @@  test(\"SPARK-32003: All shuffle files for executor should be cleaned up on fetch failure\") {\n    // reset the test context with the right shuffle service config\n    afterEach()\n    val conf = new SparkConf()\n    conf.set(config.SHUFFLE_SERVICE_ENABLED.key, \"true\")"
  },
  {
    "id" : "6716f514-9999-4098-a3aa-de92efcc251d",
    "prId" : 28848,
    "prUrl" : "https://github.com/apache/spark/pull/28848#pullrequestreview-446080802",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bedbb87b-2100-450a-8ed5-449686591948",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "unnecessary change?",
        "createdAt" : "2020-07-09T02:27:49Z",
        "updatedAt" : "2020-07-15T00:52:03Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "4bba1bf1-6921-4378-9b3d-f104ecc361c7",
        "parentId" : "bedbb87b-2100-450a-8ed5-449686591948",
        "authorId" : "119f7711-a251-4127-b455-51a922a097f1",
        "body" : "@squito requested this change, to conform with what he said was the preferred scala style (map or flatMap with \"=>\" should use braces rather than parentheses).",
        "createdAt" : "2020-07-10T02:17:33Z",
        "updatedAt" : "2020-07-15T00:52:03Z",
        "lastEditedBy" : "119f7711-a251-4127-b455-51a922a097f1",
        "tags" : [
        ]
      }
    ],
    "commit" : "0e0086288f6279569e8a11cef9d928b87c40469b",
    "line" : 37,
    "diffHunk" : "@@ -1,1 +258,262 @@        _.asRDDId.map { id => (id.rddId -> id.splitIndex)\n        }.flatMap { key => cacheLocations.get(key)\n        }.getOrElse(Seq())\n      }.toIndexedSeq\n    }"
  },
  {
    "id" : "596efe16-bb00-4a41-a827-a8b0d6fe7875",
    "prId" : 28848,
    "prUrl" : "https://github.com/apache/spark/pull/28848#pullrequestreview-453397596",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8c34b3a0-6c2a-4ebc-bfaa-82ecd73c6a90",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "nit. Do we need to describe `when external shuffle service is used` assumption here because we have `conf.set(config.SHUFFLE_SERVICE_ENABLED.key, \"true\")` at line 567?",
        "createdAt" : "2020-07-22T06:03:33Z",
        "updatedAt" : "2020-07-22T06:06:16Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "a100b1ae-ecd6-461d-b494-dd3b659e30bc",
        "parentId" : "8c34b3a0-6c2a-4ebc-bfaa-82ecd73c6a90",
        "authorId" : "e913f9af-103d-40d4-976e-9aa8e12e7211",
        "body" : "personally, I'm OK with this as is, I think its OK for some of the details to be down in the test itself and balance a super-duper long name.\r\n\r\n@dongjoon-hyun since you called this a nit I'm assuming you're OK with me merging this anyhow, but if not lemme know, can submit a quick followup.",
        "createdAt" : "2020-07-22T14:52:41Z",
        "updatedAt" : "2020-07-22T14:52:41Z",
        "lastEditedBy" : "e913f9af-103d-40d4-976e-9aa8e12e7211",
        "tags" : [
        ]
      }
    ],
    "commit" : "0e0086288f6279569e8a11cef9d928b87c40469b",
    "line" : 82,
    "diffHunk" : "@@ -1,1 +561,565 @@  }\n\n  test(\"SPARK-32003: All shuffle files for executor should be cleaned up on fetch failure\") {\n    // reset the test context with the right shuffle service config\n    afterEach()"
  }
]