[
  {
    "id" : "119adb07-e764-461e-bbf6-8f6b30f9ef57",
    "prId" : 30691,
    "prUrl" : "https://github.com/apache/spark/pull/30691#pullrequestreview-671230749",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "86fd3a71-e740-4d18-85c0-089896e5d2e1",
        "parentId" : null,
        "authorId" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "body" : "Add a check here to ensure merge results were registered.\r\n`assert(mapOutputTracker.getNumAvailableMergeResults(shuffleDep.shuffleId) == parts.size)`",
        "createdAt" : "2021-05-28T12:07:29Z",
        "updatedAt" : "2021-05-28T12:07:29Z",
        "lastEditedBy" : "d35df420-57a9-43e8-b8d5-cad0f4002681",
        "tags" : [
        ]
      }
    ],
    "commit" : "35d06150f37e01256314af4956c6e8fdcf7244b4",
    "line" : 341,
    "diffHunk" : "@@ -1,1 +3685,3689 @@      case (task, _) =>\n        (Success, makeMapStatus(\"hostA\", parts))\n    }.toSeq)\n\n    assert(mapOutputTracker.getNumAvailableMergeResults(shuffleDep.shuffleId) == parts)"
  },
  {
    "id" : "89f3ca07-e0cd-479b-a0c9-4e4455c2f646",
    "prId" : 28917,
    "prUrl" : "https://github.com/apache/spark/pull/28917#pullrequestreview-439653938",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "da0194b9-106a-4d09-b689-ba70230e1d22",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Shall we simulate the usage of `withSQLConf` instead of integrating the confs with `test()`?",
        "createdAt" : "2020-06-29T01:42:31Z",
        "updatedAt" : "2020-07-15T07:43:53Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "68fa0fc3-c18e-4453-a33d-fa443ac02363",
        "parentId" : "da0194b9-106a-4d09-b689-ba70230e1d22",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "Most SQL-related configuration parameters can be changed dynamically, but most of Core's parameters are static.",
        "createdAt" : "2020-06-30T04:01:52Z",
        "updatedAt" : "2020-07-15T07:43:53Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "ec0d8d00b64662343dc6b3945dc5999343b699a7",
    "line" : 38,
    "diffHunk" : "@@ -1,1 +289,293 @@  }\n\n  private def testWithSparkConf(testName: String, testTags: Tag*)\n      (pairs: (String, String)*)(testFun: => Any)(implicit pos: Position): Unit = {\n    super.test(testName, testTags: _*) {"
  },
  {
    "id" : "40883614-148e-4175-a48d-cdfa01dd0522",
    "prId" : 28848,
    "prUrl" : "https://github.com/apache/spark/pull/28848#pullrequestreview-433763435",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "0facbafb-5b19-43da-9e6e-9ffb5377e583",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "The test framework will call afterEach automatically. So you don't need to call it manually. ",
        "createdAt" : "2020-06-17T14:47:26Z",
        "updatedAt" : "2020-07-15T00:52:03Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "f85ed1a6-e23a-4889-96bd-24db8d9b8c15",
        "parentId" : "0facbafb-5b19-43da-9e6e-9ffb5377e583",
        "authorId" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "body" : "It is called by explicitly as the `init` which is called by the `beforeEach` was called with the `SHUFFLE_SERVICE_ENABLED` default config. So that one should be cleaned up and a new initialisation is needed see a few lines below.\r\n\r\nSome tests follow this pattern within this suite. ",
        "createdAt" : "2020-06-17T15:42:11Z",
        "updatedAt" : "2020-07-15T00:52:03Z",
        "lastEditedBy" : "123cc67f-f678-49be-8742-95bc35d8c5c4",
        "tags" : [
        ]
      },
      {
        "id" : "8235625b-52bc-49e2-b911-4491c850fbee",
        "parentId" : "0facbafb-5b19-43da-9e6e-9ffb5377e583",
        "authorId" : "119f7711-a251-4127-b455-51a922a097f1",
        "body" : "Attila summed it up.",
        "createdAt" : "2020-06-17T16:33:49Z",
        "updatedAt" : "2020-07-15T00:52:03Z",
        "lastEditedBy" : "119f7711-a251-4127-b455-51a922a097f1",
        "tags" : [
        ]
      },
      {
        "id" : "1e4305dd-2baf-48a0-b8fb-7ef9ac26c453",
        "parentId" : "0facbafb-5b19-43da-9e6e-9ffb5377e583",
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "I see, make sense.",
        "createdAt" : "2020-06-19T02:28:20Z",
        "updatedAt" : "2020-07-15T00:52:03Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      }
    ],
    "commit" : "0e0086288f6279569e8a11cef9d928b87c40469b",
    "line" : 84,
    "diffHunk" : "@@ -1,1 +563,567 @@  test(\"SPARK-32003: All shuffle files for executor should be cleaned up on fetch failure\") {\n    // reset the test context with the right shuffle service config\n    afterEach()\n    val conf = new SparkConf()\n    conf.set(config.SHUFFLE_SERVICE_ENABLED.key, \"true\")"
  },
  {
    "id" : "6716f514-9999-4098-a3aa-de92efcc251d",
    "prId" : 28848,
    "prUrl" : "https://github.com/apache/spark/pull/28848#pullrequestreview-446080802",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "bedbb87b-2100-450a-8ed5-449686591948",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "unnecessary change?",
        "createdAt" : "2020-07-09T02:27:49Z",
        "updatedAt" : "2020-07-15T00:52:03Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "4bba1bf1-6921-4378-9b3d-f104ecc361c7",
        "parentId" : "bedbb87b-2100-450a-8ed5-449686591948",
        "authorId" : "119f7711-a251-4127-b455-51a922a097f1",
        "body" : "@squito requested this change, to conform with what he said was the preferred scala style (map or flatMap with \"=>\" should use braces rather than parentheses).",
        "createdAt" : "2020-07-10T02:17:33Z",
        "updatedAt" : "2020-07-15T00:52:03Z",
        "lastEditedBy" : "119f7711-a251-4127-b455-51a922a097f1",
        "tags" : [
        ]
      }
    ],
    "commit" : "0e0086288f6279569e8a11cef9d928b87c40469b",
    "line" : 37,
    "diffHunk" : "@@ -1,1 +258,262 @@        _.asRDDId.map { id => (id.rddId -> id.splitIndex)\n        }.flatMap { key => cacheLocations.get(key)\n        }.getOrElse(Seq())\n      }.toIndexedSeq\n    }"
  },
  {
    "id" : "596efe16-bb00-4a41-a827-a8b0d6fe7875",
    "prId" : 28848,
    "prUrl" : "https://github.com/apache/spark/pull/28848#pullrequestreview-453397596",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "8c34b3a0-6c2a-4ebc-bfaa-82ecd73c6a90",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "nit. Do we need to describe `when external shuffle service is used` assumption here because we have `conf.set(config.SHUFFLE_SERVICE_ENABLED.key, \"true\")` at line 567?",
        "createdAt" : "2020-07-22T06:03:33Z",
        "updatedAt" : "2020-07-22T06:06:16Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "a100b1ae-ecd6-461d-b494-dd3b659e30bc",
        "parentId" : "8c34b3a0-6c2a-4ebc-bfaa-82ecd73c6a90",
        "authorId" : "e913f9af-103d-40d4-976e-9aa8e12e7211",
        "body" : "personally, I'm OK with this as is, I think its OK for some of the details to be down in the test itself and balance a super-duper long name.\r\n\r\n@dongjoon-hyun since you called this a nit I'm assuming you're OK with me merging this anyhow, but if not lemme know, can submit a quick followup.",
        "createdAt" : "2020-07-22T14:52:41Z",
        "updatedAt" : "2020-07-22T14:52:41Z",
        "lastEditedBy" : "e913f9af-103d-40d4-976e-9aa8e12e7211",
        "tags" : [
        ]
      }
    ],
    "commit" : "0e0086288f6279569e8a11cef9d928b87c40469b",
    "line" : 82,
    "diffHunk" : "@@ -1,1 +561,565 @@  }\n\n  test(\"SPARK-32003: All shuffle files for executor should be cleaned up on fetch failure\") {\n    // reset the test context with the right shuffle service config\n    afterEach()"
  },
  {
    "id" : "c214a6ce-1d25-444d-bff7-daf7745da2b8",
    "prId" : 28641,
    "prUrl" : "https://github.com/apache/spark/pull/28641#pullrequestreview-424262214",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "d39414fe-b54c-41e2-8262-89b005615e81",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Well, there's another concern that while we're reusing the function in more places, it gets more complicated to use after adding many new arguments.",
        "createdAt" : "2020-06-02T15:47:13Z",
        "updatedAt" : "2020-06-12T08:53:15Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "902d91f7-828b-476e-a7d6-967fb7c99fb6",
        "parentId" : "d39414fe-b54c-41e2-8262-89b005615e81",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : "@Ngone51 Thanks! Only two test cases passed `maptaskids`. I will delete this parameter.",
        "createdAt" : "2020-06-04T08:10:27Z",
        "updatedAt" : "2020-06-12T08:53:15Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      },
      {
        "id" : "0db0d60e-613d-45f9-8cd8-75ce1de89686",
        "parentId" : "d39414fe-b54c-41e2-8262-89b005615e81",
        "authorId" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "body" : " Only two test cases passed `execIds`. I will delete this parameter too.",
        "createdAt" : "2020-06-04T09:25:59Z",
        "updatedAt" : "2020-06-12T08:53:15Z",
        "lastEditedBy" : "f26e51a2-ed90-47c7-9856-a6cc134b7f39",
        "tags" : [
        ]
      }
    ],
    "commit" : "681d9e5d14800a171b97e6d73a02ae6f14eba19d",
    "line" : 137,
    "diffHunk" : "@@ -1,1 +886,890 @@   * @param hostNames - Host on which each task in the task set is executed\n   */\n  private def completeShuffleMapStageSuccessfully(\n      stageId: Int,\n      attemptIdx: Int,"
  },
  {
    "id" : "34bae23f-6374-4ade-abfc-091f85423700",
    "prId" : 27773,
    "prUrl" : "https://github.com/apache/spark/pull/27773#pullrequestreview-377834649",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "4aabdfd1-45b6-4cb4-9b0e-4995c3bb7084",
        "parentId" : null,
        "authorId" : "171fe41b-96df-4362-a600-2d1f030de577",
        "body" : "Shall we add a test for shuffle stages together with resource profiles? ",
        "createdAt" : "2020-03-17T03:50:08Z",
        "updatedAt" : "2020-03-19T20:03:38Z",
        "lastEditedBy" : "171fe41b-96df-4362-a600-2d1f030de577",
        "tags" : [
        ]
      },
      {
        "id" : "2649967d-1389-4df3-ba71-8eb4f1ef7d20",
        "parentId" : "4aabdfd1-45b6-4cb4-9b0e-4995c3bb7084",
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "sure, I'll add one",
        "createdAt" : "2020-03-19T15:31:03Z",
        "updatedAt" : "2020-03-19T20:03:38Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "2a4a73385ee948d81e51091990a37ebbc8010f61",
    "line" : 95,
    "diffHunk" : "@@ -1,1 +3140,3144 @@    assert(error.contains(\"Multiple ResourceProfile's specified in the RDDs\"))\n  }\n\n  /**\n   * Checks the DAGScheduler's internal logic for traversing an RDD DAG by making sure that"
  },
  {
    "id" : "93675d5e-47cf-44c6-865d-a01d557e04ba",
    "prId" : 27050,
    "prUrl" : "https://github.com/apache/spark/pull/27050#pullrequestreview-346784825",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "c8f4df53-0924-4c5b-b94a-778264365954",
        "parentId" : null,
        "authorId" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "body" : "this isn't really testing that the stages got cleanup, right?  The activeJobs would have been empty without this fix because it gets cleaned up in cleanupStateForJobAndIndependentStages.. ie run this test without your fix, does the test still pass? \r\ncan we instead make sure SparkListenerStageCompleted was sent on the cancel",
        "createdAt" : "2020-01-22T17:53:22Z",
        "updatedAt" : "2020-02-17T14:36:35Z",
        "lastEditedBy" : "bea80117-7be3-4703-8d54-02b7bf73632c",
        "tags" : [
        ]
      }
    ],
    "commit" : "95cfbd3fc7d64609c636e5d81a2d2b402726512a",
    "line" : 45,
    "diffHunk" : "@@ -1,1 +1973,1977 @@    runEvent(makeCompletionEvent(taskSets(1).tasks(1), Success, 42))\n    assert(completedStage === List(0, 1, 1, 0))\n    assert(scheduler.activeJobs.isEmpty)\n  }\n"
  },
  {
    "id" : "7fbfb42b-3f3e-4400-ad14-cfefb28062b4",
    "prId" : 26206,
    "prUrl" : "https://github.com/apache/spark/pull/26206#pullrequestreview-306280473",
    "prSource" : "GitHub",
    "comments" : [
      {
        "id" : "2c9821a4-99d9-4e8b-a4d5-f11d5a4d74aa",
        "parentId" : null,
        "authorId" : "7694af3d-5af2-4788-8413-c0558915c452",
        "body" : "Shall we remove this commented code?",
        "createdAt" : "2019-10-23T18:38:14Z",
        "updatedAt" : "2019-10-24T01:30:23Z",
        "lastEditedBy" : "7694af3d-5af2-4788-8413-c0558915c452",
        "tags" : [
        ]
      },
      {
        "id" : "d467c05c-ea1b-4e3e-a427-388857f0aad6",
        "parentId" : "2c9821a4-99d9-4e8b-a4d5-f11d5a4d74aa",
        "authorId" : "31afe32d-3af0-4fcf-93e2-115f5d7bab18",
        "body" : "done.",
        "createdAt" : "2019-10-24T01:52:15Z",
        "updatedAt" : "2019-10-24T01:52:15Z",
        "lastEditedBy" : "31afe32d-3af0-4fcf-93e2-115f5d7bab18",
        "tags" : [
        ]
      }
    ],
    "commit" : "f23336dd808a196af8d36b7728e59af5dea9dced",
    "line" : 47,
    "diffHunk" : "@@ -1,1 +581,585 @@      // make sure our test setup is correct\n      val initialMapStatus1 = mapOutputTracker.shuffleStatuses(firstShuffleId).mapStatuses\n      //  val initialMapStatus1 = mapOutputTracker.mapStatuses.get(0).get\n      assert(initialMapStatus1.count(_ != null) === 3)\n      assert(initialMapStatus1.map {"
  }
]